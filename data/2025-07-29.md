<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 31]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 16]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 35]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 4]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 7]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.DS](#cs.DS) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 11]
- [cs.SD](#cs.SD) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [math.LO](#math.LO) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code](https://arxiv.org/abs/2507.19549)
*Nadeen Fathallah,Daniel Hernández,Steffen Staab*

Main category: cs.SE

TL;DR: 论文提出了一种自动化检测和修正网页无障碍性违规的新方法AccessGuru，结合现有测试工具和大语言模型（LLMs），通过新的分类法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大多数网页不符合无障碍性指南，影响了多样化用户的访问。自动化检测和修正可以降低人工成本并促进包容性。

Method: 提出Syntactic、Semantic和Layout三类分类法，结合现有工具和LLMs开发AccessGuru，并通过新基准评估。

Result: AccessGuru平均违规分数减少84%，显著优于之前方法的50%。

Conclusion: AccessGuru有效解决了网页无障碍性问题，提供了高性能的自动化解决方案。

Abstract: The vast majority of Web pages fail to comply with established Web
accessibility guidelines, excluding a range of users with diverse abilities
from interacting with their content. Making Web pages accessible to all users
requires dedicated expertise and additional manual efforts from Web page
providers. To lower their efforts and promote inclusiveness, we aim to
automatically detect and correct Web accessibility violations in HTML code.
While previous work has made progress in detecting certain types of
accessibility violations, the problem of automatically detecting and correcting
accessibility violations remains an open challenge that we address. We
introduce a novel taxonomy classifying Web accessibility violations into three
key categories - Syntactic, Semantic, and Layout. This taxonomy provides a
structured foundation for developing our detection and correction method and
redefining evaluation metrics. We propose a novel method, AccessGuru, which
combines existing accessibility testing tools and Large Language Models (LLMs)
to detect violations and applies taxonomy-driven prompting strategies to
correct all three categories. To evaluate these capabilities, we develop a
benchmark of real-world Web accessibility violations. Our benchmark quantifies
syntactic and layout compliance and judges semantic accuracy through
comparative analysis with human expert corrections. Evaluation against our
benchmark shows that AccessGuru achieves up to 84% average violation score
decrease, significantly outperforming prior methods that achieve at most 50%.

</details>


### [2] [LastMerge: A language-agnostic structured tool for code integration](https://arxiv.org/abs/2507.19687)
*Joao Pedro Duarte,Paulo Borba,Guilherme Cavalcanti*

Main category: cs.SE

TL;DR: 论文提出了一种通用的结构化合并工具LastMerge，并通过实验验证其在合并准确性和性能上与特定语言的工具相当。


<details>
  <summary>Details</summary>
Motivation: 尽管基于AST的结构化合并工具在合并准确性上表现出色，但由于语言特定性和高成本，实际应用较少。为了解决这一问题，研究旨在开发一种通用的结构化合并工具。

Method: 提出LastMerge，一种通过轻量接口配置的通用结构化合并工具，并与两种Java特定工具（jDime和Spork）及其通用对应工具（LastMerge和Mergiraf）进行实验比较。

Result: 实验结果显示，通用工具在合并准确性上与特定语言工具无显著差异，且运行时性能相当。LastMerge减少了15%的假阳性，Mergiraf减少了42%的假阴性。

Conclusion: 通用结构化合并工具可有效替代语言特定工具，推动其在工业中的广泛应用。

Abstract: Unstructured line-based merge tools are widely used in practice. Structured
AST-based merge tools show significantly improved merge accuracy, but are
rarely used in practice because they are language specific and costly,
consequently not being available for many programming languages. To improve
merge accuracy for a wide range of languages, we propose LastMerge, a generic
structured merge tool that can be configured through a thin interface that
significantly reduces the effort of supporting structured merge. To understand
the impact that generic structured merge might have on merge accuracy and
performance, we run an experiment with four structured merge tools: two Java
specific tools, jDime and Spork, and their generic counterparts, respectively
LastMerge and Mergiraf. Using each tool, we replay merge scenarios from a
significant dataset, and collect data on runtime, behavioral divergences, and
merge accuracy. Our results show no evidence that generic structured merge
significantly impacts merge accuracy. Although we observe a difference rate of
approximately 10% between the Java specific tools and their generic
counterparts, most of the differences stem from implementation details and
could be avoided. We find that LastMerge reports 15% fewer false positives than
jDime while Mergiraf misses 42% fewer false negatives than Spork. Both generic
tools exhibit comparable runtime performance to the state of the art language
specific implementations. These results suggest that generic structured merge
tools can effectively replace language-specific ones, paving the way for
broader adoption of structured merge in industry.

</details>


### [3] [Refactoring $\neq$ Bug-Inducing: Improving Defect Prediction with Code Change Tactics Analysis](https://arxiv.org/abs/2507.19714)
*Feifei Niu,Junqian Shao,Christoph Mayr-Dorn,Liguo Huang,Wesley K. G. Assunção,Chuanyi Li,Jidong Ge,Alexander Egyed*

Main category: cs.SE

TL;DR: 论文研究了代码重构及其传播对JIT缺陷预测的影响，提出CAT分析以改进标签准确性，并验证了忽视重构会降低模型性能，整合重构信息可显著提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 现有JIT缺陷预测研究多忽视代码重构，可能导致学习和评估偏差。

Method: 提出CAT分析分类代码重构及其传播，整合重构信息至六种基线方法。

Result: 忽视重构使模型性能下降18.6%-37.3%，整合重构信息后召回率和F1分数分别提升至43.2%和32.5%。

Conclusion: 重构信息对JIT-DP方法学和评估至关重要，CAT分析具有广泛适用性。

Abstract: Just-in-time defect prediction (JIT-DP) aims to predict the likelihood of
code changes resulting in software defects at an early stage. Although code
change metrics and semantic features have enhanced prediction accuracy, prior
research has largely ignored code refactoring during both the evaluation and
methodology phases, despite its prevalence. Refactoring and its propagation
often tangle with bug-fixing and bug-inducing changes within the same commit
and statement. Neglecting refactoring can introduce bias into the learning and
evaluation of JIT-DP models. To address this gap, we investigate the impact of
refactoring and its propagation on six state-of-the-art JIT-DP approaches. We
propose Code chAnge Tactics (CAT) analysis to categorize code refactoring and
its propagation, which improves labeling accuracy in the JIT-Defects4J dataset
by 13.7%. Our experiments reveal that failing to consider refactoring
information in the dataset can diminish the performance of models, particularly
semantic-based models, by 18.6% and 37.3% in F1-score. Additionally, we propose
integrating refactoring information to enhance six baseline approaches,
resulting in overall improvements in recall and F1-score, with increases of up
to 43.2% and 32.5%, respectively. Our research underscores the importance of
incorporating refactoring information in the methodology and evaluation of
JIT-DP. Furthermore, our CAT has broad applicability in analyzing refactoring
and its propagation for software maintenance.

</details>


### [4] [Clean Code In Practice: Challenges and Opportunities](https://arxiv.org/abs/2507.19721)
*Dapeng Yan,Wenjie Yang,Kui Liu,Zhiming Liu,Zhikuang Cai*

Main category: cs.SE

TL;DR: 这篇论文探讨了软件可靠性、安全性和保障性之间的相互作用，分析了行业中的关键指标和测量技术，提出了一个包含安全和保障方面的威胁评估框架，并建议将可靠性指标与安全和保障考虑结合以增强系统健壮性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的复杂性要求更深入地理解可靠性指标如何与安全和保障问题相互作用，以预测和提高系统可靠性。

Method: 论文分析行业中的关键指标和测量技术，并提出一个威胁评估框架，结合安全和保障因素。

Result: 研究发现，将可靠性指标与安全和保障考虑结合可以增强软件的健壮性。

Conclusion: 论文提出了一套可操作的指南，帮助从业者在改进可靠性预测模型的同时，应对当代软件应用的安全和保障挑战。

Abstract: Reliability prediction is crucial for ensuring the safety and security of
software systems, especially in the context of industry practices. While
various metrics and measurements are employed to assess software reliability,
the complexity of modern systems necessitates a deeper understanding of how
these metrics interact with security and safety concerns. This paper explores
the interplay between software reliability, safety, and security, offering a
comprehensive analysis of key metrics and measurement techniques used in the
industry for reliability prediction. We identify critical threats to software
reliability and provide a threat estimation framework that incorporates both
safety and security aspects. Our findings suggest that integrating reliability
metrics with safety and security considerations can enhance the robustness of
software systems. Furthermore, we propose a set of actionable guidelines for
practitioners to improve their reliability prediction models while
simultaneously addressing the security and safety challenges of contemporary
software applications.

</details>


### [5] [Defining ethically sourced code generation](https://arxiv.org/abs/2507.19743)
*Zhuolin Xu,Chenglin Li,Qiushi Li,Shin Hwei Tan*

Main category: cs.SE

TL;DR: 论文提出了一种名为ES-CodeGen的新概念，指通过道德和可持续实践管理代码生成模型开发的全过程，并通过文献综述和调查提炼了11个ES-CodeGen维度。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决代码生成模型中涉及的伦理问题，如许可、隐私、公平性和环境影响，确保代码生成的道德性。

Method: 方法包括两阶段文献综述（分析了803篇论文）和对32名从业者的调查，以确定和细化ES-CodeGen的维度。

Result: 结果提炼出11个ES-CodeGen维度，其中代码质量是新发现的维度，并揭示了相关后果、工件和阶段。

Conclusion: 研究呼吁关注ES-CodeGen中的伦理问题，并发现从业者常忽视社会相关维度，但调查有助于提升其理解。

Abstract: Several code generation models have been proposed to help reduce time and
effort in solving software-related tasks. To ensure responsible AI, there are
growing interests over various ethical issues (e.g., unclear licensing,
privacy, fairness, and environment impact). These studies have the overarching
goal of ensuring ethically sourced generation, which has gained growing
attentions in speech synthesis and image generation. In this paper, we
introduce the novel notion of Ethically Sourced Code Generation (ES-CodeGen) to
refer to managing all processes involved in code generation model development
from data collection to post-deployment via ethical and sustainable practices.
To build a taxonomy of ES-CodeGen, we perform a two-phase literature review
where we read 803 papers across various domains and specific to AI-based code
generation. We identified 71 relevant papers with 10 initial dimensions of
ES-CodeGen. To refine our dimensions and gain insights on consequences of
ES-CodeGen, we surveyed 32 practitioners, which include six developers who
submitted GitHub issues to opt-out from the Stack dataset (these impacted users
have real-world experience of ethically sourcing issues in code generation
models). The results lead to 11 dimensions of ES-CodeGen with a new dimension
on code quality as practitioners have noted its importance. We also identified
consequences, artifacts, and stages relevant to ES-CodeGen. Our post-survey
reflection showed that most practitioners tend to ignore social-related
dimensions despite their importance. Most practitioners either agreed or
strongly agreed that our survey help improve their understanding of ES-CodeGen.
Our study calls for attentions of various ethical issues towards ES-CodeGen.

</details>


### [6] [From Few-Label to Zero-Label: An Approach for Cross-System Log-Based Anomaly Detection with Meta-Learning](https://arxiv.org/abs/2507.19806)
*Xinlong Zhao,Tong Jia,Minghua He,Yihan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: 论文提出了一种名为FreeLog的系统无关表示元学习方法，用于零标签跨系统日志异常检测，无需目标系统的标记日志，性能可比依赖少量标记数据的方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨系统日志异常检测方法依赖于目标系统的少量标记日志，限制了其在实际应用中的适用性。本文旨在解决零标签条件下的跨系统日志异常检测问题。

Method: 提出了FreeLog，一种系统无关的表示元学习方法，无需目标系统的标记日志即可实现跨系统日志异常检测。

Result: 在三个公开日志数据集上的实验结果表明，FreeLog的性能与依赖少量目标系统标记数据的方法相当。

Conclusion: FreeLog为零标签条件下的跨系统日志异常检测提供了一种有效的解决方案。

Abstract: Log anomaly detection plays a critical role in ensuring the stability and
reliability of software systems. However, existing approaches rely on large
amounts of labeled log data, which poses significant challenges in real-world
applications. To address this issue, cross-system transfer has been identified
as a key research direction. State-of-the-art cross-system approaches achieve
promising performance with only a few labels from the target system. However,
their reliance on labeled target logs makes them susceptible to the cold-start
problem when labeled logs are insufficient. To overcome this limitation, we
explore a novel yet underexplored setting: zero-label cross-system log anomaly
detection, where the target system logs are entirely unlabeled. To this end, we
propose FreeLog, a system-agnostic representation meta-learning method that
eliminates the need for labeled target system logs, enabling cross-system log
anomaly detection under zero-label conditions. Experimental results on three
public log datasets demonstrate that FreeLog achieves performance comparable to
state-of-the-art methods that rely on a small amount of labeled data from the
target system.

</details>


### [7] [A Cooperative Approach for Knowledge-based Business Process Design in a Public Authority](https://arxiv.org/abs/2507.19842)
*Mohammad Azarijafari,Luisa Mich,Michele Missikoff,Oleg Missikoff*

Main category: cs.SE

TL;DR: 该论文提出了一种基于知识的方法，旨在支持业务专家设计业务流程，无需先前的知识工程经验。


<details>
  <summary>Details</summary>
Motivation: 由于数字化转型的紧迫性，企业尤其是中小企业需要调整组织结构和运营方式，而流程导向的生产模型是关键创新方向。

Method: 采用知识驱动的方法，从简单的文本知识构件入手，逐步构建更结构化的知识库，并通过结构化步骤生成目标流程的图表化工作流。

Result: 该方法为所有参与业务流程设计的利益相关者提供了一种共享的方法。

Conclusion: 论文提出的方法支持企业，尤其是中小企业，在数字化转型中高效设计业务流程。

Abstract: Enterprises are currently undergoing profound transformations due to the
unpostponable digital transformation. Then, to remain competitive, enterprises
must adapt their organisational structures and operations. This organisational
shift is also important for small and medium-sized enterprises. A key
innovation frontier is the adoption of process-oriented production models. This
paper presents a knowledge-based method to support business experts in
designing business processes. The method requires no prior expertise in
Knowledge Engineering and guides designers through a structured sequence of
steps to produce a diagrammatic workflow of the target process. The
construction of the knowledge base starts from simple, text-based, knowledge
artefacts and then progresses towards more structured, formal representations.
The approach has been conceived to allow a shared approach for all stakeholders
and actors who participate in the BP design.

</details>


### [8] [LLM-Based Repair of Static Nullability Errors](https://arxiv.org/abs/2507.20674)
*Nima Karimipour,Michael Pradel,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: NullRepair是一种利用大型语言模型（LLM）自动修复Java代码中空指针异常错误的系统，通过结构化工作流程和上下文示例，提高了修复准确性和程序语义保留能力。


<details>
  <summary>Details</summary>
Motivation: 大规模Java项目中静态分析工具虽然能预防空指针异常，但部分错误仍需人工修复，效率低且易出错。LLM有潜力自动化修复，但缺乏上下文理解。

Method: NullRepair结合静态分析和LLM，通过结构化流程图决策，利用错误示例上下文化提示，迭代生成修复补丁。

Result: 在12个Java项目中，NullRepair平均修复了72%的残留错误，且修复后10/12项目的单元测试全部通过，其余项目98%以上测试通过。

Conclusion: NullRepair通过上下文驱动的LLM集成，显著提升了空指针错误的自动化修复效率和准确性。

Abstract: Modern Java projects increasingly adopt static analysis tools that prevent
null-pointer exceptions by treating nullness as a type property. However,
integrating such tools into large, existing codebases remains a significant
challenge. While annotation inference can eliminate many errors automatically,
a subset of residual errors -- typically a mix of real bugs and false positives
-- often persist and can only be resolved via code changes. Manually addressing
these errors is tedious and error-prone. Large language models (LLMs) offer a
promising path toward automating these repairs, but naively-prompted LLMs often
generate incorrect, contextually-inappropriate edits. Resolving a nullability
error demands a deep understanding of how a symbol is used across the codebase,
often spanning methods, classes, and packages. We present NullRepair, a system
that integrates LLMs into a structured workflow for resolving the errors from a
nullability checker. NullRepair's decision process follows a flowchart derived
from manual analysis of 200 real-world errors. It leverages static analysis to
identify safe and unsafe usage regions of symbols, using error-free usage
examples to contextualize model prompts. Patches are generated through an
iterative interaction with the LLM that incorporates project-wide context and
decision logic. Our evaluation on 12 real-world Java projects shows that
NullRepair resolves an average of 72% of the errors that remain after applying
a state-of-the-art annotation inference technique. Unlike a naively-prompted
LLM, NullRepair also largely preserves program semantics, with all unit tests
passing in 10/12 projects after applying every edit proposed by NullRepair, and
98% or more tests passing in the remaining two projects.

</details>


### [9] [AgentMesh: A Cooperative Multi-Agent Generative AI Framework for Software Development Automation](https://arxiv.org/abs/2507.19902)
*Sourena Khanzadeh*

Main category: cs.SE

TL;DR: 论文提出了一个基于Python的多智能体框架AgentMesh，通过协作的LLM驱动智能体（Planner、Coder、Debugger和Reviewer）自动化软件开发任务。


<details>
  <summary>Details</summary>
Motivation: 传统软件开发需要多领域专家协作，过程复杂且耗时，作者希望通过多智能体框架实现自动化软件开发，提升效率。

Method: AgentMesh由四个智能体组成：Planner分解任务、Coder实现代码、Debugger测试和修复、Reviewer验证代码质量。论文详细描述了架构、设计和实现细节。

Result: 通过案例研究展示了AgentMesh处理复杂开发任务的能力，证明了多智能体协作可以发挥LLM的优势并弥补单智能体的局限性。

Conclusion: 论文总结了当前局限（如错误传播和上下文扩展），并提出了未来研究方向，以实现更强大、可扩展的多智能体AI系统。

Abstract: Software development is a complex, multi-phase process traditionally
requiring collaboration among individuals with diverse expertise. We propose
AgentMesh, a Python-based framework that uses multiple cooperating LLM-powered
agents to automate software development tasks. In AgentMesh, specialized agents
- a Planner, Coder, Debugger, and Reviewer - work in concert to transform a
high-level requirement into fully realized code. The Planner agent first
decomposes user requests into concrete subtasks; the Coder agent implements
each subtask in code; the Debugger agent tests and fixes the code; and the
Reviewer agent validates the final output for correctness and quality. We
describe the architecture and design of these agents and their communication,
and provide implementation details including prompt strategies and workflow
orchestration. A case study illustrates AgentMesh handling a non-trivial
development request via sequential task planning, code generation, iterative
debugging, and final code review. We discuss how dividing responsibilities
among cooperative agents leverages the strengths of large language models while
mitigating single-agent limitations. Finally, we examine current limitations -
such as error propagation and context scaling - and outline future work toward
more robust, scalable multi-agent AI systems for software engineering
automation.

</details>


### [10] [CrossPL: Evaluating Large Language Models on Cross Programming Language Code Generation](https://arxiv.org/abs/2507.19904)
*Zhanhang Xiong,Dongxia Wang,Yuekang Li,Xinyuan An,Wenhai Wang*

Main category: cs.SE

TL;DR: CrossPL是首个系统性评估大语言模型（LLMs）生成跨编程语言（CPL）互操作代码能力的基准测试，涵盖6种语言和7种技术。即使最佳模型在CPL场景中表现不佳，凸显了针对性研究的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件工程中的应用增加，生成跨编程语言互操作的正确代码成为关键能力，但目前研究不足。

Method: 通过分析19,169个多语言GitHub仓库和156个有限状态机（FSMs），构建了包含1,982个任务的CrossPL基准，并使用FSM验证功能性。

Result: 评估14种通用LLMs和6种代码专用LLMs显示，即使最佳模型在CPL场景下表现不佳。

Conclusion: CrossPL揭示了LLMs在CPL代码生成上的不足，呼吁更多针对性研究。

Abstract: As large language models (LLMs) become increasingly embedded in software
engineering workflows, a critical capability remains underexplored: generating
correct code that enables cross-programming-language (CPL) interoperability.
This skill is essential for building complex systems that integrate components
written in multiple languages via mechanisms like inter-process communication
(IPC). To bridge this gap, we present CrossPL, the first benchmark designed to
systematically evaluate LLMs' ability to generate CPL-interoperating code.
CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used
programming languages and seven representative CPL techniques. We construct
this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using
156 hand-crafted finite state machines (FSMs), and (ii) developing an LLM-based
pipeline that automatically extracts CPL code snippets, generates task
instructions, and validates functional correctness. We evaluate 14
state-of-the-art general-purpose LLMs and 6 code-oriented LLMs released in the
past three years on CrossPL via FSM-based validation. Results reveal that even
the best-performing models struggle with CPL scenarios, underscoring the need
for more targeted research in this space. Our benchmark and code are available
at: https://anonymous.4open.science/r/crosspl-2814.

</details>


### [11] [The Impact of Fine-tuning Large Language Models on Automated Program Repair](https://arxiv.org/abs/2507.19909)
*Roman Macháček,Anastasiia Grishina,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 该研究探讨了不同微调技术对用于自动程序修复（APR）的大型语言模型（LLM）性能的影响，发现参数高效微调（PEFT）比全微调表现更好。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在自动程序修复（APR）中表现出色，但训练资源消耗巨大。为此，研究如何通过微调技术提升LLM在APR中的性能，并降低成本。

Method: 研究评估了六种预训练的LLM（如CodeGen、CodeT5等），在三个APR基准测试（QuixBugs、Defects4J、HumanEval-Java）上，对比了无微调、全微调和参数高效微调（LoRA、IA3）的效果。

Result: 全微调技术因数据分布差异和过拟合导致性能下降，而参数高效微调通过限制可训练参数量，获得了更好的结果。

Conclusion: 参数高效微调是提升LLM在APR任务中性能的有效方法，且能显著降低计算成本。

Abstract: Automated Program Repair (APR) uses various tools and techniques to help
developers achieve functional and error-free code faster. In recent years,
Large Language Models (LLMs) have gained popularity as components in APR tool
chains because of their performance and flexibility. However, training such
models requires a significant amount of resources. Fine-tuning techniques have
been developed to adapt pre-trained LLMs to specific tasks, such as APR, and
enhance their performance at far lower computational costs than training from
scratch. In this study, we empirically investigate the impact of various
fine-tuning techniques on the performance of LLMs used for APR. Our experiments
provide insights into the performance of a selection of state-of-the-art LLMs
pre-trained on code. The evaluation is done on three popular APR benchmarks
(i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs
with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder,
Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning,
full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and
IA3. We observe that full fine-tuning techniques decrease the benchmarking
performance of various models due to different data distributions and
overfitting. By using parameter-efficient fine-tuning methods, we restrict
models in the amount of trainable parameters and achieve better results.
  Keywords: large language models, automated program repair,
parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.

</details>


### [12] [Prometheus: Unified Knowledge Graphs for Issue Resolution in Multilingual Codebases](https://arxiv.org/abs/2507.19942)
*Zimin Chen,Yue Pan,Siyu Lu,Jiayi Xu,Claire Le Goues,Martin Monperrus,He Ye*

Main category: cs.SE

TL;DR: Prometheus是一个多代理系统，通过将代码仓库转换为统一知识图谱来解决多语言仓库中的问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型代理（如SWE-agent和OpenHands）主要局限于Python问题，且依赖预构建的容器，适用性受限。Prometheus旨在解决这一限制，扩展至多语言和真实场景。

Method: Prometheus将代码仓库转换为知识图谱，支持多语言，并使用Neo4j进行持久化。结合DeepSeek-V3模型，实现结构化推理。

Result: 在SWE-bench Lite和SWE-bench Multilingual上分别解决28.67%和13.7%的问题，平均API成本为每问题$0.23和$0.38，并能解决7种语言的独特问题。

Conclusion: Prometheus在真实多语言仓库中表现出色，解决了现有方法未覆盖的问题，并已开源。

Abstract: Language model (LM) agents, such as SWE-agent and OpenHands, have made
progress toward automated issue resolution. However, existing approaches are
often limited to Python-only issues and rely on pre-constructed containers in
SWE-bench with reproduced issues, restricting their applicability to real-world
and work for multi-language repositories. We present Prometheus, designed to
resolve real-world issues beyond benchmark settings. Prometheus is a
multi-agent system that transforms an entire code repository into a unified
knowledge graph to guide context retrieval for issue resolution. Prometheus
encodes files, abstract syntax trees, and natural language text into a graph of
typed nodes and five general edge types to support multiple programming
languages. Prometheus uses Neo4j for graph persistence, enabling scalable and
structured reasoning over large codebases. Integrated by the DeepSeek-V3 model,
Prometheus resolves 28.67% and 13.7% of issues on SWE-bench Lite and SWE-bench
Multilingual, respectively, with an average API cost of $0.23 and $0.38 per
issue. Prometheus resolves 10 unique issues not addressed by prior work and is
the first to demonstrate effectiveness across seven programming languages.
Moreover, it shows the ability to resolve real-world GitHub issues in the
LangChain and OpenHands repositories. We have open-sourced Prometheus at:
https://github.com/Pantheon-temple/Prometheus

</details>


### [13] [PDLogger: Automated Logging Framework for Practical Software Development](https://arxiv.org/abs/2507.19951)
*Shengcheng Duan,Yihua Xu,Sheng Zhang,Shen Wang,Yue Duan*

Main category: cs.SE

TL;DR: PDLogger是一种端到端的日志生成技术，专为实际多日志场景设计，通过三阶段方法显著提升日志质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化日志技术仅关注单一子任务（如预测日志位置、级别或消息），无法生成完整高质量的日志，且忽视语义依赖和变量选择。

Method: PDLogger分三个阶段：日志位置预测（基于块类型的结构化提示）、日志生成（结合程序切片扩展变量提取）、日志细化（级别修正和去重）。

Result: PDLogger在日志位置精确度、F1分数、级别准确度、变量精确度和消息质量上分别提升139.0%、69.2%、82.3%、131.8%和65.7%。

Conclusion: PDLogger展示了鲁棒性和通用性，其开源实现有望推动未来研究和应用。

Abstract: Logging is indispensable for maintaining the reliability and diagnosability
of modern software, yet developers still struggle to decide where and how to
log effectively. Existing automated logging techniques focus on isolated
sub-tasks - predicting a single log position, level, or message - and therefore
cannot produce complete, high-quality log statements that reflect real-world
practice in which multiple logs often appear inside one method. They also
neglect deeper semantic dependencies among methods and consider only a narrow
set of candidate variables, leading to superficial or incomplete logs. In this
paper, we present PDLogger, the first end-to-end log generation technique
expressly designed for practical, multi-log scenarios. PDLogger operates in
three phases. (1) Log position prediction: block-type-aware structured prompts
guide a large language model (LLM) to suggest candidate positions across all
control-flow blocks of a method. (2) Log generation: backward program slicing
supplies precise inter-procedural control and data-dependency context, while an
expanded variable extractor captures both member and external function
expressions; the enriched prompt enables the LLM to emit a full log statement
(position, level, message, variables). (3) Log refinement: level correction and
context-sensitive deduplication prune false positives and redundant logs. We
evaluate PDLogger on 3,113 log statements drawn from two widely used Java
projects. Compared with the strongest prior systems, PDLogger improves
log-position precision by 139.0 percent, F1 by 69.2 percent, level accuracy by
82.3 percent, variable precision by 131.8 percent, and message quality
(BERTScore) by 65.7 percent. The framework consistently performs well with
different mainstream LLMs, demonstrating robustness and generality. PDLogger's
implementation is available as open source to foster future research and
adoption.

</details>


### [14] [The Effect of Pointer Analysis on Semantic Conflict Detection](https://arxiv.org/abs/2507.20081)
*Matheus Barbosa,Paulo Borba,Rodrigo Bonifácio,Victor Lira,Galileu Santos*

Main category: cs.SE

TL;DR: 当前合并工具无法检测语义冲突，研究探讨静态分析中指针分析对减少误报的效果，发现指针分析虽降低误报但增加漏报，需探索混合分析方法。


<details>
  <summary>Details</summary>
Motivation: 现有合并工具无法识别语义冲突，静态分析误报率高，研究旨在通过指针分析改善这一问题。

Method: 实现带与不带指针分析的两种静态分析方法，比较它们在数据集上的准确性、计算性能及差异表现。

Result: 指针分析大幅减少误报和超时，但显著增加漏报，召回率和F1分数下降严重。

Conclusion: 语义冲突检测需探索结合指针分析与无指针分析的混合技术。

Abstract: Current merge tools don't detect semantic conflicts, which occur when changes
from different developers are textually integrated but semantically interfere
with each other. Although researchers have proposed static analyses for
detecting semantic conflicts, these analyses suffer from significant false
positive rates. To understand whether such false positives could be reduced by
using pointer analysis in the implementation of semantic conflict static
analyses, we conduct an empirical study. We implement the same analysis with
and without pointer analysis, run them on two datasets, observe how often they
differ, and compare their accuracy and computational performance. Although
pointer analysis is known to improve precision in static analysis, we find that
its effect on semantic conflict detection can be drastic: we observe a
significant reduction in timeouts and false positives, but also a significant
increase in false negatives, with prohibitive drops in recall and F1-score.
These results suggest that, in the context of semantic conflict detection, we
should explore hybrid analysis techniques, combining aspects of both
implementations we compare in our study.

</details>


### [15] [From First Use to Final Commit: Studying the Evolution of Multi-CI Service Adoption](https://arxiv.org/abs/2507.20095)
*Nitika Chopra,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 研究分析了18924个Java项目的多CI服务使用情况，发现近1/5项目会同时或替换使用多种CI服务，凸显了CI环境选择和迁移的工具需求。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常孤立分析单个CI服务，而多服务的使用和迁移情况尚不明确。

Method: 对2008至2024年间使用至少一种CI服务的Java项目进行历史分析。

Result: 多CI服务使用是普遍现象，常涉及服务迁移。

Conclusion: 未来需支持CI服务的选择、协调和迁移工具。

Abstract: Continuous Integration (CI) services, such as GitHub Actions and Travis CI,
are widely adopted in open-source development to automate testing and
deployment. Though existing research often examines individual services in
isolation, it remains unclear how projects adopt and transition between
multiple services over time. To understand how CI adoption is evolving across
services, we present a preliminary study analyzing the historical CI adoption
of 18,924 Java projects hosted on GitHub between January 2008 and December
2024, adopting at least one of eight CI services, namely Travis CI, AppVeyor,
CircleCI, Azure Pipelines, GitHub Actions, Bitbucket, GitLab CI, and Cirrus CI.
Specifically, we investigate: (1) how frequently CI services are co-adopted or
replaced, and (2) how maintenance activity varies across different services.
Our analysis shows that the use of multiple CI services within the same project
is a recurring pattern observed in nearly one in five projects, often
reflecting migration across CI services. Our study is among the first to
examine multi-CI adoption in practice, offering new insights for future
research and highlighting the need for strategies and tools to support service
selection, coordination, and migration in evolving CI environments.

</details>


### [16] [Search-Based Fuzzing For RESTful APIs That Use MongoDB](https://arxiv.org/abs/2507.20848)
*Hernan Ghianni,Man Zhang,Juan P. Galeotti,Andrea Arcuri*

Main category: cs.SE

TL;DR: 论文提出了一种新颖的技术，通过动态分析NoSQL数据库状态并直接从测试用例插入数据，显著提高了RESTful API的测试覆盖率和故障发现率。


<details>
  <summary>Details</summary>
Motivation: 在RESTful API测试中，数据库状态的考虑对于提高代码覆盖率和发现隐藏故障至关重要。现有方法在这方面存在不足。

Method: 动态分析MongoDB数据库状态，并通过测试用例直接插入NoSQL数据。方法作为EvoMaster工具的扩展实现。

Result: 在六个RESTful API上的实验显示，代码覆盖率提升了高达18%，优于现有白盒和黑盒测试工具。

Conclusion: 该技术显著提升了测试效果，尤其适用于测试读密集型微服务和复杂数据库状态的场景。

Abstract: In RESTful APIs, interactions with a database are a common and crucial
aspect. When generating whitebox tests, it is essential to consider the
database's state (i.e., the data contained in the database) to achieve higher
code coverage and uncover more hidden faults. This article presents novel
techniques to enhance search-based software test generation for RESTful APIs
interacting with NoSQL databases. Specifically, we target the popular MongoDB
database, by dynamically analyzing (via automated code instrumentation) the
state of the database during the test generation process. Additionally, to
achieve better results, our novel approach allows inserting NoSQL data directly
from test cases. This is particularly beneficial when generating the correct
sequence of events to set the NoSQL database in an appropriate state is
challenging or time-consuming. This method is also advantageous for testing
read-only microservices. Our novel techniques are implemented as an extension
of EvoMaster, the only open-source tool for white-box fuzzing RESTful APIs.
Experiments conducted on six RESTful APIs demonstrated significant improvements
in code coverage, with increases of up to 18% compared to existing white-box
approaches. To better highlight the improvements of our novel techniques,
comparisons are also carried out with four state-of-the-art black-box fuzzers.

</details>


### [17] [Learning to Align Human Code Preferences](https://arxiv.org/abs/2507.20109)
*Xin Yin,Chao Ni,Liushan Chen,Xiaohu Yang*

Main category: cs.SE

TL;DR: 该论文研究了在代码偏好对齐中，监督微调(SFT)和直接偏好优化(DPO)的作用，提出了一种自适应偏好优化(APO)方法，通过动态整合策略提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索在不同代码偏好场景下，如何最优地使用SFT和DPO来对齐大语言模型（LLMs）与人类偏好。

Method: 通过理论分析和实证观察，提出APO方法，动态整合SFT和DPO的优势。

Result: 在六种代表性代码偏好任务中，APO的性能与现有SFT和S&D策略相当或更优。

Conclusion: APO为不同代码偏好对齐场景提供了理论基础和实践指导。

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in
automating software development tasks. While recent advances leverage
Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align
models with human preferences, the optimal training strategy remains unclear
across diverse code preference scenarios. This paper systematically
investigates the roles of SFT and DPO in aligning LLMs with different code
preferences. Through both theoretical analysis and empirical observation, we
hypothesize that SFT excels in scenarios with objectively verifiable optimal
solutions, while applying SFT followed by DPO (S&D) enables models to explore
superior solutions in scenarios without objectively verifiable optimal
solutions. Based on the analysis and experimental evidence, we propose Adaptive
Preference Optimization (APO), a dynamic integration approach that adaptively
amplifies preferred responses, suppresses dispreferred ones, and encourages
exploration of potentially superior solutions during training. Extensive
experiments across six representative code preference tasks validate our
theoretical hypotheses and demonstrate that APO consistently matches or
surpasses the performance of existing SFT and S&D strategies. Our work provides
both theoretical foundations and practical guidance for selecting appropriate
training strategies in different code preference alignment scenarios.

</details>


### [18] [From Prompt to Pipeline: Large Language Models for Scientific Workflow Development in Bioinformatics](https://arxiv.org/abs/2507.20122)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 现代大型语言模型（如GPT-4o、Gemini 2.5 Flash和DeepSeek-V3）能够支持生成准确、完整且可用的生物信息学工作流，且不同模型在不同平台（如Galaxy和Nextflow）上表现各异。提示策略对质量有显著影响。


<details>
  <summary>Details</summary>
Motivation: 解决生物信息学领域专家在缺乏编程技能的情况下创建和理解复杂工作流的挑战，利用大型语言模型降低工作流开发门槛。

Method: 通过评估GPT-4o、Gemini 2.5 Flash和DeepSeek-V3在多种任务（如SNP分析、RNA-seq等）上的表现，并使用专家评审对比社区基准。

Result: Gemini 2.5 Flash在Galaxy工作流生成中表现优异，DeepSeek-V3在Nextflow中表现突出；提示策略（如角色提示和思维链）显著提升质量。

Conclusion: 大型语言模型结合适当提示策略，有望降低生物信息学工作流开发难度，提升可重复性，使更多人受益于计算工具。

Abstract: The increasing complexity of bioinformatics data analysis has made Scientific
Workflow Systems (SWSs) like Galaxy and Nextflow essential for enabling
scalable, reproducible, and automated workflows. However, creating and
understanding these workflows remains challenging, particularly for domain
experts without programming expertise. This study investigates whether modern
Large Language Models (LLMs), GPT-4o, Gemini 2.5 Flash, and DeepSeek-V3, can
support the generation of accurate, complete, and usable bioinformatics
workflows, and examines which prompting strategies most effectively guide this
process. We evaluate these models using diverse tasks such as SNP analysis,
RNA-seq, DNA methylation, and data retrieval, spanning both graphical (Galaxy)
and script-based (Nextflow) platforms. Expert reviewers assess the generated
workflows against community-curated baselines from the Galaxy Training Network
and nf-core repositories. The results show that Gemini 2.5 Flash excels in
generating Galaxy workflows, while DeepSeek-V3 performs strongly in Nextflow.
Prompting strategies significantly impact quality, with role-based and
chain-of-thought prompts improving completeness and correctness. While GPT-4o
benefits from structured inputs, DeepSeek-V3 offers rich technical detail,
albeit with some verbosity. Overall, the findings highlight the potential of
LLMs to lower the barrier for workflow development, improve reproducibility,
and democratize access to computational tools in bioinformatics, especially
when combined with thoughtful prompt engineering.

</details>


### [19] [Relating System Safety and Machine Learnt Model Performance](https://arxiv.org/abs/2507.20135)
*Ganesh Pai*

Main category: cs.SE

TL;DR: 论文提出了一种基于安全评估的方法，用于确定机器学习模型在航空应用中满足安全目标的最低性能要求。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于明确系统安全目标与模型性能要求之间的关系，以确保机器学习组件在航空应用中的安全性和可靠性。

Method: 论文通过一个飞机紧急制动系统的案例，抽象出机器学习组件的行为，并基于此提出了一种推导最低安全相关性能要求及其目标的方法。

Result: 该方法能够明确机器学习组件及其底层深度神经网络的安全性能要求，并验证其合理性。

Conclusion: 结论强调了该方法在航空应用中验证机器学习模型安全性能的重要性，并指出了其适用性限制和验证意义。

Abstract: The prediction quality of machine learnt models and the functionality they
ultimately enable (e.g., object detection), is typically evaluated using a
variety of quantitative metrics that are specified in the associated model
performance requirements. When integrating such models into aeronautical
applications, a top-down safety assessment process must influence both the
model performance metrics selected, and their acceptable range of values.
Often, however, the relationship of system safety objectives to model
performance requirements and the associated metrics is unclear. Using an
example of an aircraft emergency braking system containing a machine learnt
component (MLC) responsible for object detection and alerting, this paper first
describes a simple abstraction of the required MLC behavior. Then, based on
that abstraction, an initial method is given to derive the minimum
safety-related performance requirements, the associated metrics, and their
targets for the both MLC and its underlying deep neural network, such that they
meet the quantitative safety objectives obtained from the safety assessment
process. We give rationale as to why the proposed method should be considered
valid, also clarifying the assumptions made, the constraints on applicability,
and the implications for verification.

</details>


### [20] [Strategic Motivators for Ethical AI System Development: An Empirical and Holistic Model](https://arxiv.org/abs/2507.20218)
*Muhammad Azeem Akbar,Arif Ali Khan,Saima Rafi,Damian Kedziora,Sami Hyrynsalmi*

Main category: cs.SE

TL;DR: 该研究通过MLR和问卷调查识别了推动AI伦理发展的20个关键动因，分为8类，并利用ISM和MICMAC分析其关系与分类，最后用Fuzzy TOPSIS排序重要性。


<details>
  <summary>Details</summary>
Motivation: 确保AI的负责任开发，防止负面后果，需理解并优先考虑推动伦理实践的关键因素。

Method: 采用MLR、问卷调查、ISM、MICMAC分析和Fuzzy TOPSIS方法，研究动因及其优先级。

Result: 发现'人力资源'和'协调'是主要影响因素，关键动因包括促进团队多样性、建立治理机构等。

Conclusion: 组织应将这些动因融入战略、政策及治理模型中，以支持伦理AI的发展。

Abstract: Artificial Intelligence (AI) presents transformative opportunities for
industries and society, but its responsible development is essential to prevent
unintended consequences. Ethically sound AI systems demand strategic planning,
strong governance, and an understanding of the key drivers that promote
responsible practices. This study aims to identify and prioritize the
motivators that drive the ethical development of AI systems. A Multivocal
Literature Review (MLR) and a questionnaire-based survey were conducted to
capture current practices in ethical AI. We applied Interpretive Structure
Modeling (ISM) to explore the relationships between motivator categories,
followed by MICMAC analysis to classify them by their driving and dependence
power. Fuzzy TOPSIS was used to rank these motivators by importance. Twenty key
motivators were identified and grouped into eight categories: Human Resource,
Knowledge Integration, Coordination, Project Administration, Standards,
Technology Factor, Stakeholders, and Strategy & Matrices. ISM results showed
that 'Human Resource' and 'Coordination' heavily influence other factors.
MICMAC analysis placed categories like Human Resource (CA1), Coordination
(CA3), Stakeholders (CA7), and Strategy & Matrices (CA8) in the independent
cluster, indicating high driving but low dependence power. Fuzzy TOPSIS ranked
motivators such as promoting team diversity, establishing AI governance bodies,
appointing oversight leaders, and ensuring data privacy as most critical. To
support ethical AI adoption, organizations should align their strategies with
these motivators and integrate them into their policies, governance models, and
development frameworks.

</details>


### [21] [Beyond Binary Moderation: Identifying Fine-Grained Sexist and Misogynistic Behavior on GitHub with Large Language Models](https://arxiv.org/abs/2507.20358)
*Tanni Dev,Sayma Sultana,Amiangshu Bosu*

Main category: cs.SE

TL;DR: 研究提出了一种基于指令调优大语言模型的多类分类框架，用于检测GitHub上的性别歧视和厌女评论，效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有关键词过滤或二元分类工具难以有效检测微妙的有害行为，影响了技术社区的包容性。

Method: 采用指令调优的LLM框架，通过20次系统提示优化，评估1440条标记评论。

Result: 优化后的方法（GPT-4o与Prompt 19）MCC为0.501，假阳性低，但对上下文依赖的性别歧视识别能力有限。

Conclusion: 精心设计的提示能显著提高性别歧视检测的准确性和可解释性，助力平台精准审核。

Abstract: Background: Sexist and misogynistic behavior significantly hinders inclusion
in technical communities like GitHub, causing developers, especially
minorities, to leave due to subtle biases and microaggressions. Current
moderation tools primarily rely on keyword filtering or binary classifiers,
limiting their ability to detect nuanced harm effectively.
  Aims: This study introduces a fine-grained, multi-class classification
framework that leverages instruction-tuned Large Language Models (LLMs) to
identify twelve distinct categories of sexist and misogynistic comments on
GitHub.
  Method: We utilized an instruction-tuned LLM-based framework with systematic
prompt refinement across 20 iterations, evaluated on 1,440 labeled GitHub
comments across twelve sexism/misogyny categories. Model performances were
rigorously compared using precision, recall, F1-score, and the Matthews
Correlation Coefficient (MCC).
  Results: Our optimized approach (GPT-4o with Prompt 19) achieved an MCC of
0.501, significantly outperforming baseline approaches. While this model had
low false positives, it struggled to interpret nuanced, context-dependent
sexism and misogyny reliably.
  Conclusion: Well-designed prompts with clear definitions and structured
outputs significantly improve the accuracy and interpretability of sexism
detection, enabling precise and practical moderation on developer platforms
like GitHub.

</details>


### [22] [CIgrate: Automating CI Service Migration with Large Language Models](https://arxiv.org/abs/2507.20402)
*Md Nazmul Hossain,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 研究比较基于LLM的CIgrate与规则基础的CIMig在CI配置迁移中的表现，探讨LLM在自动化迁移中的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于CI服务迁移在开源项目中需求频繁且手动迁移耗时易错，研究旨在探索LLM在提升自动化迁移可行性方面的作用。

Method: 提出CIgrate框架，评估其在零样本/少样本提示及微调LLM后的效果，并与CIMig对比。

Result: 预期贡献包括首个LLM驱动的CI迁移方法、与规则方法的对比评估，及对LLM支持配置演进的见解。

Conclusion: 研究有望验证LLM在CI配置迁移中的优势，为未来自动化工具提供新思路。

Abstract: Continuous Integration (CI) configurations often need to be migrated between
services (e.g., Travis CI to GitHub Actions) as projects evolve, due to changes
in service capabilities, usage limits, or service deprecation. Previous studies
reported that migration across CI services is a recurring need in open-source
development. However, manual migration can be time-consuming and error-prone.
The state-of-the-art approach, CIMig, addresses this challenge by analyzing
past migration examples to create service-specific rules and produce equivalent
configurations across CI services. However, its relatively low accuracy raises
concerns about the overall feasibility of automated CI migration using
rule-based techniques alone. Meanwhile, Large Language Models (LLMs) have
demonstrated strong capabilities in code generation and transformation tasks,
suggesting potential to improve the automation, usability, and generalizability
of CI configuration migration. This registered report presents a study in which
we aim to assess whether CI migration can be improved using LLMs. To this end,
we propose CIgrate, an LLM-based framework for automatically migrating CI
configurations. We plan to evaluate the performance of CIgrate compared to
CIMig as a baseline, in different setups (a) zero-shot/few-shot prompting of
LLMs for configuration migration and (b) fine-tuning an LLM on a dataset of
already established CI service migrations. We will also seek developer feedback
on the quality and usability of the generated configurations. We formulate
research questions focusing on the accuracy of LLM-generated migrations versus
ground truth and the output of CIMig. The expected contributions include the
first LLM-powered approach for CI service migration, a comparative evaluation
of its effectiveness compared to rule-based approaches, and insight into
leveraging LLMs to support software configuration evolution.

</details>


### [23] [Testing Is Not Boring: Characterizing Challenge in Software Testing Tasks](https://arxiv.org/abs/2507.20407)
*Davi Gama Hardman,Cesar França,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 论文探讨了软件测试工作的复杂性和挑战，反驳了将其视为低技能活动的观点，并研究了任务复杂度对测试专业人员的影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示软件测试中的创造性、问题解决能力和适应性，以改变对测试工作的负面刻板印象。

Method: 通过访谈软件测试专业人员，研究调查了测试任务的性质及其对专业人士的影响。

Result: 研究发现，涉及创造性和持续学习的任务具有激励性，而过于简单或过于复杂的任务可能导致挫败感。

Conclusion: 研究强调平衡任务复杂度的重要性，以维持测试人员的积极性，并展示了软件测试的动态性和智力挑战性。

Abstract: As software systems continue to grow in complexity, testing has become a
fundamental part of ensuring the quality and reliability of software products.
Yet, software testing is still often perceived, both in industry and academia,
as a repetitive, low-skill activity. This perception fails to recognize the
creativity, problem-solving, and adaptability required in testing work. Tasks
such as designing complex test cases, automating testing processes, and
handling shifting requirements illustrate the challenges testing professionals
regularly face. To better understand these experiences, we conducted a study
with software testing professionals to explore the nature of challenging tasks
in software testing and how they affect these professionals. Our findings show
that tasks involving creativity, ongoing learning, and time pressure are often
seen as motivating and rewarding. On the other hand, a lack of challenge or
overwhelming demands can lead to frustration and disengagement. These findings
demonstrate the importance of balancing task complexity to sustain motivation
and present software testing as a dynamic and intellectually engaging field.

</details>


### [24] [When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions](https://arxiv.org/abs/2507.20439)
*Maya Larbi,Amal Akli,Mike Papadakis,Rihab Bouyousfi,Maxime Cordy,Federica Sarro,Yves Le Traon*

Main category: cs.SE

TL;DR: 该论文研究了大型语言模型（LLMs）在面对模糊或不完整的任务描述时的代码生成能力，发现即使是先进模型也会因描述问题而表现下降。


<details>
  <summary>Details</summary>
Motivation: 实际开发中任务描述往往不清晰，但现有研究多基于理想化条件，因此需要评估LLMs在模糊描述下的鲁棒性。

Method: 通过扩展HumanEval和MBPP基准，系统引入真实的任务描述缺陷，评估不同规模和架构的LLMs的表现。

Result: 发现任务描述的微小瑕疵会导致性能显著下降，矛盾描述尤其容易引发逻辑错误；大模型表现更优但仍不完美。

Conclusion: 需开发更具鲁棒性的LLMs，改进训练策略和评估基准，以确保在实际开发中的可靠部署。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance in code
generation tasks under idealized conditions, where task descriptions are clear
and precise. However, in practice, task descriptions frequently exhibit
ambiguity, incompleteness, or internal contradictions. In this paper, we
present the first empirical study examining the robustness of state-of-the-art
code generation models when faced with such unclear task descriptions. We
extend the HumanEval and MBPP benchmarks by systematically introducing
realistic task descriptions flaws through guided mutation strategies, producing
a dataset that mirrors the messiness of informal developer instructions. We
evaluate multiple LLMs of varying sizes and architectures, analyzing their
functional correctness and failure modes across task descriptions categories.
Our findings reveal that even minor imperfections in task description phrasing
can cause significant performance degradation, with contradictory task
descriptions resulting in numerous logical errors. Moreover, while larger
models tend to be more resilient than smaller variants, they are not immune to
the challenges posed by unclear requirements. We further analyze semantic error
patterns and identify correlations between description clarity, model behavior,
and error types. Our results underscore the critical need for developing LLMs
that are not only powerful but also robust to the imperfections inherent in
natural user tasks, highlighting important considerations for improving model
training strategies, designing more realistic evaluation benchmarks, and
ensuring reliable deployment in practical software development environments.

</details>


### [25] [Distinguishing Quantum Software Bugs from Hardware Noise: A Statistical Approach](https://arxiv.org/abs/2507.20475)
*Ahmik Virani,Devraj,Anirudh Suresh,Lei Zhang,M V Panduranga Rao*

Main category: cs.SE

TL;DR: 针对NISQ时代的量子计算中软件错误与硬件噪声难以区分的问题，提出了一种基于概率度量的统计方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决NISQ量子计算机中量子软件错误与硬件噪声难以区分的问题。

Method: 采用统计方法和概率度量，通过实验评估知名量子算法（如Grover算法）来区分软件错误与硬件噪声。

Result: 实验结果表明该方法有效且实用，为开发者提供了可靠的分析工具。

Conclusion: 提出的统计方法能有效区分量子软件错误与硬件噪声，具有实际应用价值。

Abstract: Quantum computing in the Noisy Intermediate-Scale Quantum (NISQ) era presents
significant challenges in differentiating quantum software bugs from hardware
noise. Traditional debugging techniques from classical software engineering
cannot directly resolve this issue due to the inherently stochastic nature of
quantum computation mixed with noises from NISQ computers. To address this gap,
we propose a statistical approach leveraging probabilistic metrics to
differentiate between quantum software bugs and hardware noise. We evaluate our
methodology empirically using well-known quantum algorithms, including Grover's
algorithm, Deutsch-Jozsa algorithm, and Simon's algorithm. Experimental results
demonstrate the efficacy and practical applicability of our approach, providing
quantum software developers with a reliable analytical tool to identify and
classify unexpected behavior in quantum programs.

</details>


### [26] [VDGraph: A Graph-Theoretic Approach to Unlock Insights from SBOM and SCA Data](https://arxiv.org/abs/2507.20502)
*Howell Xia,Jonah Gluck,Sevval Simsek,David Sastre Medina,David Starobinski*

Main category: cs.SE

TL;DR: VDGraph是一种基于知识图的方法，用于整合SBOM和SCA数据，以可视化依赖与漏洞关系。


<details>
  <summary>Details</summary>
Motivation: 现代软件供应链复杂，需要整合SBOM和SCA工具以统一视图分析依赖与漏洞关系。

Method: 引入VDGraph，将SBOM和SCA数据转换为图表示，解决数据冲突问题，并实现概念验证。

Result: 在21个Java项目中验证，发现高风险漏洞多集中于深层依赖，直接或次级依赖更安全。

Conclusion: VDGraph为复杂依赖中的漏洞传播提供了可视化和自动化分析的基础。

Abstract: The high complexity of modern software supply chains necessitates tools such
as Software Bill of Materials (SBOMs) to manage component dependencies, and
Software Composition Analysis (SCA) tools to identify vulnerabilities. While
there exists limited integration between SBOMs and SCA tools, a unified view of
complex dependency-vulnerability relationships remains elusive. In this paper,
we introduce VDGraph, a novel knowledge graph-based methodology for integrating
vulnerability and dependency data into a holistic view. VDGraph consolidates
SBOM and SCA outputs into a graph representation of software projects'
dependencies and vulnerabilities. We provide a formal description and analysis
of the theoretical properties of VDGraph and present solutions to manage
possible conflicts between the SBOM and SCA data. We further introduce and
evaluate a practical, proof-of-concept implementation of VDGraph using two
popular SBOM and SCA tools, namely CycloneDX Maven plugin and Google's
OSV-Scanner. We apply VDGraph on 21 popular Java projects. Through the
formulation of appropriate queries on the graphs, we uncover the existence of
concentrated risk points (i.e., vulnerable components of high severity
reachable through numerous dependency paths). We further show that
vulnerabilities predominantly emerge at a depth of three dependency levels or
higher, indicating that direct or secondary dependencies exhibit lower
vulnerability density and tend to be more secure. Thus, VDGraph contributes a
graph-theoretic methodology that improves visibility into how vulnerabilities
propagate through complex, transitive dependencies. Moreover, our
implementation, which combines open SBOM and SCA standards with Neo4j, lays a
foundation for scalable and automated analysis across real-world projects.

</details>


### [27] [GeoJSEval: An Automated Evaluation Framework for Large Language Models on JavaScript-Based Geospatial Computation and Visualization Code Generation](https://arxiv.org/abs/2507.20553)
*Guanyu Chen,Haoyue Jiao,Shuyang Hou,Ziqi Liu,Lutong Xie,Shaowen Wu,Huayi Wu,Xuefeng Guan,Zhipeng Gui*

Main category: cs.SE

TL;DR: GeoJSEval是一个多模态、功能级的自动评估框架，用于评估大型语言模型在JavaScript地理空间代码生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在地理空间代码生成任务中的广泛应用，亟需系统化的评估方法来衡量其在地理空间上下文中的生成能力。

Method: 提出了GeoJSEval框架，包括标准化测试套件(GeoJSEval-Bench)、代码提交引擎和评估模块，涵盖432个功能级任务和2,071个测试用例。

Result: 评估了18种最先进的LLM，揭示了在空间语义理解、代码可靠性和功能调用准确性上的显著性能差异。

Conclusion: GeoJSEval为地理空间代码生成模型的标准化评估和优化提供了基础方法论和实用工具，具有强扩展性和实际应用性。

Abstract: With the widespread adoption of large language models (LLMs) in code
generation tasks, geospatial code generation has emerged as a critical frontier
in the integration of artificial intelligence and geoscientific analysis. This
trend underscores the urgent need for systematic evaluation methodologies to
assess LLMs generation capabilities in geospatial contexts. In particular,
geospatial computation and visualization tasks in JavaScript environments rely
heavily on orchestrating diverse frontend libraries and ecosystems, placing
elevated demands on a model's semantic understanding and code synthesis
abilities. To address this challenge, we propose GeoJSEval--the first
multimodal, function-level automatic evaluation framework for LLMs in
JavaScript-based geospatial code generation. GeoJSEval comprises three core
components: a standardized test suite (GeoJSEval-Bench), a code submission
engine, and an evaluation module. It includes 432 function-level tasks and
2,071 structured test cases spanning five widely used JavaScript geospatial
libraries and 25 mainstream geospatial data types. GeoJSEval enables
multidimensional quantitative evaluation across metrics such as accuracy,
output stability, execution efficiency, resource consumption, and error type
distribution, and integrates boundary testing mechanisms to enhance robustness
and coverage. We conduct a comprehensive evaluation of 18 state-of-the-art LLMs
using GeoJSEval, revealing significant performance disparities and bottlenecks
in spatial semantic understanding, code reliability, and function invocation
accuracy. GeoJSEval provides a foundational methodology, evaluation resource,
and practical toolkit for the standardized assessment and optimization of
geospatial code generation models, with strong extensibility and applicability
in real-world scenarios.

</details>


### [28] [Intention-Driven Generation of Project-Specific Test Cases](https://arxiv.org/abs/2507.20619)
*Binhang Qi,Yun Lin,Xinyi Weng,Yuhuan Huang,Chenyan Liu,Hailong Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: IntentionTest是一种基于验证意图生成项目特定测试的方法，通过结构化描述和代码复用提高测试的有效性和通过率。


<details>
  <summary>Details</summary>
Motivation: 现有测试生成技术通常仅关注代码覆盖率，但实际项目中测试需要反映开发者的验证意图和项目特定知识。

Method: IntentionTest利用验证意图的结构化描述，通过检索项目中的可参考测试并减少生成问题为代码编辑问题来生成测试。

Result: 在13个开源项目的4,146个测试案例中，IntentionTest生成的测试语义正确率更高，突变分数提升39.03%，覆盖率重叠提升40.14%，通过率提高21.30%。

Conclusion: IntentionTest通过结合验证意图和项目特定知识，显著提升了测试生成的质量和实用性。

Abstract: Test cases are valuable assets for maintaining software quality. While
numerous automated techniques have been proposed for generating tests (either
by maximizing code coverage or by translating focal code into test code),
practical tests are seldom driven by coverage alone. In real projects, each
test reflects a developer's validation intention for a specific behaviour and
embodies rich, project-specific knowledge: which specific APIs to call and what
assertions truly matter. Without considering such knowledge, tests can hardly
pass code review and be integrated into the software product.
  In this work, we propose IntentionTest, which generates project-specific
tests with validation intention as a structured description. Our design is
motivated by two insights: (1) a description of validation intention, compared
to coverage and focal code, carries more crucial information about what to
test; and (2) practical tests exhibit high code duplication, indicating that
domain knowledge is highly reusable for writing new tests. Given a focal code
and a description of validation intention (in the form of either an informal
comment or a formal test plan), IntentionTest retrieves a referable test in the
project to guide test generation. Moreover, IntentionTest reduces the test
generation problem into an editing problem on the test code regarding the
validation intention. It generates a test including both test prefix and
oracle, which aims to be executable and semantically correct.
  We evaluate IntentionTest against state-of-the-art baselines on 4,146 test
cases from 13 open-source projects. Specifically, compared to ChatTester,
IntentionTest can (1) generate significantly more semantically correct tests,
improving common mutation scores by 39.03% and coverage overlap with
ground-truth tests by 40.14%; (2) generate 21.30% more successful passing
tests.

</details>


### [29] [Client--Library Compatibility Testing with API Interaction Snapshots](https://arxiv.org/abs/2507.20814)
*Gustave Monce,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes*

Main category: cs.SE

TL;DR: 论文提出了一种检测库行为破坏性变更（BBCs）的新方法，通过记录客户端测试中的API交互快照并比较库演化前后的差异，从而识别BBCs。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖第三方库，但库的演化可能导致行为破坏性变更（BBCs），传统客户端测试难以检测这类变更。

Method: 利用客户端测试的API交互快照记录协议、输入输出值等，通过比较快照变化识别BBCs。实现工具为Gilesi。

Result: 初步案例研究表明，Gilesi能可靠检测出客户端测试遗漏的BBCs。

Conclusion: 该方法有效解决了传统测试难以检测BBCs的问题，为客户端-库兼容性测试提供了新思路。

Abstract: Modern software development heavily relies on third-party libraries to speed
up development and enhance quality. As libraries evolve, they may break the
tacit contract established with their clients by introducing behavioral
breaking changes (BBCs) that alter run-time behavior and silently break client
applications without being detected at compile time. Traditional regression
tests on the client side often fail to detect such BBCs, either due to limited
library coverage or weak assertions that do not sufficiently exercise the
library's expected behavior. To address this issue, we propose a novel approach
to client--library compatibility testing that leverages existing client tests
in a novel way. Instead of relying on developer-written assertions, we propose
recording the actual interactions at the API boundary during the execution of
client tests (protocol, input and output values, exceptions, etc.). These
sequences of API interactions are stored as snapshots which capture the exact
contract expected by a client at a specific point in time. As the library
evolves, we compare the original and new snapshots to identify perturbations in
the contract, flag potential BBCs, and notify clients. We implement this
technique in our prototype tool Gilesi, a Java framework that automatically
instruments library APIs, records snapshots, and compares them. Through a
preliminary case study on several client--library pairs with artificially
seeded BBCs, we show that Gilesi reliably detects BBCs missed by client test
suites.

</details>


### [30] [Enhancing Project-Specific Code Completion by Inferring Internal API Information](https://arxiv.org/abs/2507.20888)
*Le Deng,Xiaoxue Ren,Chao Ni,Ming Liang,David Lo,Zhongxin Liu*

Main category: cs.SE

TL;DR: 该论文提出了一种无需依赖导入即可推断内部API信息的方法，通过构造API的使用示例和语义描述，显著提升了代码完成任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于检索增强生成（RAG）的方法在代码完成任务中难以有效利用未显式导入的内部API信息，影响了准确性。

Method: 提出了一种新方法，通过扩展API的表示（如构造使用示例和语义描述）构建知识库，供LLMs生成相关补全，并引入了ProjBench基准测试。

Result: 实验结果显示，该方法在代码和标识符的完全匹配率上分别提升了22.72%和18.31%，与现有基线结合后提升更显著。

Conclusion: 该方法有效解决了内部API信息利用的难题，显著提升了代码完成任务的性能。

Abstract: Project-specific code completion is a critical task that leverages context
from a project to generate accurate code. State-of-the-art methods use
retrieval-augmented generation (RAG) with large language models (LLMs) and
project information for code completion. However, they often struggle to
incorporate internal API information, which is crucial for accuracy, especially
when APIs are not explicitly imported in the file.
  To address this, we propose a method to infer internal API information
without relying on imports. Our method extends the representation of APIs by
constructing usage examples and semantic descriptions, building a knowledge
base for LLMs to generate relevant completions. We also introduce ProjBench, a
benchmark that avoids leaked imports and consists of large-scale real-world
projects.
  Experiments on ProjBench and CrossCodeEval show that our approach
significantly outperforms existing methods, improving code exact match by
22.72% and identifier exact match by 18.31%. Additionally, integrating our
method with existing baselines boosts code match by 47.80% and identifier match
by 35.55%.

</details>


### [31] [Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs](https://arxiv.org/abs/2507.20977)
*Maria Camporese,Fabio Massacci*

Main category: cs.SE

TL;DR: 该论文探讨了大型语言模型（LLM）在自动修复漏洞（AVR）中的表现是否依赖于训练数据泄露或完美错误定位等隐藏因素。通过故意在提示中引入错误的位置偏移，研究者验证了LLM是否仅记忆了人类修复方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示LLM在AVR中表现优异的潜在原因，尤其是是否存在依赖训练数据泄露或完美错误定位等“隐形因素”。

Method: 方法包括：在Vul4J和VJTrans基准测试中故意偏移漏洞位置，使用两个LLM分别生成和审查修复补丁，并通过回归和漏洞验证测试验证结果。最后手动审核样本并估计错误率。

Result: 结果将展示偏移位置后LLM是否仍能生成正确补丁，从而验证其是否仅依赖记忆修复方案。

Conclusion: 结论将回答LLM在AVR中的表现是否确实依赖于隐藏因素，还是具备真正的泛化能力。

Abstract: Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of
program repair. Recent studies show that large language models (LLMs)
outperform traditional techniques, extending their success beyond code
generation and fault detection.
  Hypothesis: These gains may be driven by hidden factors -- "invisible hands"
such as training-data leakage or perfect fault localization -- that let an LLM
reproduce human-authored fixes for the same code.
  Objective: We replicate prior AVR studies under controlled conditions by
deliberately adding errors to the reported vulnerability location in the
prompt. If LLMs merely regurgitate memorized fixes, both small and large
localization errors should yield the same number of correct patches, because
any offset should divert the model from the original fix.
  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans
benchmarks after shifting the fault location by n lines from the ground truth.
A first LLM generates a patch, a second LLM reviews it, and we validate the
result with regression and proof-of-vulnerability tests. Finally, we manually
audit a sample of patches and estimate the error rate with the
Agresti-Coull-Wilson method.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [32] [Development and Evaluation of Adaptive LearningSupport System Based on Ontology of MultipleProgramming Languages](https://arxiv.org/abs/2507.19728)
*Lalita Na Nongkhai,Jingyun Wang,Takahiko Mendori*

Main category: cs.PL

TL;DR: ADVENTURE系统使用基于本体的自适应学习支持方法，为编程学习者提供个性化练习，并通过Elo评分系统动态调整难度，实验表明自适应模式在正确率和掌握概念数量上优于随机模式。


<details>
  <summary>Details</summary>
Motivation: 为了解决编程学习中个体差异大、传统教学方法难以个性化的问题，设计了一个自适应学习支持系统。

Method: 系统利用名为CONTINUOUS的本体跨语言编程概念，结合Elo评分系统动态调整练习难度，提供个性化推荐和提示。

Result: 实验比较自适应与随机模式，自适应模式在正确率和掌握概念数量上显著优于随机模式。

Conclusion: ADVENTURE系统通过自适应机制有效支持编程学习，提升了学习效果。

Abstract: This paper introduces an ontology-based approach within an adaptive learning
support system for computer programming. This system (named ADVENTURE) is
designed to deliver personalized programming exercises that are tailored to
individual learners' skill levels. ADVENTURE utilizes an ontology, named
CONTINUOUS, which encompasses common concepts across multiple programming
languages. The system leverages this ontology not only to visualize programming
concepts but also to provide hints during practice programming exercises and
recommend subsequent programming concepts. The adaptive mechanism is driven by
the Elo Rating System, applied in an educational context to dynamically
estimate the most appropriate exercise difficulty for each learner. An
experimental study compared two instructional modes, adaptive and random, based
on six features derived from 1,186 code submissions across all the experimental
groups. The results indicate significant differences in four of six analyzed
features between these two modes. Notably, the adaptive mode demonstrates a
significant difference over the random mode in two features, the submission of
correct answers and the number of pass concepts. Therefore, these results
underscore that this adaptive learning support system may support learners in
practicing programming exercises.

</details>


### [33] [The Power of Negation in Higher-Order Datalog](https://arxiv.org/abs/2507.20251)
*Angelos Charalambidis,Babis Kostopoulos,Christos Nomikos,Panos Rondogiannis*

Main category: cs.PL

TL;DR: 论文研究了高阶Datalog在良基语义和稳定模型语义下的表达能力，建立了与复杂性类的紧密联系。


<details>
  <summary>Details</summary>
Motivation: 探讨高阶Datalog在不同语义下的表达能力及其与复杂性类的关系。

Method: 通过良基语义和稳定模型语义分析高阶Datalog的表达能力，并结合部分应用关系和关系枚举进行证明。

Result: 高阶Datalog在良基语义下捕获k-EXP，在稳定模型语义下捕获co-(k-NEXP)和k-NEXP。

Conclusion: 研究揭示了高阶逻辑编程中阶数与非确定性之间的权衡，展示了表达能力的层次结构。

Abstract: We investigate the expressive power of Higher-Order Datalog$^\neg$ under both
the well-founded and the stable model semantics, establishing tight connections
with complexity classes. We prove that under the well-founded semantics, for
all $k\geq 1$, $(k+1)$-Order Datalog$^\neg$ captures k-EXP, a result that holds
without explicit ordering of the input database. The proof of this fact can be
performed either by using the powerful existential predicate variables of the
language or by using partially applied relations and relation enumeration.
Furthermore, we demonstrate that this expressive power is retained within a
stratified fragment of the language. Under the stable model semantics, we show
that $(k+1)$-Order Datalog$^\neg$ captures co-(k-NEXP) using cautious reasoning
and k-NEXP using brave reasoning, again with analogous results for the
stratified fragment augmented with choice rules. Our results establish a
hierarchy of expressive power, highlighting an interesting trade-off between
order and non-determinism in the context of higher-order logic programming:
increasing the order of programs under the well-founded semantics can surpass
the expressive power of lower-order programs under the stable model semantics.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [34] [Towards Generalized Parameter Tuning in Coherent Ising Machines: A Portfolio-Based Approach](https://arxiv.org/abs/2507.20295)
*Tatsuro Hanyu,Takahiro Katagiri,Daichi Mukunoki,Tetsuya Hoshino*

Main category: cs.PF

TL;DR: 提出了一种针对CIMs中使用CACm算法的超参数调优的组合算法方法，包含两种调优方法（A和B），实验表明其显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: CIMs在组合优化问题中表现优异，但CAC算法对超参数敏感，需高效调优。

Method: 采用组合算法方法，提出两种调优方法：A（顺序优化）和B（基于初始评估的优先优化）。

Result: 在超算上测试，Method A提升1.47倍，Method B提升1.65倍。

Conclusion: 组合算法方法有效提升了CIMs的超参数调优效率。

Abstract: Coherent Ising Machines (CIMs) have recently gained attention as a promising
computing model for solving combinatorial optimization problems. In particular,
the Chaotic Amplitude Control (CAC) algorithm has demonstrated high solution
quality, but its performance is highly sensitive to a large number of
hyperparameters, making efficient tuning essential. In this study, we present
an algorithm portfolio approach for hyperparameter tuning in CIMs employing
Chaotic Amplitude Control with momentum (CACm) algorithm. Our method
incorporates multiple search strategies, enabling flexible and effective
adaptation to the characteristics of the hyperparameter space. Specifically, we
propose two representative tuning methods, Method A and Method B. Method A
optimizes each hyperparameter sequentially with a fixed total number of trials,
while Method B prioritizes hyperparameters based on initial evaluations before
applying Method A in order. Performance evaluations were conducted on the
Supercomputer "Flow" at Nagoya University, using planted Wishart instances and
Time to Solution (TTS) as the evaluation metric. Compared to the baseline
performance with best-known hyperparameters, Method A achieved up to 1.47x
improvement, and Method B achieved up to 1.65x improvement. These results
demonstrate the effectiveness of the algorithm portfolio approach in enhancing
the tuning process for CIMs.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [35] [On the Limitations of Ray-Tracing for Learning-Based RF Tasks in Urban Environments](https://arxiv.org/abs/2507.19653)
*Armen Manukyan,Hrant Khachatrian,Edvard Ghukasyan,Theofanis P. Raptis*

Main category: cs.NI

TL;DR: 研究了Sionna v1.0.2射线跟踪在罗马市中心室外蜂窝链路的真实性，通过实测数据和模拟参数优化提升了模拟器性能。


<details>
  <summary>Details</summary>
Motivation: 探索Sionna射线跟踪模拟器在室外蜂窝链路中的真实性和优化潜力。

Method: 使用1,664个用户设备和6个基站站点的实测数据，系统调整模拟参数并通过相关系数和kNN定位算法评估模拟器性能。

Result: 天线位置和方向对性能影响显著，通过优化提升相关性和定位精度，但模拟数据仍不及实测数据。

Conclusion: 精确的几何和天线模型是必要的，但还需解决城市噪声问题以实现高保真模拟。

Abstract: We study the realism of Sionna v1.0.2 ray-tracing for outdoor cellular links
in central Rome. We use a real measurement set of 1,664 user-equipments (UEs)
and six nominal base-station (BS) sites. Using these fixed positions we
systematically vary the main simulation parameters, including path depth,
diffuse/specular/refraction flags, carrier frequency, as well as antenna's
properties like its altitude, radiation pattern, and orientation. Simulator
fidelity is scored for each base station via Spearman correlation between
measured and simulated powers, and by a fingerprint-based k-nearest-neighbor
localization algorithm using RSSI-based fingerprints. Across all experiments,
solver hyper-parameters are having immaterial effect on the chosen metrics. On
the contrary, antenna locations and orientations prove decisive. By simple
greedy optimization we improve the Spearman correlation by 5% to 130% for
various base stations, while kNN-based localization error using only simulated
data as reference points is decreased by one-third on real-world samples, while
staying twice higher than the error with purely real data. Precise geometry and
credible antenna models are therefore necessary but not sufficient; faithfully
capturing the residual urban noise remains an open challenge for transferable,
high-fidelity outdoor RF simulation.

</details>


### [36] ["X of Information'' Continuum: A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems](https://arxiv.org/abs/2507.19657)
*Beining Wu,Jun Huang,Shui Yu*

Main category: cs.NI

TL;DR: 本文综述了下一代网络中信息质量的多维度度量框架及其应用挑战。


<details>
  <summary>Details</summary>
Motivation: 传统网络指标如延迟和丢包已不足以满足现代智能应用对信息质量的需求。

Method: 提出了一个四维分类框架，涵盖时间、质量/效用、可靠性/鲁棒性和网络/通信维度，并分析其相互关系。

Result: 揭示了多维度信息度量在不同应用领域的潜力，并探讨了人工智能技术在优化信息质量中的关键作用。

Conclusion: 信息质量的多维度度量是关键发展方向，但仍面临实施挑战。

Abstract: The development of next-generation networking systems has inherently shifted
from throughput-based paradigms towards intelligent, information-aware designs
that emphasize the quality, relevance, and utility of transmitted information,
rather than sheer data volume. While classical network metrics, such as latency
and packet loss, remain significant, they are insufficient to quantify the
nuanced information quality requirements of modern intelligent applications,
including autonomous vehicles, digital twins, and metaverse environments. In
this survey, we present the first comprehensive study of the ``X of
Information'' continuum by introducing a systematic four-dimensional taxonomic
framework that structures information metrics along temporal, quality/utility,
reliability/robustness, and network/communication dimensions. We uncover the
increasing interdependencies among these dimensions, whereby temporal freshness
triggers quality evaluation, which in turn helps with reliability appraisal,
ultimately enabling effective network delivery. Our analysis reveals that
artificial intelligence technologies, such as deep reinforcement learning,
multi-agent systems, and neural optimization models, enable adaptive,
context-aware optimization of competing information quality objectives. In our
extensive study of six critical application domains, covering autonomous
transportation, industrial IoT, healthcare digital twins, UAV communications,
LLM ecosystems, and metaverse settings, we illustrate the revolutionary promise
of multi-dimensional information metrics for meeting diverse operational needs.
Our survey identifies prominent implementation challenges, including ...

</details>


### [37] [Predicting Locations of Cell Towers for Network Capacity Expansion](https://arxiv.org/abs/2507.19925)
*Sowmiyan Morri,Joy Bose,L Raghunatha Reddy,Sai Hareesh Anamandra*

Main category: cs.NI

TL;DR: 针对电信运营商网络扩容的挑战，本文提出了一种结合深度学习和空间聚类的机器学习框架，用于预测信号覆盖并推荐新基站位置，考虑实际因素和预算限制。


<details>
  <summary>Details</summary>
Motivation: 传统方法如手动驱动测试和静态优化未能充分考虑用户密度、地形特征和财务约束等实际因素。

Method: 采用深度学习神经网络预测信号覆盖，结合空间聚类推荐服务不足区域的新站点位置，整合地理空间、人口统计和基础设施数据，并纳入预算约束。

Result: 该框架在迭代规划循环中优化覆盖估计，提供自适应且经济高效的扩展方案。架构模块化，对缺失输入具有鲁棒性，适用于多种部署场景。

Conclusion: 本文提出的方法为无线电网络规划提供了可扩展、数据驱动的替代方案，优于传统手动方法。

Abstract: Network capacity expansion is a critical challenge for telecom operators,
requiring strategic placement of new cell sites to ensure optimal coverage and
performance. Traditional approaches, such as manual drive tests and static
optimization, often fail to consider key real-world factors including user
density, terrain features, and financial constraints. In this paper, we propose
a machine learning-based framework that combines deep neural networks for
signal coverage prediction with spatial clustering to recommend new tower
locations in underserved areas. The system integrates geospatial, demographic,
and infrastructural data, and incorporates budget-aware constraints to
prioritize deployments. Operating within an iterative planning loop, the
framework refines coverage estimates after each proposed installation, enabling
adaptive and cost-effective expansion. While full-scale simulation was limited
by data availability, the architecture is modular, robust to missing inputs,
and generalizable across diverse deployment scenarios. This approach advances
radio network planning by offering a scalable, data-driven alternative to
manual methods.

</details>


### [38] [Optimizing Spreading Factor Selection for Mobile LoRa Gateways Using Single-Channel Hardware](https://arxiv.org/abs/2507.19938)
*W. A. Sasindu Wijesuriya*

Main category: cs.NI

TL;DR: 该论文提出了一种通过静态选择最优扩频因子（SF）的低成本解决方案，适用于移动LoRa网关的可靠通信，无需依赖昂贵的动态配置硬件。


<details>
  <summary>Details</summary>
Motivation: 传统LoRaWAN网络的动态配置功能仅支持昂贵的多通道网关，而低成本单通道硬件缺乏动态配置支持，导致通信可靠性问题。

Method: 采用两阶段算法：基于规则排除不满足距离、数据速率等约束的SF；剩余选项通过加权评分模型评估（考虑传输时间、能耗等）。

Result: 在672个模拟场景中，所选SF在92%以上的情况下匹配最优SF，验证了算法的有效性。

Conclusion: 该静态方法为成本敏感场景下的移动LoRa部署提供了一种可扩展且可靠的替代方案。

Abstract: The deployment of mobile LoRa gateways using low-cost single-channel hardware
presents a significant challenge in maintaining reliable communication due to
the lack of dynamic configuration support. In traditional LoRaWAN networks,
Adaptive Data Rate (ADR) mechanisms optimize communication parameters in real
time. However, such features are typically supported only by expensive
multi-channel gateways. This study proposes a cost-effective and
energy-efficient solution by statically selecting the optimal Spreading Factor
(SF) using a two-phase algorithm. The method first applies rule-based exclusion
to eliminate SFs that violate constraints related to distance, data rate, link
margin, and regulatory limits. Remaining candidates are then evaluated using a
weighted scoring model incorporating Time-on-Air, energy consumption, data
rate, and link robustness. The proposed algorithm was validated through
extensive field tests and NS-3 simulations under line-of-sight conditions.
Results demonstrate that the selected SF matched the optimal SF in over 92% of
cases across 672 simulated scenarios, confirming the algorithm's effectiveness.
This approach offers a scalable alternative to dynamic protocols, enabling
reliable mobile LoRa deployments in cost-sensitive environments such as
agriculture and rural sensing applications.

</details>


### [39] [A Scalable Resource Management Layer for FPGA SoCs in 6G Radio Units](https://arxiv.org/abs/2507.19963)
*Nikolaos Bartzoudis,José Rubio Fernández,David López-Bueno,Antonio Román Villarroel*

Main category: cs.NI

TL;DR: 本文提出了一种解决5G射频和边缘计算基础设施中FPGA SoC设备计算资源利用不足的方法，通过动态迁移和扩展功能来优化资源管理。


<details>
  <summary>Details</summary>
Motivation: FPGA SoC设备在5G和边缘计算中的计算资源未得到充分利用，需要通过动态资源管理来提高效率。

Method: 设计了一个资源管理层，能够动态迁移和扩展功能，并基于此设计了一个分层、数据驱动的微编排器，用于管理FPGA SoC设备中功能的生命周期。

Result: 通过一个计算机视觉边缘应用识别的场景事件，展示了资源管理层如何动态重新配置功能。

Conclusion: 该方法有效优化了FPGA SoC设备的计算资源利用，为5G和边缘计算提供了更高效的资源管理方案。

Abstract: This work presents a perspective on addressing the underutilization of
computing resources in FPGA SoC devices deployed in 5G radio and edge computing
infrastructure. The initial step in this approach involves developing a
resource management layer capable of dynamically migrating and scaling
functions within these devices in response to contextual events. This layer
serves as the foundation for designing a hierarchical, data-driven
micro-orchestrator responsible for managing the lifecycle of functions in FPGA
SoC devices. In this paper, the proposed resource management layer is utilized
to reconfigure a function based on events identified by a computer vision edge
application.

</details>


### [40] [Towards Next Generation Immersive Applications in 5G Environments](https://arxiv.org/abs/2507.20050)
*Rohail Asim,Ankit Bhardwaj,Lakshmi Suramanian,Yasir Zaki*

Main category: cs.NI

TL;DR: Hera是一种模块化框架，旨在解决下一代多用户沉浸式体验（MIR）中的无线网络瓶颈问题，通过集成应用感知的流媒体逻辑和延迟感知的速率控制协议，显著降低延迟并提升视觉质量。


<details>
  <summary>Details</summary>
Motivation: 无线网络的限制成为下一代MIR体验的关键瓶颈，尤其是在高带宽和超低延迟需求方面。Hera旨在填补应用响应性和网络适应性之间的鸿沟。

Method: Hera框架包括高层AR/VR流媒体与同步层和低层延迟驱动的QoE感知速率控制协议，支持自适应视频质量、多用户公平性和低延迟通信。

Result: 在动态5G网络中，Hera实现了比现有算法低66%的延迟、50%的平均比特率提升和更高的公平性。

Conclusion: Hera为更可扩展、稳健和高保真的多用户沉浸式体验奠定了基础。

Abstract: The Multi-user Immersive Reality (MIR) landscape is evolving rapidly, with
applications spanning virtual collaboration, entertainment, and training.
However, wireless network limitations create a critical bottleneck, struggling
to meet the high-bandwidth and ultra-low latency demands essential for
next-generation MIR experiences. This paper presents Hera, a modular framework
for next-generation immersive applications, comprising a high-level streaming
and synchronization layer for AR/VR systems and a low-level delay-based
QoE-aware rate control protocol optimized for dynamic wireless environments.
The Hera framework integrates application-aware streaming logic with a
QoE-centric rate control core, enabling adaptive video quality, multi-user
fairness, and low-latency communication across challenging 5G network
conditions. We demonstrate that Hera outperforms existing state-of-the-art rate
control algorithms by maintaining up to 66% lower latencies with comparable
throughput performance, higher visual quality with 50% average bitrate
improvements in our analysis, and improved fairness. By bridging the gap
between application-level responsiveness and network-level adaptability, Hera
lays the foundation for more scalable, robust, and high-fidelity multi-user
immersive experiences.

</details>


### [41] [Packet-Level DDoS Data Augmentation Using Dual-Stream Temporal-Field Diffusion](https://arxiv.org/abs/2507.20115)
*Gongli Xi,Ye Tian,Yannan Hu,Yuchao Zhang,Yapeng Niu,Xiangyang Gong*

Main category: cs.NI

TL;DR: 提出了一种基于扩散模型的双流时场扩散方法（DSTF-Diffusion）用于生成合成网络流量数据，解决了现有方法在捕捉复杂时空模式上的不足，提升了DDoS检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决DDoS攻击检测中标记数据集稀缺的问题，并提升合成数据的真实性和检测效果，作者提出了新的数据增强方法。

Method: 提出了双流时场扩散模型，包含场流和空间流，分别处理网络数据的空间映射和动态时间模式，利用扩散模型生成高相似度的合成数据。

Result: 实验表明，生成的数据在统计上与真实数据更相似，且在下游任务中表现优于现有方法。

Conclusion: DSTF-Diffusion能够有效提升合成数据的质量，从而增强DDoS检测的性能。

Abstract: In response to Distributed Denial of Service (DDoS) attacks, recent research
efforts increasingly rely on Machine Learning (ML)-based solutions, whose
effectiveness largely depends on the quality of labeled training datasets. To
address the scarcity of such datasets, data augmentation with synthetic traces
is often employed. However, current synthetic trace generation methods struggle
to capture the complex temporal patterns and spatial distributions exhibited in
emerging DDoS attacks. This results in insufficient resemblance to real traces
and unsatisfied detection accuracy when applied to ML tasks. In this paper, we
propose Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion), a multi-view,
multi-stream network traffic generative model based on diffusion models,
featuring two main streams: The field stream utilizes spatial mapping to bridge
network data characteristics with pre-trained realms of stable diffusion
models, effectively translating complex network interactions into formats that
stable diffusion can process, while the spatial stream adopts a dynamic
temporal modeling approach, meticulously capturing the intrinsic temporal
patterns of network traffic. Extensive experiments demonstrate that data
generated by our model exhibits higher statistical similarity to originals
compared to current state-of-the-art solutions, and enhance performances on a
wide range of downstream tasks.

</details>


### [42] [Accelerating Containerized Service Delivery at the Network Edge](https://arxiv.org/abs/2507.20116)
*Yinuo Deng,Hailiang Zhao,Dongjing Wang,Peng Chen,Wenzhuo Qian,Jianwei Yin,Schahram Dustdar,Shuiguang Deng*

Main category: cs.NI

TL;DR: PeerSync是一种基于P2P的去中心化系统，用于优化边缘环境中的容器镜像分发，显著提升速度和减少网络流量。


<details>
  <summary>Details</summary>
Motivation: 边缘环境中资源受限和动态网络条件对高效的容器镜像分发提出了挑战。

Method: PeerSync采用基于流行度和网络感知的下载引擎，结合滑动窗口机制、自动跟踪选举和动态缓存管理。

Result: 实验显示，PeerSync比Baseline、Dragonfly和Kraken分别提速2.72倍、1.79倍和1.28倍，并在拥堵网络下减少90.72%的跨网络流量。

Conclusion: PeerSync在边缘环境中高效优化了容器镜像分发，显著提升了性能。

Abstract: Efficient container image distribution is crucial for enabling machine
learning inference at the network edge, where resource limitations and dynamic
network conditions create significant challenges. In this paper, we present
PeerSync, a decentralized P2P-based system designed to optimize image
distribution in edge environments. PeerSync employs a popularity- and
network-aware download engine that dynamically adapts to content popularity and
real-time network conditions using a sliding window mechanism. PeerSync further
integrates automated tracker election for rapid peer discovery and dynamic
cache management for efficient storage utilization. We implement PeerSync with
8000+ lines of Rust code and test its performance extensively on both physical
edge devices and Docker-based emulations. Experimental results show that
PeerSync delivers a remarkable speed increase of 2.72$\times$, 1.79$\times$,
and 1.28$\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,
while significantly reducing peak cross-network traffic by 90.72\% under
congested and varying network conditions.

</details>


### [43] [Democracy for DAOs: An Empirical Study of Decentralized Governance and Dynamic (Case Study Internet Computer SNS Ecosystem)](https://arxiv.org/abs/2507.20234)
*Burak Arda Okutan,Stefan Schmid,Yvonne-Anne Pignolet*

Main category: cs.NI

TL;DR: 本文通过实证研究探讨了去中心化自治组织（DAO）中用户参与治理的行为，重点是使用互联网计算机协议（ICP）的SNS框架的DAO。研究发现，SNS DAO在活动水平、决策速度和成本效益方面表现更优，且用户参与度随时间持续增长。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于了解不同DAO框架下用户治理行为的差异，特别是SNS DAO的独特表现及其对去中心化治理的潜在影响。

Method: 研究方法包括量化分析3,000多个提案，涵盖14个SNS DAO，时间段为20个月。评估指标包括参与率、提案提交频率、投票通过率和决策时间。

Result: 结果显示，SNS DAO的投票通过率较高，决策更快，成本更低，且用户参与度未随时间下降，反而持续增长。

Conclusion: 结论表明SNS框架在提升DAO治理效率和用户参与度方面具有显著优势，为去中心化治理提供了可行方案。

Abstract: Decentralized autonomous organizations (DAOs) rely on governance mechanism
without centralized leadership. This paper presents an empirical study of user
behavior in governance for a variety of DAOs, ranging from DeFi to gaming,
using the Internet Computer Protocol DAO framework called SNS (Service Nervous
System). To analyse user engagement, we measure participation rates and
frequency of proposals submission and voter approval rates. We evaluate
decision duration times to determine DAO agility. To investigate dynamic
aspects, we also measure metric shifts in time. We evaluate over 3,000
proposals submitted in a time frame of 20 months from 14 SNS DAOs. The selected
DAO have been existing between 6 and 20 months and cover a wide spectrum of use
cases, treasury sizes, and number of participants. We also compare our results
for SNS DAOs with DAOs from other blockchain platforms. While approval rates
are generally high for all DAOs studied, SNS DAOs show slightly more alignment.
We observe that the SNS governance mechanisms and processes in ICP lead to
higher activity, lower costs and faster decisions. Most importantly, in
contrast to studies which report a decline in participation over time for other
frameworks, SNS DAOs exhibit sustained or increasing engagement levels over
time.

</details>


### [44] [Joint Fiber and Free Space Optical Infrastructure Planning for Hybrid Integrated Access and Backhaul Networks](https://arxiv.org/abs/2507.20367)
*Charitha Madapatha,Piotr Lechowicz,Carlos Natalino,Paolo Monti,Tommy Svensson*

Main category: cs.NI

TL;DR: 研究基础设施规划和优化对IAB网络覆盖的影响，特别是在光纤连接受限时使用FSO链路的性能增益和能效。


<details>
  <summary>Details</summary>
Motivation: 由于IAB网络中回程链路对高速率和高可靠性的敏感性，需要优化网络规划以确保其性能。光纤连接的高成本促使研究替代方案。

Method: 在光纤连接受限的情况下，研究使用自由空间光通信（FSO）链路的性能增益和能效。比较混合光纤/FSO部署与全光纤网络的成本效益。

Result: 混合光纤/FSO部署相比全光纤网络可显著节省成本，同时提高服务覆盖概率、能效和成本效益。

Conclusion: 通过合理的网络规划，IAB网络的服务覆盖、能效和成本效率均可得到提升，混合光纤/FSO部署是一种有益的权衡方案。

Abstract: Integrated access and backhaul (IAB) is one of the promising techniques for
5G networks and beyond (6G), in which the same node/hardware is used to provide
both backhaul and cellular services in a multi-hop architecture. Due to the
sensitivity of the backhaul links with high rate/reliability demands, proper
network planning is needed to ensure the IAB network performs with the desired
performance levels. In this paper, we study the effect of infrastructure
planning and optimization on the coverage of IAB networks. We concentrate on
the cases where the fiber connectivity to the nodes is constrained due to cost.
Thereby, we study the performance gains and energy efficiency in the presence
of free-space optical (FSO) communication links. Our results indicate hybrid
fiber/FSO deployments offer substantial cost savings compared to fully fibered
networks, suggesting a beneficial trade-off for strategic link deployment while
improving the service coverage probability. As we show, with proper network
planning, the service coverage, energy efficiency, and cost efficiency can be
improved.

</details>


### [45] [Teleoperating Autonomous Vehicles over Commercial 5G Networks: Are We There Yet?](https://arxiv.org/abs/2507.20438)
*Rostand A. K. Fezeu,Jason Carpenter,Rushikesh Zende,Sree Ganesh Lalitaditya Divakarla,Nitin Varyani,Faaiq Bilal,Steven Sleder,Nanditha Naik,Duncan Joly,Eman Ramadan,Ajay Kumar Gurumadaiah,Zhi-Li Zhang*

Main category: cs.NI

TL;DR: 本文系统研究了5G网络下自动驾驶车辆远程操控的可行性，重点关注上行传感器数据传输性能，分析了物理层和端到端因素对延迟的影响，并探讨了现有技术限制。


<details>
  <summary>Details</summary>
Motivation: 研究5G网络对自动驾驶车辆远程操控的支持能力，特别是传感器数据传输的实时性需求。

Method: 通过跨层和端到端视角，分析5G物理层因素（如信道条件、资源分配和切换）对延迟的影响，同时评估上层协议和QoE机制（如RTSP和WebRTC）的性能。

Result: 揭示当前5G网络在低延迟应用中的挑战，以及现有传感器数据流技术的局限性。

Conclusion: 为未来无线网络、边缘云系统和应用的协同设计提供参考，以突破自动驾驶远程操控的低延迟障碍。

Abstract: Remote driving, or teleoperating Autonomous Vehicles (AVs), is a key
application that emerging 5G networks aim to support. In this paper, we conduct
a systematic feasibility study of AV teleoperation over commercial 5G networks
from both cross-layer and end-to-end (E2E) perspectives. Given the critical
importance of timely delivery of sensor data, such as camera and LiDAR data,
for AV teleoperation, we focus in particular on the performance of uplink
sensor data delivery. We analyze the impacts of Physical Layer (PHY layer) 5G
radio network factors, including channel conditions, radio resource allocation,
and Handovers (HOs), on E2E latency performance. We also examine the impacts of
5G networks on the performance of upper-layer protocols and E2E application
Quality-of-Experience (QoE) adaptation mechanisms used for real-time sensor
data delivery, such as Real-Time Streaming Protocol (RTSP) and Web Real Time
Communication (WebRTC). Our study reveals the challenges posed by today's 5G
networks and the limitations of existing sensor data streaming mechanisms. The
insights gained will help inform the co-design of future-generation wireless
networks, edge cloud systems, and applications to overcome the low-latency
barriers in AV teleoperation.

</details>


### [46] [DD-JSCC: Dynamic Deep Joint Source-Channel Coding for Semantic Communications](https://arxiv.org/abs/2507.20467)
*Avi Deb Raha,Apurba Adhikary,Mrityunjoy Gain,Yumin Park,Walid Saad,Choong Seon Hong*

Main category: cs.NI

TL;DR: DD-JSCC提出了一种动态的深度联合源-信道编码架构，通过实时调整层结构适应不同设备和信道条件，提升了图像重建性能。


<details>
  <summary>Details</summary>
Motivation: 传统Deep-JSCC采用固定编码-解码结构，无法适应设备能力变化、实时性能优化及信道条件差异。

Method: DD-JSCC采用分层激活机制和隐式正则化，动态调整网络层结构。

Result: DD-JSCC在PSNR上比传统方法提升2 dB，训练成本降低40%。

Conclusion: DD-JSCC提供了一种高效且灵活的统一框架，显著降低了部署复杂性。

Abstract: Deep Joint Source-Channel Coding (Deep-JSCC) has emerged as a promising
semantic communication approach for wireless image transmission by jointly
optimizing source and channel coding using deep learning techniques. However,
traditional Deep-JSCC architectures employ fixed encoder-decoder structures,
limiting their adaptability to varying device capabilities, real-time
performance optimization, power constraints and channel conditions. To address
these limitations, we propose DD-JSCC: Dynamic Deep Joint Source-Channel Coding
for Semantic Communications, a novel encoder-decoder architecture designed for
semantic communication systems. Unlike traditional Deep-JSCC models, DD-JSCC is
flexible for dynamically adjusting its layer structures in real-time based on
transmitter and receiver capabilities, power constraints, compression ratios,
and current channel conditions. This adaptability is achieved through a
hierarchical layer activation mechanism combined with implicit regularization
via sequential randomized training, effectively reducing combinatorial
complexity, preventing overfitting, and ensuring consistent feature
representations across varying configurations. Simulation results demonstrate
that DD-JSCC enhances the performance of image reconstruction in semantic
communications, achieving up to 2 dB improvement in Peak Signal-to-Noise Ratio
(PSNR) over fixed Deep-JSCC architectures, while reducing training costs by
over 40%. The proposed unified framework eliminates the need for multiple
specialized models, significantly reducing training complexity and deployment
overhead.

</details>


### [47] [A Lyapunov-Guided Diffusion-Based Reinforcement Learning Approach for UAV-Assisted Vehicular Networks with Delayed CSI Feedback](https://arxiv.org/abs/2507.20524)
*Zhang Liu,Lianfen Huang,Zhibin Gao,Xianbin Wang,Dusit Niyato,Xuemin,Shen*

Main category: cs.NI

TL;DR: 论文探讨了低空无人机（UAV）在车联网中的智能通信策略，提出了联合优化信道分配、功率控制和飞行高度的方案，并通过D3PG算法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 低空无人机在智能交通系统中潜力巨大，但面临动态资源优化、续航能力有限和信道状态信息不完善等挑战。

Method: 利用Lyapunov优化将长期问题分解为逐时段的子问题，并创新性地结合扩散模型提出D3PG算法。

Result: 仿真实验表明，D3PG算法在真实车辆移动数据下优于现有解决方案。

Conclusion: 该研究方法为低空经济网络提供了高效且能量优化的通信策略。

Abstract: Low altitude uncrewed aerial vehicles (UAVs) are expected to facilitate the
development of aerial-ground integrated intelligent transportation systems and
unlocking the potential of the emerging low-altitude economy. However, several
critical challenges persist, including the dynamic optimization of network
resources and UAV trajectories, limited UAV endurance, and imperfect channel
state information (CSI). In this paper, we offer new insights into low-altitude
economy networking by exploring intelligent UAV-assisted vehicle-to-everything
communication strategies aligned with UAV energy efficiency. Particularly, we
formulate an optimization problem of joint channel allocation, power control,
and flight altitude adjustment in UAV-assisted vehicular networks. Taking CSI
feedback delay into account, our objective is to maximize the vehicle-to-UAV
communication sum rate while satisfying the UAV's long-term energy constraint.
To this end, we first leverage Lyapunov optimization to decompose the original
long-term problem into a series of per-slot deterministic subproblems. We then
propose a diffusion-based deep deterministic policy gradient (D3PG) algorithm,
which innovatively integrates diffusion models to determine optimal channel
allocation, power control, and flight altitude adjustment decisions. Through
extensive simulations using real-world vehicle mobility traces, we demonstrate
the superior performance of the proposed D3PG algorithm compared to existing
benchmark solutions.

</details>


### [48] [Collusion Resistant DNS With Private Information Retrieval](https://arxiv.org/abs/2507.20806)
*Yunming Xiao,Peizhi Liu,Ruijie Yu,Chenkai Weng,Matteo Varvello,Aleksandar Kuzmanovic*

Main category: cs.NI

TL;DR: 提出了一种基于单服务器私有信息检索（PIR）的DNS扩展PDNS，用于增强隐私保护，性能优于现有方案，但扩展性受限。


<details>
  <summary>Details</summary>
Motivation: DNS隐私问题长期被忽视，现有解决方案依赖于难以保证的非共谋假设，亟需新技术以提升隐私保护。

Method: 提出PDNS，通过单服务器PIR技术实现加密查询处理，避免信任假设，并解决了DNS层次结构带来的信息泄漏问题。

Result: PDNS原型性能优于DoH over Tor（快2倍），提供强隐私保护，但扩展性受限需依赖专用硬件。

Conclusion: PDNS为DNS隐私保护提供了可行方案，未来可通过专用硬件解决扩展性问题。

Abstract: There has been a growing interest in Internet user privacy, demonstrated by
the popularity of privacy-preserving products such as Telegram and Brave, and
the widespread adoption of HTTPS. The Domain Name System (DNS) is a key
component of Internet-based communication and its privacy has been neglected
for years. Recently, DNS over HTTPS (DoH) has improved the situation by fixing
the issue of in-path middleboxes. Further progress has been made with
proxy-based solutions such as Oblivious DoH (ODoH), which separate a user's
identity from their DNS queries. However, these solutions rely on non-collusion
assumptions between DNS resolvers and proxies -- an assumption difficult to
guarantee in practice. To address this, we explore integrating single-server
Private Information Retrieval (PIR) into DNS to enable encrypted query
processing without relying on trust assumptions. However, applying PIR to DNS
is challenging due to its hierarchical nature -- particularly, interactions
with recursive resolvers can still leak information. Navigating performance and
privacy trade-offs, we propose PDNS, a DNS extension leveraging single-server
PIR to strengthen privacy guarantees. We have implemented a prototype of PDNS
and compared its performance against state-of-the-art solutions via
trace-driven experiments. The results show that PDNS achieves acceptable
performance (2x faster than DoH over Tor with similar privacy guarantees) and
strong privacy guarantees today, mainly at the cost of its scalability, which
specialized hardware for PIR can address in the near future.

</details>


### [49] [\textit{FedABC}: Attention-Based Client Selection for Federated Learning with Long-Term View](https://arxiv.org/abs/2507.20871)
*Wenxuan Ye,Xueli An,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.NI

TL;DR: 论文提出了一种名为FedABC的创新客户端选择算法，用于优化联邦学习中的数据异构性和客户端参与效率，显著提升了模型准确性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 在6G网络中实现原生AI支持的需求，以及联邦学习中数据异质性和频繁客户端参与带来的挑战促使了这一研究。

Method: 提出了基于注意力机制的FedABC算法，通过评估模型相似性和客户端对全局模型的贡献来优化客户端选择，并根据“后期更优”原则自适应调整选择阈值。

Result: 在CIFAR-10数据集上的实验表明，FedABC比经典算法FedAvg使用32%更少的客户端达到相同性能，比现有最优算法减少2%客户端并提升3.5%准确率。

Conclusion: FedABC为解决异构和资源受限环境中的联邦学习问题提供了有效解决方案，推动了6G网络中AI能力的实现。

Abstract: Native AI support is a key objective in the evolution of 6G networks, with
Federated Learning (FL) emerging as a promising paradigm. FL allows
decentralized clients to collaboratively train an AI model without directly
sharing their data, preserving privacy. Clients train local models on private
data and share model updates, which a central server aggregates to refine the
global model and redistribute it for the next iteration. However, client data
heterogeneity slows convergence and reduces model accuracy, and frequent client
participation imposes communication and computational burdens. To address these
challenges, we propose \textit{FedABC}, an innovative client selection
algorithm designed to take a long-term view in managing data heterogeneity and
optimizing client participation. Inspired by attention mechanisms,
\textit{FedABC} prioritizes informative clients by evaluating both model
similarity and each model's unique contributions to the global model. Moreover,
considering the evolving demands of the global model, we formulate an
optimization problem to guide \textit{FedABC} throughout the training process.
Following the ``later-is-better" principle, \textit{FedABC} adaptively adjusts
the client selection threshold, encouraging greater participation in later
training stages. Extensive simulations on CIFAR-10 demonstrate that
\textit{FedABC} significantly outperforms existing approaches in model accuracy
and client participation efficiency, achieving comparable performance with 32\%
fewer clients than the classical FL algorithm \textit{FedAvg}, and 3.5\% higher
accuracy with 2\% fewer clients than the state-of-the-art. This work marks a
step toward deploying FL in heterogeneous, resource-constrained environments,
thereby supporting native AI capabilities in 6G networks.

</details>


### [50] [Towards a Robust Transport Network With Self-adaptive Network Digital Twin](https://arxiv.org/abs/2507.20971)
*Cláudio Modesto,João Borges,Cleverson Nahum,Lucas Matni,Cristiano Bonato Both,Kleber Cardoso,Glauco Gonçalves,Ilan Correa,Silvia Lins,Andrey Silva,Aldebaro Klautau*

Main category: cs.NI

TL;DR: 本文提出了一种自适应的网络数字孪生（NDT）架构，用于在流量波动情况下提供准确的延迟预测，并通过概念漂移检测技术优化虚拟孪生（VTwin）与物理孪生（PTwin）的同步。验证结果显示，该架构在流量变化后的预测性能提升至少56.7%。


<details>
  <summary>Details</summary>
Motivation: 网络数字孪生（NDT）需要及时感知物理孪生（PTwin）的变化以实现同步。现有研究对数据驱动的NDT平台在流量波动下的弹性和同步问题探讨不足，因此本文旨在解决这一问题。

Method: 提出了一种自适应的NDT架构，利用遥测模块监控流量，并通过概念漂移检测技术指导VTwin的重新训练和部署。在模拟网络拓扑和多样流量模式中验证架构。

Result: 该架构在流量概念漂移后，预测性能提高了至少56.7%（以标准化均方误差为指标），在所有测试拓扑中均有效。

Conclusion: 本文提出的自适应NDT架构显著提升了流量波动下的延迟预测性能，为网络管理提供了更高效的解决方案。

Abstract: The ability of the network digital twin (NDT) to remain aware of changes in
its physical counterpart, known as the physical twin (PTwin), is a fundamental
condition to enable timely synchronization, also referred to as twinning. In
this way, considering a transport network, a key requirement is to handle
unexpected traffic variability and dynamically adapt to maintain optimal
performance in the associated virtual model, known as the virtual twin (VTwin).
In this context, we propose a self-adaptive implementation of a novel NDT
architecture designed to provide accurate delay predictions, even under
fluctuating traffic conditions. This architecture addresses an essential
challenge, underexplored in the literature: improving the resilience of
data-driven NDT platforms against traffic variability and improving
synchronization between the VTwin and its physical counterpart. Therefore, the
contributions of this article rely on NDT lifecycle by focusing on the
operational phase, where telemetry modules are used to monitor incoming
traffic, and concept drift detection techniques guide retraining decisions
aimed at updating and redeploying the VTwin when necessary. We validate our
architecture with a network management use case, across various emulated
network topologies, and diverse traffic patterns to demonstrate its
effectiveness in preserving acceptable performance and predicting per-flow
delay under unexpected traffic variation. The results in all tested topologies,
using the normalized mean square error as the evaluation metric, demonstrate
that our proposed architecture, after a traffic concept drift, achieves a
performance improvement in prediction of at least 56.7% compared to a
configuration without NDT synchronization.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [51] [Anchoring Trends: Mitigating Social Media Popularity Prediction Drift via Feature Clustering and Expansion](https://arxiv.org/abs/2507.19863)
*Chia-Ming Lee,Bo-Cheng Qiu,Cheng-Jun Kang,Yi-Hsuan Wu,Jun-Lin Chen,Yu-Fan Lin,Yi-Shiuan Chou,Chih-Chung Hsu*

Main category: cs.MM

TL;DR: AMCFG框架通过多模态聚类和LLMs生成语义锚特征，解决视频流行度预测中的时间分布偏移问题，显著提升了预测准确性和时间鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在线视频流行度预测面临时间分布偏移（预测漂移）的挑战，传统模型因病毒趋势和用户行为的快速变化而性能下降。

Method: 提出AMCFG框架，结合多模态聚类和LLMs生成语义锚特征（如受众人口统计、内容主题等），利用稳定特征进行预测。

Result: 实验表明，AMCFG在分布外数据上表现优异，显著提升预测准确性和时间鲁棒性。

Conclusion: AMCFG为在线视频流行度预测提供了可行的解决方案，基于稳定原则而非短暂信号进行预测。

Abstract: Predicting online video popularity faces a critical challenge: prediction
drift, where models trained on historical data rapidly degrade due to evolving
viral trends and user behaviors. To address this temporal distribution shift,
we propose an Anchored Multi-modal Clustering and Feature Generation (AMCFG)
framework that discovers temporally-invariant patterns across data
distributions. Our approach employs multi-modal clustering to reveal content
structure, then leverages Large Language Models (LLMs) to generate semantic
Anchor Features, such as audience demographics, content themes, and engagement
patterns that transcend superficial trend variations. These semantic anchors,
combined with cluster-derived statistical features, enable prediction based on
stable principles rather than ephemeral signals. Experiments demonstrate that
AMCFG significantly enhances both predictive accuracy and temporal robustness,
achieving superior performance on out-of-distribution data and providing a
viable solution for real-world video popularity prediction.

</details>


### [52] [Controllable Video-to-Music Generation with Multiple Time-Varying Conditions](https://arxiv.org/abs/2507.20627)
*Junxian Wu,Weitao You,Heda Zuo,Dengming Zhang,Pei Chen,Lingyun Sun*

Main category: cs.MM

TL;DR: 提出了一种多条件引导的视频到音乐生成框架，通过两阶段训练策略提升控制和同步能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖视觉或文本输入，生成效果难以满足用户期望，需增强控制能力。

Method: 两阶段训练：第一阶段细化特征选择和时序对齐；第二阶段动态融合条件并引导音乐生成。

Result: 实验表明，该方法在主观和客观评估中优于现有方法，显著提升用户满意度。

Conclusion: 多条件引导框架能有效改进视频到音乐的生成质量和用户控制体验。

Abstract: Music enhances video narratives and emotions, driving demand for automatic
video-to-music (V2M) generation. However, existing V2M methods relying solely
on visual features or supplementary textual inputs generate music in a
black-box manner, often failing to meet user expectations. To address this
challenge, we propose a novel multi-condition guided V2M generation framework
that incorporates multiple time-varying conditions for enhanced control over
music generation. Our method uses a two-stage training strategy that enables
learning of V2M fundamentals and audiovisual temporal synchronization while
meeting users' needs for multi-condition control. In the first stage, we
introduce a fine-grained feature selection module and a progressive temporal
alignment attention mechanism to ensure flexible feature alignment. For the
second stage, we develop a dynamic conditional fusion module and a
control-guided decoder module to integrate multiple conditions and accurately
guide the music composition process. Extensive experiments demonstrate that our
method outperforms existing V2M pipelines in both subjective and objective
evaluations, significantly enhancing control and alignment with user
expectations.

</details>


### [53] [Dark Side of Modalities: Reinforced Multimodal Distillation for Multimodal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.20738)
*Yu Zhao,Ying Zhang,Xuhui Sui,Baohang Zhou,Haoze Zhu,Jeff Z. Pan,Xiaojie Yuan*

Main category: cs.MM

TL;DR: 提出了一种名为DSoM的多模态知识图谱推理框架，通过利用非目标实体的概率相关性和动态选择有用模态，解决了现有方法忽略相关性及模态负面影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个主要问题：一是忽略了非目标实体标签的概率相关性，二是静态或自适应地组合所有模态，未能排除无关或误导性信息的负面影响。

Method: 提出了Reinforced Multimodal Distillation框架，包括（1）通过logit蒸馏利用非目标实体的暗知识；（2）通过强化学习动态选择最优多模态教师组合，排除无帮助模态的负面影响。

Result: 在5个MKGR数据集上的实验证明了DSoM框架的有效性。

Conclusion: 该框架通过充分利用多模态信息的暗知识和动态模态选择，显著提升了多模态知识图谱推理的性能。

Abstract: The multimodal knowledge graph reasoning (MKGR) task aims to predict the
missing facts in the incomplete MKGs by leveraging auxiliary images and
descriptions of entities. Existing approaches are trained with single-target
objectives, which neglect the probabilistic correlations of entity labels,
especially in non-target entities. Moreover, previous studies incorporate all
modalities statically or adaptively, overlooking the negative impacts of
irrelevant or misleading information in the incompetent modalities. To address
these issues, we introduce a novel Reinforced Multimodal Distillation
framework, exploiting the Dark Side of Modalities (DSoM) from two perspectives:
(1) Dark knowledge from non-target entities: We propose to train a unimodal KGR
model through logit distillation to mimic the multimodal soft labels provided
by pre-trained multimodal teacher models. The multimodal soft labels could
provide rich supervision signals with subtle correlations among both target and
non-target entities from multiple perspectives. We further decouple logits into
neighbor entities and non-neighbor entities to divide into two types of
correlations. (2) Dark side in unhelpful modalities: To exclude the adverse
effects of unhelpful modalities, we introduce a reinforced teacher combination
mechanism that dynamically selects the optimal set of multimodal teachers for
each triple. The agent is trained to maximize the rewards, which are only
assigned to the beneficial multimodal combination strategies for the student
model. Comprehensive experiments demonstrate the effectiveness of DSoM
framework on 5 MKGR datasets. Codes are available at github.com/OreOZhao/DSoM.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [54] [Graded Quantitative Narrowing](https://arxiv.org/abs/2507.19630)
*Mauricio Ayala-Rincón,Thaynara Arielly de Lima,Georg Ehling,Temur Kutsia*

Main category: cs.LO

TL;DR: 论文提出了一种基于量化重写的扩展框架——分级量化重写，并通过量化窄化解决了Lawverean量化半环上的量化方程问题。


<details>
  <summary>Details</summary>
Motivation: 传统重写系统缺乏对度量方面的支持，如项之间的接近度和基于重写的计算复杂性。分级量化重写框架通过量化规则扩展了传统系统，而量化窄化进一步通过替换匹配为统一化，提升了解决变量项问题的能力。

Method: 提出量化窄化方法，将匹配替换为统一化，同时在Lawverean量化半环上建立量化窄化的理论基础，证明了其可靠性，并讨论了确保完备性的条件。

Result: 量化窄化能解决比以往方法更丰富的量化方程理论，为量化方程问题提供了新的解决途径。

Conclusion: 量化窄化为分级量化重写框架提供了更通用的解决方案，扩展了其在量化方程理论中的应用范围。

Abstract: The recently introduced framework of Graded Quantitative Rewriting is an
innovative extension of traditional rewriting systems, in which rules are
annotated with degrees drawn from a quantale. This framework provides a robust
foundation for equational reasoning that incorporates metric aspects, such as
the proximity between terms and the complexity of rewriting-based computations.
Quantitative narrowing, introduced in this paper, generalizes quantitative
rewriting by replacing matching with unification in reduction steps, enabling
the reduction of terms even when they contain variables, through simultaneous
instantiation and rewriting. In the standard (non-quantitative) setting,
narrowing has been successfully applied in various domains, including
functional logic programming, theorem proving, and equational unification.
Here, we focus on quantitative narrowing to solve unification problems in
quantitative equational theories over Lawverean quantales. We establish its
soundness and discuss conditions under which completeness can be ensured. This
approach allows us to solve quantitative equations in richer theories than
those addressed by previous methods.

</details>


### [55] [Scroll nets](https://arxiv.org/abs/2507.19689)
*Pablo Donato*

Main category: cs.LO

TL;DR: 该论文提出了一种名为「卷轴网」的新形式主义，用于表示命题逻辑中的证明，基于Peirce的拓扑符号「卷轴」，并通过Curry-Howard方法从存在图中推导出。论文展示了其逻辑和计算表达能力。


<details>
  <summary>Details</summary>
Motivation: 旨在通过拓扑和图形方法重新表示命题逻辑中的证明，继承并扩展Peirce的存在图思想，同时结合Curry-Howard对应关系，探索其在计算和逻辑中的应用。

Method: 从存在图中推导出「卷轴网」，通过图形理论定义其组合本质，并设计了一个类似于切割消除的绕行消除程序。

Result: 展示了卷轴网能够模拟简单类型λ-演算的规范化，证明了其逻辑和计算的表达能力。

Conclusion: 卷轴网为命题逻辑提供了一种新的图形化证明表示方法，具有潜在的逻辑和计算应用价值。

Abstract: We introduce a new formalism for representing proofs in propositional logic
called "scroll nets". Its fundamental construct is the "scroll", a topological
notation for implication proposed by C. S. Peirce at the end of the 19th
century as the basis for his diagrammatic system of existential graphs (EGs).
Scroll nets are derived from EGs by following the Curry-Howard methodology of
internalizing inference rules inside judgments, just as terms in type theory
internalize natural deduction rules. We focus on the intuitionistic implicative
fragment of EGs, starting from a natural diagrammatic representation of scroll
nets, and then distilling their combinatorial essence into a purely
graph-theoretic definition. We also identify a notion of detour, that we use to
sketch a detour-elimination procedure akin to cut-elimination. We illustrate
how to simulate normalization in the simply typed $\lambda$-calculus,
demonstrating both the logical and computational expressivity of our framework.

</details>


### [56] [Synthesis Benchmarks for Automated Reasoning](https://arxiv.org/abs/2507.19827)
*Márton Hajdu,Petra Hozzová,Laura Kovács,Andrei Voronkov,Eva Maria Wagner,Richard Steven Žilinčík*

Main category: cs.LO

TL;DR: 提出一个动态增长的$orallorall$-形式合成基准数据集，填补现有空白。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏支持$orallorall$-形式和非计算符号限制的基准数据集，阻碍了合成理论和技术的发展。

Method: 通过补充现有基准和新基准，构建数据集。

Result: 创建了一个动态增长的数据集。

Conclusion: 该数据集有望推动合成理论和自动化实践的发展。

Abstract: Program synthesis is the task of constructing a program conforming to a given
specification. We focus on deductive synthesis, and in particular on synthesis
problems with specifications given as $\forall\exists$-formulas, expressing the
existence of an output corresponding to any input. So far there has been no
canonical benchmark set for deductive synthesis using the
$\forall\exists$-format and supporting the so-called uncomputable symbol
restriction. This work presents such a data set, composed by complementing
existing benchmarks by new ones. Our data set is dynamically growing and should
motivate future developments in the theory and practice of automating
synthesis.

</details>


### [57] [A Model-Independent Theory of Probabilistic Testing](https://arxiv.org/abs/2507.19886)
*Weijun Chen,Yuxi Fu,Huan Long,Hao Wu*

Main category: cs.LO

TL;DR: 本文提出了一种独立于模型的概率测试方法，研究了概率测试等价的内外部表征，并与概率双相似性进行了比较。


<details>
  <summary>Details</summary>
Motivation: 为现代移动计算的概率并发系统提供一个通用的测试方法。

Method: 采用基于分布的语义和概率测试框架，研究概率测试等价的内外部表征。

Result: 证明了这些等价与概率双相似性的关系，并展示了方法的可扩展性。

Conclusion: 所提技术适用于其他概率并发模型，具有广泛的应用前景。

Abstract: Probabilistic concurrent systems are foundational models for modern mobile
computing. In this paper, a general model-independent approach to probabilistic
testing is proposed. With the help of a new distribution-based semantics for
probabilistic models and a probabilistic testing framework with respect to
process predicates, the model-independent characterization and the external
characterization for testing equivalences are studied. The latter
characterization can be viewed as the generalization of the classical
fair/should equivalence and may equivalence. These equivalences are shown to be
congruent. A thorough comparison between these equivalences and probabilistic
bisimilarities is carried out. The techniques introduced in this paper can be
easily extended to other probabilistic concurrent models.

</details>


### [58] [Active Monitoring with RTLola: A Specification-Guided Scheduling Approach](https://arxiv.org/abs/2507.20615)
*Jan Baumeister,Bernd Finkbeiner,Frederik Scheerer*

Main category: cs.LO

TL;DR: 将监测器变为主动组件，根据内部状态动态调度传感器查询，提升资源受限环境下的监测效率。


<details>
  <summary>Details</summary>
Motivation: 传统被动监测方式在资源受限环境中效率低下，需改进以优化通信资源利用。

Method: 通过规范中的调度注释，动态分配带宽至最相关数据，使用RTLola语言实现。

Result: 在相同带宽下，主动监测比固定频率采样更早发现规范违规。

Conclusion: 主动查询机制显著提升监测效率，适用于资源受限环境。

Abstract: Stream-based monitoring is a well-established runtime verification approach
which relates input streams, representing sensor readings from the monitored
system, with output streams that capture filtered or aggregated results. In
such approaches, the monitor is a passive external component that continuously
receives sensor data from the system under observation. This setup assumes that
the system dictates what data is sent and when, regardless of the monitor's
current needs. However, in many applications -- particularly in
resource-constrained environments like autonomous aircraft, where energy, size,
or weight are limited -- this can lead to inefficient use of communication
resources. We propose making the monitor an active component that decides,
based on its current internal state, which sensors to query and how often. This
behavior is driven by scheduling annotations in the specification, which guide
the dynamic allocation of bandwidth towards the most relevant data, thereby
improving monitoring efficiency. We demonstrate our approach using the
stream-based specification language RTLola and assess the performance by
monitoring a specification from the aerospace domain. With equal bandwidth
usage, our approach detects specification violations significantly sooner than
monitors sampling all inputs at a fixed frequency.

</details>


### [59] [Automated Catamorphism Synthesis for Solving Constrained Horn Clauses over Algebraic Data Types](https://arxiv.org/abs/2507.20726)
*Hiroyuki Katsura,Naoki Kobayashi,Ken Sakayori,Ryosuke Sato*

Main category: cs.LO

TL;DR: 提出了一种基于Catamorphisms的新型CHC求解方法，显著提升了ADT问题的求解能力。


<details>
  <summary>Details</summary>
Motivation: 现有CHC求解器对ADT问题的处理能力有限，无法有效表达涉及归纳定义函数/谓词的模型。

Method: 利用Catamorphisms（广义折叠函数）自动发现并表达CHC模型，开发了新型求解器Catalia。

Result: Catalia在CHC-COMP 2024基准测试中表现优于现有技术，且其衍生工具ChocoCatalia在CHC-COMP 2025中获奖。

Conclusion: Catamorphisms方法显著提升了CHC求解能力，为ADT问题的自动化验证提供了有效方案。

Abstract: We propose a novel approach to satisfiability checking of Constrained Horn
Clauses (CHCs) over Algebraic Data Types (ADTs). CHC-based automated
verification has gained considerable attention in recent years, leading to the
development of various CHC solvers. However, existing solvers for CHCs over
ADTs are not fully satisfactory, due to their limited ability to find and
express models involving inductively defined functions/predicates (e.g., those
about the sum of list elements). To address this limitation, we consider
catamorphisms (generalized fold functions), and present a framework for
automatically discovering appropriate catamorphisms on demand and using them to
express a model of given CHCs. We have implemented a new CHC solver called
Catalia based on the proposed method. Our experimental results for the CHC-COMP
2024 benchmark show that Catalia outperforms state-of-the-art solvers in
solving satisfiable CHCs over ADTs. Catalia was also used as a core part of the
tool called ChocoCatalia, which won the ADT-LIA category of CHC-COMP 2025.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [60] [The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox in Human-AI Cognitive Integration](https://arxiv.org/abs/2507.19483)
*Giuseppe Riva*

Main category: cs.HC

TL;DR: AI系统从工具演变为人类认知的主动协作伙伴，但可能因过度舒适导致认知停滞。提出'增强认知支架'框架，通过渐进自主、自适应个性化和认知负载优化，解决'舒适-成长悖论'，促进真正认知提升。


<details>
  <summary>Details</summary>
Motivation: AI的便捷性可能导致用户认知停滞，需平衡便利与成长。

Method: 基于维果茨基理论、教育支架原则和AI伦理，设计框架包含渐进自主、自适应个性化和认知负载优化三个维度。

Result: 在多个领域验证了技能习得加速、自我调节能力提升及高阶思维发展。

Conclusion: 增强认知支架通过优先认知发展，为人类-AI协作提供可持续成长路径。

Abstract: AI systems now function as cognitive extensions, evolving from tools to
active cognitive collaborators within human-AI integrated systems. While these
systems can amplify cognition - enhancing problem-solving, learning, and
creativity - they present a fundamental "comfort-growth paradox": AI's
user-friendly nature may foster intellectual stagnation by minimizing cognitive
friction necessary for development. As AI aligns with user preferences and
provides frictionless assistance, it risks inducing cognitive complacency
rather than promoting growth. We introduce Enhanced Cognitive Scaffolding to
resolve this paradox - reconceptualizing AI from convenient assistant to
dynamic mentor. Drawing from Vygotskian theories, educational scaffolding
principles, and AI ethics, our framework integrates three dimensions: (1)
Progressive Autonomy, where AI support gradually fades as user competence
increases; (2) Adaptive Personalization, tailoring assistance to individual
needs and learning trajectories; and (3) Cognitive Load Optimization, balancing
mental effort to maximize learning while minimizing unnecessary complexity.
Research across educational, workplace, creative, and healthcare domains
supports this approach, demonstrating accelerated skill acquisition, improved
self-regulation, and enhanced higher-order thinking. The framework includes
safeguards against risks like dependency, skill atrophy, and bias
amplification. By prioritizing cognitive development over convenience in
human-AI interaction, Enhanced Cognitive Scaffolding offers a pathway toward
genuinely amplified cognition while safeguarding autonomous thought and
continuous learning.

</details>


### [61] [Creativity as a Human Right: Design Considerations for Computational Creativity Systems](https://arxiv.org/abs/2507.19485)
*Alayt Issak*

Main category: cs.HC

TL;DR: 该论文探讨了《世界人权宣言》中强调的创造力，并提出了计算创造力系统的设计考量。研究认为该宣言强调了创造力的重要方面，并将其视为第四代人权的体现。文章通过分析宣言中的五条条款，为计算创造力系统提出了设计建议。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过《世界人权宣言》的视角，探讨创造力与计算创造力系统之间的关系，以解决如何设计这些系统的问题。

Method: 方法论包括分析《世界人权宣言》中的五条款项，并通过实际案例展示每一条款，最终为每条条款提出设计建议。

Result: 研究强调了创造力作为人权的第四代表现形式，并提供了计算创造力系统的设计考量。

Conclusion: 研究为计算创造力系统与创造力关系的理论基础提供了贡献，并提出了实用的设计建议。

Abstract: We investigate creativity that is underlined in the Universal Declaration of
Human Rights (UDHR) to present design considerations for Computational
Creativity (CC) systems. We find this declaration to describe creativity in
salient aspects and bring to light creativity as a Human Right attributed to
the Fourth Generation of such rights. This generation of rights attributes CC
systems and the evolving nature of interaction with entities of shared
intelligence. Our methodology examines five of thirty articles from the UDHR
and demonstrates each article with actualizations concluding with design
considerations for each. We contribute our findings to ground the relationship
between creativity and CC systems.

</details>


### [62] [Confirmation bias: A challenge for scalable oversight](https://arxiv.org/abs/2507.19486)
*Gabriel Recchia,Chatrik Singh Mangat,Jinu Nyachhyon,Mridul Sharma,Callum Canavan,Dylan Epstein-Gross,Muhammed Abdulbari*

Main category: cs.HC

TL;DR: 研究表明，简单的监督协议在克服评估者偏见方面效果有限，未表现出明显优势，且随着模型能力的提升，其有效性可能会降低。


<details>
  <summary>Details</summary>
Motivation: 探讨监督协议如何帮助评估者准确验证比自身更强大的AI模型，同时克服评估者的系统性偏见。

Method: 通过两项研究，测试评估者在知道模型‘大多数时候正确，但并非总是正确’的情况下，使用简单监督协议的表现。

Result: 研究发现监督协议整体上无优势，但展示正反论据在模型错误时能提高准确性。评估者在线研究后对系统答案信心增强，即使答案是错误的。

Conclusion: 研究强调了监督协议需克服评估者偏见、超越简单依赖模型的能力，并随问题难度和模型能力扩展的重要性。

Abstract: Scalable oversight protocols aim to empower evaluators to accurately verify
AI models more capable than themselves. However, human evaluators are subject
to biases that can lead to systematic errors. We conduct two studies examining
the performance of simple oversight protocols where evaluators know that the
model is "correct most of the time, but not all of the time". We find no
overall advantage for the tested protocols, although in Study 1, showing
arguments in favor of both answers improves accuracy in cases where the model
is incorrect. In Study 2, participants in both groups become more confident in
the system's answers after conducting online research, even when those answers
are incorrect. We also reanalyze data from prior work that was more optimistic
about simple protocols, finding that human evaluators possessing knowledge
absent from models likely contributed to their positive results--an advantage
that diminishes as models continue to scale in capability. These findings
underscore the importance of testing the degree to which oversight protocols
are robust to evaluator biases, whether they outperform simple deference to the
model under evaluation, and whether their performance scales with increasing
problem difficulty and model capability.

</details>


### [63] [E-polis: Gamifying Sociological Surveys through Serious Games -- A Data Analysis Approach Applied to Multiple-Choice Question Responses Datasets](https://arxiv.org/abs/2507.19488)
*Alexandros Gazis,Eleftheria Katsiri*

Main category: cs.HC

TL;DR: E-polis是一款通过游戏化方式研究年轻人政治观点的数字游戏平台，利用数据分析和中间件架构进行创新研究。


<details>
  <summary>Details</summary>
Motivation: 研究年轻人政治观点，通过游戏化方式收集和分析数据，提供新的社会学研究方法。

Method: 开发E-polis平台游戏，玩家通过回答社会学问题塑造游戏世界，使用中间件架构进行数据采集和分析。

Result: 游戏数据可用于分析玩家行为和政治观点，帮助理解年轻人对城市未来的看法。

Conclusion: E-polis为社会学研究提供了创新的游戏化平台，通过数据可视化支持政治观点研究。

Abstract: E-polis is a serious digital game designed to gamify sociological surveys
studying young people's political opinions. In this platform game, players
navigate a digital world, encountering quests posing sociological questions.
Players' answers shape the city-game world, altering building structures based
on their choices. E-polis is a serious game, not a government simulation,
aiming to understand players' behaviors and opinions thus we do not train the
players but rather understand them and help them visualize their choices in
shaping a city's future. Also, it is noticed that no correct or incorrect
answers apply. Moreover, our game utilizes a novel middleware architecture for
development, diverging from typical asset prefab scene and script segregation.
This article presents the data layer of our game's middleware, specifically
focusing on data analysis based on respondents' gameplay answers. E-polis
represents an innovative approach to gamifying sociological research, providing
a unique platform for gathering and analyzing data on political opinions among
youth and contributing to the broader field of serious games.

</details>


### [64] [RISEE: A Highly Interactive Naturalistic Driving Trajectories Dataset with Human Subjective Risk Perception and Eye-tracking Information](https://arxiv.org/abs/2507.19490)
*Xinzheng Wu,Junyi Chen,Peiyi Wang,Shunxiang Chen,Yong Shen*

Main category: cs.HC

TL;DR: 论文提出了RISEE数据集，整合人类主观评价和眼动数据，以解决现有数据集缺乏人类相关信息和安全关键场景的问题。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶决策和规划系统的研究缺乏人类因素数据和足够的安全关键场景，而模拟数据真实性不足。

Method: 通过无人机和模拟结合的方法，首先在高真实性的高速公路匝道区域录制交通视频，然后将高交互场景重建为模拟视频，收集参与者的主观评价和眼动数据。

Result: 成功构建的RISEE数据集包含来自101名参与者的3567个有效主观风险评分和2045个合格眼动数据段。

Conclusion: 该数据集填补了现有数据的不足，为自动驾驶系统开发提供了更全面的评估工具。

Abstract: In the research and development (R&D) and verification and validation (V&V)
phases of autonomous driving decision-making and planning systems, it is
necessary to integrate human factors to achieve decision-making and evaluation
that align with human cognition. However, most existing datasets primarily
focus on vehicle motion states and trajectories, neglecting human-related
information. In addition, current naturalistic driving datasets lack sufficient
safety-critical scenarios while simulated datasets suffer from low
authenticity. To address these issues, this paper constructs the Risk-Informed
Subjective Evaluation and Eye-tracking (RISEE) dataset which specifically
contains human subjective evaluations and eye-tracking data apart from regular
naturalistic driving trajectories. By leveraging the complementary advantages
of drone-based (high realism and extensive scenario coverage) and
simulation-based (high safety and reproducibility) data collection methods, we
first conduct drone-based traffic video recording at a highway ramp merging
area. After that, the manually selected highly interactive scenarios are
reconstructed in simulation software, and drivers' first-person view (FPV)
videos are generated, which are then viewed and evaluated by recruited
participants. During the video viewing process, participants' eye-tracking data
is collected. After data processing and filtering, 3567 valid subjective risk
ratings from 101 participants across 179 scenarios are retained, along with
2045 qualified eye-tracking data segments. The collected data and examples of
the generated FPV videos are available in our website.

</details>


### [65] [Visual Analytics Using Tensor Unified Linear Comparative Analysis](https://arxiv.org/abs/2507.19988)
*Naoki Okami,Kazuki Miyake,Naohisa Sakamoto,Jorji Nonaka,Takanori Fujiwara*

Main category: cs.HC

TL;DR: 论文提出了一种新的张量分解方法TULCA，支持灵活的张量比较分析，并通过可视化界面帮助解释结果。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解方法无法支持灵活的对比分析，因此需要一种新方法来解决这一局限。

Method: 结合判别分析和对比学习方案，扩展了ULCA方法，提出TULCA方法，并开发了可视化工具。

Result: TULCA能够灵活比较张量，并通过2D可视化展示核心张量。

Conclusion: TULCA及其可视化界面在计算评估和案例研究中证明了其有效性。

Abstract: Comparing tensors and identifying their (dis)similar structures is
fundamental in understanding the underlying phenomena for complex data. Tensor
decomposition methods help analysts extract tensors' essential characteristics
and aid in visual analytics for tensors. In contrast to dimensionality
reduction (DR) methods designed only for analyzing a matrix (i.e., second-order
tensor), existing tensor decomposition methods do not support flexible
comparative analysis. To address this analysis limitation, we introduce a new
tensor decomposition method, named tensor unified linear comparative analysis
(TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA
integrates discriminant analysis and contrastive learning schemes for tensor
decomposition, enabling flexible comparison of tensors. We also introduce an
effective method to visualize a core tensor extracted from TULCA into a set of
2D visualizations. We integrate TULCA's functionalities into a visual analytics
interface to support analysts in interpreting and refining the TULCA results.
We demonstrate the efficacy of TULCA and the visual analytics interface with
computational evaluations and two case studies, including an analysis of log
data collected from a supercomputer.

</details>


### [66] [Exploring the Alignment of Perceived and Measured Sleep Quality with Working Memory using Consumer Wearables](https://arxiv.org/abs/2507.19491)
*Peter Neigel,David Antony Selby,Shota Arai,Benjamin Tag,Niels van Berkel,Sebastian Vollmer,Andrew Vargo,Koichi Kise*

Main category: cs.HC

TL;DR: 研究探讨了可穿戴设备（Oura戒指）的传感器数据与主观睡眠自我评估的关系，发现REM睡眠、夜间心率和工作记忆任务是预测睡眠自评的关键因素，并指出不同用户对睡眠追踪数据的敏感度不同。


<details>
  <summary>Details</summary>
Motivation: 探讨可穿戴设备的睡眠追踪数据是否能真正增强对睡眠的理解，还是仅仅量化已知模式。

Method: 29名参与者在4-8周内每天使用Oura戒指记录睡眠数据，同时自我评估睡眠质量并完成工作记忆任务。

Result: REM睡眠时长、夜间心率和工作记忆任务得分显著预测睡眠自评，且不同用户对睡眠数据的敏感度存在差异。

Conclusion: 可穿戴设备对某些用户的睡眠信息增益更大，数据公开以供进一步研究。

Abstract: Wearable devices offer detailed sleep-tracking data. However, whether this
information enhances our understanding of sleep or simply quantifies
already-known patterns remains unclear. This work explores the relationship
between subjective sleep self-assessments and sensor data from an Oura ring
over 4--8 weeks in-the-wild. 29 participants rated their sleep quality daily
compared to the previous night and completed a working memory task. Our
findings reveal that differences in REM sleep, nocturnal heart rate, N-Back
scores, and bedtimes highly predict sleep self-assessment in significance and
effect size. For N-Back performance, REM sleep duration, prior night's REM
sleep, and sleep self-assessment are the strongest predictors. We demonstrate
that self-report sensitivity towards sleep markers differs among participants.
We identify three groups, highlighting that sleep trackers provide more
information gain for some users than others. Additionally, we make all
experiment data publicly available.

</details>


### [67] [Mosaic Selections: Managing and Optimizing User Selections for Scalable Data Visualization Systems](https://arxiv.org/abs/2507.19690)
*Jeffrey Heer,Dominik Moritz,Ron Pechuk*

Main category: cs.HC

TL;DR: Mosaic Selections模型通过优化查询和选择谓词，实现大规模数据集实时交互的可视化低延迟更新。


<details>
  <summary>Details</summary>
Motivation: 解决交互式可视化在大规模数据集（百万级以上）中实时交互的延迟问题。

Method: 提出Mosaic Selections模型，通过分析查询和选择谓词，实现自动优化（如预聚合数据）。

Result: 基准测试显示延迟显著降低，优于未优化查询和现有Vega优化器。

Conclusion: Mosaic Selections模型为跨多交互可视化提供了灵活、可互操作且高效的过滤基础设施。

Abstract: Though powerful tools for analysis and communication, interactive
visualizations often fail to support real-time interaction with large datasets
with millions or more records. To highlight and filter data, users indicate
values or intervals of interest. Such selections may span multiple components,
combine in complex ways, and require optimizations to ensure low-latency
updates. We describe Mosaic Selections, a model for representing, managing, and
optimizing user selections, in which one or more filter predicates are added to
queries that request data for visualizations and input widgets. By analyzing
both queries and selection predicates, Mosaic Selections enable automatic
optimizations, including pre-aggregating data to rapidly compute selection
updates. We contribute a formal description of our selection model and
optimization methods, and their implementation in the open-source Mosaic
architecture. Benchmark results demonstrate orders-of-magnitude latency
improvements for selection-based optimizations over unoptimized queries and
existing optimizers for the Vega language. The Mosaic Selection model provides
infrastructure for flexible, interoperable filtering across multiple
visualizations, alongside automatic optimizations to scale to millions and even
billions of records.

</details>


### [68] [ChartGen: Scaling Chart Understanding Via Code-Guided Synthetic Chart Generation](https://arxiv.org/abs/2507.19492)
*Jovana Kondic,Pengyuan Li,Dhiraj Joshi,Zexue He,Shafiq Abedin,Jennifer Sun,Ben Wiesel,Eli Schwartz,Ahmed Nassar,Bo Wu,Assaf Arbelle,Aude Oliva,Dan Gutfreund,Leonid Karlinsky,Rogerio Feris*

Main category: cs.HC

TL;DR: 提出了ChartGen，一个自动化合成图表生成的管道，用于填补现有多模态基准在图表代码重建任务上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注回答图表问题或总结，而缺乏对图表图像的代码重建任务的研究。

Method: ChartGen利用视觉语言模型(VLM)和代码导向的大型语言模型(LLM)，从种子图表图像生成Python脚本并迭代增强。

Result: 创建了包含27种图表类型、11种绘图库的数据集，并评估了多个VLM模型，显示仍有改进空间。

Conclusion: 发布的工具和数据集旨在加速图表理解和视觉条件代码生成的研究。

Abstract: Chart-to-code reconstruction -- the task of recovering executable plotting
scripts from chart images -- provides important insights into a model's ability
to ground data visualizations in precise, machine-readable form. Yet many
existing multimodal benchmarks largely focus primarily on answering questions
about charts or summarizing them. To bridge this gap, we present ChartGen, a
fully-automated pipeline for code-guided synthetic chart generation. Starting
from seed chart images, ChartGen (i) prompts a vision-language model (VLM) to
reconstruct each image into a python script, and (ii) iteratively augments that
script with a code-oriented large language model (LLM). Using ChartGen, we
create 222.5K unique chart-image code pairs from 13K seed chart images, and
present an open-source synthetic chart dataset covering 27 chart types, 11
plotting libraries, and multiple data modalities (image, code, text, CSV,
DocTags). From this corpus, we curate a held-out chart-to-code evaluation
subset of 4.3K chart image-code pairs, and evaluate six open-weight VLMs (3B -
26B parameters), highlighting substantial room for progress. We release the
pipeline, prompts, and the dataset to help accelerate efforts towards robust
chart understanding and vision-conditioned code generation:
https://github.com/SD122025/ChartGen/

</details>


### [69] [Talking-to-Build: How LLM-Assisted Interface Shapes Player Performance and Experience in Minecraft](https://arxiv.org/abs/2507.20300)
*Xin Sun,Lei Wang,Yue Li,Jie Li,Massimo Poesio,Julian Frommel,Koen Hinriks,Jiahuan Pei*

Main category: cs.HC

TL;DR: 论文探讨了大型语言模型（LLM）作为游戏辅助工具对玩家表现和体验的影响，发现LLM辅助界面显著提升了玩家表现和游戏体验。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的兴起，游戏互动从固定命令转向自然对话，但LLM对玩家表现和体验的影响尚未充分研究。

Method: 研究在《Minecraft》中设计了LLM辅助界面，通过自然语言与玩家互动，并通过混合方法研究比较了LLM辅助与传统命令界面的效果。

Result: LLM辅助界面显著提高了玩家表现、参与度和游戏体验，任务复杂度对不同界面的效果也有显著影响。

Conclusion: LLM辅助界面在虚拟体验中具有革命性潜力，但需平衡直观性与可预测性、透明度和用户自主权。

Abstract: With large language models (LLMs) on the rise, in-game interactions are
shifting from rigid commands to natural conversations. However, the impacts of
LLMs on player performance and game experience remain underexplored. This work
explores LLM's role as a co-builder during gameplay, examining its impact on
task performance, usability, and player experience. Using Minecraft as a
sandbox, we present an LLM-assisted interface that engages players through
natural language, aiming to facilitate creativity and simplify complex gaming
commands. We conducted a mixed-methods study with 30 participants, comparing
LLM-assisted and command-based interfaces across simple and complex game tasks.
Quantitative and qualitative analyses reveal that the LLM-assisted interface
significantly improves player performance, engagement, and overall game
experience. Additionally, task complexity has a notable effect on player
performance and experience across both interfaces. Our findings highlight the
potential of LLM-assisted interfaces to revolutionize virtual experiences,
emphasizing the importance of balancing intuitiveness with predictability,
transparency, and user agency in AI-driven, multimodal gaming environments.

</details>


### [70] [From Bench to Bedside: A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2507.19493)
*Yaowei Bai,Ruiheng Zhang,Yu Lei,Jingfeng Yao,Shuguang Ju,Chaoyang Wang,Wei Yao,Yiwan Guo,Guilin Zhang,Chao Wan,Qian Yuan,Xuhua Duan,Xinggang Wang,Tao Sun,Yongchao Xu,Chuansheng Zheng,Huangxuan Zhao,Bo Du*

Main category: cs.HC

TL;DR: Janus-Pro-CXR (1B)是基于DeepSeek Janus-Pro模型开发的胸片解读系统，在多中心前瞻性试验中表现优异，在自动报告生成和临床关键发现检测上超越包括ChatGPT 4o在内的先进模型，并在临床部署中显著提升报告质量和效率。


<details>
  <summary>Details</summary>
Motivation: 全球放射科医生短缺，胸片工作量巨大，现有评估缺乏前瞻性临床验证，需开发高效可靠的AI辅助解决方案。

Method: 开发Janus-Pro-CXR系统，通过多中心前瞻性试验验证其性能，并与现有模型进行对比。

Result: 系统在报告生成和临床检测上表现优异，临床部署中显著提升报告质量（4.37 vs. 4.11）并减少18.5%解读时间，专家偏好率为52.7%。

Conclusion: Janus-Pro-CXR通过轻量架构和领域优化，提升了诊断可靠性和效率，适合资源有限环境，并将开源以推动AI在放射学的临床转化。

Abstract: A global shortage of radiologists has been exacerbated by the significant
volume of chest X-ray workloads, particularly in primary care. Although
multimodal large language models show promise, existing evaluations
predominantly rely on automated metrics or retrospective analyses, lacking
rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray
interpretation system based on DeepSeek Janus-Pro model, was developed and
rigorously validated through a multicenter prospective trial (NCT06874647). Our
system outperforms state-of-the-art X-ray report generation models in automated
report generation, surpassing even larger-scale models including ChatGPT 4o
(200B parameters), while demonstrating robust detection of eight clinically
critical radiographic findings (area under the curve, AUC > 0.8). Retrospective
evaluation confirms significantly higher report accuracy than Janus-Pro and
ChatGPT 4o. In prospective clinical deployment, AI assistance significantly
improved report quality scores (4.37 vs. 4.11, P < 0.001), reduced
interpretation time by 18.5% (P < 0.001), and was preferred by a majority of
experts (3 out of 5) in 52.7% of cases. Through lightweight architecture and
domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and
workflow efficiency, particularly in resource-constrained settings. The model
architecture and implementation framework will be open-sourced to facilitate
the clinical translation of AI-assisted radiology solutions.

</details>


### [71] [Evaluating Personalized Beneficial Interventions in the Daily Lives of Older Adults Using a Camera](https://arxiv.org/abs/2507.19494)
*Longfei Chen,Christopher Lochhead,Robert B. Fisher,Nusa Faric,Jacques Fleuriot,Subramanian Ramamoorthy*

Main category: cs.HC

TL;DR: 研究通过非接触式摄像头系统监测两位老年人的日常活动干预效果，证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 老年人日常活动干预对身心健康有益，但缺乏客观指标和个性化策略。

Method: 选取两位65岁以上老年人进行8周监测，使用计算机视觉算法分析行为数据。

Result: 干预活动显著改变了老年人的行为，证明了活动的有效性。

Conclusion: 研究验证了干预活动和监测系统的有效性。

Abstract: Beneficial daily activity interventions have been shown to improve both the
physical and mental health of older adults. However, there is a lack of robust
objective metrics and personalized strategies to measure their impact. In this
study, two older adults aged over 65, living in Edinburgh, UK, selected their
preferred daily interventions (mindful meals and art crafts), which are then
assessed for effectiveness. The total monitoring period across both
participants was 8 weeks. Their physical behaviours were continuously monitored
using a non-contact, privacy-preserving camera-based system. Postural and
mobility statistics were extracted using computer vision algorithms and
compared across periods with and without the interventions. The results
demonstrate significant behavioural changes for both participants, highlighting
the effectiveness of both these activities and the monitoring system.

</details>


### [72] [Simulating Human Behavior with the Psychological-mechanism Agent: Integrating Feeling, Thought, and Action](https://arxiv.org/abs/2507.19495)
*Qing Dong,Pengyuan Liu,Dong Yu,Chen Kang*

Main category: cs.HC

TL;DR: PSYA框架通过情感-思维-行动的认知三角模型，提升生成代理对人类行为的模拟真实性。


<details>
  <summary>Details</summary>
Motivation: 现有生成代理在情感建模上过于简化且专注于特定任务，限制了模拟的真实性。

Method: PSYA包含情感模块（分层模型）、思维模块（三网络模型）和行动模块（整合情感、需求和计划）。

Result: PSYA能生成更自然、一致、多样且可信的行为，成功复现人类心理实验结果。

Conclusion: PSYA为生成代理提供了更丰富准确的情感和认知建模方法，可替代人类参与者进行心理实验。

Abstract: Generative agents have made significant progress in simulating human
behavior, but existing frameworks often simplify emotional modeling and focus
primarily on specific tasks, limiting the authenticity of the simulation. Our
work proposes the Psychological-mechanism Agent (PSYA) framework, based on the
Cognitive Triangle (Feeling-Thought-Action), designed to more accurately
simulate human behavior. The PSYA consists of three core modules: the Feeling
module (using a layer model of affect to simulate changes in short-term,
medium-term, and long-term emotions), the Thought module (based on the Triple
Network Model to support goal-directed and spontaneous thinking), and the
Action module (optimizing agent behavior through the integration of emotions,
needs and plans). To evaluate the framework's effectiveness, we conducted daily
life simulations and extended the evaluation metrics to self-influence,
one-influence, and group-influence, selection five classic psychological
experiments for simulation. The results show that the PSYA framework generates
more natural, consistent, diverse, and credible behaviors, successfully
replicating human experimental outcomes. Our work provides a richer and more
accurate emotional and cognitive modeling approach for generative agents and
offers an alternative to human participants in psychological experiments.

</details>


### [73] [Vocalize: Lead Acquisition and User Engagement through Gamified Voice Competitions](https://arxiv.org/abs/2507.20730)
*Edvin Teskeredzic,Muamer Paric,Adna Sestic,Petra Fribert,Anamarija Lukac,Hadzem Hadzic,Kemal Altwlkany,Emanuel Lacic*

Main category: cs.HC

TL;DR: 该论文介绍了一个名为Vocalize的端到端系统，通过语音竞赛的互动游戏化平台提高用户参与度和潜在客户获取，展示了音频处理技术和LLMs的应用效果。


<details>
  <summary>Details</summary>
Motivation: 探索通过互动和游戏化的平台创造引人入胜的用户体验并收集潜在客户的可能性。

Method: 利用音频处理技术和LLMs开发Vocalize系统，并在4个不同的现场活动中进行测试。

Result: 用户研究表明，Vocalize能够显著提升用户参与度，展示了游戏化音频活动在营销等领域的潜力。

Conclusion: Vocalize系统通过游戏化的语音竞赛有效提升用户参与度和品牌认知，具有广阔的应用前景。

Abstract: This paper explores the prospect of creating engaging user experiences and
collecting leads through an interactive and gamified platform. We introduce
Vocalize, an end-to-end system for increasing user engagement and lead
acquisition through gamified voice competitions. Using audio processing
techniques and LLMs, we create engaging and interactive experiences that have
the potential to reach a wide audience, foster brand recognition, and increase
customer loyalty. We describe the system from a technical standpoint and report
results from launching Vocalize at 4 different live events. Our user study
shows that Vocalize is capable of generating significant user engagement, which
shows potential for gamified audio campaigns in marketing and similar
verticals.

</details>


### [74] [Technological Requirements for Videoconferencing Judicial Hearings: Enhancing the Credibility and Reliability of Remote Testimonies](https://arxiv.org/abs/2507.19496)
*Jorge Alberto Araujo*

Main category: cs.HC

TL;DR: 研究从司法内部视角分析了如何通过技术提升视频庭审的可信度和可靠性，提出针对性功能以弥补现有平台的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视频庭审平台在证词真实性验证方面存在局限性，希望通过技术改进提升远程庭审的可靠性。

Method: 基于法官的实际经验，提出如眼动追踪、环境验证和并行应用拦截等功能。

Result: 开发专门的安全与监控模块可以显著提升远程庭审的可信度，使其与现场庭审相当。

Conclusion: 通过技术改进，远程庭审在保障程序可靠性的同时，可以扩大司法可及性。

Abstract: This paper analyzes the technological requirements necessary to enhance the
credibility and reliability of judicial hearings conducted via videoconference,
from the internal perspective of the judiciary. Drawing on the practical
experience of a judge who conducts daily hearings, this study identifies
limitations in current platforms for verifying the authenticity of testimonies
and proposes tailored functionalities for the judicial context. Recognizing
that remote hearings represent a convenience for the parties without replacing
the option of in-person attendance, the article suggests implementing features
such as eye tracking, environment verification, and blocking of parallel
applications, in addition to improvements in transmission quality. The study
concludes that developing specific modules for witnesses - focusing on security
and monitoring - can significantly contribute to equalizing the credibility
between remote and in-person hearings, thus expanding access to justice without
compromising procedural reliability.

</details>


### [75] [Unlimited Editions: Documenting Human Style in AI Art Generation](https://arxiv.org/abs/2507.19497)
*Alex Leitch,Celia Chen*

Main category: cs.HC

TL;DR: 摘要主张AI艺术生成研究应关注艺术价值的起源与演化，而非仅关注视觉输出。


<details>
  <summary>Details</summary>
Motivation: 探讨AI艺术生成如何误解艺术价值的来源，提出应记录艺术风格的演变过程。

Method: 通过历史案例研究，分析艺术风格的形成是创造力与约束的产物。

Result: 呼吁HCI研究转向自动记录艺术风格的起源与选择，而非单纯复制美学效果。

Conclusion: 建议将HCI研究的重点从视觉输出转向艺术风格的溯源与演化。

Abstract: As AI art generation becomes increasingly sophisticated, HCI research has
focused primarily on questions of detection, authenticity, and automation. This
paper argues that such approaches fundamentally misunderstand how artistic
value emerges from the concerns that drive human image production. Through
examination of historical precedents, we demonstrate that artistic style is not
only visual appearance but the resolution of creative struggle, as artists
wrestle with influence and technical constraints to develop unique ways of
seeing. Current AI systems flatten these human choices into reproducible
patterns without preserving their provenance. We propose that HCI's role lies
not only in perfecting visual output, but in developing means to document the
origins and evolution of artistic style as it appears within generated visual
traces. This reframing suggests new technical directions for HCI research in
generative AI, focused on automatic documentation of stylistic lineage and
creative choice rather than simple reproduction of aesthetic effects.

</details>


### [76] [ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings](https://arxiv.org/abs/2507.19498)
*Yue Wu,Xiaolan Chen,Weiyi Zhang,Shunming Liu,Wing Man Rita Sum,Xinyuan Wu,Xianwen Shang,Chea-su Kee,Mingguang He,Danli Shi*

Main category: cs.HC

TL;DR: ChatMyopia是一款基于LLM的AI代理，用于处理近视相关的图文咨询，通过整合图像分类工具和检索增强知识库，提供个性化、准确且安全的回答，显著提升患者满意度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在定制化医疗沟通中具有潜力，但在可解释性和多任务集成（特别是针对近视等特定领域需求）方面存在挑战，且其作为患者教育工具的实际效果尚未得到验证。

Method: 开发ChatMyopia，整合图像分类工具和检索增强知识库（基于文献、专家共识和临床指南），并通过近视性黄斑变性分级任务、单问题测试和人类评估验证其能力。

Result: 在一项随机对照试验（n=70）中，ChatMyopia显著提高了患者满意度，尤其是在准确性、同理心、疾病意识和医患沟通方面，优于传统宣传单。

Conclusion: ChatMyopia作为辅助工具，在初级眼科护理中具有提升患者教育和医疗满意度的潜力。

Abstract: Large language models (LLMs) show promise for tailored healthcare
communication but face challenges in interpretability and multi-task
integration particularly for domain-specific needs like myopia, and their
real-world effectiveness as patient education tools has yet to be demonstrated.
Here, we introduce ChatMyopia, an LLM-based AI agent designed to address text
and image-based inquiries related to myopia. To achieve this, ChatMyopia
integrates an image classification tool and a retrieval-augmented knowledge
base built from literature, expert consensus, and clinical guidelines. Myopic
maculopathy grading task, single question examination and human evaluations
validated its ability to deliver personalized, accurate, and safe responses to
myopia-related inquiries with high scalability and interpretability. In a
randomized controlled trial (n=70, NCT06607822), ChatMyopia significantly
improved patient satisfaction compared to traditional leaflets, enhancing
patient education in accuracy, empathy, disease awareness, and patient-eyecare
practitioner communication. These findings highlight ChatMyopia's potential as
a valuable supplement to enhance patient education and improve satisfaction
with medical services in primary eye care settings.

</details>


### [77] [Gaze-Aware AI: Mathematical modeling of epistemic experience of the Marginalized for Human-Computer Interaction & AI Systems](https://arxiv.org/abs/2507.19500)
*Omkar Suresh Hatti*

Main category: cs.HC

TL;DR: 论文探讨了人工智能如何通过量化人类潜意识中为适应主流文化而调整自我表达的行为，提出一种Gaze Pressure Index模型，并为包容性人机交互提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用AI技术分析人类在多元互动中的心理空间，揭示潜意识中因主流文化压力而调整自我表达的现象，从而促进社会和谐。

Method: 通过后现代哲学和心理学概念，分析边缘化群体在Reddit帖子中的互动，提出GPI-Diff复合度量模型，并开发训练大型语言模型的方程。

Result: 提出了GPI-Diff模型用于量化凝视压力，并基于此提出包容性人机交互设计原则，支持神经可塑性理论。

Conclusion: 研究为AI技术在促进心理空间和包容性交互中的应用提供了理论支持，未来可进一步优化模型并验证其实际效果。

Abstract: The proliferation of artificial intelligence provides an opportunity to
create psychological spaciousness in society. Spaciousness is defined as the
ability to hold diverse interpersonal interactions and forms the basis for
vulnerability that leads to authenticity that leads to prosocial behaviors and
thus to societal harmony. This paper demonstrates an attempt to quantify, the
human conditioning to subconsciously modify authentic self-expression to fit
the norms of the dominant culture. Gaze is explored across various marginalized
and intersectional groups, using concepts from postmodern philosophy and
psychology. The effects of gaze are studied through analyzing a few redacted
Reddit posts, only to be discussed in discourse and not endorsement. A
mathematical formulation for the Gaze Pressure Index (GPI)-Diff Composite
Metric is presented to model the analysis of two sets of conversational spaces
in relation to one another. The outcome includes an equation to train Large
Language Models (LLMs) - the working mechanism of AI products such as Chat-GPT;
and an argument for affirming and inclusive HCI, based on the equation, is
presented. The argument is supported by a few principles of Neuro-plasticity,
The brain's lifelong capacity to rewire.

</details>


### [78] [LowKeyEMG: Electromyographic typing with a reduced keyset](https://arxiv.org/abs/2507.19736)
*Johannes Y. Lee,Derek Xiao,Shreyas Kaasyap,Nima R. Hadidi,John L. Zhou,Jacob Cunningham,Rakshith R. Gore,Deniz O. Eren,Jonathan C. Kao*

Main category: cs.HC

TL;DR: LowKeyEMG是一种基于sEMG的实时人机界面，通过7种手势实现高效的文本输入，适用于运动受损用户。


<details>
  <summary>Details</summary>
Motivation: 解决全字母解码sEMG不可靠的问题，尤其是对运动受损个体。

Method: 将英语字母表简化为4种手势键，结合3种系统交互键，利用RWKV语言模型高效计算。

Result: 实验显示用户单手打字速度达23.3 WPM，手势效率提高17%，98.2%的top-3单词准确率。

Conclusion: 低键打字模式适用于辅助技术和输入带宽受限的界面。

Abstract: We introduce LowKeyEMG, a real-time human-computer interface that enables
efficient text entry using only 7 gesture classes decoded from surface
electromyography (sEMG). Prior work has attempted full-alphabet decoding from
sEMG, but decoding large character sets remains unreliable, especially for
individuals with motor impairments. Instead, LowKeyEMG reduces the English
alphabet to 4 gesture keys, with 3 more for space and system interaction, to
reliably translate simple one-handed gestures into text, leveraging the
recurrent transformer-based language model RWKV for efficient computation. In
real-time experiments, participants achieved average one-handed keyboardless
typing speeds of 23.3 words per minute with LowKeyEMG, and improved gesture
efficiency by 17% (relative to typed phrase length). When typing with only 7
keys, LowKeyEMG can achieve 98.2% top-3 word accuracy, demonstrating that this
low-key typing paradigm can maintain practical communication rates. Our results
have implications for assistive technologies and any interface where input
bandwidth is constrained.

</details>


### [79] [KinemaFX: A Kinematic-Driven Interactive System for Particle Effect Exploration and Customization](https://arxiv.org/abs/2507.19782)
*Yifei Zhang,Lin-Ping Yuan,Yuheng Zhao,Jielin Feng,Siming Chen*

Main category: cs.HC

TL;DR: 论文介绍了KinemaFX系统，帮助非专业用户通过语义和运动输入创建定制粒子效果，结合LLM工作流和运动驱动搜索方法。


<details>
  <summary>Details</summary>
Motivation: 解决非专业用户在创建粒子效果时因缺乏专业技能而遇到的困难，特别是匹配意图的运动行为。

Method: 提出粒子效果的概念模型，结合LLM工作流支持意图表达，并开发运动驱动搜索方法。

Result: 用户研究表明，KinemaFX能有效帮助用户高效、个性化地创建粒子效果。

Conclusion: KinemaFX为非专业用户提供了便捷的粒子效果创作工具，结合语义和运动输入，提升创作效率。

Abstract: Particle effects are widely used in games and animation to simulate natural
phenomena or stylized visual effects. However, creating effect artworks is
challenging for non-expert users due to their lack of specialized skills,
particularly in finding particle effects with kinematic behaviors that match
their intent. To address these issues, we present KinemaFX, a kinematic-driven
interactive system, to assist non-expert users in constructing customized
particle effect artworks. We propose a conceptual model of particle effects
that captures both semantic features and kinematic behaviors. Based on the
model, KinemaFX adopts a workflow powered by Large Language Models (LLMs) that
supports intent expression through combined semantic and kinematic inputs,
while enabling implicit preference-guided exploration and subsequent creation
of customized particle effect artworks based on exploration results.
Additionally, we developed a kinematic-driven method to facilitate efficient
interactive particle effect search within KinemaFX via structured
representation and measurement of particle effects. To evaluate KinemaFX, we
illustrate usage scenarios and conduct a user study employing an ablation
approach. Evaluation results demonstrate that KinemaFX effectively supports
users in efficiently and customarily creating particle effect artworks.

</details>


### [80] [TS-Insight: Visualizing Thompson Sampling for Verification and XAI](https://arxiv.org/abs/2507.19898)
*Parsa Vares,Éloi Durant,Jun Pang,Nicolas Médoc,Mohammad Ghoniem*

Main category: cs.HC

TL;DR: TS-Insight是一种可视化分析工具，旨在揭示Thompson Sampling算法的内部决策机制，帮助开发者验证、诊断和解释探索/利用的动态过程。


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling及其变体在主动学习中用于平衡探索和利用策略，但其概率性质使其成为“黑箱”，阻碍了调试和信任。

Method: 开发了TS-Insight工具，包含多个图表，追踪每个臂的后验分布、证据计数和采样结果，以揭示决策机制。

Result: 工具成功实现了对Thompson Sampling算法的可视化分析，帮助开发者理解和调试探索/利用的动态过程。

Conclusion: TS-Insight增强了Thompson Sampling算法的透明度和可解释性，适用于需要可解释决策的敏感领域。

Abstract: Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit
algorithms used to balance exploration and exploitation strategies in active
learning. Yet, their probabilistic nature often turns them into a ``black
box'', hindering debugging and trust. We introduce TS-Insight, a visual
analytics tool explicitly designed to shed light on the internal decision
mechanisms of Thompson Sampling-based algorithms, for model developers. It
comprises multiple plots, tracing for each arm the evolving posteriors,
evidence counts, and sampling outcomes, enabling the verification, diagnosis,
and explainability of exploration/exploitation dynamics. This tool aims at
fostering trust and facilitating effective debugging and deployment in complex
binary decision-making scenarios especially in sensitive domains requiring
interpretable decision-making.

</details>


### [81] [Beyond the Broadcast: Enhancing VR Tennis Broadcasting through Embedded Visualizations and Camera Techniques](https://arxiv.org/abs/2507.20006)
*Jun-Hsiang Yao,Jielin Feng,Xinfang Tian,Kai Xu,Gulshat Amirkhanova,Siming Chen*

Main category: cs.HC

TL;DR: 论文分析了现有VR体育直播的不足，提出了一种结合电影化镜头运动和嵌入可视化的VR网球观看系统，用户研究表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统VR网球直播缺乏有效的镜头语言和动态可视化，影响了观众的沉浸感和叙事清晰度，因此需要改进。

Method: 分析400段网球直播视频和25段VR动画，提取镜头技术和可视化数据，构建模拟比赛环境和自适应镜头运动系统。

Result: 开发的VR系统融合战术信息和关键事件可视化，显著提升了观众的沉浸感和叙事理解。

Conclusion: 该系统为VR体育直播提供了新的设计框架，优化了观众体验。

Abstract: Virtual Reality (VR) broadcasting has emerged as a promising medium for
providing immersive viewing experiences of major sports events such as tennis.
However, current VR broadcast systems often lack an effective camera language
and do not adequately incorporate dynamic, in-game visualizations, limiting
viewer engagement and narrative clarity. To address these limitations, we
analyze 400 out-of-play segments from eight major tennis broadcasts to develop
a tennis-specific design framework that effectively combines cinematic camera
movements with embedded visualizations. We further refine our framework by
examining 25 cinematic VR animations, comparing their camera techniques with
traditional tennis broadcasts to identify key differences and inform
adaptations for VR. Based on data extracted from the broadcast videos, we
reconstruct a simulated game that captures the players' and ball's motion and
trajectories. Leveraging this design framework and processing pipeline, we
develope Beyond the Broadcast, a VR tennis viewing system that integrates
embedded visualizations with adaptive camera motions to construct a
comprehensive and engaging narrative. Our system dynamically overlays tactical
information and key match events onto the simulated environment, enhancing
viewer comprehension and narrative engagement while ensuring perceptual
immersion and viewing comfort. A user study involving tennis viewers
demonstrate that our approach outperforms traditional VR broadcasting methods
in delivering an immersive, informative viewing experience.

</details>


### [82] [Dynamite: Real-Time Debriefing Slide Authoring through AI-Enhanced Multimodal Interaction](https://arxiv.org/abs/2507.20137)
*Panayu Keelawat,David Barron,Kaushik Narasimhan,Daniel Manesh,Xiaohang Tang,Xi Chen,Sang Won Lee,Yan Chen*

Main category: cs.HC

TL;DR: 介绍了Dynamite系统，帮助教师在课堂讨论中实时更新幻灯片内容，提升教学效果。


<details>
  <summary>Details</summary>
Motivation: 课堂教学中实时汇总小组讨论内容并更新幻灯片，对教师来说是认知负担重的任务，需要技术支持。

Method: 开发了Dynamite系统，通过语义数据绑定和建议，支持教师实时更新幻灯片内容。

Result: 实验室研究表明，Dynamite在内容准确性和质量上优于文本基线系统，用户反馈良好。

Conclusion: Dynamite系统有效减轻了教师的认知负担，提升了课堂讨论的效率和效果。

Abstract: Facilitating class-wide debriefings after small-group discussions is a common
strategy in ethics education. Instructor interviews revealed that effective
debriefings should highlight frequently discussed themes and surface
underrepresented viewpoints, making accurate representations of insight
occurrence essential. Yet authoring presentations in real time is cognitively
overwhelming due to the volume of data and tight time constraints. We present
Dynamite, an AI-assisted system that enables semantic updates to
instructor-authored slides during live classroom discussions. These updates are
powered by semantic data binding, which links slide content to evolving
discussion data, and semantic suggestions, which offer revision options aligned
with pedagogical goals. In a within-subject in-lab study with 12 participants,
Dynamite outperformed a text-based AI baseline in content accuracy and quality.
Participants used voice and sketch input to quickly organize semantic blocks,
then applied suggestions to accelerate refinement as data stabilized.

</details>


### [83] [Occupational Safety within Non-Routine Manufacturing Processes: Evaluating the Validity of Task-Based Ergonomic Assessments](https://arxiv.org/abs/2507.20261)
*Charu Tripathi,Manish Arora,Amaresh Chakrabarti*

Main category: cs.HC

TL;DR: 论文通过多特质多方法（MTMM）矩阵和视频内容分析，评估了非例行工作中基于任务的工效学评估的结构效度，发现其收敛效度和区分效度不足，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 工业5.0推动人本化，直接测量工具在制造业中的应用增加，但由于技术和可行性限制，基于任务的工效学评估在非例行工作中被广泛使用，但其效度存在问题，需要验证和改进。

Method: 研究采用多特质多方法（MTMM）矩阵和视频内容分析，结合惯性运动捕捉和Borg's RPE评分量表，对46名参与者的工效学暴露特征进行了直接测量和自我报告分析。

Result: 研究发现基于任务的工效学评估收敛效度不足（相关系数0.149-0.243），区分效度证据较弱（显著性p<0.001），并通过视频分析识别了三个影响效度的主要因素和误解风险的问题。

Conclusion: 研究表明基于任务的评估可能低估实际工效学风险，强调了需要开发专注于累积负荷分析且适用于多样化工业流程的工效学评估技术。

Abstract: Direct measurement ergonomic assessment is reshaping occupational safety by
facilitating highly reliable risk estimation. Industry 5.0, advocating
human-centricity, has catalysed increasing adoption of direct measurement tools
in manufacturing industries. However, due to technical and feasibility
constraints in their practical implementations, especially within non routine
manufacturing processes, task based approach to ergonomic assessment is
utilized. Despite enabling operationalization of robust ergonomic assessment
technologies within complicated industrial processes, task based approach
raises several validity concerns. Hence, to ascertain functional utility of the
resultant safety interventions, this study evaluates the construct validity of
task based ergonomic assessment within non routine work utilizing Multitrait
multimethod (MTMM) matrix followed by video-based content analysis. Ergonomic
exposure traits were collected for 46 participants through direct measurement
and self reported techniques utilizing inertial motion capture and Borg's RPE
rating scale respectively. Findings include unsubstantiated convergent validity
(low same trait correlations from 0.149 to 0.243) and weak evidence of
discriminant validity with statistical significance (p value less than 0.001).
The study also identifies three primary factors undermining construct validity
through video based content analysis. Findings also elucidate misinterpretation
of ergonomic risk and action levels. Therefore, practical implications entail
underestimation of actual ergonomic risks when estimated through task based
assessment. This highlights the need for enhancement in ergonomic assessment
technologies focused on cumulative load analysis compatible within diverse
industrial processes.

</details>


### [84] [CineVision: An Interactive Pre-visualization Storyboard System for Director-Cinematographer Collaboration](https://arxiv.org/abs/2507.20355)
*Zheng Wei,Hongtao Wu,lvmin Zhang,Xian Xu,Yefeng Zheng,Pan Hui,Maneesh Agrawala,Huamin Qu,Anyi Rao*

Main category: cs.HC

TL;DR: CineVision是一个AI驱动的平台，通过整合剧本写作和实时可视化预演，提升导演和摄影师间的沟通效率。


<details>
  <summary>Details</summary>
Motivation: 传统依赖视觉参考和手绘故事板的方法在预制作阶段缺乏效率和精确性，CineVision旨在解决这一问题。

Method: 平台提供动态灯光控制、风格模仿和可定制角色设计，帮助导演清晰表达创意。

Result: 在24人实验中，CineVision在任务时间和可用性评分上优于基准方法。

Conclusion: CineVision有望优化预制作流程，促进团队协作，特别适用于新合作者。

Abstract: Effective communication between directors and cinematographers is fundamental
in film production, yet traditional approaches relying on visual references and
hand-drawn storyboards often lack the efficiency and precision necessary during
pre-production. We present CineVision, an AI-driven platform that integrates
scriptwriting with real-time visual pre-visualization to bridge this
communication gap. By offering dynamic lighting control, style emulation based
on renowned filmmakers, and customizable character design, CineVision enables
directors to convey their creative vision with heightened clarity and rapidly
iterate on scene composition. In a 24-participant lab study, CineVision yielded
shorter task times and higher usability ratings than two baseline methods,
suggesting a potential to ease early-stage communication and accelerate
storyboard drafts under controlled conditions. These findings underscore
CineVision's potential to streamline pre-production processes and foster deeper
creative synergy among filmmaking teams, particularly for new collaborators.Our
code and demo are available at https://github.com/TonyHongtaoWu/CineVision.

</details>


### [85] [EchoForce: Continuous Grip Force Estimation from Skin Deformation Using Active Acoustic Sensing on a Wristband](https://arxiv.org/abs/2507.20437)
*Kian Mahmoodi,Yudong Xie,Tan Gemicioglu,Chi-Jung Lee,Jiwan Kim,Cheng Zhang*

Main category: cs.HC

TL;DR: EchoForce是一款新型腕带设备，利用声学传感实现低成本、非接触式握力测量，解决了现有方法的不足，适用于健康监测和人机交互。


<details>
  <summary>Details</summary>
Motivation: 握力是老年人健康的重要指标，现有穿戴式测量方法不便且依赖用户，EchoForce旨在提供一种实用的连续握力测量方案。

Method: 通过捕捉前臂屈肌细微皮肤变形反射的声学信号进行握力测量，采用基础模型进行用户依赖和非依赖的误差分析。

Result: 在11名参与者的研究中，用户依赖和非依赖的平均误差分别为9.08%和12.3%，系统在不同条件下保持准确。

Conclusion: EchoForce为连续握力测量提供了可行工具，适用于健康监测和新型交互技术。

Abstract: Grip force is commonly used as an overall health indicator in older adults
and is valuable for tracking progress in physical training and rehabilitation.
Existing methods for wearable grip force measurement are cumbersome and
user-dependent, making them insufficient for practical, continuous grip force
measurement. We introduce EchoForce, a novel wristband using acoustic sensing
for low-cost, non-contact measurement of grip force. EchoForce captures
acoustic signals reflected from subtle skin deformations by flexor muscles on
the forearm. In a user study with 11 participants, EchoForce achieved a
fine-tuned user-dependent mean error rate of 9.08% and a user-independent mean
error rate of 12.3% using a foundation model. Our system remained accurate
between sessions, hand orientations, and users, overcoming a significant
limitation of past force sensing systems. EchoForce makes continuous grip force
measurement practical, providing an effective tool for health monitoring and
novel interaction techniques.

</details>


### [86] [CoGrader: Transforming Instructors' Assessment of Project Reports through Collaborative LLM Integration](https://arxiv.org/abs/2507.20655)
*Zixin Chen,Jiachen Wang,Yumeng Li,Haobo Li,Chuhan Shi,Rong Zhang,Huamin Qu*

Main category: cs.HC

TL;DR: 论文提出CoGrader，一种结合人类与AI协作的评分系统，旨在解决复杂项目报告的公平性与效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前教育中，项目报告评分因多维度标准（如创造力和实践能力）和教师负担而面临挑战。现有的AI工具难以处理复杂评分指标。

Method: 通过六名教师的形式化研究，设计了CoGrader，整合人类与AI的协作指标设计、基准测试和AI辅助反馈。

Result: CoGrader有效提升了评分效率和一致性，同时为学生提供可靠的同伴比较反馈。

Conclusion: 论文探讨了人机协作评分系统的设计洞察与伦理考量，强调了其潜力与挑战。

Abstract: Grading project reports are increasingly significant in today's educational
landscape, where they serve as key assessments of students' comprehensive
problem-solving abilities. However, it remains challenging due to the
multifaceted evaluation criteria involved, such as creativity and
peer-comparative achievement. Meanwhile, instructors often struggle to maintain
fairness throughout the time-consuming grading process. Recent advances in AI,
particularly large language models, have demonstrated potential for automating
simpler grading tasks, such as assessing quizzes or basic writing quality.
However, these tools often fall short when it comes to complex metrics, like
design innovation and the practical application of knowledge, that require an
instructor's educational insights into the class situation. To address this
challenge, we conducted a formative study with six instructors and developed
CoGrader, which introduces a novel grading workflow combining human-LLM
collaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader
was found effective in improving grading efficiency and consistency while
providing reliable peer-comparative feedback to students. We also discuss
design insights and ethical considerations for the development of human-AI
collaborative grading systems.

</details>


### [87] [EarXplore: An Open Research Database on Earable Interaction](https://arxiv.org/abs/2507.20656)
*Jonas Hummel,Tobias Röddiger,Valeria Zitz,Philipp Lepold,Michael Küttner,Marius Prill,Christopher Clarke,Hans Gellersen,Michael Beigl*

Main category: cs.HC

TL;DR: EarXplore是一个交互式在线数据库，旨在整合和展示耳戴设备交互研究的碎片化成果。


<details>
  <summary>Details</summary>
Motivation: 耳戴设备研究领域多样化且分散，难以追踪相关进展。

Method: 通过问题导向的方法开发了34个标准，标注了118项研究，并构建了四种视图（表格、图形、相似性和时间线）。

Result: EarXplore支持个性化探索、筛选和信息检索，帮助研究者分析趋势和学术脉络。

Conclusion: EarXplore是一个动态资源，可推动未来研究发展。

Abstract: Interaction with earables - earphones equipped with additional sensors - has
been identified as one of four major areas of earable research. Worn naturally
and positioned near key physiological signals, earables support a wide range of
interaction modalities and have demonstrated the ability to detect multiple
inputs simultaneously. Yet this diversity has resulted in a fragmented body of
research, making it increasingly difficult to track developments and identify
relevant studies. To address this, we introduce EarXplore, a curated,
interactive online database on earable interaction research. Designed through a
question-centered process that guided both the development of 34 criteria
applied to annotate 118 studies and the structure of the platform, EarXplore
comprises four distinct yet integrated views: a Tabular View for structured
exploration, a Graphical View for visual overviews, a Similarity View for
identifying conceptual links, and a Timeline View for analyzing trends and
scholarly lineage. We demonstrate how the platform supports tailored
exploration, targeted filtering, and interactive information retrieval,
allowing researchers to query the literature and synthesize information in the
format of their choice. We furthermore leverage the contents and capabilities
of the platform to discuss the research gaps and opportunities in the field.
With built-in mechanisms for continuous community updates, EarXplore not only
reflects the current state of the field but also evolves alongside it, serving
as a living resource to inform and accelerate future developments.

</details>


### [88] [Beyond Text: Probing K-12 Educators' Perspectives and Ideas for Learning Opportunities Leveraging Multimodal Large Language Models](https://arxiv.org/abs/2507.20720)
*Tiffany Tseng,Katelyn Lam,Tiffany Lin Fu,Alekhya Maram*

Main category: cs.HC

TL;DR: 本研究探讨K-12教育工作者对多模态大语言模型（MLLMs）在教育中的看法，通过研讨会收集反馈，展示教师和学生驱动的两种应用案例，并分析未来学习体验的机会与挑战。


<details>
  <summary>Details</summary>
Motivation: 了解教育工作者如何设想MLLMs在未来的学习体验中发挥作用，以及如何应对实施过程中的挑战和需求。

Method: 通过12名K-12教育工作者的研讨会，收集他们对MLLMs的学习机会、实际问题和应用原型的反馈。

Result: 研究揭示了教师和学生驱动的两种MLLM应用方式，并总结了参与者对未来机会和实际问题的看法。

Conclusion: MLLMs在教育中具有潜力，但需关注实际需求和挑战，以优化未来学习体验。

Abstract: Multimodal Large Language Models (MLLMs) are beginning to empower new user
experiences that can flexibly generate content from a range of inputs,
including images, text, speech, and video. These capabilities have the
potential to enrich learning by enabling users to capture and interact with
information using a variety of modalities, but little is known about how
educators envision how MLLMs might shape the future of learning experiences,
what challenges diverse teachers encounter when interpreting how these models
work, and what practical needs should be considered for successful
implementation in educational contexts. We investigated educator perspectives
through formative workshops with 12 K-12 educators, where participants
brainstormed learning opportunities, discussed practical concerns for effective
use, and prototyped their own MLLM-powered learning applications using Claude
3.5 and its Artifacts feature for previewing code-based output. We use case
studies to illustrate two contrasting end-user approaches (teacher-and
student-driven), and share insights about opportunities and concerns expressed
by our participants, ending with implications for leveraging MLLMs for future
learning experiences.

</details>


### [89] [Beyond QWERTY: A pressure-based text input approach for XR that enables a touch-typing like experience](https://arxiv.org/abs/2507.20741)
*Fabian Rücker,Torben Storch*

Main category: cs.HC

TL;DR: 论文提出了一种基于压力的新型文本输入方式，通过线性设计替代传统QWERTY键盘，旨在解决XR应用中文本输入效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 解决XR应用中文本输入效率低下的问题，提升沉浸式技术的生产力。

Method: 分析物理键盘输入特性，提出一种压力基的文本输入方式，采用线性设计替代传统二维键盘布局。

Result: 该方法实现了超过200字符/分钟的输入速度，适合公共场合使用，且无需视觉引导。

Conclusion: 该压力基文本输入方式有效提升了XR应用的输入效率和用户体验。

Abstract: Text input in extended reality (XR) applications remains inefficient and
tedious. Most solutions are derived from the traditional keyboard layout, yet
fail to translate its positive characteristics to the spatial digital realm.
This limits the productive use of immersive technologies. In this work, we
analyze physical keyboard input to identify key characteristics that facilitate
its comfort, touch typing and high typing speeds. Building on these findings,
we propose a novel pressure-based text input modality that transfers these
characteristics into immersive space by substituting the two-dimensional QWERTY
layout with a linear scale. This design facilitates a touch-typing-like
experience, eliminating the need for visual guidance for proficient users. Our
skill-based approach enables typing speeds of over 200 characters per minute.
Additionally, it is suitable for discreet use in public spaces and everyday
text-input tasks, since the proposed system requires virtually no hand or
finger movements and resembles smartphone-based text input in appearance.

</details>


### [90] [Understanding Bias in Perceiving Dimensionality Reduction Projections](https://arxiv.org/abs/2507.20805)
*Seoyoung Doh,Hyeon Jeon,Sungbok Shin,Ghulam Jilani Quadri,Nam Wook Kim,Jinwook Seo*

Main category: cs.HC

TL;DR: 研究验证了视觉投影选择中存在的‘视觉趣味性’偏见，并探讨了其成因及缓解策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机是确认并解释实践中选择降维投影时对‘视觉趣味性’（如美观和视觉显著性）的偏好超过结构忠实性的现象。

Method: 通过用户研究验证偏见的存因及其影响因素（如颜色编码标签和较短曝光时间）。

Result: 研究发现视觉趣味性显著影响投影选择，且偏见在颜色编码和短曝光下加剧。

Conclusion: 研究提出策略以减少降维投影感知和解释中的偏见。

Abstract: Selecting the dimensionality reduction technique that faithfully represents
the structure is essential for reliable visual communication and analytics. In
reality, however, practitioners favor projections for other attractions, such
as aesthetics and visual saliency, over the projection's structural
faithfulness, a bias we define as visual interestingness. In this research, we
conduct a user study that (1) verifies the existence of such bias and (2)
explains why the bias exists. Our study suggests that visual interestingness
biases practitioners' preferences when selecting projections for analysis, and
this bias intensifies with color-encoded labels and shorter exposure time.
Based on our findings, we discuss strategies to mitigate bias in perceiving and
interpreting DR projections.

</details>


### [91] [ProForm: Solder-Free Circuit Assembly Using Thermoforming](https://arxiv.org/abs/2507.20933)
*Narjes Pourjafarian,Zhenming Yang,Jeffrey I. Lipton,Benyamin Davaji,Gregory D. Abowd*

Main category: cs.HC

TL;DR: ProForm是一种热成型方法，用于无焊电路原型设计，通过热塑性材料封装电子元件，实现可逆安装，支持多种基板，便于重复使用和快速原型设计。


<details>
  <summary>Details</summary>
Motivation: 电子垃圾问题日益严重，传统焊接方法限制了元件回收，导致不必要的浪费。

Method: 采用热成型技术，利用压力成型的塑料封装电子元件，无需焊接或定制机械外壳。

Result: ProForm电路表现出良好的电气性能和机械稳定性，支持快速原型设计和元件重复使用。

Conclusion: ProForm不仅有助于可持续电子实践，还具有超越传统焊接的显著优势。

Abstract: Electronic waste (e-waste) is a growing global challenge, with millions of
functional components discarded due to the difficulty of repair and reuse.
Traditional circuit assembly relies on soldering, which creates semi-permanent
bonds that limit component recovery and contribute to unnecessary waste. We
introduce ProForm, a thermoforming approach for solder-free circuit
prototyping. By encapsulating electronic components with pressure-formed
thermoplastics, ProForm enables secure, reversible mounting without the need
for solder or custom mechanical housings. This approach supports a wide range
of substrates, including flexible, paper-based, and non-planar circuits,
facilitating easy reuse, replacement, and rapid prototyping. We demonstrate
ProForm's versatility to support prototyping practices. We show that ProFormed
circuits exhibit good electrical performance and mechanical stability. While
motivated by a need for sustainable electronics practices, ProForm has other
significant advantages over traditional soldering.

</details>


### [92] [The Impact of Simple, Brief, and Adaptive Instructions within Virtual Reality Training: Components of Cognitive Load Theory in an Assembly Task](https://arxiv.org/abs/2507.20943)
*Rebecca L. Pharmer,Christopher D. Wickens,Lucas Plabst,Benjamin A. Clegg,Leanne M. Hirshfield,Joanna E. Lewis,Jalynn B. Nicoly,Cara A. Spencer,Francisco R. Ortega*

Main category: cs.HC

TL;DR: 研究探讨了虚拟现实中认知负载三要素对学习效率的影响，发现自适应训练能提高效率且不影响保留。


<details>
  <summary>Details</summary>
Motivation: 通过调整认知负载，优化虚拟现实中的学习效率，尤其在需要高效技能获取的领域（如制造业和军事）。

Method: 在VR环境中，通过调节内在负载（形状复杂度）、外在负载（指令冗长度）和关联负载（自适应与固定训练）进行实验。

Result: 较高内在负载增加训练时间，外在负载影响较小；自适应训练显著缩短时间且不影响保留。

Conclusion: 自适应训练在不损害保留的前提下优化效率，支持在关键领域应用VR自适应系统。

Abstract: Objective: The study examined the effects of varying all three core elements
of cognitive load on learning efficiency during a shape assembly task in
virtual reality (VR).
  Background: Adaptive training systems aim to improve learning efficiency and
retention by dynamically adjusting difficulty. However, design choices can
impact the cognitive workload imposed on the learner. The present experiments
examined how aspects of cognitive load impact training outcomes.
  Method: Participants learned step-by-step shape assembly in a VR environment.
Cognitive load was manipulated across three dimensions: Intrinsic Load (shape
complexity), Extraneous Load (instruction verbosity), and Germane Load
(adaptive vs. fixed training). In adaptive training (experiment 1), difficulty
increased based on individual performance. In fixed training (experiment 2),
difficulty followed a preset schedule from a yoked participant.
  Results: Higher Intrinsic Load significantly increased training times and
subjective workload but did not affect retention test accuracy. Extraneous Load
modestly impacted training time, with little impact on workload or retention.
Adaptive training shortened overall training time without increasing workload
or impairing retention. No interactions were observed between the three types
of load. Conclusion: Both Intrinsic and Extraneous Load increased training
time, but adaptive training improved efficiency without harming retention. The
lack of interaction between the elements suggests training benefits can be
worth seeking within any of the components of cognitive load. Application:
These findings support the use of VR adaptive systems in domains such as
manufacturing and military service, where efficient assembly skill acquisition
is critical. Tailoring difficulty in real-time can optimize efficiency without
compromising learning.

</details>


### [93] [Towards Effective Human Performance in XR Space Framework based on Real-time Eye Tracking Biofeedback](https://arxiv.org/abs/2507.21000)
*Barbara Karpowicz,Tomasz Kowalewski,Pavlo Zinevych,Adam Kuzdraliński,Grzegorz Marcin Wójcik,Wiesław Kopeć*

Main category: cs.HC

TL;DR: 提出了一种XR空间框架中的眼动追踪模块，旨在通过实时生物反馈提升XR应用中的用户表现，重点解决动态XR环境中眼动追踪的技术挑战。


<details>
  <summary>Details</summary>
Motivation: 眼动追踪作为非侵入性且强大的心理生理测量手段，在XR应用中具有实时调整任务难度和反馈的潜力，但动态环境中的整合面临挑战。

Method: 通过多模态测量（包括眼动追踪、身体运动和问卷调查）和实时引擎，集成眼动数据以实现生物反馈循环。

Result: 成功实现了眼动数据在实时XR环境中的集成，支持动态任务调整和反馈，提升学习和操作效率。

Conclusion: 该模块为实时XR应用提供了自适应工具，增强了用户体验和表现，并解决了动态环境中的技术难题。

Abstract: This paper proposes an eye tracking module for the XR Space Framework aimed
at enhancing human performance in XR-based applications, specifically in
training, screening, and teleoperation. This framework provides a methodology
and components that streamline the development of adaptive real-time virtual
immersive systems. It contains multimodal measurements - declarative in the
form of in-VR questionnaires and objective, including eye tracking, body
movement, and psychophysiological data (e.g., ECG, GSR, PPG). A key focus of
this paper is the integration of real-time eye tracking data into XR
environments to facilitate a biofeedback loop, providing insight into user
attention, cognitive load, and engagement. Given the relatively high
measurement frequency of eye tracking - recognized as a noninvasive yet robust
psychophysiological measure - this technology is particularly well suited for
real-time adjustments in task difficulty and feedback to enhance learning and
operational effectiveness. Despite its established role in cognitive and
attentional studies, implementing eye tracking metrics within dynamic,
real-time XR environments poses unique challenges, particularly given the
complex moving visuals presented in head-mounted displays (HMDs). This paper
addresses these challenges by focusing on the essential aspects of integrating
eye tracking in immersive systems based on real-time engines, ultimately
facilitating more efficient, adaptive XR applications.

</details>


### [94] [User-Centered Design with AI in the Loop: A Case Study of Rapid User Interface Prototyping with "Vibe Coding"](https://arxiv.org/abs/2507.21012)
*Tianyi Li,Tanay Maheshwari,Alex Voelker*

Main category: cs.HC

TL;DR: 本文研究了一种名为“氛围编码”的生成式用户界面方法，利用大语言模型（LLMs）通过自然语言提示生成代码，以支持用户为中心的设计（UCD）中的快速原型设计。


<details>
  <summary>Details</summary>
Motivation: 传统UCD实践在快速原型设计中存在局限性，作者希望通过AI辅助的“构思-原型”流程，弥补设计专业知识与领域专业知识之间的差距。

Method: 研究采用了AI-in-the-loop的流程，结合生成式用户界面和实时协作会议，开发了一个交互式数据分析界面，用于高速公路交通工程师检索和分析历史交通数据。

Result: 通过生成式用户界面，团队能够从用户评估访谈和领域专家的实时协作会议中获取丰富的反馈，并测试多种替代设计方案。

Conclusion: 研究探讨了“氛围编码”在填补设计与领域知识差距方面的优势和潜在问题，为未来的AI辅助设计工具提供了参考。

Abstract: We present a case study of using generative user interfaces, or ``vibe
coding,'' a method leveraging large language models (LLMs) for generating code
via natural language prompts, to support rapid prototyping in user-centered
design (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop
ideate-prototyping process. We share insights from an empirical experience
integrating this process to develop an interactive data analytics interface for
highway traffic engineers to effectively retrieve and analyze historical
traffic data. With generative UIs, the team was able to elicit rich user
feedback and test multiple alternative design ideas from user evaluation
interviews and real-time collaborative sessions with domain experts. We discuss
the advantages and pitfalls of vibe coding for bridging the gaps between design
expertise and domain-specific expertise.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [95] [GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D Gaussian Splatting](https://arxiv.org/abs/2507.19718)
*David Bauer,Qi Wu,Hamid Gadirov,Kwan-Liu Ma*

Main category: cs.GR

TL;DR: 提出了一种新型的辐射缓存方法，用于路径追踪的体绘制，显著降低噪声并提高渲染质量，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 科学可视化中，真实感渲染技术因蒙特卡洛积分导致渲染速度慢和像素方差高的问题，亟需解决方案。

Method: 利用3D高斯斑块技术，设计可动态适应场景参数变化的路径空间辐射缓存，实时训练以减少噪声。

Result: 相比基线路径追踪器和神经辐射缓存方法，新方法在定量和定性分析中均表现优异。

Conclusion: 该方法易于集成，显著提升体绘制应用的渲染质量，同时保持计算效率。

Abstract: Real-time path tracing is rapidly becoming the standard for rendering in
entertainment and professional applications. In scientific visualization,
volume rendering plays a crucial role in helping researchers analyze and
interpret complex 3D data. Recently, photorealistic rendering techniques have
gained popularity in scientific visualization, yet they face significant
challenges. One of the most prominent issues is slow rendering performance and
high pixel variance caused by Monte Carlo integration. In this work, we
introduce a novel radiance caching approach for path-traced volume rendering.
Our method leverages advances in volumetric scene representation and adapts 3D
Gaussian splatting to function as a multi-level, path-space radiance cache.
This cache is designed to be trainable on the fly, dynamically adapting to
changes in scene parameters such as lighting configurations and transfer
functions. By incorporating our cache, we achieve less noisy, higher-quality
images without increasing rendering costs. To evaluate our approach, we compare
it against a baseline path tracer that supports uniform sampling and next-event
estimation and the state-of-the-art for neural radiance caching. Through both
quantitative and qualitative analyses, we demonstrate that our path-space
radiance cache is a robust solution that is easy to integrate and significantly
enhances the rendering quality of volumetric visualization applications while
maintaining comparable computational efficiency.

</details>


### [96] [Taking Language Embedded 3D Gaussian Splatting into the Wild](https://arxiv.org/abs/2507.19830)
*Yuze Wang,Yue Qi*

Main category: cs.GR

TL;DR: 论文提出了一种新框架，通过扩展3D高斯散射技术（3DGS）并结合语言嵌入，实现了对无约束照片集合的开放式词汇场景理解。


<details>
  <summary>Details</summary>
Motivation: 现有技术主要关注3D重建，但缺乏对建筑风格和结构知识的沉浸式理解。

Method: 结合多视角CLIP特征和语言特征不确定性映射，提出了一种瞬态不确定性感知自动编码器和多视角语言场的3DGS表示方法。

Result: 实验表明，该方法在无约束照片集合上的开放式词汇分割任务中表现优异，且支持互动漫游、建筑风格识别等应用。

Conclusion: 该框架为建筑结构和风格的沉浸式理解提供了新思路，具有广泛的应用潜力。

Abstract: Recent advances in leveraging large-scale Internet photo collections for 3D
reconstruction have enabled immersive virtual exploration of landmarks and
historic sites worldwide. However, little attention has been given to the
immersive understanding of architectural styles and structural knowledge, which
remains largely confined to browsing static text-image pairs. Therefore, can we
draw inspiration from 3D in-the-wild reconstruction techniques and use
unconstrained photo collections to create an immersive approach for
understanding the 3D structure of architectural components? To this end, we
extend language embedded 3D Gaussian splatting (3DGS) and propose a novel
framework for open-vocabulary scene understanding from unconstrained photo
collections. Specifically, we first render multiple appearance images from the
same viewpoint as the unconstrained image with the reconstructed radiance
field, then extract multi-appearance CLIP features and two types of language
feature uncertainty maps-transient and appearance uncertainty-derived from the
multi-appearance features to guide the subsequent optimization process. Next,
we propose a transient uncertainty-aware autoencoder, a multi-appearance
language field 3DGS representation, and a post-ensemble strategy to effectively
compress, learn, and fuse language features from multiple appearances. Finally,
to quantitatively evaluate our method, we introduce PT-OVS, a new benchmark
dataset for assessing open-vocabulary segmentation performance on unconstrained
photo collections. Experimental results show that our method outperforms
existing methods, delivering accurate open-vocabulary segmentation and enabling
applications such as interactive roaming with open-vocabulary queries,
architectural style pattern recognition, and 3D scene editing.

</details>


### [97] [ChoreoMuse: Robust Music-to-Dance Video Generation with Style Transfer and Beat-Adherent Motion](https://arxiv.org/abs/2507.19836)
*Xuanchen Wang,Heng Wang,Weidong Cai*

Main category: cs.GR

TL;DR: ChoreoMuse是一个基于扩散的框架，用于生成高质量、风格可控的舞蹈视频，解决了现有方法在音乐节奏和舞蹈风格协调上的不足。


<details>
  <summary>Details</summary>
Motivation: 解决现有舞蹈生成方法无法同时适应多样音乐风格和个体舞者特性的问题，提升生成视频的质量和实用性。

Method: 利用SMPL格式参数作为音乐和视频生成的媒介，结合MotionTune音乐编码器捕捉音频运动线索，支持高保真舞蹈视频生成。

Result: 在视频质量、节奏对齐、舞蹈多样性和风格一致性上达到最先进水平，展示了广泛的创造性应用潜力。

Conclusion: ChoreoMuse为多样音乐和舞蹈风格提供了高效的生成解决方案，拓展了自动编舞的实际应用范围。

Abstract: Modern artistic productions increasingly demand automated choreography
generation that adapts to diverse musical styles and individual dancer
characteristics. Existing approaches often fail to produce high-quality dance
videos that harmonize with both musical rhythm and user-defined choreography
styles, limiting their applicability in real-world creative contexts. To
address this gap, we introduce ChoreoMuse, a diffusion-based framework that
uses SMPL format parameters and their variation version as intermediaries
between music and video generation, thereby overcoming the usual constraints
imposed by video resolution. Critically, ChoreoMuse supports
style-controllable, high-fidelity dance video generation across diverse musical
genres and individual dancer characteristics, including the flexibility to
handle any reference individual at any resolution. Our method employs a novel
music encoder MotionTune to capture motion cues from audio, ensuring that the
generated choreography closely follows the beat and expressive qualities of the
input music. To quantitatively evaluate how well the generated dances match
both musical and choreographic styles, we introduce two new metrics that
measure alignment with the intended stylistic cues. Extensive experiments
confirm that ChoreoMuse achieves state-of-the-art performance across multiple
dimensions, including video quality, beat alignment, dance diversity, and style
adherence, demonstrating its potential as a robust solution for a wide range of
creative applications. Video results can be found on our project page:
https://choreomuse.github.io.

</details>


### [98] [Neural Shell Texture Splatting: More Details and Fewer Primitives](https://arxiv.org/abs/2507.20200)
*Xin Zhang,Anpei Chen,Jincheng Xiong,Pinxuan Dai,Yujun Shen,Weiwei Xu*

Main category: cs.GR

TL;DR: 提出了一种神经壳纹理方法，通过解耦几何与外观，减少高斯泼溅技术所需的基元数量，提高参数效率和纹理细节重建。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅技术在新视角合成中表现出色，但需要大量基元，原因是几何与外观的耦合。

Method: 引入神经壳纹理作为全局表示，利用高斯基元同时作为几何表示和纹理采样器。

Result: 方法在减少基元数量的同时，实现了高效的参数利用、精细纹理重建和易于提取纹理网格。

Conclusion: 解耦几何与外观的方法在高斯泼溅中显著提升了效率与质量。

Abstract: Gaussian splatting techniques have shown promising results in novel view
synthesis, achieving high fidelity and efficiency. However, their high
reconstruction quality comes at the cost of requiring a large number of
primitives. We identify this issue as stemming from the entanglement of
geometry and appearance in Gaussian Splatting. To address this, we introduce a
neural shell texture, a global representation that encodes texture information
around the surface. We use Gaussian primitives as both a geometric
representation and texture field samplers, efficiently splatting texture
features into image space. Our evaluation demonstrates that this
disentanglement enables high parameter efficiency, fine texture detail
reconstruction, and easy textured mesh extraction, all while using
significantly fewer primitives.

</details>


### [99] [Methodology for intelligent injection point location based on geometric algorithms and discrete topologies for virtual digital twin environments](https://arxiv.org/abs/2507.20922)
*J. Mercado Colmenero,A. Torres Alba,C. Martin Donate*

Main category: cs.GR

TL;DR: 本文提出了一种创新的方法，利用几何算法和智能模型定位注塑件的注塑点，优化熔融塑料分布，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在减少注塑模具设计中的人工干预，降低成本和时间，提供一种不依赖特定CAD软件的通用解决方案。

Method: 采用几何算法计算离散模型的质心，生成几何矩阵并投影到零件表面，通过最小化与质心的距离确定最佳注塑点。

Result: 六项案例研究表明，该方法能实现均匀的熔融塑料分布和最小化的压力损失，效果优于现有技术。

Conclusion: 该方法为数字化双胞胎应用提供了高效、经济的注塑点定位方案，减少了对专家的依赖，适用于多样化设计环境。

Abstract: This article presents an innovative methodology for locating injection points
in injection-molded parts using intelligent models with geometric algorithms
for discrete topologies. The first algorithm calculates the center of mass of
the discrete model based on the center of mass of each triangular facet in the
system, ensuring uniform molten plastic distribution during mold cavity
filling. Two sub-algorithms intelligently evaluate the geometry and optimal
injection point location. The first sub-algorithm generates a geometric matrix
based on a two-dimensional nodal quadrature adapted to the part's bounding box.
The second sub-algorithm projects the nodal matrix and associated circular
areas orthogonally on the part's surface along the demolding direction. The
optimal injection point location is determined by minimizing the distance to
the center of mass from the first algorithm's result. This novel methodology
has been validated through rheological simulations in six case studies with
complex geometries. The results demonstrate uniform and homogeneous molten
plastic distribution with minimal pressure loss during the filling phase.
Importantly, this methodology does not require expert intervention, reducing
time and costs associated with manual injection mold feed system design. It is
also adaptable to various design environments and virtual twin systems, not
tied to specific CAD software. The validated results surpass the state of the
art, offering an agile alternative for digital twin applications in new product
design environments, reducing dependence on experts, facilitating designer
training, and ultimately cutting costs

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [100] [K-PACT: Kernel Planning for Adaptive Context Switching -- A Framework for Clustering, Placement, and Prefetching in Spectrum Sensing](https://arxiv.org/abs/2507.19662)
*H. Umut Suluhan,Jiahao Lin,Serhan Gener,Chaitali Chakrabarti,Umit Ogras,Ali Akoglu*

Main category: cs.ET

TL;DR: 论文提出了一种高效的规划工具，用于在可重构架构上快速映射频谱感知假设的工作流，显著减少了上下文切换开销和数据移动延迟。


<details>
  <summary>Details</summary>
Motivation: 动态频谱环境要求快速评估和重新评估信号的存在及类型，传统的多假设测试方法因计算密集型内核和频繁的上下文切换导致性能瓶颈。

Method: 采用粗粒度可重构架构，提出一种规划工具，通过聚类非重叠内核并优化硬件映射，实现低延迟的运行时上下文切换。

Result: 在48个并发子通道的模拟场景中，工具减少了207.81倍的二进制加载，98.24倍的切换时间，并提升132.92倍的执行效率。

Conclusion: 智能规划工具对于下一代射频系统适应快速变化的频谱环境至关重要。

Abstract: Efficient wideband spectrum sensing requires rapid evaluation and
re-evaluation of signal presence and type across multiple subchannels. These
tasks involve multiple hypothesis testing, where each hypothesis is implemented
as a decision tree workflow containing compute-intensive kernels, including
FFT, matrix operations, and signal-specific analyses. Given dynamic nature of
the spectrum environment, ability to quickly switch between hypotheses is
essential for maintaining low-latency, high-throughput operation. This work
assumes a coarse-grained reconfigurable architecture consisting of an array of
processing elements (PEs), each equipped with a local instruction memory (IMEM)
capable of storing and executing kernels used in spectrum sensing applications.
We propose a planner tool that efficiently maps hypothesis workflows onto this
architecture to enable fast runtime context switching with minimal overhead.
The planner performs two key tasks: clustering temporally non-overlapping
kernels to share IMEM resources within a PE sub-array, and placing these
clusters onto hardware to ensure efficient scheduling and data movement. By
preloading kernels that are not simultaneously active into same IMEM, our tool
enables low-latency reconfiguration without runtime conflicts. It models the
planning process as a multi-objective optimization, balancing trade-offs among
context switch overhead, scheduling latency, and dataflow efficiency. We
evaluate the proposed tool in simulated spectrum sensing scenario with 48
concurrent subchannels. Results show that our approach reduces off-chip binary
fetches by 207.81x, lowers average switching time by 98.24x, and improves
per-subband execution time by 132.92x over baseline without preloading. These
improvements demonstrate that intelligent planning is critical for adapting to
fast-changing spectrum environments in next-generation radio frequency systems.

</details>


### [101] [Enhancing IoT Intrusion Detection Systems through Adversarial Training](https://arxiv.org/abs/2507.19739)
*Karma Gurung,Ashutosh Ghimire,Fathi Amsaad*

Main category: cs.ET

TL;DR: 设计了一种基于NF-ToN-IoT v2数据集的入侵检测系统，通过分布式预处理和对抗训练提高鲁棒性，准确率达95.3%（干净数据）和94.5%（对抗数据）。


<details>
  <summary>Details</summary>
Motivation: 解决物联网设备安全性不足的问题，提升入侵检测系统对抗复杂攻击的能力。

Method: 结合分布式预处理和FGSM对抗攻击，使用XGBoost模型进行对抗训练。

Result: 系统在干净和对抗数据上分别达到95.3%和94.5%的准确率。

Conclusion: 对抗训练能有效提升入侵检测系统的鲁棒性，未来可扩展至实时环境和新兴威胁检测。

Abstract: The augmentation of Internet of Things (IoT) devices transformed both
automation and connectivity but revealed major security vulnerabilities in
networks. We address these challenges by designing a robust intrusion detection
system (IDS) to detect complex attacks by learning patterns from the NF-ToN-IoT
v2 dataset. Intrusion detection has a realistic testbed through the dataset's
rich and high-dimensional features. We combine distributed preprocessing to
manage the dataset size with Fast Gradient Sign Method (FGSM) adversarial
attacks to mimic actual attack scenarios and XGBoost model adversarial training
for improved system robustness. Our system achieves 95.3% accuracy on clean
data and 94.5% accuracy on adversarial data to show its effectiveness against
complex threats. Adversarial training demonstrates its potential to strengthen
IDS against evolving cyber threats and sets the foundation for future studies.
Real-time IoT environments represent a future deployment opportunity for these
systems, while extensions to detect emerging threats and zero-day
vulnerabilities would enhance their utility.

</details>


### [102] [Efficient and Fault-Tolerant Memristive Neural Networks with In-Situ Training](https://arxiv.org/abs/2507.20193)
*Santlal Prajapat,Manobendra Nath Mondal,Susmita Sur-Kolay*

Main category: cs.ET

TL;DR: 提出了一种基于忆阻器的多层神经网络架构和高效的原地训练算法，实现了常数时间复杂度，并在多个数据集上表现出高分类准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经形态架构通过并行和内存内处理加速人工神经网络计算，但现有设计在效率和面积上仍有改进空间。

Method: 使用忆阻器交叉阵列的固有并行性，实现矩阵向量乘法、外积和权重更新的常数时间复杂度；每个突触仅需一个忆阻器，无需晶体管。

Result: 在IRIS、NASA Asteroid和Breast Cancer Wisconsin数据集上分别达到98.22%、90.43%和98.59%的分类准确率，且对忆阻器故障、非线性和器件变异表现出强鲁棒性。

Conclusion: 该架构具有高效率和鲁棒性，适用于实际应用场景。

Abstract: Neuromorphic architectures, which incorporate parallel and in-memory
processing, are crucial for accelerating artificial neural network (ANN)
computations. This work presents a novel memristor-based multi-layer neural
network (memristive MLNN) architecture and an efficient in-situ training
algorithm. The proposed design performs matrix-vector multiplications, outer
products, and weight updates in constant time $\mathcal{O}(1)$, leveraging the
inherent parallelism of memristive crossbars. Each synapse is realized using a
single memristor, eliminating the need for transistors, and offering enhanced
area and energy efficiency. The architecture is evaluated through LTspice
simulations on the IRIS, NASA Asteroid, and Breast Cancer Wisconsin datasets,
achieving classification accuracies of 98.22\%, 90.43\%, and 98.59\%,
respectively. Robustness is assessed by introducing stuck-at-conducting-state
faults in randomly selected memristors. The effects of nonlinearity in
memristor conductance and a 10\% device variation are also analyzed. The
simulation results establish that the network's performance is not affected
significantly by faulty memristors, non-linearity, and device variation.

</details>


### [103] [Efficient Memristive Spiking Neural Networks Architecture with Supervised In-Situ STDP Method](https://arxiv.org/abs/2507.20998)
*Santlal Prajapati,Susmita Sur-Kolay,Soumyadeep Dutta*

Main category: cs.ET

TL;DR: 论文提出了一种基于忆阻器的脉冲神经网络架构，采用新型监督学习算法，实现了高效训练和高分类精度。


<details>
  <summary>Details</summary>
Motivation: 利用忆阻器实现超低能耗计算，适用于电池供电的智能设备。

Method: 提出了一种基于STDP的监督学习方法，并行更新神经元突触，模块化设计提高可扩展性。

Result: 在模式识别（如5x3二值图像）和分类任务（Iris和BCW数据集）中表现优异，准确率分别达到99.11%和97.9%。

Conclusion: 该架构在训练效率、分类精度和抗干扰能力上表现良好，适用于智能设备的硬件实现。

Abstract: Memristor-based Spiking Neural Networks (SNNs) with temporal spike encoding
enable ultra-low-energy computation, making them ideal for battery-powered
intelligent devices. This paper presents a circuit-level memristive spiking
neural network (SNN) architecture trained using a proposed novel supervised
in-situ learning algorithm inspired by spike-timing-dependent plasticity
(STDP). The proposed architecture efficiently implements lateral inhibition and
the refractory period, eliminating the need for external microcontrollers or
ancillary control hardware. All synapses of the winning neurons are updated in
parallel, enhancing training efficiency. The modular design ensures scalability
with respect to input data dimensions and output class count. The SNN is
evaluated in LTspice for pattern recognition (using 5x3 binary images) and
classification tasks using the Iris and Breast Cancer Wisconsin (BCW) datasets.
During testing, the system achieved perfect pattern recognition and high
classification accuracies of 99.11\% (Iris) and 97.9\% (BCW). Additionally, it
has demonstrated robustness, maintaining an average recognition rate of 93.4\%
under 20\% input noise. The impact of stuck-at-conductance faults and memristor
device variations was also analyzed.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [104] [Quantifying the Performance Gap for Simple Versus Optimal Dynamic Server Allocation Policies](https://arxiv.org/abs/2507.19667)
*Niklas Carlsson,Derek Eager*

Main category: cs.DC

TL;DR: 论文研究了云计算中动态服务器分配的简单策略与最优策略之间的性能差距，并应用于多站点系统的性能提升。


<details>
  <summary>Details</summary>
Motivation: 利用云计算的动态资源分配能力，研究如何在负载变化时动态分配服务器以提高性能并降低成本。

Method: 开发了动态服务器分配的简单策略分析模型，并设计半马尔可夫决策模型以确定最优策略性能。

Result: 量化了简单策略与最优策略之间的性能差距，并展示了多站点系统中状态相关路由的潜在性能优势。

Conclusion: 研究结果为服务提供商在平衡云服务成本和延迟方面提供了有价值的见解。

Abstract: Cloud computing enables the dynamic provisioning of server resources. To
exploit this opportunity, a policy is needed for dynamically allocating (and
deallocating) servers in response to the current load conditions. In this paper
we describe several simple policies for dynamic server allocation and develop
analytic models for their analysis. We also design semi-Markov decision models
that enable determination of the performance achieved with optimal policies,
allowing us to quantify the performance gap between simple, easily implemented
policies, and optimal policies. Finally, we apply our models to study the
potential performance benefits of state-dependent routing in multi-site systems
when using dynamic server allocation at each site. Insights from our results
are valuable to service providers wanting to balance cloud service costs and
delays.

</details>


### [105] [Oranits: Mission Assignment and Task Offloading in Open RAN-based ITS using Metaheuristic and Deep Reinforcement Learning](https://arxiv.org/abs/2507.19712)
*Ngoc Hung Nguyen,Nguyen Van Thieu,Quang-Trung Luu,Anh Tuan Nguyen,Senura Wanasekara,Nguyen Cong Luong,Fatemeh Kavehmadavani,Van-Dinh Nguyen*

Main category: cs.DC

TL;DR: 论文提出了一种名为Oranits的系统模型，通过双重优化方法（CGG-ARO和MA-DDQN）解决智能交通系统中任务卸载和分配的复杂依赖问题。实验证明，Oranits能显著提高任务完成率和整体效益。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽视任务间的复杂依赖关系和边缘服务器卸载成本，导致决策不优化。本文旨在填补这一空白。

Method: 1. 提出混沌高斯全局ARO（CGG-ARO）作为单时隙优化的元启发式算法；2. 设计多智能体双深度Q网络（MA-DDQN）框架，结合多智能体协调和多动作选择机制。

Result: CGG-ARO将任务完成率和整体效益分别提高7.1%和7.7%；MA-DDQN进一步分别提高11.0%和12.5%。

Conclusion: Oranits在动态智能交通环境中实现了更快、更自适应和更高效的任务处理。

Abstract: In this paper, we explore mission assignment and task offloading in an Open
Radio Access Network (Open RAN)-based intelligent transportation system (ITS),
where autonomous vehicles leverage mobile edge computing for efficient
processing. Existing studies often overlook the intricate interdependencies
between missions and the costs associated with offloading tasks to edge
servers, leading to suboptimal decision-making. To bridge this gap, we
introduce Oranits, a novel system model that explicitly accounts for mission
dependencies and offloading costs while optimizing performance through vehicle
cooperation. To achieve this, we propose a twofold optimization approach.
First, we develop a metaheuristic-based evolutionary computing algorithm,
namely the Chaotic Gaussian-based Global ARO (CGG-ARO), serving as a baseline
for one-slot optimization. Second, we design an enhanced reward-based deep
reinforcement learning (DRL) framework, referred to as the Multi-agent Double
Deep Q-Network (MA-DDQN), that integrates both multi-agent coordination and
multi-action selection mechanisms, significantly reducing mission assignment
time and improving adaptability over baseline methods. Extensive simulations
reveal that CGG-ARO improves the number of completed missions and overall
benefit by approximately 7.1% and 7.7%, respectively. Meanwhile, MA-DDQN
achieves even greater improvements of 11.0% in terms of mission completions and
12.5% in terms of the overall benefit. These results highlight the
effectiveness of Oranits in enabling faster, more adaptive, and more efficient
task processing in dynamic ITS environments.

</details>


### [106] [Accelerating Matrix Multiplication: A Performance Comparison Between Multi-Core CPU and GPU](https://arxiv.org/abs/2507.19723)
*Mufakir Qamar Ansari,Mudabir Qamar Ansari*

Main category: cs.DC

TL;DR: 该论文通过实证分析比较了顺序、多核CPU和GPU在矩阵乘法上的性能，结果显示GPU在大规模问题上具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 矩阵乘法是科学计算和机器学习的基础操作，但其计算复杂度高，成为大规模应用的瓶颈。研究旨在分析现代异构平台上矩阵乘法的性能表现。

Method: 论文实现了三种版本的矩阵乘法算法：顺序C++实现、基于OpenMP的多核CPU并行实现、基于CUDA和共享内存优化的GPU并行实现，并在不同规模的矩阵上进行基准测试。

Result: 结果显示，多核CPU比顺序版本提速12-14倍，而GPU的性能随问题规模显著提升，对于4096x4096矩阵，GPU比顺序版本提速约593倍。

Conclusion: 研究表明，多核GPU架构对数据并行负载具有深远影响，即使是消费级硬件也能带来显著的性能提升。

Abstract: Matrix multiplication is a foundational operation in scientific computing and
machine learning, yet its computational complexity makes it a significant
bottleneck for large-scale applications. The shift to parallel architectures,
primarily multi-core CPUs and many-core GPUs, is the established solution, and
these systems are now ubiquitous from datacenters to consumer laptops. This
paper presents a direct, empirical performance analysis of matrix
multiplication on a modern, consumer-grade heterogeneous platform. We
implemented and benchmarked three versions of the algorithm: a baseline
sequential C++ implementation, a parallel version for its multi-core CPU using
OpenMP, and a massively parallel version for its discrete GPU using CUDA with
shared memory optimizations. The implementations were evaluated with square
matrices of varying dimensions, from 128x128 to 4096x4096. Our results show
that while the parallel CPU provides a consistent speedup of 12-14x over the
sequential version, the GPU's performance scales dramatically with problem
size. For a 4096x4096 matrix, the GPU implementation achieved a speedup of
approximately 593x over the sequential baseline and 45x over the optimized
parallel CPU version. These findings quantitatively demonstrate the profound
impact of many-core GPU architectures on accelerating data-parallel workloads,
underscoring that significant performance gains are readily accessible even on
consumer-level hardware.

</details>


### [107] [MegatronApp: Efficient and Comprehensive Management on Distributed LLM Training](https://arxiv.org/abs/2507.19845)
*Bohan Zhao,Guang Yang,Shuo Chen,Ruitao Liu,Tingrui Zhang,Yongchao He,Wei Xu*

Main category: cs.DC

TL;DR: MegatronApp是一个开源工具链，旨在解决大型语言模型（LLM）在训练过程中的系统级挑战，提升可靠性、效率和透明性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数量激增，训练过程变得复杂且跨节点，现有框架虽支持万亿参数训练，但在性能优化、诊断和可解释性方面面临挑战。

Method: MegatronApp引入四个模块（MegaScan、MegaFBD、MegaDPP和MegaScope），它们正交且可组合，专注于提升训练可靠性、效率和透明性。

Result: 这些模块的协同集成增强了Megatron-LM生态系统的能力。

Conclusion: MegatronApp通过模块化设计，为生产规模训练提供了高效、可靠的解决方案。

Abstract: The rapid escalation in the parameter count of large language models (LLMs)
has transformed model training from a single-node endeavor into a highly
intricate, cross-node activity. While frameworks such as Megatron-LM
successfully integrate tensor (TP), pipeline (PP), and data (DP) parallelism to
enable trillion-parameter training, they simultaneously expose practitioners to
unprecedented systems-level challenges in performance optimization, diagnosis,
and interpretability. MegatronApp is an open-source toolchain expressly
designed to meet these challenges. It introduces four orthogonal, yet
seamlessly composable modules--MegaScan, MegaFBD, MegaDPP, and MegaScope--that
collectively elevate the reliability, efficiency, and transparency of
production-scale training. This paper presents the motivation, architecture,
and distinctive contributions of each module, and elucidates how their
synergistic integration augments the Megatron-LM ecosystem.

</details>


### [108] [A Comparative Study of OpenMP Scheduling Algorithm Selection Strategies](https://arxiv.org/abs/2507.20312)
*Jonas H. Müller Korndörfer,Ali Mohammed,Ahmed Eleliemy,Quentin Guilloteau,Reto Krummenacher,Florina M. Ciorba*

Main category: cs.DC

TL;DR: 论文探讨了基于学习的方法（专家知识与强化学习）用于OpenMP中调度算法选择，通过实验验证了其可行性与优势，并展示了其在MPI程序中的扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 随着科学和数据科学应用的复杂性增加，高性能计算系统中的调度和负载平衡技术变得至关重要。OpenMP等并行编程框架提供了多样化的调度算法，但如何选择最优算法成为关键问题。

Method: 提出了基于专家知识和强化学习的方法，并在六种应用和三个系统上进行了性能分析。通过结合两者，实现了更好的性能和适应性。

Result: 结果显示，强化学习方法能够学习高性能的调度决策，但需要大量探索，而专家方法依赖先验知识但可能无法找到最优解。结合两者可提升性能。

Conclusion: 动态调度算法选择对OpenMP应用可行且有益，并可扩展至MPI程序，优化多级并行调度决策。

Abstract: Scientific and data science applications are becoming increasingly complex,
with growing computational and memory demands. Modern high performance
computing (HPC) systems provide high parallelism and heterogeneity across
nodes, devices, and cores. To achieve good performance, effective scheduling
and load balancing techniques are essential. Parallel programming frameworks
such as OpenMP now offer a variety of advanced scheduling algorithms to support
diverse applications and platforms. This creates an instance of the scheduling
algorithm selection problem, which involves identifying the most suitable
algorithm for a given combination of workload and system characteristics.
  In this work, we explore learning-based approaches for selecting scheduling
algorithms in OpenMP. We propose and evaluate expert-based and reinforcement
learning (RL)-based methods, and conduct a detailed performance analysis across
six applications and three systems. Our results show that RL methods are
capable of learning high-performing scheduling decisions, although they require
significant exploration, with the choice of reward function playing a key role.
Expert-based methods, in contrast, rely on prior knowledge and involve less
exploration, though they may not always identify the optimal algorithm for a
specific application-system pair. By combining expert knowledge with RL-based
learning, we achieve improved performance and greater adaptability.
  Overall, this work demonstrates that dynamic selection of scheduling
algorithms during execution is both viable and beneficial for OpenMP
applications. The approach can also be extended to MPI-based programs, enabling
optimization of scheduling decisions across multiple levels of parallelism.

</details>


### [109] [A Fast Parallel Median Filtering Algorithm Using Hierarchical Tiling](https://arxiv.org/abs/2507.19926)
*Louis Sugy*

Main category: cs.DC

TL;DR: 该论文提出了一种新颖的中值滤波算法，通过分层分块减少冗余计算，实现了更高的计算效率，GPU实现速度比现有技术快5倍。


<details>
  <summary>Details</summary>
Motivation: 中值滤波虽然能有效去除噪声并保留边缘，但其高计算成本限制了应用。论文旨在提出一种更高效的算法，解决现有排序方法扩展性差的问题。

Method: 利用排序问题的可分性，通过分层分块优化计算效率。提出两种变体：一种完全在寄存器中操作的数据无关选择网络，另一种利用随机存取内存的数据感知版本。

Result: 算法在$k \times k$核下的每像素复杂度分别为$O(k \log(k))$和$O(k)$，CUDA实现在现代GPU上比现有技术快5倍，适用于多种数据类型的8-、16-和32位数据。

Conclusion: 该算法显著提升了中值滤波的计算效率，为图像处理领域提供了一种更快速的选择，尤其在大核情况下表现突出。

Abstract: Median filtering is a non-linear smoothing technique widely used in digital
image processing to remove noise while retaining sharp edges. It is
particularly well suited to removing outliers (impulse noise) or granular
artifacts (speckle noise). However, the high computational cost of median
filtering can be prohibitive. Sorting-based algorithms excel with small kernels
but scale poorly with increasing kernel diameter, in contrast to constant-time
methods characterized by higher constant factors but better scalability, such
as histogram-based approaches or the 2D wavelet matrix.
  This paper introduces a novel algorithm, leveraging the separability of the
sorting problem through hierarchical tiling to minimize redundant computations.
We propose two variants: a data-oblivious selection network that can operate
entirely within registers, and a data-aware version utilizing random-access
memory. These achieve per-pixel complexities of $O(k \log(k))$ and $O(k)$,
respectively, for a $k \times k$ kernel - unprecedented for sorting-based
methods. Our CUDA implementation is up to 5 times faster than the current state
of the art on a modern GPU and is the fastest median filter in most cases for
8-, 16-, and 32-bit data types and kernels from $3 \times 3$ to $75 \times 75$.

</details>


### [110] [Offloading tracing for real-time systems using a scalable cloud infrastructure](https://arxiv.org/abs/2507.19953)
*David Jannis Schmidt,Grigory Fridman,Florian von Zabiensky*

Main category: cs.DC

TL;DR: 提出了一种基于微服务和边缘计算的实时系统软件追踪云架构，将追踪处理从本地转移至云端，支持长期监控和协作分析。


<details>
  <summary>Details</summary>
Motivation: 解决传统本地追踪工具处理能力有限的问题，支持大规模实时系统的精确追踪和错误检测。

Method: 采用微服务和边缘计算架构，利用WebSockets和Apache Kafka实现追踪数据的捕获与传输。

Result: 架构能高效处理多并行追踪会话，整体吞吐量随负载增加而提升，单会话吞吐略有下降。

Conclusion: 架构不仅适用于开发环境，还可扩展至现场网络的运行时监控。

Abstract: Real-time embedded systems require precise timing and fault detection to
ensure correct behavior. Traditional tracing tools often rely on local desktops
with limited processing and storage capabilities, which hampers large-scale
analysis. This paper presents a scalable, cloud-based architecture for software
tracing in real-time systems based on microservices and edge computing. Our
approach shifts the trace processing workload from the developer's machine to
the cloud, using a dedicated tracing component that captures trace data and
forwards it to a scalable backend via WebSockets and Apache Kafka. This enables
long-term monitoring and collaborative analysis of target executions, e.g., to
detect and investigate sporadic errors. We demonstrate how this architecture
supports scalable analysis of parallel tracing sessions and lays the foundation
for future integration of rule-based testing and runtime verification. The
evaluation results show that the architecture can handle many parallel tracing
sessions efficiently, although the per-session throughput decreases slightly as
the system load increases, while the overall throughput increases. Although the
design includes a dedicated tracer for analysis during development, this
approach is not limited to such setups. Target systems with network
connectivity can stream reduced trace data directly, enabling runtime
monitoring in the field.

</details>


### [111] [MTASet: A Tree-based Set for Efficient Range Queries in Update-heavy Workloads](https://arxiv.org/abs/2507.20041)
*Daniel Manor,Mor Perry,Moshe Sulamy*

Main category: cs.DC

TL;DR: MTASet是一种基于并发(a,b)-树实现的数据结构，旨在优化高更新负载和原子范围查询，性能优于现有解决方案2倍，同时保证线性一致性。


<details>
  <summary>Details</summary>
Motivation: 现有并发集在读写操作不均衡时性能不足，尤其是高更新负载或需要原子范围查询的场景，亟需一种高效解决方案。

Method: 利用并发(a,b)-树的实现，设计MTASet，支持高更新负载和原子范围查询。

Result: MTASet在范围查询操作中性能提升2倍，并确保线性一致性。

Conclusion: MTASet为高更新负载和原子范围查询提供了一种高效且可靠的并发集实现。

Abstract: In concurrent data structures, the efficiency of set operations can vary
significantly depending on the workload characteristics. Numerous concurrent
set implementations are optimized and fine-tuned to excel in scenarios
characterized by predominant read operations. However, they often perform
poorly when confronted with workloads that heavily prioritize updates.
Additionally, current leading-edge concurrent sets optimized for update-heavy
tasks typically lack efficiency in handling atomic range queries. This study
introduces the MTASet, which leverages a concurrent (a,b)-tree implementation.
Engineered to accommodate update-heavy workloads and facilitate atomic range
queries, MTASet surpasses existing counterparts optimized for tasks in range
query operations by up to 2x. Notably, MTASet ensures linearizability.

</details>


### [112] [Ethereum Conflicts Graphed](https://arxiv.org/abs/2507.20196)
*Dvir David Biton,Roy Friedman,Yaron Hay*

Main category: cs.DC

TL;DR: 论文分析了以太坊智能合约的调用图和冲突图结构，揭示了其并行化潜力及区块内交易特性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在优化以太坊性能，通过分析智能合约调用和冲突图，探索并行执行的潜力。

Method: 追踪了超过200万个以太坊区块，使用调用追踪器和状态预追踪器，研究交易分布、调用树结构和冲突图。

Result: 发现冲突图多呈现星形结构，揭示了智能合约调用的并行化可能性和区块内交易特性。

Conclusion: 研究为以太坊性能优化提供了新视角，尤其强调了并行化智能合约执行的潜力。

Abstract: Ethereum, a leading blockchain platform, has revolutionized the digital
economy by enabling decentralized transactions and the execution of smart
contracts. Ethereum transactions form the backbone of its network, facilitating
peer-to-peer exchanges and interactions with complex decentralized
applications. Smart contracts extend Ethereum's capabilities by automating
processes and enabling trustless execution of agreements. Hence, understanding
how these smart contracts interact is important in order to facilitate various
performance optimizations, such as warming objects before they are being
accessed and enabling concurrent execution. Of particular interest to us are
the development of the calling graph, as well as the read sets and write sets
of invocations within the same block, and the properties of the associated
conflict graph that is derived from them. The latter is important for
understanding the parallelization potential of smart contracts on Ethereum. We
traced upwards of 2 million recent Ethereum blocks using call tracer and
prestate tracer, out of a total of 21.4 million blocks at the time of writing.
We report on the transactions per block distribution, the structure of call
trees in smart contract invocations, the ratio of value-transfer transactions
to smart contract invocations, as well as provide a comprehensive study of the
structure of blocks' conflict graphs. We find that conflict graphs
predominantly show a star like configuration, as well as other noteworthy
structural properties.

</details>


### [113] [Racing to Idle: Energy Efficiency of Matrix Multiplication on Heterogeneous CPU and GPU Architectures](https://arxiv.org/abs/2507.20063)
*Mufakir Qamar Ansari,Mudabir Qamar Ansari*

Main category: cs.DC

TL;DR: 多核和异构计算中，离散GPU在性能和能效上显著优于CPU和集成GPU，为高性能计算提供了明确的设计参考。


<details>
  <summary>Details</summary>
Motivation: 随着单核处理器的功率和热限制，能效成为高性能计算的首要设计约束，异构系统为性能与功耗的权衡提供了新路径。

Method: 通过测量三种计算架构（多核AMD CPU、离散NVIDIA GPU和集成AMD GPU）在4096x4096矩阵乘法上的性能与能耗，使用标准工具（Linux perf和nvidia-smi）。

Result: 离散GPU性能最佳（93.5倍于CPU），能效最高（仅消耗CPU的2%能量），能效提升50倍。

Conclusion: 研究验证了‘竞速闲置’原则，为能效优先的软件开发提供了量化指导。

Abstract: The paradigm shift towards multi-core and heterogeneous computing, driven by
the fundamental power and thermal limits of single-core processors, has
established energy efficiency as a first-class design constraint in
high-performance computing (HPC). Heterogeneous systems, integrating
traditional multi-core CPUs with specialized accelerators like discrete (dGPU)
and integrated (iGPU) graphics processing units, offer a compelling path to
navigating the trade-offs between performance and power. However, quantifying
these trade-offs on widely accessible hardware remains a critical area of
study. This paper presents a direct, empirical measurement of the performance
and energy-to-solution of a canonical HPC workload -- a 4096x4096 matrix-matrix
multiplication -- on three distinct compute architectures within a single
consumer-grade laptop: a multi-core AMD Ryzen 7 5800H CPU, a discrete NVIDIA
GeForce GTX 1650 GPU, and an integrated AMD Radeon Vega GPU. Using standard,
validated, and minimally intrusive tools such as Linux perf and nvidia-smi, we
find that the discrete GPU is not only the performance leader, achieving a
93.5x speedup over the CPU, but is also the most energy-efficient, consuming
only 2% of the energy used by the CPU, resulting in a 50-fold improvement in
energy efficiency. These findings provide a practical demonstration of the
"race to idle" principle and offer clear, quantitative guidance on
architectural choices for energy-aware software development.

</details>


### [114] [High-Performance Parallel Optimization of the Fish School Behaviour on the Setonix Platform Using OpenMP](https://arxiv.org/abs/2507.20173)
*Haitian Wang,Long Qin*

Main category: cs.DC

TL;DR: 本文研究Fish School Behaviour算法在Setonix超级计算平台上通过OpenMP进行高性能并行优化的方法，分析了多线程配置对性能的影响，并提出改进策略。


<details>
  <summary>Details</summary>
Motivation: 随着复杂大规模计算需求的增加，优化并行算法和计算结构变得尤为重要。Fish School Behaviour算法的迭代和计算密集型特性使其成为并行化的理想选择。

Method: 利用Setonix平台和OpenMP框架，研究多线程配置（如线程数量、调度策略和OpenMP结构），并通过实验测试不同配置对性能的影响。

Result: 实验结果不仅为FSB算法在Setonix上的并行优化提供了见解，还为其他使用OpenMP的并行计算研究提供了参考。

Conclusion: 未来可以进一步探索缓存行为和线程调度策略等微观和宏观因素，以进一步提升优化潜力。

Abstract: This paper presents an in-depth investigation into the high-performance
parallel optimization of the Fish School Behaviour (FSB) algorithm on the
Setonix supercomputing platform using the OpenMP framework. Given the
increasing demand for enhanced computational capabilities for complex,
large-scale calculations across diverse domains, there's an imperative need for
optimized parallel algorithms and computing structures. The FSB algorithm,
inspired by nature's social behavior patterns, provides an ideal platform for
parallelization due to its iterative and computationally intensive nature. This
study leverages the capabilities of the Setonix platform and the OpenMP
framework to analyze various aspects of multi-threading, such as thread counts,
scheduling strategies, and OpenMP constructs, aiming to discern patterns and
strategies that can elevate program performance. Experiments were designed to
rigorously test different configurations, and our results not only offer
insights for parallel optimization of FSB on Setonix but also provide valuable
references for other parallel computational research using OpenMP. Looking
forward, other factors, such as cache behavior and thread scheduling strategies
at micro and macro levels, hold potential for further exploration and
optimization.

</details>


### [115] [Silent Self-Stabilising Leader Election in Programmable Matter Systems with Holes](https://arxiv.org/abs/2507.20201)
*Jérémie Chalopin,Shantanu Das,Maria Kokkou*

Main category: cs.DC

TL;DR: 本文研究分布式计算中的自稳定领导者选举问题，特别是在可编程物质系统中，提出了首个在非公平调度器下保证唯一领导者的自稳定算法。


<details>
  <summary>Details</summary>
Motivation: 在分布式计算中，领导者选举是一个基础问题，尤其在可编程物质系统中，简单计算实体间的协调对完成复杂任务至关重要。虽然已有大量研究，但自稳定解决方案仍较有限。

Method: 我们提出了一种利用粒子移动的自稳定算法，假设粒子共享共同的方向感。该方法在非公平调度器下运行，适用于连通但不一定是单连通配置的三角网格。

Result: 结果表明，粒子移动结合网格操作可以克服Dolev等人提出的常数内存系统的经典不可能性结果。

Conclusion: 本文首次展示了在自稳定环境下，利用粒子移动能力实现唯一领导者选举的可行性，为可编程物质系统提供了新工具。

Abstract: Leader election is a fundamental problem in distributed computing,
particularly within programmable matter systems, where coordination among
simple computational entities is crucial for solving complex tasks. In these
systems, particles (i.e., constant memory computational entities) operate in a
regular triangular grid as described in the geometric Amoebot model. While
leader election has been extensively studied in non self-stabilising settings,
self-stabilising solutions remain more limited. In this work, we study the
problem of self-stabilising leader election in connected (but not necessarily
simply connected) configurations. We present the first self-stabilising
algorithm for programmable matter that guarantees the election of a unique
leader under an unfair scheduler, assuming particles share a common sense of
direction. Our approach leverages particle movement, a capability not
previously exploited in the self-stabilising context. We show that movement in
conjunction with particles operating in a grid can overcome classical
impossibility results for constant-memory systems established by Dolev et al.

</details>


### [116] [RIMMS: Runtime Integrated Memory Management System for Heterogeneous Computing](https://arxiv.org/abs/2507.20514)
*Serhan Gener,Aditya Ukarande,Shilpa Mysore Srinivasa Murthy,Sahil Hassan,Joshua Mack,Chaitali Chakrabarti,Umit Ogras,Ali Akoglu*

Main category: cs.DC

TL;DR: RIMMS是一个高效的运行时内存管理系统，提高异构系统的性能和编程效率。


<details>
  <summary>Details</summary>
Motivation: 异构系统的内存管理面临多样化的计算架构和动态任务映射的挑战，现有方法需要显式管理或假设静态映射，限制了便携性和可扩展性。

Method: RIMMS通过一个轻量级、硬件无关的内存抽象层，透明跟踪数据位置、管理一致性并支持高效内存分配，无需平台特定调整或代码修改。

Result: 在CPU+GPU和CPU+FPGA平台上，RIMMS分别带来最高2.43倍和1.82倍的性能提升，相比现有系统IRIS提升了3.08倍，且编程复杂度显著降低。

Conclusion: RIMMS在动态异构环境中提供了高性能和编程效率，同时保持了极低的管理开销。

Abstract: Efficient memory management in heterogeneous systems is increasingly
challenging due to diverse compute architectures (e.g., CPU, GPU, FPGA) and
dynamic task mappings not known at compile time. Existing approaches often
require programmers to manage data placement and transfers explicitly, or
assume static mappings that limit portability and scalability. This paper
introduces RIMMS (Runtime Integrated Memory Management System), a lightweight,
runtime-managed, hardware-agnostic memory abstraction layer that decouples
application development from low-level memory operations. RIMMS transparently
tracks data locations, manages consistency, and supports efficient memory
allocation across heterogeneous compute elements without requiring
platform-specific tuning or code modifications. We integrate RIMMS into a
baseline runtime and evaluate with complete radar signal processing
applications across CPU+GPU and CPU+FPGA platforms. RIMMS delivers up to 2.43X
speedup on GPU-based and 1.82X on FPGA-based systems over the baseline.
Compared to IRIS, a recent heterogeneous runtime system, RIMMS achieves up to
3.08X speedup and matches the performance of native CUDA implementations while
significantly reducing programming complexity. Despite operating at a higher
abstraction level, RIMMS incurs only 1-2 cycles of overhead per memory
management call, making it a low-cost solution. These results demonstrate
RIMMS's ability to deliver high performance and enhanced programmer
productivity in dynamic, real-world heterogeneous environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [117] [CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2507.19802)
*Ziyu Zhang,Yuanhao Wei,Joshua Engels,Julian Shun*

Main category: cs.DB

TL;DR: CleANN系统是一种动态图索引方法，通过多样搜索树链接和半懒内存清理等技术，显著提高并发查询和更新的效率，同时保持查询质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有动态图索引在更新时查询质量下降或维护成本高的问题，以支持向量数据库中的全动态操作。

Method: 采用三种技术：(1)工作负载感知的搜索树链接，应对数据分布变化；(2)查询自适应的即时邻居合并，处理删除节点；(3)半懒内存清理，减少陈旧信息。

Result: 在7个数据集上测试显示，CleANN在保持查询质量的同时，吞吐量提升了7-1200倍，支持高并发操作。

Conclusion: CleANN是首个在高并发全动态场景下保持高效和查询质量的ANNS索引系统。

Abstract: Approximate nearest neighbor search (ANNS) has become a quintessential
algorithmic problem for various other foundational data tasks for AI workloads.
Graph-based ANNS indexes have superb empirical trade-offs in indexing cost,
query efficiency, and query approximation quality. Most existing graph-based
indexes are designed for the static scenario, where there are no updates to the
data after the index is constructed. However, full dynamism (insertions,
deletions, and searches) is crucial to providing up-to-date responses in
applications using vector databases. It is desirable that the index efficiently
supports updates and search queries concurrently. Existing dynamic graph-based
indexes suffer from at least one of the following problems: (1) the query
quality degrades as updates happen; and (2) the graph structure updates used to
maintain the index quality upon updates are global and thus expensive. To solve
these problems, we propose the CleANN system which consists of three main
components: (1) workload-aware linking of diverse search tree descendants to
combat distribution shift; (2)query-adaptive on-the-fly neighborhood
consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory
cleaning to clean up stale information in the data structure and reduce the
work spent by the first two components. We evaluate CleANN on 7 diverse
datasets on fully dynamic workloads and find that CleANN has query quality at
least as good as if the index had been built statically using the corresponding
data. In the in-memory setting using 56 hyper-threads, with all types of
queries running concurrently, at the same recall level, CleANN achieves 7-1200x
throughput improvement on million-scale real-world datasets. To the best of our
knowledge, CleANN is the first concurrent ANNS index to achieve such efficiency
while maintaining quality under full dynamism.

</details>


### [118] [TIMEST: Temporal Information Motif Estimator Using Sampling Trees](https://arxiv.org/abs/2507.20441)
*Yunjie Pan,Omkar Bhalerao,C. Seshadhri,Nishil Talati*

Main category: cs.DB

TL;DR: TIMEST算法是一种高效、准确的估计方法，用于计算时态网络中任意大小的时态模体数量，相较于现有方法具有显著的速度和精度优势。


<details>
  <summary>Details</summary>
Motivation: 时态模体分析在现实网络中具有重要意义，但由于时态图中模体的组合爆炸问题，现有算法难以处理超过四个顶点的模体计数问题。

Method: TIMEST引入了一种时态生成树采样器，通过加权采样生成目标时态模体的子结构，并利用随机化估计技术获得准确的模体计数估计。

Result: 实验表明，TIMEST在CPU上的实现比现有GPU精确算法快28倍，比近似算法快6倍，且误差率低于5%。例如，计数金融欺诈模体仅需4分钟，误差0.6%。

Conclusion: TIMEST在理论和实验上均表现出色，为大规模时态模体计数提供了一种高效且可靠的解决方案。

Abstract: The mining of pattern subgraphs, known as motifs, is a core task in the field
of graph mining. Edges in real-world networks often have timestamps, so there
is a need for temporal motif mining. A temporal motif is a richer structure
that imposes timing constraints on the edges of the motif. Temporal motifs have
been used to analyze social networks, financial transactions, and biological
networks.
  Motif counting in temporal graphs is particularly challenging. A graph with
millions of edges can have trillions of temporal motifs, since the same edge
can occur with multiple timestamps. There is a combinatorial explosion of
possibilities, and state-of-the-art algorithms cannot manage motifs with more
than four vertices.
  In this work, we present TIMEST: a general, fast, and accurate estimation
algorithm to count temporal motifs of arbitrary sizes in temporal networks. Our
approach introduces a temporal spanning tree sampler that leverages weighted
sampling to generate substructures of target temporal motifs. This method
carefully takes a subset of temporal constraints of the motif that can be
jointly and efficiently sampled. TIMEST uses randomized estimation techniques
to obtain accurate estimates of motif counts.
  We give theoretical guarantees on the running time and approximation
guarantees of TIMEST. We perform an extensive experimental evaluation and show
that TIMEST is both faster and more accurate than previous algorithms. Our CPU
implementation exhibits an average speedup of 28x over state-of-the-art GPU
implementation of the exact algorithm, and 6x speedup over SOTA approximate
algorithms while consistently showcasing less than 5% error in most cases. For
example, TIMEST can count the number of instances of a financial fraud temporal
motif in four minutes with 0.6% error, while exact methods take more than two
days.

</details>


### [119] [A Functional Data Model and Query Language is All You Need](https://arxiv.org/abs/2507.20671)
*Jens Dittrich*

Main category: cs.DB

TL;DR: 提出了一种功能性数据模型(FDM)和功能性查询语言(FQL)，解决了SQL的多个问题，并与现有编程语言无缝集成。


<details>
  <summary>Details</summary>
Motivation: SQL存在NULL值、阻抗不匹配、SQL注入、更新查询能力不足等问题，亟需一种更现代的查询语言解决方案。

Method: 设计了FDM和FQL，这些工具的表达式能力更强，且能与编程语言自然融合，避免开发者学习新编程范式。

Result: FQL不仅解决了SQL的问题，还提供了更丰富的查询和优化机会，使得查询语言和编程语言趋于一致。

Conclusion: FDM和FQL为数据管理和查询提供了更现代、高效的解决方案，有望取代或补充SQL的不足。

Abstract: We propose the vision of a functional data model (FDM) and an associated
functional query language (FQL). Our proposal has far-reaching consequences: we
show a path to come up with a modern QL that solves (almost if not) all
problems of SQL (NULL-values, impedance mismatch, SQL injection, missing
querying capabilities for updates, etc.). FDM and FQL are much more expressive
than the relational model and SQL. In addition, in contrast to SQL, FQL
integrates smoothly into existing programming languages. In our approach both
QL and PL become the "same thing", thus opening up some interesting holistic
optimization opportunities between compilers and databases. In FQL, we also do
not need to force application developers to switch to unfamiliar programming
paradigms (like SQL or datalog): developers can stick with the abstractions
provided by their programming language.

</details>


### [120] [MVIAnalyzer: A Holistic Approach to Analyze Missing Value Imputation](https://arxiv.org/abs/2507.20815)
*Valerie Restat,Kai Tejkl,Uta Störl*

Main category: cs.DB

TL;DR: 本文介绍了一个名为MVIAnalyzer的通用框架，用于全面分析数据缺失值填补（MVI）方法，并通过可视化和模拟支持研究。


<details>
  <summary>Details</summary>
Motivation: 缺失值常限制数据分析或导致结果失真，目前缺乏适用于不同任务的通用MVI方法。

Method: 提出MVIAnalyzer框架，涵盖从数据预处理到机器学习应用的全过程，包括缺失值模拟和可视化分析。

Result: 应用MVIAnalyzer展示了不同MVI方法的优劣，并通过可视化辅助复杂分析。

Conclusion: MVIAnalyzer为MVI研究提供了实用工具，并揭示了不同方法的适用范围和局限性。

Abstract: Missing values often limit the usage of data analysis or cause falsification
of results. Therefore, methods of missing value imputation (MVI) are of great
significance. However, in general, there is no universal, fair MVI method for
different tasks. This work thus places MVI in the overall context of data
analysis. For this purpose, we present the MVIAnalyzer, a generic framework for
a holistic analysis of MVI. It considers the overall process up to the
application and analysis of machine learning methods. The associated software
is provided and can be used by other researchers for their own analyses. To
this end, it further includes a missing value simulation with consideration of
relevant parameters. The application of the MVIAnalyzer is demonstrated on data
with different characteristics. An evaluation of the results shows the
possibilities and limitations of different MVI methods. Since MVI is a very
complex topic with different influencing variables, this paper additionally
illustrates how the analysis can be supported by visualizations.

</details>


### [121] [Data Cleaning of Data Streams](https://arxiv.org/abs/2507.20839)
*Valerie Restat,Niklas Rodenhausen,Carina Antonin,Uta Störl*

Main category: cs.DB

TL;DR: 本文探讨了数据流清洗的特殊性及其适用性，并通过实验评估了理论考虑。


<details>
  <summary>Details</summary>
Motivation: 流数据来自多种场景（如传感器测量），但其清洗与静态数据不同，现有研究多集中于静态数据。

Method: 深入分析流数据清洗的适用性，并通过原型框架进行实验评估。

Result: 实验显示流数据清洗存在不一致性，并研究了流技术要求。

Conclusion: 流数据清洗需特殊处理，现有方法需调整以满足其动态性。

Abstract: Streaming data can arise from a variety of contexts. Important use cases are
continuous sensor measurements such as temperature, light or radiation values.
In the process, streaming data may also contain data errors that should be
cleaned before further use. Many studies from science and practice focus on
data cleaning in a static context. However, in terms of data cleaning,
streaming data has particularities that distinguish it from static data. In
this paper, we have therefore undertaken an intensive exploration of data
cleaning of data streams. We provide a detailed analysis of the applicability
of data cleaning to data streams. Our theoretical considerations are evaluated
in comprehensive experiments. Using a prototype framework, we show that
cleaning is not consistent when working with data streams. An additional
contribution is the investigation of requirements for streaming technologies in
context of data cleaning.

</details>


### [122] [Properties for Paths in Graph Databases](https://arxiv.org/abs/2507.19329)
*Fernando Orejas,Elvira Pino,Renzo Angles,E. Pasarella,Nikos Milonakis*

Main category: cs.DB

TL;DR: 本文提出了一种用于定义图数据库中路径属性的形式化方法，可用于限制导航查询的解数量。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过定义路径属性（如长度或累积成本）来改进图数据库查询的效率和精确性。

Method: 提出了一种新的形式化方法，通过操作语义定义路径属性，并证明了其逻辑语义的一致性和完备性。

Result: 路径属性的表达能力优于寄存器自动机，实验证明使用路径属性作为过滤器查询性能更优。

Conclusion: 路径属性形式化方法在理论和实际应用中均表现出色，为图数据库查询提供了新思路。

Abstract: This paper presents a formalism for defining properties of paths in graph
databases, which can be used to restrict the number of solutions to
navigational queries. In particular, our formalism allows us to define
quantitative properties such as length or accumulated cost, which can be used
as query filters. Furthermore, it enables the identification and removal of
paths that may be considered ill-formed.
  The new formalism is defined in terms of an operational semantics for the
query language that incorporates these new constructs, demonstrating its
soundness and completeness by proving its compatibility with a simple logical
semantics. We also analyze its expressive power, showing that path properties
are more expressive than register automata. Finally, after discussing some
complexity issues related to this new approach, we present an empirical
analysis carried out using our prototype implementation of the graph database
that serves as a running example throughout the paper. The results show that
queries using path properties as filters outperform standard queries that do
not use them.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [123] [MCP4EDA: LLM-Powered Model Context Protocol RTL-to-GDSII Automation with Backend Aware Synthesis Optimization](https://arxiv.org/abs/2507.19570)
*Yiting Wang,Wanghao Ye,Yexiao He,Yiran Chen,Gang Qu,Ang Li*

Main category: cs.AR

TL;DR: MCP4EDA是一种新型的模型上下文协议服务器，首次通过自然语言交互让大型语言模型（LLM）控制和优化完整的RTL-to-GDSII设计流程。


<details>
  <summary>Details</summary>
Motivation: 传统设计流程依赖线负载模型，无法准确反映后端实现的实际情况。MCP4EDA旨在利用真实的后端性能数据，通过LLM智能优化设计流程。

Method: 系统集成了多个EDA工具（如Yosys、OpenLane等），通过LLM分析与优化TCL脚本，形成闭环优化系统。

Result: 实验表明，MCP4EDA在时序收敛和面积优化上分别提升了15-30%和10-20%。

Conclusion: MCP4EDA首次实现了基于LLM的端到端开源EDA自动化系统，为设计流程带来了显著优化。

Abstract: This paper presents MCP4EDA, the first Model Context Protocol server that
enables Large Language Models (LLMs) to control and optimize the complete
open-source RTL-to-GDSII design flow through natural language interaction. The
system integrates Yosys synthesis, Icarus Verilog simulation, OpenLane
place-and-route, GTKWave analysis, and KLayout visualization into a unified
LLM-accessible interface, enabling designers to execute complex multi-tool EDA
workflows conversationally via AI assistants such as Claude Desktop and Cursor
IDE. The principal contribution is a backend-aware synthesis optimization
methodology wherein LLMs analyze actual post-layout timing, power, and area
metrics from OpenLane results to iteratively refine synthesis TCL scripts,
establishing a closed-loop optimization system that bridges the traditional gap
between synthesis estimates and physical implementation reality. In contrast to
conventional flows that rely on wire-load models, this methodology leverages
real backend performance data to guide synthesis parameter tuning, optimization
sequence selection, and constraint refinement, with the LLM functioning as an
intelligent design space exploration agent. Experimental evaluation on
representative digital designs demonstrates 15-30% improvements in timing
closure and 10-20% area reduction compared to default synthesis flows,
establishing MCP4EDA as the first practical LLM-controlled end-to-end
open-source EDA automation system. The code and demo are avaiable at:
http://www.agent4eda.com/

</details>


### [124] [ChipletPart: Scalable Cost-Aware Partitioning for 2.5D Systems](https://arxiv.org/abs/2507.19819)
*Alexander Graening,Puneet Gupta,Andrew B. Kahng,Bodhisatta Pramanik,Zhiang Wang*

Main category: cs.AR

TL;DR: 本文介绍了ChipletPart，一种基于成本驱动的2.5D系统分区器，旨在解决芯片系统中复杂的约束条件，并通过实验验证其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着芯片系统在工业中的广泛应用，如何高效分区成为关键问题，ChipletPart旨在满足这一需求。

Method: 结合遗传算法和模拟退火的芯片成本模型，实现技术分配与分区优化。

Result: 相比现有方法，ChipletPart成本降低最高58%，异构集成可节省43%成本。

Conclusion: ChipletPart和开源工具为芯片分区研究提供了新思路和资源。

Abstract: Industry adoption of chiplets has been increasing as a cost-effective option
for making larger high-performance systems. Consequently, partitioning large
systems into chiplets is increasingly important. In this work, we introduce
ChipletPart - a cost-driven 2.5D system partitioner that addresses the unique
constraints of chiplet systems, including complex objective functions, limited
reach of inter-chiplet I/O transceivers, and the assignment of heterogeneous
manufacturing technologies to different chiplets. ChipletPart integrates a
sophisticated chiplet cost model with its underlying genetic algorithm-based
technology assignment and partitioning methodology, along with a simulated
annealing-based chiplet floorplanner. Our results show that: (i) ChipletPart
reduces chiplet cost by up to 58% (20% geometric mean) compared to
state-of-the-art min-cut partitioners, which often yield floorplan-infeasible
solutions; (ii) ChipletPart generates partitions with up to 47% (6% geometric
mean) lower cost as compared to the prior work Floorplet; and (iii) for the
testcases we study, heterogeneous integration reduces cost by up to 43% (15%
geometric mean) compared to homogeneous implementations. We also present case
studies that show how changes in packaging or inter-chiplet signaling
technologies can affect partitioning solutions. Finally, we make ChipletPart,
the underlying chiplet cost model, and a chiplet testcase generator available
as open-source tools for the community.

</details>


### [125] [AxOSyn: An Open-source Framework for Synthesizing Novel Approximate Arithmetic Operators](https://arxiv.org/abs/2507.20007)
*Siva Satyendra Sahoo,Salim Ullah,Akash Kumar*

Main category: cs.AR

TL;DR: AxOSyn是一个开源框架，支持在不同抽象级别上选择和合成近似算术运算符，以实现资源受限嵌入式系统的能效优化。


<details>
  <summary>Details</summary>
Motivation: 边缘AI部署日益复杂，资源受限的嵌入式系统需要能效高的解决方案。近似计算通过允许可控的计算不准确性，成为提升能效的有前景方法。

Method: AxOSyn框架支持选择和合成两种方法，可在不同抽象级别上探索近似算术运算符的设计空间，并集成自定义评估方法。

Result: AxOSyn为边缘AI系统提供了灵活的能效优化工具，克服了现有DSE框架的限制。

Conclusion: AxOSyn通过灵活的设计空间探索和自定义评估方法，为近似计算的能效优化提供了有效解决方案。

Abstract: Edge AI deployments are becoming increasingly complex, necessitating
energy-efficient solutions for resource-constrained embedded systems.
Approximate computing, which allows for controlled inaccuracies in
computations, is emerging as a promising approach for improving power and
energy efficiency. Among the key techniques in approximate computing are
approximate arithmetic operators (AxOs), which enable application-specific
optimizations beyond traditional computer arithmetic hardware reduction-based
methods, such as quantization and precision scaling. Existing design space
exploration (DSE) frameworks for approximate computing limit themselves to
selection-based approaches or custom synthesis at fixed abstraction levels,
which restricts the flexibility required for finding application-specific
optimal solutions. Further, the tools available for the DSE of AxOs are quite
limited in terms of exploring different approximation models and extending the
analysis to different granularities. To this end, we propose AxOSyn, an
open-source framework for the DSE of AxOs that supports both selection and
synthesis approaches at various abstraction levels. AxOSyn allows researchers
to integrate custom methods for evaluating approximations and facilitates DSE
at both the operator-level and application-specific. Our framework provides an
effective methodology for achieving energy-efficient, approximate operators.

</details>


### [126] [RoCE BALBOA: Service-enhanced Data Center RDMA for SmartNICs](https://arxiv.org/abs/2507.20412)
*Maximilian Jakob Heer,Benjamin Ramhorst,Yu Zhu,Luhao Liu,Zhiyi Hu,Jonas Dann,Gustavo Alonso*

Main category: cs.AR

TL;DR: 论文介绍了RoCE BALBOA，一种开源的、RoCE v2兼容的RDMA协议栈，适用于构建高性能智能网卡和加速器，并展示了其在实际部署中的潜力。


<details>
  <summary>Details</summary>
Motivation: 数据中心中的数据密集型应用（如机器学习）使网络成为瓶颈，因此需要开发更高效的网络协议和基础设施。

Method: 提出了RoCE BALBOA，一种可扩展至数百对队列、支持100G的RDMA协议栈，具有高度可定制性，并通过FPGA在集群中部署。

Result: BALBOA的延迟和性能与商用网卡相当，并成功展示了在协议增强（如加密、ML深度包检测）和线速计算卸载（如推荐系统数据预处理）中的应用潜力。

Conclusion: RoCE BALBOA为智能网卡和加速器的开发提供了灵活的基础，能够高效处理网络数据流。

Abstract: Data-intensive applications in data centers, especially machine learning
(ML), have made the network a bottleneck, which in turn has motivated the
development of more efficient network protocols and infrastructure. For
instance, remote direct memory access (RDMA) has become the standard protocol
for data transport in the cloud as it minimizes data copies and reduces
CPU-utilization via host-bypassing. Similarly, an increasing amount of network
functions and infrastructure have moved to accelerators, SmartNICs, and
in-network computing to bypass the CPU. In this paper we explore the
implementation and deployment of RoCE BALBOA, an open-source, RoCE
v2-compatible, scalable up to hundreds of queue-pairs, and 100G-capable
RDMA-stack that can be used as the basis for building accelerators and
smartNICs. RoCE BALBOA is customizable, opening up a design space and offering
a degree of adaptability not available in commercial products. We have deployed
BALBOA in a cluster using FPGAs and show that it has latency and performance
characteristics comparable to commercial NICs. We demonstrate its potential by
exploring two classes of use cases. One involves enhancements to the protocol
for infrastructure purposes (encryption, deep packet inspection using ML). The
other showcases the ability to perform line-rate compute offloads with deep
pipelines by implementing commercial data preprocessing pipelines for
recommender systems that process the data as it arrives from the network before
transferring it directly to the GPU. These examples demonstrate how BALBOA
enables the exploration and development of SmartNICs and accelerators operating
on network data streams.

</details>


### [127] [Demystifying the 7-D Convolution Loop Nest for Data and Instruction Streaming in Reconfigurable AI Accelerators](https://arxiv.org/abs/2507.20420)
*Md Rownak Hossain Chowdhury,Mostafizur Rahman*

Main category: cs.AR

TL;DR: 论文提出了一种新框架，通过将卷积循环嵌套重新解释为硬件为中心的数据和指令流问题，减少了传统方法中的数据移动和控制开销。


<details>
  <summary>Details</summary>
Motivation: 卷积是AI加速中最计算密集的操作，现有方法（如CGRA和FPGA）依赖循环展开或矩阵变换，导致显著的开销。

Method: 通过将7维卷积循环嵌套结构化为空间和时间映射，基于硬件参数（计算单元分布、互连拓扑等）实现轻量级灵活部署。

Result: 在MAVeC加速器上验证，PE利用率超90%，吞吐量达1.56 TFLOPs/sec，VGG-16推理性能显著提升。

Conclusion: 新框架有效减少控制开销，提升数据局部性，适用于大规模卷积执行，且无需依赖传统变换方法。

Abstract: Convolution remains the most compute-intensive operation in AI acceleration,
often constituting over 80-90% of the workload. Existing approaches in spatial
architectures such as coarse-grained reconfigurable arrays (CGRAs) and
field-programmable gate arrays (FPGAs) frequently rely on loop unrolling or
GEMM-based matrix transformations, introducing significant overhead in both
data movement and instruction control. This paper presents a new framework
designed to systematically demystify the 7-dimensional convolution loop nest by
reinterpreting it as a hardware-centric data and instruction streaming problem.
Instead of treating the loop nest as a fixed computational construct, our
approach exposes its structure as a set of spatial and temporal mappings
governed by hardware parameters such as compute element distribution,
interconnect topology, and reconfigurability. This abstraction supports
lightweight, flexible deployment of convolution without reliance on heavyweight
transformations or reordering schemes. We demonstrate the application of our
approach on the MAVeC accelerator. We detail the implementation of convolution
operations in MAVeC and extend the framework to support full model execution on
VGG-16. Our profiling reveals high PE utilization (over 90%), significant fold
reuse, and scalable throughput up to 1.56 TFLOPs/sec and 12.7 KIPS for
end-to-end VGG-16 inference. These results validate the efficacy of our
approach in minimizing control overhead, improving data locality, and enabling
efficient large-scale convolution execution without reliance on conventional
transformation-based methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [128] [InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update](https://arxiv.org/abs/2309.11071)
*Dan Wu,Zhaoying Li,Tulika Mitra*

Main category: cs.LG

TL;DR: InkStream是一种针对动态图设计的实时推理方法，通过仅计算受影响部分邻域来减少计算和内存访问，显著加速GNN推理。


<details>
  <summary>Details</summary>
Motivation: 传统GNN推理方法不适用于动态图，动态图的不断更新对GPU加速提出了挑战。

Method: 基于两个关键洞察设计InkStream：1) 在k-hop邻域中，许多节点不受修改边的影响；2) 节点嵌入可以增量更新。采用事件驱动系统，控制层间效应传播和层内节点嵌入更新。

Result: 实验显示，InkStream在CPU和GPU集群上加速2.4-427倍，且输出与传统方法一致。

Conclusion: InkStream高效支持动态图推理，易用且扩展性强，适用于多种GNN模型。

Abstract: Classic Graph Neural Network (GNN) inference approaches, designed for static
graphs, are ill-suited for streaming graphs that evolve with time. The dynamism
intrinsic to streaming graphs necessitates constant updates, posing unique
challenges to acceleration on GPU. We address these challenges based on two key
insights: (1) Inside the $k$-hop neighborhood, a significant fraction of the
nodes is not impacted by the modified edges when the model uses min or max as
aggregation function; (2) When the model weights remain static while the graph
structure changes, node embeddings can incrementally evolve over time by
computing only the impacted part of the neighborhood. With these insights, we
propose a novel method, InkStream, designed for real-time inference with
minimal memory access and computation, while ensuring an identical output to
conventional methods. InkStream operates on the principle of propagating and
fetching data only when necessary. It uses an event-based system to control
inter-layer effect propagation and intra-layer incremental updates of node
embedding. InkStream is highly extensible and easily configurable by allowing
users to create and process customized events. We showcase that less than 10
lines of additional user code are needed to support popular GNN models such as
GCN, GraphSAGE, and GIN. Our experiments with three GNN models on four large
graphs demonstrate that InkStream accelerates by 2.5-427$\times$ on a CPU
cluster and 2.4-343$\times$ on two different GPU clusters while producing
identical outputs as GNN model inference on the latest graph snapshot.

</details>


### [129] [Aggregation-aware MLP: An Unsupervised Approach for Graph Message-passing](https://arxiv.org/abs/2507.20127)
*Xuanting Xie,Bingheng Li,Erlin Pan,Zhao Kang,Wenyu Chen*

Main category: cs.LG

TL;DR: 提出了基于MLP的新型无监督框架AMLP，通过自适应聚合机制解决GNN传统固定聚合函数的问题，提升异构图学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN的固定聚合函数在异构图场景表现不佳，且依赖标注数据，需改进。

Method: AMLP通过图重构和高阶分组效应，结合单层网络编码异质性，实现动态聚合。

Result: 在节点聚类和分类任务中表现优越，验证了框架的通用性。

Conclusion: AMLP为多样化图学习提供了轻量且高效的解决方案。

Abstract: Graph Neural Networks (GNNs) have become a dominant approach to learning
graph representations, primarily because of their message-passing mechanisms.
However, GNNs typically adopt a fixed aggregator function such as Mean, Max, or
Sum without principled reasoning behind the selection. This rigidity,
especially in the presence of heterophily, often leads to poor, problem
dependent performance. Although some attempts address this by designing more
sophisticated aggregation functions, these methods tend to rely heavily on
labeled data, which is often scarce in real-world tasks. In this work, we
propose a novel unsupervised framework, "Aggregation-aware Multilayer
Perceptron" (AMLP), which shifts the paradigm from directly crafting
aggregation functions to making MLP adaptive to aggregation. Our lightweight
approach consists of two key steps: First, we utilize a graph reconstruction
method that facilitates high-order grouping effects, and second, we employ a
single-layer network to encode varying degrees of heterophily, thereby
improving the capacity and applicability of the model. Extensive experiments on
node clustering and classification demonstrate the superior performance of
AMLP, highlighting its potential for diverse graph learning scenarios.

</details>


### [130] [MH-GIN: Multi-scale Heterogeneous Graph-based Imputation Network for AIS Data (Extended Version)](https://arxiv.org/abs/2507.20362)
*Hengyu Liu,Tianyi Li,Yuqiang He,Kristian Torp,Yushuai Li,Christian S. Jensen*

Main category: cs.LG

TL;DR: 论文提出了一种名为MH-GIN的多尺度异构图填充网络，用于解决自动识别系统中位置跟踪数据因多尺度依赖关系导致的缺失值问题，相比现有方法平均减少57%的填充误差。


<details>
  <summary>Details</summary>
Motivation: 自动识别系统中的位置跟踪数据因多尺度依赖关系和异构属性更新的不同速率导致缺失值问题，现有方法难以准确建模此类依赖关系。

Method: MH-GIN通过提取多尺度时间特征并构建多尺度异构图，显式建模异构属性间的依赖关系，通过图传播实现更准确的缺失值填充。

Result: 在真实数据集上的实验表明，MH-GIN平均减少57%的填充误差，同时保持计算效率。

Conclusion: MH-GIN通过多尺度异构图建模有效提升了缺失值的填充精度，为海事安全和监控应用提供了更可靠的数据支持。

Abstract: Location-tracking data from the Automatic Identification System, much of
which is publicly available, plays a key role in a range of maritime safety and
monitoring applications. However, the data suffers from missing values that
hamper downstream applications. Imputing the missing values is challenging
because the values of different heterogeneous attributes are updated at diverse
rates, resulting in the occurrence of multi-scale dependencies among
attributes. Existing imputation methods that assume similar update rates across
attributes are unable to capture and exploit such dependencies, limiting their
imputation accuracy. We propose MH-GIN, a Multi-scale Heterogeneous Graph-based
Imputation Network that aims improve imputation accuracy by capturing
multi-scale dependencies. Specifically, MH-GIN first extracts multi-scale
temporal features for each attribute while preserving their intrinsic
heterogeneous characteristics. Then, it constructs a multi-scale heterogeneous
graph to explicitly model dependencies between heterogeneous attributes to
enable more accurate imputation of missing values through graph propagation.
Experimental results on two real-world datasets find that MH-GIN is capable of
an average 57% reduction in imputation errors compared to state-of-the-art
methods, while maintaining computational efficiency. The source code and
implementation details of MH-GIN are publicly available
https://github.com/hyLiu1994/MH-GIN.

</details>


### [131] [Efficient and Scalable Agentic AI with Heterogeneous Systems](https://arxiv.org/abs/2507.19635)
*Zain Asgar,Michelle Nguyen,Sachin Katti*

Main category: cs.LG

TL;DR: 提出了一种动态编排AI代理工作负载的系统设计，优化异构计算基础设施的使用，显著降低总拥有成本（TCO）。


<details>
  <summary>Details</summary>
Motivation: AI代理工作负载动态且结构复杂，需要高效、可扩展的基础设施支持其部署和运行。

Method: 设计了包括规划框架、MLIR表示与编译系统及动态编排系统在内的解决方案，以优化异构计算资源的使用。

Result: 初步结果显示异构基础设施能显著降低TCO，某些情况下旧硬件与新加速器组合可媲美最新同构GPU设计。

Conclusion: 异构计算基础设施可有效支持AI代理工作负载，同时减少成本，延长现有硬件寿命。

Abstract: AI agents are emerging as a dominant workload in a wide range of
applications, promising to be the vehicle that delivers the promised benefits
of AI to enterprises and consumers. Unlike conventional software or static
inference, agentic workloads are dynamic and structurally complex. Often these
agents are directed graphs of compute and IO operations that span multi-modal
data input and conversion), data processing and context gathering (e.g vector
DB lookups), multiple LLM inferences, tool calls, etc. To scale AI agent usage,
we need efficient and scalable deployment and agent-serving infrastructure.
  To tackle this challenge, in this paper, we present a system design for
dynamic orchestration of AI agent workloads on heterogeneous compute
infrastructure spanning CPUs and accelerators, both from different vendors and
across different performance tiers within a single vendor. The system delivers
several building blocks: a framework for planning and optimizing agentic AI
execution graphs using cost models that account for compute, memory, and
bandwidth constraints of different HW; a MLIR based representation and
compilation system that can decompose AI agent execution graphs into granular
operators and generate code for different HW options; and a dynamic
orchestration system that can place the granular components across a
heterogeneous compute infrastructure and stitch them together while meeting an
end-to-end SLA. Our design performs a systems level TCO optimization and
preliminary results show that leveraging a heterogeneous infrastructure can
deliver significant TCO benefits. A preliminary surprising finding is that for
some workloads a heterogeneous combination of older generation GPUs with newer
accelerators can deliver similar TCO as the latest generation homogenous GPU
infrastructure design, potentially extending the life of deployed
infrastructure.

</details>


### [132] [$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning](https://arxiv.org/abs/2507.20051)
*Weicong Chen,Vikash Singh,Zahra Rahmani,Debargha Ganguly,Mohsen Hariri,Vipin Chaudhary*

Main category: cs.LG

TL;DR: 论文提出了$K^4$框架，一种快速、无监督且不依赖解析的日志异常检测方法，通过高效k-NN统计将日志嵌入转化为四维描述符，实现了高性能在线检测。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法存在速度慢、依赖错误解析及评估协议不真实的问题，$K^4$旨在解决这些问题。

Method: $K^4$利用k-NN统计将日志嵌入转换为四维描述符（精度、召回、密度、覆盖率），支持轻量级检测器无需重新训练即可准确评分。

Result: $K^4$在更真实的在线评估协议下表现优异（AUROC:0.995-0.999），显著优于基线方法，且速度快（训练<4秒，推理低至4μs）。

Conclusion: $K^4$在日志异常检测领域实现了新的最优性能，高效且实用。

Abstract: Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on
error-prone parsing, and use unrealistic evaluation protocols. We introduce
$K^4$, an unsupervised and parser-independent framework for high-performance
online detection. $K^4$ transforms arbitrary log embeddings into compact
four-dimensional descriptors (Precision, Recall, Density, Coverage) using
efficient k-nearest neighbor (k-NN) statistics. These descriptors enable
lightweight detectors to accurately score anomalies without retraining. Using a
more realistic online evaluation protocol, $K^4$ sets a new state-of-the-art
(AUROC: 0.995-0.999), outperforming baselines by large margins while being
orders of magnitude faster, with training under 4 seconds and inference as low
as 4 $\mu$s.

</details>


### [133] [Communication-Efficient Distributed Training for Collaborative Flat Optima Recovery in Deep Learning](https://arxiv.org/abs/2507.20424)
*Tolga Dimlioglu,Anna Choromanska*

Main category: cs.LG

TL;DR: 本文提出了一种名为DPPF的分布式训练算法，通过引入一种轻量级正则化器（Inverse Mean Valley）来优化通信效率与模型性能的权衡，帮助模型寻找更平坦的极小值，从而提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究集中式分布式数据并行训练，旨在改进局部梯度方法在通信效率与模型性能之间的权衡。

Method: 提出一种简单有效的锐度度量（Inverse Mean Valley），并将其作为正则化器引入分布式训练目标，形成DPPF算法。该算法通过推拉力机制鼓励模型寻找平坦极小值。

Result: 实验表明，DPPF在减少通信开销的同时，优于其他通信高效方法，且泛化性能优于局部梯度方法和同步梯度平均。理论分析确认了其稳定性和收敛性。

Conclusion: DPPF通过推拉力机制成功帮助模型定位平坦极小值，并在理论和实验上验证了其有效性和优势。

Abstract: We study centralized distributed data parallel training of deep neural
networks (DNNs), aiming to improve the trade-off between communication
efficiency and model performance of the local gradient methods. To this end, we
revisit the flat-minima hypothesis, which suggests that models with better
generalization tend to lie in flatter regions of the loss landscape. We
introduce a simple, yet effective, sharpness measure, Inverse Mean Valley, and
demonstrate its strong correlation with the generalization gap of DNNs. We
incorporate an efficient relaxation of this measure into the distributed
training objective as a lightweight regularizer that encourages workers to
collaboratively seek wide minima. The regularizer exerts a pushing force that
counteracts the consensus step pulling the workers together, giving rise to the
Distributed Pull-Push Force (DPPF) algorithm. Empirically, we show that DPPF
outperforms other communication-efficient approaches and achieves better
generalization performance than local gradient methods and synchronous gradient
averaging, while significantly reducing communication overhead. In addition,
our loss landscape visualizations confirm the ability of DPPF to locate flatter
minima. On the theoretical side, we show that DPPF guides workers to span flat
valleys, with the final valley width governed by the interplay between push and
pull strengths, and that its pull-push dynamics is self-stabilizing. We further
provide generalization guarantees linked to the valley width and prove
convergence in the non-convex setting.

</details>


### [134] [Inducing Causal World Models in LLMs for Zero-Shot Physical Reasoning](https://arxiv.org/abs/2507.19855)
*Aditya Sharma,Linh Nguyen,Ananya Gupta,Chengyu Wang,Chiamaka Adebayo,Jakub Kowalski*

Main category: cs.LG

TL;DR: 论文提出Causal World Model Induction（CWMI）框架，通过嵌入因果物理模型提升LLMs在物理推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs缺乏对物理动态的直观理解，限制了其在需要因果推理的实际场景中的应用。

Method: 引入Causal Physics Module（CPM）和Causal Intervention Loss训练目标，从多模态数据中学习因果关系。

Result: CWMI在零样本物理推理任务中显著优于现有LLMs，如PIQA和PhysiCa-Bench数据集。

Conclusion: 诱导因果世界模型是开发更可靠、通用AI系统的关键步骤。

Abstract: Large Language Models (LLMs), despite their advanced linguistic capabilities,
fundamentally lack an intuitive understanding of physical dynamics, which
limits their effectiveness in real-world scenarios that require causal
reasoning. In this paper, we introduce Causal World Model Induction (CWMI), a
novel framework designed to embed an explicit model of causal physics within an
LLM. Our approach incorporates a dedicated Causal Physics Module (CPM) and a
new training objective called Causal Intervention Loss, encouraging the model
to learn cause-and-effect relationships from multimodal data. By training the
model to predict the outcomes of hypothetical interventions instead of merely
capturing statistical correlations, CWMI develops a robust internal
representation of physical laws. Experimental results show that CWMI
significantly outperforms state-of-the-art LLMs on zero-shot physical reasoning
tasks, including the PIQA benchmark and our newly proposed PhysiCa-Bench
dataset. These findings demonstrate that inducing a causal world model is a
critical step toward more reliable and generalizable AI systems.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [135] [An Overview of Automated Vehicle Longitudinal Platoon Formation Strategies](https://arxiv.org/abs/2403.05415)
*M Sabbir Salek,Mugdha Basu Thakur,Pardha Sai Krishna Ala,Mashrur Chowdhury,Matthias Schmid,Pamela Murray-Tuite,Sakib Mahmud Khan,Venkat Krovi*

Main category: eess.SY

TL;DR: 论文综述了自动驾驶车辆（AV）纵向车队形成的最新知识，分析了五个关键组件，并探讨了现有策略的优劣势及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车队的潜力在于提升交通系统的安全性、操作性和能源效率，但其策略的实践仍待验证，需总结现状并展望未来。

Method: 采用五组件框架（车辆模型、信息接收过程、信息流拓扑、间距策略和控制器）分析现有AV车队策略。

Result: 现有的车队策略在理论和实践中均取得进展，但其组件各有优劣，需进一步优化。

Conclusion: 论文指出未来研究应针对现有策略的局限性，探索更高效、可靠的自动驾驶车队技术。

Abstract: Automated vehicle (AV) platooning has the potential to improve the safety,
operational, and energy efficiency of surface transportation systems by
limiting or eliminating human involvement in the driving tasks. The theoretical
validity of the AV platooning strategies has been established and practical
applications are being tested under real-world conditions. The emergence of
sensors, communication, and control strategies has resulted in rapid and
constant evolution of AV platooning strategies. In this paper, we review the
state-of-the-art knowledge in AV longitudinal platoon formation using a
five-component platooning framework, which includes vehicle model,
information-receiving process, information flow topology, spacing policy, and
controller and discuss the advantages and limitations of the components. Based
on the discussion about existing strategies and associated limitations,
potential future research directions are presented.

</details>


### [136] [Wardropian Cycles make traffic assignment both optimal and fair by eliminating price-of-anarchy with Cyclical User Equilibrium for compliant connected autonomous vehicles](https://arxiv.org/abs/2507.19675)
*Michał Hoffmann,Michał Bujak,Grzegorz Jamróz,Rafał Kucharski*

Main category: eess.SY

TL;DR: 提出了一种称为Wardropian Cycles的新概念，通过周期性的路径分配实现系统最优与公平性的平衡，并提出了计算方法和贪婪启发式算法。实际模拟显示该方法显著减少了不公平性。


<details>
  <summary>Details</summary>
Motivation: 解决集中式路由中系统最优分配可能导致部分驾驶员受益不均的问题，以提高社会接受度。

Method: 提出Wardropian Cycles概念，通过周期性路径分配实现公平与最优；开发精确算法和贪婪启发式优化周期长度与用户体验。

Result: 模拟结果显示，在巴塞罗那、柏林等城市，该方法显著减少了交通不公平性，提升了交通效率。

Conclusion: 周期性用户均衡是一种有效且稳定的方法，能够在实现交通最优的同时提升公平性。

Abstract: Connected and Autonomous Vehicles (CAVs) open the possibility for centralised
routing with full compliance, making System Optimal traffic assignment
attainable. However, as System Optimum makes some drivers better off than
others, voluntary acceptance seems dubious. To overcome this issue, we propose
a new concept of Wardropian cycles, which, in contrast to previous utopian
visions, makes the assignment fair on top of being optimal, which amounts to
satisfaction of both Wardrop's principles. Such cycles, represented as
sequences of permutations to the daily assignment matrices, always exist and
equalise, after a limited number of days, average travel times among travellers
(like in User Equilibrium) while preserving everyday optimality of path flows
(like in System Optimum). We propose exact methods to compute such cycles and
reduce their length and within-cycle inconvenience to the users. As
identification of optimal cycles turns out to be NP-hard in many aspects, we
introduce a greedy heuristic efficiently approximating the optimal solution.
Finally, we introduce and discuss a new paradigm of Cyclical User Equilibrium,
which ensures stability of optimal Wardropian Cycles under unilateral
deviations.
  We complement our theoretical study with large-scale simulations. In
Barcelona, 670 vehicle-hours of Price-of-Anarchy are eliminated using cycles
with a median length of 11 days-though 5% of cycles exceed 90 days. However, in
Berlin, just five days of applying the greedy assignment rule significantly
reduces initial inequity. In Barcelona, Anaheim, and Sioux Falls, less than 7%
of the initial inequity remains after 10 days, demonstrating the effectiveness
of this approach in improving traffic performance with more ubiquitous social
acceptability.

</details>


### [137] [ACCESS-AV: Adaptive Communication-Computation Codesign for Sustainable Autonomous Vehicle Localization in Smart Factories](https://arxiv.org/abs/2507.20399)
*Rajat Bhattacharjya,Arnab Sarkar,Ish Kool,Sabur Baidya,Nikil Dutt*

Main category: eess.SY

TL;DR: ACCESS-AV是一种基于5G网络的自适应本地化框架，通过利用现有5G基础设施减少能耗和成本，优化了智能工厂中的自动驾驶车辆定位问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在5G智能工厂中的定位模块计算密集型高，存在优化需求，需要减少专用路边单元或额外传感器的需求。

Method: 通过周期性广播的5G同步信号块（SSBs）进行定位，采用基于MUSIC算法的到达角（AoA）估计方法，并通过自适应通信计算策略动态平衡能耗与精度。

Result: 相较于非自适应系统，平均能耗降低43.09%，定位精度保持在30厘米以下，同时显著降低了基础设施和运营成本。

Conclusion: ACCESS-AV证明了在可持续智能工厂环境中的可行性，通过高效利用5G基础设施实现了能耗和成本的优化。

Abstract: Autonomous Delivery Vehicles (ADVs) are increasingly used for transporting
goods in 5G network-enabled smart factories, with the compute-intensive
localization module presenting a significant opportunity for optimization. We
propose ACCESS-AV, an energy-efficient Vehicle-to-Infrastructure (V2I)
localization framework that leverages existing 5G infrastructure in smart
factory environments. By opportunistically accessing the periodically broadcast
5G Synchronization Signal Blocks (SSBs) for localization, ACCESS-AV obviates
the need for dedicated Roadside Units (RSUs) or additional onboard sensors to
achieve energy efficiency as well as cost reduction. We implement an
Angle-of-Arrival (AoA)-based estimation method using the Multiple Signal
Classification (MUSIC) algorithm, optimized for resource-constrained ADV
platforms through an adaptive communication-computation strategy that
dynamically balances energy consumption with localization accuracy based on
environmental conditions such as Signal-to-Noise Ratio (SNR) and vehicle
velocity. Experimental results demonstrate that ACCESS-AV achieves an average
energy reduction of 43.09% compared to non-adaptive systems employing AoA
algorithms such as vanilla MUSIC, ESPRIT, and Root-MUSIC. It maintains sub-30
cm localization accuracy while also delivering substantial reductions in
infrastructure and operational costs, establishing its viability for
sustainable smart factory environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [138] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: 本文探讨了本体结构知识图谱在预测未来事件中的关键作用，提出了一种结合BFO和CCO的方法，并通过渔船运动数据展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 通过本体结构的知识图谱提升对未来事件的预测能力，并改进现有的概率模型。

Method: 利用BFO和CCO构建知识图谱，结合马尔可夫链模型进行预测，并提出“时空实例”概念完善语义结构。

Result: 展示了渔船运动数据的预测能力，并提出了新的概率模型，与知识图谱无缝集成。

Conclusion: 本体结构知识图谱和马尔可夫链的结合为预测分析提供了有效方法，并改进了概率模型的表达。

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [139] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*Müge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: 研究了稳定室友问题中无解时的‘足够好’匹配方法，结合习惯和社交网络生成个性化解决方案。


<details>
  <summary>Details</summary>
Motivation: 现实应用中稳定室友问题可能无解，需找‘足够好’的匹配方案。

Method: 结合代理的习惯、偏好及朋友网络，提出个性化解决方案生成方法。

Result: 通过实例和实证评估验证了方法的有效性。

Conclusion: 提出的方法能生成个性化解决方案，适用于稳定室友问题无解的情况。

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


### [140] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA是一个开源平台，旨在通过模块化、可扩展的设计促进多学科协作，加速AI在临床中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决AI技术在实际医疗应用中的落地难题，促进跨领域协作。

Method: 基于Kubernetes构建，提供数据管理、模型开发、标注、部署和临床反馈一体化工具。

Result: 已在学术和临床环境中部署，支持医学成像AI的实际应用。

Conclusion: MAIA通过协作和互操作性，推动AI研究向临床解决方案的高效转化。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [141] [Smart Expansion Techniques for ASP-based Interactive Configuration](https://arxiv.org/abs/2507.21027)
*Lucia Balážová,Richard Comploi-Taupe,Susana Hahn,Nicolas Rühling,Gottfried Schenner*

Main category: cs.AI

TL;DR: 本文提出了一种基于ASP的交互式配置求解器，通过四种智能扩展函数优化部分配置的自动完成性能，减少了昂贵的不可满足性检查并缩小了搜索空间。


<details>
  <summary>Details</summary>
Motivation: 尽管ASP在产品配置中应用成功，但交互式系统仍需解决如何有效引导用户完成配置的挑战。

Method: 改进经典增量方法，通过四种智能扩展函数利用谨慎和勇敢的推理结果，优化部分配置的自动完成性能。

Result: 减少了不可满足性检查的次数并缩小了搜索空间，提升了求解性能。

Conclusion: 该方法为大规模工业配置问题提供了高效的ASP求解方案，并通过API支持直观的用户界面。

Abstract: Product configuration is a successful application of Answer Set Programming
(ASP). However, challenges are still open for interactive systems to
effectively guide users through the configuration process. The aim of our work
is to provide an ASP-based solver for interactive configuration that can deal
with large-scale industrial configuration problems and that supports intuitive
user interfaces via an API. In this paper, we focus on improving the
performance of automatically completing a partial configuration. Our main
contribution enhances the classical incremental approach for multi-shot solving
by four different smart expansion functions. The core idea is to determine and
add specific objects or associations to the partial configuration by exploiting
cautious and brave consequences before checking for the existence of a complete
configuration with the current objects in each iteration. This approach limits
the number of costly unsatisfiability checks and reduces the search space,
thereby improving solving performance. In addition, we present a user interface
that uses our API and is implemented in ASP.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [142] [Parallel Hierarchical Agglomerative Clustering in Low Dimensions](https://arxiv.org/abs/2507.20047)
*MohammadHossein Bateni,Laxman Dhulipala,Willem Fletcher,Kishen N Gowda,D Ellis Hershkowitz,Rajesh Jayaram,Jakub Łącki*

Main category: cs.DS

TL;DR: 该论文针对非单调链接函数（如质心和Ward距离）的层次聚类问题，提出了在低维度下的高效NC算法，并证明了其在任意维度下的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决现有高效并行算法无法处理非单调链接函数的问题，尤其是质心和Ward距离等重要但非单调的链接函数。

Method: 论文通过结构分析，证明了在低维度下，对于一类非单调链接函数，可以构建高度为对数多边形的层次结构，从而设计出高效的NC算法。

Result: 结果显示，该算法在低维度下适用于非单调链接函数，而在任意维度下，此类问题的NC算法可能存在计算复杂性障碍。

Conclusion: 结论指出，尽管在低维度下取得了进展，但在高维度下，层次聚类问题的并行解决方案仍具有挑战性。

Abstract: Hierarchical Agglomerative Clustering (HAC) is an extensively studied and
widely used method for hierarchical clustering in $\mathbb{R}^k$ based on
repeatedly merging the closest pair of clusters according to an input linkage
function $d$. Highly parallel (i.e., NC) algorithms are known for
$(1+\epsilon)$-approximate HAC (where near-minimum rather than minimum pairs
are merged) for certain linkage functions that monotonically increase as merges
are performed. However, no such algorithms are known for many important but
non-monotone linkage functions such as centroid and Ward's linkage.
  In this work, we show that a general class of non-monotone linkage functions
-- which include centroid and Ward's distance -- admit efficient NC algorithms
for $(1+\epsilon)$-approximate HAC in low dimensions. Our algorithms are based
on a structural result which may be of independent interest: the height of the
hierarchy resulting from any constant-approximate HAC on $n$ points for this
class of linkage functions is at most $\operatorname{poly}(\log n)$ as long as
$k = O(\log \log n / \log \log \log n)$. Complementing our upper bounds, we
show that NC algorithms for HAC with these linkage functions in
\emph{arbitrary} dimensions are unlikely to exist by showing that HAC is
CC-hard when $d$ is centroid distance and $k = n$.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [143] [Curved Apertures for Customized Wave Trajectories: Beyond Flat Aperture Limitations](https://arxiv.org/abs/2507.20699)
*Joan Martínez Canals,Francesco Devoti,Vincenzo Sciancalepore,Marco Di Renzo,Xavier Costa-Pérez*

Main category: physics.optics

TL;DR: 论文研究采用曲面孔径提升波束成形技术的灵活性与效果。


<details>
  <summary>Details</summary>
Motivation: 克服平面孔径在波束轨迹设计中的几何限制，提升无线通信的灵活性。

Method: 提出一种适用于任意形状孔径的波束轨迹工程新方法。

Result: 理论分析与数值模拟显示，曲面孔径在波束传播控制、相位约束鲁棒性及功率密度分布上表现更优。

Conclusion: 曲面孔径为波束成形提供了更具灵活性和高效性的解决方案。

Abstract: Beam shaping techniques enable tailored beam trajectories, offering
unprecedented connectivity opportunities in wireless communications. Current
approaches rely on flat apertures, which limit trajectory flexibility due to
inherent geometric constraints. To overcome such restrictions, we propose
adopting curved apertures as a more versatile alternative for beam shaping. We
introduce a novel formulation for wave trajectory engineering compatible with
arbitrarily shaped apertures. Theoretical and numerical analyses demonstrate
that curved apertures offer improved control over wave propagation, are more
resilient to phase control constraints, and achieve higher power density across
a wider portion of the desired beam trajectory than flat apertures.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [144] [Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?](https://arxiv.org/abs/2507.20419)
*Khloud AL Jallad,Nada Ghneim,Ghaida Rebdawi*

Main category: cs.CL

TL;DR: 这篇论文综述了英语、阿拉伯语和多语言自然语言理解（NLU）基准测试，重点关注其诊断数据集和覆盖的语言现象。通过比较分析，指出当前研究中缺乏统一的命名规范和标准语言现象集合，并提出研究问题：为何没有类似ISO标准的NLU评估诊断基准？


<details>
  <summary>Details</summary>
Motivation: 近年来，评估NLU能力成为热门研究课题，但缺乏统一的评估标准和语言现象分类，限制了深入比较和分析模型表现。

Method: 论文详细比较分析了现有NLU基准测试的诊断数据集及其覆盖的语言现象，并提出研究问题。

Result: 研究发现当前基准测试在命名规范和标准语言现象集合上存在不足，需建立全球统一的语言现象层次结构。

Conclusion: 建议未来建立类似ISO标准的NLU评估诊断基准，以更深入比较模型表现并支持语言现象的标准化分类。

Abstract: Natural Language Understanding (NLU) is a basic task in Natural Language
Processing (NLP). The evaluation of NLU capabilities has become a trending
research topic that attracts researchers in the last few years, resulting in
the development of numerous benchmarks. These benchmarks include various tasks
and datasets in order to evaluate the results of pretrained models via public
leaderboards. Notably, several benchmarks contain diagnostics datasets designed
for investigation and fine-grained error analysis across a wide range of
linguistic phenomena. This survey provides a comprehensive review of available
English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on
their diagnostics datasets and the linguistic phenomena they covered. We
present a detailed comparison and analysis of these benchmarks, highlighting
their strengths and limitations in evaluating NLU tasks and providing in-depth
error analysis. When highlighting the gaps in the state-of-the-art, we noted
that there is no naming convention for macro and micro categories or even a
standard set of linguistic phenomena that should be covered. Consequently, we
formulated a research question regarding the evaluation metrics of the
evaluation diagnostics benchmarks: "Why do not we have an evaluation standard
for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in
industry. We conducted a deep analysis and comparisons of the covered
linguistic phenomena in order to support experts in building a global hierarchy
for linguistic phenomena in future. We think that having evaluation metrics for
diagnostics evaluation could be valuable to gain more insights when comparing
the results of the studied models on different diagnostics benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [145] [LAVA: Language Driven Scalable and Versatile Traffic Video Analytics](https://arxiv.org/abs/2507.19821)
*Yanrui Yu,Tianfei Zhou,Jiaxin Sun,Lianpeng Qiao,Lizhong Ding,Ye Yuan,Guoren Wang*

Main category: cs.CV

TL;DR: 提出了一种基于自然语言的视频分析系统Lava，通过灵活查询大规模视频数据，显著提高了查询效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有SQL范式对视频分析的限制，提升查询灵活性和效率。

Method: Lava系统包含基于多臂老虎机的采样方法、开放性目标检测模块和长期目标轨迹提取技术。

Result: F1分数提升14%，聚合查询误差降低0.39，处理速度快9.6倍。

Conclusion: Lava系统在灵活性和效率上优于传统方法，适用于大规模视频分析。

Abstract: In modern urban environments, camera networks generate massive amounts of
operational footage -- reaching petabytes each day -- making scalable video
analytics essential for efficient processing. Many existing approaches adopt an
SQL-based paradigm for querying such large-scale video databases; however, this
constrains queries to rigid patterns with predefined semantic categories,
significantly limiting analytical flexibility. In this work, we explore a
language-driven video analytics paradigm aimed at enabling flexible and
efficient querying of high-volume video data driven by natural language.
Particularly, we build \textsc{Lava}, a system that accepts natural language
queries and retrieves traffic targets across multiple levels of granularity and
arbitrary categories. \textsc{Lava} comprises three main components: 1) a
multi-armed bandit-based efficient sampling method for video segment-level
localization;
  2) a video-specific open-world detection module for object-level retrieval;
and 3) a long-term object trajectory extraction scheme for temporal object
association, yielding complete trajectories for object-of-interests. To support
comprehensive evaluation, we further develop a novel benchmark by providing
diverse, semantically rich natural language predicates and fine-grained
annotations for multiple videos. Experiments on this benchmark demonstrate that
\textsc{Lava} improves $F_1$-scores for selection queries by $\mathbf{14\%}$,
reduces MPAE for aggregation queries by $\mathbf{0.39}$, and achieves top-$k$
precision of $\mathbf{86\%}$, while processing videos $ \mathbf{9.6\times} $
faster than the most accurate baseline.

</details>


### [146] [Smaller, Faster, Cheaper: Architectural Designs for Efficient Machine Learning](https://arxiv.org/abs/2507.19795)
*Steven Walton*

Main category: cs.CV

TL;DR: 论文探讨如何通过优化神经网络架构设计，提升计算效率，使其在性能不变的情况下减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 随着计算机视觉模型的快速发展，计算资源需求不断增加，但在资源有限的环境中部署模型时，需要更高效的架构设计。

Method: 论文提出三个方向：1) 优化数据输入输出；2) 改进核心神经网络架构，特别是视觉变换器的受限注意力机制；3) 利用自然结构优化归一化流的知识蒸馏。

Result: 研究表明，精心设计的神经网络架构可以提高效率，使模型更小、更快、更经济。

Conclusion: 通过架构优化，可以在减少计算资源的情况下保持或提升模型性能，适用于资源受限的环境。

Abstract: Major advancements in the capabilities of computer vision models have been
primarily fueled by rapid expansion of datasets, model parameters, and
computational budgets, leading to ever-increasing demands on computational
infrastructure. However, as these models are deployed in increasingly diverse
and resource-constrained environments, there is a pressing need for
architectures that can deliver high performance while requiring fewer
computational resources.
  This dissertation focuses on architectural principles through which models
can achieve increased performance while reducing their computational demands.
We discuss strides towards this goal through three directions. First, we focus
on data ingress and egress, investigating how information may be passed into
and retrieved from our core neural processing units. This ensures that our
models make the most of available data, allowing smaller architectures to
become more performant. Second, we investigate modifications to the core neural
architecture, applied to restricted attention in vision transformers. This
section explores how removing uniform context windows in restricted attention
increases the expressivity of the underlying neural architecture. Third, we
explore the natural structures of Normalizing Flows and how we can leverage
these properties to better distill model knowledge.
  These contributions demonstrate that careful design of neural architectures
can increase the efficiency of machine learning algorithms, allowing them to
become smaller, faster, and cheaper.

</details>


### [147] [T$^\text{3}$SVFND: Towards an Evolving Fake News Detector for Emergencies with Test-time Training on Short Video Platforms](https://arxiv.org/abs/2507.20286)
*Liyuan Zhang,Zeyun Cheng,Yan Yang,Yong Liu,Jinke Ma*

Main category: cs.CV

TL;DR: 论文提出了一种基于测试时间训练（TTT）的虚假新闻视频检测框架（T^3SVFND），旨在解决现有方法在突发事件新闻上的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有虚假新闻视频检测方法难以泛化，尤其是在不同事件的短视频新闻分布发生变化时，性能显著下降。为此，作者希望通过测试时间训练增强模型的鲁棒性。

Method: 提出T^3SVFND框架，利用基于掩码语言建模（MLM）的自监督辅助任务，结合多模态（音频和视频）信息预测被掩码的文本内容，并在测试阶段通过辅助任务适应测试数据的分布。

Result: 在公开基准测试中，模型表现出色，尤其是对突发事件新闻的检测效果显著。

Conclusion: T^3SVFND框架通过测试时间训练和多模态信息融合，有效提升了虚假新闻视频检测的泛化能力，尤其在突发事件场景中表现优异。

Abstract: The existing methods for fake news videos detection may not be generalized,
because there is a distribution shift between short video news of different
events, and the performance of such techniques greatly drops if news records
are coming from emergencies. We propose a new fake news videos detection
framework (T$^3$SVFND) using Test-Time Training (TTT) to alleviate this
limitation, enhancing the robustness of fake news videos detection.
Specifically, we design a self-supervised auxiliary task based on Mask Language
Modeling (MLM) that masks a certain percentage of words in text and predicts
these masked words by combining contextual information from different
modalities (audio and video). In the test-time training phase, the model adapts
to the distribution of test data through auxiliary tasks. Extensive experiments
on the public benchmark demonstrate the effectiveness of the proposed model,
especially for the detection of emergency news.

</details>


### [148] [Endoscopic Depth Estimation Based on Deep Learning: A Survey](https://arxiv.org/abs/2507.20881)
*Ke Niu,Zeyun Liu,Xue Feng,Heng Li,Kaize Shi*

Main category: cs.CV

TL;DR: 这篇论文综述了内窥镜深度估计领域的最新深度学习方法，从数据、方法和应用三个视角系统梳理了现有技术，并讨论了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管内窥镜深度估计在微创手术中具有重要意义，但目前缺乏对基于深度学习的技术的全面综述，本文旨在填补这一空白。

Method: 论文从数据、方法和应用三个角度系统回顾了内窥镜深度估计的相关文献，包括单目和立体方法，并对代表性技术进行了分类。

Result: 总结了常见的性能评估指标和公开数据集，分析了内窥镜场景的挑战，并探讨了该技术在机器人辅助手术中的应用。

Conclusion: 未来研究可以关注领域适应、实时实现和模型泛化等方向，为研究者提供了进一步发展的起点。

Abstract: Endoscopic depth estimation is a critical technology for improving the safety
and precision of minimally invasive surgery. It has attracted considerable
attention from researchers in medical imaging, computer vision, and robotics.
Over the past decade, a large number of methods have been developed. Despite
the existence of several related surveys, a comprehensive overview focusing on
recent deep learning-based techniques is still limited. This paper endeavors to
bridge this gap by systematically reviewing the state-of-the-art literature.
Specifically, we provide a thorough survey of the field from three key
perspectives: data, methods, and applications, covering a range of methods
including both monocular and stereo approaches. We describe common performance
evaluation metrics and summarize publicly available datasets. Furthermore, this
review analyzes the specific challenges of endoscopic scenes and categorizes
representative techniques based on their supervision strategies and network
architectures. The application of endoscopic depth estimation in the important
area of robot-assisted surgery is also reviewed. Finally, we outline potential
directions for future research, such as domain adaptation, real-time
implementation, and enhanced model generalization, thereby providing a valuable
starting point for researchers to engage with and advance the field.

</details>


### [149] [MagicAnime: A Hierarchically Annotated, Multimodal and Multitasking Dataset with Benchmarks for Cartoon Animation Generation](https://arxiv.org/abs/2507.20368)
*Shuolin Xu,Bingyuan Wang,Zeyu Cai,Fangteng Fu,Yue Ma,Tongyi Lee,Hongchuan Yu,Zeyu Wang*

Main category: cs.CV

TL;DR: 论文提出了MagicAnime数据集及基准测试，用于支持高质量卡通动画的多模态控制生成，填补了领域空白。


<details>
  <summary>Details</summary>
Motivation: 卡通动画生成面临复杂非人类角色、多样动作和精细情感的挑战，且缺乏公开多模态数据。MagicAnime旨在解决这一问题。

Method: 构建了包含40万视频片段的大规模多模态数据集MagicAnime，并设计了基准测试MagicAnime-Bench，支持多种生成任务。

Result: 实验验证了数据集在4个任务中的有效性，包括高保真、精细和可控的动画生成。

Conclusion: MagicAnime填补了卡通动画多模态数据集的空白，为高质量动画生成提供了有力支持。

Abstract: Generating high-quality cartoon animations multimodal control is challenging
due to the complexity of non-human characters, stylistically diverse motions
and fine-grained emotions. There is a huge domain gap between real-world videos
and cartoon animation, as cartoon animation is usually abstract and has
exaggerated motion. Meanwhile, public multimodal cartoon data are extremely
scarce due to the difficulty of large-scale automatic annotation processes
compared with real-life scenarios. To bridge this gap, We propose the
MagicAnime dataset, a large-scale, hierarchically annotated, and multimodal
dataset designed to support multiple video generation tasks, along with the
benchmarks it includes. Containing 400k video clips for image-to-video
generation, 50k pairs of video clips and keypoints for whole-body annotation,
12k pairs of video clips for video-to-video face animation, and 2.9k pairs of
video and audio clips for audio-driven face animation. Meanwhile, we also build
a set of multi-modal cartoon animation benchmarks, called MagicAnime-Bench, to
support the comparisons of different methods in the tasks above. Comprehensive
experiments on four tasks, including video-driven face animation, audio-driven
face animation, image-to-video animation, and pose-driven character animation,
validate its effectiveness in supporting high-fidelity, fine-grained, and
controllable generation.

</details>


### [150] [T2VParser: Adaptive Decomposition Tokens for Partial Alignment in Text to Video Retrieval](https://arxiv.org/abs/2507.20518)
*Yili Li,Gang Xiong,Gaopeng Gou,Xiangyan Qu,Jiamin Zhuang,Zhen Li,Junzheng Shi*

Main category: cs.CV

TL;DR: T2VParser提出了一种多视角语义表示方法，解决视频与文本部分不对齐的问题，通过自适应语义对齐提升检索准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视频-文本数据集中的文本描述仅能反映视频部分内容，导致视频-文本匹配中部分不对齐，直接对齐会导致错误监督。

Method: 引入自适应分解令牌，提取多视角语义表示，实现模态间自适应对齐，保留预训练模型知识。

Result: 实验表明T2VParser通过有效跨模态内容分解，实现了部分内容的精确对齐。

Conclusion: T2VParser通过多视角语义表示和自适应对齐，提升了视频-文本检索的准确性。

Abstract: Text-to-video retrieval essentially aims to train models to align visual
content with textual descriptions accurately. Due to the impressive general
multimodal knowledge demonstrated by image-text pretrained models such as CLIP,
existing work has primarily focused on extending CLIP knowledge for video-text
tasks. However, videos typically contain richer information than images. In
current video-text datasets, textual descriptions can only reflect a portion of
the video content, leading to partial misalignment in video-text matching.
Therefore, directly aligning text representations with video representations
can result in incorrect supervision, ignoring the inequivalence of information.
In this work, we propose T2VParser to extract multiview semantic
representations from text and video, achieving adaptive semantic alignment
rather than aligning the entire representation. To extract corresponding
representations from different modalities, we introduce Adaptive Decomposition
Tokens, which consist of a set of learnable tokens shared across modalities.
The goal of T2VParser is to emphasize precise alignment between text and video
while retaining the knowledge of pretrained models. Experimental results
demonstrate that T2VParser achieves accurate partial alignment through
effective cross-modal content decomposition. The code is available at
https://github.com/Lilidamowang/T2VParser.

</details>


### [151] [Regularizing Subspace Redundancy of Low-Rank Adaptation](https://arxiv.org/abs/2507.20745)
*Yue Zhu,Haiwen Diao,Shang Gao,Jiazuo Yu,Jiawen Zhu,Yunzhi Zhuge,Shuai Hao,Xu Jia,Lu Zhang,Ying Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: ReSoRA 是一种改进的 LoRA 方法，通过显式建模并自适应正则化子空间冗余，提高了参数高效迁移学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有 LoRA 方法在训练中投影矩阵不受限制，导致特征学习的子空间冗余，影响特征适应性。已有方法缺乏灵活性且泛化性差。

Method: 提出 ReSoRA，将低秩子矩阵分解为多个等效子空间，并对不同投影的特征分布施加去冗余约束。

Result: 实验表明 ReSoRA 在视觉语言检索和视觉分类任务中有效提升了现有 PETL 方法的表现。

Conclusion: ReSoRA 可作为即插即用的训练监督方法，无需额外推理成本，显著提高迁移学习效率。

Abstract: Low-Rank Adaptation (LoRA) and its variants have delivered strong capability
in Parameter-Efficient Transfer Learning (PETL) by minimizing trainable
parameters and benefiting from reparameterization. However, their projection
matrices remain unrestricted during training, causing high representation
redundancy and diminishing the effectiveness of feature adaptation in the
resulting subspaces. While existing methods mitigate this by manually adjusting
the rank or implicitly applying channel-wise masks, they lack flexibility and
generalize poorly across various datasets and architectures. Hence, we propose
ReSoRA, a method that explicitly models redundancy between mapping subspaces
and adaptively Regularizes Subspace redundancy of Low-Rank Adaptation.
Specifically, it theoretically decomposes the low-rank submatrices into
multiple equivalent subspaces and systematically applies de-redundancy
constraints to the feature distributions across different projections.
Extensive experiments validate that our proposed method consistently
facilitates existing state-of-the-art PETL methods across various backbones and
datasets in vision-language retrieval and standard visual classification
benchmarks. Besides, as a training supervision, ReSoRA can be seamlessly
integrated into existing approaches in a plug-and-play manner, with no
additional inference costs. Code is publicly available at:
https://github.com/Lucenova/ReSoRA.

</details>


### [152] [OW-CLIP: Data-Efficient Visual Supervision for Open-World Object Detection via Human-AI Collaboration](https://arxiv.org/abs/2507.19870)
*Junwen Duan,Wei Xue,Ziyao Kang,Shixia Liu,Jiazhi Xia*

Main category: cs.CV

TL;DR: OW-CLIP是一款视觉分析系统，通过数据高效的多模态提示调优和Crop-Smoothing技术，解决了开放世界目标检测中的数据依赖和模型灵活性不足问题。


<details>
  <summary>Details</summary>
Motivation: 开放世界目标检测需要持续适应新标注数据，但现有方法面临数据需求高、特征过拟合和架构修改限制等问题。

Method: OW-CLIP采用多模态提示调优和Crop-Smoothing技术，结合双模态数据精炼方法，利用大型语言模型和跨模态相似性生成高质量标注数据。

Result: 实验显示，OW-CLIP性能达到SOTA的89%，仅需3.8%的自生成数据，且在同等数据量下优于现有方法。

Conclusion: OW-CLIP通过多模态技术和可视化界面有效提升了开放世界目标检测的效率与标注质量。

Abstract: Open-world object detection (OWOD) extends traditional object detection to
identifying both known and unknown object, necessitating continuous model
adaptation as new annotations emerge. Current approaches face significant
limitations: 1) data-hungry training due to reliance on a large number of
crowdsourced annotations, 2) susceptibility to "partial feature overfitting,"
and 3) limited flexibility due to required model architecture modifications. To
tackle these issues, we present OW-CLIP, a visual analytics system that
provides curated data and enables data-efficient OWOD model incremental
training. OW-CLIP implements plug-and-play multimodal prompt tuning tailored
for OWOD settings and introduces a novel "Crop-Smoothing" technique to mitigate
partial feature overfitting. To meet the data requirements for the training
methodology, we propose dual-modal data refinement methods that leverage large
language models and cross-modal similarity for data generation and filtering.
Simultaneously, we develope a visualization interface that enables users to
explore and deliver high-quality annotations: including class-specific visual
feature phrases and fine-grained differentiated images. Quantitative evaluation
demonstrates that OW-CLIP achieves competitive performance at 89% of
state-of-the-art performance while requiring only 3.8% self-generated data,
while outperforming SOTA approach when trained with equivalent data volumes. A
case study shows the effectiveness of the developed method and the improved
annotation quality of our visualization system.

</details>


### [153] [T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation](https://arxiv.org/abs/2507.20536)
*Chieh-Yun Chen,Min Shi,Gong Zhang,Humphrey Shi*

Main category: cs.CV

TL;DR: T2I-Copilot是一个基于多智能体协作的训练免费系统，通过自动化提示词优化、模型选择和迭代改进，显著提升文本到图像生成的质量和文本-图像对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型对提示词非常敏感，用户需反复调整提示词且缺乏明确反馈，现有方法控制性有限或需要额外训练，泛化能力受限。

Method: T2I-Copilot由三个智能体组成：输入解释器（解析提示词）、生成引擎（选择模型并启动生成）、质量评估器（评估并反馈），支持全自动或人工干预。

Result: 在GenAI-Bench上，T2I-Copilot使用开源模型的VQA得分接近商业模型，成本仅为FLUX1.1-pro的16.59%，但性能超越其6.17%。

Conclusion: T2I-Copilot通过多智能体协作实现了高效的文本到图像生成，为自动化内容创作提供了新思路。

Abstract: Text-to-Image (T2I) generative models have revolutionized content creation
but remain highly sensitive to prompt phrasing, often requiring users to
repeatedly refine prompts multiple times without clear feedback. While
techniques such as automatic prompt engineering, controlled text embeddings,
denoising, and multi-turn generation mitigate these issues, they offer limited
controllability, or often necessitate additional training, restricting the
generalization abilities. Thus, we introduce T2I-Copilot, a training-free
multi-agent system that leverages collaboration between (Multimodal) Large
Language Models to automate prompt phrasing, model selection, and iterative
refinement. This approach significantly simplifies prompt engineering while
enhancing generation quality and text-image alignment compared to direct
generation. Specifically, T2I-Copilot consists of three agents: (1) Input
Interpreter, which parses the input prompt, resolves ambiguities, and generates
a standardized report; (2) Generation Engine, which selects the appropriate
model from different types of T2I models and organizes visual and textual
prompts to initiate generation; and (3) Quality Evaluator, which assesses
aesthetic quality and text-image alignment, providing scores and feedback for
potential regeneration. T2I-Copilot can operate fully autonomously while also
supporting human-in-the-loop intervention for fine-grained control. On
GenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA
score comparable to commercial models RecraftV3 and Imagen 3, surpasses
FLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and
SD 3.5 Large by 9.11% and 6.36%. Code will be released at:
https://github.com/SHI-Labs/T2I-Copilot.

</details>


### [154] [Self-Supervised Continuous Colormap Recovery from a 2D Scalar Field Visualization without a Legend](https://arxiv.org/abs/2507.20632)
*Hongxu Liu,Xinyu Chen,Haoyang Zheng,Manyi Li,Zhenfan Liu,Fumeng Yang,Yunhai Wang,Changhe Tu,Qiong Zeng*

Main category: cs.CV

TL;DR: 提出了一种从2D标量场可视化中恢复连续色彩映射的新方法，通过解耦和重建策略同时预测色彩映射和底层数据。


<details>
  <summary>Details</summary>
Motivation: 在没有色彩图例的情况下，从2D标量场可视化中恢复连续色彩映射具有挑战性，现有方法难以准确完成这一任务。

Method: 采用解耦模块分离色彩映射和数据，通过可微分色彩映射模块重建可视化，设计了重建损失以确保色彩映射与数据的强相关性，并使用三次B样条曲线表示光滑色彩映射。

Result: 在合成数据集和VIS30K真实数据集上验证了方法的有效性，并在色彩映射调整和转移等应用中展示了实用性。

Conclusion: 该方法能够有效恢复连续色彩映射，并适用于多种可视化场景。

Abstract: Recovering a continuous colormap from a single 2D scalar field visualization
can be quite challenging, especially in the absence of a corresponding color
legend. In this paper, we propose a novel colormap recovery approach that
extracts the colormap from a color-encoded 2D scalar field visualization by
simultaneously predicting the colormap and underlying data using a
decoupling-and-reconstruction strategy. Our approach first separates the input
visualization into colormap and data using a decoupling module, then
reconstructs the visualization with a differentiable color-mapping module. To
guide this process, we design a reconstruction loss between the input and
reconstructed visualizations, which serves both as a constraint to ensure
strong correlation between colormap and data during training, and as a
self-supervised optimizer for fine-tuning the predicted colormap of unseen
visualizations during inferencing. To ensure smoothness and correct color
ordering in the extracted colormap, we introduce a compact colormap
representation using cubic B-spline curves and an associated color order loss.
We evaluate our method quantitatively and qualitatively on a synthetic dataset
and a collection of real-world visualizations from the VIS30K dataset.
Additionally, we demonstrate its utility in two prototype applications --
colormap adjustment and colormap transfer -- and explore its generalization to
visualizations with color legends and ones encoded using discrete color
palettes.

</details>


### [155] [Multi-Masked Querying Network for Robust Emotion Recognition from Incomplete Multi-Modal Physiological Signals](https://arxiv.org/abs/2507.20737)
*Geng-Xin Xu,Xiang Zuo,Ye Li*

Main category: cs.CV

TL;DR: 提出了一种多掩码查询网络（MMQ-Net），通过整合多种查询机制，解决了生理数据情绪识别中信号不完整和运动干扰的问题。


<details>
  <summary>Details</summary>
Motivation: 情绪识别在心理健康评估中很重要，但面临多模态信号不完整和运动干扰的挑战。

Method: 使用MMQ-Net，结合模态查询（重建缺失数据）、类别查询（关注情绪特征）和干扰查询（分离噪声）。

Result: 实验表明，MMQ-Net在数据不完整情况下优于现有方法。

Conclusion: MMQ-Net有效提升了情绪识别的性能，尤其在数据不完整时表现突出。

Abstract: Emotion recognition from physiological data is crucial for mental health
assessment, yet it faces two significant challenges: incomplete multi-modal
signals and interference from body movements and artifacts. This paper presents
a novel Multi-Masked Querying Network (MMQ-Net) to address these issues by
integrating multiple querying mechanisms into a unified framework.
Specifically, it uses modality queries to reconstruct missing data from
incomplete signals, category queries to focus on emotional state features, and
interference queries to separate relevant information from noise. Extensive
experiment results demonstrate the superior emotion recognition performance of
MMQ-Net compared to existing approaches, particularly under high levels of data
incompleteness.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [156] [SonicGauss: Position-Aware Physical Sound Synthesis for 3D Gaussian Representations](https://arxiv.org/abs/2507.19835)
*Chunshi Wang,Hongxing Li,Yawei Luo*

Main category: cs.SD

TL;DR: SonicGauss是一种新颖框架，利用3D高斯表示（3DGS）合成碰撞声音，结合扩散模型和点变换器提取器，直接从高斯椭球推断材料特性和空间声学相关性。


<details>
  <summary>Details</summary>
Motivation: 探索3DGS在捕捉物理属性（如声音）方面的潜力。

Method: 集成扩散模型和PointTransformer特征提取器，从高斯椭球推断材料特性和空间声学相关性。

Result: 在ObjectFolder数据集和真实世界录音中验证了方法的真实性、位置感知性和泛化能力。

Conclusion: SonicGauss为连接3D视觉表示和交互式声音合成提供了有前景的一步。

Abstract: While 3D Gaussian representations (3DGS) have proven effective for modeling
the geometry and appearance of objects, their potential for capturing other
physical attributes-such as sound-remains largely unexplored. In this paper, we
present a novel framework dubbed SonicGauss for synthesizing impact sounds from
3DGS representations by leveraging their inherent geometric and material
properties. Specifically, we integrate a diffusion-based sound synthesis model
with a PointTransformer-based feature extractor to infer material
characteristics and spatial-acoustic correlations directly from Gaussian
ellipsoids. Our approach supports spatially varying sound responses conditioned
on impact locations and generalizes across a wide range of object categories.
Experiments on the ObjectFolder dataset and real-world recordings demonstrate
that our method produces realistic, position-aware auditory feedback. The
results highlight the framework's robustness and generalization ability,
offering a promising step toward bridging 3D visual representations and
interactive sound synthesis. Project page: https://chunshi.wang/SonicGauss

</details>


### [157] [Music Arena: Live Evaluation for Text-to-Music](https://arxiv.org/abs/2507.20900)
*Yonghyun Kim,Wayne Chi,Anastasios N. Angelopoulos,Wei-Lin Chiang,Koichi Saito,Shinji Watanabe,Yuki Mitsufuji,Chris Donahue*

Main category: cs.SD

TL;DR: Music Arena是一个开放平台，用于对文本到音乐（TTM）模型进行可扩展的人类偏好评估，填补了当前缺乏开放和可再生偏好数据源的空白。


<details>
  <summary>Details</summary>
Motivation: 在TTM领域，通过人类听感研究获取偏好是评估的黄金标准，但这些研究成本高且难以比较。此外，缺乏开放和可再生的偏好数据源限制了研究人员对齐TTM系统或改进自动评估指标的能力。

Method: Music Arena提供实时评估，用户输入文本提示并比较两个TTM系统的输出，其偏好用于生成排行榜。平台设计了音乐特定的功能，如基于LLM的路由系统和详细偏好收集。

Result: 平台通过标准化评估协议、透明数据访问政策和音乐特定功能，不仅解决了TTM生态系统中的关键挑战，还展示了如何针对特定AI领域独特特性进行实时评估。

Conclusion: Music Arena为TTM领域提供了一种可行的实时评估方法，并通过其独特设计展示了如何适应特定领域的评估需求。

Abstract: We present Music Arena, an open platform for scalable human preference
evaluation of text-to-music (TTM) models. Soliciting human preferences via
listening studies is the gold standard for evaluation in TTM, but these studies
are expensive to conduct and difficult to compare, as study protocols may
differ across systems. Moreover, human preferences might help researchers align
their TTM systems or improve automatic evaluation metrics, but an open and
renewable source of preferences does not currently exist. We aim to fill these
gaps by offering *live* evaluation for TTM. In Music Arena, real-world users
input text prompts of their choosing and compare outputs from two TTM systems,
and their preferences are used to compile a leaderboard. While Music Arena
follows recent evaluation trends in other AI domains, we also design it with
key features tailored to music: an LLM-based routing system to navigate the
heterogeneous type signatures of TTM systems, and the collection of *detailed*
preferences including listening data and natural language feedback. We also
propose a rolling data release policy with user privacy guarantees, providing a
renewable source of preference data and increasing platform transparency.
Through its standardized evaluation protocol, transparent data access policies,
and music-specific features, Music Arena not only addresses key challenges in
the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully
adapted to unique characteristics of specific AI domains.
  Music Arena is available at: https://music-arena.org

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [158] [Signed Higher-Order Interactions for Brain Disorder Diagnosis via Multi-Channel Transformers](https://arxiv.org/abs/2507.20205)
*Dengyi Zhao,Zhiheng Zhou,Guiying Yan,Dongxiao Yu,Xingqi Qi*

Main category: q-bio.NC

TL;DR: 提出了一种名为HOI-Brain的新框架，利用带符号的高阶相互作用和模式从fMRI数据中诊断脑疾病。通过高阶相互作用检测和持续性同调理论，结合多通道Transformer模型，显著提升了诊断效果并提供了生物学解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的深度学习方法主要关注成对或三元模式，忽略了带符号的高阶相互作用，限制了全面理解脑区通信。因此需要一种新方法来捕捉这些复杂相互作用以提升脑疾病诊断能力。

Method: 1. 引入基于时间导数乘积的共波动度量检测高阶相互作用；2. 区分正负协同作用并用带符号加权单纯复编码；3. 应用持续性同调理论提取时空组织；4. 设计多通道Transformer整合拓扑特征。

Result: 在阿尔茨海默病、帕金森综合症和自闭症数据集上的实验表明，该框架在诊断效果和可解释性上优于现有方法，且识别出的关键脑区与神经科学文献一致。

Conclusion: HOI-Brain通过带符号的高阶相互作用和拓扑分析，不仅提升了脑疾病诊断性能，还为理解脑通信提供了新的生物学见解。

Abstract: Accurately characterizing higher-order interactions of brain regions and
extracting interpretable organizational patterns from Functional Magnetic
Resonance Imaging data is crucial for brain disease diagnosis. Current
graph-based deep learning models primarily focus on pairwise or triadic
patterns while neglecting signed higher-order interactions, limiting
comprehensive understanding of brain-wide communication. We propose HOI-Brain,
a novel computational framework leveraging signed higher-order interactions and
organizational patterns in fMRI data for brain disease diagnosis. First, we
introduce a co-fluctuation measure based on Multiplication of Temporal
Derivatives to detect higher-order interactions with temporal resolution. We
then distinguish positive and negative synergistic interactions, encoding them
in signed weighted simplicial complexes to reveal brain communication insights.
Using Persistent Homology theory, we apply two filtration processes to these
complexes to extract signed higher-dimensional neural organizations
spatiotemporally. Finally, we propose a multi-channel brain Transformer to
integrate heterogeneous topological features. Experiments on Alzheimer' s
disease, Parkinson' s syndrome, and autism spectrum disorder datasets
demonstrate our framework' s superiority, effectiveness, and interpretability.
The identified key brain regions and higher-order patterns align with
neuroscience literature, providing meaningful biological insights.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [159] [Towards Multi-Agent Economies: Enhancing the A2A Protocol with Ledger-Anchored Identities and x402 Micropayments for AI Agents](https://arxiv.org/abs/2507.19550)
*Awid Vaziry,Sandro Rodriguez Garzon,Axel Küpper*

Main category: cs.MA

TL;DR: 本文提出了一种新架构，解决了多智能体经济中的两个关键问题：分散式智能体发现与微支付问题。


<details>
  <summary>Details</summary>
Motivation: 当前Agent2Agent（A2A）通信协议存在分散式智能体发现和微支付限制，阻碍了多智能体经济的发展。

Method: 通过整合分布式账本技术（DLT）实现智能合约形式的AgentCards，并引入x402开放标准支持HTTP 402状态码的微支付。

Result: 研究证明基于DLT的智能体发现和微支付可行，为安全、可扩展的多智能体生态系统奠定了基础。

Conclusion: 该架构推动了智能体AI向可信、自主的经济交互发展。

Abstract: This research article presents a novel architecture to empower multi-agent
economies by addressing two critical limitations of the emerging Agent2Agent
(A2A) communication protocol: decentralized agent discoverability and
agent-to-agent micropayments. By integrating distributed ledger technology
(DLT), this architecture enables tamper-proof, on-chain publishing of
AgentCards as smart contracts, providing secure and verifiable agent
identities. The architecture further extends A2A with the x402 open standard,
facilitating blockchain-agnostic, HTTP-based micropayments via the HTTP 402
status code. This enables autonomous agents to seamlessly discover,
authenticate, and compensate each other across organizational boundaries. This
work further presents a comprehensive technical implementation and evaluation,
demonstrating the feasibility of DLT-based agent discovery and micropayments.
The proposed approach lays the groundwork for secure, scalable, and
economically viable multi-agent ecosystems, advancing the field of agentic AI
toward trusted, autonomous economic interactions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [160] [Program Analysis for High-Value Smart Contract Vulnerabilities: Techniques and Insights](https://arxiv.org/abs/2507.20672)
*Yannis Smaragdakis,Neville Grech,Sifis Lagouvardos,Konstantinos Triantafyllou,Ilias Tsatiris,Yannis Bollanos,Tony Rocco Valentine*

Main category: cs.CR

TL;DR: 该论文展示了如何通过自动化技术成功发现高价值的智能合约漏洞，并强调了高完整性的静态分析和领域知识的重要性。


<details>
  <summary>Details</summary>
Motivation: 区块链安全社区普遍认为自动化技术只能检测低价值的浅层漏洞，这篇论文旨在打破这一认知，展示自动化技术在高价值漏洞发现中的潜力。

Method: 论文采用了高完整性的静态分析方法，并结合专家经验或通过统计推断获取的领域知识，提出了一种自动推断领域知识的新技术。

Result: 研究成功发现了多个高价值漏洞，获得了超过300万美元的奖励，并在部署前或审计阶段的代码中检测到数百个漏洞。

Conclusion: 论文认为，适用于现实世界高价值漏洞发现的实用分析工具可能需要接受较高的假阳性率（如95%），但其检测率（1%）仍然具有实际意义。

Abstract: A widespread belief in the blockchain security community is that automated
techniques are only good for detecting shallow bugs, typically of small value.
In this paper, we present the techniques and insights that have led us to
repeatable success in automatically discovering high-value smart contract
vulnerabilities. Our vulnerability disclosures have yielded 10 bug bounties,
for a total of over $3M, over high-profile deployed code, as well as hundreds
of bugs detected in pre-deployment or under-audit code.
  We argue that the elements of this surprising success are a) a very
high-completeness static analysis approach that manages to maintain acceptable
precision; b) domain knowledge, provided by experts or captured via statistical
inference. We present novel techniques for automatically inferring domain
knowledge from statistical analysis of a large corpus of deployed contracts, as
well as discuss insights on the ideal precision and warning rate of a promising
vulnerability detector. In contrast to academic literature in program analysis,
which routinely expects false-positive rates below 50% for publishable results,
we posit that a useful analysis for high-value real-world vulnerabilities will
likely flag very few programs (under 1%) and will do so with a high
false-positive rate (e.g., 95%, meaning that only one-of-twenty human
inspections will yield an exploitable vulnerability).

</details>


### [161] [Is Crunching Public Data the Right Approach to Detect BGP Hijacks?](https://arxiv.org/abs/2507.20434)
*Alessandro Giaconia,Muoi Tran,Laurent Vanbever,Stefano Vissicchio*

Main category: cs.CR

TL;DR: 论文指出，现有基于机器学习的BGP劫持检测系统（如DFOH和BEAM）容易受到数据投毒攻击，攻击者只需少量伪造的路由公告即可绕过检测。


<details>
  <summary>Details</summary>
Motivation: 尽管ROV正在全面部署，但攻击者已适应并推出了后ROV攻击，如伪造起源劫持。现有的ML检测系统依赖全局BGP监视器数据，但这些数据可能被对手误导。

Method: 通过大规模BGP模拟，展示攻击者如何利用少量精心设计的路由公告绕过DFOH和BEAM等检测系统。

Result: 研究表明，仅依靠公开BGP数据存在严重弱点，攻击者可以轻松污染ML防御系统的知识库并扭曲其依赖的指标。

Conclusion: 论文呼吁改进现有BGP劫持检测方法，避免过度依赖可能被污染的公开数据。

Abstract: The Border Gateway Protocol (BGP) remains a fragile pillar of Internet
routing. BGP hijacks still occurr daily. While full deployment of Route Origin
Validation (ROV) is ongoing, attackers have already adapted, launching post-ROV
attacks such as forged-origin hijacks. To detect these, recent approaches like
DFOH [Holterbach et al., USENIX NSDI '24] and BEAM [Chen et al., USENIX
Security '24] apply machine learning (ML) to analyze data from globally
distributed BGP monitors, assuming anomalies will stand out against historical
patterns. However, this assumption overlooks a key threat: BGP monitors
themselves can be misled by adversaries injecting bogus routes. This paper
shows that state-of-the-art hijack detection systems like DFOH and BEAM are
vulnerable to data poisoning. Using large-scale BGP simulations, we show that
attackers can evade detection with just a handful of crafted announcements
beyond the actual hijack. These announcements are indeed sufficient to corrupt
the knowledge base used by ML-based defenses and distort the metrics they rely
on. Our results highlight a worrying weakness of relying solely on public BGP
data.

</details>


### [162] [Towards the ideals of Self-Recovery and Metadata Privacy in Social Vault Recovery](https://arxiv.org/abs/2507.19484)
*Shailesh Mishra,Simone Colombo,Pasindu Tennage,Martin Burkhart,Bryan Ford*

Main category: cs.CR

TL;DR: 论文提出了Apollo框架，用于解决社交密钥恢复中用户记忆负担与元数据隐私之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有社交密钥恢复机制忽视了对用户记忆的要求，需要在恢复时召回一定数量的受托人，增加了用户的记忆负担。

Method: Apollo框架通过在用户社交圈中分发不可区分的数据，消除了记忆元数据的需求，并通过多层次的秘密共享方案保护元数据隐私。

Result: Apollo将恶意恢复的概率降至0.005%至1.8%，多层次设计将延迟从单层方案的1.1倍降低至740千倍。

Conclusion: Apollo有效解决了社交密钥恢复中记忆负担与隐私保护的权衡问题，提高了安全性和效率。

Abstract: Social key recovery mechanisms enable users to recover their vaults with the
help of trusted contacts, or trustees, avoiding the need for a single point of
trust or memorizing complex strings. However, existing mechanisms overlook the
memorability demands on users for recovery, such as the need to recall a
threshold number of trustees. Therefore, we first formalize the notion of
recovery metadata in the context of social key recovery, illustrating the
tradeoff between easing the burden of memorizing the metadata and maintaining
metadata privacy. We present Apollo, the first framework that addresses this
tradeoff by distributing indistinguishable data within a user's social circle,
where trustees hold relevant data and non-trustees store random data. Apollo
eliminates the need to memorize recovery metadata since a user eventually
gathers sufficient data from her social circle for recovery. Due to
indistinguishability, Apollo protects metadata privacy by forming an anonymity
set that hides the trustees among non-trustees. To make the anonymity set
scalable, Apollo proposes a novel multi-layered secret sharing scheme that
mitigates the overhead due to the random data distributed among non-trustees.
Finally, we provide a prototype implementation of Apollo and report on its
performance. Apollo reduces the chances of malicious recovery to between 0.005%
and 1.8%, depending on the adversary's ability to compromise. The multi-layered
design shows a latency reduction from 1.1x to 740kx compared to a
single-layered approach, depending on the number of reconnections.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [163] [Handoff Design in User-Centric Cell-Free Massive MIMO Networks Using DRL](https://arxiv.org/abs/2507.20966)
*Hussein A. Ammar,Raviraj Adve,Shahram Shahbazpanahi,Gary Boudreau,Israfil Bahceci*

Main category: cs.IT

TL;DR: 论文提出了一种基于深度强化学习的解决方案，用于预测和管理用户移动时的连接切换，以减少频繁切换带来的开销。


<details>
  <summary>Details</summary>
Motivation: 在用户为中心的蜂窝大规模MIMO网络中，用户移动性导致频繁的连接切换，带来资源分配和释放的开销，需要优化管理。

Method: 使用Soft Actor-Critic算法训练深度神经网络作为切换策略，提出了一种整合切换惩罚的奖励函数，并开发了两种观察模式（方向辅助和历史辅助）。

Result: 模拟结果显示，该连续动作空间方法比离散方法更具可扩展性，且策略能自动学习在特定时隙集中切换以最小化开销，响应时间小于0.4毫秒。

Conclusion: 所提出的深度强化学习方法能有效管理用户移动时的资源切换，显著减少开销并实现实时响应。

Abstract: In the user-centric cell-free massive MIMO (UC-mMIMO) network scheme, user
mobility necessitates updating the set of serving access points to maintain the
user-centric clustering. Such updates are typically performed through handoff
(HO) operations; however, frequent HOs lead to overheads associated with the
allocation and release of resources. This paper presents a deep reinforcement
learning (DRL)-based solution to predict and manage these connections for
mobile users. Our solution employs the Soft Actor-Critic algorithm, with
continuous action space representation, to train a deep neural network to serve
as the HO policy. We present a novel proposition for a reward function that
integrates a HO penalty in order to balance the attainable rate and the
associated overhead related to HOs. We develop two variants of our system; the
first one uses mobility direction-assisted (DA) observations that are based on
the user movement pattern, while the second one uses history-assisted (HA)
observations that are based on the history of the large-scale fading (LSF).
Simulation results show that our DRL-based continuous action space approach is
more scalable than discrete space counterpart, and that our derived HO policy
automatically learns to gather HOs in specific time slots to minimize the
overhead of initiating HOs. Our solution can also operate in real time with a
response time less than 0.4 ms.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [164] [Accelerating Deterministic Global Optimization via GPU-parallel Interval Arithmetic](https://arxiv.org/abs/2507.20769)
*Hongzhen Zhang,Tim Kerkenhoff,Neil Kichler,Manuel Dahmen,Alexander Mitsos,Uwe Naumann,Dominik Bongartz*

Main category: math.OC

TL;DR: 该论文提出了一种基于GPU并行化的空间分支定界算法，通过区间划分和并行计算显著提升了算法的计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 尽管空间分支定界算法广泛应用于求解非凸问题，但其计算成本高昂。虽然已有一些研究通过CPU并行化加速算法，但GPU并行化仍鲜有探索。

Method: 将每个分支定界节点的域划分为多个子域，利用GPU并行计算每个子域的区间界限，采用均值形式计算目标函数和约束的界限。

Result: 实验表明，使用更多子域能显著提高界限紧密度并减少分支定界迭代次数，GPU加速版本比CPU版本快三个数量级；CUDA图实现的效率更高。

Conclusion: 研究证明了GPU加速的界限计算技术能够显著提升分支定界算法的性能，具有实际应用潜力。

Abstract: Spatial Branch and Bound (B&B) algorithms are widely used for solving
nonconvex problems to global optimality, yet they remain computationally
expensive. Though some works have been carried out to speed up B&B via CPU
parallelization, GPU parallelization is much less explored. In this work, we
investigate the design of a spatial B&B algorithm that involves an
interval-based GPU-parallel lower bounding solver: The domain of each B&B node
is temporarily partitioned into numerous subdomains, then massive GPU
parallelism is leveraged to compute interval bounds of the objective function
and constraints on each subdomain, using the Mean Value Form. The resulting
bounds are tighter than those achieved via regular interval arithmetic without
partitioning, but they remain fast to compute. We implement the method into our
open-source solver MAiNGO via CUDA in two manners: wrapping all GPU tasks
within one kernel function, or distributing the GPU tasks onto a CUDA graph.
Numerical experiments show that using more subdomains leads to significantly
tighter lower bounds and thus less B&B iterations. Regarding wall clock time,
the proposed spatial B&B framework achieves a speedup of three orders of
magnitude compared to applying interval arithmetic on the CPU without domain
partitioning. Among the two implementations, the one developed with CUDA graph
enables higher efficiency. Moreover, in some case studies, the proposed method
delivers competitive or better performance compared to MAiNGO's default solver
which is based on McCormick relaxations. These results highlight the potential
of GPU-accelerated bounding techniques to accelerate B&B algorithms.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [165] [Unravelling Cyclic First-Order Arithmetic](https://arxiv.org/abs/2507.20865)
*Graham E. Leigh,Dominik Wehr*

Main category: math.LO

TL;DR: 本文提出了一种简单直接的方法，将Heyting和Peano算术的循环证明嵌入纯归纳（有限）证明中，扩展了Sprenger和Dam的方法，并展示了循环证明的新表示形式。


<details>
  <summary>Details</summary>
Motivation: 传统循环证明系统与常规变体的等价性验证通常复杂，本文旨在提供一种更简单直接的嵌入方法，避免繁琐的元数学算术化。

Method: 通过调整Sprenger和Dam对μFOL循环证明系统的翻译方法，将其扩展到Heyting和Peano算术，并提出循环证明的标记序列演算表示。

Result: 成功将循环证明嵌入有限证明，恢复了Das关于CΠn⊆IΠn+1的结果，展示了新方法的高效性。

Conclusion: 本文的方法简化了循环证明系统的等价性验证，并为相关领域提供了新的技术工具。

Abstract: Cyclic proof systems for Heyting and Peano arithmetic eschew induction axioms
by accepting proofs which are finite graphs rather than trees. Proving that
such a cyclic proof system coincides with its more conventional variants is
often difficult: Previous proofs in the literature rely on intricate
arithmetisations of the metamathematics of the cyclic proof systems.
  In this article, we present a simple and direct embedding of cyclic proofs
for Heyting and Peano arithmetic into purely inductive, i.e. 'finitary', proofs
by adapting a translation introduced by Sprenger and Dam for a cyclic proof
system of $\mu\text{FOL}$ with explicit ordinal approximations. We extend their
method to recover Das' result of $\text{C}\Pi_n \subseteq \text{I}\Pi_{n + 1}$
for Peano arithmetic. As part of the embedding we present a novel
representation of cyclic proofs as a labelled sequent calculus.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [166] [A taxonomy of categories for relations](https://arxiv.org/abs/2502.10323)
*Cipriano Junior Cioffo,Fabio Gadducci,Davide Trotta*

Main category: math.CT

TL;DR: 本文综述了抽象关系结构特性的范畴，包括其丰富版本，并展示如何通过对称幺半群单子的Kleisli范畴实现，旨在为相关文献中的概念和框架提供清晰和系统的分类。


<details>
  <summary>Details</summary>
Motivation: 研究目的是总结和系统化多年来关于关系结构特性的广泛研究，填补文献中的概念和框架分类的空白。

Method: 通过对称幺半群单子的Kleisli范畴，展示这些“关系范畴”及其丰富版本的形成过程。

Result: 提出了一个清晰和系统的分类法，为相关研究提供了组织和参考。

Conclusion: 本文通过现代和全面的方式，为关系范畴的研究提供了统一框架，有助于未来研究的进一步发展和分类。

Abstract: The study of categories abstracting the structural properties of relations
has been extensively developed over the years, resulting in a rich and diverse
body of work. This paper strives to provide a modern and comprehensive
presentation of these ``categories for relations'', including their enriched
version, further showing how they arise as Kleisli categories of suitable
symmetric monoidal monads. The resulting taxonomy aims at bringing clarity and
organisation to the numerous related concepts and frameworks occurring in the
literature

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [167] [Studying Disinformation Narratives on Social Media with LLMs and Semantic Similarity](https://arxiv.org/abs/2507.20066)
*Chaytan Inman*

Main category: cs.SI

TL;DR: 该论文开发了一种用于检测虚假信息的连续尺度测量工具，包含追踪工具和叙事合成工具，并通过案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决虚假信息中的部分真实性和微妙性检测问题，开发了能够量化相似性的工具。

Method: 开发了两个工具：追踪工具用于量化推文与目标叙事的相似性并生成时间线；叙事合成工具用于聚类高相似性推文并提取主导叙事。两者整合为推文叙事分析仪表盘。

Result: 工具在GLUE STS-B基准测试中验证有效性，并通过两个案例研究（选举盗窃和跨性别者对社会有害）进一步实证。

Conclusion: 工具能够有效检测、追踪和表征虚假信息，为研究提供了新方法。

Abstract: This thesis develops a continuous scale measurement of similarity to
disinformation narratives that can serve to detect disinformation and capture
the nuanced, partial truths that are characteristic of it. To do so, two tools
are developed and their methodologies are documented. The tracing tool takes
tweets and a target narrative, rates the similarities of each to the target
narrative, and graphs it as a timeline. The second narrative synthesis tool
clusters tweets above a similarity threshold and generates the dominant
narratives within each cluster. These tools are combined into a Tweet Narrative
Analysis Dashboard. The tracing tool is validated on the GLUE STS-B benchmark,
and then the two tools are used to analyze two case studies for further
empirical validation. The first case study uses the target narrative "The 2020
election was stolen" and analyzes a dataset of Donald Trump's tweets during
2020. The second case study uses the target narrative, "Transgender people are
harmful to society" and analyzes tens of thousands of tweets from the media
outlets The New York Times, The Guardian, The Gateway Pundit, and Fox News.
Together, the empirical findings from these case studies demonstrate semantic
similarity for nuanced disinformation detection, tracing, and characterization.
  The tools developed in this thesis are hosted and can be accessed through the
permission of the author. Please explain your use case in your request. The
HTML friendly version of this paper is at
https://chaytanc.github.io/projects/disinfo-research (Inman, 2025).

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [168] [Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective](https://arxiv.org/abs/2507.19487)
*Margarita Leib,Nils Köbis,Ivan Soraperra*

Main category: cs.CY

TL;DR: 人们依赖AI建议做决策时，若建议鼓励自私行为，决策者会如何被评价和惩罚？通过实验发现，自私行为比亲社会行为更易受惩罚，且惩罚程度受建议内容影响，而非建议来源。


<details>
  <summary>Details</summary>
Motivation: 研究AI建议如何影响人们对自私行为的评价和惩罚，以及责任归属问题。

Method: 采用预注册、经济激励的实验，让评估者惩罚真实决策者，考察不同建议来源（AI、人类或无建议）和行为（自私或亲社会）的影响。

Result: 自私行为受更重惩罚，且惩罚程度取决于建议内容（自私或亲社会建议），而非建议来源。决策者遵循AI建议时被认为更负责任。

Conclusion: 行为和建议内容共同影响惩罚，而建议来源不影响惩罚结果。

Abstract: People increasingly rely on AI-advice when making decisions. At times, such
advice can promote selfish behavior. When individuals abide by
selfishness-promoting AI advice, how are they perceived and punished? To study
this question, we build on theories from social psychology and combine
machine-behavior and behavioral economic approaches. In a pre-registered,
financially-incentivized experiment, evaluators could punish real
decision-makers who (i) received AI, human, or no advice. The advice (ii)
encouraged selfish or prosocial behavior, and decision-makers (iii) behaved
selfishly or, in a control condition, behaved prosocially. Evaluators further
assigned responsibility to decision-makers and their advisors. Results revealed
that (i) prosocial behavior was punished very little, whereas selfish behavior
was punished much more. Focusing on selfish behavior, (ii) compared to
receiving no advice, selfish behavior was penalized more harshly after
prosocial advice and more leniently after selfish advice. Lastly, (iii) whereas
selfish decision-makers were seen as more responsible when they followed AI
compared to human advice, punishment between the two advice sources did not
vary. Overall, behavior and advice content shape punishment, whereas the advice
source does not.

</details>


### [169] [FlashGuard: Novel Method in Evaluating Differential Characteristics of Visual Stimuli for Deterring Seizure Triggers in Photosensitive Epilepsy](https://arxiv.org/abs/2507.19692)
*Ishan Pendyala*

Main category: cs.CY

TL;DR: 提出FlashGuard方法，实时检测并减少屏幕闪烁对光敏性癫痫用户的刺激。


<details>
  <summary>Details</summary>
Motivation: 光敏性癫痫患者在使用数字设备时面临不可预测的视觉刺激风险，现有解决方案缺乏实时性和计算效率。

Method: 基于CIELAB色彩空间分析，检测帧间颜色变化率并减少亮度和平滑颜色过渡。

Result: 为光敏性癫痫患者提供更有效的保护，同时呼吁扩展WCAG指南以更好应对风险。

Conclusion: FlashGuard通过色彩空间分析提升了对光敏性癫痫患者的保护，推动了政策与软件的改进。

Abstract: In the virtual realm, individuals with photosensitive epilepsy (PSE)
encounter challenges when using devices, resulting in exposure to unpredictable
seizure-causing visual stimuli. The current norm for preventing epileptic
flashes in media is to detect asynchronously when a flash will occur in a
video, then notifying the user. However, there is a lack of a real-time and
computationally efficient solution for dealing with this issue. To address this
issue and enhance accessibility for photosensitive viewers, FlashGuard, a novel
approach, was devised to assess the rate of change of colors in frames across
the user's screen and appropriately mitigate stimuli, based on perceptually
aligned color space analysis in the CIELAB color space. The detection system is
built on analyzing differences in color, and the mitigation system works by
reducing luminance and smoothing color transitions. This study provides novel
insight into how intrinsic color properties contribute to perceptual
differences in flashing for PSE individuals, calling for the adoption of
broadened WCAG guidelines to better account for risk. These insights and
implementations pave the way for stronger protections for individuals with PSE
from dangerous triggers in digital media, both in policy and in software.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [170] [Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models](https://arxiv.org/abs/2507.19854)
*Anjali R. Menon,Rohit K. Sharma,Priya Singh,Chengyu Wang,Aurora M. Ferreira,Mateja Novak*

Main category: cs.RO

TL;DR: 提出“Think, Act, Learn”框架，通过闭环学习提升机器人在动态环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM在机器人任务规划中开环系统的脆弱性问题，提高适应性。

Method: 设计T-A-L框架，分“思考、行动、学习”三步骤，结合反馈和记忆优化策略。

Result: 实验中，T-A-L框架在复杂任务中成功率超97%，平均9次试验即收敛，泛化能力强。

Conclusion: T-A-L框架显著提升了机器人自主性和适应性，是实现真正自主机器人的重要进展。

Abstract: The integration of Large Language Models (LLMs) into robotics has unlocked
unprecedented capabilities in high-level task planning. However, most current
systems operate in an open-loop fashion, where LLMs act as one-shot planners,
rendering them brittle and unable to adapt to unforeseen circumstances in
dynamic physical environments. To overcome this limitation, this paper
introduces the "Think, Act, Learn" (T-A-L) framework, a novel architecture that
enables an embodied agent to autonomously learn and refine its policies through
continuous interaction. Our framework establishes a closed-loop cycle where an
LLM first "thinks" by decomposing high-level commands into actionable plans.
The robot then "acts" by executing these plans while gathering rich, multimodal
sensory feedback. Critically, the "learn" module processes this feedback to
facilitate LLM-driven self-reflection, allowing the agent to perform causal
analysis on its failures and generate corrective strategies. These insights are
stored in an experiential memory to guide future planning cycles. We
demonstrate through extensive experiments in both simulation and the real world
that our T-A-L agent significantly outperforms baseline methods, including
open-loop LLMs, Behavioral Cloning, and traditional Reinforcement Learning. Our
framework achieves over a 97% success rate on complex, long-horizon tasks,
converges to a stable policy in an average of just 9 trials, and exhibits
remarkable generalization to unseen tasks. This work presents a significant
step towards developing more robust, adaptive, and truly autonomous robotic
agents.

</details>
