<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 5]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 12]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.AI](#cs.AI) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Vision: An Extensible Methodology for Formal Software Verification in Microservice Systems](https://arxiv.org/abs/2509.02860)
*Connor Wojtak,Darek Gajewski,Tomas Cerny*

Main category: cs.SE

TL;DR: 提出一种静态重构微服务源代码为系统模型的方法，支持形式化验证以提升系统可维护性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 微服务系统的分散开发和持续演进可能导致沟通不畅和实现不兼容，影响系统可维护性和可靠性。

Method: 将微服务源代码静态重构为形式化系统模型，并从中导出SMT约束集进行验证。

Result: 方法支持跨多个横切关注点（如系统架构和安全性）的形式化验证。

Conclusion: 该方法具有扩展性，未来将进一步评估和扩展。

Abstract: Microservice systems are becoming increasingly adopted due to their
scalability, decentralized development, and support for continuous integration
and delivery (CI/CD). However, this decentralized development by separate teams
and continuous evolution can introduce miscommunication and incompatible
implementations, undermining system maintainability and reliability across
aspects from security policy to system architecture. We propose a novel
methodology that statically reconstructs microservice source code into a formal
system model. From this model, a Satisfiability Modulo Theories (SMT)
constraint set can be derived, enabling formal verification. Our methodology is
extensible, supporting software verification across multiple cross-cutting
concerns. We focus on applying the methodology to verify the system
architecture concern, presenting formal reasoning to validate the methodology's
correctness and applicability for this concern. Additional concerns such as
security policy implementation are considered. Future directions are
established to extend and evaluate the methodology.

</details>


### [2] [Are We SOLID Yet? An Empirical Study on Prompting LLMs to Detect Design Principle Violations](https://arxiv.org/abs/2509.03093)
*Fatih Pehlivan,Arçin Ülkü Ergüzen,Sahand Moslemi Yengejeh,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: 论文提出了一种利用LLMs检测多语言代码中SOLID原则违反的新方法，评估了四种主流LLMs的表现，结果显示GPT-4o Mini表现最佳，但提示策略和语言特性对检测准确性有显著影响。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析方法难以检测SOLID原则违反，现有解决方案局限于单一原则或语言，因此需要一种能跨多语言检测所有五原则的新方法。

Method: 通过定制提示工程评估四种LLMs（CodeLlama、DeepSeekCoder、QwenCoder和GPT-4o Mini）在多语言代码中检测SOLID原则违反的能力，构建了240个手动验证代码示例的数据集，并测试了四种提示策略。

Result: GPT-4o Mini表现最优，但提示策略对检测准确性有显著影响，不同策略在不同原则检测中表现各异。准确性受语言特性和代码复杂度影响较大。

Conclusion: AI驱动的设计分析需要根据具体设计场景选择合适模型和提示策略，LLMs在辅助代码可维护性分析方面具有潜力。

Abstract: Traditional static analysis methods struggle to detect semantic design flaws,
such as violations of the SOLID principles, which require a strong
understanding of object-oriented design patterns and principles. Existing
solutions typically focus on individual SOLID principles or specific
programming languages, leaving a gap in the ability to detect violations across
all five principles in multi-language codebases. This paper presents a new
approach: a methodology that leverages tailored prompt engineering to assess
LLMs on their ability to detect SOLID violations across multiple languages. We
present a benchmark of four leading LLMs-CodeLlama, DeepSeekCoder, QwenCoder,
and GPT-4o Mini-on their ability to detect violations of all five SOLID
principles. For this evaluation, we construct a new benchmark dataset of 240
manually validated code examples. Using this dataset, we test four distinct
prompt strategies inspired by established zero-shot, few-shot, and
chain-of-thought techniques to systematically measure their impact on detection
accuracy. Our emerging results reveal a stark hierarchy among models, with
GPT-4o Mini decisively outperforming others, yet even struggles with
challenging principles like DIP. Crucially, we show that prompt strategy has a
dramatic impact, but no single strategy is universally best; for instance, a
deliberative ENSEMBLE prompt excels at OCP detection while a hint-based EXAMPLE
prompt is superior for DIP violations. Across all experiments, detection
accuracy is heavily influenced by language characteristics and degrades sharply
with increasing code complexity. These initial findings demonstrate that
effective, AI-driven design analysis requires not a single best model, but a
tailored approach that matches the right model and prompt to the specific
design context, highlighting the potential of LLMs to support maintainability
through AI-assisted code analysis.

</details>


### [3] [AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation](https://arxiv.org/abs/2509.03270)
*Martin Skoglund,Fredrik Warg,Aria Mirzai,Anders Thorsen,Karl Lundgren,Peter Folkesson,Bastian Havers-zulka*

Main category: cs.SE

TL;DR: 论文探讨了如何结合ISO 26262和ISO/PAS 8800标准，对电动汽车中的AI组件进行独立安全评估，重点通过电池SOC估计案例展示方法和鲁棒性测试。


<details>
  <summary>Details</summary>
Motivation: AI技术在电动汽车中的应用带来了新的安全挑战，传统评估方法无法满足需求，需结合新标准进行改进。

Method: 结合ISO 26262和ISO/PAS 8800标准，以AI驱动的SOC电池估计为例，进行故障注入实验以测试鲁棒性。

Result: 研究提出了扩展评估方法的关键特征，并通过实验验证了AI组件对输入变化的鲁棒性。

Conclusion: 结合新旧标准可有效评估电动汽车中AI的安全性，但仍需进一步改进标准与实践。

Abstract: Integrating Artificial Intelligence (AI) technology in electric vehicles (EV)
introduces unique challenges for safety assurance, particularly within the
framework of ISO 26262, which governs functional safety in the automotive
domain. Traditional assessment methodologies are not geared toward evaluating
AI-based functions and require evolving standards and practices. This paper
explores how an independent assessment of an AI component in an EV can be
achieved when combining ISO 26262 with the recently released ISO/PAS 8800,
whose scope is AI safety for road vehicles. The AI-driven State of Charge (SOC)
battery estimation exemplifies the process. Key features relevant to the
independent assessment of this extended evaluation approach are identified. As
part of the evaluation, robustness testing of the AI component is conducted
using fault injection experiments, wherein perturbed sensor inputs are
systematically introduced to assess the component's resilience to input
variance.

</details>


### [4] [VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing Large Language Model Vulnerability Repair Capabilities](https://arxiv.org/abs/2509.03331)
*Weizhe Wang,Wei Ma,Qiang Hu,Yao Zhang,Jianfei Sun,Bin Wu,Yang Liu,Guangquan Xu,Lingxiao Jiang*

Main category: cs.SE

TL;DR: 论文提出了VulnRepairEval框架，基于功能性PoC漏洞验证评估LLMs在软件漏洞修复中的表现，发现现有方法性能被高估。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞修复评估数据集依赖表面验证而非基于漏洞利用的验证，导致性能评估过高，无法反映真实安全场景的需求。

Method: 提出VulnRepairEval框架，通过容器化评估流程和功能性PoC验证全面评估12种流行LLMs的性能。

Result: 最佳模型仅成功修复5/23漏洞实例（21.7%），表明LLMs在安全应用中的显著弱点。失败分析显示主要问题为漏洞识别不准确和补丁错误。

Conclusion: 该研究强调了基于真实漏洞利用场景的评估框架的必要性，并揭示了LLMs在安全修复中的局限性，需进一步改进。

Abstract: The adoption of Large Language Models (LLMs) for automated software
vulnerability patching has shown promising outcomes on carefully curated
evaluation sets. Nevertheless, existing datasets predominantly rely on
superficial validation methods rather than exploit-based verification, leading
to overestimated performance in security-sensitive applications. This paper
introduces VulnRepairEval, an evaluation framework anchored in functional
Proof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,
containerized evaluation pipeline that enables reproducible differential
assessment, where repair success requires the original exploit to fail
execution against the modified code. The benchmark construction involved
extensive data curation: we processed over 400 CVEs and approximately 2,500
potential sources to extract a collection of authentic vulnerability instances
(23 Python CVEs) amenable to automated testing with working PoCs. Through
VulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and
observe a significant performance deficit: even the top-performing model
successfully addresses merely 5/23 instances (about 21.7%), exposing critical
weaknesses in security-focused applications. Our failure analysis reveals that
most unsuccessful attempts stem from imprecise vulnerability identification and
patches containing syntactic or semantic errors. Enhanced prompting strategies
and multi-agent approaches yield minimal improvements, with overall
effectiveness remaining largely unaffected. This work contributes a stringent,
practical evaluation framework for LLM-driven vulnerability remediation and
underscores the necessity for assessment protocols that authentically reflect
real-world exploitation scenarios.

</details>


### [5] [The Impact of Critique on LLM-Based Model Generation from Natural Language: The Case of Activity Diagrams](https://arxiv.org/abs/2509.03463)
*Parham Khamsepour,Mark Cole,Ish Ashraf,Sandeep Puri,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 这篇论文介绍了LADEX，一种基于大型语言模型的迭代生成-评价-优化流程，用于从自然语言描述中提取活动图，并结合算法和LLM检查提升结构正确性和语义对齐。实验表明该方法显著提升了结果质量。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用大型语言模型（LLMs）自动化生成模型，解决生成模型的结构正确性和语义对齐问题。

Method: 提出LADEX，通过LLM驱动的生成-评价-优化循环，结合算法和LLM检查生成活动图。设计了五种变体研究不同检查方式的效果。

Result: 实验表明，迭代优化过程显著提升结构有效性和语义对齐；算法结构检查比纯LLM检查更有效，结合两者性能最佳，正确性和完整性分别达86.37%和88.56%。

Conclusion: LADEX通过结合算法和LLM检查，显著提升从自然语言生成活动图的质量和效率。

Abstract: Large Language Models (LLMs) show strong potential for automating the
generation of models from natural-language descriptions. A common approach is
an iterative generate-critique-refine loop, where candidate models are
produced, evaluated, and updated based on detected issues. This process needs
to address: (1) structural correctness - compliance with well-formedness rules
- and (2) semantic alignment - accurate reflection of the intended meaning in
the source text. We present LADEX (LLM-based Activity Diagram Extractor), a
pipeline for deriving activity diagrams from natural-language process
descriptions using an LLM-driven critique-refine process. Structural checks in
LADEX can be performed either algorithmically or by an LLM, while alignment
checks are always performed by an LLM. We design five ablated variants of LADEX
to study: (i) the impact of the critique-refine loop itself, (ii) the role of
LLM-based semantic checks, and (iii) the comparative effectiveness of
algorithmic versus LLM-based structural checks.
  To evaluate LADEX, we compare the generated activity diagrams with
expert-created ground truths using trace-based operational semantics. This
enables automated measurement of correctness and completeness. Experiments on
two datasets indicate that: (1) the critique-refine loop improves structural
validity, correctness, and completeness compared to single-pass generation; (2)
algorithmic structural checks eliminate inconsistencies that LLM-based checks
fail to detect, improving correctness by an average of 17.81% and completeness
by 13.24% over LLM-only checks; and (3) combining algorithmic structural checks
with LLM-based semantic checks, implemented using the reasoning-focused O4
Mini, achieves the best overall performance - yielding average correctness of
up to 86.37% and average completeness of up to 88.56% - while requiring fewer
than five LLM calls on average.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [6] [Semantically Reflected Programs](https://arxiv.org/abs/2509.03318)
*Eduard Kamburjan,Vidar Norstein Klungre,Yuanwei Qu,Rudolf Schlatte,Egor V. Kostylev,Martin Giese,Einar Broch Johnsen*

Main category: cs.PL

TL;DR: 该论文通过语义提升程序解决了结构知识与行为知识形式化之间的二分问题，探索程序与知识图谱之间的直观联系。


<details>
  <summary>Details</summary>
Motivation: 知识图谱和本体论在表示系统个体和通用形式化知识方面非常有用，而编程语言则描述系统演变。为了解决这种二分问题，论文引入了语义提升技术。

Method: 提出一种面向对象编程语言的语义提升方法，将程序执行状态转换为知识图谱，并将其作为语义反射层在编程语言中暴露。

Result: 论文形式化了语义提升和语义反射，并通过SMOL语言的案例研究展示了其应用。

Conclusion: 该方法成功弥合了结构知识与行为知识形式化之间的鸿沟，并展示了在地质建模等领域的应用潜力。

Abstract: This paper addresses the dichotomy between the formalization of structural
and the formalization of behavioral knowledge by means of semantically lifted
programs, which explore an intuitive connection between programs and knowledge
graphs. While knowledge graphs and ontologies are eminently useful to represent
formal knowledge about a system's individuals and universals, programming
languages are designed to describe the system's evolution. To address this
dichotomy, we introduce a semantic lifting of the program states of an
executing program into a knowledge graph, for an object-oriented programming
language. The resulting graph is exposed as a semantic reflection layer within
the programming language, allowing programmers to leverage knowledge of the
application domain in their programs. In this paper, we formalize semantic
lifting and semantic reflection for a small programming language, SMOL, explain
the operational aspects of the language, and consider type correctness and
virtualisation for runtime program queries through the semantic reflection
layer. We illustrate semantic lifting and semantic reflection through a case
study of geological modelling and discuss different applications of the
technique. The language implementation is open source and available online.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [7] [Safe Sharing of Fast Kernel-Bypass I/O Among Nontrusting Applications](https://arxiv.org/abs/2509.02899)
*Alan Beadle,Michael L. Scott,John Criswell*

Main category: cs.OS

TL;DR: 本文解决了受保护用户级库设计的未解决问题，提出优化方法，降低了延迟并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何让互不信任的应用安全共享内核旁路服务，提升性能与安全性。

Method: 通过限制库函数执行时间、防止缓冲区攻击、动态任务分配等方法优化设计。

Result: 原型DDS服务实现了更低延迟和更高吞吐量，性能优于商业实现。

Conclusion: 扩展了受保护库模型，为未来系统开发提供了安全与性能准则。

Abstract: Protected user-level libraries have been proposed as a way to allow mutually
distrusting applications to safely share kernel-bypass services. In this paper,
we identify and solve several previously unaddressed obstacles to realizing
this design and identify several optimization opportunities. First, to preserve
the kernel's ability to reclaim failed processes, protected library functions
must complete in modest, bounded time. We show how to move unbounded waits
outside the library itself, enabling synchronous interaction among processes
without the need for polling. Second, we show how the bounded time requirement
can be leveraged to achieve lower and more stable latency for inter-process
interactions. Third, we observe that prior work on protected libraries is
vulnerable to a buffer unmapping attack; we prevent this attack by preventing
applications from removing pages that they share with the protected library.
Fourth, we show how a trusted daemon can respond to asynchronous events and
dynamically divide work with application threads in a protected library.
  By extending and improving the protected library model, our work provides a
new way to structure OS services, combining the advantages of kernel bypass and
microkernels. We present a set of safety and performance guidelines for
developers of protected libraries, and a set of recommendations for developers
of future protected library operating systems. We demonstrate the convenience
and performance of our approach with a prototype version of the DDS
communication service. To the best of our knowledge, this prototype represents
the first successful sharing of a kernel-bypass NIC among mutually untrusting
applications. Relative to the commercial FastDDS implementation, we achieve
approximately 50\% lower latency and up to 7x throughput, with lower CPU
utilization.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [8] [BISCAY: Practical Radio KPI Driven Congestion Control for Mobile Networks](https://arxiv.org/abs/2509.02806)
*Jon Larrea,Tanya Shreedhar,Mahesh K. Marina*

Main category: cs.NI

TL;DR: 论文提出了一种名为Biscay的基于实时无线电KPI的拥塞控制系统设计，用于优化移动网络应用性能，显著降低延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 移动应用性能受限于底层传输的拥塞控制设计，而现有方法难以应对蜂窝链路带宽的快速波动。通过利用设备芯片组的无线电KPI（关键性能指标），可以更精准、及时地测量带宽，从而优化拥塞控制。

Method: 提出Biscay系统，结合OpenDiag工具实时提取无线电KPI，并基于KPI动态调整拥塞窗口，以最小化延迟并充分利用可用带宽。

Result: 实验表明，Biscay在不同4G和5G场景中大幅降低了平均和尾部延迟（通常减少90%以上），同时吞吐量表现优于或持平现有方案（如BBR和CUBIC）。OpenDiag工具的KPI测量粒度比现有方案提升了100%。

Conclusion: Biscay是一种实用且可部署的解决方案，通过实时无线电KPI优化拥塞控制，显著提升了移动应用的网络性能。

Abstract: Mobile application performance relies heavily on the congestion control
design of the underlying transport, which is typically bottlenecked by cellular
link and has to cope with rapid cellular link bandwidth fluctuations. We
observe that radio KPI measurements from the mobile device chipset can be
exploited for precise and timely measurement of available bandwidth on the
cellular link. Building on this insight, we propose Biscay, a practical and
radio KPI-driven congestion control system design for mobile networks. Biscay
leverages OpenDiag, the in-kernel real-time radio KPI extraction tool we
introduce in this paper, along with our KPI-based accurate bandwidth
determination layer towards dynamically adjusting the congestion window to
optimally use the available bandwidth while keeping delay to the minimum. Our
solution is practical and deployable, as shown through our implementation of
Biscay and OpenDiag on unrooted Android 5G phones. We extensively evaluate
Biscay against different state-of-the-art congestion control designs including
BBR and CUBIC with emulations driven by real measurement traces as well as
real-world experiments spanning diverse 4G and 5G scenarios, and show that it
provides significant average and tail delay improvements (typically over 90%
reduction) while yielding better or similar throughput. These gains are enabled
by 100% improvement in the granularity of on-device radio KPI measurements with
OpenDiag compared to existing alternatives like MobileInsight.

</details>


### [9] [Performance Evaluation of LoRa for IoT Applications in Non-Terrestrial Networks via ns-3](https://arxiv.org/abs/2509.02811)
*Alessandro Traspadini,Michele Zorzi,Marco Giordani*

Main category: cs.NI

TL;DR: 探讨了通过LEO卫星网关利用LoRa技术支持大规模物联网连接的可行性和性能，开发了新仿真模块并验证了其有效性，但需优化网络以减少碰撞概率。


<details>
  <summary>Details</summary>
Motivation: 在基础设施有限的偏远地区，将物联网与NTN结合以提供连接是关键需求，而LoRa因其高效能和灵活性具有潜力。

Method: 开发了ns3-LoRa-NTN仿真模块，整合并扩展了ns3-LoRa和ns3-NTN模块，进行端到端卫星通信仿真。

Result: LoRa能有效支持地面与LEO卫星的直接通信，但在长距离使用相同SF时需优化以减少碰撞概率。

Conclusion: LoRa适用于NTN环境，但需进一步优化网络配置以提高性能。

Abstract: The integration of Internet of Things (IoT) and Non-Terrestrial Networks
(NTNs) has emerged as a key paradigm to provide connectivity for sensors and
actuators via satellite gateways in remote areas where terrestrial
infrastructure is limited or unavailable. Among other Low-Power Wide-Area
Network (LPWAN) technologies for IoT, Long Range (LoRa) holds great potential
given its long range, energy efficiency, and flexibility. In this paper, we
explore the feasibility and performance of LoRa to support large-scale IoT
connectivity through Low Earth Orbit (LEO) satellite gateways. To do so, we
developed a new ns3-LoRa-NTN simulation module, which integrates and extends
the ns3-LoRa and ns3-NTN modules, to enable full-stack end-to-end simulation of
satellite communication in LoRa networks. Our results, given in terms of
average data rate and Packet Reception Ratio (PRR), confirm that LoRa can
effectively support direct communication from the ground to LEO satellites, but
network optimization is required to mitigate collision probability when end
nodes use the same Spreading Factors (SFs) over long distances.

</details>


### [10] [GPS Spoofing Attacks on Automated Frequency Coordination System in Wi-Fi 6E and Beyond](https://arxiv.org/abs/2509.02824)
*Yilu Dong,Tianchang Yang,Arupjyoti Bhuyan,Syed Rafiul Hussain*

Main category: cs.NI

TL;DR: 研究发现Wi-Fi 6E/7的GPS定位报告可被低成本设备伪造，导致频谱干扰或AP禁用，暴露了AFC系统的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 6 GHz频段的无序使用可能干扰关键任务系统，需AFC系统协调，但GPS定位报告的安全性问题未被充分研究。

Method: 通过低成本无线电设备伪造Wi-Fi AP的GPS定位，验证对商业AP和AFC系统的攻击效果。

Result: 实验室测试显示伪造定位可操控AP行为，引发干扰或禁用，暴露AFC系统的安全缺陷。

Conclusion: 研究揭示了AFC安全假设的漏洞，呼吁加强定位完整性保护措施。

Abstract: The 6 GHz spectrum, recently opened for unlicensed use under Wi-Fi 6E and
Wi-Fi 7, overlaps with frequencies used by mission-critical incumbent systems
such as public safety communications and utility infrastructure. To prevent
interference, the FCC mandates the use of Automated Frequency Coordination
(AFC) systems, which assign safe frequency and power levels based on Wi-Fi
Access Point (AP)-reported locations. In this work, we demonstrate that
GPS-based location reporting, which Wi-Fi APs use, can be spoofed using
inexpensive, off-the-shelf radio equipment. This enables attackers to
manipulate AP behavior, gain unauthorized spectrum access, cause harmful
interference, or disable APs entirely by spoofing them into foreign locations.
We validate these attacks in a controlled lab setting against a commercial AP
and evaluate a commercial AFC system under spoofed scenarios. Our findings
highlight critical gaps in the security assumptions of AFC and motivate the
need for stronger location integrity protections.

</details>


### [11] [Closing the Visibility Gap: A Monitoring Framework for Verifiable Open RAN Operations](https://arxiv.org/abs/2509.03000)
*Hexuan Yu,Md Mohaimin Al Barat,Yang Xiao,Y. Thomas Hou,Wenjing Lou*

Main category: cs.NI

TL;DR: 论文提出了一种针对低信任度O-RAN环境的监控框架，用于主动验证配置状态与控制行为，以减少因误配或组件被入侵导致的安全风险。


<details>
  <summary>Details</summary>
Motivation: O-RAN的开放性和跨厂商互操作性带来了新的安全挑战，尤其是多运营商共享组件时，现有零信任架构假设认证组件会遵守政策，但误配或被入侵的组件可能无声违反政策。

Method: 提出了一个监控框架，通过主动验证配置状态和控制行为，确保其符合租户定义的政策。

Result: 实现了框架并以标准化O-RAN配置进行测试，总处理延迟约200毫秒，证明了其在多运营商部署中的高效性和实用性。

Conclusion: 该框架可增强O-RAN操作的透明度和信任，适用于多运营商环境中的政策执行和合规审计。

Abstract: Open Radio Access Network (Open RAN) is reshaping mobile network architecture
by promoting openness, disaggregation, and cross-vendor interoperability.
However, this architectural flexibility introduces new security challenges,
especially in deployments where multiple mobile network operators (MNOs)
jointly operate shared components. Existing Zero Trust Architectures (ZTA) in
O-RAN, as defined by governmental and industry standards, implicitly assume
that authenticated components will comply with operational policies. However,
this assumption creates a critical blind spot: misconfigured or compromised
components can silently violate policies, misuse resources, or corrupt
downstream processes (e.g., ML-based RIC xApps).
  To address this critical gap, we propose a monitoring framework for low-trust
O-RAN environments that proactively verifies configuration state and control
behavior against tenant-defined policies. Our system provides scalable,
verifiable oversight to enhance transparency and trust in O-RAN operations. We
implement and evaluate the framework using standardized O-RAN configurations,
with total processing latency of approximately 200 ms, demonstrating its
efficiency and practicality for timely policy enforcement and compliance
auditing in multi-MNO deployments.

</details>


### [12] [Multi-layer Digital Twin System for Future Mobile Metaverse](https://arxiv.org/abs/2509.03049)
*Gaosheng Zhao,Dong In Kim*

Main category: cs.NI

TL;DR: 本文提出了一个多层数字孪生（DT）系统，用于未来6G时代通信网络的主动适应。


<details>
  <summary>Details</summary>
Motivation: 解决6G时代通信网络的复杂性和动态性挑战，利用DT技术实现网络从被动响应到主动适应的转变。

Method: 设计了一个协调本地DT、边缘DT和云端DT的多层DT系统。

Result: 该系统可实现实时数据驱动的决策和数字代理功能，并以分布式、分层的方式运行，同时为元宇宙应用提供支持。

Conclusion: 多层DT系统在6G时代具有重要潜力，能够支持网络的主动适应和元宇宙服务的发展。

Abstract: In the upcoming 6G era, the communication networks are expected to face
unprecedented challenges in terms of complexity and dynamics. Digital Twin (DT)
technology, with its various digital capabilities, holds great potential to
facilitate the transformation of the communication network from passive
responding to proactive adaptation. Thus, in this paper, we propose a
multi-layer DT system that coordinates local DT, edge DT, and cloud DT for
future network architecture and functions. In our vision, the proposed DT
system will not only achieve real-time data-driven decision-making and digital
agent functions previously handled by centralized DT, but will do so in a more
distributed, mobile, layer-by-layer manner. Moreover, it will supply essential
data, pre-trained models, and open interfaces for future metaverse
applications, enabling creators and users to efficiently develop and experience
metaverse services.

</details>


### [13] [Machine Learning-Driven Anomaly Detection for 5G O-RAN Performance Metrics](https://arxiv.org/abs/2509.03290)
*Babak Azkaei,Kishor Chandra Joshi,George Exarchakos*

Main category: cs.NI

TL;DR: 本文提出了两种异常检测算法，用于主动检测UE吞吐量下降和减少切换失败，以优化网络性能并实现6G自愈网络。


<details>
  <summary>Details</summary>
Motivation: 随着关键服务对网络基础设施的依赖增加，以及5G/6G网络的操作复杂性提升，主动和自动化的网络故障管理变得尤为重要。O-RAN规范和AI/ML的集成为实现主动网络健康监测和异常检测提供了新的可能性。

Method: 提出了两种异常检测算法：第一种通过分析资源块利用率和信号质量等KPI指标，识别可能面临吞吐量下降风险的UE；第二种通过评估相邻小区无线覆盖质量，过滤掉信号强度或干扰异常的候选小区。

Result: 第二种算法平均减少了41.27%的切换候选目标，两种算法共同显著降低了切换失败和吞吐量下降，且运行速度远快于近实时延迟要求。

Conclusion: 这些方法为自愈6G网络的实现铺平了道路。

Abstract: The ever-increasing reliance of critical services on network infrastructure
coupled with the increased operational complexity of beyond-5G/6G networks
necessitate the need for proactive and automated network fault management. The
provision for open interfaces among different radio access network\,(RAN)
elements and the integration of AI/ML into network architecture enabled by the
Open RAN\,(O-RAN) specifications bring new possibilities for active network
health monitoring and anomaly detection. In this paper we leverage these
advantages and develop an anomaly detection framework that proactively detect
the possible throughput drops for a UE and minimize the post-handover failures.
We propose two actionable anomaly detection algorithms tailored for real-world
deployment. The first algorithm identifies user equipment (UE) at risk of
severe throughput degradation by analyzing key performance indicators (KPIs)
such as resource block utilization and signal quality metrics, enabling
proactive handover initiation. The second algorithm evaluates neighbor cell
radio coverage quality, filtering out cells with anomalous signal strength or
interference levels. This reduces candidate targets for handover by 41.27\% on
average. Together, these methods mitigate post-handover failures and throughput
drops while operating much faster than the near-real-time latency constraints.
This paves the way for self-healing 6G networks.

</details>


### [14] [Dependency Chain Analysis of ROS 2 DDS QoS Policies: From Lifecycle Tutorial to Static Verification](https://arxiv.org/abs/2509.03381)
*Sanghoon Lee,Junha Kang,Kyung-Joon Park*

Main category: cs.NI

TL;DR: ROS 2用户缺乏关于DDS QoS策略组合和验证的明确指导，导致运行时问题。论文提出了分阶段分析QoS策略的方法、依赖关系链和静态验证工具QoS Guard，以提高系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决ROS 2用户在使用DDS QoS策略时因缺乏指导而导致的调试困难和运行时失败问题。

Method: 1. 分析DDS通信的生命周期（发现、数据交换和解关联）；2. 提出QoS依赖链和41种依赖违规规则；3. 开发静态验证工具QoS Guard。

Result: 提供了QoS策略的概念指导和实用工具，能早期发现配置错误，提升ROS 2系统的可靠性和资源效率。

Conclusion: 通过理论分析和工具支持，论文帮助用户安全配置QoS策略，减少运行时问题。

Abstract: Robot Operating System 2 (ROS 2) relies on the Data Distribution Service
(DDS), which offers more than 20 Quality of Service (QoS) policies governing
availability, reliability, and resource usage. Yet ROS 2 users lack clear
guidance on safe policy combinations and validation processes prior to
deployment, which often leads to trial-and-error tuning and unexpected runtime
failures. To address these challenges, we analyze DDS Publisher-Subscriber
communication over a life cycle divided into Discovery, Data Exchange, and
Disassociation, and provide a user oriented tutorial explaining how 16 QoS
policies operate in each phase. Building on this analysis, we derive a QoS
dependency chain that formalizes inter-policy relationships and classifies 41
dependency violation rules, capturing constraints that commonly cause
communication failures in practice. Finally, we introduce QoS Guard, a ROS 2
package that statically validates DDS XML profiles offline, flags conflicts,
and enables safe, predeployment tuning without establishing a live ROS 2
session. Together, these contributions give ROS 2 users both conceptual insight
and a concrete tool that enables early detection of misconfigurations,
improving the reliability and resource efficiency of ROS 2 based robotic
systems.

</details>


### [15] [Hierarchical Low-Altitude Wireless Network Empowered Air Traffic Management](https://arxiv.org/abs/2509.03386)
*Ziye Jia,Jia He,Yuanhao Cui,Qiuming Zhu,Ligang Yuan,Fuhui Zhou,Qihui Wu,Dusit Niyato,Zhu Han*

Main category: cs.NI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As the increasing development of low-altitude aircrafts, the rational design
of low-altitude networks directly impacts the aerial safety and resource
utilization. To address the challenges of environmental complexity and aircraft
diversity in the traffic management, we propose a hierarchical low-altitude
wireless network (HLWN) framework. Empowered by the threedimensional spatial
discretization and integrated wireless monitoring mechanisms in HLWN, we design
low-altitude air corridors to guarantee safe operation and optimization.
Besides, we develop the multi-dimensional flight risk assessment through
conflict detection and probabilistic collision analysis, facilitating dynamic
collision avoidance for heterogeneous aircrafts. Finally, the open issues and
future directions are investigated to provide insights into HLAN development.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [16] [Simulacra Naturae: Generative Ecosystem driven by Agent-Based Simulations and Brain Organoid Collective Intelligence](https://arxiv.org/abs/2509.02924)
*Nefeli Manoudaki,Mert Toka,Iason Paterakis,Diarmid Flatley*

Main category: cs.MM

TL;DR: 论文摘要介绍了一个名为‘Simulacra Naturae’的数据驱动媒体装置，通过生物计算、材料生态和生成系统的结合探索集体关怀。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过非人类认知（如脑类器官的神经活动）与生成系统的结合，重新定义数据可视化为一种关怀实践，并推动伦理、共情和生态调谐。

Method: 利用预记录的脑类器官神经活动，通过实时系统转化为多感官环境（生成视觉、空间音频、活体植物和粘土艺术品），模拟自然系统的行为。

Result: 创造了一个由非人类认知塑造的多感官环境，展示了生物信号与生成系统的协同创新潜力。

Conclusion: 该装置通过将抽象数据与生命材料和体验结合，重新定义了可视化作为关怀实践的可能性，并为混合计算系统中的伦理和共情开辟了新空间。

Abstract: Simulacra Naturae is a data-driven media installation that explores
collective care through the entanglement of biological computation, material
ecologies, and generative systems. The work translates pre-recorded neural
activity from brain organoids, lab-grown three-dimensional clusters of neurons,
into a multi-sensory environment composed of generative visuals, spatial audio,
living plants, and fabricated clay artifacts. These biosignals, streamed
through a real-time system, modulate emergent agent behaviors inspired by
natural systems such as termite colonies and slime molds. Rather than using
biosignals as direct control inputs, Simulacra Naturae treats organoid activity
as a co-creative force, allowing neural rhythms to guide the growth, form, and
atmosphere of a generative ecosystem. The installation features computationally
fabricated clay prints embedded with solenoids, adding physical sound
resonances to the generative surround composition. The spatial environment,
filled with live tropical plants and a floor-level projection layer featuring
real-time generative AI visuals, invites participants into a sensory field
shaped by nonhuman cognition. By grounding abstract data in living materials
and embodied experience, Simulacra Naturae reimagines visualization as a
practice of care, one that decentralizes human agency and opens new spaces for
ethics, empathy, and ecological attunement within hybrid computational systems.

</details>


### [17] [Automatically Generating High-Precision Simulated Road Networking in Traffic Scenario](https://arxiv.org/abs/2509.02990)
*Liang Xie,Wenke Huang*

Main category: cs.MM

TL;DR: 提出了一种自动生成高精度车道级仿真路网的方法，通过街景数据和深度学习技术降低成本和人力需求。


<details>
  <summary>Details</summary>
Motivation: 现有车道级仿真路网生成方法数据收集和人工编辑成本高，效率低。

Method: 利用开源街景数据构建车道线数据集，基于深度学习的车道线检测模型提取信息，结合坐标变换和地图匹配算法生成高精度路网。

Result: 显著降低了数据收集和人工编辑成本，提高了生成效率和精度。

Conclusion: 为城市交通仿真和智能交通系统提供了高效、自动化的解决方案。

Abstract: Existing lane-level simulation road network generation is labor-intensive,
resource-demanding, and costly due to the need for large-scale data collection
and manual post-editing. To overcome these limitations, we propose
automatically generating high-precision simulated road networks in traffic
scenario, an efficient and fully automated solution. Initially, real-world road
street view data is collected through open-source street view map platforms,
and a large-scale street view lane line dataset is constructed to provide a
robust foundation for subsequent analysis. Next, an end-to-end lane line
detection approach based on deep learning is designed, where a neural network
model is trained to accurately detect the number and spatial distribution of
lane lines in street view images, enabling automated extraction of lane
information. Subsequently, by integrating coordinate transformation and map
matching algorithms, the extracted lane information from street views is fused
with the foundational road topology obtained from open-source map service
platforms, resulting in the generation of a high-precision lane-level
simulation road network. This method significantly reduces the costs associated
with data collection and manual editing while enhancing the efficiency and
accuracy of simulation road network generation. It provides reliable data
support for urban traffic simulation, autonomous driving navigation, and the
development of intelligent transportation systems, offering a novel technical
pathway for the automated modeling of large-scale urban road networks.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [Lattice Annotated Temporal (LAT) Logic for Non-Markovian Reasoning](https://arxiv.org/abs/2509.02958)
*Kaustuv Mukherji,Jaikrishna Manojkumar Patil,Dyuman Aditya,Paulo Shakarian,Devendra Parkar,Lahari Pokala,Clark Dorman,Gerardo I. Simari*

Main category: cs.LO

TL;DR: 论文提出了一种扩展的LAT逻辑，结合时间推理和开放世界语义，展示出高效性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 为了解决动态和不确定环境中的开放世界时间推理问题，论文扩展了GAPs逻辑，引入时间推理和开放世界语义。

Method: 通过下格结构实现开放世界语义，并结合时间逻辑编程，提出LAT逻辑，同时在理论和实现层面验证其高效性和扩展性。

Result: 实验证明LAT逻辑在性能和内存上有显著提升，同时在强化学习环境中表现优异。

Conclusion: LAT逻辑是一个统一且可扩展的框架，适用于动态和不确定环境中的开放世界时间推理。

Abstract: We introduce Lattice Annotated Temporal (LAT) Logic, an extension of
Generalized Annotated Logic Programs (GAPs) that incorporates temporal
reasoning and supports open-world semantics through the use of a lower lattice
structure. This logic combines an efficient deduction process with temporal
logic programming to support non-Markovian relationships and open-world
reasoning capabilities. The open-world aspect, a by-product of the use of the
lower-lattice annotation structure, allows for efficient grounding through a
Skolemization process, even in domains with infinite or highly diverse
constants.
  We provide a suite of theoretical results that bound the computational
complexity of the grounding process, in addition to showing that many of the
results on GAPs (using an upper lattice) still hold with the lower lattice and
temporal extensions (though different proof techniques are required). Our
open-source implementation, PyReason, features modular design, machine-level
optimizations, and direct integration with reinforcement learning environments.
Empirical evaluations across multi-agent simulations and knowledge graph tasks
demonstrate up to three orders of magnitude speedup and up to five orders of
magnitude memory reduction while maintaining or improving task performance.
Additionally, we evaluate LAT Logic's value in reinforcement learning
environments as a non-Markovian simulator, achieving up to three orders of
magnitude faster simulation with improved agent performance, including a 26%
increase in win rate due to capturing richer temporal dependencies. These
results highlight LAT Logic's potential as a unified, extensible framework for
open-world temporal reasoning in dynamic and uncertain environments. Our
implementation is available at: pyreason.syracuse.edu.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [19] [EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared Shadow Casting](https://arxiv.org/abs/2509.03430)
*Vimal Mollyn,Nathan DeVrio,Chris Harrison*

Main category: cs.HC

TL;DR: 提出了一种新型头戴式技术，通过红外发射器和摄像头检测日常表面的触摸事件，精度高且适应多种条件。


<details>
  <summary>Details</summary>
Motivation: 实现无需仪器辅助的日常表面触摸检测，提升混合现实系统的交互性能。

Method: 利用头戴设备中的摄像头和红外发射器生成结构化阴影，通过阴影分析估计悬停距离和触摸接触。

Result: 悬停距离的平均误差为6.9毫米，触摸检测准确率达98%。

Conclusion: 该方法在多种条件下均表现良好，为混合现实交互提供了实用解决方案。

Abstract: The ability to detect touch events on uninstrumented, everyday surfaces has
been a long-standing goal for mixed reality systems. Prior work has shown that
virtual interfaces bound to physical surfaces offer performance and ergonomic
benefits over tapping at interfaces floating in the air. A wide variety of
approaches have been previously developed, to which we contribute a new
headset-integrated technique called \systemname. We use a combination of a
computer-triggered camera and one or more infrared emitters to create
structured shadows, from which we can accurately estimate hover distance (mean
error of 6.9~mm) and touch contact (98.0\% accuracy). We discuss how our
technique works across a range of conditions, including surface material,
interaction orientation, and environmental lighting.

</details>


### [20] [STRive: An association rule-based system for the exploration of spatiotemporal categorical data](https://arxiv.org/abs/2509.02732)
*Mauro Diaz,Luis Sante,Joel Perca,João Victor da Silva,Nivan Ferreira,Jorge Poco*

Main category: cs.HC

TL;DR: STRive是一个结合关联规则挖掘和可视化分析的系统，用于发现和理解时空数据中的模式。


<details>
  <summary>Details</summary>
Motivation: 现有工具很少同时支持时空分析和属性关系挖掘，因此需要一种综合方法。

Method: STRive通过关联规则挖掘生成规则，并进行规则聚类和交互式可视化。

Result: 两个真实案例验证了STRive在复杂时空数据中发现可解释模式的能力。

Conclusion: STRive为时空数据分析提供了有效的可视化和交互工具。

Abstract: Effectively analyzing spatiotemporal data plays a central role in
understanding real-world phenomena and informing decision-making. Capturing the
interaction between spatial and temporal dimensions also helps explain the
underlying structure of the data. However, most datasets do not reveal
attribute relationships, requiring additional algorithms to extract meaningful
patterns. Existing visualization tools often focus either on attribute
relationships or spatiotemporal analysis, but rarely support both
simultaneously. In this paper, we present STRive (SpatioTemporal Rule
Interactive Visual Explorer), a visual analytics system that enables users to
uncover and explore spatial and temporal patterns in data. At the core of
STRive lies Association Rule Mining (ARM), which we apply to spatiotemporal
datasets to generate interpretable and actionable insights. We combine ARM with
multiple interactive mechanisms to analyze the extracted relationships.
Association rules serve as interpretable guidance mechanisms for visual
analytics by highlighting the meaningful aspects of the data that users should
investigate. Our methodology includes three key steps: rule generation, rule
clustering, and interactive visualization. STRive offers two modes of analysis.
The first operates at the rule cluster level and includes four coordinated
views, each showing a different facet of a cluster, including its temporal and
spatial behavior. The second mode mirrors the first but focuses on individual
rules within a selected cluster. We evaluate the effectiveness of STRive
through two case studies involving real-world datasets -- fatal vehicle
accidents and urban crime. Results demonstrate the system's ability to support
the discovery and analysis of interpretable patterns in complex spatiotemporal
contexts.

</details>


### [21] [SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using UWB and IMU Data](https://arxiv.org/abs/2509.03451)
*Nathan DeVrio,Vimal Mollyn,Chris Harrison*

Main category: cs.HC

TL;DR: 利用智能手机和智能手表结合超宽带（UWB）功能，无需训练数据即可估计用户手臂姿势，位置误差中位数为11.0厘米。


<details>
  <summary>Details</summary>
Motivation: 现有系统需摄像头或多传感器，存在隐私问题或不便使用，本研究旨在提供一种低成本、易用的解决方案。

Method: 通过智能手机和智能手表的UWB功能测量绝对距离，结合惯性数据避免漂移问题。

Result: 提出的软件方案无需训练数据，手腕和肘部位置估计的中位误差为11.0厘米。

Conclusion: 该方法证明了利用消费级设备实现高精度手臂姿势跟踪的可行性。

Abstract: The ability to track a user's arm pose could be valuable in a wide range of
applications, including fitness, rehabilitation, augmented reality input, life
logging, and context-aware assistants. Unfortunately, this capability is not
readily available to consumers. Systems either require cameras, which carry
privacy issues, or utilize multiple worn IMUs or markers. In this work, we
describe how an off-the-shelf smartphone and smartwatch can work together to
accurately estimate arm pose. Moving beyond prior work, we take advantage of
more recent ultra-wideband (UWB) functionality on these devices to capture
absolute distance between the two devices. This measurement is the perfect
complement to inertial data, which is relative and suffers from drift. We
quantify the performance of our software-only approach using off-the-shelf
devices, showing it can estimate the wrist and elbow joints with a \hl{median
positional error of 11.0~cm}, without the user having to provide training data.

</details>


### [22] [Designing a Lightweight GenAI Interface for Visual Data Analysis](https://arxiv.org/abs/2509.02878)
*Ratanond Koonchanok,Alex Kale,Khairi Reda*

Main category: cs.HC

TL;DR: 本文介绍了一种结合生成式AI和可视化分析的混合系统，旨在解决传统LLM依赖带来的问题，提升数据分析的透明度和用户控制。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型（LLM）的数据分析系统存在幻觉、不透明推理和用户控制不足的问题，作者希望通过结合生成式AI和可视化技术改进这一问题。

Method: 系统采用混合方法：生成式AI负责将自然语言意图转化为统计模型，而交互式可视化展示模型行为、残差模式和假设比较；统计建模和诊断完全由R后端处理。

Result: 该方法在提升数据分析可访问性的同时保持了严谨性，并通过案例展示了其有效性。

Conclusion: 结合生成式AI和可视化分析的系统在扩展建模工具使用的同时，确保了透明度和用户控制，为未来研究提供了新方向。

Abstract: Recent advances in Generative AI have transformed how users interact with
data analysis through natural language interfaces. However, many systems rely
too heavily on LLMs, creating risks of hallucination, opaque reasoning, and
reduced user control. We present a hybrid visual analysis system that
integrates GenAI in a constrained, high-level role to support statistical
modeling while preserving transparency and user agency. GenAI translates
natural language intent into formal statistical formulations, while interactive
visualizations surface model behavior, residual patterns, and hypothesis
comparisons to guide iterative exploration. Model fitting, diagnostics, and
hypothesis testing are delegated entirely to a structured R-based backend,
ensuring correctness, interpretability, and reproducibility. By combining
GenAI-assisted intent translation with visualization-driven reasoning, our
approach broadens access to modeling tools without compromising rigor. We
present an example use case of the tool and discuss challenges and
opportunities for future research.

</details>


### [23] [The Basic B*** Effect: The Use of LLM-based Agents Reduces the Distinctiveness and Diversity of People's Choices](https://arxiv.org/abs/2509.02910)
*Sandra C. Matz,C. Blaine Horton,Sofie Goethals*

Main category: cs.HC

TL;DR: 研究表明，大型语言模型（LLM）代理会减少人们在选择和偏好上的独特性与多样性，尤其是通用代理，而个性化代理虽减轻同质化但限制了探索广度。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM代理如何影响人们在身份定义选择上的独特性（人际）和多样性（个人），揭示潜在的同质化风险。

Method: 基于1000名美国用户的11万条社交媒体行为数据，对比通用代理、个性化代理与人类基准的表现。

Result: 代理（尤其是通用型）会引导用户选择更流行的选项，降低独特性；个性化代理虽减轻同质化但减少了探索多样性。

Conclusion: 设计AI系统需权衡独特性与多样性，以增强而非限制人类能动性，并保护思想、品位和表达的多样性。

Abstract: Large language models (LLMs) increasingly act on people's behalf: they write
emails, buy groceries, and book restaurants. While the outsourcing of human
decision-making to AI can be both efficient and effective, it raises a
fundamental question: how does delegating identity-defining choices to AI
reshape who people become? We study the impact of agentic LLMs on two
identity-relevant outcomes: interpersonal distinctiveness - how unique a
person's choices are relative to others - and intrapersonal diversity - the
breadth of a single person's choices over time. Using real choices drawn from
social-media behavior of 1,000 U.S. users (110,000 choices in total), we
compare a generic and personalized agent to a human baseline. Both agents shift
people's choices toward more popular options, reducing the distinctiveness of
their behaviors and preferences. While the use of personalized agents tempers
this homogenization (compared to the generic AI), it also more strongly
compresses the diversity of people's preference portfolios by narrowing what
they explore across topics and psychological affinities. Understanding how AI
agents might flatten human experience, and how using generic versus
personalized agents involves distinctiveness-diversity trade-offs, is critical
for designing systems that augment rather than constrain human agency, and for
safeguarding diversity in thought, taste, and expression.

</details>


### [24] [Demonstrating Visual Information Manipulation Attacks in Augmented Reality: A Hands-On Miniature City-Based Setup](https://arxiv.org/abs/2509.02933)
*Yanming Xiu,Maria Gorlatova*

Main category: cs.HC

TL;DR: 论文摘要探讨了增强现实（AR）中的视觉信息操纵（VIM）攻击，如何通过修改视觉线索误导用户，并通过一个微型城市设置的实际演示展示了其影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于AR技术虽然提升了用户与现实世界的互动，但也引入了VIM攻击等安全漏洞，可能对用户决策产生负面影响。

Method: 研究者通过Meta Quest 3设备，设计了一个微型城市设置的演示，让用户体验被操纵的AR内容，以此评估VIM攻击的效果。

Result: 演示显示了VIM攻击能显著干扰用户决策，突显了在AR系统中实施有效安全措施的必要性。

Conclusion: 未来工作包括用户研究和跨平台测试，以进一步验证安全措施的可行性。

Abstract: Augmented reality (AR) enhances user interaction with the real world but also
presents vulnerabilities, particularly through Visual Information Manipulation
(VIM) attacks. These attacks alter important real-world visual cues, leading to
user confusion and misdirected actions. In this demo, we present a hands-on
experience using a miniature city setup, where users interact with manipulated
AR content via the Meta Quest 3. The demo highlights the impact of VIM attacks
on user decision-making and underscores the need for effective security
measures in AR systems. Future work includes a user study and cross-platform
testing.

</details>


### [25] [OPRA-Vis: Visual Analytics System to Assist Organization-Public Relationship Assessment with Large Language Models](https://arxiv.org/abs/2509.03164)
*Sangbong Yoo,Seongbum Seo,Chanyoung Yoon,Hyelim Lee,Jeong-Nam Kim,Chansoo Kim,Yun Jang,Takanori Fujiwara*

Main category: cs.HC

TL;DR: OPRA-Vis是一种可视化分析系统，利用大型语言模型（LLM）分析公众意见数据，无需大量标注数据，通过结合PR专业知识指导LLM推理，并提供可视化工具帮助用户理解和改进模型决策。


<details>
  <summary>Details</summary>
Motivation: 传统的PR分析需要大量标注数据进行模型微调，这对PR研究者来说既费力又知识密集。因此，开发一种无需大量标注数据的工具来支持组织-公众关系评估（OPRA）具有重要意义。

Method: 通过Chain-of-Thought提示方法，将PR专业知识直接融入LLM的推理过程，并结合可视化工具展示LLM的推理路径和线索。

Result: OPRA-Vis在真实用例中表现出色，定量和定性评估均证明了其有效性、可用性及专家认可度。

Conclusion: OPRA-Vis为PR分析提供了一种高效、可解释且用户友好的解决方案，推动了LLM在PR领域的应用。

Abstract: Analysis of public opinions collected from digital media helps organizations
maintain positive relationships with the public. Such public relations (PR)
analysis often involves assessing opinions, for example, measuring how strongly
people trust an organization. Pre-trained Large Language Models (LLMs) hold
great promise for supporting Organization-Public Relationship Assessment (OPRA)
because they can map unstructured public text to OPRA dimensions and articulate
rationales through prompting. However, adapting LLMs for PR analysis typically
requires fine-tuning on large labeled datasets, which is both labor-intensive
and knowledge-intensive, making it difficult for PR researchers to apply these
models. In this paper, we present OPRA-Vis, a visual analytics system that
leverages LLMs for OPRA without requiring extensive labeled data. Our framework
employs Chain-of-Thought prompting to guide LLMs in analyzing public opinion
data by incorporating PR expertise directly into the reasoning process.
Furthermore, OPRA-Vis provides visualizations that reveal the clues and
reasoning paths used by LLMs, enabling users to explore, critique, and refine
model decisions. We demonstrate the effectiveness of OPRA-Vis through two
real-world use cases and evaluate it quantitatively, through comparisons with
alternative LLMs and prompting strategies, and qualitatively, through
assessments of usability, effectiveness, and expert feedback.

</details>


### [26] [Beyond Words: Interjection Classification for Improved Human-Computer Interaction](https://arxiv.org/abs/2509.03181)
*Yaniv Goren,Yuval Cohen,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.HC

TL;DR: 论文提出了一种新的任务——插话分类，填补了人机交互中插话（如“mmm”和“hmm”）被自动语音识别忽略的空白。作者发布了一个专用于插话分类的数据集，并通过深度学习和数据增强技术显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 插话在人机交互中是表达情感或信息的重要方式，但通常被自动语音识别系统忽略。为了填补这一研究空白，作者首次提出插话分类任务。

Method: 作者收集了专用的插话数据集，利用深度学习模型进行训练和评估，并通过音高和节奏变换等数据增强技术提升模型表现。

Result: 数据增强技术显著提高了插话分类的准确率，使模型更具鲁棒性。作者还公开了数据集、基线模型和增强工具。

Conclusion: 该研究为插话分类领域奠定了基础，公开的资源将促进未来相关研究。

Abstract: In the realm of human-computer interaction, fostering a natural dialogue
between humans and machines is paramount. A key, often overlooked, component of
this dialogue is the use of interjections such as "mmm" and "hmm". Despite
their frequent use to express agreement, hesitation, or requests for
information, these interjections are typically dismissed as "non-words" by
Automatic Speech Recognition (ASR) engines. Addressing this gap, we introduce a
novel task dedicated to interjection classification, a pioneer in the field to
our knowledge. This task is challenging due to the short duration of
interjection signals and significant inter- and intra-speaker variability. In
this work, we present and publish a dataset of interjection signals collected
specifically for interjection classification. We employ this dataset to train
and evaluate a baseline deep learning model. To enhance performance, we augment
the training dataset using techniques such as tempo and pitch transformation,
which significantly improve classification accuracy, making models more robust.
The interjection dataset, a Python library for the augmentation pipeline,
baseline model, and evaluation scripts, are available to the research
community.

</details>


### [27] [Finding My Way: Influence of Different Audio Augmented Reality Navigation Cues on User Experience and Subjective Usefulness](https://arxiv.org/abs/2509.03199)
*Sina Hinzmann,Francesco Vona,Juliane Henning,Mohamed Amer,Omar Abdellatif,Tanja Kojic,Jan-Niklas Voigt-Antons*

Main category: cs.HC

TL;DR: 研究探讨了不同类型音频提示（人工声音、自然声音、Spearcons、乐器声音和听觉图标）在户外导航任务中的效果和用户体验。结果显示，人工声音和乐器声音在新颖性和刺激性上表现优于Spearcons，自然声音和人工声音最受欢迎。


<details>
  <summary>Details</summary>
Motivation: 随着AR在移动和情境感知应用中的普及，音频提示在引导用户方面的作用日益重要，因此需要研究不同音频类型的用户体验和效果。

Method: 20名参与者使用Meta Quest 3头戴设备，在户外导航任务中体验五种音频提示类型（人工声音、自然声音、Spearcons、乐器声音和听觉图标），并通过主观评价收集数据。

Result: 人工声音和乐器声音在新颖性和刺激性上评分更高；自然声音和人工声音最受青睐。

Conclusion: 在AR导航系统的听觉反馈设计中融入新颖性和用户参与度可以提升系统效果。

Abstract: As augmented reality (AR) becomes increasingly prevalent in mobile and
context-aware applications, the role of auditory cues in guiding users through
physical environments is becoming critical. This study investigates the
effectiveness and user experience of various categories of audio cues,
including fully non-verbal sounds and speech-derived Spearcons, during outdoor
navigation tasks using the Meta Quest 3 headset. Twenty participants navigated
five outdoor routes using audio-only cue types: Artificial Sounds, Nature
Sounds, Spearcons, Musical Instruments, and Auditory Icons. Subjective
evaluations were collected to assess the perceived effectiveness and user
experience of each sound type. Results revealed significant differences in
perceived novelty and stimulation across sound types. Artificial Sounds and
Musical Instruments were rated higher than Spearcons in novelty, while
Artificial Sounds were also rated higher than Spearcons in stimulation. Overall
preference was evenly split between Nature Sounds and Artificial Sounds. These
findings suggest that incorporating aspects of novelty and user engagement in
auditory feedback design may enhance the effectiveness of AR navigation
systems.

</details>


### [28] [Card Sorting with Fewer Cards and the Same Mental Models? A Re-examination of an Established Practice](https://arxiv.org/abs/2509.03232)
*Eduard Kuric,Peter Demcak,Matus Krajcovic*

Main category: cs.HC

TL;DR: 研究发现，使用60%随机卡片子集进行卡片分类可得到与标准方法相似的结果，但类别主题模式可能不同。需要更大的样本量（25-35人），且人格特质和认知反思能力会影响结果。


<details>
  <summary>Details</summary>
Motivation: 探讨随机子集对卡片分类数据的影响，填补数十年未系统研究的空白。

Method: 160名参与者参与实验，比较完整卡片集和60%随机子集的效果，分析样本量需求及人格、认知因素的影响。

Result: 随机子集生成相似性矩阵与标准方法接近，但主题模式可能不同，样本量需增加，人格与认知能力影响结果。

Conclusion: 研究为卡片分类提供了基于证据的实践建议，揭示了研究设计和个体差异对心理模型测量的影响。

Abstract: To keep card sorting with a lot of cards concise, a common strategy for
gauging mental models involves presenting participants with fewer randomly
selected cards instead of the full set. This is a decades-old practice, but its
effects lacked systematic examination. To assess how randomized subsets affect
data, we conducted an experiment with 160 participants. We compared results
between full and randomized 60\% card sets, then analyzed sample size
requirements and the impacts of individual personality and cognitive factors.
Our results demonstrate that randomized subsets can yield comparable similarity
matrices to standard card sorting, but thematic patterns in categories can
differ. Increased data variability also warrants larger sample sizes (25-35 for
60% card subset). Results indicate that personality traits and cognitive
reflection interact with card sorting. Our research suggests evidence-based
practices for conducting card sorting while exposing the influence of study
design and individual differences on measurement of mental models.

</details>


### [29] [Beyond Quantification: Navigating Uncertainty in Professional AI Systems](https://arxiv.org/abs/2509.03271)
*Sylvie Delacroix,Diana Robinson,Umang Bhatt,Jacopo Domenicucci,Jessica Montgomery,Gael Varoquaux,Carl Henrik Ek,Vincent Fortuin,Yulan He,Tom Diethe,Neill Campbell,Mennatallah El-Assady,Soren Hauberg,Ivana Dusparic,Neil Lawrence*

Main category: cs.HC

TL;DR: 论文探讨了大语言模型在专业领域中的不确定性表达问题，提出超越简单量化的方法，通过参与式细化过程让专业社区共同塑造不确定性表达方式。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗、教育和法律等领域的广泛应用，如何表达不确定性成为关键问题。许多专业领域的不确定性无法简单量化，需要更丰富的表达方式。

Method: 作者提出参与式细化过程，让专业社区共同开发不确定性表达方法，强调这是一个集体意义构建过程而非算法优化问题。

Result: 论文认为不确定性表达是专业意义构建的一部分，需要社区协作而非技术优化，从而更好地整合AI到专业领域。

Conclusion: 研究呼吁超越量化，通过集体开发的方式实现更有效的不确定性表达，促进AI在专业领域的益处最大化。

Abstract: The growing integration of large language models across professional domains
transforms how experts make critical decisions in healthcare, education, and
law. While significant research effort focuses on getting these systems to
communicate their outputs with probabilistic measures of reliability, many
consequential forms of uncertainty in professional contexts resist such
quantification. A physician pondering the appropriateness of documenting
possible domestic abuse, a teacher assessing cultural sensitivity, or a
mathematician distinguishing procedural from conceptual understanding face
forms of uncertainty that cannot be reduced to percentages. This paper argues
for moving beyond simple quantification toward richer expressions of
uncertainty essential for beneficial AI integration. We propose participatory
refinement processes through which professional communities collectively shape
how different forms of uncertainty are communicated. Our approach acknowledges
that uncertainty expression is a form of professional sense-making that
requires collective development rather than algorithmic optimization.

</details>


### [30] [More AI Assistance Reduces Cognitive Engagement: Examining the AI Assistance Dilemma in AI-Supported Note-Taking](https://arxiv.org/abs/2509.03392)
*Xinyue Chen,Kunlin Ruan,Kexin Phyllis Ju,Nathan Yap,Xu Wang*

Main category: cs.HC

TL;DR: 论文研究了AI辅助在笔记任务中的影响，发现中等AI辅助（实时摘要）能带来最高测试成绩，但用户更喜欢高辅助的全自动模式，揭示了便利性与认知收益之间的差异。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具在高认知需求任务（如笔记）中是否增强或削弱用户认知参与，研究不同的AI支持级别如何影响用户的学习效果和偏好。

Method: 采用被试内实验设计，30名参与者在讲座视频中记笔记，分别体验高辅助（自动结构化笔记）、中等辅助（实时摘要）和低辅助（纯文本转录）三种条件。

Result: 中等AI辅助的组别后测成绩最高，全自动辅助的成绩最低；但参与者更偏好全自动模式，因为其易用性和低认知负荷。

Conclusion: 研究为设计既能保留认知参与度又能提供适度支持的AI工具提供了参考，强调了平衡用户偏好与认知收益的重要性。

Abstract: As AI tools become increasingly embedded in cognitively demanding tasks such
as note-taking, questions remain about whether they enhance or undermine
cognitive engagement. This paper examines the "AI Assistance Dilemma" in
note-taking, investigating how varying levels of AI support affect user
engagement and comprehension. In a within-subject experiment, we asked
participants (N=30) to take notes during lecture videos under three conditions:
Automated AI (high assistance with structured notes), Intermediate AI (moderate
assistance with real-time summary, and Minimal AI (low assistance with
transcript). Results reveal that Intermediate AI yields the highest post-test
scores and Automated AI the lowest. Participants, however, preferred the
automated setup due to its perceived ease of use and lower cognitive effort,
suggesting a discrepancy between preferred convenience and cognitive benefits.
Our study provides insights into designing AI assistance that preserves
cognitive engagement, offering implications for designing moderate AI support
in cognitive tasks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [31] [A Novel IaaS Tax Model as Leverage Towards Green Cloud Computing](https://arxiv.org/abs/2509.02767)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.DC

TL;DR: 该论文提出了一种名为GreenCloud税的经济方法，通过税收模型惩罚能源效率低的数据中心，同时鼓励高效数据中心，从而降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心能耗不断增加或保持稳定，需要创新方法来提升能源效率，减少环境负担。

Method: 开发了GreenCloud税模型，利用CloudSim模拟环境，并基于SPEC基准的真实数据集进行评估。

Result: GreenCloud税能够有效转移工作负载至高效数据中心，降低整体能耗。

Conclusion: 经济手段（税收）可作为提升数据中心能源效率的有效策略。

Abstract: The cloud computing technology uses datacenters, which require energy. Recent
trends show that the required energy for these datacenters will rise over time,
or at least remain constant. Hence, the scientific community developed
different algorithms, architectures, and approaches for improving the energy
efficiency of cloud datacenters, which are summarized under the umbrella term
Green Cloud computing. In this paper, we use an economic approach - taxes - for
reducing the energy consumption of datacenters. We developed a tax model called
GreenCloud tax, which penalizes energy-inefficient datacenters while fostering
datacenters that are energy-efficient. Hence, providers running
energy-efficient datacenters are able to offer cheaper prices to consumers,
which consequently leads to a shift of workloads from energy-inefficient
datacenters to energy-efficient datacenters. The GreenCloud tax approach was
implemented using the simulation environment CloudSim. We applied real data
sets published in the SPEC benchmark for the executed simulation scenarios,
which we used for evaluating the GreenCloud tax.

</details>


### [32] [Mycroft: Tracing Dependencies in Collective Communication Towards Reliable LLM Training](https://arxiv.org/abs/2509.03018)
*Yangtao Deng,Lei Zhang,Qinlong Wang,Xiaoyun Zhi,Xinlei Zhang,Zhuo Jiang,Haohan Xu,Lei Wang,Zuquan Song,Gaohong Liu,Yang Bai,Shuguang Wang,Wencong Xiao,Jianxi Ye,Minlan Yu,Hong Xu*

Main category: cs.DC

TL;DR: Mycroft是一个轻量级分布式追踪和根因分析系统，旨在解决集体通信中的可靠性问题，已在ByteDance部署六个月，表现优异。


<details>
  <summary>Details</summary>
Motivation: 集体通信库作为黑盒运行，缺乏关键信息，导致可靠性问题难以解决，浪费资源并影响LLM训练性能。

Method: Mycroft通过追踪集体通信状态并利用内部控制和数据依赖关系来解决可靠性问题。

Result: Mycroft在90%的情况下15秒内检测到异常，60%的情况下20秒内识别出根本原因。

Conclusion: Mycroft能有效提升LLM训练的可靠性。

Abstract: Reliability is essential for ensuring efficiency in LLM training. However,
many real-world reliability issues remain difficult to resolve, resulting in
wasted resources and degraded model performance. Unfortunately, today's
collective communication libraries operate as black boxes, hiding critical
information needed for effective root cause analysis. We propose Mycroft, a
lightweight distributed tracing and root cause analysis system designed to
address previously hidden reliability issues in collective communication.
Mycroft's key idea is to trace collective communication states and leverage
internal control and data dependencies to resolve reliability problems in LLM
training. Mycroft has been deployed at ByteDance for over six months to debug
collective communication related issues at runtime. It detected anomalies
within 15 seconds in 90% of cases and identified the root cause within 20
seconds in 60% of cases. We also conducted extensive fault injection
experiments to demonstrate Mycroft's capability and efficiency.

</details>


### [33] [CloudFormer: An Attention-based Performance Prediction for Public Clouds with Unknown Workload](https://arxiv.org/abs/2509.03394)
*Amirhossein Shahbazinia,Darong Huang,Luis Costero,David Atienza*

Main category: cs.DC

TL;DR: CloudFormer是一个基于双分支Transformer的模型，用于预测黑盒环境中虚拟机的性能下降，通过联合建模时间动态和系统级交互，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在多租户云环境中，共享资源的竞争导致性能下降，现有技术由于虚拟机的不透明性和动态工作负载难以准确预测性能。

Method: 提出CloudFormer模型，利用206个系统指标，以秒级分辨率建模时间和系统交互，无需场景特定调优。

Result: CloudFormer在多种评估指标上优于现有方法，MAE仅为7.8%，比其他方法至少提升28%。

Conclusion: CloudFormer能有效适应动态工作负载，显著提升性能预测准确性，适用于黑盒云环境。

Abstract: Cloud platforms are increasingly relied upon to host diverse,
resource-intensive workloads due to their scalability, flexibility, and
cost-efficiency. In multi-tenant cloud environments, virtual machines are
consolidated on shared physical servers to improve resource utilization. While
virtualization guarantees resource partitioning for CPU, memory, and storage,
it cannot ensure performance isolation. Competition for shared resources such
as last-level cache, memory bandwidth, and network interfaces often leads to
severe performance degradation. Existing management techniques, including VM
scheduling and resource provisioning, require accurate performance prediction
to mitigate interference. However, this remains challenging in public clouds
due to the black-box nature of VMs and the highly dynamic nature of workloads.
To address these limitations, we propose CloudFormer, a dual-branch
Transformer-based model designed to predict VM performance degradation in
black-box environments. CloudFormer jointly models temporal dynamics and
system-level interactions, leveraging 206 system metrics at one-second
resolution across both static and dynamic scenarios. This design enables the
model to capture transient interference effects and adapt to varying workload
conditions without scenario-specific tuning. Complementing the methodology, we
provide a fine-grained dataset that significantly expands the temporal
resolution and metric diversity compared to existing benchmarks. Experimental
results demonstrate that CloudFormer consistently outperforms state-of-the-art
baselines across multiple evaluation metrics, achieving robust generalization
across diverse and previously unseen workloads. Notably, CloudFormer attains a
mean absolute error (MAE) of just 7.8%, representing a substantial improvement
in predictive accuracy and outperforming existing methods at least by 28%.

</details>


### [34] [FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale Training of LLMs](https://arxiv.org/abs/2509.03047)
*Haijun Zhang,Jinxiang Wang,Zhenhua Yu,Yanyong Zhang,Xuejie Ji,Kaining Mao,Jun Zhang,Yaqing Zhang,Ting Wu,Fei Jie,Xiemin Huang,Zhifang Cai,Junhua Cheng,Shuwei Wang,Wei Li,Xiaoming Bao,Hua Xu,Shixiong Zhao,Jun Li,Hongwei Sun,Ziyang Zhang,Yi Xiong,Chunsheng Li*

Main category: cs.DC

TL;DR: FlashRecovery是一种快速低成本的故障恢复系统，旨在解决大规模语言模型训练中的系统可靠性问题，通过实时检测、独立规模任务重启和无检查点恢复三个核心模块实现高效恢复。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型训练中硬件和软件故障会导致大量训练时间损失，传统恢复方法效率低，FlashRecovery旨在解决这一问题。

Method: 系统包含三个模块：(1)实时故障检测；(2)规模无关的任务重启；(3)单步无检查点恢复。

Result: 实验显示，FlashRecovery能在150秒内恢复4800个设备的训练集群，且恢复时间与任务规模无关。

Conclusion: FlashRecovery显著提升了长时间训练任务的可靠性和效率，实现了优化的恢复目标。

Abstract: Large language models (LLMs) have made a profound impact across various
fields due to their advanced capabilities. However, training these models at
unprecedented scales requires extensive AI accelerator clusters and
sophisticated parallelism strategies, which pose significant challenges in
maintaining system reliability over prolonged training periods. A major concern
is the substantial loss of training time caused by inevitable hardware and
software failures. To address these challenges, we present FlashRecovery, a
fast and low-cost failure recovery system comprising three core modules: (1)
Active and real-time failure detection. This module performs continuous
training state monitoring, enabling immediate identification of hardware and
software failures within seconds, thus ensuring rapid incident response; (2)
Scale-independent task restart. By employing different recovery strategies for
normal and faulty nodes, combined with an optimized communication group
reconstruction protocol, our approach ensures that the recovery time remains
nearly constant, regardless of cluster scale; (3) Checkpoint-free recovery
within one step. Our novel recovery mechanism enables single-step restoration,
completely eliminating dependence on traditional checkpointing methods and
their associated overhead. Collectively, these innovations enable FlashRecovery
to achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective
(RPO), substantially improving the reliability and efficiency of long-duration
LLM training. Experimental results demonstrate that FlashRecovery system can
achieve training restoration on training cluster with 4, 800 devices in 150
seconds. We also verify that the time required for failure recovery is nearly
consistent for different scales of training tasks.

</details>


### [35] [The High Cost of Keeping Warm: Characterizing Overhead in Serverless Autoscaling Policies](https://arxiv.org/abs/2509.03104)
*Leonid Kondrashov,Boxi Zhou,Hancheng Wang,Dmitrii Ustiugov*

Main category: cs.DC

TL;DR: 论文通过设计一个模仿商业服务器供应商行为的开源系统，分析了同步和异步自动扩展策略的性能和成本效益。


<details>
  <summary>Details</summary>
Motivation: 理解服务器计算控制平面设计的性能-成本权衡，填补开源基准测试和系统分析的空白。

Method: 设计一个模拟AWS Lambda和Google Cloud Run行为的服务器系统，采用真实工作负载回放和调整扩展参数的方法。

Result: 发现服务器系统存在显著的CPU和内存开销，并指出减少这些开销会导致性能下降，需要新的高效扩展策略。

Conclusion: 研究为服务器计算的扩展策略提供了可重现的实验基础，并揭示了优化方向。

Abstract: Serverless computing is transforming cloud application development, but the
performance-cost trade-offs of control plane designs remain poorly understood
due to a lack of open, cross-platform benchmarks and detailed system analyses.
In this work, we address these gaps by designing a serverless system that
approximates the scaling behaviors of commercial providers, including AWS
Lambda and Google Cloud Run. We systematically compare the performance and
cost-efficiency of both synchronous and asynchronous autoscaling policies by
replaying real-world workloads and varying key autoscaling parameters.
  We demonstrate that our open-source systems can closely replicate the
operational characteristics of commercial platforms, enabling reproducible and
transparent experimentation. By evaluating how autoscaling parameters affect
latency, memory usage, and CPU overhead, we reveal several key findings. First,
we find that serverless systems exhibit significant computational overhead due
to instance churn equivalent to 10-40% of the CPU cycles spent on request
handling, primarily originating from worker nodes. Second, we observe high
memory allocation due to scaling policy: 2-10 times more than actively used.
Finally, we demonstrate that reducing these overheads typically results in
significant performance degradation in the current systems, underscoring the
need for new, cost-efficient autoscaling strategies. Additionally, we employ a
hybrid methodology that combines real control plane deployments with
large-scale simulation to extend our evaluation closer to a production scale,
thereby bridging the gap between small research clusters and real-world
environments.

</details>


### [36] [Efficient and Secure Sleepy Model for BFT Consensus](https://arxiv.org/abs/2509.03145)
*Pengkun Ren,Hai Dong,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: 提出了一种结合预提交机制和PVSS的BFT协议，显著减少通信轮次并提升安全性，在常见场景下仅需4Δ延迟。


<details>
  <summary>Details</summary>
Motivation: 动态可用系统中的BFT协议需要平衡延迟和安全性，现有解决方案因多轮投票导致高延迟或抗攻击能力有限。

Method: 通过将预提交机制与PVSS集成到消息传输中，将用户身份绑定到消息上，以减少通信轮次。

Result: 协议在常见场景下仅需4Δ延迟，能抵抗多达1/2的对抗参与者，实验显示其显著减少分叉并提升链稳定性。

Conclusion: 该协议在不牺牲完整性的情况下提升了效率和安全性，特别适合参与度波动适中的场景。

Abstract: Byzantine Fault Tolerant (BFT) consensus protocols for dynamically available
systems face a critical challenge: balancing latency and security in
fluctuating node participation. Existing solutions often require multiple
rounds of voting per decision, leading to high latency or limited resilience to
adversarial behavior. This paper presents a BFT protocol integrating a
pre-commit mechanism with publicly verifiable secret sharing (PVSS) into
message transmission. By binding users' identities to their messages through
PVSS, our approach reduces communication rounds. Compared to other
state-of-the-art methods, our protocol typically requires only four network
delays (4$\Delta$) in common scenarios while being resilient to up to 1/2
adversarial participants. This integration enhances the efficiency and security
of the protocol without compromising integrity. Theoretical analysis
demonstrates the robustness of the protocol against Byzantine attacks.
Experimental evaluations show that, compared to traditional BFT protocols, our
protocol significantly prevents fork occurrences and improves chain stability.
Furthermore, compared to longest-chain protocol, our protocol maintains
stability and lower latency in scenarios with moderate participation
fluctuations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [37] [Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving](https://arxiv.org/abs/2509.02718)
*Fangzhou Wu,Sandeep Silwal*

Main category: cs.DB

TL;DR: 该论文提出了一种免训练算法，用于在线LLM路由场景，通过近似最近邻搜索和一次性优化，提高了性能和成本效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）服务需求的增长，部署和计算成本大幅增加，LLM路由成为成本高效的解决方案。

Method: 利用近似最近邻搜索估计查询特征，并通过一次性优化学习路由策略。

Result: 算法在3个基准数据集和8个基线模型上的实验显示，性能提升3.55倍，成本效率提高1.85倍，吞吐量提升近4.25倍。

Conclusion: 该算法在在线路由场景中表现出色，提供了高效且可扩展的路由解决方案。

Abstract: Increasing demand for Large Language Models (LLMs) services imposes
substantial deployment and computation costs on providers. LLM routing offers a
cost-efficient solution by directing queries to the optimal LLM based on model
and query features. However, existing works primarily focus on offline
scenarios and struggle to adapt to online settings with high query volume and
constrained token budgets. In this work, we introduce the first training-free
algorithm for online routing scenarios. Our algorithm leverages approximate
nearest neighbor search to efficiently estimate query features and performs a
one-time optimization over a small set of initial queries to learn a routing
strategy that guides future routing. We provide theoretical guarantees
demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$
under natural assumptions, which is further validated by extensive experiments
across 3 benchmark datasets and 8 baselines, showing an average improvement of
3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and
nearly 4.25$\times$ in throughput.

</details>


### [38] [Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees](https://arxiv.org/abs/2509.02896)
*Sepanta Zeighami,Shreya Shankar,Aditya Parameswaran*

Main category: cs.DB

TL;DR: BARGAIN方法通过智能使用低成本大语言模型（LLMs）显著降低数据处理成本，并提供强大的理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在处理大规模文本数据时存在高昂成本和准确性之间的权衡，现有模型级联框架的成本节省和理论保证不足。

Method: BARGAIN采用自适应采样策略和统计估计方法，结合数据和任务特性，提供准确估计和严格理论保证。

Result: 实验结果显示，BARGAIN在8个真实数据集上平均降低成本86%，同时提供更强的输出准确性保证。

Conclusion: BARGAIN为LLMs在数据处理中的应用提供了一种高效且可靠的解决方案。

Abstract: Large Language Models (LLMs) are being increasingly used as a building block
in data systems to process large text datasets. To do so, LLM model providers
offer multiple LLMs with different sizes, spanning various cost-quality
trade-offs when processing text at scale. Top-of-the-line LLMs (e.g., GPT-4o,
Claude Sonnet) operate with high accuracy but are prohibitively expensive when
processing many records. To avoid high costs, more affordable but lower quality
LLMs (e.g., GPT-4o-mini, Claude Haiku) can be used to process records, but we
need to ensure that the overall accuracy does not deviate substantially from
that of the top-of-the-line LLMs. The model cascade framework provides a
blueprint to manage this trade-off, by using the confidence of LLMs in their
output (e.g., log-probabilities) to decide on which records to use the
affordable LLM. However, existing solutions following this framework provide
only marginal cost savings and weak theoretical guarantees because of poor
estimation of the quality of the affordable LLM's outputs. We present BARGAIN,
a method that judiciously uses affordable LLMs in data processing to
significantly reduce cost while providing strong theoretical guarantees on the
solution quality. BARGAIN employs a novel adaptive sampling strategy and
statistical estimation procedure that uses data and task characteristics and
builds on recent statistical tools to make accurate estimations with tight
theoretical guarantees. Variants of BARGAIN can support guarantees on accuracy,
precision, or recall of the output. Experimental results across 8 real-world
datasets show that BARGAIN reduces cost, on average, by up to 86% more than
state-of-the-art, while providing stronger theoretical guarantees on accuracy
of output, with similar gains when guaranteeing a desired level of precision or
recall.

</details>


### [39] [CARPO: Leveraging Listwise Learning-to-Rank for Context-Aware Query Plan Optimization](https://arxiv.org/abs/2509.03102)
*Wenrui Zhou,Qiyu Liu,Jingshu Peng,Aoqian Zhang,Lei Chen*

Main category: cs.DB

TL;DR: CARPO是一个基于列表学习排序的通用框架，通过Transformer模型全局评估候选计划集，结合混合决策机制，显著提升查询优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于成本的优化器因启发式和成本模型不准确常生成次优计划，现有LQOs方法存在不一致性和次优性问题。

Method: 提出CARPO框架，利用列表学习排序和Transformer模型，集成OOD检测和top-k后备策略。

Result: 在TPC-H和STATS基准测试中，CARPO显著优于PostgreSQL和Lero，Top-1 Rate达74.54%。

Conclusion: CARPO通过上下文感知优化和鲁棒决策机制，显著提升查询计划生成质量和执行效率。

Abstract: Efficient data processing is increasingly vital, with query optimizers
playing a fundamental role in translating SQL queries into optimal execution
plans. Traditional cost-based optimizers, however, often generate suboptimal
plans due to flawed heuristics and inaccurate cost models, leading to the
emergence of Learned Query Optimizers (LQOs). To address challenges in existing
LQOs, such as the inconsistency and suboptimality inherent in pairwise ranking
methods, we introduce CARPO, a generic framework leveraging listwise
learning-to-rank for context-aware query plan optimization. CARPO distinctively
employs a Transformer-based model for holistic evaluation of candidate plan
sets and integrates a robust hybrid decision mechanism, featuring
Out-Of-Distribution (OOD) detection with a top-$k$ fallback strategy to ensure
reliability. Furthermore, CARPO can be seamlessly integrated with existing plan
embedding techniques, demonstrating strong adaptability. Comprehensive
experiments on TPC-H and STATS benchmarks demonstrate that CARPO significantly
outperforms both native PostgreSQL and Lero, achieving a Top-1 Rate of
\textbf{74.54\%} on the TPC-H benchmark compared to Lero's 3.63\%, and reducing
the total execution time to 3719.16 ms compared to PostgreSQL's 22577.87 ms.

</details>


### [40] [Adaptive KV-Cache Compression without Manually Setting Budget](https://arxiv.org/abs/2509.03136)
*Chenxia Tang,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.DB

TL;DR: GVote是一种自适应的KV-cache压缩方案，通过蒙特卡洛采样预测未来查询需求，优化缓存预算，实现高效内存使用和性能平衡。


<details>
  <summary>Details</summary>
Motivation: 解决KV-cache现有压缩方法因固定压缩比导致的资源分配和性能不佳问题。

Method: 采用蒙特卡洛采样预测未来查询注意力需求，自适应确定最佳缓存预算。

Result: 在多个基准测试中，GVote内存占用减少2倍，同时保持或提升准确率。

Conclusion: GVote通过自适应压缩在内存效率和性能间取得优越平衡。

Abstract: Large language models (LLMs) inference relies heavily on KV-caches to
accelerate autoregressive decoding, but the resulting memory footprint grows
rapidly with sequence length, posing significant efficiency challenges. Current
KV-cache compression methods suffer from a Procrustes' bed problem: they force
diverse workloads into fixed compression ratios, leading to suboptimal resource
allocation and inference performance. To this end, we present GVote, an
adaptive KV-cache compression scheme that eliminates manual budget
specification while achieving superior accuracy-efficiency trade-offs. GVote
operates on the principle that the important keys are the aggregation of keys
required by future queries. The method predicts future query attention demands
by Monte-Carlo style sampling potential queries and aggregating selected keys
to determine the optimal cache budget without manual specification.
Experimental evaluation demonstrates GVote's effectiveness across multiple
benchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote
exhibits 2$\times$ memory reduction while the accuracy maintains higher or
comparable.

</details>


### [41] [BAMG: A Block-Aware Monotonic Graph Index for Disk-Based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2509.03226)
*Huiling Li,Jianliang Xu*

Main category: cs.DB

TL;DR: 论文提出了一种新型图结构BMRNG，联合考虑了几何距离和存储布局，并进一步开发了高效的BAMG变种，显著提升了高维向量近似最近邻搜索的性能和I/O效率。


<details>
  <summary>Details</summary>
Motivation: 高维向量近似最近邻搜索（ANNS）在大规模数据库中是基础性问题，磁盘I/O常成为性能瓶颈。现有解决方案未能综合考虑几何距离和存储布局的互动关系。

Method: 提出Block-aware Monotonic Relative Neighborhood Graph（BMRNG），理论上保证I/O单调搜索路径；并开发高效的BAMG变种，结合块感知边剪枝和解耦存储设计。

Result: 实验表明BAMG在真实数据集上吞吐量提升2.1倍，I/O读取减少52%，同时保持相近的召回率。

Conclusion: BAMG通过联合优化几何距离和存储布局，显著提升了ANNS的性能和效率，为大规模数据应用提供了实用解决方案。

Abstract: Approximate Nearest Neighbor Search (ANNS) over high-dimensional vectors is a
foundational problem in databases, where disk I/O often emerges as the dominant
performance bottleneck at scale. Existing graph indexing solutions for
disk-based ANNS typically either optimize the storage layout for a given graph
or construct the graph independently of the storage layout, thus overlooking
their interaction. In this paper, we propose the Block-aware Monotonic Relative
Neighborhood Graph (BMRNG), a novel graph structure that jointly considers both
geometric distance and storage layout for edge selection, theoretically
guaranteeing the existence of I/O monotonic search paths. To address the
scalability challenge of BMRNG construction, we further develop a practical and
efficient variant, the Block-Aware Monotonic Graph (BAMG), which can be
constructed in linear time from a monotonic graph considering the storage
layout. BAMG integrates block-aware edge pruning with a decoupled storage
design that separates raw vectors from the graph index, thereby maximizing
block utilization and minimizing redundant disk reads. Additionally, we design
a multi-layer navigation graph for adaptive and efficient query entry, along
with a block-first search algorithm that prioritizes intra-block traversal to
fully exploit each disk I/O operation. Extensive experiments on real-world
datasets demonstrate that BAMG achieves up to 2.1x higher throughput and
reduces I/O reads by up to 52% compared to state-of-the-art methods, while
maintaining comparable recall.

</details>


### [42] [NeurStore: Efficient In-database Deep Learning Model Management System](https://arxiv.org/abs/2509.03228)
*Siqi Xiang,Sheng Wang,Xiaokui Xiao,Cong Yue,Zhanhao Zhao,Beng Chin Ooi*

Main category: cs.DB

TL;DR: NeurStore是一种新型的数据库内模型管理系统，通过细粒度存储和高效的压缩技术显著降低了深度学习模型的存储开销。


<details>
  <summary>Details</summary>
Motivation: 随着数据库内AI分析的普及，深度学习模型数量和规模的快速增长需要更高效的存储和管理方式。现有系统将模型存储为单一文件或应用忽略模型结构特征的压缩技术，导致存储效率低下。

Method: NeurStore采用基于张量的模型存储引擎，通过增强HNSW图索引张量并存储相似张量的增量实现细粒度存储，并提出增量量化算法压缩张量。此外，设计了压缩感知的模型加载机制，支持直接对压缩张量进行计算。

Result: 实验表明，NeurStore在压缩率和模型加载吞吐量上均优于现有方法。

Conclusion: NeurStore通过创新的存储和压缩技术，为深度学习模型的高效管理提供了可行方案。

Abstract: With the prevalence of in-database AI-powered analytics, there is an
increasing demand for database systems to efficiently manage the ever-expanding
number and size of deep learning models. However, existing database systems
typically store entire models as monolithic files or apply compression
techniques that overlook the structural characteristics of deep learning
models, resulting in suboptimal model storage overhead. This paper presents
NeurStore, a novel in-database model management system that enables efficient
storage and utilization of deep learning models. First, NeurStore employs a
tensor-based model storage engine to enable fine-grained model storage within
databases. In particular, we enhance the hierarchical navigable small world
(HNSW) graph to index tensors, and only store additional deltas for tensors
within a predefined similarity threshold to ensure tensor-level deduplication.
Second, we propose a delta quantization algorithm that effectively compresses
delta tensors, thus achieving a superior compression ratio with controllable
model accuracy loss. Finally, we devise a compression-aware model loading
mechanism, which improves model utilization performance by enabling direct
computation on compressed tensors. Experimental evaluations demonstrate that
NeurStore achieves superior compression ratios and competitive model loading
throughput compared to state-of-the-art approaches.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [43] [Portable Targeted Sampling Framework Using LLVM](https://arxiv.org/abs/2509.02873)
*Zhantong Qiu,Mahyar Samani,Jason Lowe-Power*

Main category: cs.AR

TL;DR: Nugget框架通过LLVM IR实现跨平台采样分析，显著降低了仿真成本，提升性能评估效率。


<details>
  <summary>Details</summary>
Motivation: 全面架构评估因慢速仿真和二进制采样流程受限，Nugget旨在解决这一问题。

Method: 在LLVM IR级别进行二进制无关的区间分析，生成轻量级跨平台可执行文件（nuggets）。

Result: 在SPEC CPU2017等测试中，成本降低高达578倍，单线程开销低，支持原生速度验证。

Conclusion: Nugget使采样方法研究更快、更便携。

Abstract: Comprehensive architectural evaluation of full workloads is throttled by slow
simulation and per-binary sampling pipelines. We present Nugget, a flexible
framework for portable sampling across simulators and real hardware, ISAs, and
libraries. Nugget operates at the LLVM IR level to perform binary-agnostic
interval analysis, then emits lightweight, cross-platform
executables--nuggets--that can be validated on real machines before driving
simulation. Across SPEC CPU2017, NPB, and LSMS, Nugget cuts interval-analysis
cost by orders of magnitude relative to functional simulation (up to ~578X on
multithreaded NPB), keeps single-thread overhead low, and enables native-speed
validation of selected samples. Case studies with gem5 show that nuggets
support evaluation of system performance and model accuracy. Nugget makes
sampling methodology research faster and more portable.

</details>


### [44] [FastCaps: A Design Methodology for Accelerating Capsule Network on Field Programmable Gate Arrays](https://arxiv.org/abs/2509.03103)
*Abdul Rahoof,Vivek Chaturvedi,Muhammad Shafique*

Main category: cs.AR

TL;DR: 该论文提出了一种新颖的两步法，通过剪枝和简化路由算法，首次在FPGA上实现了完整的CapsNet加速。


<details>
  <summary>Details</summary>
Motivation: 由于胶囊网络的复杂性和动态路由机制，直接在FPGA上加速完整的CapsNet具有挑战性。现有工作仅实现了有限加速。

Method: 提出Look-Ahead Kernel Pruning（LAKP）方法剪枝，并简化非线性操作、重排循环和平行化路由算法操作。

Result: 在MNIST和F-MNIST数据集上，压缩率分别达99.26%和98.84%，吞吐量提升至1351 FPS和934 FPS。

Conclusion: 该方法为低成本的边缘设备提供了高效的CapsNet部署方案。

Abstract: Capsule Network (CapsNet) has shown significant improvement in understanding
the variation in images along with better generalization ability compared to
traditional Convolutional Neural Network (CNN). CapsNet preserves spatial
relationship among extracted features and apply dynamic routing to efficiently
learn the internal connections between capsules. However, due to the capsule
structure and the complexity of the routing mechanism, it is non-trivial to
accelerate CapsNet performance in its original form on Field Programmable Gate
Array (FPGA). Most of the existing works on CapsNet have achieved limited
acceleration as they implement only the dynamic routing algorithm on FPGA,
while considering all the processing steps synergistically is important for
real-world applications of Capsule Networks. Towards this, we propose a novel
two-step approach that deploys a full-fledged CapsNet on FPGA. First, we prune
the network using a novel Look-Ahead Kernel Pruning (LAKP) methodology that
uses the sum of look-ahead scores of the model parameters. Next, we simplify
the nonlinear operations, reorder loops, and parallelize operations of the
routing algorithm to reduce CapsNet hardware complexity. To the best of our
knowledge, this is the first work accelerating a full-fledged CapsNet on FPGA.
Experimental results on the MNIST and F-MNIST datasets (typical in Capsule
Network community) show that the proposed LAKP approach achieves an effective
compression rate of 99.26% and 98.84%, and achieves a throughput of 82 FPS and
48 FPS on Xilinx PYNQ-Z1 FPGA, respectively. Furthermore, reducing the hardware
complexity of the routing algorithm increases the throughput to 1351 FPS and
934 FPS respectively. As corroborated by our results, this work enables highly
performance-efficient deployment of CapsNets on low-cost FPGA that are popular
in modern edge devices.

</details>


### [45] [CapsBeam: Accelerating Capsule Network based Beamformer for Ultrasound Non-Steered Plane Wave Imaging on Field Programmable Gate Array](https://arxiv.org/abs/2509.03201)
*Abdul Rahoof,Vivek Chaturvedi,Mahesh Raveendranatha Panicker,Muhammad Shafique*

Main category: cs.AR

TL;DR: 本文提出了一种基于胶囊网络的超声波束形成器CapsBeam，通过实验验证了其在图像质量上的显著提升，并采用多层LookAhead Kernel Pruning（LAKP-ML）方法和量化技术优化了模型，最终在FPGA上实现了高效硬件加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的超声波束形成算法在资源受限的边缘设备上部署困难，亟需一种高效且轻量化的解决方案。

Method: 提出CapsBeam网络，结合非导向平面波检测，采用LAKP-ML方法剪枝和量化优化模型，并设计专用硬件加速器架构。

Result: CapsBeam在图像质量上优于传统DAS算法（如对比度提升32.31%），剪枝后模型压缩85%且不影响质量，硬件加速器实现了30 GOPS的卷积运算吞吐量。

Conclusion: CapsBeam在提升超声成像质量的同时，通过模型优化和硬件加速，为边缘设备部署提供了可行方案。

Abstract: In recent years, there has been a growing trend in accelerating
computationally complex non-real-time beamforming algorithms in ultrasound
imaging using deep learning models. However, due to the large size and
complexity these state-of-the-art deep learning techniques poses significant
challenges when deploying on resource-constrained edge devices. In this work,
we propose a novel capsule network based beamformer called CapsBeam, designed
to operate on raw radio-frequency data and provide an envelope of beamformed
data through non-steered plane wave insonification. Experiments on in-vivo
data, CapsBeam reduced artifacts compared to the standard Delay-and-Sum (DAS)
beamforming. For in-vitro data, CapsBeam demonstrated a 32.31% increase in
contrast, along with gains of 16.54% and 6.7% in axial and lateral resolution
compared to the DAS. Similarly, in-silico data showed a 26% enhancement in
contrast, along with improvements of 13.6% and 21.5% in axial and lateral
resolution, respectively, compared to the DAS. To reduce the parameter
redundancy and enhance the computational efficiency, we pruned the model using
our multi-layer LookAhead Kernel Pruning (LAKP-ML) methodology, achieving a
compression ratio of 85% without affecting the image quality. Additionally, the
hardware complexity of the proposed model is reduced by applying quantization,
simplification of non-linear operations, and parallelizing operations. Finally,
we proposed a specialized accelerator architecture for the pruned and optimized
CapsBeam model, implemented on a Xilinx ZU7EV FPGA. The proposed accelerator
achieved a throughput of 30 GOPS for the convolution operation and 17.4 GOPS
for the dynamic routing operation.

</details>


### [46] [Amplifying Effective CXL Memory Bandwidth for LLM Inference via Transparent Near-Data Processing](https://arxiv.org/abs/2509.03377)
*Rui Xie,Asad Ul Haq,Linsen Ma,Yunhua Fang,Zirak Burzin Engineer,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: CXL-NDP是一种透明近数据处理架构，通过动态量化和无损压缩提升CXL内存带宽，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决CXL内存带宽限制对LLM推理的瓶颈问题，无需修改CXL.mem接口或AI模型。

Method: 集成精度可调的位平面布局和透明无损压缩技术于CXL设备，动态量化权重和KV缓存。

Result: 吞吐量提升43%，最大上下文长度扩展87%，KV缓存占用减少46.9%且无精度损失。

Conclusion: CXL-NDP硬件实现可行性高，为生成式AI基础设施提供高效可扩展的CXL内存解决方案。

Abstract: Large language model (LLM) inference is bottlenecked by the limited bandwidth
of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a
transparent near-data processing architecture that amplifies effective CXL
bandwidth without requiring changes to the CXL.mem interface or AI models.
CXL-NDP integrates a precision-scalable bit-plane layout for dynamic
quantization with transparent lossless compression of weights and KV caches
directly within the CXL device. In end-to-end serving, CXL-NDP improves
throughput by 43%, extends the maximum context length by 87%, and reduces the
KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms
its practicality with a modest silicon footprint, lowering the barrier for
adopting efficient, scalable CXL-based memory in generative AI infrastructure.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [47] [The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile Manipulation](https://arxiv.org/abs/2509.03222)
*Sophia Bianchi Moyen,Rickmer Krohn,Sophie Lueth,Kay Pompetzki,Jan Peters,Vignesh Prasad,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 论文比较了两种机器人控制范式（耦合与解耦）和两种视觉反馈（VR与屏幕），发现VR增加任务时间和认知负荷，而耦合控制对模仿学习更有利。


<details>
  <summary>Details</summary>
Motivation: 研究直观远程操作界面，以提升移动机器人数据收集质量和减少操作者负担。

Method: 比较耦合与解耦控制范式，评估VR与屏幕视觉反馈，实验多阶段任务序列。

Result: VR增加操作时间和认知负荷；耦合控制与解耦控制用户负担相似，但耦合数据对模仿学习更优。

Conclusion: 直观远程操作界面设计有助于大规模高质量数据收集，耦合控制和屏幕反馈更优。

Abstract: Intuitive Teleoperation interfaces are essential for mobile manipulation
robots to ensure high quality data collection while reducing operator workload.
A strong sense of embodiment combined with minimal physical and cognitive
demands not only enhances the user experience during large-scale data
collection, but also helps maintain data quality over extended periods. This
becomes especially crucial for challenging long-horizon mobile manipulation
tasks that require whole-body coordination. We compare two distinct robot
control paradigms: a coupled embodiment integrating arm manipulation and base
navigation functions, and a decoupled embodiment treating these systems as
separate control entities. Additionally, we evaluate two visual feedback
mechanisms: immersive virtual reality and conventional screen-based
visualization of the robot's field of view. These configurations were
systematically assessed across a complex, multi-stage task sequence requiring
integrated planning and execution. Our results show that the use of VR as a
feedback modality increases task completion time, cognitive workload, and
perceived effort of the teleoperator. Coupling manipulation and navigation
leads to a comparable workload on the user as decoupling the embodiments, while
preliminary experiments suggest that data acquired by coupled teleoperation
leads to better imitation learning performance. Our holistic view on intuitive
teleoperation interfaces provides valuable insight into collecting
high-quality, high-dimensional mobile manipulation data at scale with the human
operator in mind. Project
website:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/

</details>


### [48] [Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in Infectious Pandemic Management](https://arxiv.org/abs/2509.03436)
*Md Mhamud Hussen Sifat,Md Maruf,Md Rokunuzzaman*

Main category: cs.RO

TL;DR: 论文探讨了机器人技术在医疗保健中的应用，特别是在COVID-19疫情期间，展示了护士机器人在患者健康评估和药物管理中的作用。


<details>
  <summary>Details</summary>
Motivation: 由于技术进步，机器人技术在医疗保健中越来越受欢迎，尤其是在COVID-19疫情期间，自动化需求增加，以减少感染风险并改善患者护理。

Method: 研究提出了一个由物联网控制的护士机器人系统，能够评估患者健康状况并进行相应的操作，同时评估其在药物管理、健康监测和生命周期方面的性能。

Result: 护士机器人系统能够有效减少感染风险并在大流行环境中改善患者护理效果。

Conclusion: 机器人技术在医疗保健中具有潜力，特别是在自动化需求增加的疫情期间，能够显著提升医疗系统的可持续性和盈利能力。

Abstract: The utilization of robotic technology has gained traction in healthcare
facilities due to progress in the field that enables time and cost savings,
minimizes waste, and improves patient care. Digital healthcare technologies
that leverage automation, such as robotics and artificial intelligence, have
the potential to enhance the sustainability and profitability of healthcare
systems in the long run. However, the recent COVID-19 pandemic has amplified
the need for cyber-physical robots to automate check-ups and medication
administration. A robot nurse is controlled by the Internet of Things (IoT) and
can serve as an automated medical assistant while also allowing supervisory
control based on custom commands. This system helps reduce infection risk and
improves outcomes in pandemic settings. This research presents a test case with
a nurse robot that can assess a patient's health status and take action
accordingly. We also evaluate the system's performance in medication
administration, health-status monitoring, and life-cycle considerations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [49] [VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results](https://arxiv.org/abs/2509.02969)
*Dasong Li,Sizhuo Ma,Hang Hua,Wenjie Li,Jian Wang,Chris Wei Zhou,Fengbin Guan,Xin Li,Zihao Yu,Yiting Lu,Ru-Ling Liao,Yan Ye,Zhibo Chen,Wei Sun,Linhan Cao,Yuqin Cao,Weixia Zhang,Wen Wen,Kaiwei Zhang,Zijian Chen,Fangfang Lu,Xiongkuo Min,Guangtao Zhai,Erjia Xiao,Lingfeng Zhang,Zhenjie Su,Hao Cheng,Yu Liu,Renjing Xu,Long Chen,Xiaoshuai Hao,Zhenpeng Zeng,Jianqin Wu,Xuxu Wang,Qian Yu,Bo Hu,Weiwei Wang,Pinxin Liu,Yunlong Tang,Luchuan Song,Jinxi He,Jiaru Wu,Hanjia Lyu*

Main category: cs.CV

TL;DR: VQualA 2025挑战赛聚焦于社交媒体平台上用户生成短视频的受欢迎度建模，旨在推动多模态特征建模策略的发展。


<details>
  <summary>Details</summary>
Motivation: 理解用户生成短视频的受欢迎度及其影响因素。

Method: 利用包含视觉内容、音频和创作者元数据的多模态特征进行建模。

Result: 吸引了97名参与者，收到15份有效测试提交，推动了短视频参与度预测的进步。

Conclusion: 挑战赛成功促进了短视频用户参与度的建模研究。

Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement
Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge
focuses on understanding and modeling the popularity of user-generated content
(UGC) short videos on social media platforms. To support this goal, the
challenge uses a new short-form UGC dataset featuring engagement metrics
derived from real-world user interactions. This objective of the Challenge is
to promote robust modeling strategies that capture the complex factors
influencing user engagement. Participants explored a variety of multi-modal
features, including visual content, audio, and metadata provided by creators.
The challenge attracted 97 participants and received 15 valid test submissions,
contributing significantly to progress in short-form UGC video engagement
prediction.

</details>


### [50] [Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data](https://arxiv.org/abs/2509.03501)
*Honglu Zhou,Xiangyu Peng,Shrikant Kendre,Michael S. Ryoo,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: Strefer是一个合成的指令数据生成框架，旨在增强视频大型语言模型（Video LLMs）的时空参考和推理能力，无需依赖专有模型或高成本的人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有Video LLMs在细粒度时空推理上表现不佳，尤其是面对依赖时间事件或手势的空间定位查询时。

Method: Strefer通过数据引擎生成多样化的指令调整数据，伪标注密集的时间视频元数据，结构化捕捉时空信息。

Result: 实验表明，使用Strefer数据的模型在时空消歧任务上优于基线，并展现出更强的时空感知推理能力。

Conclusion: Strefer为视频LLMs提供了时空感知推理的基础，推动了更实用的AI伴侣发展。

Abstract: Next-generation AI companions must go beyond general video understanding to
resolve spatial and temporal references in dynamic, real-world environments.
Existing Video Large Language Models (Video LLMs), while capable of
coarse-level comprehension, struggle with fine-grained, spatiotemporal
reasoning, especially when user queries rely on time-based event references for
temporal anchoring, or gestural cues for spatial anchoring to clarify object
references and positions. To bridge this critical gap, we introduce Strefer, a
synthetic instruction data generation framework designed to equip Video LLMs
with spatiotemporal referring and reasoning capabilities. Strefer produces
diverse instruction-tuning data using a data engine that pseudo-annotates
temporally dense, fine-grained video metadata, capturing rich spatial and
temporal information in a structured manner, including subjects, objects, their
locations as masklets, and their action descriptions and timelines. Our
approach enhances the ability of Video LLMs to interpret spatial and temporal
references, fostering more versatile, space-time-aware reasoning essential for
real-world AI companions. Without using proprietary models, costly human
annotation, or the need to annotate large volumes of new videos, experimental
evaluations show that models trained with data produced by Strefer outperform
baselines on tasks requiring spatial and temporal disambiguation. Additionally,
these models exhibit enhanced space-time-aware reasoning, establishing a new
foundation for perceptually grounded, instruction-tuned Video LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [Estudio de la eficiencia en la escalabilidad de GPUs para el entrenamiento de Inteligencia Artificial](https://arxiv.org/abs/2509.03263)
*David Cortes,Carlos Juiz,Belen Bermejo*

Main category: cs.LG

TL;DR: 分析MLPerf Training v4.1中四种工作负载（BERT、Llama2 LoRA、RetinaNet和Stable Diffusion）的配置，找到性能、GPU使用率和效率之间的优化关系。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型的训练已成为科学界和工业界的关键挑战，但大量使用GPU虽提高速度却降低效率。

Method: 通过对MLPerf Training v4.1中四种工作负载的报告时间进行详细分析，识别优化配置。

Result: 存在一个平衡点，可以在减少训练时间的同时最大化效率。

Conclusion: 通过优化GPU配置，能够在训练效率和性能之间找到最佳平衡点。

Abstract: Training large-scale deep learning models has become a key challenge for the
scientific community and industry. While the massive use of GPUs can
significantly speed up training times, this approach has a negative impact on
efficiency. In this article, we present a detailed analysis of the times
reported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA,
RetinaNet, and Stable Diffusion, showing that there are configurations that
optimise the relationship between performance, GPU usage, and efficiency. The
results point to a break-even point that allows training times to be reduced
while maximising efficiency.

</details>


### [52] [Structure Transfer: an Inference-Based Calculus for the Transformation of Representations](https://arxiv.org/abs/2509.03249)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.LG

TL;DR: 该论文提出了一种称为“结构转移”的新颖演算方法，用于在不同表示系统之间进行表示转换，并通过模式确保源表示和目标表示满足指定关系。


<details>
  <summary>Details</summary>
Motivation: 解决如何在表示系统中设计通用的表示转换和选择技术的未解决问题。

Method: 利用模式编码表示系统的知识，开发结构转移演算方法，确保源和目标表示满足任何指定关系。

Result: 结构转移是一种通用演算方法，适用于多种实际场景中的表示转换。

Conclusion: 结构转移为表示选择和转换提供了一种系统无关的解决方案，具有广泛的适用性。

Abstract: Representation choice is of fundamental importance to our ability to
communicate and reason effectively. A major unsolved problem, addressed in this
paper, is how to devise \textit{representational-system (RS) agnostic}
techniques that drive representation transformation and choice. We present a
novel calculus, called \textit{structure transfer}, that enables representation
transformation across diverse RSs. Specifically, given a \textit{source}
representation drawn from a source RS, the rules of structure transfer allow us
to generate a \textit{target} representation for a target RS. The generality of
structure transfer comes in part from its ability to ensure that the source
representation and the generated target representation satisfy \textit{any}
specified relation (such as semantic equivalence). This is done by exploiting
\textit{schemas}, which encode knowledge about RSs. Specifically, schemas can
express \textit{preservation of information} across relations between any pair
of RSs, and this knowledge is used by structure transfer to derive a structure
for the target representation which ensures that the desired relation holds. We
formalise this using Representational Systems Theory~\cite{raggi2022rst},
building on the key concept of a \textit{construction space}. The abstract
nature of construction spaces grants them the generality to model RSs of
diverse kinds, including formal languages, geometric figures and diagrams, as
well as informal notations. Consequently, structure transfer is a
system-agnostic calculus that can be used to identify alternative
representations in a wide range of practical settings.

</details>


### [53] [Event Detection and Classification for Long Range Sensing of Elephants Using Seismic Signal](https://arxiv.org/abs/2509.02920)
*Jaliya L. Wijayaraja,Janaka L. Wijekoon,Malitha Wijesundara*

Main category: cs.LG

TL;DR: 该论文提出了一种用于实时检测大象足印的分类框架，通过Contextually Customized Windowing (CCW)技术提高准确性，支持向量机分类在多种环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决人类-大象冲突（HEC）中实时检测大象足印的限制，提出资源受限实现的分类框架。

Method: 引入CCW事件检测技术，并与STA/LTA方法对比，使用SVM分类器进行足印分类。

Result: 最大检测范围为155.6米（控制条件）和140米（自然环境）；SVM分类器在控制环境中准确率达99%，自然栖息地为73%。

Conclusion: CCW技术和SVM分类器有效提升了大象足印检测的准确性和适用性，尤其在HEC高风险区域。

Abstract: Detecting elephants through seismic signals is an emerging research topic
aimed at developing solutions for Human-Elephant Conflict (HEC). Despite the
promising results, such solutions heavily rely on manual classification of
elephant footfalls, which limits their applicability for real-time
classification in natural settings. To address this limitation and build on our
previous work, this study introduces a classification framework targeting
resource-constrained implementations, prioritizing both accuracy and
computational efficiency. As part of this framework, a novel event detection
technique named Contextually Customized Windowing (CCW), tailored specifically
for detecting elephant footfalls, was introduced, and evaluations were
conducted by comparing it with the Short-Term Average/Long-Term Average
(STA/LTA) method. The yielded results show that the maximum validated detection
range was 155.6 m in controlled conditions and 140 m in natural environments.
Elephant footfall classification using Support Vector Machine (SVM) with a
Radial Basis Function (RBF) kernel demonstrated superior performance across
multiple settings, achieving an accuracy of 99% in controlled environments, 73%
in natural elephant habitats, and 70% in HEC-prone human habitats, the most
challenging scenario. Furthermore, feature impact analysis using explainable AI
identified the number of Zero Crossings and Dynamic Time Warping (DTW)
Alignment Cost as the most influential factors in all experiments, while
Predominant Frequency exhibited significant influence in controlled settings.

</details>


### [54] [TopoMap: A Feature-based Semantic Discriminator of the Topographical Regions in the Test Input Space](https://arxiv.org/abs/2509.03242)
*Gianmarco De Vita,Nargiz Humbatova,Paolo Tonella*

Main category: cs.LG

TL;DR: 本文提出了一种名为TopoMap的模型无关方法，用于为深度学习模型输入特征空间创建地形图，通过降维和聚类技术区分输入，并利用DNN选择最优配置，实验效果优于随机选择。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习测试方法在扰动输入时可能忽略不同特征空间的失败诱导特征，因此需要一种通用方法来绘制输入特征空间的地形图，以系统化测试覆盖。

Method: TopoMap基于输入特征空间，采用降维和聚类技术对输入进行分组，并通过DNN模拟人类评估，自动选择最优的嵌入/聚类配置。

Result: 实验表明，TopoMap生成的地形图具有可区分和有意义的区域，且在突变分析中表现优于随机选择，平均提升35%（可杀死突变）和61%（不可杀死突变）。

Conclusion: TopoMap提供了一种高效、通用的方法，用于系统化测试深度学习的输入特征空间。

Abstract: Testing Deep Learning (DL)-based systems is an open challenge. Although it is
relatively easy to find inputs that cause a DL model to misbehave, the grouping
of inputs by features that make the DL model under test fail is largely
unexplored. Existing approaches for DL testing introduce perturbations that may
focus on specific failure-inducing features, while neglecting others that
belong to different regions of the feature space. In this paper, we create an
explicit topographical map of the input feature space. Our approach, named
TopoMap, is both black-box and model-agnostic as it relies solely on features
that characterise the input space. To discriminate the inputs according to the
specific features they share, we first apply dimensionality reduction to obtain
input embeddings, which are then subjected to clustering. Each DL model might
require specific embedding computations and clustering algorithms to achieve a
meaningful separation of inputs into discriminative groups. We propose a novel
way to evaluate alternative configurations of embedding and clustering
techniques. We used a deep neural network (DNN) as an approximation of a human
evaluator who could tell whether a pair of clusters can be discriminated based
on the features of the included elements. We use such a DNN to automatically
select the optimal topographical map of the inputs among all those that are
produced by different embedding/clustering configurations. The evaluation
results show that the maps generated by TopoMap consist of distinguishable and
meaningful regions. In addition, we evaluate the effectiveness of TopoMap using
mutation analysis. In particular, we assess whether the clusters in our
topographical map allow for an effective selection of mutation-killing inputs.
Experimental results show that our approach outperforms random selection by 35%
on average on killable mutants; by 61% on non-killable ones.

</details>


### [55] [DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling](https://arxiv.org/abs/2509.03472)
*Yubo Gao,Renbo Tu,Gennady Pekhimenko,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: DP-SGD在低精度量化下会导致更高的准确性下降，QPQuant通过动态量化框架有效解决这一问题，优化了精度与计算的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究量化在DP-SGD中对模型准确性的负面影响及其原因，并提出解决方案以改善隐私保护训练中的效率问题。

Method: 提出QPQuant框架，结合概率性采样层和损失敏感层优先化的动态量化策略，减少量化方差。

Result: QPQuant在多数据集和模型上显著优于静态量化基线，提升计算效率且准确性损失低于2%。

Conclusion: QPQuant通过动态量化优化了DP-SGD的训练效率与准确性，适用于低精度硬件。

Abstract: Differentially-Private SGD (DP-SGD) is a powerful technique to protect user
privacy when using sensitive data to train neural networks. During training,
converting model weights and activations into low-precision formats, i.e.,
quantization, can drastically reduce training times, energy consumption, and
cost, and is thus a widely used technique. In this work, we demonstrate that
quantization causes significantly higher accuracy degradation in DP-SGD
compared to regular SGD. We observe that this is caused by noise injection in
DP-SGD, which amplifies quantization variance, leading to disproportionately
large accuracy degradation. To address this challenge, we present QPQuant, a
dynamic quantization framework that adaptively selects a changing subset of
layers to quantize at each epoch. Our method combines two key ideas that
effectively reduce quantization variance: (i) probabilistic sampling of the
layers that rotates which layers are quantized every epoch, and (ii) loss-aware
layer prioritization, which uses a differentially private loss sensitivity
estimator to identify layers that can be quantized with minimal impact on model
quality. This estimator consumes a negligible fraction of the overall privacy
budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50,
and DenseNet121 across a range of datasets demonstrate that DPQuant
consistently outperforms static quantization baselines, achieving near
Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical
throughput improvements on low-precision hardware, with less than 2% drop in
validation accuracy.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [56] [Multi-level SSL Feature Gating for Audio Deepfake Detection](https://arxiv.org/abs/2509.03409)
*Hoan My Tran,Damien Lolive,Aghilas Sini,Arnaud Delhay,Pierre-François Marteau,David Guennec*

Main category: cs.SD

TL;DR: 论文提出了一种基于XLS-R模型的门控机制和多核卷积分类器，结合CKA相似性度量，用于检测合成语音的深度伪造，并在多语言数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步使合成语音更逼真，但也带来欺诈和身份盗窃等风险，现有检测方法对新攻击和多语言的泛化能力有限。

Method: 利用XLS-R模型提取特征，结合多核卷积分类器和CKA相似性度量，提高对合成语音模式的多样化学习。

Result: 实验表明，该方法在域内和域外数据集（包括多语言样本）上均达到最优性能。

Conclusion: 该方法是一种检测语音深度伪造的通用解决方案，具有良好的泛化能力。

Abstract: Recent advancements in generative AI, particularly in speech synthesis, have
enabled the generation of highly natural-sounding synthetic speech that closely
mimics human voices. While these innovations hold promise for applications like
assistive technologies, they also pose significant risks, including misuse for
fraudulent activities, identity theft, and security threats. Current research
on spoofing detection countermeasures remains limited by generalization to
unseen deepfake attacks and languages. To address this, we propose a gating
mechanism extracting relevant feature from the speech foundation XLS-R model as
a front-end feature extractor. For downstream back-end classifier, we employ
Multi-kernel gated Convolution (MultiConv) to capture both local and global
speech artifacts. Additionally, we introduce Centered Kernel Alignment (CKA) as
a similarity metric to enforce diversity in learned features across different
MultiConv layers. By integrating CKA with our gating mechanism, we hypothesize
that each component helps improving the learning of distinct synthetic speech
patterns. Experimental results demonstrate that our approach achieves
state-of-the-art performance on in-domain benchmarks while generalizing
robustly to out-of-domain datasets, including multilingual speech samples. This
underscores its potential as a versatile solution for detecting evolving speech
deepfake threats.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [57] [On the Optimization of Methods for Establishing Well-Connected Communities](https://arxiv.org/abs/2509.02590)
*Mohammad Dindoost,Oliver Alvarado Rodriguez,Bartosz Bryg,Minhyuk Park,George Chacko,Tandy Warnow,David A. Bader*

Main category: cs.SI

TL;DR: 论文提出了针对大规模图的高效并行实现WCC和CM算法，显著提升了社区检测的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的社区检测方法在处理大规模图时，常常面临计算效率低下的问题，尤其是WCC和CM算法虽能改进聚类结果，但其计算复杂度限制了其在大规模图上的应用。

Method: 利用HPE Chapel编程语言的并行特性，设计了高效的并行算法，并将其集成到Arkouda/Arachne框架中，以支持大规模图的快速处理。

Result: 实现了在128核上对超过20亿边的Open-Alex数据集进行分钟级别的社区检测，显著提升了性能。

Conclusion: 该优化方案为大规模图的连通性保持聚类提供了实用的解决方案，填补了现有技术的空白。

Abstract: Community detection plays a central role in uncovering meso scale structures
in networks. However, existing methods often suffer from disconnected or weakly
connected clusters, undermining interpretability and robustness. Well-Connected
Clusters (WCC) and Connectivity Modifier (CM) algorithms are post-processing
techniques that improve the accuracy of many clustering methods. However, they
are computationally prohibitive on massive graphs. In this work, we present
optimized parallel implementations of WCC and CM using the HPE Chapel
programming language. First, we design fast and efficient parallel algorithms
that leverage Chapel's parallel constructs to achieve substantial performance
improvements and scalability on modern multicore architectures. Second, we
integrate this software into Arkouda/Arachne, an open-source, high-performance
framework for large-scale graph analytics. Our implementations uniquely enable
well-connected community detection on massive graphs with more than 2 billion
edges, providing a practical solution for connectivity-preserving clustering at
web scale. For example, our implementations of WCC and CM enable community
detection of the over 2-billion edge Open-Alex dataset in minutes using 128
cores, a result infeasible to compute previously.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [58] [Treasure Hunt in Anonymous Graphs with Quantum Pebbles by Oblivious Agents](https://arxiv.org/abs/2509.02909)
*Gaurav Gaur,Barun Gorain,Rishi Ranjan Singh,Daya Gaur*

Main category: quant-ph

TL;DR: 提出了一种利用量子信息在匿名图中寻找静态宝藏的新方法，使用量子石子替代传统石子，显著提高了搜索效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在匿名图中搜索宝藏时依赖经典石子，信息传输受限且遍历复杂度高，量子石子提供了一种更高效的解决方案。

Method: 利用量子石子周期性发射量子态，代理通过多基测量识别路径，量子态的独特编码提高了搜索效率。

Result: 代理能在D步内用D个量子石子找到宝藏，且每个节点仅需少量测量即可确保高成功概率。

Conclusion: 量子石子不仅模拟了经典石子的功能，还提高了搜索效率，为量子增强分布式算法提供了新方向。

Abstract: We investigate the problem of finding a static treasure in anonymous graphs
using oblivious agents and introduce a novel approach that leverages quantum
information. In anonymous graphs, vertices are unlabelled, indistinguishable,
and edges are locally labelled with port numbers. Agents typically rely on
stationary classical pebbles placed by an oracle to guide their search.
However, this classical approach is constrained by limited information
transmission and high traversal complexity. Classical pebbles are not
sufficient for search if the agents are oblivious. We propose the first use of
quantum pebbles for search in anonymous graphs. Quantum pebbles periodically
emit qubits in a fixed quantum state. Each pebble encodes the port number to
the next node using a unique quantum state. The agent determines the correct
path by performing measurements in multiple bases, exploiting the probabilistic
nature of quantum measurement to distinguish states. We show that this strategy
enables an oblivious agent to locate the treasure in $D$ steps using $D$
quantum pebbles, where $D$ is the length of the shortest path between the
starting point and the treasure. Moreover, only $O((\log D + \log \Delta)/(\log
1/\delta))$ measurements per node are required to ensure high success
probability in a graph with maximum degree $\Delta$ where $\delta =
\cos^2(\frac{\pi}{2\Delta})$. We propose the use of quantum information as a
guidance mechanism in anonymous graph search. We demonstrate that quantum
pebbles can not only emulate the functionality of classical pebbles but can do
so with improved efficiency, offering a promising direction for future
quantum-enhanced distributed algorithms.

</details>


### [59] [Programmable Quantum Matter: Heralding Large Cluster States in Driven Inhomogeneous Spin Ensembles](https://arxiv.org/abs/2509.02992)
*Pratyush Anand,Louis Follet,Odiel Hooybergs,Dirk R. Englund*

Main category: quant-ph

TL;DR: 提出一种利用原子发射器非均匀性的框架，将生成光学预告自旋簇态的资源从O(N_q)降至O(1)，同时优化脉冲序列以提升门保真度和相干时间，并引入高效的单光子纠缠协议。


<details>
  <summary>Details</summary>
Motivation: 解决固态原子发射器非均匀性带来的量子控制难题，提升资源效率和计算鲁棒性。

Method: 利用非均匀性设计框架，优化脉冲序列，结合CPMG动态解耦协议和单光子纠缠协议。

Result: 单量子比特门保真度超99.99%，相干时间提升7倍，生成更多纠缠链接。

Conclusion: 为异构自旋系综的量子计算提供了可扩展且鲁棒的工具。

Abstract: Atom-like emitters in solids are promising platforms for quantum sensing and
information processing, but inhomogeneities in the emitter fine structure
complicate quantum control. We present a framework that leverages this
diversity to reduce the resources for generating optically heralded spin
cluster states across $N_q$ emitters from the conventional order $O(N_q)$ to
$O(1)$ in ensembles of $N_q \sim 10$-$100$. An optimized pulse sequence
simultaneously corrects pulse-length and detuning errors, achieving
single-qubit gate fidelities exceeding $99.99\%$ for errors (normalized
relative to the Rabi drive strength) up to 0.3, while maintaining fidelities
above $99\%$ for errors as large as 0.4. Applied as a Carr-Purcell-Meiboom-Gill
(CPMG) dynamical decoupling protocol to the dominant noise spectrum of
silicon-vacancy centers in diamond, it enhances ensemble coherence times by
over $7\times$ compared to interleaved bang-bang based CPMG. For
state-of-the-art dilution refrigerators, global resonant optimal decoupling
across $N_q$ spins sharply reduces heating, addressing the trade-off between
the spin coherence and scaling to $N_q \gg 1$. We further introduce a modified
single-photon entanglement protocol with an efficient algorithm for
deterministic entanglement compilation. Depending on the decoupling time
window, our method yields order $O(10^2$-$10^4)$ more entanglement links than
bang-bang sequences, with theoretical guarantees of order $\Omega(N_q)$ unique
links, improvable by control tuning. Together, these techniques provide
scalable tools - including global control, phase denoising, remote
entanglement, and compilation - for robust quantum computing architectures with
heterogeneous spin ensembles.

</details>


### [60] [An experience-based classification of quantum bugs in quantum software](https://arxiv.org/abs/2509.03280)
*Nils Quetschlich,Olivia Di Matteo*

Main category: quant-ph

TL;DR: 论文探讨了量子计算中调试量子错误的挑战，分析了14个量子错误的来源和解决方法，发现错误通常由多因素交互引起，且调试策略与错误类别间无明显关系。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，亟需专门的调试方法来解决量子错误，但现有指导不足。

Method: 研究通过开发者的经验和分析GitHub开源仓库，总结了14个量子错误的背景、症状及修复技术。

Result: 发现量子错误多为多因素交互导致，且调试策略与错误类别间无明显关联。

Conclusion: 需要进一步研究以系统化量子调试策略。

Abstract: As quantum computers continue to improve in quality and scale, there is a
growing need for accessible software frameworks for programming them. However,
the unique behavior of quantum systems means specialized approaches, beyond
traditional software development, are required. This is particularly true for
debugging due to quantum bugs, i.e., bugs that occur precisely because an
algorithm is a quantum algorithm. Pinpointing a quantum bug's root cause often
requires significant developer time, as there is little established guidance
for quantum debugging techniques. Developing such guidance is the main
challenge we sought to address. In this work, we describe a set of 14 quantum
bugs, sourced primarily from our experience as quantum software developers, and
supplemented by analysis of open-source GitHub repositories. We detail their
context, symptoms, and the techniques applied to identify and fix them. While
classifying these bugs based on existing schemes, we observed that most emerged
due to unique interactions between multiple aspects of an algorithm or
workflow. In other words, they occurred because more than one thing went wrong,
which provided important insight into why quantum debugging is more
challenging. Furthermore, based on this clustering, we found that -
unexpectedly - there is no clear relationship between debugging strategies and
bug classes. Further research is needed to develop effective and systematic
quantum debugging strategies.

</details>


### [61] [\textit{In Silico} Benchmarking of Detectable Byzantine Agreement in Noisy Quantum Networks](https://arxiv.org/abs/2509.02629)
*Mayank Bhatia,Shaan Doshi,Daniel Winton,Brian Doolittle,Bruno Abreu,Santiago Núñez-Corrales*

Main category: quant-ph

TL;DR: 该论文研究利用EPR对的量子可检测拜占庭协议（QDBA），通过实验模拟评估其在现实量子网络中的性能，为实验实现提供参考。


<details>
  <summary>Details</summary>
Motivation: 量子通信资源在分布式协议中具有优势，尤其是在需要对抗干扰的拜占庭协议（BA）中。QDBA通过纠缠量子态超越经典限制，但其实际实现仍需探索。

Method: 使用EPR对实现QDBA，通过Aliro量子网络模拟器在现实条件下全面评估协议性能，考察网络规模、节点数量、纠缠资源和噪声模型等因素。

Result: 通过数值实验揭示了物理参数对协议性能的影响，确定了关键阈值和最优操作区间，为实验实现提供了指导。

Conclusion: 该研究将量子共识协议的理论进展与实践网络实现相结合，为在现实噪声环境中评估和优化QDBA提供了具体参考。

Abstract: Quantum communication resources offer significant advantages for
fault-tolerant distributed protocols, particularly in Byzantine Agreement (BA),
where reliability against adversarial interference is essential. Quantum
Detectable Byzantine Agreement (QDBA) enables consensus protocols that surpass
classical limitations by leveraging entangled quantum states. In this work, we
focus on the practical realization of QDBA using Einstein-Podolsky-Rosen (EPR)
pairs, the simplest maximally entangled quantum resources, making the protocol
experimentally accessible across current quantum hardware platforms. We present
a comprehensive computational study of the EPRQDBA protocol under realistic
quantum network conditions, utilizing the Aliro Quantum Network Simulator to
evaluate the performance and robustness of the protocol. Our simulations
systematically explore the protocol's parameter space --including variations in
network size, traitorous node count, the amount of entanglement consumed in the
protocol, and physically motivated noise models tailored specifically for
superconducting and photonic qubit technologies. Through extensive numerical
experiments, we provide insights into how these physically realistic parameters
impact protocol performance, establishing critical thresholds and optimal
operational regimes for experimental implementations. This work bridges
theoretical advances in quantum consensus protocols with practical network
implementations, offering a concrete reference for experimentalists. Our
findings serve as a guideline for evaluating and optimizing QDBA
implementations in realistic, noisy environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [62] [app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding](https://arxiv.org/abs/2509.03310)
*Evgenii Kniazev,Arseny Kravchenko,Igor Rekun,James Broadhead,Nikita Shamgunov,Pranav Sah,Pratik Nichite,Ivan Yamshchikov*

Main category: cs.AI

TL;DR: app.build是一个开源框架，通过系统验证和结构化环境改善LLM应用生成，展示了全面验证和高性能的开源实现。


<details>
  <summary>Details</summary>
Motivation: 提升基于LLM的应用生成的可靠性和效率，同时证明结构化环境对AI代理扩展的重要性。

Method: 结合多层次验证流程、特定堆栈编排和模型无关架构，并在三个参考堆栈中实现。

Result: 在30个生成任务中，全面验证实现了73.3%的可行率和30%的完美质量得分，开源模型性能达到闭源模型的80.8%。

Conclusion: 扩展可靠AI代理需要扩展环境而不仅仅是模型，开源框架已获社区广泛采用，生成超3000个应用。

Abstract: We present app.build (https://github.com/appdotbuild/agent/), an open-source
framework that improves LLM-based application generation through systematic
validation and structured environments. Our approach combines multi-layered
validation pipelines, stack-specific orchestration, and model-agnostic
architecture, implemented across three reference stacks. Through evaluation on
30 generation tasks, we demonstrate that comprehensive validation achieves
73.3% viability rate with 30% reaching perfect quality scores, while
open-weights models achieve 80.8% of closed-model performance when provided
structured environments. The open-source framework has been adopted by the
community, with over 3,000 applications generated to date. This work
demonstrates that scaling reliable AI agents requires scaling environments, not
just models -- providing empirical insights and complete reference
implementations for production-oriented agent systems.

</details>


### [63] [Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics](https://arxiv.org/abs/2509.02751)
*Matthew Russo,Tim Kraska*

Main category: cs.AI

TL;DR: 结合语义算子的优化执行与Deep Research系统的灵活性，提出原型以提升AI驱动分析的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有语义算子成本高且不适合交互任务，而Deep Research系统缺乏查询计划优化，需结合两者优势。

Method: 构建原型使Deep Research代理能编写并执行优化的语义算子程序。

Result: 原型在基础查询中优于手工语义算子程序和开放Deep Research系统，F1得分提升1.95倍，成本节约76.8%。

Conclusion: 原型成功结合语义算子与Deep Research系统，为高效AI驱动分析提供了新方向。

Abstract: With advances in large language models (LLMs), researchers are creating new
systems that can perform AI-driven analytics over large unstructured datasets.
Recent work has explored executing such analytics queries using semantic
operators -- a declarative set of AI-powered data transformations with natural
language specifications. However, even when optimized, these operators can be
expensive to execute on millions of records and their iterator execution
semantics make them ill-suited for interactive data analytics tasks. In another
line of work, Deep Research systems have demonstrated an ability to answer
natural language question(s) over large datasets. These systems use one or more
LLM agent(s) to plan their execution, process the dataset(s), and iteratively
refine their answer. However, these systems do not explicitly optimize their
query plans which can lead to poor plan execution. In order for AI-driven
analytics to excel, we need a runtime which combines the optimized execution of
semantic operators with the flexibility and more dynamic execution of Deep
Research systems. As a first step towards this vision, we build a prototype
which enables Deep Research agents to write and execute optimized semantic
operator programs. We evaluate our prototype and demonstrate that it can
outperform a handcrafted semantic operator program and open Deep Research
systems on two basic queries. Compared to a standard open Deep Research agent,
our prototype achieves up to 1.95x better F1-score. Furthermore, even if we
give the agent access to semantic operators as tools, our prototype still
achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its
optimized execution.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [64] [OpenAIs HealthBench in Action: Evaluating an LLM-Based Medical Assistant on Realistic Clinical Queries](https://arxiv.org/abs/2509.02594)
*Sandhanakrishnan Ravichandran,Shivesh Kumar,Rogerio Corga Da Silva,Miguel Romano,Reinhard Berkels,Michiel van der Heijden,Olivier Fail,Valentine Emmanuel Gnanapragasam*

Main category: q-bio.QM

TL;DR: 论文评估了DR.INFO这一基于RAG的临床支持助手在复杂、高风险的临床场景中的表现，使用HealthBench这一开放式的专家标注健康对话基准，显示出其优于其他前沿LLMs和类似助手。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法（如多选题）无法全面反映LLMs在临床场景中的能力（如上下文推理、情境意识和不确定性处理等），因此需要更复杂的基准来评估。

Method: 通过HealthBench这一基于评分的开放对话基准，评估DR.INFO在1,000个困难案例中的表现，并与前沿LLMs和类似RAG助手比较。

Result: DR.INFO在Hard子集上得分为0.51，优于GPT-5、GPT-4等；在与其他RAG助手比较中得分为0.54，显示出其在沟通、指令遵循和准确性上的优势。

Conclusion: 研究表明，基于行为的评分基准对开发可靠、可信赖的AI临床助手至关重要，同时揭示了DR.INFO在情境意识和回答完整性方面的改进空间。

Abstract: Evaluating large language models (LLMs) on their ability to generate
high-quality, accurate, situationally aware answers to clinical questions
requires going beyond conventional benchmarks to assess how these systems
behave in complex, high-stake clincal scenarios. Traditional evaluations are
often limited to multiple-choice questions that fail to capture essential
competencies such as contextual reasoning, awareness and uncertainty handling
etc. To address these limitations, we evaluate our agentic, RAG-based clinical
support assistant, DR.INFO, using HealthBench, a rubric-driven benchmark
composed of open-ended, expert-annotated health conversations. On the Hard
subset of 1,000 challenging examples, DR.INFO achieves a HealthBench score of
0.51, substantially outperforming leading frontier LLMs (GPT-5, o3, Grok 3,
GPT-4, Gemini 2.5, etc.) across all behavioral axes (accuracy, completeness,
instruction following, etc.). In a separate 100-sample evaluation against
similar agentic RAG assistants (OpenEvidence, Pathway.md), it maintains a
performance lead with a health-bench score of 0.54. These results highlight
DR.INFOs strengths in communication, instruction following, and accuracy, while
also revealing areas for improvement in context awareness and completeness of a
response. Overall, the findings underscore the utility of behavior-level,
rubric-based evaluation for building a reliable and trustworthy AI-enabled
clinical support assistant.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [65] [Who Owns The Robot?: Four Ethical and Socio-technical Questions about Wellbeing Robots in the Real World through Community Engagement](https://arxiv.org/abs/2509.02624)
*Minja Axelsson,Jiaee Cheong,Rune Nyrup,Hatice Gunes*

Main category: cs.CY

TL;DR: 探讨了机器人教练在健康促进中的伦理和社会技术问题，通过社区调查提出四个关键问题，指导机器人开发。


<details>
  <summary>Details</summary>
Motivation: 研究机器人健康教练在实际应用中的伦理和社会技术挑战，以确保技术发展与公共利益一致。

Method: 通过三个代表性不足的社区（科学节公众、女性计算机科学家、人文学者）的定性数据分析，使用主题分析法。

Result: 提取了四个关键伦理和社会技术问题，为机器人开发提供框架。

Conclusion: 提出了四个问题框架，鼓励开发者在机器人应用中进行伦理和社会技术维度的反思与对话。

Abstract: Recent studies indicate that robotic coaches can play a crucial role in
promoting wellbeing. However, the real-world deployment of wellbeing robots
raises numerous ethical and socio-technical questions and concerns. To explore
these questions, we undertake a community-centered investigation to examine
three different communities' perspectives on using robotic wellbeing coaches in
real-world environments. We frame our work as an anticipatory ethical
investigation, which we undertake to better inform the development of robotic
technologies with communities' opinions, with the ultimate goal of aligning
robot development with public interest. We conducted workshops with three
communities who are under-represented in robotics development: 1) members of
the public at a science festival, 2) women computer scientists at a conference,
and 3) humanities researchers interested in history and philosophy of science.
In the workshops, we collected qualitative data using the Social Robot
Co-Design Canvas on Ethics. We analysed the collected qualitative data with
Thematic Analysis, informed by notes taken during workshops. Through our
analysis, we identify four themes regarding key ethical and socio-technical
questions about the real-world use of wellbeing robots. We group participants'
insights and discussions around these broad thematic questions, discuss them in
light of state-of-the-art literature, and highlight areas for future
investigation. Finally, we provide the four questions as a broad framework that
roboticists can and should use during robotic development and deployment, in
order to reflect on the ethics and socio-technical dimensions of their robotic
applications, and to engage in dialogue with communities of robot users. The
four questions are: 1) Is the robot safe and how can we know that?, 2) Who is
the robot built for and with?, 3) Who owns the robot and the data?, and 4) Why
a robot?.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [66] [Secure Password Generator Based on Secure Pseudo-Random Number Generator](https://arxiv.org/abs/2509.02578)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 本文提出一种使用密码学安全伪随机数生成器（PRNG）的安全密码生成方法，通过多种MAC算法生成高随机性密码，并通过NIST标准验证其熵和IID特性。


<details>
  <summary>Details</summary>
Motivation: 近年来网站账户和密码泄露事件频发，凸显密码安全的重要性，急需一种能生成高安全性密码的方法。

Method: 采用密码学安全的PRNG，结合HMAC、CMAC和KMAC等MAC算法生成随机值，用于密码生成。

Result: 实验表明，该方法满足NIST SP 800-90B的熵和IID要求，能生成高随机性和安全性的密码。

Conclusion: 所提方法能有效提升密码安全性，为系统安全和个人数据保护提供支持。

Abstract: In recent years, numerous incidents involving the leakage of website accounts
and text passwords (referred to as passwords) have raised significant concerns
regarding the potential exposure of personal information. These events
underscore the critical importance of both information security and password
protection. While many of these breaches are attributable to vulnerabilities
within website infrastructure, the strength and security of the passwords
themselves also play a crucial role. Consequently, the creation of secure
passwords constitutes a fundamental aspect of enhancing overall system security
and protecting personal data. In response to these challenges, this study
presents a secure password generation approach utilizing a cryptographically
secure Pseudo-Random Number Generator (PRNG). The generator is implemented
using a range of Message Authentication Code (MAC) algorithms, including the
Keyed-Hash Message Authentication Code (HMAC), Cipher-based Message
Authentication Code (CMAC), and KECCAK Message Authentication Code (KMAC), to
produce robust random values suitable for password generation. To evaluate the
proposed method, empirical assessments were conducted in accordance with the
guidelines provided in the National Institute of Standards and Technology
(NIST) Special Publication (SP) 800-90B. The evaluation focused on two primary
aspects: entropy estimation and verification of independent and identically
distributed (IID) properties. Experimental results indicate that the proposed
method satisfies both entropy and IID requirements, thereby demonstrating its
ability to generate passwords with a high degree of randomness and security.

</details>


### [67] [TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum](https://arxiv.org/abs/2509.03037)
*Shuzheng Wang,Yue Huang,Zhuoer Xu,Yuming Huang,Jing Tang*

Main category: cs.CR

TL;DR: TraceLLM 是一个利用大语言模型（LLMs）结合执行轨迹和反编译合约代码的安全分析框架，显著提高了以太坊智能合约攻击分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的安全分析方法难以全面分析以太坊智能合约的复杂攻击场景，尤其是针对未验证代码和代理架构的场景。

Method: 提出 TraceLLM，结合异常执行路径识别算法和优化的反编译工具，为大语言模型提供明确攻击路径。

Result: 在27个真实案例中，TraceLLM识别攻击者地址的精度达85.19%，报告准确率比最佳基线高25.93%。在148个现实事件中，专家验证的准确率为66.22%。

Conclusion: TraceLLM通过整合轨迹与代码分析，显著提升了安全分析的自动化能力和准确性。

Abstract: Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet
comprehensive security analysis remains difficult due to unverified code,
proxy-based architectures, and the reliance on manual inspection of complex
execution traces. Existing approaches fall into two main categories: anomaly
transaction detection, which flags suspicious transactions but offers limited
insight into specific attack strategies hidden in execution traces inside
transactions, and code vulnerability detection, which cannot analyze unverified
contracts and struggles to show how identified flaws are exploited in real
incidents. As a result, analysts must still manually align transaction traces
with contract code to reconstruct attack scenarios and conduct forensics. To
address this gap, TraceLLM is proposed as a framework that leverages LLMs to
integrate execution trace-level detection with decompiled contract code. We
introduce a new anomaly execution path identification algorithm and an
LLM-refined decompile tool to identify vulnerable functions and provide
explicit attack paths to LLM. TraceLLM establishes the first benchmark for
joint trace and contract code-driven security analysis. For comparison, proxy
baselines are created by jointly transmitting the results of three
representative code analysis along with raw traces to LLM. TraceLLM identifies
attacker and victim addresses with 85.19\% precision and produces automated
reports with 70.37\% factual precision across 27 cases with ground truth expert
reports, achieving 25.93\% higher accuracy than the best baseline. Moreover,
across 148 real-world Ethereum incidents, TraceLLM automatically generates
reports with 66.22\% expert-verified accuracy, demonstrating strong
generalizability.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [68] [A description of the radio astronomy data processing tool DDF Pipeline](https://arxiv.org/abs/2509.03075)
*Mathis Certenais,François Bodin,Laurent Morin*

Main category: astro-ph.IM

TL;DR: 介绍了DDF Pipeline，一种射电天文学数据处理工具，初步设计用于LO-FAR射电望远镜，并计划用于SKA项目。


<details>
  <summary>Details</summary>
Motivation: 为LO-FAR和SKA提供高效的数据处理工具。

Method: 描述了DDF Pipeline软件，并进行了粗粒度性能分析。

Result: 展示了DDF Pipeline的性能特征。

Conclusion: DDF Pipeline是一种适用于射电天文数据处理的工具，有望用于SKA项目。

Abstract: This paper presents the DDF Pipeline, a radio astronomy data processing tool
initially designed for the LOw-Frequency ARray (LO- FAR) radio-telescope and a
candidate for processing data from the Square Kilometre Array (SKA). This work
describes the DDF Pipeline software and presents a coarse-grain profiling
execution to characterize its performance.

</details>
