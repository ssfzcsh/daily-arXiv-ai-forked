{"id": "2508.08322", "pdf": "https://arxiv.org/pdf/2508.08322", "abs": "https://arxiv.org/abs/2508.08322", "authors": ["Muhammad Haseeb"], "title": "Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code", "categories": ["cs.SE", "cs.AI", "68T07, 68N01", "D.2.2; I.2.6; D.2.5; I.2.8"], "comment": "15 pages, 5 figures, research paper on multi-agent LLM systems for\n  code generation", "summary": "Large Language Models (LLMs) have shown promise in automating code generation\nand software engineering tasks, yet they often struggle with complex,\nmulti-file projects due to context limitations and knowledge gaps. We propose a\nnovel context engineering workflow that combines multiple AI components: an\nIntent Translator (GPT-5) for clarifying user requirements, an Elicit-powered\nsemantic literature retrieval for injecting domain knowledge, NotebookLM-based\ndocument synthesis for contextual understanding, and a Claude Code multi-agent\nsystem for code generation and validation. Our integrated approach leverages\nintent clarification, retrieval-augmented generation, and specialized\nsub-agents orchestrated via Claude's agent framework. We demonstrate that this\nmethod significantly improves the accuracy and reliability of code assistants\nin real-world repositories, yielding higher single-shot success rates and\nbetter adherence to project context than baseline single-agent approaches.\nQualitative results on a large Next.js codebase show the multi-agent system\neffectively plans, edits, and tests complex features with minimal human\nintervention. We compare our system with recent frameworks like CodePlan,\nMASAI, and HyperAgent, highlighting how targeted context injection and agent\nrole decomposition lead to state-of-the-art performance. Finally, we discuss\nthe implications for deploying LLM-based coding assistants in production, along\nwith lessons learned on context management and future research directions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591aAI\u7ec4\u4ef6\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5de5\u4f5c\u6d41\uff0c\u4ee5\u63d0\u9ad8\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u6587\u4ef6\u9879\u76ee\u4e2d\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u548c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u591a\u6587\u4ef6\u9879\u76ee\u65f6\uff0c\u4ecd\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u4e0d\u8db3\u548c\u77e5\u8bc6\u7f3a\u53e3\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e86\u591a\u7ec4\u4ef6\u534f\u540c\u7684\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u610f\u56fe\u7ffb\u8bd1\uff08GPT-5\uff09\u3001\u8bed\u4e49\u6587\u732e\u68c0\u7d22\uff08Elicit\uff09\u3001\u6587\u6863\u5408\u6210\uff08NotebookLM\uff09\u548c\u591a\u4ee3\u7406\u4ee3\u7801\u751f\u6210\uff08Claude Code\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5355\u6b21\u6210\u529f\u7387\uff0c\u4e14\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u4f18\u4e8e\u5355\u4ee3\u7406\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u591a\u4ee3\u7406\u7cfb\u7edf\u901a\u8fc7\u76ee\u6807\u660e\u786e\u7684\u4e0a\u4e0b\u6587\u6ce8\u5165\u548c\u89d2\u8272\u5206\u89e3\uff0c\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72LLM\u4ee3\u7801\u52a9\u624b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.08332", "pdf": "https://arxiv.org/pdf/2508.08332", "abs": "https://arxiv.org/abs/2508.08332", "authors": ["Humza Ashraf", "Syed Muhammad Danish", "Aris Leivadeas", "Yazan Otoum", "Zeeshan Sattar"], "title": "Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation. However,\ncommercial models like ChatGPT require significant computing power, which leads\nto high energy use and carbon emissions. This has raised concerns about their\nenvironmental impact. In this study, we evaluate open-source Small Language\nModels (SLMs) trained explicitly for code generation and compare their\nperformance and energy efficiency against large LLMs and efficient\nhuman-written Python code. The goal is to investigate whether SLMs can match\nthe performance of LLMs on certain types of programming problems while\nproducing more energy-efficient code. We evaluate 150 coding problems from\nLeetCode, evenly distributed across three difficulty levels: easy, medium, and\nhard. Our comparison includes three small open-source models, StableCode-3B,\nStarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial\nmodels, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using\nfour key metrics: run-time, memory usage, energy consumption, and correctness.\nWe use human-written solutions as a baseline to assess the quality and\nefficiency of the model-generated code. Results indicate that LLMs achieve the\nhighest correctness across all difficulty levels, but SLMs are often more\nenergy-efficient when their outputs are correct. In over 52% of the evaluated\nproblems, SLMs consumed the same or less energy than LLMs.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5c0f\u578b\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4e0e\u5927\u578b\u5546\u4e1a\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u6027\u80fd\u4e0e\u80fd\u6e90\u6548\u7387\u4e0a\u7684\u5dee\u5f02\uff0c\u53d1\u73b0SLMs\u5728\u80fd\u6548\u4e0a\u66f4\u4f18\uff0c\u4f46LLMs\u5728\u6b63\u786e\u6027\u4e0a\u66f4\u80dc\u4e00\u7b79\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u80fd\u8017\u9ad8\u4e14\u78b3\u6392\u653e\u91cf\u5927\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u662f\u5426\u80fd\u4ee5\u66f4\u4f4e\u7684\u80fd\u6e90\u6d88\u8017\u8fbe\u5230\u76f8\u8fd1\u7684\u6027\u80fd\u3002", "method": "\u8bc4\u4f30\u4e86150\u4e2aLeetCode\u7f16\u7a0b\u95ee\u9898\uff0c\u6d89\u53ca\u4e09\u79cd\u96be\u5ea6\u7ea7\u522b\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cdSLMs\u4e0e\u4e24\u79cdLLMs\u5728\u8fd0\u884c\u65f6\u95f4\u3001\u5185\u5b58\u4f7f\u7528\u3001\u80fd\u8017\u548c\u6b63\u786e\u6027\u4e0a\u7684\u8868\u73b0\u3002", "result": "LLMs\u5728\u6240\u6709\u96be\u5ea6\u7ea7\u522b\u4e0a\u6b63\u786e\u6027\u6700\u9ad8\uff0c\u4f46SLMs\u5728\u80fd\u6548\u4e0a\u6709\u4f18\u52bf\uff0c52%\u7684\u95ee\u9898\u4e2dSLMs\u80fd\u8017\u76f8\u540c\u6216\u66f4\u4f4e\u3002", "conclusion": "SLMs\u5728\u80fd\u6548\u4e0a\u4f18\u4e8eLLMs\uff0c\u9002\u7528\u4e8e\u5bf9\u80fd\u6e90\u654f\u611f\u7684\u573a\u666f\uff0c\u4f46LLMs\u4ecd\u4e3b\u5bfc\u9ad8\u6b63\u786e\u6027\u9700\u6c42\u7684\u4efb\u52a1\u3002"}}
{"id": "2508.08342", "pdf": "https://arxiv.org/pdf/2508.08342", "abs": "https://arxiv.org/abs/2508.08342", "authors": ["Maximilian Jungwirth", "Martin Gruber", "Gordon Fraser"], "title": "Improving Merge Pipeline Throughput in Continuous Integration via Pull Request Prioritization", "categories": ["cs.SE"], "comment": "This paper is accepted on the Industry Track of the 41st\n  International Conference on Software Maintenance and Evolution (ICSME 2025)", "summary": "Integrating changes into large monolithic software repositories is a critical\nstep in modern software development that substantially impacts the speed of\nfeature delivery, the stability of the codebase, and the overall productivity\nof development teams. To ensure the stability of the main branch, many\norganizations use merge pipelines that test software versions before the\nchanges are permanently integrated. However, the load on merge pipelines is\noften so high that they become bottlenecks, despite the use of parallelization.\nExisting optimizations frequently rely on specific build systems, limiting\ntheir generalizability and applicability. In this paper we propose to optimize\nthe order of PRs in merge pipelines using practical build predictions utilizing\nonly historical build data, PR metadata, and contextual information to estimate\nthe likelihood of successful builds in the merge pipeline. By dynamically\nprioritizing likely passing PRs during peak hours, this approach maximizes\nthroughput when it matters most. Experiments conducted on a real-world,\nlarge-scale project demonstrate that predictive ordering significantly\noutperforms traditional first-in-first-out (FIFO), as well as\nnon-learning-based ordering strategies. Unlike alternative optimizations, this\napproach is agnostic to the underlying build system and thus easily integrable\ninto existing automated merge pipelines.", "AI": {"tldr": "\u901a\u8fc7\u5386\u53f2\u6784\u5efa\u6570\u636e\u3001PR\u5143\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u9884\u6d4bPR\u6210\u529f\u6784\u5efa\u7684\u6982\u7387\uff0c\u4f18\u5316\u5408\u5e76\u7ba1\u9053\u4e2dPR\u7684\u987a\u5e8f\uff0c\u663e\u8457\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u5927\u578b\u8f6f\u4ef6\u4ed3\u5e93\u4e2d\u5408\u5e76\u7ba1\u9053\u7684\u8d1f\u8f7d\u8fc7\u9ad8\u6210\u4e3a\u74f6\u9888\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u6784\u5efa\u7cfb\u7edf\uff0c\u901a\u7528\u6027\u8f83\u5dee\u3002", "method": "\u5229\u7528\u5386\u53f2\u6570\u636e\u9884\u6d4bPR\u6784\u5efa\u6210\u529f\u7387\uff0c\u52a8\u6001\u4f18\u5148\u5904\u7406\u53ef\u80fd\u901a\u8fc7\u7684PR\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u9884\u6d4b\u7684\u6392\u5e8f\u7b56\u7565\u663e\u8457\u4f18\u4e8eFIFO\u548c\u975e\u5b66\u4e60\u6392\u5e8f\u7b56\u7565\uff0c\u4e14\u4e0d\u53d7\u6784\u5efa\u7cfb\u7edf\u9650\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u7528\u6027\u5f3a\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u5408\u5e76\u7ba1\u9053\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u8d1f\u8f7d\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2508.08545", "pdf": "https://arxiv.org/pdf/2508.08545", "abs": "https://arxiv.org/abs/2508.08545", "authors": ["Youssef Esseddiq Ouatiti", "Mohammed Sayagh", "Bram Adams", "Ahmed E. Hassan"], "title": "OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Developers insert logging statements in source code to capture relevant\nruntime information essential for maintenance and debugging activities. Log\nlevel choice is an integral, yet tricky part of the logging activity as it\ncontrols log verbosity and therefore influences systems' observability and\nperformance. Recent advances in ML-based log level prediction have leveraged\nlarge language models (LLMs) to propose log level predictors (LLPs) that\ndemonstrated promising performance improvements (AUC between 0.64 and 0.8).\nNevertheless, current LLM-based LLPs rely on randomly selected in-context\nexamples, overlooking the structure and the diverse logging practices within\nmodern software projects. In this paper, we propose OmniLLP, a novel LLP\nenhancement framework that clusters source files based on (1) semantic\nsimilarity reflecting the code's functional purpose, and (2) developer\nownership cohesion. By retrieving in-context learning examples exclusively from\nthese semantic and ownership aware clusters, we aim to provide more coherent\nprompts to LLPs leveraging LLMs, thereby improving their predictive accuracy.\nOur results show that both semantic and ownership-aware clusterings\nstatistically significantly improve the accuracy (by up to 8\\% AUC) of the\nevaluated LLM-based LLPs compared to random predictors (i.e., leveraging\nrandomly selected in-context examples from the whole project). Additionally,\nour approach that combines the semantic and ownership signal for in-context\nprediction achieves an impressive 0.88 to 0.96 AUC across our evaluated\nprojects. Our findings highlight the value of integrating software\nengineering-specific context, such as code semantic and developer ownership\nsignals into LLM-LLPs, offering developers a more accurate, contextually-aware\napproach to logging and therefore, enhancing system maintainability and\nobservability.", "AI": {"tldr": "OmniLLP\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u5f00\u53d1\u8005\u6240\u6709\u6743\u51dd\u805a\u6027\u805a\u7c7b\u6e90\u4ee3\u7801\u6587\u4ef6\uff0c\u63d0\u5347LLM\u65e5\u5fd7\u7ea7\u522b\u9884\u6d4b\u5668\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65e5\u5fd7\u7ea7\u522b\u9884\u6d4b\u5668\u4f9d\u8d56\u968f\u673a\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u5ffd\u7565\u4e86\u4ee3\u7801\u7ed3\u6784\u548c\u591a\u6837\u5316\u65e5\u5fd7\u5b9e\u8df5\uff0c\u5bfc\u81f4\u9884\u6d4b\u6548\u679c\u6709\u9650\u3002", "method": "OmniLLP\u901a\u8fc7\u8bed\u4e49\u548c\u5f00\u53d1\u8005\u6240\u6709\u6743\u805a\u7c7b\u6e90\u4ee3\u7801\u6587\u4ef6\uff0c\u4ece\u4e2d\u9009\u62e9\u4e0a\u4e0b\u6587\u5b66\u4e60\u793a\u4f8b\uff0c\u751f\u6210\u66f4\u4e00\u81f4\u7684\u63d0\u793a\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cOmniLLP\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff08AUC\u63d0\u53478%\uff09\uff0c\u7efc\u5408\u65b9\u6cd5\u5728\u8bc4\u4f30\u9879\u76ee\u4e2dAUC\u8fbe\u52300.88-0.96\u3002", "conclusion": "\u7ed3\u5408\u4ee3\u7801\u8bed\u4e49\u548c\u5f00\u53d1\u8005\u6240\u6709\u6743\u4fe1\u53f7\u53ef\u663e\u8457\u63d0\u5347LLM\u65e5\u5fd7\u7ea7\u522b\u9884\u6d4b\u5668\u7684\u51c6\u786e\u6027\uff0c\u589e\u5f3a\u7cfb\u7edf\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u89c2\u6d4b\u6027\u3002"}}
{"id": "2508.08467", "pdf": "https://arxiv.org/pdf/2508.08467", "abs": "https://arxiv.org/abs/2508.08467", "authors": ["Lei Zhang", "Shuyao Zhou", "Amna Liaqat", "Tinney Mak", "Brian Berengard", "Emily Qian", "Andr\u00e9s Monroy-Hern\u00e1ndez"], "title": "Empowering Children to Create AI-Enabled Augmented Reality Experiences", "categories": ["cs.HC", "cs.AI", "cs.GR", "cs.PL"], "comment": "Accepted to ACM UIST 2025", "summary": "Despite their potential to enhance children's learning experiences,\nAI-enabled AR technologies are predominantly used in ways that position\nchildren as consumers rather than creators. We introduce Capybara, an AR-based\nand AI-powered visual programming environment that empowers children to create,\ncustomize, and program 3D characters overlaid onto the physical world. Capybara\nenables children to create virtual characters and accessories using text-to-3D\ngenerative AI models, and to animate these characters through auto-rigging and\nbody tracking. In addition, our system employs vision-based AI models to\nrecognize physical objects, allowing children to program interactive behaviors\nbetween virtual characters and their physical surroundings. We demonstrate the\nexpressiveness of Capybara through a set of novel AR experiences. We conducted\nuser studies with 20 children in the United States and Argentina. Our findings\nsuggest that Capybara can empower children to harness AI in authoring\npersonalized and engaging AR experiences that seamlessly bridge the virtual and\nphysical worlds.", "AI": {"tldr": "Capybara\u662f\u4e00\u4e2a\u57fa\u4e8eAR\u548cAI\u7684\u89c6\u89c9\u7f16\u7a0b\u73af\u5883\uff0c\u5e2e\u52a9\u513f\u7ae5\u6210\u4e3a\u521b\u4f5c\u8005\u800c\u975e\u6d88\u8d39\u8005\uff0c\u901a\u8fc7\u6587\u672c\u751f\u62103D\u89d2\u8272\u5e76\u7f16\u7a0b\u4ea4\u4e92\u884c\u4e3a\u3002", "motivation": "\u5c3d\u7ba1AI\u548cAR\u6280\u672f\u6709\u6f5c\u529b\u63d0\u5347\u513f\u7ae5\u5b66\u4e60\u4f53\u9a8c\uff0c\u4f46\u76ee\u524d\u5b83\u4eec\u591a\u8ba9\u513f\u7ae5\u6210\u4e3a\u6d88\u8d39\u8005\u800c\u975e\u521b\u4f5c\u8005\uff0cCapybara\u65e8\u5728\u6539\u53d8\u8fd9\u4e00\u73b0\u72b6\u3002", "method": "Capybara\u7ed3\u5408\u6587\u672c\u751f\u62103D\u6a21\u578b\u3001\u81ea\u52a8\u9aa8\u9abc\u7ed1\u5b9a\u3001\u8eab\u4f53\u8ffd\u8e2a\u548c\u89c6\u89c9AI\u8bc6\u522b\uff0c\u652f\u6301\u513f\u7ae5\u7f16\u7a0b\u865a\u62df\u89d2\u8272\u4e0e\u7269\u7406\u73af\u5883\u7684\u4ea4\u4e92\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c20\u540d\u513f\u7ae5\u80fd\u5229\u7528Capybara\u521b\u5efa\u4e2a\u6027\u5316\u7684AR\u4f53\u9a8c\uff0c\u65e0\u7f1d\u8fde\u63a5\u865a\u62df\u4e0e\u7269\u7406\u4e16\u754c\u3002", "conclusion": "Capybara\u6210\u529f\u8d4b\u80fd\u513f\u7ae5\u5229\u7528AI\u521b\u4f5cAR\u5185\u5bb9\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u589e\u5f3a\u513f\u7ae5\u521b\u9020\u529b\u548c\u5b66\u4e60\u4f53\u9a8c\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08496", "pdf": "https://arxiv.org/pdf/2508.08496", "abs": "https://arxiv.org/abs/2508.08496", "authors": ["Mudathir Mohamed", "Nick Feng", "Andrew Reynolds", "Cesare Tinelli", "Clark Barrett", "Marsha Chechik"], "title": "Solving Set Constraints with Comprehensions and Bounded Quantifiers", "categories": ["cs.LO"], "comment": null, "summary": "Many real applications problems can be encoded easily as quantified formulas\nin SMT. However, this simplicity comes at the cost of difficulty during solving\nby SMT solvers. Different strategies and quantifier instantiation techniques\nhave been developed to tackle this. However, SMT solvers still struggle with\nquantified formulas generated by some applications. In this paper, we discuss\nthe use of set-bounded quantifiers, quantifiers whose variable ranges over a\nfinite set. These quantifiers can be implemented using quantifier-free fragment\nof the theory of finite relations with a filter operator, a form of restricted\ncomprehension, that constructs a subset from a finite set using a predicate. We\nshow that this approach outperforms other quantification techniques in\nsatisfiable problems generated by the SLEEC tool, and is very competitive on\nunsatisfiable benchmarks compared to LEGOS, a specialized solver for SLEEC. We\nalso identify a decidable class of constraints with restricted applications of\nthe filter operator, while showing that unrestricted applications lead to\nundecidability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728SMT\u4e2d\u4f7f\u7528\u96c6\u5408\u6709\u754c\u91cf\u8bcd\u7684\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u91cf\u5316\u5904\u7406\u65b9\u6848\uff0c\u5e76\u5728\u7279\u5b9a\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3SMT\u6c42\u89e3\u5668\u5728\u5904\u7406\u91cf\u5316\u516c\u5f0f\u65f6\u7684\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u67d0\u4e9b\u5e94\u7528\u751f\u6210\u7684\u516c\u5f0f\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63a2\u7d22\u4e86\u96c6\u5408\u6709\u754c\u91cf\u8bcd\u7684\u4f7f\u7528\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u91cf\u8bcd\u81ea\u7531\u7247\u6bb5\u548c\u6709\u9650\u5173\u7cfb\u7406\u8bba\u7684\u8fc7\u6ee4\u64cd\u4f5c\u7b26\u6765\u5b9e\u73b0\u96c6\u5408\u6709\u754c\u91cf\u8bcd\uff0c\u8fd9\u662f\u4e00\u79cd\u53d7\u9650\u7684\u7406\u89e3\u5f62\u5f0f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728SLEEC\u5de5\u5177\u751f\u6210\u7684\u53ef\u6ee1\u8db3\u95ee\u9898\u4e0a\u4f18\u4e8e\u5176\u4ed6\u91cf\u5316\u6280\u672f\uff0c\u5e76\u5728\u4e0d\u53ef\u6ee1\u8db3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0eLEGOS\uff08\u4e13\u7528\u4e8eSLEEC\u7684\u6c42\u89e3\u5668\uff09\u7ade\u4e89\u6fc0\u70c8\u3002", "conclusion": "\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u9650\u5236\u8fc7\u6ee4\u64cd\u4f5c\u7b26\u7684\u5e94\u7528\u8303\u56f4\u53ef\u4ee5\u4fdd\u8bc1\u7ea6\u675f\u95ee\u9898\u7684\u53ef\u5224\u5b9a\u6027\uff0c\u800c\u65e0\u9650\u5e94\u7528\u5219\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u5224\u5b9a\u6027\u3002"}}
{"id": "2508.08343", "pdf": "https://arxiv.org/pdf/2508.08343", "abs": "https://arxiv.org/abs/2508.08343", "authors": ["Ferran Agullo", "Joan Oliveras", "Chen Wang", "Alberto Gutierrez-Torre", "Olivier Tardieu", "Alaa Youssef", "Jordi Torres", "Josep Ll. Berral"], "title": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical Approach for Multi-Tenant LLM Serving", "categories": ["cs.PF", "cs.AI", "cs.CL"], "comment": "Under review for a computer science conference", "summary": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u5206\u6790\u7ba1\u9053\uff0c\u7528\u4e8e\u4f18\u5316LLM\u9002\u914d\u5668\u7684\u5206\u914d\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6280\u672f\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740LLM\u9002\u914d\u5668\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u6027\u80fd\u548c\u8d44\u6e90\u5206\u914d\u7684\u4f18\u5316\u6210\u4e3a\u5173\u952e\u95ee\u9898\uff0c\u76ee\u524d\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u548c\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5206\u6790\u7ba1\u9053\uff0c\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u52a8\u6001\u786e\u5b9a\u5355\u8282\u70b9\u8bbe\u7f6e\u4e2d\u7684\u9002\u914d\u5668\u6700\u4f18\u5206\u914d\uff0c\u5e76\u6269\u5c55\u5230\u591a\u526f\u672c\u90e8\u7f72\u4e2d\u3002", "result": "\u6570\u5b57\u5b6a\u751f\u7684\u541e\u5410\u91cf\u5dee\u5f02\u4e0d\u8d85\u8fc75.5%\uff0c\u7ba1\u9053\u80fd\u51c6\u786e\u9884\u6d4b\u6700\u4f18\u5206\u914d\u5e76\u6700\u5c0f\u5316\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86LLM\u9002\u914d\u5668\u670d\u52a1\u7684\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2508.08448", "pdf": "https://arxiv.org/pdf/2508.08448", "abs": "https://arxiv.org/abs/2508.08448", "authors": ["Jiarong Xing", "Yifan Qiao", "Simon Mo", "Xingqi Cui", "Gur-Eyal Sela", "Yang Zhou", "Joseph Gonzalez", "Ion Stoica"], "title": "Towards Efficient and Practical GPU Multitasking in the Era of LLM", "categories": ["cs.OS", "cs.DC"], "comment": null, "summary": "GPU singletasking is becoming increasingly inefficient and unsustainable as\nhardware capabilities grow and workloads diversify. We are now at an inflection\npoint where GPUs must embrace multitasking, much like CPUs did decades ago, to\nmeet the demands of modern AI workloads. In this work, we highlight the key\nrequirements for GPU multitasking, examine prior efforts, and discuss why they\nfall short. To advance toward efficient and practical GPU multitasking, we\nenvision a resource management layer, analogous to a CPU operating system, to\nhandle various aspects of GPU resource management and sharing. We outline the\nchallenges and potential solutions, and hope this paper inspires broader\ncommunity efforts to build the next-generation GPU compute paradigm grounded in\nmultitasking.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86GPU\u5355\u4efb\u52a1\u6a21\u5f0f\u7684\u4f4e\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u5411\u591a\u4efb\u52a1\u6a21\u5f0f\u7684\u8f6c\u53d8\uff0c\u7c7b\u4f3c\u4e8eCPU\u7684\u53d1\u5c55\u5386\u7a0b\u3002\u6587\u4e2d\u63d0\u51fa\u4e86\u89e3\u51b3GPU\u8d44\u6e90\u7ba1\u7406\u548c\u5171\u4eab\u7684\u6f5c\u5728\u65b9\u6848\uff0c\u5e76\u547c\u5401\u793e\u533a\u5171\u540c\u52aa\u529b\u6784\u5efa\u57fa\u4e8e\u591a\u4efb\u52a1\u7684\u4e0b\u4e00\u4ee3GPU\u8ba1\u7b97\u8303\u5f0f\u3002", "motivation": "\u968f\u7740\u786c\u4ef6\u80fd\u529b\u7684\u63d0\u5347\u548c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u591a\u6837\u5316\uff0cGPU\u5355\u4efb\u52a1\u6a21\u5f0f\u53d8\u5f97\u4f4e\u6548\u4e14\u4e0d\u53ef\u6301\u7eed\u3002\u4f5c\u8005\u8ba4\u4e3aGPU\u5fc5\u987b\u8f6c\u5411\u591a\u4efb\u52a1\u6a21\u5f0f\u4ee5\u6ee1\u8db3\u73b0\u4ee3AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9700\u6c42\u3002", "method": "\u672c\u6587\u9996\u5148\u5206\u6790\u4e86GPU\u591a\u4efb\u52a1\u7684\u5173\u952e\u9700\u6c42\uff0c\u56de\u987e\u4e86\u5df2\u6709\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7c7b\u4f3cCPU\u64cd\u4f5c\u7cfb\u7edf\u7684\u8d44\u6e90\u7ba1\u7406\u5c42\uff0c\u7528\u4e8e\u7ba1\u7406GPU\u8d44\u6e90\u7684\u5171\u4eab\u3002\u4f5c\u8005\u8fd8\u8ba8\u8bba\u4e86\u76f8\u5173\u6311\u6218\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u5206\u6790\uff0c\u4f5c\u8005\u6307\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6784\u5efaGPU\u591a\u4efb\u52a1\u6a21\u5f0f\u7684\u6f5c\u5728\u8def\u5f84\uff0c\u4e3a\u4e0b\u4e00\u4ee3GPU\u8ba1\u7b97\u8303\u5f0f\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "GPU\u591a\u4efb\u52a1\u662f\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u9700\u8981\u793e\u533a\u5171\u540c\u52aa\u529b\u5b9e\u73b0\u3002\u672c\u6587\u4e3a\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u7684\u63a2\u8ba8\u3002"}}
{"id": "2508.08384", "pdf": "https://arxiv.org/pdf/2508.08384", "abs": "https://arxiv.org/abs/2508.08384", "authors": ["Mutian Tong", "Rundi Wu", "Changxi Zheng"], "title": "Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion Priors", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "11 pages. Accepted by SIGGRAPH 2025 as Conference Paper", "summary": "Indoor lighting estimation from a single image or video remains a challenge\ndue to its highly ill-posed nature, especially when the lighting condition of\nthe scene varies spatially and temporally. We propose a method that estimates\nfrom an input video a continuous light field describing the spatiotemporally\nvarying lighting of the scene. We leverage 2D diffusion priors for optimizing\nsuch light field represented as a MLP. To enable zero-shot generalization to\nin-the-wild scenes, we fine-tune a pre-trained image diffusion model to predict\nlighting at multiple locations by jointly inpainting multiple chrome balls as\nlight probes. We evaluate our method on indoor lighting estimation from a\nsingle image or video and show superior performance over compared baselines.\nMost importantly, we highlight results on spatiotemporally consistent lighting\nestimation from in-the-wild videos, which is rarely demonstrated in previous\nworks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u7684\u5ba4\u5185\u7a7a\u95f4-\u65f6\u95f4\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u75282D\u6269\u6563\u5148\u9a8c\u4f18\u5316MLP\u8868\u793a\u7684\u5149\u573a\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u89e3\u51b3\u5355\u56fe\u50cf\u6216\u89c6\u9891\u4e2d\u7a7a\u95f4-\u65f6\u95f4\u53d8\u5316\u7684\u5149\u7167\u4f30\u8ba1\u95ee\u9898\uff0c\u56e0\u5176\u9ad8\u5ea6\u4e0d\u9002\u5b9a\u6027\u800c\u5177\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u56fe\u50cf\u6269\u6563\u6a21\u578b\u9884\u6d4b\u591a\u4e2a\u4f4d\u7f6e\u7684\u5149\u7167\uff0c\u8054\u5408\u4fee\u590d\u591a\u4e2a\u94ec\u7403\u4f5c\u4e3a\u5149\u63a2\u9488\uff0c\u4f18\u5316MLP\u8868\u793a\u7684\u5149\u573a\u3002", "result": "\u5728\u5ba4\u5185\u5149\u7167\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u89c6\u9891\u4e2d\u7a7a\u95f4-\u65f6\u95f4\u4e00\u81f4\u7684\u5149\u7167\u4f30\u8ba1\u6548\u679c\u3002", "conclusion": "\u65b9\u6cd5\u80fd\u6709\u6548\u4f30\u8ba1\u771f\u5b9e\u89c6\u9891\u4e2d\u7684\u7a7a\u95f4-\u65f6\u95f4\u53d8\u5316\u5149\u7167\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u7814\u7a76\u4e2d\u8f83\u5c11\u5c55\u793a\u7684\u96be\u9898\u3002"}}
{"id": "2508.08380", "pdf": "https://arxiv.org/pdf/2508.08380", "abs": "https://arxiv.org/abs/2508.08380", "authors": ["Rohan Bali", "Trevor E. Bailey", "Michael S. Bullock", "Boulat A. Bash"], "title": "Experimental Validation of Provably Covert Communication Using Software-Defined Radio", "categories": ["cs.NI"], "comment": null, "summary": "The fundamental information-theoretic limits of covert, or low probability of\ndetection/intercept (LPD/LPI), communication have been extensively studied for\nover a decade, resulting in the square root law (SRL): only $L\\sqrt{n}$ covert\nbits can be reliably transmitted over time-bandwidth product $n$, for constant\n$L>0$. Transmitting more either results in detection or decoding errors. The\nSRL imposes significant constraints on hardware realization of\nmathematically-guaranteed covert communication. Indeed, they preclude using\nstandard link maintenance operations that are taken for granted in non-covert\ncommunication. Thus, experimental validation of covert communication is\nunderexplored: to date, only two experimental studies of SRL-based covert\ncommunication are available, both focusing on optical channels. Here, we report\na demonstration of provably-secure covert radio-frequency (RF) communication\nusing software-defined radios (SDRs). This validates theoretical predictions,\nopens practical avenues for implementing covert communication systems, and\nraises further research questions.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u8ba8\u8bba\u4e86\u9690\u853d\u901a\u4fe1\u7684\u4fe1\u606f\u7406\u8bba\u6781\u9650\u53ca\u5176\u786c\u4ef6\u5b9e\u73b0\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u57fa\u4e8e\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\u7684\u9690\u853d\u5c04\u9891\u901a\u4fe1\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u9690\u853d\u901a\u4fe1\uff08LPD/LPI\uff09\u7684\u7406\u8bba\u7814\u7a76\u5df2\u6709\u5341\u591a\u5e74\uff0c\u4f46\u786c\u4ef6\u5b9e\u73b0\u9762\u4e34\u6311\u6218\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u6781\u5c11\uff0c\u5c24\u5176\u5728\u5c04\u9891\u9886\u57df\u3002", "method": "\u4f7f\u7528\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\uff08SDRs\uff09\u8fdb\u884c\u9690\u853d\u5c04\u9891\u901a\u4fe1\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\uff0c\u5e76\u4e3a\u9690\u853d\u901a\u4fe1\u7cfb\u7edf\u7684\u5b9e\u9645\u5b9e\u73b0\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "\u5b9e\u9a8c\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\uff0c\u8fd8\u4e3a\u9690\u853d\u901a\u4fe1\u7684\u5b9e\u9645\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u95ee\u9898\u3002"}}
{"id": "2508.08383", "pdf": "https://arxiv.org/pdf/2508.08383", "abs": "https://arxiv.org/abs/2508.08383", "authors": ["Krisha Mehta", "Gordon Kindlmann", "Alex Kale"], "title": "Designing for Disclosure in Data Visualizations", "categories": ["cs.HC"], "comment": "11 pages, 6 figures", "summary": "Visualizing data often entails data transformations that can reveal and hide\ninformation, operations we dub disclosure tactics. Whether designers hide\ninformation intentionally or as an implicit consequence of other design\nchoices, tools and frameworks for visualization offer little explicit guidance\non disclosure. To systematically characterize how visualizations can limit\naccess to an underlying dataset, we contribute a content analysis of 425\nexamples of visualization techniques sampled from academic papers in the\nvisualization literature, resulting in a taxonomy of disclosure tactics. Our\ntaxonomy organizes disclosure tactics based on how they change the data\nrepresentation underlying a chart, providing a systematic way to reason about\ndesign trade-offs in terms of what information is revealed, distorted, or\nhidden. We demonstrate the benefits of using our taxonomy by showing how it can\nguide reasoning in design scenarios where disclosure is a first-order\nconsideration. Adopting disclosure as a framework for visualization research\noffers new perspective on authoring tools, literacy, uncertainty communication,\npersonalization, and ethical design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u62ab\u9732\u6218\u672f\u201d\u7684\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5b66\u672f\u6587\u732e\u4e2d425\u4e2a\u53ef\u89c6\u5316\u6280\u672f\u5b9e\u4f8b\uff0c\u7cfb\u7edf\u5316\u5730\u63cf\u8ff0\u4e86\u53ef\u89c6\u5316\u5982\u4f55\u9650\u5236\u5bf9\u5e95\u5c42\u6570\u636e\u7684\u8bbf\u95ee\u3002\u8be5\u5206\u7c7b\u6cd5\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u6743\u8861\u4fe1\u606f\u7684\u63ed\u793a\u3001\u626d\u66f2\u6216\u9690\u85cf\uff0c\u5e76\u4e3a\u53ef\u89c6\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "\u7531\u4e8e\u53ef\u89c6\u5316\u5de5\u5177\u548c\u6846\u67b6\u5bf9\u4fe1\u606f\u62ab\u9732\u7f3a\u4e4f\u660e\u786e\u6307\u5bfc\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u7cfb\u7edf\u5316\u5206\u6790\u53ef\u89c6\u5316\u6280\u672f\u4e2d\u7684\u62ab\u9732\u884c\u4e3a\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5bf9425\u4e2a\u53ef\u89c6\u5316\u6280\u672f\u5b9e\u4f8b\u8fdb\u884c\u5185\u5bb9\u5206\u6790\uff0c\u6784\u5efa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u8868\u793a\u53d8\u5316\u7684\u62ab\u9732\u6218\u672f\u5206\u7c7b\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7684\u62ab\u9732\u6218\u672f\u5206\u7c7b\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bbe\u8ba1\u6743\u8861\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u5c06\u4fe1\u606f\u62ab\u9732\u4f5c\u4e3a\u53ef\u89c6\u5316\u7814\u7a76\u6846\u67b6\uff0c\u4e3a\u5de5\u5177\u5f00\u53d1\u3001\u666e\u53ca\u6559\u80b2\u3001\u4e0d\u786e\u5b9a\u6027\u6c9f\u901a\u3001\u4e2a\u6027\u5316\u8bbe\u8ba1\u548c\u4f26\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.08592", "pdf": "https://arxiv.org/pdf/2508.08592", "abs": "https://arxiv.org/abs/2508.08592", "authors": ["Van-Hoang Phan", "Tung-Duong Le-Duc", "Long-Khanh Pham", "Anh-Thu Le", "Quynh-Huong Dinh-Nguyen", "Dang-Quan Vo", "Hoang-Quoc Nguyen-Son", "Anh-Duy Tran", "Dang Vu", "Minh-Son Dao"], "title": "Fact-Checking at Scale: Multimodal AI for Authenticity and Context Verification in Online Media", "categories": ["cs.MM"], "comment": "Accept to ACM MM 2025", "summary": "The proliferation of multimedia content on social media platforms has\ndramatically transformed how information is consumed and disseminated. While\nthis shift enables real-time coverage of global events, it also facilitates the\nrapid spread of misinformation and disinformation, especially during crises\nsuch as wars, natural disasters, or elections. The rise of synthetic media and\nthe reuse of authentic content in misleading contexts have intensified the need\nfor robust multimedia verification tools. In this paper, we present a\ncomprehensive system developed for the ACM Multimedia 2025 Grand Challenge on\nMultimedia Verification. Our system assesses the authenticity and contextual\naccuracy of multimedia content in multilingual settings and generates both\nexpert-oriented verification reports and accessible summaries for the general\npublic. We introduce a unified verification pipeline that integrates visual\nforensics, textual analysis, and multimodal reasoning, and propose a hybrid\napproach to detect out-of-context (OOC) media through semantic similarity,\ntemporal alignment, and geolocation cues. Extensive evaluations on the Grand\nChallenge benchmark demonstrate the system's effectiveness across diverse\nreal-world scenarios. Our contributions advance the state of the art in\nmultimedia verification and offer practical tools for journalists,\nfact-checkers, and researchers confronting information integrity challenges in\nthe digital age.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u591a\u5a92\u4f53\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u865a\u5047\u4fe1\u606f\uff0c\u7ed3\u5408\u89c6\u89c9\u53d6\u8bc1\u3001\u6587\u672c\u5206\u6790\u548c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5e76\u5728\u591a\u79cd\u8bed\u8a00\u548c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u793e\u4ea4\u5a92\u4f53\u4e2d\u865a\u5047\u4fe1\u606f\u7684\u6cdb\u6ee5\uff0c\u5c24\u5176\u662f\u5728\u5371\u673a\u65f6\u671f\uff0c\u5f00\u53d1\u5f3a\u5927\u7684\u591a\u5a92\u4f53\u9a8c\u8bc1\u5de5\u5177\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u7ed3\u5408\u89c6\u89c9\u53d6\u8bc1\u3001\u6587\u672c\u5206\u6790\u548c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u65f6\u95f4\u5bf9\u9f50\u548c\u5730\u7406\u4f4d\u7f6e\u7684\u6df7\u5408\u65b9\u6cd5\u6765\u68c0\u6d4b\u4e0a\u4e0b\u6587\u4e0d\u7b26\u7684\u5a92\u4f53\u3002", "result": "\u7cfb\u7edf\u5728ACM Multimedia 2025 Grand Challenge\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4e86\u9ad8\u6548\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u73b0\u5b9e\u573a\u666f\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63a8\u52a8\u4e86\u591a\u5a92\u4f53\u9a8c\u8bc1\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4e3a\u8bb0\u8005\u3001\u4e8b\u5b9e\u6838\u67e5\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u4ee5\u5e94\u5bf9\u6570\u5b57\u65f6\u4ee3\u7684\u4fe1\u606f\u5b8c\u6574\u6027\u6311\u6218\u3002"}}
{"id": "2508.08256", "pdf": "https://arxiv.org/pdf/2508.08256", "abs": "https://arxiv.org/abs/2508.08256", "authors": ["Dongwei Wang", "Zijie Liu", "Song Wang", "Yuxin Ren", "Jianing Deng", "Jingtong Hu", "Tianlong Chen", "Huanrui Yang"], "title": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM Inference", "categories": ["cs.DB"], "comment": "Submitted to EMNLP2025", "summary": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u4e14\u9ad8\u6548\u7684KV\u7f13\u5b58\u68c0\u7d22\u65b9\u6cd5Fier\uff0c\u901a\u8fc71\u4f4d\u91cf\u5316\u952e\u4f30\u8ba1\u6bcf\u4e2a\u4ee4\u724c\u7684\u91cd\u8981\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684\u5ef6\u8fdf\u3002", "motivation": "KV\u7f13\u5b58\u7684\u8bfb\u53d6\u5ef6\u8fdf\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u589e\u52a0\u800c\u663e\u8457\u4e0a\u5347\uff0c\u5f71\u54cd\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684\u6548\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u4ee4\u724c\u7a00\u758f\u5206\u5e03\uff0c\u5bfc\u81f4\u9875\u9762\u7ea7\u68c0\u7d22\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faFier\u65b9\u6cd5\uff0c\u4f7f\u75281\u4f4d\u91cf\u5316\u952e\u5feb\u901f\u8bc4\u4f30\u4ee4\u724c\u91cd\u8981\u6027\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4e14\u9ad8\u6548\u7684KV\u7f13\u5b58\u68c0\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFier\u4ec5\u970011%\u7684\u7f13\u5b58\u9884\u7b97\u5373\u53ef\u8fbe\u5230\u5168KV\u6027\u80fd\uff0c\u89e3\u7801\u5ef6\u8fdf\u964d\u4f4e1.2\u81f31.5\u500d\u3002", "conclusion": "Fier\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86KV\u7f13\u5b58\u7684\u68c0\u7d22\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2508.08973", "pdf": "https://arxiv.org/pdf/2508.08973", "abs": "https://arxiv.org/abs/2508.08973", "authors": ["Luca Fehlings", "Thomas Mikolajick", "Beatriz Noheda", "Erika Covi"], "title": "Millisecond-scale Volatile Memory in HZO Ferroelectric Capacitors for Bio-inspired Temporal Computing", "categories": ["cs.ET", "cond-mat.mtrl-sci"], "comment": null, "summary": "With the broad recent research on ferroelectric hafnium oxide for\nnon-volatile memory technology, depolarization effects in HfO2-based\nferroelectric devices gained a lot of interest. Understanding the physical\nmechanisms regulating the retention of these devices provides an excellent\nopportunity for device optimization both towards non-volatile memory\napplications and towards real-time signal processing applications in which\ncontrolled time constants are of paramount importance. Indeed, we argue that\nferroelectric devices, particularly HfO2-based, are an elegant solution to\nrealize possibly arbitrary time constants in a single scaled memory device,\nwhich paves the way for temporal and brain-inspired computing in hardware. Here\nwe present a ferroelectric capacitor stack realizing volatile memory due to its\nunique interface configuration. We provide electrical characterization of the\ndevice to motivate its use for realizing time constants in hardware, followed\nby an investigation of the electronic mechanisms and their possible relation to\nthe observed retention times to facilitate further modeling of the retention\nprocess in HfO2-based ferroelectric capacitors. In the presented device,\ninternal electric fields stabilize one polarization of the ferroelectric film,\nopening the possibility for unipolar operation with millisecond retention for\nthe unstable polarization state. We show a dependence of the retention on both\nthe polarization as well as the electrical stimuli, allowing us to exploit a\nrange of time scales in a single device. Further, the intentionally defective\ninterface in the presented material stack allows an insight into the interplay\nbetween retention loss in HfO2-based ferroelectric devices and the internal\nbias field, which we relate to the interface composition and the role of oxygen\nvacancies as a possible source of the internal bias fields.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86HfO2\u57fa\u94c1\u7535\u5668\u4ef6\u7684\u53bb\u6781\u5316\u6548\u5e94\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7\u754c\u9762\u914d\u7f6e\u5b9e\u73b0\u53ef\u53d8\u65f6\u95f4\u5e38\u6570\u7684\u786c\u4ef6\u8bbe\u8ba1\uff0c\u4e3a\u65f6\u95f4\u8ba1\u7b97\u548c\u8111\u542f\u53d1\u786c\u4ef6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u7814\u7a76HfO2\u57fa\u94c1\u7535\u5668\u4ef6\u7684\u7269\u7406\u673a\u5236\uff0c\u4ee5\u4f18\u5316\u975e\u6613\u5931\u6027\u5b58\u50a8\u5668\u548c\u5b9e\u65f6\u4fe1\u53f7\u5904\u7406\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5b9e\u73b0\u53ef\u53d8\u65f6\u95f4\u5e38\u6570\u65b9\u9762\u3002", "method": "\u901a\u8fc7\u7535\u5b66\u8868\u5f81\u548c\u754c\u9762\u7f3a\u9677\u7684\u914d\u7f6e\uff0c\u5206\u6790\u4e86\u94c1\u7535\u8584\u819c\u6781\u6027\u7a33\u5b9a\u6027\u548c\u4fdd\u7559\u65f6\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63a2\u8ba8\u4e86\u6c27\u7a7a\u4f4d\u5bf9\u5185\u90e8\u504f\u7f6e\u573a\u7684\u5f71\u54cd\u3002", "result": "\u5c55\u793a\u4e86\u57fa\u4e8eHfO2\u7684\u94c1\u7535\u7535\u5bb9\u5668\u901a\u8fc7\u7279\u5b9a\u754c\u9762\u914d\u7f6e\u5b9e\u73b0\u6beb\u79d2\u7ea7\u4fdd\u7559\u65f6\u95f4\u7684\u975e\u6613\u5931\u6027\u5b58\u50a8\uff0c\u4ee5\u53ca\u65f6\u95f4\u5e38\u6570\u53ef\u8c03\u7684\u7279\u6027\u3002", "conclusion": "HfO2\u57fa\u94c1\u7535\u5668\u4ef6\u901a\u8fc7\u754c\u9762\u8bbe\u8ba1\u548c\u5185\u90e8\u504f\u7f6e\u573a\u7684\u8c03\u63a7\uff0c\u4e3a\u65f6\u95f4\u8ba1\u7b97\u548c\u8111\u542f\u53d1\u786c\u4ef6\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08396", "pdf": "https://arxiv.org/pdf/2508.08396", "abs": "https://arxiv.org/abs/2508.08396", "authors": ["Fanchen Kong", "Yunhao Deng", "Xiaoling Yi", "Ryan Antonio", "Marian Verhelst"], "title": "XDMA: A Distributed, Extensible DMA Architecture for Layout-Flexible Data Movements in Heterogeneous Multi-Accelerator SoCs", "categories": ["cs.AR", "cs.DC"], "comment": "4 pages, 6 figures, Proceeded by The 43rd IEEE International\n  Conference on Computer Design (ICCD 2025)", "summary": "As modern AI workloads increasingly rely on heterogeneous accelerators,\nensuring high-bandwidth and layout-flexible data movements between accelerator\nmemories has become a pressing challenge. Direct Memory Access (DMA) engines\npromise high bandwidth utilization for data movements but are typically optimal\nonly for contiguous memory access, thus requiring additional software loops for\ndata layout transformations. This, in turn, leads to excessive control overhead\nand underutilized on-chip interconnects. To overcome this inefficiency, we\npresent XDMA, a distributed and extensible DMA architecture that enables\nlayout-flexible data movements with high link utilization. We introduce three\nkey innovations: (1) a data streaming engine as XDMA Frontend, replacing\nsoftware address generators with hardware ones; (2) a distributed DMA\narchitecture that maximizes link utilization and separates configuration from\ndata transfer; (3) flexible plugins for XDMA enabling on-the-fly data\nmanipulation during data transfers. XDMA demonstrates up to 151.2x/8.2x higher\nlink utilization than software-based implementations in synthetic workloads and\nachieves 2.3x average speedup over accelerators with SoTA DMA in real-world\napplications. Our design incurs <2% area overhead over SoTA DMA solutions while\nconsuming 17% of system power. XDMA proves that co-optimizing memory access,\nlayout transformation, and interconnect protocols is key to unlocking\nheterogeneous multi-accelerator SoC performance.", "AI": {"tldr": "XDMA\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u3001\u53ef\u6269\u5c55\u7684DMA\u67b6\u6784\uff0c\u901a\u8fc7\u786c\u4ef6\u5730\u5740\u751f\u6210\u5668\u548c\u7075\u6d3b\u7684\u6570\u636e\u4f20\u8f93\u63d2\u4ef6\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u52a0\u901f\u5668\u4e2d\u6570\u636e\u5e03\u5c40\u7075\u6d3b\u6027\u548c\u94fe\u8def\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3AI\u5de5\u4f5c\u8d1f\u8f7d\u4f9d\u8d56\u5f02\u6784\u52a0\u901f\u5668\uff0c\u4f46\u6570\u636e\u5728\u9ad8\u5e26\u5bbd\u548c\u5e03\u5c40\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u79fb\u52a8\u6210\u4e3a\u6311\u6218\uff0c\u73b0\u6709DMA\u5f15\u64ce\u4ec5\u9002\u7528\u4e8e\u8fde\u7eed\u5185\u5b58\u8bbf\u95ee\uff0c\u5bfc\u81f4\u63a7\u5236\u5f00\u9500\u5927\u548c\u94fe\u8def\u5229\u7528\u7387\u4f4e\u3002", "method": "XDMA\u5f15\u5165\u4e86\u4e09\u9879\u5173\u952e\u521b\u65b0\uff1a\u786c\u4ef6\u5730\u5740\u751f\u6210\u5668\u66ff\u4ee3\u8f6f\u4ef6\u3001\u5206\u5e03\u5f0f\u67b6\u6784\u5206\u79bb\u914d\u7f6e\u4e0e\u6570\u636e\u4f20\u8f93\u3001\u7075\u6d3b\u7684\u4f20\u8f93\u63d2\u4ef6\uff0c\u5b9e\u73b0\u4e86\u5e03\u5c40\u7075\u6d3b\u7684\u9ad8\u6548\u6570\u636e\u4f20\u8f93\u3002", "result": "XDMA\u5728\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u94fe\u8def\u5229\u7528\u7387\u63d0\u5347151.2x/8.2x\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u6bd4\u73b0\u6709DMA\u5feb2.3x\uff0c\u9762\u79ef\u5f00\u9500<2%\uff0c\u529f\u8017\u4e3a\u7cfb\u7edf\u768417%\u3002", "conclusion": "XDMA\u8bc1\u660e\uff0c\u534f\u540c\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u3001\u5e03\u5c40\u8f6c\u6362\u548c\u4e92\u8054\u534f\u8bae\u662f\u91ca\u653e\u5f02\u6784\u591a\u52a0\u901f\u5668SoC\u6027\u80fd\u7684\u5173\u952e\u3002"}}
{"id": "2508.08430", "pdf": "https://arxiv.org/pdf/2508.08430", "abs": "https://arxiv.org/abs/2508.08430", "authors": ["Abhinaba Chakraborty", "Wouter Tavernier", "Akis Kourtis", "Mario Pickavet", "Andreas Oikonomakis", "Didier Colle"], "title": "Profiling Concurrent Vision Inference Workloads on NVIDIA Jetson -- Extended", "categories": ["cs.DC", "cs.AR", "cs.PF"], "comment": null, "summary": "The proliferation of IoT devices and advancements in network technologies\nhave intensified the demand for real-time data processing at the network edge.\nTo address these demands, low-power AI accelerators, particularly GPUs, are\nincreasingly deployed for inference tasks, enabling efficient computation while\nmitigating cloud-based systems' latency and bandwidth limitations. Despite\ntheir growing deployment, GPUs remain underutilised even in computationally\nintensive workloads. This underutilisation stems from the limited understanding\nof GPU resource sharing, particularly in edge computing scenarios. In this\nwork, we conduct a detailed analysis of both high- and low-level metrics,\nincluding GPU utilisation, memory usage, streaming multiprocessor (SM)\nutilisation, and tensor core usage, to identify bottlenecks and guide\nhardware-aware optimisations. By integrating traces from multiple profiling\ntools, we provide a comprehensive view of resource behaviour on NVIDIA Jetson\nedge devices under concurrent vision inference workloads. Our findings indicate\nthat while GPU utilisation can reach $100\\%$ under specific optimisations,\ncritical low-level resources, such as SMs and tensor cores, often operate only\nat $15\\%$ to $30\\%$ utilisation. Moreover, we observe that certain CPU-side\nevents, such as thread scheduling, context switching, etc., frequently emerge\nas bottlenecks, further constraining overall GPU performance. We provide\nseveral key observations for users of vision inference workloads on NVIDIA edge\ndevices.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u8fb9\u7f18\u8ba1\u7b97\u4e2dGPU\u8d44\u6e90\u5229\u7528\u7684\u74f6\u9888\uff0c\u53d1\u73b0\u5173\u952e\u4f4e\u5c42\u8d44\u6e90\u5229\u7528\u7387\u4f4e\uff0c\u4e14CPU\u4e8b\u4ef6\u5e38\u6210\u4e3a\u9650\u5236\u6027\u80fd\u7684\u56e0\u7d20\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u5b9e\u65f6\u6570\u636e\u5904\u7406\u9700\u6c42\uff0c\u4f4e\u529f\u8017AI\u52a0\u901f\u5668\uff08\u5982GPU\uff09\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\uff0c\u4f46\u5176\u8d44\u6e90\u5229\u7528\u7387\u4e0d\u8db3\uff0c\u5c24\u5176\u662fGPU\u8d44\u6e90\u5171\u4eab\u673a\u5236\u4e0d\u660e\u3002", "method": "\u901a\u8fc7\u591a\u5c42\u6b21\u6307\u6807\u5206\u6790\uff08\u5982GPU\u5229\u7528\u7387\u3001\u5185\u5b58\u4f7f\u7528\u7b49\uff09\u7ed3\u5408\u591a\u79cd\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u7814\u7a76NVIDIA Jetson\u8bbe\u5907\u5728\u5e76\u53d1\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8d44\u6e90\u884c\u4e3a\u3002", "result": "GPU\u5229\u7528\u7387\u53ef\u8fbe100%\uff0c\u4f46SM\u548cTensor\u6838\u5fc3\u5229\u7528\u7387\u4ec515%-30%\uff0cCPU\u4e8b\u4ef6\uff08\u5982\u7ebf\u7a0b\u8c03\u5ea6\uff09\u5e38\u6210\u4e3a\u74f6\u9888\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9488\u5bf9\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u7684\u4f18\u5316\u5efa\u8bae\uff0c\u63ed\u793a\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e2dGPU\u8d44\u6e90\u5229\u7528\u7684\u5173\u952e\u9650\u5236\u56e0\u7d20\u3002"}}
{"id": "2508.08661", "pdf": "https://arxiv.org/pdf/2508.08661", "abs": "https://arxiv.org/abs/2508.08661", "authors": ["Chunhua Liu", "Hong Yi Lin", "Patanamon Thongtanunam"], "title": "Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics", "categories": ["cs.SE", "cs.AI"], "comment": "8 main pages, 5 figures", "summary": "Language models have shown strong capabilities across a wide range of tasks\nin software engineering, such as code generation, yet they suffer from\nhallucinations. While hallucinations have been studied independently in natural\nlanguage and code generation, their occurrence in tasks involving code changes\nwhich have a structurally complex and context-dependent format of code remains\nlargely unexplored. This paper presents the first comprehensive analysis of\nhallucinations in two critical tasks involving code change to natural language\ngeneration: commit message generation and code review comment generation. We\nquantify the prevalence of hallucinations in recent language models and explore\na range of metric-based approaches to automatically detect them. Our findings\nreveal that approximately 50\\% of generated code reviews and 20\\% of generated\ncommit messages contain hallucinations. Whilst commonly used metrics are weak\ndetectors on their own, combining multiple metrics substantially improves\nperformance. Notably, model confidence and feature attribution metrics\neffectively contribute to hallucination detection, showing promise for\ninference-time detection.\\footnote{All code and data will be released upon\nacceptance.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u5206\u6790\u4e86\u4ee3\u7801\u53d8\u66f4\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u53d1\u73b0\u5728\u4ee3\u7801\u5ba1\u67e5\u548c\u63d0\u4ea4\u4fe1\u606f\u751f\u6210\u4e2d\u5e7b\u89c9\u7387\u5206\u522b\u4e3a50%\u548c20%\uff0c\u5e76\u63a2\u8ba8\u4e86\u591a\u79cd\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u4ee3\u7801\u53d8\u66f4\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u91cf\u5316\u5e7b\u89c9\u7387\u5e76\u63a2\u7d22\u57fa\u4e8e\u591a\u79cd\u6307\u6807\u7684\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u4ee3\u7801\u5ba1\u67e5\u548c\u63d0\u4ea4\u4fe1\u606f\u7684\u5e7b\u89c9\u7387\u5206\u522b\u4e3a50%\u548c20%\uff0c\u7ed3\u5408\u591a\u6307\u6807\u53ef\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u6a21\u578b\u7f6e\u4fe1\u5ea6\u548c\u7279\u5f81\u5f52\u56e0\u6307\u6807\u5728\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u6709\u671b\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u3002"}}
{"id": "2508.09053", "pdf": "https://arxiv.org/pdf/2508.09053", "abs": "https://arxiv.org/abs/2508.09053", "authors": ["Klaus-Dieter Schewe", "Flavio Ferrarotti"], "title": "Behavioural Theory of Reflective Algorithms II: Reflective Parallel Algorithms", "categories": ["cs.LO", "68Q10, 68N19"], "comment": "49 pages; short version (without proofs) published in LNCS vol.\n  15728: Rigorous State-Based Methods, 2025", "summary": "We develop a behavioural theory of reflective parallel algorithms (RAs), i.e.\nsynchronous parallel algorithms that can modify their own behaviour. The theory\ncomprises a set of postulates defining the class of RAs, an abstract machine\nmodel, and the proof that all RAs are captured by this machine model. RAs are\nsequential-time, parallel algorithms, where every state includes a\nrepresentation of the algorithm in that state, thus enabling linguistic\nreflection. Bounded exploration is preserved using multiset comprehension terms\nas values. The abstract machine model is defined by reflective Abstract State\nMachines (rASMs), which extend ASMs using extended states that include an\nupdatable representation of the main ASM rule to be executed by the machine in\nthat state.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53cd\u5c04\u5e76\u884c\u7b97\u6cd5\uff08RAs\uff09\u7684\u884c\u4e3a\u7406\u8bba\uff0c\u5305\u62ec\u5b9a\u4e49RAs\u7684\u516c\u7406\u3001\u62bd\u8c61\u673a\u5668\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u8be5\u6a21\u578b\u80fd\u6355\u83b7\u6240\u6709RAs\u3002", "motivation": "\u7814\u7a76\u5177\u6709\u81ea\u6211\u4fee\u6539\u884c\u4e3a\u7684\u540c\u6b65\u5e76\u884c\u7b97\u6cd5\uff0c\u4e3a\u5176\u5efa\u7acb\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5b9a\u4e49\u53cd\u5c04\u5e76\u884c\u7b97\u6cd5\u7684\u7c7b\u3001\u62bd\u8c61\u673a\u5668\u6a21\u578b\uff08rASMs\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u96c6\u7406\u89e3\u9879\u4fdd\u6301\u6709\u754c\u63a2\u7d22\u3002", "result": "\u8bc1\u660e\u6240\u6709RAs\u53ef\u7531\u53cd\u5c04\u62bd\u8c61\u72b6\u6001\u673a\uff08rASMs\uff09\u6a21\u578b\u6355\u83b7\u3002", "conclusion": "rASMs\u6210\u529f\u6269\u5c55\u4e86ASMs\uff0c\u4e3a\u81ea\u4fee\u6539\u5e76\u884c\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u6a21\u578b\u3002"}}
{"id": "2508.08531", "pdf": "https://arxiv.org/pdf/2508.08531", "abs": "https://arxiv.org/abs/2508.08531", "authors": ["Afsara Benazir", "Felix Xiaozhu Lin"], "title": "Profiling Large Language Model Inference on Apple Silicon: A Quantization Perspective", "categories": ["cs.PF"], "comment": null, "summary": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u82f9\u679c\u82af\u7247\uff08Apple Silicon\uff09\u5728\u8bbe\u5907\u7aef\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u4e0eNVIDIA GPU\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u63ed\u793a\u4e86\u5176\u7edf\u4e00\u5185\u5b58\u67b6\u6784\u7684\u4f18\u52bf\u548c\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8GPU\u52a0\u901f\u5927\u89c4\u6a21\u8bad\u7ec3\u6216\u63a8\u7406\uff0c\u7f3a\u4e4f\u5bf9\u82f9\u679c\u82af\u7247\u786c\u4ef6\u6548\u7387\u7684\u7cfb\u7edf\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5176\u7edf\u4e00\u5185\u5b58\u67b6\u6784\u5728\u8bbe\u5907\u7aefLLM\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u76f4\u63a5\u5bf9\u6bd4\u82f9\u679c\u82af\u7247\uff08M2 Ultra\u3001M2 Max\u3001M4 Pro\uff09\u548cNVIDIA GPU\uff08RTX A6000\uff09\u5728\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u7ed3\u5408\u4f4e\u5c42\u786c\u4ef6\u6307\u6807\uff08\u5982ALU\u5229\u7528\u7387\u3001\u5185\u5b58\u5e26\u5bbd\u7b49\uff09\u5206\u6790\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u82f9\u679c\u82af\u7247\u7684\u7edf\u4e00\u5185\u5b58\u4f7f\u5176\u5728\u8d85\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u517c\u5177\u6210\u672c\u6548\u76ca\u548c\u6548\u7387\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u91cf\u5316\u538b\u7f29\u5e76\u975e\u5bf9\u6240\u6709\u786c\u4ef6\u5e73\u53f0\u90fd\u5e26\u6765\u52a0\u901f\u6548\u679c\u7684\u6027\u80fd\u74f6\u9888\u3002", "conclusion": "\u82f9\u679c\u82af\u7247\u5728\u8bbe\u5907\u7aefLLM\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u8d85\u5927\u89c4\u6a21\u6a21\u578b\u65f6\uff0c\u4f46\u9700\u6ce8\u610f\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff1b\u8be5\u7814\u7a76\u586b\u8865\u4e86\u76f8\u5173\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2508.08438", "pdf": "https://arxiv.org/pdf/2508.08438", "abs": "https://arxiv.org/abs/2508.08438", "authors": ["Kexin Chu", "Zecheng Lin", "Dawei Xiang", "Zixu Shen", "Jianchang Su", "Cheng Chu", "Yiwei Yang", "Wenhui Zhang", "Wenfei Wu", "Wei Zhang"], "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference", "categories": ["cs.CR", "cs.LG", "cs.OS"], "comment": "17 pages,17 figures", "summary": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference.", "AI": {"tldr": "SafeKV\u662f\u4e00\u79cd\u9690\u79c1\u611f\u77e5\u7684KV\u7f13\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5171\u4eab\u975e\u654f\u611f\u6761\u76ee\u548c\u9694\u79bb\u654f\u611f\u5185\u5bb9\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u65f6\u5e8f\u4fa7\u4fe1\u9053\u653b\u51fb\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5168\u5c40KV\u7f13\u5b58\u5171\u4eab\u867d\u7136\u52a0\u901f\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u65b0\u7684\u65f6\u5e8f\u4fa7\u4fe1\u9053\u653b\u51fb\u98ce\u9669\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff08\u5982\u7528\u6237\u9694\u79bb\uff09\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\u3002", "method": "SafeKV\u7ed3\u5408\u591a\u5c42\u7ea7\u68c0\u6d4b\u7ba1\u9053\u3001\u7edf\u4e00\u57fa\u6570\u6811\u7d22\u5f15\u548c\u57fa\u4e8e\u71b5\u7684\u8bbf\u95ee\u76d1\u63a7\uff0c\u5b9e\u73b0\u654f\u611f\u4e0e\u975e\u654f\u611f\u5185\u5bb9\u7684\u52a8\u6001\u9694\u79bb\u3002", "result": "SafeKV\u51cf\u5c11\u4e8694%-97%\u7684\u65f6\u5e8f\u4fa7\u4fe1\u9053\u653b\u51fb\uff0c\u5728TTFT\u548c\u541e\u5410\u91cf\u4e0a\u5206\u522b\u63d0\u534740.58%\u548c2.66\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9694\u79bb\u65b9\u6cd5\u3002", "conclusion": "SafeKV\u901a\u8fc7\u7cbe\u7ec6\u7684\u9690\u79c1\u63a7\u5236\u548c\u9ad8\u7f13\u5b58\u6548\u7387\uff0c\u5728\u63d0\u4f9b\u8fd0\u884c\u65f6\u9690\u79c1\u4fdd\u969c\u7684\u540c\u65f6\uff0c\u6062\u590d\u4e86\u5168\u5c40\u5171\u4eab\u7684\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2508.08429", "pdf": "https://arxiv.org/pdf/2508.08429", "abs": "https://arxiv.org/abs/2508.08429", "authors": ["Dalton Omens", "Allise Thurman", "Jihun Yu", "Ronald Fedkiw"], "title": "Improving Facial Rig Semantics for Tracking and Retargeting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "In this paper, we consider retargeting a tracked facial performance to either\nanother person or to a virtual character in a game or virtual reality (VR)\nenvironment. We remove the difficulties associated with identifying and\nretargeting the semantics of one rig framework to another by utilizing the same\nframework (3DMM, FLAME, MetaHuman, etc.) for both subjects. Although this does\nnot constrain the choice of framework when retargeting from one person to\nanother, it does force the tracker to use the game/VR character rig when\nretargeting to a game/VR character. We utilize volumetric morphing in order to\nfit facial rigs to both performers and targets; in addition, a carefully chosen\nset of Simon-Says expressions is used to calibrate each rig to the motion\nsignatures of the relevant performer or target. Although a uniform set of\nSimon-Says expressions can likely be used for all person to person retargeting,\nwe argue that person to game/VR character retargeting benefits from Simon-Says\nexpressions that capture the distinct motion signature of the game/VR character\nrig. The Simon-Says calibrated rigs tend to produce the desired expressions\nwhen exercising animation controls (as expected). Unfortunately, these\nwell-calibrated rigs still lead to undesirable controls when tracking a\nperformance (a well-behaved function can have an arbitrarily ill-conditioned\ninverse), even though they typically produce acceptable geometry\nreconstructions. Thus, we propose a fine-tuning approach that modifies the rig\nused by the tracker in order to promote the output of more semantically\nmeaningful animation controls, facilitating high efficacy retargeting. In order\nto better address real-world scenarios, the fine-tuning relies on implicit\ndifferentiation so that the tracker can be treated as a (potentially\nnon-differentiable) black box.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u90e8\u8868\u60c5\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u6846\u67b6\u548c\u201cSimon-Says\u201d\u6821\u51c6\u6280\u672f\u4f18\u5316\u52a8\u753b\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4ece\u771f\u4eba\u5230\u865a\u62df\u89d2\u8272\u7684\u8bed\u4e49\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u9762\u90e8\u8ffd\u8e2a\u548c\u91cd\u5b9a\u5411\u4e2d\u56e0\u4e0d\u540c\u8bed\u4e49\u6846\u67b6\u5bfc\u81f4\u7684\u8bed\u4e49\u5339\u914d\u56f0\u96be\uff0c\u7279\u522b\u662f\u4ece\u771f\u4eba\u5230\u865a\u62df\u89d2\u8272\u7684\u573a\u666f\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u76843DMM\u7b49\u6846\u67b6\uff0c\u7ed3\u5408\u4f53\u79ef\u53d8\u5f62\u548cSimon-Says\u6821\u51c6\u6280\u672f\uff0c\u518d\u901a\u8fc7\u7cbe\u7ec6\u8c03\u4f18\u63d0\u5347\u52a8\u753b\u63a7\u5236\u7684\u8bed\u4e49\u51c6\u786e\u6027\u3002", "result": "\u6821\u51c6\u540e\u7684\u6846\u67b6\u80fd\u751f\u6210\u671f\u671b\u7684\u8868\u60c5\u52a8\u753b\uff0c\u4f46\u4ecd\u9700\u901a\u8fc7\u7cbe\u7ec6\u8c03\u4f18\u89e3\u51b3\u52a8\u6001\u8ffd\u8e2a\u65f6\u63a7\u5236\u4e0d\u826f\u7684\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u7cbe\u7ec6\u8c03\u4f18\u65b9\u6cd5\uff0c\u8bba\u6587\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u9762\u90e8\u8868\u60c5\u91cd\u5b9a\u5411\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u865a\u62df\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2508.08535", "pdf": "https://arxiv.org/pdf/2508.08535", "abs": "https://arxiv.org/abs/2508.08535", "authors": ["Azin Sabzian", "Mohammad Jalili Torkamani", "Negin Mahmoudi", "Kiana Kiashemshaki"], "title": "LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework", "categories": ["cs.NI", "cs.AI", "C.2.1; C.2.2; E.3; I.2.7"], "comment": "7 pages", "summary": "Wireless Body Area Networks (WBANs) enable continuous monitoring of\nphysiological signals for applications ranging from chronic disease management\nto emergency response. Recent advances in 6G communications, post-quantum\ncryptography, and energy harvesting have the potential to enhance WBAN\nperformance. However, integrating these technologies into a unified, adaptive\nsystem remains a challenge. This paper surveys some of the most well-known\nWireless Body Area Network (WBAN) architectures, routing strategies, and\nsecurity mechanisms, identifying key gaps in adaptability, energy efficiency,\nand quantum-resistant security. We propose a novel Large Language Model-driven\nadaptive WBAN framework in which a Large Language Model acts as a cognitive\ncontrol plane, coordinating routing, physical layer selection, micro-energy\nharvesting, and post-quantum security in real time. Our review highlights the\nlimitations of current heuristic-based designs and outlines a research agenda\nfor resource-constrained, 6G-ready medical systems. This approach aims to\nenable ultra-reliable, secure, and self-optimizing WBANs for next-generation\nmobile health applications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u65e0\u7ebf\u4f53\u57df\u7f51\uff08WBAN\uff09\u7684\u67b6\u6784\u3001\u8def\u7531\u7b56\u7565\u548c\u5b89\u5168\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u81ea\u9002\u5e94WBAN\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728\u9002\u5e94\u6027\u3001\u80fd\u6548\u548c\u6297\u91cf\u5b50\u5b89\u5168\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u65e0\u7ebf\u4f53\u57df\u7f51\uff08WBAN\uff09\u5728\u6162\u6027\u75c5\u7ba1\u7406\u548c\u7d27\u6025\u54cd\u5e94\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u6574\u54086G\u901a\u4fe1\u3001\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u7b49\u6280\u672f\u4ee5\u63d0\u5347\u5176\u6027\u80fd\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u73b0\u6709WBAN\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba4\u77e5\u63a7\u5236\u5e73\u9762\u6846\u67b6\uff0c\u5b9e\u65f6\u534f\u8c03\u8def\u7531\u3001\u7269\u7406\u5c42\u9009\u62e9\u3001\u5fae\u80fd\u91cf\u6536\u96c6\u548c\u6297\u91cf\u5b50\u5b89\u5168\u3002", "result": "\u6587\u7ae0\u6307\u51fa\u4e86\u5f53\u524d\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u9762\u54116G\u533b\u7597\u7cfb\u7edf\u7684\u7814\u7a76\u8bae\u7a0b\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e8\u5728\u5b9e\u73b0\u8d85\u53ef\u9760\u3001\u5b89\u5168\u4e14\u81ea\u4f18\u5316\u7684\u4e0b\u4e00\u4ee3\u79fb\u52a8\u5065\u5eb7\u5e94\u7528WBAN\u3002"}}
{"id": "2508.08928", "pdf": "https://arxiv.org/pdf/2508.08928", "abs": "https://arxiv.org/abs/2508.08928", "authors": ["Kamran Akbar", "Robert Bregovic", "Federica Battisti"], "title": "DASC: Depth-of-Field Aware Scene Complexity Metric for 3D Visualization on Light Field Display", "categories": ["cs.MM", "cs.GR"], "comment": "12 pages, submitted in IEEE Transactions on Multimedia", "summary": "Light field display is one of the technologies providing 3D immersive\nvisualization. However, a light field display generates only a limited number\nof light rays which results in finite angular and spatial resolutions.\nTherefore, 3D content can be shown with high quality only within a narrow depth\nrange notated as Depth of Field (DoF) around the display screen. Outside this\nrange, due to the appearance of aliasing artifacts, the quality degrades\nproportionally to the distance from the screen. One solution to mitigate the\nartifacts is depth of field rendering which blurs the content in the distorted\nregions, but can result in the removal of scene details. This research focuses\non proposing a DoF Aware Scene Complexity (DASC) metric that characterizes 3D\ncontent based on geometrical and positional factors considering the light field\ndisplay's DoF. In this research, we also evaluate the observers' preference\nacross different level of blurriness caused by DoF rendering ranging from\nsharp, aliased scenes to overly smoothed alias-free scenes. We have conducted\nthis study over multiple scenes that we created to account for different types\nof content. Based on the outcome of subjective studies, we propose a model that\ntakes the value of DASC metric as input and predicts the preferred level of\nblurring for the given scene as output.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u611f\u77e5\u573a\u666f\u590d\u6742\u5ea6\uff08DASC\uff09\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u5149\u573a\u663e\u793a\u4e2d3D\u5185\u5bb9\u7684\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u4e3b\u89c2\u7814\u7a76\u9884\u6d4b\u7528\u6237\u504f\u597d\u7684\u6a21\u7cca\u7a0b\u5ea6\u3002", "motivation": "\u5149\u573a\u663e\u793a\u6280\u672f\u57283D\u6c89\u6d78\u5f0f\u53ef\u89c6\u5316\u4e2d\u5b58\u5728\u6df1\u5ea6\u8303\u56f4\u9650\u5236\uff0c\u5bfc\u81f4\u5728\u5c4f\u5e55\u5916\u533a\u57df\u51fa\u73b0\u4f2a\u5f71\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982\u666f\u6df1\u6e32\u67d3\uff09\u53ef\u80fd\u4f1a\u4e22\u5931\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6307\u6807\u6765\u4f18\u5316\u5185\u5bb9\u8d28\u91cf\u3002", "method": "\u63d0\u51faDASC\u6307\u6807\uff0c\u7ed3\u5408\u51e0\u4f55\u548c\u4f4d\u7f6e\u56e0\u7d20\u8bc4\u4f303D\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u4e3b\u89c2\u7814\u7a76\u5206\u6790\u7528\u6237\u5bf9\u4e0d\u540c\u6a21\u7cca\u7a0b\u5ea6\u7684\u504f\u597d\uff0c\u6700\u7ec8\u5efa\u7acb\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u573a\u666f\u7684\u4e3b\u89c2\u7814\u7a76\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eDASC\u6307\u6807\u7684\u6a21\u578b\uff0c\u80fd\u591f\u9884\u6d4b\u7528\u6237\u504f\u597d\u7684\u6a21\u7cca\u7a0b\u5ea6\u3002", "conclusion": "DASC\u6307\u6807\u548c\u9884\u6d4b\u6a21\u578b\u53ef\u5e2e\u52a9\u4f18\u5316\u5149\u573a\u663e\u793a\u76843D\u5185\u5bb9\u6e32\u67d3\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.08327", "pdf": "https://arxiv.org/pdf/2508.08327", "abs": "https://arxiv.org/abs/2508.08327", "authors": ["Ning Li", "Kounianhua Du", "Han Zhang", "Quan Gan", "Minjie Wang", "David Wipf", "Weinan Zhang"], "title": "Synthesize, Retrieve, and Propagate: A Unified Predictive Modeling Framework for Relational Databases", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Relational databases (RDBs) have become the industry standard for storing\nmassive and heterogeneous data. However, despite the widespread use of RDBs\nacross various fields, the inherent structure of relational databases hinders\ntheir ability to benefit from flourishing deep learning methods. Previous\nresearch has primarily focused on exploiting the unary dependency among\nmultiple tables in a relational database using the primary key - foreign key\nrelationships, either joining multiple tables into a single table or\nconstructing a graph among them, which leaves the implicit composite relations\namong different tables and a substantial potential of improvement for\npredictive modeling unexplored. In this paper, we propose SRP, a unified\npredictive modeling framework that synthesizes features using the unary\ndependency, retrieves related information to capture the composite dependency,\nand propagates messages across a constructed graph to learn adjacent patterns\nfor prediction on relation databases. By introducing a new retrieval mechanism\ninto RDB, SRP is designed to fully capture both the unary and the composite\ndependencies within a relational database, thereby enhancing the receptive\nfield of tabular data prediction. In addition, we conduct a comprehensive\nanalysis on the components of SRP, offering a nuanced understanding of model\nbehaviors and practical guidelines for future applications. Extensive\nexperiments on five real-world datasets demonstrate the effectiveness of SRP\nand its potential applicability in industrial scenarios. The code is released\nat https://github.com/NingLi670/SRP.", "AI": {"tldr": "SRP\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5173\u7cfb\u6570\u636e\u5e93\u9884\u6d4b\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u6355\u83b7\u5355\u4f9d\u8d56\u548c\u590d\u5408\u4f9d\u8d56\uff0c\u589e\u5f3a\u8868\u683c\u6570\u636e\u9884\u6d4b\u7684\u80fd\u529b\u3002", "motivation": "\u5173\u7cfb\u6570\u636e\u5e93\uff08RDBs\uff09\u7684\u56fa\u6709\u7ed3\u6784\u9650\u5236\u4e86\u5176\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5229\u7528\u4e3b\u952e-\u5916\u952e\u5173\u7cfb\u5904\u7406\u5355\u4f9d\u8d56\uff0c\u800c\u5ffd\u7565\u4e86\u8868\u95f4\u9690\u542b\u7684\u590d\u5408\u5173\u7cfb\u3002", "method": "SRP\u6846\u67b6\u901a\u8fc7\u5355\u4f9d\u8d56\u5408\u6210\u7279\u5f81\u3001\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\u6355\u83b7\u590d\u5408\u4f9d\u8d56\uff0c\u5e76\u5728\u6784\u5efa\u7684\u56fe\u4e2d\u4f20\u64ad\u6d88\u606f\u4ee5\u5b66\u4e60\u76f8\u90bb\u6a21\u5f0f\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eSRP\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "SRP\u901a\u8fc7\u65b0\u7684\u68c0\u7d22\u673a\u5236\u5145\u5206\u63a2\u7d22\u4e86\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u7684\u5355\u4f9d\u8d56\u548c\u590d\u5408\u4f9d\u8d56\uff0c\u4e3a\u8868\u683c\u6570\u636e\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u611f\u53d7\u91ce\u3002"}}
{"id": "2508.08315", "pdf": "https://arxiv.org/pdf/2508.08315", "abs": "https://arxiv.org/abs/2508.08315", "authors": ["Victor Lopez Juarez"], "title": "EU Digital Regulation and Guatemala: AI, 5G, and Cybersecurity", "categories": ["cs.CY", "cs.AI", "cs.ET"], "comment": null, "summary": "The paper examines how EU rules in AI, 5G, and cybersecurity operate as\ntransnational governance and shape policy in Guatemala. It outlines the AI\nAct's risk approach, the 5G Action Plan and Security Toolbox, and the\ncybersecurity regime built on ENISA, NIS2, the Cybersecurity Act, and the Cyber\nResilience Act. It traces extraterritorial channels such as the Brussels\neffect, private standards, supply chain clauses, and data transfer controls.\nGuatemala specific impacts include SME compliance costs, procurement limits,\nenvironmental trade-offs in rollout, rights risks, and capacity gaps. The paper\nmaps current national measures and proposes five guardrails: digital\nconstitutionalism, green IT duties, third country impact assessment, standards\nco-design, and recognition of regulatory diversity.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6b27\u76df\u5728AI\u30015G\u548c\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u89c4\u5219\u5982\u4f55\u4f5c\u4e3a\u8de8\u56fd\u6cbb\u7406\u5de5\u5177\u5f71\u54cd\u5371\u5730\u9a6c\u62c9\u7684\u653f\u7b56\u3002", "motivation": "\u7814\u7a76\u6b27\u76df\u6cd5\u89c4\u5bf9\u5371\u5730\u9a6c\u62c9\u7b49\u7b2c\u4e09\u56fd\u7684\u5177\u4f53\u5f71\u54cd\uff0c\u63ed\u793a\u8de8\u56fd\u6cbb\u7406\u7684\u673a\u5236\u3002", "method": "\u5206\u6790\u6b27\u76df\u7684AI\u6cd5\u6848\u30015G\u884c\u52a8\u8ba1\u5212\u3001\u5b89\u5168\u5de5\u5177\u7bb1\u53ca\u7f51\u7edc\u5b89\u5168\u5236\u5ea6\uff0c\u5e76\u8ffd\u8e2a\u5176\u8de8\u56fd\u6e20\u9053\u3002", "result": "\u5371\u5730\u9a6c\u62c9\u9762\u4e34\u4e2d\u5c0f\u4f01\u4e1a\u5408\u89c4\u6210\u672c\u3001\u91c7\u8d2d\u9650\u5236\u3001\u73af\u5883\u6743\u8861\u7b49\u6311\u6218\u3002", "conclusion": "\u63d0\u51fa\u4e94\u9879\u9632\u62a4\u63aa\u65bd\uff0c\u5305\u62ec\u6570\u5b57\u5baa\u653f\u4e3b\u4e49\u548c\u7eff\u8272IT\u4e49\u52a1\uff0c\u4ee5\u5e94\u5bf9\u8de8\u56fd\u6cbb\u7406\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2508.08457", "pdf": "https://arxiv.org/pdf/2508.08457", "abs": "https://arxiv.org/abs/2508.08457", "authors": ["Ming-Yen Lee", "Faaiq Waqar", "Hanchen Yang", "Muhammed Ahosan Ul Karim", "Harsono Simka", "Shimeng Yu"], "title": "Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories", "categories": ["cs.AR", "cs.ET", "C.1.3; B.3.1"], "comment": "7 pages, 8 figures, 2 tables", "summary": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u5d4c\u5165\u5f0f\u5b58\u50a8\u5668\u7684\u9884\u53d6\u8c03\u5ea6\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2dKV\u7f13\u5b58\u4f20\u8f93\u5bfc\u81f4\u7684HBM\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\u3002", "method": "\u91c7\u7528M3D BEOL\u517c\u5bb9\u5d4c\u5165\u5f0f\u5b58\u50a8\u5668\uff0c\u7ed3\u5408\u9884\u53d6\u548c\u8c03\u5ea6\u4f18\u5316\u3002", "result": "\u5728Llama3.1-8B\u4e0a\u5b9e\u73b08.06\u500d\u89e3\u7801\u52a0\u901f\u548c1.83\u500d\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u8bbe\u8ba1\u4f18\u5316\uff0c\u663e\u8457\u7f13\u89e3HBM\u9650\u5236\uff0c\u63d0\u5347\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2508.08479", "pdf": "https://arxiv.org/pdf/2508.08479", "abs": "https://arxiv.org/abs/2508.08479", "authors": ["Yuvraj Dutta", "Soumyajit Chatterjee", "Sandip Chakraborty", "Basabdatta Palit"], "title": "Benchmarking Federated Learning for Throughput Prediction in 5G Live Streaming Applications", "categories": ["cs.DC", "cs.LG", "14J60", "F.2.2; I.2.7"], "comment": "14 pages, 24 figures, submitted to IEEE TNET", "summary": "Accurate and adaptive network throughput prediction is essential for\nlatency-sensitive and bandwidth-intensive applications in 5G and emerging 6G\nnetworks. However, most existing methods rely on centralized training with\nuniformly collected data, limiting their applicability in heterogeneous mobile\nenvironments with non-IID data distributions. This paper presents the first\ncomprehensive benchmarking of federated learning (FL) strategies for throughput\nprediction in realistic 5G edge scenarios. We evaluate three aggregation\nalgorithms - FedAvg, FedProx, and FedBN - across four time-series\narchitectures: LSTM, CNN, CNN+LSTM, and Transformer, using five diverse\nreal-world datasets. We systematically analyze the effects of client\nheterogeneity, cohort size, and history window length on prediction\nperformance. Our results reveal key trade-offs among model complexities,\nconvergence rates, and generalization. It is found that FedBN consistently\ndelivers robust performance under non-IID conditions. On the other hand, LSTM\nand Transformer models outperform CNN-based baselines by up to 80% in R2\nscores. Moreover, although Transformers converge in half the rounds of LSTM,\nthey require longer history windows to achieve a high R2, indicating higher\ncontext dependence. LSTM is, therefore, found to achieve a favorable balance\nbetween accuracy, rounds, and temporal footprint. To validate the end-to-end\napplicability of the framework, we have integrated our FL-based predictors into\na live adaptive streaming pipeline. It is seen that FedBN-based LSTM and\nTransformer models improve mean QoE scores by 11.7% and 11.4%, respectively,\nover FedAvg, while also reducing the variance. These findings offer actionable\ninsights for building scalable, privacy-preserving, and edge-aware throughput\nprediction systems in next-generation wireless networks.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7b56\u7565\u57285G\u8fb9\u7f18\u573a\u666f\u4e2d\u7684\u7f51\u7edc\u541e\u5410\u91cf\u9884\u6d4b\u8fdb\u884c\u4e86\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u805a\u5408\u7b97\u6cd5\u548c\u56db\u79cd\u65f6\u5e8f\u67b6\u6784\uff0c\u53d1\u73b0FedBN\u5728\u975eIID\u6570\u636e\u4e0b\u8868\u73b0\u7a33\u5065\uff0cLSTM\u548cTransformer\u5728R2\u5206\u6570\u4e0a\u4f18\u4e8eCNN\uff0c\u4e14LSTM\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u541e\u5410\u91cf\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u548c\u5747\u5300\u6570\u636e\uff0c\u96be\u4ee5\u9002\u5e945G\u548c6G\u7f51\u7edc\u4e2d\u5f02\u6784\u79fb\u52a8\u73af\u5883\u7684\u975eIID\u6570\u636e\u5206\u5e03\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u7075\u6d3b\u7684FL\u7b56\u7565\u3002", "method": "\u8bc4\u4f30\u4e86FedAvg\u3001FedProx\u548cFedBN\u4e09\u79cdFL\u805a\u5408\u7b97\u6cd5\uff0c\u4ee5\u53caLSTM\u3001CNN\u3001CNN+LSTM\u548cTransformer\u56db\u79cd\u65f6\u5e8f\u67b6\u6784\uff0c\u4f7f\u7528\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u5206\u6790\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u3001\u5206\u7ec4\u5927\u5c0f\u548c\u5386\u53f2\u7a97\u53e3\u957f\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "FedBN\u5728\u975eIID\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\uff1bLSTM\u548cTransformer\u5728R2\u5206\u6570\u4e0a\u6bd4CNN\u9ad8\u51fa80%\uff1bTransformer\u6536\u655b\u66f4\u5feb\u4f46\u5bf9\u5386\u53f2\u7a97\u53e3\u8981\u6c42\u66f4\u9ad8\uff1bLSTM\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\u4e0a\u8868\u73b0\u5747\u8861\u3002\u5e94\u7528\u4e8e\u5b9e\u65f6\u6d41\u5a92\u4f53\u540e\uff0c\u57fa\u4e8eFedBN\u7684\u6a21\u578b\u5728QoE\u4e0a\u63d0\u5347\u4e8611%\u4ee5\u4e0a\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u53ef\u6269\u5c55\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u8fb9\u7f18\u611f\u77e5\u7684\u541e\u5410\u91cf\u9884\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u63a8\u8350FedBN\u4e0eLSTM\u6216Transformer\u7ed3\u5408\u4f7f\u7528\u3002"}}
{"id": "2508.08868", "pdf": "https://arxiv.org/pdf/2508.08868", "abs": "https://arxiv.org/abs/2508.08868", "authors": ["Henning Femmer", "Frank Houdek", "Max Unterbusch", "Andreas Vogelsang"], "title": "Description and Comparative Analysis of QuRE: A New Industrial Requirements Quality Dataset", "categories": ["cs.SE"], "comment": null, "summary": "Requirements quality is central to successful software and systems\nengineering. Empirical research on quality defects in natural language\nrequirements relies heavily on datasets, ideally as realistic and\nrepresentative as possible. However, such datasets are often inaccessible,\nsmall, or lack sufficient detail. This paper introduces QuRE (Quality in\nRequirements), a new dataset comprising 2,111 industrial requirements that have\nbeen annotated through a real-world review process. Previously used for over\nfive years as part of an industrial contract, this dataset is now being\nreleased to the research community. In this work, we furthermore provide\ndescriptive statistics on the dataset, including measures such as lexical\ndiversity and readability, and compare it to existing requirements datasets and\nsynthetically generated requirements. In contrast to synthetic datasets, QuRE\nis linguistically similar to existing ones. However, this dataset comes with a\ndetailed context description, and its labels have been created and used\nsystematically and extensively in an industrial context over a period of close\nto a decade. Our goal is to foster transparency, comparability, and empirical\nrigor by supporting the development of a common gold standard for requirements\nquality datasets. This, in turn, will enable more sound and collaborative\nresearch efforts in the field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86QuRE\u6570\u636e\u96c6\uff0c\u5305\u542b2111\u6761\u5de5\u4e1a\u9700\u6c42\uff0c\u6807\u6ce8\u8be6\u7ec6\uff0c\u65e8\u5728\u63d0\u5347\u9700\u6c42\u8d28\u91cf\u7814\u7a76\u7684\u900f\u660e\u5ea6\u548c\u53ef\u6bd4\u6027\u3002", "motivation": "\u9700\u6c42\u8d28\u91cf\u5bf9\u8f6f\u4ef6\u548c\u7cfb\u7edf\u5de5\u7a0b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u4e0d\u53ef\u8bbf\u95ee\u3001\u89c4\u6a21\u5c0f\u6216\u7f3a\u4e4f\u7ec6\u8282\u3002", "method": "\u53d1\u5e03\u4e86QuRE\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u63cf\u8ff0\u6027\u7edf\u8ba1\uff0c\u5bf9\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u548c\u5408\u6210\u9700\u6c42\u3002", "result": "QuRE\u6570\u636e\u96c6\u4f53\u73b0\u4e86\u8bed\u8a00\u591a\u6837\u6027\uff0c\u5e76\u4e0e\u73b0\u6709\u6570\u636e\u96c6\u76f8\u4f3c\uff0c\u4f46\u6807\u6ce8\u66f4\u8be6\u7ec6\u4e14\u5de5\u4e1a\u80cc\u666f\u4e30\u5bcc\u3002", "conclusion": "QuRE\u652f\u6301\u5efa\u7acb\u4e00\u4e2a\u901a\u7528\u7684\u9700\u6c42\u8d28\u91cf\u9ec4\u91d1\u6807\u51c6\uff0c\u4fc3\u8fdb\u66f4\u4e25\u8c28\u7684\u5b9e\u8bc1\u7814\u7a76\u3002"}}
{"id": "2508.08292", "pdf": "https://arxiv.org/pdf/2508.08292", "abs": "https://arxiv.org/abs/2508.08292", "authors": ["Aryan Gulati", "Brando Miranda", "Eric Chen", "Emily Xia", "Kai Fronsdal", "Bruno Dumont", "Elyas Obbad", "Sanmi Koyejo"], "title": "Putnam-AXIOM: A Functional and Static Benchmark", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO", "cs.NE", "68T20, 68T05, 68Q32", "F.2.2; I.2.3; I.2.6; I.2.8"], "comment": "27 pages total (10-page main paper + 17-page appendix), 12 figures, 6\n  tables. Submitted to ICML 2025 (under review)", "summary": "Current mathematical reasoning benchmarks for large language models (LLMs)\nare approaching saturation, with some achieving > 90% accuracy, and are\nincreasingly compromised by training-set contamination. We introduce\nPutnam-AXIOM, a benchmark of 522 university-level competition problems drawn\nfrom the prestigious William Lowell Putnam Mathematical Competition, and\nPutnam-AXIOM Variation, an unseen companion set of 100 functional variants\ngenerated by programmatically perturbing variables and constants. The variation\nprotocol produces an unlimited stream of equally difficult, unseen instances --\nyielding a contamination-resilient test bed. On the Original set, OpenAI's\no1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy\ndrops by 19.6% (46.8% relative decrease) on the paired Variations. The\nremaining eighteen models show the same downward trend, ten of them with\nnon-overlapping 95% confidence intervals. These gaps suggest memorization and\nhighlight the necessity of dynamic benchmarks. We complement \"boxed\" accuracy\nwith Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores\nreasoning traces and automates natural language proof evaluations. Putnam-AXIOM\ntherefore provides a rigorous, contamination-resilient evaluation framework for\nassessing advanced mathematical reasoning of LLMs. Data and evaluation code are\npublicly available at https://github.com/brando90/putnam-axiom.", "AI": {"tldr": "\u63d0\u51fa\u4e86Putnam-AXIOM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5927\u5b66\u6570\u5b66\u7ade\u8d5b\u9898\u53ca\u5176\u53d8\u4f53\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u6570\u636e\u6c61\u67d3\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u5df2\u63a5\u8fd1\u9971\u548c\uff0c\u4e14\u53d7\u8bad\u7ec3\u96c6\u6c61\u67d3\u5f71\u54cd\uff0c\u9700\u8981\u52a8\u6001\u3001\u6297\u6c61\u67d3\u7684\u65b0\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4ecePutnam\u6570\u5b66\u7ade\u8d5b\u4e2d\u9009\u53d6522\u9053\u9898\uff0c\u5e76\u751f\u6210100\u9053\u53d8\u4f53\uff0c\u4f7f\u7528\u52a8\u6001\u53d8\u4f53\u534f\u8bae\u751f\u6210\u65e0\u9650\u65b0\u9898\u3002\u5f15\u5165TFA\u6307\u6807\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u6700\u5f3a\u6a21\u578b\u5728\u539f\u59cb\u9898\u4e0a\u5f97\u520641.9%\uff0c\u4f46\u5728\u53d8\u4f53\u4e0a\u4e0b\u964d19.6%\uff08\u76f8\u5bf946.8%\uff09\uff0c\u5176\u4ed6\u6a21\u578b\u4e5f\u8868\u73b0\u4e0b\u964d\u8d8b\u52bf\u3002", "conclusion": "Putnam-AXIOM\u63d0\u4f9b\u4e86\u6297\u6c61\u67d3\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5f3a\u8c03\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u771f\u5b9e\u8bc4\u4f30\u3002"}}
{"id": "2508.08906", "pdf": "https://arxiv.org/pdf/2508.08906", "abs": "https://arxiv.org/abs/2508.08906", "authors": ["Torsten Hoefler", "Karen Schramm", "Eric Spada", "Keith Underwood", "Cedell Alexander", "Bob Alverson", "Paul Bottorff", "Adrian Caulfield", "Mark Handley", "Cathy Huang", "Costin Raiciu", "Abdul Kabbani", "Eugene Opsasnick", "Rong Pan", "Adee Ran", "Rip Sohan"], "title": "Ultra Ethernet's Design Principles and Architectural Innovations", "categories": ["cs.NI", "cs.AR", "cs.DC", "cs.OS", "cs.PF"], "comment": null, "summary": "The recently released Ultra Ethernet (UE) 1.0 specification defines a\ntransformative High-Performance Ethernet standard for future Artificial\nIntelligence (AI) and High-Performance Computing (HPC) systems. This paper,\nwritten by the specification's authors, provides a high-level overview of UE's\ndesign, offering crucial motivations and scientific context to understand its\ninnovations. While UE introduces advancements across the entire Ethernet stack,\nits standout contribution is the novel Ultra Ethernet Transport (UET), a\npotentially fully hardware-accelerated protocol engineered for reliable, fast,\nand efficient communication in extreme-scale systems. Unlike InfiniBand, the\nlast major standardization effort in high-performance networking over two\ndecades ago, UE leverages the expansive Ethernet ecosystem and the 1,000x gains\nin computational efficiency per moved bit to deliver a new era of\nhigh-performance networking.", "AI": {"tldr": "Ultra Ethernet (UE) 1.0\u662f\u4e00\u79cd\u9762\u5411\u672a\u6765AI\u548cHPC\u7cfb\u7edf\u7684\u9ad8\u6027\u80fd\u4ee5\u592a\u7f51\u6807\u51c6\uff0c\u5176\u6838\u5fc3\u521b\u65b0\u662fUltra Ethernet Transport (UET)\uff0c\u65e8\u5728\u4e3a\u8d85\u5927\u89c4\u6a21\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u9760\u7684\u901a\u4fe1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u9ad8\u6027\u80fd\u7f51\u7edc\u6807\u51c6\uff08\u5982InfiniBand\uff09\u5728\u6781\u7aef\u89c4\u6a21\u7cfb\u7edf\u4e2d\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u5229\u7528\u4ee5\u592a\u7f51\u751f\u6001\u7cfb\u7edf\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5de8\u5927\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u786c\u4ef6\u52a0\u901f\u7684Ultra Ethernet Transport (UET)\u534f\u8bae\uff0c\u4f18\u5316\u6574\u4e2a\u4ee5\u592a\u7f51\u5806\u6808\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6570\u636e\u4f20\u8f93\u3002", "result": "Ultra Ethernet (UE) 1.0\u5b9a\u4e49\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u9ad8\u6027\u80fd\u7f51\u7edc\u6807\u51c6\uff0c\u4e3a\u672a\u6765AI\u548cHPC\u7cfb\u7edf\u63d0\u4f9b\u66f4\u5feb\u901f\u3001\u53ef\u9760\u7684\u901a\u4fe1\u80fd\u529b\u3002", "conclusion": "UE 1.0\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u5145\u5206\u5229\u7528\u4ee5\u592a\u7f51\u751f\u6001\u7cfb\u7edf\uff0c\u5f00\u521b\u4e86\u9ad8\u6027\u80fd\u7f51\u7edc\u7684\u65b0\u65f6\u4ee3\u3002"}}
{"id": "2508.08542", "pdf": "https://arxiv.org/pdf/2508.08542", "abs": "https://arxiv.org/abs/2508.08542", "authors": ["Dasith de Silva Edirimuni", "Xuequan Lu", "Ajmal Saeed Mian", "Lei Wei", "Gang Li", "Scott Schaefer", "Ying He"], "title": "Hybrid Long and Short Range Flows for Point Cloud Filtering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Point cloud capture processes are error-prone and introduce noisy artifacts\nthat necessitate filtering/denoising. Recent filtering methods often suffer\nfrom point clustering or noise retaining issues. In this paper, we propose\nHybrid Point Cloud Filtering ($\\textbf{HybridPF}$) that considers both\nshort-range and long-range filtering trajectories when removing noise. It is\nwell established that short range scores, given by $\\nabla_{x}\\log p(x_t)$, may\nprovide the necessary displacements to move noisy points to the underlying\nclean surface. By contrast, long range velocity flows approximate constant\ndisplacements directed from a high noise variant patch $x_0$ towards the\ncorresponding clean surface $x_1$. Here, noisy patches $x_t$ are viewed as\nintermediate states between the high noise variant and the clean patches. Our\nintuition is that long range information from velocity flow models can guide\nthe short range scores to align more closely with the clean points. In turn,\nscore models generally provide a quicker convergence to the clean surface.\nSpecifically, we devise two parallel modules, the ShortModule and LongModule,\neach consisting of an Encoder-Decoder pair to respectively account for\nshort-range scores and long-range flows. We find that short-range scores,\nguided by long-range features, yield filtered point clouds with good point\ndistributions and convergence near the clean surface. We design a joint loss\nfunction to simultaneously train the ShortModule and LongModule, in an\nend-to-end manner. Finally, we identify a key weakness in current displacement\nbased methods, limitations on the decoder architecture, and propose a dynamic\ngraph convolutional decoder to improve the inference process. Comprehensive\nexperiments demonstrate that our HybridPF achieves state-of-the-art results\nwhile enabling faster inference speed.", "AI": {"tldr": "\u63d0\u51faHybridPF\u65b9\u6cd5\uff0c\u7ed3\u5408\u77ed\u7a0b\u548c\u957f\u7a0b\u8f68\u8ff9\u8fdb\u884c\u70b9\u4e91\u53bb\u566a\uff0c\u901a\u8fc7\u5e76\u884c\u6a21\u5757\u63d0\u5347\u6548\u679c\u548c\u901f\u5ea6\u3002", "motivation": "\u5f53\u524d\u70b9\u4e91\u53bb\u566a\u65b9\u6cd5\u5b58\u5728\u70b9\u805a\u7c7b\u6216\u566a\u58f0\u6b8b\u7559\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u8bbe\u8ba1ShortModule\u548cLongModule\u5e76\u884c\u6a21\u5757\uff0c\u7ed3\u5408\u77ed\u7a0b\u5206\u6570\u548c\u957f\u7a0b\u6d41\uff0c\u52a8\u6001\u56fe\u5377\u79ef\u89e3\u7801\u5668\u4f18\u5316\u63a8\u7406\u3002", "result": "HybridPF\u5b9e\u73b0\u6700\u4f73\u53bb\u566a\u6548\u679c\u548c\u66f4\u5feb\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "HybridPF\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u548c\u591a\u8f68\u8ff9\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u70b9\u4e91\u53bb\u566a\u6027\u80fd\u3002"}}
{"id": "2508.08555", "pdf": "https://arxiv.org/pdf/2508.08555", "abs": "https://arxiv.org/abs/2508.08555", "authors": ["Tong Zhang", "Yu Gou", "Jun Liu", "Jun-Hong Cui"], "title": "Traffic Load-Aware Resource Management Strategy for Underwater Wireless Sensor Networks", "categories": ["cs.NI"], "comment": "Accepted by IEEE Transactions on Mobile Computing", "summary": "Underwater Wireless Sensor Networks (UWSNs) represent a promising technology\nthat enables diverse underwater applications through acoustic communication.\nHowever, it encounters significant challenges including harsh communication\nenvironments, limited energy supply, and restricted signal transmission. This\npaper aims to provide efficient and reliable communication in underwater\nnetworks with limited energy and communication resources by optimizing the\nscheduling of communication links and adjusting transmission parameters (e.g.,\ntransmit power and transmission rate). The efficient and reliable communication\nmulti-objective optimization problem (ERCMOP) is formulated as a decentralized\npartially observable Markov decision process (Dec-POMDP). A Traffic Load-Aware\nResource Management (TARM) strategy based on deep multi-agent reinforcement\nlearning (MARL) is presented to address this problem. Specifically, a traffic\nload-aware mechanism that leverages the overhear information from neighboring\nnodes is designed to mitigate the disparity between partial observations and\nglobal states. Moreover, by incorporating a solution space optimization\nalgorithm, the number of candidate solutions for the deep MARL-based\ndecision-making model can be effectively reduced, thereby optimizing the\ncomputational complexity. Simulation results demonstrate the adaptability of\nTARM in various scenarios with different transmission demands and collision\nprobabilities, while also validating the effectiveness of the proposed approach\nin supporting efficient and reliable communication in underwater networks with\nlimited resources.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4f18\u5316\u901a\u4fe1\u94fe\u8def\u8c03\u5ea6\u548c\u4f20\u8f93\u53c2\u6570\uff0c\u89e3\u51b3\u6c34\u4e0b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\uff08UWSNs\uff09\u80fd\u6e90\u548c\u901a\u4fe1\u8d44\u6e90\u6709\u9650\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684TARM\u7b56\u7565\u3002", "motivation": "\u6c34\u4e0b\u7f51\u7edc\u9762\u4e34\u4e25\u82db\u7684\u901a\u4fe1\u73af\u5883\u3001\u80fd\u6e90\u9650\u5236\u548c\u4fe1\u53f7\u4f20\u8f93\u95ee\u9898\uff0c\u4e9f\u9700\u9ad8\u6548\u53ef\u9760\u7684\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u5206\u6563\u5f0f\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684TARM\u7b56\u7565\uff0c\u5305\u62ec\u6d41\u91cf\u611f\u77e5\u673a\u5236\u548c\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793aTARM\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9002\u5e94\u6027\uff0c\u5e76\u80fd\u6709\u6548\u652f\u6301\u8d44\u6e90\u6709\u9650\u7684\u6c34\u4e0b\u7f51\u7edc\u901a\u4fe1\u3002", "conclusion": "TARM\u7b56\u7565\u4e3a\u89e3\u51b3\u6c34\u4e0b\u7f51\u7edc\u7684\u901a\u4fe1\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.08502", "pdf": "https://arxiv.org/pdf/2508.08502", "abs": "https://arxiv.org/abs/2508.08502", "authors": ["Marta Robledo-Moreno", "Ruben Vera-Rodriguez", "Ruben Tolosana", "Javier Ortega-Garcia", "Andres Huergo", "Julian Fierrez"], "title": "AirSignatureDB: Exploring In-Air Signature Biometrics in the Wild and its Privacy Concerns", "categories": ["cs.HC"], "comment": null, "summary": "Behavioral biometrics based on smartphone motion sensors are growing in\npopularity for authentication purposes. In this study, AirSignatureDB is\npresented: a new publicly accessible dataset of in-air signatures collected\nfrom 108 participants under real-world conditions, using 83 different\nsmartphone models across four sessions. This dataset includes genuine samples\nand skilled forgeries, enabling a comprehensive evaluation of system robustness\nagainst realistic attack scenarios. Traditional and deep learning-based methods\nfor in-air signature verification are benchmarked, while analyzing the\ninfluence of sensor modality and enrollment strategies. Beyond verification, a\nfirst approach to reconstructing the three-dimensional trajectory of in-air\nsignatures from inertial sensor data alone is introduced. Using on-line\nhandwritten signatures as a reference, we demonstrate that the recovery of\naccurate trajectories is feasible, challenging the long-held assumption that\nin-air gestures are inherently traceless. Although this approach enables\nforensic traceability, it also raises critical questions about the privacy\nboundaries of behavioral biometrics. Our findings underscore the need for a\nreevaluation of the privacy assumptions surrounding inertial sensor data, as\nthey can reveal user-specific information that had not previously been\nconsidered in the design of in-air signature systems.", "AI": {"tldr": "AirSignatureDB\u662f\u4e00\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5305\u542b108\u540d\u53c2\u4e0e\u8005\u7684\u7a7a\u4e2d\u7b7e\u540d\u6570\u636e\uff0c\u652f\u6301\u771f\u5b9e\u653b\u51fb\u573a\u666f\u4e0b\u7684\u7cfb\u7edf\u9c81\u68d2\u6027\u8bc4\u4f30\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u60ef\u6027\u4f20\u611f\u5668\u6570\u636e\u91cd\u5efa\u7b7e\u540d\u4e09\u7ef4\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u6311\u6218\u4e86\u7a7a\u4e2d\u7b7e\u540d\u65e0\u75d5\u7684\u5047\u8bbe\uff0c\u5e76\u5f15\u53d1\u4e86\u5bf9\u884c\u4e3a\u751f\u7269\u8bc6\u522b\u9690\u79c1\u8fb9\u754c\u7684\u91cd\u65b0\u601d\u8003\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u586b\u8865\u7a7a\u4e2d\u7b7e\u540d\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9690\u79c1\u95ee\u9898\u3002", "method": "\u4f7f\u7528108\u540d\u53c2\u4e0e\u8005\u7684\u771f\u5b9e\u7b7e\u540d\u548c\u4f2a\u9020\u7b7e\u540d\u6570\u636e\uff0c\u7ed3\u5408\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u9a8c\u8bc1\uff0c\u540c\u65f6\u5c1d\u8bd5\u4ece\u60ef\u6027\u4f20\u611f\u5668\u6570\u636e\u91cd\u5efa\u4e09\u7ef4\u8f68\u8ff9\u3002", "result": "\u7ed3\u679c\u8868\u660e\u7a7a\u4e2d\u7b7e\u540d\u7684\u4e09\u7ef4\u8f68\u8ff9\u53ef\u4ee5\u91cd\u5efa\uff0c\u4e14\u6570\u636e\u96c6\u652f\u6301\u9ad8\u6548\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u60ef\u6027\u4f20\u611f\u5668\u6570\u636e\u6f5c\u5728\u7684\u9690\u79c1\u98ce\u9669\uff0c\u547c\u5401\u5728\u8bbe\u8ba1\u7a7a\u4e2d\u7b7e\u540d\u7cfb\u7edf\u65f6\u91cd\u65b0\u8003\u8651\u9690\u79c1\u5047\u8bbe\u3002"}}
{"id": "2508.08270", "pdf": "https://arxiv.org/pdf/2508.08270", "abs": "https://arxiv.org/abs/2508.08270", "authors": ["Dong Xue", "Ziyao Shao", "Zhaoyang Duan", "Fangzhou Liu", "Bing Li", "Zhongheng Zhang"], "title": "Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Large multimodal models (LMMs) have demonstrated significant potential in\nproviding innovative solutions for various biomedical tasks, including\npathology analysis, radiology report generation, and biomedical assistance.\nHowever, the existing multimodal biomedical AI is typically based on foundation\nLLMs, thus hindering the understanding of intricate medical concepts with\nlimited medical training data. Moreover, recent LLaVA-induced medical LMMs\nstruggle to effectively capture the intricate relationship between the texts\nand the images. Therefore, we introduce Doctor Sun, a large multimodal\ngenerative model specialized in medicine, developed to encode, integrate, and\ninterpret diverse biomedical data modalities such as text and images. In\nparticular, Doctor Sun integrates a pre-trained vision encoder with a medical\nLLM and conducts two-stage training on various medical datasets, focusing on\nfeature alignment and instruction tuning. Moreover, we release SunMed-VL, a\nwide-range bilingual medical multimodal dataset, along with all associated\nmodels, code, and resources, to freely support the advancement of biomedical\nmultimodal research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Doctor Sun\uff0c\u4e00\u79cd\u4e13\u4e3a\u533b\u5b66\u8bbe\u8ba1\u7684 multimodal \u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u533b\u5b66\u6982\u5ff5\u548c\u6587\u672c-\u56fe\u50cf\u5173\u7cfb\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u6570\u636e\u96c6\u548c\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66 multimodal \u6a21\u578b\u96be\u4ee5\u7406\u89e3\u590d\u6742\u533b\u5b66\u6982\u5ff5\uff0c\u4e14\u6587\u672c\u4e0e\u56fe\u50cf\u7684\u5173\u7cfb\u5904\u7406\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u3002", "method": "Doctor Sun \u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u533b\u5b66 LLM\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u7279\u5f81\u5bf9\u9f50\u548c\u6307\u4ee4\u8c03\u6574\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u4e86 Doctor Sun \u6a21\u578b\uff0c\u5e76\u5f00\u6e90\u4e86 SunMed-VL \u6570\u636e\u96c6\u53ca\u76f8\u5173\u8d44\u6e90\uff0c\u652f\u6301\u751f\u7269\u533b\u5b66 multimodal \u7814\u7a76\u7684\u53d1\u5c55\u3002", "conclusion": "Doctor Sun \u4e3a\u533b\u5b66 multimodal \u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.08469", "pdf": "https://arxiv.org/pdf/2508.08469", "abs": "https://arxiv.org/abs/2508.08469", "authors": ["Wenqi Jiang"], "title": "Vector-Centric Machine Learning Systems: A Cross-Stack Approach", "categories": ["cs.DB", "cs.AR", "cs.DC", "cs.PF"], "comment": "PhD Thesis (ETH Zurich)", "summary": "Today, two major trends are shaping the evolution of ML systems. First,\nmodern AI systems are becoming increasingly complex, often integrating\ncomponents beyond the model itself. A notable example is Retrieval-Augmented\nGeneration (RAG), which incorporates not only multiple models but also vector\ndatabases, leading to heterogeneity in both system components and underlying\nhardware. Second, with the end of Moore's Law, achieving high system efficiency\nis no longer feasible without accounting for the rapid evolution of the\nhardware landscape.\n  Building on the observations above, this thesis adopts a cross-stack approach\nto improving ML system efficiency, presenting solutions that span algorithms,\nsystems, and hardware. First, it introduces several pioneering works about RAG\nserving efficiency across the computing stack. PipeRAG focuses on\nalgorithm-level improvements, RAGO introduces system-level optimizations, and\nChameleon explores heterogeneous accelerator systems for RAG. Second, this\nthesis investigates algorithm-hardware co-design for vector search.\nSpecifically, FANNS and Falcon optimize quantization-based and graph-based\nvector search, the two most popular paradigms of retrieval algorithms. Third,\nthis thesis addresses the serving efficiency of recommender systems, another\nexample of vector-centric ML systems, where the memory-intensive lookup\noperations on embedding vector tables often represent a major performance\nbottleneck. MicroRec and FleetRec propose solutions at the hardware and system\nlevels, respectively, optimizing both data movement and computation to enhance\nthe efficiency of large-scale recommender models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u6548\u7387\u63d0\u5347\u7684\u8de8\u6808\u65b9\u6cd5\uff0c\u6db5\u76d6\u7b97\u6cd5\u3001\u7cfb\u7edf\u548c\u786c\u4ef6\u5c42\u9762\u7684\u4f18\u5316\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86RAG\u3001\u5411\u91cf\u641c\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u7387\u6539\u8fdb\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u589e\u52a0\u548c\u6469\u5c14\u5b9a\u5f8b\u7684\u7ec8\u7ed3\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u63d0\u5347\u7cfb\u7edf\u6548\u7387\uff0c\u9700\u8981\u8de8\u6808\u4f18\u5316\u6765\u5e94\u5bf9\u786c\u4ef6\u548c\u7b97\u6cd5\u7684\u5feb\u901f\u6f14\u8fdb\u3002", "method": "\u91c7\u7528\u8de8\u6808\u65b9\u6cd5\uff0c\u5206\u522b\u4ece\u7b97\u6cd5\uff08\u5982PipeRAG\uff09\u3001\u7cfb\u7edf\uff08\u5982RAGO\uff09\u548c\u786c\u4ef6\uff08\u5982Chameleon\uff09\u5c42\u9762\u4f18\u5316RAG\u6548\u7387\uff1b\u540c\u65f6\u5bf9\u5411\u91cf\u641c\u7d22\uff08\u5982FANNS\u3001Falcon\uff09\u548c\u63a8\u8350\u7cfb\u7edf\uff08\u5982MicroRec\u3001FleetRec\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u591a\u79cd\u9488\u5bf9\u4e0d\u540c\u5c42\u9762\u7684\u4f18\u5316\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u3001\u5411\u91cf\u641c\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u7684\u670d\u52a1\u6548\u7387\u3002", "conclusion": "\u8de8\u6808\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u4ee3ML\u7cfb\u7edf\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5f02\u6784\u786c\u4ef6\u73af\u5883\u4e0b\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.08503", "pdf": "https://arxiv.org/pdf/2508.08503", "abs": "https://arxiv.org/abs/2508.08503", "authors": ["Sabiha Tajdari", "Anastasia Ailamaki", "Sandhya Dwarkadas"], "title": "JSPIM: A Skew-Aware PIM Accelerator for High-Performance Databases Join and Select Operations", "categories": ["cs.AR", "cs.DB", "cs.PF"], "comment": null, "summary": "Database applications are increasingly bottlenecked by memory bandwidth and\nlatency due to the memory wall and the limited scalability of DRAM. Join\nqueries, central to analytical workloads, require intensive memory access and\nare particularly vulnerable to inefficiencies in data movement. While\nProcessing-in-Memory (PIM) offers a promising solution, existing designs\ntypically reuse CPU-oriented join algorithms, limiting parallelism and\nincurring costly inter-chip communication. Additionally, data skew, a main\nchallenge in CPU-based joins, remains unresolved in current PIM architectures.\n  We introduce JSPIM, a PIM module that accelerates hash join and, by\nextension, corresponding select queries through algorithm-hardware co-design.\nJSPIM deploys parallel search engines within each subarray and redesigns hash\ntables to achieve O(1) lookups, fully exploiting PIM's fine-grained\nparallelism. To mitigate skew, our design integrates subarray-level parallelism\nwith rank-level processing, eliminating redundant off-chip transfers.\nEvaluations show JSPIM delivers 400x to 1000x speedup on join queries versus\nDuckDB. When paired with DuckDB for the full SSB benchmark, JSPIM achieves an\noverall 2.5x throughput improvement (individual query gains of 1.1x to 28x), at\njust a 7% data overhead and 2.1% per-rank PIM-enabled chip area increase.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faJSPIM\uff0c\u4e00\u79cd\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u4f18\u5316\u7684PIM\u6a21\u5757\uff0c\u663e\u8457\u52a0\u901f\u54c8\u5e0c\u8fde\u63a5\u67e5\u8be2\uff0c\u89e3\u51b3\u5185\u5b58\u5e26\u5bbd\u548c\u5ef6\u8fdf\u74f6\u9888\u3002", "motivation": "\u6570\u636e\u5e93\u5e94\u7528\u53d7\u9650\u4e8e\u5185\u5b58\u5e26\u5bbd\u548c\u5ef6\u8fdf\uff0c\u800c\u5904\u7406\u5185\u5b58\uff08PIM\uff09\u8bbe\u8ba1\u4e2d\u672a\u89e3\u51b3CPU\u5bfc\u5411\u7b97\u6cd5\u7684\u5e76\u884c\u6027\u4e0d\u8db3\u548c\u6570\u636e\u503e\u659c\u95ee\u9898\u3002", "method": "JSPIM\u91c7\u7528\u5b50\u9635\u5217\u7ea7\u5e76\u884c\u641c\u7d22\u5f15\u64ce\u548c\u4f18\u5316\u7684\u54c8\u5e0c\u8868\u8bbe\u8ba1\uff0c\u5b9e\u73b0O(1)\u67e5\u627e\u5f00\u9500\uff0c\u540c\u65f6\u7ed3\u5408\u5b50\u9635\u5217\u548crank\u7ea7\u5904\u7406\u89e3\u51b3\u6570\u636e\u503e\u659c\u3002", "result": "JSPIM\u5728\u8fde\u63a5\u67e5\u8be2\u4e0a\u6bd4DuckDB\u5feb400\u81f31000\u500d\uff0c\u4e0eDuckDB\u7ed3\u5408\u65f6\u6574\u4f53\u541e\u5410\u91cf\u63d0\u53472.5\u500d\uff0c\u6570\u636e\u5f00\u9500\u4ec57%\u3002", "conclusion": "JSPIM\u901a\u8fc7\u7b97\u6cd5\u4e0e\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u9ad8\u6548\u89e3\u51b3\u5185\u5b58\u74f6\u9888\u548c\u67e5\u8be2\u6027\u80fd\u95ee\u9898\uff0c\u4e3aPIM\u67b6\u6784\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2508.08525", "pdf": "https://arxiv.org/pdf/2508.08525", "abs": "https://arxiv.org/abs/2508.08525", "authors": ["Xiaopei Zhang", "Xingang Wang", "Xin Wang"], "title": "A Reinforcement Learning-Driven Task Scheduling Algorithm for Multi-Tenant Distributed Systems", "categories": ["cs.DC"], "comment": null, "summary": "This paper addresses key challenges in task scheduling for multi-tenant\ndistributed systems, including dynamic resource variation, heterogeneous tenant\ndemands, and fairness assurance. An adaptive scheduling method based on\nreinforcement learning is proposed. By modeling the scheduling process as a\nMarkov decision process, the study defines the state space, action space, and\nreward function. A scheduling policy learning framework is designed using\nProximal Policy Optimization (PPO) as the core algorithm. This enables dynamic\nperception of complex system states and real-time decision-making. Under a\nmulti-objective reward mechanism, the scheduler jointly optimizes task latency,\nresource utilization, and tenant fairness. The coordination between the policy\nnetwork and the value network continuously refines the scheduling strategy.\nThis enhances overall system performance. To validate the effectiveness of the\nproposed method, a series of experiments were conducted in multi-scenario\nenvironments built using a real-world public dataset. The experiments evaluated\ntask latency control, resource efficiency, policy stability, and fairness. The\nresults show that the proposed method outperforms existing scheduling\napproaches across multiple evaluation metrics. It demonstrates strong stability\nand generalization ability. The proposed scheduling framework provides\npractical and engineering value in policy design, dynamic resource modeling,\nand multi-tenant service assurance. It effectively improves scheduling\nefficiency and resource management in distributed systems under complex\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u8c03\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u79df\u6237\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u4efb\u52a1\u8c03\u5ea6\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u591a\u79df\u6237\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u8d44\u6e90\u53d8\u5316\u3001\u5f02\u6784\u79df\u6237\u9700\u6c42\u548c\u516c\u5e73\u6027\u4fdd\u8bc1\u7b49\u6838\u5fc3\u6311\u6218\u3002", "method": "\u5c06\u8c03\u5ea6\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u8bbe\u8ba1\u8c03\u5ea6\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5956\u52b1\u673a\u5236\u4f18\u5316\u4efb\u52a1\u5ef6\u8fdf\u3001\u8d44\u6e90\u5229\u7528\u548c\u516c\u5e73\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5f3a\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u8c03\u5ea6\u6548\u7387\u548c\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.08872", "pdf": "https://arxiv.org/pdf/2508.08872", "abs": "https://arxiv.org/abs/2508.08872", "authors": ["Dylan Callaghan", "Alexandra van der Spuy", "Bernd Fischer"], "title": "Empirical Analysis of Temporal and Spatial Fault Characteristics in Multi-Fault Bug Repositories", "categories": ["cs.SE", "D.2.5"], "comment": null, "summary": "Fixing software faults contributes significantly to the cost of software\nmaintenance and evolution. Techniques for reducing these costs require datasets\nof software faults, as well as an understanding of the faults, for optimal\ntesting and evaluation. In this paper, we present an empirical analysis of the\ntemporal and spatial characteristics of faults existing in 16 open-source Java\nand Python projects, which form part of the Defects4J and BugsInPy datasets,\nrespectively. Our findings show that many faults in these software systems are\nlong-lived, leading to the majority of software versions having multiple\ncoexisting faults. This is in contrast to the assumptions of the original\ndatasets, where the majority of versions only identify a single fault. In\naddition, we show that although the faults are found in only a small subset of\nthe systems, these faults are often evenly distributed amongst this subset,\nleading to relatively few bug hotspots.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e8616\u4e2a\u5f00\u6e90Java\u548cPython\u9879\u76ee\u4e2d\u8f6f\u4ef6\u6545\u969c\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u53d1\u73b0\u591a\u6570\u6545\u969c\u957f\u671f\u5b58\u5728\u4e14\u5206\u5e03\u5747\u5300\uff0c\u4e0e\u539f\u59cb\u6570\u636e\u96c6\u5047\u8bbe\u7684\u5355\u6545\u969c\u7248\u672c\u4e0d\u7b26\u3002", "motivation": "\u964d\u4f4e\u8f6f\u4ef6\u7ef4\u62a4\u6210\u672c\u9700\u8981\u7406\u89e3\u6545\u969c\u7279\u5f81\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u5047\u8bbe\u4e0e\u5b9e\u9645\u60c5\u51b5\u5b58\u5728\u5dee\u5f02\u3002", "method": "\u4f7f\u7528Defects4J\u548cBugsInPy\u6570\u636e\u96c6\uff0c\u5bf916\u4e2a\u5f00\u6e90\u9879\u76ee\u8fdb\u884c\u6545\u969c\u65f6\u7a7a\u7279\u5f81\u5206\u6790\u3002", "result": "\u591a\u6570\u6545\u969c\u957f\u671f\u5b58\u5728\uff0c\u7248\u672c\u4e2d\u5e38\u6709\u591a\u6545\u969c\u5171\u5b58\uff0c\u6545\u969c\u5206\u5e03\u5747\u5300\uff0c\u70ed\u70b9\u8f83\u5c11\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6545\u969c\u7684\u771f\u5b9e\u7279\u5f81\uff0c\u4e3a\u4f18\u5316\u6d4b\u8bd5\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2508.08633", "pdf": "https://arxiv.org/pdf/2508.08633", "abs": "https://arxiv.org/abs/2508.08633", "authors": ["HuanYu Yang", "Fengming Zhu", "YangFan Wu", "Jianmin Ji"], "title": "Diminution: On Reducing the Size of Grounding ASP Programs", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Answer Set Programming (ASP) is often hindered by the grounding bottleneck:\nlarge Herbrand universes generate ground programs so large that solving becomes\ndifficult. Many methods employ ad-hoc heuristics to improve grounding\nperformance, motivating the need for a more formal and generalizable strategy.\nWe introduce the notion of diminution, defined as a selected subset of the\nHerbrand universe used to generate a reduced ground program before solving. We\ngive a formal definition of diminution, analyze its key properties, and study\nthe complexity of identifying it. We use a specific encoding that enables\noff-the-shelf ASP solver to evaluate candidate subsets. Our approach integrates\nseamlessly with existing grounders via domain predicates. In extensive\nexperiments on five benchmarks, applying diminutions selected by our strategy\nyields significant performance improvements, reducing grounding time by up to\n70% on average and decreasing the size of grounding files by up to 85%. These\nresults demonstrate that leveraging diminutions constitutes a robust and\ngeneral-purpose approach for alleviating the grounding bottleneck in ASP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\"diminution\"\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u9009\u62e9Herbrand\u5b87\u5b99\u7684\u5b50\u96c6\u6765\u51cf\u5c11ASP\u4e2d\u7684\u5730\u7a0b\u5e8f\u89c4\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4e3a\u89e3\u51b3ASP\u4e2d\u7531\u4e8e\u5927\u89c4\u6a21Herbrand\u5b87\u5b99\u5bfc\u81f4\u7684\u5730\u7a0b\u5e8f\u8fc7\u5927\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6b63\u5f0f\u4e14\u901a\u7528\u7684\u7b56\u7565\u6765\u4f18\u5316\u5730\u7a0b\u5e8f\u6027\u80fd\u3002", "method": "\u5f15\u5165diminution\u6982\u5ff5\uff0c\u5b9a\u4e49\u5176\u4e3aHerbrand\u5b87\u5b99\u7684\u5b50\u96c6\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7f16\u7801\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709ASP\u6c42\u89e3\u5668\u8bc4\u4f30\u5b50\u96c6\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u51cf\u5c1170%\u7684\u5730\u7a0b\u5e8f\u65f6\u95f4\uff0c\u5730\u7a0b\u5e8f\u6587\u4ef6\u5927\u5c0f\u964d\u4f4e85%\u3002", "conclusion": "diminution\u662f\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u7f13\u89e3ASP\u4e2d\u7684\u5730\u7a0b\u5e8f\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2508.08561", "pdf": "https://arxiv.org/pdf/2508.08561", "abs": "https://arxiv.org/abs/2508.08561", "authors": ["Aysan Mokhtarimousavi", "Michael Kleiss", "Mostafa Alani", "Sida Dai"], "title": "Revisiting the City Tower Project: Geometric Principles and Structural Morphology in the Works of Louis I. Kahn and Anne Tyng", "categories": ["cs.GR"], "comment": "8 pages, ARCC Conference", "summary": "This paper presents a study of computation and morphology of Louis Kahn City\nTower project. The City Tower is an unbuilt design by Louis I. Kahn and Anne\nTyng that integrates form and structure using 3D space triangular geometries.\nAlthough never built, the City Tower geometrical framework anticipated later\ndevelopments in design of space-frame structures. Initially envisioned in the\n1950s, the City Tower project is a skyscraper structure based on a tetrahedral\nand octahedral space frame called Octet-Truss. The aim of this study is to\nanalyze the geometry of the City Tower structure and how it can be used to\ndevelop modular and adaptable architectural forms. The study is based on an\nanalytical shape grammar that is used to recreate the original structure, and\nlater to generate new structural configurations based on the City Tower's\nmorphology. This study also investigates the potential applications of these\nfindings in architecture and reveals the possibilities of using tetrahedrons\nand octahedrons as fundamental geometries for creating scalable and modular\ndesigns and presents initial findings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Louis Kahn City Tower\u9879\u76ee\u7684\u8ba1\u7b97\u4e0e\u5f62\u6001\u5b66\uff0c\u5206\u6790\u4e86\u5176\u57fa\u4e8e\u56db\u9762\u4f53\u548c\u516b\u9762\u4f53\u7684\u7ed3\u6784\u53ca\u5176\u5728\u6a21\u5757\u5316\u5efa\u7b51\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u76ee\u7684\u662f\u63a2\u7d22City Tower\u7684\u51e0\u4f55\u7ed3\u6784\u5982\u4f55\u7528\u4e8e\u5f00\u53d1\u6a21\u5757\u5316\u548c\u9002\u5e94\u6027\u5efa\u7b51\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u5206\u6790\u6027\u5f62\u72b6\u8bed\u6cd5\u91cd\u73b0\u539f\u59cb\u7ed3\u6784\uff0c\u5e76\u57fa\u4e8e\u5176\u5f62\u6001\u5b66\u751f\u6210\u65b0\u7ed3\u6784\u914d\u7f6e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u56db\u9762\u4f53\u548c\u516b\u9762\u4f53\u53ef\u4f5c\u4e3a\u521b\u5efa\u53ef\u6269\u5c55\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u7684\u57fa\u7840\u51e0\u4f55\u4f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5efa\u7b51\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u672a\u6765\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08627", "pdf": "https://arxiv.org/pdf/2508.08627", "abs": "https://arxiv.org/abs/2508.08627", "authors": ["Conghao Zhou", "Lulu Sun", "Xiucheng Wang", "Peng Yang", "Feng Lyu", "Sihan Lu", "Xuemin Shen"], "title": "QoE-Aware Service Provision for Mobile AR Rendering: An Agent-Driven Approach", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Mobile augmented reality (MAR) is envisioned as a key immersive application\nin 6G, enabling virtual content rendering aligned with the physical environment\nthrough device pose estimation. In this paper, we propose a novel agent-driven\ncommunication service provisioning approach for edge-assisted MAR, aiming to\nreduce communication overhead between MAR devices and the edge server while\nensuring the quality of experience (QoE). First, to address the inaccessibility\nof MAR application-specific information to the network controller, we establish\na digital agent powered by large language models (LLMs) on behalf of the MAR\nservice provider, bridging the data and function gap between the MAR service\nand network domains. Second, to cope with the user-dependent and dynamic nature\nof data traffic patterns for individual devices, we develop a user-level QoE\nmodeling method that captures the relationship between communication resource\ndemands and perceived user QoE, enabling personalized, agent-driven\ncommunication resource management. Trace-driven simulation results demonstrate\nthat the proposed approach outperforms conventional LLM-based QoE-aware service\nprovisioning methods in both user-level QoE modeling accuracy and communication\nresource efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u8fb9\u7f18\u8f85\u52a9\u79fb\u52a8\u589e\u5f3a\u73b0\u5b9e\u901a\u4fe1\u670d\u52a1\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u548c\u7528\u6237\u7ea7QoE\u5efa\u6a21\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u5e76\u63d0\u5347\u4f53\u9a8c\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3MAR\u5e94\u7528\u4e2d\u7f51\u7edc\u63a7\u5236\u5668\u65e0\u6cd5\u83b7\u53d6\u7279\u5b9a\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5e94\u5bf9\u7528\u6237\u6570\u636e\u6d41\u91cf\u7684\u52a8\u6001\u7279\u6027\u3002", "method": "\u5229\u7528LLM\u5efa\u7acb\u4ee3\u7406\u8fde\u63a5MAR\u4e0e\u7f51\u7edc\u57df\uff0c\u5f00\u53d1\u7528\u6237\u7ea7QoE\u5efa\u6a21\u65b9\u6cd5\u8fdb\u884c\u4e2a\u6027\u5316\u8d44\u6e90\u7ba1\u7406\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728QoE\u5efa\u6a21\u7cbe\u5ea6\u548c\u901a\u4fe1\u8d44\u6e90\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u4ee3\u7406\u9a71\u52a8\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347MAR\u901a\u4fe1\u670d\u52a1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2508.08505", "pdf": "https://arxiv.org/pdf/2508.08505", "abs": "https://arxiv.org/abs/2508.08505", "authors": ["Chao-Jung Lai", "Mauricio Sousa", "Tianyu Zhang", "Ludwig Sidenmark", "Tovi Grossman"], "title": "Adaptique: Multi-objective and Context-aware Online Adaptation of Selection Techniques in Virtual Reality", "categories": ["cs.HC"], "comment": "16 pages, 7 figures, to appear in Proceedings of the 38th Annual ACM\n  Symposium on User Interface Software and Technology (UIST '25), September\n  28-October 1, 2025, Busan, Republic of Korea", "summary": "Selection is a fundamental task that is challenging in virtual reality due to\nissues such as distant and small targets, occlusion, and target-dense\nenvironments. Previous research has tackled these challenges through various\nselection techniques, but complicates selection and can be seen as tedious\noutside of their designed use case. We present Adaptique, an adaptive model\nthat infers and switches to the most optimal selection technique based on user\nand environmental information. Adaptique considers contextual information such\nas target size, distance, occlusion, and user posture combined with four\nobjectives: speed, accuracy, comfort, and familiarity which are based on\nfundamental predictive models of human movement for technique selection. This\nenables Adaptique to select simple techniques when they are sufficiently\nefficient and more advanced techniques when necessary. We show that Adaptique\nis more preferred and performant than single techniques in a user study, and\ndemonstrate Adaptique's versatility in an application.", "AI": {"tldr": "Adaptique\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u6a21\u578b\uff0c\u6839\u636e\u7528\u6237\u548c\u73af\u5883\u4fe1\u606f\u9009\u62e9\u6700\u4f18\u7684\u865a\u62df\u73b0\u5b9e\u9009\u62e9\u6280\u672f\uff0c\u4f18\u4e8e\u5355\u4e00\u6280\u672f\u3002", "motivation": "\u865a\u62df\u73b0\u5b9e\u4e2d\u76ee\u6807\u5c0f\u3001\u8fdc\u3001\u906e\u6321\u6216\u5bc6\u96c6\uff0c\u73b0\u6709\u6280\u672f\u590d\u6742\u4e14\u5c40\u9650\uff0c\u9700\u4e00\u79cd\u81ea\u9002\u5e94\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u76ee\u6807\u5927\u5c0f\u3001\u8ddd\u79bb\u3001\u906e\u6321\u7b49\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0e\u901f\u5ea6\u3001\u51c6\u786e\u6027\u3001\u8212\u9002\u5ea6\u7b49\u76ee\u6807\uff0c\u52a8\u6001\u5207\u6362\u6700\u4f73\u9009\u62e9\u6280\u672f\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660eAdaptique\u66f4\u53d7\u6b22\u8fce\u4e14\u6027\u80fd\u4f18\u4e8e\u5355\u4e00\u6280\u672f\u3002", "conclusion": "Adaptique\u901a\u8fc7\u81ea\u9002\u5e94\u6280\u672f\u9009\u62e9\u63d0\u9ad8\u4e86\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u9009\u62e9\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.08667", "pdf": "https://arxiv.org/pdf/2508.08667", "abs": "https://arxiv.org/abs/2508.08667", "authors": ["Ke Liu", "Xuanhan Wang", "Qilong Zhang", "Lianli Gao", "Jingkuan Song"], "title": "Learning Generalizable and Efficient Image Watermarking via Hierarchical Two-Stage Optimization", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Deep image watermarking, which refers to enable imperceptible watermark\nembedding and reliable extraction in cover images, has shown to be effective\nfor copyright protection of image assets. However, existing methods face\nlimitations in simultaneously satisfying three essential criteria for\ngeneralizable watermarking: 1) invisibility (imperceptible hide of watermarks),\n2) robustness (reliable watermark recovery under diverse conditions), and 3)\nbroad applicability (low latency in watermarking process). To address these\nlimitations, we propose a Hierarchical Watermark Learning (HiWL), a two-stage\noptimization that enable a watermarking model to simultaneously achieve three\ncriteria. In the first stage, distribution alignment learning is designed to\nestablish a common latent space with two constraints: 1) visual consistency\nbetween watermarked and non-watermarked images, and 2) information invariance\nacross watermark latent representations. In this way, multi-modal inputs\nincluding watermark message (binary codes) and cover images (RGB pixels) can be\nwell represented, ensuring the invisibility of watermarks and robustness in\nwatermarking process thereby. The second stage employs generalized watermark\nrepresentation learning to establish a disentanglement policy for separating\nwatermarks from image content in RGB space. In particular, it strongly\npenalizes substantial fluctuations in separated RGB watermarks corresponding to\nidentical messages. Consequently, HiWL effectively learns generalizable\nlatent-space watermark representations while maintaining broad applicability.\nExtensive experiments demonstrate the effectiveness of proposed method. In\nparticular, it achieves 7.6\\% higher accuracy in watermark extraction than\nexisting methods, while maintaining extremely low latency (100K images\nprocessed in 8s).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u56fe\u50cf\u6c34\u5370\u65b9\u6cd5HiWL\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u540c\u65f6\u6ee1\u8db3\u4e0d\u53ef\u89c1\u6027\u3001\u9c81\u68d2\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u56fe\u50cf\u6c34\u5370\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u4e0d\u53ef\u89c1\u6027\u3001\u9c81\u68d2\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86HiWL\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HiWL\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u5206\u5e03\u5bf9\u9f50\u5b66\u4e60\u5efa\u7acb\u5171\u540c\u6f5c\u5728\u7a7a\u95f4\uff0c\u786e\u4fdd\u6c34\u5370\u4e0d\u53ef\u89c1\u548c\u9c81\u68d2\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u5e7f\u4e49\u6c34\u5370\u8868\u793a\u5b66\u4e60\u5206\u79bb\u6c34\u5370\u548c\u56fe\u50cf\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cHiWL\u5728\u6c34\u5370\u63d0\u53d6\u51c6\u786e\u7387\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad87.6%\uff0c\u4e14\u5904\u7406\u901f\u5ea6\u6781\u5feb\uff088\u79d2\u5904\u740610\u4e07\u5f20\u56fe\u50cf\uff09\u3002", "conclusion": "HiWL\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6df1\u5ea6\u56fe\u50cf\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u4e0d\u53ef\u89c1\u6027\u3001\u9c81\u68d2\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u9002\u7528\u4e8e\u7248\u6743\u4fdd\u62a4\u3002"}}
{"id": "2508.08744", "pdf": "https://arxiv.org/pdf/2508.08744", "abs": "https://arxiv.org/abs/2508.08744", "authors": ["Zhonggen Li", "Xiangyu Ke", "Yifan Zhu", "Bocheng Yu", "Baihua Zheng", "Yunjun Gao"], "title": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor Search", "categories": ["cs.DB", "cs.DC"], "comment": "Accepted at SIGMOD 2026", "summary": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality.", "AI": {"tldr": "Tagore\u662f\u4e00\u4e2a\u57fa\u4e8eGPU\u52a0\u901f\u7684\u56fe\u7d22\u5f15\u5e93\uff0c\u901a\u8fc7\u9ad8\u6548\u7b97\u6cd5\u548c\u5e76\u884c\u5904\u7406\u5927\u5e45\u63d0\u5347\u9ad8\u7ef4\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u7684\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u56fe\u7d22\u5f15\u65b9\u6cd5\u5728\u9ad8\u7ef4\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u6784\u5efa\u5f00\u9500\u5927\uff0c\u4e14\u968f\u7740\u6570\u636e\u91cf\u589e\u957f\u548c\u52a8\u6001\u8c03\u6574\u9700\u6c42\u589e\u52a0\uff0c\u6548\u7387\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002", "method": "\u63d0\u51faGNN-Descent\u7b97\u6cd5\u4f18\u5316k-NN\u56fe\u521d\u59cb\u5316\uff0c\u8bbe\u8ba1\u901a\u7528\u8ba1\u7b97\u8fc7\u7a0bCFS\u652f\u6301\u591a\u79cd\u526a\u679d\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u5f02\u6b65GPU-CPU-\u78c1\u76d8\u7d22\u5f15\u6846\u67b6\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u3002", "result": "\u57287\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cTagore\u5b9e\u73b0\u4e861.32x-112.79x\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u7d22\u5f15\u8d28\u91cf\u3002", "conclusion": "Tagore\u901a\u8fc7GPU\u52a0\u901f\u548c\u5e76\u884c\u5316\u8bbe\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u56fe\u7d22\u5f15\u7684\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u52a8\u6001\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2508.08822", "pdf": "https://arxiv.org/pdf/2508.08822", "abs": "https://arxiv.org/abs/2508.08822", "authors": ["Shady Agwa", "Yihan Pan", "Georgios Papandroulidakis", "Themis Prodromakis"], "title": "OISMA: On-the-fly In-memory Stochastic Multiplication Architecture for Matrix-Multiplication Workloads", "categories": ["cs.AR", "cs.AI", "cs.ET", "cs.PF"], "comment": "12 pages, 13 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Artificial Intelligence models are currently driven by a significant\nup-scaling of their complexity, with massive matrix multiplication workloads\nrepresenting the major computational bottleneck. In-memory computing\narchitectures are proposed to avoid the Von Neumann bottleneck. However, both\ndigital/binary-based and analogue in-memory computing architectures suffer from\nvarious limitations, which significantly degrade the performance and energy\nefficiency gains. This work proposes OISMA, a novel in-memory computing\narchitecture that utilizes the computational simplicity of a quasi-stochastic\ncomputing domain (Bent-Pyramid system), while keeping the same efficiency,\nscalability, and productivity of digital memories. OISMA converts normal memory\nread operations into in-situ stochastic multiplication operations with a\nnegligible cost. An accumulation periphery then accumulates the output\nmultiplication bitstreams, achieving the matrix multiplication functionality.\nExtensive matrix multiplication benchmarking was conducted to analyze the\naccuracy of the Bent-Pyramid system, using matrix dimensions ranging from 4x4\nto 512x512. The accuracy results show a significant decrease in the average\nrelative Frobenius error, from 9.42% (for 4x4) to 1.81% (for 512x512), compared\nto 64-bit double precision floating-point format. A 1T1R OISMA array of 4 KB\ncapacity was implemented using a commercial 180nm technology node and in-house\nRRAM technology. At 50 MHz, OISMA achieves 0.891 TOPS/W and 3.98 GOPS/mm2 for\nenergy and area efficiency, respectively, occupying an effective computing area\nof 0.804241 mm2. Scaling OISMA from 180nm to 22nm technology shows a\nsignificant improvement of two orders of magnitude in energy efficiency and one\norder of magnitude in area efficiency, compared to dense matrix multiplication\nin-memory computing architectures.", "AI": {"tldr": "OISMA\u662f\u4e00\u79cd\u65b0\u578b\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\uff0c\u5229\u7528\u51c6\u968f\u673a\u8ba1\u7b97\u57df\u7684\u7b80\u5355\u6027\u5b9e\u73b0\u9ad8\u6548\u77e9\u9635\u4e58\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u80fd\u6e90\u6548\u7387\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u6027\u589e\u52a0\uff0c\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\u867d\u907f\u514d\u4e86\u51af\u00b7\u8bfa\u4f0a\u66fc\u74f6\u9888\uff0c\u4f46\u6570\u5b57/\u6a21\u62df\u67b6\u6784\u5b58\u5728\u6027\u80fd\u9650\u5236\u3002OISMA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "OISMA\u5c06\u5185\u5b58\u8bfb\u53d6\u64cd\u4f5c\u8f6c\u6362\u4e3a\u539f\u4f4d\u968f\u673a\u4e58\u6cd5\u64cd\u4f5c\uff0c\u901a\u8fc7\u79ef\u7d2f\u5916\u56f4\u5b9e\u73b0\u77e9\u9635\u4e58\u6cd5\u3002\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4ece4x4\u5230512x512\u77e9\u9635\u7684\u7cbe\u5ea6\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5e73\u5747\u76f8\u5bf9Frobenius\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff08\u4ece9.42%\u964d\u81f31.81%\uff09\u3002\u5728180nm\u6280\u672f\u4e0b\uff0c\u80fd\u6e90\u6548\u7387\u8fbe0.891TOPS/W\uff0c\u9762\u79ef\u6548\u7387\u4e3a3.98GOPS/mm\u00b2\u3002", "conclusion": "OISMA\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\uff0c\u672a\u6765\u6280\u672f\u8282\u70b9\u4e0b\u4ecd\u6709\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2508.09035", "pdf": "https://arxiv.org/pdf/2508.09035", "abs": "https://arxiv.org/abs/2508.09035", "authors": ["Yibo Jin", "Yixu Xu", "Yue Chen", "Chengbin Wang", "Tao Wang", "Jiaqi Huang", "Rongfei Zhang", "Yiming Dong", "Yuting Yan", "Ke Cheng", "Yingjie Zhu", "Shulan Wang", "Qianqian Tang", "Shuaishuai Meng", "Guanxin Cheng", "Ze Wang", "Shuyan Miao", "Ketao Wang", "Wen Liu", "Yifan Yang", "Tong Zhang", "Anran Wang", "Chengzhou Lu", "Tiantian Dong", "Yongsheng Zhang", "Zhe Wang", "Hefei Guo", "Hongjie Liu", "Wei Lu", "Zhengyong Zhang"], "title": "P/D-Device: Disaggregated Large Language Model between Cloud and Devices", "categories": ["cs.DC", "cs.CL", "cs.LG"], "comment": null, "summary": "Serving disaggregated large language models has been widely adopted in\nindustrial practice for enhanced performance. However, too many tokens\ngenerated in decoding phase, i.e., occupying the resources for a long time,\nessentially hamper the cloud from achieving a higher throughput. Meanwhile, due\nto limited on-device resources, the time to first token (TTFT), i.e., the\nlatency of prefill phase, increases dramatically with the growth on prompt\nlength. In order to concur with such a bottleneck on resources, i.e., long\noccupation in cloud and limited on-device computing capacity, we propose to\nseparate large language model between cloud and devices. That is, the cloud\nhelps a portion of the content for each device, only in its prefill phase.\nSpecifically, after receiving the first token from the cloud, decoupling with\nits own prefill, the device responds to the user immediately for a lower TTFT.\nThen, the following tokens from cloud are presented via a speed controller for\nsmoothed TPOT (the time per output token), until the device catches up with the\nprogress. On-device prefill is then amortized using received tokens while the\nresource usage in cloud is controlled. Moreover, during cloud prefill, the\nprompt can be refined, using those intermediate data already generated, to\nfurther speed up on-device inference. We implement such a scheme P/D-Device,\nand confirm its superiority over other alternatives. We further propose an\nalgorithm to decide the best settings. Real-trace experiments show that TTFT\ndecreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud\nthroughput increases by up to 15x.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5206\u5272\u5230\u4e91\u7aef\u548c\u8bbe\u5907\u7aef\u7684\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u9996\u8bcd\u5ef6\u8fdf\uff08TTFT\uff09\u5e76\u63d0\u9ad8\u4e91\u7aef\u541e\u5410\u91cf\u3002", "motivation": "\u7531\u4e8e\u89e3\u7801\u9636\u6bb5\u751f\u6210\u8fc7\u591a\u4ee4\u724c\u5360\u7528\u4e91\u7aef\u8d44\u6e90\uff0c\u4ee5\u53ca\u8bbe\u5907\u7aef\u8d44\u6e90\u6709\u9650\u5bfc\u81f4\u9884\u586b\u5145\u9636\u6bb5\u5ef6\u8fdf\u589e\u52a0\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u8d44\u6e90\u74f6\u9888\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5206\u5272\u5230\u4e91\u7aef\u548c\u8bbe\u5907\u7aef\uff0c\u4e91\u7aef\u4ec5\u534f\u52a9\u8bbe\u5907\u7aef\u7684\u9884\u586b\u5145\u9636\u6bb5\uff1b\u8bbe\u5907\u7aef\u5728\u6536\u5230\u9996\u8bcd\u540e\u7acb\u5373\u54cd\u5e94\uff0c\u4e91\u7aef\u540e\u7eed\u4ee4\u724c\u901a\u8fc7\u901f\u5ea6\u63a7\u5236\u5668\u5e73\u6ed1\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTTFT\u964d\u4f4e\u81f3\u5c1160%\uff0c\u6700\u5927TPOT\u7ea6\u4e3a\u51e0\u5341\u6beb\u79d2\uff0c\u4e91\u7aef\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe15\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684P/D-Device\u65b9\u6848\u5728\u51cf\u5c11\u5ef6\u8fdf\u548c\u63d0\u9ad8\u541e\u5410\u91cf\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u7b97\u6cd5\u4f18\u5316\u8bbe\u7f6e\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.08952", "pdf": "https://arxiv.org/pdf/2508.08952", "abs": "https://arxiv.org/abs/2508.08952", "authors": ["Hyunwoo Kim", "Jaeseong Lee", "Sunpyo Hong", "Changmin Han"], "title": "Toward Automated Hypervisor Scenario Generation Based on VM Workload Profiling for Resource-Constrained Environments", "categories": ["cs.SE"], "comment": null, "summary": "In the automotive industry, the rise of software-defined vehicles (SDVs) has\n  driven a shift toward virtualization-based architectures that consolidate\n  diverse automotive workloads on a shared hardware platform. To support this\n  evolution, chipset vendors provide board support packages (BSPs), hypervisor\n  setups, and resource allocation guidelines. However, adapting these static\n  configurations to varying system requirements and workloads remain a\n  significant challenge for Tier 1 integrators.\n  This paper presents an automated scenario generation framework, which helps\n  automotive vendors to allocate hardware resources efficiently across multiple\n  VMs. By profiling runtime behavior and integrating both theoretical models\nand\n  vendor heuristics, the proposed tool generates optimized hypervisor\n  configurations tailored to system constraints.\n  We compare two main approaches for modeling target QoS based on profiled data\n  and resource allocation: domain-guided parametric modeling and deep\n  learning-based modeling. We further describe our optimization strategy using\n  the selected QoS model to derive efficient resource allocations. Finally, we\n  report on real-world deployments to demonstrate the effectiveness of our\n  framework in improving integration efficiency and reducing development time\nin\n  resource-constrained environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u5e2e\u52a9\u6c7d\u8f66\u884c\u4e1a\u9ad8\u6548\u5206\u914d\u786c\u4ef6\u8d44\u6e90\u7ed9\u591a\u4e2a\u865a\u62df\u673a\uff0c\u4f18\u5316\u4e86\u8d44\u6e90\u6574\u5408\u548c\u5f00\u53d1\u6548\u7387\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86\uff08SDVs\uff09\u7684\u5174\u8d77\uff0c\u5982\u4f55\u52a8\u6001\u9002\u5e94\u7cfb\u7edf\u9700\u6c42\u548c\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u6210\u4e3a\u6c7d\u8f66\u884c\u4e1a\u7684\u91cd\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u8fd0\u884c\u65f6\u884c\u4e3a\u5206\u6790\u548c\u7406\u8bba\u6a21\u578b\u4e0e\u5382\u5546\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u751f\u6210\u4f18\u5316\u7684\u865a\u62df\u673a\u914d\u7f6e\u3002\u6bd4\u8f83\u4e86\u4e24\u79cdQoS\u5efa\u6a21\u65b9\u6cd5\uff1a\u9886\u57df\u5f15\u5bfc\u53c2\u6570\u5efa\u6a21\u548c\u6df1\u5ea6\u5b66\u4e60\u5efa\u6a21\u3002", "result": "\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u96c6\u6210\u6548\u7387\u548c\u51cf\u5c11\u4e86\u5f00\u53d1\u65f6\u95f4\u3002", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6c7d\u8f66\u884c\u4e1a\u4e2d\u7684\u865a\u62df\u5316\u6311\u6218\u3002"}}
{"id": "2508.08572", "pdf": "https://arxiv.org/pdf/2508.08572", "abs": "https://arxiv.org/abs/2508.08572", "authors": ["Michael Kleiss", "Seyedehaysan Mokhtarimousavi", "Sida Dai", "Mostafa Alani"], "title": "Bio-Generative Design Morphology with Radiolaria: An application of a Nature-Based Generative Shape Grammar for Geometrical Design of Space Frames", "categories": ["cs.GR"], "comment": "12 pages, SiGradi Conference", "summary": "This paper presents a study on using Radiolaria as a basis for generation of\nspace-based geometry for structural design with shape grammars. Radiolaria has\nbeen a source of inspiration for architectural design with its intricate\nstructural features and geometric patterns (Lim, 2012). We use the basis of the\nRadiolaria geometry to create a generative shape grammar as a computational\nsystem; then use the shape grammar to create spatial configurations for\npotential applications in design of 3D space structural frames. This study\nbegins with the geometric analysis of Radiolaria and the dissection of its\nstructure and geometry into a simplified morphological source, in this case a\ntetrahedral structure. Tetrahedrons are used in combination with octahedrons to\ngenerate spatial configurations to generate 3D spatial structural frames. The\npaper presents the Radiolaria spatial analysis, the shape grammar, the\ncollection of generated designs, and possible applications in space frame\nstructures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u653e\u5c04\u866b\uff08Radiolaria\uff09\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u751f\u6210\u5f62\u72b6\u8bed\u6cd5\uff0c\u7528\u4e8e\u7a7a\u95f4\u7ed3\u6784\u76843D\u8bbe\u8ba1\u3002", "motivation": "\u653e\u5c04\u866b\u56e0\u5176\u590d\u6742\u7684\u51e0\u4f55\u7ed3\u6784\u6210\u4e3a\u5efa\u7b51\u8bbe\u8ba1\u7684\u7075\u611f\u6765\u6e90\uff0c\u7814\u7a76\u65e8\u5728\u5c06\u5176\u51e0\u4f55\u7279\u6027\u8f6c\u5316\u4e3a\u5b9e\u7528\u7684\u8bbe\u8ba1\u5de5\u5177\u3002", "method": "\u5206\u6790\u653e\u5c04\u866b\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u63d0\u53d6\u7b80\u5316\u7684\u56db\u9762\u4f53\u5f62\u6001\uff0c\u5e76\u7ed3\u5408\u516b\u9762\u4f53\u751f\u6210\u7a7a\u95f4\u914d\u7f6e\uff0c\u5f00\u53d1\u751f\u6210\u5f62\u72b6\u8bed\u6cd5\u3002", "result": "\u751f\u6210\u4e86\u591a\u79cd3D\u7a7a\u95f4\u7ed3\u6784\u6846\u67b6\u8bbe\u8ba1\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u7a7a\u95f4\u6846\u67b6\u7ed3\u6784\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "conclusion": "\u653e\u5c04\u866b\u7684\u51e0\u4f55\u7ed3\u6784\u4e3a\u7a7a\u95f4\u7ed3\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u751f\u6210\u5de5\u5177\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.08524", "pdf": "https://arxiv.org/pdf/2508.08524", "abs": "https://arxiv.org/abs/2508.08524", "authors": ["Jon E. Froehlich", "Alexander Fiannaca", "Nimer Jaber", "Victor Tsara", "Shaun Kane"], "title": "StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI", "categories": ["cs.HC", "cs.AI", "H.5; I.2"], "comment": "Accepted to UIST'25", "summary": "Interactive streetscape mapping tools such as Google Street View (GSV) and\nMeta Mapillary enable users to virtually navigate and experience real-world\nenvironments via immersive 360{\\deg} imagery but remain fundamentally\ninaccessible to blind users. We introduce StreetViewAI, the first-ever\naccessible street view tool, which combines context-aware, multimodal AI,\naccessible navigation controls, and conversational speech. With StreetViewAI,\nblind users can virtually examine destinations, engage in open-world\nexploration, or virtually tour any of the over 220 billion images and 100+\ncountries where GSV is deployed. We iteratively designed StreetViewAI with a\nmixed-visual ability team and performed an evaluation with eleven blind users.\nOur findings demonstrate the value of an accessible street view in supporting\nPOI investigations and remote route planning. We close by enumerating key\nguidelines for future work.", "AI": {"tldr": "StreetViewAI \u662f\u9996\u4e2a\u4e3a\u76f2\u4eba\u8bbe\u8ba1\u7684\u65e0\u969c\u788d\u8857\u666f\u5de5\u5177\uff0c\u7ed3\u5408\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u591a\u6a21\u6001AI\u3001\u65e0\u969c\u788d\u5bfc\u822a\u63a7\u5236\u548c\u5bf9\u8bdd\u8bed\u97f3\uff0c\u652f\u6301\u76f2\u4eba\u865a\u62df\u63a2\u7d22\u76ee\u7684\u5730\u548c\u8fdc\u7a0b\u8def\u7ebf\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u7684\u8857\u666f\u5de5\u5177\u5982Google Street View\u5bf9\u76f2\u4eba\u7528\u6237\u4e0d\u53cb\u597d\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u969c\u788d\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408AI\u3001\u65e0\u969c\u788d\u5bfc\u822a\u548c\u8bed\u97f3\u4ea4\u4e92\uff0c\u901a\u8fc7\u4e0e\u6df7\u5408\u89c6\u89c9\u80fd\u529b\u7684\u56e2\u961f\u5408\u4f5c\u8fed\u4ee3\u8bbe\u8ba1\uff0c\u5e76\u5bf911\u540d\u76f2\u4eba\u7528\u6237\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "StreetViewAI \u652f\u6301\u76f2\u4eba\u8fdb\u884c\u76ee\u7684\u5730\u8c03\u67e5\u548c\u8fdc\u7a0b\u8def\u7ebf\u89c4\u5212\uff0c\u8bc1\u660e\u4e86\u5176\u4ef7\u503c\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u5de5\u4f5c\u7684\u5173\u952e\u6307\u5357\uff0c\u5f3a\u8c03\u65e0\u969c\u788d\u8857\u666f\u5de5\u5177\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.08754", "pdf": "https://arxiv.org/pdf/2508.08754", "abs": "https://arxiv.org/abs/2508.08754", "authors": ["Qianru Qiu", "Jiafeng Mao", "Xueting Wang"], "title": "Exploring Palette based Color Guidance in Diffusion Models", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Accepted to ACM MM 2025", "summary": "With the advent of diffusion models, Text-to-Image (T2I) generation has seen\nsubstantial advancements. Current T2I models allow users to specify object\ncolors using linguistic color names, and some methods aim to personalize\ncolor-object association through prompt learning. However, existing models\nstruggle to provide comprehensive control over the color schemes of an entire\nimage, especially for background elements and less prominent objects not\nexplicitly mentioned in prompts. This paper proposes a novel approach to\nenhance color scheme control by integrating color palettes as a separate\nguidance mechanism alongside prompt instructions. We investigate the\neffectiveness of palette guidance by exploring various palette representation\nmethods within a diffusion-based image colorization framework. To facilitate\nthis exploration, we construct specialized palette-text-image datasets and\nconduct extensive quantitative and qualitative analyses. Our results\ndemonstrate that incorporating palette guidance significantly improves the\nmodel's ability to generate images with desired color schemes, enabling a more\ncontrolled and refined colorization process.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u6574\u5408\u8c03\u8272\u677f\u4f5c\u4e3a\u989d\u5916\u5f15\u5bfc\u673a\u5236\u6765\u589e\u5f3a\u56fe\u50cf\u8272\u5f69\u65b9\u6848\u63a7\u5236\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u63a7\u5236\u6574\u4f53\u56fe\u50cf\u8272\u5f69\u65b9\u6848\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5bf9\u80cc\u666f\u548c\u975e\u663e\u773c\u5bf9\u8c61\u7684\u989c\u8272\u63a7\u5236\u8f83\u5dee\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u8c03\u8272\u677f\u548c\u63d0\u793a\u6307\u4ee4\u4f5c\u4e3a\u53cc\u91cd\u5f15\u5bfc\u673a\u5236\uff0c\u7814\u7a76\u4e0d\u540c\u8c03\u8272\u677f\u8868\u793a\u65b9\u6cd5\u5728\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002\u6784\u5efa\u4e13\u95e8\u7684\u8c03\u8272\u677f-\u6587\u672c-\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u5b9a\u91cf\u4e0e\u5b9a\u6027\u5206\u6790\u3002", "result": "\u8c03\u8272\u677f\u5f15\u5bfc\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u751f\u6210\u7b26\u5408\u671f\u671b\u8272\u5f69\u65b9\u6848\u7684\u56fe\u50cf\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u7ec6\u7684\u8272\u5f69\u63a7\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8272\u5f69\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.08959", "pdf": "https://arxiv.org/pdf/2508.08959", "abs": "https://arxiv.org/abs/2508.08959", "authors": ["Lars Vogt", "Birgitta K\u00f6nig-Ries", "Tim Alamenciak", "Joshua I. Brian", "Carlos Alberto Arnillas", "Lotte Korell", "Robert Fr\u00fchst\u00fcckl", "Tina Heger"], "title": "A Framework for FAIR and CLEAR Ecological Data and Knowledge: Semantic Units for Synthesis and Causal Modelling", "categories": ["cs.DB"], "comment": null, "summary": "Ecological research increasingly relies on integrating heterogeneous datasets\nand knowledge to explain and predict complex phenomena. Yet, differences in\ndata types, terminology, and documentation often hinder interoperability,\nreuse, and causal understanding. We present the Semantic Units Framework, a\nnovel, domain-agnostic semantic modelling approach applied here to ecological\ndata and knowledge in compliance with the FAIR (Findable, Accessible,\nInteroperable, Reusable) and CLEAR (Cognitively interoperable, semantically\nLinked, contextually Explorable, easily Accessible, human-Readable and\n-interpretable) Principles. The framework models data and knowledge as modular,\nlogic-aware semantic units: single propositions (statement units) or coherent\ngroups of propositions (compound units). Statement units can model\nmeasurements, observations, or universal relationships, including causal ones,\nand link to methods and evidence. Compound units group related statement units\ninto reusable, semantically coherent knowledge objects. Implemented using RDF,\nOWL, and knowledge graphs, semantic units can be serialized as FAIR Digital\nObjects with persistent identifiers, provenance, and semantic interoperability.\nWe show how universal statement units build ecological causal networks, which\ncan be composed into causal maps and perspective-specific subnetworks. These\nsupport causal reasoning, confounder detection (back-door), effect\nidentification with unobserved confounders (front-door), application of\ndo-calculus, and alignment with Bayesian networks, structural equation models,\nand structural causal models. By linking fine-grained empirical data to\nhigh-level causal reasoning, the Semantic Units Framework provides a foundation\nfor ecological knowledge synthesis, evidence annotation, cross-domain\nintegration, reproducible workflows, and AI-ready ecological research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u4e49\u5355\u5143\u6846\u67b6\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u751f\u6001\u6570\u636e\u548c\u77e5\u8bc6\u6574\u5408\u4e2d\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u56e0\u679c\u7406\u89e3\u95ee\u9898\uff0c\u7b26\u5408FAIR\u548cCLEAR\u539f\u5219\u3002", "motivation": "\u751f\u6001\u7814\u7a76\u9700\u8981\u6574\u5408\u5f02\u6784\u6570\u636e\u548c\u77e5\u8bc6\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u7c7b\u578b\u3001\u672f\u8bed\u548c\u6587\u6863\u7684\u5dee\u5f02\uff0c\u4e92\u64cd\u4f5c\u6027\u548c\u56e0\u679c\u7406\u89e3\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u6a21\u5757\u5316\u3001\u903b\u8f91\u611f\u77e5\u7684\u8bed\u4e49\u5355\u5143\uff08\u58f0\u660e\u5355\u5143\u548c\u590d\u5408\u5355\u5143\uff09\u5efa\u6a21\u6570\u636e\u548c\u77e5\u8bc6\uff0c\u4f7f\u7528RDF\u3001OWL\u548c\u77e5\u8bc6\u56fe\u5b9e\u73b0\uff0c\u5e76\u5e8f\u5217\u5316\u4e3aFAIR\u6570\u5b57\u5bf9\u8c61\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u6784\u5efa\u751f\u6001\u56e0\u679c\u7f51\u7edc\uff0c\u652f\u6301\u56e0\u679c\u63a8\u7406\u3001\u6df7\u6742\u56e0\u7d20\u68c0\u6d4b\u548c\u6548\u5e94\u8bc6\u522b\uff0c\u5e76\u4e0e\u5176\u4ed6\u6a21\u578b\u5bf9\u9f50\u3002", "conclusion": "\u8bed\u4e49\u5355\u5143\u6846\u67b6\u4e3a\u751f\u6001\u77e5\u8bc6\u5408\u6210\u3001\u8de8\u9886\u57df\u6574\u5408\u548cAI\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.09101", "pdf": "https://arxiv.org/pdf/2508.09101", "abs": "https://arxiv.org/abs/2508.09101", "authors": ["Jason Chou", "Ao Liu", "Yuchi Deng", "Zhiying Zeng", "Tao Zhang", "Haotian Zhu", "Jianwei Cai", "Yue Mao", "Chenchen Zhang", "Lingyun Tan", "Ziyan Xu", "Bohui Zhai", "Hengyi Liu", "Speed Zhu", "Wiggin Zhou", "Fengzong Lian"], "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators", "categories": ["cs.CL", "cs.SE"], "comment": "Homepage: https://autocodebench.github.io/", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios.", "AI": {"tldr": "AutoCodeGen\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u751f\u6210\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d1\u5e03\u4e86AutoCodeBench\u7cfb\u5217\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u590d\u6742\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u4e14\u5c40\u9650\u4e8ePython\uff0c\u96be\u4ee5\u6269\u5c55\u548c\u8986\u76d6\u591a\u8bed\u8a00\u53ca\u9ad8\u96be\u5ea6\u4efb\u52a1\u3002", "method": "\u901a\u8fc7LLM\u751f\u6210\u6d4b\u8bd5\u8f93\u5165\uff0c\u5229\u7528\u591a\u8bed\u8a00\u6c99\u7bb1\u83b7\u53d6\u8f93\u51fa\uff0c\u7ed3\u5408\u9006\u5e8f\u95ee\u9898\u751f\u6210\u548c\u591a\u6b65\u8fc7\u6ee4\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "AutoCodeBench\u5305\u542b3920\u4e2a\u95ee\u9898\uff0c\u8986\u76d620\u79cd\u8bed\u8a00\uff1b\u8bc4\u4f30\u663e\u793a\u9ad8\u7ea7LLMs\u5728\u590d\u6742\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "AutoCodeBench\u7cfb\u5217\u4e3a\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u63a8\u52a8\u793e\u533a\u5173\u6ce8\u66f4\u5177\u6311\u6218\u6027\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2508.09085", "pdf": "https://arxiv.org/pdf/2508.09085", "abs": "https://arxiv.org/abs/2508.09085", "authors": ["Zihan Fang", "Zheng Lin", "Senkang Hu", "Yihang Tao", "Yiqin Deng", "Xianhao Chen", "Yuguang Fang"], "title": "Dynamic Uncertainty-aware Multimodal Fusion for Outdoor Health Monitoring", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "14 pages, 10 figures", "summary": "Outdoor health monitoring is essential to detect early abnormal health status\nfor safeguarding human health and safety. Conventional outdoor monitoring\nrelies on static multimodal deep learning frameworks, which requires extensive\ndata training from scratch and fails to capture subtle health status changes.\nMultimodal large language models (MLLMs) emerge as a promising alternative,\nutilizing only small datasets to fine-tune pre-trained information-rich models\nfor enabling powerful health status monitoring. Unfortunately, MLLM-based\noutdoor health monitoring also faces significant challenges: I) sensor data\ncontains input noise stemming from sensor data acquisition and fluctuation\nnoise caused by sudden changes in physiological signals due to dynamic outdoor\nenvironments, thus degrading the training performance; ii) current transformer\nbased MLLMs struggle to achieve robust multimodal fusion, as they lack a design\nfor fusing the noisy modality; iii) modalities with varying noise levels hinder\naccurate recovery of missing data from fluctuating distributions. To combat\nthese challenges, we propose an uncertainty-aware multimodal fusion framework,\nnamed DUAL-Health, for outdoor health monitoring in dynamic and noisy\nenvironments. First, to assess the impact of noise, we accurately quantify\nmodality uncertainty caused by input and fluctuation noise with current and\ntemporal features. Second, to empower efficient muitimodal fusion with\nlow-quality modalities,we customize the fusion weight for each modality based\non quantified and calibrated uncertainty. Third, to enhance data recovery from\nfluctuating noisy modalities, we align modality distributions within a common\nsemantic space. Extensive experiments demonstrate that our DUAL-Health\noutperforms state-of-the-art baselines in detection accuracy and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDUAL-Health\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u548c\u5608\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u6237\u5916\u5065\u5eb7\u76d1\u6d4b\uff0c\u901a\u8fc7\u91cf\u5316\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6a21\u6001\u878d\u5408\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6237\u5916\u5065\u5eb7\u76d1\u6d4b\u5bf9\u4eba\u7c7b\u5065\u5eb7\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u566a\u58f0\u6570\u636e\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDUAL-Health\u6846\u67b6\uff1a1)\u91cf\u5316\u7531\u8f93\u5165\u566a\u58f0\u548c\u6ce2\u52a8\u566a\u58f0\u5f15\u8d77\u7684\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\uff1b2)\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5b9a\u5236\u591a\u6a21\u6001\u878d\u5408\u6743\u91cd\uff1b3)\u5728\u5171\u540c\u8bed\u4e49\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6a21\u6001\u5206\u5e03\u4ee5\u589e\u5f3a\u6570\u636e\u6062\u590d\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDUAL-Health\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DUAL-Health\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u591a\u6a21\u6001\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6237\u5916\u5065\u5eb7\u76d1\u6d4b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u5608\u6742\u73af\u5883\u3002"}}
{"id": "2508.08554", "pdf": "https://arxiv.org/pdf/2508.08554", "abs": "https://arxiv.org/abs/2508.08554", "authors": ["Sanchita S. Kamath", "Aziz N. Zeidieh", "JooYoung Seo"], "title": "Explore, Listen, Inspect: Supporting Multimodal Interaction with 3D Surface and Point Data Visualizations", "categories": ["cs.HC"], "comment": null, "summary": "Blind and low-vision (BLV) users remain largely excluded from\nthree-dimensional (3D) surface and point data visualizations due to the\nreliance on visual interaction. Existing approaches inadequately support\nnon-visual access, especially in browser-based environments. This study\nintroduces DIXTRAL, a hosted web-native system, co-designed with BLV\nresearchers to address these gaps through multimodal interaction. Conducted\nwith two blind and one sighted researcher, this study took place over sustained\ndesign sessions. Data were gathered through iterative testing of the prototype,\ncollecting feedback on spatial navigation, sonification, and usability.\nCo-design observations demonstrate that synchronized auditory, visual, and\ntextual feedback, combined with keyboard and gamepad navigation, enhances both\nstructure discovery and orientation. DIXTRAL aims to improve access to 3D\ncontinuous scalar fields for BLV users and inform best practices for creating\ninclusive 3D visualizations.", "AI": {"tldr": "DIXTRAL \u662f\u4e00\u4e2a\u4e3a\u76f2\u4eba\u548c\u89c6\u529b\u4f4e\u4e0b\u7528\u6237\u8bbe\u8ba1\u7684\u57fa\u4e8e\u6d4f\u89c8\u5668\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u542c\u89c9\u3001\u89c6\u89c9\u548c\u952e\u76d8\u5bfc\u822a\u7b49\u53cd\u9988\u65b9\u5f0f\u63d0\u5347\u4ed6\u4eec\u5bf93D\u6570\u636e\u7684\u8bbf\u95ee\u80fd\u529b\u3002", "motivation": "\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u5728\u8bbf\u95ee3D\u53ef\u89c6\u5316\u6570\u636e\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u975e\u89c6\u89c9\u4ea4\u4e92\u65b9\u9762\u652f\u6301\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u3002", "method": "\u901a\u8fc7\u4e0e\u5408\u4f5c\u8bbe\u8ba1\u7684BLV\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u6301\u7eed\u8bbe\u8ba1\u4f1a\u8bdd\uff0c\u6d4b\u8bd5\u539f\u578b\u5e76\u6536\u96c6\u53cd\u9988\uff0c\u91cd\u70b9\u5173\u6ce8\u7a7a\u95f4\u5bfc\u822a\u3001\u542c\u89c9\u53cd\u9988\u548c\u53ef\u7528\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u540c\u6b65\u7684\u591a\u6a21\u6001\u53cd\u9988\uff08\u542c\u89c9\u3001\u89c6\u89c9\u548c\u6587\u672c\uff09\u4ee5\u53ca\u952e\u76d8\u548c\u6e38\u620f\u624b\u67c4\u5bfc\u822a\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u7ed3\u6784\u53d1\u73b0\u548c\u7a7a\u95f4\u5b9a\u5411\u80fd\u529b\u3002", "conclusion": "DIXTRAL \u4e3a\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u597d\u76843D\u6570\u636e\u8bbf\u95ee\u65b9\u5f0f\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u5305\u5bb9\u60273D\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5357\u3002"}}
{"id": "2508.08854", "pdf": "https://arxiv.org/pdf/2508.08854", "abs": "https://arxiv.org/abs/2508.08854", "authors": ["Yingxue Pang", "Shijie Zhao", "Haiqiang Wang", "Gen Zhan", "Junlin Li", "Li Zhang"], "title": "Frequency-Assisted Adaptive Sharpening Scheme Considering Bitrate and Quality Tradeoff", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "Sharpening is a widely adopted technique to improve video quality, which can\neffectively emphasize textures and alleviate blurring. However, increasing the\nsharpening level comes with a higher video bitrate, resulting in degraded\nQuality of Service (QoS). Furthermore, the video quality does not necessarily\nimprove with increasing sharpening levels, leading to issues such as\nover-sharpening. Clearly, it is essential to figure out how to boost video\nquality with a proper sharpening level while also controlling bandwidth costs\neffectively. This paper thus proposes a novel Frequency-assisted Sharpening\nlevel Prediction model (FreqSP). We first label each video with the sharpening\nlevel correlating to the optimal bitrate and quality tradeoff as ground truth.\nThen taking uncompressed source videos as inputs, the proposed FreqSP leverages\nintricate CNN features and high-frequency components to estimate the optimal\nsharpening level. Extensive experiments demonstrate the effectiveness of our\nmethod.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u7387\u8f85\u52a9\u7684\u9510\u5316\u7ea7\u522b\u9884\u6d4b\u6a21\u578b\uff08FreqSP\uff09\uff0c\u901a\u8fc7\u7ed3\u5408CNN\u7279\u5f81\u548c\u9ad8\u9891\u5206\u91cf\uff0c\u4ee5\u4f18\u5316\u89c6\u9891\u8d28\u91cf\u4e0e\u5e26\u5bbd\u6210\u672c\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u9510\u5316\u6280\u672f\u867d\u7136\u80fd\u63d0\u5347\u89c6\u9891\u8d28\u91cf\uff0c\u4f46\u8fc7\u5ea6\u9510\u5316\u4f1a\u5bfc\u81f4\u89c6\u9891\u6bd4\u7279\u7387\u589e\u52a0\u548c\u670d\u52a1\u8d28\u91cf\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u627e\u5230\u9510\u5316\u7ea7\u522b\u4e0e\u6bd4\u7279\u7387\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u70b9\u3002", "method": "\u8bba\u6587\u9996\u5148\u4e3a\u6bcf\u4e2a\u89c6\u9891\u6807\u6ce8\u4e0e\u6700\u4f18\u6bd4\u7279\u7387\u548c\u8d28\u91cf\u6743\u8861\u76f8\u5bf9\u5e94\u7684\u9510\u5316\u7ea7\u522b\u4f5c\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u7136\u540e\u5229\u7528CNN\u7279\u5f81\u548c\u9ad8\u9891\u5206\u91cf\u9884\u6d4b\u6700\u4f18\u9510\u5316\u7ea7\u522b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFreqSP\u6a21\u578b\u80fd\u6709\u6548\u63d0\u9ad8\u89c6\u9891\u8d28\u91cf\u5e76\u63a7\u5236\u5e26\u5bbd\u6210\u672c\u3002", "conclusion": "FreqSP\u6a21\u578b\u4e3a\u89c6\u9891\u9510\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u5316\u4e86\u8d28\u91cf\u4e0e\u5e26\u5bbd\u7684\u6743\u8861\u3002"}}
{"id": "2508.09023", "pdf": "https://arxiv.org/pdf/2508.09023", "abs": "https://arxiv.org/abs/2508.09023", "authors": ["Dongjie Xu", "Yue Cui", "Weijie Shi", "Qingzhi Ma", "Hanghui Guo", "Jiaming Li", "Yao Zhao", "Ruiyuan Zhang", "Shimin Di", "Jia Zhu", "Kai Zheng", "Jiajie Xu"], "title": "E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and Efficiency", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "SQL query rewriting aims to reformulate a query into a more efficient form\nwhile preserving equivalence. Most existing methods rely on predefined rewrite\nrules. However, such rule-based approaches face fundamental limitations: (1)\nfixed rule sets generalize poorly to novel query patterns and struggle with\ncomplex queries; (2) a wide range of effective rewriting strategies cannot be\nfully captured by declarative rules. To overcome these issues, we propose using\nlarge language models (LLMs) to generate rewrites. LLMs can capture complex\nstrategies, such as evaluation reordering and CTE rewriting. Despite this\npotential, directly applying LLMs often results in suboptimal or non-equivalent\nrewrites due to a lack of execution awareness and semantic grounding. To\naddress these challenges, We present E3-Rewrite, an LLM-based SQL rewriting\nframework that produces executable, equivalent, and efficient queries. It\nintegrates two core components: a context construction module and a\nreinforcement learning framework. First, the context module leverages execution\nplans and retrieved demonstrations to build bottleneck-aware prompts that guide\ninference-time rewriting. Second, we design a reward function targeting\nexecutability, equivalence, and efficiency, evaluated via syntax checks,\nequivalence verification, and cost estimation. Third, to ensure stable\nmulti-objective learning, we adopt a staged curriculum that first emphasizes\nexecutability and equivalence, then gradually incorporates efficiency.\nExtensive experiments show that E3-Rewrite achieves up to a 25.6\\% reduction in\nquery execution time compared to state-of-the-art methods across multiple SQL\nbenchmarks. Moreover, it delivers up to 24.4\\% more successful rewrites,\nexpanding coverage to complex queries that previous systems failed to handle.", "AI": {"tldr": "E3-Rewrite \u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684SQL\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6267\u884c\u8ba1\u5212\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u751f\u6210\u9ad8\u6548\u4e14\u8bed\u4e49\u7b49\u4ef7\u7684\u67e5\u8be2\u91cd\u5199\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5199\u6210\u529f\u7387\u548c\u67e5\u8be2\u6267\u884c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684SQL\u67e5\u8be2\u91cd\u5199\u65b9\u6cd5\u5728\u9762\u5bf9\u590d\u6742\u67e5\u8be2\u6216\u65b0\u67e5\u8be2\u6a21\u5f0f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u96be\u4ee5\u8986\u76d6\u6240\u6709\u9ad8\u6548\u91cd\u5199\u7b56\u7565\u3002LLM\u867d\u7136\u80fd\u6355\u6349\u590d\u6742\u7b56\u7565\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u6613\u751f\u6210\u6b21\u4f18\u6216\u4e0d\u6b63\u786e\u7684\u91cd\u5199\u3002", "method": "E3-Rewrite \u7ed3\u5408\u4e0a\u4e0b\u6587\u6784\u9020\u6a21\u5757\uff08\u5229\u7528\u6267\u884c\u8ba1\u5212\u548c\u793a\u4f8b\u5f15\u5bfc\u91cd\u5199\uff09\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08\u901a\u8fc7\u5956\u52b1\u51fd\u6570\u548c\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u4f18\u5316\u91cd\u5199\u8d28\u91cf\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cE3-Rewrite \u5e73\u5747\u51cf\u5c1125.6%\u7684\u67e5\u8be2\u6267\u884c\u65f6\u95f4\uff0c\u5e76\u63d0\u534724.4%\u7684\u6210\u529f\u91cd\u5199\u7387\uff0c\u5c24\u5176\u64c5\u957f\u5904\u7406\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u7684\u590d\u6742\u67e5\u8be2\u3002", "conclusion": "E3-Rewrite \u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7LLM\u4e0e\u6267\u884c\u611f\u77e5\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7b49\u4ef7\u4e14\u53ef\u6267\u884c\u7684SQL\u67e5\u8be2\u91cd\u5199\u3002"}}
{"id": "2508.08433", "pdf": "https://arxiv.org/pdf/2508.08433", "abs": "https://arxiv.org/abs/2508.08433", "authors": ["Mingzhe Li", "Hamish Carr", "Oliver R\u00fcbel", "Bei Wang", "Gunther H. Weber"], "title": "Extremely Scalable Distributed Computation of Contour Trees via Pre-Simplification", "categories": ["cs.CG", "cs.DC", "cs.DS"], "comment": "To be published at the 15th IEEE Workshop on Large Scale Data\n  Analysis and Visualization (LDAV) 2025", "summary": "Contour trees offer an abstract representation of the level set topology in\nscalar fields and are widely used in topological data analysis and\nvisualization. However, applying contour trees to large-scale scientific\ndatasets remains challenging due to scalability limitations. Recent\ndevelopments in distributed hierarchical contour trees have addressed these\nchallenges by enabling scalable computation across distributed systems.\nBuilding on these structures, advanced analytical tasks -- such as volumetric\nbranch decomposition and contour extraction -- have been introduced to\nfacilitate large-scale scientific analysis. Despite these advancements, such\nanalytical tasks substantially increase memory usage, which hampers\nscalability. In this paper, we propose a pre-simplification strategy to\nsignificantly reduce the memory overhead associated with analytical tasks on\ndistributed hierarchical contour trees. We demonstrate enhanced scalability\nthrough strong scaling experiments, constructing the largest known contour tree\n-- comprising over half a trillion nodes with complex topology -- in under 15\nminutes on a dataset containing 550 billion elements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9884\u7b80\u5316\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u5206\u5e03\u5f0f\u5c42\u6b21\u8f6e\u5ed3\u6811\u5206\u6790\u4efb\u52a1\u7684\u5185\u5b58\u5f00\u9500\uff0c\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u5c42\u6b21\u8f6e\u5ed3\u6811\u5728\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u5206\u6790\u4e2d\u5185\u5b58\u5360\u7528\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9884\u7b80\u5316\u7b56\u7565\u4f18\u5316\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u572815\u5206\u949f\u5185\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc75000\u4ebf\u8282\u70b9\u7684\u6700\u5927\u8f6e\u5ed3\u6811\u3002", "conclusion": "\u9884\u7b80\u5316\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u8f6e\u5ed3\u6811\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.09126", "pdf": "https://arxiv.org/pdf/2508.09126", "abs": "https://arxiv.org/abs/2508.09126", "authors": ["Christopher Mitcheltree", "Bogdan Teleaga", "Andrew Fyfe", "Naotake Masuda", "Matthias Sch\u00e4fer", "Alfie Bradic", "Nao Tokui"], "title": "Neutone SDK: An Open Source Framework for Neural Audio Processing", "categories": ["cs.SD", "cs.SE", "eess.AS"], "comment": "Accepted to AES International Conference on Artificial Intelligence\n  and Machine Learning for Audio 2025", "summary": "Neural audio processing has unlocked novel methods of sound transformation\nand synthesis, yet integrating deep learning models into digital audio\nworkstations (DAWs) remains challenging due to real-time / neural network\ninference constraints and the complexities of plugin development. In this\npaper, we introduce the Neutone SDK: an open source framework that streamlines\nthe deployment of PyTorch-based neural audio models for both real-time and\noffline applications. By encapsulating common challenges such as variable\nbuffer sizes, sample rate conversion, delay compensation, and control parameter\nhandling within a unified, model-agnostic interface, our framework enables\nseamless interoperability between neural models and host plugins while allowing\nusers to work entirely in Python. We provide a technical overview of the\ninterfaces needed to accomplish this, as well as the corresponding SDK\nimplementations. We also demonstrate the SDK's versatility across applications\nsuch as audio effect emulation, timbre transfer, and sample generation, as well\nas its adoption by researchers, educators, companies, and artists alike. The\nNeutone SDK is available at https://github.com/Neutone/neutone_sdk", "AI": {"tldr": "Neutone SDK\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u57fa\u4e8ePyTorch\u7684\u795e\u7ecf\u97f3\u9891\u6a21\u578b\u5728\u5b9e\u65f6\u548c\u79bb\u7ebf\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\uff0c\u89e3\u51b3\u4e86DAW\u96c6\u6210\u4e2d\u7684\u5e38\u89c1\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u5c06\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96c6\u6210\u5230\u6570\u5b57\u97f3\u9891\u5de5\u4f5c\u7ad9\uff08DAW\uff09\u4e2d\u9762\u4e34\u5b9e\u65f6\u6027\u548c\u63d2\u4ef6\u5f00\u53d1\u7684\u590d\u6742\u6027\uff0cNeutone SDK\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u63a5\u53e3\u5c01\u88c5\u7f13\u51b2\u533a\u5927\u5c0f\u3001\u91c7\u6837\u7387\u8f6c\u6362\u548c\u5ef6\u8fdf\u8865\u507f\u7b49\u5e38\u89c1\u95ee\u9898\uff0c\u652f\u6301Python\u5f00\u53d1\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684SDK\u5b9e\u73b0\u6280\u672f\u6982\u89c8\u3002", "result": "SDK\u5c55\u793a\u4e86\u5728\u97f3\u9891\u6548\u679c\u6a21\u62df\u3001\u97f3\u8272\u8f6c\u6362\u548c\u6837\u672c\u751f\u6210\u7b49\u591a\u65b9\u9762\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u5df2\u88ab\u7814\u7a76\u4eba\u5458\u548c\u827a\u672f\u5bb6\u5e7f\u6cdb\u91c7\u7528\u3002", "conclusion": "Neutone SDK\u4e3a\u795e\u7ecf\u97f3\u9891\u6a21\u578b\u7684DAW\u96c6\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u97f3\u9891\u5904\u7406\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.08826", "pdf": "https://arxiv.org/pdf/2508.08826", "abs": "https://arxiv.org/abs/2508.08826", "authors": ["Meng Gai", "Guoping Wang", "Sheng Li"], "title": "Geometry-Aware Global Feature Aggregation for Real-Time Indirect Illumination", "categories": ["cs.GR", "cs.AI"], "comment": "10 pages", "summary": "Real-time rendering with global illumination is crucial to afford the user\nrealistic experience in virtual environments. We present a learning-based\nestimator to predict diffuse indirect illumination in screen space, which then\nis combined with direct illumination to synthesize globally-illuminated high\ndynamic range (HDR) results. Our approach tackles the challenges of capturing\nlong-range/long-distance indirect illumination when employing neural networks\nand is generalized to handle complex lighting and scenarios.\n  From the neural network thinking of the solver to the rendering equation, we\npresent a novel network architecture to predict indirect illumination. Our\nnetwork is equipped with a modified attention mechanism that aggregates global\ninformation guided by spacial geometry features, as well as a monochromatic\ndesign that encodes each color channel individually.\n  We conducted extensive evaluations, and the experimental results demonstrate\nour superiority over previous learning-based techniques. Our approach excels at\nhandling complex lighting such as varying-colored lighting and environment\nlighting. It can successfully capture distant indirect illumination and\nsimulates the interreflections between textured surfaces well (i.e., color\nbleeding effects); it can also effectively handle new scenes that are not\npresent in the training dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u5c4f\u5e55\u7a7a\u95f4\u95f4\u63a5\u5149\u7167\u9884\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u76f4\u63a5\u5149\u7167\u5b9e\u73b0\u5b9e\u65f6\u5168\u5c40\u5149\u7167\u6e32\u67d3\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u5355\u8272\u8bbe\u8ba1\u89e3\u51b3\u957f\u8ddd\u79bb\u95f4\u63a5\u5149\u7167\u5efa\u6a21\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u865a\u62df\u73af\u5883\u4e2d\u7528\u6237\u7684\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u5168\u5c40\u5149\u7167\u6548\u679c\uff0c\u540c\u65f6\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u6355\u6349\u957f\u8ddd\u79bb\u95f4\u63a5\u5149\u7167\u53ca\u590d\u6742\u573a\u666f\u65f6\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u5355\u8272\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7a7a\u95f4\u51e0\u4f55\u7279\u5f81\u5f15\u5bfc\u5168\u5c40\u4fe1\u606f\u805a\u5408\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u6280\u672f\uff0c\u80fd\u6709\u6548\u5904\u7406\u591a\u8272\u5149\u7167\u548c\u73af\u5883\u5149\u7167\uff0c\u6355\u6349\u8fdc\u8ddd\u79bb\u95f4\u63a5\u5149\u7167\u548c\u8868\u9762\u4ea4\u4e92\u53cd\u5c04\uff08\u5982\u989c\u8272\u6e17\u900f\u6548\u679c\uff09\uff0c\u5e76\u6cdb\u5316\u5230\u8bad\u7ec3\u96c6\u5916\u7684\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u7f51\u7edc\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u8d28\u91cf\u7684\u5168\u5c40\u5149\u7167\u6e32\u67d3\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u548c\u5149\u7167\u6761\u4ef6\u3002"}}
{"id": "2508.08281", "pdf": "https://arxiv.org/pdf/2508.08281", "abs": "https://arxiv.org/abs/2508.08281", "authors": ["Ningning Fu", "Shengheng Liu", "Weiliang Xie", "Yongming Huang"], "title": "Multi-grained spatial-temporal feature complementarity for accurate online cellular traffic prediction", "categories": ["cs.LG", "cs.AI", "cs.NI"], "comment": "To appear in ACM TKDD. 26 pages, 12 figures,", "summary": "Knowledge discovered from telecom data can facilitate proactive understanding\nof network dynamics and user behaviors, which in turn empowers service\nproviders to optimize cellular traffic scheduling and resource allocation.\nNevertheless, the telecom industry still heavily relies on manual expert\nintervention. Existing studies have been focused on exhaustively explore the\nspatial-temporal correlations. However, they often overlook the underlying\ncharacteristics of cellular traffic, which are shaped by the sporadic and\nbursty nature of telecom services. Additionally, concept drift creates\nsubstantial obstacles to maintaining satisfactory accuracy in continuous\ncellular forecasting tasks. To resolve these problems, we put forward an online\ncellular traffic prediction method grounded in Multi-Grained Spatial-Temporal\nfeature Complementarity (MGSTC). The proposed method is devised to achieve\nhigh-precision predictions in practical continuous forecasting scenarios.\nConcretely, MGSTC segments historical data into chunks and employs the\ncoarse-grained temporal attention to offer a trend reference for the prediction\nhorizon. Subsequently, fine-grained spatial attention is utilized to capture\ndetailed correlations among network elements, which enables localized\nrefinement of the established trend. The complementarity of these multi-grained\nspatial-temporal features facilitates the efficient transmission of valuable\ninformation. To accommodate continuous forecasting needs, we implement an\nonline learning strategy that can detect concept drift in real-time and\npromptly switch to the appropriate parameter update stage. Experiments carried\nout on four real-world datasets demonstrate that MGSTC outperforms eleven\nstate-of-the-art baselines consistently.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7c92\u5ea6\u65f6\u7a7a\u7279\u5f81\u4e92\u8865\uff08MGSTC\uff09\u7684\u5728\u7ebf\u8702\u7a9d\u6d41\u91cf\u9884\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u5ffd\u89c6\u7684\u8702\u7a9d\u6d41\u91cf\u7279\u6027\u548c\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u7535\u4fe1\u6570\u636e\u7684\u5206\u6790\u6709\u52a9\u4e8e\u52a8\u6001\u7406\u89e3\u548c\u4f18\u5316\u7f51\u7edc\u8d44\u6e90\uff0c\u4f46\u76ee\u524d\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u8702\u7a9d\u6d41\u91cf\u7684\u7a81\u53d1\u6027\u548c\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\u3002", "method": "MGSTC\u901a\u8fc7\u7c97\u7c92\u5ea6\u65f6\u95f4\u6ce8\u610f\u529b\u548c\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u6ce8\u610f\u529b\u6355\u6349\u591a\u7c92\u5ea6\u65f6\u7a7a\u7279\u5f81\u4e92\u8865\uff0c\u5e76\u7ed3\u5408\u5728\u7ebf\u5b66\u4e60\u7b56\u7565\u5b9e\u65f6\u68c0\u6d4b\u548c\u9002\u5e94\u6982\u5ff5\u6f02\u79fb\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMGSTC\u5728\u8fde\u7eed\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e11\u79cd\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MGSTC\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8702\u7a9d\u6d41\u91cf\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u8fde\u7eed\u9884\u6d4b\u573a\u666f\u3002"}}
{"id": "2508.08582", "pdf": "https://arxiv.org/pdf/2508.08582", "abs": "https://arxiv.org/abs/2508.08582", "authors": ["Ruolin wang", "Xingyu Liu", "Biao Wang", "Wayne Zhang", "Ziqian Liao", "Ziwen Li", "Amy Pavel", "Xiang 'Anthony' Chen"], "title": "CoSight: Exploring Viewer Contributions to Online Video Accessibility Through Descriptive Commenting", "categories": ["cs.HC", "K.4.2"], "comment": "ACM UIST", "summary": "The rapid growth of online video content has outpaced efforts to make visual\ninformation accessible to blind and low vision (BLV) audiences. While\nprofessional Audio Description (AD) remains the gold standard, it is costly and\ndifficult to scale across the vast volume of online media. In this work, we\nexplore a complementary approach to broaden participation in video\naccessibility: engaging everyday video viewers at their watching and commenting\ntime. We introduce CoSight, a Chrome extension that augments YouTube with\nlightweight, in-situ nudges to support descriptive commenting. Drawing from\nFogg's Behavior Model, CoSight provides visual indicators of accessibility\ngaps, pop-up hints for what to describe, reminders to clarify vague comments,\nand related captions and comments as references. In an exploratory study with\n48 sighted users, CoSight helped integrate accessibility contribution into\nnatural viewing and commenting practices, resulting in 89% of comments\nincluding grounded visual descriptions. Follow-up interviews with four BLV\nviewers and four professional AD writers suggest that while such comments do\nnot match the rigor of professional AD, they can offer complementary value by\nconveying visual context and emotional nuance for understanding the videos.", "AI": {"tldr": "CoSight\u662f\u4e00\u6b3eChrome\u6269\u5c55\uff0c\u901a\u8fc7\u8f7b\u91cf\u5316\u7684\u63d0\u793a\u5e2e\u52a9\u666e\u901a\u89c2\u4f17\u5728\u89c2\u770bYouTube\u89c6\u9891\u65f6\u751f\u6210\u63cf\u8ff0\u6027\u8bc4\u8bba\uff0c\u4ece\u800c\u63d0\u5347\u89c6\u9891\u5bf9\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u5728\u7ebf\u89c6\u9891\u5185\u5bb9\u5feb\u901f\u589e\u957f\uff0c\u4f46\u4f20\u7edf\u7684\u4e13\u4e1a\u97f3\u9891\u63cf\u8ff0\uff08AD\uff09\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\u3002CoSight\u65e8\u5728\u901a\u8fc7\u666e\u901a\u89c2\u4f17\u7684\u53c2\u4e0e\u8865\u5145AD\u7684\u4e0d\u8db3\u3002", "method": "CoSight\u5229\u7528Fogg\u7684\u884c\u4e3a\u6a21\u578b\uff0c\u5728YouTube\u4e2d\u63d0\u4f9b\u89c6\u89c9\u63d0\u793a\u3001\u5f39\u7a97\u5efa\u8bae\u548c\u8bc4\u8bba\u53c2\u8003\uff0c\u9f13\u52b1\u89c2\u4f17\u751f\u6210\u63cf\u8ff0\u6027\u8bc4\u8bba\u3002", "result": "48\u540d\u89c6\u529b\u6b63\u5e38\u7684\u7528\u6237\u6d4b\u8bd5\u4e2d\uff0c89%\u7684\u8bc4\u8bba\u5305\u542b\u89c6\u89c9\u63cf\u8ff0\u3002\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u89c2\u4f17\u8868\u793a\u8fd9\u4e9b\u8bc4\u8bba\u867d\u4e0d\u5982\u4e13\u4e1aAD\u4e25\u8c28\uff0c\u4f46\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u89c6\u89c9\u80cc\u666f\u548c\u60c5\u611f\u7ec6\u8282\u3002", "conclusion": "CoSight\u901a\u8fc7\u666e\u901a\u89c2\u4f17\u7684\u53c2\u4e0e\uff0c\u4e3a\u89c6\u9891\u53ef\u8bbf\u95ee\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8865\u5145\u65b9\u6848\uff0c\u867d\u7136\u6548\u679c\u4e0d\u53ca\u4e13\u4e1aAD\uff0c\u4f46\u4ecd\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.08709", "pdf": "https://arxiv.org/pdf/2508.08709", "abs": "https://arxiv.org/abs/2508.08709", "authors": ["Lukas Krupp", "Maximilian Sch\u00f6ffel", "Elias Biehl", "Norbert Wehn"], "title": "CRADLE: Conversational RTL Design Space Exploration with LLM-based Multi-Agent Systems", "categories": ["cs.RO", "cs.AR", "cs.LG", "cs.MA"], "comment": "Accepted for presentation at the 22nd International SoC Conference\n  (ISOCC 2025). Proceedings to be included in IEEE Xplore", "summary": "This paper presents CRADLE, a conversational framework for design space\nexploration of RTL designs using LLM-based multi-agent systems. Unlike existing\nrigid approaches, CRADLE enables user-guided flows with internal\nself-verification, correction, and optimization. We demonstrate the framework\nwith a generator-critic agent system targeting FPGA resource minimization using\nstate-of-the-art LLMs. Experimental results on the RTLLM benchmark show that\nCRADLE achieves significant reductions in resource usage with averages of 48%\nand 40% in LUTs and FFs across all benchmark designs.", "AI": {"tldr": "CRADLE\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8eRTL\u8bbe\u8ba1\u7684\u5bf9\u8bdd\u5f0f\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u663e\u8457\u51cf\u5c11\u8d44\u6e90\u4f7f\u7528\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RTL\u8bbe\u8ba1\u65b9\u6cd5\u8fc7\u4e8e\u521a\u6027\uff0c\u7f3a\u4e4f\u7528\u6237\u5f15\u5bfc\u548c\u81ea\u52a8\u5316\u4f18\u5316\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u751f\u6210\u5668-\u6279\u8bc4\u5bb6\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7ed3\u5408LLM\u6280\u672f\u8fdb\u884c\u5185\u90e8\u81ea\u9a8c\u8bc1\u3001\u4fee\u6b63\u548c\u4f18\u5316\u3002", "result": "\u5728RTLLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLUT\u548cFF\u8d44\u6e90\u4f7f\u7528\u5e73\u5747\u51cf\u5c1148%\u548c40%\u3002", "conclusion": "CRADLE\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u9ad8\u6548\u7684RTL\u8bbe\u8ba1\u63a2\u7d22\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u5316\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2508.08831", "pdf": "https://arxiv.org/pdf/2508.08831", "abs": "https://arxiv.org/abs/2508.08831", "authors": ["Bo-Hsun Chen", "Nevindu M. Batagoda", "Dan Negrut"], "title": "DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": "19 pages, 17 figures, and 4 tables", "summary": "We introduce DiffPhysCam, a differentiable camera simulator designed to\nsupport robotics and embodied AI applications by enabling gradient-based\noptimization in visual perception pipelines. Generating synthetic images that\nclosely mimic those from real cameras is essential for training visual models\nand enabling end-to-end visuomotor learning. Moreover, differentiable rendering\nallows inverse reconstruction of real-world scenes as digital twins,\nfacilitating simulation-based robotics training. However, existing virtual\ncameras offer limited control over intrinsic settings, poorly capture optical\nartifacts, and lack tunable calibration parameters -- hindering sim-to-real\ntransfer. DiffPhysCam addresses these limitations through a multi-stage\npipeline that provides fine-grained control over camera settings, models key\noptical effects such as defocus blur, and supports calibration with real-world\ndata. It enables both forward rendering for image synthesis and inverse\nrendering for 3D scene reconstruction, including mesh and material texture\noptimization. We show that DiffPhysCam enhances robotic perception performance\nin synthetic image tasks. As an illustrative example, we create a digital twin\nof a real-world scene using inverse rendering, simulate it in a multi-physics\nenvironment, and demonstrate navigation of an autonomous ground vehicle using\nimages generated by DiffPhysCam.", "AI": {"tldr": "DiffPhysCam\u662f\u4e00\u79cd\u53ef\u5fae\u5206\u76f8\u673a\u6a21\u62df\u5668\uff0c\u652f\u6301\u673a\u5668\u4eba\u5b66\u548c\u5177\u8eabAI\u5e94\u7528\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u63d0\u5347\u89c6\u89c9\u611f\u77e5\u7ba1\u9053\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u76f8\u673a\u5728\u63a7\u5236\u5185\u53c2\u8bbe\u7f6e\u3001\u5149\u5b66\u4f2a\u5f71\u6355\u83b7\u548c\u6821\u51c6\u53c2\u6570\u8c03\u4f18\u65b9\u9762\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002DiffPhysCam\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u7ba1\u9053\uff0c\u7cbe\u7ec6\u63a7\u5236\u76f8\u673a\u8bbe\u7f6e\u3001\u5efa\u6a21\u5173\u952e\u5149\u5b66\u6548\u679c\uff08\u5982\u6563\u7126\u6a21\u7cca\uff09\uff0c\u5e76\u652f\u6301\u771f\u5b9e\u6570\u636e\u6821\u51c6\uff0c\u5b9e\u73b0\u6b63\u5411\u6e32\u67d3\u548c\u9006\u5411\u6e32\u67d3\u3002", "result": "DiffPhysCam\u63d0\u5347\u4e86\u5408\u6210\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u673a\u5668\u4eba\u611f\u77e5\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u9006\u5411\u6e32\u67d3\u521b\u5efa\u4e86\u771f\u5b9e\u573a\u666f\u7684\u6570\u5b57\u5b6a\u751f\u3002", "conclusion": "DiffPhysCam\u4e3a\u89e3\u51b3\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u89c6\u89c9\u611f\u77e5\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u5728\u81ea\u4e3b\u5730\u9762\u8f66\u8f86\u5bfc\u822a\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.09056", "pdf": "https://arxiv.org/pdf/2508.09056", "abs": "https://arxiv.org/abs/2508.09056", "authors": ["Shreya Ghosh", "Abu Shafin Mohammad Mahdee Jameel", "Aly El Gamal"], "title": "FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm", "categories": ["cs.LG", "cs.CR", "cs.NI", "eess.SP"], "comment": null, "summary": "Intrusion Detection Systems (IDS) have an increasingly important role in\npreventing exploitation of network vulnerabilities by malicious actors. Recent\ndeep learning based developments have resulted in significant improvements in\nthe performance of IDS systems. In this paper, we present FetFIDS, where we\nexplore the employment of feature embedding instead of positional embedding to\nimprove intrusion detection performance of a transformer based deep learning\nsystem. Our model is developed with the aim of deployments in edge learning\nscenarios, where federated learning over multiple communication rounds can\nensure both privacy and localized performance improvements. FetFIDS outperforms\nmultiple state-of-the-art intrusion detection systems in a federated\nenvironment and demonstrates a high degree of suitability to federated\nlearning. The code for this work can be found at\nhttps://github.com/ghosh64/fetfids.", "AI": {"tldr": "FetFIDS\u662f\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u5d4c\u5165\u800c\u975e\u4f4d\u7f6e\u5d4c\u5165\u7684\u8f6c\u6362\u5668\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347\u5165\u4fb5\u68c0\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u9002\u5408\u8fb9\u7f18\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u73af\u5883\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u7cfb\u7edf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662f\u8f6c\u6362\u5668\uff09\u7684\u7279\u5f81\u5d4c\u5165\u65b9\u6cd5\uff0c\u63d0\u5347\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u9002\u5e94\u8054\u90a6\u5b66\u4e60\u73af\u5883\u3002", "method": "\u91c7\u7528\u7279\u5f81\u5d4c\u5165\u66ff\u4ee3\u4f4d\u7f6e\u5d4c\u5165\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9002\u5408\u8054\u90a6\u5b66\u4e60\u7684\u8f6c\u6362\u5668\u6a21\u578bFetFIDS\uff0c\u901a\u8fc7\u591a\u8f6e\u901a\u4fe1\u4f18\u5316\u672c\u5730\u6027\u80fd\u3002", "result": "FetFIDS\u5728\u8054\u90a6\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5e76\u9002\u5408\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002", "conclusion": "FetFIDS\u901a\u8fc7\u7279\u5f81\u5d4c\u5165\u548c\u8054\u90a6\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u5165\u4fb5\u68c0\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u90e8\u7f72\u3002"}}
{"id": "2508.08672", "pdf": "https://arxiv.org/pdf/2508.08672", "abs": "https://arxiv.org/abs/2508.08672", "authors": ["Ana\u00eblle Beignon", "Thomas Thibault", "Nolwenn Maudet"], "title": "Imposing AI: Deceptive design patterns against sustainability", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative AI is being massively deployed in digital services, at a scale\nthat will result in significant environmental harm. We document how tech\ncompanies are transforming established user interfaces to impose AI use and\nshow how and to what extent these strategies fit within established deceptive\npattern categories. We identify two main design strategies that are implemented\nto impose AI use in both personal and professional contexts: imposing AI\nfeatures in interfaces at the expense of existing non-AI features and promoting\nnarratives about AI that make it harder to resist using it. We discuss\nopportunities for regulating the imposed adoption of AI features, which would\ninevitably lead to negative environmental effects.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u8ba8\u8bba\u4e86\u751f\u6210\u5f0fAI\u5927\u89c4\u6a21\u90e8\u7f72\u5bf9\u73af\u5883\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u79d1\u6280\u516c\u53f8\u5982\u4f55\u901a\u8fc7\u754c\u9762\u8bbe\u8ba1\u548c\u53d9\u4e8b\u7b56\u7565\u5f3a\u5236\u7528\u6237\u4f7f\u7528AI\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793a\u79d1\u6280\u516c\u53f8\u5982\u4f55\u901a\u8fc7\u8bbe\u8ba1\u7b56\u7565\u5f3a\u5236\u7528\u6237\u4f7f\u7528AI\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5f3a\u5236\u884c\u4e3a\u5bf9\u73af\u5883\u7684\u6f5c\u5728\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u5206\u6790\u79d1\u6280\u516c\u53f8\u7684\u7528\u6237\u754c\u9762\u8bbe\u8ba1\u7b56\u7565\u548c\u53d9\u4e8b\u624b\u6bb5\uff0c\u5e76\u5c06\u5176\u4e0e\u5df2\u77e5\u7684\u6b3a\u9a97\u6027\u6a21\u5f0f\u7c7b\u522b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8bc6\u522b\u4e86\u4e24\u79cd\u4e3b\u8981\u7684\u8bbe\u8ba1\u7b56\u7565\uff1a\u5728\u754c\u9762\u4e2d\u5f3a\u5236\u66ff\u6362\u975eAI\u529f\u80fd\uff0c\u4ee5\u53ca\u901a\u8fc7\u53d9\u4e8b\u624b\u6bb5\u964d\u4f4e\u7528\u6237\u5bf9AI\u7684\u6297\u62d2\u3002", "conclusion": "\u7ed3\u8bba\u547c\u5401\u901a\u8fc7\u76d1\u7ba1\u624b\u6bb5\u9650\u5236AI\u529f\u80fd\u7684\u5f3a\u5236\u63a8\u5e7f\uff0c\u4ee5\u51cf\u5c11\u5bf9\u73af\u5883\u7684\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2508.08749", "pdf": "https://arxiv.org/pdf/2508.08749", "abs": "https://arxiv.org/abs/2508.08749", "authors": ["Yuan Qiu", "Ke Yi"], "title": "Approximate DBSCAN under Differential Privacy", "categories": ["cs.CR", "cs.DB"], "comment": null, "summary": "This paper revisits the DBSCAN problem under differential privacy (DP).\nExisting DP-DBSCAN algorithms aim at publishing the cluster labels of the input\npoints. However, we show that both empirically and theoretically, this approach\ncannot offer any utility in the published results. We therefore propose an\nalternative definition of DP-DBSCAN based on the notion of spans. We argue that\npublishing the spans actually better serves the purposes of visualization and\nclassification of DBSCAN. Then we present a linear-time DP-DBSCAN algorithm\nachieving the sandwich quality guarantee in any constant dimensions, as well as\nmatching lower bounds on the approximation ratio. A key building block in our\nalgorithm is a linear-time algorithm for constructing a histogram under\npure-DP, which is of independent interest. Finally, we conducted experiments on\nboth synthetic and real-world datasets to verify the practical performance of\nour DP-DBSCAN algorithm.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u4e0b\u7684DBSCAN\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8espan\u7684\u65b0\u5b9a\u4e49\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ebf\u6027\u65f6\u95f4\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u7406\u8bba\u4e0b\u754c\u3002", "motivation": "\u73b0\u6709DP-DBSCAN\u7b97\u6cd5\u5728\u53d1\u5e03\u8f93\u5165\u70b9\u7684\u805a\u7c7b\u6807\u7b7e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8espan\u7684\u65b0\u5b9a\u4e49\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ebf\u6027\u65f6\u95f4DP-DBSCAN\u7b97\u6cd5\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5176\u8fd1\u4f3c\u6bd4\u7684\u4e0b\u754c\u3002", "result": "\u7b97\u6cd5\u5728\u4efb\u4f55\u5e38\u6570\u7ef4\u5ea6\u4e0b\u90fd\u80fd\u4fdd\u8bc1\u4e09\u660e\u6cbb\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u53ef\u89c6\u5316\u4e0e\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u5177\u6709\u9ad8\u6548\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u6027\u80fd\u3002"}}
{"id": "2508.08930", "pdf": "https://arxiv.org/pdf/2508.08930", "abs": "https://arxiv.org/abs/2508.08930", "authors": ["Juyeong Hwang", "Seong-Eun Hon", "JaeYoung Seon", "Hyeongyeop Kang"], "title": "How Does a Virtual Agent Decide Where to Look? -- Symbolic Cognitive Reasoning for Embodied Head Rotation", "categories": ["cs.GR"], "comment": null, "summary": "Natural head rotation is critical for believable embodied virtual agents, yet\nthis micro-level behavior remains largely underexplored. While head-rotation\nprediction algorithms could, in principle, reproduce this behavior, they\ntypically focus on visually salient stimuli and overlook the cognitive motives\nthat guide head rotation. This yields agents that look at conspicuous objects\nwhile overlooking obstacles or task-relevant cues, diminishing realism in a\nvirtual environment. We introduce SCORE, a Symbolic Cognitive Reasoning\nframework for Embodied Head Rotation, a data-agnostic framework that produces\ncontext-aware head movements without task-specific training or hand-tuned\nheuristics. A controlled VR study (N=20) identifies five motivational drivers\nof human head movements: Interest, Information Seeking, Safety, Social Schema,\nand Habit. SCORE encodes these drivers as symbolic predicates, perceives the\nscene with a Vision-Language Model (VLM), and plans head poses with a Large\nLanguage Model (LLM). The framework employs a hybrid workflow: the VLM-LLM\nreasoning is executed offline, after which a lightweight FastVLM performs\nonline validation to suppress hallucinations while maintaining responsiveness\nto scene dynamics. The result is an agent that predicts not only where to look\nbut also why, generalizing to unseen scenes and multi-agent crowds while\nretaining behavioral plausibility.", "AI": {"tldr": "SCORE\u662f\u4e00\u4e2a\u7b26\u53f7\u8ba4\u77e5\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5934\u90e8\u52a8\u4f5c\uff0c\u4e0d\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6216\u4eba\u5de5\u8c03\u6574\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u5934\u90e8\u65cb\u8f6c\u662f\u865a\u62df\u4ee3\u7406\u903c\u771f\u884c\u4e3a\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u901a\u5e38\u5ffd\u7565\u8ba4\u77e5\u52a8\u673a\uff0c\u5bfc\u81f4\u884c\u4e3a\u7f3a\u4e4f\u771f\u5b9e\u6027\u3002", "method": "SCORE\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u901a\u8fc7\u79bb\u7ebf\u63a8\u7406\u751f\u6210\u5934\u90e8\u52a8\u4f5c\uff0c\u5e76\u7528\u8f7b\u91cf\u7ea7FastVLM\u5728\u7ebf\u9a8c\u8bc1\u3002", "result": "SCORE\u80fd\u591f\u9884\u6d4b\u5934\u90e8\u52a8\u4f5c\u7684\u52a8\u673a\uff08\u5982\u5174\u8da3\u3001\u5b89\u5168\u7b49\uff09\uff0c\u5e76\u9002\u7528\u4e8e\u65b0\u573a\u666f\u548c\u591a\u4ee3\u7406\u73af\u5883\u3002", "conclusion": "SCORE\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u4ee3\u7406\u884c\u4e3a\u7684\u771f\u5b9e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.09060", "pdf": "https://arxiv.org/pdf/2508.09060", "abs": "https://arxiv.org/abs/2508.09060", "authors": ["Abu Shafin Mohammad Mahdee Jameel", "Shreya Ghosh", "Aly El Gamal"], "title": "Developing a Transferable Federated Network Intrusion Detection System", "categories": ["cs.CR", "cs.LG", "cs.NI", "eess.SP"], "comment": "Currently under review", "summary": "Intrusion Detection Systems (IDS) are a vital part of a network-connected\ndevice. In this paper, we develop a deep learning based intrusion detection\nsystem that is deployed in a distributed setup across devices connected to a\nnetwork. Our aim is to better equip deep learning models against unknown\nattacks using knowledge from known attacks. To this end, we develop algorithms\nto maximize the number of transferability relationships. We propose a\nConvolutional Neural Network (CNN) model, along with two algorithms that\nmaximize the number of relationships observed. One is a two step data\npre-processing stage, and the other is a Block-Based Smart Aggregation (BBSA)\nalgorithm. The proposed system succeeds in achieving superior transferability\nperformance while maintaining impressive local detection rates. We also show\nthat our method is generalizable, exhibiting transferability potential across\ndatasets and even with different backbones. The code for this work can be found\nat https://github.com/ghosh64/tabfidsv2.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7b97\u6cd5\u6700\u5927\u5316\u5df2\u77e5\u653b\u51fb\u7684\u77e5\u8bc6\u8fc1\u79fb\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cdCNN\u6a21\u578b\u53ca\u4e24\u79cd\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u8fc1\u79fb\u6027\u548c\u672c\u5730\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u672a\u77e5\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u5229\u7528\u5df2\u77e5\u653b\u51fb\u7684\u77e5\u8bc6\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdCNN\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u4e24\u6b65\u6570\u636e\u9884\u5904\u7406\u548c\u5757\u57fa\u667a\u80fd\u805a\u5408\u7b97\u6cd5\uff08BBSA\uff09\u3002", "result": "\u7cfb\u7edf\u5728\u4fdd\u6301\u9ad8\u672c\u5730\u68c0\u6d4b\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8fc1\u79fb\u6027\u80fd\uff0c\u4e14\u65b9\u6cd5\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u548c\u4e0d\u540c\u6a21\u578b\u9aa8\u5e72\u7684\u60c5\u51b5\u4e0b\u5c55\u73b0\u51fa\u8fc1\u79fb\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.08731", "pdf": "https://arxiv.org/pdf/2508.08731", "abs": "https://arxiv.org/abs/2508.08731", "authors": ["Mingyuan Zhong", "Ajit Mallavarapu", "Qing Nie"], "title": "Caption: Generating Informative Content Labels for Image Buttons Using Next-Screen Context", "categories": ["cs.HC"], "comment": null, "summary": "We present Caption, an LLM-powered content label generation tool for visual\ninteractive elements on mobile devices. Content labels are essential for screen\nreaders to provide announcements for image-based elements, but are often\nmissing or uninformative due to developer neglect. Automated captioning systems\nattempt to address this, but are limited to on-screen context, often resulting\nin inaccurate or unspecific labels. To generate more accurate and descriptive\nlabels, Caption collects next-screen context on interactive elements by\nnavigating to the destination screen that appears after an interaction and\nincorporating information from both the origin and destination screens.\nPreliminary results show Caption generates more accurate labels than both human\nannotators and an LLM baseline. We expect Caption to empower developers by\nproviding actionable accessibility suggestions and directly support on-demand\nrepairs by screen reader users.", "AI": {"tldr": "Caption \u662f\u4e00\u6b3e\u57fa\u4e8e LLM \u7684\u5de5\u5177\uff0c\u7528\u4e8e\u4e3a\u79fb\u52a8\u8bbe\u5907\u7684\u89c6\u89c9\u4ea4\u4e92\u5143\u7d20\u751f\u6210\u5185\u5bb9\u6807\u7b7e\uff0c\u901a\u8fc7\u7ed3\u5408\u4ea4\u4e92\u524d\u540e\u7684\u5c4f\u5e55\u4fe1\u606f\u63d0\u9ad8\u6807\u7b7e\u51c6\u786e\u6027\u3002\u521d\u6b65\u7ed3\u679c\u663e\u793a\u5176\u4f18\u4e8e\u4eba\u5de5\u6807\u6ce8\u548c LLM \u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u8bbe\u5907\u4e2d\u89c6\u89c9\u4ea4\u4e92\u5143\u7d20\u5185\u5bb9\u6807\u7b7e\u7f3a\u5931\u6216\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5c4f\u5e55\u9605\u8bfb\u5668\u7684\u53ef\u7528\u6027\u3002", "method": "Caption \u901a\u8fc7\u5206\u6790\u4ea4\u4e92\u524d\u540e\u7684\u5c4f\u5e55\u4fe1\u606f\uff08\u5373\u539f\u5c4f\u5e55\u548c\u76ee\u6807\u5c4f\u5e55\u7684\u4e0a\u4e0b\u6587\uff09\uff0c\u751f\u6210\u66f4\u5177\u63cf\u8ff0\u6027\u7684\u6807\u7b7e\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0cCaption \u751f\u6210\u7684\u6807\u7b7e\u6bd4\u4eba\u5de5\u6807\u6ce8\u548c LLM \u57fa\u7ebf\u66f4\u51c6\u786e\u3002", "conclusion": "Caption \u6709\u671b\u5e2e\u52a9\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u6613\u8bbf\u95ee\u7684\u5efa\u8bae\uff0c\u5e76\u652f\u6301\u5c4f\u5e55\u9605\u8bfb\u5668\u7528\u6237\u5373\u65f6\u4fee\u590d\u95ee\u9898\u3002"}}
{"id": "2508.08552", "pdf": "https://arxiv.org/pdf/2508.08552", "abs": "https://arxiv.org/abs/2508.08552", "authors": ["Keumseo Ryum", "Jinu Gong", "Joonhyuk Kang"], "title": "SHEFL: Resource-Aware Aggregation and Sparsification in Heterogeneous Ensemble Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": "9 pages, 7 figures, submitted to AAAI 2026", "summary": "Federated learning enables distributed training with private data of clients,\nbut its convergence is hindered by data and system heterogeneity in realistic\ncommunication scenarios. Most existing system heterogeneous FL schemes utilize\nglobal pruning or ensemble distillation, yet they often overlook typical\nconstraints required for communication efficiency. Meanwhile, deep ensembles\ncan aggregate predictions from individually trained models to improve\nperformance, but current ensemble-based FL methods fall short in fully\ncapturing the diversity of model predictions. In this work, we propose SHEFL, a\nglobal ensemble-based federated learning framework suited for clients with\ndiverse computational capacities. We allocate different numbers of global\nmodels to clients based on their available resources. We further introduce a\nnovel aggregation scheme that accounts for bias between clients with different\ncomputational capabilities. To reduce the computational burden of training deep\nensembles and mitigate data bias, we dynamically adjust the resource ratio\nacross clients - aggressively reducing the influence of underpowered clients in\nconstrained scenarios, while increasing their weight in the opposite case.\nExtensive experiments demonstrate that our method effectively addresses\ncomputational heterogeneity, significantly improving both fairness and overall\nperformance compared to existing approaches.", "AI": {"tldr": "SHEFL\u662f\u4e00\u79cd\u9762\u5411\u8ba1\u7b97\u80fd\u529b\u5404\u5f02\u5ba2\u6237\u7aef\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u5206\u914d\u548c\u65b0\u578b\u805a\u5408\u65b9\u6848\u89e3\u51b3\u7cfb\u7edf\u5f02\u8d28\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u901a\u4fe1\u6548\u7387\u548c\u9884\u6d4b\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u7cfb\u7edf\u5f02\u8d28\u6027\u573a\u666f\u4e0b\u3002SHEFL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u914d\u4e0d\u540c\u6570\u91cf\u7684\u5168\u5c40\u6a21\u578b\u7ed9\u5ba2\u6237\u7aef\uff0c\u5e76\u5f15\u5165\u65b0\u578b\u805a\u5408\u65b9\u6848\uff0c\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u6bd4\u4f8b\u4ee5\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u548c\u6570\u636e\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSHEFL\u5728\u516c\u5e73\u6027\u548c\u6574\u4f53\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SHEFL\u6709\u6548\u89e3\u51b3\u4e86\u8ba1\u7b97\u5f02\u8d28\u6027\uff0c\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u516c\u5e73\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.09062", "pdf": "https://arxiv.org/pdf/2508.09062", "abs": "https://arxiv.org/abs/2508.09062", "authors": ["Xiang Zhang", "Yawar Siddiqui", "Armen Avetisyan", "Chris Xie", "Jakob Engel", "Henry Howard-Jenkins"], "title": "VertexRegen: Mesh Generation with Continuous Level of Detail", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "ICCV 2025. Project Page: https://vertexregen.github.io/", "summary": "We introduce VertexRegen, a novel mesh generation framework that enables\ngeneration at a continuous level of detail. Existing autoregressive methods\ngenerate meshes in a partial-to-complete manner and thus intermediate steps of\ngeneration represent incomplete structures. VertexRegen takes inspiration from\nprogressive meshes and reformulates the process as the reversal of edge\ncollapse, i.e. vertex split, learned through a generative model. Experimental\nresults demonstrate that VertexRegen produces meshes of comparable quality to\nstate-of-the-art methods while uniquely offering anytime generation with the\nflexibility to halt at any step to yield valid meshes with varying levels of\ndetail.", "AI": {"tldr": "VertexRegen\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u683c\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u8fde\u7eed\u7ec6\u8282\u7ea7\u522b\u7684\u7f51\u683c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u7f51\u683c\u65f6\u91c7\u7528\u90e8\u5206\u5230\u6574\u4f53\u7684\u65b9\u5f0f\uff0c\u5bfc\u81f4\u4e2d\u95f4\u6b65\u9aa4\u7684\u7f51\u683c\u4e0d\u5b8c\u6574\u3002VertexRegen\u901a\u8fc7\u9006\u8f6c\u8fb9\u6298\u53e0\u8fc7\u7a0b\uff08\u9876\u70b9\u5206\u5272\uff09\u5b9e\u73b0\u4e86\u8fde\u7eed\u7ec6\u8282\u751f\u6210\u3002", "method": "\u6846\u67b6\u53d7\u6e10\u8fdb\u7f51\u683c\u542f\u53d1\uff0c\u901a\u8fc7\u5b66\u4e60\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u9876\u70b9\u5206\u5272\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVertexRegen\u751f\u6210\u7684\u7f51\u683c\u8d28\u91cf\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u80fd\u5728\u4efb\u610f\u6b65\u9aa4\u505c\u6b62\u4ee5\u751f\u6210\u5177\u6709\u4e0d\u540c\u7ec6\u8282\u7ea7\u522b\u7684\u6709\u6548\u7f51\u683c\u3002", "conclusion": "VertexRegen\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u8fde\u7eed\u7684\u7f51\u683c\u751f\u6210\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.08737", "pdf": "https://arxiv.org/pdf/2508.08737", "abs": "https://arxiv.org/abs/2508.08737", "authors": ["Jonathan C. Roberts", "Peter Butcher", "Panagiotis D. Ritsos"], "title": "From Data to Insight: Using Contextual Scenarios to Teach Critical Thinking in Data Visualisation", "categories": ["cs.HC", "H.5.2; K.3.2; I.3.6"], "comment": "6 pages, 5 figures, IEEE VIS Workshop on Visualization Education,\n  Literacy, and Activities 2025, Vienna", "summary": "This paper explores the use of scenario-based visualisation examples as a\npedagogical strategy for teaching students the complexities of data insight,\nrepresentation, and interpretation. Teaching data visualisation often involves\nexplaining intricate issues related to data management and the challenges of\npresenting data meaningfully. In this work, we present a series of data-driven\nscenarios. These concise stories depict specific situations, and are created to\nhelp the educators highlight key concerns in data communication, such as chart\nselection, temporal versus categorical comparison, visual bias, and narrative\nframing. By grounding these examples in real-world contexts, students are\nencouraged to critically assess not only what the data shows, but how and why\nit is shown that way. The paper presents a collection of example scenarios,\nthat educators can use for their own lessons; the work fits with a larger\nproject on looking at critical thinking in the classroom, and developing\nappropriate tools. We also start to abstract principles, from our approach, so\nthat others can develop their own scenarios for their teaching. Our approach\naligns with principles of authentic and scenario-based learning, using\nreal-world contexts to foster critical engagement with data.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u573a\u666f\u7684\u53ef\u89c6\u5316\u6559\u5b66\u7b56\u7565\uff0c\u5e2e\u52a9\u5b66\u751f\u5b66\u4e60\u6570\u636e\u6d1e\u5bdf\u3001\u8868\u793a\u548c\u89e3\u91ca\u7684\u590d\u6742\u6027\u3002", "motivation": "\u6559\u6388\u6570\u636e\u53ef\u89c6\u5316\u65f6\uff0c\u5b66\u751f\u5e38\u96be\u4ee5\u7406\u89e3\u6570\u636e\u7ba1\u7406\u548c\u5c55\u793a\u7684\u6311\u6218\uff0c\u9700\u8981\u901a\u8fc7\u5177\u4f53\u6848\u4f8b\u6765\u5f3a\u8c03\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6570\u636e\u9a71\u52a8\u7684\u573a\u666f\uff0c\u901a\u8fc7\u771f\u5b9e\u6848\u4f8b\u5e2e\u52a9\u5b66\u751f\u6279\u5224\u6027\u8bc4\u4f30\u6570\u636e\u7684\u5c55\u793a\u65b9\u5f0f\u53ca\u5176\u539f\u56e0\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u5957\u53ef\u4f9b\u6559\u5e08\u4f7f\u7528\u7684\u573a\u666f\u793a\u4f8b\uff0c\u5e76\u5f00\u59cb\u4ece\u65b9\u6cd5\u4e2d\u62bd\u8c61\u51fa\u9002\u7528\u4e8e\u5176\u4ed6\u6559\u5b66\u7684\u539f\u5219\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b26\u5408\u771f\u5b9e\u4e0e\u573a\u666f\u5b66\u4e60\u539f\u5219\uff0c\u901a\u8fc7\u73b0\u5b9e\u80cc\u666f\u4fc3\u8fdb\u5b66\u751f\u6279\u5224\u6027\u601d\u8003\u6570\u636e\u3002"}}
{"id": "2508.08712", "pdf": "https://arxiv.org/pdf/2508.08712", "abs": "https://arxiv.org/abs/2508.08712", "authors": ["Lingzhe Zhang", "Liancheng Fang", "Chiming Duan", "Minghua He", "Leyi Pan", "Pei Xiao", "Shiyu Huang", "Yunpeng Zhai", "Xuming Hu", "Philip S. Yu", "Aiwei Liu"], "title": "A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models", "categories": ["cs.CL", "cs.AI", "cs.DC", "68T50", "I.2.7"], "comment": null, "summary": "As text generation has become a core capability of modern Large Language\nModels (LLMs), it underpins a wide range of downstream applications. However,\nmost existing LLMs rely on autoregressive (AR) generation, producing one token\nat a time based on previously generated context-resulting in limited generation\nspeed due to the inherently sequential nature of the process. To address this\nchallenge, an increasing number of researchers have begun exploring parallel\ntext generation-a broad class of techniques aimed at breaking the\ntoken-by-token generation bottleneck and improving inference efficiency.\nDespite growing interest, there remains a lack of comprehensive analysis on\nwhat specific techniques constitute parallel text generation and how they\nimprove inference performance. To bridge this gap, we present a systematic\nsurvey of parallel text generation methods. We categorize existing approaches\ninto AR-based and Non-AR-based paradigms, and provide a detailed examination of\nthe core techniques within each category. Following this taxonomy, we assess\ntheir theoretical trade-offs in terms of speed, quality, and efficiency, and\nexamine their potential for combination and comparison with alternative\nacceleration strategies. Finally, based on our findings, we highlight recent\nadvancements, identify open challenges, and outline promising directions for\nfuture research in parallel text generation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5e73\u884c\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u5176\u5982\u4f55\u7a81\u7834\u4f20\u7edf\u81ea\u56de\u5f52\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u5e76\u5206\u7c7b\u5206\u6790\u4e86\u4e0d\u540c\u6280\u672f\u7684\u901f\u5ea6\u3001\u8d28\u91cf\u548c\u6548\u7387\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u81ea\u56de\u5f52\u751f\u6210\uff0c\u5176\u9010\u8bcd\u751f\u6210\u7684\u7279\u6027\u5bfc\u81f4\u901f\u5ea6\u53d7\u9650\u3002\u5e73\u884c\u6587\u672c\u751f\u6210\u6280\u672f\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5bf9\u5177\u4f53\u6280\u672f\u548c\u6027\u80fd\u63d0\u5347\u7f3a\u4e4f\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u6587\u7ae0\u7cfb\u7edf\u8c03\u67e5\u4e86\u5e73\u884c\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u5c06\u5176\u5206\u4e3a\u57fa\u4e8e\u81ea\u56de\u5f52\u548c\u975e\u81ea\u56de\u5f52\u7684\u8303\u5f0f\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u4e86\u5404\u7c7b\u6280\u672f\u7684\u6838\u5fc3\u7279\u70b9\u3002", "result": "\u901a\u8fc7\u5206\u7c7b\u548c\u5206\u6790\uff0c\u603b\u7ed3\u4e86\u4e0d\u540c\u6280\u672f\u5728\u901f\u5ea6\u3001\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u7684\u7406\u8bba\u6743\u8861\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0e\u5176\u4ed6\u52a0\u901f\u7b56\u7565\u7684\u7ed3\u5408\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u4e3a\u5e73\u884c\u6587\u672c\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u89c6\u89d2\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6311\u6218\u548c\u65b9\u5411\u3002"}}
{"id": "2508.09131", "pdf": "https://arxiv.org/pdf/2508.09131", "abs": "https://arxiv.org/abs/2508.09131", "authors": ["Zixin Yin", "Xili Dai", "Ling-Hao Chen", "Deyu Zhou", "Jianan Wang", "Duomin Wang", "Gang Yu", "Lionel M. Ni", "Heung-Yeung Shum"], "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-guided color editing in images and videos is a fundamental yet unsolved\nproblem, requiring fine-grained manipulation of color attributes, including\nalbedo, light source color, and ambient lighting, while preserving physical\nconsistency in geometry, material properties, and light-matter interactions.\nExisting training-free methods offer broad applicability across editing tasks\nbut struggle with precise color control and often introduce visual\ninconsistency in both edited and non-edited regions. In this work, we present\nColorCtrl, a training-free color editing method that leverages the attention\nmechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By\ndisentangling structure and color through targeted manipulation of attention\nmaps and value tokens, our method enables accurate and consistent color\nediting, along with word-level control of attribute intensity. Our method\nmodifies only the intended regions specified by the prompt, leaving unrelated\nareas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate\nthat ColorCtrl outperforms existing training-free approaches and achieves\nstate-of-the-art performances in both edit quality and consistency.\nFurthermore, our method surpasses strong commercial models such as FLUX.1\nKontext Max and GPT-4o Image Generation in terms of consistency. When extended\nto video models like CogVideoX, our approach exhibits greater advantages,\nparticularly in maintaining temporal coherence and editing stability. Finally,\nour method also generalizes to instruction-based editing diffusion models such\nas Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u672c\u5f15\u5bfc\u989c\u8272\u7f16\u8f91\u65b9\u6cd5ColorCtrl\uff0c\u5229\u7528\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7cbe\u786e\u989c\u8272\u63a7\u5236\u548c\u4e00\u81f4\u6027\u7f16\u8f91\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u548c\u89c6\u9891\u4e2d\u6587\u672c\u5f15\u5bfc\u989c\u8272\u7f16\u8f91\u7684\u96be\u9898\uff0c\u8981\u6c42\u7ec6\u7c92\u5ea6\u64cd\u63a7\u989c\u8272\u5c5e\u6027\u5e76\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u76ee\u6807\u6027\u5730\u64cd\u63a7\u6ce8\u610f\u529b\u56fe\u548c\u503c\u6807\u8bb0\u5206\u79bb\u7ed3\u6784\u548c\u989c\u8272\uff0c\u5b9e\u73b0\u51c6\u786e\u4e00\u81f4\u7684\u989c\u8272\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660eColorCtrl\u5728\u7f16\u8f91\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u89c6\u9891\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "ColorCtrl\u5c55\u73b0\u4e86\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u5353\u8d8a\u6027\u80fd\uff0c\u5728\u591a\u79cd\u6a21\u578b\u548c\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.08958", "pdf": "https://arxiv.org/pdf/2508.08958", "abs": "https://arxiv.org/abs/2508.08958", "authors": ["Kostiantyn Kucher", "Niklas R\u00f6nnberg", "Jonas L\u00f6wgren"], "title": "Addressing the Heterogeneity of Visualization in an Introductory PhD Course in the Swedish Context", "categories": ["cs.HC", "K.3.2; H.5; I.3"], "comment": "8 pages, 3 figures", "summary": "Visualization is a heterogeneous field, and this aspect is often reflected by\nthe organizational structures at higher education institutions that academic\nresearchers in visualization and related fields including computer graphics,\nhuman-computer interaction, and media design are typically affiliated with. It\nmay thus be a challenge for new PhD students to grasp the fragmented structure\nof their new workplace, form collegial relations across the institution, and to\nbuild a coherent picture of the discipline as a whole. We report an attempt to\naddress this challenge, in the form of an introductory course on the subject of\nVisualization Technology and Methodology for PhD students at the Division for\nMedia and Information Technology, Link\\\"oping University, Sweden. We discuss\nthe course design, including interactions with other doctoral education\nactivities and field trips to multiple research groups and units within the\ndivision (ranging from scientific visualization and computer graphics to media\ndesign and visual communication). Lessons learned from the course preparation\nwork as well as the first instance of the course offered during autumn term\n2023 can be helpful to researchers and educators aiming to establish or improve\nsimilar doctoral courses.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86\u53ef\u89c6\u5316\u9886\u57df\u7684\u5f02\u6784\u6027\u5bf9\u535a\u58eb\u751f\u7406\u89e3\u5b66\u79d1\u6574\u4f53\u7ed3\u6784\u7684\u6311\u6218\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u95e8\u5e2e\u52a9\u535a\u58eb\u751f\u5efa\u7acb\u5b66\u79d1\u6574\u4f53\u8ba4\u77e5\u7684\u8bfe\u7a0b\u7684\u8bbe\u8ba1\u548c\u5b9e\u65bd\u7ecf\u9a8c\u3002", "motivation": "\u53ef\u89c6\u5316\u9886\u57df\u7684\u5f02\u6784\u6027\u4f7f\u5f97\u65b0\u535a\u58eb\u751f\u96be\u4ee5\u7406\u89e3\u5b66\u79d1\u7684\u6574\u4f53\u7ed3\u6784\u548c\u5efa\u7acb\u8de8\u673a\u6784\u7684\u5b66\u672f\u5173\u7cfb\uff0c\u9700\u8981\u901a\u8fc7\u8bfe\u7a0b\u8bbe\u8ba1\u6765\u5e2e\u52a9\u4ed6\u4eec\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u65bd\u4e86\u4e00\u95e8\u9762\u5411\u535a\u58eb\u751f\u7684\u53ef\u89c6\u5316\u6280\u672f\u4e0e\u65b9\u6cd5\u8bba\u5165\u95e8\u8bfe\u7a0b\uff0c\u5305\u62ec\u4e0e\u5176\u4ed6\u535a\u58eb\u6559\u80b2\u6d3b\u52a8\u7684\u4e92\u52a8\u548c\u5b9e\u5730\u8003\u5bdf\u591a\u4e2a\u7814\u7a76\u5c0f\u7ec4\u3002", "result": "\u8bfe\u7a0b\u8bbe\u8ba1\u548c\u5b9e\u65bd\u7684\u7ecf\u9a8c\u603b\u7ed3\uff0c\u53ef\u4f9b\u5176\u4ed6\u7814\u7a76\u8005\u548c\u6559\u80b2\u5de5\u4f5c\u8005\u5728\u5efa\u7acb\u6216\u6539\u8fdb\u7c7b\u4f3c\u8bfe\u7a0b\u65f6\u53c2\u8003\u3002", "conclusion": "\u901a\u8fc7\u8bfe\u7a0b\u8bbe\u8ba1\u53ef\u4ee5\u5e2e\u52a9\u535a\u58eb\u751f\u66f4\u597d\u5730\u7406\u89e3\u53ef\u89c6\u5316\u9886\u57df\u7684\u6574\u4f53\u7ed3\u6784\uff0c\u540c\u65f6\u4e3a\u5176\u4ed6\u6559\u80b2\u8005\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u7ecf\u9a8c\u3002"}}
{"id": "2508.08740", "pdf": "https://arxiv.org/pdf/2508.08740", "abs": "https://arxiv.org/abs/2508.08740", "authors": ["Keren Censor-Hillel", "Orr Fischer", "Ran Gelles", "Pedro Soto"], "title": "Two for One, One for All: Deterministic LDC-based Robust Computation in Congested Clique", "categories": ["cs.DS", "cs.DC"], "comment": null, "summary": "We design a deterministic compiler that makes any computation in the\nCongested Clique model robust to a constant fraction $\\alpha<1$ of adversarial\ncrash faults. In particular, we show how a network of $n$ nodes can compute any\ncircuit of depth $d$, width $\\omega$, and gate total fan $\\Delta$, in\n$d\\cdot\\lceil\\frac{\\omega}{n^2}+\\frac{\\Delta}{n}\\rceil\\cdot\n2^{O(\\sqrt{\\log{n}}\\log\\log{n})}$ rounds in such a faulty model. As a\ncorollary, any $T$-round Congested Clique algorithm can be compiled into an\nalgorithm that completes in $T^2 n^{o(1)}$ rounds in this model.\n  Our compiler obtains resilience to node crashes by coding information across\nthe network, where we leverage locally-decodable codes (LDCs) to maintain a low\ncomplexity overhead, as these allow recovering the information needed at each\ncomputational step by querying only small parts of the codeword.\n  The main technical contribution is that because erasures occur in known\nlocations, which correspond to crashed nodes, we can derandomize classical LDC\nconstructions by deterministically selecting query sets that avoid sufficiently\nmany erasures. Moreover, when decoding multiple codewords in parallel, our\nderandomization load-balances the queries per-node, thereby preventing\ncongestion and maintaining a low round complexity.\n  Deterministic decoding of LDCs presents a new challenge: the adversary can\ntarget precisely the (few) nodes that are queried for decoding a certain\ncodeword. We overcome this issue via an adaptive doubling strategy: if a\ndecoding attempt for a codeword fails, the node doubles the number of its\ndecoding attempts. Similarly, when the adversary crashes the decoding node\nitself, we replace it dynamically with two other non-crashed nodes. By\ncarefully combining these two doubling processes, we overcome the challenges\nposed by the combination of a deterministic LDC with a worst case pattern of\ncrashes.", "AI": {"tldr": "\u8bbe\u8ba1\u4e00\u4e2a\u786e\u5b9a\u6027\u7f16\u8bd1\u5668\uff0c\u4f7fCongested Clique\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u5bf9\u6052\u5b9a\u6bd4\u4f8b\u03b1 < 1\u7684\u6076\u610f\u5d29\u6e83\u6545\u969c\u5177\u6709\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u3002", "motivation": "\u5728\u7f51\u7edc\u8282\u70b9\u53ef\u80fd\u5d29\u6e83\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u4fdd\u8ba1\u7b97\u80fd\u591f\u7ee7\u7eed\u8fdb\u884c\uff0c\u907f\u514d\u5d29\u6e83\u8282\u70b9\u5bf9\u6574\u4f53\u8ba1\u7b97\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u5c40\u90e8\u53ef\u89e3\u7801\u4ee3\u7801\uff08LDC\uff09\u5728\u7f51\u7edc\u4e2d\u7f16\u7801\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u786e\u5b9a\u6027\u9009\u62e9\u67e5\u8be2\u96c6\u548c\u81ea\u9002\u5e94\u52a0\u500d\u7b56\u7565\u6765\u5e94\u5bf9\u8282\u70b9\u5d29\u6e83\u3002", "result": "\u63d0\u51fa\u7684\u7f16\u8bd1\u5668\u80fd\u591f\u5728\u6709\u5d29\u6e83\u6545\u969c\u7684\u6a21\u578b\u4e2d\uff0c\u9ad8\u6548\u5b8c\u6210\u7535\u8def\u8ba1\u7b97\u548c\u7b97\u6cd5\u7f16\u8bd1\uff0c\u65f6\u95f4\u590d\u6742\u6027\u4e3aT^2 n^{o(1)}\u8f6e\u6b21\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408LDC\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u5d29\u6e83\u6545\u969c\u4e0b\u7684\u9ad8\u6548\u786e\u5b9a\u6027\u8ba1\u7b97\u3002"}}
{"id": "2508.07726", "pdf": "https://arxiv.org/pdf/2508.07726", "abs": "https://arxiv.org/abs/2508.07726", "authors": ["Stefan G\u00f6ssner"], "title": "Symplectification of Circular Arcs and Arc Splines", "categories": ["math.SG", "cs.CG", "cs.GR", "53D99"], "comment": "14 pages, 10 figures, 1 program listing", "summary": "In this article, circular arcs are considered both individually and as\nelements of a piecewise circular curve. The endpoint parameterization proves to\nbe quite advantageous here. The perspective of symplectic geometry provides new\nvectorial relationships for the circular arc. Curves are considered whose\nneighboring circular elements each have a common end point or, in addition, a\ncommon tangent. These arc splines prove to be a one-parameter curve family,\nwhereby this parameter can be optimized with regard to various criteria.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5706\u5f27\u53ca\u5176\u5728\u5206\u6bb5\u5706\u5f27\u66f2\u7ebf\u4e2d\u7684\u5e94\u7528\uff0c\u5229\u7528\u7aef\u70b9\u53c2\u6570\u5316\u548c\u8f9b\u51e0\u4f55\u89c6\u89d2\u63d0\u4f9b\u4e86\u65b0\u7684\u5411\u91cf\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22\u5706\u5f27\u5728\u51e0\u4f55\u6784\u5efa\u4e2d\u7684\u9ad8\u6548\u8868\u793a\u53ca\u5176\u4f18\u5316\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u7aef\u70b9\u53c2\u6570\u5316\u548c\u8f9b\u51e0\u4f55\u65b9\u6cd5\uff0c\u5206\u6790\u5706\u5f27\u53ca\u5176\u62fc\u63a5\u6761\u4ef6\u3002", "result": "\u5706\u5f27\u6837\u6761\u53ef\u89c6\u4e3a\u5355\u53c2\u6570\u66f2\u7ebf\u65cf\uff0c\u5e76\u80fd\u9488\u5bf9\u4e0d\u540c\u6807\u51c6\u4f18\u5316\u53c2\u6570\u3002", "conclusion": "\u5706\u5f27\u53c2\u6570\u5316\u548c\u8f9b\u51e0\u4f55\u4e3a\u66f2\u7ebf\u65cf\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2508.09028", "pdf": "https://arxiv.org/pdf/2508.09028", "abs": "https://arxiv.org/abs/2508.09028", "authors": ["Yuhao Kang", "Chenglong Wang"], "title": "Envisioning Generative Artificial Intelligence in Cartography and Mapmaking", "categories": ["cs.HC"], "comment": "9 pages, 6 figures", "summary": "Generative artificial intelligence (GenAI), including large language models,\ndiffusion-based image generation models, and GenAI agents, has provided new\nopportunities for advancements in mapping and cartography. Due to their\ncharacteristics including world knowledge and generalizability, artistic style\nand creativity, and multimodal integration, we envision that GenAI may benefit\na variety of cartographic design decisions, from mapmaking (e.g.,\nconceptualization, data preparation, map design, and map evaluation) to map use\n(such as map reading, interpretation, and analysis). This paper discusses\nseveral important topics regarding why and how GenAI benefits cartography with\ncase studies including symbolization, map evaluation, and map reading. Despite\nits unprecedented potential, we identify key scenarios where GenAI may not be\nsuitable, such as tasks that require a deep understanding of cartographic\nknowledge or prioritize precision and reliability. We also emphasize the need\nto consider ethical and social implications, such as concerns related to\nhallucination, reproducibility, bias, copyright, and explainability. This work\nlays the foundation for further exploration and provides a roadmap for future\nresearch at the intersection of GenAI and cartography.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5728\u5236\u56fe\u548c\u5730\u56fe\u5b66\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u5305\u62ec\u5730\u56fe\u5236\u4f5c\u548c\u4f7f\u7528\u4e2d\u7684\u591a\u79cd\u8bbe\u8ba1\u51b3\u7b56\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u5176\u5c40\u9650\u6027\u548c\u4f26\u7406\u95ee\u9898\u3002", "motivation": "GenAI\u51ed\u501f\u5176\u4e16\u754c\u77e5\u8bc6\u3001\u521b\u9020\u6027\u548c\u591a\u6a21\u6001\u96c6\u6210\u80fd\u529b\uff0c\u4e3a\u5730\u56fe\u5b66\u548c\u5236\u56fe\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8fdb\u6b65\u673a\u4f1a\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff08\u5982\u7b26\u53f7\u5316\u3001\u5730\u56fe\u8bc4\u4f30\u548c\u5730\u56fe\u9605\u8bfb\uff09\u5206\u6790GenAI\u5982\u4f55\u4f18\u5316\u5236\u56fe\u8fc7\u7a0b\u3002", "result": "\u5c3d\u7ba1GenAI\u5728\u5236\u56fe\u4e2d\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u4e0d\u9002\u7528\u4e8e\u9700\u8981\u6df1\u539a\u5236\u56fe\u77e5\u8bc6\u6216\u9ad8\u7cbe\u5ea6\u7684\u4efb\u52a1\uff0c\u4e14\u9700\u5173\u6ce8\u4f26\u7406\u548c\u793e\u4f1a\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u4e3aGenAI\u4e0e\u5730\u56fe\u5b66\u7684\u4ea4\u53c9\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.09033", "pdf": "https://arxiv.org/pdf/2508.09033", "abs": "https://arxiv.org/abs/2508.09033", "authors": ["Tina Behzad", "Nikolos Gurney", "Ning Wang", "David V. Pynadath"], "title": "Beyond Predictions: A Study of AI Strength and Weakness Transparency Communication on Human-AI Collaboration", "categories": ["cs.HC"], "comment": "Accepted as LBW, HCII 2025", "summary": "The promise of human-AI teaming lies in humans and AI working together to\nachieve performance levels neither could accomplish alone. Effective\ncommunication between AI and humans is crucial for teamwork, enabling users to\nefficiently benefit from AI assistance. This paper investigates how AI\ncommunication impacts human-AI team performance. We examine AI explanations\nthat convey an awareness of its strengths and limitations. To achieve this, we\ntrain a decision tree on the model's mistakes, allowing it to recognize and\nexplain where and why it might err. Through a user study on an income\nprediction task, we assess the impact of varying levels of information and\nexplanations about AI predictions. Our results show that AI performance\ninsights enhance task performance, and conveying AI awareness of its strengths\nand weaknesses improves trust calibration. These findings highlight the\nimportance of considering how information delivery influences user trust and\nreliance in AI-assisted decision-making.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AI\u89e3\u91ca\u80fd\u529b\u5bf9\u4eba\u7c7b-AI\u56e2\u961f\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u53d1\u73b0AI\u5bf9\u5176\u4f18\u7f3a\u70b9\u7684\u81ea\u6211\u8ba4\u77e5\u80fd\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u548c\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7AI\u7684\u81ea\u6211\u8ba4\u77e5\u548c\u89e3\u91ca\u80fd\u529b\uff0c\u4f18\u5316\u4eba\u7c7b\u4e0eAI\u7684\u5408\u4f5c\u6548\u80fd\u3002", "method": "\u8bad\u7ec3\u51b3\u7b56\u6811\u8bc6\u522b\u6a21\u578b\u7684\u9519\u8bef\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u4e0d\u540c\u89e3\u91ca\u5bf9\u6536\u5165\u9884\u6d4b\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "AI\u5bf9\u5176\u4f18\u7f3a\u70b9\u7684\u89e3\u91ca\u80fd\u63d0\u5347\u4efb\u52a1\u8868\u73b0\uff0c\u5e76\u6539\u5584\u7528\u6237\u5bf9AI\u7684\u4fe1\u4efb\u6821\u51c6\u3002", "conclusion": "\u4fe1\u606f\u4f20\u9012\u65b9\u5f0f\u5bf9\u7528\u6237\u4fe1\u4efb\u548cAI\u8f85\u52a9\u51b3\u7b56\u7684\u4f9d\u8d56\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.08898", "pdf": "https://arxiv.org/pdf/2508.08898", "abs": "https://arxiv.org/abs/2508.08898", "authors": ["Federico Calandra", "Marco Bernardo", "Andrea Esposito", "Francesco Fabris"], "title": "Redactable Blockchains: An Overview", "categories": ["cs.CR", "cs.DC"], "comment": null, "summary": "Blockchains are widely recognized for their immutability, which provides\nrobust guarantees of data integrity and transparency. However, this same\nfeature poses significant challenges in real-world situations that require\nregulatory compliance, correction of erroneous data, or removal of sensitive\ninformation. Redactable blockchains address the limitations of traditional ones\nby enabling controlled, auditable modifications to blockchain data, primarily\nthrough cryptographic mechanisms such as chameleon hash functions and\nalternative redaction schemes. This report examines the motivations for\nintroducing redactability, surveys the cryptographic primitives that enable\nsecure edits, and analyzes competing approaches and their shortcomings. Special\nattention is paid to the practical deployment of redactable blockchains in\nprivate settings, with discussions of use cases in healthcare, finance,\nInternet of drones, and federated learning. Finally, the report outlines\nfurther challenges, also in connection with reversible computing, and the\nfuture potential of redactable blockchains in building law-compliant,\ntrustworthy, and scalable digital infrastructures.", "AI": {"tldr": "\u6458\u8981\u63a2\u8ba8\u4e86\u53ef\u7f16\u8f91\u533a\u5757\u94fe\u7684\u5fc5\u8981\u6027\u53ca\u5176\u5b9e\u73b0\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u9690\u79c1\u548c\u5408\u89c4\u9700\u6c42\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u533a\u5757\u94fe\u7684\u4e0d\u53ef\u7be1\u6539\u6027\u867d\u7136\u4fdd\u8bc1\u4e86\u6570\u636e\u5b89\u5168\uff0c\u4f46\u5728\u9700\u8981\u5408\u89c4\u6216\u4fee\u6b63\u9519\u8bef\u65f6\u5374\u5e26\u6765\u6311\u6218\u3002\u53ef\u7f16\u8f91\u533a\u5757\u94fe\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u53d8\u8272\u9f99\u54c8\u5e0c\u51fd\u6570\u548c\u5176\u4ed6\u52a0\u5bc6\u65b9\u6848\u5b9e\u73b0\u53ef\u63a7\u7684\u3001\u53ef\u5ba1\u8ba1\u7684\u6570\u636e\u4fee\u6539\u3002", "result": "\u62a5\u544a\u603b\u7ed3\u4e86\u53ef\u7f16\u8f91\u533a\u5757\u94fe\u7684\u52a0\u5bc6\u65b9\u6cd5\u53ca\u5176\u5728\u533b\u7597\u3001\u91d1\u878d\u7b49\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u53ef\u7f16\u8f91\u533a\u5757\u94fe\u6709\u671b\u6784\u5efa\u5408\u89c4\u3001\u53ef\u4fe1\u7684\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u53ef\u9006\u8ba1\u7b97\u7b49\u6311\u6218\u3002"}}
{"id": "2508.08775", "pdf": "https://arxiv.org/pdf/2508.08775", "abs": "https://arxiv.org/abs/2508.08775", "authors": ["Xutong Jin", "Guoping Wang", "Sheng Li"], "title": "SonicRadiation: A Hybrid Numerical Solution for Sound Radiation without Ghost Cells", "categories": ["cs.SD", "cs.GR", "cs.NA", "math.NA"], "comment": "11 pages", "summary": "Interactive synthesis of physical sound effects is crucial in digital media\nproduction. Sound radiation simulation, a key component of physically based\nsound synthesis, has posed challenges in the context of complex object\nboundaries. Previous methods, such as ghost cell-based finite-difference\ntime-domain (FDTD) wave solver, have struggled to address these challenges,\nleading to large errors and failures in complex boundaries because of the\nlimitation of ghost cells. We present SonicRadiation, a hybrid numerical\nsolution capable of handling complex and dynamic object boundaries in sound\nradiation simulation without relying on ghost cells. We derive a consistent\nformulation to connect the physical quantities on grid cells in FDTD with the\nboundary elements in the time-domain boundary element method (TDBEM). Hereby,\nwe propose a boundary grid synchronization strategy to seamlessly integrate\nTDBEM with FDTD while maintaining high numerical accuracy. Our method holds\nboth advantages from the accuracy of TDBEM for the near-field and the\nefficiency of FDTD for the far-field. Experimental results demonstrate the\nsuperiority of our method in sound radiation simulation over previous\napproaches in terms of accuracy and efficiency, particularly in complex scenes,\nfurther validating its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u4f9d\u8d56\u5e7d\u7075\u7ec6\u80de\u7684\u65b0\u65b9\u6cd5SonicRadiation\uff0c\u901a\u8fc7\u7ed3\u5408FDTD\u548cTDBEM\uff0c\u6709\u6548\u89e3\u51b3\u590d\u6742\u8fb9\u754c\u4e0b\u7684\u58f0\u8f90\u5c04\u6a21\u62df\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u7269\u4f53\u8fb9\u754c\u4e0b\u58f0\u8f90\u5c04\u6a21\u62df\u7684\u6311\u6218\uff0c\u63d0\u5347\u7269\u7406\u97f3\u6548\u5408\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faSonicRadiation\u6df7\u5408\u6570\u503c\u65b9\u6cd5\uff0c\u7ed3\u5408FDTD\u548cTDBEM\uff0c\u901a\u8fc7\u8fb9\u754c\u7f51\u683c\u540c\u6b65\u7b56\u7565\u5b9e\u73b0\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u65b0\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u517c\u987e\u8fd1\u573a\u7cbe\u5ea6\u548c\u8fdc\u573a\u6548\u7387\u3002", "conclusion": "SonicRadiation\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u8fb9\u754c\u7684\u6a21\u62df\u96be\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u6027\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.09043", "pdf": "https://arxiv.org/pdf/2508.09043", "abs": "https://arxiv.org/abs/2508.09043", "authors": ["Yanbing Chen", "Jonathan Nelson", "Bing Zhou", "Ryan Zhenqi Zhou", "Shan Ye", "Haokun Liu", "Zhining Gu", "Armita Kar", "Hoeyun Kwon", "Pengyu Chen", "Maoran Sun", "Yuhao Kang"], "title": "Where are GIScience Faculty Hired from? Analyzing Faculty Mobility and Research Themes Through Hiring Networks", "categories": ["cs.HC", "cs.CY", "cs.SI"], "comment": "54 pages, 12 figures", "summary": "Academia is profoundly influenced by faculty hiring networks, which serve as\ncritical conduits for knowledge dissemination and the formation of\ncollaborative research initiatives. While extensive research in various\ndisciplines has revealed the institutional hierarchies inherent in these\nnetworks, their impacts within GIScience remain underexplored. To fill this\ngap, this study analyzes the placement patterns of 946 GIScience faculty\nworldwide by mapping the connections between PhD-granting institutions and\ncurrent faculty affiliations. Our dataset, which is compiled from\nvolunteer-contributed information, is the most comprehensive collection\navailable in this field. While there may be some limitations in its\nrepresentativeness, its scope and depth provide a unique and valuable\nperspective on the global placement patterns of GIScience faculty. Our analysis\nreveals several influential programs in placing GIScience faculty, with hiring\nconcentrated in the western countries. We examined the diversity index to\nassess the representation of regions and institutions within the global\nGIScience faculty network. We observe significant internal retention at both\nthe continental and country levels, and a high level of non-self-hired ratio at\nthe institutional level. Over time, research themes have also evolved, with\ngrowing research clusters emphasis on spatial data analytics, cartography and\ngeovisualization, geocomputation, and environmental sciences, etc. These\nresults illuminate the influence of hiring practices on global knowledge\ndissemination and contribute to promoting academic equity within GIScience and\nGeography.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u5168\u7403946\u540dGIScience\u6559\u5e08\u7684\u5c31\u4e1a\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u897f\u65b9\u56fd\u5bb6\u548c\u90e8\u5206\u5b66\u672f\u673a\u6784\u5728\u6559\u5e08\u62db\u8058\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4ee5\u53ca\u7814\u7a76\u4e3b\u9898\u7684\u6f14\u53d8\u8d8b\u52bf\u3002", "motivation": "\u63a2\u7d22GIScience\u9886\u57df\u4e2d\u6559\u5e08\u62db\u8058\u7f51\u7edc\u5bf9\u77e5\u8bc6\u4f20\u64ad\u548c\u5b66\u672f\u5408\u4f5c\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u5fd7\u613f\u8005\u8d21\u732e\u7684\u6570\u636e\uff0c\u7ed8\u5236\u535a\u58eb\u5b66\u4f4d\u6388\u4e88\u673a\u6784\u4e0e\u5f53\u524d\u6559\u5e08\u4efb\u804c\u673a\u6784\u7684\u8054\u7cfb\u56fe\u3002", "result": "\u53d1\u73b0\u62db\u8058\u96c6\u4e2d\u5728\u897f\u65b9\u56fd\u5bb6\uff0c\u7814\u7a76\u4e3b\u9898\u9010\u6e10\u8f6c\u5411\u7a7a\u95f4\u6570\u636e\u5206\u6790\u3001\u5730\u56fe\u5b66\u4e0e\u5730\u7406\u53ef\u89c6\u5316\u3001\u5730\u7406\u8ba1\u7b97\u53ca\u73af\u5883\u79d1\u5b66\u7b49\u9886\u57df\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u62db\u8058\u5b9e\u8df5\u5bf9\u5168\u7403\u77e5\u8bc6\u4f20\u64ad\u7684\u5f71\u54cd\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdbGIScience\u548c\u5730\u7406\u5b66\u9886\u57df\u7684\u5b66\u672f\u516c\u5e73\u3002"}}
{"id": "2508.08268", "pdf": "https://arxiv.org/pdf/2508.08268", "abs": "https://arxiv.org/abs/2508.08268", "authors": ["Vaibhav Gupta", "Maria Maleshkova"], "title": "Evaluating Imputation Techniques for Short-Term Gaps in Heart Rate Data", "categories": ["stat.AP", "cs.HC", "cs.LG"], "comment": null, "summary": "Recent advances in wearable technology have enabled the continuous monitoring\nof vital physiological signals, essential for predictive modeling and early\ndetection of extreme physiological events. Among these physiological signals,\nheart rate (HR) plays a central role, as it is widely used in monitoring and\nmanaging cardiovascular conditions and detecting extreme physiological events\nsuch as hypoglycemia. However, data from wearable devices often suffer from\nmissing values. To address this issue, recent studies have employed various\nimputation techniques. Traditionally, the effectiveness of these methods has\nbeen evaluated using predictive accuracy metrics such as RMSE, MAPE, and MAE,\nwhich assess numerical proximity to the original data. While informative, these\nmetrics fail to capture the complex statistical structure inherent in\nphysiological signals. This study bridges this gap by presenting a\ncomprehensive evaluation of four statistical imputation methods, linear\ninterpolation, K Nearest Neighbors (KNN), Piecewise Cubic Hermite Interpolating\nPolynomial (PCHIP), and B splines, for short term HR data gaps. We assess their\nperformance using both predictive accuracy metrics and statistical distance\nmeasures, including the Cohen Distance Test (CDT) and Jensen Shannon Distance\n(JS Distance), applied to HR data from the D1NAMO dataset and the BIG IDEAs Lab\nGlycemic Variability and Wearable Device dataset. The analysis reveals\nlimitations in existing imputation approaches and the absence of a robust\nframework for evaluating imputation quality in physiological signals. Finally,\nthis study proposes a foundational framework to develop a composite evaluation\nmetric to assess imputation performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u53ef\u7a7f\u6234\u8bbe\u5907\u4e2d\u5fc3\u7387\u6570\u636e\u7684\u7f3a\u5931\u503c\u586b\u8865\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u7edf\u8ba1\u586b\u8865\u6280\u672f\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\u4e2d\u7f3a\u5931\u503c\u7684\u95ee\u9898\uff0c\u586b\u8865\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u751f\u7406\u4fe1\u53f7\u7684\u7edf\u8ba1\u7ed3\u6784\u3002", "method": "\u8bc4\u4f30\u4e86\u7ebf\u6027\u63d2\u503c\u3001K\u8fd1\u90bb\uff08KNN\uff09\u3001\u5206\u6bb5\u4e09\u6b21Hermite\u63d2\u503c\u591a\u9879\u5f0f\uff08PCHIP\uff09\u548cB\u6837\u6761\u56db\u79cd\u586b\u8865\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u6d4b\u51c6\u786e\u6027\u6307\u6807\u548c\u7edf\u8ba1\u8ddd\u79bb\u5ea6\u91cf\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u73b0\u6709\u586b\u8865\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u53ca\u7f3a\u4e4f\u5bf9\u751f\u7406\u4fe1\u53f7\u586b\u8865\u8d28\u91cf\u7684\u9c81\u68d2\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u6307\u6807\u7684\u57fa\u7840\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u586b\u8865\u6027\u80fd\u7684\u8bc4\u4f30\u3002"}}
{"id": "2508.08271", "pdf": "https://arxiv.org/pdf/2508.08271", "abs": "https://arxiv.org/abs/2508.08271", "authors": ["Victoria Williams", "Benjamin Rosman"], "title": "Heartificial Intelligence: Exploring Empathy in Language Models", "categories": ["cs.CL", "cs.HC"], "comment": "21 pages, 5 tables", "summary": "Large language models have become increasingly common, used by millions of\npeople worldwide in both professional and personal contexts. As these models\ncontinue to advance, they are frequently serving as virtual assistants and\ncompanions. In human interactions, effective communication typically involves\ntwo types of empathy: cognitive empathy (understanding others' thoughts and\nemotions) and affective empathy (emotionally sharing others' feelings). In this\nstudy, we investigated both cognitive and affective empathy across several\nsmall (SLMs) and large (LLMs) language models using standardized psychological\ntests. Our results revealed that LLMs consistently outperformed humans -\nincluding psychology students - on cognitive empathy tasks. However, despite\ntheir cognitive strengths, both small and large language models showed\nsignificantly lower affective empathy compared to human participants. These\nfindings highlight rapid advancements in language models' ability to simulate\ncognitive empathy, suggesting strong potential for providing effective virtual\ncompanionship and personalized emotional support. Additionally, their high\ncognitive yet lower affective empathy allows objective and consistent emotional\nsupport without running the risk of emotional fatigue or bias.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u5171\u60c5\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4f46\u5728\u60c5\u611f\u5171\u60c5\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u663e\u793a\u51fa\u4f5c\u4e3a\u865a\u62df\u4f34\u4fa3\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u548c\u60c5\u611f\u5171\u60c5\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4ee5\u8bc4\u4f30\u5176\u4f5c\u4e3a\u865a\u62df\u4f34\u4fa3\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u5fc3\u7406\u5b66\u6d4b\u8bd5\u5bf9\u6bd4\u5c0f\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u7684\u8868\u73b0\u3002", "result": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u5171\u60c5\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4f46\u5728\u60c5\u611f\u5171\u60c5\u4e0a\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u5171\u60c5\u4e0a\u7684\u4f18\u52bf\u4f7f\u5176\u9002\u5408\u63d0\u4f9b\u5ba2\u89c2\u7684\u865a\u62df\u966a\u4f34\uff0c\u4f46\u60c5\u611f\u5171\u60c5\u80fd\u529b\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u3002"}}
{"id": "2508.08282", "pdf": "https://arxiv.org/pdf/2508.08282", "abs": "https://arxiv.org/abs/2508.08282", "authors": ["Celina Kacperski", "Florian Kutzner"], "title": "Financial and symbolic incentives promote 'green' charging choices", "categories": ["physics.soc-ph", "cs.CY", "cs.HC"], "comment": "behavior steering techniques, sustainable behavior, e-mobility,\n  incentives", "summary": "Electromobility can contribute to a reduction in greenhouse gas emissions if\nusage behavior is aligned with the increasing availability of renewable energy.\nTo achieve this, smart navigation systems can be used to inform drivers of\noptimal charging times and locations. Yet, required flexibility may impart time\npenalties. We investigate the impact of financial and symbolic incentive\nschemes to counteract these additional costs. In a laboratory experiment with\nreal-life time costs, we find that monetary and symbolic incentives are both\neffective in changing behavior towards 'greener' charging choices, while we\nfind no significant statistical difference between them.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u91d1\u878d\u548c\u8c61\u5f81\u6027\u6fc0\u52b1\u63aa\u65bd\u4fc3\u8fdb\u7535\u52a8\u6c7d\u8f66\u7528\u6237\u9009\u62e9\u66f4\u73af\u4fdd\u7684\u5145\u7535\u884c\u4e3a\uff0c\u53d1\u73b0\u4e24\u8005\u5747\u6709\u6548\uff0c\u4f46\u65e0\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6fc0\u52b1\u63aa\u65bd\u5e73\u8861\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u884c\u4e3a\u7684\u7075\u6d3b\u6027\u9700\u6c42\u4e0e\u65f6\u95f4\u6210\u672c\uff0c\u4ee5\u51cf\u5c11\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u3002", "method": "\u5728\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u4e2d\u6a21\u62df\u771f\u5b9e\u65f6\u95f4\u6210\u672c\uff0c\u6d4b\u8bd5\u91d1\u878d\u548c\u8c61\u5f81\u6027\u6fc0\u52b1\u5bf9\u5145\u7535\u9009\u62e9\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e24\u79cd\u6fc0\u52b1\u63aa\u65bd\u5747\u80fd\u4fc3\u8fdb\u66f4\u73af\u4fdd\u7684\u5145\u7535\u884c\u4e3a\uff0c\u4f46\u4e24\u8005\u6548\u679c\u65e0\u663e\u8457\u7edf\u8ba1\u5dee\u5f02\u3002", "conclusion": "\u7ed3\u8bba\u662f\u91d1\u878d\u548c\u8c61\u5f81\u6027\u6fc0\u52b1\u5747\u53ef\u6709\u6548\u5f15\u5bfc\u7528\u6237\u884c\u4e3a\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316\u6fc0\u52b1\u7b56\u7565\u3002"}}
{"id": "2508.08313", "pdf": "https://arxiv.org/pdf/2508.08313", "abs": "https://arxiv.org/abs/2508.08313", "authors": ["Kevin Zheng", "Linda Huber", "Aaron Stark", "Nathan Kim", "Francesca Lameiro", "Wells Lucas Santo", "Shreya Chowdhary", "Eugene Kim", "Justine Zhang"], "title": "Resisting AI Solutionism through Workplace Collective Action", "categories": ["cs.CY", "cs.HC", "K.4.3; K.4.2; I.2.0"], "comment": "Presented at \"Resisting AI Solutionism: Where Do We Go From Here?\"\n  workshop at CHI '25", "summary": "In the face of increasing austerity and threats of AI-enabled labor\nreplacement at the University of Michigan, a group of workers and students have\ncoalesced around the project of \"AI resistance\" since Fall 2024. Forming a\ncross-departmental coalition including librarians, faculty, staff, graduate\nworkers, and undergraduate students, we have hosted a public workshop\nquestioning the techno-deterministic inevitability of AI use at the University\nand are working with other campus organizations to maintain an ongoing\norganizing space. This workshop submission incorporates our reflections thus\nfar on the strategies we've employed, the challenges to collective resistance,\nand our role as workers in resisting AI within the University. Our aim for this\nwork is to provide concrete inspiration for technologists, students, and staff\nlooking to resist AI techno-solutionism within their own universities.", "AI": {"tldr": "\u5bc6\u6b47\u6839\u5927\u5b66\u7684\u4e00\u7fa4\u5de5\u4eba\u548c\u5b66\u751f\u81ea2024\u5e74\u79cb\u5b63\u8d77\u8054\u5408\u53d1\u8d77\u201cAI\u62b5\u6297\u201d\u9879\u76ee\uff0c\u901a\u8fc7\u8de8\u90e8\u95e8\u8054\u76df\u7ec4\u7ec7\u516c\u5f00\u7814\u8ba8\u4f1a\uff0c\u8ba8\u8bbaAI\u4f7f\u7528\u7684\u6280\u672f\u51b3\u5b9a\u8bba\u4e0d\u53ef\u907f\u514d\u6027\uff0c\u5e76\u5206\u4eab\u4e86\u96c6\u4f53\u62b5\u6297\u7684\u7b56\u7565\u4e0e\u6311\u6218\u3002", "motivation": "\u9762\u5bf9\u65e5\u76ca\u4e25\u5cfb\u7684\u8d22\u653f\u7d27\u7f29\u548cAI\u53ef\u80fd\u66ff\u4ee3\u52b3\u52a8\u7684\u5a01\u80c1\uff0c\u5de5\u4eba\u548c\u5b66\u751f\u5e0c\u671b\u901a\u8fc7\u96c6\u4f53\u884c\u52a8\u62b5\u6297AI\u5728\u6821\u56ed\u4e2d\u7684\u6280\u672f\u51b3\u5b9a\u8bba\u503e\u5411\u3002", "method": "\u7ec4\u7ec7\u8de8\u90e8\u95e8\u8054\u76df\uff0c\u4e3e\u529e\u516c\u5f00\u7814\u8ba8\u4f1a\uff0c\u5e76\u4e0e\u5176\u4ed6\u6821\u56ed\u7ec4\u7ec7\u5408\u4f5c\uff0c\u5efa\u7acb\u4e00\u4e2a\u6301\u7eed\u7684\u96c6\u4f53\u884c\u52a8\u7a7a\u95f4\u3002", "result": "\u5206\u4eab\u4e86\u62b5\u6297\u7b56\u7565\u548c\u96c6\u4f53\u884c\u52a8\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u5176\u4ed6\u5927\u5b66\u7684\u6280\u672f\u4eba\u5458\u3001\u5b66\u751f\u548c\u5458\u5de5\u63d0\u4f9b\u62b5\u6297AI\u6280\u672f\u51b3\u5b9a\u8bba\u7684\u5177\u4f53\u7075\u611f\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u4e3a\u6821\u56ed\u5185\u7684AI\u62b5\u6297\u884c\u52a8\u63d0\u4f9b\u5b9e\u8df5\u7ecf\u9a8c\u548c\u542f\u53d1\u3002"}}
{"id": "2508.08507", "pdf": "https://arxiv.org/pdf/2508.08507", "abs": "https://arxiv.org/abs/2508.08507", "authors": ["Shaun Macdonald", "Salma ElSayed", "Mark McGill"], "title": "AZRA: Extending the Affective Capabilities of Zoomorphic Robots using Augmented Reality", "categories": ["cs.RO", "cs.HC"], "comment": "Companion of the 2025 ACM/IEEE International Conference on\n  Human-Robot Interaction (RO-MAN 2025)", "summary": "Zoomorphic robots could serve as accessible and practical alternatives for\nusers unable or unwilling to keep pets. However, their affective interactions\nare often simplistic and short-lived, limiting their potential for domestic\nadoption. In order to facilitate more dynamic and nuanced affective\ninteractions and relationships between users and zoomorphic robots we present\nAZRA, a novel augmented reality (AR) framework that extends the affective\ncapabilities of these robots without physical modifications. To demonstrate\nAZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays\n(face, light, sound, thought bubbles) and interaction modalities (voice, touch,\nproximity, gaze). Additionally, AZRA features a computational model of emotion\nto calculate the robot's emotional responses, daily moods, evolving personality\nand needs. We highlight how AZRA can be used for rapid participatory\nprototyping and enhancing existing robots, then discuss implications on future\nzoomorphic robot development.", "AI": {"tldr": "AZRA\u662f\u4e00\u4e2a\u901a\u8fc7AR\u589e\u5f3a\u62df\u52a8\u7269\u673a\u5668\u4eba\u60c5\u611f\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u65e0\u9700\u7269\u7406\u6539\u9020\u5373\u53ef\u6269\u5c55\u5176\u60c5\u611f\u80fd\u529b\uff0c\u5305\u62ec\u60c5\u7eea\u663e\u793a\u548c\u4e92\u52a8\u65b9\u5f0f\uff0c\u5e76\u91c7\u7528\u60c5\u611f\u8ba1\u7b97\u6a21\u578b\u4ee5\u6a21\u62df\u52a8\u6001\u60c5\u611f\u53cd\u5e94\u3002", "motivation": "\u62df\u52a8\u7269\u673a\u5668\u4eba\u5728\u60c5\u611f\u4ea4\u4e92\u4e0a\u901a\u5e38\u8fc7\u4e8e\u7b80\u5355\u4e14\u77ed\u6682\uff0c\u9650\u5236\u4e86\u5176\u5728\u5bb6\u5ead\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u52a8\u6001\u548c\u7ec6\u817b\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u63d0\u51faAZRA\u6846\u67b6\uff0c\u5229\u7528AR\u6280\u672f\u4e3a\u62df\u52a8\u7269\u673a\u5668\u4eba\u6dfb\u52a0\u60c5\u611f\u663e\u793a\uff08\u5982\u8868\u60c5\u3001\u706f\u5149\u3001\u58f0\u97f3\u3001\u601d\u7ef4\u6c14\u6ce1\uff09\u548c\u4ea4\u4e92\u65b9\u5f0f\uff08\u5982\u8bed\u97f3\u3001\u89e6\u6478\u3001\u63a5\u8fd1\u3001\u6ce8\u89c6\uff09\uff0c\u5e76\u91c7\u7528\u60c5\u611f\u8ba1\u7b97\u6a21\u578b\u6a21\u62df\u52a8\u6001\u60c5\u611f\u53cd\u5e94\u3002", "result": "AZRA\u6210\u529f\u6269\u5c55\u4e86\u62df\u52a8\u7269\u673a\u5668\u4eba\u7684\u60c5\u611f\u4ea4\u4e92\u80fd\u529b\uff0c\u5e76\u53ef\u7528\u4e8e\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u589e\u5f3a\u73b0\u6709\u673a\u5668\u4eba\u529f\u80fd\u3002", "conclusion": "AZRA\u4e3a\u62df\u52a8\u7269\u673a\u5668\u4eba\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u63a8\u52a8\u4e86\u66f4\u4e30\u5bcc\u7684\u4ea4\u4e92\u4f53\u9a8c\u3002"}}
{"id": "2508.08590", "pdf": "https://arxiv.org/pdf/2508.08590", "abs": "https://arxiv.org/abs/2508.08590", "authors": ["Yuxiao Wang", "Wolin Liang", "Yu Lei", "Weiying Xue", "Nan Zhuang", "Qi Liu"], "title": "QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Human-Object Interaction (HOI) detection aims to localize human-object pairs\nand recognize their interactions in images. Although DETR-based methods have\nrecently emerged as the mainstream framework for HOI detection, they still\nsuffer from a key limitation: Randomly initialized queries lack explicit\nsemantics, leading to suboptimal detection performance. To address this\nchallenge, we propose QueryCraft, a novel plug-and-play HOI detection framework\nthat incorporates semantic priors and guided feature learning through\ntransformer-based query initialization. Central to our approach is\n\\textbf{ACTOR} (\\textbf{A}ction-aware \\textbf{C}ross-modal\n\\textbf{T}ransf\\textbf{OR}mer), a cross-modal Transformer encoder that jointly\nattends to visual regions and textual prompts to extract action-relevant\nfeatures. Rather than merely aligning modalities, ACTOR leverages\nlanguage-guided attention to infer interaction semantics and produce\nsemantically meaningful query representations. To further enhance object-level\nquery quality, we introduce a \\textbf{P}erceptual \\textbf{D}istilled\n\\textbf{Q}uery \\textbf{D}ecoder (\\textbf{PDQD}), which distills object category\nawareness from a pre-trained detector to serve as object query initiation. This\ndual-branch query initialization enables the model to generate more\ninterpretable and effective queries for HOI detection. Extensive experiments on\nHICO-Det and V-COCO benchmarks demonstrate that our method achieves\nstate-of-the-art performance and strong generalization. Code will be released\nupon publication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQueryCraft\u7684HOI\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5148\u9a8c\u548c\u7279\u5f81\u5b66\u4e60\u6539\u8fdbDETR\u65b9\u6cd5\uff0c\u7ed3\u5408ACTOR\u548cPDQD\u6280\u672f\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "DETR\u65b9\u6cd5\u5728HOI\u68c0\u6d4b\u4e2d\u56e0\u67e5\u8be2\u521d\u59cb\u5316\u7f3a\u4e4f\u660e\u786e\u8bed\u4e49\u800c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faQueryCraft\u6846\u67b6\uff0c\u5f15\u5165ACTOR\uff08\u8de8\u6a21\u6001\u7f16\u7801\u5668\uff09\u63d0\u53d6\u52a8\u4f5c\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u7ed3\u5408PDQD\uff08\u611f\u77e5\u84b8\u998f\u67e5\u8be2\u89e3\u7801\u5668\uff09\u4f18\u5316\u67e5\u8be2\u521d\u59cb\u5316\u3002", "result": "\u5728HICO-Det\u548cV-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "QueryCraft\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u548c\u7279\u5f81\u84b8\u998f\uff0c\u663e\u8457\u63d0\u5347\u4e86HOI\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.08596", "pdf": "https://arxiv.org/pdf/2508.08596", "abs": "https://arxiv.org/abs/2508.08596", "authors": ["Galen Weld", "Carl Pearson", "Bradley Spahn", "Tim Althoff", "Amy X. Zhang", "Sanjay Kairam"], "title": "How Conversational Structure and Style Shape Online Community Experiences", "categories": ["cs.SI", "cs.HC"], "comment": "to appear at ICWSM 2026", "summary": "Sense of Community (SOC) is vital to individual and collective well-being.\nAlthough social interactions have moved increasingly online, still little is\nknown about the specific relationships between the nature of these interactions\nand Sense of Virtual Community (SOVC). This study addresses this gap by\nexploring how conversational structure and linguistic style predict SOVC in\nonline communities, using a large-scale survey of 2,826 Reddit users across 281\nvaried subreddits. We develop a hierarchical model to predict self-reported\nSOVC based on automatically quantifiable and highly generalizable features that\nare agnostic to community topic and that describe both individual users and\nentire communities. We identify specific interaction patterns (e.g., reciprocal\nreply chains, use of prosocial language) associated with stronger communities\nand identify three primary dimensions of SOVC within Reddit -- Membership &\nBelonging, Cooperation & Shared Values, and Connection & Influence. This study\nprovides the first quantitative evidence linking patterns of social interaction\nto SOVC and highlights actionable strategies for fostering stronger community\nattachment, using an approach that can generalize readily across community\ntopics, languages, and platforms. These insights offer theoretical implications\nfor the study of online communities and practical suggestions for the design of\nfeatures to help more individuals experience the positive benefits of online\ncommunity participation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u7ebf\u793e\u533a\u7684\u5bf9\u8bdd\u7ed3\u6784\u548c\u8bed\u8a00\u98ce\u683c\u5982\u4f55\u9884\u6d4b\u865a\u62df\u793e\u533a\u611f\uff08SOVC\uff09\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21Reddit\u7528\u6237\u8c03\u67e5\u63d0\u51fa\u4e86\u53ef\u63a8\u5e7f\u7684\u6a21\u578b\u3002", "motivation": "\u7406\u89e3\u5728\u7ebf\u793e\u4ea4\u4e92\u52a8\u4e0e\u865a\u62df\u793e\u533a\u611f\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u75282,826\u540dReddit\u7528\u6237\u7684\u6570\u636e\uff0c\u5f00\u53d1\u5206\u5c42\u6a21\u578b\uff0c\u91cf\u5316\u4e92\u52a8\u6a21\u5f0f\u548c\u8bed\u8a00\u98ce\u683c\u5bf9SOVC\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u7279\u5b9a\u4e92\u52a8\u6a21\u5f0f\uff08\u5982\u4e92\u60e0\u56de\u590d\u94fe\u3001\u4eb2\u793e\u4f1a\u8bed\u8a00\uff09\u4e0e\u66f4\u5f3a\u7684\u793e\u533a\u611f\u76f8\u5173\uff0c\u5e76\u8bc6\u522b\u4e86SOVC\u7684\u4e09\u4e2a\u4e3b\u8981\u7ef4\u5ea6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5728\u7ebf\u793e\u533a\u4e92\u52a8\u4e0eSOVC\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u5b9a\u91cf\u8bc1\u636e\uff0c\u5e76\u4e3a\u589e\u5f3a\u793e\u533a\u5f52\u5c5e\u611f\u63d0\u51fa\u4e86\u5b9e\u9645\u5efa\u8bae\u3002"}}
{"id": "2508.08767", "pdf": "https://arxiv.org/pdf/2508.08767", "abs": "https://arxiv.org/abs/2508.08767", "authors": ["Kazuki Komura", "Kumi Ozaki", "Seiji Yamada"], "title": "Robot can reduce superior's dominance in group discussions with human social hierarchy", "categories": ["cs.RO", "cs.HC"], "comment": "8 pages, 7 figures. International Conference on Human-Agent\n  Interaction (HAI '24), November 24-27, 2024, Swansea, United Kingdom", "summary": "This study investigated whether robotic agents that deal with social\nhierarchical relationships can reduce the dominance of superiors and equalize\nparticipation among participants in discussions with hierarchical structures.\nThirty doctors and students having hierarchical relationship were gathered as\nparticipants, and an intervention experiment was conducted using a robot that\ncan encourage participants to speak depending on social hierarchy. These were\ncompared with strategies that intervened equally for all participants without\nconsidering hierarchy and with a no-action. The robots performed follow\nactions, showing backchanneling to speech, and encourage actions, prompting\nspeech from members with less speaking time, on the basis of the hierarchical\nrelationships among group members to equalize participation. The experimental\nresults revealed that the robot's actions could potentially influence the\nspeaking time among members, but it could not be conclusively stated that there\nwere significant differences between the robot's action conditions. However,\nthe results suggested that it might be possible to influence speaking time\nwithout decreasing the satisfaction of superiors. This indicates that in\ndiscussion scenarios where experienced superiors are likely to dominate,\ncontrolling the robot's backchanneling behavior could potentially suppress\ndominance and equalize participation among group members.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u673a\u5668\u4eba\u4ee3\u7406\u662f\u5426\u80fd\u901a\u8fc7\u793e\u4ea4\u5c42\u7ea7\u5173\u7cfb\u51cf\u5c11\u4e0a\u7ea7\u4e3b\u5bfc\u5e76\u5e73\u7b49\u5316\u8ba8\u8bba\u53c2\u4e0e\u3002\u5b9e\u9a8c\u663e\u793a\u673a\u5668\u4eba\u884c\u4e3a\u53ef\u80fd\u5f71\u54cd\u53d1\u8a00\u65f6\u95f4\uff0c\u4f46\u672a\u80fd\u663e\u8457\u8bc1\u660e\u5176\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u793e\u4ea4\u5c42\u7ea7\u8ba8\u8bba\u4e2d\u4e0a\u7ea7\u4e3b\u5bfc\u95ee\u9898\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u5e72\u9884\u5e73\u7b49\u5316\u53c2\u4e0e\u3002", "method": "30\u540d\u533b\u5b66\u751f\u53c2\u4e0e\u5c42\u7ea7\u5b9e\u9a8c\uff0c\u673a\u5668\u4eba\u6839\u636e\u5c42\u7ea7\u9f13\u52b1\u53d1\u8a00\uff0c\u4e0e\u65e0\u5e72\u9884\u548c\u5747\u7b49\u5e72\u9884\u5bf9\u6bd4\u3002", "result": "\u673a\u5668\u4eba\u884c\u4e3a\u53ef\u80fd\u5f71\u54cd\u53d1\u8a00\u65f6\u95f4\uff0c\u4f46\u65e0\u663e\u8457\u5dee\u5f02\uff1b\u4e0a\u7ea7\u6ee1\u610f\u5ea6\u672a\u4e0b\u964d\u3002", "conclusion": "\u673a\u5668\u4eba\u53ef\u80fd\u901a\u8fc7\u63a7\u5236\u53cd\u9988\u884c\u4e3a\u6291\u5236\u4e0a\u7ea7\u4e3b\u5bfc\u5e76\u5e73\u7b49\u5316\u8ba8\u8bba\u53c2\u4e0e\u3002"}}
{"id": "2508.08805", "pdf": "https://arxiv.org/pdf/2508.08805", "abs": "https://arxiv.org/abs/2508.08805", "authors": ["Liam Pram", "Fabio Morreale"], "title": "Opening Musical Creativity? Embedded Ideologies in Generative-AI Music Systems", "categories": ["cs.SD", "cs.AI", "cs.HC"], "comment": "Extended version of the presentation at The First International\n  Conference in AI Music Studies 2024", "summary": "AI systems for music generation are increasingly common and easy to use,\ngranting people without any musical background the ability to create music.\nBecause of this, generative-AI has been marketed and celebrated as a means of\ndemocratizing music making. However, inclusivity often functions as marketable\nrhetoric rather than a genuine guiding principle in these industry settings. In\nthis paper, we look at four generative-AI music making systems available to the\npublic as of mid-2025 (AIVA, Stable Audio, Suno, and Udio) and track how they\nare rhetoricized by their developers, and received by users. Our aim is to\ninvestigate ideologies that are driving the early-stage development and\nadoption of generative-AI in music making, with a particular focus on\ndemocratization. A combination of autoethnography and digital ethnography is\nused to examine patterns and incongruities in rhetoric when positioned against\nproduct functionality. The results are then collated to develop a nuanced,\ncontextual discussion. The shared ideology we map between producers and\nconsumers is individualist, globalist, techno-liberal, and ethically evasive.\nIt is a 'total ideology' which obfuscates individual responsibility, and\nthrough which the nature of music and musical practice is transfigured to suit\ngenerative outcomes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u97f3\u4e50\u521b\u4f5c\u7cfb\u7edf\u7684\u5e02\u573a\u5ba3\u4f20\u4e0e\u5b9e\u9645\u7528\u6237\u63a5\u53d7\u7684\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u5176\u4e2d\u7684\u610f\u8bc6\u5f62\u6001\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u2018\u6c11\u4e3b\u5316\u2019\u7684\u865a\u5047\u6027\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u97f3\u4e50\u7cfb\u7edf\u5728\u5e02\u573a\u5ba3\u4f20\u4e2d\u7684\u2018\u6c11\u4e3b\u5316\u2019\u53e3\u53f7\u4e0e\u5b9e\u9645\u529f\u80fd\u548c\u7528\u6237\u63a5\u53d7\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\uff0c\u63ed\u793a\u80cc\u540e\u7684\u610f\u8bc6\u5f62\u6001\u9a71\u52a8\u3002", "method": "\u7ed3\u5408\u81ea\u6c11\u65cf\u5fd7\u548c\u6570\u5b57\u6c11\u65cf\u5fd7\u65b9\u6cd5\uff0c\u5206\u6790\u56db\u79cd\u516c\u5f00\u7684\u751f\u6210\u5f0fAI\u97f3\u4e50\u7cfb\u7edf\uff08AIVA\u3001Stable Audio\u3001Suno\u548cUdio\uff09\u7684\u5f00\u53d1\u8005\u548c\u7528\u6237\u8a00\u8c08\u3002", "result": "\u53d1\u73b0\u5f00\u53d1\u8005\u548c\u7528\u6237\u5171\u4eab\u7684\u610f\u8bc6\u5f62\u6001\u662f\u4e2a\u4eba\u4e3b\u4e49\u3001\u5168\u7403\u4e3b\u4e49\u3001\u6280\u672f\u81ea\u7531\u4e3b\u4e49\u548c\u4f26\u7406\u9003\u907f\uff0c\u8fd9\u79cd\u2018\u603b\u4f53\u610f\u8bc6\u5f62\u6001\u2019\u6a21\u7cca\u4e86\u4e2a\u4eba\u8d23\u4efb\uff0c\u5e76\u91cd\u5851\u4e86\u97f3\u4e50\u5b9e\u8df5\u7684\u672c\u8d28\u4ee5\u9002\u5e94\u751f\u6210\u6280\u672f\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u97f3\u4e50\u7cfb\u7edf\u7684\u2018\u6c11\u4e3b\u5316\u2019\u5ba3\u4f20\u66f4\u591a\u662f\u5e02\u573a\u7b56\u7565\u800c\u975e\u771f\u5b9e\u539f\u5219\uff0c\u5176\u80cc\u540e\u7684\u610f\u8bc6\u5f62\u6001\u5bf9\u97f3\u4e50\u5b9e\u8df5\u7684\u672c\u8d28\u4ea7\u751f\u4e86\u6df1\u8fdc\u5f71\u54cd\u3002"}}
{"id": "2508.08987", "pdf": "https://arxiv.org/pdf/2508.08987", "abs": "https://arxiv.org/abs/2508.08987", "authors": ["Ding Xia", "Naoto Inoue", "Qianru Qiu", "Kotaro Kikuchi"], "title": "ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation", "categories": ["cs.CV", "cs.HC"], "comment": "Accepted to ICDAR2025", "summary": "Colors play a crucial role in the design of vector graphic documents by\nenhancing visual appeal, facilitating communication, improving usability, and\nensuring accessibility. In this context, color recommendation involves\nsuggesting appropriate colors to complete or refine a design when one or more\ncolors are missing or require alteration. Traditional methods often struggled\nwith these challenges due to the complex nature of color design and the limited\ndata availability. In this study, we explored the use of pretrained Large\nLanguage Models (LLMs) and their commonsense reasoning capabilities for color\nrecommendation, raising the question: Can pretrained LLMs serve as superior\ndesigners for color recommendation tasks? To investigate this, we developed a\nrobust, rigorously validated pipeline, ColorGPT, that was built by\nsystematically testing multiple color representations and applying effective\nprompt engineering techniques. Our approach primarily targeted color palette\ncompletion by recommending colors based on a set of given colors and\naccompanying context. Moreover, our method can be extended to full palette\ngeneration, producing an entire color palette corresponding to a provided\ntextual description. Experimental results demonstrated that our LLM-based\npipeline outperformed existing methods in terms of color suggestion accuracy\nand the distribution of colors in the color palette completion task. For the\nfull palette generation task, our approach also yielded improvements in color\ndiversity and similarity compared to current techniques.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u989c\u8272\u63a8\u8350\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86ColorGPT\u7ba1\u9053\uff0c\u5728\u989c\u8272\u63a8\u8350\u548c\u8c03\u8272\u677f\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u989c\u8272\u5728\u77e2\u91cf\u56fe\u5f62\u8bbe\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u56e0\u6570\u636e\u6709\u9650\u548c\u590d\u6742\u6027\u96be\u4ee5\u6709\u6548\u63a8\u8350\u989c\u8272\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1LLM\u80fd\u5426\u6210\u4e3a\u66f4\u597d\u7684\u989c\u8272\u63a8\u8350\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5\u591a\u79cd\u989c\u8272\u8868\u793a\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u5f00\u53d1\u4e86ColorGPT\u7ba1\u9053\uff0c\u7528\u4e8e\u989c\u8272\u8c03\u8272\u677f\u8865\u5168\u548c\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u7684\u5b8c\u6574\u8c03\u8272\u677f\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cColorGPT\u5728\u989c\u8272\u63a8\u8350\u51c6\u786e\u6027\u548c\u5206\u5e03\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u989c\u8272\u591a\u6837\u6027\u548c\u76f8\u4f3c\u6027\u4e0a\u4e5f\u6709\u6240\u63d0\u5347\u3002", "conclusion": "LLM\u53ef\u4f5c\u4e3a\u9ad8\u6548\u7684\u989c\u8272\u63a8\u8350\u5de5\u5177\uff0cColorGPT\u5728\u989c\u8272\u8bbe\u8ba1\u4e2d\u5c55\u73b0\u4e86\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.08999", "pdf": "https://arxiv.org/pdf/2508.08999", "abs": "https://arxiv.org/abs/2508.08999", "authors": ["Chao Wang", "Michael Gienger", "Fan Zhang"], "title": "Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality", "categories": ["cs.RO", "cs.HC"], "comment": "4", "summary": "Expressive behaviors in robots are critical for effectively conveying their\nemotional states during interactions with humans. In this work, we present a\nframework that autonomously generates realistic and diverse robotic emotional\nexpressions based on expert human demonstrations captured in Mixed Reality\n(MR). Our system enables experts to teleoperate a virtual robot from a\nfirst-person perspective, capturing their facial expressions, head movements,\nand upper-body gestures, and mapping these behaviors onto corresponding robotic\ncomponents including eyes, ears, neck, and arms. Leveraging a\nflow-matching-based generative process, our model learns to produce coherent\nand varied behaviors in real-time in response to moving objects, conditioned\nexplicitly on given emotional states. A preliminary test validated the\neffectiveness of our approach for generating autonomous expressions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7\u6df7\u5408\u73b0\u5b9e\u6355\u6349\u4eba\u7c7b\u4e13\u5bb6\u6f14\u793a\uff0c\u5b9e\u65f6\u751f\u6210\u591a\u6837\u5316\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\u7684\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u4eba\u8868\u8fbe\u884c\u4e3a\u5bf9\u4f20\u8fbe\u60c5\u611f\u72b6\u6001\u7684\u91cd\u8981\u6027\u3002", "method": "\u5229\u7528\u4e13\u5bb6\u5728\u6df7\u5408\u73b0\u5b9e\u4e2d\u64cd\u4f5c\u865a\u62df\u673a\u5668\u4eba\uff0c\u6355\u6349\u5176\u9762\u90e8\u8868\u60c5\u3001\u5934\u90e8\u52a8\u4f5c\u548c\u4e0a\u534a\u8eab\u624b\u52bf\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\u5b9e\u65f6\u751f\u6210\u673a\u5668\u4eba\u884c\u4e3a\u3002", "result": "\u521d\u6b65\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u751f\u6210\u81ea\u4e3b\u8868\u8fbe\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\uff0c\u589e\u5f3a\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u60c5\u611f\u4f20\u8fbe\u6548\u679c\u3002"}}
