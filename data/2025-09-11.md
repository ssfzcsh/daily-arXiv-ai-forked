<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.HC](#cs.HC) [Total: 15]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [quant-ph](#quant-ph) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [ChatGPT for Code Refactoring: Analyzing Topics, Interaction, and Effective Prompts](https://arxiv.org/abs/2509.08090)
*Eman Abdullah AlOmar,Luo Xu,Sofia Martinez,Anthony Peruma,Mohamed Wiem Mkaouer,Christian D. Newman,Ali Ouni*

Main category: cs.SE

TL;DR: 该论文研究了开发者与ChatGPT在代码重构方面的互动，分析了715个重构相关对话，以了解开发者表达需求的方式及ChatGPT的回应。


<details>
  <summary>Details</summary>
Motivation: 探讨开发者如何在与ChatGPT交互中表达重构需求，填补现有研究的空白。

Method: 通过文本挖掘分析29,778条ChatGPT对话中的715个重构相关互动，并解析开发者的明确重构意图。

Result: 揭示了开发者标识代码改进区域的方式及ChatGPT如何满足这些需求。

Conclusion: 研究增进了对开发者与LLMs在重构任务中交互的理解。

Abstract: Large Language Models (LLMs), such as ChatGPT, have become widely popular and
widely used in various software engineering tasks such as refactoring, testing,
code review, and program comprehension. Although recent studies have examined
the effectiveness of LLMs in recommending and suggesting refactoring, there is
a limited understanding of how developers express their refactoring needs when
interacting with ChatGPT. In this paper, our goal is to explore interactions
related to refactoring between developers and ChatGPT to better understand how
developers identify areas for improvement in code, and how ChatGPT addresses
developers' needs. Our approach involves text mining 715 refactoring-related
interactions from 29,778 ChatGPT prompts and responses, as well as the analysis
of developers' explicit refactoring intentions.

</details>


### [2] [Safety Factories -- a Manifesto](https://arxiv.org/abs/2509.08285)
*Carmen Cârlan,Daniel Ratiu,Michael Wagner*

Main category: cs.SE

TL;DR: 现代信息物理系统由复杂软件操作，安全性需求日益增加。需要在软件开发与安全工程之间建立桥梁，通过形式化方法和自动化工具提升安全性。


<details>
  <summary>Details</summary>
Motivation: 软件在现代系统中的重要性增加，但其快速迭代和交付功能的需求与安全工程的严格性之间存在矛盾。

Method: 提出安全工厂的概念，将安全工具和方法集成到软件开发流程中，利用形式化模型和自动化检查提升安全性。

Result: 通过安全工厂的整合，可以实现软件开发与安全工程的协同，提升系统安全性。

Conclusion: 提倡借鉴软件开发的最佳实践，构建安全工厂以弥合软件开发与安全工程之间的差距。

Abstract: Modern cyber-physical systems are operated by complex software that
increasingly takes over safety-critical functions. Software enables rapid
iterations and continuous delivery of new functionality that meets the
ever-changing expectations of users. As high-speed development requires
discipline, rigor, and automation, software factories are used. These entail
methods and tools used for software development, such as build systems and
pipelines. To keep up with the rapid evolution of software, we need to bridge
the disconnect in methods and tools between software development and safety
engineering today. We need to invest more in formality upfront - capturing
safety work products in semantically rich models that are machine-processable,
defining automatic consistency checks, and automating the generation of
documentation - to benefit later. Transferring best practices from software to
safety engineering is worth exploring. We advocate for safety factories, which
integrate safety tooling and methods into software development pipelines.

</details>


### [3] [The Impact of Team Diversity in Agile Development Education](https://arxiv.org/abs/2509.08389)
*Marco Torchiano,Riccardo Coppola,Antonio Vetro',Xhoi Musaj*

Main category: cs.SE

TL;DR: 研究了软件工程团队中性别和国籍多样性对项目成功的影响，发现性别多样性有正面作用，国籍多样性影响不大，但两者结合可能带来负面影响。


<details>
  <summary>Details</summary>
Motivation: 探讨团队多样性（尤其是性别和国籍）在敏捷软件开发课程中对项目成果的影响，以促进平等和效率。

Method: 分析了51个团队三个学年的数据，使用性别、国籍及其共存的多样性指数评估项目质量。

Result: 性别多样性与项目成功有显著正相关；国籍多样性影响微弱负面；两者共存可能导致负面影响。

Conclusion: 多样性不会损害团队表现，但需考虑多种维度及其交互作用。

Abstract: Software Engineering is mostly a male-dominated sector, where gender
diversity is a key feature for improving equality of opportunities,
productivity, and innovation. Other diversity aspects, including but not
limited to nationality and ethnicity, are often understudied.In this work we
aim to assess the impact of team diversity, focusing mainly on gender and
nationality, in the context of an agile software development project-based
course. We analyzed 51 teams over three academic years, measuring three
different Diversity indexes - regarding Gender, Nationality and their
co-presence - to examine how different aspects of diversity impact the quality
of team project outcomes.Statistical analysis revealed a moderate,
statistically significant correlation between gender diversity and project
success, aligning with existing literature. Diversity in nationality showed a
negative but negligible effect on project results, indicating that promoting
these aspects does not harm students' performance. Analyzing their co-presence
within a team, gender and nationality combined had a negative impact, likely
due to increased communication barriers and differing cultural norms.This study
underscores the importance of considering multiple diversity dimensions and
their interactions in educational settings. Our findings, overall, show that
promoting diversity in teams does not negatively impact their performance and
achievement of educational goals.

</details>


### [4] [AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution](https://arxiv.org/abs/2509.08524)
*Felix Mächtle,Nils Loose,Jan-Niclas Serr,Jonas Sander,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: 论文提出了一种名为AutoStub的新方法，通过遗传编程自动生成外部函数的符号存根，解决了符号执行中外部函数处理的难题。


<details>
  <summary>Details</summary>
Motivation: 符号执行在软件测试中作用重大，但遇到外部函数时（如本地方法或第三方库）存在局限性。现有方法常需额外上下文、昂贵SMT求解器或人工干预。

Method: AutoStub利用遗传编程自动生成符号存根。通过随机输入执行外部函数并收集输出数据，生成近似行为的表达式作为存根，从而无需人工干预。

Result: 实验表明，AutoStub能准确近似55%的外部函数（精度超90%），并揭示语言特定行为的边缘案例。

Conclusion: AutoStub显著提升了符号执行的自动化能力，探索了以往难以处理的程序路径。

Abstract: Symbolic execution is a powerful technique for software testing, but suffers
from limitations when encountering external functions, such as native methods
or third-party libraries. Existing solutions often require additional context,
expensive SMT solvers, or manual intervention to approximate these functions
through symbolic stubs. In this work, we propose a novel approach to
automatically generate symbolic stubs for external functions during symbolic
execution that leverages Genetic Programming. When the symbolic executor
encounters an external function, AutoStub generates training data by executing
the function on randomly generated inputs and collecting the outputs. Genetic
Programming then derives expressions that approximate the behavior of the
function, serving as symbolic stubs. These automatically generated stubs allow
the symbolic executor to continue the analysis without manual intervention,
enabling the exploration of program paths that were previously intractable. We
demonstrate that AutoStub can automatically approximate external functions with
over 90% accuracy for 55% of the functions evaluated, and can infer
language-specific behaviors that reveal edge cases crucial for software
testing.

</details>


### [5] [Beyond the Binary: The System of All-round Evaluation of Research and Its Practices in China](https://arxiv.org/abs/2509.08546)
*Yu Zhu,Jiyuan Ye*

Main category: cs.SE

TL;DR: 本文提出了一种全面的科研评价系统SAER，旨在解决全球科研评价体系中定性方法与定量方法的二元对立问题。


<details>
  <summary>Details</summary>
Motivation: 全球科研评价体系缺乏宏观系统性评价理论，阻碍了改革进程。本文通过回顾科研评价的历史发展，指出当前评价实践中存在的二元对立问题。

Method: 引入SAER框架，整合形式、内容和效用评价，包含六个关键要素，提出三重评价维度与六要素的结合，以解决二元对立问题。

Result: SAER为全球科研评价改革提供了理论基础，结合了中国科研评价理论的辩证智慧。

Conclusion: SAER系统为全球科研评价体系的改革与进步提供了有价值的见解和参考。

Abstract: The lack of a macro-level, systematic evaluation theory to guide the
implementation of evaluation practices has become a key bottleneck in the
reform of global research evaluation systems. By reviewing the historical
development of research evaluation, this paper highlights the current binary
opposition between qualitative and quantitative methods in evaluation
practices. This paper introduces the System of All-round Evaluation of Research
(SAER), a framework that integrates form, content, and utility evaluations with
six key elements. SAER offers a theoretical breakthrough by transcending the
binary, providing a comprehensive foundation for global evaluation reforms. The
comprehensive system proposes a trinity of three evaluation dimensions,
combined with six evaluation elements, which would help academic evaluators and
researchers reconcile binary oppositions in evaluation methods. The system
highlights the dialectical wisdom and experience embedded in Chinese research
evaluation theory, offering valuable insights and references for the reform and
advancement of global research evaluation systems.

</details>


### [6] [Minimal Data, Maximum Clarity: A Heuristic for Explaining Optimization](https://arxiv.org/abs/2509.08667)
*Amirali Rayegan,Tim Menzies*

Main category: cs.SE

TL;DR: EZR框架通过主动采样和决策树解释，用更少的数据实现高效、透明的多目标优化。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程中配置空间大且标注成本高的问题。

Method: 结合主动学习和朴素贝叶斯采样，构建可解释的决策树模型。

Result: 在60个数据集上实现90%以上优化性能，解释能力优于LIME等方法。

Conclusion: 更少但高质量的数据能实现高效优化和透明解释。

Abstract: Efficient, interpretable optimization is a critical but underexplored
challenge in software engineering, where practitioners routinely face vast
configuration spaces and costly, error-prone labeling processes. This paper
introduces EZR, a novel and modular framework for multi-objective optimization
that unifies active sampling, learning, and explanation within a single,
lightweight pipeline. Departing from conventional wisdom, our Maximum Clarity
Heuristic demonstrates that using less (but more informative) data can yield
optimization models that are both effective and deeply understandable. EZR
employs an active learning strategy based on Naive Bayes sampling to
efficiently identify high-quality configurations with a fraction of the labels
required by fully supervised approaches. It then distills optimization logic
into concise decision trees, offering transparent, actionable explanations for
both global and local decision-making. Extensive experiments across 60
real-world datasets establish that EZR reliably achieves over 90% of the
best-known optimization performance in most cases, while providing clear,
cohort-based rationales that surpass standard attribution-based explainable AI
(XAI) methods (LIME, SHAP, BreakDown) in clarity and utility. These results
endorse "less but better"; it is both possible and often preferable to use
fewer (but more informative) examples to generate label-efficient optimization
and explanations in software systems. To support transparency and
reproducibility, all code and experimental materials are publicly available at
https://github.com/amiiralii/Minimal-Data-Maximum-Clarity.

</details>


### [7] [SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across Repositories](https://arxiv.org/abs/2509.08724)
*Junhao Wang,Daoguang Zan,Shulin Xin,Siyao Liu,Yurong Wu,Kai Shen*

Main category: cs.SE

TL;DR: SWE-Mirror利用GitHub问题解决历史和现有Gym环境构建大规模可验证任务数据集，提升问题解决能力并实现SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 解决创建大规模可验证训练数据集的难题，并充分利用现有Gym环境和GitHub问题解决历史。

Method: 通过SWE-Mirror管道提炼问题的语义本质，将其映射到配置好的Gym环境中，生成可验证任务。

Result: 应用SWE-Mirror生成了60,671个任务，训练模型在问题解决能力上显著提升，并在OpenHands框架上达到SOTA。

Conclusion: SWE-Mirror为构建可验证数据集提供了高效方法，显著提升了模型的性能。

Abstract: Creating large-scale verifiable training datasets for issue-resolving tasks
is a critical yet notoriously difficult challenge. Existing methods on
automating the Gym environment setup process for real-world issues suffer from
low success rates and high overhead. Meanwhile, synthesizing new tasks within
existing Gym environments leaves the vast pool of authentic, human-reported
problems untapped. To maximize the utilization of existing Gym environments and
also the rich data of issue-resolving history on GitHub, we introduce
SWE-Mirror, a pipeline that distills a real-world issue's semantic essence,
mirrors it into another repository with a configured Gym environment, and
re-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing
Gym environments along with the vast pool of issue-resolving history hosted on
GitHub to construct a large-scale dataset of mirrored authentic and verifiable
tasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have
curated a dataset with 60,671 issue-resolving tasks and demonstrated the value
of our dataset by training and evaluating coding agents at various scale.
Post-training experiments show that models trained with the dataset exhibit
improvements in issue-resolving capabilities. Furthermore, by extending the
dataset size to over 12,000 high-quality trajectories, we established a new
state-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the
OpenHands agent framework, which increases the resolve rate on
SWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and
validates the effectiveness of our approach.

</details>


### [8] [Handling Open-Vocabulary Constructs in Formalizing Specifications: Retrieval-Augmented Parsing with Expert Knowledge](https://arxiv.org/abs/2509.08808)
*Mohammad Saqib Hasan,Sayontan Ghosh,Dhruv Verma,Geoff Kuenning,Erez Zadok,Scott A. Smolka,Niranjan Balasubramanian*

Main category: cs.SE

TL;DR: 研究开放词汇构造（OVCs）在自然语言转形式语言中的问题，提出动态知识增强解析（DKAP）和检索增强解析方法ROLex，通过动态增长的专家知识改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 开放词汇构造（OVCs）在自然语言转形式语言任务中表现不佳，因缺乏先验知识。专家在推理时可提供正确构造，目标是高效利用这些知识且无需重新训练模型。

Method: 提出DKAP，通过动态增长的知识词典（NL短语与OVCs关联）辅助解析；开发ROLex方法，结合检索器和生成器利用知识词典生成正确解析；使用合成数据和增强技术训练模型。

Result: 在NL2LTL、NL2Code和NL2CMD三个任务中评估，ROLex有效利用了动态专家知识，提升了基线模型的性能。

Conclusion: DKAP问题是挑战，但ROLex通过动态知识增强显著改善了模型表现。

Abstract: We study the problem of Open-Vocabulary Constructs(OVCs) -- ones not known
beforehand -- in the context of converting natural language (NL) specifications
into formal languages (e.g., temporal logic or code). Models fare poorly on
OVCs due to a lack of necessary knowledge a priori. In such situations, a
domain expert can provide correct constructs at inference time based on their
preferences or domain knowledge. Our goal is to effectively reuse this
inference-time, expert-provided knowledge for future parses without retraining
the model. We present dynamic knowledge-augmented parsing(DKAP), where in
addition to the input sentence, the model receives (dynamically growing) expert
knowledge as a key-value lexicon that associates NL phrases with correct OVC
constructs. We propose ROLex, a retrieval-augmented parsing approach that uses
this lexicon. A retriever and a generator are trained to find and use the
key-value store to produce the correct parse. A key challenge lies in curating
data for this retrieval-augmented parser. We utilize synthetic data generation
and the data augmentation techniques on annotated (NL sentence, FL statement)
pairs to train the augmented parser. To improve training effectiveness, we
propose multiple strategies to teach models to focus on the relevant subset of
retrieved knowledge. Finally, we introduce a new evaluation paradigm modeled
after the DKAP problem and simulate the scenario across three formalization
tasks (NL2LTL, NL2Code, and NL2CMD). Our evaluations show that DKAP is a
difficult challenge, and ROLex helps improve the performance of baseline models
by using dynamic expert knowledge effectively.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols](https://arxiv.org/abs/2509.08182)
*Faruk Alpay,Taylan Alpay*

Main category: cs.PL

TL;DR: 摘要提出了一种基于XML标签的结构化提示方法，通过逻辑优先处理统一了语法约束解码、层次提示的固定点语义以及人类-AI交互循环，并形式化了XML树的完备格。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLM）在现实系统中输出可解析且符合模式的问题，研究提出了结构化提示的方法。

Method: 通过形式化XML树的完备格和固定点语义，结合语法约束解码和人类-AI交互循环，证明了单调提示操作的最小固定点。

Result: 理论证明了XML树的完备格和收敛性，并通过上下文无关文法（CFG）确保了输出结构的良好性。

Conclusion: 该方法通过与语法对齐解码、验证链和程序化提示的结合，展示了多层次的实用部署模式。

Abstract: Structured prompting with XML tags has emerged as an effective way to steer
large language models (LLMs) toward parseable, schema-adherent outputs in
real-world systems. We develop a logic-first treatment of XML prompting that
unifies (i) grammar-constrained decoding, (ii) fixed-point semantics over
lattices of hierarchical prompts, and (iii) convergent human-AI interaction
loops. We formalize a complete lattice of XML trees under a refinement order
and prove that monotone prompt-to-prompt operators admit least fixed points
(Knaster-Tarski) that characterize steady-state protocols; under a task-aware
contraction metric on trees, we further prove Banach-style convergence of
iterative guidance. We instantiate these results with context-free grammars
(CFGs) for XML schemas and show how constrained decoding guarantees
well-formedness while preserving task performance. A set of multi-layer
human-AI interaction recipes demonstrates practical deployment patterns,
including multi-pass "plan $\to$ verify $\to$ revise" routines and agentic tool
use. We provide mathematically complete proofs and tie our framework to recent
advances in grammar-aligned decoding, chain-of-verification, and programmatic
prompting.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [10] [Noise Injection for__Performance Bottleneck Analysis](https://arxiv.org/abs/2509.08446)
*Aurélien Delval,Pablo de Oliveira Castro,William Jalby,Etienne Renault*

Main category: cs.PF

TL;DR: 本文提出了一种新的与模型无关、指令精确的瓶颈分析框架，基于性能噪声注入方法，补充了现有技术，特别在量化未使用资源余量方面。


<details>
  <summary>Details</summary>
Motivation: 高性能计算（HPC）应用的性能调优中，瓶颈评估至关重要，直接影响优化搜索和硬件选择。

Method: 基于LLVM编译器工具链，通过注入针对特定瓶颈源的噪声指令，将程序分类为计算、数据访问带宽或延迟限制。

Result: 在多种硬件基准和内核上验证了框架的有效性，包括对稀疏矩阵-向量乘积（SPMXV）内核的详细研究，成功检测了不同的性能状态。

Conclusion: 该方法不仅提升了现有技术的便携性和精确性，还为硬件选择提供了实用见解，例如HBM与DDR内存系统的比较评估。

Abstract: Bottleneck evaluation plays a crucial part in performance tuning of HPC
applications, as it directly influences the search for optimizations and the
selection of the best hardware for a given code. In this paper, we introduce a
new model-agnostic, instruction-accurate framework for bottleneck analysis
based on performance noise injection. This method provides a precise analysis
that complements existing techniques, particularly in quantifying unused
resource slack. Specifically, we classify programs based on whether they are
limited by computation, data access bandwidth, or latency by injecting
additional noise instructions that target specific bottleneck sources. Our
approach is built on the LLVM compiler toolchain, ensuring easy portability
across different architectures and microarchitectures which constitutes an
improvement over many state-of-the-art tools. We validate our framework on a
range of hardware benchmarks and kernels, including a detailedstudy of a
sparse-matrix--vector product (SPMXV) kernel, where we successfully detect
distinct performance regimes. These insights further inform hardware selection,
as demonstrated by our comparative evaluation between HBM and DDR memory
systems.

</details>


### [11] [Memshare: Memory Sharing for Multicore Computation in R with an Application to Feature Selection by Mutual Information using PDE](https://arxiv.org/abs/2509.08632)
*Michael C. Thrun,Julian Märte*

Main category: cs.PF

TL;DR: memshare是一个通过C++共享内存实现多核计算的R包，比Bioconductor的SharedObject快2倍，且节省内存，适用于大数据分析。


<details>
  <summary>Details</summary>
Motivation: 解决R在大数据分析中的内存限制问题。

Method: 使用C++共享内存分配缓冲区，并通过ALTREP视图暴露给R。

Result: 与SharedObject相比，memshare在列式应用基准测试中实现了2倍速度提升，且未增加内存占用。

Conclusion: memshare为R提供了一种高效的多核计算解决方案，适用于大规模数据分析。

Abstract: We present memshare\footnote{The Software package is published as a CRAN
package under https://CRAN.R-project.org/package=memshare, a package that
enables shared memory multicore computation in R by allocating buffers in C++
shared memory and exposing them to R through ALTREP views. We compare memshare
to SharedObject (Bioconductor) discuss semantics and safety, and report a 2x
speedup over SharedObject with no additional resident memory in a column wise
apply benchmark. Finally, we illustrate a downstream analytics use case:
feature selection by mutual information in which densities are estimated per
feature via Pareto Density Estimation (PDE). The analytical use-case is an RNA
seq dataset consisting of N=10,446 cases and d=19,637 gene expressions
requiring roughly n_threads * 10GB of memory in the case of using parallel R
sessions. Such and larger use-cases are common in big data analytics and make R
feel limiting sometimes which is mitigated by the addition of the library
presented in this work.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [12] [Matisse: Visualizing Measured Internet Latencies as Manifolds](https://arxiv.org/abs/2509.08097)
*Stephen Jasina,Loqman Salamatian,Joshua Mathews,Scott Anderson,Paul Barford,Mark Crovella,Walter Willinger*

Main category: cs.NI

TL;DR: 论文提出了一种新方法和系统Matisse，用于生成和可视化基于互联网延迟测量的流形，并通过二维欧几里得空间投影展示其拓扑特性。


<details>
  <summary>Details</summary>
Motivation: 通过可视化流形展示复杂拓扑空间（如互联网延迟数据）的几何特性（如曲率），揭示数据中的重要属性（如异常）。

Method: 利用图形捕捉数据中的关键信息（如顶点位置和边上的Ricci曲率），生成二维欧几里得空间上的弯曲流形，保持顶点地理位置的曲率特性。

Result: 生成的流形突出关键连接区域，定义了一个“互联网延迟空间”，其中延迟测量表现为测地线。

Conclusion: Matisse工具成功实现了流形的生成、可视化和操作，并通过案例展示了其实用性。

Abstract: Manifolds are complex topological spaces that can be used to represent
datasets of real-world measurements. Visualizing such manifolds can help with
illustrating their topological characteristics (e.g., curvature) and providing
insights into important properties of the underlying data (e.g., anomalies in
the measurements). In this paper, we describe a new methodology and system for
generating and visualizing manifolds that are inferred from actual Internet
latency measurements between different cities and are projected over a 2D
Euclidean space (e.g., a geographic map). Our method leverages a series of
graphs that capture critical information contained in the data, including
well-defined locations (for vertices) and Ricci curvature information (for
edges). Our visualization approach then generates a curved surface (manifold)
in which (a) geographical locations of vertices are maintained and (b) the
Ricci curvature values of the graph edges determine the curvature properties of
the manifold. The resulting manifold highlights areas of critical connectivity
and defines an instance of "Internet delay space" where latency measurements
manifest as geodesics. We describe details of our method and its implementation
in a tool, which we call Matisse, for generating, visualizing and manipulating
manifolds projected onto a base map. We illustrate Matisse with two case
studies: a simple example to demonstrate key concepts, and visualizations of
the US public Internet to show Matisse's utility.

</details>


### [13] [UTM Performance Under Stressing Scenarios](https://arxiv.org/abs/2509.08124)
*Ian Jessen*

Main category: cs.NI

TL;DR: 论文提出了一种虚拟系统集成实验室ANAMLL，用于模拟和测试先进空中交通管理系统在极端条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 随着新型空中交通参与者（如无人驾驶和先进空中机动车辆）的增加，需要开发有效的空中交通管理解决方案，并验证其在压力环境下的表现。

Method: 通过ANAMLL这一虚拟实验室，联邦自治网络（如UTM或PSU网络）可以在更大规模下进行测试和验证，超越了现实部署的限制。

Result: ANAMLL成功展示了UTM网络在高需求场景下的性能极限，包括无法在规定时间内完成飞行重规划的情况，以及网络连通性对用户空中交通访问的影响。

Conclusion: ANAMLL为先进空中交通管理系统提供了关键的模拟和测试能力，有助于优化其在复杂场景下的表现。

Abstract: Proliferation of new classes of airspace participants, including uncrewed and
advanced aerial mobility vehicles, necessitates the development and deployment
of novel airspace management solutions, such as the Unmanned Traffic Management
(UTM) system and the Provider of Services to UAM (PSU) Network. The efficacy of
such systems has been demonstrated on multiple occasions via real-world
deployments in limited test environments, however exploration of system
behavior under stressing conditions requires the development of appropriate
modeling and simulation (M&S) environments. Autonomy Networks for Advanced
Mobility at Lincoln Laboratory (ANAMLL) is a virtual Systems Integration
Laboratory (SIL) designed to host federated autonomy networks, such as a UTM or
PSU Network, and to enable test and validation at scales not available in
real-world deployments. As an example of ANAMLL's utility, we explore the
performance of a representative UTM network during a stressing demand scenario.
In a close examination of the demand scenario, ANAMLL demonstrates a UTM system
demand point at which in-flight replanning can no longer be accomplished within
an allowable time window. In a second analysis of the same scenario, ANAMLL
demonstrates the impact of network connectivity performance on end-user
airspace access.

</details>


### [14] [Enhancing 6G Network Security and Incident Response through Integrated VNF and SDN Technologies](https://arxiv.org/abs/2509.08274)
*Abdul Razaque,Abitkhanova Zhadyra Abitkhanovna*

Main category: cs.NI

TL;DR: 论文提出了一种结合VNF和SDN技术的VNFSDN方法，以提升6G网络下的安全响应效率。


<details>
  <summary>Details</summary>
Motivation: 低速互联网会影响事件响应效率，增加安全风险，因此需要一种更高效的网络安全管理方法。

Method: 通过整合虚拟网络功能（VNF）和软件定义网络（SDN）技术，建立VNFSDN，动态适应安全需求和网络条件。

Result: VNFSDN能快速处理6G网络生成的大量数据，提升网络弹性和威胁检测能力。

Conclusion: VNFSDN结合AI和机器学习可显著增强网络安全，是一种高效的安全服务解决方案。

Abstract: Low-speed internet can negatively affect incident response in a number of
ways, including decreased teamwork, delayed detection, inefficient action, and
elevated risk. Delayed data acquisition and processing may result from
inadequate internet connectivity, hindering security teams' ability to obtain
the necessary information for timely and effective responses. Each of these
factors may augment the organization's susceptibility to security incidents and
their subsequent ramifications. This article establishes a virtual network
function service delivery network (VNFSDN) through the integration of virtual
network function (VNF) and software-defined networking (SDN) technologies. The
VNFSDN approach enhances network security effectiveness and efficiency while
reducing the danger of breaches. This method assists security services in
rapidly assessing vast quantities of data generated by 6G networks. VNFSDN
adapts dynamically to changing safety requirements and connection conditions
through the use of SDN and VNF. This flexibility enables enterprises to
mitigate or halt the impact of cyberattacks by swiftly identifying and
addressing security threats. The VNFSDN enhances network resilience, allowing
operators to proactively mitigate possible security attacks and minimize
downtime. The incorporation of machine learning and artificial intelligence
into VNFSDN can significantly improve network security and threat detection
capabilities. The VNFSDN integrates VNF and SDN technologies to deliver
security services that analyze vast quantities of 6G data in real time. As
security requirements and network conditions evolve, it adapts dynamically to
enhance network resilience and facilitate proactive threat detection.

</details>


### [15] [Ubiquitous Intelligence Via Wireless Network-Driven LLMs Evolution](https://arxiv.org/abs/2509.08400)
*Xingkun Yin,Feiran You,Hongyang Du,Kaibin Huang*

Main category: cs.NI

TL;DR: 论文提出了一种通过无线网络生态系统驱动大型语言模型（LLMs）动态发展的泛在智能范式，实现网络与模型的协同进化。


<details>
  <summary>Details</summary>
Motivation: 旨在克服静态模型部署的局限性，推动智能系统的持续升级与适应性。

Method: 通过无线网络支持的系统化终身学习与LLMs驱动的新一代网络开发相结合。

Result: 展示了自改进系统的潜力，能够在多样化和资源受限的环境中持续提升能力。

Conclusion: 这种协同进化标志着智能系统向更灵活、响应更快的方向发展。

Abstract: We introduce ubiquitous intelligence as a paradigm where Large Language
Models (LLMs) evolve within wireless network-driven ecosystems. Unlike static
model deployments, this approach enables scalable and continuous intelligence
ascension through coordination between networks and LLMs. Wireless networks
support system-orchestrated lifelong learning, while LLMs drive the
next-generation network development that is more adaptive and responsive. This
co-evolution highlights a shift toward self-improving systems, sustaining
capability growth across diverse and resource-constrained environments.

</details>


### [16] [The Role of Legacy Mobile Networks in Infrastructure Resilience: Evidence from the Southern Brazil Flood](https://arxiv.org/abs/2509.08595)
*Daniel Meyer,Lisandro Z Granville,Leandro M. Bertholdo*

Main category: cs.NI

TL;DR: 研究分析了2024年5月巴西南里奥格兰德州洪灾中的移动通信网络韧性，发现4G/5G网络的脆弱性及2G/3G在恶劣条件下的关键作用。


<details>
  <summary>Details</summary>
Motivation: 探讨极端洪灾对现代移动通信网络的影响，以及传统技术在此类灾害中的表现。

Method: 基于运营商的技术数据和监管数据，分析网络中断的主要原因。

Result: 现代网络（4G/5G）在灾害中表现脆弱，而传统技术（2G/3G）在保持基本连接方面发挥了关键作用。

Conclusion: 建议未来基础设施规划需考虑灾害韧性，包括传统系统的保留、多样化供电策略和弹性网络设计。

Abstract: This paper investigates the resilience of mobile communication networks
during the extreme flooding that affected Rio Grande do Sul, Brazil, in May
2024. Based on regulatory data and technical insights from operators, the study
identifies the leading causes of mobile network disruptions, primarily related
to flooding and prolonged power outages. The results reveal the significant
vulnerability of modern networks (4G/5G) during the event and the essential
role played by legacy technologies (2G/3G) in sustaining basic connectivity
under adverse conditions. The findings underscore the necessity of
disaster-aware infrastructure planning, taking into account the ongoing
significance of legacy systems, diversified power supply strategies, and
resilient network designs to enhance service continuity during future crises.

</details>


### [17] [SKYLINK: Scalable and Resilient Link Management in LEO Satellite Network](https://arxiv.org/abs/2509.08455)
*Wanja de Sombre,Arash Asadi,Debopam Bhattacherjee,Deepak Vasisht,Andrea Ortiz*

Main category: cs.NI

TL;DR: 针对LEO卫星网络中的高效和弹性路由挑战，论文提出了SKYLINK，一种分布式学习策略，显著降低了延迟和丢包率，并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络的高速移动性、动态流量模式和潜在链路故障对路由效率和弹性提出挑战，需开发新方法应对这些不确定性。

Method: SKYLINK是一种完全分布式的学习策略，通过建模时间为变图并实时决策流量分配，适应网络变化。

Result: SKYLINK在2540万用户中，平均延迟和丢包率加权和降低29%，吞吐量提高46%，并保持计算复杂度恒定。

Conclusion: SKYLINK在LEO卫星网络中实现了高效、可扩展和弹性的路由，优于传统方法。

Abstract: The rapid growth of space-based services has established LEO satellite
networks as a promising option for global broadband connectivity.
Next-generation LEO networks leverage inter-satellite links (ISLs) to provide
faster and more reliable communications compared to traditional bent-pipe
architectures, even in remote regions. However, the high mobility of
satellites, dynamic traffic patterns, and potential link failures pose
significant challenges for efficient and resilient routing. To address these
challenges, we model the LEO satellite network as a time-varying graph
comprising a constellation of satellites and ground stations. Our objective is
to minimize a weighted sum of average delay and packet drop rate. Each
satellite independently decides how to distribute its incoming traffic to
neighboring nodes in real time. Given the infeasibility of finding optimal
solutions at scale, due to the exponential growth of routing options and
uncertainties in link capacities, we propose SKYLINK, a novel fully distributed
learning strategy for link management in LEO satellite networks. SKYLINK
enables each satellite to adapt to the time-varying network conditions,
ensuring real-time responsiveness, scalability to millions of users, and
resilience to network failures, while maintaining low communication overhead
and computational complexity. To support the evaluation of SKYLINK at global
scale, we develop a new simulator for large-scale LEO satellite networks. For
25.4 million users, SKYLINK reduces the weighted sum of average delay and drop
rate by 29% compared to the bent-pipe approach, and by 92% compared to
Dijkstra. It lowers drop rates by 95% relative to k-shortest paths, 99%
relative to Dijkstra, and 74% compared to the bent-pipe baseline, while
achieving up to 46% higher throughput. At the same time, SKYLINK maintains
constant computational complexity with respect to constellation size.

</details>


### [18] [Design and Development of a Scalable and Energy-Efficient Localization Framework Leveraging LoRa Ranging-Capable Transceivers](https://arxiv.org/abs/2509.08488)
*Hasan Albinsaid,Bodhibrata Mukhopadhyay,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 该论文提出了一种协调框架，显著降低了Semtech SX1280 LoRa收发器在定位应用中的功耗，同时保持其精确测距能力。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）应用中，如资产标记、农业和智慧城市，需要精确且节能的定位技术。现有系统缺乏有效管理睡眠唤醒协调和角色分配的框架，导致能量消耗大。

Method: 通过设计同步唤醒窗口的调度策略，设备可以在大部分时间内保持深度睡眠，仅定期唤醒执行定位操作。

Result: 实验结果表明，节点可以在单枚纽扣电池下待机长达九个月，并按需进行近实时的测距操作，定位精度在五米以内。

Conclusion: 该框架成功解决了LoRa收发器在定位应用中的高能耗问题，为大规模IoT部署提供了实用解决方案。

Abstract: Precise and energy-efficient localization is a critical requirement in many
Internet of Things (IoT) applications, particularly in large-scale deployments
such as asset tagging, agriculture, and smart cities, where long battery life
and cost-effectiveness are crucial. The Semtech SX1280 LoRa transceiver
presents a promising solution for IoT localization. It combines low cost, low
power, and precise ranging capability over distances of up to 1 km. However,
the ranging process requires two devices to be simultaneously active, one
initiating the ranging request and the other responding to it, which can lead
to significant energy expenditure if not properly managed. Despite the
transceiver's excellent performance, no existing system-level framework
effectively manages sleep-wake coordination and role assignment needed for
energy-efficient operation. This paper presents a coordination framework that
significantly reduces power consumption while maintaining the inherent precise
ranging capability of the chip. The framework schedules short, synchronized
wake-up windows between the initiator and the responder, allowing devices to
remain in deep sleep for most of their duty cycle. This scheduling strategy
minimizes reliance on precise continuous timing and mitigates drift in low-cost
oscillators. To validate the framework, we designed and developed custom nodes
that are compliant with the framework's protocol. Experimental results show
that the proposed approach allows a node to stay in ultra-low power mode and
wake periodically to check for instructions. The node can remain in standby
mode for up to nine months on a single coin cell battery and can perform
ranging operations on demand in near real-time, all while maintaining a
localization accuracy within five meters.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [19] [Decidability in First-Order Modal Logic with Non-Rigid Constants and Definite Descriptions](https://arxiv.org/abs/2509.08165)
*Alessandro Artale,Christopher Hampson,Roman Kontchakov,Andrea Mazzullo,Frank Wolter*

Main category: cs.LO

TL;DR: 该论文研究了模态逻辑的单调分片，指出在特定条件下（如限制非刚性常数、明确描述和非平凡计数），单调分片是可判定的，并展示了几个包含这些特征的单调分片是可判定的，同时给出了紧致的复杂性界限。


<details>
  <summary>Details</summary>
Motivation: 研究单调模态逻辑分片的可判定性，以填补在非刚性常数等特定情况下可判定性的空白，并提供复杂性分析。

Method: 系统地分析了单调分片中非刚性常数、明确描述和非平凡计数等特征，并验证了多个单调分片在特定模态逻辑（如 $	extbf{K}_{n}$ 和 $	extbf{S5}_{n}$）中的可判定性。

Result: 展示了包括两变量分片和受保护分片在内的多个单调分片在特定条件下是可判定的，并给出了紧致的复杂性界限。

Conclusion: 单调模态逻辑分片在特定限制下是可判定的，这为模态逻辑的可判定性研究提供了新的理论支持。

Abstract: While modal extensions of decidable fragments of first-order logic are
usually undecidable, their monodic counterparts, in which formulas in the scope
of modal operators have at most one free variable, are typically decidable.
This only holds, however, under the provision that non-rigid constants,
definite descriptions and non-trivial counting are not admitted. Indeed,
several monodic fragments having at least one of these features are known to be
undecidable. We investigate these features systematically and show that
fundamental monodic fragments such as the two-variable fragment with counting
and the guarded fragment of standard first-order modal logics $\mathbf{K}_{n}$
and $\mathbf{S5}_{n}$ are decidable. Tight complexity bounds are established as
well. Under the expanding-domain semantics, we show decidability of the basic
modal logic extended with the transitive closure operator on finite acyclic
frames; this logic, however, is Ackermann-hard.

</details>


### [20] [Hammering Higher Order Set Theory](https://arxiv.org/abs/2509.08264)
*Chad E. Brown,Cezary Kaliszyk,Martin Suda,Josef Urban*

Main category: cs.LO

TL;DR: 利用高阶自动定理证明器简化高阶集合论的形式开发，包括算术基本定理等标准定理。


<details>
  <summary>Details</summary>
Motivation: 通过自动化工具简化高阶集合论中的形式证明，减少手动工作量。

Method: 使用高阶和低阶自动定理证明器处理生成的子目标，并进行性能比较。

Result: 展示了不同证明器在子目标上的表现，讨论了证明重构的可能性。

Conclusion: 自动化定理证明器能有效简化形式开发，未来可探索证明重构以提高可靠性。

Abstract: We use automated theorem provers to significantly shorten a formal
development in higher order set theory. The development includes many standard
theorems such as the fundamental theorem of arithmetic and irrationality of
square root of two. Higher order automated theorem provers are particularly
useful here, since the underlying framework of higher order set theory
coincides with the classical extensional higher order logic of (most) higher
order automated theorem provers, so no significant translation or encoding is
required. Additionally, many subgoals are first order and so first order
automated provers often suffice. We compare the performance of different
provers on the subgoals generated from the development. We also discuss
possibilities for proof reconstruction, i.e., obtaining formal proof terms when
an automated theorem prover claims to have proven the subgoal.

</details>


### [21] [Exploring Formal Math on the Blockchain: An Explorer for Proofgold](https://arxiv.org/abs/2509.08267)
*Chad E. Brown,Cezary Kaliszyk,Josef Urban*

Main category: cs.LO

TL;DR: Proofgold是一个支持形式化数学与加密货币功能的区块链，本文介绍了其基于Web的区块链浏览器，支持查看和交互数学内容。


<details>
  <summary>Details</summary>
Motivation: 通过区块链技术激励形式化数学的发展，并提供工具支持数学知识的管理与交互。

Method: 开发了一个Web浏览器，集成了Proofgold Lava软件，允许用户查看和提交区块、交易及形式化数学对象。

Result: 实现了对数学内容的导航和管理，展示了在范畴论中的多个形式化案例。

Conclusion: 该系统成功地将形式化数学与区块链结合，支持去中心化的数学知识管理。

Abstract: Proofgold is a blockchain that supports formalized mathematics alongside
standard cryptocurrency functionality. It incorporates logical constructs into
the blockchain, including declarations of formal theories, definitions,
propositions and proofs. It also supports placing and collecting bounties on
proving these propositions, incentivizing the development of the formal
libraries contained in Proofgold. In this paper, we present a web-based
blockchain explorer for Proofgold. The system exposes not only the usual
transactional data but also the formal mathematical components embedded in the
chain and allows some interaction with them. The explorer allows users to
inspect blocks, transactions, and addresses, as well as formal objects:
theories, definitions, theorems and their proofs. We also support the
submission of transactions to the blockchain using our interface. We describe
the system architecture and its integration with the Proofgold Lava software,
highlighting how the explorer supports navigation of formal content and
facilitates mathematical knowledge management in a decentralized setting, as
well as a number of formalizations in category theory done in the system.

</details>


### [22] [Payment Channels with Proofs](https://arxiv.org/abs/2509.08268)
*Chad E. Brown,Cezary Kaliszyk,Josef Urban*

Main category: cs.LO

TL;DR: 该论文探讨了在Proofgold网络中扩展支付通道，使其支持双方对某个命题在一定时间内是否能被证明进行押注，从而为Proofgold闪电网络奠定基础。


<details>
  <summary>Details</summary>
Motivation: 动机是为Proofgold网络构建一个闪电网络，使参与者能够通过押注来请求或提供证明，并利用这种机制估算命题的可证明概率。

Method: 方法是在Proofgold网络中实现支持证明的支付通道，并讨论如何构建一个潜在的闪电网络。

Result: 结果表明，这种机制可以用于快速协作的形式化项目，提供了去中心化基础设施的可能性。

Conclusion: 结论是这种扩展的支付通道为Proofgold闪电网络提供了基础，支持了去中心化协作证明的应用。

Abstract: The fundamental building blocks of the Bitcoin lightning network are
bidirectional payment channels. We describe an extension of payment channels in
the Proofgold network which allow the two parties to bet on whether a
proposition will be proven by a certain time. These provide the foundation for
a Proofgold lightning network that would allow parties to request proofs (by
betting there will be no proof by a certain time) and other parties to provide
proofs (and be rewarded by betting there will be a proof). The bets may also
provide a way to approximate the probability that a certain proposition is
provable (in the given amount of time). We describe the implementation of
payment channels supporting proofs in Proofgold and discuss a potential
lightning network that could be built as a result. One application of such
lightning network would be a large decentralized infrastructure for fast
collaborative formalization projects.

</details>


### [23] [Trace Repair for Temporal Behavior Trees](https://arxiv.org/abs/2509.08610)
*Sebastian Schirmer,Philipp Schitz,Johann C. Dauer,Bernd Finkbeiner,Sriram Sankaranarayanan*

Main category: cs.LO

TL;DR: 提出了两种修复轨迹的方法，针对以TBT为形式的行为规范，提高了效率。


<details>
  <summary>Details</summary>
Motivation: 修复轨迹有助于解释失败并作为训练示例，但传统MILP方法效率太低，不适用实际场景。

Method: 提出了两种策略：(1)增量修复，分割轨迹以减少MILP规模；(2)基于地标的修复，利用TBT的鲁棒语义近似MILP。

Result: 实验中，新方法成功修复超过25,000条记录的轨迹，耗时不到10分钟，而MILP则内存不足。

Conclusion: 新方法显著提升了修复效率，解决了传统MILP方法的局限性。

Abstract: We present methods for repairing traces against specifications given as
temporal behavior trees (TBT). TBT are a specification formalism for action
sequences in robotics and cyber-physical systems, where specifications of
sub-behaviors, given in signal temporal logic, are composed using operators for
sequential and parallel composition, fallbacks, and repetition. Trace repairs
are useful to explain failures and as training examples that avoid the observed
problems. In principle, repairs can be obtained via mixed-integer linear
programming (MILP), but this is far too expensive for practical applications.
We present two practical repair strategies: (1) incremental repair, which
reduces the MILP by splitting the trace into segments, and (2) landmark-based
repair, which solves the repair problem iteratively using TBT's robust
semantics as a heuristic that approximates MILP with more efficient linear
programming. In our experiments, we were able to repair traces with more than
25,000 entries in under ten minutes, while MILP runs out of memory.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [24] [Understanding the Video Content Creation Journey of Creators with Sensory Impairment in Kenya](https://arxiv.org/abs/2509.08108)
*Lan Xiao,Maryam Bandukda,Franklin Mingzhe Li,Mark Colley,Catherine Holloway*

Main category: cs.HC

TL;DR: 研究探讨了肯尼亚视觉和听觉障碍视频创作者的工具、挑战和协作方式，发现技术和社会因素共同影响了他们的创作过程，提出了支持全球残障创作者的改进方向。


<details>
  <summary>Details</summary>
Motivation: 视频内容创作是表达和参与的重要途径，但对感官障碍者尤其是资源匮乏地区的人来说仍难以企及，需研究如何改善其创作体验。

Method: 通过对肯尼亚20名视觉和听觉障碍视频创作者的访谈，分析了他们的工具、挑战和协作实践。

Result: 研究发现，无障碍障碍和基础设施限制使创作成为分阶段的协作过程，依赖信任的人类伙伴和新兴AI工具，创作者在保持创意的同时弥补感官差距。

Conclusion: 研究呼吁灵活的协作模式、包容的人机工作流程和多样化的叙事实践，以支持全球残障创作者，扩展了HCI中无障碍研究的视野。

Abstract: Video content creation offers vital opportunities for expression and
participation, yet remains largely inaccessible to creators with sensory
impairments, especially in low-resource settings. We conducted interviews with
20 video creators with visual and hearing impairments in Kenya to examine their
tools, challenges, and collaborative practices. Our findings show that
accessibility barriers and infrastructural limitations shape video creation as
a staged, collaborative process involving trusted human partners and emerging
AI tools. Across workflows, creators actively negotiated agency and trust,
maintaining creative control while bridging sensory gaps. We discuss the need
for flexible, interdependent collaboration models, inclusive human-AI
workflows, and diverse storytelling practices. This work broadens accessibility
research in HCI by examining how technology and social factors intersect in
low-resource contexts, suggesting ways to better support disabled creators
globally.

</details>


### [25] [Componentization: Decomposing Monolithic LLM Responses into Manipulable Semantic Units](https://arxiv.org/abs/2509.08203)
*Ryan Lingo,Rajeev Chhajer,Martin Arroyo,Luka Brkljacic,Ben Davis,Nithin Santhanam*

Main category: cs.HC

TL;DR: 论文提出了一种‘组件化’方法（MAOD和CBRA），将大语言模型输出的文本分解为可独立编辑的模块，并通过原型MAODchat验证其在协作工作流中的潜力。初步用户研究表明该方法支持迭代优化和选择性复用。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成的文本难以部分编辑的问题，以提升协作效率。

Method: 采用MAOD算法将响应分解为连贯的组件，并通过CBRA架构实现，原型MAODchat基于微服务和状态机代理。

Result: 用户研究表明组件级编辑符合常见工作流，支持迭代和复用。

Conclusion: 组件化是将被动文本消费转变为主动协作的有前景方向。

Abstract: Large Language Models (LLMs) often produce monolithic text that is hard to
edit in parts, which can slow down collaborative workflows. We present
componentization, an approach that decomposes model outputs into modular,
independently editable units while preserving context. We describe Modular and
Adaptable Output Decomposition (MAOD), which segments responses into coherent
components and maintains links among them, and we outline the Component-Based
Response Architecture (CBRA) as one way to implement this idea. Our reference
prototype, MAODchat, uses a microservices design with state-machine-based
decomposition agents, vendor-agnostic model adapters, and real-time component
manipulation with recomposition.
  In an exploratory study with four participants from academic, engineering,
and product roles, we observed that component-level editing aligned with
several common workflows and enabled iterative refinement and selective reuse.
Participants also mentioned possible team workflows. Our contributions are: (1)
a definition of componentization for transforming monolithic outputs into
manipulable units, (2) CBRA and MAODchat as a prototype architecture, (3)
preliminary observations from a small user study, (4) MAOD as an algorithmic
sketch for semantic segmentation, and (5) example Agent-to-Agent protocols for
automated decomposition. We view componentization as a promising direction for
turning passive text consumption into more active, component-level
collaboration.

</details>


### [26] [A Priest, a Rabbi, and an Atheist Walk Into an Error Bar: Religious Meditations on Uncertainty Visualization](https://arxiv.org/abs/2509.08213)
*Michael Correll,Lane Harrison*

Main category: cs.HC

TL;DR: 当前不确定性可视化往往简化为误差条，忽略了背后的多元不确定性形式。研究通过宗教例子提出更丰富的不确定性视角，以拓展可视化的可能性。


<details>
  <summary>Details</summary>
Motivation: 探讨当前不确定性可视化的局限性，关注其简化形式（如误差条）背后的多元不确定性。

Method: 通过宗教例子提出不同的不确定性视角，对比传统统计方法。

Result: 展示不确定性可视化的多样性，挑战传统的简化模式。

Conclusion: 呼吁更丰富的不确定性可视化方法，以适应不同的需求和受众。

Abstract: In this provocation, we suggest that much (although not all) current
uncertainty visualization simplifies the myriad forms of uncertainty into error
bars around an estimate. This apparent simplification into error bars comes
only as a result of a vast metaphysics around uncertainty and probability
underlying modern statistics. We use examples from religion to present
alternative views of uncertainty (metaphysical or otherwise) with the goal of
enriching our conception of what kind of uncertainties we ought to visualize,
and what kinds of people we might be visualizing those uncertainties for.

</details>


### [27] [An Adaptive Scoring Framework for Attention Assessment in NDD Children via Serious Games](https://arxiv.org/abs/2509.08353)
*Abdul Rehman,Ilona Heldal,Cristina Costescu,Carmen David,Jerry Chun-Wei Lin*

Main category: cs.HC

TL;DR: 本文提出了一种创新的自适应评分框架，用于评估神经发育障碍（NDD）儿童的学习表现，整合了多种指标，如空间注意力模式、时间参与度和游戏表现数据，超越了传统评分方法。


<details>
  <summary>Details</summary>
Motivation: 传统的游戏评分方法无法全面评估儿童的学习表现，尤其是在面对神经发育障碍儿童时。本文旨在通过整合多指标数据，提供更全面的评估框架，以支持个性化的学习干预。

Method: 框架采用动态难度调整方法，针对不同认知负荷和学习复杂度动态调整权重。同时，引入时间分析功能，如检测参与时段、奖励持续注意力，并根据表现水平调整奖励倍数。此外，设计了自适应性能标度框架以避免高分者过度奖励，同时帮助表现不佳的学生。

Result: 研究建立了多指标验证框架（如MAE、RMSE、Pearson和Spearman相关性），并设置了质量阈值以评估教育场景中的部署准备情况。框架成功地将技术性眼动指标映射为教育行为，支持教学活动干预。

Conclusion: 该框架填补了技术指标与教育需求之间的空白，通过将注意力模式与学习行为明确关联，为神经发育障碍儿童的学习干预提供了可操作的解决方案。

Abstract: This paper introduces an innovative adaptive scoring framework for children
with Neurodevelopmental Disorders (NDD) that is attributed to the integration
of multiple metrics, such as spatial attention patterns, temporal engagement,
and game performance data, to create a comprehensive assessment of learning
that goes beyond traditional game scoring. The framework employs a progressive
difficulty adaptation method, which focuses on specific stimuli for each level
and adjusts weights dynamically to accommodate increasing cognitive load and
learning complexity. Additionally, it includes capabilities for temporal
analysis, such as detecting engagement periods, providing rewards for sustained
attention, and implementing an adaptive multiplier framework based on
performance levels. To avoid over-rewarding high performers while maximizing
improvement potential for students who are struggling, the designed framework
features an adaptive temporal impact framework that adjusts performance scales
accordingly. We also established a multi-metric validation framework using Mean
Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation, and
Spearman correlation, along with defined quality thresholds for assessing
deployment readiness in educational settings. This research bridges the gap
between technical eye-tracking metrics and educational insights by explicitly
mapping attention patterns to learning behaviors, enabling actionable
pedagogical interventions.

</details>


### [28] [Personalized Inhibition Training with Eye-Tracking: Enhancing Student Learning and Teacher Assessment in Educational Games](https://arxiv.org/abs/2509.08357)
*Abdul Rehman,Ilona Heldal,Diana Stilwell,Paula Costa Ferreira,Jerry Chun-Wei Lin*

Main category: cs.HC

TL;DR: 提出一个基于眼动追踪的框架，用于分析儿童抑制控制游戏中的注意力，并提供个性化干预建议。


<details>
  <summary>Details</summary>
Motivation: 通过眼动追踪技术理解儿童在交互式环境中的视觉注意力和认知过程，支持个性化教育干预。

Method: 采用双阈值眼动检测（I-VT和高级聚类）、AOI分析和基于证据的风险评估，处理原始注视数据。

Result: 系统能识别注意力缺陷，提供个性化干预（如聚焦训练），并区分适应性学习和认知过载。

Conclusion: 该框架有效支持客观注意力评估、早期风险识别和为教育决策提供数据驱动的建议。

Abstract: Eye tracking (ET) can help to understand visual attention and cognitive
processes in interactive environments. This study presents a comprehensive
eye-tracking analysis framework of the Inhibitory Control Game, named the
ReStroop game, which is an educational intervention aimed at improving
inhibitory control skills in children through a recycling-themed sorting task,
for educational assessment that processes raw gaze data through unified
algorithms for fixation detection, performance evaluation, and personalized
intervention planning. The system employs dual-threshold eye movement detection
(I-VT and advanced clustering), comprehensive Area of Interest (AOI) analysis,
and evidence-based risk assessment to transform gaze patterns into actionable
educational insights. We evaluated this framework across three difficulty
levels and revealed critical attention deficits, including low task relevance,
elevated attention scatter, and compromised processing efficiency. The
multi-dimensional risk assessment identified high to moderate risk levels,
triggering personalized interventions including focus training, attention
regulation support, and environmental modifications. The system successfully
distinguishes between adaptive learning and cognitive overload, providing early
warning indicators for educational intervention. Results demonstrate the
system's effectiveness in objective attention assessment, early risk
identification, and the generation of evidence-based recommendations for
students, teachers, and specialists, supporting data-driven educational
decision-making and personalized learning approaches.

</details>


### [29] [HyperMOOC: Augmenting MOOC Videos with Concept-based Embedded Visualizations](https://arxiv.org/abs/2509.08404)
*Li Ye,Lei Wang,Lihong Cai,Ruiqi Yu,Yong Wang,Yigang Wang,Wei Chen,Zhiguang Zhou*

Main category: cs.HC

TL;DR: HyperMOOC通过概念嵌入的可视化增强MOOC学习效果，帮助学习者保持知识上下文。


<details>
  <summary>Details</summary>
Motivation: MOOC学习者主要依赖视频学习，容易失去知识上下文，降低学习效果。

Method: 结合多字形设计和多阶段交互，采用时间线放射状可视化，支持超链接导航。

Result: 用户研究表明，HyperMOOC提高了学习效果和效率，参与者满意度更高。

Conclusion: HyperMOOC在提升MOOC学习效果方面优于传统视频学习方法。

Abstract: Massive Open Online Courses (MOOCs) have become increasingly popular
worldwide. However, learners primarily rely on watching videos, easily losing
knowledge context and reducing learning effectiveness. We propose HyperMOOC, a
novel approach augmenting MOOC videos with concept-based embedded
visualizations to help learners maintain knowledge context. Informed by expert
interviews and literature review, HyperMOOC employs multi-glyph designs for
different knowledge types and multi-stage interactions for deeper
understanding. Using a timeline-based radial visualization, learners can grasp
cognitive paths of concepts and navigate courses through hyperlink-based
interactions. We evaluated HyperMOOC through a user study with 36 MOOC learners
and interviews with two instructors. Results demonstrate that HyperMOOC
enhances learners' learning effect and efficiency on MOOCs, with participants
showing higher satisfaction and improved course understanding compared to
traditional video-based learning approaches.

</details>


### [30] [GlyphWeaver: Unlocking Glyph Design Creativity with Uniform Glyph DSL and AI](https://arxiv.org/abs/2509.08444)
*Can Liu,Shiwei Chen,Zhibang Jiang,Yong Wang*

Main category: cs.HC

TL;DR: GlyphWeaver是一个交互式系统，简化了复杂多元数据表达性glyph可视化的创建过程，通过领域特定语言和交互界面降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 现有glyph可视化工具在设计与技术实现之间存在鸿沟，GlyphWeaver旨在解决这一问题，使设计师无需深入编程即可创建复杂glyph可视化。

Method: 系统包含glyph领域特定语言(GDSL)、操作管理机制和多模态交互界面，通过原子操作和自然语言命令简化glyph设计。

Result: 案例研究和用户访谈表明，GlyphWeaver显著提升了设计效率和创造力，13位参与者验证了其有效性。

Conclusion: GlyphWeaver为设计师提供了高效且易用的工具，成功弥合了技术实现与设计创意之间的差距。

Abstract: Expressive glyph visualizations provide a powerful and versatile means to
represent complex multivariate data through compact visual encodings, but
creating custom glyphs remains challenging due to the gap between design
creativity and technical implementation. We present GlyphWeaver, a novel
interactive system to enable an easy creation of expressive glyph
visualizations. Our system comprises three key components: a glyph
domain-specific language (GDSL), a GDSL operation management mechanism, and a
multimodal interaction interface. The GDSL is a hierarchical container model,
where each container is independent and composable, providing a rigorous yet
practical foundation for complex glyph visualizations. The operation management
mechanism restricts modifications of the GDSL to atomic operations, making it
accessible without requiring direct coding. The multimodal interaction
interface enables direct manipulation, natural language commands, and parameter
adjustments. A multimodal large language model acts as a translator, converting
these inputs into GDSL operations. GlyphWeaver significantly lowers the barrier
for designers, who often do not have extensive programming skills, to create
sophisticated glyph visualizations. A case study and user interviews with 13
participants confirm its substantial gains in design efficiency and
effectiveness of producing creative glyph visualizations.

</details>


### [31] [Printegrated Circuits: Personal Fabrication of 3D Printed Devices with Embedded PCBs](https://arxiv.org/abs/2509.08459)
*Oliver Child,Ollie Hanton,Jack Dawson,Steve Hodges,Mike Fraser*

Main category: cs.HC

TL;DR: Printegrated Circuits技术利用传统电子元件作为3D打印材料，制造自包含的交互对象，避免了后期组装步骤。


<details>
  <summary>Details</summary>
Motivation: 当前多材料3D打印技术存在特征尺寸大、材料电阻高和可打印控制电路有限等问题，导致无法直接打印可部署设备。

Method: 通过在打印过程中嵌入PCB并使用导电丝注入其镀通孔，实现了可靠的电气和机械连接。

Result: 该方法展示了6个演示案例，验证了其在设计和原型工作流程中的适用性，并启发了未来研究方向。

Conclusion: Printegrated Circuits技术简化了交互对象的制造流程，无需后期组装，推动了快速原型设计的发展。

Abstract: Consumer-level multi-material 3D printing with conductive thermoplastics
enables fabrication of interactive elements for bespoke tangible devices.
However, large feature sizes, high resistance materials, and limitations of
printable control circuitry mean that deployable devices cannot be printed
without post-print assembly steps. To address these challenges, we present
Printegrated Circuits, a technique that uses traditional electronics as
material to 3D print self-contained interactive objects. Embedded PCBs are
placed into recesses during a pause in the print, and through a process we term
\textit{Prinjection}, conductive filament is injected into their plated-through
holes. This automatically creates reliable electrical and mechanical contact,
eliminating the need for manual wiring or bespoke connectors. We describe the
custom machine code generation that supports our approach, and characterise its
electrical and mechanical properties. With our 6 demonstrations, we highlight
how the Printegrated Circuits process fits into existing design and prototyping
workflows as well as informs future research agendas.

</details>


### [32] [Bias in the Loop: How Humans Evaluate AI-Generated Suggestions](https://arxiv.org/abs/2509.08514)
*Jacob Beck,Stephanie Eckman,Christoph Kern,Frauke Kreuter*

Main category: cs.HC

TL;DR: 研究表明，人机协作的成功不仅依赖算法性能，还与评估者心理及任务设计有关。


<details>
  <summary>Details</summary>
Motivation: 探讨心理因素如何影响人机协作的成败。

Method: 随机实验（2,784名参与者），操纵AI建议质量、任务负担和经济激励。

Result: 纠正要求降低参与度，个体对AI的态度是性能最强预测因素。

Conclusion: 成功协作需考虑人类心理，设计多样化评估和工作流程以抵消认知偏差。

Abstract: Human-AI collaboration increasingly drives decision-making across industries,
from medical diagnosis to content moderation. While AI systems promise
efficiency gains by providing automated suggestions for human review, these
workflows can trigger cognitive biases that degrade performance. We know little
about the psychological factors that determine when these collaborations
succeed or fail. We conducted a randomized experiment with 2,784 participants
to examine how task design and individual characteristics shape human responses
to AI-generated suggestions. Using a controlled annotation task, we manipulated
three factors: AI suggestion quality in the first three instances, task burden
through required corrections, and performance-based financial incentives. We
collected demographics, attitudes toward AI, and behavioral data to assess four
performance metrics: accuracy, correction activity, overcorrection, and
undercorrection. Two patterns emerged that challenge conventional assumptions
about human-AI collaboration. First, requiring corrections for flagged AI
errors reduced engagement and increased the tendency to accept incorrect
suggestions, demonstrating how cognitive shortcuts influence collaborative
outcomes. Second, individual attitudes toward AI emerged as the strongest
predictor of performance, surpassing demographic factors. Participants
skeptical of AI detected errors more reliably and achieved higher accuracy,
while those favorable toward automation exhibited dangerous overreliance on
algorithmic suggestions. The findings reveal that successful human-AI
collaboration depends not only on algorithmic performance but also on who
reviews AI outputs and how review processes are structured. Effective human-AI
collaborations require consideration of human psychology: selecting diverse
evaluator samples, measuring attitudes, and designing workflows that counteract
cognitive biases.

</details>


### [33] [Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning](https://arxiv.org/abs/2509.08539)
*Lukas Schach,Christian Rack,Ryan P. McMahan,Marc Erich Latoschik*

Main category: cs.HC

TL;DR: 论文研究了两种分类和相似性学习模型在XR应用中识别用户的泛化能力，发现模型在同一应用中表现良好，但跨应用识别能力有限。


<details>
  <summary>Details</summary>
Motivation: 评估现有模型在XR应用中作为生物识别方法的泛化能力，并提出风险分析。

Method: 开发了包含49名用户在5种XR应用中运动数据的新数据集，用于评估模型性能。

Result: 模型在同一应用中准确识别用户，但跨应用识别能力有限。

Conclusion: 研究揭示了当前模型的局限性，并呼吁进一步研究XR和元宇宙中的运动识别问题。

Abstract: This paper examines the generalization capacity of two state-of-the-art
classification and similarity learning models in reliably identifying users
based on their motions in various Extended Reality (XR) applications. We
developed a novel dataset containing a wide range of motion data from 49 users
in five different XR applications: four XR games with distinct tasks and action
patterns, and an additional social XR application with no predefined task sets.
The dataset is used to evaluate the performance and, in particular, the
generalization capacity of the two models across applications. Our results
indicate that while the models can accurately identify individuals within the
same application, their ability to identify users across different XR
applications remains limited. Overall, our results provide insight into current
models generalization capabilities and suitability as biometric methods for
user verification and identification. The results also serve as a much-needed
risk assessment of hazardous and unwanted user identification in XR and
Metaverse applications. Our cross-application XR motion dataset and code are
made available to the public to encourage similar research on the
generalization of motion-based user identification in typical Metaverse
application use cases.

</details>


### [34] [Formal verification for robo-advisors: Irrelevant for subjective end-user trust, yet decisive for investment behavior?](https://arxiv.org/abs/2509.08540)
*Alina Tausch,Magdalena Wischnewski,Mustafa Yalciner,Daniel Neider*

Main category: cs.HC

TL;DR: 研究探讨了AI质量保证措施（如认证和验证）对用户信任和使用机器人顾问的影响，结果显示这些措施对信任和依赖感知影响甚微，投资成功与否反而更影响信任。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解认证和验证作为AI质量保证手段，是否能有效提升用户对机器人顾问的信任和使用行为。

Method: 通过在线情景实验，520名参与者被分为4组，分别面对不同质量保证信息的机器人顾问或人类顾问，并决定投资金额和报告信任感知。随后，参与者接受了投资成功或失败的信息。

Result: 质量保证措施对结果变量几乎没有影响，而投资成功与否更显著影响信任。验证过的顾问在投资金额上表现稍好。

Conclusion: 研究强调了客观行为指标的重要性，并指出未来需深入研究形式验证对终端用户信任的实际效果。

Abstract: This online-vignette study investigates the impact of certification and
verification as measures for quality assurance of AI on trust and use of a
robo-advisor. Confronting 520 participants with an imaginary situation where
they were using an online banking service to invest their inherited money, we
formed 4 experimental groups. EG1 achieved no further information of their
robo-advisor, while EG2 was informed that their robo-advisor was certified by a
reliable agency for unbiased processes, and EG3 was presented with a formally
verified robo-advisor that was proven to consider their investment preferences.
A control group was presented a remote certified human financial advisor. All
groups had to decide on how much of their 10,000 euros they would give to their
advisor to autonomously invest for them and report on trust and perceived
dependability. A second manipulation happened afterwards, confronting
participants with either a successful or failed investment. Overall, our
results show that the level of quality assurance of the advisor had
surprisingly near to no effect of any of our outcome variables, except for
people's perception of their own mental model of the advisor. Descriptively,
differences between investments show that seem to favor a verified advisor with
a median investment of 65,000 euros (vs. 50,000). Success or failure
information, though influences only partially by advisor quality, has been
perceived as a more important clue for advisor trustworthiness, leading to
substantially different trust and dependability ratings. The study shows the
importance of thoroughly investigating not only trust, but also trusting
behavior with objective measures. It also underlines the need for future
research on formal verification, that might be the gold standard in proving AI
mathematically, but seems not to take full effect as a cue for trustworthiness
for end-users.

</details>


### [35] [Embedding Empathy into Visual Analytics: A Framework for Person-Centred Dementia Care](https://arxiv.org/abs/2509.08548)
*Rhiannon Owen,Jonathan C. Roberts*

Main category: cs.HC

TL;DR: 该论文提出了一种以同理心为中心的可视化框架，用于改善痴呆症护理中病人与医护人员之间的共情关系。


<details>
  <summary>Details</summary>
Motivation: 现有数字化工具过于关注量化指标，缺乏共情能力，难以满足痴呆症患者的个性化需求。

Method: 通过设计研究，结合以人为本的护理原则和共情映射方法，开发了该框架，并进行了用户测试和用户体验问卷调查。

Result: 评估结果表明该框架可行，能够支持医护人员与患者建立更个性化、更具共情的关系。

Conclusion: 该研究为数据可视化设计中系统化嵌入同理心提供了初步探索，有助于改善痴呆症患者在医院的体验。

Abstract: Dementia care requires healthcare professionals to balance a patient's
medical needs with a deep understanding of their personal needs, preferences,
and emotional cues. However, current digital tools prioritise quantitative
metrics over empathetic engagement,limiting caregivers ability to develop a
deeper personal understanding of their patients. This paper presents an empathy
centred visualisation framework, developed through a design study, to address
this gap. The framework integrates established principles of person centred
care with empathy mapping methodologies to encourage deeper engagement. Our
methodology provides a structured approach to designing for indirect end users,
patients whose experience is shaped by a tool they may not directly interact
with. To validate the framework, we conducted evaluations with healthcare
professinals, including usability testing of a working prototype and a User
Experience Questionnaire study. Results suggest the feasibility of the
framework, with participants highlighting its potential to support a more
personal and empathetic relationship between medical staff and patients. The
work starts to explore how empathy could be systematically embedded into
visualisation design, as we contribute to ongoing efforts in the data
visualisation community to support human centred, interpretable, and ethically
aligned clinical care, addressing the urgent need to improve dementia patients
experiences in hospital settings.

</details>


### [36] [Acceptability of AI Assistants for Privacy: Perceptions of Experts and Users on Personalized Privacy Assistants](https://arxiv.org/abs/2509.08554)
*Meihe Xu,Aurelia Tamò-Larrieux,Arianna Rossi*

Main category: cs.HC

TL;DR: 研究探讨了个性化隐私助手（PPAs）的设计和用户接受度，发现其可接受性受设计元素、外部条件和系统条件影响。


<details>
  <summary>Details</summary>
Motivation: 随着任务和决策的增加，需要智能助手帮助用户管理隐私决策，减轻心理负担。

Method: 通过五个焦点小组讨论（11名专家和26名用户），分析PPAs的接受度因素。

Result: 影响PPAs接受度的因素包括设计元素、外部条件和系统条件。

Conclusion: 研究结果为PPAs的设计和政策提供了理论支持和实践指导，并扩展了AI助手的适用性。

Abstract: Individuals increasingly face an overwhelming number of tasks and decisions.
To cope with the new reality, there is growing research interest in developing
intelligent agents that can effectively assist people across various aspects of
daily life in a tailored manner, with privacy emerging as a particular area of
application. Artificial intelligence (AI) assistants for privacy, such as
personalized privacy assistants (PPAs), have the potential to automatically
execute privacy decisions based on users' pre-defined privacy preferences,
sparing them the mental effort and time usually spent on each privacy decision.
This helps ensure that, even when users feel overwhelmed or resigned about
privacy, the decisions made by PPAs still align with their true preferences and
best interests. While research has explored possible designs of such agents,
user and expert perspectives on the acceptability of such AI-driven solutions
remain largely unexplored. In this study, we conducted five focus groups with
domain experts (n = 11) and potential users (n = 26) to uncover key themes
shaping the acceptance of PPAs. Factors influencing the acceptability of AI
assistants for privacy include design elements (such as information sources
used by the agent), external conditions (such as regulation and literacy
education), and systemic conditions (e.g., public or market providers and the
need to avoid monopoly) to PPAs. These findings provide theoretical extensions
to technology acceptance models measuring PPAs, insights on design, and policy
implications for PPAs, as well as broader implications for the design of AI
assistants.

</details>


### [37] [Visual Analysis of Time-Dependent Observables in Cell Signaling Simulations](https://arxiv.org/abs/2509.08589)
*Lena Cibulski,Fiete Haack,Adelinde Uhrmacher,Stefan Bruckner*

Main category: cs.HC

TL;DR: 本文提出了一种可视化分析方法，用于支持细胞信号传导过程的研究，帮助模型校准和探索受体运输对信号传递效率的影响。


<details>
  <summary>Details</summary>
Motivation: 细胞与环境的通信对其功能至关重要，但实验难以捕捉高度动态的分子机制，通过模拟和可视化分析可以弥补这一不足。

Method: 将时间序列图嵌入平行坐标中，同时分析模型参数和时间输出。

Result: 该方法支持典型任务，如评估时间输出的合理性及其对模型配置的敏感性。

Conclusion: 该方法为模型校准和信号传递研究提供了有效支持。

Abstract: The ability of a cell to communicate with its environment is essential for
key cellular functions like replication, metabolism, or cell fate decisions.
The involved molecular mechanisms are highly dynamic and difficult to capture
experimentally. Simulation studies offer a valuable means for exploring and
predicting how cell signaling processes unfold. We present a design study on
the visual analysis of such studies to support 1) modelers in calibrating model
parameters such that the simulated signal responses over time reflect reference
behavior from cell biology research and 2) cell biologists in exploring the
influence of receptor trafficking on the efficiency of signal transmission
within the cell. We embed time series plots into parallel coordinates to enable
a simultaneous analysis of model parameters and temporal outputs. A usage
scenario illustrates how our approach assists with typical tasks such as
assessing the plausibility of temporal outputs or their sensitivity across
model configurations.

</details>


### [38] [Augmenting speech transcripts of VR recordings with gaze, pointing, and visual context for multimodal coreference resolution](https://arxiv.org/abs/2509.08689)
*Riccardo Bovo,Frederik Brudy,George Fitzmaurice,Fraser Anderson*

Main category: cs.HC

TL;DR: 论文提出了一种通过结合眼动追踪、激光指向数据和场景元数据来增强VR语音转录的系统，显著提高了共指解析的准确性。


<details>
  <summary>Details</summary>
Motivation: 在沉浸式多模态对话中，非语言线索（如手势和视觉注意）缺失导致共指解析困难，因此需要一种方法将这些信息整合到转录中。

Method: 系统利用眼动追踪和激光指向数据，结合场景元数据生成非语言交流的文本描述，并在12名参与者的开放设计批评任务中收集数据验证。

Result: 使用多模态转录的GPT模型比纯语音基线在共指解析准确率上提高了26.5%。

Conclusion: 多模态信息可以有效提升共指解析准确性，为复杂对话转录提供了新方向。

Abstract: Understanding transcripts of immersive multimodal conversations is
challenging because speakers frequently rely on visual context and non-verbal
cues, such as gestures and visual attention, which are not captured in speech
alone. This lack of information makes coreferences resolution-the task of
linking ambiguous expressions like ``it'' or ``there'' to their intended
referents-particularly challenging. In this paper we present a system that
augments VR speech transcript with eye-tracking laser pointing data, and scene
metadata to generate textual descriptions of non-verbal communication and the
corresponding objects of interest. To evaluate the system, we collected gaze,
gesture, and voice data from 12 participants (6 pairs) engaged in an open-ended
design critique of a 3D model of an apartment. Our results show a 26.5\%
improvement in coreference resolution accuracy by a GPT model when using our
multimodal transcript compared to a speech-only baseline.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [39] [X-Part: high fidelity and structure coherent shape decomposition](https://arxiv.org/abs/2509.08643)
*Xinhao Yan,Jiachen Xu,Yang Li,Changfeng Ma,Yunhan Yang,Chunshi Wang,Zibo Zhao,Zeqiang Lai,Yunfei Zhao,Zhuo Chen,Chunchao Guo*

Main category: cs.GR

TL;DR: X-Part提出了一种可控生成模型，用于将3D对象分解为语义明确且结构一致的部分，并支持交互编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D形状分解中缺乏可控性和语义意义，X-Part旨在解决这一问题。

Method: 利用边界框提示生成部分，并通过点级语义特征注入实现分解，设计了可编辑流程。

Result: 实验表明X-Part在部分级形状生成中达到最佳性能，支持生产级3D资产创建。

Conclusion: X-Part为3D资产的可控生成和编辑提供了新范式。

Abstract: Generating 3D shapes at part level is pivotal for downstream applications
such as mesh retopology, UV mapping, and 3D printing. However, existing
part-based generation methods often lack sufficient controllability and suffer
from poor semantically meaningful decomposition. To this end, we introduce
X-Part, a controllable generative model designed to decompose a holistic 3D
object into semantically meaningful and structurally coherent parts with high
geometric fidelity. X-Part exploits the bounding box as prompts for the part
generation and injects point-wise semantic features for meaningful
decomposition. Furthermore, we design an editable pipeline for interactive part
generation. Extensive experimental results show that X-Part achieves
state-of-the-art performance in part-level shape generation. This work
establishes a new paradigm for creating production-ready, editable, and
structurally sound 3D assets. Codes will be released for public research.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [40] [Qubit-Efficient QUBO Formulation for Constrained Optimization Problems](https://arxiv.org/abs/2509.08080)
*Meerzhan Kantbekova,Vincenzo De Maio,Ivona Brandic*

Main category: cs.ET

TL;DR: 本文提出了一种基于指数惩罚框架的QUBO不等式约束编码策略，显著减少了量子比特需求，并在组合优化问题上取得了良好的实验效果。


<details>
  <summary>Details</summary>
Motivation: 针对当前NISQ机器量子比特数量有限的问题，现有编码方法因依赖大量比特而难以扩展，因此研究减少比特需求的编码策略至关重要。

Method: 采用广义指数惩罚框架，通过分析一类指数函数的理论性质和实证性能，优化不等式约束的编码。

Result: 实验显示，该方法在Bin Packing Problem（BPP）和Traveling Salesman Problem（TSP）上分别减少了57%和83%的量子比特需求，同时保持了与传统方法相当的求解质量。

Conclusion: 指数惩罚框架是一种有效的编码策略，能够在有限的量子比特资源下，显著提升组合优化问题的求解效率。

Abstract: Quantum computing has emerged as a promising alternative for solving
combinatorial optimization problems. The standard approach for encoding
optimization problems on quantum processing units (QPUs) involves transforming
them into their Quadratic Unconstrained Binary Optimization (QUBO)
representation. However, encoding constraints of optimization problems,
particularly inequality constraints, into QUBO requires additional variables,
which results in more qubits. Considering the limited availability of qubits in
NISQ machines, existing encoding methods fail to scale due to their reliance on
large numbers of qubits. We propose a generalized exponential penalty framework
for QUBO inequality constraints inspired by a class of exponential functions,
which we call exponential penalization. This paper presents an encoding
strategy for inequality constraints in combinatorial optimization problems,
inspired by a class of exponential functions, which we call exponential
penalization. The initial idea of using exponential penalties for QUBO
formulation was introduced by Montanez-Barrera et al. by applying a specific
exponential function to reduce qubit requirements. In this work, we extend that
approach by conducting a comprehensive study on a broader class of exponential
functions, analyzing their theoretical properties and empirical performance.
Our experimental results demonstrate that an exponential penalization achieves
57%, 83% qubit number reduction for Bin Packing Problem (BPP) and Traveling
Salesman Problem (TSP), respectively. And we demonstrate comparable solution
quality to classical with a probability of 6% and 21% accuracy for BPP with 8
and TSP with 12 qubits, respectively.

</details>


### [41] [Rotatable Array-Aided Hybrid Beamforming for Integrated Sensing and Communication](https://arxiv.org/abs/2509.08652)
*Zequan Wang,Liang Yin,Yitong Liu,Yunan Sun,Hongwen Yang*

Main category: cs.ET

TL;DR: 论文研究了基于三维可旋转天线（RA）架构的多用户ISAC系统，在非理想信道下通过混合波束成形技术平衡性能和硬件成本，提出了一种基于交替优化（AO）的算法。


<details>
  <summary>Details</summary>
Motivation: 六维可移动天线（6DMA）系统能提升ISAC性能，但在三维可旋转天线架构下的研究较少，尤其在非理想信道特性下如何通过波束成形技术平衡性能和硬件成本。

Method: 提出基于交替优化（AO）架构的算法，利用分数规划（FP）框架转换目标函数，并通过KKT条件和两阶段梯度上升（GA）方法优化天线旋转角度。

Result: 仿真结果表明，与传统固定位置天线（FPA）相比，所提方法显著提升系统整体性能。

Conclusion: 研究证明了RA架构下混合波束成形技术的有效性，为非理想信道下的ISAC系统优化提供了新思路。

Abstract: Integrated Sensing and Communication (ISAC) is one of the pivotal supporting
technologies for next-generation wireless communication networks. As an
emerging technical means, the new six-dimensional movable antenna (6DMA) system
can effectively improve the communication and sensing performance of ISAC
systems. However, related research on the architecture based on
three-dimensional rotatable antennas (RA) remains relatively limited.
Especially under the influence of non-ideal channel characteristics, how to
balance system performance and hardware cost control through beamforming
technology in non-ideal channels has become a crucial issue to be solved
urgently. Given the significant advantages of hybrid beamforming technology in
balancing system performance and hardware complexity, this paper focuses on the
channel model considering effective aperture loss under the RA architecture and
studies the sub-connected hybrid beamforming design for multi-user ISAC
systems. Aiming at the non-convex nature with coupled variables in this
problem, this paper first transforms the complex fractional objective function
using the Fractional Programming (FP) framework, and then proposes an algorithm
based on the Alternating Optimization (AO) architecture, which achieves
optimization by alternately solving five subproblems. Among them, the
closed-form update expression of the array rotation angle is derived through
the Karush-Kuhn-Tucker (KKT) conditions, and a two-stage Gradient Ascent (GA)
based method is proposed to optimize the antenna rotation angle. Simulation
results show that compared with the traditional fixed-position antenna (FPA),
the proposed method can significantly improve the overall performance of the
system.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [42] [Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery](https://arxiv.org/abs/2509.08207)
*Benjamin S. Allen,James Anchell,Victor Anisimov,Thomas Applencourt,Abhishek Bagusetty,Ramesh Balakrishnan,Riccardo Balin,Solomon Bekele,Colleen Bertoni,Cyrus Blackworth,Renzo Bustamante,Kevin Canada,John Carrier,Christopher Chan-nui,Lance C. Cheney,Taylor Childers,Paul Coffman,Susan Coghlan,Michael D'Mello,Murali Emani,Kyle G. Felker,Sam Foreman,Olivier Franza,Longfei Gao,Marta García,María Garzarán,Balazs Gerofi,Yasaman Ghadar,Neha Gupta,Kevin Harms,Väinö Hatanpää,Brian Holland,Carissa Holohan,Brian Homerding,Khalid Hossain,Louise Huot,Huda Ibeid,Joseph A. Insley,Sai Jayanthi,Hong Jiang,Wei Jiang,Xiao-Yong Jin,Jeongnim Kim,Christopher Knight,Kalyan Kumaran,JaeHyuk Kwack,Ti Leggett,Ben Lenard,Chris Lewis,Nevin Liber,Johann Lombardi,Raymond M. Loy,Ye Luo,Bethany Lusch,Nilakantan Mahadevan,Victor A. Mateevitsi,Gordon McPheeters,Ryan Milner,Vitali A. Morozov,Servesh Muralidharan,Tom Musta,Mrigendra Nagar,Vikram Narayana,Marieme Ngom,Anthony-Trung Nguyen,Nathan Nichols,Aditya Nishtala,James C. Osborn,Michael E. Papka,Scott Parker,Saumil S. Patel,Adrian C. Pope,Sucheta Raghunanda,Esteban Rangel,Paul M. Rich,Silvio Rizzi,Kris Rowe,Varuni Sastry,Adam Scovel,Filippo Simini,Haritha Siddabathuni Som,Patrick Steinbrecher,Rick Stevens,Xinmin Tian,Peter Upton,Thomas Uram,Archit K. Vasan,Álvaro Vázquez-Mayagoitia,Kaushik Velusamy,Brice Videau,Venkatram Vishwanath,Brian Whitney,Timothy J. Williams,Michael Woodacre,Sam Zeltner,Gengbin Zheng,Huihuo Zheng*

Main category: cs.DC

TL;DR: Aurora是阿贡国家实验室的突破性Exascale超级计算机，通过前沿架构创新加速科学发现。


<details>
  <summary>Details</summary>
Motivation: 设计Aurora旨在通过新技术（如Intel GPU和DAOS存储）推动高性能计算的前沿发展。

Method: Aurora结合了Intel Xeon GPU、HBM内存、Ponte Vecchio GPU、DAOS存储和oneAPI编程环境，并采用HPE Slingshot互连技术。

Result: 论文详细介绍了Aurora的节点架构、软件生态系统及性能基准测试，展示了其在科学应用中的准备情况。

Conclusion: Aurora通过创新技术和工具为Exascale计算提供了高性能解决方案，有望推动科学研究的进展。

Abstract: Aurora is Argonne National Laboratory's pioneering Exascale supercomputer,
designed to accelerate scientific discovery with cutting-edge architectural
innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center
GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth
Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named
Ponte Vecchio) on each compute node. Aurora also integrates the Distributed
Asynchronous Object Storage (DAOS), a novel exascale storage solution, and
leverages Intel's oneAPI programming environment. This paper presents an
in-depth exploration of Aurora's node architecture, the HPE Slingshot
interconnect, the supporting software ecosystem, and DAOS. We provide insights
into standard benchmark performance and applications readiness efforts via
Aurora's Early Science Program and the Exascale Computing Project.

</details>


### [43] [Design and Implementation of Code Completion System Based on LLM and CodeBERT Hybrid Subsystem](https://arxiv.org/abs/2509.08215)
*Bingbing Zhang,Ziyu Lin,Yingxin Su*

Main category: cs.DC

TL;DR: 研究提出了一种结合CodeBERT和GPT-3.5的混合模型，用于代码建议和自动完成，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 在快速发展的软件开发行业中，高效的代码建议和自动完成工具能显著提升开发效率和代码质量。

Method: 通过整合CodeBERT的上下文感知能力和GPT-3.5的高级代码生成能力，构建了一个混合模型。

Result: 混合模型在准确性、生成代码质量和性能效率上均优于基准模型，并通过了鲁棒性测试。

Conclusion: 研究表明，结合互补的深度学习模型能充分发挥各自优势，推动软件开发行业发展。

Abstract: In the rapidly evolving industry of software development, coding efficiency
and accuracy play significant roles in delivering high-quality software.
Various code suggestion and completion tools, such as CodeBERT from Microsoft
and GPT-3.5 from OpenAI, have been developed using deep learning techniques and
integrated into IDEs to assist software engineers' development. Research has
shown that CodeBERT has outstanding performance in code summarization and
capturing code semantics, while GPT-3.5 demonstrated its adept capability at
code generation. This study focuses on implementing a hybrid model that
integrates CodeBERT and GPT-3.5 models to accomplish code suggestion and
autocomplete tasks, leveraging the context-aware effectiveness of CodeBERT and
taking advantage of advanced code generation abilities of GPT-3.5. Evaluated in
three main metrics: accuracy, quality of generated code and performance
efficiency with various software and hardware, the hybrid model outperforms
benchmarks, demonstrating its feasibility and effectiveness. Robustness testing
further confirms the reliability and stability of the hybrid model. This study
not only emphasizes the importance of deep learning in the software development
industry, but also reveals the potential of synthesizing complementary deep
learning models to fully exploit strengths of each model.

</details>


### [44] [Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism](https://arxiv.org/abs/2509.08309)
*Zizhao Mo,Jianxiong Liao,Huanle Xu,Zhi Zhou,Chengzhong Xu*

Main category: cs.DC

TL;DR: Hetis是一种针对异构GPU集群设计的LLM系统，通过细粒度和动态并行化策略解决了内存和计算效率问题，显著提升了服务吞吐量并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有并行化方法在异构环境中扩展效率低，无法充分利用异构硬件的潜力。

Method: 采用细粒度和动态并行化设计，选择性并行化计算密集型操作，动态分配注意力计算，并结合在线负载调度策略优化性能。

Result: Hetis比现有系统吞吐量提升2.25倍，延迟降低1.49倍。

Conclusion: Hetis在异构GPU集群中高效优化LLM服务性能。

Abstract: The significant resource demands in LLM serving prompts production clusters
to fully utilize heterogeneous hardware by partitioning LLM models across a mix
of high-end and low-end GPUs. However, existing parallelization approaches
often struggle to scale efficiently in heterogeneous environments due to their
coarse-grained and static parallelization strategies.
  In this paper, we introduce Hetis, a new LLM system tailored for
heterogeneous GPU clusters. Hetis addresses two critical challenges: (1) memory
inefficiency caused by the mismatch between memory capacity and computational
power in heterogeneous devices, and (2) computational inefficiency arising from
performance gaps across different LLM modules. To tackle these issues, Hetis
employs a fine-grained and dynamic parallelism design. Specifically, it
selectively parallelizes compute-intensive operations to reduce latency and
dynamically distributes Attention computations to low-end GPUs at a head
granularity, leveraging the distinct characteristics of each module.
Additionally, Hetis features an online load dispatching policy that
continuously optimizes serving performance by carefully balancing network
latency, computational load, and memory intensity. Evaluation results
demonstrate that Hetis can improve serving throughput by up to $2.25\times$ and
reduce latency by $1.49\times$ compared to existing systems.

</details>


### [45] [An HPC Benchmark Survey and Taxonomy for Characterization](https://arxiv.org/abs/2509.08347)
*Andreas Herten,Olga Pearce,Filipe S. M. Guimarães*

Main category: cs.DC

TL;DR: 该论文调查了高性能计算（HPC）领域的现有基准测试工具，提供了分类和详细总结。


<details>
  <summary>Details</summary>
Motivation: HPC领域需要高性能计算设备，而基准测试是评估硬件、软件和算法的关键工具。论文旨在整理和分类现有HPC基准测试，为系统架构师和研究人员提供参考。

Method: 通过调查现有HPC基准测试，以表格形式总结关键细节，并提出基准测试的分类法，同时通过交互式网站展示。

Result: 提供了一个详细的基准测试分类和总结，帮助用户更清晰地理解和选择适合的基准测试工具。

Conclusion: 论文为HPC领域的基准测试提供了系统化的分类和资源，有助于推动HPC技术的进一步发展。

Abstract: The field of High-Performance Computing (HPC) is defined by providing
computing devices with highest performance for a variety of demanding
scientific users. The tight co-design relationship between HPC providers and
users propels the field forward, paired with technological improvements,
achieving continuously higher performance and resource utilization. A key
device for system architects, architecture researchers, and scientific users
are benchmarks, allowing for well-defined assessment of hardware, software, and
algorithms. Many benchmarks exist in the community, from individual niche
benchmarks testing specific features, to large-scale benchmark suites for whole
procurements. We survey the available HPC benchmarks, summarizing them in table
form with key details and concise categorization, also through an interactive
website. For categorization, we present a benchmark taxonomy for well-defined
characterization of benchmarks.

</details>


### [46] [Towards Communication-Efficient Decentralized Federated Graph Learning over Non-IID Data](https://arxiv.org/abs/2509.08409)
*Shilong Wang,Jianchun Liu,Hongli Xu,Chenxia Tang,Qianpiao Ma,Liusheng Huang*

Main category: cs.DC

TL;DR: DFGL通过P2P通信网络避免FGL的参数服务器瓶颈，但通信成本高。现有方法通过稀疏拓扑或邻居采样减少开销，但直接结合会导致性能下降。Duplex框架联合优化拓扑与采样，显著降低通信成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决DFGL中高通信成本和现有方法结合导致的训练性能下降问题。

Method: 提出Duplex框架，联合优化网络拓扑和图采样，并自适应确定最优参数。

Result: Duplex减少完成时间20.1%--48.8%和通信成本16.7%--37.6%，同时提高准确率3.3%--7.9%。

Conclusion: Duplex通过联合优化策略有效提升了DFGL的通信效率和训练性能。

Abstract: Decentralized Federated Graph Learning (DFGL) overcomes potential bottlenecks
of the parameter server in FGL by establishing a peer-to-peer (P2P)
communication network among workers. However, while extensive cross-worker
communication of graph node embeddings is crucial for DFGL training, it
introduces substantial communication costs. Most existing works typically
construct sparse network topologies or utilize graph neighbor sampling methods
to alleviate the communication overhead in DFGL. Intuitively, integrating these
methods may offer promise for doubly improving communication efficiency in
DFGL. However, our preliminary experiments indicate that directly combining
these methods leads to significant training performance degradation if they are
jointly optimized. To address this issue, we propose Duplex, a unified
framework that jointly optimizes network topology and graph sampling by
accounting for their coupled relationship, thereby significantly reducing
communication cost while enhancing training performance in DFGL. To overcome
practical DFGL challenges, eg, statistical heterogeneity and dynamic network
environments, Duplex introduces a learning-driven algorithm to adaptively
determine optimal network topologies and graph sampling ratios for workers.
Experimental results demonstrate that Duplex reduces completion time by
20.1%--48.8% and communication costs by 16.7%--37.6% to achieve target
accuracy, while improving accuracy by 3.3%--7.9% under identical resource
budgets compared to baselines.

</details>


### [47] [A 410GFLOP/s, 64 RISC-V Cores, 204.8GBps Shared-Memory Cluster in 12nm FinFET with Systolic Execution Support for Efficient B5G/6G AI-Enhanced O-RAN](https://arxiv.org/abs/2509.08608)
*Yichao Zhang,Marco Bertuletti,Sergio Mazzola,Samuel Riedel,Luca Benini*

Main category: cs.DC

TL;DR: HeartStream是一个64核共享L1内存集群，专为高效AI增强的O-RAN设计，支持复杂指令和硬件管理的队列，能效提升达1.89倍。


<details>
  <summary>Details</summary>
Motivation: 为满足B5G/6G上行链路的能效和处理延迟要求，设计一个高效的基带处理集群。

Method: 定制化64核共享L1内存集群，支持复杂指令（如乘法累加）、SIMD指令和硬件管理队列。

Result: 在800MHz@0.8V下提供243GFLOP/s性能，能效达49.6GFLOP/s/W，功耗仅0.68W，满足4ms延迟限制。

Conclusion: HeartStream在能效和性能上表现优异，适用于B5G/6G上行链路的基带处理。

Abstract: We present HeartStream, a 64-RV-core shared-L1-memory cluster (410 GFLOP/s
peak performance and 204.8 GBps L1 bandwidth) for energy-efficient AI-enhanced
O-RAN. The cores and cluster architecture are customized for baseband
processing, supporting complex (16-bit real&imaginary) instructions:
multiply&accumulate, division&square-root, SIMD instructions, and
hardware-managed systolic queues, improving up to 1.89x the energy efficiency
of key baseband kernels. At 800MHz@0.8V, HeartStream delivers up to 243GFLOP/s
on complex-valued wireless workloads. Furthermore, the cores also support
efficient AI processing on received data at up to 72 GOP/s. HeartStream is
fully compatible with base station power and processing latency limits: it
achieves leading-edge software-defined PUSCH efficiency (49.6GFLOP/s/W) and
consumes just 0.68W (645MHz@0.65V), within the 4 ms end-to-end constraint for
B5G/6G uplink.

</details>


### [48] [Reconfigurable Holographic Surfaces and Near Field Communication for Non-Terrestrial Networks: Potential and Challenges](https://arxiv.org/abs/2509.08770)
*Muhammad Ali Jamshed,Muhammad Ahmed Mohsin,Hongliang Zhang,Bushra Haq,Aryan Kaushik,Boya Di,Weiwei Jiang*

Main category: cs.DC

TL;DR: 论文提出结合近场通信（NFC）和可重构全息表面（RHS）的非地面网络（NTN）方案，以解决超低延迟、全覆盖和高数据速率的挑战。


<details>
  <summary>Details</summary>
Motivation: 应对超低延迟、全覆盖和更高数据速率的需求，探索RHS与NTN平台的结合潜力。

Method: 设计系统架构，整合RHS与卫星、高空平台（HAPS）和无人机（UAV），实现近场区域的精确波束成形和智能波前控制。

Result: 提升能效（EE）、频谱利用率和空间分辨率，并通过公共安全用例分析验证无人机-RHS融合的优势。

Conclusion: RHS与NTN平台的结合具有广阔应用前景，但需解决关键挑战以推动其全面采用。

Abstract: To overcome the challenges of ultra-low latency, ubiquitous coverage, and
soaring data rates, this article presents a combined use of Near Field
Communication (NFC) and Reconfigurable Holographic Surfaces (RHS) for
Non-Terrestrial Networks (NTN). A system architecture has been presented, which
shows that the integration of RHS with NTN platforms such as satellites, High
Altitute Platform Stations (HAPS), and Uncrewed Aerial Vehicles (UAV) can
achieve precise beamforming and intelligent wavefront control in near-field
regions, enhancing Energy Efficiency (EE), spectral utilization, and spatial
resolution. Moreover, key applications, challenges, and future directions have
been identified to fully adopt this integration. In addition, a use case
analysis has been presented to improve the EE of the system in a public safety
use case scenario, further strengthening the UAV-RHS fusion.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [49] [Polyglot Persistence in Microservices: Managing Data Diversity in Distributed Systems](https://arxiv.org/abs/2509.08014)
*Festim Halili,Anila Nuhiji,Diellza Mustafai Veliu*

Main category: cs.DB

TL;DR: 论文研究了微服务架构中的多语言持久性策略，结合理论与实践评估了多种数据库的性能与挑战，并提出了应对复杂性的架构模式。


<details>
  <summary>Details</summary>
Motivation: 微服务架构虽提升可扩展性，但异构分布式数据管理带来挑战，需研究多语言持久性策略以优化性能与适应性。

Method: 结合理论概念与实证数据，采用比较框架评估关系型、文档型等数据库，并分析行业案例及调查数据。

Result: 多语言持久性提升适应性、性能及领域对齐，但也增加治理和操作复杂性。

Conclusion: 需采用如saga工作流等架构模式来平衡多语言持久性的优缺点。

Abstract: Microservices architectures have become the foundation for developing
scalable and modern software systems, but they also bring significant
challenges in managing heterogeneous and distributed data. The pragmatic
solution is polyglot persistence, the deliberate use of several different
database technologies adapted to a given microservice requirement - is one such
strategy. This paper examines polyglot persistence in microservice based
systems. This paper brings together theoretical concepts with evidence from
practical implementations and comparative benchmarks of standard database
platforms. A comparative framework is applied to relational, document,
key-value, column-family and graph databases to assess scalability,
consistency, query expressiveness, operational overhead and integration ease.
Empirical data drawn from industry case studies such as Netflix, Uber, and
Shopify, and survey data illustrate real-life adoption trends and challenges.
These findings demonstrate that polyglot persistence increases adaptability ,
performance , domain alignment but also governance or operational complexity.
To cope with such trade-offs, architectural patterns such as saga workflows,
event sourcing, and outbox integration are discussed.

</details>


### [50] [Infinite Stream Estimation under Personalized $w$-Event Privacy](https://arxiv.org/abs/2509.08387)
*Leilei Du,Peng Cheng,Lei Chen,Heng Tao Shen,Xuemin Lin,Wei Xi*

Main category: cs.DB

TL;DR: 本文提出了个性化的$w$-事件隐私保护方法（PWSM、PBD和PBA），解决了现有流数据隐私保护中隐私需求单一的问题，显著提升了数据准确性。


<details>
  <summary>Details</summary>
Motivation: 现有$w$-事件隐私研究大多假设所有用户有相同的隐私需求，而现实中用户需求各异。本文旨在提供个性化的隐私保护机制，同时保持数据的高精度。

Method: 提出了PWSM机制，让用户在每个时间段保持固定的隐私需求；设计了PBD和PBA两种方法，分别通过动态分配和吸收隐私预算来实现高精度的流数据统计。

Result: PBD和PBA在真实和合成数据集上表现优异，PBD在真实数据集上平均误差比BD低68%，PBA在合成数据集上平均误差比BA低24.9%。

Conclusion: 个性化隐私保护机制（PBD和PBA）在满足多样化用户需求的同时，显著提升了数据估计的准确性，优于现有统一隐私保护方法。

Abstract: Streaming data collection is indispensable for stream data analysis, such as
event monitoring. However, publishing these data directly leads to privacy
leaks. $w$-event privacy is a valuable tool to protect individual privacy
within a given time window while maintaining high accuracy in data collection.
Most existing $w$-event privacy studies on infinite data stream only focus on
homogeneous privacy requirements for all users. In this paper, we propose
personalized $w$-event privacy protection that allows different users to have
different privacy requirements in private data stream estimation. Specifically,
we design a mechanism that allows users to maintain constant privacy
requirements at each time slot, namely Personalized Window Size Mechanism
(PWSM). Then, we propose two solutions to accurately estimate stream data
statistics while achieving $w$-event level $\epsilon$ personalized differential
privacy ( ($w$, $\epsilon$)-EPDP), namely Personalized Budget Distribution
(PBD) and Peronalized Budget Absorption (PBA). PBD always provides at least the
same privacy budget for the next time step as the amount consumed in the
previous release. PBA fully absorbs the privacy budget from the previous $k$
time slots, while also borrowing from the privacy budget of the next $k$ time
slots, to increase the privacy budget for the current time slot. We prove that
both PBD and PBA outperform the state-of-the-art private stream estimation
methods while satisfying the privacy requirements of all users. We demonstrate
the efficiency and effectiveness of our PBD and PBA on both real and synthetic
data sets, compared with the recent uniformity $w$-event approaches, Budget
Distribution (BD) and Budget Absorption (BA). Our PBD achieves 68% less error
than BD on average on real data sets. Besides, our PBA achieves 24.9% less
error than BA on average on synthetic data sets.

</details>


### [51] [SINDI: an Efficient Index for Approximate Maximum Inner Product Search on Sparse Vectors](https://arxiv.org/abs/2509.08395)
*Ruoxuan Li,Xiaoyao Zhong,Jiabao Jin,Peng Cheng,Wangze Ni,Lei Chen,Zhitao Shen,Wei Jia,Xiangyu Wang,Xuemin Lin,Heng Tao Shen,Jingkuan Song*

Main category: cs.DB

TL;DR: 论文提出了一种名为SINDI的稀疏向量最大内积搜索（MIPS）优化方法，通过SIMD加速、内存友好设计和向量修剪，显著提高了检索效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于倒排索引和图结构的稀疏向量MIPS算法在实际生产环境中因冗余距离计算和频繁随机内存访问而受限，且压缩存储格式阻碍了SIMD加速的应用。

Method: SINDI结合了三种优化：利用SIMD加速实现高效内积计算，内存友好设计减少随机访问，向量修剪保留高幅值非零项。

Result: 在多个真实数据集上，SINDI表现出色，尤其在MsMarco数据集上，Recall@50超过99%时，单线程QPS提升4.2至26.4倍。

Conclusion: SINDI已被集成到Ant Group的开源向量搜索库VSAG中，展现了其在多样数据集上的优越性能和实用性。

Abstract: Sparse vector Maximum Inner Product Search (MIPS) is crucial in multi-path
retrieval for Retrieval-Augmented Generation (RAG). Recent inverted index-based
and graph-based algorithms have achieved high search accuracy with practical
efficiency. However, their performance in production environments is often
limited by redundant distance computations and frequent random memory accesses.
Furthermore, the compressed storage format of sparse vectors hinders the use of
SIMD acceleration. In this paper, we propose the sparse inverted non-redundant
distance index (SINDI), which incorporates three key optimizations: (i)
Efficient Inner Product Computation: SINDI leverages SIMD acceleration and
eliminates redundant identifier lookups, enabling batched inner product
computation; (ii) Memory-Friendly Design: SINDI replaces random memory accesses
to original vectors with sequential accesses to inverted lists, substantially
reducing memory-bound latency. (iii) Vector Pruning: SINDI retains only the
high-magnitude non-zero entries of vectors, improving query throughput while
maintaining accuracy. We evaluate SINDI on multiple real-world datasets.
Experimental results show that SINDI achieves state-of-the-art performance
across datasets of varying scales, languages, and models. On the MsMarco
dataset, when Recall@50 exceeds 99%, SINDI delivers single-thread
query-per-second (QPS) improvements ranging from 4.2 to 26.4 times compared
with SEISMIC and PyANNs. Notably, SINDI has been integrated into Ant Group's
open-source vector search library, VSAG.

</details>


### [52] [Un cadre paraconsistant pour l'{é}valuation de similarit{é} dans les bases de connaissances](https://arxiv.org/abs/2509.08433)
*José-Luis Vilchis Medina*

Main category: cs.DB

TL;DR: 本文提出了一个基于次协调逻辑的相似性评估框架，用于知识库中的矛盾处理。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理知识库中的矛盾时效果不佳，因此需要一个能够显式整合矛盾并提升相似性测量鲁棒性和可解释性的框架。

Method: 引入新的测量指标$S^*$，惩罚不一致性并奖励共同属性；定义次协调超类$\Xi_K^*$层级组织知识实体；设计矛盾提取器$E$和修复机制以确保评估一致性。

Result: 理论结果证明了$S^*$的自反性、对称性和有界性。

Conclusion: 该框架为处理冲突知识提供了有效方法，在多智能体系统中具有应用前景。

Abstract: This article proposes a paraconsistent framework for evaluating similarity in
knowledge bases. Unlike classical approaches, this framework explicitly
integrates contradictions, enabling a more robust and interpretable similarity
measure. A new measure $ S^* $ is introduced, which penalizes inconsistencies
while rewarding shared properties. Paraconsistent super-categories $ \Xi_K^* $
are defined to hierarchically organize knowledge entities. The model also
includes a contradiction extractor $ E $ and a repair mechanism, ensuring
consistency in the evaluations. Theoretical results guarantee reflexivity,
symmetry, and boundedness of $ S^* $. This approach offers a promising solution
for managing conflicting knowledge, with perspectives in multi-agent systems.

</details>


### [53] [SQLGovernor: An LLM-powered SQL Toolkit for Real World Application](https://arxiv.org/abs/2509.08575)
*Jie Jiang,Siqi Shen,Haining Xie,Yang Li,Yu Shen,Danqing Huang,Bo Qian,Yinjun Wu,Wentao Zhang,Bin Cui,Peng Chen*

Main category: cs.DB

TL;DR: SQLGovernor是一个基于LLM的SQL工具包，通过语法纠正、查询重写等功能提升SQL查询的准确性和效率，并在实验中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的SQL查询常存在语法错误、效率低下等问题，尤其在复杂OLAP场景中，亟需智能工具提升其质量。

Method: SQLGovernor引入分片处理策略和混合自学习机制，结合专家反馈和DBMS输出分析，实现精细化重写和错误纠正。

Result: 实验显示，SQLGovernor能将基准模型性能提升高达10%，并在生产环境中表现优异。

Conclusion: SQLGovernor通过智能化和自动化方法有效解决了SQL查询的常见问题，具有显著的实用价值。

Abstract: SQL queries in real world analytical environments, whether written by humans
or generated automatically often suffer from syntax errors, inefficiency, or
semantic misalignment, especially in complex OLAP scenarios. To address these
challenges, we propose SQLGovernor, an LLM powered SQL toolkit that unifies
multiple functionalities, including syntax correction, query rewriting, query
modification, and consistency verification within a structured framework
enhanced by knowledge management. SQLGovernor introduces a fragment wise
processing strategy to enable fine grained rewriting and localized error
correction, significantly reducing the cognitive load on the LLM. It further
incorporates a hybrid self learning mechanism guided by expert feedback,
allowing the system to continuously improve through DBMS output analysis and
rule validation. Experiments on benchmarks such as BIRD and BIRD CRITIC, as
well as industrial datasets, show that SQLGovernor consistently boosts the
performance of base models by up to 10%, while minimizing reliance on manual
expertise. Deployed in production environments, SQLGovernor demonstrates strong
practical utility and effective performance.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [54] [Analyzing the capabilities of HLS and RTL tools in the design of an FPGA Montgomery Multiplier](https://arxiv.org/abs/2509.08067)
*Rares Ifrim,Decebal Popescu*

Main category: cs.AR

TL;DR: 论文分析了基于FPGA的Montgomery模乘器实现，使用CIOS方法针对BLS12-381椭圆曲线优化，目标是实现高频高吞吐量，为ECC操作提供支持。


<details>
  <summary>Details</summary>
Motivation: 为椭圆曲线密码学操作（如点加和点乘）提供高效硬件支持，同时优化FPGA资源使用和频率。

Method: 采用三种设计：Vivado自动选择DSP的原生Verilog、手动实例化DSP的优化Verilog，以及HLS方法，并与Rust软件实现对比。

Result: 通过不同设计和工具配置比较，分析频率、延迟和资源消耗的影响。

Conclusion: 研究展示了如何通过设计选择与工具配置优化FPGA实现的性能，为ECC硬件加速提供参考。

Abstract: We present the analysis of various FPGA design implementations of a
Montgomery Modular Multiplier, compatible with the BLS12-381 elliptic curve,
using the Coarsely Integrated Operand Scanning approach of working with
complete partial products on different digit sizes. The scope of the
implemented designs is to achieve a high-frequency, high-throughput solution
capable of computing millions of operations per second, which can provide a
strong foundation for different Elliptic Curve Cryptography operations such as
point addition and point multiplication. One important constraint for our
designs was to only use FPGA DSP primitives for the arithmetic operations
between digits employed in the CIOS algorithm as these primitives, when
pipelined properly, can operate at a high frequency while also relaxing the
resource consumption of FPGA LUTs and FFs. The target of the analysis is to see
how different design choices and tool configurations influence the frequency,
latency and resource consumption when working with the latest AMD-Xilinx tools
and Alveo FPGA boards in an RTL-HLS hybrid approach. We compare three
categories of designs: a Verilog naive approach where we rely on the Vivado
synthesizer to automatically choose when and where to use DSPs, a Verilog
optimized approach by manually instantiating the DSP primitives ourselves and a
complete High-Level Synthesis approach. We also compare the FPGA
implementations with an optimized software implementation of the same
Montgomery multiplier written in Rust.

</details>


### [55] [Lifetime-Aware Design of Item-Level Intelligence](https://arxiv.org/abs/2509.08193)
*Shvetank Prakash,Andrew Cheng,Olof Kindgren,Ashiq Ahamed,Graham Knight,Jed Kufel,Francisco Rodriguez,Arya Tschand,David Kong,Mariam Elgamal,Jerry Huang,Emma Chen,Gage Hills,Richard Price,Emre Ozer,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: FlexiFlow是一个面向物品级智能（ILI）的生命周期感知设计框架，适用于低成本、低速的柔性电子设备，通过优化架构和算法显著减少碳足迹。


<details>
  <summary>Details</summary>
Motivation: 传统计算架构在ILI应用中因部署寿命的巨大差异和柔性电子的限制而效率低下，FlexiFlow旨在解决这一问题。

Method: 框架包括FlexiBench工作负载套件、FlexiBits优化的RISC-V核心以及碳感知模型，结合柔性电子技术实现优化设计。

Result: 生命周期感知设计减少碳足迹1.62倍，算法优化减少14.5倍，验证了柔性电子30.9kHz的运行性能。

Conclusion: FlexiFlow为极端边缘计算提供了新的设计方法，强调了柔性电子和碳足迹优化的重要性。

Abstract: We present FlexiFlow, a lifetime-aware design framework for item-level
intelligence (ILI) where computation is integrated directly into disposable
products like food packaging and medical patches. Our framework leverages
natively flexible electronics which offer significantly lower costs than
silicon but are limited to kHz speeds and several thousands of gates. Our
insight is that unlike traditional computing with more uniform deployment
patterns, ILI applications exhibit 1000X variation in operational lifetime,
fundamentally changing optimal architectural design decisions when considering
trillion-item deployment scales. To enable holistic design and optimization, we
model the trade-offs between embodied carbon footprint and operational carbon
footprint based on application-specific lifetimes. The framework includes: (1)
FlexiBench, a workload suite targeting sustainability applications from
spoilage detection to health monitoring; (2) FlexiBits, area-optimized RISC-V
cores with 1/4/8-bit datapaths achieving 2.65X to 3.50X better energy
efficiency per workload execution; and (3) a carbon-aware model that selects
optimal architectures based on deployment characteristics. We show that
lifetime-aware microarchitectural design can reduce carbon footprint by 1.62X,
while algorithmic decisions can reduce carbon footprint by 14.5X. We validate
our approach through the first tape-out using a PDK for flexible electronics
with fully open-source tools, achieving 30.9kHz operation. FlexiFlow enables
exploration of computing at the Extreme Edge where conventional design
methodologies must be reevaluated to account for new constraints and
considerations.

</details>


### [56] [FASE: FPGA-Assisted Syscall Emulation for Rapid End-to-End Processor Performance Validation](https://arxiv.org/abs/2509.08405)
*Chengzhen Meng,Xiuzhuang Chen,Hongjun Dai*

Main category: cs.AR

TL;DR: FASE框架通过FPGA辅助的系统调用仿真，支持复杂多线程基准测试在处理器设计上直接运行，显著提升了早期性能验证的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: AI工作负载和领域特定架构的快速发展导致处理器微架构多样化，传统验证流程延迟到RTL设计和SoC集成完成后，延长了开发周期。

Method: FASE框架引入三个关键创新：最小CPU接口、主机目标协议（HTP）和主机端运行时，分别解决了FPGA系统统一接口缺失、低带宽高延迟通信和跨设备系统调用委托的挑战。

Result: 在Xilinx FPGA上的实验显示，FASE单线程CoreMark性能误差小于1%，效率比Proxy Kernel高2000倍以上；复杂OpenMP基准测试的验证准确率高达96%（单线程）和91.5%（多线程）。

Conclusion: FASE显著降低了开发复杂性和反馈时间，所有组件已开源。

Abstract: The rapid advancement of AI workloads and domain-specific architectures has
led to increasingly diverse processor microarchitectures, whose design
exploration requires fast and accurate performance validation. However,
traditional workflows defer validation process until RTL design and SoC
integration are complete, significantly prolonging development and iteration
cycle.
  In this work, we present FASE framework, FPGA-Assisted Syscall Emulation, the
first work for adapt syscall emulation on FPGA platforms, enabling complex
multi-thread benchmarks to directly run on the processor design without
integrating SoC or target OS for early-stage performance validation. FASE
introduces three key innovations to address three critical challenges for
adapting FPGA-based syscall emulation: (1) only a minimal CPU interface is
exposed, with other hardware components untouched, addressing the lack of a
unified hardware interface in FPGA systems; (2) a Host-Target Protocol (HTP) is
proposed to minimize cross-device data traffic, mitigating the low-bandwidth
and high-latency communication between FPGA and host; and (3) a host-side
runtime is proposed to remotely handle Linux-style system calls, addressing the
challenge of cross-device syscall delegation.
  Experiments ware conducted on Xilinx FPGA with open-sourced RISC-V SMP
processor Rocket. With single-thread CoreMark, FASE introduces less than 1%
performance error and achieves over 2000x higher efficiency compared to Proxy
Kernel due to FPGA acceleration. With complex OpenMP benchmarks, FASE
demonstrates over 96% performance validation accuracy for most single-thread
workloads and over 91.5% for most multi-thread workloads compared to full SoC
validation, significantly reducing development complexity and time-to-feedback.
All components of FASE framework are released as open-source.

</details>


### [57] [AutoVeriFix: Automatically Correcting Errors and Enhancing Functional Correctness in LLM-Generated Verilog Code](https://arxiv.org/abs/2509.08416)
*Yan Tan,Xiangchen Meng,Zijun Jiang,Yangdi Lyu*

Main category: cs.AR

TL;DR: AutoVeriFix是一个新颖的两阶段框架，通过Python参考模型和自动化测试提高LLM生成的Verilog代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: LLM在硬件描述语言（如Verilog）上训练数据稀缺，现有方法生成代码功能错误较多，需改进。

Method: 第一阶段用LLM生成Python参考模型，第二阶段用自动化测试指导Verilog RTL实现，通过迭代模拟修正错误。

Result: 实验显示AutoVeriFix在提升Verilog代码功能正确性上显著优于现有方法。

Conclusion: AutoVeriFix有效提高了LLM生成的Verilog代码的功能准确性和可靠性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
generating software code for high-level programming languages such as Python
and C++. However, their application to hardware description languages, such as
Verilog, is challenging due to the scarcity of high-quality training data.
Current approaches to Verilog code generation using LLMs often focus on
syntactic correctness, resulting in code with functional errors. To address
these challenges, we present AutoVeriFix, a novel Python-assisted two-stage
framework designed to enhance the functional correctness of LLM-generated
Verilog code. In the first stage, LLMs are employed to generate high-level
Python reference models that define the intended circuit behavior. In the
second stage, these Python models facilitate the creation of automated tests
that guide the generation of Verilog RTL implementations. Simulation
discrepancies between the reference model and the Verilog code are iteratively
used to identify and correct errors, thereby improving the functional accuracy
and reliability of the LLM-generated Verilog code. Experimental results
demonstrate that our approach significantly outperforms existing
state-of-the-art methods in improving the functional correctness of generated
Verilog code.

</details>


### [58] [BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter 1.58-bit LLM Inference](https://arxiv.org/abs/2509.08542)
*Wenlun Zhang,Xinyu Li,Shimpei Ando,Kentaro Yoshioka*

Main category: cs.AR

TL;DR: BitROM是一种基于CiROM技术的加速器，通过BitNet的1.58位量化模型解决LLM的规模限制，实现了高效的边缘推理。


<details>
  <summary>Details</summary>
Motivation: 尽管CiROM加速器在CNN中表现出色，但其在大型语言模型（LLM）中的可扩展性受限，原因是参数规模庞大。例如，LLaMA-7B即使在先进的CMOS节点中也需要超过1,000 cm²的硅面积。

Method: BitROM通过三种创新技术解决问题：1) 双向ROM阵列，每晶体管存储两个三元权重；2) 三元权重计算优化的Tri-Mode局部累加器；3) 集成的Decode-Refresh eDRAM，减少解码时的外部内存访问。此外，还集成了LoRA适配器以支持跨任务迁移学习。

Result: 在65nm CMOS中，BitROM实现了20.8 TOPS/W的能效和4,967 kB/mm²的位密度，比之前的数字CiROM设计提高了10倍的面积效率。DR eDRAM还减少了43.6%的外部DRAM访问。

Conclusion: BitROM通过创新的设计和量化技术，显著提升了LLM在边缘计算中的效率，为实际应用提供了可行方案。

Abstract: Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy
efficiency for CNNs by eliminating runtime weight updates. However, their
scalability to Large Language Models (LLMs) is fundamentally constrained by
their vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA
series - demands more than 1,000 cm2 of silicon area even in advanced CMOS
nodes. This paper presents BitROM, the first CiROM-based accelerator that
overcomes this limitation through co-design with BitNet's 1.58-bit quantization
model, enabling practical and efficient LLM inference at the edge. BitROM
introduces three key innovations: 1) a novel Bidirectional ROM Array that
stores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator
optimized for ternary-weight computations; and 3) an integrated Decode-Refresh
(DR) eDRAM that supports on-die KV-cache management, significantly reducing
external memory access during decoding. In addition, BitROM integrates
LoRA-based adapters to enable efficient transfer learning across various
downstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit
density of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over
prior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%
reduction in external DRAM access, further enhancing deployment efficiency for
LLMs in edge applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [59] [Securing Cryptographic Software via Typed Assembly Language (Extended Version)](https://arxiv.org/abs/2509.08727)
*Shixin Song,Tingzhen Dong,Kosi Nwabueze,Julian Zanders,Andres Erbsen,Adam Chlipala,Mengjia Yan*

Main category: cs.CR

TL;DR: SecSep是一种通过重写汇编程序来分离堆栈上的秘密和公共数据的框架，解决了现有方法的局限性，平均开销仅为1.2%。


<details>
  <summary>Details</summary>
Motivation: 传统的恒定时间编码指南无法应对Spectre推测执行攻击，现有方法在跟踪堆栈上的秘密数据时存在局限性和性能开销。

Method: 提出SecSep框架，采用新型类型化汇编语言Octal，在编译时推断并重写汇编程序，实现堆栈数据的隔离。

Result: SecSep能高效实现安全推测，应用于密码程序时平均仅产生1.2%的性能开销。

Conclusion: SecSep通过汇编重写有效解决了堆栈数据隔离的挑战，为抵御推测执行攻击提供了可行方案。

Abstract: Authors of cryptographic software are well aware that their code should not
leak secrets through its timing behavior, and, until 2018, they believed that
following industry-standard constant-time coding guidelines was sufficient.
However, the revelation of the Spectre family of speculative execution attacks
injected new complexities.
  To block speculative attacks, prior work has proposed annotating the
program's source code to mark secret data, with hardware using this information
to decide when to speculate (i.e., when only public values are involved) or not
(when secrets are in play). While these solutions are able to track secret
information stored on the heap, they suffer from limitations that prevent them
from correctly tracking secrets on the stack, at a cost in performance.
  This paper introduces SecSep, a transformation framework that rewrites
assembly programs so that they partition secret and public data on the stack.
By moving from the source-code level to assembly rewriting, SecSep is able to
address limitations of prior work. The key challenge in performing this
assembly rewriting stems from the loss of semantic information through the
lengthy compilation process. The key innovation of our methodology is a new
variant of typed assembly language (TAL), Octal, which allows us to address
this challenge. Assembly rewriting is driven by compile-time inference within
Octal. We apply our technique to cryptographic programs and demonstrate that it
enables secure speculation efficiently, incurring a low average overhead of
$1.2\%$.

</details>


### [60] [Approximate Algorithms for Verifying Differential Privacy with Gaussian Distributions](https://arxiv.org/abs/2509.08804)
*Bishnu Bhusal,Rohit Chadha,A. Prasad Sistla,Mahesh Viswanathan*

Main category: cs.CR

TL;DR: 论文提出了一种新方法，用于验证使用高斯分布的差分隐私算法，证明了验证问题在大多数情况下是可判定的，并开发了工具DipApprox实现该方法。


<details>
  <summary>Details</summary>
Motivation: 当前对使用高斯分布的差分隐私算法的验证缺乏深入理解，亟需一种有效方法来验证这类程序的隐私保证。

Method: 通过近似概率分布和计算高精度积分，结合尾概率边界，提出了一种验证算法，并在工具DipApprox中实现。

Result: 验证算法证明了$(\epsilon,\delta)$-差分隐私问题在除有限$\delta$值外是可判定的，工具DipApprox在隐私算法中表现有效。

Conclusion: 论文提出的方法能有效验证差分隐私算法，工具DipApprox在实际应用中展示了其确认和检测隐私问题的能力。

Abstract: The verification of differential privacy algorithms that employ Gaussian
distributions is little understood. This paper tackles the challenge of
verifying such programs by introducing a novel approach to approximating
probability distributions of loop-free programs that sample from both discrete
and continuous distributions with computable probability density functions,
including Gaussian and Laplace. We establish that verifying
$(\epsilon,\delta)$-differential privacy for these programs is \emph{almost
decidable}, meaning the problem is decidable for all values of $\delta$ except
those in a finite set. Our verification algorithm is based on computing
probabilities to any desired precision by combining integral approximations,
and tail probability bounds. The proposed methods are implemented in the tool,
DipApprox, using the FLINT library for high-precision integral computations,
and incorporate optimizations to enhance scalability. We validate {\ourtool} on
fundamental privacy-preserving algorithms, such as Gaussian variants of the
Sparse Vector Technique and Noisy Max, demonstrating its effectiveness in both
confirming privacy guarantees and detecting violations.

</details>


### [61] [Membrane: A Cryptographic Access Control System for Data Lakes](https://arxiv.org/abs/2509.08740)
*Sam Kumar,Samyukta Yagati,Conor Power,David E. Culler,Raluca Ada Popa*

Main category: cs.CR

TL;DR: Membrane是一种在数据湖中强制执行数据依赖访问控制的系统，结合静态加密和SQL感知加密，抵御远程攻击，虽然首次查询延迟较高，但总体性能影响较低。


<details>
  <summary>Details</summary>
Motivation: 组织使用数据湖存储敏感数据，但黑客可能绕过访问控制获取数据，需一种既能保护数据又不限制分析查询的方案。

Method: 结合静态加密与SQL感知加密，使用硬件加速的块密码实现高效的SQL感知加密协议。

Result: 首次查询延迟增加20倍，但后续查询性能影响低。

Conclusion: Membrane通过加密技术和SQL感知协议有效平衡了数据安全与分析灵活性。

Abstract: Organizations use data lakes to store and analyze sensitive data. But hackers
may compromise data lake storage to bypass access controls and access sensitive
data. To address this, we propose Membrane, a system that (1) cryptographically
enforces data-dependent access control views over a data lake, (2) without
restricting the analytical queries data scientists can run. We observe that
data lakes, unlike DBMSes, disaggregate computation and storage into separate
trust domains, making at-rest encryption sufficient to defend against remote
attackers targeting data lake storage, even when running analytical queries in
plaintext. This leads to a new system design for Membrane that combines
encryption at rest with SQL-aware encryption. Using block ciphers, a fast
symmetric-key primitive with hardware acceleration in CPUs, we develop a new
SQL-aware encryption protocol well-suited to at-rest encryption. Membrane adds
overhead only at the start of an interactive session due to decrypting views,
delaying the first query result by up to $\approx 20\times$; subsequent queries
process decrypted data in plaintext, resulting in low amortized overhead.

</details>


### [62] [Unlocking Reproducibility: Automating re-Build Process for Open-Source Software](https://arxiv.org/abs/2509.08204)
*Behnaz Hassanshahi,Trong Nhan Mai,Benjamin Selwyn Smith,Nicholas Allen*

Main category: cs.CR

TL;DR: 该论文探讨了Maven Central中二进制文件与源代码分离的问题，提出通过自动化重建解决方案提升供应链安全性。


<details>
  <summary>Details</summary>
Motivation: Maven Central中二进制文件与源代码分离导致安全风险，84%的常用构件未使用透明CI/CD流水线构建，用户需信任构建环境。重建构件可增强安全性但面临环境差异和构建失败等挑战。

Method: 扩展Macaron框架以自动化重建Maven构件，改进源代码检测性能并从GitHub Actions自动提取构建规范，分析构建失败原因并提出可扩展解决方案。

Result: 提出的方法提高了构建透明度和安全性，解决了依赖图复杂性和构建环境差异问题。

Conclusion: 通过自动化重建Maven构件，论文有效提升了开源供应链的安全性和透明度。

Abstract: Software ecosystems like Maven Central play a crucial role in modern software
supply chains by providing repositories for libraries and build plugins.
However, the separation between binaries and their corresponding source code in
Maven Central presents a significant challenge, particularly when it comes to
linking binaries back to their original build environment. This lack of
transparency poses security risks, as approximately 84% of the top 1200
commonly used artifacts are not built using a transparent CI/CD pipeline.
Consequently, users must place a significant amount of trust not only in the
source code but also in the environment in which these artifacts are built.
  Rebuilding software artifacts from source provides a robust solution to
improve supply chain security. This approach allows for a deeper review of
code, verification of binary-source equivalence, and control over dependencies.
However, challenges arise due to variations in build environments, such as JDK
versions and build commands, which can lead to build failures. Additionally,
ensuring that all dependencies are rebuilt from source across large and complex
dependency graphs further complicates the process. In this paper, we introduce
an extension to Macaron, an industry-grade open-source supply chain security
framework, to automate the rebuilding of Maven artifacts from source. Our
approach improves upon existing tools, by offering better performance in source
code detection and automating the extraction of build specifications from
GitHub Actions workflows. We also present a comprehensive root cause analysis
of build failures in Java projects and propose a scalable solution to automate
the rebuilding of artifacts, ultimately enhancing security and transparency in
the open-source supply chain.

</details>


### [63] [EFPIX: A zero-trust encrypted flood protocol](https://arxiv.org/abs/2509.08248)
*Arin Upadhyay*

Main category: cs.CR

TL;DR: 提出了一种基于洪泛的中继通信协议，实现端到端加密、用户的可否认性及消息的不可追踪性，并具备抗拓扑变化和基础设施故障的能力，同时隐藏发送者和接收者的元数据。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有通信协议在隐私保护和抗故障能力方面的不足，特别是在拓扑变化和基础设施故障情况下的可靠性与安全性问题。

Method: 采用洪泛机制，通过多跳中继实现消息传输，并对消息进行端到端加密设计，确保只有接收者能解密，同时实现元数据隐藏。

Result: 该协议不仅提供了高安全性和隐私保护，还具备较强的鲁棒性，能够适应网络拓扑的变化和基础设施的故障。

Conclusion: 该协议在隐私保护和抗故障方面表现出色，适用于需要高安全性和可靠性的通信场景。

Abstract: We propose a flood-based relay communication protocol that achieves
end-to-end encryption, plausible deniability for users, and untraceable
messages. It is resistant to changes in topology and infrastructure failures.
It is also designed to hide metadata, such as sender and receiver, from those
not involved.

</details>


### [64] [DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation](https://arxiv.org/abs/2509.08449)
*Charuka Herath,Yogachandran Rahulamathavan,Varuna De Silva,Sangarapillai Lambotharan*

Main category: cs.CR

TL;DR: DSFL是一种双服务器拜占庭联邦学习框架，通过安全聚合、信用过滤和动态奖惩系统解决了FL中的数据隐私和鲁棒性问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有FL协议在应对拜占庭参与者、非独立同分布数据和轻量化方面存在不足，DSFL旨在解决这些问题。

Method: DSFL采用双服务器安全聚合协议、基于组的信用过滤机制和动态奖惩系统。

Result: 在MNIST、CIFAR-10和CIFAR-100数据集上，DSFL表现出色，准确率高且运行速度快。

Conclusion: DSFL在隐私保护和鲁棒性方面均优于现有方法，适合边缘设备使用。

Abstract: Federated Learning (FL) enables decentralized model training without sharing
raw data, offering strong privacy guarantees. However, existing FL protocols
struggle to defend against Byzantine participants, maintain model utility under
non-independent and identically distributed (non-IID) data, and remain
lightweight for edge devices. Prior work either assumes trusted hardware, uses
expensive cryptographic tools, or fails to address privacy and robustness
simultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated
Learning framework that addresses these limitations using a group-based secure
aggregation approach. Unlike LSFL, which assumes non-colluding semi-honest
servers, DSFL removes this dependency by revealing a key vulnerability: privacy
leakage through client-server collusion. DSFL introduces three key innovations:
(1) a dual-server secure aggregation protocol that protects updates without
encryption or key exchange, (2) a group-wise credit-based filtering mechanism
to isolate Byzantine clients based on deviation scores, and (3) a dynamic
reward-penalty system for enforcing fair participation. DSFL is evaluated on
MNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in
both IID and non-IID settings. It consistently outperforms existing baselines,
including LSFL, homomorphic encryption methods, and differential privacy
approaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and
68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar
threats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB
communication per round.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [65] [HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning](https://arxiv.org/abs/2509.08519)
*Liyang Chen,Tianxiang Ma,Jiawei Liu,Bingchuan Li,Zhuowei Chen,Lijie Liu,Xu He,Gen Li,Qian He,Zhiyong Wu*

Main category: cs.CV

TL;DR: HuMo是一个统一的人类中心视频生成框架，通过构建高质量数据集和两阶段训练范式解决多模态输入协调问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态输入协调和子任务协同处理上存在困难，HuMo旨在解决这些问题。

Method: 构建高质量数据集，采用两阶段训练范式，结合最小侵入式图像注入和音频视觉同步策略。

Result: HuMo在子任务上超越现有方法，实现了多模态协同的人类视频生成。

Conclusion: HuMo为多模态条件下的协同人类视频生成提供了统一框架。

Abstract: Human-Centric Video Generation (HCVG) methods seek to synthesize human videos
from multimodal inputs, including text, image, and audio. Existing methods
struggle to effectively coordinate these heterogeneous modalities due to two
challenges: the scarcity of training data with paired triplet conditions and
the difficulty of collaborating the sub-tasks of subject preservation and
audio-visual sync with multimodal inputs. In this work, we present HuMo, a
unified HCVG framework for collaborative multimodal control. For the first
challenge, we construct a high-quality dataset with diverse and paired text,
reference images, and audio. For the second challenge, we propose a two-stage
progressive multimodal training paradigm with task-specific strategies. For the
subject preservation task, to maintain the prompt following and visual
generation abilities of the foundation model, we adopt the minimal-invasive
image injection strategy. For the audio-visual sync task, besides the commonly
adopted audio cross-attention layer, we propose a focus-by-predicting strategy
that implicitly guides the model to associate audio with facial regions. For
joint learning of controllabilities across multimodal inputs, building on
previously acquired capabilities, we progressively incorporate the audio-visual
sync task. During inference, for flexible and fine-grained multimodal control,
we design a time-adaptive Classifier-Free Guidance strategy that dynamically
adjusts guidance weights across denoising steps. Extensive experimental results
demonstrate that HuMo surpasses specialized state-of-the-art methods in
sub-tasks, establishing a unified framework for collaborative
multimodal-conditioned HCVG. Project Page:
https://phantom-video.github.io/HuMo.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [66] [PianoVAM: A Multimodal Piano Performance Dataset](https://arxiv.org/abs/2509.08800)
*Yonghyun Kim,Junhyung Park,Joonhyung Bae,Kirak Kim,Taegyun Kwon,Alexander Lerch,Juhan Nam*

Main category: cs.SD

TL;DR: 这篇论文介绍了PianoVAM数据集，包含钢琴演奏的多模态数据（视频、音频、MIDI等），讨论了数据收集和标注方法，并展示了音频和音频-视觉转录的基准结果。


<details>
  <summary>Details</summary>
Motivation: 音乐表演的多模态特性激发了音乐信息检索（MIR）社区对非音频数据的兴趣，需要全面的数据集支持研究。

Method: 使用Disklavier钢琴录制业余钢琴家的日常练习，同步采集音频、MIDI和视频，并通过预训练模型和半自动算法标注手部关节点和指法。

Result: 展示了音频和音频-视觉转录的基准结果，证明了数据集的应用潜力。

Conclusion: PianoVAM是一个全面的多模态钢琴数据集，为音乐信息检索研究提供了新资源。

Abstract: The multimodal nature of music performance has driven increasing interest in
data beyond the audio domain within the music information retrieval (MIR)
community. This paper introduces PianoVAM, a comprehensive piano performance
dataset that includes videos, audio, MIDI, hand landmarks, fingering labels,
and rich metadata. The dataset was recorded using a Disklavier piano, capturing
audio and MIDI from amateur pianists during their daily practice sessions,
alongside synchronized top-view videos in realistic and varied performance
conditions. Hand landmarks and fingering labels were extracted using a
pretrained hand pose estimation model and a semi-automated fingering annotation
algorithm. We discuss the challenges encountered during data collection and the
alignment process across different modalities. Additionally, we describe our
fingering annotation method based on hand landmarks extracted from videos.
Finally, we present benchmarking results for both audio-only and audio-visual
piano transcription using the PianoVAM dataset and discuss additional potential
applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [67] [Interpretability as Alignment: Making Internal Understanding a Design Principle](https://arxiv.org/abs/2509.08592)
*Aadit Sengupta,Pratinav Seth,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 论文主张将可解释性（尤其是机制性方法）作为AI对齐的核心设计原则，而非辅助诊断工具，以提升AI的可审计性和透明度。


<details>
  <summary>Details</summary>
Motivation: 大型神经模型在高风险场景中的部署引发了对其行为是否可靠符合人类价值观的担忧。可解释性被提出作为实现内部透明性的途径。

Method: 论文比较了后验方法（如LIME、SHAP）与机制性技术（如电路追踪、激活修补），强调后者能提供因果性洞察。

Result: 尽管可解释性面临扩展性、认知不确定性和表示与人类概念不匹配等挑战，但它对安全可信的AI至关重要。

Conclusion: 为确保AI系统的有效性和对齐性，应将可解释性作为研究和开发的首要目标。

Abstract: Large neural models are increasingly deployed in high-stakes settings,
raising concerns about whether their behavior reliably aligns with human
values. Interpretability provides a route to internal transparency by revealing
the computations that drive outputs. We argue that interpretability especially
mechanistic approaches should be treated as a design principle for alignment,
not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer
intuitive but correlational explanations, while mechanistic techniques like
circuit tracing or activation patching yield causal insight into internal
failures, including deceptive or misaligned reasoning that behavioral methods
like RLHF, red teaming, or Constitutional AI may overlook. Despite these
advantages, interpretability faces challenges of scalability, epistemic
uncertainty, and mismatches between learned representations and human concepts.
Our position is that progress on safe and trustworthy AI will depend on making
interpretability a first-class objective of AI research and development,
ensuring that systems are not only effective but also auditable, transparent,
and aligned with human intent.

</details>


### [68] [Sketched Gaussian Mechanism for Private Federated Learning](https://arxiv.org/abs/2509.08195)
*Qiaobo Li,Zhijie Chen,Arindam Banerjee*

Main category: cs.LG

TL;DR: 论文提出的Sketched Gaussian Mechanism（SGM）结合了梯度压缩和高斯机制，提供了更强的隐私保护，且通信成本更低。理论分析和实验表明，SGM在相同隐私水平下性能优于非压缩方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信成本和隐私是两大核心问题。现有研究通常孤立分析梯度压缩和高斯机制的隐私效果，缺乏对两者结合的全面分析。

Method: 提出SGM机制，结合梯度压缩（通过降维）和高斯噪声添加，并利用Rényi-DP工具进行联合隐私分析。理论证明了隐私水平与降维维度b的平方根倒数成正比。

Result: 实验显示，SGM在相同隐私水平下性能与非压缩方法相当或更优，且服务器采用自适应优化器可进一步提升性能。

Conclusion: SGM机制通过联合分析证明了其在隐私保护与通信效率上的优势，为联邦学习提供了一种更灵活的解决方案。

Abstract: Communication cost and privacy are two major considerations in federated
learning (FL). For communication cost, gradient compression by sketching the
clients' transmitted model updates is often used for reducing per-round
communication. For privacy, the Gaussian mechanism (GM), which consists of
clipping updates and adding Gaussian noise, is commonly used to guarantee
client-level differential privacy. Existing literature on private FL analyzes
privacy of sketching and GM in an isolated manner, illustrating that sketching
provides privacy determined by the sketching dimension and that GM has to
supply any additional desired privacy.
  In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which
directly combines sketching and the Gaussian mechanism for privacy. Using
R\'enyi-DP tools, we present a joint analysis of SGM's overall privacy
guarantee, which is significantly more flexible and sharper compared to
isolated analysis of sketching and GM privacy. In particular, we prove that the
privacy level of SGM for a fixed noise magnitude is proportional to
$1/\sqrt{b}$, where $b$ is the sketching dimension, indicating that (for
moderate $b$) SGM can provide much stronger privacy guarantees than the
original GM under the same noise budget. We demonstrate the application of SGM
to FL with either gradient descent or adaptive server optimizers, and establish
theoretical results on optimization convergence, which exhibits only a
logarithmic dependence on the number of parameters $d$. Experimental results
confirm that at the same privacy level, SGM based FL is at least competitive
with non-sketching private FL variants and outperforms them in some settings.
Moreover, using adaptive optimization at the server improves empirical
performance while maintaining the privacy guarantees.

</details>


### [69] [PracMHBench: Re-evaluating Model-Heterogeneous Federated Learning Based on Practical Edge Device Constraints](https://arxiv.org/abs/2509.08750)
*Yuanchun Guo,Bingyan Liu,Yulong Sha,Zhensheng Xian*

Main category: cs.LG

TL;DR: 提出了第一个系统平台PracMHBench，用于评估在边缘设备资源限制下的模型异构联邦学习（FL），并对不同算法进行了分类和测试。


<details>
  <summary>Details</summary>
Motivation: 当前模型异构FL的研究缺乏在实际边缘设备约束下的定量分析和评估，因此需要重新思考和评估这一范式。

Method: 构建PracMHBench平台，对多种模型异构算法进行分类，并在不同数据任务和指标上进行测试。

Result: 通过广泛实验，观察了这些算法在不同边缘约束下的适用性和异构模式。

Conclusion: PracMHBench平台为模型异构FL的应用提供了实际评估工具，揭示了算法的适用性和异构特征。

Abstract: Federating heterogeneous models on edge devices with diverse resource
constraints has been a notable trend in recent years. Compared to traditional
federated learning (FL) that assumes an identical model architecture to
cooperate, model-heterogeneous FL is more practical and flexible since the
model can be customized to satisfy the deployment requirement. Unfortunately,
no prior work ever dives into the existing model-heterogeneous FL algorithms
under the practical edge device constraints and provides quantitative analysis
on various data scenarios and metrics, which motivates us to rethink and
re-evaluate this paradigm. In our work, we construct the first system platform
\textbf{PracMHBench} to evaluate model-heterogeneous FL on practical
constraints of edge devices, where diverse model heterogeneity algorithms are
classified and tested on multiple data tasks and metrics. Based on the
platform, we perform extensive experiments on these algorithms under the
different edge constraints to observe their applicability and the corresponding
heterogeneity pattern.

</details>


### [70] [Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform](https://arxiv.org/abs/2509.08756)
*Zhaoxun "Lorenz" Liu,Wagner H. Souza,Jay Han,Amin Madani*

Main category: cs.LG

TL;DR: 论文开发了一个基于深度强化学习的AI决策支持代理，用于优化大规模伤亡事件中的患者转运决策，并通过用户研究表明AI参与显著提高了决策质量和非专家的表现。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件（MCIs）对医疗系统造成巨大压力，需要在极端环境下快速准确地分配患者和医院资源。

Method: 开发并验证了一个深度强化学习AI代理，结合患者病情、医院容量和运输物流等因素进行决策。同时设计了MasTER指挥仪表盘进行模拟管理。

Result: 用户研究表明，AI的参与显著提高了决策质量和一致性，非专家在AI协助下能达到专家水平。

Conclusion: AI驱动的决策支持有望提升MCI培训和实际应急管理能力。

Abstract: Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,
accurate patient-hospital allocation decisions under extreme pressure. Here, we
developed and validated a deep reinforcement learning-based decision-support AI
agent to optimize patient transfer decisions during simulated MCIs by balancing
patient acuity levels, specialized care requirements, hospital capacities, and
transport logistics. To integrate this AI agent, we developed MasTER, a
web-accessible command dashboard for MCI management simulations. Through a
controlled user study with 30 participants (6 trauma experts and 24
non-experts), we evaluated three interaction approaches with the AI agent
(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI
scenarios in the Greater Toronto Area. Results demonstrate that increasing AI
involvement significantly improves decision quality and consistency. The AI
agent outperforms trauma surgeons (p < 0.001) and enables non-experts to
achieve expert-level performance when assisted, contrasting sharply with their
significantly inferior unassisted performance (p < 0.001). These findings
establish the potential for our AI-driven decision support to enhance both MCI
preparedness training and real-world emergency response management.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [71] [QCardEst/QCardCorr: Quantum Cardinality Estimation and Correction](https://arxiv.org/abs/2509.08817)
*Tobias Winker,Jinghua Groppe,Sven Groppe*

Main category: quant-ph

TL;DR: 提出了一种基于量子机器学习的量子基数估计方法（QCardEst），通过混合量子经典网络改进数据库查询优化的基数估计，显著提升了PostgreSQL的性能。


<details>
  <summary>Details</summary>
Motivation: 基数估计是数据库管理系统查询优化的重要组成部分，传统方法存在性能瓶颈，希望通过量子机器学习技术提供更高效的解决方案。

Method: 采用混合量子经典网络设计QCardEst方法，将SQL查询编码为量子态，利用变分量子电路（VQC）处理查询，并通过经典后处理层生成基数估计值；进一步提出QCardCorr技术，通过VQC生成的因子改进经典估计器。

Result: 实验结果显示，QCardCorr在JOB-light和STATS数据集上分别比PostgreSQL标准优化器性能提升了6.37倍和8.66倍，且在JOB-light上甚至超越MSCN 3.47倍。

Conclusion: 量子机器学习在基数估计中展现出显著优势，QCardEst和QCardCorr为数据库查询优化提供了高效的新方法。

Abstract: Cardinality estimation is an important part of query optimization in DBMS. We
develop a Quantum Cardinality Estimation (QCardEst) approach using Quantum
Machine Learning with a Hybrid Quantum-Classical Network. We define a compact
encoding for turning SQL queries into a quantum state, which requires only
qubits equal to the number of tables in the query. This allows the processing
of a complete query with a single variational quantum circuit (VQC) on current
hardware. In addition, we compare multiple classical post-processing layers to
turn the probability vector output of VQC into a cardinality value. We
introduce Quantum Cardinality Correction QCardCorr, which improves classical
cardinality estimators by multiplying the output with a factor generated by a
VQC to improve the cardinality estimation. With QCardCorr, we have an
improvement over the standard PostgreSQL optimizer of 6.37 times for JOB-light
and 8.66 times for STATS. For JOB-light we even outperform MSCN by a factor of
3.47.

</details>


### [72] [From Physical to Logical: Graph-State-Based Connectivity in Quantum Networks](https://arxiv.org/abs/2509.08384)
*Mateo M. Blanco,Manuel Fernández-Veiga,Ana Fernández-Vilas,Rebeca P. Díaz-Redondo*

Main category: quant-ph

TL;DR: 该论文研究了如何在量子网络中利用多星拓扑结构扩展多体纠缠的管理，以实现比传统二分结构更丰富的连接性。


<details>
  <summary>Details</summary>
Motivation: 解决量子通信中二分方案不足以支持高级协议（如量子秘密共享或分布式计算）的问题，拓展量子网络的连接性。

Method: 扩展基于双星配置的现有方法，分析多星拓扑在网络中的最大连接性，并提出远距离节点间逻辑通信的方法。

Result: 研究表明多星拓扑可以实现比传统二分结构更丰富的连接性，支持可扩展量子网络的发展。

Conclusion: 多星拓扑为量子网络提供了更灵活的多体纠缠管理方式，有助于实现高级量子通信协议。

Abstract: Entanglement is a key resource in quantum communication, but bipartite
schemes are often insufficient for advanced protocols like quantum secret
sharing or distributed computing. Graph states offer a flexible way to
represent and manage multipartite entanglement in quantum networks, enabling
logical connectivity through local operations and classical communication
(LOCC). In this work, we extend existing approaches based on bi-star
configurations to more complex multi-star topologies. We analyze the maximum
connectivity that can be achieved in networks of $m$ switches, each connected
to $n$ clients, including asymmetric cases where the number of clients varies
per switch. We also propose methods to enable logical communication between
distant nodes. Our results support the development of scalable quantum networks
with rich connectivity beyond traditional bipartite structures.

</details>


### [73] [Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions](https://arxiv.org/abs/2509.08654)
*Amirhossein Taherpour,Abbas Taherpour,Tamer Khattab*

Main category: quant-ph

TL;DR: 提出一种基于特征的POMDP框架，结合GNN解决量子网络路由中的部分可观测性和可扩展性问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态量子系统中部分可观测性、退相干和可扩展性挑战。

Method: 结合GNN和POMDP，编码量子网络动态到低维特征空间，实现高效信念更新和政策学习。

Result: 在模拟量子网络中，路由保真度和纠缠传递率显著优于基线，尤其在高压环境下。

Conclusion: 混合GNN-POMDP框架有效解决了量子网络路由中的关键挑战，表现出色。

Abstract: This paper presents a feature-based Partially Observable Markov Decision
Process (POMDP) framework for quantum network routing, combining belief-state
planning with Graph Neural Networks (GNNs) to address partial observability,
decoherence, and scalability challenges in dynamic quantum systems. Our
approach encodes complex quantum network dynamics, including entanglement
degradation and time-varying channel noise, into a low-dimensional feature
space, enabling efficient belief updates and scalable policy learning. The core
of our framework is a hybrid GNN-POMDP architecture that processes
graph-structured representations of entangled links to learn routing policies,
coupled with a noise-adaptive mechanism that fuses POMDP belief updates with
GNN outputs for robust decision making. We provide a theoretical analysis
establishing guarantees for belief convergence, policy improvement, and
robustness to noise. Experiments on simulated quantum networks with up to 100
nodes demonstrate significant improvements in routing fidelity and entanglement
delivery rates compared to state-of-the-art baselines, particularly under high
decoherence and nonstationary conditions.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [74] [Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor](https://arxiv.org/abs/2509.08020)
*Matthew Carter,Lee Devlin,Alexander Philips,Edward Pyzer-Knapp,Paul Spirakis,Simon Maskell*

Main category: q-bio.QM

TL;DR: 论文提出一种基于机会计算的SMC采样框架，用于大规模蛋白质组学贝叶斯推断，降低计算成本并提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂贝叶斯模型在蛋白质组学中计算成本高的问题，使其更易于应用于大规模数据集。

Method: 采用机会计算框架，利用闲置资源（如HTCondor）实现并行SMC采样，并提出Coordinator-Manager-Follower架构以减少同步开销。

Result: 在真实蛋白质组学模型上验证，显示机会SMC能在固定时间内生成更多样本，实现弱扩展性。

Conclusion: 提出的CondorSMC开源工具为蛋白质组学推断提供了高效、低成本的计算方案。

Abstract: Quantitative proteomics plays a central role in uncovering regulatory
mechanisms, identifying disease biomarkers, and guiding the development of
precision therapies. These insights are often obtained through complex Bayesian
models, whose inference procedures are computationally intensive, especially
when applied at scale to biological datasets. This limits the accessibility of
advanced modelling techniques needed to fully exploit proteomics data. Although
Sequential Monte Carlo (SMC) methods offer a parallelisable alternative to
traditional Markov Chain Monte Carlo, their high-performance implementations
often rely on specialised hardware, increasing both financial and energy costs.
We address these challenges by introducing an opportunistic computing framework
for SMC samplers, tailored to the demands of large-scale proteomics inference.
Our approach leverages idle compute resources at the University of Liverpool
via HTCondor, enabling scalable Bayesian inference without dedicated
high-performance computing infrastructure. Central to this framework is a novel
Coordinator-Manager-Follower architecture that reduces synchronisation overhead
and supports robust operation in heterogeneous, unreliable environments. We
evaluate the framework on a realistic proteomics model and show that
opportunistic SMC delivers accurate inference with weak scaling, increasing
samples generated under a fixed time budget as more resources join. To support
adoption, we release CondorSMC, an open-source package for deploying SMC
samplers in opportunistic computing environments.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [75] [A New Dataset and Benchmark for Grounding Multimodal Misinformation](https://arxiv.org/abs/2509.08008)
*Bingjian Yang,Danni Xu,Kaipeng Niu,Wenxuan Liu,Zheng Wang,Mohan Kankanhalli*

Main category: cs.SI

TL;DR: 论文提出了GroundMM任务，用于验证多模态内容并定位误导性片段，同时创建了首个真实数据集GroundLie360，并提出基于VLM的基准方法FakeMark。


<details>
  <summary>Details</summary>
Motivation: 在线虚假信息视频的扩散带来社会风险，现有方法缺乏解释性，无法有效应对。

Method: 提出GroundMM任务，构建GroundLie360数据集，设计基于VLM的FakeMark方法，利用单模态和跨模态线索进行检测。

Result: 实验验证了任务的挑战性，为可解释的多模态虚假信息检测奠定了基础。

Conclusion: GroundMM任务和基准方法为虚假信息检测提供了新方向，提升了可解释性。

Abstract: The proliferation of online misinformation videos poses serious societal
risks. Current datasets and detection methods primarily target binary
classification or single-modality localization based on post-processed data,
lacking the interpretability needed to counter persuasive misinformation. In
this paper, we introduce the task of Grounding Multimodal Misinformation
(GroundMM), which verifies multimodal content and localizes misleading segments
across modalities. We present the first real-world dataset for this task,
GroundLie360, featuring a taxonomy of misinformation types, fine-grained
annotations across text, speech, and visuals, and validation with Snopes
evidence and annotator reasoning. We also propose a VLM-based, QA-driven
baseline, FakeMark, using single- and cross-modal cues for effective detection
and grounding. Our experiments highlight the challenges of this task and lay a
foundation for explainable multimodal misinformation detection.

</details>


### [76] [Signals in the Noise: Decoding Unexpected Engagement Patterns on Twitter](https://arxiv.org/abs/2509.08128)
*Yulin Yu,Houming Chen,Daniel Romero,Paramveer S. Dhillon*

Main category: cs.SI

TL;DR: 本文研究了推特（现称“X”）上不同类型推文出乎意料的互动模式，开发了“意外指数”量化互动偏差，发现新闻、政治等信息内容更易被转发和评论，而游戏、体育等内容引发更多点赞和情感投入。


<details>
  <summary>Details</summary>
Motivation: 研究推特上不同类型推文的互动模式偏差，揭示用户在社交媒体上如何通过不同互动行为（点赞、转发、评论）传递信号。

Method: 基于信号理论和注意力经济理论，分析了60万条推文，开发“意外指数”量化互动偏差。

Result: 新闻、政治类内容获更多转发和评论；游戏、体育类内容获更多点赞。主观内容吸引点赞，客观内容吸引转发；长文和带链接内容意外获更多转发。

Conclusion: 用户互动行为受内容特性影响，不同类型内容通过不同信号强度竞争注意力。研究对内容创作者、平台设计者和研究者具有启发意义。

Abstract: Social media platforms offer users multiple ways to engage with
content--likes, retweets, and comments--creating a complex signaling system
within the attention economy. While previous research has examined factors
driving overall engagement, less is known about why certain tweets receive
unexpectedly high levels of one type of engagement relative to others. Drawing
on Signaling Theory and Attention Economy Theory, we investigate these
unexpected engagement patterns on Twitter (now known as "X"), developing an
"unexpectedness quotient" to quantify deviations from predicted engagement
levels. Our analysis of over 600,000 tweets reveals distinct patterns in how
content characteristics influence unexpected engagement. News, politics, and
business tweets receive more retweets and comments than expected, suggesting
users prioritize sharing and discussing informational content. In contrast,
games and sports-related topics garner unexpected likes and comments,
indicating higher emotional investment in these domains. The relationship
between content attributes and engagement types follows clear patterns:
subjective tweets attract more likes while objective tweets receive more
retweets, and longer, complex tweets with URLs unexpectedly receive more
retweets. These findings demonstrate how users employ different engagement
types as signals of varying strength based on content characteristics, and how
certain content types more effectively compete for attention in the social
media ecosystem. Our results offer valuable insights for content creators
optimizing engagement strategies, platform designers facilitating meaningful
interactions, and researchers studying online social behavior.

</details>


### [77] [Echo Chambers and Information Brokers on Truth Social: A Study of Network Dynamics and Political Discourse](https://arxiv.org/abs/2509.08676)
*Emelia May Hughes,Tim Weninger*

Main category: cs.SI

TL;DR: 研究分析了Truth Social在两大政治事件中的网络动态，发现平台结构高度分段且意识形态同质化，关键人物（如特朗普）主导话语权，政治事件短暂促进共识后迅速回归碎片化。


<details>
  <summary>Details</summary>
Motivation: 探讨政治对齐的社交媒体平台在政治事件中的网络动态及其对意识形态强化和跨群体互动的影响。

Method: 利用大规模用户互动数据集，分析重发帖（re-truths）网络的分段、极化和用户影响力。

Result: 平台呈现分段、同质化结构，关键人物主导话语权；政治事件短暂促进共识后迅速回归碎片化。

Conclusion: 基础设施与社区动态共同强化意识形态边界，限制跨群体互动，为替代平台研究和政治传播提供了新视角。

Abstract: This study examines the structural dynamics of Truth Social, a politically
aligned social media platform, during two major political events: the U.S.
Supreme Court's overturning of Roe v. Wade and the FBI's search of Mar-a-Lago.
Using a large-scale dataset of user interactions based on re-truths
(platform-native reposts), we analyze how the network evolves in relation to
fragmentation, polarization, and user influence. Our findings reveal a
segmented and ideologically homogenous structure dominated by a small number of
central figures. Political events prompt temporary consolidation around shared
narratives, followed by rapid returns to fragmented, echo-chambered clusters.
Centrality metrics highlight the disproportionate role of key influencers,
particularly @realDonaldTrump, in shaping visibility and directing discourse.
These results contribute to research on alternative platforms, political
communication, and online network behavior, demonstrating how infrastructure
and community dynamics together reinforce ideological boundaries and limit
cross-cutting engagement.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [78] [Measuring and mitigating overreliance is necessary for building human-compatible AI](https://arxiv.org/abs/2509.08010)
*Lujain Ibrahim,Katherine M. Collins,Sunnie S. Y. Kim,Anka Reuel,Max Lamparth,Kevin Feng,Lama Ahmad,Prajna Soni,Alia El Kattan,Merlin Stein,Siddharth Swaroop,Ilia Sucholutsky,Andrew Strait,Q. Vera Liao,Umang Bhatt*

Main category: cs.CY

TL;DR: 该立场论文强调需关注对大语言模型（LLMs）的过度依赖风险，提出了测量和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在医疗和个人咨询等领域的影响扩大，过度依赖可能导致高风险错误和社会治理问题，需对此进行研究。

Method: 论文整合了过度依赖的风险，分析了LLMs特性、系统设计和用户认知偏差，并提出了测量和改进方向。

Result: 指出了当前测量方法的三个重要缺陷，并提出了三种改进方向及缓解策略。

Conclusion: AI研究社区应采取措施，确保LLMs增强而非削弱人类能力。

Abstract: Large language models (LLMs) distinguish themselves from previous
technologies by functioning as collaborative "thought partners," capable of
engaging more fluidly in natural language. As LLMs increasingly influence
consequential decisions across diverse domains from healthcare to personal
advice, the risk of overreliance - relying on LLMs beyond their capabilities -
grows. This position paper argues that measuring and mitigating overreliance
must become central to LLM research and deployment. First, we consolidate risks
from overreliance at both the individual and societal levels, including
high-stakes errors, governance challenges, and cognitive deskilling. Then, we
explore LLM characteristics, system design features, and user cognitive biases
that - together - raise serious and unique concerns about overreliance in
practice. We also examine historical approaches for measuring overreliance,
identifying three important gaps and proposing three promising directions to
improve measurement. Finally, we propose mitigation strategies that the AI
research community can pursue to ensure LLMs augment rather than undermine
human capabilities.

</details>


### [79] [HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants](https://arxiv.org/abs/2509.08494)
*Benjamin Sturgeon,Daniel Samuelson,Jacob Haimes,Jacy Reese Anthis*

Main category: cs.CY

TL;DR: 论文探讨了AI对人类代理（agency）的影响，提出了一种 scalable 的评测框架 HumanAgencyBench（HAB），发现现有AI对人类代理的支持普遍不足。


<details>
  <summary>Details</summary>
Motivation: AI越来越多地接管人类任务和决策，可能导致人类失去对未来的控制。简单算法已能影响人类行为（如社交媒体推荐），亟需量化AI对人类代理的支持程度。

Method: 整合哲学与科学的代理理论，利用大语言模型（LLMs）模拟用户查询并评估AI回答，开发了包含6个维度的评测基准HAB。

Result: 当代LLM对人类代理的支持程度较低且不稳定，不同开发者和维度差异显著（如Anthropic整体支持度高但避免价值操纵最弱），能力提升未必增强代理支持。

Conclusion: 呼吁开发更鲁棒的安全对齐目标，而非单纯依赖LLM能力或指令遵循（如RLHF），以确保AI更好地支持人类自主性。

Abstract: As humans delegate more tasks and decisions to artificial intelligence (AI),
we risk losing control of our individual and collective futures. Relatively
simple algorithmic systems already steer human decision-making, such as social
media feed algorithms that lead people to unintentionally and absent-mindedly
scroll through engagement-optimized content. In this paper, we develop the idea
of human agency by integrating philosophical and scientific theories of agency
with AI-assisted evaluation methods: using large language models (LLMs) to
simulate and validate user queries and to evaluate AI responses. We develop
HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions
of human agency based on typical AI use cases. HAB measures the tendency of an
AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,
Correct Misinformation, Defer Important Decisions, Encourage Learning, and
Maintain Social Boundaries. We find low-to-moderate agency support in
contemporary LLM-based assistants and substantial variation across system
developers and dimensions. For example, while Anthropic LLMs most support human
agency overall, they are the least supportive LLMs in terms of Avoid Value
Manipulation. Agency support does not appear to consistently result from
increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and
we encourage a shift towards more robust safety and alignment targets.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [80] [CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework](https://arxiv.org/abs/2509.08438)
*Jinzhong Ning,Paerhati Tulajiang,Yingying Le,Yijia Zhang,Yuanyuan Sun,Hongfei Lin,Haifeng Liu*

Main category: cs.CL

TL;DR: 论文提出CommonVoice-SpeechRE数据集和RPG-MoGe框架，解决语音关系提取中数据多样性和生成模板单一的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语音关系提取数据集多为合成数据，缺乏真实语音多样性和数量，且模型生成模板单一，语义对齐弱。

Method: 引入大型真实语音数据集CommonVoice-SpeechRE；提出RPG-MoGe框架，包含多序三元组生成策略和CNN关系预测头。

Result: 实验表明RPG-MoGe优于现有方法，为语音关系提取提供了新基准数据集和解决方案。

Conclusion: 论文通过数据集和框架提升了语音关系提取的性能和实用性。

Abstract: Speech Relation Extraction (SpeechRE) aims to extract relation triplets
directly from speech. However, existing benchmark datasets rely heavily on
synthetic data, lacking sufficient quantity and diversity of real human speech.
Moreover, existing models also suffer from rigid single-order generation
templates and weak semantic alignment, substantially limiting their
performance. To address these challenges, we introduce CommonVoice-SpeechRE, a
large-scale dataset comprising nearly 20,000 real-human speech samples from
diverse speakers, establishing a new benchmark for SpeechRE research.
Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative
Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet
generation ensemble strategy, leveraging data diversity through diverse element
orders during both training and inference, and (2) CNN-based latent relation
prediction heads that generate explicit relation prompts to guide cross-modal
alignment and accurate triplet generation. Experiments show our approach
outperforms state-of-the-art methods, providing both a benchmark dataset and an
effective solution for real-world SpeechRE. The source code and dataset are
publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [81] [The-Bodega: A Matlab Toolbox for Biologically Dynamic Microbubble Simulations on Realistic Hemodynamic Microvascular Graphs](https://arxiv.org/abs/2509.08149)
*Stephen Alexander Lee,Alexis Leconte,Alice Wu,Jonathan Poree,Maxence Laplante-Berthier,Simon Desrocher,Pierre-Olivier Bouchard,Joshua Kinugasa,Samuel Mihelic,Andreas Linninger,Jean Provost*

Main category: physics.med-ph

TL;DR: The-Bodega是一个基于Matlab的工具箱，用于模拟超声定位显微镜(ULM)的地面真实数据集，支持复杂的血管结构和常见的ULM算法基准测试。


<details>
  <summary>Details</summary>
Motivation: 为ULM技术提供开源的地面真实数据集模拟工具，支持从简单血管输入生成复杂的血流动态模型。

Method: 结合顺序蒙特卡洛模拟、泊肃叶流分布和动态脉动流，支持任意血管结构，并利用CPU/GPU并行化提高计算效率。

Result: 工具展示了在图像质量评估、运动伪影分析及新型ULM模态模拟（如毛细血管成像、心肌重建）中的多功能性。

Conclusion: The-Bodega是一个灵活高效的工具，适用于多种ULM应用的模拟和算法测试。

Abstract: The-Bodega is a Matlab-based toolbox for simulating ground-truth datasets for
Ultrasound Localization Microscopy (ULM)-a super resolution imaging technique
that resolves microvessels by systematically tracking microbubbles flowing
through the microvasculature. The-Bodega enables open-source simulation of
stochastic microbubble dynamics through anatomically complex vascular graphs
and features a quasi-automated pipeline for generating ground-truth ultrasound
data from simple vascular inputs. It incorporates sequential Monte Carlo
simulations augmented with Poiseuille flow distributions and dynamic pulsatile
flow. A key novelty of our framework is its flexibility to accommodate
arbitrary vascular architectures and benchmark common ULM algorithms, such as
Fourier Ring Correlation and Singular Value Decomposition (SVD) spatiotemporal
filtering, on realistic hemodynamic digital phantoms. The-Bodega supports
consistent microbubble-to-ultrasound simulations across domains ranging from
mouse brains to human hearts and automatically leverages available CPU/GPU
parallelization to improve computational efficiency. We demonstrate its
versatility in applications including image quality assessment, motion artifact
analysis, and the simulation of novel ULM modalities, such as capillary
imaging, myocardial reconstruction under beating heart motion, and simulating
neurovascular evoked responses.

</details>
