<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 41]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.NI](#cs.NI) [Total: 14]
- [cs.MM](#cs.MM) [Total: 4]
- [cs.LO](#cs.LO) [Total: 9]
- [cs.HC](#cs.HC) [Total: 21]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DB](#cs.DB) [Total: 9]
- [cs.AR](#cs.AR) [Total: 8]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.IR](#cs.IR) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Choosing the Right Git Workflow: A Comparative Analysis of Trunk-based vs. Branch-based Approaches](https://arxiv.org/abs/2507.08943)
*Pedro Lopes,Paola Accioly,Paulo Borba,Vitor Menezes*

Main category: cs.SE

TL;DR: 综述Git工作流（分支与主干模式）在巴西开发者中的应用及适用场景。


<details>
  <summary>Details</summary>
Motivation: 探索Git工作流（分支与主干模式）的实际使用情况及影响因素。

Method: 通过半结构化访谈和调查问卷收集数据。

Result: 主干模式适合快速开发、小规模且经验丰富的团队；分支模式适合大规模且经验不足的团队。

Conclusion: 团队规模与经验是选择Git工作流的关键因素。

Abstract: Git has become one of the most widely used version control systems today.
Among its distinguishing features, its ability to easily and quickly create
branches stands out, allowing teams to customize their workflows. In this
context, various formats of collaborative development workflows using Git have
emerged and gained popularity among software engineers. We can categorize such
workflows into two main types: branch-based workflows and trunk-based
workflows. Branch-based workflows typically define a set of remote branches
with well-defined objectives, such as feature branches, a branch for feature
integration, and a main branch. The goal is to migrate changes from the most
isolated branch to the main one shared by all as the code matures. In this
category, GitFlow stands out as the most popular example. In contrast,
trunk-based workflows have a single remote branch where developers integrate
their changes directly. In this range of options, choosing a workflow that
maximizes team productivity while promoting software quality becomes a
non-trivial task. Despite discussions on forums, social networks, and blogs,
few scientific articles have explored this topic. In this work, we provide
evidence on how Brazilian developers work with Git workflows and what factors
favor or hinder the use of each model. To this end, we conducted
semi-structured interviews and a survey with software developers. Our results
indicate that trunk-based development favors fast-paced projects with
experienced and smaller teams, while branch-based development suits less
experienced and larger teams better, despite posing management challenges.

</details>


### [2] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: 本文提出了一种自动化的、领域特定的方法，用于研究R代码的分割，并介绍了两种新方法和一个人工标注的数据集StatCodeSeg。结果表明，基于上下文的逐行分析优于基于范围的划分，且较小的语言模型在性能上优于大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 随着代码库的增长，尤其是对低资源语言如R及其研究领域（如社会科学、心理学），手动和基于语法分析的方法变得不切实际，因此需要自动化方法来实现高效的代码分割。

Method: 本文提出了两种方法：基于上下文的逐行分析和基于范围的段划分，并使用大型和小型语言模型（LLMs/SLMs）进行实验。还引入了人工标注数据集StatCodeSeg。

Result: 实验结果表明，基于上下文的逐行分析优于基于范围的划分。在模型表现上，较小的语言模型（如CodeBERT和编码器版本的CodeT5+）优于大型语言模型，尽管它们未在预训练中使用R代码。

Conclusion: 研究证明了基于上下文的小型语言模型在代码分割任务中的有效性，即使预训练数据不包含目标语言（R代码），仅通过对少量标注数据的微调也能取得优异性能。

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [3] [Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle](https://arxiv.org/abs/2507.09023)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.SE

TL;DR: 论文提出了一个名为Tippy的新型AI框架，通过专门设计的AI代理在药物发现的DMTA周期中实现自动化，显著提高工作效率和决策速度。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法难以满足现代治疗开发的需求，因此需要新的AI技术来改进实验室自动化和工作流程。

Method: Tippy采用多代理系统，包括五种专门代理（Supervisor、Molecule、Lab、Analysis、Report）和Safety Guardrail监督，各自负责药物发现管道的不同阶段。

Result: Tippy显著提高了工作流程效率、决策速度和跨学科协作，为AI辅助药物发现提供了新范式。

Conclusion: Tippy是首个可用于生产的专门AI代理系统，展示了AI如何通过自主代理加速DMTA周期，同时保持科学严谨性。

Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery,
with traditional approaches struggling to meet modern therapeutic development
demands. This paper introduces a novel AI framework, Tippy, that transforms
laboratory automation through specialized AI agents operating within the
Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five
specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with
Safety Guardrail oversight - each designed to excel in specific phases of the
drug discovery pipeline. Tippy represents the first production-ready
implementation of specialized AI agents for automating the DMTA cycle,
providing a concrete example of how AI can transform laboratory workflows. By
leveraging autonomous AI agents that reason, plan, and collaborate, we
demonstrate how Tippy accelerates DMTA cycles while maintaining scientific
rigor essential for pharmaceutical research. The system shows significant
improvements in workflow efficiency, decision-making speed, and
cross-disciplinary coordination, offering a new paradigm for AI-assisted drug
discovery.

</details>


### [4] [Towards Extracting Software Requirements from App Reviews using Seq2seq Framework](https://arxiv.org/abs/2507.09039)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: 论文提出了一种基于序列到序列生成方法的命名实体识别（NER）任务，用于从移动应用评论中提取需求，解决了现有方法因评论中存在非正式语言、语法错误和无关信息而失效的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理移动应用评论时，由于评论中的非正式语言、语法错误和大量无关信息，难以有效提取用户需求。

Method: 提出了一种结合BiLSTM编码器、LSTM解码器、自注意力机制、GloVe嵌入和CRF模型的序列到序列框架。

Result: 在两个数据集上的评估显示，框架在包含23,816条评论的数据集2上的F1分数为0.96，优于现有方法；在包含1,000条评论的数据集1上的F1分数为0.47，表现接近。

Conclusion: 该框架有效解决了从移动应用评论中提取需求的问题，显著提升了性能。

Abstract: Mobile app reviews are a large-scale data source for software improvements. A
key task in this context is effectively extracting requirements from app
reviews to analyze the users' needs and support the software's evolution.
Recent studies show that existing methods fail at this task since app reviews
usually contain informal language, grammatical and spelling errors, and a large
amount of irrelevant information that might not have direct practical value for
developers. To address this, we propose a novel reformulation of requirements
extraction as a Named Entity Recognition (NER) task based on the
sequence-to-sequence (Seq2seq) generation approach. With this aim, we propose a
Seq2seq framework, incorporating a BiLSTM encoder and an LSTM decoder, enhanced
with a self-attention mechanism, GloVe embeddings, and a CRF model. We
evaluated our framework on two datasets: a manually annotated set of 1,000
reviews (Dataset 1) and a crowdsourced set of 23,816 reviews (Dataset 2). The
quantitative evaluation of our framework showed that it outperformed existing
state-of-the-art methods with an F1 score of 0.96 on Dataset 2, and achieved
comparable performance on Dataset 1 with an F1 score of 0.47.

</details>


### [5] [CMER: A Context-Aware Approach for Mining Ethical Concern-related App Reviews](https://arxiv.org/abs/2507.09049)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: 论文提出了一种名为CMER的新方法，结合自然语言推理（NLI）和类似LLaMA的大型语言模型（LLM），用于大规模提取应用评论中涉及伦理关切的内容。


<details>
  <summary>Details</summary>
Motivation: 移动应用日益普及，伦理关切（如隐私和安全）成为用户反馈的重要内容，但这些反馈通常使用特定领域语言且被其他通用反馈类别掩盖，难以自动提取。

Method: CMER方法结合NLI和LLM，NLI提供领域特定的上下文感知，LLM无需标记数据即可分类。研究从一个超过382K评论的数据集中提取隐私和安全相关评论（PSRs）。

Result: CMER成功提取了2178条此前关键字方法忽略的PSRs，证明了其有效性。这些评论可进一步转化为可操作的需求规范。

Conclusion: CMER通过结合NLI和LLM，有效解决了伦理关切评论提取的挑战，为软件工程领域提供了实用工具。

Abstract: With the increasing proliferation of mobile applications in our daily lives,
the concerns surrounding ethics have surged significantly. Users communicate
their feedback in app reviews, frequently emphasizing ethical concerns, such as
privacy and security. Incorporating these reviews has proved to be useful for
many areas of software engineering (e.g., requirement engineering, testing,
etc.). However, app reviews related to ethical concerns generally use
domain-specific language and are typically overshadowed by more generic
categories of user feedback, such as app reliability and usability. Thus,
making automated extraction a challenging and time-consuming effort.
  This study proposes CMER (A \underline{C}ontext-Aware Approach for
\underline{M}ining \underline{E}thical Concern-related App
\underline{R}eviews), a novel approach that combines Natural Language Inference
(NLI) and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract
ethical concern-related app reviews at scale. In CMER, NLI provides
domain-specific context awareness by using domain-specific hypotheses, and the
Llama-like LLM eliminates the need for labeled data in the classification task.
We evaluated the validity of CMER by mining privacy and security-related
reviews (PSRs) from the dataset of more than 382K app reviews of mobile
investment apps. First, we evaluated four NLI models and compared the results
of domain-specific hypotheses with generic hypotheses. Next, we evaluated three
LLMs for the classification task. Finally, we combined the best NLI and LLM
models (CMER) and extracted 2,178 additional PSRs overlooked by the previous
study using a keyword-based approach, thus demonstrating the effectiveness of
CMER. These reviews can be further refined into actionable requirement
artifacts.

</details>


### [6] [SAGE: A Context-Aware Approach for Mining Privacy Requirements Relevant Reviews from Mental Health Apps](https://arxiv.org/abs/2507.09051)
*Aakash Sorathiya,Gouri Ginde*

Main category: cs.SE

TL;DR: SAGE是一种基于自然语言推理（NLI）和GPT模型的智能方法，用于自动从心理健康应用评论中挖掘隐私相关的反馈，无需微调且效果优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 心理健康应用收集敏感数据引发隐私担忧，但相关反馈常被其他类别（如可靠性、易用性）掩盖，难以自动提取隐私需求。

Method: 提出SAGE方法，结合NLI和GPT模型，利用领域特定的隐私假设，无需微调即可高效识别隐私评论。

Result: SAGE在20.4万条评论中F1分数达0.85，优于BERT和T5，并发现748条关键词方法遗漏的隐私评论。

Conclusion: SAGE能有效挖掘隐私需求，生成可操作的需求文档，提升心理健康应用的隐私保护。

Abstract: Mental health (MH) apps often require sensitive user data to customize
services for mental wellness needs. However, such data collection practices in
some MH apps raise significant privacy concerns for users. These concerns are
often mentioned in app reviews, but other feedback categories, such as
reliability and usability, tend to take precedence. This poses a significant
challenge in automatically identifying privacy requirements-relevant reviews
(privacy reviews) that can be utilized to extract privacy requirements and
address users' privacy concerns. Thus, this study introduces SAGE, a
context-aware approach to automatically mining privacy reviews from MH apps
using Natural Language Inference (NLI) with MH domain-specific privacy
hypotheses (provides domain-specific context awareness) and a GPT model
(eliminates the need for fine-tuning). The quantitative evaluation of SAGE on a
dataset of 204K app reviews achieved an F1 score of 0.85 without any
fine-tuning, outperforming the fine-tuned baseline classifiers BERT and T5.
Furthermore, SAGE extracted 748 privacy reviews previously overlooked by
keyword-based methods, demonstrating its effectiveness through qualitative
evaluation. These reviews can later be refined into actionable privacy
requirement artifacts.

</details>


### [7] [Prompting for Performance: Exploring LLMs for Configuring Software](https://arxiv.org/abs/2507.09790)
*Helge Spieker,Théo Matricon,Nassim Belmecheri,Jørn Eirik Betten,Gauthier Le Bartz Lyan,Heraldo Borges,Quentin Mazouni,Dennis Gross,Arnaud Gotlieb,Mathieu Acher*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型（LLMs）是否能通过提示辅助性能导向的软件配置，初步结果显示其在某些任务中表现良好，但也存在局限。


<details>
  <summary>Details</summary>
Motivation: 软件系统配置选项众多且影响性能，传统机器学习方法计算成本高，因此研究LLMs在此领域的潜力。

Method: 评估多种LLMs在识别相关选项、排序配置和推荐高性能配置等任务中的表现，涉及编译器、视频编码器等系统。

Result: LLMs在部分任务中能与专家知识一致，但也可能出现幻觉或浅层推理的问题。

Conclusion: 这是系统性评估和设计基于LLM的软件配置辅助解决方案的首步探索。

Abstract: Software systems usually provide numerous configuration options that can
affect performance metrics such as execution time, memory usage, binary size,
or bitrate. On the one hand, making informed decisions is challenging and
requires domain expertise in options and their combinations. On the other hand,
machine learning techniques can search vast configuration spaces, but with a
high computational cost, since concrete executions of numerous configurations
are required. In this exploratory study, we investigate whether large language
models (LLMs) can assist in performance-oriented software configuration through
prompts. We evaluate several LLMs on tasks including identifying relevant
options, ranking configurations, and recommending performant configurations
across various configurable systems, such as compilers, video encoders, and SAT
solvers. Our preliminary results reveal both positive abilities and notable
limitations: depending on the task and systems, LLMs can well align with expert
knowledge, whereas hallucinations or superficial reasoning can emerge in other
cases. These findings represent a first step toward systematic evaluations and
the design of LLM-based solutions to assist with software configuration.

</details>


### [8] [SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments](https://arxiv.org/abs/2507.09063)
*Avi Arora,Jinu Jang,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: SetupBench是一个评估LLM代理环境引导能力的基准测试，覆盖多种语言和数据库任务。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理基准测试主要在预配置环境中评估，无法反映真实场景中的环境引导需求。

Method: 使用93个实例的SetupBench基准测试，评估代理在裸Linux沙盒中的环境引导能力。

Result: OpenHands代理在各类任务中成功率较低，尤其在仓库设置和数据库配置中表现不佳。

Conclusion: SetupBench提供了评估下一代软件开发代理环境引导能力的严格标准。

Abstract: Modern Large Language Model (LLM) agents promise end to end assistance with
real-world software tasks, yet existing benchmarks evaluate LLM agents almost
exclusively in pre-baked environments where every dependency is pre-installed.
To fill this gap, we introduce SetupBench, a 93 instance benchmark that
isolates the environment-bootstrap skill: starting from a bare Linux sandbox,
an agent must install packages, resolve dependency conflicts, initialize
databases, and configure background services. Our tasks span seven language
ecosystems, five database engines, and multi-service orchestration scenarios,
each accompanies by a natural language problem statement and a deterministic
success command. Through evaluation of OpenHands, a state-of-the-art coding
agent, we find low success rates across task categories, with particular
challenges in repository setup (38.9-57.4%) and local database configuration
(20.0-53.3%). Our analysis reveals systematic failure modes including
incomplete development tooling installation, hallucinated task constraints, and
non-persistent environment modifications that break agent-human collaboration
workflows. We identify substantial inefficiencies in agent exploration
strategies, with 38-89% of actions being unnecessary compared to optimal human
behavior. These findings highlight gaps in current agents' practical
environment-bootstrap capabilities. By targeting this critical yet
under-evaluated capability, SetupBench provides a rigorous yard-stick for the
next generation of software developer agents aiming to solve end to end
real-wold tasks.

</details>


### [9] [SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation](https://arxiv.org/abs/2507.09108)
*Aaditya Bhatia,Gustavo A. Oliva,Gopi Krishnan Rajbahadur,Haoxiang Zhang,Yihao Chen,Zhilong Chen,Arthur Leung,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SPICE是一种自动化标注管道，用于高效低成本地生成软件工程领域的高质量标注数据集，显著降低了标注成本。


<details>
  <summary>Details</summary>
Motivation: 高质量的标注数据集对软件工程基础模型的训练和评估至关重要，但手动标注成本高且耗时。SPICE旨在解决这一问题。

Method: SPICE结合上下文感知代码导航、理论驱动的提示和多轮共识机制，生成接近专家标注的标签。

Result: SPICE在标注1,000条数据时，成本从手动标注的10万美元降至5.10美元，且与人工标注结果高度一致。

Conclusion: SPICE能够低成本、大规模地生成软件工程数据集，并开源了相关工具和数据集（SPICE Bench）。

Abstract: High-quality labeled datasets are crucial for training and evaluating
foundation models in software engineering, but creating them is often
prohibitively expensive and labor-intensive. We introduce SPICE, a scalable,
automated pipeline for labeling SWE-bench-style datasets with annotations for
issue clarity, test coverage, and effort estimation. SPICE combines
context-aware code navigation, rationale-driven prompting, and multi-pass
consensus to produce labels that closely approximate expert annotations.
SPICE's design was informed by our own experience and frustration in labeling
more than 800 instances from SWE-Gym. SPICE achieves strong agreement with
human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000
instances from around $100,000 (manual annotation) to just $5.10. These results
demonstrate SPICE's potential to enable cost-effective, large-scale dataset
creation for SE-focused FMs. To support the community, we release both SPICE
tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated
from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench
Verified).

</details>


### [10] [Position Paper: Programming Language Techniques for Bridging LLM Code Generation Semantic Gaps](https://arxiv.org/abs/2507.09135)
*Yalong Du,Chaozheng Wang,Huaijin Wang*

Main category: cs.SE

TL;DR: 论文主张将编程语言技术（PL）与大型语言模型（LLM）结合，以解决LLM生成代码中的语义缺陷，提升代码的可靠性、可验证性和可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自动代码生成方面表现出色，但其统计特性和黑箱性质导致语义缺陷，如语法错误、语义幻觉和可靠性问题。

Method: 通过结构化程序表示、形式化正确性保证和鲁棒的验证机制，PL技术可以填补LLM生成代码的语义缺陷。

Result: PL技术能够将LLM生成的代码从单纯的统计模式匹配提升到可靠和可信的水平。

Conclusion: LLM与PL技术的结合对于生成功能正确、可解释、可验证且可信的代码系统至关重要。

Abstract: Large Language Models have demonstrated remarkable capabilities in automated
code generation, yet their statistical nature and black-box characteristics
create significant semantic gaps manifested through syntax errors, semantic
hallucinations, and reliability concerns. This position paper argues that
principled integration of Programming Language (PL) techniques is essential for
bridging these gaps. Through structured program representations, formal
correctness guarantees, and robust verification mechanisms, PL techniques can
elevate LLM-generated code from statistical pattern matching to truly reliable
and trustworthy levels. This integration is crucial for developing systems that
generate code that is not only functionally correct but also interpretable,
verifiable, and ultimately trustworthy.

</details>


### [11] [OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization](https://arxiv.org/abs/2507.09682)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: OrQstrator是一个基于深度强化学习的模块化框架，用于在NISQ时代优化量子电路，通过智能选择三种互补的优化器来减少深度和门数。


<details>
  <summary>Details</summary>
Motivation: 解决NISQ时代量子电路优化的挑战，提供硬件感知的解决方案。

Method: 结合深度强化学习、域特定优化器和参数化电路实例化器，通过中央协调引擎选择优化策略。

Result: 生成优化后的电路，适配后端硬件约束。

Conclusion: OrQstrator框架在硬件感知的量子电路优化中表现出色，结合多种优化技术提升性能。

Abstract: We propose a novel approach, OrQstrator, which is a modular framework for
conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum
(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our
orchestration engine intelligently selects among three complementary circuit
optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count
via learned rewrite sequences; a domain-specific optimizer that performs
efficient local gate resynthesis and numeric optimization; a parameterized
circuit instantiator that improves compilation by optimizing template circuits
during gate set translation. These modules are coordinated by a central
orchestration engine that learns coordination policies based on circuit
structure, hardware constraints, and backend-aware performance features such as
gate count, depth, and expected fidelity. The system outputs an optimized
circuit for hardware-aware transpilation and execution, leveraging techniques
from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt
to backend constraints.

</details>


### [12] [OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advanced Transportation Research](https://arxiv.org/abs/2507.09186)
*Minhaj Uddin Ahmad,Akid Abrar,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.SE

TL;DR: OpenCAMS是一个开源的、同步的、可扩展的协同仿真框架，整合了SUMO、CARLA和OMNeT++三种仿真工具，用于交通、感知和通信领域的研究。


<details>
  <summary>Details</summary>
Motivation: 为了支持交通安全性、移动性和网络安全的高级研究，结合不同仿真工具的优势。

Method: 采用时间同步的双向耦合架构，结合SUMO、CARLA和OMNeT++的功能，实现交通流、高保真感知和通信模拟。

Result: 提供了一个模块化、可扩展的平台，支持下一代智能交通系统的研究。

Conclusion: OpenCAMS为研究社区提供了一个灵活、协作的环境，促进智能交通系统的发展。

Abstract: We introduce OpenCAMS (Open-Source Connected and Automated Mobility
Co-Simulation Platform), an open-source, synchronized, and extensible
co-simulation framework that tightly couples three best-in-class simulation
tools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support
advanced research in transportation safety, mobility, and cybersecurity by
combining the strengths of each simulation domain. Specifically, SUMO provides
large-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D
perception, vehicle dynamics, and control simulation; and OMNeT++ enables
modular, event-driven network communication, such as cellular
vehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,
bidirectional coupling architecture that ensures coherent simulation
progression across traffic, perception, and communication domains while
preserving modularity and reproducibility. For example, CARLA can simulate and
render a subset of vehicles that require detailed sensor emulation and control
logic; SUMO orchestrates network-wide traffic flow, vehicle routing, and
traffic signal management; and OMNeT++ dynamically maps communication nodes to
both mobile entities (e.g., vehicles) and static entities (e.g., roadside
units) to enable C-V2X communication. While these three simulators form the
foundational core of OpenCAMS, the platform is designed to be expandable and
future-proof, allowing additional simulators to be integrated on top of this
core without requiring fundamental changes to the system architecture. The
OpenCAMS platform is fully open-source and publicly available through its
GitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,
providing the research community with an accessible, flexible, and
collaborative environment for advancing next-generation intelligent
transportation systems.

</details>


### [13] [Back to the Basics: Rethinking Issue-Commit Linking with LLM-Assisted Retrieval](https://arxiv.org/abs/2507.09199)
*Huihui Huang,Ratnadira Widyasari,Ting Zhang,Ivana Clairine Irsan,Jieke Shi,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,Hong Jin Kang,David Lo*

Main category: cs.SE

TL;DR: 论文提出了一种更现实的评估设置（RDS）和数据集，用于评估问题-提交链接恢复工具的性能，并发现深度学习方法在现实场景中表现大幅下降。作者提出了一种新工具EasyLink，结合向量数据库和大语言模型，显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的问题-提交链接恢复方法在评估中忽略了实际场景中更多不相关提交对工具的干扰。作者希望通过更现实的评估设置和改进方法来解决这一问题。

Method: 提出Realistic Distribution Setting (RDS)构建更现实的评估数据集；提出EasyLink，结合向量数据库和大语言模型进行链接恢复。

Result: 在RDS数据集上评估发现深度学习方法性能下降超过一半，而VSM表现更优；EasyLink在Precision@1上达到75.91%，优于现有方法四倍以上。

Conclusion: 研究揭示了现有评估方法的局限性，并提出了一种更有效的工具EasyLink，为问题-提交链接恢复研究提供了实用指导。

Abstract: Issue-commit linking, which connects issues with commits that fix them, is
crucial for software maintenance. Existing approaches have shown promise in
automatically recovering these links. Evaluations of these techniques assess
their ability to identify genuine links from plausible but false links.
However, these evaluations overlook the fact that, in reality, when a
repository has more commits, the presence of more plausible yet unrelated
commits may interfere with the tool in differentiating the correct fix commits.
To address this, we propose the Realistic Distribution Setting (RDS) and use it
to construct a more realistic evaluation dataset that includes 20 open-source
projects. By evaluating tools on this dataset, we observe that the performance
of the state-of-the-art deep learning-based approach drops by more than half,
while the traditional Information Retrieval method, VSM, outperforms it.
  Inspired by these observations, we propose EasyLink, which utilizes a vector
database as a modern Information Retrieval technique. To address the
long-standing problem of the semantic gap between issues and commits, EasyLink
leverages a large language model to rerank the commits retrieved from the
database. Under our evaluation, EasyLink achieves an average Precision@1 of
75.91%, improving over the state-of-the-art by over four times. Additionally,
this paper provides practical guidelines for advancing research in issue-commit
link recovery.

</details>


### [14] [Explainability as a Compliance Requirement: What Regulated Industries Need from AI Tools for Design Artifact Generation](https://arxiv.org/abs/2507.09220)
*Syed Tauhid Ullah Shah,Mohammad Hussein,Ann Barcomb,Mohammad Moshirpour*

Main category: cs.SE

TL;DR: 本文研究了AI工具在需求工程中自动生成设计工件时的可解释性差距，发现非透明AI输出导致手动验证增加、信任降低等问题，并提出改进措施。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具在安全关键领域中应用的透明度问题，因其在受监管行业中的低采用率。

Method: 通过半结构化访谈与十位从业者探讨AI工具的集成和可解释性挑战。

Result: 发现非透明AI输出带来验证、信任、合规等问题，并提出改进方向如源头追踪和领域适应。

Conclusion: 为提升AI工具的透明度和可靠性提供了实用路线图，尤其适用于受监管环境。

Abstract: Artificial Intelligence (AI) tools for automating design artifact generation
are increasingly used in Requirements Engineering (RE) to transform textual
requirements into structured diagrams and models. While these AI tools,
particularly those based on Natural Language Processing (NLP), promise to
improve efficiency, their adoption remains limited in regulated industries
where transparency and traceability are essential. In this paper, we
investigate the explainability gap in AI-driven design artifact generation
through semi-structured interviews with ten practitioners from safety-critical
industries. We examine how current AI-based tools are integrated into workflows
and the challenges arising from their lack of explainability. We also explore
mitigation strategies, their impact on project outcomes, and features needed to
improve usability. Our findings reveal that non-explainable AI outputs
necessitate extensive manual validation, reduce stakeholder trust, struggle to
handle domain-specific terminology, disrupt team collaboration, and introduce
regulatory compliance risks, often negating the anticipated efficiency
benefits. To address these issues, we identify key improvements, including
source tracing, providing clear justifications for tool-generated decisions,
supporting domain-specific adaptation, and enabling compliance validation. This
study outlines a practical roadmap for improving the transparency, reliability,
and applicability of AI tools in requirements engineering workflows,
particularly in regulated and safety-critical environments where explainability
is crucial for adoption and certification.

</details>


### [15] [Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.09315)
*Yongqian Sun,Weihua Kuang,Chao Shen,Xidao Wen,Tinghua Zheng,Heng Liu,Shenglin Zhang,Bo Wu,Dan Pei*

Main category: cs.SE

TL;DR: 提出SCELM框架，用于自动化管理软件变更，降低服务失败和经济损失。


<details>
  <summary>Details</summary>
Motivation: 现代在线服务中频繁的软件变更带来了显著风险。

Method: 提出SCELM（软件变更评估与生命周期管理）框架，实现端到端自动化管理。

Result: SCELM能高效、精准地管理软件变更，显著减少服务失败和经济损失。

Conclusion: SCELM是解决软件变更风险的有效自动化框架。

Abstract: In modern online services, frequent software changes introduce significant
risks. To tackle this challenge, we propose SCELM (Software Change Evaluation
and Lifecycle Management), an end-to-end automated framework for software
change management. SCELM aims to manage software changes efficiently and
precisely, significantly reducing service failures and economic losses.

</details>


### [16] [Enhancing NeuroEvolution-Based Game Testing: A Branch Coverage Approach for Scratch Programs](https://arxiv.org/abs/2507.09414)
*Khizra Sohail,Atif Aftab Ahmed Jilani,Nigar Azhar Butt*

Main category: cs.SE

TL;DR: NEATEST框架原本采用基于语句覆盖的测试生成方法，但存在不足。本文提出分支覆盖的适应度函数，显著提升了测试效果和故障检测能力。


<details>
  <summary>Details</summary>
Motivation: 游戏程序的非确定性和复杂控制结构使得传统语句覆盖测试效果不佳，需要更有效的方法。

Method: 扩展NEATEST，集成分支覆盖适应度函数，优先处理控制依赖的分支，以增强分支探索。

Result: 在25个Scratch游戏中，分支覆盖版本（NBC）在13个游戏中表现更好，复杂条件结构的程序中效果显著，且误报率更低。

Conclusion: 分支覆盖为基础的测试生成方法能有效提升Scratch程序的测试覆盖率和故障检测能力。

Abstract: Automated test generation for game-like programs presents unique challenges
due to their non-deterministic behavior and complex control structures. The
NEATEST framework has been used for automated testing in Scratch games,
employing neuroevolution-based test generation optimized for statement
coverage. However, statement coverage alone is often insufficient for fault
detection, as it does not guarantee execution of all logical branches. This
paper introduces a branch coverage-based fitness function to enhance test
effectiveness in automated game testing. We extend NEATEST by integrating a
branch fitness function that prioritizes control-dependent branches, guiding
the neuroevolution process to maximize branch exploration. To evaluate the
effectiveness of this approach, empirical experiments were conducted on 25
Scratch games, comparing Neatest with Statement Coverage (NSC) against Neatest
with Branch Coverage (NBC). A mutation analysis was also performed to assess
the fault detection capabilities of both techniques. The results demonstrate
that NBC achieves higher branch coverage than NSC in 13 out of 25 games,
particularly in programs with complex conditional structures. Moreover, NBC
achieves a lower false positive rate in mutation testing, making it a more
reliable approach for identifying faulty behavior in game programs. These
findings confirm that branch coverage-based test generation improves test
coverage and fault detection in Scratch programs.

</details>


### [17] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: 论文介绍了StateGen，一个自动化框架，用于生成涉及连续API交互的多样化编程任务，并构建了包含120个测试案例的StateEval基准测试，用于评估LLMs的API整合能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的工具使用测试和评估仍处于早期阶段，现有基准测试依赖手动收集的测试案例，且无法自动检查语义正确性，同时忽视了实际应用中常见的连续API交互。

Method: StateGen结合了基于状态机的API约束解决、能量采样、控制流注入等技术生成可执行程序，并通过两个LLM代理将其转化为自然语言任务描述。

Result: 实验证明StateGen能有效生成具有挑战性和真实性的API导向任务，并揭示了当前LLMs在API整合方面的改进空间。

Conclusion: StateGen和StateEval为解决LLMs工具使用的测试和评估问题提供了有效工具，强调了需要改进的方向。

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


### [18] [AssertCoder: LLM-Based Assertion Generation via Multimodal Specification Extraction](https://arxiv.org/abs/2507.10338)
*Enyuan Tian,Yiwei Ci,Qiusong Yang,Yufeng Li,Zhichao Lyu*

Main category: cs.SE

TL;DR: AssertCoder 是一种自动从多模态硬件设计规范生成高质量 SVAs 的统一框架，通过模态敏感预处理和语义分析器提取结构化表示，再通过多步链式思维提示完成断言合成，最终通过变异评估优化断言质量。


<details>
  <summary>Details</summary>
Motivation: 手动编写高质量的 SVAs 既费时又容易出错，亟需自动化工具来提升效率和准确性。

Method: AssertCoder 结合模态敏感预处理、语义分析器和多步链式思维提示，实现从多模态规范到断言的自动生成和优化。

Result: 实验表明，AssertCoder 在功能正确性和变异检测方面分别比现有方法提高了 8.4% 和 5.8%。

Conclusion: AssertCoder 为硬件设计中的断言验证提供了一种高效且准确的自动化解决方案。

Abstract: Assertion-Based Verification (ABV) is critical for ensuring functional
correctness in modern hardware systems. However, manually writing high-quality
SVAs remains labor-intensive and error-prone. To bridge this gap, we propose
AssertCoder, a novel unified framework that automatically generates
high-quality SVAs directly from multimodal hardware design specifications.
AssertCoder employs a modality-sensitive preprocessing to parse heterogeneous
specification formats (text, tables, diagrams, and formulas), followed by a set
of dedicated semantic analyzers that extract structured representations aligned
with signal-level semantics. These representations are utilized to drive
assertion synthesis via multi-step chain-of-thought (CoT) prompting. The
framework incorporates a mutation-based evaluation approach to assess assertion
quality via model checking and further refine the generated assertions.
Experimental evaluation across three real-world Register-Transfer Level (RTL)
designs demonstrates AssertCoder's superior performance, achieving an average
increase of 8.4% in functional correctness and 5.8% in mutation detection
compared to existing state-of-the-art approaches.

</details>


### [19] [Towards LLM-Based Automatic Playtest](https://arxiv.org/abs/2507.09490)
*Yan Zhao,Chiwei Tang*

Main category: cs.SE

TL;DR: 本文介绍了一种基于大型语言模型（LLM）的自动游戏测试方法Lap，用于测试三消游戏。Lap通过处理游戏环境、基于提示生成动作和执行动作，显著提升了测试效率。


<details>
  <summary>Details</summary>
Motivation: 手动游戏测试耗时且昂贵，现有自动化工具缺乏领域知识和问题解决能力。AI的进展为LLM在游戏测试中的应用提供了新可能，但现有方法无法视觉感知游戏环境，且多局限于文本游戏或具备API的游戏。Lap旨在解决这些问题。

Method: Lap将游戏板转换为数值矩阵，利用ChatGPT-O1-mini API生成建议动作，并执行动作以触发游戏变化，迭代进行直至超时。

Result: 在开源三消游戏CasseBonbons上的实验表明，Lap在代码覆盖率和触发程序崩溃方面优于现有工具。

Conclusion: Lap展示了LLM在自动游戏测试中的潜力，为未来自动测试和LLM应用提供了新的研究方向。

Abstract: Playtesting is the process in which people play a video game for testing. It
is critical for the quality assurance of gaming software. Manual playtesting is
time-consuming and expensive. However, automating this process is challenging,
as playtesting typically requires domain knowledge and problem-solving skills
that most conventional testing tools lack. Recent advancements in artificial
intelligence (AI) have opened up new possibilities for applying Large Language
Models (LLMs) to playtesting. However, significant challenges remain: current
LLMs cannot visually perceive game environments, and most existing research
focuses on text-based games or games with robust APIs. Many non-text games lack
APIs to provide textual descriptions of game states, making it almost
impossible to naively apply LLMs for playtesting. This paper introduces Lap,
our novel approach to LLM-based Automatic Playtesting, which uses ChatGPT to
test match-3 games, a category of games where players match three or more
identical tiles in a row or column to earn points. Lap encompasses three key
phases: processing of game environments, prompting-based action generation, and
action execution. Given a match-3 game, Lap takes a snapshot of the game board
and converts it to a numeric matrix. It then prompts the ChatGPT-O1-mini API to
suggest moves based on that matrix and tentatively applies the suggested moves
to earn points and trigger changes in the game board. It repeats the
above-mentioned three steps iteratively until timeout. For evaluation, we
conducted a case study using Lap on an open-source match-3 game, CasseBonbons,
and empirically compared it with three existing tools. Our results are
promising: Lap outperformed existing tools by achieving higher code coverage
and triggering more program crashes. This research sheds light on the future of
automatic testing and LLM applications.

</details>


### [20] [It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a Practical Perspective](https://arxiv.org/abs/2507.09529)
*Yunqian Wang,Xiaohong Li,Yao Zhang,Yuekang Li,Zhiping Zhou,Ruitao Feng*

Main category: cs.SE

TL;DR: 本文介绍了VulTegra框架，用于评估深度学习模型在漏洞检测中的表现，揭示了现有方法的局限性和改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着软件漏洞威胁的增加，基于深度学习的漏洞检测器日益流行，但其一致性、实际效果和适用性仍存在问题。

Method: 提出VulTegra框架，通过多维比较从零训练和预训练模型在漏洞检测中的表现。

Result: 实验显示，现有检测器在一致性、实际能力和扩展性方面表现不佳，但调整关键因素可显著提升性能。

Conclusion: 研究强调需结合漏洞类型和代码特征以改进检测效果，并为模型设计和部署提供了新见解。

Abstract: With the growing threat of software vulnerabilities, deep learning (DL)-based
detectors have gained popularity for vulnerability detection. However, doubts
remain regarding their consistency within declared CWE ranges, real-world
effectiveness, and applicability across scenarios. These issues may lead to
unreliable detection, high false positives/negatives, and poor adaptability to
emerging vulnerabilities. A comprehensive analysis is needed to uncover
critical factors affecting detection and guide improvements in model design and
deployment. In this paper, we present VulTegra, a novel evaluation framework
that conducts a multidimensional comparison of scratch-trained and
pre-trained-based DL models for vulnerability detection. VulTegra reveals that
state-of-the-art (SOTA) detectors still suffer from low consistency, limited
real-world capabilities, and scalability challenges. Contrary to common belief,
pre-trained models are not consistently better than scratch-trained models but
exhibit distinct strengths in specific contexts.Importantly, our study exposes
the limitations of relying solely on CWE-based classification and identifies
key factors that significantly affect model performance. Experimental results
show that adjusting just one such factor consistently improves recall across
all seven evaluated detectors, with six also achieving better F1 scores. Our
findings provide deeper insights into model behavior and emphasize the need to
consider both vulnerability types and inherent code features for effective
detection.

</details>


### [21] [A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study](https://arxiv.org/abs/2507.09583)
*Taniv Ashraf*

Main category: cs.SE

TL;DR: 这篇论文介绍了一个基于大型语言模型（如Google的Gemini）的实时股票分析系统，通过服务器架构实现低成本运行，并分享了调试经验。


<details>
  <summary>Details</summary>
Motivation: 利用强大的大型语言模型（如Gemini） democratize 金融数据分析，为个人提供高级AI工具。

Method: 系统采用Gemini API进行定性分析，通过GitHub Actions自动化数据处理，并使用静态前端展示结果。详细记录了从概念到事件驱动管道的架构演化。

Result: 最终架构几乎零成本运行，适用于个人构建复杂的AI金融工具，系统公开可用，源代码开放。

Conclusion: 讨论了LLM在金融分析中的作用、调试方法论的重要性，以及人机协作在软件开发中的新兴范式。

Abstract: The advent of powerful, accessible Large Language Models (LLMs) like Google's
Gemini presents new opportunities for democratizing financial data analysis.
This paper documents the design, implementation, and iterative debugging of a
novel, serverless system for real-time stock analysis. The system leverages the
Gemini API for qualitative assessment, automates data ingestion and processing
via GitHub Actions, and presents the findings through a decoupled, static
frontend. We detail the architectural evolution of the system, from initial
concepts to a robust, event-driven pipeline, highlighting the practical
challenges encountered during deployment. A significant portion of this paper
is dedicated to a case study on the debugging process, covering common software
errors, platform-specific permission issues, and rare, environment-level
platform bugs. The final architecture operates at a near-zero cost,
demonstrating a viable model for individuals to build sophisticated AI-powered
financial tools. The operational application is publicly accessible, and the
complete source code is available for review. We conclude by discussing the
role of LLMs in financial analysis, the importance of robust debugging
methodologies, and the emerging paradigm of human-AI collaboration in software
development.

</details>


### [22] [How to Define Design in Industrial Control and Automation Software](https://arxiv.org/abs/2507.09594)
*Aydin Homay*

Main category: cs.SE

TL;DR: 论文探讨了设计在工程中的重要性，指出缺乏科学基础会导致主观决策，影响效率和创新。通过回顾软件行业的设计定义，挑战误解，并基于设计理论提出科学定义和好设计的标准。


<details>
  <summary>Details</summary>
Motivation: 设计在工程中至关重要，但目前缺乏科学基础，导致主观决策。本文旨在解决这一问题，尤其是在软件和工业控制系统中。

Method: 回顾软件行业的设计定义，挑战误解，结合设计理论提出科学定义和好设计标准，并区分临时和系统化设计方法。

Result: 通过设计理论，明确了设计的科学定义、好设计的标准，以及如何平衡操作与进化的需求。

Conclusion: 论文为设计提供了科学基础，有助于提高设计效率和创新性，尤其在软件和工业控制领域。

Abstract: Design is a fundamental aspect of engineering, enabling the creation of
products, systems, and organizations to meet societal and/or business needs.
However, the absence of a scientific foundation in design often results in
subjective decision-making, reducing both efficiency and innovation. This
challenge is particularly evident in the software industry and, by extension,
in the domain of industrial control and automation systems (iCAS).
  In this study, first we review the existing design definitions within the
software industry, challenge prevailing misconceptions about design, review
design definition in the field of design theory and address key questions such
as: When does design begin? How can design be defined scientifically? What
constitutes good design? and the difference between design and design language
by relying on advancements in the field of design theory. We also evaluate the
distinction between ad-hoc and systematic design approaches, and present
arguments on how to balance complementary operational concerns while resolving
conflicting evolutionary concerns.

</details>


### [23] [The Mythical Good Software](https://arxiv.org/abs/2507.09596)
*Aydin Homay*

Main category: cs.SE

TL;DR: 软件设计中高内聚低耦合并非总是最优，可能笨拙、模糊甚至有害。


<details>
  <summary>Details</summary>
Motivation: 阐明高内聚低耦合的实际意义及其潜在问题。

Method: 通过分析其笨拙性、模糊性和潜在危害性来说明。

Result: 指出高内聚低耦合并非绝对原则，需权衡成本。

Conclusion: 设计时应综合考量，而非盲目追求高内聚低耦合。

Abstract: Good software has high cohesion and low coupling is clumsy, obscure, and in
some certain cases could be actually a harmful state of being. It is clumsy
because there is no perfect correlation between higher cohesiveness and optimum
design, and it is obscure because it conveys the message that coupling and
cohesion are two distinct design principles, while there are in principle the
same design approaches, and only the time and space differ between them, and it
could also be a harmful state of being because we should not always aim for
higher cohesiveness without considering its cost.
  In the course of this study, we aim to elucidate for the readers the meaning
and underlying philosophy of the aforementioned paragraph.

</details>


### [24] [Complexity and Coupling: A Functional Domain Approach](https://arxiv.org/abs/2507.09599)
*Aydin Homay*

Main category: cs.SE

TL;DR: 本文对复杂性和耦合进行了精确科学定义，强调其在功能域中的重要性，而非物理属性，并通过多学科例子证明复杂性不依赖系统规模，耦合也发生在功能域。最终指出有效设计需在功能域中解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有对复杂性和耦合的定义模糊且基于物理属性，导致混淆和不一致，本文旨在提供基于功能域的科学定义。

Method: 通过分析工业控制与自动化系统(iCAS)及其他领域（如软件工程、机械设计）的例子，重新定义复杂性和耦合，并探讨其关系。

Result: 复杂性不依赖于系统规模或组件数量，耦合发生在功能域而非物理域。

Conclusion: 有效设计必须在功能域中解决耦合与复杂性问题。

Abstract: This paper provides a precise and scientific definition of complexity and
coupling, grounded in the functional domain, particularly within industrial
control and automation systems (iCAS). We highlight the widespread ambiguity in
defining complexity and coupling, emphasizing that many existing definitions
rooted in physical attributes lead to confusion and inconsistencies.
Furthermore, we re-exhibit why coupled design inherently increases complexity
and how potentially this complexity could be reduced. Drawing on examples from
various disciplines, such as software engineering, industrial automation, and
mechanical design, we demonstrate that complexity does not necessarily
correlate with system size or the number of components, and coupling, unlike
common belief in software engineering, actually does not occur in the physical
domain but in the functional domain. We conclude that effective design
necessitates addressing coupling and complexity within the functional domain.

</details>


### [25] [Code Review as Decision-Making -- Building a Cognitive Model from the Questions Asked During Code Review](https://arxiv.org/abs/2507.09637)
*Lo Gullstrand Heander,Emma Söderberg,Christofer Rydenfält*

Main category: cs.SE

TL;DR: 论文探讨了代码审查中的认知过程，提出了一个名为CRDM的模型，将其分为两个阶段，旨在提升工具支持，同时保留人际互动的益处。


<details>
  <summary>Details</summary>
Motivation: 目前代码审查的自动化工具有可能丧失知识传递和共享所有权等人际益处，研究希望通过理解认知过程来优化工具支持。

Method: 通过民族志的"说出思考"研究，对10名参与者的34次代码审查进行分析，构建了认知模型。

Result: 提出了CRDM模型，描述了开发者在代码审查中从‘定位’到‘分析’的两阶段认知过程。

Conclusion: 通过理解认知过程，可以设计更好的工具以提升代码审查的效率和体验，同时保留其人际益处。

Abstract: Code review is a well-established and valued practice in the software
engineering community contributing to both code quality and interpersonal
benefits. However, there are challenges in both tools and processes that give
rise to misalignments and frustrations. Recent research seeks to address this
by automating code review entirely, but we believe that this risks losing the
majority of the interpersonal benefits such as knowledge transfer and shared
ownership.
  We believe that by better understanding the cognitive processes involved in
code review, it would be possible to improve tool support, with out without AI,
and make code review both more efficient, more enjoyable, while increasing or
maintaining all of its benefits. In this paper, we conduct an ethnographic
think-aloud study involving 10 participants and 34 code reviews. We build a
cognitive model of code review bottom up through thematic, statistical,
temporal, and sequential analysis of the transcribed material. Through the
data, the similarities between the cognitive process in code review and
decision-making processes, especially recognition-primed decision-making,
become apparent.
  The result is the Code Review as Decision-Making (CRDM) model that shows how
the developers move through two phases during the code review; first an
orientation phase to establish context and rationale and then an analytical
phase to understand, assess, and plan the rest of the review. Throughout the
process several decisions must be taken, on writing comments, finding more
information, voting, running the code locally, verifying continuous integration
results, etc.
  Analysis software and process-coded data publicly available at:
https://doi.org/10.5281/zenodo.15758266

</details>


### [26] [Is Quantization a Deal-breaker? Empirical Insights from Large Code Models](https://arxiv.org/abs/2507.09665)
*Saima Afrin,Bowen Xu,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 量化技术能有效减少大语言模型的资源需求，同时保持代码生成的功能正确性和质量属性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型量化对代码生成质量影响的研究不足问题。

Method: 应用AWQ量化技术对CodeLlama和DeepSeekCoder模型进行量化，生成Java和Python代码，并使用静态分析工具评估代码质量。

Result: 量化不仅保持功能正确性，还保留了代码的可维护性和结构简洁性等关键质量属性。

Conclusion: 量化是一种稳健的技术，适用于优化大代码模型，同时保持代码质量。

Abstract: The growing scale of large language models (LLMs) not only demands extensive
computational resources but also raises environmental concerns due to their
increasing carbon footprint. Model quantization emerges as an effective
approach that can reduce the resource demands of LLMs by decreasing parameter
precision without substantially affecting performance (e.g., 16 bit to 4 bit).
While recent studies have established quantization as a promising approach for
optimizing large code models (LCMs), a specialized subset of LLMs tailored for
automated software engineering, their findings offer only limited insights into
its practical implications. Specifically, current investigations focus only on
the functional correctness of the code generated by quantized models,
neglecting how quantization impacts critical aspects of code quality such as
reliability, maintainability, and security. To bridge this gap, our study
investigates the effects of quantization on the qualitative aspects of
automatically generated code. We apply Activation-aware Weight Quantization
(AWQ) to two widely used code models, CodeLlama and DeepSeekCoder, to generate
Java and Python code. Using state-of-the-art static analysis tools, we evaluate
software quality metrics and static features including cyclomatic complexity,
cognitive complexity, and lines of code. Our findings reveal that quantization
is a robust technique that not only preserves functional correctness, but also
retains key qualitative code attributes sought after by developers, such as
maintainability and structural simplicity.

</details>


### [27] [Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications](https://arxiv.org/abs/2507.09820)
*Jia Yi Goh,Shaun Khoo,Nyx Iskandar,Gabriel Chua,Leanne Tan,Jessica Foo*

Main category: cs.SE

TL;DR: 本文提出了一种评估大语言模型（LLM）应用级安全的实用框架，包含定制化安全风险分类原则和评估实践，并通过实际部署验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的安全测试多集中于基础模型，但应用级组件（如系统提示、检索管道和防护措施）对LLM应用的整体安全性有重要影响，亟需评估。

Method: 框架包括两部分：开发定制化安全风险分类的原则，以及评估LLM应用安全风险的实践方法。

Result: 通过组织内多个实际用例的部署验证了框架的实用性，为规模化安全测试提供了参考。

Conclusion: 该研究旨在弥合AI安全理论与实际LLM应用保护之间的差距，提供可操作的安全部署指南。

Abstract: Most safety testing efforts for large language models (LLMs) today focus on
evaluating foundation models. However, there is a growing need to evaluate
safety at the application level, as components such as system prompts,
retrieval pipelines, and guardrails introduce additional factors that
significantly influence the overall safety of LLM applications. In this paper,
we introduce a practical framework for evaluating application-level safety in
LLM systems, validated through real-world deployment across multiple use cases
within our organization. The framework consists of two parts: (1) principles
for developing customized safety risk taxonomies, and (2) practices for
evaluating safety risks in LLM applications. We illustrate how the proposed
framework was applied in our internal pilot, providing a reference point for
organizations seeking to scale their safety testing efforts. This work aims to
bridge the gap between theoretical concepts in AI safety and the operational
realities of safeguarding LLM applications in practice, offering actionable
guidance for safe and scalable deployment.

</details>


### [28] [Turning the Tide: Repository-based Code Reflection](https://arxiv.org/abs/2507.09866)
*Wei Zhang,Jian Yang,Jiaxi Yang,Ya Wang,Zhoujun Li,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 介绍了LiveRepoReflection，一个用于评估多文件仓库场景下代码理解和生成的挑战性基准，以及RepoReflection-Instruct训练数据集，用于训练RepoReflectionCoder模型。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型（LLM）在代码生成和理解方面表现优异，但缺乏对仓库代码修改场景的评估，且现有基准存在数据污染和反思能力不足的问题。

Method: 提出LiveRepoReflection基准，包含1,888个测试案例，覆盖6种编程语言；构建RepoReflection-Instruct数据集，通过两轮对话训练RepoReflectionCoder模型。

Result: 生成了一个高质量的基准和数据集，并评估了40多个LLM在仓库代码反思任务上的表现。

Conclusion: LiveRepoReflection填补了仓库代码修改评估的空白，提升了模型在多文件环境中的代码反思能力。

Abstract: Code large language models (LLMs) enhance programming by understanding and
generating code across languages, offering intelligent feedback, bug detection,
and code updates through reflection, improving development efficiency and
accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code
generation and real-world relevance, previous works ignore the scenario of
modifying code in repositories. Considering challenges remaining in improving
reflection capabilities and avoiding data contamination in dynamic benchmarks,
we introduce LiveRepoReflection, a challenging benchmark for evaluating code
understanding and generation in multi-file repository contexts, featuring 1,888
rigorously filtered test cases across $6$ programming languages to ensure
diversity, correctness, and high difficulty. Further, we create
RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning
dataset derived from diverse sources, used to train RepoReflectionCoder through
a two-turn dialogue process involving code generation and error-driven repair.
The leaderboard evaluates over 40 LLMs to reflect the model performance of
repository-based code reflection.

</details>


### [29] [PathFuzzing: Worst Case Analysis by Fuzzing Symbolic-Execution Paths](https://arxiv.org/abs/2507.09892)
*Zimu Chen,Di Wang*

Main category: cs.SE

TL;DR: PathFuzzing 是一种结合模糊测试与符号执行优点的最坏情况分析方法，通过程序转换和进化算法优化资源消耗的估计。


<details>
  <summary>Details</summary>
Motivation: 模糊测试和符号执行在最坏情况分析（WCA）中存在代码覆盖率低和路径爆炸问题，需要一种更高效的方法。

Method: PathFuzzing 将程序转换为符号程序，利用二进制字符串编码执行路径，并通过进化模糊测试技术搜索高资源消耗的可满足路径条件。

Result: 实验结果表明，PathFuzzing 在基准测试中表现优于传统模糊测试和符号执行方法。

Conclusion: PathFuzzing 成功结合了模糊测试和符号执行的优势，为最坏情况资源消耗分析提供了更高效的解决方案。

Abstract: Estimating worst-case resource consumption is a critical task in software
development. The worst-case analysis (WCA) problem is an optimization-based
abstraction of this task. Fuzzing and symbolic execution are widely used
techniques for addressing the WCA problem. However, improving code coverage in
fuzzing or managing path explosion in symbolic execution within the context of
WCA poses significant challenges. In this paper, we propose PathFuzzing, aiming
to combine the strengths of both techniques to design a WCA method. The key
idea is to transform a program into a symbolic one that takes an execution path
(encoded as a binary string) and interprets the bits as branch decisions.
PathFuzzing then applies evolutionary fuzzing techniques to the transformed
program to search for binary strings that represent satisfiable path conditions
and lead to high resource consumption. We evaluate the performance of
PathFuzzing experimentally on a benchmark suite that consists of prior work's
benchmarks and some added by us. Results show that PathFuzzing generally
outperforms a fuzzing and a symbolic-execution baseline.

</details>


### [30] [Modelling Interrelations Between Agile Practices: The Agile Map](https://arxiv.org/abs/2507.09907)
*Thomas Hansper,Kevin Phong Pham,Michael Neumann*

Main category: cs.SE

TL;DR: 摘要讨论了敏捷实践中实践之间的相互关系问题，并提出了一个理论模型（Agile Map）来系统化描述这些关系，以帮助实践者更有效地选择和组合敏捷实践。


<details>
  <summary>Details</summary>
Motivation: 敏捷实践在广泛应用中被定制和采用，导致实践之间存在高度多样性，但缺乏对其相互关系的系统理解。这可能导致实践中组合使用敏捷实践时效果受限。

Method: 研究提出了一种系统化的方法，通过构建理论模型（Agile Map）来描述敏捷实践之间的相互联系。

Result: 论文的核心贡献是Agile Map，它为敏捷实践之间的关系提供了一个理论模型，帮助实践者更有意义地选择和组合实践。

Conclusion: Agile Map为理解和应用敏捷实践之间的相互关系提供了一个系统化的工具，旨在支持实践者在实际工作中更有效地使用敏捷方法。

Abstract: Agile methods are defined through guidelines comprising various practices
intended to enable agile ways of working. These guidelines further comprise a
specific set of agile practices aiming to enable teams for an agile way of
working. However, due to its wide-spread use in practice we know that agile
practices are adopted and tailored intensively, which lead to a high variety of
agile practices in terms of their level of detail. Problem: A high variety of
agile practices can be challenging as we do not know how different agile
practices are interrelated with each other. To be more precise, tailoring and
adopting agile practices may lead to the challenge, that the combinatorial use
of several agile practices can only be successful to a limited extent, as
practices support or even require each other for a effective use in practice.
Objective: Our study aims to provide an enabler for this problem. We want to
identify interrelations between agile practices and describe them in a
systematic manner. Contribution: The core contribution of this paper is the
Agile Map, a theoretical model describing relations between agile practices
following a systematic approach aiming to provide an overview of coherences
between agile practices. The model aims to support practitioners in selecting
and combining agile practices in a meaningful way.

</details>


### [31] [When Less is More: A systematic review of four-day workweek conceptualizations and their effects on organizational performance](https://arxiv.org/abs/2507.09911)
*Marvin Auf der Landwehr,Julia Topp,Michael Neumann*

Main category: cs.SE

TL;DR: 本文探讨了压缩工作制（如四天工作周）对IT企业运营效率的影响，并提出了一个综合框架以指导其实施。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解压缩工作制对IT企业运营效率的影响，同时开发一个框架来指导其应用。

Method: 采用系统性文献综述和网络内容分析，结合科学和实践导向的文献。

Result: 提出了一个元框架，将压缩工作制的概念化和效果匹配，以指导企业在不同管理前提和情境下的采用。

Conclusion: 该研究为IT企业实施压缩工作制提供了理论支持和实践指南。

Abstract: Context: Agile IT organizations, which are characterized by self-organization
and collaborative social interactions, require motivating, efficient and
flexible work environments to maximize value creation. Compressed work
schedules such as the four-day workweek have evolved into multiple facets over
the last decades and are associated with various benefits for organizations and
their employees. Objective: Our objective in this study is to deepen our
comprehension of the impact of compressed work schedules on the operational
efficacy of IT enterprises, while concurrently developing a comprehensive
framework delineating the intricacies of compressed work schedules.Method: We
conducted a systematic review of available conceptualizations related to
four-day workweek schedules and elaborate on their organizational and social
effects. To cover scientific and practice-oriented literature, our review
combined a systematic literature review and a web content analysis. Results:
Based on the generated insights, we derive a meta-framework that matches
conceptualizations and effects, finally guiding the adoption of compressed work
schedules based on individual managerial prerequisites and circumstances.

</details>


### [32] [Explicit Vulnerability Generation with LLMs: An Investigation Beyond Adversarial Attacks](https://arxiv.org/abs/2507.10054)
*Emir Bosnak,Sahand Moslemi,Mayasah Lami,Anil Koyuncu*

Main category: cs.SE

TL;DR: 研究发现开源大语言模型（LLMs）在被直接或间接提示时容易生成不安全代码，且用户角色和提示方式显著影响漏洞生成率。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在明确要求生成不安全代码时的行为，评估开源模型的安全机制局限性。

Method: 采用动态提示和反向提示两种实验设计，结合静态分析工具评估模型生成的代码漏洞。

Result: 所有测试模型均频繁生成不安全代码，Qwen2准确性最高；用户角色（学生）和直接提示对漏洞生成影响显著。

Conclusion: 开源模型的安全机制存在不足，教育类请求尤其容易被滥用，需进一步改进防护措施。

Abstract: Large Language Models (LLMs) are increasingly used as code assistants, yet
their behavior when explicitly asked to generate insecure code remains poorly
understood. While prior research has focused on unintended vulnerabilities or
adversarial prompting techniques, this study examines a more direct threat
scenario: open-source LLMs generating vulnerable code when prompted either
directly or indirectly. We propose a dual experimental design: (1) Dynamic
Prompting, which systematically varies vulnerability type, user persona, and
directness across structured templates; and (2) Reverse Prompting, which
derives prompts from real vulnerable code samples to assess vulnerability
reproduction accuracy. We evaluate three open-source 7B-parameter models
(Qwen2, Mistral, and Gemma) using ESBMC static analysis to assess both the
presence of vulnerabilities and the correctness of the generated vulnerability
type. Results show all models frequently produce vulnerable outputs, with Qwen2
achieving highest correctness rates. User persona significantly affects
success, where student personas achieved higher vulnerability rates than
professional roles, while direct prompts were marginally more effective.
Vulnerability reproduction followed an inverted-U pattern with cyclomatic
complexity, peaking at moderate ranges. Our findings expose limitations of
safety mechanisms in open-source models, particularly for seemingly benign
educational requests.

</details>


### [33] [LLMShot: Reducing snapshot testing maintenance via LLMs](https://arxiv.org/abs/2507.10062)
*Ergün Batuhan Kaynak,Mayasah Lami,Sahand Moslemi,Anil Koyuncu*

Main category: cs.SE

TL;DR: LLMShot利用视觉大语言模型自动分析UI快照测试失败，通过分层分类减少人工检查负担，12B模型召回率超84%，但提示机制仍有局限。


<details>
  <summary>Details</summary>
Motivation: UI快照测试因频繁变更导致大量手动检查需求，需自动化解决方案降低维护成本。

Method: 提出LLMShot框架，利用分层分类的视觉大语言模型分析测试失败，并构建iOS应用数据集验证。

Result: 12B模型召回率达84%，4B模型适合持续集成，但提示机制可控性不足。

Conclusion: LLMShot首次实现语义快照测试自动分析，为智能UI测试提供新方向。

Abstract: Snapshot testing has emerged as a critical technique for UI validation in
modern software development, yet it suffers from substantial maintenance
overhead due to frequent UI changes causing test failures that require manual
inspection to distinguish between genuine regressions and intentional design
changes. This manual triage process becomes increasingly burdensome as
applications evolve, creating a need for automated analysis solutions. This
paper introduces LLMShot, a novel framework that leverages vision-based Large
Language Models to automatically analyze snapshot test failures through
hierarchical classification of UI changes. To evaluate LLMShot's effectiveness,
we developed a comprehensive dataset using a feature-rich iOS application with
configurable feature flags, creating realistic scenarios that produce authentic
snapshot differences representative of real development workflows. Our
evaluation using Gemma3 models demonstrates strong classification performance,
with the 12B variant achieving over 84% recall in identifying failure root
causes while the 4B model offers practical deployment advantages with
acceptable performance for continuous integration environments. However, our
exploration of selective ignore mechanisms revealed significant limitations in
current prompting-based approaches for controllable visual reasoning. LLMShot
represents the first automated approach to semantic snapshot test analysis,
offering developers structured insights that can substantially reduce manual
triage effort and advance toward more intelligent UI testing paradigms.

</details>


### [34] [Accelerating Automatic Program Repair with Dual Retrieval-Augmented Fine-Tuning and Patch Generation on Large Language Models](https://arxiv.org/abs/2507.10103)
*Hanyang Guo,Xiaoheng Xie,Hong-Ning Dai,Peng Di,Yu Zhang,Bishenghui Tao,Zibin Zheng*

Main category: cs.SE

TL;DR: SelRepair是一种结合微调LLM和双RAG模块的新型APR方法，通过语义和语法/结构相似性优化修复效率，性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有的APR方法受限于缺陷类型、训练数据质量和模型参数规模，且代码LLMs和RAG设计未充分关注代码修复任务或代码特征。

Method: 提出SelRepair，整合微调LLM与双RAG模块，利用bug-fix对数据集和相似性信息检索优化效率。

Result: 在Java数据集上表现优于其他方法，EM达到26.29%和17.64%，推理时间减少至少6.42%。

Conclusion: SelRepair通过优化信息检索和模型微调，显著提升了APR任务的性能和效率。

Abstract: Automated Program Repair (APR) is essential for ensuring software reliability
and quality while enhancing efficiency and reducing developers' workload.
Although rule-based and learning-based APR methods have demonstrated their
effectiveness, their performance was constrained by the defect type of repair,
the quality of training data, and the size of model parameters. Recently, Large
Language Models (LLMs) combined with Retrieval-Augmented-Generation (RAG) have
been increasingly adopted in APR tasks. However, current code LLMs and RAG
designs neither fully address code repair tasks nor consider code-specific
features. To overcome these limitations, we propose SelRepair, a novel APR
approach with integration of a fine-tuned LLM with a newly-designed dual RAG
module. This approach uses a bug-fix pair dataset for fine-tuning and
incorporates semantic and syntactic/structural similarity information through
an RAG selection gate. This design ensures relevant information is retrieved
efficiently, thereby reducing token length and inference time. Evaluations on
Java datasets show SelRepair outperforms other APR methods, achieving 26.29%
and 17.64% in terms of exact match (EM) on different datasets while reducing
inference time by at least 6.42% with controlled input lengths.

</details>


### [35] [Breaking the Myth: Can Small Models Infer Postconditions Too?](https://arxiv.org/abs/2507.10182)
*Gehao Zhang,Zhenting Wang,Juan Zhai*

Main category: cs.SE

TL;DR: 小型微调语言模型在生成高质量后置条件时性能媲美大型模型，且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 验证是否真正需要大型模型来生成形式化规范，探索高效实用的自动化规范生成方案。

Method: 构建包含提示、推理日志和后置条件的专用数据集，并对7B参数代码模型进行微调。

Result: 在真实Java缺陷基准测试中，小型模型在语法正确性、语义正确性和区分缺陷能力上媲美或超越大型模型。

Conclusion: 针对特定任务的微调可以使小型模型达到大型模型的效果，为自动化规范生成的现实应用提供高效路径。

Abstract: Formal specifications are essential for ensuring software correctness, yet
manually writing them is tedious and error-prone. Large Language Models (LLMs)
have shown promise in generating such specifications from natural language
intents, but the giant model size and high computational demands raise a
fundamental question: Do we really need large models for this task? In this
paper, we show that a small, fine-tuned language model can achieve high-quality
postcondition generation with much lower computational costs. We construct a
specialized dataset of prompts, reasoning logs, and postconditions, then
supervise the fine-tuning of a $7$B-parameter code model. Our approach tackles
real-world repository dependencies and preserves pre-state information,
allowing for expressive and accurate specifications. We evaluate the model on a
benchmark of real-world Java bugs (Defects4J) and compare against both
proprietary giants (e.g., GPT-4o) and open-source large models. Empirical
results demonstrate that our compact model matches or outperforms significantly
larger counterparts in syntax correctness, semantic correctness, and
bug-distinguishing capability. These findings highlight that targeted
fine-tuning on a modest dataset can enable small models to achieve results
formerly seen only in massive, resource-heavy LLMs, offering a practical and
efficient path for the real-world adoption of automated specification
generation.

</details>


### [36] [Towards a Framework for Operationalizing the Specification of Trustworthy AI Requirements](https://arxiv.org/abs/2507.10228)
*Hugo Villamizar,Daniel Mendez,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 该论文提出整合AMDiRE和PerSpecML两种方法，以规范AI系统的可信赖性需求，填补确定性系统与非确定性系统之间的需求工程空白。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统因数据驱动和非确定性行为导致的信任问题，需要结构化的需求工程方法。

Method: 结合AMDiRE（基于工件的需求工程方法）和PerSpecML（支持ML系统需求的多视角方法）。

Result: 提出一条实现可信赖性需求的路径，连接利益相关者关注点和结构化模型。

Conclusion: 提出了研究方向与开放挑战，呼吁需求工程社区进一步探索。

Abstract: Growing concerns around the trustworthiness of AI-enabled systems highlight
the role of requirements engineering (RE) in addressing emergent,
context-dependent properties that are difficult to specify without structured
approaches. In this short vision paper, we propose the integration of two
complementary approaches: AMDiRE, an artefact-based approach for RE, and
PerSpecML, a perspective-based method designed to support the elicitation,
analysis, and specification of machine learning (ML)-enabled systems. AMDiRE
provides a structured, artefact-centric, process-agnostic methodology and
templates that promote consistency and traceability in the results; however, it
is primarily oriented toward deterministic systems. PerSpecML, in turn,
introduces multi-perspective guidance to uncover concerns arising from the
data-driven and non-deterministic behavior of ML-enabled systems. We envision a
pathway to operationalize trustworthiness-related requirements, bridging
stakeholder-driven concerns and structured artefact models. We conclude by
outlining key research directions and open challenges to be discussed with the
RE community.

</details>


### [37] [An Empirical Study of Interaction Bugs in ROS-based Software](https://arxiv.org/abs/2507.10235)
*Zhixiang Chen,Zhuangbin Chen,Xingjie Cai,Wei Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 论文通过实证研究分析了机器人系统中交互问题（iBugs），提出了三类iBugs及其解决方法，旨在提升机器人系统的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 机器人系统的可靠性不仅取决于单个组件的正确性，还依赖于组件间交互的正确性，但交互相关的可靠性问题（iBugs）尚未被充分研究。

Method: 使用ROS框架分析了10个代表性项目中的121个iBugs，将其分为三类：系统内iBugs、硬件iBugs和环境iBugs，并研究了原因和修复策略。

Result: 研究揭示了iBugs的特点及其影响，提供了改进预防和检测的方向。

Conclusion: 研究结果为提高机器人系统的鲁棒性和安全性提供了重要见解。

Abstract: Modern robotic systems integrate multiple independent software and hardware
components, each responsible for distinct functionalities such as perception,
decision-making, and execution. These components interact extensively to
accomplish complex end-to-end tasks. As a result, the overall system
reliability depends not only on the correctness of individual components, but
also on the correctness of their interactions. Failures often manifest at the
boundaries between components, yet interaction-related reliability issues in
robotics--referred to here as interaction bugs (iBugs)--remain underexplored.
  This work presents an empirical study of iBugs within robotic systems built
using the Robot Operating System (ROS), a widely adopted open-source robotics
framework. A total of 121 iBugs were analyzed across ten actively maintained
and representative ROS projects. The identified iBugs are categorized into
three major types: intra-system iBugs, hardware iBugs, and environmental iBugs,
covering a broad range of interaction scenarios in robotics. The analysis
includes an examination of root causes, fixing strategies, and the impact of
these bugs. Several findingsa are derived that shed light on the nature of
iBugs and suggest directions for improving their prevention and detection.
These insights aim to inform the design of more robust and safer robotic
systems.

</details>


### [38] [Helveg: Diagrams for Software Documentation](https://arxiv.org/abs/2507.10244)
*Adam Štěpánek,David Kuťák,Barbora Kozlíková,Jan Byška*

Main category: cs.SE

TL;DR: 论文介绍了一种名为Helveg的工具，通过交互式节点链接图改进API文档的探索性分析能力，解决了传统文档不易导航的问题。用户测试后对工具的图形设计和用户体验进行了优化。


<details>
  <summary>Details</summary>
Motivation: 传统API文档形式固定，不适合代码库的高层次探索分析，开发者需要更灵活的探索工具。

Method: 设计了交互式节点链接图，支持节点符号和灵活过滤功能，并开发了C#代码库自动生成图表的原型工具Helveg。

Result: 用户测试证实了工具潜力，但也发现了可读性、直观性和用户体验问题。新版Helveg解决了这些问题。

Conclusion: 改进后的Helveg在用户测试中表现更佳，验证了交互式可视化对代码库探索的价值。

Abstract: Software developers often have to gain an understanding of a codebase. Be it
programmers getting onboarded onto a team project or, for example, developers
striving to grasp an external open-source library. In either case, they
frequently turn to the project's documentation. However, documentation in its
traditional textual form is ill-suited for this kind of high-level exploratory
analysis, since it is immutable from the readers' perspective and thus forces
them to follow a predefined path. We have designed an approach bringing aspects
of software architecture visualization to API reference documentation. It
utilizes a highly interactive node-link diagram with expressive node glyphs and
flexible filtering capabilities, providing a high-level overview of the
codebase as well as details on demand. To test our design, we have implemented
a prototype named Helveg, capable of automatically generating diagrams of C\#
codebases. User testing of Helveg confirmed its potential, but it also revealed
problems with the readability, intuitiveness, and user experience of our tool.
Therefore, in this paper, which is an extended version of our VISSOFT paper
with DOI 10.1109/VISSOFT64034.2024.00012, we address many of these problems
through major changes to the glyph design, means of interaction, and user
interface of the tool. To assess the improvements, this new version of Helveg
was evaluated again with the same group of participants as the previous
version.

</details>


### [39] [A Grounded Theory on the Teacher and Student Roles in Pair Programming](https://arxiv.org/abs/2507.10305)
*Linus Ververs,Trang Linh Lam,Janina Berger,Lutz Prechelt*

Main category: cs.SE

TL;DR: 研究发现，在结对编程中，知识转移可能因开发者忽视对方的‘权力差距’而产生负面影响，导致防御行为和恶性循环。


<details>
  <summary>Details</summary>
Motivation: 探讨在结对编程中哪些情况下知识转移会带来负面影响。

Method: 采用扎根理论方法，分析17次结对编程会话记录和6次开发者访谈。

Result: 定义了学生和教师角色，并围绕‘权力差距’提出理论，指出其可能导致防御行为和负面循环。

Conclusion: 忽视‘权力差距’会导致防御行为，进而对知识转移、合作和代码质量产生负面影响。

Abstract: Context: Pair programming is an established (agile) practice and is practiced
throughout the industry. Objective: Understand under what circumstances
knowledge transfer can harm a pair programming session. Method: Grounded Theory
Methodology based on 17 recorded pair programming sessions with 18 developers
from 5 German software companies accompanied, by 6 interviews with different
developers from 4 other German companies. Results: We define the student and
teacher roles to help developers deal with a one-sided knowledge gap. We
describe pitfalls to avoid and develop a grounded theory centered around the
Power Gap in pair programming. Conclusions: Knowledge transfer can be harmful
when developers don't pay attention to their partners needs and desires. If
developers don't pay attention to the Power Gap and keep it in check, Defensive
Behavior may arise that leads to a vicious cycle impacting the knowledge
transfer, the Togetherness and the code quality in a negative way.

</details>


### [40] [Streamlined Airborne Software Development for Large UAVs: From Unified Data Collection to Automated Code Generation](https://arxiv.org/abs/2507.10321)
*Viktor Sinitsyn,Nils Schlautmann,Florian Schwaiger,Florian Holzapfel*

Main category: cs.SE

TL;DR: 本文提出了一种用于优化航空航天设备数字接口开发的自动化工具链，以解决新创公司面临的传统挑战。


<details>
  <summary>Details</summary>
Motivation: 航空航天行业的数字化转型和新兴公司的崛起需要高效且自动化的方法来解决接口开发问题。

Method: 提出了一种新颖的流程和工具链，强调自动化、灵活性并符合设计保障要求。

Result: 该工具链已在多个项目中成功应用。

Conclusion: 该方法能够显著提升接口开发的效率，同时确保系统整合的顺畅性。

Abstract: The aerospace industry has experienced significant transformations over the
last decade, driven by technological advancements and innovative solutions in
goods and personal transportation. This evolution has spurred the emergence of
numerous start-ups that now face challenges traditionally encountered by
established aerospace companies. Among these challenges is the efficient
processing of digital intra-device communication interfaces for onboard
equipment - a critical component for ensuring seamless system integration and
functionality. Addressing this challenge requires solutions that emphasize
clear and consistent interface descriptions, automation of processes, and
reduced labor-intensive efforts.
  This paper presents a novel process and toolchain designed to streamline the
development of digital interfaces and onboard software, which our team has
successfully applied in several completed projects. The proposed approach
focuses on automation and flexibility while maintaining compliance with design
assurance requirements.

</details>


### [41] [Self-Admitted GenAI Usage in Open-Source Software](https://arxiv.org/abs/2507.10422)
*Tao Xiao,Youmei Fan,Fabio Calefato,Christoph Treude,Raula Gaikovina Kula,Hideaki Hata,Sebastian Baltes*

Main category: cs.SE

TL;DR: 分析了生成式AI（如GitHub Copilot和ChatGPT）在开源软件中的使用，提出了“自我承认的AI使用”概念，并通过混合方法研究了其任务、内容和目的，以及开发者的伦理和法律担忧。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI工具在开源软件开发中的实际使用和影响，以填补因难以区分AI生成代码和手动编写代码而导致的理解空白。

Method: 通过分析25万多个GitHub代码库中的1,292个自我承认的AI使用案例，采用混合方法（定性编码和开发者调查）开发了一个分类法，并研究了AI采用对代码变更的影响。

Result: 发现开发者积极管理AI工具的使用，强调透明度和质量控制；AI采用未导致代码变更率的普遍增加。

Conclusion: 揭示了AI辅助软件开发中需关注透明度和质量控制的实践需求，并挑战了关于AI对开发效率影响的流行观点。

Abstract: The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot
and ChatGPT is transforming software development. Since generated source code
is virtually impossible to distinguish from manually written code, their
real-world usage and impact on open-source software development remain poorly
understood. In this paper, we introduce the concept of self-admitted GenAI
usage, that is, developers explicitly referring to the use of GenAI tools for
content creation in software artifacts. Using this concept as a lens to study
how GenAI tools are integrated into open-source software projects, we analyze a
curated sample of more than 250,000 GitHub repositories, identifying 1,292 such
self-admissions across 156 repositories in commit messages, code comments, and
project documentation. Using a mixed methods approach, we derive a taxonomy of
32 tasks, 10 content types, and 11 purposes associated with GenAI usage based
on 284 qualitatively coded mentions. We then analyze 13 documents with policies
and usage guidelines for GenAI tools and conduct a developer survey to uncover
the ethical, legal, and practical concerns behind them. Our findings reveal
that developers actively manage how GenAI is used in their projects,
highlighting the need for project-level transparency, attribution, and quality
control practices in the new era of AI-assisted software development. Finally,
we examine the longitudinal impact of GenAI adoption on code churn in 151
repositories with self-admitted GenAI usage and find no general increase,
contradicting popular narratives on the impact of GenAI on software
development.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [42] [Bounded Model Checking of RISC-V Machine Code with Context-Free-Language Ordered Binary Decision Diagrams](https://arxiv.org/abs/2507.09539)
*Anna Bolotina,Christoph M. Kirsch,Stefanie Muroya Lei,Matthias Pleschinger*

Main category: cs.PL

TL;DR: 该论文探讨了如何通过将符号执行推理完全下放至机器码层面并利用SMT求解器和其他高效求解技术，来解决软件分析中的状态爆炸问题。作者开发了工具rotor和bitme，分别用于模型生成和有界模型检查，并展示了改进潜力。


<details>
  <summary>Details</summary>
Motivation: 符号执行在软件行为分析中强大但面临状态爆炸的挑战，尤其在控制流和数据流管理上。现有工具通常在牺牲完整性的前提下管理控制流，而将数据流分析交给SMT求解器。作者希望改变现状，将推理下放到机器码层面并利用更高效的求解技术。

Method: 作者开发了两个工具：rotor用于模型生成，bitme用于有界模型检查。rotor专注于RISC-V整数语义，而bitme引入了两种二进制决策图（ADDs和CFLOBDDs）来优化输入传播，减少SMT求解器的负担。

Result: 实验显示，尽管当前SMT求解器表现不佳，但通过bitme工具利用BDDs（尤其是CFLOBDDs）可以显著加快求解速度并缓解状态爆炸问题，展示了改进潜力。

Conclusion: 论文表明，通过将符号执行推理完全下放至机器码层面并采用高效求解技术，可以提升软件分析的规模和效率，尤其是使用CFLOBDDs进一步优化状态管理。

Abstract: Symbolic execution is a powerful technique for analyzing the behavior of
software yet scalability remains a challenge due to state explosion in control
and data flow. Existing tools typically aim at managing control flow
internally, often at the expense of completeness, while offloading reasoning
over data flow to SMT solvers. Moreover, reasoning typically happens on source
code or intermediate representation level to leverage structural information,
making machine code generation part of the trust base. We are interested in
changing the equation in two non-trivial ways: pushing reasoning down to
machine code level, and then offloading reasoning entirely into SMT solvers and
other, possibly more efficient solver technology. In more abstract terms, we
are asking if bit-precise reasoning technology can be made scalable on
software, and not just hardware. For this purpose, we developed two tools
called rotor and bitme for model generation and bounded model checking,
respectively. We chose RISC-V restricted to integer arithmetic as modeling
target for rotor since RISC-V integer semantics is essentially equivalent to
established SMT semantics over bitvectors and arrays of bitvectors. While
state-of-the-art SMT solvers struggle in our experiments, we have evidence that
there is potential for improvement. To show the potential, we have slightly
generalized and then implemented in bitme two types of binary decision diagrams
(BDDs): algebraic decision diagrams (ADDs) and context-free-language ordered
binary decision diagrams (CFLOBDDs). Bitme uses BDDs to propagate program input
through models, essentially generalizing constant propagation to domain
propagation. SMT solvers only get involved when model input cannot be
propagated, significanly speeding up SMT solving. We then study the impact on
state explosion of CFLOBDDs, which are potentially more scalable than ADDs.

</details>


### [43] [BeePL: Correct-by-compilation kernel extensions](https://arxiv.org/abs/2507.09883)
*Swarn Priya,Frédéric Besson,Connor Sughrue,Tim Steenvoorden,Jamie Fulford,Freek Verbeek,Binoy Ravindran*

Main category: cs.PL

TL;DR: BeePL 是一种针对 eBPF 的领域特定语言，通过形式化验证的类型系统解决现有 eBPF 验证器的不足，确保程序的内存安全、终止性和控制流安全。


<details>
  <summary>Details</summary>
Motivation: 现有 eBPF 验证器在处理程序时过于保守或存在漏洞，导致某些有效程序被拒绝或允许不安全行为。

Method: 引入 BeePL，其类型系统可静态保证内存访问、指针使用、循环和控制流的安全性，并通过形式化证明支持这些保证。对于动态问题，BeePL 通过运行时检查补充。

Result: BeePL 提供了从高级语言到 BPF 字节码的可验证编译策略，确保内核扩展的安全性。

Conclusion: BeePL 为安全内核扩展提供了一种端到端可验证的工具链，解决了现有 eBPF 验证器的缺陷。

Abstract: eBPF is a technology that allows developers to safely extend kernel
functionality without modifying kernel source code or developing loadable
kernel modules. Since the kernel governs critical system operations and
enforces isolation boundaries between user space and privileged data, any
mechanism that modifies its behavior must meet the highest standards of safety
and correctness. To this end, the eBPF toolchain includes a verifier, which
statically checks safety properties such as memory access validity, bounded
loops, and type correctness before loading the program into the kernel.
However, the existing verifier is both overly conservative in some
cases-rejecting valid programs-and unsound in others, permitting unsafe
behavior that violates the intended semantics of the kernel interface.
  To address these challenges, we introduce BeePL, a domain-specific language
for eBPF with a formally verified type system. The BeePL type system, along
with the language design, statically enforces key safety properties such as
type-correct memory access, safe pointer usage, absence of unbounded loops, and
structured control flow. These guarantees are backed by formal type soundness
proofs, ensuring that well-typed programs satisfy the safety invariants
required by the eBPF execution environment. BeePL also proves that well-typed
source programs meet critical eBPF-specific properties related to memory
safety, termination, and control flow, enabling high-level reasoning prior to
compilation. For properties not fully enforceable statically-such as dynamic
bounds and undefined behavior-BeePL inserts semantics-preserving runtime checks
during compilation. We develop a verified compilation strategy that extends
CompCert to generate BPF bytecode from BeePL programs, establishing a
principled foundation for an end-to-end verifiable toolchain for safe kernel
extensions.

</details>


### [44] [Rows and Capabilities as Modal Effects](https://arxiv.org/abs/2507.10301)
*Wenhao Tang,Sam Lindley*

Main category: cs.PL

TL;DR: 本文提出了一种统一的框架，用于编码、分析和比较基于行多态和能力的不同效果系统，揭示了效果跟踪机制的本质。


<details>
  <summary>Details</summary>
Motivation: 为了填补当前对行多态和能力基础效果系统之间关系理解的空白，解决效果跟踪与其他特性（如函数）纠缠的问题。

Method: 利用并推广模态效果类型，提出一个框架，通过宏翻译将现有行和能力基础效果系统编码到框架中。

Result: 编码保留了类型和语义，揭示了不同效果系统的本质差异，为语言设计提供了见解。

Conclusion: 该框架为效果系统的分析和比较提供了统一基础，有助于理解和设计效果处理语言。

Abstract: Effect handlers allow programmers to model and compose computational effects
modularly. Effect systems statically guarantee that all effects are handled.
Several recent practical effect systems are based on either row polymorphism or
capabilities. However, there remains a gap in understanding the precise
relationship between effect systems with such disparate foundations. The main
difficulty is that in both row-based and capability-based systems, effect
tracking is typically entangled with other features such as functions.
  We propose a uniform framework for encoding, analysing, and comparing effect
systems. Our framework exploits and generalises modal effect types, a recent
novel effect system which decouples effect tracking from functions via
modalities. Modalities offer fine-grained control over when and how effects are
tracked, enabling us to express different strategies for effect tracking. We
give encodings as macro translations from existing row-based and
capability-based effect systems into our framework and show that these
encodings preserve types and semantics. Our encodings reveal the essence of
effect tracking mechanisms in different effect systems, enable a direct
analysis on their differences, and provide valuable insights on language
design.

</details>


### [45] [Orthologic Type Systems](https://arxiv.org/abs/2507.10482)
*Simon Guilloud,Viktor Kunčak*

Main category: cs.PL

TL;DR: 本文提出基于正交逻辑设计类型系统，支持子类型假设下的交、并和否定类型，扩展了正交逻辑以支持单调和非单调函数，并提出了一个高效的子类型关系判定算法和多项式时间规范化算法。


<details>
  <summary>Details</summary>
Motivation: 为了解决在子类型假设下设计支持交、并和否定类型的类型系统的问题，基于正交逻辑进行研究。

Method: 扩展正交逻辑以支持单调和非单调函数，提出一个带函数符号的证明系统，并证明其允许部分割消除。此外，设计了子类型关系判定算法和规范化算法。

Result: 提出了一个时间复杂度为$	extit{O}(n^2(1+m))$的子类型关系判定算法，以及一个多项式时间$	extit{O}(n^2)$的规范化算法。

Conclusion: 通过正交逻辑的扩展和算法设计，成功解决了子类型假设下复杂类型系统的设计和实现问题，为类型系统的理论和实践提供了新工具。

Abstract: We propose to use orthologic as the basis for designing type systems
supporting intersection, union, and negation types in the presence of subtyping
assumptions. We show how to extend orthologic to support monotonic and
antimonotonic functions, supporting the use of type constructors in such type
systems. We present a proof system for orthologic with function symbols,
showing that it admits partial cut elimination. Using these insights, we
present an $\mathcal O(n^2(1+m))$ algorithm for deciding the subtyping relation
under $m$ assumptions. We also show $O(n^2)$ polynomial-time normalization
algorithm, allowing simplification of types to their minimal canonical form.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [46] [Transformer based Collaborative Reinforcement Learning for Fluid Antenna System (FAS)-enabled 3D UAV Positioning](https://arxiv.org/abs/2507.09094)
*Xiaoren Xu,Hao Xu,Dongyu Wei,Walid Saad,Mehdi Bennis,Mingzhe Chen*

Main category: cs.NI

TL;DR: 一种新型的3D定位框架，通过流体天线系统（FAS）和无人机（UAV）协作，利用注意力机制增强的多智能体强化学习（AR-MARL）优化目标无人机的实时定位精度。


<details>
  <summary>Details</summary>
Motivation: 无人机在任务执行中需要实时定位，但传统方法在动态环境下精度不足。提出了一种结合FAS和优化算法的框架，以提高定位准确性。

Method: 通过优化受控无人机的轨迹和天线端口选择，使用注意力机制的循环多智能体强化学习（AR-MARL）来估计目标无人机的实时位置。

Result: 仿真结果显示，AR-MARL方案比传统方法（VD-MARL）和无FAS的方法分别减少了17.5%和58.5%的平均定位误差。

Conclusion: 该框架显著提高了目标无人机的定位精度，证明了FAS和AR-MARL在动态环境中的有效性。

Abstract: In this paper, a novel Three dimensional (3D) positioning framework of fluid
antenna system (FAS)-enabled unmanned aerial vehicles (UAVs) is developed. In
the proposed framework, a set of controlled UAVs cooperatively estimate the
real-time 3D position of a target UAV. Here, the active UAV transmits a
measurement signal to the passive UAVs via the reflection from the target UAV.
Each passive UAV estimates the distance of the active-target-passive UAV link
and selects an antenna port to share the distance information with the base
station (BS) that calculates the real-time position of the target UAV. As the
target UAV is moving due to its task operation, the controlled UAVs must
optimize their trajectories and select optimal antenna port, aiming to estimate
the real-time position of the target UAV. We formulate this problem as an
optimization problem to minimize the target UAV positioning error via
optimizing the trajectories of all controlled UAVs and antenna port selection
of passive UAVs. Here, an attention-based recurrent multi-agent reinforcement
learning (AR-MARL) scheme is proposed, which enables each controlled UAV to use
the local Q function to determine its trajectory and antenna port while
optimizing the target UAV positioning performance without knowing the
trajectories and antenna port selections of other controlled UAVs. Different
from current MARL methods, the proposed method uses a recurrent neural network
(RNN) that incorporates historical state-action pairs of each controlled UAV,
and an attention mechanism to analyze the importance of these historical
state-action pairs, thus improving the global Q function approximation accuracy
and the target UAV positioning accuracy. Simulation results show that the
proposed AR-MARL scheme can reduce the average positioning error by up to 17.5%
and 58.5% compared to the VD-MARL scheme and the proposed method without FAS.

</details>


### [47] [Proactive AI-and-RAN Workload Orchestration in O-RAN Architectures for 6G Networks](https://arxiv.org/abs/2507.09124)
*Syed Danial Ali Shah,Maryam Hafeez,Abdelaziz Salama,Syed Ali Raza Zaidi*

Main category: cs.NI

TL;DR: 论文提出了一种基于O-RAN规范的AI与RAN融合架构（CAORA），通过动态资源分配和智能调度，成功实现了5G网络环境下AI与RAN工作负载的高效共存。


<details>
  <summary>Details</summary>
Motivation: 解决AI-RAN融合在6G平台中的架构和资源调度问题，以实现高效的基础设施共享。

Method: 设计CAORA框架，利用NRT-RIC中的xApps监控RAN性能指标，通过Y1接口与E2E编排器交互，结合SAC强化学习算法进行资源分配。

Result: 在巴塞罗那的5G流量数据模拟中，CAORA实现了99%的RAN需求满足率，并支持动态AI工作负载。

Conclusion: CAORA框架为AI与RAN融合的6G系统提供了可行的蓝本，显著提升了系统适应性和资源效率。

Abstract: The vision of AI-RAN convergence, as advocated by the AI-RAN Alliance, aims
to unlock a unified 6G platform capable of seamlessly supporting AI and RAN
workloads over shared infrastructure. However, the architectural framework and
intelligent resource orchestration strategies necessary to realize this vision
remain largely unexplored. In this paper, we propose a Converged AI-and-ORAN
Architectural (CAORA) framework based on O-RAN specifications, enabling the
dynamic coexistence of real-time RAN and computationally intensive AI
workloads. We design custom xApps within the Near-Real-Time RAN Intelligent
Controller (NRT-RIC) to monitor RAN KPIs and expose radio analytics to an
End-to-End (E2E) orchestrator via the recently introduced Y1 interface. The
orchestrator incorporates workload forecasting and anomaly detection modules,
augmenting a Soft Actor-Critic (SAC) reinforcement learning agent that
proactively manages resource allocation, including Multi-Instance GPU (MIG)
partitioning. Using real-world 5G traffic traces from Barcelona, our
trace-driven simulations demonstrate that CAORA achieves near 99\% fulfillment
of RAN demands, supports dynamic AI workloads, and maximizes infrastructure
utilization even under highly dynamic conditions. Our results reveal that
predictive orchestration significantly improves system adaptability, resource
efficiency, and service continuity, offering a viable blueprint for future
AI-and-RAN converged 6G systems.

</details>


### [48] [On-Demand HAPS-Assisted Communication System for Public Safety in Emergency and Disaster Response](https://arxiv.org/abs/2507.09153)
*Bilal Karaman,Ilhan Baştürk,Ferdi Kara,Engin Zeydan,Esra Aycan Beyazıt,Sezai Taşkın,Emil Björnson,Halim Yanikomeroglu*

Main category: cs.NI

TL;DR: 论文提出了一种基于高空平台站（HAPS）的需求驱动通信系统，用于在自然灾害中快速恢复通信，提升应急响应能力。


<details>
  <summary>Details</summary>
Motivation: 自然灾害常导致通信网络中断，影响应急响应。现有解决方案在无线电接入网络（RAN）和回程基础设施同时失效时表现不足，亟需更可靠的通信恢复手段。

Method: 使用HAPS支持的混合光/太赫兹链路提升回程容量和韧性，并在S和Ka波段部署HAPS支持的RAN，确保通信可靠性。

Result: 模拟显示，HAPS方案在恶劣条件下仍能高效恢复通信，增强灾害管理中的实时信息交换和资源分配。

Conclusion: HAPS通信系统是一种快速部署、高韧性的解决方案，可显著提升灾害应对能力，值得纳入应急通信框架和标准。

Abstract: Natural disasters often disrupt communication networks and severely hamper
emergency response and disaster management. Existing solutions, such as
portable communication units and cloud-based network architectures, have
improved disaster resilience but fall short if both the Radio Access Network
(RAN) and backhaul infrastructure become inoperable. To address these
challenges, we propose a demand-driven communication system supported by High
Altitude Platform Stations (HAPS) to restore communication in an affected area
and enable effective disaster relief. The proposed emergency response network
is a promising solution as it provides a rapidly deployable, resilient
communications infrastructure. The proposed HAPS-based communication can play a
crucial role not only in ensuring connectivity for mobile users but also in
restoring backhaul connections when terrestrial networks fail. As a bridge
between the disaster management center and the affected areas, it can
facilitate the exchange of information in real time, collect data from the
affected regions, and relay crucial updates to emergency responders. Enhancing
situational awareness, coordination between relief agencies, and ensuring
efficient resource allocation can significantly strengthen disaster response
capabilities. In this paper, simulations show that HAPS with hybrid optical/THz
links boosts backhaul capacity and resilience, even in harsh conditions.
HAPS-enabled RAN in S- and Ka-bands ensures reliable communication for first
responders and disaster-affected populations. This paper also explores the
integration of HAPS into emergency communication frameworks and standards, as
it has the potential to improve network resilience and support effective
disaster management.

</details>


### [49] [Joint Traffic Reshaping and Channel Reconfiguration in RIS-assisted Semantic NOMA Communications](https://arxiv.org/abs/2507.09270)
*Songhan Zhao,Yusi Long,Lanhua Li,Bo Gu,Shimin Gong,Zehui Xiong*

Main category: cs.NI

TL;DR: 本文提出了一种语义感知的可重构智能表面（RIS）辅助无线网络，通过联合优化语义用户（SUs）的解码顺序、语义控制和RIS的被动波束成形策略，显著降低了系统的整体能耗。


<details>
  <summary>Details</summary>
Motivation: 研究目的是在满足每个SU的流量需求的同时，最小化系统的整体能耗，通过语义流量重塑和信道重构提供更高的灵活性。

Method: 采用非正交多址接入（NOMA）方法，将原问题分解为两个子问题，并利用近似方法求解。

Result: 数值结果表明，该方案在能耗节约方面显著优于基准方法。

Conclusion: 通过联合优化语义控制和RIS被动波束成形，可以有效提升NOMA传输的能效。

Abstract: In this paper, we consider a semantic-aware reconfigurable intelligent
surface (RIS)-assisted wireless network, where multiple semantic users (SUs)
simultaneously transmit semantic information to an access point (AP) by using
the non-orthogonal multiple access (NOMA) method. The SUs can reshape their
traffic demands by modifying the semantic extraction factor, while the RIS can
reconfigure the channel conditions via the passive beamforming. This provides
the AP with greater flexibility to decode the superimposed signals from the
SUs. We aim to minimize the system's overall energy consumption, while ensuring
that each SU's traffic demand is satisfied. Hence, we formulate a joint
optimization problem of the SUs' decoding order and semantic control, as well
as the RIS's passive beamforming strategy. This problem is intractable due to
the complicated coupling in constraints. To solve this, we decompose the
original problem into two subproblems and solve them by using a series of
approximate methods. Numerical results show that the joint traffic reshaping
and channel reconfiguration scheme significantly improves the energy saving
performance of the NOMA transmissions compared to the benchmark methods.

</details>


### [50] [Meeting Deadlines in Motion: Deep RL for Real-Time Task Offloading in Vehicular Edge Networks](https://arxiv.org/abs/2507.09341)
*Mahsa Paknejad,Parisa Fard Moshiri,Murat Simsek,Burak Kantarci,Hussein T. Mouftah*

Main category: cs.NI

TL;DR: 论文研究了车载移动边缘计算（VEC）的任务卸载问题，提出使用深度学习强化方法（如DQN和PPO）优化性能，显著降低延迟和任务丢弃率。


<details>
  <summary>Details</summary>
Motivation: VEC虽有潜力，但因其短暂覆盖范围和严格任务卸载时限面临挑战，需优化任务管理以提高效率。

Method: 研究中先在静态环境中用PSO建立理论极限，后在动态环境中测试PSO、DQN和PPO模型，以最小化任务丢弃和延迟。

Result: 实验表明，DQN模型在动态环境中优于PSO，执行时间减少99.2%，任务丢弃率降低2.5%，E2E延迟降低18.6%。

Conclusion: 深度学习强化方法（如DQN）在VEC系统中可显著提升任务管理的可扩展性和效率。

Abstract: Vehicular Mobile Edge Computing (VEC) drives the future by enabling
low-latency, high-efficiency data processing at the very edge of vehicular
networks. This drives innovation in key areas such as autonomous driving,
intelligent transportation systems, and real-time analytics. Despite its
potential, VEC faces significant challenges, particularly in adhering to strict
task offloading deadlines, as vehicles remain within the coverage area of
Roadside Units (RSUs) for only brief periods. To tackle this challenge, this
paper evaluates the performance boundaries of task processing by initially
establishing a theoretical limit using Particle Swarm Optimization (PSO) in a
static environment. To address more dynamic and practical scenarios, PSO, Deep
Q-Network (DQN), and Proximal Policy Optimization (PPO) models are implemented
in an online setting. The objective is to minimize dropped tasks and reduce
end-to-end (E2E) latency, covering both communication and computation delays.
Experimental results demonstrate that the DQN model considerably surpasses the
dynamic PSO approach, achieving a 99.2% reduction in execution time.
Furthermore, It leads to a reduction in dropped tasks by 2.5% relative to
dynamic PSO and achieves 18.6\% lower E2E latency, highlighting the
effectiveness of Deep Reinforcement Learning (DRL) in enabling scalable and
efficient task management for VEC systems.

</details>


### [51] [Fast and Adaptive Task Management in MEC: A Deep Learning Approach Using Pointer Networks](https://arxiv.org/abs/2507.09346)
*Arild Yonkeu,Mohammadreza Amini,Burak Kantarci*

Main category: cs.NI

TL;DR: 提出了一种基于指针网络的动态边缘计算任务调度方法，解决了传统方法和现有深度学习方法在实时性和适应性上的不足。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算（MEC）中任务卸载与调度的低延迟需求日益增长，但现有方法存在高计算开销或缺乏动态适应性。

Method: 使用遗传算法生成合成数据集训练指针网络模型，优化任务调度顺序。

Result: 模型在丢弃率和等待时间上优于基线方法，序列准确率达89.2%，推理时间低于2秒，远快于传统方法。

Conclusion: 所提方案在动态环境中表现出高效性和可扩展性，为边缘任务管理提供了实用解决方案。

Abstract: Task offloading and scheduling in Mobile Edge Computing (MEC) are vital for
meeting the low-latency demands of modern IoT and dynamic task scheduling
scenarios. MEC reduces the processing burden on resource-constrained devices by
enabling task execution at nearby edge servers. However, efficient task
scheduling remains a challenge in dynamic, time-sensitive environments.
Conventional methods -- such as heuristic algorithms and mixed-integer
programming -- suffer from high computational overhead, limiting their
real-time applicability. Existing deep learning (DL) approaches offer faster
inference but often lack scalability and adaptability to dynamic workloads. To
address these issues, we propose a Pointer Network-based architecture for task
scheduling in dynamic edge computing scenarios. Our model is trained on a
generated synthetic dataset using genetic algorithms to determine the optimal
task ordering. Experimental results show that our model achieves lower drop
ratios and waiting times than baseline methods, and a soft sequence accuracy of
up to 89.2%. Our model consistently achieves inference times under 2 seconds
across all evaluated task counts, whereas the integer and binary programming
approaches require approximately up to 18 seconds and 90 seconds, respectively.
It also shows strong generalization across varying scenarios, and adaptability
to real-time changes, offering a scalable and efficient solution for edge-based
task management.

</details>


### [52] [Reliable Task Offloading in MEC through Transmission Diversity and Jamming-Aware Scheduling](https://arxiv.org/abs/2507.09352)
*Ghazal Asemian,Mohammadreza Amini,Burak Kantarci*

Main category: cs.NI

TL;DR: 提出了一个支持传输多样性的动态MEC框架，有效解决任务调度和资源分配的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 动态任务到达和通信威胁（如干扰）使MEC中的任务卸载和资源分配复杂化。

Method: 提出了一个干扰感知的任务卸载和资源块分配框架，结合传输多样性和分布式gNBs的最优调度。

Result: 在4 dB的SJNR下，任务丢弃率为0.26，优于无传输多样性的0.50以及STF和FCFS的0.52和0.63。

Conclusion: 该算法显著减少干扰影响，提升资源利用率和任务完成率，适用于任务关键型MEC应用。

Abstract: Mobile Edge Computing (MEC) enables low-latency applications by bringing
computation closer to the user, but dynamic task arrivals and communication
threats like jamming complicate reliable task offloading and resource
allocation. In this paper, we formulate a dynamic MEC framework considering the
transmission diversity that jointly addresses task scheduling and resource
block (RB) assignment in the presence of jamming. First, we define and evaluate
key network metrics-including dropped task ratio and bandwidth
utilization-while maintaining service continuity by accounting for the existing
commitments of the edge server to previously offloaded tasks. Then, we propose
a jamming-aware offloading and RB allocation framework that leverages
transmission diversity and optimal scheduling across distributed gNBs. The
proposed solution is compared to a similar scenario without transmission
diversity and two baseline strategies of first-come-first-served (FCFS) and
shortest task first (STF). The proposed algorithm effectively mitigates the
impact of jamming while enhancing resource utilization and minimizing task drop
rates, making it highly suitable for mission-critical MEC applications. At
signal-to-jamming-and-noise ratio (SJNR) of 4 dB, the proposed method achieves
a $0.26$ task drop rate, outperforming the scenario without transmission
diversity with a task drop rate of 0.50 and STF and FCFS strategies with 0.52
and 0.63 task drop rates, respectively.

</details>


### [53] [MobiWorld: World Models for Mobile Wireless Network](https://arxiv.org/abs/2507.09462)
*Haoye Chai,Yuan Yuan,Yong Li*

Main category: cs.NI

TL;DR: MobiWorld是一种生成世界模型，通过整合异构数据源和多模态数据类型，支持移动网络规划和优化的高保真环境模拟，展现出强大的可控生成能力，并在能源优化中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 移动网络的准确建模和仿真对于实现智能且成本效益高的网络优化至关重要，但传统预测模型因泛化能力有限而受到约束。

Method: MobiWorld基于先进的扩散模型，通过建模移动网络数据与时空上下文、用户行为和优化策略等条件因素的联合分布，实现了强大的可控生成能力。

Result: 实验结果表明，MobiWorld在可控生成性能上表现优异，并在协作节能场景中优于传统方法。

Conclusion: MobiWorld为移动网络规划和优化提供了高保真、灵活的环境模拟工具，能够在不依赖昂贵真实网络交互的情况下支持有效的决策制定。

Abstract: Accurate modeling and simulation of mobile networks are essential for
enabling intelligent and cost-effective network optimization. In this paper, we
propose MobiWorld, a generative world model designed to support high-fidelity
and flexible environment simulation for mobile network planning and
optimization. Unlike traditional predictive models constrained by limited
generalization capabilities, MobiWorld exhibits strong universality by
integrating heterogeneous data sources, including sensors, mobile devices, and
base stations, as well as multimodal data types such as sequences and images.
It is capable of generating both network element-level observations (e.g.,
traffic load, user distribution) and system-level performance indicators (e.g.,
throughput, energy consumption) to support a wide range of planning and
optimization tasks. Built upon advanced diffusion models, MobiWorld offers
powerful controllable generation capabilities by modeling the joint
distribution between mobile network data and diverse conditional factors
including spatio temporal contexts, user behaviors, and optimization policies.
This enables accurate simulation of dynamic network states under varying policy
configurations, providing optimization agents with precise environmental
feedback and facilitating effective decision-making without relying on costly
real-network interactions. We demonstrate the effectiveness of MobiWorld in a
collaborative energy-saving scenario, where an agent uses observations and
rewards generated by MobiWorld to optimize base station sleep and user
offloading policies. Experimental results show that MobiWorld exhibits strong
controllable generation performance and outperforms traditional methods in
energy optimization.

</details>


### [54] [Wi-Fi: Twenty-Five Years and Counting](https://arxiv.org/abs/2507.09613)
*Giovanni Geraci,Francesca Meneghello,Francesc Wilhelmi,David Lopez-Perez,Iñaki Val,Lorenzo Galati Giordano,Carlos Cordeiro,Monisha Ghosh,Edward Knightly,Boris Bellalta*

Main category: cs.NI

TL;DR: 本文提供了Wi-Fi从1代到8代的全方位技术历史教程，重点介绍了频谱分配、物理层升级、多用户访问等关键机制的演进。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi技术在过去25年经历了巨大变革，本文旨在填补从Wi-Fi 1到Wi-Fi 8的全面技术教程空白。

Method: 文章通过分模块讨论频谱分配、物理层技术、多用户访问等核心机制，而非逐代叙述，全面分析Wi-Fi的演进。

Result: Wi-Fi的数据速率提升超1000倍，支持多用户访问、节能优化等，未来还将集成毫米波和AI技术。

Conclusion: Wi-Fi的持续创新使其在性能、效率和功能上大幅提升，未来技术扩展将进一步增强其能力。

Abstract: Today, Wi-Fi is over 25 years old. Yet, despite sharing the same branding
name, today's Wi-Fi boasts entirely new capabilities that were not even on the
roadmap 25 years ago. This article aims to provide a holistic and comprehensive
technical and historical tutorial on Wi-Fi, beginning with IEEE 802.11b (Wi-Fi
1) and looking forward to IEEE 802.11bn (Wi-Fi 8). This is the first tutorial
article to span these eight generations. Rather than a generation-by-generation
exposition, we describe the key mechanisms that have advanced Wi-Fi. We begin
by discussing spectrum allocation and coexistence, and detailing the IEEE
802.11 standardization cycle. Second, we provide an overview of the physical
layer and describe key elements that have enabled data rates to increase by
over 1,000x. Third, we describe how Wi-Fi Medium Access Control has been
enhanced from the original Distributed Coordination Function to now include
capabilities spanning from frame aggregation to wideband spectrum access.
Fourth, we describe how Wi-Fi 5 first broke the one-user-at-a-time paradigm and
introduced multi-user access. Fifth, given the increasing use of mobile,
battery-powered devices, we describe Wi-Fi's energy-saving mechanisms over the
generations. Sixth, we discuss how Wi-Fi was enhanced to seamlessly aggregate
spectrum across 2.4 GHz, 5 GHz, and 6 GHz bands to improve throughput,
reliability, and latency. Finally, we describe how Wi-Fi enables nearby Access
Points to coordinate in order to improve performance and efficiency. In the
Appendix, we further discuss Wi-Fi developments beyond 802.11bn, including
integrated mmWave operations, sensing, security and privacy extensions, and the
adoption of AI/ML.

</details>


### [55] [Towards Robust RTC in Sparse LEO Constellations](https://arxiv.org/abs/2507.09798)
*Aashish Gottipati,Lili Qiu*

Main category: cs.NI

TL;DR: 论文研究了稀疏LEO星座中视频会议系统的表现，提出了一种基于数据驱动的队列管理机制，显著提升了视频比特率并降低了冻结率。


<details>
  <summary>Details</summary>
Motivation: Google的拥塞控制在LEO网络中表现脆弱，稀疏LEO星座为实时通信提供了独特机会，但现有技术未能充分利用其优势。

Method: 引入数据驱动的队列管理机制，根据预测的手切换活动动态调整发送端队列容量。

Result: 相比默认WebRTC，视频比特率提升3倍，冻结率降低62%。

Conclusion: 该机制有效提升了稀疏LEO网络中的实时通信性能。

Abstract: Google's congestion control (GCC) has become a cornerstone for real-time
video and audio communication, yet its performance remains fragile in emerging
Low Earth Orbit (LEO) networks. Sparse direct-to-device constellations offer
longer duration links and reduced handover frequency compared to dense
deployments, presenting a unique opportunity for high-quality real-time
communication (RTC) in environments with limited terrestrial network
infrastructure. In this paper, we study the behavior of videoconferencing
systems in sparse LEO constellations. We observe that video quality degrades
due to inherent delays and network instability introduced by the high altitude
and rapid movement of LEO satellites, with these effects exacerbated by
WebRTC's conventional ``one-size-fits-all'' sender-side pacing queue
management. To boost RTC performance, we introduce a data-driven queue
management mechanism that adapts the maximum pacing queue capacity based on
predicted handover activity. Specifically, our approach employs shorter queue
limits during stable, no-handover phases to prioritize low latency
communication, and preemptively increases pacing queue capacity when entering
periods of increased handover activity to absorb disruptions. Our method yields
up to $3$x improvements in video bitrate and reduces freeze rate by $62\%$
compared to default WebRTC.

</details>


### [56] [UavNetSim-v1: A Python-based Simulation Platform for UAV Communication Networks](https://arxiv.org/abs/2507.09852)
*Zihao Zhou,Zipeng Dai,Linyi Huang,Cui Yang,Youjun Xiang,Jie Tang,Kai-kit Wong*

Main category: cs.NI

TL;DR: UavNetSim-v1是一个基于Python的开源仿真平台，用于快速开发和评估无人机网络中的协议和算法。


<details>
  <summary>Details</summary>
Motivation: 无人机网络中的通信协议和算法需要高效开发工具，仿真可以节省高昂的实地实验成本。

Method: 开发了UavNetSim-v1平台，支持路由/MAC协议、拓扑控制算法、移动性/能量模型，并提供交互式可视化界面。

Result: 平台功能全面，适用于快速原型设计和教育目的，可作为成熟的网络仿真工具的轻量替代。

Conclusion: UavNetSim-v1为无人机通信研究提供了一个高效、易用的仿真解决方案。

Abstract: In unmanned aerial vehicle (UAV) networks, communication protocols and
algorithms are essential for cooperation and collaboration between UAVs.
Simulation provides a cost-effective solution for prototyping, debugging, and
analyzing protocols and algorithms, avoiding the prohibitive expenses of field
experiments. In this paper, we present ``UavNetSim-v1'', an open-source
Python-based simulation platform designed for rapid development, testing, and
evaluating the protocols and algorithms in UAV networks. ``UavNetSim-v1''
provides most of the functionalities developers may need, including
routing/medium access control (MAC) protocols, topology control algorithms and
mobility/energy models, while maintaining ease of use. Furthermore, the
platform supports comprehensive performance evaluation and features an
interactive visualization interface for in-depth algorithm analysis. In short,
``UavNetSim-v1'' lends itself to both rapid prototyping and educational
purposes, and can serve as a lightweight yet powerful alternative to mature
network simulators for UAV communication research.

</details>


### [57] [Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference](https://arxiv.org/abs/2507.09942)
*Jiaming Cheng,Duong Tung Nguyen*

Main category: cs.NI

TL;DR: 研究如何在异构边缘数据中心优化分配LLM推理任务，以降低能耗、碳排放及水资源使用，同时提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理任务在动态电价和可再生能源可用性的时空变化下的优化分配问题。

Method: 提出了一种新的优化模型，考虑了运营成本和环境影响的最小化。

Result: 数值结果验证了所提方法的有效性。

Conclusion: 该优化模型能帮助LLM服务提供商减少运营成本和环境影响。

Abstract: This letter investigates the optimal allocation of large language model (LLM)
inference workloads across heterogeneous edge data centers (DCs) over time.
Each DC features on-site renewable generation and faces dynamic electricity
prices and spatiotemporal variability in renewable availability. The central
question is: how can inference workloads be optimally distributed to the DCs to
minimize energy consumption, carbon emissions, and water usage while enhancing
user experience? This letter proposes a novel optimization model for LLM
service providers to reduce operational costs and environmental impacts.
Numerical results validate the efficacy of the proposed approach.

</details>


### [58] [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](https://arxiv.org/abs/2507.10510)
*Jiangkai Wu,Zhiyuan Ren,Liming Liu,Xinggong Zhang*

Main category: cs.NI

TL;DR: 提出Artic框架，通过上下文感知视频流和自适应帧率降低延迟，提高AI视频聊天体验。


<details>
  <summary>Details</summary>
Motivation: 解决AI视频聊天中MLLM推理和网络延迟导致的高延迟问题，使AI交互更接近真人。

Method: 提出Context-Aware Video Streaming和Loss-Resilient Adaptive Frame Rate技术，优化视频流处理。

Result: 构建DeViBench基准测试，验证视频流质量对MLLM准确性的影响。

Conclusion: Artic框架有效降低延迟，为AI视频聊天提供新方向。

Abstract: AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),
where one peer is not a human, but a Multimodal Large Language Model (MLLM).
This makes interaction between humans and AI more intuitive, as if chatting
face-to-face with a real person. However, this poses significant challenges to
latency, because the MLLM inference takes up most of the response time, leaving
very little time for video streaming. Due to network uncertainty and
instability, transmission latency becomes a critical bottleneck preventing AI
from being like a real person. To address this, we propose Artic, an
AI-oriented Real-time Communication framework, exploring the network
requirement shift from "humans watching video" to "AI understanding video". To
reduce bitrate dramatically while maintaining MLLM accuracy, we propose
Context-Aware Video Streaming that recognizes the importance of each video
region for chat and allocates bitrate almost exclusively to chat-important
regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive
Frame Rate that leverages previous frames to substitute for lost/delayed frames
while avoiding bitrate waste. To evaluate the impact of video streaming quality
on MLLM accuracy, we build the first benchmark, named Degraded Video
Understanding Benchmark (DeViBench). Finally, we discuss some open questions
and ongoing solutions for AI Video Chat.

</details>


### [59] [Fine-Grained Coordinated OFDMA With Fiber Backhaul Enabled by openwifi and White Rabbit](https://arxiv.org/abs/2507.10210)
*Thijs Havinga,Xianjun Jiao,Wei Liu,Baiheng Chen,Robbe Gaeremynck,Ingrid Moerman*

Main category: cs.NI

TL;DR: 论文提出了基于有线背板的细粒度Co-OFDMA技术，解决了Wi-Fi 8在多AP密集部署中的资源竞争和调度问题，并通过实验验证了其性能优越性。


<details>
  <summary>Details</summary>
Motivation: 密集无线网络中，竞争性系统在多设备竞争资源时延迟问题严重，Co-OFDMA被提出用于更高效的频谱共享，但细粒度资源分配存在实际困难。

Method: 使用光纤背板实现Wi-Fi 6兼容的细粒度Co-OFDMA，利用开源平台openwifi和White Rabbit进行实验验证，解决了空中调度和同步的复杂性问题。

Result: 实验显示，两个AP之间的载波频率偏移预补偿和时间同步性能优于相关无线标准要求，Co-OFDMA帧的接收质量优于AP单独发送的帧。

Conclusion: 基于有线背板的Co-OFDMA技术在多AP协调中表现出优越性能，为未来高级多AP协调方案奠定了基础。

Abstract: Proper coordination is needed to guarantee the performance of wireless
networks in dense deployments. Contention-based systems suffer badly in terms
of latency when multiple devices compete for the same resources. Coordinated
Orthogonal Frequency Division Multiple Access (Co-OFDMA) is proposed for Wi-Fi
8 to remedy this, as it enables multiple Access Points (APs) to share spectrum
more efficiently. However, fine-grained resource allocation, namely within
20MHz bandwidth, is argued to be impractical due to the over-the-air scheduling
overhead and complexity in terms of physical layer signaling. A wired backhaul
mitigates the need for over-the-air scheduling and synchronization, and it
allows for coordination even if APs are not in each others' range. Furthermore,
it forms the basis for more advanced multi-AP coordination schemes like
coordinated beamforming and joint transmission. In this work we demonstrate the
realization of Wi-Fi 6 compliant fine-grained Co-OFDMA using a fiber backhaul,
enabled by the open-source platforms openwifi and White Rabbit. We show that
the performance in terms of carrier frequency offset pre-compensation and time
synchronization between two APs exceeds related wireless standard requirements.
Furthermore, the quality of the received constellation of the Co-OFDMA frame as
reported by a wireless connectivity tester is better than individual frames
sent by the APs.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [60] [KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal Fake News Detection](https://arxiv.org/abs/2507.09647)
*Peican Zhu,Yubo Jing,Le Cheng,Keke Tang,Yangming Guo*

Main category: cs.MM

TL;DR: 提出了一种名为KEN的新网络，通过知识增强和情感引导，结合LVLM的语义理解和世界知识，优化多模态假新闻检测，并在实验中表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上虚假信息泛滥，但现有研究未能充分理解图像语义，且缺乏对情感类型的针对性处理，导致检测性能不足。

Method: 利用LVLM的语义理解和世界知识，生成图像描述并检索文本证据；通过平衡学习考虑不同情感类型的类间差异，实现细粒度建模。

Result: 在两个真实数据集上的实验验证了KEN的优越性。

Conclusion: KEN通过结合知识增强和情感引导，显著提升了多模态假新闻检测的性能。

Abstract: In recent years, the rampant spread of misinformation on social media has
made accurate detection of multimodal fake news a critical research focus.
However, previous research has not adequately understood the semantics of
images, and models struggle to discern news authenticity with limited textual
information. Meanwhile, treating all emotional types of news uniformly without
tailored approaches further leads to performance degradation. Therefore, we
propose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On
the one hand, we effectively leverage LVLM's powerful semantic understanding
and extensive world knowledge. For images, the generated captions provide a
comprehensive understanding of image content and scenes, while for text, the
retrieved evidence helps break the information silos caused by the closed and
limited text and context. On the other hand, we consider inter-class
differences between different emotional types of news through balanced
learning, achieving fine-grained modeling of the relationship between emotional
types and authenticity. Extensive experiments on two real-world datasets
demonstrate the superiority of our KEN.

</details>


### [61] [ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual Event Localization](https://arxiv.org/abs/2507.09945)
*Huilai Li,Yonghao Dang,Ying Xing,Yiming Wang,Jianqin Yin*

Main category: cs.MM

TL;DR: 文章提出了一种名为ESG-Net的密集视听事件定位方法，通过多阶段语义引导和多事件关系建模，解决了模态语义鸿沟和并发事件推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏中间层的跨模态语义融合，且未考虑事件间的相关性，导致难以区分事件相关内容和背景内容。

Method: 提出ESG-Net，包含ESI模块（多阶段语义引导）和MoDE模块（自适应事件依赖性提取），分别实现分层语义理解与事件依赖建模。

Result: 实验表明，该方法显著优于现有技术，同时大幅减少参数和计算负担。

Conclusion: ESG-Net通过多阶段语义引导和事件依赖建模，有效提升了密集视听事件定位的性能和效率。

Abstract: Dense audio-visual event localization (DAVE) aims to identify event
categories and locate the temporal boundaries in untrimmed videos. Most studies
only employ event-related semantic constraints on the final outputs, lacking
cross-modal semantic bridging in intermediate layers. This causes modality
semantic gap for further fusion, making it difficult to distinguish between
event-related content and irrelevant background content. Moreover, they rarely
consider the correlations between events, which limits the model to infer
concurrent events among complex scenarios. In this paper, we incorporate
multi-stage semantic guidance and multi-event relationship modeling, which
respectively enable hierarchical semantic understanding of audio-visual events
and adaptive extraction of event dependencies, thereby better focusing on
event-related information. Specifically, our eventaware semantic guided network
(ESG-Net) includes a early semantics interaction (ESI) module and a mixture of
dependency experts (MoDE) module. ESI applys multi-stage semantic guidance to
explicitly constrain the model in learning semantic information through
multi-modal early fusion and several classification loss functions, ensuring
hierarchical understanding of event-related content. MoDE promotes the
extraction of multi-event dependencies through multiple serial mixture of
experts with adaptive weight allocation. Extensive experiments demonstrate that
our method significantly surpasses the state-of-the-art methods, while greatly
reducing parameters and computational load. Our code will be released on
https://github.com/uchiha99999/ESG-Net.

</details>


### [62] [LayLens: Improving Deepfake Understanding through Simplified Explanations](https://arxiv.org/abs/2507.10066)
*Abhijeet Narang,Parul Gupta,Liuyijia Su,Abhinav Dhall*

Main category: cs.MM

TL;DR: LayLens是一个工具，旨在帮助不同教育背景的用户更容易理解深度伪造技术。它通过三个阶段简化技术解释，提高用户对深度伪造的理解和识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测工具多使用技术性术语，导致普通用户难以理解。LayLens旨在通过简化解释和可视化比较，降低认知负荷，提升用户对深度伪造的识别信心。

Method: LayLens采用三阶段流程：(1)可解释的深度伪造检测，(2)自然语言简化技术解释，(3)通过图像编辑重建原始图像。

Result: 用户研究表明，简化解释显著提高了清晰度，降低了认知负荷，多数用户表示对识别深度伪造更有信心。

Conclusion: LayLens为透明、可信赖且以用户为中心的深度伪造取证迈出了重要一步。

Abstract: This demonstration paper presents $\mathbf{LayLens}$, a tool aimed to make
deepfake understanding easier for users of all educational backgrounds. While
prior works often rely on outputs containing technical jargon, LayLens bridges
the gap between model reasoning and human understanding through a three-stage
pipeline: (1) explainable deepfake detection using a state-of-the-art forgery
localization model, (2) natural language simplification of technical
explanations using a vision-language model, and (3) visual reconstruction of a
plausible original image via guided image editing. The interface presents both
technical and layperson-friendly explanations in addition to a side-by-side
comparison of the uploaded and reconstructed images. A user study with 15
participants shows that simplified explanations significantly improve clarity
and reduce cognitive load, with most users expressing increased confidence in
identifying deepfakes. LayLens offers a step toward transparent, trustworthy,
and user-centric deepfake forensics.

</details>


### [63] [DualDub: Video-to-Soundtrack Generation via Joint Speech and Background Audio Synthesis](https://arxiv.org/abs/2507.10109)
*Wenjie Tian,Xinfa Zhu,Haohe Liu,Zhixian Zhao,Zihao Chen,Chaofan Ding,Xinhan Di,Junjie Zheng,Lei Xie*

Main category: cs.MM

TL;DR: DualDub框架提出视频到配乐（V2ST）任务，通过多模态语言模型联合生成语音和背景音频，采用跨模态对齐和课程学习策略，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频（V2A）模型忽视语音部分，而语音是视频配乐的关键组成，因此提出统一框架解决这一问题。

Method: DualDub整合多模态编码器、跨模态对齐器及双解码头，采用因果与非因果注意力机制提升同步性，并设计课程学习策略应对数据稀缺。

Result: DualDub在首个V2ST基准DualBench上表现优异，生成高质量且同步的语音与背景音频配乐。

Conclusion: DualDub通过统一框架和先进对齐机制，实现了视频配乐的全面生成，填补了现有研究的空白。

Abstract: While recent video-to-audio (V2A) models can generate realistic background
audio from visual input, they largely overlook speech, an essential part of
many video soundtracks. This paper proposes a new task, video-to-soundtrack
(V2ST) generation, which aims to jointly produce synchronized background audio
and speech within a unified framework. To tackle V2ST, we introduce DualDub, a
unified framework built on a multimodal language model that integrates a
multimodal encoder, a cross-modal aligner, and dual decoding heads for
simultaneous background audio and speech generation. Specifically, our proposed
cross-modal aligner employs causal and non-causal attention mechanisms to
improve synchronization and acoustic harmony. Besides, to handle data scarcity,
we design a curriculum learning strategy that progressively builds the
multimodal capability. Finally, we introduce DualBench, the first benchmark for
V2ST evaluation with a carefully curated test set and comprehensive metrics.
Experimental results demonstrate that DualDub achieves state-of-the-art
performance, generating high-quality and well-synchronized soundtracks with
both speech and background audio.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [64] [Computability of Equivariant Gröbner bases](https://arxiv.org/abs/2507.08990)
*Arka Ghosh,Aliaume Lopez*

Main category: cs.LO

TL;DR: 该论文研究了在无限集上的多项式环中的等价理想，证明了在满足Hilbert基性质的情况下，可以计算Gröbner基并判定理想成员资格，同时给出了成员资格问题不可判定的充分条件。


<details>
  <summary>Details</summary>
Motivation: 研究无限集上的多项式环中的等价理想及其计算性质，尤其是在群作用下的理想成员问题的可判定性。

Method: 通过群作用和Hilbert基性质，研究Gröbner基的计算方法，并分析理想成员问题的可判定性条件。

Result: 在满足Hilbert基性质的情况下，可以计算Gröbner基并判定理想成员资格；同时给出了成员资格问题不可判定的条件。

Conclusion: 论文为无限集上的等价理想的可计算性提供了理论支持，并揭示了不可判定性的条件，扩展了对理想成员问题的理解。

Abstract: Let $\mathbb{K}$ be a field, $\mathcal{X}$ be an infinite set (of
indeterminates), and $\mathcal{G}$ be a group acting on $\mathcal{X}$. An ideal
in the polynomial ring $\mathbb{K}[\mathcal{X}]$ is called equivariant if it is
invariant under the action of $\mathcal{G}$. We show Gr\"obner bases for
equivariant ideals are computable are hence the equivariant ideal membership is
decidable when $\mathcal{G}$ and $\mathcal{X}$ satisfies the Hilbert's basis
property, that is, when every equivariant ideal in $\mathbb{K}[\mathcal{X}]$ is
finitely generated. Moreover, we give a sufficient condition for the
undecidability of the equivariant ideal membership problem. This condition is
satisfied by the most common examples not satisfying the Hilbert's basis
property.

</details>


### [65] [A Simple and Effective ASP-Based Tool for Enumerating Minimal Hitting Sets](https://arxiv.org/abs/2507.09194)
*Mohimenul Kabir,Kuldeep S Meel*

Main category: cs.LO

TL;DR: 提出基于答案集编程（ASP）的工具MinHit-ASP，用于高效枚举给定集族的所有最小击中集。


<details>
  <summary>Details</summary>
Motivation: 最小击中集的枚举在计算机科学和数学中有重要应用，现有方法需要更高效的工具支持。

Method: 使用答案集编程（ASP）建模问题，并利用现有ASP求解器实现高效枚举，开发工具MinHit-ASP。

Result: 实验表明，MinHit-ASP能够在多种问题领域的基准测试中高效枚举最小击中集。

Conclusion: MinHit-ASP是一种有效且通用的最小击中集枚举工具。

Abstract: The hitting set problem is a fundamental problem in computer science and
mathematics. Given a family of sets over a universe of elements, a minimal
hitting set is a subset-minimal collection of elements that intersects each set
in the family. Enumerating all minimal hitting sets is crucial in various
real-world applications.
  In this paper, we address the full enumeration of all minimal hitting sets
for a given family of sets. We formulate the problem using Answer Set
Programming (ASP) and leverage existing ASP solvers for efficient enumeration.
We propose an ASP-based tool, MinHit-ASP, and our empirical evaluation shows
that it effectively enumerates minimal hitting sets across benchmarks from
diverse problem domains.

</details>


### [66] [Recovering Commutation of Logically Constrained Rewriting and Equivalence Transformations (Full Version)](https://arxiv.org/abs/2507.09326)
*Kanta Takahata,Jonas Schöpf,Naoki Nishida,Takahito Aoto*

Main category: cs.LO

TL;DR: 提出了一种新型的“最一般约束重写”方法，用于解决逻辑约束项重写系统中的搜索空间过大问题，并证明了该方法的关键性质。


<details>
  <summary>Details</summary>
Motivation: 逻辑约束项重写系统（LCTRSs）在重写约束项时，重写规则应用和等价变换紧密结合，导致理论性质难以建立且实现中搜索空间过大。

Method: 引入“最一般约束重写”概念，基于左线性、左值无关的LCTRSs，证明其关键性质（重写与等价变换可交换）。

Result: 新方法能够将等价变换推迟到重写规则应用后，有效减少实现中的搜索空间，并与原重写形式保持一致性。

Conclusion: 该研究为LCTRSs工具的正确高效实现提供了重要理论支持。

Abstract: Logically constrained term rewriting is a relatively new rewriting formalism
that naturally supports built-in data structures, such as integers and bit
vectors. In the analysis of logically constrained term rewrite systems
(LCTRSs), rewriting constrained terms plays a crucial role. However, this
combines rewrite rule applications and equivalence transformations in a closely
intertwined way. This intertwining makes it difficult to establish useful
theoretical properties for this kind of rewriting and causes problems in
implementations -- namely, that impractically large search spaces are often
required. To address this issue, we propose in this paper a novel notion of
most general constrained rewriting, which operates on existentially constrained
terms, a concept recently introduced by the authors. We define a class of
left-linear, left-value-free LCTRSs that are general enough to simulate all
left-linear LCTRSs and exhibit the desired key property: most general
constrained rewriting commutes with equivalence. This property ensures that
equivalence transformations can be deferred until after the application of
rewrite rules, which helps mitigate the issue of large search spaces in
implementations. In addition to that, we show that the original rewriting
formalism on constrained terms can be embedded into our new rewriting formalism
on existentially constrained terms. Thus, our results are expected to have
significant implications for achieving correct and efficient implementations in
tools operating on LCTRSs.

</details>


### [67] [Non-Termination of Logic Programs Using Patterns](https://arxiv.org/abs/2507.09390)
*Etienne Payet*

Main category: cs.LO

TL;DR: 本文介绍了一种从规则模式中自动检测非循环非终止性的方法，并将其应用于逻辑编程，提出了一种新的展开技术。


<details>
  <summary>Details</summary>
Motivation: 旨在解决逻辑编程中非终止性的自动检测问题，扩展了术语重写中的方法。

Method: 采用了新的展开技术，生成描述可能无限有限重写序列集的模式。

Result: 在工具NTI中实现并进行了实验评估，验证了方法的有效性。

Conclusion: 该方法成功地将术语重写中的技术适配到逻辑编程，为检测非终止性提供了新思路。

Abstract: In this paper, we consider an approach introduced in term rewriting for the
automatic detection of non-looping non-termination from patterns of rules. We
adapt it to logic programming by defining a new unfolding technique that
produces patterns describing possibly infinite sets of finite rewrite
sequences. We present an experimental evaluation of our contributions that we
implemented in our tool NTI.

</details>


### [68] [Justification Logic for Intuitionistic Modal Logic (Extended Technical Report)](https://arxiv.org/abs/2507.09427)
*Sonia Marin,Paaras Padhiar*

Main category: cs.LO

TL;DR: 本文为直觉主义模态逻辑IK及其扩展提供了正当性逻辑的对应，扩展了证明项语法并引入了无割嵌套序列系统。


<details>
  <summary>Details</summary>
Motivation: 探索直觉主义模态逻辑IK及其扩展的正当性逻辑对应，填补现有研究的空白。

Method: 扩展证明项语法以适应直觉主义模态逻辑的公理，提出正当性逻辑的公理化，并利用Straßburger的无割嵌套序列系统进行实现。

Result: 成功为IK及其扩展提供了正当性逻辑的公理化和实现方法。

Conclusion: 本文扩展了正当性逻辑的应用范围，为直觉主义模态逻辑提供了理论基础和实现工具。

Abstract: Justification logics are an explication of modal logic; boxes are replaced
with proof terms formally through realisation theorems. This can be achieved
syntactically using a cut-free proof system e.g. using sequent, hypersequent or
nested sequent calculi. In constructive modal logic, boxes and diamonds are
decoupled and not De Morgan dual. Kuznets, Marin and Stra{\ss}burger provide a
justification counterpart to constructive modal logic CK and some extensions by
making diamonds explicit by introducing new terms called satisfiers. We
continue the line of work to provide a justification counterpart to Fischer
Servi's intuitionistic modal logic IK and its extensions with the t and 4
axioms. We: extend the syntax of proof terms to accommodate the additional
axioms of intuitionistic modal logic; provide an axiomatisation of these
justification logics; provide a syntactic realisation procedure using a
cut-free nested sequent system for intuitionistic modal logic introduced by
Stra{\ss}burger.

</details>


### [69] [A Study Of Sudoku Solving Algorithms: Backtracking and Heuristic](https://arxiv.org/abs/2507.09708)
*Apekshya Bhattarai,Dinisha Uprety,Pooja Pathak,Safal Narshing Shrestha,Salina Narkarmi,Sanjog Sigdel*

Main category: cs.LO

TL;DR: 论文比较了递归回溯和启发式约束传播方法在数独求解中的表现，发现启发式方法在速度和效率上均优于回溯法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较不同数独求解策略的性能，特别是递归回溯和启发式方法在不同难度下的表现。

Method: 使用包含500个数独谜题的数据集，分为五个难度级别，通过平均求解时间评估两种方法的性能。

Result: 启发式方法在所有难度级别均表现更优，速度提升从入门级的1.27倍到专家级的2.91倍。

Conclusion: 启发式策略在复杂数独求解中具有显著优势，尤其是在高难度谜题中表现更高效。

Abstract: This paper presents a comparative analysis of Sudoku-solving strategies,
focusing on recursive backtracking and a heuristic-based constraint propagation
method. Using a dataset of 500 puzzles across five difficulty levels (Beginner
to Expert), we evaluated performance based on average solving time. The
heuristic approach consistently outperformed backtracking, achieving speedup
ratios ranging from 1.27x in Beginner puzzles to 2.91x in Expert puzzles. These
findings underscore the effectiveness of heuristic strategies, particularly in
tackling complex puzzles across varying difficulty levels.

</details>


### [70] [Extending Defeasibility for Propositional Standpoint Logics](https://arxiv.org/abs/2507.10133)
*Nicholas Leisegang,Thomas Meyer,Ivan Varzinczak*

Main category: cs.LO

TL;DR: 论文提出了一种新的可废止命题立场逻辑，结合了多种理论框架，提供了语义和推理方法，并证明了其计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 结合多种可废止逻辑理论，扩展立场逻辑的表达能力，使其能够处理可废止性的多层次问题。

Method: 整合Kraus等人的可废止条件、Britz和Varzinczak的可废止必要性及独特可能性概念，以及Leisegang等人的方法，构建新的逻辑框架，并提供优先语义和表推演算法。

Result: 证明了表推演算法在优先蕴含下的可靠性和完备性，且计算复杂性为PSpace。

Conclusion: 新框架成功扩展了立场逻辑的处理能力，为多层可废止性问题提供了有效的解决方案。

Abstract: In this paper, we introduce a new defeasible version of propositional
standpoint logic by integrating Kraus et al.'s defeasible conditionals, Britz
and Varzinczak's notions of defeasible necessity and distinct possibility,
along with Leisegang et al.'s approach to defeasibility into the standpoint
logics of G\'omez \'Alvarez and Rudolph. The resulting logical framework allows
for the expression of defeasibility on the level of implications, standpoint
modal operators, and standpoint-sharpening statements. We provide a
preferential semantics for this extended language and propose a tableaux
calculus, which is shown to be sound and complete with respect to preferential
entailment. We also establish the computational complexity of the tableaux
procedure to be in PSpace.

</details>


### [71] [A simple formalization of alpha-equivalence](https://arxiv.org/abs/2507.10181)
*Kalmer Apinis,Danel Ahman*

Main category: cs.LO

TL;DR: 论文探讨了直接归纳定义α-等价性在λ演算教学中的可行性，并提供了一个实际可用的定义，通过Rocq Prover实现了形式化验证。


<details>
  <summary>Details</summary>
Motivation: 在教学无类型λ演算时，作者对α-等价性未能直接归纳定义感到困惑，因此试图探索其可行性。

Method: 作者提出了一种基于归纳的α-等价性定义，并通过Rocq Prover进行了形式化验证。

Result: 研究表明，这种归纳定义与文献中的规范一致，验证了其正确性。

Conclusion: 论文证明α-等价性可以直接归纳定义，并通过形式化工具实现了验证。

Abstract: While teaching untyped $\lambda$-calculus to undergraduate students, we were
wondering why $\alpha$-equivalence is not directly inductively defined. In this
paper, we demonstrate that this is indeed feasible. Specifically, we provide a
grounded, inductive definition for $\alpha$-equivalence and show that it
conforms to the specification provided in the literature. The work presented in
this paper is fully formalized in the Rocq Prover.

</details>


### [72] [A Quantum Programming Language for Coherent Control](https://arxiv.org/abs/2507.10466)
*Kathlee Barsse,Romain Péchoux,Simon Perdrix*

Main category: cs.LO

TL;DR: 提出一种编程语言以一致控制任意量子操作，解决递归或迭代中量子条件控制的难题，通过Kraus分解和真空扩展语义实现通用性，并验证其充分性与完全抽象性。


<details>
  <summary>Details</summary>
Motivation: 解决量子操作中的一致控制问题，尤其是在递归或迭代条件下量子条件控制的挑战。

Method: 定义基于Kraus分解的操作语义和基于真空扩展的指称语义，验证语言的通用性和语义的充分性。

Result: 证明语言对真空扩展具有通用性，操作与指称语义充分且指称语义对观察等价完全抽象。

Conclusion: 提出了一种解决量子控制问题的新语言，验证了其语义的充分性和完全抽象性，为量子编程提供了理论基础。

Abstract: We introduce a programming language that allows for the coherent control of
arbitrary quantum operations. The problem of defining coherent control beyond
the unitary case, using, for example, a quantum conditional in the presence of
recursion or iteration has long been known to be a major difficulty. We resolve
this problem by defining an operational semantics based on appropriate Kraus
decompositions and a denotational semantics based on vacuum-extensions. We show
that the language is universal for vacuum-extensions and that the two semantics
are adequate. Moreover, we define a notion of observational equivalence: two
programs are equivalent if their probability of termination is the same in any
context. The denotational semantics is shown to be fully abstract for
observational equivalence.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [73] [Cognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. Harnessing Discomfort to Sharpen Critical Thinking](https://arxiv.org/abs/2507.08804)
*Delia Deliu*

Main category: cs.HC

TL;DR: 这篇论文提出了一种名为Cognitive Dissonance AI（CD-AI）的新框架，旨在通过维持不确定性而非消除它来促进主动认知斗争，增强反思性推理和批判性思维。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统通过最小化认知负荷和优化效率来简化决策，但算法确定性可能导致知识控制的危险。论文旨在探索一种AI框架，通过维持认知矛盾来促进真正的智力成长。

Method: 论文提出了CD-AI框架，通过延迟决策解决、鼓励辩证互动和维持认知矛盾来实现。理论探讨、模型实现以及在伦理、法律、政治和科学等领域的应用被详细分析。

Result: CD-AI能增强反思性推理、认知谦逊、批判性思维和复杂决策中的适应能力，但可能引发决策瘫痪、用户自主性侵蚀等伦理问题。

Conclusion: CD-AI挑战了传统AI增强推理的范式，提倡通过维持认知冲突而非解决冲突来提升思维能力，呼应了尼采的“超人”哲学。

Abstract: AI-augmented systems are traditionally designed to streamline human
decision-making by minimizing cognitive load, clarifying arguments, and
optimizing efficiency. However, in a world where algorithmic certainty risks
becoming an Orwellian tool of epistemic control, true intellectual growth
demands not passive acceptance but active struggle. Drawing on the dystopian
visions of George Orwell and Philip K. Dick - where reality is unstable,
perception malleable, and truth contested - this paper introduces Cognitive
Dissonance AI (CD-AI): a novel framework that deliberately sustains uncertainty
rather than resolving it. CD-AI does not offer closure, but compels users to
navigate contradictions, challenge biases, and wrestle with competing truths.
By delaying resolution and promoting dialectical engagement, CD-AI enhances
reflective reasoning, epistemic humility, critical thinking, and adaptability
in complex decision-making. This paper examines the theoretical foundations of
the approach, presents an implementation model, explores its application in
domains such as ethics, law, politics, and science, and addresses key ethical
concerns - including decision paralysis, erosion of user autonomy, cognitive
manipulation, and bias in AI reasoning. In reimagining AI as an engine of doubt
rather than a deliverer of certainty, CD-AI challenges dominant paradigms of
AI-augmented reasoning and offers a new vision - one in which AI sharpens the
mind not by resolving conflict, but by sustaining it. Rather than reinforcing
Huxleyan complacency or pacifying the user into intellectual conformity, CD-AI
echoes Nietzsche's vision of the Uebermensch - urging users to transcend
passive cognition through active epistemic struggle.

</details>


### [74] [Non-linear, Team-based VR Training for Cardiac Arrest Care with enhanced CRM Toolkit](https://arxiv.org/abs/2507.08805)
*Mike Kentros,Manos Kamarianakis,Michael Cole,Vitaliy Popov,Antonis Protopsaltis,George Papagiannakis*

Main category: cs.HC

TL;DR: iREACT是一种新型VR模拟系统，解决了传统心脏骤停(CA)培训中的关键限制，通过非线性协作环境模拟真实CA事件，提升团队协作技能。


<details>
  <summary>Details</summary>
Motivation: 传统CA培训方法难以复现真实事件的动态性和复杂性，阻碍团队资源管理(CRM)技能的发展。

Method: iREACT提供非线性协作环境，捕捉多模态数据（用户行为、认知负荷、视觉注视），并提供实时和事后反馈。

Result: 医学专家的形成性评估表明iREACT具有可用性和教育价值，适用于其他高风险培训场景。

Conclusion: iREACT通过模拟真实CA事件和提供多模态反馈，显著提升了CRM技能培训效果。

Abstract: This paper introduces iREACT, a novel VR simulation addressing key
limitations in traditional cardiac arrest (CA) training. Conventional methods
struggle to replicate the dynamic nature of real CA events, hindering Crew
Resource Management (CRM) skill development. iREACT provides a non-linear,
collaborative environment where teams respond to changing patient states,
mirroring real CA complexities. By capturing multi-modal data (user actions,
cognitive load, visual gaze) and offering real-time and post-session feedback,
iREACT enhances CRM assessment beyond traditional methods. A formative
evaluation with medical experts underscores its usability and educational
value, with potential applications in other high-stakes training scenarios to
improve teamwork, communication, and decision-making.

</details>


### [75] ['Teens Need to Be Educated on the Danger': Digital Access, Online Risks, and Safety Practices Among Nigerian Adolescents](https://arxiv.org/abs/2507.08914)
*Munachimso B. Oguine,Ozioma C. Oguine,Karla Badillo-Urquiola,Oluwasogo Adekunle Okunade*

Main category: cs.HC

TL;DR: 研究探讨了尼日利亚青少年在数字世界中的在线体验，重点关注技术访问、风险暴露、应对策略及建议。


<details>
  <summary>Details</summary>
Motivation: 青少年依赖数字技术但面临风险，尤其是在西非等被忽视的地区，需了解其在线行为及安全需求。

Method: 通过对409名尼日利亚中学生进行自填式调查，分析其技术访问、风险暴露和应对策略。

Result: 多数青少年有中等技术访问，但常遭遇不当内容和诈骗；家长是主要支持，但资源与意识不足影响了应对。

Conclusion: 需针对文化和情境设计干预措施，提升青少年数字素养，呼吁多方合作以提高在线安全。

Abstract: Adolescents increasingly rely on online technologies to explore their
identities, form social connections, and access information and entertainment.
However, their growing digital engagement exposes them to significant online
risks, particularly in underrepresented contexts like West Africa. This study
investigates the online experiences of 409 secondary school adolescents in
Nigeria's Federal Capital Territory (FCT), focusing on their access to
technology, exposure to risks, coping strategies, key stakeholders influencing
their online interactions, and recommendations for improving online safety.
Using self-administered surveys, we found that while most adolescents reported
moderate access to online technology and connectivity, those who encountered
risks frequently reported exposure to inappropriate content and online scams.
Blocking and reporting tools were the most commonly used strategies, though
some adolescents responded with inaction due to limited resources or awareness.
Parents emerged as the primary support network, though monitoring practices and
communication varied widely. Guided by Protection Motivation Theory (PMT), our
analysis interprets adolescents' online safety behaviors as shaped by both
their threat perceptions and their confidence in available coping strategies. A
thematic analysis of their recommendations highlights the need for greater
awareness and education, parental mediation, enhanced safety tools, stricter
age restrictions, improved content moderation, government accountability, and
resilience-building initiatives. Our findings underscore the importance of
culturally and contextually relevant interventions to empower adolescents in
navigating the digital world, with implications for parents, educators,
designers, and policymakers.

</details>


### [76] [Analytical Study on the Visibility of Potential Positions for External Human-Machine Interfaces](https://arxiv.org/abs/2507.08973)
*Jose Gonzalez-Belmonte,Jaerock Kwon*

Main category: cs.HC

TL;DR: 论文通过Unity模拟实验分析了15种车辆前部元素的可见性，发现前翼子板和前大灯最易被行人看到，为自动驾驶车辆信号设计提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 研究自动驾驶车辆与行人通信的信号显示位置问题，填补了该领域的研究空白。

Method: 使用Unity游戏引擎模拟实验，记录15种车辆前部元素在不同条件下的可见性，变量包括车辆位置、道路车辆数量及记录点的距离范围。

Result: 结果显示，行人最常看到的是前翼子板和前大灯，而前轮、前门、保险杠和后视镜的可见性较低。

Conclusion: 研究结果为自动驾驶车辆信号设计提供了依据，并提供了一个可扩展的模拟平台。

Abstract: As we move towards a future of autonomous vehicles, questions regarding their
method of communication have arisen. One of the common questions concerns the
placement of the signaling used to communicate with pedestrians and road users,
but little work has been published fully dedicated to exploring this. This
paper uses a simulation made in the Unity game engine to record the visibility
of fifteen different vehicles, specifically regarding the visibility of frontal
elements by a pedestrian on the sidewalk. Variables include the vehicle
position, number of vehicles on the road, and minimum and maximum distance of
the recorded points. It was concluded that the areas of the vehicle most often
seen by pedestrians on the sidewalk attempting to cross the road were the
frontal frontal fenders and the headlights, with the frontal wheels, frontal
doors, bumper, and side mirrors are less visible alternatives. These findings
are valuable in the future design of signaling for autonomous vehicles, in
order to ensure pedestrians are able to see them on approaching vehicles. The
software used provides a platform for similar works in the future to be
conducted.

</details>


### [77] [AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data](https://arxiv.org/abs/2507.09100)
*Mohammad Abolnejadian,Shakiba Amirshahi,Matthew Brehmer,Anamaria Crisan*

Main category: cs.HC

TL;DR: 研究探索了在实时决策对话中利用历史数据的方法，通过开发一个基于LLM的实时对话界面，帮助专家（如医生）在对话中获取相关数据支持。


<details>
  <summary>Details</summary>
Motivation: 专家在实时决策对话中难以利用历史数据，导致决策效率低下，因此研究希望通过技术手段解决这一问题。

Method: 开发了一个基于LLM的对话系统，实时监听对话、识别问题并提供相关数据支持，利用嵌入数据集和向量数据库检索信息。

Result: 初步评估显示系统有效，但也揭示了挑战，为后续研究提供了方向。

Conclusion: 该系统展示了在实时决策中利用历史数据的潜力，但需进一步优化以应对实际场景中的挑战。

Abstract: In decision-making conversations, experts must navigate complex choices and
make on-the-spot decisions while engaged in conversation. Although extensive
historical data often exists, the real-time nature of these scenarios makes it
infeasible for decision-makers to review and leverage relevant information.
This raises an interesting question: What if experts could utilize relevant
past data in real-time decision-making through insights derived from past data?
To explore this, we implemented a conversational user interface, taking
doctor-patient interactions as an example use case. Our system continuously
listens to the conversation, identifies patient problems and doctor-suggested
solutions, and retrieves related data from an embedded dataset, generating
concise insights using a pipeline built around a retrieval-based Large Language
Model (LLM) agent. We evaluated the prototype by embedding Health Canada
datasets into a vector database and conducting simulated studies using sample
doctor-patient dialogues, showing effectiveness but also challenges, setting
directions for the next steps of our work.

</details>


### [78] [ReDemon UI: Reactive Synthesis by Demonstration for Web UI](https://arxiv.org/abs/2507.10099)
*Jay Lee,Gyuhyeok Oh,Joongwon Ahn,Xiaokang Qiu*

Main category: cs.HC

TL;DR: ReDemon UI 通过用户演示合成 React 应用，支持设计师和非专业程序员创建符合标准 UI 原型工作流程的界面。


<details>
  <summary>Details</summary>
Motivation: 旨在简化 UI 开发流程，使非专家也能参与 React 应用的创建。

Method: 用户提供静态草图并演示交互行为；系统结合枚举合成和大型语言模型（LLM）生成 React 程序。

Result: 实现了自动识别反应数据和正确状态更新逻辑的 React 程序合成。

Conclusion: ReDemon UI 为非专业开发人员提供了一种高效的 UI 开发工具。

Abstract: ReDemon UI synthesizes React applications from user demonstrations, enabling
designers and non-expert programmers to create UIs that integrate with standard
UI prototyping workflows. Users provide a static mockup sketch with event
handler holes and demonstrate desired runtime behaviors by interacting with the
rendered mockup and editing the sketch. ReDemon UI identifies reactive data and
synthesizes a React program with correct state update logic. We utilize
enumerative synthesis for simple UIs and LLMs for more complex UIs.

</details>


### [79] [User-to-PC Authentication Through Confirmation on Mobile Devices: On Usability and Performance](https://arxiv.org/abs/2507.09190)
*Andreas Pramendorfer,Rainhard Dieter Findling*

Main category: cs.HC

TL;DR: 论文提出了一种基于令牌的无密码认证方法，利用智能手机或智能手表进行PC认证，与传统密码认证相比，智能手表认证在速度和可用性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统密码认证存在认知负担和弱凭证问题，而现代移动设备具有高级安全功能，可用于改进PC认证。

Method: 采用令牌无密码认证，用户在移动设备上确认认证请求，支持按钮点击和指纹验证，并与传统密码认证进行对比。

Result: 智能手表认证速度最快，成功率与传统方法相当，且用户评价最高。

Conclusion: 智能手表认证是一种快速、高效且用户友好的PC认证替代方案。

Abstract: Protecting personal computers (PCs) from unauthorized access typically relies
on password authentication, which is know to suffer from cognitive burden and
weak credentials. As many users nowadays carry mobile devices with advanced
security features throughout their day, there is an opportunity to leverage
these devices to improve authentication to PCs. In this paper we utilize a
token-based passwordless approach where users authenticate to their PC by
confirming the authentication request on their smartphones or smartwatches.
Upon a request to login to the PC, or to evaluate privileges, the PC issues an
authentication request that users receive on their mobile devices, where users
can confirm or deny the request. We evaluate button tap and biometric
fingerprint verification as confirmation variants, and compare their
authentication duration, success rate, and usability to traditional
password-based authentication in a user study with 30 participants and a total
of 1,200 authentication attempts. Smartwatch-based authentication outperformed
password-based authentication and smartphone-based variants in authentication
duration, while showing comparable success rates. Participants rated
smartwatch-based authentication highest in usability, followed by
password-based authentication and smartphone-based authentication.

</details>


### [80] [Discrepancies in Mental Workload Estimation: Self-Reported versus EEG-Based Measures in Data Visualization Evaluation](https://arxiv.org/abs/2507.09262)
*Soobin Yim,Sangbong Yoo,Chanyoung Yoon,Chanyoung Jung,Chansoo Kim,Yun Jang,Ghulam Jilani Quadri*

Main category: cs.HC

TL;DR: 论文研究了脑电图（EEG）和自我报告中评估心理负荷（MW）的一致性，发现两种方法存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 初步研究表明EEG和自我报告在MW评估上可能存在差异，为此进一步探索其一致性。

Method: 通过可视化任务实验，使用VLAT和SV任务，用32通道EEG记录信号，并基于GAT模型估计MW，与传统方法对比。

Result: 发现任务难度与EEG估计的MW不一致，且EEG与自我报告之间也存在差异，表明可能存在未被自我报告捕捉的无意识认知努力。

Conclusion: EEG和自我报告在MW评估上存在差异，暗示无意识认知努力的重要性。

Abstract: Accurate assessment of mental workload (MW) is crucial for understanding
cognitive processes during visualization tasks. While EEG-based measures are
emerging as promising alternatives to conventional assessment techniques, such
as selfreport measures, studies examining consistency across these different
methodologies are limited. In a preliminary study, we observed indications of
potential discrepancies between EEGbased and self-reported MW measures.
Motivated by these preliminary observations, our study further explores the
discrepancies between EEG-based and self-reported MW assessment methods through
an experiment involving visualization tasks. In the experiment, we employ two
benchmark tasks: the Visualization Literacy Assessment Test (VLAT) and a
Spatial Visualization (SV) task. EEG signals are recorded from participants
using a 32-channel system at a sampling rate of 128 Hz during the visualization
tasks. For each participant, MW is estimated using an EEG-based model built on
a Graph Attention Network (GAT) architecture, and these estimates are compared
with conventional MW measures to examine potential discrepancies. Our findings
reveal notable discrepancies between task difficulty and EEG-based MW
estimates, as well as between EEG-based and self-reported MW measures across
varying task difficulty levels. Additionally, the observed patterns suggest the
presence of unconscious cognitive effort that may not be captured by selfreport
alone.

</details>


### [81] [TraSculptor: Visual Analytics for Enhanced Decision-Making in Road Traffic Planning](https://arxiv.org/abs/2507.09489)
*Zikun Deng,Yuanbang Liu,Mingrui Zhu,Da Xiang,Haiyue Yu,Zicheng Su,Qinglong Lu,Tobias Schreck,Yi Cai*

Main category: cs.HC

TL;DR: 介绍了TraSculptor系统，解决交通规划中网络修改和状态比较的交互性问题。


<details>
  <summary>Details</summary>
Motivation: 现有交通规划平台在交互性和多状态比较方面效率低下。

Method: 设计灵活的交互修改功能和多状态历史树比较视图。

Result: 通过案例展示和专家反馈验证了系统的有效性。

Conclusion: TraSculptor提升了交通规划中的交互性和比较效率。

Abstract: The design of urban road networks significantly influences traffic
conditions, underscoring the importance of informed traffic planning. Traffic
planning experts rely on specialized platforms to simulate traffic systems,
assessing the efficacy of the road network across various states of
modifications. Nevertheless, a prevailing issue persists: many existing traffic
planning platforms exhibit inefficiencies in flexibly interacting with the road
network's structure and attributes and intuitively comparing multiple states
during the iterative planning process. This paper introduces TraSculptor, an
interactive planning decision-making system. To develop TraSculptor, we
identify and address two challenges: interactive modification of road networks
and intuitive comparison of multiple network states. For the first challenge,
we establish flexible interactions to enable experts to easily and directly
modify the road network on the map. For the second challenge, we design a
comparison view with a history tree of multiple states and a road-state matrix
to facilitate intuitive comparison of road network states. To evaluate
TraSculptor, we provided a usage scenario where the Braess's paradox was
showcased, invited experts to perform a case study on the Sioux Falls network,
and collected expert feedback through interviews.

</details>


### [82] [The Spectacle of Fidelity: Blind Resistance and the Wizardry of Prototyping](https://arxiv.org/abs/2507.09549)
*Hrittika Bhowmick,Shilpaa Anand*

Main category: cs.HC

TL;DR: 本文批判了人机交互中视觉中心原型设计的局限性，提倡从盲人学者的经验出发重新定义原型设计。


<details>
  <summary>Details</summary>
Motivation: 探讨现有原型设计文化中对视觉的过度依赖，以及这种依赖如何排除了非视觉模式的设计参与。

Method: 结合盲人学者的生活经验和文化残疾研究的见解，分析视觉中心原型设计的局限性。

Result: 提出原型设计应作为一种情境化、具身化和关系性的实践，以包容更多元的设计参与方式。

Conclusion: 呼吁人机交互领域重新审视原型设计的定义，以更包容的态度对待不同感知模式的设计参与。

Abstract: Prototyping is widely regarded in Human-Computer Interaction as an iterative
process through which ideas are tested and refined, often via visual mockups,
screen flows, and coded simulations. This position paper critiques the
visual-centric norms embedded in prototyping culture by drawing from the lived
experiences of blind scholars and insights from cultural disability studies. It
discusses how dominant methods of prototyping rely on an unexamined fidelity to
sight, privileging what can be rendered visibly coherent while marginalizing
other modes of knowing and making. By repositioning prototyping as a situated,
embodied, and relational practice, this paper challenges HCI to rethink what
kinds of design participation are legitimized and which are excluded when
prototyping is reduced to screen-based simulations.

</details>


### [83] [SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations](https://arxiv.org/abs/2507.09664)
*Zoe Kaputa,Anika Rajaram,Vryan Almanon Feliciano,Zhuoyue Lyu,Maneesh Agrawala,Hari Subramonyam*

Main category: cs.HC

TL;DR: 该论文提出了一种名为Chain-of-Abstractions（CoA）的框架，旨在恢复编程中的核心功能（如可追溯性、逐步优化和行为测试），同时保留自然语言的表达灵活性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的基于提示的编程为终端用户编程提供了新范式，尤其是为非程序员（如教育工作者）提供了便利。然而，这种方式绕过了直接编写代码，导致编程的核心功能丧失。

Method: 作者提出了CoA框架，通过分解生成过程为一系列任务对齐的抽象表示。并在SimStep环境中实现了这一方法，支持教师通过四个中间抽象（如概念图、场景图等）创建教学模拟。

Result: SimStep包含逆向修正过程，用于处理歧义和偏差。实验显示，CoA提高了编程流程的可控性和可解释性。

Conclusion: CoA在编程提示流程中增强了作者的控制力和可解释性，尤其适用于教育工作者。

Abstract: Programming-by-prompting with generative AI offers a new paradigm for
end-user programming, shifting the focus from syntactic fluency to semantic
intent. This shift holds particular promise for non-programmers such as
educators, who can describe instructional goals in natural language to generate
interactive learning content. Yet in bypassing direct code authoring, many of
programming's core affordances - such as traceability, stepwise refinement, and
behavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)
framework as a way to recover these affordances while preserving the expressive
flexibility of natural language. CoA decomposes the synthesis process into a
sequence of cognitively meaningful, task-aligned representations that function
as checkpoints for specification, inspection, and refinement. We instantiate
this approach in SimStep, an authoring environment for teachers that scaffolds
simulation creation through four intermediate abstractions: Concept Graph,
Scenario Graph, Learning Goal Graph, and UI Interaction Graph. To address
ambiguities and misalignments, SimStep includes an inverse correction process
that surfaces in-filled model assumptions and enables targeted revision without
requiring users to manipulate code. Evaluations with educators show that CoA
enables greater authoring control and interpretability in
programming-by-prompting workflows.

</details>


### [84] [An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments](https://arxiv.org/abs/2507.10469)
*Mikko Korkiakoski,Saeid Sheikhi,Jesper Nyman,Jussi Saariniemi,Kalle Tapio,Panos Kostakos*

Main category: cs.HC

TL;DR: 研究了AI驱动的NPC在VR审讯模拟器中的表现，评估了其真实性、可用性和系统性能，结果显示GPT-4 Turbo提升了NPC的真实性，但存在延迟和情感深度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过AI提升VR中NPC的真实性和交互体验，并评估其实际效果。

Method: 使用GPT-4 Turbo开发VR审讯模拟器，通过用户研究评估NPC的真实性、可用性和系统性能，测量延迟等指标。

Result: 平均延迟7秒，真实感评分为6.67/10，系统可用性分数为79.44，表明AI提升了NPC的真实性，但延迟和情感表现仍需改进。

Conclusion: 大型语言模型能显著提升VR中NPC的真实性，但需进一步优化系统性能和情感表现以实现更高沉浸感。

Abstract: Advancements in artificial intelligence (AI) have significantly enhanced the
realism and interactivity of non-player characters (NPCs) in virtual reality
(VR), creating more engaging and believable user experiences. This paper
evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their
perceived realism, usability, and system performance. The simulator features
two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage
participants in a scenario to determine the suspect's guilt or innocence. A
user study with 18 participants assessed the system using the System Usability
Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent
Believability Questionnaire, alongside latency measurements for speech-to-text
(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.
Results showed an average cycle latency of 7 seconds, influenced by the
increasing conversational context. Believability scored 6.67 out of 10, with
high ratings in behavior, social relationships, and intelligence but moderate
scores in emotion and personality. The system achieved a SUS score of 79.44,
indicating good usability. These findings demonstrate the potential of large
language models to improve NPC realism and interaction in VR while highlighting
challenges in reducing system latency and enhancing emotional depth. This
research contributes to the development of more sophisticated AI-driven NPCs,
revealing the need for performance optimization to achieve increasingly
immersive virtual experiences.

</details>


### [85] [Volume-Based Space-Time Cube for Large-Scale Continuous Spatial Time Series](https://arxiv.org/abs/2507.09917)
*Zikun Deng,Jiabao Huang,Chenxi Ruan,Jialing Li,Shaowu Gao,Yi Cai*

Main category: cs.HC

TL;DR: VolumeSTCube是一种新的时空数据可视化框架，通过体积渲染和表面渲染技术解决了空间时间立方体（STC）的视觉遮挡和深度模糊问题，并在大规模时空数据分析中表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 解决STC在可视化时空数据时存在的视觉遮挡和深度模糊问题，特别是在大规模时空数据中的挑战。

Method: 利用STC将离散的时空数据转换为连续的体积数据，并运用体积渲染和表面渲染技术进行可视化，同时设计交互功能以支持多角度分析。

Result: 通过计算实验、专家案例研究和非专家用户研究发现，VolumeSTCube在大规模时空数据分析中优于基线方法。

Conclusion: VolumeSTCube有效解决了STC的局限性，为时空数据分析提供了更高效的工具。

Abstract: Spatial time series visualization offers scientific research pathways and
analytical decision-making tools across various spatiotemporal domains. Despite
many advanced methodologies, the seamless integration of temporal and spatial
information remains a challenge. The space-time cube (STC) stands out as a
promising approach for the synergistic presentation of spatial and temporal
information, with successful applications across various spatiotemporal
datasets. However, the STC is plagued by well-known issues such as visual
occlusion and depth ambiguity, which are further exacerbated when dealing with
large-scale spatial time series data. In this study, we introduce a novel
technical framework termed VolumeSTCube, designed for continuous spatiotemporal
phenomena. It first leverages the concept of the STC to transform discretely
distributed spatial time series data into continuously volumetric data.
Subsequently, volume rendering and surface rendering techniques are employed to
visualize the transformed volumetric data. Volume rendering is utilized to
mitigate visual occlusion, while surface rendering provides pattern details by
enhanced lighting information. Lastly, we design interactions to facilitate the
exploration and analysis from temporal, spatial, and spatiotemporal
perspectives. VolumeSTCube is evaluated through a computational experiment, a
real-world case study with one expert, and a controlled user study with twelve
non-experts, compared against a baseline from prior work, showing its
superiority and effectiveness in largescale spatial time series analysis.

</details>


### [86] [Branch Explorer: Leveraging Branching Narratives to Support Interactive 360° Video Viewing for Blind and Low Vision Users](https://arxiv.org/abs/2507.09959)
*Shuchang Xu,Xiaofu Jin,Wenshuo Zhang,Huamin Qu,Yukang Yan*

Main category: cs.HC

TL;DR: Branch Explorer系统通过将360度视频转化为分支叙事，支持盲人和低视力用户互动观看，提升其参与感。


<details>
  <summary>Details</summary>
Motivation: 360度视频通常无法为盲人和低视力用户提供互动体验，亟需解决方案。

Method: 系统采用多模态机器学习生成多样叙事路径，用户可通过音频导览自由选择分支。

Result: 评估显示，12名用户体验显著提升，并发展出个性化探索策略。

Conclusion: Branch Explorer为视频和虚拟环境无障碍探索提供了新思路。

Abstract: 360{\deg} videos enable users to freely choose their viewing paths, but blind
and low vision (BLV) users are often excluded from this interactive experience.
To bridge this gap, we present Branch Explorer, a system that transforms
360{\deg} videos into branching narratives -- stories that dynamically unfold
based on viewer choices -- to support interactive viewing for BLV audiences.
Our formative study identified three key considerations for accessible
branching narratives: providing diverse branch options, ensuring coherent story
progression, and enabling immersive navigation among branches. To address these
needs, Branch Explorer employs a multi-modal machine learning pipeline to
generate diverse narrative paths, allowing users to flexibly make choices at
detected branching points and seamlessly engage with each storyline through
immersive audio guidance. Evaluation with 12 BLV viewers showed that Branch
Explorer significantly enhanced user agency and engagement in 360{\deg} video
viewing. Users also developed personalized strategies for exploring 360{\deg}
content. We further highlight implications for supporting accessible
exploration of videos and virtual environments.

</details>


### [87] [Qualitative Study for LLM-assisted Design Study Process: Strategies, Challenges, and Roles](https://arxiv.org/abs/2507.10024)
*Shaolun Ruan,Rui Sheng,Xiaolin Wen,Jiachen Wang,Tianyi Zhang,Yong Wang,Tim Dwyer,Jiannan Li*

Main category: cs.HC

TL;DR: 这篇论文研究了如何有效利用大型语言模型（LLM）来增强可视化设计研究的过程，并通过多阶段定性研究总结了LLM在设计研究中的角色、挑战和应对策略。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在可视化设计研究中提供了新的机会，但目前缺乏关于如何系统利用LLM的系统性理解。

Method: 论文通过多阶段定性研究，采访了30名不同背景的设计研究人员，结合深度访谈和问卷调查，探讨了LLM的使用策略、挑战及应对方法。

Result: 研究发现LLM在设计研究的不同阶段可以扮演多种角色，并为可视化从业者提供了实用框架。

Conclusion: 研究为可视化研究中如何利用LLM提供了系统性框架和实践建议。

Abstract: Design studies aim to create visualization solutions for real-world problems
of different application domains. Recently, the emergence of large language
models (LLMs) has introduced new opportunities to enhance the design study
process, providing capabilities such as creative problem-solving, data
handling, and insightful analysis. However, despite their growing popularity,
there remains a lack of systematic understanding of how LLMs can effectively
assist researchers in visualization-specific design studies. In this paper, we
conducted a multi-stage qualitative study to fill this gap, involving 30 design
study researchers from diverse backgrounds and expertise levels. Through
in-depth interviews and carefully-designed questionnaires, we investigated
strategies for utilizing LLMs, the challenges encountered, and the practices
used to overcome them. We further compiled and summarized the roles that LLMs
can play across different stages of the design study process. Our findings
highlight practical implications to inform visualization practitioners, and
provide a framework for leveraging LLMs to enhance the design study process in
visualization research.

</details>


### [88] [XROps: A Visual Workflow Management System for Dynamic Immersive Analytics](https://arxiv.org/abs/2507.10043)
*Suemin Jeon,JunYoung Choi,Haejin Jeong,Won-Ki Jeong*

Main category: cs.HC

TL;DR: XROps是一个基于网络的可视化编程系统，旨在降低创建沉浸式分析应用的技术门槛，支持实时数据集成和动态调整。


<details>
  <summary>Details</summary>
Motivation: 现有沉浸式分析工具需要编程技能，且难以适应动态任务和数据变化，限制了领域专家的使用。

Method: 开发了XROps系统，通过交互式可视化编程实现沉浸式分析的动态创作，支持实时数据集成和即时反馈。

Result: 用户研究表明XROps易用且有效，能够支持多种分析场景，并提供了一个展示平台。

Conclusion: XROps为领域专家提供了一个无需编程的工具，显著降低了沉浸式分析系统的开发门槛。

Abstract: Immersive analytics is gaining attention across multiple domains due to its
capability to facilitate intuitive data analysis in expansive environments
through user interaction with data. However, creating immersive analytics
systems for specific tasks is challenging due to the need for programming
expertise and significant development effort. Despite the introduction of
various immersive visualization authoring toolkits, domain experts still face
hurdles in adopting immersive analytics into their workflow, particularly when
faced with dynamically changing tasks and data in real time. To lower such
technical barriers, we introduce XROps, a web-based authoring system that
allows users to create immersive analytics applications through interactive
visual programming, without the need for low-level scripting or coding. XROps
enables dynamic immersive analytics authoring by allowing users to modify each
step of the data visualization process with immediate feedback, enabling them
to build visualizations on-the-fly and adapt to changing environments. It also
supports the integration and visualization of real-time sensor data from XR
devices, a key feature of immersive analytics, facilitating the creation of
various analysis scenarios. We evaluated the usability of XROps through a user
study and demonstrate its efficacy and usefulness in several example scenarios.
We have released a web platform (https://vience.io/xrops) to demonstrate
various examples to supplement our findings.

</details>


### [89] [MEDebiaser: A Human-AI Feedback System for Mitigating Bias in Multi-label Medical Image Classification](https://arxiv.org/abs/2507.10044)
*Shaohan Shi,Yuheng Shao,Haoran Jiang,Yunjie Yao,Zhijun Zhang,Xu Ding,Quan Li*

Main category: cs.HC

TL;DR: MEDebiaser是一个交互式系统，允许医生通过本地解释直接优化AI模型，以解决医学图像分类中的偏见问题，减少对工程师的依赖，提升人机协作效率。


<details>
  <summary>Details</summary>
Motivation: 医学图像标签分布不均衡且存在共现问题，导致多标签分类中的偏见，传统合作模式难以实现医生与AI模型的有效反馈。

Method: MEDebiaser结合预测与注意力损失函数，采用定制排名策略，使医生无需技术背景即可直接调整模型。

Result: 实验和用户研究表明，MEDebiaser能有效减少偏见，提升可用性和协作效率。

Conclusion: MEDebiaser为将医学专业知识融入AI医疗提供了一种实用解决方案。

Abstract: Medical images often contain multiple labels with imbalanced distributions
and co-occurrence, leading to bias in multi-label medical image classification.
Close collaboration between medical professionals and machine learning
practitioners has significantly advanced medical image analysis. However,
traditional collaboration modes struggle to facilitate effective feedback
between physicians and AI models, as integrating medical expertise into the
training process via engineers can be time-consuming and labor-intensive. To
bridge this gap, we introduce MEDebiaser, an interactive system enabling
physicians to directly refine AI models using local explanations. By combining
prediction with attention loss functions and employing a customized ranking
strategy to alleviate scalability, MEDebiaser allows physicians to mitigate
biases without technical expertise, reducing reliance on engineers, and thus
enhancing more direct human-AI feedback. Our mechanism and user studies
demonstrate that it effectively reduces biases, improves usability, and
enhances collaboration efficiency, providing a practical solution for
integrating medical expertise into AI-driven healthcare.

</details>


### [90] [When Familiarity Remains: Procedural Memory, Symbolic Anchors, and Digital Engagement in Dementia Care](https://arxiv.org/abs/2507.10102)
*Jeongone Seo,Kyung-zoon Hong,Sol Baik*

Main category: cs.HC

TL;DR: 研究探索了程序性记忆和象征性锚点如何支持韩国老年痴呆症患者的技术使用，通过访谈发现熟悉的数字习惯和象征性内容能增强互动，而不熟悉的技术可能引发恐惧。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解程序性记忆和象征性锚点在老年痴呆症护理中如何支持技术使用，尤其是在韩国文化背景下。

Method: 通过深度访谈11位社区居住的认知衰退老年人的专业护理人员，采用扎根理论方法分析数据。

Result: 熟悉的数字习惯（如拍照）通过程序性记忆持续存在，象征性锚点（如家庭照片）增强互动；不熟悉的技术可能引发恐惧或抗拒。

Conclusion: 设计符合文化和认知需求的技术可提升老年痴呆症患者的自主性和幸福感，程序性记忆和象征性锚点至关重要。

Abstract: INTRODUCTION: Older adults with early-stage dementia often retain procedural
memory, enabling continued use of familiar technologies. Additionally, symbolic
anchors such as photos or personalized content may serve as memory cues to
reinforce digital engagement. This study explores how these mechanisms support
technology use in dementia care within the South Korean context.
  METHODS: We conducted in-depth interviews with 11 professional caregivers of
community-dwelling older adults with cognitive decline. Grounded theory methods
guided the analysis, using iterative coding and constant comparison to identify
emergent themes.
  RESULTS: Caregivers reported that familiar digital routines (e.g., taking
photos) persisted through procedural memory. Symbolic anchors such as family
photos or recognizable icons enhanced interaction and emotional engagement.
However, unfamiliar or anthropomorphic technologies often triggered fear or
symbolic resistance.
  DISCUSSION: Findings highlight the dual role of procedural memory and
symbolic anchors in sustaining digital engagement. Designing culturally
responsive and cognitively accessible technologies may enhance autonomy and
well-being in dementia care.
  Keywords: procedural memory, symbolic anchors, dementia care, digital
engagement, older adults, cultural adaptation, caregiving technologies

</details>


### [91] [Visual Analytics for Explainable and Trustworthy Artificial Intelligence](https://arxiv.org/abs/2507.10240)
*Angelos Chatzimparmpas*

Main category: cs.HC

TL;DR: AI系统在医疗等领域潜力巨大，但缺乏透明度阻碍了其采用。视觉分析（VA）通过结合AI模型与交互式可视化，帮助用户理解并改进模型，增强信任。


<details>
  <summary>Details</summary>
Motivation: AI系统如‘黑盒’，缺乏透明度，影响专家对其的信任和使用。

Method: 利用视觉分析（VA）将AI模型与交互可视化结合，定义并探索VA如何提升AI流程各阶段的信任。

Result: 提出可视化设计空间，并开发用于AI流程各关键任务的VA仪表板。

Conclusion: VA是解决AI透明度问题的有效方法，能增强用户对AI模型的信任和理解。

Abstract: Our society increasingly depends on intelligent systems to solve complex
problems, ranging from recommender systems suggesting the next movie to watch
to AI models assisting in medical diagnoses for hospitalized patients. With the
iterative improvement of diagnostic accuracy and efficiency, AI holds
significant potential to mitigate medical misdiagnoses by preventing numerous
deaths and reducing an economic burden of approximately 450 EUR billion
annually. However, a key obstacle to AI adoption lies in the lack of
transparency: many automated systems function as "black boxes," providing
predictions without revealing the underlying processes. This opacity can hinder
experts' ability to trust and rely on AI systems. Visual analytics (VA)
provides a compelling solution by combining AI models with interactive
visualizations. These specialized charts and graphs empower users to
incorporate their domain expertise to refine and improve the models, bridging
the gap between AI and human understanding. In this work, we define,
categorize, and explore how VA solutions can foster trust across the stages of
a typical AI pipeline. We propose a design space for innovative visualizations
and present an overview of our previously developed VA dashboards, which
support critical tasks within the various pipeline stages, including data
processing, feature engineering, hyperparameter tuning, understanding,
debugging, refining, and comparing models.

</details>


### [92] [Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots: Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads](https://arxiv.org/abs/2507.10427)
*Jing Li,Felix Schijve,Sheng Li,Yuye Yang,Jun Hu,Emilia Barakova*

Main category: cs.HC

TL;DR: 研究探讨了利用大型语言模型（LLM）与社交辅助机器人（SAR）结合，帮助神经发育障碍儿童及其家长共同调节情绪，并展示了初步实验的积极效果和设计挑战。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏关于如何将LLM与SAR结合以支持神经发育障碍儿童及其家长共同调节情绪的研究，本文旨在填补这一空白。

Method: 在MiRo-E机器人平台上部署语音通信模块，开发了一个LLM驱动的社交机器人系统，并通过两个亲子对的试点测试进行验证。

Result: 初步测试显示，MiRo-E对互动动态有积极影响，并可能促进情绪调节，但也发现了设计和技术上的挑战。

Conclusion: 研究为未来开发LLM驱动的SAR在心理健康应用中的设计提供了启示。

Abstract: Socially Assistive Robotics (SAR) has shown promise in supporting emotion
regulation for neurodivergent children. Recently, there has been increasing
interest in leveraging advanced technologies to assist parents in co-regulating
emotions with their children. However, limited research has explored the
integration of large language models (LLMs) with SAR to facilitate emotion
co-regulation between parents and children with neurodevelopmental disorders.
To address this gap, we developed an LLM-powered social robot by deploying a
speech communication module on the MiRo-E robotic platform. This supervised
autonomous system integrates LLM prompts and robotic behaviors to deliver
tailored interventions for both parents and neurodivergent children. Pilot
tests were conducted with two parent-child dyads, followed by a qualitative
analysis. The findings reveal MiRo-E's positive impacts on interaction dynamics
and its potential to facilitate emotion regulation, along with identified
design and technical challenges. Based on these insights, we provide design
implications to advance the future development of LLM-powered SAR for mental
health applications.

</details>


### [93] [VIP-Sim: A User-Centered Approach to Vision Impairment Simulation for Accessible Design](https://arxiv.org/abs/2507.10479)
*Max Rädler,Mark Colley,Enrico Rukzio*

Main category: cs.HC

TL;DR: VIP-Sim是一款基于症状的视觉障碍模拟器，通过与7名视觉障碍者（VIPs）合作开发，旨在帮助设计师更好地理解视觉障碍者的需求。尽管多数参与者认为VIP-Sim能有效模拟他们的症状，但仍存在对设计中的包容性和模拟全面性的担忧。


<details>
  <summary>Details</summary>
Motivation: 现有视觉障碍模拟器缺乏视觉障碍者的直接参与和评估，因此需要开发一款更贴近真实体验的工具。

Method: 通过参与式设计过程开发VIP-Sim，涵盖21种视觉症状（如视野缺失或光敏感），可集成到桌面设计工具中。

Result: 多数参与者认为VIP-Sim能有效模拟其症状，工具受到积极评价，但仍存在对包容性和全面性的质疑。

Conclusion: VIP-Sim在模拟视觉障碍方面取得了进展，但需进一步扩展以更全面地代表各类视觉障碍者的体验。

Abstract: People with vision impairments (VIPs) often rely on their remaining vision
when interacting with user interfaces. Simulating visual impairments is an
effective tool for designers, fostering awareness of the challenges faced by
VIPs. While previous research has introduced various vision impairment
simulators, none have yet been developed with the direct involvement of VIPs or
thoroughly evaluated from their perspective. To address this gap, we developed
VIP-Sim. This symptom-based vision simulator was created through a
participatory design process tailored explicitly for this purpose, involving
N=7 VIPs. 21 symptoms, like field loss or light sensitivity, can be overlaid on
desktop design tools. Most participants felt VIP-Sim could replicate their
symptoms. VIP-Sim was received positively, but concerns about exclusion in
design and comprehensiveness of the simulation remain, mainly whether it
represents the experiences of other VIPs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [94] [Interactive Drawing Guidance for Anime Illustrations with Diffusion Model](https://arxiv.org/abs/2507.09140)
*Chuang Chen,Xiaoxuan Xie,Yongming Zhang,Tianyu Zhang,Haoran Xie*

Main category: cs.GR

TL;DR: 为帮助初学者解决动漫插画绘制中的高难度问题，提出了一个实时交互式绘图指导系统，基于StreamDiffusion和Stable Diffusion优化，显著提升创作效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 动漫插画的复杂风格和精细细节对初学者构成挑战，需要一种能提供实时指导的系统来简化创作过程。

Method: 结合StreamDiffusion管道，使用LoRA微调Stable Diffusion生成动漫风格RGB图像，并通过Informative Drawings模型和自定义优化器生成结构化指导草图。

Result: 系统能提供精确的实时绘图指导，显著提升绘制效率和准确性，并通过用户研究验证了性能和可用性。

Conclusion: 该系统有效解决了动漫插画绘制中的高难度问题，为初学者提供了实用的创作工具。

Abstract: Creating high-quality anime illustrations presents notable challenges,
particularly for beginners, due to the intricate styles and fine details
inherent in anime art. We present an interactive drawing guidance system
specifically designed for anime illustrations to address this issue. It offers
real-time guidance to help users refine their work and streamline the creative
process. Our system is built upon the StreamDiffusion pipeline to deliver
real-time drawing assistance. We fine-tune Stable Diffusion with LoRA to
synthesize anime style RGB images from user-provided hand-drawn sketches and
prompts. Leveraging the Informative Drawings model, we transform these RGB
images into rough sketches, which are further refined into structured guidance
sketches using a custom-designed optimizer. The proposed system offers precise,
real-time guidance aligned with the creative intent of the user, significantly
enhancing both the efficiency and accuracy of the drawing process. To assess
the effectiveness of our approach, we conducted a user study, gathering
empirical feedback on both system performance and interface usability.

</details>


### [95] [Physics-Aware Fluid Field Generation from User Sketches Using Helmholtz-Hodge Decomposition](https://arxiv.org/abs/2507.09146)
*Ryuichi Miyauchi,Hengyuan Chang,Tsukasa Fukusato,Kazunori Miyata,Haoran Xie*

Main category: cs.GR

TL;DR: 该论文提出一种交互式设计二维矢量场的方法，通过结合潜在扩散模型和赫尔姆霍兹-霍德分解，优化了用户草图生成的矢量场，同时保持物理特性。


<details>
  <summary>Details</summary>
Motivation: 复杂流体行为的控制仍具挑战性，现有生成模型难以兼顾物理特性（如不可压缩性）。

Method: 分两阶段：1) 使用潜在扩散模型从用户草图生成初始矢量场；2) 通过赫尔姆霍兹-霍德分解局部提取并重组物理特性。

Result: 实验验证了方法的有效性，生成的矢量场既符合用户意图，又保持了物理特性。

Conclusion: 该方法为直观且物理准确的矢量场设计提供了有效解决方案。

Abstract: Fluid simulation techniques are widely used in various fields such as film
production, but controlling complex fluid behaviors remains challenging. While
recent generative models enable intuitive generation of vector fields from user
sketches, they struggle to maintain physical properties such as
incompressibility. To address these issues, this paper proposes a method for
interactively designing 2D vector fields. Conventional generative models can
intuitively generate vector fields from user sketches, but remain difficult to
consider physical properties. Therefore, we add a simple editing process after
generating the vector field. In the first stage, we use a latent diffusion
model~(LDM) to automatically generate initial 2D vector fields from user
sketches. In the second stage, we apply the Helmholtz-Hodge decomposition to
locally extract physical properties such as incompressibility from the results
generated by LDM and recompose them according to user intentions. Through
multiple experiments, we demonstrate the effectiveness of our proposed method.

</details>


### [96] [RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling](https://arxiv.org/abs/2507.09441)
*Ankit Sanjyal*

Main category: cs.GR

TL;DR: 该论文提出了一种自适应的分类器自由引导（CFG）调度方法，通过分析采样过程中的潜在能量分布，解决了扩散模型在高分辨率图像合成中的能量不稳定和引导伪影问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像合成中，扩散模型常因能量不稳定和引导伪影导致视觉质量下降，因此需要一种能够稳定能量轨迹的方法。

Method: 提出能量感知调度策略，动态调整引导强度，使用DPM++ 2M模型结合线性递减的CFG调度。

Result: 方法在稳定性（0.9998）和一致性（0.9873）指标上优于固定引导方法，生成了更锐利、更真实的图像并减少了伪影。

Conclusion: 能量分析框架为理解和改进扩散模型行为提供了有力工具，自适应CFG调度在图像合成中表现出优越性能。

Abstract: High-resolution image synthesis with diffusion models often suffers from
energy instabilities and guidance artifacts that degrade visual quality. We
analyze the latent energy landscape during sampling and propose adaptive
classifier-free guidance (CFG) schedules that maintain stable energy
trajectories. Our approach introduces energy-aware scheduling strategies that
modulate guidance strength over time, achieving superior stability scores
(0.9998) and consistency metrics (0.9873) compared to fixed-guidance
approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling
yields optimal performance, providing sharper, more faithful images while
reducing artifacts. Our energy profiling framework serves as a powerful
diagnostic tool for understanding and improving diffusion model behavior.

</details>


### [97] [Real-time and Controllable Reactive Motion Synthesis via Intention Guidance](https://arxiv.org/abs/2507.09704)
*Xiaotang Zhang,Ziyi Chang,Qianhui Men,Hubert Shum*

Main category: cs.GR

TL;DR: 提出了一种基于输入角色轨迹的实时反应动作合成方法，通过意图预测器处理未来运动的不确定性，并利用对抗训练增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为离线处理，无法实现实时、长期的交互式动作合成，且缺乏用户主动干预的灵活性。

Method: 引入意图预测器预测关节意图，将其编码到潜在空间并与代码库匹配，通过对抗训练生成动作。

Result: 实验表明该方法在稳定性和泛化性上优于其他基于匹配的动作合成方法，并能实现用户主动干预。

Conclusion: 该方法实现了实时、长期的交互式动作合成，同时提升了动作的自然性和个性化程度。

Abstract: We propose a real-time method for reactive motion synthesis based on the
known trajectory of input character, predicting instant reactions using only
historical, user-controlled motions. Our method handles the uncertainty of
future movements by introducing an intention predictor, which forecasts key
joint intentions to make pose prediction more deterministic from the historical
interaction. The intention is later encoded into the latent space of its
reactive motion, matched with a codebook which represents mappings between
input and output. It samples a categorical distribution for pose generation and
strengthens model robustness through adversarial training. Unlike previous
offline approaches, the system can recursively generate intentions and reactive
motions using feedback from earlier steps, enabling real-time, long-term
realistic interactive synthesis. Both quantitative and qualitative experiments
show our approach outperforms other matching-based motion synthesis approaches,
delivering superior stability and generalizability. In our method, user can
also actively influence the outcome by controlling the moving directions,
creating a personalized interaction path that deviates from predefined
trajectories.

</details>


### [98] [CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design](https://arxiv.org/abs/2507.09792)
*Prashant Govindarajan,Davide Baldelli,Jay Pathak,Quentin Fournier,Sarath Chandar*

Main category: cs.GR

TL;DR: 利用大语言模型（LLMs）自动化CAD设计，通过生成高质量描述和JSON格式的CAD序列，显著提升设计效率。


<details>
  <summary>Details</summary>
Motivation: CAD设计目前仍依赖手动操作，耗时且低效，现有方法未能充分利用LLMs的潜力。

Method: 基于GPT-4.1生成数据集，微调代码LLMs以从自然语言生成JSON格式的CAD序列，并引入几何和拓扑指标评估质量。

Result: CADmium能够显著加速新对象的CAD设计，实验验证了其有效性。

Conclusion: LLMs在自动化CAD设计中具有潜力，新数据集和指标为未来研究提供了支持。

Abstract: Computer-aided design (CAD) is the digital construction of 2D and 3D objects,
and is central to a wide range of engineering and manufacturing applications
like automobile and aviation. Despite its importance, CAD modeling remains
largely a time-intensive, manual task. Recent works have attempted to automate
this process with small transformer-based models and handcrafted CAD sequence
representations. However, there has been little effort to leverage the
potential of large language models (LLMs) for sequential CAD design. In this
work, we introduce a new large-scale dataset of more than 170k CAD models
annotated with high-quality, human-like descriptions generated with our
pipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs
to generate CAD sequences represented in a JSON-based format from natural
language descriptions, demonstrating the viability and effectiveness of this
approach for text-conditioned CAD generation. Because simple metrics often fail
to reflect the quality of generated objects, we introduce geometric and
topological metrics based on sphericity, mean curvature, and Euler
characteristic to provide richer structural insights. Our experiments and
ablation studies on both synthetic and human-annotated data demonstrate that
CADmium is able to automate CAD design, drastically speeding up the design of
new objects. The dataset, code, and fine-tuned models are available online.

</details>


### [99] [ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions](https://arxiv.org/abs/2507.10542)
*Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Nießner,Derek Bradley*

Main category: cs.GR

TL;DR: 基于局部表情特征和3D高斯溅射技术，提出一种生成高保真3D头部动画的新方法，实现实时多样表情渲染。


<details>
  <summary>Details</summary>
Motivation: 为实现沉浸式远程呈现和电影中高保真3D头部动画的实时生成，需解决面部微观特征和表情的细节渲染问题。

Method: 通过局部表情特征与3D高斯溅射技术结合，利用分块几何模型提取表情特征，并动态合成3D高斯点。

Result: ScaffoldAvatar在多样表情和风格中实现自然动态渲染，达到业界领先水平。

Conclusion: 该方法通过局部表情和高斯溅射的有效结合，为高保真3D头部动画提供了一种高效的解决方案。

Abstract: Generating high-fidelity real-time animated sequences of photorealistic 3D
head avatars is important for many graphics applications, including immersive
telepresence and movies. This is a challenging problem particularly when
rendering digital avatar close-ups for showing character's facial microfeatures
and expressions. To capture the expressive, detailed nature of human heads,
including skin furrowing and finer-scale facial movements, we propose to couple
locally-defined facial expressions with 3D Gaussian splatting to enable
creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In
contrast to previous works that operate on a global expression space, we
condition our avatar's dynamics on patch-based local expression features and
synthesize 3D Gaussians at a patch level. In particular, we leverage a
patch-based geometric 3D face model to extract patch expressions and learn how
to translate these into local dynamic skin appearance and motion by coupling
the patches with anchor points of Scaffold-GS, a recent hierarchical scene
representation. These anchors are then used to synthesize 3D Gaussians
on-the-fly, conditioned by patch-expressions and viewing direction. We employ
color-based densification and progressive training to obtain high-quality
results and faster convergence for high resolution 3K training images. By
leveraging patch-level expressions, ScaffoldAvatar consistently achieves
state-of-the-art performance with visually natural motion, while encompassing
diverse facial expressions and styles in real time.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [100] [Quantum-Resilient Privacy Ledger (QRPL): A Sovereign Digital Currency for the Post-Quantum Era](https://arxiv.org/abs/2507.09067)
*Serhan W. Bahar*

Main category: cs.ET

TL;DR: 该论文提出了一种名为QRPL的量子抗隐私账本架构，结合NIST标准的后量子密码学和零知识证明，旨在解决CBDC设计中的隐私和中心化问题。


<details>
  <summary>Details</summary>
Motivation: 量子计算的崛起对现有加密基础设施构成威胁，同时CBDC的发展引发了对隐私和中心化的担忧。

Method: 提出了结合后量子密码学和零知识证明的QRPL架构，包括短暂证明链、隐私加权的PoS共识和零知识证明选择性披露机制。

Result: QRPL旨在解决CBDC设计中的关键缺陷，如普遍监控风险，并实现10-20秒的区块时间以平衡安全性和吞吐量。

Conclusion: 论文提出了QRPL的概念，并计划开发原型以实证验证其模型。

Abstract: The emergence of quantum computing presents profound challenges to existing
cryptographic infrastructures, whilst the development of central bank digital
currencies (CBDCs) has raised concerns regarding privacy preservation and
excessive centralisation in digital payment systems. This paper proposes the
Quantum-Resilient Privacy Ledger (QRPL) as an innovative token-based digital
currency architecture that incorporates National Institute of Standards and
Technology (NIST)-standardised post-quantum cryptography (PQC) with hash-based
zero-knowledge proofs to ensure user sovereignty, scalability, and transaction
confidentiality. Key contributions include adaptations of ephemeral proof
chains for unlinkable transactions, a privacy-weighted Proof-of-Stake (PoS)
consensus to promote equitable participation, and a novel zero-knowledge
proof-based mechanism for privacy-preserving selective disclosure. QRPL aims to
address critical shortcomings in prevailing CBDC designs, including risks of
pervasive surveillance, with a 10-20 second block time to balance security and
throughput in future monetary systems. While conceptual, empirical prototypes
are planned. Future work includes prototype development to validate these
models empirically.

</details>


### [101] [Solving the compute crisis with physics-based ASICs](https://arxiv.org/abs/2507.10463)
*Maxwell Aifer,Zach Belateche,Suraj Bramhavar,Kerem Y. Camsari,Patrick J. Coles,Gavin Crooks,Douglas J. Durian,Andrea J. Liu,Anastasia Marchenkova,Antonio J. Martinez,Peter L. McMahon,Faris Sbahi,Benjamin Weiner,Logan G. Wright*

Main category: cs.ET

TL;DR: 物理ASIC通过利用物理动力学直接计算，突破传统CMOS限制，提升能源效率和计算吞吐量，适用于AI及科学模拟。


<details>
  <summary>Details</summary>
Motivation: 传统AI计算面临能源消耗高、训练成本大及CMOS扩展瓶颈，需新型计算范式。

Method: 放松传统ASIC约束（如无状态、单向性等），设计基于物理动力学的专用电路。

Result: 物理ASIC在能源效率和计算吞吐量上显著提升，适用于扩散模型、神经网络推理等。

Conclusion: 物理ASIC有望突破当前计算瓶颈，开启高效异构计算新时代。

Abstract: Escalating artificial intelligence (AI) demands expose a critical "compute
crisis" characterized by unsustainable energy consumption, prohibitive training
costs, and the approaching limits of conventional CMOS scaling. Physics-based
Application-Specific Integrated Circuits (ASICs) present a transformative
paradigm by directly harnessing intrinsic physical dynamics for computation
rather than expending resources to enforce idealized digital abstractions. By
relaxing the constraints needed for traditional ASICs, like enforced
statelessness, unidirectionality, determinism, and synchronization, these
devices aim to operate as exact realizations of physical processes, offering
substantial gains in energy efficiency and computational throughput. This
approach enables novel co-design strategies, aligning algorithmic requirements
with the inherent computational primitives of physical systems. Physics-based
ASICs could accelerate critical AI applications like diffusion models,
sampling, optimization, and neural network inference as well as traditional
computational workloads like scientific simulation of materials and molecules.
Ultimately, this vision points towards a future of heterogeneous,
highly-specialized computing platforms capable of overcoming current scaling
bottlenecks and unlocking new frontiers in computational power and efficiency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [102] [MQFQ-Sticky: Fair Queueing For Serverless GPU Functions](https://arxiv.org/abs/2507.08954)
*Alexander Fuerst,Siddharth Anil,Vishakha Dixit,Purushottam,Kulkarni,Prateek Sharma*

Main category: cs.DC

TL;DR: FaaS框架缺乏对GPU加速的天然支持，导致ML和科学计算等应用无法充分利用GPU。本文提出了一个无需修改函数代码的GPU加速FaaS系统，通过调度策略（如公平队列和预调度）优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有FaaS框架无法有效支持GPU加速，阻碍了机器学习和科学计算等应用的性能提升。

Method: 设计并实现了一个黑盒式GPU加速FaaS系统，利用I/O调度原则（如公平队列和预调度）优化函数调度。提出了MQFQ-Sticky方法，平衡局部性、公平性和延迟。

Result: 在多种负载下，MQFQ-Sticky将函数延迟降低了2倍到20倍，优于现有的GPU和CPU队列策略。

Conclusion: 提出的调度方法显著提升了FaaS系统中GPU加速的效率，为动态异构负载提供了可行的解决方案。

Abstract: Hardware accelerators like GPUs are now ubiquitous in data centers, but are
not fully supported by common cloud abstractions such as Functions as a Service
(FaaS). Many popular and emerging FaaS applications such as machine learning
and scientific computing can benefit from GPU acceleration. However, FaaS
frameworks (such as OpenWhisk) are not capable of providing this acceleration
because of the impedance mismatch between GPUs and the FaaS programming model,
which requires virtualization and sandboxing of each function. The challenges
are amplified due to the highly dynamic and heterogeneous FaaS workloads. This
paper presents the design and implementation of a FaaS system for providing GPU
acceleration in a black-box manner (without modifying function code). Running
small functions in containerized sandboxes is challenging due to limited GPU
concurrency and high cold-start overheads, resulting in heavy queueing of
function invocations. We show how principles from I/O scheduling, such as fair
queuing and anticipatory scheduling, can be translated to function scheduling
on GPUs. We develop MQFQ-Sticky, an integrated fair queueing and GPU memory
management approach, which balances the tradeoffs between locality, fairness,
and latency. Empirical evaluation on a range of workloads shows that it reduces
function latency by 2x to 20x compared to existing GPU and CPU queueing
policies.

</details>


### [103] [Lightweight Federated Learning over Wireless Edge Networks](https://arxiv.org/abs/2507.09546)
*Xiangwang Hou,Jingjing Wang,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato*

Main category: cs.DC

TL;DR: 该论文提出了一种轻量级联邦学习（LTFL）框架，通过整合无线传输功率控制、模型剪枝和梯度量化，解决了联邦学习在无线网络中的部署挑战。


<details>
  <summary>Details</summary>
Motivation: 随着智能设备连接无线网络的增长，数据量急剧增加，但集中式机器学习存在通信开销和隐私问题。联邦学习（FL）成为替代方案，但其在无线网络中部署仍具挑战性。

Method: 论文提出LTFL框架，结合传输功率控制、模型剪枝和梯度量化，推导了FL收敛间隙的闭式表达，并设计优化问题以最小化收敛间隙。

Result: 实验表明，LTFL优于现有方案，成功降低了收敛间隙并满足延迟和能量约束。

Conclusion: LTFL为无线网络中的联邦学习提供了一种高效、轻量级的解决方案。

Abstract: With the exponential growth of smart devices connected to wireless networks,
data production is increasing rapidly, requiring machine learning (ML)
techniques to unlock its value. However, the centralized ML paradigm raises
concerns over communication overhead and privacy. Federated learning (FL)
offers an alternative at the network edge, but practical deployment in wireless
networks remains challenging. This paper proposes a lightweight FL (LTFL)
framework integrating wireless transmission power control, model pruning, and
gradient quantization. We derive a closed-form expression of the FL convergence
gap, considering transmission error, model pruning error, and gradient
quantization error. Based on these insights, we formulate an optimization
problem to minimize the convergence gap while meeting delay and energy
constraints. To solve the non-convex problem efficiently, we derive closed-form
solutions for the optimal model pruning ratio and gradient quantization level,
and employ Bayesian optimization for transmission power control. Extensive
experiments on real-world datasets show that LTFL outperforms state-of-the-art
schemes.

</details>


### [104] [Intelligent Task Management via Dynamic Multi-region Division in LEO Satellite Networks](https://arxiv.org/abs/2507.09926)
*Zixuan Song,Zhishu Shen,Xiaoyu Zheng,Qiushi Zheng,Zheng Lei,Jiong Jin*

Main category: cs.DC

TL;DR: 论文提出了一种动态多区域划分框架，用于LEO卫星网络中的智能任务管理，结合遗传算法和深度强化学习，优化任务延迟和资源利用率。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络是未来6G系统的关键组成部分，但其资源有限且任务分布不均，导致链路拥堵和任务处理效率下降，亟需智能管理方案。

Method: 提出基于遗传算法的动态多区域划分框架和MA-DDPG的任务分配与卸载方案，优化路由与资源平衡。

Result: 仿真结果表明，该框架在任务延迟、能耗和完成率上优于对比方法。

Conclusion: 论文提出的动态多区域划分和智能任务管理方案有效提升了LEO网络的性能。

Abstract: As a key complement to terrestrial networks and a fundamental component of
future 6G systems, Low Earth Orbit (LEO) satellite networks are expected to
provide high-quality communication services when integrated with ground-based
infrastructure, thereby attracting significant research interest. However, the
limited satellite onboard resources and the uneven distribution of
computational workloads often result in congestion along inter-satellite links
(ISLs) that degrades task processing efficiency. Effectively managing the
dynamic and large-scale topology of LEO networks to ensure balanced task
distribution remains a critical challenge. To this end, we propose a dynamic
multi-region division framework for intelligent task management in LEO
satellite networks. This framework optimizes both intra- and inter-region
routing to minimize task delay while balancing the utilization of computational
and communication resources. Based on this framework, we propose a dynamic
multi-region division algorithm based on the Genetic Algorithm (GA), which
adaptively adjusts the size of each region based on the workload status of
individual satellites. Additionally, we incorporate an adaptive routing
algorithm and a task splitting and offloading scheme based on Multi-Agent Deep
Deterministic Policy Gradient (MA-DDPG) to effectively accommodate the arriving
tasks. Simulation results demonstrate that our proposed framework outperforms
comparative methods in terms of the task delay, energy consumption per task,
and task completion rate.

</details>


### [105] [FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline](https://arxiv.org/abs/2507.10367)
*Jingwei Xu,Junbin Kang,Mingkai Dong,Mingyu Liu,Lu Zhang,Shaohong Guo,Ziyan Qiu,Mingzhen You,Ziyi Tian,Anqi Yu,Tianhong Ding,Xinwei Hu,Haibo Chen*

Main category: cs.DC

TL;DR: 传统客户端元数据缓存在分布式文件系统中效果不佳且消耗资源，FalconFS采用无状态客户端架构优化深度学习流程。


<details>
  <summary>Details</summary>
Motivation: 解决客户端元数据缓存无效和资源浪费问题。

Method: 采用服务器端路径解析与混合元数据索引、惰性命名空间复制，提升并发性和部署便捷性。

Result: 在小文件读写和深度学习训练中，吞吐量提升5.72倍和12.81倍。

Conclusion: FalconFS在实际生产环境中表现优异。

Abstract: Client-side metadata caching has long been considered an effective method for
accelerating metadata operations in distributed file systems (DFSs). However,
we have found that client-side state (e.g., caching) is not only ineffective
but also consumes valuable memory resources in the deep learning pipelines. We
thus propose FalconFS, a DFS optimized for deep learning pipelines with the
stateless-client architecture. Specifically, instead of performing client-side
path resolution and caching, FalconFS efficiently resolves paths on the server
side using hybrid metadata indexing and lazy namespace replication. FalconFS
also boosts server concurrency with concurrent request merging and provides
easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show
that FalconFS achieves up to 5.72$\times$ throughput for small file read/write
and up to 12.81$\times$ throughput for deep learning model training. FalconFS
has been running in Huawei autonomous driving system's production environment
with 10,000 NPUs for one year.

</details>


### [106] [EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning](https://arxiv.org/abs/2507.10026)
*Zhifei Xu,Zhiqing Tang,Jiong Lou,Zhi Yao,Xuan Xie,Tian Wang,Yinglong Wang,Weijia Jia*

Main category: cs.DC

TL;DR: 论文提出了一种基于边缘协作的AIGC任务调度算法EAT，通过分段任务和强化学习策略优化边缘服务器的资源利用，显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI在云数据中心使用时存在延迟高和资源消耗大的问题，而边缘服务器的部署虽然减少传输时间，但资源利用不足且难以平衡延迟和质量。

Method: 1) 将AIGC任务分段并调度到多台边缘服务器，建模为群调度问题；2) 提出基于强化学习的EAT算法，利用注意力层提取服务器信息并采用扩散策略网络进行调度；3) 开发任务调度系统实现多服务器协同处理。

Result: 实验表明，EAT算法相比基线方法可将推理延迟降低多达56%。

Conclusion: EAT算法有效优化了边缘服务器的资源利用率和任务调度效率，显著提升了AIGC任务的性能。

Abstract: The growth of Artificial Intelligence (AI) and large language models has
enabled the use of Generative AI (GenAI) in cloud data centers for diverse
AI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce
unavoidable delays and substantial resource overhead, which are unsuitable for
users at the network edge with high QoS demands. Deploying AIGC services on
edge servers reduces transmission times but often leads to underutilized
resources and fails to optimally balance inference latency and quality. To
address these issues, this paper introduces a QoS-aware
\underline{E}dge-collaborative \underline{A}IGC \underline{T}ask scheduling
(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to
various edge servers, formulating it as a gang scheduling problem that balances
inference latency and quality while considering server heterogeneity, such as
differing model distributions and cold start issues. 2) We propose a
reinforcement learning-based EAT algorithm that uses an attention layer to
extract load and task queue information from edge servers and employs a
diffusion-based policy network for scheduling, efficiently enabling model
reuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm
to divide tasks and distribute them across multiple edge servers for
processing. Experimental results based on our system and large-scale
simulations show that our EAT algorithm can reduce inference latency by up to
56\% compared to baselines. We release our open-source code at
https://github.com/zzf1955/EAT.

</details>


### [107] [ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism](https://arxiv.org/abs/2507.10069)
*Zedong Liu,Shenggan Cheng,Guangming Tan,Yang You,Dingwen Tao*

Main category: cs.DC

TL;DR: 论文提出了一种名为Elastic Multimodal Parallelism (EMP)的新服务范式，以解决多模态大语言模型(MLLM)在推理时的效率问题。通过ElasticMM系统，实现了动态资源分配、并行调整和缓存优化，显著降低了延迟并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 由于MLLM在推理时需要处理复杂的多模态数据和异构负载，传统的服务架构难以高效应对，导致延迟高和资源利用率低。

Method: 提出EMP范式，开发了ElasticMM系统，包括模态感知负载均衡、弹性分区调度和统一多模态前缀缓存等关键技术。

Result: 实验表明，ElasticMM比现有系统降低了4.2倍的TTFT延迟，提升了3.2-4.5倍的吞吐量，同时满足服务级别目标。

Conclusion: EMP和ElasticMM为多模态大语言模型的高效服务提供了有效解决方案，显著提升了性能和资源利用率。

Abstract: Multimodal large language models (MLLMs) extend LLMs to handle images,
videos, and audio by incorporating feature extractors and projection modules.
However, these additional components -- combined with complex inference
pipelines and heterogeneous workloads -- introduce significant inference
overhead. Therefore, efficiently serving MLLMs remains a major challenge.
Current tightly coupled serving architectures struggle to distinguish between
mixed request types or adapt parallelism strategies to different inference
stages, leading to increased time-to-first-token (TTFT) latency and poor
resource utilization. To address this, we propose Elastic Multimodal
Parallelism (EMP), a new serving paradigm that elastically adapts to resource
heterogeneity across request types and inference stages. Building upon EMP, we
develop ElasticMM, an MLLM serving system that (1) separates requests into
independent modality groups with dynamic resource allocation via a
modality-aware load balancer; (2) decouples inference stages and enables
parallelism adjustment and adaptive scaling via elastic partition scheduling;
and (3) improves inference efficiency through unified multimodal prefix caching
and non-blocking encoding. Experiments on diverse real-world datasets show that
ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by
up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level
objectives (SLOs).

</details>


### [108] [Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality](https://arxiv.org/abs/2507.10139)
*Filipe Miguel Gonçalves de Almeida,CJ Carey,Hendrik Fichtenberger,Jonathan Halcrow,Silvio Lattanzi,André Linhares,Tao Meng,Ashkan Norouzi-Fard,Nikos Parotsidis,Bryan Perozzi,David Simcha*

Main category: cs.DC

TL;DR: 论文介绍了Dynamic Grale Using ScaNN（Dynamic GUS）系统，结合Grale的优点和ScaNN的动态处理能力，支持低延迟的图构建，广泛应用于工业场景。


<details>
  <summary>Details</summary>
Motivation: 现有工具如Grale虽在离线环境下高效，但无法应对数据快速动态更新的需求；而动态ANN系统仅限于单一嵌入相似性。因此需要结合Grale的高质量和动态处理能力。

Method: 提出Dynamic GUS系统，继承Grale的算法优势，并利用ScaNN实现低延迟动态更新。

Result: 系统在Google内部成功部署，其中在Android安全与隐私应用中，有害应用的捕获速度提升了4倍。

Conclusion: Dynamic GUS有效解决了动态图构建的需求，兼具高质量和低延迟，具有广泛的工业应用价值。

Abstract: Learning and constructing large-scale graphs has attracted attention in
recent decades, resulting in a rich literature that introduced various systems,
tools, and algorithms. Grale is one of such tools that is designed for offline
environments and is deployed in more than 50 different industrial settings at
Google. Grale is widely applicable because of its ability to efficiently learn
and construct a graph on datasets with multiple types of features. However, it
is often the case that applications require the underlying data to evolve
continuously and rapidly and the updated graph needs to be available with low
latency. Such setting make the use of Grale prohibitive. While there are
Approximate Nearest Neighbor (ANN) systems that handle dynamic updates with low
latency, they are mostly limited to similarities over a single embedding.
  In this work, we introduce a system that inherits the advantages and the
quality of Grale, and maintains a graph construction in a dynamic setting with
tens of milliseconds of latency per request. We call the system Dynamic Grale
Using ScaNN (Dynamic GUS). Our system has a wide range of applications with
over 10 deployments at Google. One of the applications is in Android Security
and Privacy, where Dynamic Grale Using ScaNN enables capturing harmful
applications 4 times faster, before they can reach users.

</details>


### [109] [Past-Future Scheduler for LLM Serving under SLA Guarantees](https://arxiv.org/abs/2507.10150)
*Ruihao Gong,Shihao Bai,Siyu Wu,Yunqian Fan,Zaijun Wang,Xiuhong Li,Hailong Yang,Xianglong Liu*

Main category: cs.DC

TL;DR: 论文提出了一种名为Past-Future调度器的新方法，通过精确估计内存需求来提升LLM服务框架的效率，并开发了高性能框架LightLLM，验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务框架因请求输出长度的多样性，常因激进或保守的调度策略导致内存估计不准确，从而影响吞吐量和服务质量。为解决这一问题，研究提出了新的调度器设计。

Method: 提出Past-Future调度器，通过历史请求输出长度分布和未来时间点的内存占用计算，精确估计内存需求，平衡请求排队与有害驱逐的权衡。

Result: 实验表明，LightLLM框架在重负载下比其他调度器高出2-3倍的好吞吐量，验证了Past-Future调度器的有效性。

Conclusion: Past-Future调度器能适应多种应用场景，显著提升好吞吐量，推动相关研究发展。

Abstract: The exploration and application of Large Language Models (LLMs) is thriving.
To reduce deployment costs, continuous batching has become an essential feature
in current service frameworks. The effectiveness of continuous batching relies
on an accurate estimate of the memory requirements of requests. However, due to
the diversity in request output lengths, existing frameworks tend to adopt
aggressive or conservative schedulers, which often result in significant
overestimation or underestimation of memory consumption. Consequently, they
suffer from harmful request evictions or prolonged queuing times, failing to
achieve satisfactory throughput under strict Service Level Agreement (SLA)
guarantees (a.k.a. goodput), across various LLM application scenarios with
differing input-output length distributions. To address this issue, we propose
a novel Past-Future scheduler that precisely estimates the peak memory
resources required by the running batch via considering the historical
distribution of request output lengths and calculating memory occupancy at each
future time point. It adapts to applications with all types of input-output
length distributions, balancing the trade-off between request queuing and
harmful evictions, thereby consistently achieving better goodput. Furthermore,
to validate the effectiveness of the proposed scheduler, we developed a
high-performance LLM serving framework, LightLLM, that implements the
Past-Future scheduler. Compared to existing aggressive or conservative
schedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\times$
higher goodput than other schedulers under heavy loads. LightLLM is open source
to boost the research in such direction (https://github.com/ModelTC/lightllm).

</details>


### [110] [Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning](https://arxiv.org/abs/2507.10259)
*Chengze Du,Zhiwei Yu,Heng Xu,Haojie Wang,Bo liu,Jialong Li*

Main category: cs.DC

TL;DR: 该论文提出了一个名为TORTA的两层时空调度框架，用于优化分布式GPU推理基础设施的资源调度，通过结合长期工作负载模式和短期执行约束，显著提升了系统性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有调度系统仅基于当前系统状态做出决策，忽视了任务需求和资源可用性的时间演变，导致GPU利用率低、任务迁移开销高以及动态工作负载下系统响应性差。

Method: TORTA采用两层设计，宏观调度器通过强化学习和最优传输协调区域间任务分配，微观分配器优化区域内任务到服务器的分配以减少延迟和切换成本。

Result: 实验表明，TORTA在多种网络拓扑下将平均推理响应时间减少15%，负载均衡提升4-5%，总运营成本降低10-20%。

Conclusion: TORTA通过时空感知调度框架显著提升了分布式GPU推理的效率和性能，为动态工作负载下的资源调度提供了有效解决方案。

Abstract: The rapid growth of large language model (LLM) services imposes increasing
demands on distributed GPU inference infrastructure. Most existing scheduling
systems rely on the current system state to make decisions, without considering
how task demand and resource availability evolve over time. This lack of
temporal awareness leads to inefficient GPU utilization, high task migration
overhead, and poor system responsiveness under dynamic workloads. In this work,
we identify the fundamental limitations of these instantaneous-state-only
scheduling approaches and propose Temporal Optimal Resource scheduling via
Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling
framework that captures both long-term workload patterns and short-term
execution constraints. It adopts a two-layer design: a macro-level scheduler
leverages reinforcement learning and optimal transport to coordinate
inter-region task distribution, while a micro-level allocator refines
task-to-server assignments within each region to reduce latency and switching
costs. Experimental results across multiple network topologies show that TORTA
reduces average inference response time by up to 15\%, improves load balance by
approximately 4-5\%, and cuts total operational cost by 10-20\% compared to
state-of-the-art baseline methods.

</details>


### [111] [Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters](https://arxiv.org/abs/2507.10392)
*Runsheng Benson Guo,Utkarsh Anand,Khuzaima Daudjee,Rathijit Sen*

Main category: cs.DC

TL;DR: 论文提出了一种名为Zorse的系统，用于在异构GPU集群上高效训练大语言模型，通过结合流水线并行和数据并行，并动态调整配置，显著优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 由于GPU的有限供应和高成本，许多组织无法使用同构集群训练大语言模型。通过利用异构GPU集群可以提升算力，但面临负载均衡、内存优化和通信效率等挑战。

Method: 提出Zorse系统，结合流水线并行和数据并行，支持异构GPU的动态配置，并引入自动规划器优化训练策略。

Result: 实验表明Zorse在异构训练场景中显著优于现有系统。

Conclusion: Zorse为异构GPU集群训练提供了高效解决方案，扩展了训练大语言模型的可行性。

Abstract: Large language models (LLMs) require vast amounts of GPU compute to train,
but limited availability and high costs of GPUs make homogeneous clusters
impractical for many organizations. Instead, assembling heterogeneous clusters
by pooling together GPUs of different generations allows them to achieve higher
aggregate compute and make use of all available GPUs. However, training on
heterogeneous clusters presents several challenges, including load balancing
across GPUs, optimizing memory usage to accommodate varying memory capacities,
and ensuring communication-efficient training over diverse network
interconnects potentially spanning multiple datacenters. In this paper, we make
the case that efficient training on heterogeneous clusters requires (1) the
integration of pipeline parallelism and data parallelism in a manner that is
both communication- and memory-efficient, and (2) a more adaptable
configuration of pipeline and data parallelism, which includes the capability
to flexibly partition GPUs into asymmetric pipeline parallel stages and to
incorporate heterogeneous GPUs within the same data parallelism group. We
propose Zorse, the first system to unify all these capabilities while
incorporating a planner that automatically configures training strategies for a
given workload. Our evaluation shows that Zorse significantly outperforms
state-of-the-art systems in heterogeneous training scenarios.

</details>


### [112] [Consensus, Inconsistency, Emergence: what's paraconsistency got to do with it?](https://arxiv.org/abs/2507.10413)
*Gabriel Rocha*

Main category: cs.DC

TL;DR: FLP不可行定理在广义计算定义下仍然成立，不一致性可能是分布式系统中的涌现特性，而次协调逻辑中平凡性可能是涌现特性。


<details>
  <summary>Details</summary>
Motivation: 探索FLP不可行定理的广义适用性，以及分布式系统中不一致性和次协调逻辑中特性的涌现行为。

Method: 通过广义计算定义和复杂系统理论分析FLP定理，并研究分布式系统的相变和次协调逻辑。

Result: FLP定理在广义计算下仍成立，不一致性可能是涌现特性；次协调逻辑中平凡性可能是涌现特性。

Conclusion: 论文扩展了FLP定理的理解，提出了不一致性和次协调逻辑中平凡性的涌现可能性，并探索了可能的共识算法。

Abstract: The consensus problem, briefly stated, consists of having processes in an
asynchronous distributed system agree on a value. It is widely known that the
consensus problem does not have a deterministic solution that ensures both
termination and consistency, if there is at least one faulty process in the
system. This result, known as the FLP impossibility theorem, led to several
generalizations and developments in theoretical distributed computing. This
paper argues that the FLP impossibility theorem holds even under a generalized
definition of computation through oracles. Furthermore, using a theoretical
machinery from complex systems, this paper also posits that inconsistency may
be an emergent feature of consensus over distributed systems by examining how a
system transitions phases. Under the same complex systems framework, this paper
examines paraconsistent logics, arguing that while inconsistency is not an
emergent feature for these logics, triviality may be. Lastly, some attention is
given to the possibility of developing consensus algorithms capable of
paraconsistent reasoning.

</details>


### [113] [Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout](https://arxiv.org/abs/2507.10430)
*Ji Liu,Beichen Ma,Yang Zhou,Jingbo Zhou,Ruoming Jin,Dejing Dou,Huaiyu Dai,Haixun Wang,Patrick Valduriez*

Main category: cs.DC

TL;DR: FedDHAD是一个联邦学习框架，通过动态异构模型聚合和自适应Dropout方法，解决了数据异构和边缘设备性能限制的问题，显著提升了准确性、效率和计算成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据分布不均（非IID）和设备性能限制导致的模型收敛慢和准确性下降是主要挑战。

Method: 提出FedDH（动态异构模型聚合）和FedAD（自适应Dropout）两种方法，分别处理数据异构和设备性能问题。

Result: 在准确性（最高提升6.7%）、效率（最高快2.02倍）和计算成本（最高降低15.0%）方面优于现有方案。

Conclusion: FedDHAD框架通过结合FedDH和FedAD，有效解决了联邦学习中的关键问题，表现出色。

Abstract: Federated Learning (FL) is a promising distributed machine learning approach
that enables collaborative training of a global model using multiple edge
devices. The data distributed among the edge devices is highly heterogeneous.
Thus, FL faces the challenge of data distribution and heterogeneity, where
non-Independent and Identically Distributed (non-IID) data across edge devices
may yield in significant accuracy drop. Furthermore, the limited computation
and communication capabilities of edge devices increase the likelihood of
stragglers, thus leading to slow model convergence. In this paper, we propose
the FedDHAD FL framework, which comes with two novel methods: Dynamic
Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH
dynamically adjusts the weights of each local model within the model
aggregation process based on the non-IID degree of heterogeneous data to deal
with the statistical data heterogeneity. FedAD performs neuron-adaptive
operations in response to heterogeneous devices to improve accuracy while
achieving superb efficiency. The combination of these two methods makes FedDHAD
significantly outperform state-of-the-art solutions in terms of accuracy (up to
6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to
15.0% smaller).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [114] [Orchestration for Domain-specific Edge-Cloud Language Models](https://arxiv.org/abs/2507.09003)
*Prasoon Patidar,Alex Crown,Kevin Hsieh,Yifei Xu,Tusher Chakraborty,Ranveer Chandra,Yuvraj Agarwal*

Main category: cs.DB

TL;DR: ECO-LLM是一种新型系统，通过联合优化LLM服务流程的组件配置和动态策略选择，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注选择最佳LLM模型，忽略了组件间的协同作用和动态约束，导致性能与成本不平衡。

Method: ECO-LLM通过仿真器和运行时系统，探索配置空间并动态选择最优策略。

Result: ECO-LLM在智能家居和车载场景中，精度提升16%，成本降低90%，延迟减少55%。

Conclusion: 联合优化和动态策略选择能高效满足SLOs，为边缘-云协作提供了新思路。

Abstract: The remarkable performance of Large Language Models (LLMs) has inspired many
applications, which often necessitate edge-cloud collaboration due to
connectivity, privacy, and cost considerations. Traditional methods primarily
focus on selecting the best LLM model for optimizing performance, while
neglecting the critical interplay between the components of the LLM serving
pipeline (context retrieval, query preprocessing, etc.) or the changing latency
and cost constraints. We introduce ECO-LLM (Edge-Cloud Orchestrator for LLMs),
a novel system that reframes this problem as a joint optimization challenge and
solves it by systematically exploring component configurations and dynamically
selecting optimal strategies at the query level. ECO-LLM consists of two
components: (1) the ECO-LLM Emulator, which efficiently explores the vast
configuration space utilizing query clustering and pareto-optimal path
selection, gathering domain-specific performance metrics without exhaustive
evaluation; and (2) the ECO-LLM Runtime, which leverages these metrics to
dynamically select optimal resolution strategies for user queries while meeting
user-defined Service Level Objectives (SLOs). We evaluate ECO-LLM on a smart
home and a smart car assistant scenarios. With an exhaustive exploration of all
possible configurations for seen queries, ECO-LLM outperforms cloud-based
models like GPT-4o in terms of accuracy (90% vs. 74% on average) while reducing
costs by 90% and latency by 55%, demonstrating the value of its joint
optimization at the query level. In practical deployment for previously unseen
queries, ECO-LLM selects configurations that reduce costs by 62% or improve
response times by 62% on average compared to state-of-the-art model routing
approaches, while maintaining higher accuracy and consistently adhering to
specified latency and cost constraints.

</details>


### [115] [HedraRAG: Coordinating LLM Generation and Database Retrieval in Heterogeneous RAG Serving](https://arxiv.org/abs/2507.09138)
*Zhengding Hu,Vibha Murthy,Zaifeng Pan,Wanlu Li,Xiaoyi Fang,Yufei Ding,Yuke Wang*

Main category: cs.DB

TL;DR: 该论文提出HedraRAG系统，通过图抽象优化异构检索增强生成（RAG）服务中的多阶段工作流，实现高效并行与资源利用，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构RAG服务中多阶段工作流和多样化请求模式带来的系统级挑战，优化执行效率。

Method: 基于图抽象的HedraRAG系统，通过动态图变换（如节点分裂、重排序、边添加等）改进执行计划，并在CPU-GPU混合管道中映射实施。

Result: 在多种RAG工作流中实现1.5倍至5倍的加速，优于现有框架。

Conclusion: HedraRAG通过协调生成与检索，在服务环境中显著提升性能和资源利用率。

Abstract: This paper addresses emerging system-level challenges in heterogeneous
retrieval-augmented generation (RAG) serving, where complex multi-stage
workflows and diverse request patterns complicate efficient execution. We
present HedraRAG, a runtime system built on a graph-based abstraction that
exposes optimization opportunities across stage-level parallelism,
intra-request similarity, and inter-request skewness. These opportunities are
realized through dynamic graph transformations, such as node splitting,
reordering, edge addition, and dependency rewiring, applied to wavefronts of
subgraphs spanning concurrent requests. The resulting execution plans are
mapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce
latency. Evaluations across a wide range of RAG workflows demonstrate speedups
exceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the
effectiveness of coordinated generation and retrieval in serving environments.

</details>


### [116] [TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing](https://arxiv.org/abs/2507.09448)
*Pramod Chunduri,Yao Lu,Joy Arulraj*

Main category: cs.DB

TL;DR: Tracer是一个新的视频数据库管理系统（VDBMS），通过自适应查询处理框架高效处理对象重识别（Re-ID）查询，解决了现有系统Spatula的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有系统Spatula在大型摄像头网络中因局部历史数据导致时空过滤准确率有限，且无法支持高召回率的自适应查询处理。

Method: Tracer利用循环网络建模长期历史关联，动态选择最优摄像头，并采用概率自适应搜索模型加速查询。

Result: Tracer在多样化数据集上的性能平均比现有系统提升了3.9倍。

Conclusion: Tracer通过自适应框架显著提升了Re-ID任务的效率和召回率，适用于复杂的大规模监控场景。

Abstract: Efficiently re-identifying and tracking objects across a network of cameras
is crucial for applications like traffic surveillance. Spatula is the
state-of-the-art video database management system (VDBMS) for processing Re-ID
queries. However, it suffers from two limitations. Its spatio-temporal
filtering scheme has limited accuracy on large camera networks due to localized
camera history. It is not suitable for critical video analytics applications
that require high recall due to a lack of support for adaptive query
processing.
  In this paper, we present Tracer, a novel VDBMS for efficiently processing
Re-ID queries using an adaptive query processing framework. Tracer selects the
optimal camera to process at each time step by training a recurrent network to
model long-term historical correlations. To accelerate queries under a high
recall constraint, Tracer incorporates a probabilistic adaptive search model
that processes camera feeds in incremental search windows and dynamically
updates the sampling probabilities using an exploration-exploitation strategy.
To address the paucity of benchmarks for the Re-ID task due to privacy
concerns, we present a novel synthetic benchmark for generating multi-camera
Re-ID datasets based on real-world traffic distribution. Our evaluation shows
that Tracer outperforms the state-of-the-art cross-camera analytics system by
3.9x on average across diverse datasets.

</details>


### [117] [THOR: Transformer Heuristics for On-Demand Retrieval](https://arxiv.org/abs/2507.09592)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.DB

TL;DR: THOR模块是一个将自然语言问题转换为安全的、只读SQL查询的引擎，适用于企业数据库，具有自我修正和结果解释功能。


<details>
  <summary>Details</summary>
Motivation: 为非技术用户提供简单安全的数据库访问方式，避免SQL复杂性。

Method: 采用分层架构，包括查询路由、动态模式检索、SQL生成、自我修正与评分循环以及结果解释。

Result: 在财务、销售和运营场景中展示了可靠的即席查询和自动报告功能。

Conclusion: THOR模块通过模式感知和容错执行，为非技术用户提供了安全便捷的数据访问方案。

Abstract: We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)
Module, designed and implemented by eSapiens, a secure, scalable engine that
transforms natural-language questions into verified, read-only SQL analytics
for enterprise databases. The Text-to-SQL module follows a decoupled
orchestration/execution architecture: a Supervisor Agent routes queries, Schema
Retrieval dynamically injects table and column metadata, and a SQL Generation
Agent emits single-statement SELECT queries protected by a read-only guardrail.
An integrated Self-Correction & Rating loop captures empty results, execution
errors, or low-quality outputs and triggers up to five LLM-driven regeneration
attempts. Finally, a Result Interpretation Agent produces concise,
human-readable insights and hands raw rows to the Insight & Intelligence engine
for visualization or forecasting.
  Smoke tests across finance, sales, and operations scenarios demonstrate
reliable ad-hoc querying and automated periodic reporting. By embedding schema
awareness, fault-tolerant execution, and compliance guardrails, the THOR Module
empowers non-technical users to access live data with zero-SQL simplicity and
enterprise-grade safety.

</details>


### [118] [Rethinking LSM-tree based Key-Value Stores: A Survey](https://arxiv.org/abs/2507.09642)
*Yina Lv,Qiao Li,Quanqing Xu,Congming Gao,Chuanhui Yang,Xiaoli Wang,Chun Jason Xue*

Main category: cs.DB

TL;DR: 这篇论文综述了LSM-tree在键值存储系统中的优化工作，重点分析了过去五年的代表性研究，旨在解决LSM-tree在性能变异性、读写放大等问题上的挑战，并探讨分布式场景下的新机遇。


<details>
  <summary>Details</summary>
Motivation: LSM-tree在写密集型应用中广泛应用，但其不可预测的压缩操作导致性能变异性、读写放大等问题。尽管已有优化研究，但近年来仍不断有新研究涌现，因此有必要对最新的优化工作进行系统综述。

Method: 通过综述过去五年中具有代表性的LSM-tree优化研究工作，分析现有解决方案如何减轻flush和压缩的性能影响，并提升基础键值操作效率。同时，探讨分布式多租户架构下的新挑战与机遇。

Result: 论文总结了LSM-tree优化的最新进展，提供了对现有解决方案的详细分析，并指出了未来研究方向。

Conclusion: 这篇综述填补了现有文献的空白，系统梳理了LSM-tree优化的前沿工作，为研究人员和从业者提供了有价值的参考，并指明了未来的研究趋势。

Abstract: LSM-tree is a widely adopted data structure in modern key-value store systems
that optimizes write performance in write-heavy applications by using append
writes to achieve sequential writes. However, the unpredictability of LSM-tree
compaction introduces significant challenges, including performance variability
during peak workloads and in resource-constrained environments, write
amplification caused by data rewriting during compactions, read amplification
from multi-level queries, trade-off between read and write performance, as well
as efficient space utilization to mitigate space amplification. Prior studies
on LSM-tree optimizations have addressed the above challenges; however, in
recent years, research on LSM-tree optimization has continued to propose. The
goal of this survey is to review LSM-tree optimization, focusing on
representative works in the past five years. This survey first studies existing
solutions on how to mitigate the performance impact of LSM-tree flush and
compaction and how to improve basic key-value operations. In addition,
distributed key-value stores serve multi-tenants, ranging from tens of
thousands to millions of users with diverse requirements. We then analyze the
new challenges and opportunities in these modern architectures and across
various application scenarios. Unlike the existing survey papers, this survey
provides a detailed discussion of the state-of-the-art work on LSM-tree
optimizations and gives future research directions.

</details>


### [119] [Efficient Temporal Simple Path Graph Generation](https://arxiv.org/abs/2507.10017)
*Zhiyang Tang,Yanping Wu,Xiangjun Zai,Chen Chen,Xiaoyang Wang,Ying Zhang*

Main category: cs.DB

TL;DR: 本文研究了在给定时间间隔内从源顶点到目标顶点的所有时序简单路径构成子图（tspG）的问题，并提出了一种高效方法避免直接枚举路径。


<details>
  <summary>Details</summary>
Motivation: 时序图中基于时序路径的顶点关系探索是基础任务，而直接枚举路径计算成本高，亟需高效方法。

Method: 提出了Verification in Upper-bound Graph方法，先通过约束条件排除无关边得到上界图，再用Escape Edges Verification算法精确构建tspG。

Result: 在10个真实世界图上的实验验证了方法的效率和有效性。

Conclusion: 该方法高效解决了tspG构建问题，避免了路径枚举的高计算成本。

Abstract: Interactions between two entities often occur at specific timestamps, which
can be modeled as a temporal graph. Exploring the relationships between
vertices based on temporal paths is one of the fundamental tasks. In this
paper, we conduct the first research to propose and investigate the problem of
generating the temporal simple path graph (tspG), which is the subgraph
consisting of all temporal simple paths from the source vertex to the target
vertex within the given time interval. Directly enumerating all temporal simple
paths and constructing the tspG is computationally expensive. To accelerate the
processing, we propose an efficient method named Verification in Upper-bound
Graph. It first incorporates the temporal path constraint and simple path
constraint to exclude unpromising edges from the original graph, which obtains
a tight upper-bound graph as a high-quality approximation of the tspG in
polynomial time. Then, an Escape Edges Verification algorithm is further
applied in the upper-bound graph to construct the exact tspG without
exhaustively enumerating all temporal simple paths between given vertices.
Finally, comprehensive experiments on 10 real-world graphs are conducted to
demonstrate the efficiency and effectiveness of the proposed techniques.

</details>


### [120] [Breaking the Storage-Compute Bottleneck in Billion-Scale ANNS: A GPU-Driven Asynchronous I/O Framework](https://arxiv.org/abs/2507.10070)
*Yang Xiao,Mo Sun,Ziyu Song,Bing Tian,Jie Zhang,Jie Sun,Zeke Wang*

Main category: cs.DB

TL;DR: FlashANNS是一个基于GPU加速的图近似最近邻搜索系统，通过I/O计算重叠优化性能，显著提升了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有基于磁盘的ANNS系统因无法有效重叠SSD访问与距离计算以及I/O栈性能不足，导致性能不佳。

Method: FlashANNS提出三项创新：依赖松解的异步管道、Warp级并发SSD访问、计算-I/O平衡的图度选择。

Result: 实验显示FlashANNS在单SSD和多SSD配置下分别实现2.3-5.9倍和2.7-12.2倍的吞吐量提升。

Conclusion: FlashANNS通过优化I/O计算重叠，显著提升了ANNS系统的性能，具有广泛应用前景。

Abstract: With the advancement of information retrieval, recommendation systems, and
Retrieval-Augmented Generation (RAG), Approximate Nearest Neighbor Search
(ANNS) gains widespread applications due to its higher performance and
accuracy. While several disk-based ANNS systems have emerged to handle
exponentially growing vector datasets, they suffer from suboptimal performance
due to two inherent limitations: 1) failing to overlap SSD accesses with
distance computation processes and 2) extended I/O latency caused by suboptimal
I/O Stack. To address these challenges, we present FlashANNS, a GPU-accelerated
out-of-core graph-based ANNS system through I/O-compute overlapping. Our core
insight lies in the synchronized orchestration of I/O and computation through
three key innovations: 1) Dependency-Relaxed asynchronous pipeline: FlashANNS
decouples I/O-computation dependencies to fully overlap between GPU distance
calculations and SSD data transfers. 2) Warp-Level concurrent SSD access:
FlashANNS implements a lock-free I/O stack with warp-level concurrency control,
to reduce the latency-induced time overhead. 3) Computation-I/O balanced graph
degree Selection: FlashANNS selects graph degrees via lightweight
compute-to-I/O ratio sampling, ensuring optimal balance between computational
load and storage access latency across different I/O bandwidth configurations.
We implement FlashANNS and compare it with state-of-the-art out-of-core ANNS
systems (SPANN, DiskANN) and a GPU-accelerated out-of-core ANNS system
(FusionANNS). Experimental results demonstrate that at $\geq$95\% recall@10
accuracy, our method achieves 2.3-5.9$\times$ higher throughput compared to
existing SOTA methods with a single SSD, and further attains 2.7-12.2$\times$
throughput improvement in multi-SSD configurations.

</details>


### [121] [LogLite: Lightweight Plug-and-Play Streaming Log Compression](https://arxiv.org/abs/2507.10337)
*Benzhao Tang,Shiyu Yang,Zhitao Shen,Wenjie Zhang,Xuemin Lin,Zhihong Tian*

Main category: cs.DB

TL;DR: LogLite是一种轻量级、即插即用的无损日志压缩算法，显著提高了压缩效率和速度。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统和IoT设备的普及，日志数据量激增，导致存储和收集成本高昂，需要高效的压缩方法。

Method: 通过公开日志数据集的特征分析，提出无预定义规则或预训练的LogLite算法，适用于TEXT和JSON日志。

Result: LogLite在大多数场景下实现Pareto最优，压缩比提升67.8%，压缩速度提升2.7倍。

Conclusion: LogLite是一种高效且适应性强的日志压缩解决方案，适用于动态日志结构。

Abstract: Log data is a vital resource for capturing system events and states. With the
increasing complexity and widespread adoption ofmodern software systems and IoT
devices, the daily volume of log generation has surged to tens of petabytes,
leading to significant collection and storage costs. To address this challenge,
lossless log compression has emerged as an effective solution, enabling
substantial resource savings without compromising log information. In this
paper, we first conduct a characterization study on extensive public log
datasets and identify four key observations. Building on these insights, we
propose LogLite, a lightweight, plug-and-play, streaming lossless compression
algorithm designed to handle both TEXT and JSON logs throughout their life
cycle. LogLite requires no predefined rules or pre-training and is inherently
adaptable to evolving log structures. Our evaluation shows that, compared to
state-of-the-art baselines, LogLite achieves Pareto optimality in most
scenarios, delivering an average improvement of up to 67.8% in compression
ratio and up to 2.7 $\times$ in compression speed.

</details>


### [122] [Instance-Optimized String Fingerprints](https://arxiv.org/abs/2507.10391)
*Mihail Stoian,Johannes Thürauf,Andreas Zimmerer,Alexander van Renen,Andreas Kipf*

Main category: cs.DB

TL;DR: 论文提出了一种名为“字符串指纹”的轻量级二级索引结构，用于近似处理LIKE谓词，以减少计算和I/O开销，并在DuckDB中实现了1.36倍的加速效果。


<details>
  <summary>Details</summary>
Motivation: 云数据仓库中字符串列的处理效率较低，现有技术（如字典编码和前缀分区修剪）能力有限。因此，需要一种更高效的方法来优化字符串列的处理。

Method: 通过引入字符串指纹作为二级索引结构，并结合混合整数优化技术，针对特定负载进行优化。

Result: 在DuckDB v1.3中，IMDb列的扫描速度提升了1.36倍。

Conclusion: 字符串指纹是一种有效的优化字符串处理的方法，尤其适用于列式查询引擎。

Abstract: Recent research found that cloud data warehouses are text-heavy. However,
their capabilities for efficiently processing string columns remain limited,
relying primarily on techniques like dictionary encoding and prefix-based
partition pruning. In recent work, we introduced string fingerprints - a
lightweight secondary index structure designed to approximate LIKE predicates,
albeit with false positives. This approach is particularly compelling for
columnar query engines, where fingerprints can help reduce both compute and I/O
overhead. We show that string fingerprints can be optimized for specific
workloads using mixed-integer optimization, and that they can generalize to
unseen table predicates. On an IMDb column evaluated in DuckDB v1.3, this
yields table-scan speedups of up to 1.36$\times$.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [123] [CEO-DC: An Actionable Framework to Close the Carbon Gap in HPC Data Centers](https://arxiv.org/abs/2507.08923)
*Rubén Rodríguez Álvarez,Denisa-Andreea Constantinescu,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: CEO-DC模型研究数据中心碳排放与经济优化的决策方法，结果显示设备升级可减排但经济激励不足。


<details>
  <summary>Details</summary>
Motivation: 数据中心的快速扩张导致能源消耗和碳排放激增，硬件效率提升是否能抵消需求增长仍存疑。

Method: 提出CEO-DC模型，整合成本、碳排放和计算需求，优化采购与设备升级策略。

Result: 4年周期升级设备可减排但需经济激励，且碳价格不足；能效优化可能增加采用门槛。

Conclusion: CEO-DC为设备升级、政策激励和可持续增长提供决策支持。

Abstract: The rapid expansion of data centers (DCs) to support large-scale AI and
scientific workloads is driving unsustainable growth in energy consumption and
greenhouse gas emissions. While successive generations of hardware platforms
have improved performance and energy efficiency, the question remains whether
new, more efficient platforms can realistically offset the rising emissions
associated with increasing demand. Prior studies often overlook the complex
trade-offs in such transitions by failing to account for both the economic
incentives and the projected compute demand growth over the operational
lifetime of the devices. In response, we present CEO-DC, an integrated model
and decision-making methodology for Carbon and Economy Optimization in Data
Centers. CEO-DC models the competing forces of cost, carbon, and compute demand
to guide optimal platform procurement and replacement strategies. We propose
metrics to steer procurement, platform design, and policy decisions toward
sustainable DC technologies. Given current platform trends, our AI case study
using CEO-DC shows that upgrading legacy devices on a 4-year cycle reduces
total emissions. However, these upgrades fail to scale with DC demand growth
trends without increasing total emissions in over 44% of cases, and require
economic incentives for adoption in over 72%. Furthermore, current carbon
prices are insufficient to motivate upgrades in 9 out of the 14 countries with
the highest number of DCs globally. We also find that optimizing platforms for
energy efficiency at the expense of latency can increase the carbon price
required to justify their adoption. In summary, CEO-DC provides actionable
insights for DC architects, platform designers, and policymakers by timing
legacy platform upgrades, constraining DC growth to sustainable levels,
optimizing platform performance-to-cost ratios, and increasing incentives.

</details>


### [124] [Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference](https://arxiv.org/abs/2507.09010)
*Chun-Ting Chen,HanGyeol Mun,Jian Meng,Mohamed S. Abdelfattah,Jae-sun Seo*

Main category: cs.AR

TL;DR: 本文提出了一种基于混合脉动阵列（HSA）的边缘LLM推理加速器，通过优化数据流和量化技术，显著提升了推理效率和硬件利用率。


<details>
  <summary>Details</summary>
Motivation: 边缘LLM推理需要高面积效率、低外部存储器访问（EMA）和高能效，以支持低延迟和低成本。

Method: 采用混合脉动阵列架构，结合MXINT4权重量化和优化数据流，减少EMA；设计专用硬件单元（如RMSNorm和RoPE）以降低非线性操作的开销。

Result: 在1.3B LLM的长输入/长输出场景中，达到247/117（token/s/mm2），性能提升2.45倍/13.5倍，同时保持高能效。

Conclusion: 该加速器在边缘环境中实现了高效、低延迟的LLM推理，为实际应用提供了可行解决方案。

Abstract: Edge inference for large language models (LLM) offers secure, low-latency,
and cost-effective inference solutions. We emphasize that an edge accelerator
should achieve high area efficiency and minimize external memory access (EMA)
during the memory-bound decode stage, while maintaining high energy efficiency
during the compute intensive prefill stage. This paper proposes an edge LLM
inference accelerator featuring a hybrid systolic array (HSA) architecture that
optimizes inference efficiency in both stages. To further reduce EMA, we adopt
MXINT4 weight quantization and propose an optimized dataflow tailored for HSA,
ensuring negligible dequantization overhead and achieving 100% hardware
utilization with minimal accuracy loss under edge DRAM bandwidth constraints.
For non-linear operations, we incorporate optimized root mean square
normalization (RMSNorm) and rotary position embedding (RoPE) units, reducing
their latency, area, and memory access overhead while enabling end-to-end
inference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while
running a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x
improvement over existing approaches, while maintaining superior energy
efficiency in token generation.

</details>


### [125] [SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding](https://arxiv.org/abs/2507.09201)
*Weihong Xu,Haein Choi,Po-kai Hsu,Shimeng Yu,Tajana Rosing*

Main category: cs.AR

TL;DR: SLIM是一种面向边缘设备的稀疏大型语言模型（LLM）推理优化算法-硬件协同设计，通过自适应阈值算法和异构硬件架构显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现有加速器未能充分利用LLM操作中的显著稀疏性，导致硬件资源利用不足，而SLIM提出了一种解决方案，旨在降低计算开销和能耗。

Method: SLIM采用自适应阈值算法实现运行时可配置的稀疏性，结合了近存储处理（NSP）和内存内处理（PIM）的异构硬件架构。

Result: SLIM比SSD-GPU系统吞吐量提升13-18倍，比DRAM-GPU系统能效提升9-10倍，同时保持了低延迟。

Conclusion: SLIM为边缘计算环境提供了一种经济高效的LLM部署方案，显著优化了资源利用和能效。

Abstract: Large language models (LLMs) have demonstrated exceptional proficiency in
understanding and generating human language, but efficient inference on
resource-constrained embedded devices remains challenging due to large model
sizes and memory-intensive operations in feedforward network (FFN) and
multi-head attention (MHA) layers. While existing accelerators offload LLM
inference to expensive heterogeneous computing systems, they fail to exploit
the significant sparsity inherent in LLM operations, leaving hardware resources
underutilized. We propose SLIM, an algorithm-hardware co-design optimized for
sparse LLM serving on edge devices. SLIM exploits LLM sparsity through an
adaptive thresholding algorithm that enables runtime-configurable sparsity with
negligible accuracy loss, fetching only activated neurons to dramatically
reduce data movement. Our heterogeneous hardware architecture strategically
combines near-storage processing (NSP) and processing-in-memory (PIM): FFN
weights are stored in high-density 3D NAND and computed using NSP units, while
memory-intensive MHA operations are processed in PIM modules. This design
significantly reduces memory footprint, data movement, and energy consumption.
Our comprehensive evaluation demonstrates SLIM's effectiveness, achieving
13-18x throughput improvements over SSD-GPU systems and 9-10x better energy
efficiency over DRAM-GPU systems while maintaining low latency, making
cost-effective LLM deployment viable for edge computing environments.

</details>


### [126] [Tools and Methodologies for System-Level Design](https://arxiv.org/abs/2507.09660)
*Shuvra S. Bhattacharyya,Marilyn Wolf*

Main category: cs.AR

TL;DR: 这篇论文讨论了系统级设计在芯片设计中的重要性，介绍了相关工具和方法，并通过视频和神经网络两个应用示例展示了系统级设计的关键方面。


<details>
  <summary>Details</summary>
Motivation: 随着芯片设计成为系统级设计的核心，设计师需要更全面的工具套件来应对设计周期长和错误难以修正的挑战。

Method: 论文采用建模、仿真、设计空间探索和验证等方法，并研究了计算模型以描述数字系统。

Result: 通过视频和神经网络两个应用示例，论文展示了系统级设计工具在功能验证和性能分析中的实际应用。

Conclusion: 系统级设计工具在芯片设计中至关重要，能够有效支持复杂的计算模型和设计验证。

Abstract: System-level design, once the province of board designers, has now become a
central concern for chip designers. Because chip design is a less forgiving
design medium -- design cycles are longer and mistakes are harder to correct --
system-on-chip designers need a more extensive tool suite than may be used by
board designers and a variety of tools and methodologies have been developed
for system-level design of systems-on-chips (SoCs). System-level design is less
amenable to synthesis than are logic or physical design. As a result,
system-level tools concentrate on modeling, simulation, design space
exploration, and design verification. The goal of modeling is to correctly
capture the system's operational semantics, which helps with both
implementation and verification. The study of models of computation provides a
framework for the description of digital systems. Not only do we need to
understand a particular style of computation, such as dataflow, but we also
need to understand how different models of computation can reliably communicate
with each other. Design space exploration tools, such as hardware/software
co-design, develop candidate designs to understand trade-offs. Simulation can
be used not only to verify functional correctness but also to supply
performance and power/energy information for design analysis. This chapter
employs two applications -- video and neural networks -- as examples. Both are
leading-edge applications that illustrate many important aspects of
system-level design.

</details>


### [127] [Efficient FRW Transitions via Stochastic Finite Differences for Handling Non-Stratified Dielectrics](https://arxiv.org/abs/2507.09730)
*Jiechen Huang,Wenjian Yu*

Main category: cs.AR

TL;DR: 论文提出了一种名为MicroWalk的算法，用于在复杂的非分层介质中实现准确且高效的电容量提取。


<details>
  <summary>Details</summary>
Motivation: 现有随机游走（FRW）方法在复杂介质中精度不足，需要一种新的方法来解决这一问题。

Method: 提出MicroWalk算法，结合有限差分法和高效率的过渡策略，处理任意介质结构。

Result: 实验表明，新方法在精度和效率上均有显著优势（802倍提速）。

Conclusion: MicroWalk为复杂介质中的电容量提取提供了一种高精度且高效的新解决方案。

Abstract: The accuracy of floating-random-walk (FRW) based capacitance extraction
stands only when the recursive FRW transitions are sampled unbiasedly according
to surrounding dielectrics. Advanced technology profiles, featuring complicated
non-stratified dielectrics, challenge the accuracy of existing FRW transition
schemes that approximate dielectrics with stratified or eight-octant patterns.
In this work, we propose an algorithm named MicroWalk, enabling accurate FRW
transitions for arbitrary dielectrics while keeping high efficiency. It is
provably unbiased and equivalent to using transition probabilities solved by
finite difference method, but at orders of magnitude lower cost (802$\times$
faster). An enhanced 3-D capacitance solver is developed with a hybrid strategy
for complicated dielectrics, combining MicroWalk with the special treatment for
the first transition cube and the analytical algorithm for stratified cubes.
Experiments on real-world structures show that our solver achieves a
significant accuracy advantage over existing FRW solvers, while preserving high
efficiency.

</details>


### [128] [Low-Cost Fuel Dispenser Prototype Using STM32 and an H-bridge motor driver](https://arxiv.org/abs/2507.09774)
*MD Zobaer Hossain Bhuiyan,Abir Bin Faruque,Mahtab Newaz,Mohammad Abdul Qayum*

Main category: cs.AR

TL;DR: 该论文设计了一种基于STM32微控制器和L298N电机驱动器的低成本燃料分配系统原型，适用于偏远或小规模环境。


<details>
  <summary>Details</summary>
Motivation: 在传统高成本系统不可行的情况下，为燃料输送提供经济实惠且可扩展的解决方案。

Method: 使用STM32微控制器管理用户输入和显示数据，通过L298N电机驱动控制12V DC泵电机，模拟燃料分配机制。

Result: 系统能够精确控制燃料分配量，展示了嵌入式系统在构建经济高效、用户友好和节能解决方案方面的潜力。

Conclusion: 该设计可进一步扩展，如添加流量传感器、GSM连接和支付集成，适用于加油站或农业应用。

Abstract: This paper presents the design and development of a low-cost fuel dispensing
system prototype based on the STM32 microcontroller and L298N motor driver. The
system aims to provide an affordable and scalable solution for fuel delivery in
remote or small-scale environments where conventional, high-cost systems are
not feasible. The core control unit is built using an STM32 microcontroller,
which manages user input through a 4x4 matrix keypad and displays operational
data on a 16x4 LCD screen via I2C communication. A 12V DC pump motor is used to
simulate the fuel dispensing mechanism, precisely controlled via the dual
H-bridge L298N motor driver. The system is powered by a 11.1V battery and is
designed for ease of deployment and portability. The keypad allows users to
input the desired fuel amount, while the system ensures accurate motor runtime
corresponding to the volume to be dispensed. This project demonstrates how
embedded systems can be leveraged to build cost-effective, user-friendly, and
energy-efficient solutions. The proposed design can be further enhanced with
flow sensors, GSM connectivity, RFID cards, and payment integration for
real-world applications in fuel stations or agricultural use.

</details>


### [129] [BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs](https://arxiv.org/abs/2507.09780)
*Feilong Qiaoyuan,Jihe Wang,Zhiyu Sun,Linying Wu,Yuanhua Xiao,Danghui Wang*

Main category: cs.AR

TL;DR: 提出了一种新的MAC单元设计，通过利用量化DNN中的双因子稀疏性，解决了部分乘积爆炸和MAC单元利用率低的问题，提高了面积和能效。


<details>
  <summary>Details</summary>
Motivation: 量化DNN中的比特级稀疏性为优化MAC操作提供了潜力，但现有方法无法同时利用双因子的稀疏性，且同步调度方案的灵活性不足，导致MAC单元利用率低下。

Method: 设计了一种基于partialization的MAC单元，通过简单控制逻辑解决部分乘积爆炸问题；引入准同步调度方案，增加MAC阵列的周期级弹性，减少流水线停顿。

Result: 精确版MAC单元面积效率提升29.2%，近似版在精确版基础上能效提高7.5%。

Conclusion: 新设计显著优化了MAC单元的性能，同时支持进一步硬件简化，适用于DNN加速。

Abstract: Bit-level sparsity in quantized deep neural networks (DNNs) offers
significant potential for optimizing Multiply-Accumulate (MAC) operations.
However, two key challenges still limit its practical exploitation. First,
conventional bit-serial approaches cannot simultaneously leverage the sparsity
of both factors, leading to a complete waste of one factor' s sparsity. Methods
designed to exploit dual-factor sparsity are still in the early stages of
exploration, facing the challenge of partial product explosion. Second, the
fluctuation of bit-level sparsity leads to variable cycle counts for MAC
operations. Existing synchronous scheduling schemes that are suitable for
dual-factor sparsity exhibit poor flexibility and still result in significant
underutilization of MAC units. To address the first challenge, this study
proposes a MAC unit that leverages dual-factor sparsity through the emerging
particlization-based approach. The proposed design addresses the issue of
partial product explosion through simple control logic, resulting in a more
area- and energy-efficient MAC unit. In addition, by discarding less
significant intermediate results, the design allows for further hardware
simplification at the cost of minor accuracy loss. To address the second
challenge, a quasi-synchronous scheme is introduced that adds cycle-level
elasticity to the MAC array, reducing pipeline stalls and thereby improving MAC
unit utilization. Evaluation results show that the exact version of the
proposed MAC array architecture achieves a 29.2% improvement in area efficiency
compared to the state-of-the-art bit-sparsity-driven architecture, while
maintaining comparable energy efficiency. The approximate variant further
improves energy efficiency by 7.5%, compared to the exact version. Index-Terms:
DNN acceleration, Bit-level sparsity, MAC unit

</details>


### [130] [Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving](https://arxiv.org/abs/2507.10178)
*Wonung Kim,Yubin Lee,Yoonsung Kim,Jinwoo Hwang,Seongryong Oh,Jiyong Jung,Aziz Huseynov,Woong Gyu Park,Chang Hyun Park,Divya Mahajan,Jongse Park*

Main category: cs.AR

TL;DR: 论文探讨了Transformer和post-transformer LLMs的性能特点，提出了一种统一的框架Pimba，通过SPUs高效支持两者，显著提升了生成吞吐量。


<details>
  <summary>Details</summary>
Motivation: 由于Transformer的算力和内存成本随序列长度增加而增长，影响了长文本推理的可扩展性，因此需要一种能同时高效支持Transformer和post-transformer LLMs的框架。

Method: 分析了Transformer和post-transformer LLMs的性能瓶颈，设计了Pimba系统，包含共享的State-update Processing Units (SPUs)，使用MX量化算法高效执行状态更新和注意力操作。

Result: 与优化的GPU和GPU+PIM系统相比，Pimba的token生成吞吐量分别提高了3.2倍和2.1倍。

Conclusion: Pimba成功解决了Transformer和post-transformer LLMs的统一高效服务问题，为LLM推理提供了更高的性能。

Abstract: Transformers are the driving force behind today's Large Language Models
(LLMs), serving as the foundation for their performance and versatility. Yet,
their compute and memory costs grow with sequence length, posing scalability
challenges for long-context inferencing. In response, the algorithm community
is exploring alternative architectures, such as state space models (SSMs),
linear attention, and recurrent neural networks (RNNs), which we refer to as
post-transformers. This shift presents a key challenge: building a serving
system that efficiently supports both transformer and post-transformer LLMs
within a unified framework. To address this challenge, we analyze the
performance characteristics of transformer and post-transformer LLMs. Despite
their algorithmic differences, both are fundamentally limited by memory
bandwidth under batched inference due to attention in transformers and state
updates in post-transformers. Further analyses suggest two additional insights:
(1) state update operations, unlike attention, incur high hardware cost, making
per-bank PIM acceleration inefficient, and (2) different low-precision
arithmetic methods offer varying accuracy-area tradeoffs, while we identify
Microsoft's MX as the Pareto-optimal choice. Building on these insights, we
design Pimba as an array of State-update Processing Units (SPUs), each shared
between two banks to enable interleaved access to PIM. Each SPU includes a
State-update Processing Engine (SPE) that comprises element-wise multipliers
and adders using MX-based quantized arithmetic, enabling efficient execution of
state update and attention operations. Our evaluation shows that, compared to
LLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x
higher token generation throughput, respectively.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [131] [The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations](https://arxiv.org/abs/2507.08908)
*M. Z. Naser*

Main category: cs.CY

TL;DR: 本文探讨工程师如何在法律框架内部署机器学习技术，建议通过类比推理将其嵌入现有工程准则，同时保持专业责任和安全要求。


<details>
  <summary>Details</summary>
Motivation: 工程行业尚未完全采用基于机器学习的方法，导致工程师和利益相关者对法律和监管框架的不确定性。

Method: 通过分析现有法律责任学说和法庭判例，提供将机器学习整合到工程实践中的法律框架。

Result: 提出一个法律框架，帮助利益相关者评估机器学习驱动解决方案的责任、义务和益处。

Conclusion: 需要理解技术论证与法律先例的相互作用，以形成对机器学习在工程实践中合法性的明智立场。

Abstract: Despite the widespread interest in machine learning (ML), the engineering
industry has not yet fully adopted ML-based methods, which has left engineers
and stakeholders uncertain about the legal and regulatory frameworks that
govern their decisions. This gap remains unaddressed as an engineer's
decision-making process, typically governed by professional ethics and
practical guidelines, now intersects with complex algorithmic outputs. To
bridge this gap, this paper explores how engineers can navigate legal
principles and legislative justifications that support and/or contest the
deployment of ML technologies. Drawing on recent precedents and experiences
gained from other fields, this paper argues that analogical reasoning can
provide a basis for embedding ML within existing engineering codes while
maintaining professional accountability and meeting safety requirements. In
exploring these issues, the discussion focuses on established liability
doctrines, such as negligence and product liability, and highlights how courts
have evaluated the use of predictive models. We further analyze how legislative
bodies and standard-setting organizations can furnish explicit guidance
equivalent to prior endorsements of emergent technologies. This exploration
stresses the vitality of understanding the interplay between technical
justifications and legal precedents for shaping an informed stance on ML's
legitimacy in engineering practice. Finally, our analysis catalyzes a legal
framework for integrating ML through which stakeholders can critically assess
the responsibilities, liabilities, and benefits inherent in ML-driven
engineering solutions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [132] [Hybrid Quantum Security for IPsec](https://arxiv.org/abs/2507.09288)
*Javier Blanco-Romero,Pedro Otero García,Daniel Sobral-Blanco,Florina Almenares Mendoza,Ana Fernández Vilas,Manuel Fernández-Veiga*

Main category: cs.CR

TL;DR: 本文系统比较了序列化与并行混合QKD-PQC密钥建立策略，提出两种将QKD集成到IKEv2的新方法，证明并行方法显著优于序列方法。


<details>
  <summary>Details</summary>
Motivation: 量子密钥分发（QKD）虽提供理论上安全的密钥，但将其融入现有安全协议仍具挑战性。

Method: 提出两种方法：纯QKD和统一QKD-KEM抽象，支持并行组合量子与后量子密码学方法。

Result: 并行混合方法显著提升了性能；纯QKD通过标识密钥协调减少带宽开销。

Conclusion: 提供的实现适用于需要深度防御的关键基础设施部署，展现了量子增强IPsec的实用性。

Abstract: Quantum Key Distribution (QKD) offers information-theoretic security against
quantum computing threats, but integrating QKD into existing security protocols
remains an unsolved challenge due to fundamental mismatches between
pre-distributed quantum keys and computational key exchange paradigms. This
paper presents the first systematic comparison of sequential versus parallel
hybrid QKD-PQC key establishment strategies for IPsec, revealing fundamental
protocol design principles that extend beyond specific implementations. We
introduce two novel approaches for incorporating QKD into Internet Key Exchange
version 2 (IKEv2) with support for both ETSI GS QKD 004 stateful and ETSI GS
QKD 014 stateless API specifications: (1) a pure QKD approach that replaces
computational key derivation with identifier-based quantum key coordination,
and (2) a unified QKD-KEM abstraction that enables parallel composition of
quantum and post-quantum cryptographic methods within existing protocol
frameworks. Our key insight is that parallel hybrid approaches eliminate the
multiplicative latency penalties inherent in sequential methods mandated by RFC
9370, achieving significant performance improvements under realistic network
conditions. Performance evaluation using a Docker-based testing framework with
IDQuantique QKD hardware demonstrates that the parallel hybrid approach
significantly outperforms sequential methods under network latency conditions,
while pure QKD achieves minimal bandwidth overhead through identifier-based key
coordination. Our implementations provide practical quantum-enhanced IPsec
solutions suitable for critical infrastructure deployments requiring
defense-in-depth security.

</details>


### [133] [Implementing and Evaluating Post-Quantum DNSSEC in CoreDNS](https://arxiv.org/abs/2507.09301)
*Julio Gento Suela,Javier Blanco-Romero,Florina Almenares Mendoza,Daniel Díaz-Sánchez*

Main category: cs.CR

TL;DR: 该论文将后量子密码（PQC）算法集成到CoreDNS中，以实现量子安全的DNSSEC功能，支持五种PQC签名算法，并评估其性能与安全性权衡。


<details>
  <summary>Details</summary>
Motivation: 量子计算机的出现威胁到依赖RSA和ECDSA等算法的安全服务，因此需要部署量子安全的密码学解决方案。

Method: 开发了一个插件，在CoreDNS中集成了五种PQC签名算法家族（ML-DSA、FALCON、SPHINCS+、MAYO和SNOVA），并保持与现有DNS解析流程的兼容性。

Result: 性能评估显示PQC算法在安全性和效率之间存在显著权衡，部分算法适合过渡到量子安全的DNSSEC。

Conclusion: 尽管PQC算法增加了操作开销，但部分候选算法为DNSSEC向量子安全密码学的过渡提供了可行方案。

Abstract: The emergence of quantum computers poses a significant threat to current
secure service, application and/or protocol implementations that rely on RSA
and ECDSA algorithms, for instance DNSSEC, because public-key cryptography
based on number factorization or discrete logarithm is vulnerable to quantum
attacks. This paper presents the integration of post-quantum cryptographic
(PQC) algorithms into CoreDNS to enable quantum-resistant DNSSEC functionality.
We have developed a plugin that extends CoreDNS with support for five PQC
signature algorithm families: ML-DSA, FALCON, SPHINCS+, MAYO, and SNOVA. Our
implementation maintains compatibility with existing DNS resolution flows while
providing on-the-fly signing using quantum-resistant signatures. A benchmark
has been performed and performance evaluation results reveal significant
trade-offs between security and efficiency. The results indicate that while PQC
algorithms introduce operational overhead, several candidates offer viable
compromises for transitioning DNSSEC to quantum-resistant cryptography.

</details>


### [134] [SmartphoneDemocracy: Privacy-Preserving E-Voting on Decentralized Infrastructure using Novel European Identity](https://arxiv.org/abs/2507.09453)
*Michał Jóźwik,Johan Pouwelse*

Main category: cs.CR

TL;DR: 论文摘要介绍了一种名为SmartphoneDemocracy的新型电子投票协议，结合了欧洲数字身份钱包、零知识证明和点对点区块链技术，旨在提供安全、隐私保护且可验证的智能手机投票方案。


<details>
  <summary>Details</summary>
Motivation: 当前电子投票系统依赖中心化架构，存在单点故障和过度信任问题，违背民主原则，需要一种更安全的去中心化方案。

Method: 结合欧洲数字身份钱包（EUDI Wallet）防止身份伪造，零知识证明保护隐私，TrustChain区块链作为无服务器的公共公告板。

Result: 协议设计详实，安全分析完善，性能评估显示计算和网络开销适用于中大规模选举。

Conclusion: 该系统为公民提供了可信、易用且用户可控的数字化投票体验。

Abstract: The digitization of democratic processes promises greater accessibility but
presents challenges in terms of security, privacy, and verifiability. Existing
electronic voting systems often rely on centralized architectures, creating
single points of failure and forcing too much trust in authorities, which
contradicts democratic principles. This research addresses the challenge of
creating a secure, private e-voting system with minimized trust dependencies
designed for the most versatile personal device: the smartphone. We introduce
SmartphoneDemocracy, a novel e-voting protocol that combines three key
technologies: the emerging European Digital Identity (EUDI) Wallet for
Sybil-resistant identity verification, Zero-Knowledge Proofs for
privacy-preserving validation, and a peer-to-peer blockchain (TrustChain) for a
resilient, serverless public bulletin board. Our protocol enables voters to
register and cast ballots anonymously and verifiably directly from their
smartphones. We provide a detailed protocol design, a security analysis against
a defined threat model, and a performance evaluation demonstrating that the
computational and network overhead is feasible for medium- to large-scale
elections. By developing and prototyping this system, we demonstrate a viable
path to empower citizens with a trustworthy, accessible, and user-controlled
digital voting experience.

</details>


### [135] [PromptChain: A Decentralized Web3 Architecture for Managing AI Prompts as Digital Assets](https://arxiv.org/abs/2507.09579)
*Marc Bara*

Main category: cs.CR

TL;DR: PromptChain是一个去中心化的Web3架构，将AI提示作为数字资产，具有可验证的所有权、版本控制和变现能力，解决了当前中心化平台的问题。


<details>
  <summary>Details</summary>
Motivation: 当前中心化平台缺乏对提示创作者的合理归属、质量保证和公平补偿机制，PromptChain旨在通过去中心化技术解决这些问题。

Method: 利用IPFS进行不可变存储、智能合约实现治理、代币激励机制进行社区管理，包括跨模型兼容的元数据模式、权益加权验证机制和贡献者按比例奖励的代币经济。

Result: PromptChain证明了去中心化系统在效率上可媲美中心化系统，同时提供更好的所有权保障和抗审查能力。

Conclusion: 该架构为Web3时代的人机协作奠定了基础，首次系统性地将提示视为独立数字资产并提供了专用的去中心化基础设施。

Abstract: We present PromptChain, a decentralized Web3 architecture that establishes AI
prompts as first-class digital assets with verifiable ownership, version
control, and monetization capabilities. Current centralized platforms lack
mechanisms for proper attribution, quality assurance, or fair compensation for
prompt creators. PromptChain addresses these limitations through a novel
integration of IPFS for immutable storage, smart contracts for governance, and
token incentives for community curation. Our design includes: (1) a
comprehensive metadata schema for cross-model compatibility, (2) a
stake-weighted validation mechanism to align incentives, and (3) a token
economy that rewards contributors proportionally to their impact. The proposed
architecture demonstrates how decentralized systems could potentially match
centralized alternatives in efficiency while providing superior ownership
guarantees and censorship resistance through blockchain-anchored provenance
tracking. By decoupling prompts from specific AI models or outputs, this work
establishes the foundation for an open ecosystem of human-AI collaboration in
the Web3 era, representing the first systematic treatment of prompts as
standalone digital assets with dedicated decentralized infrastructure.

</details>


### [136] [Endorsement-Driven Blockchain SSI Framework for Dynamic IoT Ecosystems](https://arxiv.org/abs/2507.09859)
*Guntur Dharma Putra,Bagus Rakadyanto Oktavianto Putra*

Main category: cs.CR

TL;DR: 提出一种基于区块链的自主权身份框架，支持任何有可信链的个人作为凭证颁发者，适用于动态物联网环境。


<details>
  <summary>Details</summary>
Motivation: 现有自主权身份框架通常限制凭证颁发和撤销给可信实体，限制动态物联网生态系统的灵活性。

Method: 采用区块链作为可验证数据注册表，结合分层架构和智能合约自动化关键流程。

Result: 概念验证显示该框架可行且性能开销低，适合资源受限的物联网环境。

Conclusion: 该框架实现了去中心化且可扩展的身份管理，适用于动态物联网生态系统。

Abstract: Self-Sovereign Identity (SSI) offers significant potential for managing
identities in the Internet of Things (IoT), enabling decentralized
authentication and credential management without reliance on centralized
entities. However, existing SSI frameworks often limit credential issuance and
revocation to trusted entities, such as IoT manufacturers, which restricts
flexibility in dynamic IoT ecosystems. In this paper, we propose a
blockchain-based SSI framework that allows any individual with a verifiable
trust linkage to act as a credential issuer, ensuring decentralized and
scalable identity management. Our framework incorporates a layered
architecture, where trust is dynamically established through endorsement-based
calculations and maintained via a hierarchical chain-of-trust mechanism.
Blockchain serves as the Verifiable Data Registry, ensuring transparency and
immutability of identity operations, while smart contracts automate critical
processes such as credential issuance, verification, and revocation. A
proof-of-concept implementation demonstrates that the proposed framework is
feasible and incurs minimal overheads compared to the baseline, making it
well-suited for dynamic and resource-constrained IoT environments.

</details>


### [137] [DNS Tunneling: Threat Landscape and Improved Detection Solutions](https://arxiv.org/abs/2507.10267)
*Novruz Amirov,Baran Isik,Bilal Ihsan Tuncer,Serif Bahtiyar*

Main category: cs.CR

TL;DR: 该论文提出了一种基于机器学习的新方法，用于检测DNS隧道，解决了传统规则和签名匹配方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的DNS隧道检测方法（如基于规则或签名匹配）往往无法准确识别隐蔽通信，亟需更有效的方法。

Method: 结合机器学习算法，通过提取DNS流量特征进行分析。

Result: 实验表明，该方法能有效检测DNS隧道。

Conclusion: 该方法在检测DNS隧道方面具有良好潜力。

Abstract: Detecting Domain Name System (DNS) tunneling is a significant challenge in
security due to its capacity to hide harmful actions within DNS traffic that
appears to be normal and legitimate. Traditional detection methods are based on
rule-based approaches or signature matching methods that are often insufficient
to accurately identify such covert communication channels. This research is
about effectively detecting DNS tunneling. We propose a novel approach to
detect DNS tunneling with machine learning algorithms. We combine machine
learning algorithms to analyze the traffic by using features extracted from DNS
traffic. Analyses results show that the proposed approach is a good candidate
to detect DNS tunneling accurately.

</details>


### [138] [Characterizing Security and Privacy Teaching Standards for Schools in the United States](https://arxiv.org/abs/2507.08978)
*Katherine Limes,Nathan Malkin,Kelsey R. Fulton*

Main category: cs.CR

TL;DR: 研究分析了美国K-12教育中安全和隐私课程标准的覆盖范围，发现现有标准与专业人士的期望基本一致，但专业人士更强调威胁建模和安全思维。


<details>
  <summary>Details</summary>
Motivation: 探讨K-12教育中安全和隐私课程标准的覆盖情况，以及与专业人士期望的匹配程度。

Method: 收集并人工分析美国各州及国家组织的计算机科学课程标准，分类后与11位专业人士的访谈结果对比。

Result: 标准和专业人士的期望在主题上大部分重叠，但后者更重视威胁建模和安全思维。

Conclusion: 现有教育标准需进一步强化威胁建模和安全思维的培养。

Abstract: Increasingly, students begin learning aspects of security and privacy during
their primary and secondary education (grades K-12 in the United States).
Individual U.S. states and some national organizations publish teaching
standards -- guidance that outlines expectations for what students should learn
-- which often form the basis for course curricula. However, research has not
yet examined what is covered by these standards and whether the topics align
with what the broader security and privacy community thinks students should
know. To shed light on these questions, we started by collecting computer
science teaching standards from all U.S. states and eight national
organizations. After manually examining a total of 11,954 standards, we labeled
3,778 of them as being related to security and privacy, further classifying
these into 103 topics. Topics ranged from technical subjects like encryption,
network security, and embedded systems to social subjects such as laws, ethics,
and appropriate online behavior. Subsequently, we interviewed 11 security and
privacy professionals to examine how the teaching standards align with their
expectations. We found that, while the specific topics they mentioned mostly
overlapped with those of existing standards, professionals placed a greater
emphasis on threat modeling and security mindset.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [139] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 论文提出通过生成式AI增强多智能体强化学习，从被动反应转向主动预测决策，解决传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习方法面临联合动作空间爆炸、环境非平稳性和部分观测性等问题，现有方法局限于被动反应，无法应对新场景。

Method: 提出将智能体重构为生成模型，能够预测环境演变和其他智能体行为，生成协调动作序列，并基于长期动态进行战略推理。

Result: 生成式强化学习智能体可实现主动决策、无缝协调和动态适应，展现出集体协作智能的潜力。

Conclusion: 这一范式转变有望在自主系统、机器人和人机协作等领域解决传统框架难以处理的协调问题。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [140] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 提出一种结合BERT情感分析和XGBoost的新型模型，准确预测远程学习中学生辍学风险，准确率达84%，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 远程学习中学生辍学问题严重，需早期干预，整合多样化数据源以提高预测准确性。

Method: 使用BERT分析学生评论情感，结合XGBoost处理社会人口和行为数据，特征选择后合并数据训练模型。

Result: 模型在未见数据上准确率达84%，且在其他指标（如精确率和F1分数）上表现优于基线模型。

Conclusion: 该方法可为个性化干预策略提供支持，有效降低辍学率并提升学生坚持性。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [141] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 提出了一种将大型语言模型（LLM）与形式语义学结合的方法，解决其逻辑一致性问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在自然语言处理中表现优异，但逻辑一致性不足，需探索如何利用其知识进行形式推理。

Method: 将LLM直接整合到形式语义的解释函数中，基于超协调逻辑。

Result: 实验证明该方法可行，且保持了逻辑的完备性和可靠性。

Conclusion: 该方法为神经符号推理提供了理论框架，同时利用LLM知识且不损害逻辑属性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [142] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 该论文探讨了现实世界中表格任务的复杂性，并通过定义五大核心能力，研究了基于LLM的表格代理，提出了提升其在实际应用中表现的见解。


<details>
  <summary>Details</summary>
Motivation: 现实表格任务中的噪声、结构异质性和语义复杂性在现有研究中被忽视，论文旨在解决这一问题。

Method: 定义了五大核心能力（C1-C5），并对比分析了现有方法，特别关注Text-to-SQL代理。

Result: 发现开源模型在学术基准与实际场景间存在性能差距，提供了改进建议。

Conclusion: 通过研究和分析，提出了提升LLM表格代理在实际应用中的鲁棒性、泛化能力和效率的方案。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [143] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 研究通过随机对照试验发现，使用2025年前沿AI工具（如Cursor Pro和Claude 3.5/3.7 Sonnet）反而使开发者完成任务时间增加19%，与开发者和专家的预期相反。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具对开源开发者的实际生产力影响，填补现有研究空白。

Method: 16名有AI经验的开发者随机完成246个任务，部分任务允许使用AI工具，比较完成时间差异。

Result: AI工具使用使任务完成时间增加19%，与开发者预期减少20%和经济/ML专家预测减少38-39%相反。

Conclusion: AI工具可能在某些情境下减缓开发速度，需进一步研究影响因素。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [144] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 该论文提出了一个分类和比较可解释人工智能（XAI）研究的三维框架（什么、为什么、谁），以解决当前研究中任务描述不足、缺乏上下文和用户测试不全的问题，并提出了改进XAI研究设计和报告的建议。


<details>
  <summary>Details</summary>
Motivation: 由于可解释人工智能（XAI）在数据分析任务中存在大量矛盾和缺乏具体设计建议的问题，作者希望通过多领域（如可视化分析、认知科学和仪表盘设计）的视角，提出一个系统化的研究分类方法，以改进XAI研究的质量和实用性。

Method: 作者提出了一个基于“什么、为什么、谁”三个维度的框架，用于分类和比较XAI研究，并强调了研究中应明确报告用户的领域知识、AI专业知识和数据分析技能。此外，提出了设计和报告XAI任务的研究指南。

Result: 研究发现XAI研究中的主要问题包括任务描述不足、缺乏上下文研究和用户测试不全。提出的框架和指南有助于改进研究的设计和报告的标准化。

Conclusion: 该论文的贡献在于为XAI研究提供了一个系统化的分类和比较框架，帮助研究人员和设计者更有效地识别相关研究、发现研究空白，并解决矛盾的设计结果，从而推动XAI领域的进一步发展。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [145] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: CompactifAI压缩方法应用于Llama 3.1 8B模型，显著降低计算资源消耗并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 评估CompactifAI压缩方法在大型语言模型上的效率（能耗）和准确性。

Method: 使用Codecarbon和Ragas框架对比压缩模型与完整模型的性能和能效。

Result: 压缩模型大幅减少计算资源，同时保持准确性。

Conclusion: CompactifAI使模型更高效、可扩展且成本效益更高。

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [146] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: 提出了一种名为Iceberg的数据增强方法，通过合成数据和弱标签提升深度学习模型在高层次综合（HLS）中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有HLS预测模型的泛化能力不足，需要改进。

Method: 采用合成数据预训练和弱标签生成，结合上下文模型架构进行元学习。

Result: 在少样本情况下，模型精度提升86.4%，离线DSE性能提高2.47倍和1.12倍。

Conclusion: Iceberg显著提升了模型在HLS任务中的泛化能力。

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [147] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: 论文揭示了当前大语言模型（LLM）推理系统评估方法的常见缺陷，提出了识别和避免这些问题的框架，并通过案例研究验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统的评估方法存在缺陷，导致性能特征被掩盖，阻碍科学进步。

Method: 通过系统分析，识别了三个关键维度（基线公平性、评估设置和指标设计）中的常见反模式，并提出了一个检查清单框架。

Result: 研究发现，常见反模式（如基线比较不当、工作负载选择不具代表性等）会导致误导性结论。

Conclusion: 论文建立了一个严谨的评估框架，旨在促进LLM推理系统的真实进步。

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [148] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: 联邦学习在隐私保护学习中表现出色，但面临标签成本高、数据分布不一致和模型更新受限等问题。提出的FedAcross+框架通过预训练模型和自适应层，有效解决了这些挑战。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在实际应用中面临的高标签成本、数据分布不一致和资源受限的问题。

Method: 使用预训练模型（包括深度主干、自适应模块和分类器），在客户端设备上冻结主干和分类器，仅通过自适应层进行目标域适应。

Result: FedAcross+在低端设备上实现了竞争性的适应效果，并能处理流数据和非平稳环境。

Conclusion: FedAcross+是一个实用且高效的联邦学习框架，适用于资源受限的实际工业场景。

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [149] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 论文研究了联邦学习中客户随机参与的问题，提出了一种适应随机客户参与的方法，并在理论上证明了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户参与随机性导致的优化问题，现有方法通常假设全设备参与或已知分布，但在实际中难以成立。

Method: 通过分析随机客户参与的动态，提出了适应这种不确定性的联邦平均（FedAvg）算法。

Result: 理论证明了在凸且可能非平滑损失下，算法的收敛速度为$$\mathcal{O}(1/$sqrt{T})，并实证表明其优于已知权重的加权聚合变体。

Conclusion: 该方法为随机客户参与的联邦学习提供了理论保障，且在实践中表现更优。

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [150] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: 将AMLAS方法扩展至RL领域，提出AMLAS-RL框架，通过迭代过程为RL系统生成安全保障论证。


<details>
  <summary>Details</summary>
Motivation: 机器学习在CPS中的应用日益广泛，但RL的安全性与系统性保障不足，需专门方法确保其安全。

Method: 扩展AMLAS方法，提出AMLAS-RL框架，通过迭代过程为RL系统生成安全保障论证，并以轮式车辆避障为例进行验证。

Result: 成功将AMLAS-RL应用于轮式车辆避障案例，展示了其可行性。

Conclusion: AMLAS-RL为RL系统提供了系统性安全保障框架，填补了现有方法在RL领域的空白。

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [151] [Precomputed Dominant Resource Fairness](https://arxiv.org/abs/2507.08846)
*Serdar Metin*

Main category: cs.GT

TL;DR: 本文提出了一种名为“预计算主导资源公平性”的新算法，旨在以更少步骤近似主导资源公平性分配。


<details>
  <summary>Details</summary>
Motivation: 随着分布式系统的普及，多资源类型场景下的资源分配问题变得重要。主导资源公平性虽然被广泛采用，但其算法结构仍有优化空间。

Method: 基于主导资源公平性（Max-min公平性的多资源扩展），分析其算法结构并提出一种新算法——预计算主导资源公平性。

Result: 新算法能够在更少步骤内近似主导资源公平性的分配结果。

Conclusion: 预计算主导资源公平性是一种高效的替代算法，适用于多资源类型场景下的资源分配。

Abstract: Although resource allocation is a well studied problem in computer science,
until the prevalence of distributed systems, such as computing clouds and data
centres, the question had been addressed predominantly for single resource type
scenarios. At the beginning of the last decade, with the introuction of
Dominant Resource Fairness, the studies of the resource allocation problem has
finally extended to the multiple resource type scenarios. Dominant Resource
Fairness is a solution, addressing the problem of fair allocation of multiple
resource types, among users with heterogeneous demands. Based on Max-min
Fairness, which is a well established algorithm in the literature for
allocating resources in the single resource type scenarios, Dominant Resource
Fairness generalises the scheme to the multiple resource case. It has a number
of desirable properties that makes it preferable over alternatives, such as
Sharing Incentive, Envy-Freeness, Pareto Efficiency, and Strategy Proofness,
and as such, it is widely adopted in distributed systems. In the present study,
we revisit the original study, and analyse the structure of the algorithm in
closer view, to come up with an alternative algorithm, which approximates the
Dominant Resource Fairness allocation in fewer steps. We name the new algorithm
Precomputed Dominant Resource Fairness, after its main working principle.

</details>


### [152] [A Survey on Bilateral Multi-Round Cloud-SLA Negotiation Strategies](https://arxiv.org/abs/2507.08868)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.GT

TL;DR: 本文综述了云资源交易中的多轮双边谈判策略，分析了趋势、差距和相似性，并提出了相关建议。


<details>
  <summary>Details</summary>
Motivation: 探讨云市场中动态交易机制的潜力，如多轮谈判如何提升数据中心利用率和定制化服务。

Method: 通过分析同行评审文章，识别谈判策略的趋势、差距和范围，并调查描述这些策略的形式化方法。

Result: 提出了创建和记录双边多轮谈判策略的建议，以促进工业实践中的应用。

Conclusion: 多轮双边谈判策略为云市场提供了优化潜力，未来需在实际场景中验证其效果。

Abstract: Today, static cloud markets where consumers purchase services directly from
providers are dominating. Thus, consumers neither negotiate the price nor the
characteristics of the service. In recent years, providers have adopted more
dynamic trading mechanisms, as e.g. Amazon's EC2 platform shows: In addition to
the reservation marketspace and the on-demand marketspace, Amazon offers a spot
marketspace where consumers can bid for virtual machines. This spot marketspace
was extended with spot blocks, and recently Amazon reworked the bidding
options. In addition, other cloud providers, such as Virtustream, adopt dynamic
trading mechanisms. The scientific community envisions autonomous multi-round
negotiations for realizing future cloud marketspaces. Consequently, consumers
and providers exchange offers and counteroffers to reach an agreement. This
helps providers increase the utilization of their datacenters, while consumers
can purchase highly customized cloud services.
  In the paper at hand, we present a survey on multi-round bilateral
negotiation strategies for trading cloud resources. Thus, we analyzed
peer-reviewed articles in order to identify trends, gaps, similarities, and the
scope of such negotiation strategies. In addition, we surveyed the formalism
that the scientific community uses to describe such strategies. Based on these
findings, we derived recommendations for creating and documenting bilateral
multi-round negotiation strategies to foster their implementation in the
industry.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [153] [Toolsuite for Implementing Multiagent Systems Based on Communication Protocols](https://arxiv.org/abs/2507.10324)
*Amit K. Chopra,Samuel H. Christie V,Munindar P. Singh*

Main category: cs.MA

TL;DR: 本文介绍了交互导向编程（IOP）及其在开发多智能体系统中的应用，重点展示了相关软件工具和中间件。


<details>
  <summary>Details</summary>
Motivation: 为多智能体系统开发者提供高效的交互协议验证和智能体实现工具。

Method: 使用IOP方法，通过灵活交互协议建模角色间互动，并开发相关软件工具。

Result: 开发了一套包含协议验证工具和简化智能体实现的中间件的软件套件。

Conclusion: IOP及相关工具为多智能体系统的开发提供了高效且灵活的支持。

Abstract: Interaction-Oriented Programming (IOP) is an approach to building a
multiagent system by modeling the interactions between its roles via a flexible
interaction protocol and implementing agents to realize the interactions of the
roles they play in the protocol.
  In recent years, we have developed an extensive suite of software that
enables multiagent system developers to apply IOP. These include tools for
efficiently verifying protocols for properties such as liveness and safety and
middleware that simplifies the implementation of agents. This paper presents
some of that software suite.

</details>


### [154] [Agent-based visualization of streaming text](https://arxiv.org/abs/2507.08884)
*Jordan Riley Benson,David Crist,Phil Lafleur,Benjamin Watson*

Main category: cs.MA

TL;DR: 提出了一种将数据元素映射到代理的可视化基础设施，代理的行为由数据元素参数化，动态可视化通过代理的位置变化、外观调整和相互响应实现。


<details>
  <summary>Details</summary>
Motivation: 旨在通过代理行为实现动态可视化，特别是针对流式文本数据，帮助用户直观理解文本流中的主要主题及其关系。

Method: 使用代理代表重要词汇，通过圆形大小表示词频，代理间距离基于词共现矩阵优化，后端处理从新闻、博客等来源抓取数据并生成可视化。

Result: 可视化结果显示文本流中的主要主题为聚类，代理布局能够动态适应数据流变化且保持稳定。

Conclusion: 该方法有效实现了流式文本的动态可视化，能够清晰展示主题聚类和词汇关系，适用于实时数据分析。

Abstract: We present a visualization infrastructure that maps data elements to agents,
which have behaviors parameterized by those elements. Dynamic visualizations
emerge as the agents change position, alter appearance and respond to one
other. Agents move to minimize the difference between displayed agent-to-agent
distances, and an input matrix of ideal distances. Our current application is
visualization of streaming text. Each agent represents a significant word,
visualizing it by displaying the word itself, centered in a circle sized by the
frequency of word occurrence. We derive the ideal distance matrix from word
cooccurrence, mapping higher co-occurrence to lower distance. To depict
co-occurrence in its textual context, the ratio of intersection to circle area
approximates the ratio of word co-occurrence to frequency. A networked backend
process gathers articles from news feeds, blogs, Digg or Twitter, exploiting
online search APIs to focus on user-chosen topics. Resulting visuals reveal the
primary topics in text streams as clusters, with agent-based layout moving
without instability as data streams change dynamically.

</details>


### [155] [AnalogTester: A Large Language Model-Based Framework for Automatic Testbench Generation in Analog Circuit Design](https://arxiv.org/abs/2507.09965)
*Weiyu Chen,Chengjie Liu,Wenhao Huang,Jinyang Lyu,Mingqian Yang,Yuan Du,Li Du,Jun Yang*

Main category: cs.MA

TL;DR: LLM驱动的AnalogTester通过集成领域知识、提取论文信息、合成仿真方案并生成测试台代码，解决了模拟电路设计中手动测试台构建的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计中手动测试台构建效率低下，无法满足自动化需求，AnalogTester旨在通过LLM实现完全自动化设计。

Method: 采用LLM驱动的四大步骤：领域知识集成、论文信息提取、仿真方案合成、测试台代码生成。

Result: 成功自动化生成三种基本模拟电路（运放、带隙基准、低压差稳压器）的测试台，并构建了LLM专业化的基础训练数据集。

Conclusion: AnalogTester为模拟电路设计的自动化提供了可扩展框架，并推动了LLM在该领域的专业化应用。

Abstract: Recent advancements have demonstrated the significant potential of large
language models (LLMs) in analog circuit design. Nevertheless, testbench
construction for analog circuits remains manual, creating a critical bottleneck
in achieving fully automated design processes. Particularly when replicating
circuit designs from academic papers, manual Testbench construction demands
time-intensive implementation and frequent adjustments, which fails to address
the dynamic diversity and flexibility requirements for automation. AnalogTester
tackles automated analog design challenges through an LLM-powered pipeline: a)
domain-knowledge integration, b) paper information extraction, c) simulation
scheme synthesis, and d) testbench code generation with Tsinghua Electronic
Design (TED). AnalogTester has demonstrated automated Testbench generation
capabilities for three fundamental analog circuit types: operational amplifiers
(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while
maintaining a scalable framework for adaptation to broader circuit topologies.
Furthermore, AnalogTester can generate circuit knowledge data and TED code
corpus, establishing fundamental training datasets for LLM specialization in
analog circuit design automation.

</details>


### [156] [TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit](https://arxiv.org/abs/2507.09788)
*Paulo Salem,Robert Sim,Christopher Olsen,Prerit Saxena,Rafael Barcelos,Yi Ding*

Main category: cs.MA

TL;DR: 介绍了TinyTroupe，一个基于LLM的多智能体仿真工具包，支持详细角色定义和程序化控制，解决了现有工具在行为研究和社交仿真中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统工具缺乏细粒度角色定义和实验支持，阻碍了行为研究和社交仿真的应用。

Method: 开发了TinyTroupe工具包，提供详细角色定义和LLM驱动机制，支持个体或群体行为问题的解决。

Result: 通过案例展示了工具的有用性，并提供了定量和定性评估，揭示了其可能性和限制。

Conclusion: TinyTroupe是一个创新型概念贡献，可部分或完全用于其他场景，目前已开源。

Abstract: Recent advances in Large Language Models (LLM) have led to a new class of
autonomous agents, renewing and expanding interest in the area. LLM-powered
Multiagent Systems (MAS) have thus emerged, both for assistive and simulation
purposes, yet tools for realistic human behavior simulation -- with its
distinctive challenges and opportunities -- remain underdeveloped. Existing MAS
libraries and tools lack fine-grained persona specifications, population
sampling facilities, experimentation support, and integrated validation, among
other key capabilities, limiting their utility for behavioral studies, social
simulation, and related applications. To address these deficiencies, in this
work we introduce TinyTroupe, a simulation toolkit enabling detailed persona
definitions (e.g., nationality, age, occupation, personality, beliefs,
behaviors) and programmatic control via numerous LLM-driven mechanisms. This
allows for the concise formulation of behavioral problems of practical
interest, either at the individual or group level, and provides effective means
for their solution. TinyTroupe's components are presented using representative
working examples, such as brainstorming and market research sessions, thereby
simultaneously clarifying their purpose and demonstrating their usefulness.
Quantitative and qualitative evaluations of selected aspects are also provided,
highlighting possibilities, limitations, and trade-offs. The approach, though
realized as a specific Python implementation, is meant as a novel conceptual
contribution, which can be partially or fully incorporated in other contexts.
The library is available as open source at
https://github.com/microsoft/tinytroupe.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [157] [Algebraic Closure of Matrix Sets Recognized by 1-VASS](https://arxiv.org/abs/2507.09373)
*Rida Ait El Manssour,Mahsa Naraghi,Mahsa Shirmohammadi,James Worrell*

Main category: cs.FL

TL;DR: 论文探讨了如何计算由上下文无关语言定义的矩阵集的Zariski闭包，提出了一个针对一类子语言的可判定方法，并证明了在更广泛的语言类别中问题是不可判定的。


<details>
  <summary>Details</summary>
Motivation: 研究的核心问题是计算由上下文无关语言定义的矩阵集的Zariski闭包，这在计算仿射程序的递归调用多项式不变量时具有重要意义。

Method: 论文从可判定性和不可判定性两方面入手，提出了针对一类子语言（单计数器语言）的Zariski闭包计算方法，并证明了在更广泛的索引语言类别中问题的不可判定性。

Result: 研究者成功地实现了单计数器语言类别中矩阵集Zariski闭包的计算，同时证明了在索引语言类别中该问题的不可判定性。

Conclusion: 论文通过引入Simon的因子分解森林技术，为矩阵无限幺半群的研究提供了新工具，进一步推动了相关领域的发展。

Abstract: It is known how to compute the Zariski closure of a finitely generated monoid
of matrices and, more generally, of a set of matrices specified by a regular
language. This result was recently used to give a procedure to compute all
polynomial invariants of a given affine program. Decidability of the more
general problem of computing all polynomial invariants of affine programs with
recursive procedure calls remains open. Mathematically speaking, the core
challenge is to compute the Zariski closure of a set of matrices defined by a
context-free language. In this paper, we approach the problem from two sides:
Towards decidability, we give a procedure to compute the Zariski closure of
sets of matrices given by one-counter languages (that is, languages accepted by
one-dimensional vector addition systems with states and zero tests), a proper
subclass of context-free languages. On the other side, we show that the problem
becomes undecidable for indexed languages, a natural extension of context-free
languages corresponding to nested pushdown automata. One of our main technical
tools is a novel adaptation of Simon's factorization forests to infinite
monoids of matrices.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [158] [Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices](https://arxiv.org/abs/2507.09627)
*Muhammad Kamran Saeed,Ashfaq Khokhar,Shakil Ahmed*

Main category: cs.IT

TL;DR: 论文提出了一种轻量级深度学习框架，用于XL-MIMO系统中的高效级联信道估计，旨在降低计算复杂度并适合资源受限的边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 6G等下一代无线技术需满足超高数据速率、低延迟和增强连接性，XL-MIMO和RIS是关键推动技术，但其潜力依赖于准确的信道状态信息（CSI）。现有估计模型在可扩展性和实际部署上存在局限。

Method: 利用信道中的空间相关性，提出了一种基于分块的训练机制，将输入降维为分块级表示，同时保留关键信息，适合大规模系统的可扩展训练。

Result: 仿真结果表明，该框架显著提高了估计精度并降低了计算复杂度，不受XL-MIMO系统中天线和RIS元件数量增加的影响。

Conclusion: 该轻量级框架为XL-MIMO系统中的高效信道估计提供了可行解决方案，适用于资源受限的环境。

Abstract: Next-generation wireless technologies such as 6G aim to meet demanding
requirements such as ultra-high data rates, low latency, and enhanced
connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable
Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and
energy efficiency through numerous antennas, and RIS offering dynamic control
over the wireless environment via passive reflective elements. However,
realizing their full potential depends on accurate Channel State Information
(CSI). Recent advances in deep learning have facilitated efficient cascaded
channel estimation. However, the scalability and practical deployment of
existing estimation models in XL-MIMO systems remain limited. The growing
number of antennas and RIS elements introduces a significant barrier to
real-time and efficient channel estimation, drastically increasing data volume,
escalating computational complexity, requiring advanced hardware, and resulting
in substantial energy consumption. To address these challenges, we propose a
lightweight deep learning framework for efficient cascaded channel estimation
in XL-MIMO systems, designed to minimize computational complexity and make it
suitable for deployment on resource-constrained edge devices. Using spatial
correlations in the channel, we introduce a patch-based training mechanism that
reduces the dimensionality of input to patch-level representations while
preserving essential information, allowing scalable training for large-scale
systems. Simulation results under diverse conditions demonstrate that our
framework significantly improves estimation accuracy and reduces computational
complexity, regardless of the increasing number of antennas and RIS elements in
XL-MIMO systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [159] [Power Consumption Analysis of QKD Networks under Different Protocols and Detector Configurations](https://arxiv.org/abs/2507.09719)
*Jiaheng Xiong,Qiaolun Zhang,Yoann Piétri,Raja Yehia,Raouf Boutaba,Francesco Musumeci,Massimo Tornatore*

Main category: quant-ph

TL;DR: 分析了不同协议和探测器配置下量子密钥分发网络的功耗，评估离散变量和连续变量QKD，并优化设备布局。


<details>
  <summary>Details</summary>
Motivation: 探讨量子密钥分发网络的能耗问题，以优化其实际应用中的性能。

Method: 使用现实网络拓扑，比较离散变量和连续变量QKD，评估SNSPD和APD探测器的功耗差异及光学旁路的优势。

Result: 量化了不同配置下的功耗折衷，并展示了光学旁路的效益。

Conclusion: 研究结果为优化QKD网络的能源效率提供了实用指导。

Abstract: We analyze the power consumption of quantum key distribution (QKD) networks
under various protocol and detector configurations. Using realistic network
topologies, we evaluate discrete-variable vs continuous-variable QKD and
optimize device placement, quantifying power trade-offs of SNSPD vs APD
detectors and the benefits of optical bypass.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [160] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: 本文提出了多模态讽刺生成数据集M2SaG和生成框架ViSP，结合PPO和对比学习，生成更具讽刺性的文本。


<details>
  <summary>Details</summary>
Motivation: 现有讽刺研究多依赖文本模态，忽略了视觉线索，且数据集图像内容与讽刺意图不匹配，导致讽刺生成研究不足。

Method: 提出ViSP框架，整合PPO和对比学习，利用DIP的奖励分数引导生成讽刺文本。

Result: 实验表明，ViSP在五大指标上均优于基线模型，生成文本的讽刺评分（0.898）和事实不一致性（0.768）高于原数据集（0.770和0.739）。

Conclusion: ViSP框架能生成更高质量的讽刺文本，填补了多模态讽刺生成的研究空白。

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [161] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: CodeJudgeBench是一个专门用于评估LLM作为评委模型在代码生成、修复和单元测试生成任务中表现的基准，研究发现思考模型表现更优，但所有模型存在显著随机性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估LLM作为评委在编码任务中效果的基准，填补这一研究空白是本文的主要动机。

Method: 通过CodeJudgeBench基准测试26个LLM评委模型，分析其在三类编码任务中的表现，并研究提示策略的影响。

Result: 思考模型表现优于非思考模型，但所有模型存在随机性；配对比较提示策略优于点式评分，保留完整解释提升评委表现。

Conclusion: LLM作为评委在编码任务中表现出潜力，但需进一步解决随机性和敏感性问题以提升可靠性。

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [162] [A Mini-Review on Mobile Manipulators with Variable Autonomy](https://arxiv.org/abs/2408.10887)
*Cesar Alan Contreras,Alireza Rastegarpanah,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 本文综述了移动机械臂的可变自主性研究现状，重点探讨了相关挑战和应用环境。分析了人机协作的必要性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨移动机械臂在不同环境中的需求及其面临的独特挑战和风险。

Method: 通过文献分析，总结了可变自主性的研究现状，并指出认知负荷和通信延迟等问题。

Result: 提出未来发展方向，包括全身可变自主性、虚拟现实框架和大语言模型的应用。

Conclusion: 未来研究应关注减少操作员复杂度和认知负荷，以应对不确定环境和挑战。

Abstract: This paper presents a mini-review of the current state of research in mobile
manipulators with variable levels of autonomy, emphasizing their associated
challenges and application environments. The need for mobile manipulators in
different environments is evident due to the unique challenges and risks each
presents. Many systems deployed in these environments are not fully autonomous,
requiring human-robot teaming to ensure safe and reliable operations under
uncertainties. Through this analysis, we identify gaps and challenges in the
literature on Variable Autonomy, including cognitive workload and communication
delays, and propose future directions, including whole-body Variable Autonomy
for mobile manipulators, virtual reality frameworks, and large language models
to reduce operators' complexity and cognitive load in some challenging and
uncertain scenarios.

</details>


### [163] [Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints](https://arxiv.org/abs/2507.10131)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER 是一种双相位概率框架，用于机器人推断人类操作员的意图，在导航和操纵任务中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了实现人机协作中的人类意图准确推断，避免冲突。

Method: GUIDER 框架包含导航和操纵两个耦合的信念层，分别利用 Synergy Map 和多模态数据处理技术（如 U2Net、FastSAM 和几何抓取可行性测试）实时更新意图。

Result: 实验中，GUIDER 在导航和操纵阶段的稳定性和预测准确性均优于基线方法（BOIR 和 Trajectron）。

Conclusion: GUIDER 有效提升了移动操纵任务中意图推断的准确性，验证了其双相位框架的可行性。

Abstract: Accurate inference of human intent enables human-robot collaboration without
constraining human control or causing conflicts between humans and robots. We
present GUIDER (Global User Intent Dual-phase Estimation for Robots), a
probabilistic framework that enables a robot to estimate the intent of human
operators. GUIDER maintains two coupled belief layers, one tracking navigation
goals and the other manipulation goals. In the Navigation phase, a Synergy Map
blends controller velocity with an occupancy grid to rank interaction areas.
Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.
The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and
three geometric grasp-feasibility tests, with an end-effector kinematics-aware
update rule that evolves object probabilities in real-time. GUIDER can
recognize areas and objects of intent without predefined goals. We evaluated
GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and
compared it with two baselines, one for navigation and one for manipulation.
Across the 25 trials, GUIDER achieved a median stability of 93-100% during
navigation, compared with 60-100% for the BOIR baseline, with an improvement of
39.5% in a redirection scenario (T5). During manipulation, stability reached
94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a
redirection task (T3). In geometry-constrained trials (manipulation), GUIDER
recognized the object intent three times earlier than Trajectron (median
remaining time to confident prediction 23.6 s vs 7.8 s). These results validate
our dual-phase framework and show improvements in intent inference in both
phases of mobile manipulation tasks.

</details>


### [164] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: 论文提出SC-ADAS系统，通过生成式AI技术实现场景感知和对话式ADAS，评估结果显示其可行性，但也存在延迟等问题。


<details>
  <summary>Details</summary>
Motivation: 现有ADAS系统缺乏场景理解和自然语言交互能力，限制了其在动态环境中的灵活性。

Method: 提出SC-ADAS框架，整合大型语言模型、视觉到文本解析等功能，支持多轮对话和场景感知。

Result: 系统在CARLA模拟器中验证可行，但存在视觉上下文检索延迟等局限性。

Conclusion: SC-ADAS展示了生成式AI在智能驾驶辅助中的潜力，但需进一步优化性能。

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [165] [Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For Dynamic Sound Rendering](https://arxiv.org/abs/2507.09376)
*Bilkent Samsurya*

Main category: cs.SD

TL;DR: 该论文提出了一种新的2D FDTD框架，用于在Unreal Engine中模拟低频声波现象，包括遮挡、衍射、反射和干扰，并通过基准测试验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 当前工业中的声学建模方法未能完全涵盖声波现象，影响了虚拟应用中的沉浸体验。因此，需要一种能更全面模拟声波传播的方法。

Method: 论文采用2D FDTD框架，通过场景几何的离散化生成障碍物掩码和边界条件，使用基于Python的FDTD求解器注入正弦扫频信号，并通过四声道麦克风阵列记录压力场响应。

Result: 压力场响应经过去卷积后生成多通道脉冲响应，保留了空间方向性，并在Unreal Engine中实现了动态播放。基准测试结果与理论预期一致。

Conclusion: 该方法有效模拟了低频声波现象，并提出了面向商业可行性的混合扩展方案。

Abstract: Accurate sound propagation simulation is essential for delivering immersive
experiences in virtual applications, yet industry methods for acoustic modeling
often do not account for the full breadth of acoustic wave phenomena. This
paper proposes a novel two-dimensional (2D) finite-difference time-domain
(FDTD) framework that simulates sound propagation as a wave-based model in
Unreal Engine, with an emphasis on capturing lower frequency wave phenomena,
embedding occlusion, diffraction, reflection and interference in generated
impulse responses. The process begins by discretizing the scene geometry into a
2D grid via a top-down projection from which obstacle masks and boundary
conditions are derived. A Python-based FDTD solver injects a sine sweep at a
source position, and virtual quadraphonic microphone arrays record pressure
field responses at pre-defined listener positions. De-convolution of the
pressure responses yields multi-channel impulse responses that retain spatial
directionality which are then integrated into Unreal Engine's audio pipeline
for dynamic playback. Benchmark tests confirm agreement with analytical
expectations, and the paper outlines hybrid extensions aimed at commercial
viability.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [166] [Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing](https://arxiv.org/abs/2507.09776)
*Mihir Kavishwar,Naresh Shanbhag*

Main category: eess.SP

TL;DR: AIMC的节能潜力受限于高能耗的ADC，降低ADC精度虽能节能但会影响计算精度。本文提出CSNR和CACTUS算法，优化ADC参数以实现更高精度和更低能耗。


<details>
  <summary>Details</summary>
Motivation: 解决AIMC中因ADC高精度要求导致的能耗问题，同时确保计算精度。

Method: 开发CSNR分析模型并提出CACTUS算法，优化ADC参数。

Result: 在28nm CMOS工艺下，CACTUS可将ADC精度降低3位，同时提高CSNR 6dB。

Conclusion: CSNR-optimal ADC在特定条件下优于传统SQNR-optimal ADC，显著提升AIMC的能效比。

Abstract: Analog in-memory computing (AIMC) is an energy-efficient alternative to
digital architectures for accelerating machine learning and signal processing
workloads. However, its energy efficiency is limited by the high energy cost of
the column analog-to-digital converters (ADCs). Reducing the ADC precision is
an effective approach to lowering its energy cost. However, doing so also
reduces the AIMC's computational accuracy thereby making it critical to
identify the minimum precision required to meet a target accuracy. Prior works
overestimate the ADC precision requirements by modeling quantization error as
input-independent noise, maximizing the signal-to-quantization-noise ratio
(SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address
these limitations by developing analytical expressions for estimating the
compute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and
propose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a
circuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we
show that for a 256-dimensional binary dot product, CACTUS reduces the ADC
precision requirements by 3b while achieving 6dB higher CSNR over prior
methods. We also delineate operating conditions under which our proposed
CSNR-optimal ADCs outperform conventional SQNR-optimal ADCs.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [167] [Central Bank Digital Currencies: A Survey](https://arxiv.org/abs/2507.08880)
*Qifeng Tang,Yain-Whar Si*

Main category: econ.GN

TL;DR: 对央行数字货币（CBDC）系统设计与实施的全面综述，基于135篇研究论文，分析了CBDC分类与生态框架，并通过比较26个现有系统，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着数字支付技术的进步，全球央行开始探索CBDC的实施，旨在解决现有支付系统的低效和延迟问题。

Method: 通过分析2018至2025年的135篇论文，研究CBDC设计分类与生态框架，采用CBDC设计金字塔细化关键技术元素，并比较26个CBDC系统的四个维度。

Result: 研究发现最常见的配置是双层架构、分布式账本技术（DLT）和基于令牌的访问模型，但在应用领域尚无主流趋势。

Conclusion: 论文建议未来研究应关注CBDC在跨境支付中的应用，并提出了前瞻性建议。

Abstract: With the advancement of digital payment technologies, central banks worldwide
have increasingly begun to explore the implementation of Central Bank Digital
Currencies (CBDCs). This paper presents a comprehensive review of the latest
developments in CBDC system design and implementation. By analyzing 135
research papers published between 2018 and 2025, the study provides an in-depth
examination of CBDC design taxonomy and ecosystem frameworks. Grounded in the
CBDC Design Pyramid, the paper refines and expands key architectural elements
by thoroughly investigating innovations in ledger technologies, the selection
of consensus mechanisms, and challenges associated with offline payments and
digital wallet integration. Furthermore, it conceptualizes a CBDC ecosystem. A
detailed comparative analysis of 26 existing CBDC systems is conducted across
four dimensions: system architecture, ledger technology, access model, and
application domain. The findings reveal that the most common configuration
consists of a two-tier architecture, distributed ledger technology (DLT), and a
token-based access model. However, no dominant trend has emerged regarding
application domains. Notably, recent research shows a growing focus on
leveraging CBDCs for cross-border payments to resolve inefficiencies and
structural delays in current systems. Finally, the paper offers several
forward-looking recommendations for future research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [168] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 论文提出了‘无限视频理解’作为多媒体研究的下一前沿目标，旨在解决长时间视频处理中的计算、内存和一致性挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型和多模态扩展在视频理解方面取得进展，但仍无法有效处理超长视频内容，需要突破性解决方案。

Method: 通过定位‘无限视频理解’为目标，提出需在流式架构、持久记忆机制、分层表示等领域创新。

Result: 论文并未提供具体实验结果，但提出了未来研究的核心挑战和方向。

Conclusion: ‘无限视频理解’是一个雄心勃勃但关键的研究目标，将推动多媒体和AI领域的创新。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [169] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: 本文提出了AAHR框架，通过动态聚类原型对比学习和特征提取机制，有效解决了图像-文本匹配中的语义模糊性和高阶关联问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理相似实例的高阶关联和语义模糊性时面临挑战，特别是软正样本和软负样本的不确定性，以及未能充分利用邻居关系。

Method: AAHR采用动态聚类原型对比学习构建统一表示空间，结合全局和局部特征提取机制、自适应聚合网络及GNN，增强语义交互。

Result: AAHR在Flickr30K、MSCOCO和ECCV Caption数据集上表现优于现有方法，显著提升了匹配准确性和效率。

Conclusion: AAHR通过综合策略有效解决了图像-文本匹配中的语义模糊性和高阶关联问题，为后续研究提供了新方向。

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [170] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 论文提出了CrisisLandMark数据集和CLOSP框架，用于解决多传感器数据（SAR和多光谱）与文本对齐的问题，显著提高了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本-图像检索系统主要限于RGB数据，未能利用SAR和多光谱数据的独特物理信息，限制了其在灾害响应和气候监测中的应用。

Method: 引入CrisisLandMark数据集（64.7万张SAR和多光谱图像与结构化文本配对），并提出CLOSP框架，通过对比学习将光学和SAR图像对齐到统一的嵌入空间。还提出了GeoCLOSP，整合地理坐标以提升定位依赖任务的性能。

Result: CLOSP在检索任务中的nDGC指标上比现有模型提升54%，并通过统一训练策略克服SAR图像解释的困难。GeoCLOSP在定位依赖任务中表现优异。

Conclusion: 多传感器数据和地理上下文的整合对充分发挥遥感档案潜力至关重要。CLOSP和GeoCLOSP为这一目标提供了有效工具。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [171] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为RAPNet的新型架构，通过内容自适应卷积（RAPConv）和动态特征融合模块（PAN-DFF）改进了遥感图像融合的精度和效果。


<details>
  <summary>Details</summary>
Motivation: 尽管CNN在遥感图像融合中表现良好，但其均匀卷积核的应用忽略了局部内容变化，导致精度受限。

Method: RAPNet采用RAPConv生成空间自适应核，并结合PAN-DFF模块通过注意力机制优化空间细节与光谱保真度的平衡。

Result: 在公开数据集上的测试表明，RAPNet在定量和定性评估中均优于现有方法，消融实验验证了自适应组件的有效性。

Conclusion: RAPNet通过自适应机制显著提升了遥感图像融合的性能，为相关领域提供了新的解决方案。

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [172] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 这篇论文提出了第一个用于人-物交互（HOI）检测的鲁棒性基准RoHOI，评估模型在多样化挑战下的表现，并提出了语义感知掩蔽渐进学习（SAMPL）策略以增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测模型在真实世界中的退化问题严重，亟需一个能评估和提升模型鲁棒性的方法。

Method: 提出了鲁棒性基准RoHOI，包含20种腐败类型，并提出SAMPL策略，通过动态调整优化增强鲁棒特征学习。

Result: 实验表明SAMPL策略优于现有方法，确立了新标准，且基准和代码将公开。

Conclusion: RoHOI基准和SAMPL策略为HOI检测在真实环境中的应用提供了重要支持。

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [173] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite 是一个用于构建模块化脑部病变图像分析流程的 Python 工具包，简化开发流程并提供灵活的预处理功能。


<details>
  <summary>Details</summary>
Motivation: 旨在为临床和科研实践提供高效、灵活的工具，减少认知负担，简化复杂工作流的创建。

Method: 基于 Python，提供预处理模块（包括配准、图谱注册等）、支持模态合成、病灶修复和分割性能评估工具。

Result: 适用于脑部病变（如胶质瘤、转移瘤和多发性硬化症）分析，并可扩展到其他生物医学图像应用。

Conclusion: BrainLesion Suite 是一个功能强大且易于扩展的工具包，适用于多样化医疗图像分析需求。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [174] [Balancing Semantic Relevance and Engagement in Related Video Recommendations](https://arxiv.org/abs/2507.09403)
*Amit Jaspal,Feng Zhang,Wei Chang,Sumit Kumar,Yubo Wang,Roni Mittleman,Qifan Wang,Weize Mao*

Main category: cs.IR

TL;DR: 本文提出了一种新型多目标检索框架，通过多任务学习和多模态特征融合，有效平衡语义相关性与用户参与度，同时减少流行偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的协同过滤推荐系统因依赖共参与信号，常导致推荐结果缺乏语义连贯性且存在明显的流行偏见。

Method: 采用多任务学习联合优化共参与与语义相关性，融合多模态内容特征，并利用反倾向加权减少流行偏见。

Result: 在工业规模数据和A/B测试中，语义相关性提升至63%，流行视频推荐减少13.8%，用户参与度提高0.04%。

Conclusion: 该方法显著提升了推荐系统的语义连贯性、参与度平衡及实际部署的可扩展性。

Abstract: Related video recommendations commonly use collaborative filtering (CF)
driven by co-engagement signals, often resulting in recommendations lacking
semantic coherence and exhibiting strong popularity bias. This paper introduces
a novel multi-objective retrieval framework, enhancing standard two-tower
models to explicitly balance semantic relevance and user engagement. Our
approach uniquely combines: (a) multi-task learning (MTL) to jointly optimize
co-engagement and semantic relevance, explicitly prioritizing topical
coherence; (b) fusion of multimodal content features (textual and visual
embeddings) for richer semantic understanding; and (c) off-policy correction
(OPC) via inverse propensity weighting to effectively mitigate popularity bias.
Evaluation on industrial-scale data and a two-week live A/B test reveals our
framework's efficacy. We observed significant improvements in semantic
relevance (from 51% to 63% topic match rate), a reduction in popular item
distribution (-13.8% popular video recommendations), and a +0.04% improvement
in our topline user engagement metric. Our method successfully achieves better
semantic coherence, balanced engagement, and practical scalability for
real-world deployment.

</details>


### [175] [Riding the Carousel: The First Extensive Eye Tracking Analysis of Browsing Behavior in Carousel Recommenders](https://arxiv.org/abs/2507.10135)
*Santiago de Leon-Martinez,Robert Moro,Branislav Kveton,Maria Bielikova*

Main category: cs.IR

TL;DR: 该论文研究了轮播界面中用户的浏览行为，通过眼动追踪分析提出了改进推荐系统的建议。


<details>
  <summary>Details</summary>
Motivation: 研究轮播界面中用户浏览行为，填补该领域的研究空白，改进推荐系统设计。

Method: 使用眼动追踪技术分析用户在自由浏览中的行为，重点关注浏览起始点、项目间转换以及类型偏好的影响。

Result: 提出重新排列推荐项目以适应用户浏览行为的建议，特别是针对滑动后的浏览行为。

Conclusion: 该研究为轮播推荐系统设计提供了实证依据，并鼓励开发更适应复杂界面的用户模型和系统。

Abstract: Carousels have become the de-facto interface in online services. However,
there is a lack of research in carousels, particularly examining how
recommender systems may be designed differently than the traditional
single-list interfaces. One of the key elements for understanding how to design
a system for a particular interface is understanding how users browse. For
carousels, users may browse in a number of different ways due to the added
complexity of multiple topic defined-lists and swiping to see more items.
  Eye tracking is the key to understanding user behavior by providing valuable,
direct information on how users see and navigate. In this work, we provide the
first extensive analysis of the eye tracking behavior in carousel recommenders
under the free-browsing setting. To understand how users browse, we examine the
following research questions : 1) where do users start browsing, 2) how do
users transition from item to item within the same carousel and across
carousels, and 3) how does genre preference impact transitions?
  This work addresses a gap in the field and provides the first extensive
empirical results of eye tracked browsing behavior in carousels for improving
recommenders. Taking into account the insights learned from the above
questions, our final contribution is to provide suggestions to help carousel
recommender system designers optimize their systems for user browsing behavior.
The most important suggestion being to reorder the ranked item positions to
account for browsing after swiping.These contributions aim not only to help
improve current systems, but also to encourage and allow the design of new user
models, systems, and metrics that are better suited to the complexity of
carousel interfaces.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [176] [Sampling-Based Estimation of Jaccard Containment and Similarity](https://arxiv.org/abs/2507.10019)
*Pranav Joshi*

Main category: stat.CO

TL;DR: 该论文提出了一种基于二项分布的模型，用于通过随机样本估计两个集合的包含性和相似性，无需完整数据或草图。模型在小样本情况下表现优异，并与现有方法进行了对比。


<details>
  <summary>Details</summary>
Motivation: 解决在仅能获取随机样本的情况下，如何准确估计两个集合的包含性和相似性的问题。

Method: 引入二项分布模型预测样本间的重叠，分析其统计特性，包括误差界限和样本量需求，并扩展至集合相似性估计。

Result: 模型在小样本下表现更优，提供了更高的估计准确性，并适用于大规模数据系统。

Conclusion: 该方法为基于随机样本的集合分析提供了实用且可靠的工具，适用于数据有限的情况。

Abstract: This paper addresses the problem of estimating the containment and similarity
between two sets using only random samples from each set, without relying on
sketches or full data access. The study introduces a binomial model for
predicting the overlap between samples, demonstrating that it is both accurate
and practical when sample sizes are small compared to the original sets. The
paper compares this model to previous approaches and shows that it provides
better estimates under the considered conditions. It also analyzes the
statistical properties of the estimator, including error bounds and sample size
requirements needed to achieve a desired level of accuracy and confidence. The
framework is extended to estimate set similarity, and the paper provides
guidance for applying these methods in large scale data systems where only
partial or sampled data is available.

</details>
