{"id": "2506.19045", "pdf": "https://arxiv.org/pdf/2506.19045", "abs": "https://arxiv.org/abs/2506.19045", "authors": ["Ahmadreza Saboor Yaraghi", "Golnaz Gharachorlu", "Sakina Fatima", "Lionel C. Briand", "Ruiyuan Wan", "Ruifeng Gao"], "title": "Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation", "categories": ["cs.SE"], "comment": null, "summary": "Fault localization (FL) is a critical step in debugging which typically\nrelies on repeated executions to pinpoint faulty code regions. However,\nrepeated executions can be impractical in the presence of non-deterministic\nfailures or high execution costs. While recent efforts have leveraged Large\nLanguage Models (LLMs) to aid execution-free FL, these have primarily focused\non identifying faults in the system under test (SUT) rather than in the often\ncomplex system test code. However, the latter is also important as, in\npractice, many failures are triggered by faulty test code. To overcome these\nchallenges, we introduce a fully static, LLM-driven approach for system test\ncode fault localization (TCFL) that does not require executing the test case.\nOur method uses a single failure execution log to estimate the test's execution\ntrace through three novel algorithms that identify only code statements likely\ninvolved in the failure. This pruned trace, combined with the error message, is\nused to prompt the LLM to rank potential faulty locations. Our black-box,\nsystem-level approach requires no access to the SUT source code and is\napplicable to large test scripts that assess full system behavior. We evaluate\nour technique at function, block, and line levels using an industrial dataset\nof faulty test cases not previously used in pre-training LLMs. Results show\nthat our best estimated trace closely match actual traces, with an F1 score of\naround 90%. Additionally, pruning the complex system test code reduces the\nLLM's inference time by up to 34% without any loss in FL performance. Our\nresults further suggest that block-level TCFL offers a practical balance,\nnarrowing the search space while preserving useful context, achieving an 81%\nhit rate at top-3 (Hit@3)."}
{"id": "2506.19153", "pdf": "https://arxiv.org/pdf/2506.19153", "abs": "https://arxiv.org/abs/2506.19153", "authors": ["Krzysztof Fonal"], "title": "Dataset of Yul Contracts to Support Solidity Compiler Research", "categories": ["cs.SE"], "comment": "4 pages", "summary": "The YulCode dataset presents a comprehensive collection of 348,840 Yul-based\nsmart contract instances, comprising approximately 135,013 unique contracts.\nThese contracts were generated through the compilation of Solidity source files\nthat have been deployed on the Ethereum mainnet, making the dataset directly\nrepresentative of real-world decentralized applications. YulCode provides a\nrich foundation for a variety of research and development tasks, including but\nnot limited to machine learning applications, formal verification, optimization\nanalysis, and software engineering tool evaluation in the context of low-level\nsmart contract code. To the best of our knowledge at the time of writing,\nYulCode is the first and only publicly available dataset that focuses\nspecifically on Yul, an intermediate language designed for the Ethereum Virtual\nMachine (EVM). As such, it fills a critical gap in the current ecosystem of\nsmart contract datasets and opens new avenues for research and tooling aimed at\nlow-level contract analysis and generation."}
{"id": "2506.19287", "pdf": "https://arxiv.org/pdf/2506.19287", "abs": "https://arxiv.org/abs/2506.19287", "authors": ["Yaoxuan Wu", "Xiaojie Zhou", "Ahmad Humayun", "Muhammad Ali Gulzar", "Miryung Kim"], "title": "Generating and Understanding Tests via Path-Aware Symbolic Execution with LLMs", "categories": ["cs.SE"], "comment": null, "summary": "Symbolic execution is a widely used technique for test generation, offering\nsystematic exploration of program paths through constraint solving. However, it\nis fundamentally constrained by the capability to model the target code\nincluding library functions in terms of symbolic constraint and the capability\nof underlying constraint solvers. As a result, many paths involving complex\nfeatures remain unanalyzed or insufficiently modeled. Recent advances in large\nlanguage models (LLMs) have shown promise in generating diverse and valid test\ninputs. Yet, LLMs lack mechanisms for systematically enumerating program paths\nand often fail to cover subtle corner cases. We observe that directly prompting\nan LLM with the full program leads to missed coverage of interesting paths. In\nthis paper, we present PALM, a test generation system that combines symbolic\npath enumeration with LLM-assisted test generation. PALM statically enumerates\npossible paths through AST-level analysis and transforms each into an\nexecutable variant with embedded assertions that specify the target path. This\navoids the need to translate path constraints into SMT formulae, by instead\nconstructing program variants that LLM can interpret. Importantly, PALM is the\nfirst to provide an interactive frontend that visualizes path coverage\nalongside generated tests, assembling tests based on the specific paths they\nexercise. A user study with 12 participants demonstrates that PALM's frontend\nhelps users better understand path coverage and identify which paths are\nactually exercised by PALM-generated tests, through verification and\nvisualization of their path profiles."}
{"id": "2506.19425", "pdf": "https://arxiv.org/pdf/2506.19425", "abs": "https://arxiv.org/abs/2506.19425", "authors": ["Ang Jia", "He Jiang", "Zhilei Ren", "Xiaochen Li", "Ming Fan", "Ting Liu"], "title": "What Makes the Best Decomposition? Investigating Binary Decomposition Under FCG Variance", "categories": ["cs.SE"], "comment": null, "summary": "Binary decomposition, which decomposes binary files into modules, plays a\ncritical role in binary reuse detection. Existing binary decomposition works\neither apply anchor-based methods by extending anchor functions to generate\nmodules, or apply clustering-based methods by using clustering algorithms to\ngroup binary functions, which all rely on that reused code shares similar\nfunction call relationships. However, we find that function call graphs (FCGs)\nvary a lot when using different compilation settings, especially with diverse\nfunction inlining decisions.\n  In this work, we conduct the first systematic empirical study on the variance\nof FCGs compiled by various compilation settings and explore its effect on\nbinary decomposition methods. We first construct a dataset compiled by 17\ncompilers, using 6 optimizations to 4 architectures and analyze the changes and\nmappings of the FCGs. We find that the size of FCGs changes dramatically, while\nthe FCGs are still linked by three different kinds of mappings. Then we\nevaluate the existing works under the FCG variance, and results show that\nexisting works are facing great challenges when conducting cross-compiler\nevaluation with diverse optimization settings. Finally, we propose a method to\nidentify the optimal decomposition and compare the existing decomposition works\nwith the optimal decomposition. Existing works either suffer from low coverage\nor cannot generate stable community similarities."}
{"id": "2506.18923", "pdf": "https://arxiv.org/pdf/2506.18923", "abs": "https://arxiv.org/abs/2506.18923", "authors": ["Yifan Zong", "Yuntian Deng", "Pengyu Nie"], "title": "Mix-of-Language-Experts Architecture for Multilingual Programming", "categories": ["cs.PL", "cs.CL", "cs.SE"], "comment": "Accepted at LLM4Code @ ICSE 2025", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\naiding developers with tasks like code comprehension, generation, and\ntranslation. Supporting multilingual programming -- i.e., coding tasks across\nmultiple programming languages -- typically requires either (1) finetuning a\nsingle LLM across all programming languages, which is cost-efficient but\nsacrifices language-specific specialization and performance, or (2) finetuning\nseparate LLMs for each programming language, which allows for specialization\nbut is computationally expensive and storage-intensive due to the duplication\nof parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel\narchitecture that balances efficiency and specialization for multilingual\nprogramming. MoLE is composed of a base model, a shared LoRA (low-rank\nadaptation) module, and a collection of language-specific LoRA modules. These\nmodules are jointly optimized during the finetuning process, enabling effective\nknowledge sharing and specialization across programming languages. During\ninference, MoLE automatically routes to the language-specific LoRA module\ncorresponding to the programming language of the code token being generated.\nOur experiments demonstrate that MoLE achieves greater parameter efficiency\ncompared to training separate language-specific LoRAs, while outperforming a\nsingle shared LLM finetuned for all programming languages in terms of accuracy."}
{"id": "2506.19084", "pdf": "https://arxiv.org/pdf/2506.19084", "abs": "https://arxiv.org/abs/2506.19084", "authors": ["Maryam Riahi", "Benjamin Watson"], "title": "Am I Playing Better Now? The Effects of G-SYNC in 60Hz Gameplay", "categories": ["cs.GR"], "comment": null, "summary": "G-SYNC technology matches formerly regular display refreshes to irregular\nframe updates, improving frame rates and interactive latency. In a previous\nstudy of gaming at the 30Hz frame rates common on consoles, players of\nBattlefield 4 were unable to discern when G-SYNC was in use, but scored higher\nwith G-SYNC and were affected emotionally. We build on that study with the\nfirst examination of G-SYNC's effects at the 60Hz frame rate more common in PC\ngaming and on emerging consoles. Though G-SYNC's effects are less at 60Hz than\nthey were at 30Hz, G-SYNC can still improve the performance of veteran players,\nparticularly when games are challenging. G-SYNC's effects on emotion and\nexperience were limited."}
{"id": "2506.19030", "pdf": "https://arxiv.org/pdf/2506.19030", "abs": "https://arxiv.org/abs/2506.19030", "authors": ["Boyi Liu", "Yongguang Lu", "Jianguo Zhao", "Qiang Yang", "Wen Wu", "Lin Chen", "Jagmohan Chauhan", "Jun Zhang"], "title": "WiLLM: An Open Wireless LLM Communication System", "categories": ["cs.NI"], "comment": null, "summary": "The rapid evolution of LLMs threatens to overwhelm existing wireless\ninfrastructure, necessitating architectural innovations for burgeoning mobile\nLLM services. This paper introduces WiLLM, the first open-source wireless\nsystem specifically designed for these services. First, we establish a new\nparadigm by deploying LLMs in core networks (CNs) with abundant GPUs. This\nenables distributed inference services, strategically positioning LLM inference\nat the convergence of backbone bandwidth and the cellular network's edge.\nSecond, we propose an innovative \"Tree-Branch-Fruit\" extension to the\nconventional network slicing architecture. This specialized design allows\ntelecom operators to monetize LLM services through slice subscriptions while\nmaintaining infrastructure ownership. Finally, to realize this vision, WiLLM\naddresses critical limitations in current solutions with several novel\ncapabilities. It features enhanced slice orchestration through a dual-layer\nslicing architecture, enabling coordinated multi-UE-multi-slice scheduling for\nfiner-grained resource allocation. To ensure universal compatibility, an\napplication-layer tunneling mechanism allows legacy devices without native\nslicing to access LLM slice services without hardware upgrades. Furthermore,\nits dual-mode scheduling and cross-layer APIs support flexible deployment from\nCNs to servers. Built on OpenAirInterface, WiLLM extends this established\nframework, lowering the adoption barrier for researchers. We also release the\nfirst LLM wireless communication dataset with 1,649,996 records and\nsynchronized 58-dimensional metrics, alongside two benchmarks. A case study\nwith smart glasses demonstrates practical viability for resource-constrained\ndevices. WiLLM aims to foster an open platform for cross-layer optimization and\nAI-telecom convergence. The code, datasets, and hardware details are available\nat https://openwillm.github.io."}
{"id": "2506.19651", "pdf": "https://arxiv.org/pdf/2506.19651", "abs": "https://arxiv.org/abs/2506.19651", "authors": ["Letian Kang", "Shixian Luo", "Yiqiang Li", "Xiaoyang Yu", "Shenxuan Zhou", "Yong Wu"], "title": "PEVLM: Parallel Encoding for Vision-Language Models", "categories": ["cs.CV", "cs.LG", "cs.PF"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated strong performance in\nvideo-language tasks, yet their application to long video understanding remains\nconstrained by the quadratic complexity of standard attention mechanisms. In\nthis paper, we propose \\textbf{PEVLM}, a parallel encoding strategy\nspecifically designed to improve the prefill efficiency of VLMs without\nrequiring model finetuning. PEVLM partitions the input into block-wise segments\nwith a shared sink, preserves full-attention positional embeddings, and aligns\nattention weights to mimic full-attention distributions. This design reduces\nattention computation from $O((T \\times N)^2)$ to $O(T \\times N)$ while\nmaintaining high accuracy. Extensive experiments on the LongVideoBench\nbenchmark show that PEVLM achieves up to 8.37\\% accuracy improvement over\nexisting inference-efficient methods and delivers up to 7.47x speedup in\nattention computation and 40\\% reduction in end-to-end latency. Under strict\nlatency constraints, PEVLM significantly outperforms baselines, raising\naccuracy from 23.26\\% to 61.03\\%. These results highlight PEVLM's effectiveness\nfor low-latency, long-context video understanding, making it well-suited for\nreal-world applications such as autonomous driving."}
{"id": "2506.19019", "pdf": "https://arxiv.org/pdf/2506.19019", "abs": "https://arxiv.org/abs/2506.19019", "authors": ["Peng Shu", "Junhao Chen", "Zhengliang Liu", "Huaqin Zhao", "Xinliang Li", "Tianming Liu"], "title": "Survey of HPC in US Research Institutions", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "The rapid growth of AI, data-intensive science, and digital twin technologies\nhas driven an unprecedented demand for high-performance computing (HPC) across\nthe research ecosystem. While national laboratories and industrial hyperscalers\nhave invested heavily in exascale and GPU-centric architectures,\nuniversity-operated HPC systems remain comparatively under-resourced. This\nsurvey presents a comprehensive assessment of the HPC landscape across U.S.\nuniversities, benchmarking their capabilities against Department of Energy\n(DOE) leadership-class systems and industrial AI infrastructures. We examine\nover 50 premier research institutions, analyzing compute capacity,\narchitectural design, governance models, and energy efficiency. Our findings\nreveal that university clusters, though vital for academic research, exhibit\nsignificantly lower growth trajectories (CAGR $\\approx$ 18%) than their\nnational ($\\approx$ 43%) and industrial ($\\approx$ 78%) counterparts. The\nincreasing skew toward GPU-dense AI workloads has widened the capability gap,\nhighlighting the need for federated computing, idle-GPU harvesting, and\ncost-sharing models. We also identify emerging paradigms, such as decentralized\nreinforcement learning, as promising opportunities for democratizing AI\ntraining within campus environments. Ultimately, this work provides actionable\ninsights for academic leaders, funding agencies, and technology partners to\nensure more equitable and sustainable HPC access in support of national\nresearch priorities."}
{"id": "2506.18962", "pdf": "https://arxiv.org/pdf/2506.18962", "abs": "https://arxiv.org/abs/2506.18962", "authors": ["Weiheng Lu", "Chunfeng Song", "Jiamin Wu", "Pengyu Zhu", "Yuchen Zhou", "Weijian Mai", "Qihao Zheng", "Wanli Ouyang"], "title": "UniMind: Unleashing the Power of LLMs for Unified Multi-Task Brain Decoding", "categories": ["cs.HC"], "comment": "19pages,4 figures", "summary": "Decoding human brain activity from electroencephalography (EEG) signals is a\ncentral challenge at the intersection of neuroscience and artificial\nintelligence, enabling diverse applications in mental state assessment,\nclinical monitoring, and human-machine interaction. Recent efforts have\nextensively explored EEG-based brain foundation models for generalized brain\ndecoding, employing large-scale training on multiple datasets. However, most of\nthese attempts struggle with generalizability and fail to achieve satisfactory\nperformance without task-specific tuning due to pronounced inherent\nheterogeneity among decoding tasks. To address these challenges, we present\nUniMind, a general-purpose EEG foundation model for unified multi-task brain\ndecoding by uniquely unleashing the power of large language models to\ncomprehend complex neural patterns. UniMind offers several advantages. First,\nwe design a Neuro-Language Connector to bridge the modality gap between neural\nsignals and large language models, distilling and transforming the\nspatiotemporal neural patterns of EEG data into representations understandable\nby language models. Second, a Task-aware Query Selection module is proposed to\ninject task-awareness into the cross-modal alignment by dynamically generating\ntask-adaptive query tokens, enabling learning of task-relevant neural patterns\nacross diverse tasks. Extensive experiments across ten datasets demonstrate\nthat UniMind substantially outperforms state-of-the-art multi-task decoding\nmodels, with an average gain of 12 percent, while also offering valuable\nneuroscientific insights into neural functional correlations across tasks. The\ncode will be made publicly available."}
{"id": "2506.19067", "pdf": "https://arxiv.org/pdf/2506.19067", "abs": "https://arxiv.org/abs/2506.19067", "authors": ["Hossein Taji", "José Miranda", "Miguel Peón-Quirós", "David Atienza"], "title": "MEDEA: A Design-Time Multi-Objective Manager for Energy-Efficient DNN Inference on Heterogeneous Ultra-Low Power Platforms", "categories": ["cs.AR", "C.3; C.1.3"], "comment": "Submitted to ACM Transactions on Embedded Computing Systems (TECS)", "summary": "The growing demand for on-device AI necessitates energy-efficient execution\nof DNN based applications on resource-constrained ultra-low power (ULP)\nplatforms. Heterogeneous architectures, combining specialized processing\nelements (PEs), have emerged as a key solution for achieving the required\nperformance and energy efficiency. However, optimizing energy while executing\napplications on these platforms requires efficiently managing platform\nresources like PEs, power features, and memory footprint, all while adhering to\ncritical application deadlines. This paper presents MEDEA, a novel design-time\nmulti-objective manager for energy-efficient DNN inference on Heterogeneous ULP\n(HULP) platforms. MEDEA uniquely integrates: kernel-level dynamic voltage and\nfrequency scaling (DVFS) for dynamic energy adaptation; kernel-level\ngranularity scheduling, suitable for specialized accelerators; memory-aware\nadaptive tiling to navigate severe memory constraints; and all within a timing\nconstraint-based optimization strategy, which minimizes energy based on\napplication deadline. To showcase practical viability, we evaluate MEDEA on\nHEEPtimize, a heterogeneous ULP platform (22 nm, FPGA-prototyped) featuring a\nRISC-V processor besides Near-Memory Computing (NMC) and Coarse-Grained\nReconfigurable Array (CGRA) accelerators. Experimental results, using a\nbiomedical seizure detection case study, demonstrate that MEDEA achieves\noverall energy reductions of up to 38% compared to representative\nstate-of-the-art methods, while consistently meeting all timing and memory\nrequirements. This effectiveness is attributed to its integrated features, with\nour analysis showing that kernel-level DVFS alone can be responsible for over\n31% of the energy savings in specific scenarios."}
{"id": "2506.19769", "pdf": "https://arxiv.org/pdf/2506.19769", "abs": "https://arxiv.org/abs/2506.19769", "authors": ["Shulan Ruan", "Rongwei Wang", "Xuchen Shen", "Huijie Liu", "Baihui Xiao", "Jun Shi", "Kun Zhang", "Zhenya Huang", "Yu Liu", "Enhong Chen", "You He"], "title": "A Survey of Multi-sensor Fusion Perception for Embodied AI: Background, Methods, Challenges and Prospects", "categories": ["cs.MM", "cs.AI"], "comment": null, "summary": "Multi-sensor fusion perception (MSFP) is a key technology for embodied AI,\nwhich can serve a variety of downstream tasks (e.g., 3D object detection and\nsemantic segmentation) and application scenarios (e.g., autonomous driving and\nswarm robotics). Recently, impressive achievements on AI-based MSFP methods\nhave been reviewed in relevant surveys. However, we observe that the existing\nsurveys have some limitations after a rigorous and detailed investigation. For\none thing, most surveys are oriented to a single task or research field, such\nas 3D object detection or autonomous driving. Therefore, researchers in other\nrelated tasks often find it difficult to benefit directly. For another, most\nsurveys only introduce MSFP from a single perspective of multi-modal fusion,\nwhile lacking consideration of the diversity of MSFP methods, such as\nmulti-view fusion and time-series fusion. To this end, in this paper, we hope\nto organize MSFP research from a task-agnostic perspective, where methods are\nreported from various technical views. Specifically, we first introduce the\nbackground of MSFP. Next, we review multi-modal and multi-agent fusion methods.\nA step further, time-series fusion methods are analyzed. In the era of LLM, we\nalso investigate multimodal LLM fusion methods. Finally, we discuss open\nchallenges and future directions for MSFP. We hope this survey can help\nresearchers understand the important progress in MSFP and provide possible\ninsights for future research."}
{"id": "2506.19487", "pdf": "https://arxiv.org/pdf/2506.19487", "abs": "https://arxiv.org/abs/2506.19487", "authors": ["Ama Bandara", "Abhijit Das", "Fatima Rodriguez-Galan", "Eduard Alarcon", "Sergi Abadal"], "title": "TRMAC: A Time-Reversal-based MAC Protocol for Wireless Networks within Computing Packages", "categories": ["cs.ET"], "comment": null, "summary": "As chiplet-based integration and many-core architectures become the norm in\nhigh-performance computing, on-chip wireless communication has emerged as a\ncompelling alternative to traditional interconnects. However, scalable Medium\nAccess Control (MAC) remains a fundamental challenge, particularly under dense\ntraffic and limited spectral resources. This paper presents TRMAC, a novel\ncross-layer MAC protocol that exploits the spatial focusing capability of Time\nReversal (TR) to enable multiple parallel transmissions over a shared frequency\nchannel. By leveraging the quasi-deterministic nature of on-chip wireless\nchannels, TRMAC pre-characterizes channel impulse responses to coordinate\naccess using energy-based thresholds, eliminating the need for orthogonal\nresource allocation or centralized arbitration. Through detailed physical-layer\nsimulation and system-level evaluation on diverse traffic, TRMAC demonstrates\ncomparable or superior performance to existing multi-channel MAC protocols,\nachieving low latency, high throughput, and strong scalability across hundreds\nof cores. TRMAC provides a low-complexity, high-efficiency solution for future\nWireless Networks-on-Chip (WNoCs), particularly in chiplet-based systems where\nspatial reuse and modularity are critical. With simulations we prove that TRMAC\ncan be utilized for parallel transmissions with a single frequency channel with\na similar throughput and latency as in using multiple frequency bands omitting\nthe need for complex transceivers. This work establishes a new design direction\nfor MAC protocols that are tightly integrated with the underlying channel\nphysics to meet the demands of next-generation computing platforms."}
{"id": "2506.18951", "pdf": "https://arxiv.org/pdf/2506.18951", "abs": "https://arxiv.org/abs/2506.18951", "authors": ["Jinyang Li", "Xiaolong Li", "Ge Qu", "Per Jacobsson", "Bowen Qin", "Binyuan Hui", "Shuzheng Si", "Nan Huo", "Xiaohan Xu", "Yue Zhang", "Ziwei Tang", "Yuanshuai Li", "Florensia Widjaja", "Xintong Zhu", "Feige Zhou", "Yongfeng Huang", "Yannis Papakonstantinou", "Fatma Ozcan", "Chenhao Ma", "Reynold Cheng"], "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications", "categories": ["cs.DB", "cs.AI"], "comment": "26 pages, 9 figures", "summary": "Resolution of complex SQL issues persists as a significant bottleneck in\nreal-world database applications. Current Large Language Models (LLMs), while\nadept at text-to-SQL translation, have not been rigorously evaluated on the\nmore challenging task of debugging SQL issues. To address this gap, we\nintroduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530\nPostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks\n(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within\nnew environments to facilitate rigorous evaluation. Baseline evaluations\nunderscore the task's complexity, with the leading reasoning model O3-Mini\nachieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on\nBIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks\nis crucial for empowering local development while safeguarding data privacy.\nTherefore, we present Six-Gym (Sql-fIX-Gym), a training environment for\nelevating open-source model capabilities for SQL issue debugging. This\nenvironment leverages SQL-Rewind strategy, which automatically generates\nexecutable issue-solution datasets by reverse-engineering issues from verified\nSQLs. However, popular trajectory-based fine-tuning methods do not explore\nsubstantial supervisory signals. We further propose f-Plan Boosting, which\nextracts high-level debugging plans from SQL solutions, enabling teacher LLMs\nto produce 73.7% more successful trajectories for training. We integrate these\ncomponents into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,\nBird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on\nBIRD-CRITIC-Multi, surpassing leading proprietary models such as\nClaude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing\nsophisticated SQL-debugging capabilities. The leaderboard and source code are\navailable: https://bird-critic.github.io/"}
{"id": "2506.19421", "pdf": "https://arxiv.org/pdf/2506.19421", "abs": "https://arxiv.org/abs/2506.19421", "authors": ["Markus Lohrey", "Sebastian Maneth", "Markus L. Schmid"], "title": "FO-Query Enumeration over SLP-Compressed Structures of Bounded Degree", "categories": ["cs.LO"], "comment": null, "summary": "Enumerating the result set of a first-order query over a relational structure\nof bounded degree can be done with linear preprocessing and constant delay. In\nthis work, we extend this result towards the compressed perspective where the\nstructure is given in a potentially highly compressed form by a straight-line\nprogram (SLP). Our main result is an algorithm that enumerates the result set\nof a first-order query over a structure of bounded degree that is represented\nby an SLP satisfying the so-called apex condition. For a fixed formula, the\nenumeration algorithm has constant delay and needs a preprocessing time that is\nlinear in the size of the SLP."}
{"id": "2506.19481", "pdf": "https://arxiv.org/pdf/2506.19481", "abs": "https://arxiv.org/abs/2506.19481", "authors": ["Shahbaz Siddeeq", "Muhammad Waseem", "Zeeshan Rasheed", "Md Mahade Hasan", "Jussi Rasku", "Mika Saari", "Henri Terho", "Kalle Makela", "Kai-Kristian Kemell", "Pekka Abrahamsson"], "title": "LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code", "categories": ["cs.SE"], "comment": "arXiv admin note: text overlap with arXiv:2502.07928", "summary": "Refactoring is a constant activity in software development and maintenance.\nScale and maintain software systems are based on code refactoring. However,\nthis process is still labor intensive, as it requires programmers to analyze\nthe codebases in detail to avoid introducing new defects. In this research, we\nput forward a large language model (LLM)-based multi-agent system to automate\nthe refactoring process on Haskell code. The objective of this research is to\nevaluate the effect of LLM-based agents in performing structured and\nsemantically accurate refactoring on Haskell code. Our proposed multi-agent\nsystem based on specialized agents with distinct roles, including code\nanalysis, refactoring execution, verification, and debugging. To test the\neffectiveness and practical applicability of the multi-agent system, we\nconducted evaluations using different open-source Haskell codebases. The\nresults of the experiments carried out showed that the proposed LLM-based\nmulti-agent system could average 11.03% decreased complexity in code, an\nimprovement of 22.46% in overall code quality, and increase performance\nefficiency by an average of 13.27%. Furthermore, memory allocation was\noptimized by up to 14.57%. These results highlight the ability of LLM-based\nmulti-agent in managing refactoring tasks targeted toward functional\nprogramming paradigms. Our findings hint that LLM-based multi-agent systems\nintegration into the refactoring of functional programming languages can\nenhance maintainability and support automated development workflows."}
{"id": "2506.19457", "pdf": "https://arxiv.org/pdf/2506.19457", "abs": "https://arxiv.org/abs/2506.19457", "authors": ["Tom T. P. Franken", "Thomas Neele", "Jan Friso Groote"], "title": "The Autonomous Data Language -- Concepts, Design and Formal Verification", "categories": ["cs.PL", "cs.DC", "D.3.1; F.3.1; F.3.2"], "comment": "48 pages, preprint submitted to Elsevier", "summary": "Nowadays, the main advances in computational power are due to parallelism.\nHowever, most parallel languages have been designed with a focus on processors\nand threads. This makes dealing with data and memory in programs hard, which\ndistances the implementation from its original algorithm. We propose a new\nparadigm for parallel programming, the data-autonomous paradigm, where\ncomputation is performed by autonomous data elements. Programs in this paradigm\nare focused on making the data collaborate in a highly parallel fashion. We\nfurthermore present AuDaLa, the first data autonomous programming language, and\nprovide a full formalisation that includes a type system and operational\nsemantics. Programming in AuDaLa is very natural, as illustrated by examples,\nalbeit in a style very different from sequential and contemporary parallel\nprogramming. Additionally, it lends itself for the formal verification of\nparallel programs, which we demonstrate."}
{"id": "2506.19139", "pdf": "https://arxiv.org/pdf/2506.19139", "abs": "https://arxiv.org/abs/2506.19139", "authors": ["Lukas Radl", "Felix Windisch", "Thomas Deixelberger", "Jozef Hladky", "Michael Steiner", "Dieter Schmalstieg", "Markus Steinberger"], "title": "SOF: Sorted Opacity Fields for Fast Unbounded Surface Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advances in 3D Gaussian representations have significantly improved\nthe quality and efficiency of image-based scene reconstruction. Their explicit\nnature facilitates real-time rendering and fast optimization, yet extracting\naccurate surfaces - particularly in large-scale, unbounded environments -\nremains a difficult task. Many existing methods rely on approximate depth\nestimates and global sorting heuristics, which can introduce artifacts and\nlimit the fidelity of the reconstructed mesh. In this paper, we present Sorted\nOpacity Fields (SOF), a method designed to recover detailed surfaces from 3D\nGaussians with both speed and precision. Our approach improves upon prior work\nby introducing hierarchical resorting and a robust formulation of Gaussian\ndepth, which better aligns with the level-set. To enhance mesh quality, we\nincorporate a level-set regularizer operating on the opacity field and\nintroduce losses that encourage geometrically-consistent primitive shapes. In\naddition, we develop a parallelized Marching Tetrahedra algorithm tailored to\nour opacity formulation, reducing meshing time by up to an order of magnitude.\nAs demonstrated by our quantitative evaluation, SOF achieves higher\nreconstruction accuracy while cutting total processing time by more than a\nfactor of three. These results mark a step forward in turning efficient\nGaussian-based rendering into equally efficient geometry extraction."}
{"id": "2506.19044", "pdf": "https://arxiv.org/pdf/2506.19044", "abs": "https://arxiv.org/abs/2506.19044", "authors": ["Ellis Duncalfe", "Milena Radenkovic"], "title": "Enhancing Evacuation Safety: Detecting Post-Nuclear Event Radiation Levels in an Urban Area", "categories": ["cs.NI"], "comment": null, "summary": "The detonation of an improvised nuclear device (IND) in an urban area would\ncause catastrophic damage, followed by hazardous radioactive fallout. Timely\ndissemination of radiation data is crucial for evacuation and casualty\nreduction. However, conventional communication infrastructure is likely to be\nseverely disrupted. This study designs and builds a pseudorealistic,\ngeospatially and temporally dynamic post-nuclear event (PNE) scenario using the\nOpportunistic Network Environment (ONE) simulator. It integrates radiation\nsensing by emergency responders, unmanned aerial vehicles (UAVs), and civilian\ndevices as dynamic nodes within Delay-Tolerant Networks (DTNs). The performance\nof two DTN routing protocols, Epidemic and PRoPHET, was evaluated across\nmultiple PNE phases. Both protocols achieve high message delivery rates, with\nPRoPHET exhibiting lower network overhead but higher latency. Findings\ndemonstrate the potential of DTN-based solutions to support emergency response\nand evacuation safety by ensuring critical radiation data propagation despite\nsevere infrastructure damage."}
{"id": "2506.19197", "pdf": "https://arxiv.org/pdf/2506.19197", "abs": "https://arxiv.org/abs/2506.19197", "authors": ["Calum Buchanan", "Puck Rombach", "James Bagrow", "Hamid R. Ossareh"], "title": "Vertex addition to a ball graph with application to reliability and area coverage in autonomous swarms", "categories": ["cs.DC", "math.CO", "math.OC", "05C85 (primary), 68R10, 93B51 (secondary)"], "comment": null, "summary": "A unit ball graph consists of a set of vertices, labeled by points in\nEuclidean space, and edges joining all pairs of points within distance $1$.\nThese geometric graphs are used to model a variety of spatial networks,\nincluding communication networks between agents in an autonomous swarm. In such\nan application, vertices and/or edges of the graph may not be perfectly\nreliable; an agent may experience failure or a communication link rendered\ninoperable. With the goal of designing robust swarm formations, or unit ball\ngraphs with high reliability (probability of connectedness), in a preliminary\nconference paper we provided an algorithm with cubic time complexity to\ndetermine all possible changes to a unit ball graph by repositioning a single\nvertex. Using this algorithm and Monte Carlo simulations, one obtains an\nefficient method to modify a unit ball graph by moving a single vertex to a\nlocation which maximizes the reliability. Another important consideration in\nmany swarm missions is area coverage, yet highly reliable ball graphs often\ncontain clusters of vertices. Here, we generalize our previous algorithm to\nimprove area coverage as well as reliability. Our algorithm determines a\nlocation to add or move a vertex within a unit ball graph which maximizes the\nreliability, under the constraint that no other vertices of the graph be within\nsome fixed distance. We compare this method of obtaining graphs with high\nreliability and evenly distributed area coverage to another method which uses a\nmodified Fruchterman-Reingold algorithm for ball graphs."}
{"id": "2506.19017", "pdf": "https://arxiv.org/pdf/2506.19017", "abs": "https://arxiv.org/abs/2506.19017", "authors": ["Lorenzo Porcelli", "Francesco Palmieri"], "title": "Raise Awareness of the Environmental Impacts of Retail Food Products: A User-Centered Scenario-Based Approach", "categories": ["cs.HC"], "comment": null, "summary": "The climate is warming rapidly, and atmospheric concentrations of greenhouse\ngases (GHGs) are at their highest levels ever recorded. As a result of these\nclimate changes, caused mainly by human activities, disasters have increased\nfivefold over the past 50 years, causing death and economic loss. Civic\nengagement and awareness are essential to mitigate climate change and its\nimpacts. In this work, we proposed a user interface that makes users aware of\nthe environmental impact of the food products they buy when shopping. A\nuser-centered scenario-based design was followed in the development of the\ninterface. Gamification elements were added to increase civic participation in\nclimate action."}
{"id": "2506.19379", "pdf": "https://arxiv.org/pdf/2506.19379", "abs": "https://arxiv.org/abs/2506.19379", "authors": ["Subrata Paul", "Sukanta Das", "Biplab K Sikdar"], "title": "In-Memory Sorting-Searching with Cayley Tree", "categories": ["cs.FL", "cs.AR"], "comment": null, "summary": "This work proposes a computing model to reduce the workload of CPU. It relies\non the data intensive computation in memory, where the data reside, and\neffectively realizes an in-memory computing (IMC) platform. Each memory word,\nwith additional logic, acts as a tiny processing element which forms the node\nof a Cayley tree. The Cayley tree in turn defines the framework for solving the\ndata intensive computational problems. It finds the solutions for in-memory\nsearching, computing the max (min) in-memory and in-memory sorting while\nreducing the involvement of CPU. The worst case time complexities of the IMC\nbased solutions for in-memory searching and computing max (min) in-memory are\n$\\mathcal{O}\\log{n}$. Such solutions are independent of the order of elements\nin the list. The worst case time complexity of in-memory sorting, on the other\nhand, is $\\mathcal{O}(n\\log{n})$. Two types of hardware implementations of the\nIMC platform are proposed. One is based on the existing/conventional memory\narchitecture, and the other one is on a newly defined memory architecture. The\nsolutions are further implemented in FPGA platform to prove the effectiveness\nof the IMC architecture while comparing with the state-of-the art designs."}
{"id": "2506.19051", "pdf": "https://arxiv.org/pdf/2506.19051", "abs": "https://arxiv.org/abs/2506.19051", "authors": ["Georgii Bychkov", "Khaled Abud", "Egor Kovalev", "Alexander Gushchin", "Dmitriy Vatolin", "Anastasia Antsiferova"], "title": "NIC-RobustBench: A Comprehensive Open-Source Toolkit for Neural Image Compression and Robustness Analysis", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": "arXiv admin note: text overlap with arXiv:2411.11795", "summary": "Adversarial robustness of neural networks is an increasingly important area\nof research, combining studies on computer vision models, large language models\n(LLMs), and others. With the release of JPEG AI -- the first standard for\nend-to-end neural image compression (NIC) methods -- the question of evaluating\nNIC robustness has become critically significant. However, previous research\nhas been limited to a narrow range of codecs and attacks. To address this, we\npresent \\textbf{NIC-RobustBench}, the first open-source framework to evaluate\nNIC robustness and adversarial defenses' efficiency, in addition to comparing\nRate-Distortion (RD) performance. The framework includes the largest number of\ncodecs among all known NIC libraries and is easily scalable. The paper\ndemonstrates a comprehensive overview of the NIC-RobustBench framework and\nemploys it to analyze NIC robustness. Our code is available online at\nhttps://github.com/msu-video-group/NIC-RobustBench."}
{"id": "2506.19491", "pdf": "https://arxiv.org/pdf/2506.19491", "abs": "https://arxiv.org/abs/2506.19491", "authors": ["Genís Castillo Gómez-Raya", "Álmos Veres-Vitályos", "Filip Lemic", "Pablo Royo", "Mario Montagud", "Sergi Fernández", "Sergi Abadal", "Xavier Costa-Pérez"], "title": "Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications", "categories": ["cs.ET", "cs.AI", "cs.CV", "cs.NI", "eess.IV"], "comment": "6 pages, 7 figures, 2 tables, accepted at IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications 2025", "summary": "The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has\nexpanded their deployment potential to indoor and hard-to-reach areas. However,\nthis trend introduces distinct challenges, particularly in terms of flight\ndynamics and power consumption, which limit the UAVs' autonomy and mission\ncapabilities. This paper presents a novel approach to overcoming these\nlimitations by integrating Neural 3D Reconstruction (N3DR) with small UAV\nsystems for fine-grained 3-Dimensional (3D) digital reconstruction of small\nstatic objects. Specifically, we design, implement, and evaluate an N3DR-based\npipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and\nSplatfacto, to improve the quality of 3D reconstructions using images of the\nobject captured by a fleet of small UAVs. We assess the performance of the\nconsidered models using various imagery and pointcloud metrics, comparing them\nagainst the baseline Structure from Motion (SfM) algorithm. The experimental\nresults demonstrate that the N3DR-enhanced pipeline significantly improves\nreconstruction quality, making it feasible for small UAVs to support\nhigh-precision 3D mapping and anomaly detection in constrained environments. In\nmore general terms, our results highlight the potential of N3DR in advancing\nthe capabilities of miniaturized UAV systems."}
{"id": "2506.19661", "pdf": "https://arxiv.org/pdf/2506.19661", "abs": "https://arxiv.org/abs/2506.19661", "authors": ["Maciej Besta", "Shriram Chandran", "Jakub Cudak", "Patrick Iff", "Marcin Copik", "Robert Gerstenberger", "Tomasz Szydlo", "Jürgen Müller", "Torsten Hoefler"], "title": "Higher-Order Graph Databases", "categories": ["cs.DB", "cs.IR", "cs.LG", "cs.SI"], "comment": null, "summary": "Recent advances in graph databases (GDBs) have been driving interest in\nlarge-scale analytics, yet current systems fail to support higher-order (HO)\ninteractions beyond first-order (one-hop) relations, which are crucial for\ntasks such as subgraph counting, polyadic modeling, and HO graph learning. We\naddress this by introducing a new class of systems, higher-order graph\ndatabases (HO-GDBs) that use lifting and lowering paradigms to seamlessly\nextend traditional GDBs with HO. We provide a theoretical analysis of OLTP and\nOLAP queries, ensuring correctness, scalability, and ACID compliance. We\nimplement a lightweight, modular, and parallelizable HO-GDB prototype that\noffers native support for hypergraphs, node-tuples, subgraphs, and other HO\nstructures under a unified API. The prototype scales to large HO OLTP & OLAP\nworkloads and shows how HO improves analytical tasks, for example enhancing\naccuracy of graph neural networks within a GDB by 44%. Our work ensures low\nlatency and high query throughput, and generalizes both ACID-compliant and\neventually consistent systems."}
{"id": "2506.19568", "pdf": "https://arxiv.org/pdf/2506.19568", "abs": "https://arxiv.org/abs/2506.19568", "authors": ["Gabriel Dengler", "Carlos E. Budde", "Laura Carnevali", "Arnd Hartmanns"], "title": "Time-Sensitive Importance Splitting", "categories": ["cs.LO", "cs.NA", "math.NA"], "comment": "Accepted at QEST+FORMATS 2025", "summary": "State-of-the-art methods for rare event simulation of non-Markovian models\nface practical or theoretical limits if observing the event of interest\nrequires prior knowledge or information on the timed behavior of the system. In\nthis paper, we attack both limits by extending importance splitting with a\ntime-sensitive importance function. To this end, we perform backwards\nreachability search from the target states, considering information about the\nlower and upper bounds of the active timers in order to steer the generation of\npaths towards the rare event. We have developed a prototype implementation of\nthe approach for input/output stochastic automata within the Modest Toolset.\nPreliminary experiments show the potential of the approach in estimating rare\nevent probabilities for an example from reliability engineering."}
{"id": "2506.19511", "pdf": "https://arxiv.org/pdf/2506.19511", "abs": "https://arxiv.org/abs/2506.19511", "authors": ["Nina Haugland Andersen", "Anastasiia Tkalich", "Nils Brede Moe", "Darja Smite", "Asgaut Mjølne Söderbom", "Ola Hast", "Viktoria Stray"], "title": "Integrating Pair Programming as a Work Practice", "categories": ["cs.SE"], "comment": "The pre-print is submitted to the Journal of Systems and Software", "summary": "Context: Pair programming (PP) is more relevant than ever. As modern systems\ngrow in complexity, knowledge sharing and collaboration across teams have\nbecome essential. However, despite well-documented benefits of PP, its adoption\nremains inconsistent across software teams. Objective: This study aims to\nunderstand the factors that facilitate or hinder team members' adoption as well\nas lasting engagement in PP. Method: We have conducted an exploratory\nsingle-case study in a mature agile company in Norway. We collected data\nthrough two rounds of interviews with team members in different roles and\nperformed a thematic analysis of the interviews. Results: Our key finding is\nthat multiple factors, related to the perceptions of how PP contributes to\ndaily work, efforts associated with engaging in PP sessions, company and team\nattitudes, resources, infrastructure, and task characteristics, affect PP\nengagement. Conclusion: Long-term engagement in PP requires expected benefits\nwith the practice being confirmed in firsthand experiences. Adapting the\npractice to each unique team, with insights drawn from collective learning, is\nalso beneficial. Our findings will be beneficial for software practitioners\nseeking to make PP an integrated part of their team's workflow."}
{"id": "2506.19278", "pdf": "https://arxiv.org/pdf/2506.19278", "abs": "https://arxiv.org/abs/2506.19278", "authors": ["Tianshan Zhang", "Hao Tang"], "title": "Style Transfer: A Decade Survey", "categories": ["cs.GR"], "comment": "32 pages", "summary": "The revolutionary advancement of Artificial Intelligence Generated Content\n(AIGC) has fundamentally transformed the landscape of visual content creation\nand artistic expression. While remarkable progress has been made in image\ngeneration and style transfer, the underlying mechanisms and aesthetic\nimplications of these technologies remain insufficiently understood. This paper\npresents a comprehensive survey of AIGC technologies in visual arts, tracing\ntheir evolution from early algorithmic frameworks to contemporary deep\ngenerative models. We identify three pivotal paradigms: Variational\nAutoencoders (VAE), Generative Adversarial Networks (GANs), and Diffusion\nModels, and examine their roles in bridging the gap between human creativity\nand machine synthesis. To support our analysis, we systematically review over\n500 research papers published in the past decade, spanning both foundational\ndevelopments and state-of-the-art innovations. Furthermore, we propose a\nmultidimensional evaluation framework that incorporates Technical Innovation,\nArtistic Merit, Visual Quality, Computational Efficiency, and Creative\nPotential. Our findings reveal both the transformative capacities and current\nlimitations of AIGC systems, emphasizing their profound impact on the future of\ncreative practices. Through this extensive synthesis, we offer a unified\nperspective on the convergence of artificial intelligence and artistic\nexpression, while outlining key challenges and promising directions for future\nresearch in this rapidly evolving field."}
{"id": "2506.19304", "pdf": "https://arxiv.org/pdf/2506.19304", "abs": "https://arxiv.org/abs/2506.19304", "authors": ["Zhiyi Zhu", "Eiji Takimoto", "Patrick Finnerty", "Chikara Ohta"], "title": "A Study on E2E Performance Improvement of Platooning Using Outdoor LiFi", "categories": ["cs.NI"], "comment": "2 pages", "summary": "Platooning within autonomous vehicles has proven effective in addressing\ndriver shortages and reducing fuel consumption. However, as platooning lengths\nincrease, traditional C-V2X (cellular vehicle-to-everything) architectures are\nsusceptible to end-to-end (E2E) latency increases. This is due to the necessity\nof relaying information through multiple hops from the leader vehicle to the\nlast vehicle. To address this problem, this paper proposes a hybrid\ncommunication architecture based on a simulation that integrates light fidelity\n(LiFi) and C-V2X. The proposed architecture introduces multiple-leader vehicles\nequipped with outdoor LiFi communication nodes in platoons to achieve\nhigh-speed and low-delay communication between leader vehicles, which reduces\nE2E delay."}
{"id": "2506.19233", "pdf": "https://arxiv.org/pdf/2506.19233", "abs": "https://arxiv.org/abs/2506.19233", "authors": ["Guy Goren", "Andrew Hariri", "Timothy D. R. Hartley", "Ravi Kappiyoor", "Alexander Spiegelman", "David Zmick"], "title": "Shelby: Decentralized Storage Designed to Serve", "categories": ["cs.DC"], "comment": null, "summary": "Existing decentralized storage protocols fall short of the service required\nby real-world applications. Their throughput, latency, cost-effectiveness, and\navailability are insufficient for demanding workloads such as video streaming,\nlarge-scale data analytics, or AI training. As a result, Web3 data-intensive\napplications are predominantly dependent on centralized infrastructure.\n  Shelby is a high-performance decentralized storage protocol designed to meet\ndemanding needs. It achieves fast, reliable access to large volumes of data\nwhile preserving decentralization guarantees. The architecture reflects lessons\nfrom Web2 systems: it separates control and data planes, uses erasure coding\nwith low replication overhead and minimal repair bandwidth, and operates over a\ndedicated backbone connecting RPC and storage nodes. Reads are paid, which\nincentivizes good performance. Shelby also introduces a novel auditing protocol\nthat provides strong cryptoeconomic guarantees without compromising\nperformance, a common limitation of other decentralized solutions. The result\nis a decentralized system that brings Web2-grade performance to\nproduction-scale, read-intensive Web3 applications."}
{"id": "2506.19107", "pdf": "https://arxiv.org/pdf/2506.19107", "abs": "https://arxiv.org/abs/2506.19107", "authors": ["Ruiwei Xiao", "Xinying Hou", "Runlong Ye", "Majeed Kazemitabaar", "Nicholas Diana", "Michael Liut", "John Stamper"], "title": "Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education", "categories": ["cs.HC", "cs.AI"], "comment": "Under review for Computer & Education: Artificial Intelligence.\n  Journal policy allows submitting as preprint", "summary": "With the proliferation of large language model (LLM) applications since 2022,\ntheir use in education has sparked both excitement and concern. Recent studies\nconsistently highlight students' (mis)use of LLMs can hinder learning outcomes.\nThis work aims to teach students how to effectively prompt LLMs to improve\ntheir learning. We first proposed pedagogical prompting, a\ntheoretically-grounded new concept to elicit learning-oriented responses from\nLLMs. To move from concept design to a proof-of-concept learning intervention\nin real educational settings, we selected early undergraduate CS education\n(CS1/CS2) as the example context. We began with a formative survey study with\ninstructors (N=36) teaching early-stage undergraduate-level CS courses to\ninform the instructional design based on classroom needs. Based on their\ninsights, we designed and developed a learning intervention through an\ninteractive system with scenario-based instruction to train pedagogical\nprompting skills. Finally, we evaluated its instructional effectiveness through\na user study with CS novice students (N=22) using pre/post-tests. Through mixed\nmethods analyses, our results indicate significant improvements in learners'\nLLM-based pedagogical help-seeking skills, along with positive attitudes toward\nthe system and increased willingness to use pedagogical prompts in the future.\nOur contributions include (1) a theoretical framework of pedagogical prompting;\n(2) empirical insights into current instructor attitudes toward pedagogical\nprompting; and (3) a learning intervention design with an interactive learning\ntool and scenario-based instruction leading to promising results on teaching\nLLM-based help-seeking. Our approach is scalable for broader implementation in\nclassrooms and has the potential to be integrated into tools like ChatGPT as an\non-boarding experience to encourage learning-oriented use of generative AI."}
{"id": "2506.19597", "pdf": "https://arxiv.org/pdf/2506.19597", "abs": "https://arxiv.org/abs/2506.19597", "authors": ["Haruki Uchiito", "Akhilesh Bhat", "Koji Kusaka", "Xiaoya Zhang", "Hiraku Kinjo", "Honoka Uehara", "Motoki Koyama", "Shinji Natsume"], "title": "Robotics Under Construction: Challenges on Job Sites", "categories": ["cs.RO", "cs.AI", "cs.AR", "cs.ET", "cs.SY", "eess.SY"], "comment": "Workshop on Field Robotics, ICRA", "summary": "As labor shortages and productivity stagnation increasingly challenge the\nconstruction industry, automation has become essential for sustainable\ninfrastructure development. This paper presents an autonomous payload\ntransportation system as an initial step toward fully unmanned construction\nsites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous\nnavigation, fleet management, and GNSS-based localization to facilitate\nmaterial transport in construction site environments. While the current system\ndoes not yet incorporate dynamic environment adaptation algorithms, we have\nbegun fundamental investigations into external-sensor based perception and\nmapping system. Preliminary results highlight the potential challenges,\nincluding navigation in evolving terrain, environmental perception under\nconstruction-specific conditions, and sensor placement optimization for\nimproving autonomy and efficiency. Looking forward, we envision a construction\necosystem where collaborative autonomous agents dynamically adapt to site\nconditions, optimizing workflow and reducing human intervention. This paper\nprovides foundational insights into the future of robotics-driven construction\nautomation and identifies critical areas for further technological development."}
{"id": "2506.19642", "pdf": "https://arxiv.org/pdf/2506.19642", "abs": "https://arxiv.org/abs/2506.19642", "authors": ["B. Paroli", "F. Borghi", "M. A. C. Potenza", "P. Milani"], "title": "The receptron is a nonlinear threshold logic gate with intrinsic multi-dimensional selective capabilities for analog inputs", "categories": ["cs.ET", "cs.AI"], "comment": "12 pages, 7 figures", "summary": "Threshold logic gates (TLGs) have been proposed as artificial counterparts of\nbiological neurons with classification capabilities based on a linear predictor\nfunction combining a set of weights with the feature vector. The linearity of\nTLGs limits their classification capabilities requiring the use of networks for\nthe accomplishment of complex tasks. A generalization of the TLG model called\nreceptron, characterized by input-dependent weight functions allows for a\nsignificant enhancement of classification performances even with the use of a\nsingle unit. Here we formally demonstrate that a receptron, characterized by\nnonlinear input-dependent weight functions, exhibit intrinsic selective\nactivation properties for analog inputs, when the input vector is within cubic\ndomains in a 3D space. The proposed model can be extended to the n-dimensional\ncase for multidimensional applications. Our results suggest that\nreceptron-based networks can represent a new class of devices capable to manage\na large number of analog inputs, for edge applications requiring high\nselectivity and classification capabilities without the burden of complex\ntraining."}
{"id": "2506.18916", "pdf": "https://arxiv.org/pdf/2506.18916", "abs": "https://arxiv.org/abs/2506.18916", "authors": ["Ganesh Parab", "Zishan Ahmad", "Dagnachew Birru"], "title": "HI-SQL: Optimizing Text-to-SQL Systems through Dynamic Hint Integration", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "Text-to-SQL generation bridges the gap between natural language and\ndatabases, enabling users to query data without requiring SQL expertise. While\nlarge language models (LLMs) have significantly advanced the field, challenges\nremain in handling complex queries that involve multi-table joins, nested\nconditions, and intricate operations. Existing methods often rely on multi-step\npipelines that incur high computational costs, increase latency, and are prone\nto error propagation. To address these limitations, we propose HI-SQL, a\npipeline that incorporates a novel hint generation mechanism utilizing\nhistorical query logs to guide SQL generation. By analyzing prior queries, our\nmethod generates contextual hints that focus on handling the complexities of\nmulti-table and nested operations. These hints are seamlessly integrated into\nthe SQL generation process, eliminating the need for costly multi-step\napproaches and reducing reliance on human-crafted prompts. Experimental\nevaluations on multiple benchmark datasets demonstrate that our approach\nsignificantly improves query accuracy of LLM-generated queries while ensuring\nefficiency in terms of LLM calls and latency, offering a robust and practical\nsolution for enhancing Text-to-SQL systems."}
{"id": "2506.19746", "pdf": "https://arxiv.org/pdf/2506.19746", "abs": "https://arxiv.org/abs/2506.19746", "authors": ["Georg Schindling"], "title": "Homomorphism Indistinguishability and Game Comonads for Restricted Conjunction and Requantification", "categories": ["cs.LO", "03C13", "F.4.1; G.2.2"], "comment": "Full version of a conference paper accepted for publication at MFCS\n  2025", "summary": "The notion of homomorphism indistinguishability offers a combinatorial\nframework for characterizing equivalence relations of graphs, in particular\nequivalences in counting logics within finite model theory. That is, for\ncertain graph classes, two structures agree on all homomorphism counts from the\nclass if and only if they satisfy the same sentences in a corresponding logic.\nThis perspective often reveals connections between the combinatorial properties\nof graph classes and the syntactic structure of logical fragments. In this\nwork, we extend this perspective to logics with restricted requantification,\nrefining the stratification of logical resources in finite-variable counting\nlogics. Specifically, we generalize Lov\\'asz-type theorems for these logics\nwith either restricted conjunction or bounded quantifier-rank and present new\ncombinatorial proofs of existing results. To this end, we introduce novel path\nand tree decompositions that incorporate the concept of reusability and develop\ncharacterizations based on pursuit-evasion games. Leveraging this framework, we\nestablish that classes of bounded pathwidth and treewidth with reusability\nconstraints are homomorphism distinguishing closed. Finally, we develop a\ncomonadic perspective on requantification by constructing new comonads that\nencapsulate restricted-reusability pebble games. We show a tight correspondence\nbetween their coalgebras and path/tree decompositions, yielding categorical\ncharacterizations of reusability in graph decompositions. This unifies logical,\ncombinatorial, and categorical perspectives on the notion of reusability."}
{"id": "2506.19539", "pdf": "https://arxiv.org/pdf/2506.19539", "abs": "https://arxiv.org/abs/2506.19539", "authors": ["Julian Fragner", "Christian Macho", "Bernhard Dieber", "Martin Pinzger"], "title": "Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language", "categories": ["cs.SE", "cs.AI", "D.2.7"], "comment": "18 pages, 7 tables, 18 figures", "summary": "Log files provide valuable information for detecting and diagnosing problems\nin enterprise software applications and data centers. Several log analytics\ntools and platforms were developed to help filter and extract information from\nlogs, typically using regular expressions (RegExes). Recent commercial log\nanalytics platforms provide domain-specific languages specifically designed for\nlog parsing, such as Grok or the Dynatrace Pattern Language (DPL). However,\nusers who want to migrate to these platforms must manually convert their\nRegExes into the new pattern language, which is costly and error-prone. In this\nwork, we present Reptile, which combines a rule-based approach for converting\nRegExes into DPL patterns with a best-effort approach for cases where a full\nconversion is impossible. Furthermore, it integrates GPT-4 to optimize the\nobtained DPL patterns. The evaluation with 946 RegExes collected from a large\ncompany shows that Reptile safely converted 73.7% of them. The evaluation of\nReptile's pattern optimization with 23 real-world RegExes showed an F1-score\nand MCC above 0.91. These results are promising and have ample practical\nimplications for companies that migrate to a modern log analytics platform,\nsuch as Dynatrace."}
{"id": "2506.19400", "pdf": "https://arxiv.org/pdf/2506.19400", "abs": "https://arxiv.org/abs/2506.19400", "authors": ["Liang Zhou", "Xinyi Gou", "Daniel Weiskopf"], "title": "Continuous Indexed Points for Multivariate Volume Visualization", "categories": ["cs.GR"], "comment": "Peer reviewed and accepted by Computational Visual Media", "summary": "We introduce continuous indexed points for improved multivariate volume\nvisualization. Indexed points represent linear structures in parallel\ncoordinates and can be used to encode local correlation of multivariate\n(including multifield, multifaceted, and multiattribute) volume data. First, we\nperform local linear fitting in the spatial neighborhood of each volume sample\nusing principal component analysis, accelerated by hierarchical spatial data\nstructures. This local linear information is then visualized as continuous\nindexed points in parallel coordinates: a density representation of indexed\npoints in a continuous domain. With our new method, multivariate volume data\ncan be analyzed using the eigenvector information from local spatial\nembeddings. We utilize both 1-flat and 2-flat indexed points, allowing us to\nidentify correlations between two variables and even three variables,\nrespectively. An interactive occlusion shading model facilitates good spatial\nperception of the volume rendering of volumetric correlation characteristics.\nInteractive exploration is supported by specifically designed multivariate\ntransfer function widgets working in the image plane of parallel coordinates.\nWe show that our generic technique works for multi-attribute datasets. The\neffectiveness and usefulness of our new method is demonstrated through a case\nstudy, an expert user study, and domain expert feedback."}
{"id": "2506.19366", "pdf": "https://arxiv.org/pdf/2506.19366", "abs": "https://arxiv.org/abs/2506.19366", "authors": ["Marat Zaidyn", "Sayat Akhtanov", "Dana Turlykozhayeva", "Symbat Temesheva", "Almat Akhmetali", "Alisher Skabylov", "Nurzhan Ussipov"], "title": "Fractality of Wireless Mesh Networks: Dimensional Effects on Network Performance", "categories": ["cs.NI", "cs.CG"], "comment": "11 pages, 7 figures, 2 tables", "summary": "Wireless mesh networks (WMNs) depend on the spatial distribution of nodes,\nwhich directly influences connectivity, routing efficiency, and overall network\nperformance. Conventional models typically assume uniform or random node\nplacement, which inadequately represent the complex, hierarchical spatial\npatterns observed in practical deployments. In this study, we present a novel\nalgorithm that constructs WMN topologies with tunable fractal dimensions,\nallowing precise control over spatial self-similarity. By systematically\nvarying the fractal dimension, the algorithm generates network layouts spanning\na continuum of spatial complexities, ranging from sparse fragmented clusters to\ndense, cohesive structures. Through NS-3 simulations, Key performance metrics\nincluding throughput, latency, jitter, and packet delivery ratio were evaluated\nacross a range of fractal dimensions. Comparative evaluations against classical\nrandom, small-world, and scale-free network models reveal that high-dimensional\nfractal topologies achieve enhanced resilience and throughput under equivalent\nconditions. These findings demonstrate the potential of fractal geometry as a\ndesign paradigm for scalable and efficient WMN architectures."}
{"id": "2506.19333", "pdf": "https://arxiv.org/pdf/2506.19333", "abs": "https://arxiv.org/abs/2506.19333", "authors": ["Craig Steven Wright"], "title": "The Autonomy of the Lightning Network: A Mathematical and Economic Proof of Structural Decoupling from BTC", "categories": ["cs.DC", "cs.CC", "cs.ET", "cs.GT", "econ.GN", "q-fin.EC", "68M10, 91B50, 94A15", "C.2.2; G.2.2; H.1.1"], "comment": "59 pages, 4 figures, includes TikZ diagrams and formal proofs.\n  Targeted for journal submission", "summary": "This paper presents a formal analysis of the Lightning Network as a monetary\nsystem structurally diverging from Bitcoin's base-layer settlement model. We\ndemonstrate that under increasing transaction demand, BTC transaction fees rise\nsuperlinearly due to throughput constraints, while Lightning Network routing\ncosts approach a bounded asymptote. Using mathematical modeling, game-theoretic\nproofs, and complexity analysis, we show that Lightning enables indefinite\noff-chain operation via the emergence of liquidity hub oligopolies. These hubs\nexhibit properties of unregulated financial intermediaries, including rent\nextraction, opacity, and systemic fragility. Strategic agent models show that\nchannel closure becomes economically infeasible, and routing problems approach\nhardness limits in P-Space complexity. We conclude that Lightning does not\nmerely extend Bitcoin, but constitutes a synthetic financial system with\nshadowbank characteristics, lacking reserve discipline, transparency, or\nenforceable settlement guarantees."}
{"id": "2506.19210", "pdf": "https://arxiv.org/pdf/2506.19210", "abs": "https://arxiv.org/abs/2506.19210", "authors": ["Bhanuka Gamage", "Nicola McDowell", "Dijana Kovacic", "Leona Holloway", "Thanh-Toan Do", "Nicholas Price", "Arthur Lowery", "Kim Marriott"], "title": "Smart Glasses for CVI: Co-Designing Extended Reality Solutions to Support Environmental Perception by People with Cerebral Visual Impairment", "categories": ["cs.HC"], "comment": "Author's conditionally accepted version of a paper to be published at\n  ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '25)", "summary": "Cerebral Visual Impairment (CVI) is the set to be the leading cause of vision\nimpairment, yet remains underrepresented in assistive technology research.\nUnlike ocular conditions, CVI affects higher-order visual processing-impacting\nobject recognition, facial perception, and attention in complex environments.\nThis paper presents a co-design study with two adults with CVI investigating\nhow smart glasses, i.e. head-mounted extended reality displays, can support\nunderstanding and interaction with the immediate environment. Guided by the\nDouble Diamond design framework, we conducted a two-week diary study, two\nideation workshops, and ten iterative development sessions using the Apple\nVision Pro. Our findings demonstrate that smart glasses can meaningfully\naddress key challenges in locating objects, reading text, recognising people,\nengaging in conversations, and managing sensory stress. With the rapid\nadvancement of smart glasses and increasing recognition of CVI as a distinct\nform of vision impairment, this research addresses a timely and under-explored\nintersection of technology and need."}
{"id": "2506.18941", "pdf": "https://arxiv.org/pdf/2506.18941", "abs": "https://arxiv.org/abs/2506.18941", "authors": ["Neha Rani", "Sharan Majumder", "Ishan Bhardwaj", "Pedro Guillermo Feijoo Garcia"], "title": "Can AI support student engagement in classroom activities in higher education?", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "Lucrative career prospects and creative opportunities often attract students\nto enroll in computer science majors and pursue advanced studies in the field.\nConsequently, there has been a significant surge in enrollment in computer\nscience courses, resulting in large class sizes that can range from hundreds to\neven thousands of students. A common challenge in such large classrooms is the\nlack of engagement between students and both the instructor and the learning\nmaterial. However, with advancements in technology and improvements in large\nlanguage models (LLMs), there is a considerable opportunity to utilize\nLLM-based AI models, such as conversational artificial intelligence (CAI), to\nenhance student engagement with learning content in large classes. To explore\nthe potential of CAI to support engagement, especially with learning content,\nwe designed an activity in a software Engineering course (with a large class\nsize) where students used CAI for an in-class activity. We conducted a\nwithin-subject investigation in a large classroom at a US university where we\ncompared student engagement during an in-class activity that used CAI tool vs.\none without CAI tool. The CAI tool we used was ChatGPT due to its widespread\npopularity and familiarity. Our results indicate that CAI (ChatGPT) has the\npotential to support engagement with learning content during in-class\nactivities, especially in large class sizes. We further discuss the\nimplications of our findings."}
{"id": "2506.19402", "pdf": "https://arxiv.org/pdf/2506.19402", "abs": "https://arxiv.org/abs/2506.19402", "authors": ["Samuel Mimram", "Émile Oleon"], "title": "Hypercubical manifolds in homotopy type theory", "categories": ["math.AT", "cs.LO"], "comment": null, "summary": "Homotopy type theory is a logical setting in which one can perform geometric\nconstructions and proofs in a synthetic way. Namely, types can be interpreted\nas spaces up to homotopy, and proofs as homotopy invariant constructions. In\nthis context, we introduce a type which corresponds to the hypercubical\nmanifold, a space first introduced by Poincar\\'e in 1895. Its importance stems\nfrom the fact that it provides an approximation of the group Q of quaternionic\nunits, in the sense of being the first step of a cellular resolution of Q. In\norder to ensure the validity of our definition, we show that it satisfies the\nexpected property: it is the homotopy quotient of the 3-sphere by the expected\naction of Q. This is non-trivial and requires performing subtle combinatorial\ncomputations based on the flattening lemma, thus illustrating the effective\nnature of homotopy type theory. Finally, based on the previous construction, we\nintroduce new higher-dimensional generalizations of this manifold, which\ncorrespond to better cellular approximations of Q, converging toward a\ndelooping of Q."}
{"id": "2506.19653", "pdf": "https://arxiv.org/pdf/2506.19653", "abs": "https://arxiv.org/abs/2506.19653", "authors": ["Antonios Saravanos"], "title": "Simulating the Waterfall Model: A Systematic Review", "categories": ["cs.SE"], "comment": null, "summary": "This systematic mapping study examines how the Waterfall Model has been\nrepresented in computational simulations within peer-reviewed literature. While\nAgile methodologies dominate contemporary software design practices, the\nWaterfall Model persists, particularly, within hybrid approaches that fuse\nstructured, sequential workflows with the adaptability of agile practices.\nDespite its continued presence, little attention has been given to how the\nWaterfall Model is simulated in research contexts. A structured search of major\nacademic databases identified 68 peer-reviewed studies published between 2000\nand 2024. After applying inclusion criteria, selected studies were analyzed\nacross four dimensions: (1) simulation methodologies (e.g., discrete-event\nsimulation, system dynamics), (2) platforms and tools (e.g., Simphony.NET,\nSimPy), (3) geographic and temporal trends, and (4) fidelity to Royce's\noriginal seven-phase model. Discrete-event simulation was most commonly used,\nreflecting the model's sequential nature. Early work relied on proprietary\nplatforms, while recent studies increasingly use open-source, Python-based\ntools. No studies fully implemented Royce's original formulation, most employed\nadaptations. These findings suggest that although niche, simulation of the\nWaterfall Model is present in academic discourse. This work highlights the need\nfor accessible modeling tools and calls for future research that integrates the\nwaterfall software process model with modern hybrid practices."}
{"id": "2506.19415", "pdf": "https://arxiv.org/pdf/2506.19415", "abs": "https://arxiv.org/abs/2506.19415", "authors": ["Jonathan Haberl", "Philipp Fleck", "Clemens Arth"], "title": "Virtual Memory for 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.HC"], "comment": "Based on the Master Thesis from Jonathan Haberl from 2024, Submitted\n  to TVCG in Feb. 2025;", "summary": "3D Gaussian Splatting represents a breakthrough in the field of novel view\nsynthesis. It establishes Gaussians as core rendering primitives for highly\naccurate real-world environment reconstruction. Recent advances have\ndrastically increased the size of scenes that can be created. In this work, we\npresent a method for rendering large and complex 3D Gaussian Splatting scenes\nusing virtual memory. By leveraging well-established virtual memory and virtual\ntexturing techniques, our approach efficiently identifies visible Gaussians and\ndynamically streams them to the GPU just in time for real-time rendering.\nSelecting only the necessary Gaussians for both storage and rendering results\nin reduced memory usage and effectively accelerates rendering, especially for\nhighly complex scenes. Furthermore, we demonstrate how level of detail can be\nintegrated into our proposed method to further enhance rendering speed for\nlarge-scale scenes. With an optimized implementation, we highlight key\npractical considerations and thoroughly evaluate the proposed technique and its\nimpact on desktop and mobile devices."}
{"id": "2506.19760", "pdf": "https://arxiv.org/pdf/2506.19760", "abs": "https://arxiv.org/abs/2506.19760", "authors": ["Antonio Calagna", "Stefano Maxenti", "Leonardo Bonati", "Salvatore D'Oro", "Tommaso Melodia", "Carla Fabiana Chiasserini"], "title": "CORMO-RAN: Lossless Migration of xApps in O-RAN", "categories": ["cs.NI"], "comment": "14 pages, 16 figures, 4 tables", "summary": "Open Radio Access Network (RAN) is a key paradigm to attain unprecedented\nflexibility of the RAN via disaggregation and Artificial Intelligence\n(AI)-based applications called xApps. In dense areas with many active RAN\nnodes, compute resources are engineered to support potentially hundreds of\nxApps monitoring and controlling the RAN to achieve operator's intents.\nHowever, such resources might become underutilized during low-traffic periods,\nwhere most cells are sleeping and, given the reduced RAN complexity, only a few\nxApps are needed for its control. In this paper, we propose CORMO-RAN, a\ndata-driven orchestrator that dynamically activates compute nodes based on xApp\nload to save energy, and performs lossless migration of xApps from nodes to be\nturned off to active ones while ensuring xApp availability during migration.\nCORMO-RAN tackles the trade-off among service availability, scalability, and\nenergy consumption while (i) preserving xApps' internal state to prevent RAN\nperformance degradation during migration; (ii) accounting for xApp diversity in\nstate size and timing constraints; and (iii) implementing several migration\nstrategies and providing guidelines on best strategies to use based on resource\navailability and requirements. We prototype CORMO-RAN as an rApp, and\nexperimentally evaluate it on an O-RAN private 5G testbed hosted on a Red Hat\nOpenShift cluster with commercial radio units. Results demonstrate that\nCORMO-RAN is effective in minimizing energy consumption of the RAN Intelligent\nController (RIC) cluster, yielding up to 64% energy saving when compared to\nexisting approaches."}
{"id": "2506.19349", "pdf": "https://arxiv.org/pdf/2506.19349", "abs": "https://arxiv.org/abs/2506.19349", "authors": ["Huashan Yu", "Xiaolin Wang", "Yingwei Luo"], "title": "A Heuristic Algorithm for Shortest Path Search", "categories": ["cs.DC"], "comment": null, "summary": "The Single-Source Shortest Path (SSSP) problem is well-known for the\nchallenges in developing fast, practical, and work-efficient parallel\nalgorithms. This work introduces a novel shortest path search method. It allows\npaths with different lengths to be extended in parallel at the cost of almost\nnegligible repeated relaxations. A dynamic-stepping heuristic is proposed for\nthe method to efficiently reduce the extended paths and the synchronizations. A\ntraversal-optimization heuristic is proposed to improve the method by\nefficiently reducing the created paths and alleviating the load imbalance.\nBased on the method, the two heuristics are used to develop a practical SSSP\nalgorithm, which tactfully reduces workload and overhead. The heuristics and\nthe algorithm were evaluated on 73 real-world and synthetic graphs. The\nalgorithm was also compared with five state-of-the-art SSSP implementations. On\neach GAP benchmark suite graph except Road, its speedup to the best achieved by\nthese five implementations is 2.5x to 5.83x."}
{"id": "2506.19268", "pdf": "https://arxiv.org/pdf/2506.19268", "abs": "https://arxiv.org/abs/2506.19268", "authors": ["Timoteo Kelly", "Abdulkadir Korkmaz", "Samuel Mallet", "Connor Souders", "Sadra Aliakbarpour", "Praveen Rao"], "title": "HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps", "categories": ["cs.HC", "cs.CR", "cs.ET", "cs.LG"], "comment": "Under review at The 34th ACM International Conference on Information\n  and Knowledge Management (CIKM'25)", "summary": "We present HARPT, a large-scale annotated corpus of mobile health app store\nreviews aimed at advancing research in user privacy and trust. The dataset\ncomprises over 480,000 user reviews labeled into seven categories that capture\ncritical aspects of trust in applications, trust in providers and privacy\nconcerns. Creating HARPT required addressing multiple complexities, such as\ndefining a nuanced label schema, isolating relevant content from large volumes\nof noisy data, and designing an annotation strategy that balanced scalability\nwith accuracy. This strategy integrated rule-based filtering, iterative manual\nlabeling with review, targeted data augmentation, and weak supervision using\ntransformer-based classifiers to accelerate coverage. In parallel, a carefully\ncurated subset of 7,000 reviews was manually annotated to support model\ndevelopment and evaluation. We benchmark a broad range of classification\nmodels, demonstrating that strong performance is achievable and providing a\nbaseline for future research. HARPT is released as a public resource to support\nwork in health informatics, cybersecurity, and natural language processing."}
{"id": "2506.19136", "pdf": "https://arxiv.org/pdf/2506.19136", "abs": "https://arxiv.org/abs/2506.19136", "authors": ["Cyrill Bösch", "Geoffrey Roeder", "Marc Serra-Garcia", "Ryan P. Adams"], "title": "Local Learning Rules for Out-of-Equilibrium Physical Generative Models", "categories": ["cs.LG", "cond-mat.mes-hall", "cs.ET", "cs.NE"], "comment": "6 pages, 2 figures", "summary": "We show that the out-of-equilibrium driving protocol of score-based\ngenerative models (SGMs) can be learned via a local learning rule. The gradient\nwith respect to the parameters of the driving protocol are computed directly\nfrom force measurements or from observed system dynamics. As a demonstration,\nwe implement an SGM in a network of driven, nonlinear, overdamped oscillators\ncoupled to a thermal bath. We first apply it to the problem of sampling from a\nmixture of two Gaussians in 2D. Finally, we train a network of 10x10\noscillators to sample images of 0s and 1s from the MNIST dataset."}
{"id": "2506.19677", "pdf": "https://arxiv.org/pdf/2506.19677", "abs": "https://arxiv.org/abs/2506.19677", "authors": ["Shi Chang", "Boyuan Chen", "Kishanthan Thangarajah", "Hanan Lutfiyya", "Ahmed E. Hassan"], "title": "Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees", "categories": ["cs.SE"], "comment": null, "summary": "Code Large Language Models (CodeLLMs) are increasingly integrated into modern\nsoftware development workflows, yet efficiently serving them in\nresource-constrained, self-hosted environments remains a significant challenge.\nExisting LLM serving systems employs Continuous Batching for throughput\nimprovement. However, they rely on static batch size configurations that cannot\nadapt to fluctuating request rates or heterogeneous workloads, leading to\nfrequent SLA (Service Level Agreement) violations and unstable performance. In\nthis study, We propose SABER, a dynamic batching strategy that predicts\nper-request SLA feasibility and adjusts decisions in real time. SABER improves\ngoodput by up to 26% over the best static configurations and reduces latency\nvariability by up to 45%, all without manual tuning or service restarts. Our\nresults demonstrate that SLA-aware, adaptive scheduling is key to robust,\nhigh-performance CodeLLM serving."}
{"id": "2506.19708", "pdf": "https://arxiv.org/pdf/2506.19708", "abs": "https://arxiv.org/abs/2506.19708", "authors": ["Matyas Bohacek", "Thomas Fel", "Maneesh Agrawala", "Ekdeep Singh Lubana"], "title": "Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite their impressive performance, generative image models trained on\nlarge-scale datasets frequently fail to produce images with seemingly simple\nconcepts -- e.g., human hands or objects appearing in groups of four -- that\nare reasonably expected to appear in the training data. These failure modes\nhave largely been documented anecdotally, leaving open the question of whether\nthey reflect idiosyncratic anomalies or more structural limitations of these\nmodels. To address this, we introduce a systematic approach for identifying and\ncharacterizing \"conceptual blindspots\" -- concepts present in the training data\nbut absent or misrepresented in a model's generations. Our method leverages\nsparse autoencoders (SAEs) to extract interpretable concept embeddings,\nenabling a quantitative comparison of concept prevalence between real and\ngenerated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with\n32,000 concepts -- the largest such SAE to date -- enabling fine-grained\nanalysis of conceptual disparities. Applied to four popular generative models\n(Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals\nspecific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces\non documents) and exaggerated blindspots (e.g., wood background texture and\npalm trees). At the individual datapoint level, we further isolate memorization\nartifacts -- instances where models reproduce highly specific visual templates\nseen during training. Overall, we propose a theoretically grounded framework\nfor systematically identifying conceptual blindspots in generative models by\nassessing their conceptual fidelity with respect to the underlying\ndata-generating process."}
{"id": "2506.19491", "pdf": "https://arxiv.org/pdf/2506.19491", "abs": "https://arxiv.org/abs/2506.19491", "authors": ["Genís Castillo Gómez-Raya", "Álmos Veres-Vitályos", "Filip Lemic", "Pablo Royo", "Mario Montagud", "Sergi Fernández", "Sergi Abadal", "Xavier Costa-Pérez"], "title": "Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications", "categories": ["cs.ET", "cs.AI", "cs.CV", "cs.NI", "eess.IV"], "comment": "6 pages, 7 figures, 2 tables, accepted at IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications 2025", "summary": "The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has\nexpanded their deployment potential to indoor and hard-to-reach areas. However,\nthis trend introduces distinct challenges, particularly in terms of flight\ndynamics and power consumption, which limit the UAVs' autonomy and mission\ncapabilities. This paper presents a novel approach to overcoming these\nlimitations by integrating Neural 3D Reconstruction (N3DR) with small UAV\nsystems for fine-grained 3-Dimensional (3D) digital reconstruction of small\nstatic objects. Specifically, we design, implement, and evaluate an N3DR-based\npipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and\nSplatfacto, to improve the quality of 3D reconstructions using images of the\nobject captured by a fleet of small UAVs. We assess the performance of the\nconsidered models using various imagery and pointcloud metrics, comparing them\nagainst the baseline Structure from Motion (SfM) algorithm. The experimental\nresults demonstrate that the N3DR-enhanced pipeline significantly improves\nreconstruction quality, making it feasible for small UAVs to support\nhigh-precision 3D mapping and anomaly detection in constrained environments. In\nmore general terms, our results highlight the potential of N3DR in advancing\nthe capabilities of miniaturized UAV systems."}
{"id": "2506.19365", "pdf": "https://arxiv.org/pdf/2506.19365", "abs": "https://arxiv.org/abs/2506.19365", "authors": ["Prabhat Kumar Chand", "Manish Kumar", "Anisur Rahaman Molla"], "title": "Computing Tree Structures in Anonymous Graphs via Mobile Agents", "categories": ["cs.DC", "cs.MA"], "comment": null, "summary": "Minimum Spanning Tree (MST) and Breadth-First Search (BFS) tree constructions\nare classical problems in distributed computing, traditionally studied in the\nmessage-passing model, where static nodes communicate via messages. This paper\ninvestigates MST and BFS tree construction in an agent-based network, where\nmobile agents explore a graph and compute. Each node hosts one agent, and\ncommunication occurs when agents meet at a node. We consider $n$ agents\ninitially dispersed (one per node) in an anonymous, arbitrary $n$-node,\n$m$-edge graph $G$. The goal is to construct the BFS and MST trees from this\nconfiguration such that each tree edge is known to at least one of its\nendpoints, while minimizing time and memory per agent. We work in a synchronous\nmodel and assume agents have no prior knowledge of any graph parameters such as\n$n$, $m$, $D$, $\\Delta$ (graph diameter and maximum degree). Prior work solves\nBFS in $O(D\\Delta)$ rounds with $O(\\log n)$ bits per agent, assuming the root\nis known. We give a deterministic algorithm that constructs the BFS tree in\n$O(\\min(D\\Delta, m\\log n) + n\\log n + \\Delta \\log^2 n)$ rounds using $O(\\log\nn)$ bits per agent without root knowledge. To determine the root, we solve\nleader election and MST construction. We elect a leader and construct the MST\nin $O(n\\log n + \\Delta \\log^2 n)$ rounds, with $O(\\log n)$ bits per agent.\nPrior MST algorithms require $O(m + n\\log n)$ rounds and $\\max(\\Delta, \\log n)\n\\log n$ bits. Our results significantly improve memory efficiency and time,\nachieving nearly linear-time leader election and MST. Agents are assumed to\nknow $\\lambda$, the maximum identifier, bounded by a polynomial in $n$."}
{"id": "2506.19307", "pdf": "https://arxiv.org/pdf/2506.19307", "abs": "https://arxiv.org/abs/2506.19307", "authors": ["Qing Zhang", "Zixiong Su", "Yoshihito Kondoh", "Kazunori Asada", "Thad Starner", "Kai Kunze", "Yuta Itoh", "Jun Rekimoto"], "title": "OpticalAging: Real-time Presbyopia Simulation for Inclusive Design via Tunable Lenses", "categories": ["cs.HC"], "comment": "Under Submission", "summary": "Presbyopia, a common age-related vision condition affecting most people as\nthey age, often remains inadequately understood by those unaffected. To help\nbridge the gap between abstract accessibility knowledge and a more grounded\nappreciation of perceptual challenges, this study presents OpticalAging, an\noptical see-through simulation approach. Unlike VR-based methods, OpticalAging\nuses dynamically controlled tunable lenses to simulate the first-person visual\nperspective of presbyopia's distance-dependent blur during real-world\ninteraction, aiming to enhance awareness. While acknowledging critiques\nregarding simulation's limitations in fully capturing lived experience, we\nposition this tool as a complement to user-centered methods. Our user study (N\n= 19, 18-35 years old) provides validation: quantitative measurements show\nstatistically significant changes in near points across three age modes (40s,\n50s, 60s), while qualitative results suggest increases in reported\nunderstanding and empathy among participants. The integration of our tool into\na design task showcases its potential applicability within age-inclusive design\nworkflows when used critically alongside direct user engagement."}
{"id": "2506.19268", "pdf": "https://arxiv.org/pdf/2506.19268", "abs": "https://arxiv.org/abs/2506.19268", "authors": ["Timoteo Kelly", "Abdulkadir Korkmaz", "Samuel Mallet", "Connor Souders", "Sadra Aliakbarpour", "Praveen Rao"], "title": "HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps", "categories": ["cs.HC", "cs.CR", "cs.ET", "cs.LG"], "comment": "Under review at The 34th ACM International Conference on Information\n  and Knowledge Management (CIKM'25)", "summary": "We present HARPT, a large-scale annotated corpus of mobile health app store\nreviews aimed at advancing research in user privacy and trust. The dataset\ncomprises over 480,000 user reviews labeled into seven categories that capture\ncritical aspects of trust in applications, trust in providers and privacy\nconcerns. Creating HARPT required addressing multiple complexities, such as\ndefining a nuanced label schema, isolating relevant content from large volumes\nof noisy data, and designing an annotation strategy that balanced scalability\nwith accuracy. This strategy integrated rule-based filtering, iterative manual\nlabeling with review, targeted data augmentation, and weak supervision using\ntransformer-based classifiers to accelerate coverage. In parallel, a carefully\ncurated subset of 7,000 reviews was manually annotated to support model\ndevelopment and evaluation. We benchmark a broad range of classification\nmodels, demonstrating that strong performance is achievable and providing a\nbaseline for future research. HARPT is released as a public resource to support\nwork in health informatics, cybersecurity, and natural language processing."}
{"id": "2506.19757", "pdf": "https://arxiv.org/pdf/2506.19757", "abs": "https://arxiv.org/abs/2506.19757", "authors": ["Rodrigo Oliveira Zacarias", "Léo Carvalho Ramos Antunes", "Márcio de Oliveira Barros", "Rodrigo Pereira dos Santos", "Patricia Lago"], "title": "Exploring Developer Experience Factors in Software Ecosystems", "categories": ["cs.SE", "cs.HC"], "comment": "58 pages", "summary": "Context: Developer experience (DX) plays a key role in developers'\nperformance and their continued involvement in a software ecosystem (SECO)\nplatform. While researchers and practitioners have recognized several factors\naffecting DX in SECO platforms, a clear roadmap of the most influential factors\nis still missing. This is particularly important given the direct impact on\ndevelopers' interest in SECO and their ongoing engagement with the common\ntechnological platform. Goal: This work aims to identify key DX factors and\nunderstand how they influence third-party developers' decisions to adopt and\nkeep contributing to a SECO. Methods: We conducted a systematic mapping study\n(SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO.\nAdditionally, we conducted a Delphi study to evaluate the influence of 27 DX\nfactors (identified in our SMS) from the perspective of 21 third-party\ndevelopers to adopt and keep contributing to a SECO. Results: The factors that\nmost strongly influence developers' adoption and ongoing contributions to a\nSECO are: financial costs for using the platform, desired technical resources\nfor development, low barriers to entry into the applications market, and more\nfinancial gains. Conclusion: DX is essential for the success and sustainability\nof SECO. Our set of DX factors provides valuable insights and recommendations\nfor researchers and practitioners to address key DX concerns from the\nperspective of third-party developers."}
{"id": "2506.19282", "pdf": "https://arxiv.org/pdf/2506.19282", "abs": "https://arxiv.org/abs/2506.19282", "authors": ["Yang Zhou", "Xiaoning Ren"], "title": "A Batch-Insensitive Dynamic GNN Approach to Address Temporal Discontinuity in Graph Streams", "categories": ["cs.LG", "cs.GR"], "comment": "8pages, 5figures", "summary": "In dynamic graphs, preserving temporal continuity is critical. However,\nMemory-based Dynamic Graph Neural Networks (MDGNNs) trained with large batches\noften disrupt event sequences, leading to temporal information loss. This\ndiscontinuity not only deteriorates temporal modeling but also hinders\noptimization by increasing the difficulty of parameter convergence. Our\ntheoretical study quantifies this through a Lipschitz upper bound, showing that\nlarge batch sizes enlarge the parameter search space. In response, we propose\nBADGNN, a novel batch-agnostic framework consisting of two core components: (1)\nTemporal Lipschitz Regularization (TLR) to control parameter search space\nexpansion, and (2) Adaptive Attention Adjustment (A3) to alleviate attention\ndistortion induced by both regularization and batching. Empirical results on\nthree benchmark datasets show that BADGNN maintains strong performance while\nenabling significantly larger batch sizes and faster training compared to TGN.\nOur code is available at Code:\nhttps://anonymous.4open.science/r/TGN_Lipichitz-C033/."}
{"id": "2506.19499", "pdf": "https://arxiv.org/pdf/2506.19499", "abs": "https://arxiv.org/abs/2506.19499", "authors": ["Noa Jie Vives Zaguirre", "Oscar Lasierra", "Filip Lemic", "Gerard Calvo Bartra", "Pablo José Galván Calderón", "Gines Garcia-Aviles", "Sergi Abadal", "Xavier Costa-Pérez"], "title": "Experimental Assessment of A Framework for In-body RF-backscattering Localization", "categories": ["eess.SP", "cs.NI"], "comment": "7 pages, 7 figures, 2 tables, accepted at IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications 2025", "summary": "Localization of in-body devices is beneficial for Gastrointestinal (GI)\ndiagnosis and targeted treatment. Traditional methods such as imaging and\nendoscopy are invasive and limited in resolution, highlighting the need for\ninnovative alternatives. This study presents an experimental framework for\nRadio Frequency (RF)-backscatter-based in-body localization, inspired by the\nReMix approach, and evaluates its performance in real-world conditions. The\nexperimental setup includes an in-body backscatter device and various off-body\nantenna configurations to investigate harmonic generation and reception in air,\nchicken and pork tissues. The results indicate that optimal backscatter device\npositioning, antenna selection, and gain settings significantly impact\nperformance, with denser biological tissues leading to greater attenuation. The\nstudy also highlights challenges such as external interference and plastic\nenclosures affecting propagation. The findings emphasize the importance of\ninterference mitigation and refined propagation models to enhance performance."}
{"id": "2506.19578", "pdf": "https://arxiv.org/pdf/2506.19578", "abs": "https://arxiv.org/abs/2506.19578", "authors": ["Ozgur O. Kilic", "David K. Park", "Yihui Ren", "Tatiana Korchuganova", "Sairam Sri Vatsavai", "Joseph Boudreau", "Tasnuva Chowdhury", "Shengyu Feng", "Raees Khan", "Jaehyung Kim", "Scott Klasky", "Tadashi Maeno", "Paul Nilsson", "Verena Ingrid Martinez Outschoorn", "Norbert Podhorszki", "Frédéric Suter", "Wei Yang", "Yiming Yang", "Shinjae Yoo", "Alexei Klimentov", "Adolfy Hoisie"], "title": "Towards an Introspective Dynamic Model of Globally Distributed Computing Infrastructures", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Large-scale scientific collaborations like ATLAS, Belle II, CMS, DUNE, and\nothers involve hundreds of research institutes and thousands of researchers\nspread across the globe. These experiments generate petabytes of data, with\nvolumes soon expected to reach exabytes. Consequently, there is a growing need\nfor computation, including structured data processing from raw data to\nconsumer-ready derived data, extensive Monte Carlo simulation campaigns, and a\nwide range of end-user analysis. To manage these computational and storage\ndemands, centralized workflow and data management systems are implemented.\nHowever, decisions regarding data placement and payload allocation are often\nmade disjointly and via heuristic means. A significant obstacle in adopting\nmore effective heuristic or AI-driven solutions is the absence of a quick and\nreliable introspective dynamic model to evaluate and refine alternative\napproaches. In this study, we aim to develop such an interactive system using\nreal-world data. By examining job execution records from the PanDA workflow\nmanagement system, we have pinpointed key performance indicators such as\nqueuing time, error rate, and the extent of remote data access. The dataset\nincludes five months of activity. Additionally, we are creating a generative AI\nmodel to simulate time series of payloads, which incorporate visible features\nlike category, event count, and submitting group, as well as hidden features\nlike the total computational load-derived from existing PanDA records and\ncomputing site capabilities. These hidden features, which are not visible to\njob allocators, whether heuristic or AI-driven, influence factors such as\nqueuing times and data movement."}
{"id": "2506.19364", "pdf": "https://arxiv.org/pdf/2506.19364", "abs": "https://arxiv.org/abs/2506.19364", "authors": ["Angxuan Chen", "Jingjing Lian", "Xinran Kuang", "Jiyou Jia"], "title": "Can theory-driven learning analytics dashboard enhance human-AI collaboration in writing learning? Insights from an empirical experiment", "categories": ["cs.HC"], "comment": null, "summary": "The integration of Generative AI (GenAI) into education has raised concerns\nabout over-reliance and superficial learning, particularly in writing tasks in\nhigher education. This study explores whether a theory-driven learning\nanalytics dashboard (LAD) can enhance human-AI collaboration in the academic\nwriting task by improving writing knowledge gains, fostering self-regulated\nlearning (SRL) skills and building different human-AI dialogue characteristics.\nGrounded in Zimmerman's SRL framework, the LAD provided real-time feedback on\nlearners' goal-setting, writing processes and reflection, while monitoring the\nquality of learner-AI interactions. A quasi-experiment was conducted involving\n52 postgraduate students divided into an experimental group (EG) using the LAD\nto a control group (CG) without it in a human-AI collaborative writing task.\nPre- and post- knowledge tests, questionnaires measuring SRL and cognitive\nload, and students' dialogue data with GenAI were collected and analyzed.\nResults showed that the EG achieved significantly higher writing knowledge\ngains and improved SRL skills, particularly in self-efficacy and cognitive\nstrategies. However, the EG also reported increased test anxiety and cognitive\nload, possibly due to heightened metacognitive awareness. Epistemic Network\nAnalysis revealed that the EG engaged in more reflective, evaluative\ninteractions with GenAI, while the CG focused on more transactional and\ninformation-seeking exchanges. These findings contribute to the growing body of\nliterature on the educational use of GenAI and highlight the importance of\ndesigning interventions that complement GenAI tools, ensuring that technology\nenhances rather than undermines the learning process."}
{"id": "2506.19333", "pdf": "https://arxiv.org/pdf/2506.19333", "abs": "https://arxiv.org/abs/2506.19333", "authors": ["Craig Steven Wright"], "title": "The Autonomy of the Lightning Network: A Mathematical and Economic Proof of Structural Decoupling from BTC", "categories": ["cs.DC", "cs.CC", "cs.ET", "cs.GT", "econ.GN", "q-fin.EC", "68M10, 91B50, 94A15", "C.2.2; G.2.2; H.1.1"], "comment": "59 pages, 4 figures, includes TikZ diagrams and formal proofs.\n  Targeted for journal submission", "summary": "This paper presents a formal analysis of the Lightning Network as a monetary\nsystem structurally diverging from Bitcoin's base-layer settlement model. We\ndemonstrate that under increasing transaction demand, BTC transaction fees rise\nsuperlinearly due to throughput constraints, while Lightning Network routing\ncosts approach a bounded asymptote. Using mathematical modeling, game-theoretic\nproofs, and complexity analysis, we show that Lightning enables indefinite\noff-chain operation via the emergence of liquidity hub oligopolies. These hubs\nexhibit properties of unregulated financial intermediaries, including rent\nextraction, opacity, and systemic fragility. Strategic agent models show that\nchannel closure becomes economically infeasible, and routing problems approach\nhardness limits in P-Space complexity. We conclude that Lightning does not\nmerely extend Bitcoin, but constitutes a synthetic financial system with\nshadowbank characteristics, lacking reserve discipline, transparency, or\nenforceable settlement guarantees."}
{"id": "2506.18923", "pdf": "https://arxiv.org/pdf/2506.18923", "abs": "https://arxiv.org/abs/2506.18923", "authors": ["Yifan Zong", "Yuntian Deng", "Pengyu Nie"], "title": "Mix-of-Language-Experts Architecture for Multilingual Programming", "categories": ["cs.PL", "cs.CL", "cs.SE"], "comment": "Accepted at LLM4Code @ ICSE 2025", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\naiding developers with tasks like code comprehension, generation, and\ntranslation. Supporting multilingual programming -- i.e., coding tasks across\nmultiple programming languages -- typically requires either (1) finetuning a\nsingle LLM across all programming languages, which is cost-efficient but\nsacrifices language-specific specialization and performance, or (2) finetuning\nseparate LLMs for each programming language, which allows for specialization\nbut is computationally expensive and storage-intensive due to the duplication\nof parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel\narchitecture that balances efficiency and specialization for multilingual\nprogramming. MoLE is composed of a base model, a shared LoRA (low-rank\nadaptation) module, and a collection of language-specific LoRA modules. These\nmodules are jointly optimized during the finetuning process, enabling effective\nknowledge sharing and specialization across programming languages. During\ninference, MoLE automatically routes to the language-specific LoRA module\ncorresponding to the programming language of the code token being generated.\nOur experiments demonstrate that MoLE achieves greater parameter efficiency\ncompared to training separate language-specific LoRAs, while outperforming a\nsingle shared LLM finetuned for all programming languages in terms of accuracy."}
{"id": "2506.19660", "pdf": "https://arxiv.org/pdf/2506.19660", "abs": "https://arxiv.org/abs/2506.19660", "authors": ["Shuhang Xu", "Yunfei Gu", "Linhui Liu", "Chentao Wu"], "title": "PS-WL: A Probability-Sensitive Wear Leveling scheme for SSD array scaling", "categories": ["cs.DC"], "comment": null, "summary": "As flash-based Solid State Drive (SSD) arrays become essential to modern data\ncenters, scaling these arrays to meet explosive data growth is a frequent and\ncritical operation. However, the conventional wear-leveling (WL) paradigm\napplied during scaling suffers from a fundamental flaw: it ignores the\nnon-linear relationship between wear and failure probability, potentially\npushing the most vulnerable, aged disks towards premature failure. To address\nthis critical issue at its root, we propose the Probability-Sensitive Wear\nLeveling (PS-WL) scheme, which shifts the optimization goal from balancing wear\nto directly balancing failure risk. At its core, PS-WL introduces an \"effective\nlifetime\" model derived from a realistic failure probability to more accurately\nassess disk lifetime. This model guides a PID controller for wear leveling\noperation, with a conservative zone minimizes performance overhead by\nrestricting warm data migration. Comprehensive simulations validate the\nsuperiority of PS-WL over state-of-the-art methods. The results demonstrate\nthat our approach significantly reduces performance overhead while, most\ncritically, consistently and effectively lowering the aggregated array failure\nrisk across diverse system configurations and workloads. This proves that by\ndirectly optimizing for reliability, PS-WL builds a scalable storage system\nthat is, by design, fundamentally safer, more efficient, and more stable."}
{"id": "2506.19430", "pdf": "https://arxiv.org/pdf/2506.19430", "abs": "https://arxiv.org/abs/2506.19430", "authors": ["Adrien Coppens", "Valérie Maquil"], "title": "Integrating AIs With Body Tracking Technology for Human Behaviour Analysis: Challenges and Opportunities", "categories": ["cs.HC"], "comment": "This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. The Version of Record of this\n  contribution is published in Engineering Interactive Computer Systems (EICS)\n  2024 International Workshops, and is available online at\n  https://doi.org/10.1007/978-3-031-91760-8_4", "summary": "The automated analysis of human behaviour provides many opportunities for the\ncreation of interactive systems and the post-experiment investigations for user\nstudies. Commodity depth cameras offer reasonable body tracking accuracy at a\nlow price point, without the need for users to wear or hold any extra\nequipment. The resulting systems typically perform body tracking through a\ndedicated machine learning model, but they can be enhanced with additional AI\ncomponents providing extra capabilities. This leads to opportunities but also\nchallenges, for example regarding the orchestration of such AI components and\nthe engineering of the resulting tracking pipeline. In this paper, we discuss\nthese elements, based on our experience with the creation of a remote\ncollaboration system across distant wall-sized displays, that we built using\nexisting and readily available building blocks, including AI-based recognition\nmodels."}
{"id": "2506.19597", "pdf": "https://arxiv.org/pdf/2506.19597", "abs": "https://arxiv.org/abs/2506.19597", "authors": ["Haruki Uchiito", "Akhilesh Bhat", "Koji Kusaka", "Xiaoya Zhang", "Hiraku Kinjo", "Honoka Uehara", "Motoki Koyama", "Shinji Natsume"], "title": "Robotics Under Construction: Challenges on Job Sites", "categories": ["cs.RO", "cs.AI", "cs.AR", "cs.ET", "cs.SY", "eess.SY"], "comment": "Workshop on Field Robotics, ICRA", "summary": "As labor shortages and productivity stagnation increasingly challenge the\nconstruction industry, automation has become essential for sustainable\ninfrastructure development. This paper presents an autonomous payload\ntransportation system as an initial step toward fully unmanned construction\nsites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous\nnavigation, fleet management, and GNSS-based localization to facilitate\nmaterial transport in construction site environments. While the current system\ndoes not yet incorporate dynamic environment adaptation algorithms, we have\nbegun fundamental investigations into external-sensor based perception and\nmapping system. Preliminary results highlight the potential challenges,\nincluding navigation in evolving terrain, environmental perception under\nconstruction-specific conditions, and sensor placement optimization for\nimproving autonomy and efficiency. Looking forward, we envision a construction\necosystem where collaborative autonomous agents dynamically adapt to site\nconditions, optimizing workflow and reducing human intervention. This paper\nprovides foundational insights into the future of robotics-driven construction\nautomation and identifies critical areas for further technological development."}
{"id": "2506.19620", "pdf": "https://arxiv.org/pdf/2506.19620", "abs": "https://arxiv.org/abs/2506.19620", "authors": ["Mustafa Adam", "Kangfeng Ye", "David A. Anisi", "Ana Cavalcanti", "Jim Woodcock", "Robert Morris"], "title": "Probabilistic modelling and safety assurance of an agriculture robot providing light-treatment", "categories": ["cs.RO", "cs.FL", "cs.SE", "cs.SY", "eess.SY"], "comment": null, "summary": "Continued adoption of agricultural robots postulates the farmer's trust in\nthe reliability, robustness and safety of the new technology. This motivates\nour work on safety assurance of agricultural robots, particularly their ability\nto detect, track and avoid obstacles and humans. This paper considers a\nprobabilistic modelling and risk analysis framework for use in the early\ndevelopment phases. Starting off with hazard identification and a risk\nassessment matrix, the behaviour of the mobile robot platform, sensor and\nperception system, and any humans present are captured using three state\nmachines. An auto-generated probabilistic model is then solved and analysed\nusing the probabilistic model checker PRISM. The result provides unique insight\ninto fundamental development and engineering aspects by quantifying the effect\nof the risk mitigation actions and risk reduction associated with distinct\ndesign concepts. These include implications of adopting a higher performance\nand more expensive Object Detection System or opting for a more elaborate\nwarning system to increase human awareness. Although this paper mainly focuses\non the initial concept-development phase, the proposed safety assurance\nframework can also be used during implementation, and subsequent deployment and\noperation phases."}
{"id": "2506.19730", "pdf": "https://arxiv.org/pdf/2506.19730", "abs": "https://arxiv.org/abs/2506.19730", "authors": ["Orestis Alpos", "Oleg Fomenko", "Dimitris Karakostas", "Oleksandr Kurbatov", "Andrey Sabelnikov"], "title": "Formalization and security analysis of the Bridgeless protocol", "categories": ["cs.DC"], "comment": null, "summary": "This paper formalizes the proves the security of the Bridgeless protocol, a\nprotocol able to bridge tokens between various chains. The Bridgeless protocol\nis run by a set of validators, responsible for verifying deposit transactions\non the source chain and generating the corresponding withdrawals on the target\nchain. The protocol is designed to be chain-agnostic and the validators\ninteract with each supported chain via a chain client. It currently supports\nEVM-compatible chains, the Zano, and the Bitcoin chains. The paper formalizes\nall involved subprotocols and describes the conditions under which the protocol\nmaintains safety and liveness."}
{"id": "2506.19495", "pdf": "https://arxiv.org/pdf/2506.19495", "abs": "https://arxiv.org/abs/2506.19495", "authors": ["Russell Beale", "Eugenia Sergueeva"], "title": "5 Days, 5 Stories: Using Technology to Promote Empathy in the Workplace", "categories": ["cs.HC"], "comment": null, "summary": "Empathy is widely recognized as a vital attribute for effective collaboration\nand communication in the workplace, yet developing empathic skills and\nfostering it among colleagues remains a challenge. This study explores the\npotential of a collaborative digital storytelling platform - In Your Shoes -\ndesigned to promote empathic listening and interpersonal understanding through\nthe structured exchange of personal narratives. A one-week intervention was\nconducted with employees from multiple organizations using the platform.\nEmploying a mixed methods approach, we assessed quantitative changes in empathy\nusing the Empathy Quotient (EQ) and qualitatively analyzed participant\nexperiences through grounded theory. While quantitative analysis revealed no\nstatistically significant shift in dispositional empathy, qualitative findings\nsuggested the tool facilitated situational empathy, prompted self-reflection,\nimproved emotional resonance, and enhanced workplace relationships.\nParticipants reported feelings of psychological safety, connection, and, in\nsome cases, therapeutic benefits from sharing and responding to stories. These\nresults highlight the promise of asynchronous, structured narrative-based\ndigital tools for supporting empathic engagement in professional settings,\noffering insights for the design of emotionally intelligent workplace\ntechnologies."}
{"id": "2506.19622", "pdf": "https://arxiv.org/pdf/2506.19622", "abs": "https://arxiv.org/abs/2506.19622", "authors": ["Mustafa Adam", "David A. Anisi", "Pedro Ribeiro"], "title": "A Verification Methodology for Safety Assurance of Robotic Autonomous Systems", "categories": ["cs.RO", "cs.FL", "cs.SE", "cs.SY", "eess.SY"], "comment": "In Proc. of the 26th TAROS (Towards Autonomous Robotic Systems)\n  Conference, York, UK, August, 2025", "summary": "Autonomous robots deployed in shared human environments, such as agricultural\nsettings, require rigorous safety assurance to meet both functional reliability\nand regulatory compliance. These systems must operate in dynamic, unstructured\nenvironments, interact safely with humans, and respond effectively to a wide\nrange of potential hazards. This paper presents a verification workflow for the\nsafety assurance of an autonomous agricultural robot, covering the entire\ndevelopment life-cycle, from concept study and design to runtime verification.\nThe outlined methodology begins with a systematic hazard analysis and risk\nassessment to identify potential risks and derive corresponding safety\nrequirements. A formal model of the safety controller is then developed to\ncapture its behaviour and verify that the controller satisfies the specified\nsafety properties with respect to these requirements. The proposed approach is\ndemonstrated on a field robot operating in an agricultural setting. The results\nshow that the methodology can be effectively used to verify safety-critical\nproperties and facilitate the early identification of design issues,\ncontributing to the development of safer robots and autonomous systems."}
{"id": "2506.19164", "pdf": "https://arxiv.org/pdf/2506.19164", "abs": "https://arxiv.org/abs/2506.19164", "authors": ["Amir Faiyaz", "Tara Salman"], "title": "GradualDiff-Fed: A Federated Learning Specialized Framework for Large Language Model", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "The rapid proliferation of large language models (LLMs) has created an\nunprecedented demand for fine-tuning models for specialized domains, such as\nmedical science. While federated learning (FL) offers a decentralized and\nprivacy-preserving approach to collaboratively fine-tune LLMs without sharing\nraw data, it presents significant challenges, particularly in performance and\nmanaging large model sizes efficiently. In this paper, we introduce\nGradualDiff-Fed, an FL framework designed explicitly for LLMs, and their\nchallenge of handling the high parameter size. GradualDiff-Fed reduces\ncommunication costs by transmitting only the difference of model weights rather\nthan the entire model during training rounds. Such an approach significantly\nimproves scalability and communication efficiency, making it more feasible to\nfine-tune LLMs across distributed clients without compromising performance. Our\nevaluation demonstrates that GradualDiff-Fed achieves performance on par with\ncentralized training while drastically reducing communication overhead. These\nresults highlight the potential of GradualDiff-Fed as an efficient solution for\nfine-tuning large models from distributed data in privacy-preserving settings\nwithout comprising performance."}
{"id": "2506.19519", "pdf": "https://arxiv.org/pdf/2506.19519", "abs": "https://arxiv.org/abs/2506.19519", "authors": ["Panagiotis Kourtesis", "Evgenia Giatzoglou", "Panagiotis Vorias", "Katerina Alkisti Gounari", "Eleni Orfanidou", "Chrysanthi Nega"], "title": "Examination of Eye-Tracking, Head-Gaze, and Controller-Based Ray-casting in TMT-VR: Performance and Usability Across Adulthood", "categories": ["cs.HC", "B.8; C.4; D.0; J.4"], "comment": "29 pages, 11 Figures, 7 Tables", "summary": "Virtual reality (VR) can enrich neuropsychological testing, yet the ergonomic\ntrade-offs of its input modes remain under-examined. Seventy-seven healthy\nvolunteers-young (19-29 y) and middle-aged (27-56 y)-completed a VR\nTrail-Making Test with three pointing methods: eye-tracking, head-gaze, and a\nsix-degree-of-freedom hand controller. Completion time, spatial accuracy, and\nerror counts for the simple (Trail A) and alternating (Trail B) sequences were\nanalysed in 3 x 2 x 2 mixed-model ANOVAs; post-trial scales captured usability\n(SUS), user experience (UEQ-S), and acceptability. Age dominated behaviour:\nyounger adults were reliably faster, more precise, and less error-prone.\nAgainst this backdrop, input modality mattered. Eye-tracking yielded the best\nspatial accuracy and shortened Trail A time relative to manual control;\nhead-gaze matched eye-tracking on Trail A speed and became the quickest, least\nerror-prone option on Trail B. Controllers lagged on every metric. Subjective\nratings were high across the board, with only a small usability dip in\nmiddle-aged low-gamers. Overall, gaze-based ray-casting clearly outperformed\nmanual pointing, but optimal choice depended on task demands: eye-tracking\nmaximised spatial precision, whereas head-gaze offered calibration-free\nenhanced speed and error-avoidance under heavier cognitive load. TMT-VR appears\nto be accurate, engaging, and ergonomically adaptable assessment, yet it\nrequires age-specific-stratified norms."}
{"id": "2506.19175", "pdf": "https://arxiv.org/pdf/2506.19175", "abs": "https://arxiv.org/abs/2506.19175", "authors": ["Benjamin Brock", "Willow Ahrens", "Hameer Abbasi", "Timothy A. Davis", "Juni Kim", "James Kitchen", "Spencer Patty", "Isaac Virshup", "Erik Welch"], "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices and Tensors", "categories": ["cs.MS", "cs.DC", "cs.DS"], "comment": null, "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."}
{"id": "2506.19524", "pdf": "https://arxiv.org/pdf/2506.19524", "abs": "https://arxiv.org/abs/2506.19524", "authors": ["Zheyuan Zhang", "Jingjing Sun", "Dorian Peters", "Rafael A. Calvo"], "title": "Beyond Wellbeing Apps: Co-Designing Immersive, Embodied, and Collective Digital Wellbeing Interventions for Healthcare Professionals", "categories": ["cs.HC"], "comment": "21 pages, DIS '25: Designing Interactive Systems Conference, Funchal,\n  Portugal, July 2025", "summary": "Healthcare professionals (HCPs) face increasing levels of stress and burnout.\nTechnological wellbeing interventions provide accessible and flexible support\nfor HCPs. While most studies have focused on mobile- and web-based programs,\nalternative technologies like virtual reality (VR), augmented reality (AR),\ntangible interfaces, and embodied technologies are emerging as engaging and\neffective tools for wellbeing interventions. However, there is still a lack of\nresearch on how such technologies are perceived among HCPs. This study explored\nHCPs' perceptions and preferences for various types of wellbeing technologies,\nby conducting a 2-phase co-design study involving 26 HCPs in idea generation,\nconcept evaluation, prototype testing, and design iteration. From our findings,\nHCPs highly valued the potential of technologies to support mental health with\nimmersive, embodied, and collective experiences. Furthermore, we provided\ndesign recommendations for wellbeing technologies for HCPs that sustain user\nengagement by meeting their needs for autonomy, competence, and relatedness in\nthe experiences."}
{"id": "2506.19220", "pdf": "https://arxiv.org/pdf/2506.19220", "abs": "https://arxiv.org/abs/2506.19220", "authors": ["Conor Snedeker", "Xinyu Zhou", "Raef Bassily"], "title": "Private Model Personalization Revisited", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "comment": "ICML 2025", "summary": "We study model personalization under user-level differential privacy (DP) in\nthe shared representation framework. In this problem, there are $n$ users whose\ndata is statistically heterogeneous, and their optimal parameters share an\nunknown embedding $U^* \\in\\mathbb{R}^{d\\times k}$ that maps the user parameters\nin $\\mathbb{R}^d$ to low-dimensional representations in $\\mathbb{R}^k$, where\n$k\\ll d$. Our goal is to privately recover the shared embedding and the local\nlow-dimensional representations with small excess risk in the federated\nsetting. We propose a private, efficient federated learning algorithm to learn\nthe shared embedding based on the FedRep algorithm in [CHM+21]. Unlike\n[CHM+21], our algorithm satisfies differential privacy, and our results hold\nfor the case of noisy labels. In contrast to prior work on private model\npersonalization [JRS+21], our utility guarantees hold under a larger class of\nusers' distributions (sub-Gaussian instead of Gaussian distributions).\nAdditionally, in natural parameter regimes, we improve the privacy error term\nin [JRS+21] by a factor of $\\widetilde{O}(dk)$. Next, we consider the binary\nclassification setting. We present an information-theoretic construction to\nprivately learn the shared embedding and derive a margin-based accuracy\nguarantee that is independent of $d$. Our method utilizes the\nJohnson-Lindenstrauss transform to reduce the effective dimensions of the\nshared embedding and the users' data. This result shows that\ndimension-independent risk bounds are possible in this setting under a margin\nloss."}
{"id": "2506.19611", "pdf": "https://arxiv.org/pdf/2506.19611", "abs": "https://arxiv.org/abs/2506.19611", "authors": ["Miriam Doh", "Corinna Canali", "Nuria Oliver"], "title": "Filters of Identity: AR Beauty and the Algorithmic Politics of the Digital Body", "categories": ["cs.HC"], "comment": "This work was presented at the \"Body Politics: Unpacking Tensions and\n  Future Perspectives For Body-Centric Design Research in HCI\" workshop at the\n  ACM (Association for Computing Machinery) CHI conference on Human Factors in\n  Computing Systems 2025", "summary": "This position paper situates AR beauty filters within the broader debate on\nBody Politics in HCI. We argue that these filters are not neutral tools but\ntechnologies of governance that reinforce racialized, gendered, and ableist\nbeauty standards. Through naming conventions, algorithmic bias, and platform\ngovernance, they impose aesthetic norms while concealing their influence. To\naddress these challenges, we advocate for transparency-driven interventions and\na critical rethinking of algorithmic aesthetics and digital embodiment."}
{"id": "2506.19260", "pdf": "https://arxiv.org/pdf/2506.19260", "abs": "https://arxiv.org/abs/2506.19260", "authors": ["Murtaza Rangwala", "Richard O. Sinnott", "Rajkumar Buyya"], "title": "Network Structures as an Attack Surface: Topology-Based Privacy Leakage in Federated Learning", "categories": ["cs.CR", "cs.DC", "cs.LG", "I.2.6; C.2.4; K.6.5"], "comment": "13 pages, 7 figures, 5 tables. Data from the experiments and source\n  code can be found here: https://doi.org/10.5281/zenodo.15622123", "summary": "Federated learning systems increasingly rely on diverse network topologies to\naddress scalability and organizational constraints. While existing privacy\nresearch focuses on gradient-based attacks, the privacy implications of network\ntopology knowledge remain critically understudied. We conduct the first\ncomprehensive analysis of topology-based privacy leakage across realistic\nadversarial knowledge scenarios, demonstrating that adversaries with varying\ndegrees of structural knowledge can infer sensitive data distribution patterns\neven under strong differential privacy guarantees. Through systematic\nevaluation of 4,720 attack instances, we analyze six distinct adversarial\nknowledge scenarios: complete topology knowledge and five partial knowledge\nconfigurations reflecting real-world deployment constraints. We propose three\ncomplementary attack vectors: communication pattern analysis, parameter\nmagnitude profiling, and structural position correlation, achieving success\nrates of 84.1%, 65.0%, and 47.2% under complete knowledge conditions.\nCritically, we find that 80% of realistic partial knowledge scenarios maintain\nattack effectiveness above security thresholds, with certain partial knowledge\nconfigurations achieving performance superior to the baseline complete\nknowledge scenario. To address these vulnerabilities, we propose and\nempirically validate structural noise injection as a complementary defense\nmechanism across 808 configurations, demonstrating up to 51.4% additional\nattack reduction when properly layered with existing privacy techniques. These\nresults establish that network topology represents a fundamental privacy\nvulnerability in federated learning systems while providing practical pathways\nfor mitigation through topology-aware defense mechanisms."}
{"id": "2506.19644", "pdf": "https://arxiv.org/pdf/2506.19644", "abs": "https://arxiv.org/abs/2506.19644", "authors": ["M. Michelessa", "J. Ng", "C. Hurter", "B. Y. Lim"], "title": "Varif.ai to Vary and Verify User-Driven Diversity in Scalable Image Generation", "categories": ["cs.HC"], "comment": "DIS2025, code available at github.com/mario-michelessa/varifai", "summary": "Diversity in image generation is essential to ensure fair representations and\nsupport creativity in ideation. Hence, many text-to-image models have\nimplemented diversification mechanisms. Yet, after a few iterations of\ngeneration, a lack of diversity becomes apparent, because each user has their\nown diversity goals (e.g., different colors, brands of cars), and there are\ndiverse attributions to be specified. To support user-driven diversity control,\nwe propose Varif.ai that employs text-to-image and Large Language Models to\niteratively i) (re)generate a set of images, ii) verify if user-specified\nattributes have sufficient coverage, and iii) vary existing or new attributes.\nThrough an elicitation study, we uncovered user needs for diversity in image\ngeneration. A pilot validation showed that Varif.ai made achieving diverse\nimage sets easier. In a controlled evaluation with 20 participants, Varif.ai\nproved more effective than baseline methods across various scenarios. Thus,\nthis supports user control of diversity in image generation for creative\nideation and scalable image generation."}
{"id": "2506.19457", "pdf": "https://arxiv.org/pdf/2506.19457", "abs": "https://arxiv.org/abs/2506.19457", "authors": ["Tom T. P. Franken", "Thomas Neele", "Jan Friso Groote"], "title": "The Autonomous Data Language -- Concepts, Design and Formal Verification", "categories": ["cs.PL", "cs.DC", "D.3.1; F.3.1; F.3.2"], "comment": "48 pages, preprint submitted to Elsevier", "summary": "Nowadays, the main advances in computational power are due to parallelism.\nHowever, most parallel languages have been designed with a focus on processors\nand threads. This makes dealing with data and memory in programs hard, which\ndistances the implementation from its original algorithm. We propose a new\nparadigm for parallel programming, the data-autonomous paradigm, where\ncomputation is performed by autonomous data elements. Programs in this paradigm\nare focused on making the data collaborate in a highly parallel fashion. We\nfurthermore present AuDaLa, the first data autonomous programming language, and\nprovide a full formalisation that includes a type system and operational\nsemantics. Programming in AuDaLa is very natural, as illustrated by examples,\nalbeit in a style very different from sequential and contemporary parallel\nprogramming. Additionally, it lends itself for the formal verification of\nparallel programs, which we demonstrate."}
{"id": "2506.18941", "pdf": "https://arxiv.org/pdf/2506.18941", "abs": "https://arxiv.org/abs/2506.18941", "authors": ["Neha Rani", "Sharan Majumder", "Ishan Bhardwaj", "Pedro Guillermo Feijoo Garcia"], "title": "Can AI support student engagement in classroom activities in higher education?", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "Lucrative career prospects and creative opportunities often attract students\nto enroll in computer science majors and pursue advanced studies in the field.\nConsequently, there has been a significant surge in enrollment in computer\nscience courses, resulting in large class sizes that can range from hundreds to\neven thousands of students. A common challenge in such large classrooms is the\nlack of engagement between students and both the instructor and the learning\nmaterial. However, with advancements in technology and improvements in large\nlanguage models (LLMs), there is a considerable opportunity to utilize\nLLM-based AI models, such as conversational artificial intelligence (CAI), to\nenhance student engagement with learning content in large classes. To explore\nthe potential of CAI to support engagement, especially with learning content,\nwe designed an activity in a software Engineering course (with a large class\nsize) where students used CAI for an in-class activity. We conducted a\nwithin-subject investigation in a large classroom at a US university where we\ncompared student engagement during an in-class activity that used CAI tool vs.\none without CAI tool. The CAI tool we used was ChatGPT due to its widespread\npopularity and familiarity. Our results indicate that CAI (ChatGPT) has the\npotential to support engagement with learning content during in-class\nactivities, especially in large class sizes. We further discuss the\nimplications of our findings."}
{"id": "2506.19079", "pdf": "https://arxiv.org/pdf/2506.19079", "abs": "https://arxiv.org/abs/2506.19079", "authors": ["Iosif Tsangko", "Andreas Triantafyllopoulos", "Adem Abdelmoula", "Adria Mallol-Ragolta", "Bjoern W. Schuller"], "title": "Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Foundation Models (FMs) are rapidly transforming Affective Computing (AC),\nwith Vision Language Models (VLMs) now capable of recognising emotions in zero\nshot settings. This paper probes a critical but underexplored question: what\nvisual cues do these models rely on to infer affect, and are these cues\npsychologically grounded or superficially learnt? We benchmark varying scale\nVLMs on a teeth annotated subset of AffectNet dataset and find consistent\nperformance shifts depending on the presence of visible teeth. Through\nstructured introspection of, the best-performing model, i.e., GPT-4o, we show\nthat facial attributes like eyebrow position drive much of its affective\nreasoning, revealing a high degree of internal consistency in its\nvalence-arousal predictions. These patterns highlight the emergent nature of\nFMs behaviour, but also reveal risks: shortcut learning, bias, and fairness\nissues especially in sensitive domains like mental health and education."}
{"id": "2506.19202", "pdf": "https://arxiv.org/pdf/2506.19202", "abs": "https://arxiv.org/abs/2506.19202", "authors": ["Claire Yang", "Heer Patel", "Max Kleiman-Weiner", "Maya Cakmak"], "title": "Preserving Sense of Agency: User Preferences for Robot Autonomy and User Control across Household Tasks", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Roboticists often design with the assumption that assistive robots should be\nfully autonomous. However, it remains unclear whether users prefer highly\nautonomous robots, as prior work in assistive robotics suggests otherwise. High\nrobot autonomy can reduce the user's sense of agency, which represents feeling\nin control of one's environment. How much control do users, in fact, want over\nthe actions of robots used for in-home assistance? We investigate how robot\nautonomy levels affect users' sense of agency and the autonomy level they\nprefer in contexts with varying risks. Our study asked participants to rate\ntheir sense of agency as robot users across four distinct autonomy levels and\nranked their robot preferences with respect to various household tasks. Our\nfindings revealed that participants' sense of agency was primarily influenced\nby two factors: (1) whether the robot acts autonomously, and (2) whether a\nthird party is involved in the robot's programming or operation. Notably, an\nend-user programmed robot highly preserved users' sense of agency, even though\nit acts autonomously. However, in high-risk settings, e.g., preparing a snack\nfor a child with allergies, they preferred robots that prioritized their\ncontrol significantly more. Additional contextual factors, such as trust in a\nthird party operator, also shaped their preferences."}
{"id": "2506.19280", "pdf": "https://arxiv.org/pdf/2506.19280", "abs": "https://arxiv.org/abs/2506.19280", "authors": ["Feiting Yang", "Antoine Moevus", "Steve Lévesque"], "title": "Emotion Detection on User Front-Facing App Interfaces for Enhanced Schedule Optimization: A Machine Learning Approach", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Human-Computer Interaction (HCI) has evolved significantly to incorporate\nemotion recognition capabilities, creating unprecedented opportunities for\nadaptive and personalized user experiences. This paper explores the integration\nof emotion detection into calendar applications, enabling user interfaces to\ndynamically respond to users' emotional states and stress levels, thereby\nenhancing both productivity and engagement. We present and evaluate two\ncomplementary approaches to emotion detection: a biometric-based method\nutilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals\nprocessed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)\nneural networks to predict the emotional dimensions of Valence, Arousal, and\nDominance; and a behavioral method analyzing computer activity through multiple\nmachine learning models to classify emotions based on fine-grained user\ninteractions such as mouse movements, clicks, and keystroke patterns. Our\ncomparative analysis, from real-world datasets, reveals that while both\napproaches demonstrate effectiveness, the computer activity-based method\ndelivers superior consistency and accuracy, particularly for mouse-related\ninteractions, which achieved approximately 90\\% accuracy. Furthermore, GRU\nnetworks outperformed LSTM models in the biometric approach, with Valence\nprediction reaching 84.38\\% accuracy."}
{"id": "2506.19352", "pdf": "https://arxiv.org/pdf/2506.19352", "abs": "https://arxiv.org/abs/2506.19352", "authors": ["Jisu Shin", "Juhyun Oh", "Eunsu Kim", "Hoyun Song", "Alice Oh"], "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of ACL 2025; github repo:\n  https://github.com/ddindidu/atomic-persona-evaluation/", "summary": "Ensuring persona fidelity in large language models (LLMs) is essential for\nmaintaining coherent and engaging human-AI interactions. However, LLMs often\nexhibit Out-of-Character (OOC) behavior, where generated responses deviate from\nan assigned persona, leading to inconsistencies that affect model reliability.\nExisting evaluation methods typically assign single scores to entire responses,\nstruggling to capture subtle persona misalignment, particularly in long-form\ntext generation. To address this limitation, we propose an atomic-level\nevaluation framework that quantifies persona fidelity at a finer granularity.\nOur three key metrics measure the degree of persona alignment and consistency\nwithin and across generations. Our approach enables a more precise and\nrealistic assessment of persona fidelity by identifying subtle deviations that\nreal users would encounter. Through our experiments, we demonstrate that our\nframework effectively detects persona inconsistencies that prior methods\noverlook. By analyzing persona fidelity across diverse tasks and personality\ntypes, we reveal how task structure and persona desirability influence model\nadaptability, highlighting challenges in maintaining consistent persona\nexpression."}
{"id": "2506.19415", "pdf": "https://arxiv.org/pdf/2506.19415", "abs": "https://arxiv.org/abs/2506.19415", "authors": ["Jonathan Haberl", "Philipp Fleck", "Clemens Arth"], "title": "Virtual Memory for 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.HC"], "comment": "Based on the Master Thesis from Jonathan Haberl from 2024, Submitted\n  to TVCG in Feb. 2025;", "summary": "3D Gaussian Splatting represents a breakthrough in the field of novel view\nsynthesis. It establishes Gaussians as core rendering primitives for highly\naccurate real-world environment reconstruction. Recent advances have\ndrastically increased the size of scenes that can be created. In this work, we\npresent a method for rendering large and complex 3D Gaussian Splatting scenes\nusing virtual memory. By leveraging well-established virtual memory and virtual\ntexturing techniques, our approach efficiently identifies visible Gaussians and\ndynamically streams them to the GPU just in time for real-time rendering.\nSelecting only the necessary Gaussians for both storage and rendering results\nin reduced memory usage and effectively accelerates rendering, especially for\nhighly complex scenes. Furthermore, we demonstrate how level of detail can be\nintegrated into our proposed method to further enhance rendering speed for\nlarge-scale scenes. With an optimized implementation, we highlight key\npractical considerations and thoroughly evaluate the proposed technique and its\nimpact on desktop and mobile devices."}
{"id": "2506.19484", "pdf": "https://arxiv.org/pdf/2506.19484", "abs": "https://arxiv.org/abs/2506.19484", "authors": ["Russell Beale"], "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning", "categories": ["cs.CL", "cs.AI", "cs.HC", "K.3.2; I.2.6; H.4.m"], "comment": null, "summary": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned."}
{"id": "2506.19757", "pdf": "https://arxiv.org/pdf/2506.19757", "abs": "https://arxiv.org/abs/2506.19757", "authors": ["Rodrigo Oliveira Zacarias", "Léo Carvalho Ramos Antunes", "Márcio de Oliveira Barros", "Rodrigo Pereira dos Santos", "Patricia Lago"], "title": "Exploring Developer Experience Factors in Software Ecosystems", "categories": ["cs.SE", "cs.HC"], "comment": "58 pages", "summary": "Context: Developer experience (DX) plays a key role in developers'\nperformance and their continued involvement in a software ecosystem (SECO)\nplatform. While researchers and practitioners have recognized several factors\naffecting DX in SECO platforms, a clear roadmap of the most influential factors\nis still missing. This is particularly important given the direct impact on\ndevelopers' interest in SECO and their ongoing engagement with the common\ntechnological platform. Goal: This work aims to identify key DX factors and\nunderstand how they influence third-party developers' decisions to adopt and\nkeep contributing to a SECO. Methods: We conducted a systematic mapping study\n(SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO.\nAdditionally, we conducted a Delphi study to evaluate the influence of 27 DX\nfactors (identified in our SMS) from the perspective of 21 third-party\ndevelopers to adopt and keep contributing to a SECO. Results: The factors that\nmost strongly influence developers' adoption and ongoing contributions to a\nSECO are: financial costs for using the platform, desired technical resources\nfor development, low barriers to entry into the applications market, and more\nfinancial gains. Conclusion: DX is essential for the success and sustainability\nof SECO. Our set of DX factors provides valuable insights and recommendations\nfor researchers and practitioners to address key DX concerns from the\nperspective of third-party developers."}
