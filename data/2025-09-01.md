<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 8]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]
- [math.OC](#math.OC) [Total: 1]
- [math.LO](#math.LO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Model-Driven Quantum Code Generation Using Large Language Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2508.21097)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 该论文提出了一种利用大型语言模型（LLMs）和检索增强生成（RAG）管道的新研究方向，专注于量子及混合量子-经典软件系统的模型到文本/代码转换。通过实验验证了从UML模型生成Qiskit代码的可行性，结果显示优化后的提示能显著提升代码质量。未来研究方向包括扩展RAG管道的应用范围和代码转换任务。


<details>
  <summary>Details</summary>
Motivation: 量子计算软件的开发面临异构平台和开发者技能不足的挑战，模型驱动的方法可以降低成本并减少风险。

Method: 基于LLMs和RAG管道，从UML模型实例生成Python代码（使用Qiskit库），并结合GitHub上的示例代码优化生成结果。

Result: 实验表明，经过优化的提示可以将CodeBLEU评分提升多达四倍，生成的量子代码更准确和一致。

Conclusion: 该研究为量子软件系统提供了一种有效的代码生成方法，并提出了未来扩展RAG管道和代码转换的潜在研究方向。

Abstract: This paper introduces a novel research direction for model-to-text/code
transformations by leveraging Large Language Models (LLMs) that can be enhanced
with Retrieval-Augmented Generation (RAG) pipelines. The focus is on quantum
and hybrid quantum-classical software systems, where model-driven approaches
can help reduce the costs and mitigate the risks associated with the
heterogeneous platform landscape and lack of developers' skills. We validate
one of the proposed ideas regarding generating code out of UML model instances
of software systems. This Python code uses a well-established library, called
Qiskit, to execute on gate-based or circuit-based quantum computers. The RAG
pipeline that we deploy incorporates sample Qiskit code from public GitHub
repositories. Experimental results show that well-engineered prompts can
improve CodeBLEU scores by up to a factor of four, yielding more accurate and
consistent quantum code. However, the proposed research direction can go beyond
this through further investigation in the future by conducting experiments to
address our other research questions and ideas proposed here, such as deploying
software system model instances as the source of information in the RAG
pipelines, or deploying LLMs for code-to-code transformations, for instance,
for transpilation use cases.

</details>


### [2] [Learning to Generate Unit Test via Adversarial Reinforcement Learning](https://arxiv.org/abs/2508.21107)
*Dongjun Lee,Changho Hwang,Kimin Lee*

Main category: cs.SE

TL;DR: UTRL是一种新型强化学习框架，通过对抗训练两个LLM（单元测试生成器和代码生成器）来生成高质量单元测试，效果优于传统监督微调和前沿模型。


<details>
  <summary>Details</summary>
Motivation: 单元测试是编程中的核心实践，但目前训练LLM生成高质量单元测试的方法尚未充分探索。

Method: 采用对抗性强化学习框架UTRL，迭代训练测试生成器和代码生成器，分别优化歧视奖励和代码奖励。

Result: 实验表明，UTRL训练的模型生成的单元测试质量更高，优于监督微调和GPT-4.1等前沿模型。

Conclusion: UTRL框架在训练LLM生成高质量单元测试方面具有显著效果。

Abstract: Unit testing is a core practice in programming, enabling systematic
evaluation of programs produced by human developers or large language models
(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have
been employed to automate test generation, yet methods for training LLMs to
produce high-quality tests remain underexplored. In this work, we propose UTRL,
a novel reinforcement learning framework that trains an LLM to generate
high-quality unit tests given a programming instruction. Our key idea is to
iteratively train two LLMs, the unit test generator and the code generator, in
an adversarial manner via reinforcement learning. The unit test generator is
trained to maximize a discrimination reward, which reflects its ability to
produce tests that expose faults in the code generator's solutions, and the
code generator is trained to maximize a code reward, which reflects its ability
to produce solutions that pass the unit tests generated by the test generator.
In our experiments, we demonstrate that unit tests generated by Qwen3-4B
trained via UTRL show higher quality compared to unit tests generated by the
same model trained via supervised fine-tuning on human-written ground-truth
unit tests, yielding code evaluations that more closely align with those
induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL
outperforms frontier models such as GPT-4.1 in generating high-quality unit
tests, highlighting the effectiveness of UTRL in training LLMs for this task.

</details>


### [3] [Automated Bug Triaging using Instruction-Tuned Large Language Models](https://arxiv.org/abs/2508.21156)
*Kiana Kiashemshaki,Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan*

Main category: cs.SE

TL;DR: 提出了一种轻量级框架，通过指令调优的LLM和LoRA适配器，结合候选约束解码，实现高效的Bug分派任务。


<details>
  <summary>Details</summary>
Motivation: 解决大型项目中Bug分派任务耗时且不一致的问题。

Method: 使用指令调优的LLM和LoRA适配器，并采用候选约束解码确保分派的有效性。

Result: 在EclipseJDT和Mozilla数据集上表现出色（Hit at 10达0.753），且在近期数据上准确率显著提升。

Conclusion: 指令调优的LLM为Bug分派提供了一种可行且高效的替代方案，减少了对特征工程和基于图的方法的依赖。

Abstract: Bug triaging, the task of assigning new issues to developers, is often slow
and inconsistent in large projects. We present a lightweight framework that
instruction-tuned large language model (LLM) with LoRA adapters and uses
candidate-constrained decoding to ensure valid assignments. Tested on
EclipseJDT and Mozilla datasets, the model achieves strong shortlist quality
(Hit at 10 up to 0.753) despite modest exact Top-1 accuracy. On recent
snapshots, accuracy rises sharply, showing the framework's potential for
real-world, human-in-the-loop triaging. Our results suggest that
instruction-tuned LLMs offer a practical alternative to costly feature
engineering and graph-based methods.

</details>


### [4] [The Complexity Trap: Simple Observation Masking Is as Efficient as LLM Summarization for Agent Context Management](https://arxiv.org/abs/2508.21433)
*Tobias Lindenbauer,Igor Slinko,Ludwig Felder,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: 研究表明，在SWE-agent任务中，简单的观察屏蔽策略比LLM摘要更经济高效，性能相当甚至略优。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM代理在处理复杂任务时产生的长且高成本的上下文历史问题，作者比较了LLM摘要与简单观察屏蔽策略的性能和成本效益。

Method: 在SWE-agent上使用SWE-bench Verified数据集，对五种不同模型配置进行了系统比较，评估了LLM摘要和观察屏蔽策略的效果。

Result: 观察屏蔽策略将成本减半，同时解决率与LLM摘要相当或略优（例如Qwen3-Coder 480B从53.8%提升至54.8%）。

Conclusion: 在SWE-agent任务中，最简单的上下文管理策略（观察屏蔽）是最经济高效的解决方案。

Abstract: Large Language Model (LLM)-based agents solve complex tasks through iterative
reasoning, exploration, and tool-use, a process that can result in long,
expensive context histories. While state-of-the-art Software Engineering ( SE)
agents like OpenHands or Cursor use LLM-based summarization to tackle this
issue, it is unclear whether the increased complexity offers tangible
performance benefits compared to simply omitting older observations. We present
a systematic comparison of these strategies within SWE-agent on SWE-bench
Verified across five diverse model configurations. We find that a simple
observation-masking strategy halves cost relative to a raw agent while
matching, and sometimes slightly exceeding, the solve rate of LLM
summarization. For example, with Qwen3-Coder 480B, masking improves solve rate
from 53.8% (raw agent) to 54.8%, while remaining competitive with summarization
at a lower cost. These results suggest that, at least within SWE-agent on
SWE-bench Verified, the most effective and efficient context management can be
the simplest. We release code and data for reproducibility

</details>


### [5] [Enhancing Semantic Understanding in Pointer Analysis using Large Language Models](https://arxiv.org/abs/2508.21454)
*Baijun Cheng,Kailong Wang,Ling Shi,Haoyu Wang,Yao Guo,Ding Li,Xiangqun Chen*

Main category: cs.SE

TL;DR: 论文提出了LMPA，一种利用大语言模型（LLM）增强指针分析精度和扩展性的方法。


<details>
  <summary>Details</summary>
Motivation: 现有指针分析框架因缺乏对代码的语义理解，导致对用户定义函数的处理过于保守，影响分析精度。

Method: 通过LLM识别类似系统API的用户定义函数并建模，改进基于摘要的分析方法，推断初始点集并引入自然语言增强的摘要策略。

Result: LMPA有望减少错误的跨调用上下文传播，提升分析精度和扩展性。

Conclusion: LMPA为指针分析提供了新思路，但仍需解决实现中的关键挑战。

Abstract: Pointer analysis has been studied for over four decades. However, existing
frameworks continue to suffer from the propagation of incorrect facts. A major
limitation stems from their insufficient semantic understanding of code,
resulting in overly conservative treatment of user-defined functions. Recent
advances in large language models (LLMs) present new opportunities to bridge
this gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a
vision that integrates LLMs into pointer analysis to enhance both precision and
scalability. LMPA identifies user-defined functions that resemble system APIs
and models them accordingly, thereby mitigating erroneous cross-calling-context
propagation. Furthermore, it enhances summary-based analysis by inferring
initial points-to sets and introducing a novel summary strategy augmented with
natural language. Finally, we discuss the key challenges involved in realizing
this vision.

</details>


### [6] [Reusable Test Suites for Reinforcement Learning](https://arxiv.org/abs/2508.21553)
*Jørn Eirik Betten,Quentin Mazouni,Dennis Gross,Pedro Lind,Helge Spieker*

Main category: cs.SE

TL;DR: 提出了一种名为MPTCS的多策略测试用例选择方法，用于强化学习环境的自动化测试套件选择，旨在通过难度分数和多样性来提取可重用且策略无关的测试用例。


<details>
  <summary>Details</summary>
Motivation: 验证强化学习代理策略在部署时的可靠性和性能具有挑战性，现有测试方法通常仅针对单个策略，难以普遍适用。

Method: MPTCS利用一组策略从候选池中选择测试用例，基于难度分数和多样性（通过质量多样性算法启发的方法）来选择。

Result: 评估了难度分数的有效性，并分析了方法的效果和成本与策略数量的关系。同时研究了多样性方法对状态空间的覆盖和触发错误行为的能力。

Conclusion: MPTCS提供了一种通用且高效的测试用例选择方法，能够揭示代理行为的典型缺陷，适用于多策略环境。

Abstract: Reinforcement learning (RL) agents show great promise in solving sequential
decision-making tasks. However, validating the reliability and performance of
the agent policies' behavior for deployment remains challenging. Most
reinforcement learning policy testing methods produce test suites tailored to
the agent policy being tested, and their relevance to other policies is
unclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel
automated test suite selection method for RL environments, designed to extract
test cases generated by any policy testing framework based on their
solvability, diversity, and general difficulty. MPTCS uses a set of policies to
select a diverse collection of reusable policy-agnostic test cases that reveal
typical flaws in the agents' behavior. The set of policies selects test cases
from a candidate pool, which can be generated by any policy testing method,
based on a difficulty score. We assess the effectiveness of the difficulty
score and how the method's effectiveness and cost depend on the number of
policies in the set. Additionally, a method for promoting diversity in the test
suite, a discretized general test case descriptor surface inspired by
quality-diversity algorithms, is examined to determine how it covers the state
space and which policies it triggers to produce faulty behaviors.

</details>


### [7] [Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity](https://arxiv.org/abs/2508.21634)
*Domenico Cotroneo,Cristina Improta,Pietro Liguori*

Main category: cs.SE

TL;DR: 该研究比较了人类开发者与三种先进大语言模型（如ChatGPT）编写的代码，发现在代码缺陷、安全漏洞和结构复杂性等方面存在显著差异。AI生成的代码更简单但更易出现高风险漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码助手在软件开发中的普及，了解其代码与人类编写代码的差异对确保可靠性、可维护性和安全性至关重要。

Method: 研究通过正交缺陷分类和通用漏洞枚举方法，对Python和Java两种语言的50万份代码样本进行了多维度评估。

Result: AI生成的代码结构更简单、重复性高，但包含更多未使用构造和高风险安全漏洞；人类代码则更复杂且存在更多可维护性问题。

Conclusion: 研究表明AI和人类编写的代码缺陷特征明显不同，强调了在AI辅助编程中需要专门的质量保证措施。

Abstract: As AI code assistants become increasingly integrated into software
development workflows, understanding how their code compares to human-written
programs is critical for ensuring reliability, maintainability, and security.
In this paper, we present a large-scale comparison of code authored by human
developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and
Qwen-Coder, on multiple dimensions of software quality: code defects, security
vulnerabilities, and structural complexity. Our evaluation spans over 500k code
samples in two widely used languages, Python and Java, classifying defects via
Orthogonal Defect Classification and security vulnerabilities using the Common
Weakness Enumeration. We find that AI-generated code is generally simpler and
more repetitive, yet more prone to unused constructs and hardcoded debugging,
while human-written code exhibits greater structural complexity and a higher
concentration of maintainability issues. Notably, AI-generated code also
contains more high-risk security vulnerabilities. These findings highlight the
distinct defect profiles of AI- and human-authored code and underscore the need
for specialized quality assurance practices in AI-assisted programming.

</details>


### [8] [The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry](https://arxiv.org/abs/2508.21811)
*Ashley Hourigan,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 本文研究了IT行业中Agile和DevOps实践的可行性及适用性，通过访谈和主题分析，得出两者结合的19个主题，并提出新理解。


<details>
  <summary>Details</summary>
Motivation: IT行业对快速软件交付的需求增加，Agile和DevOps方法成为主流，但两者如何结合尚需研究。

Method: 对IT行业各领域的11位Agile和DevOps从业者进行半结构化访谈，通过主题分析提取51个代码并合成19个主题。

Result: 研究详细描述了Agile方法在DevOps实践中的相互关系，并实现了研究目标。

Conclusion: Agile和DevOps实践在IT行业中具有可行性和适用性，为进一步优化软件交付提供了新视角。

Abstract: The demand for rapid software delivery in the Information Technology (IT)
industry has significantly intensified, emphasising the need for faster
software products and service releases with enhanced features to meet customer
expectations. Agile methodologies are replacing traditional approaches such as
Waterfall, where flexibility, iterative development and adaptation to change
are favoured over rigid planning and execution. DevOps, a subsequent evolution
from Agile, emphasises collaborative efforts in development and operations
teams, focusing on continuous integration and deployment to deliver resilient
and high-quality software products and services. This study aims to critically
assess both Agile and DevOps practices in the IT industry to identify the
feasibility and applicability of Agile methods in DevOps practices. Eleven
semi-structured interviews were conducted with Agile and DevOps practitioners
in varying capacities across several sectors within the IT industry. Through
thematic analysis, 51 unique codes were extracted and synthesised into 19
themes that reported on each phase of the DevOps lifecycle, specifically
regarding the integration and implementation of Agile methods into DevOps
practices. Based on the findings, a new understanding detailing the
interrelationship of Agile methods in DevOps practices was discussed that met
the research objectives.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL是一种通用编程语言翻译器，通过统一中间表示CrossGL实现多语言双向翻译，避免传统方法中语言对间翻译的指数级复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统翻译器需要为每对语言单独开发，复杂度高，无法高效扩展。CrossTL旨在通过统一中间表示简化多语言翻译。

Method: 系统包括语言特定解析器（生成AST）、双向CrossGL翻译模块（包括ToCrossGLConverter和CodeGen类），以及完整后端实现。

Result: 实验验证表明，CrossTL支持多种编程领域和语言（如CUDA、Rust等），成功实现编译和执行。

Conclusion: CrossTL通过统一IR和模块化架构，显著提升多语言翻译效率，支持“一次编写、随处部署”的愿景。

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


### [10] [Growing Mathlib: maintenance of a large scale mathematical library](https://arxiv.org/abs/2508.21593)
*Anne Baanen,Matthew Robert Ballard,Johan Commelin,Bryan Gin-ge Chen,Michael Rothgang,Damiano Testa*

Main category: cs.PL

TL;DR: Mathlib是一个快速增长的数学形式化库，本文探讨了管理其增长、避免维护过载的策略，包括处理破坏性变更、使用代码质量工具、优化编译时间和应对技术债务。


<details>
  <summary>Details</summary>
Motivation: 随着Mathlib的快速增长，如何有效管理库的变更和避免维护者过载成为关键问题。

Method: 通过建立废弃系统处理破坏性变更，使用代码分析工具（如linter）提供用户反馈，优化库设计以加快编译时间，以及开发定制工具帮助审查新贡献。

Result: 这些策略帮助Mathlib在快速发展的同时保持代码质量和维护效率。

Conclusion: 通过系统化的工具和策略，可以有效管理大型数学形式化库的增长和变更。

Abstract: The Lean mathematical library Mathlib is one of the fastest-growing libraries
of formalised mathematics. We describe various strategies to manage this
growth, while allowing for change and avoiding maintainer overload. This
includes dealing with breaking changes via a deprecation system, using code
quality analysis tools (linters) to provide direct user feedback about common
pitfalls, speeding up compilation times through conscious library (re-)design,
dealing with technical debt as well as writing custom tooling to help with the
review and triage of new contributions.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [A Combined Push-Pull Access Framework for Digital Twin Alignment and Anomaly Reporting](https://arxiv.org/abs/2508.21516)
*Federico Chiariotti,Fabio Saggese,Andrea Munari,Leonardo Badia,Petar Popovski*

Main category: cs.NI

TL;DR: 该论文提出了一种动态分配通信资源的推拉调度器（PPS）框架，用于优化数字孪生（DT）的更新机制，平衡正常状态下的对齐和异常报告，显著降低了信息漂移年龄（AoII）。


<details>
  <summary>Details</summary>
Motivation: 数字孪生的准确性依赖于与物理系统的实时同步更新。现有的更新机制分为拉取更新（pull-updates）和推送更新（push-updates），但如何在资源有限的情况下平衡两者的优先级是一个挑战。

Method: 设计了一种推拉调度器（PPS）框架，动态分配通信资源用于两种类型的更新，以优化资源使用并减少信息漂移年龄（AoII）。

Result: 与现有方案相比，PPS框架将AoII降低了20%以上，同时保持了相同的异常检测性能，并在平均漂移AoII为1毫秒的条件下，将最坏情况下的异常检测AoII从70毫秒降至20毫秒。

Conclusion: PPS框架在资源分配和信息同步方面取得了显著改进，为数字孪生的实时更新提供了高效解决方案。

Abstract: A digital twin (DT) contains a set of virtual models of real systems and
processes that are synchronized to their physical counterparts. This enables
experimentation and examination of counterfactuals, simulating the consequences
of decisions in real time. However, the DT accuracy relies on timely updates
that maintain alignment with the real system. We can distinguish between: (i)
pull-updates, which follow a request from the DT to the sensors, to decrease
its drift from the physical state; (ii) push-updates, which are sent directly
by the sensors since they represent urgent information, such as anomalies. In
this work, we devise a push-pull scheduler (PPS) medium access framework, which
dynamically allocates the communication resources used for these two types of
updates. Our scheme strikes a balance in the trade-off between DT alignment in
normal conditions and anomaly reporting, optimizing resource usage and reducing
the drift age of incorrect information (AoII) by over 20% with respect to
state-of-the-art solutions, while maintaining the same anomaly detection
guarantees, as well as reducing the worst-case anomaly detection AoII from 70
ms to 20 ms when considering a 1 ms average drift AoII constraint.

</details>


### [12] [QoS-Aware Proportional Fairness Scheduling for Multi-Flow 5G UEs: A Smart Factory Perspective](https://arxiv.org/abs/2508.21783)
*Mohamed Seliem,Utz Roedig,Cormac Sreenan,Dirk Pesch*

Main category: cs.NI

TL;DR: 摘要介绍了如何通过扩展Simu5G和引入新的QoS-PF调度器来模拟5G网络中多流行为，以优化资源分配，并在智能工厂场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有仿真框架缺乏在QFI级别建模多流行为的能力，这限制了5G网络在智能工厂等场景中的应用。

Method: 通过扩展Simu5G支持每QFI建模，并提出动态平衡延迟、GBR和优先级的QoS-PF调度器。

Result: 在智能工厂场景中，QoS-PF提高了截止时间遵守率和公平性，同时不影响吞吐量。

Conclusion: 该工作为工业5G部署中的高级QoS策略仿真和分析提供了方法和架构基础，且开源支持未来研究。

Abstract: Private 5G networks are emerging as key enablers for smart factories, where a
single device often handles multiple concurrent traffic flows with distinct
Quality of Service (QoS) requirements. Existing simulation frameworks, however,
lack the fidelity to model such multi-flow behavior at the QoS Flow Identifier
(QFI) level. This paper addresses this gap by extending Simu5G to support
per-QFI modeling and by introducing a novel QoS-aware Proportional Fairness
(QoS-PF) scheduler. The scheduler dynamically balances delay, Guaranteed Bit
Rate (GBR), and priority metrics to optimize resource allocation across
heterogeneous flows. We evaluate the proposed approach in a realistic smart
factory scenario featuring edge-hosted machine vision, real-time control loops,
and bulk data transfer. Results show that QoS-PF improves deadline adherence
and fairness without compromising throughput. All extensions are implemented in
a modular and open-source manner to support future research. Our work provides
both a methodological and architectural foundation for simulating and analyzing
advanced QoS policies in industrial 5G deployments.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [13] [lifeXplore at the Lifelog Search Challenge 2020](https://arxiv.org/abs/2508.21397)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: 本文介绍了lifeXplore系统，一个结合了特征地图浏览、概念搜索、过滤和手绘草图的生命日志视频探索与检索工具，并在原有基础上增加了YOLO9000、OCR和均匀采样功能。


<details>
  <summary>Details</summary>
Motivation: 通过Lifelog Search Challenge（LSC）这一互动竞赛，团队旨在快速检索不断增长的生命日志数据集，并提升系统的检索效率与功能。

Method: 开发了lifeXplore系统，整合特征地图浏览、概念搜索、过滤和手绘草图功能，并新增YOLO9000、OCR及均匀采样技术。

Result: 系统在LSC2018和LSC2019中得到应用，提升了生命日志数据的检索能力和用户体验。

Conclusion: lifeXplore系统通过多模态检索技术的结合与改进，显著增强了生命日志数据的搜索效率与灵活性。

Abstract: Since its first iteration in 2018, the Lifelog Search Challenge (LSC) - an
interactive competition for retrieving lifelogging moments - is co-located at
the annual ACM International Conference on Multimedia Retrieval (ICMR) and has
drawn international attention. With the goal of making an ever growing public
lifelogging dataset searchable, several teams develop systems for quickly
solving time-limited queries during the challenge. Having participated in both
previous LSC iterations, i.e. LSC2018 and LSC2019, we present our lifeXplore
system - a video exploration and retrieval tool combining feature map browsing,
concept search and filtering as well as hand-drawn sketching. The system is
improved by including additional deep concept YOLO9000, optical character
recognition (OCR) as well as adding uniform sampling as an alternative to the
system's traditional underlying shot segmentation.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [14] [Interpolation for Converse PDL](https://arxiv.org/abs/2508.21485)
*Johannes Kloibhofer,Valentina Trucco Dalmas,Yde Venema*

Main category: cs.LO

TL;DR: Converse PDL（命题动态逻辑的扩展）具有局部克里普克插值性质，并基于Maehara的证明理论方法构建了一个循环序列系统。


<details>
  <summary>Details</summary>
Motivation: 研究Converse PDL的插值性质和Beth可定义性。

Method: 采用Maehara的证明理论方法，构建了一个包含分析性切割规则和焦点机制的循环序列系统。

Result: 证明了Converse PDL具有局部克里普克插值性质，并确立了Beth可定义性。

Conclusion: 提出的循环序列系统为Converse PDL提供了新的理论支持，证明了其逻辑性质的优越性。

Abstract: Converse PDL is the extension of propositional dynamic logic with a converse
operation on programs. Our main result states that Converse PDL enjoys the
(local) Craig Interpolation Property, with respect to both atomic programs and
propositional variables. As a corollary we establish the Beth Definability
Property for the logic.
  Our interpolation proof is based on an adaptation of Maehara's
proof-theoretic method. For this purpose we introduce a sound and complete
cyclic sequent system for this logic. This calculus features an analytic cut
rule and uses a focus mechanism for recognising successful cycles.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [15] [Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?](https://arxiv.org/abs/2508.21087)
*Bin Han,Deuksin Kwon,Spencer Lin,Kaleen Shrestha,Jonathan Gratch*

Main category: cs.HC

TL;DR: 该研究提出了一个框架，利用大型语言模型（LLM）通过性格提示生成虚拟代理的言语和非言语行为。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能够生成与性格特征一致的虚拟代理行为，并评估这些行为是否能被人类观察者识别。

Method: 通过内向和外向虚拟代理在谈判和破冰场景中的表现进行评估，包括代理间模拟实验和用户研究。

Result: 结果表明，LLM可以生成与性格特征一致的言语和非言语行为，且用户能够通过行为识别这些特征。

Conclusion: 该研究凸显了LLM在塑造性格一致的虚拟代理方面的潜力。

Abstract: This study proposes a framework that employs personality prompting with Large
Language Models to generate verbal and nonverbal behaviors for virtual agents
based on personality traits. Focusing on extraversion, we evaluated the system
in two scenarios: negotiation and ice breaking, using both introverted and
extroverted agents. In Experiment 1, we conducted agent to agent simulations
and performed linguistic analysis and personality classification to assess
whether the LLM generated language reflected the intended traits and whether
the corresponding nonverbal behaviors varied by personality. In Experiment 2,
we carried out a user study to evaluate whether these personality aligned
behaviors were consistent with their intended traits and perceptible to human
observers. Our results show that LLMs can generate verbal and nonverbal
behaviors that align with personality traits, and that users are able to
recognize these traits through the agents' behaviors. This work underscores the
potential of LLMs in shaping personality aligned virtual agents.

</details>


### [16] [Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses](https://arxiv.org/abs/2508.21209)
*Vanessa Figueiredo*

Main category: cs.HC

TL;DR: 研究了巴西儿童使用对话代理（CAs）的行为，并通过结构化脚手架提升交互效果。


<details>
  <summary>Details</summary>
Motivation: 探究儿童如何利用对话代理完成学习、探索和娱乐任务，并研究如何优化其交互体验。

Method: 包括两个研究：研究1通过访谈、观察和认知工作分析，设计了基于家长支持的脚手架；研究2用GPT-4o-mini评估了结构化提示的效果。

Result: 结构化提示在可读性、问题深度和多样性上表现更优，并提出了设计建议。

Conclusion: 首次在巴西儿童中应用CWA，提出了一种有效的结构化提示方法，为儿童对话代理设计提供了新思路。

Abstract: This paper presents two studies on how Brazilian children (ages 9--11) use
conversational agents (CAs) for schoolwork, discovery, and entertainment, and
how structured scaffolds can enhance these interactions. In Study 1, a
seven-week online investigation with 23 participants (children, parents,
teachers) employed interviews, observations, and Cognitive Work Analysis to map
children's information-processing flows, the role of more knowledgeable others,
functional uses, contextual goals, and interaction patterns to inform
conversation-tree design. We identified three CA functions: School, Discovery,
Entertainment, and derived ``recipe'' scaffolds mirroring parent-child support.
In Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges,
comparing conversation-tree recipes based on structured-prompting to an
unstructured baseline. Quantitative evaluation of readability, question
count/depth/diversity, and coherence revealed gains for the recipe approach.
Building on these findings, we offer design recommendations: scaffolded
conversation-trees, child-dedicated profiles for personalized context, and
caregiver-curated content. Our contributions include the first CWA application
with Brazilian children, an empirical framework of child-CA information flows,
and an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective,
scaffolded learning.

</details>


### [17] [Design and evaluation of a serious game in virtual reality to increase empathy towards students with phonological dyslexia](https://arxiv.org/abs/2508.21283)
*Jose Manuel Alcalde-Llergo,Andrea Zingoni,Pilar Aparicio-Martinez,Sara Pinzi,Enrique Yeguas-Bolivar*

Main category: cs.HC

TL;DR: 本研究开发了一款虚拟现实严肃游戏，旨在提高非诵读困难群体对诵读困难（尤其是语音性诵读困难）的理解。通过游戏体验，参与者能更好地共情诵读困难者的日常挑战，结果显示游戏显著提升了20%的共情能力。


<details>
  <summary>Details</summary>
Motivation: 诵读困难影响5%至10%的人口，但社会对其认知和支持不足。本研究旨在通过虚拟现实游戏激发对诵读困难者的共情，提升社会对此问题的意识和支持。

Method: 开发一款针对教育者、学生和非诵读困难者的虚拟现实严肃游戏，通过模拟诵读困难者的体验促进共情。101名参与者测试游戏并完成问卷调查。

Result: 游戏显著提高了参与者的共情能力，平均提升20%，验证了虚拟现实工具在促进共情方面的有效性。

Conclusion: 虚拟现实严肃游戏是提升对诵读困难共情的有效工具，有助于推动社会对支持方法重要性的认识。

Abstract: Dyslexia is a neurodevelopmental disorder estimated to strike approximately 5
to 10 per cent of the population. In particular, phonological dyslexia causes
problems in connecting the sounds of words with their written forms.
Consequently, affected individuals may encounter issues such as slow reading
speed, inaccurate reading, and difficulty decoding unfamiliar words. To address
these complexities, the use of compensatory tools and strategies is essential
to ensure equitable opportunities for dyslexic students. However, the general
underestimation of the issue and lack of awareness regarding the significance
of support methodologies pose significant obstacles. One of the ways to enhance
consciousness towards a certain issue is by stimulating empathy with whom is
affected by it. In light of this, this study introduces a serious game in
virtual reality, targeted at educators, students, and, in general, at the
non-dyslexic community. The game seeks to enhance understanding of the
challenges that individuals with dyslexia experience daily, highlighting the
relevance of supportive measures. This approach encourages players to empathize
with the struggles of dyslexic individuals and to learn firsthand the
importance of supportive methodologies. The final version of the experience was
tested by 101 participants and evaluated through a specific collection of
questionnaires validated in the literature. The results show that using the
proposed virtual reality tool to promote empathy for individuals with
phonological dyslexia is highly effective, leading to an average 20 per cent
increase in participants' empathy after playing the game.

</details>


### [18] [Conflict in Community-Based Design: A Case Study of a Relationship Breakdown](https://arxiv.org/abs/2508.21308)
*Alekhya Gandu,Aakash Gautam*

Main category: cs.HC

TL;DR: 社区设计项目中，当社区成员的实践涉及压迫性结构时，设计师应如何应对？本文通过印度南部非营利组织的案例，探讨了价值冲突下的多元应对路径。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决社区设计中的权力差异问题，但当社区成员的实践涉及压迫性结构（如种姓制度）时，如何平衡设计与伦理成为挑战。

Method: 通过为期两年的与非营利组织的合作，分析知识管理与转移中的组织缺陷，并在设计中应对种姓等级问题。

Result: 由于拒绝将压迫性实践纳入技术设计，导致与合作组织关系破裂。

Conclusion: 提出社区设计师在价值冲突下的多元应对策略，如反思、角色重构或必要时退出合作。

Abstract: Community-based design efforts rightly seek to reduce the power differences
between researchers and community participants by aligning with community
values and furthering their priorities. However, what should designers do when
key community members' practices seem to enact an oppressive and harmful
structure? We reflect on our two-year-long engagement with a non-profit
organization in southern India that supports women subjected to domestic abuse
or facing mental health crises. We highlight the organizational gaps in
knowledge management and transfer, which became an avenue for our design
intervention. During design, we encountered practices that upheld caste
hierarchies. These practices were expected to be incorporated into our
technology. Anticipating harms to indirect stakeholders, we resisted this
incorporation. It led to a breakdown in our relationship with the partner
organization. Reflecting on this experience, we outline pluralistic pathways
that community-based designers might inhabit when navigating value conflicts.
These include making space for reflection before and during engagements,
strategically repositioning through role reframing or appreciative inquiry, and
exiting the engagement if necessary.

</details>


### [19] [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456)
*Yi-Hao Peng,Dingzeyu Li,Jeffrey P. Bigham,Amy Pavel*

Main category: cs.HC

TL;DR: Morae是一个UI代理，通过在任务执行中识别决策点并暂停以让用户选择，提升了盲人和低视力用户的代理权和满意度。


<details>
  <summary>Details</summary>
Motivation: 当前UI代理在完成任务时通常不考虑用户的关键选择权或上下文信息，降低了用户代理权。例如，BLV用户可能被迫接受自动选择的商品，而不知有其他选项。

Method: Morae利用大型多模态模型解析用户查询、UI代码和截图，在需要决策时暂停任务并提示用户选择。

Result: 研究表明，Morae比基线代理（如OpenAI Operator）帮助用户完成了更多任务，并选择了更符合偏好的选项。

Conclusion: Morae展示了混合主动方法的优势，用户在享受自动化便利的同时能表达个人偏好。

Abstract: User interface (UI) agents promise to make inaccessible or complex UIs easier
to access for blind and low-vision (BLV) users. However, current UI agents
typically perform tasks end-to-end without involving users in critical choices
or making them aware of important contextual information, thus reducing user
agency. For example, in our field study, a BLV participant asked to buy the
cheapest available sparkling water, and the agent automatically chose one from
several equally priced options, without mentioning alternative products with
different flavors or better ratings. To address this problem, we introduce
Morae, a UI agent that automatically identifies decision points during task
execution and pauses so that users can make choices. Morae uses large
multimodal models to interpret user queries alongside UI code and screenshots,
and prompt users for clarification when there is a choice to be made. In a
study over real-world web tasks with BLV participants, Morae helped users
complete more tasks and select options that better matched their preferences,
as compared to baseline agents, including OpenAI Operator. More broadly, this
work exemplifies a mixed-initiative approach in which users benefit from the
automation of UI agents while being able to express their preferences.

</details>


### [20] [MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality](https://arxiv.org/abs/2508.21736)
*Simon Burbach,Maria Maleshkova,Florian Centler,Tanja Joan Schmidt*

Main category: cs.HC

TL;DR: MicroLabVR是一个用户友好的虚拟现实工具，用于交互式探索微生物组的时空模拟数据，旨在简化数据分析并改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 微生物组在人体中起重要作用，但目前研究其时空数据的工具功能有限且使用复杂，因此需要开发更直观的分析工具。

Method: 开发了MicroLabVR，将时空数据转化为虚拟现实环境，支持用户直观探索微生物组数据，如人口增长、物质浓度变化和代谢通量分布。

Result: MicroLabVR实现了用户在VR环境中交互式分析数据集的功能，增强了数据在空间背景下的理解。

Conclusion: MicroLabVR为微生物组研究提供了一个易用且功能强大的可视化工具，有助于简化数据分析并促进研究。

Abstract: Microbiomes are a vital part of the human body, engaging in tasks like food
digestion and immune defense. Their structure and function must be understood
in order to promote host health and facilitate swift recovery during disease.
Due to the difficulties in experimentally studying these systems in situ, more
research is being conducted in the field of mathematical modeling. Visualizing
spatiotemporal data is challenging, and current tools that simulate microbial
communities' spatial and temporal development often only provide limited
functionalities, often requiring expert knowledge to generate useful results.
To overcome these limitations, we provide a user-friendly tool to interactively
explore spatiotemporal simulation data, called MicroLabVR, which transfers
spatial data into virtual reality (VR) while following guidelines to enhance
user experience (UX). With MicroLabVR, users can import CSV datasets containing
population growth, substance concentration development, and metabolic flux
distribution data. The implemented visualization methods allow users to
evaluate the dataset in a VR environment interactively. MicroLabVR aims to
improve data analysis for the user by allowing the exploration of microbiome
data in their spatial context.

</details>


### [21] [Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education](https://arxiv.org/abs/2508.21666)
*Imran S. A. Khan,Emmanuel G. Blanchard,Sébastien George*

Main category: cs.HC

TL;DR: 介绍了FACTS系统，通过实时数据和AI反馈提升气候适应教育效果。


<details>
  <summary>Details</summary>
Motivation: 提升气候适应教育的效果和参与度。

Method: 结合IoT传感器数据与AI生成个性化学习挑战和反馈。

Result: 用户评价表明系统易用且有效。

Conclusion: IoT和AI的结合对气候教育有显著潜力。

Abstract: This paper introduces the Future Atmospheric Conditions Training System
(FACTS), a novel platform that advances climate resilience education through
place-based, adaptive learning experiences. FACTS combines real-time
atmospheric data collected by IoT sensors with curated resources from a
Knowledge Base to dynamically generate localized learning challenges. Learner
responses are analyzed by a Generative AI powered server, which delivers
personalized feedback and adaptive support. Results from a user evaluation
indicate that participants found the system both easy to use and effective for
building knowledge related to climate resilience. These findings suggest that
integrating IoT and Generative AI into atmospherically adaptive learning
technologies holds significant promise for enhancing educational engagement and
fostering climate awareness.

</details>


### [22] [Developer Insights into Designing AI-Based Computer Perception Tools](https://arxiv.org/abs/2508.21733)
*Maya Guhan,Meghan E. Hurley,Eric A. Storch,John Herrington,Casey Zampella,Julia Parish-Morris,Gabriel Lázaro-Muñoz,Kristin Kostick-Quenet*

Main category: cs.HC

TL;DR: 研究探讨AI感知工具在临床中的设计与开发者角色，提出四大设计优先级和实现平衡的建议。


<details>
  <summary>Details</summary>
Motivation: AI感知技术通过移动传感器收集数据支持临床决策，但需平衡临床效用与用户接受度。

Method: 对20名AI感知工具开发者进行深度访谈，进行主题分析。

Result: 识别出四大设计优先级：上下文解释性、工作流对齐、定制化设计和创新与范式平衡。

Conclusion: 开发者需兼具技术架构师与伦理管理角色，建议透明记录设计选择、限制定制范围、输出信息透明及用户培训。

Abstract: Artificial intelligence (AI)-based computer perception (CP) technologies use
mobile sensors to collect behavioral and physiological data for clinical
decision-making. These tools can reshape how clinical knowledge is generated
and interpreted. However, effective integration of these tools into clinical
workflows depends on how developers balance clinical utility with user
acceptability and trustworthiness. Our study presents findings from 20 in-depth
interviews with developers of AI-based CP tools. Interviews were transcribed
and inductive, thematic analysis was performed to identify 4 key design
priorities: 1) to account for context and ensure explainability for both
patients and clinicians; 2) align tools with existing clinical workflows; 3)
appropriately customize to relevant stakeholders for usability and
acceptability; and 4) push the boundaries of innovation while aligning with
established paradigms. Our findings highlight that developers view themselves
as not merely technical architects but also ethical stewards, designing tools
that are both acceptable by users and epistemically responsible (prioritizing
objectivity and pushing clinical knowledge forward). We offer the following
suggestions to help achieve this balance: documenting how design choices around
customization are made, defining limits for customization choices,
transparently conveying information about outputs, and investing in user
training. Achieving these goals will require interdisciplinary collaboration
between developers, clinicians, and ethicists.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [23] [ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes](https://arxiv.org/abs/2508.21095)
*Thomas Besnier,Sylvain Arguillère,Mohamed Daoudi*

Main category: cs.GR

TL;DR: 提出了一种无需骨架的数据驱动框架，用于未注册表面网格的运动预测与传递，结合运动嵌入网络和顶点特征场生成时空变形场。


<details>
  <summary>Details</summary>
Motivation: 由于未注册网格缺乏点对应关系且存在噪声，传统方法难以实现合理的变形。

Method: 结合鲁棒的运动嵌入网络和学习的顶点特征场，生成时空变形场驱动网格变形。

Result: 在行走和跑步等任务上的定量和定性评估展示了方法的有效性和适用性。

Conclusion: 该方法在挑战性未注册网格上表现优异，具有广泛适用性。

Abstract: Unregistered surface meshes, especially raw 3D scans, present significant
challenges for automatic computation of plausible deformations due to the lack
of established point-wise correspondences and the presence of noise in the
data. In this paper, we propose a new, rig-free, data-driven framework for
motion prediction and transfer on such body meshes. Our method couples a robust
motion embedding network with a learned per-vertex feature field to generate a
spatio-temporal deformation field, which drives the mesh deformation. Extensive
evaluations, including quantitative benchmarks and qualitative visuals on tasks
such as walking and running, demonstrate the effectiveness and versatility of
our approach on challenging unregistered meshes.

</details>


### [24] [ARGS: Advanced Regularization on Aligning Gaussians over the Surface](https://arxiv.org/abs/2508.21344)
*Jeong Uk Lee,Sung Hee Choi*

Main category: cs.GR

TL;DR: 本文提出两种正则化策略，改进3D高斯泼溅的重建质量，提升视觉保真度和场景一致性。


<details>
  <summary>Details</summary>
Motivation: 从3D高斯泼溅（3DGS）重建高质量3D网格和视觉效果仍是计算机图形学的核心挑战。尽管现有模型（如SuGaR）在渲染方面提供了有效解决方案，但在视觉保真度和场景一致性方面仍有改进空间。

Method: 1. 引入有效秩正则化，抑制高斯形状的极端各向异性（如'针状'），鼓励更平衡的'盘状'形式；2. 将神经有符号距离函数（SDF）融入优化过程，通过Eikonal损失正则化，提供连续全局表面先验，引导高斯更好地对齐底层几何。

Result: 这两种正则化策略显著提升了单个高斯基元的保真度及其整体表面行为，最终模型能够从3DGS数据生成更准确且一致的视觉效果。

Conclusion: 通过结合两种互补的正则化策略，本研究有效改进了3D高斯泼溅的重建质量，为高保真3D视觉生成提供了新思路。

Abstract: Reconstructing high-quality 3D meshes and visuals from 3D Gaussian
Splatting(3DGS) still remains a central challenge in computer graphics.
Although existing models such as SuGaR offer effective solutions for rendering,
there is is still room to improve improve both visual fidelity and scene
consistency. This work builds upon SuGaR by introducing two complementary
regularization strategies that address common limitations in both the shape of
individual Gaussians and the coherence of the overall surface. The first
strategy introduces an effective rank regularization, motivated by recent
studies on Gaussian primitive structures. This regularization discourages
extreme anisotropy-specifically, "needle-like" shapes-by favoring more
balanced, "disk-like" forms that are better suited for stable surface
reconstruction. The second strategy integrates a neural Signed Distance
Function (SDF) into the optimization process. The SDF is regularized with an
Eikonal loss to maintain proper distance properties and provides a continuous
global surface prior, guiding Gaussians toward better alignment with the
underlying geometry. These two regularizations aim to improve both the fidelity
of individual Gaussian primitives and their collective surface behavior. The
final model can make more accurate and coherent visuals from 3DGS data.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [25] [Fast and Scalable Mixed Precision Euclidean Distance Calculations Using GPU Tensor Cores](https://arxiv.org/abs/2508.21230)
*Brian Curless,Michael Gowanlock*

Main category: cs.DC

TL;DR: 论文提出了一种名为FaSTED的算法，利用GPU张量核心（TCs）进行欧式距离计算，实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代GPU的张量核心（TCs）在AI任务中常用于矩阵乘法，但其高计算吞吐量在数据密集型任务如欧式距离计算中尚未充分利用。作者探索了利用TCs进行欧式距离计算的可能性。

Method: 提出FaSTED算法，通过分层数据重用和优化内存利用率（全局内存、共享内存和寄存器）来提高计算吞吐量。采用FP16-32混合精度进一步优化性能。

Result: 在四个高维数据集（128-960维）上，FaSTED比现有最好的FP64算法快2.5-51倍，且精度损失小于0.06%。

Conclusion: FaSTED算法显著提升了欧式距离计算的性能，同时保持高精度，为数据密集型应用提供了有效的解决方案。

Abstract: Modern GPUs are equipped with tensor cores (TCs) that are commonly used for
matrix multiplication in artificial intelligence workloads. However, because
they have high computational throughput, they can lead to significant
performance gains in other algorithms if they can be successfully exploited. We
examine using TCs to compute Euclidean distance calculations, which are used in
many data analytics applications. Prior work has only investigated using 64 bit
floating point (FP64) data for computation; however, TCs can operate on lower
precision floating point data (i.e., 16 bit matrix multiplication and 32 bit
accumulation), which we refer to as FP16-32. FP16-32 TC peak throughput is so
high that TCs are easily starved of data. We propose a Fast and Scalable Tensor
core Euclidean Distance (FaSTED) algorithm. To achieve high computational
throughput, we design FaSTED for significant hierarchical reuse of data and
maximize memory utilization at every level (global memory, shared memory, and
registers). We apply FaSTED to the application of similarity searches, which
typically employ an indexing data structure to eliminate superfluous Euclidean
distance calculations. We compare to the state-of-the-art (SOTA) TC Euclidean
distance algorithm in the literature that employs FP64, as well as to two
single precision (FP32) CUDA core algorithms that both employ an index. We find
that across four real-world high-dimensional datasets spanning 128-960
dimensions, the mixed-precision brute force approach achieves a speedup over
the SOTA algorithms of 2.5-51x. We also quantify the accuracy loss of our mixed
precision algorithm to be less than <0.06% when compared to the FP64 baseline.

</details>


### [26] [Decentralized Federated Averaging via Random Walk](https://arxiv.org/abs/2508.21286)
*Changheng Wang,Zhiqing Wei,Lizhe Liu,Qiao Deng,Yingda Wu,Yangyang Niu,Yashan Pang,Zhiyong Feng*

Main category: cs.DC

TL;DR: 论文提出了一种基于随机游走的去中心化联邦学习算法（DFedRW），通过局部随机游走更新和量化通信，在异构数据下提升了收敛速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习在异构数据下收敛慢和模型次优的问题，同时避免中心化范式的瓶颈和隐私风险。

Method: 提出DFedRW算法，用随机游走更新替代多局部更新，并允许部分聚合；进一步提出量化版本以提升通信效率。

Result: 在凸条件下，DFedRW的收敛上界为O(1/k^(1-q))，实验显示其比FedAvg在异构数据下提升测试精度38.3%和37.5%。

Conclusion: DFedRW通过新颖的随机游走机制和量化策略，显著提升了去中心化联邦学习的性能和效率。

Abstract: Federated Learning (FL) is a communication-efficient distributed machine
learning method that allows multiple devices to collaboratively train models
without sharing raw data. FL can be categorized into centralized and
decentralized paradigms. The centralized paradigm relies on a central server to
aggregate local models, potentially resulting in single points of failure,
communication bottlenecks, and exposure of model parameters. In contrast, the
decentralized paradigm, which does not require a central server, provides
improved robustness and privacy. The essence of federated learning lies in
leveraging multiple local updates for efficient communication. However, this
approach may result in slower convergence or even convergence to suboptimal
models in the presence of heterogeneous and imbalanced data. To address this
challenge, we study decentralized federated averaging via random walk (DFedRW),
which replaces multiple local update steps on a single device with random walk
updates. Traditional Federated Averaging (FedAvg) and its decentralized
versions commonly ignore stragglers, which reduces the amount of training data
and introduces sampling bias. Therefore, we allow DFedRW to aggregate partial
random walk updates, ensuring that each computation contributes to the model
update. To further improve communication efficiency, we also propose a
quantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves
convergence upper bound of order $\mathcal{O}(\frac{1}{k^{1-q}})$ under convex
conditions. Furthermore, we propose a sufficient condition that reveals when
quantization balances communication and convergence. Numerical analysis
indicates that our proposed algorithms outperform (decentralized) FedAvg in
both convergence rate and accuracy, achieving a 38.3\% and 37.5\% increase in
test accuracy under high levels of heterogeneities.

</details>


### [27] [Addressing Reproducibility Challenges in HPC with Continuous Integration](https://arxiv.org/abs/2508.21289)
*Valérie Hayot-Sasson,Nathaniel Hudson,André Bauer,Maxime Gonthier,Ian Foster,Kyle Chard*

Main category: cs.DC

TL;DR: 论文讨论了高性能计算（HPC）中可重复性研究的挑战，提出通过持续集成（CI）和完整溯源信息作为替代方案，并介绍了一个名为CORRECT的GitHub Action工具来提升HPC应用的可重复性。


<details>
  <summary>Details</summary>
Motivation: HPC领域存在独特的软硬件环境和严格的访问限制，导致许多研究难以满足可重复性要求。作者希望通过改进CI方案来提升HPC应用的可重复性。

Method: 通过调查现有的可重复性倡议，分析HPC中可重复性的障碍，并提出一个名为CORRECT的GitHub Action工具，支持在远程HPC资源上安全执行测试。

Result: CORRECT工具在三种不同类型的HPC应用中进行了评估，证明了其在自动化和记录可重复性评估方面的有效性。

Conclusion: 更好的HPC兼容CI解决方案（如CORRECT）可以显著提升HPC应用的可重复性，为研究社区提供实用工具。

Abstract: The high-performance computing (HPC) community has adopted incentive
structures to motivate reproducible research, with major conferences awarding
badges to papers that meet reproducibility requirements. Yet, many papers do
not meet such requirements. The uniqueness of HPC infrastructure and software,
coupled with strict access requirements, may limit opportunities for
reproducibility. In the absence of resource access, we believe that regular
documented testing, through continuous integration (CI), coupled with complete
provenance information, can be used as a substitute. Here, we argue that better
HPC-compliant CI solutions will improve reproducibility of applications. We
present a survey of reproducibility initiatives and describe the barriers to
reproducibility in HPC. To address existing limitations, we present a GitHub
Action, CORRECT, that enables secure execution of tests on remote HPC
resources. We evaluate CORRECT's usability across three different types of HPC
applications, demonstrating the effectiveness of using CORRECT for automating
and documenting reproducibility evaluations.

</details>


### [28] [A Knowledge Distillation-empowered Adaptive Federated Reinforcement Learning Framework for Multi-Domain IoT Applications Scheduling](https://arxiv.org/abs/2508.21328)
*Zhiyu Wang,Mohammad Goudarzi,Mingming Gong,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出KD-AFRL框架，通过知识蒸馏和自适应联邦强化学习解决多域IoT应用调度问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构Cloud-Edge-IoT环境中分布式调度优化的挑战，包括非IID数据分布、计算异构性和跨域协作不足。

Method: 提出三种创新：资源感知的混合架构生成机制、隐私保护的聚类联邦学习方法、面向异构模型的知识蒸馏机制。

Result: 实验显示KD-AFRL在收敛速度、完成时间、能耗和成本上分别提升21%、15.7%、10.8%和13.9%，且扩展性表现优异。

Conclusion: KD-AFRL为多域IoT调度提供了高效、可扩展的解决方案，显著优于现有方法。

Abstract: The rapid proliferation of Internet of Things (IoT) applications across
heterogeneous Cloud-Edge-IoT environments presents significant challenges in
distributed scheduling optimization. Existing approaches face issues, including
fixed neural network architectures that are incompatible with computational
heterogeneity, non-Independent and Identically Distributed (non-IID) data
distributions across IoT scheduling domains, and insufficient cross-domain
collaboration mechanisms. This paper proposes KD-AFRL, a Knowledge
Distillation-empowered Adaptive Federated Reinforcement Learning framework that
addresses multi-domain IoT application scheduling through three core
innovations. First, we develop a resource-aware hybrid architecture generation
mechanism that creates dual-zone neural networks enabling heterogeneous devices
to participate in collaborative learning while maintaining optimal resource
utilization. Second, we propose a privacy-preserving environment-clustered
federated learning approach that utilizes differential privacy and K-means
clustering to address non-IID challenges and facilitate effective collaboration
among compatible domains. Third, we introduce an environment-oriented
cross-architecture knowledge distillation mechanism that enables efficient
knowledge transfer between heterogeneous models through temperature-regulated
soft targets. Comprehensive experiments with real Cloud-Edge-IoT infrastructure
demonstrate KD-AFRL's effectiveness using diverse IoT applications. Results
show significant improvements over the best baseline, with 21% faster
convergence and 15.7%, 10.8%, and 13.9% performance gains in completion time,
energy consumption, and weighted cost, respectively. Scalability experiments
reveal that KD-AFRL achieves 3-5 times better performance retention compared to
existing solutions as the number of domains increases.

</details>


### [29] [Unpacking Maximum Extractable Value on Polygon: A Study on Atomic Arbitrage](https://arxiv.org/abs/2508.21473)
*Daniil Vostrikov,Yash Madhwal,Andrey Seoev,Anastasiia Smirnova,Yury Yanovich,Alexey Smirnov,Vladimir Gorgadze*

Main category: cs.DC

TL;DR: 该论文研究了Polygon区块链上的最大可提取价值（MEV），特别是原子套利交易（AA），分析其动态、策略及其对网络的影响。


<details>
  <summary>Details</summary>
Motivation: 探索MEV在Polygon区块链上的表现，尤其是AA交易，以了解其对去中心化金融生态系统的挑战和潜在改进方向。

Method: 通过分析涵盖23百万个区块的22个月数据集，研究AA交易的关键因素，如搜索者行为、投标动态和代币使用，并比较Spam-based和Auction-based的后置策略。

Result: 发现Spam-based交易更普遍，但Auction-based交易更具盈利性。网络架构、交易排序和MEV提取之间有明显的相互作用。

Conclusion: 强调了强大交易排序机制的必要性，并揭示了新兴MEV策略对区块链网络的广泛影响。

Abstract: The evolution of blockchain technology, from its origins as a decentralized
ledger for cryptocurrencies to its broader applications in areas like
decentralized finance (DeFi), has significantly transformed financial
ecosystems while introducing new challenges such as Maximum Extractable Value
(MEV). This paper explores MEV on the Polygon blockchain, with a particular
focus on Atomic Arbitrage (AA) transactions. We establish criteria for
identifying AA transactions and analyze key factors such as searcher behavior,
bidding dynamics, and token usage. Utilizing a dataset spanning 22 months and
covering 23 million blocks, we examine MEV dynamics with a focus on Spam-based
and Auction-based backrunning strategies. Our findings reveal that while
Spam-based transactions are more prevalent, Auction-based transactions
demonstrate greater profitability. Through detailed examples and analysis, we
investigate the interactions between network architecture, transaction
sequencing, and MEV extraction, offering comprehensive insights into the
evolution and challenges of MEV in decentralized ecosystems. These results
emphasize the need for robust transaction ordering mechanisms and highlight the
implications of emerging MEV strategies for blockchain networks.

</details>


### [30] [Odyssey: Adaptive Policy Selection for Resilient Distributed Training](https://arxiv.org/abs/2508.21613)
*Yuhang Zhou,Zhibin Wang,Peng Jiang,Haoran Xia,Junhe Lu,Qianyu Jiang,Rong Gu,Hengxi Xu,Xinjing Huang,Guanghuan Fang,Zhiheng Hu,Jingyi Zhang,Yongjin Cai,Jian He,Chen Tian*

Main category: cs.DC

TL;DR: Odyssey是一种自适应容错系统，能在训练大语言模型时智能选择最优恢复策略，性能损失仅为11%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练常因故障中断，现有容错方法性能开销大。

Method: Odyssey通过性能模型、快速搜索执行计划、优化通信等实现智能恢复。

Result: 实验显示性能损失仅11%，吞吐量最高提升1.355倍。

Conclusion: Odyssey在容错和性能平衡上优于现有方法。

Abstract: Training large language models faces frequent interruptions due to various
faults, demanding robust fault-tolerance. Existing backup-free methods, such as
redundant computation, dynamic parallelism, and data rerouting, each incur
performance penalties, whether from ongoing overhead, lengthy reconfigurations,
or post-recovery inefficiencies. We propose Odyssey, an adaptive fault-tolerant
system that intelligently selects optimal recovery strategies when a failure
occurs. Odyssey achieves this through a unified performance model, expedient
execution plan search, accurate performance estimation, and efficient
communication optimizations. Experiments on a 32-card cluster show that Odyssey
maintains a performance gap of within 11.00% between post-recovery and
failure-free training, while preserving model convergence and efficient memory
usage. Compared to state-of-the-art methods, Odyssey achieves up to 1.229x and
1.355x higher average throughput than Oobleck and Recycle, respectively.

</details>


### [31] [Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding](https://arxiv.org/abs/2508.21706)
*Zhibin Wang,Zhonghui Zhang,Yuhang Zhou,Zibo Wang,Mo Zhou,Peng Jiang,Weilin Cai,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: 本文提出了SpecMoEOff，一种基于推测解码技术的MoE模型卸载方法，通过优化GPU和CPU的协同工作，显著提高了硬件利用率和解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前的MoE模型卸载技术由于I/O瓶颈和稀疏计算问题，导致硬件利用率低下。

Method: 采用推测解码技术扩增专家工作量，结合理论和经验分析优化GPU和CPU协同工作，并开发专用CPU分块注意力验证核以减少额外开销。

Result: 实验结果表明，SpecMoEOff的解码吞吐量比现有最佳卸载技术提高了2.5倍。

Conclusion: SpecMoEOff通过推测解码和硬件优化，显著提升了MoE模型的卸载效率和性能。

Abstract: Recent advancements in Mixture of Experts (MoE) models have significantly
increased their parameter scale as well as model performance. Extensive
offloading techniques have been proposed to address the GPU memory limitations
of MoE inference. However, due to the I/O bottleneck and sparse computation of
MoE models, existing offloading techniques still suffer from low hardware
utilization. To fully utilize the hardware resources, we propose SpecMoEOff,
which employs the speculative decoding technique to enlarge the workload of
each expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and
empirical roofline analysis. In addition, we develop a dedicated CPU chunked
attention verification kernel to fit the speculative decoding in offloading
scenarios as well as minimizing the additional overhead led by draft models.
SpecMoEOff further integrates an optimizer to automatically tune the
hyperparameters of speculative decoding under given hardware and workload.
Experimental results show that SpecMoEOff achieves up to 2.5x decode throughput
improvement over the state-of-the-art MoE offloading techniques.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [32] [ORCA: ORchestrating Causal Agent](https://arxiv.org/abs/2508.21304)
*Joanie Hayoun Chung,Chaemyung Lim,Sumin Lee,Sungbin Lim*

Main category: cs.DB

TL;DR: ORCA是一个基于LLM的自动因果推理系统，能自动化RDBMS中的数据分析流程，并通过人机交互保留专家监督，显著提升业务决策效率。


<details>
  <summary>Details</summary>
Motivation: 应对非专家在关系数据库中执行复杂数据分析流程时的瓶颈，提升因果推理的效率与准确性。

Method: 提出ORCA系统，集成自然语言查询解析、SQL生成、数据预处理及因果建模，支持人机交互。

Result: 在基准测试中，ORCA在查询生成和因果效应估计上表现优异，比GPT-4o mini提升7倍以上。

Conclusion: ORCA通过自动化与专家监督结合，为业务决策提供高效可靠的因果分析工具。

Abstract: Causal inference is essential for decision-making science while the
complexity of the data analysis workflow, ranging from data wrangling to causal
analysis, increases substantially as the scale of data grows in complicated
business environments. Especially, the execution of the workflow in relational
databases by non-experts can result in repetitive bottlenecks which impede
timely and responsible business insights. To address this challenge, we propose
ORCA (Orchestrating Causal Agent), an LLM agentic system that can automate
routine workflows in RDBMS while preserving expert oversight via human-AI
interactions. ORCA orchestrates the full data analysis pipeline: interpreting
natural language queries, navigating tables from DB servers, generating proper
SQL codes, preprocessing data, and configuring modeling processes using causal
inference libraries. Domain experts still can control the automation through
iterative interactions with ORCA, enabling robust data-driven decision making
with less technical expertise in statistical computing. Empirical evaluations
on benchmark and synthetic e-commerce datasets demonstrate competitive
performance of ORCA in table understanding, query generation, and cause-effect
estimation -- achieving over $7\times$ improvement in estimating average
treatment compared to GPT-4o mini.

</details>


### [33] [Hilbert Forest in the SISAP 2025 Indexing Challenge](https://arxiv.org/abs/2508.21682)
*Yasunobu Imamura,Takeshi Shinohara,Naoya Higuchi,Kouichi Hirata,Tetsuji Kuboyama*

Main category: cs.DB

TL;DR: 论文介绍了一种名为Hilbert森林的新型索引技术，用于高效高维数据的近似最近邻搜索，并在SISAP 2025挑战赛中展示了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据在大规模数据集上的近似最近邻搜索问题，尤其是在严格的内存限制条件下。

Method: 基于Hilbert空间填充曲线的快速排序算法，构建多个Hilbert树（Hilbert森林）以支持高效搜索。

Result: 在Task 1中表现竞争性，在Task 2中实现了最快的构建时间，同时满足召回率要求。

Conclusion: Hilbert排序索引技术在严格内存限制下具有实际应用价值。

Abstract: We report our participation in the SISAP 2025 Indexing Challenge using a
novel indexing technique called the Hilbert forest. The method is based on the
fast Hilbert sort algorithm, which efficiently orders high-dimensional points
along a Hilbert space-filling curve, and constructs multiple Hilbert trees to
support approximate nearest neighbor search. We submitted implementations to
both Task 1 (approximate search on the PUBMED23 dataset) and Task 2 (k-nearest
neighbor graph construction on the GOOAQ dataset) under the official resource
constraints of 16 GB RAM and 8 CPU cores. The Hilbert forest demonstrated
competitive performance in Task 1 and achieved the fastest construction time in
Task 2 while satisfying the required recall levels. These results highlight the
practical effectiveness of Hilbert order-based indexing under strict memory
limitations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [34] [SCE-NTT: A Hardware Accelerator for Number Theoretic Transform Using Superconductor Electronics](https://arxiv.org/abs/2508.21265)
*Sasan Razmkhah,Mingye Li,Zeming Cheng,Robert S. Aviles,Kyle Jackman,Joey Delport,Lieze Schindler,Wenhui Luo,Takuya Suzuki,Mehdi Kamal,Christopher L. Ayala,Coenrad J. Fourie,Nabuyuki Yoshikawa,Peter A. Beerel,Sandeep Gupta,Massoud Pedram*

Main category: cs.AR

TL;DR: 该研究提出了一种基于超导电子学（SCE）的硬件加速器SCE-NTT，用于加速全同态加密（FHE）中的数论变换（NTT）。通过新型超导单磁通量子（SFQ）逻辑和存储器设计，实现了高性能和能效。


<details>
  <summary>Details</summary>
Motivation: 传统CMOS技术在全同态加密的核心计算瓶颈（NTT）中面临性能与能效限制，需探索超导电子学（SCE）的潜力。

Method: 设计了一款深度流水线的NTT-128架构，利用移位寄存器内存（SRM）解决SFQ技术的RAM和扇入/扇出限制，并开发了新的RSFQ单元库。

Result: NTT-128单元在34 GHz频率下每秒执行5.31亿次NTT，性能是同类CMOS的100倍以上，并能扩展到更大规模。

Conclusion: SCE加速器在全同态加密和后量子安全性方面表现出巨大潜力，未来可通过制造技术进步进一步提升性能。

Abstract: This research explores the use of superconductor electronics (SCE) for
accelerating fully homomorphic encryption (FHE), focusing on the
Number-Theoretic Transform (NTT), a key computational bottleneck in FHE
schemes. We present SCE-NTT, a dedicated hardware accelerator based on
superconductive single flux quantum (SFQ) logic and memory, targeting high
performance and energy efficiency beyond the limits of conventional CMOS. To
address SFQ constraints such as limited dense RAM and restricted fanin/fanout,
we propose a deeply pipelined NTT-128 architecture using shift register memory
(SRM). Designed for N=128 32-bit coefficients, NTT-128 comprises log2(N)=7
processing elements (PEs), each featuring a butterfly unit (BU), dual
coefficient memories operating in ping-pong mode via FIFO-based SRM queues, and
twiddle factor buffers. The BU integrates a Shoup modular multiplier optimized
for a small area, leveraging precomputed twiddle factors. A new RSFQ cell
library with over 50 parameterized cells, including compound logic units, was
developed for implementation. Functional and timing correctness were validated
using JoSIM analog simulations and Verilog models. A multiphase clocking scheme
was employed to enhance robustness and reduce path-balancing overhead,
improving circuit reliability. Fabricated results show the NTT-128 unit
achieves 531 million NTT/sec at 34 GHz, over 100x faster than state-of-the-art
CMOS equivalents. We also project that the architecture can scale to larger
sizes, such as a 2^14-point NTT in approximately 482 ns. Key-switch throughput
is estimated at 1.63 million operations/sec, significantly exceeding existing
hardware. These results demonstrate the strong potential of SCE-based
accelerators for scalable, energy-efficient secure computation in the
post-quantum era, with further gains anticipated through advances in
fabrication.

</details>


### [35] [Catwalk: Unary Top-K for Efficient Ramp-No-Leak Neuron Design for Temporal Neural Networks](https://arxiv.org/abs/2508.21267)
*Devon Lister,Prabhu Vellaisamy,John Paul Shen,Di Wu*

Main category: cs.AR

TL;DR: 该论文提出了一种名为Catwalk的神经元实现方法，通过利用输入稀疏性优化硬件效率，在面积和功耗上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于实际脉冲流中只有少量输入在计算周期内活跃，利用这种稀疏性可提升硬件效率。

Method: 采用Catwalk神经元实现，通过排序并聚类稀疏输入（使用一元top-k方法）来减少并行计数器的成本。

Result: 实验结果显示，Catwalk在面积和功耗上分别比现有方法提升了1.39倍和1.86倍。

Conclusion: Catwalk方法通过优化稀疏输入处理，显著提升了硬件效率，适用于时序神经网络的实现。

Abstract: Temporal neural networks (TNNs) are neuromorphic neural networks that utilize
bit-serial temporal coding. TNNs are composed of columns, which in turn employ
neurons as their building blocks. Each neuron processes volleys of input
spikes, modulated by associated synaptic weights, on its dendritic inputs.
Recently proposed neuron implementation in CMOS employs a Spike Response Model
(SRM) with a ramp-no-leak (RNL) response function and assumes all the inputs
can carry spikes. However, in actual spike volleys, only a small subset of the
dendritic inputs actually carry spikes in each compute cycle. This form of
sparsity can be exploited to achieve better hardware efficiency. In this paper,
we propose a Catwalk neuron implementation by relocating spikes in a spike
volley as a sorted subset cluster via unary top-k. Such relocation can
significantly reduce the cost of the subsequent parallel counter (PC) for
accumulating the response functions from the spiking inputs. This can lead to
improvements on area and power efficiency in RNL neuron implementation.
Place-and-route results show Catwalk is 1.39x and 1.86x better in area and
power, respectively, as compared to existing SRM0-RNL neurons.

</details>


### [36] [SIRA: Scaled-Integer Range Analysis for Optimizing FPGA Dataflow Neural Network Accelerators](https://arxiv.org/abs/2508.21493)
*Yaman Umuroglu,Christoph Berganski,Felix Jentzsch,Michal Danilowicz,Tomasz Kryjak,Charalampos Bezaitis,Magnus Sjalander,Ian Colbert,Thomas Preusser,Jakoba Petri-Koenig,Michaela Blott*

Main category: cs.AR

TL;DR: 该论文提出了一种名为SIRA的静态分析技术，用于优化量化神经网络中的非矩阵乘法操作，从而减少FPGA加速器的资源消耗。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络虽然降低了矩阵乘法的成本，但过度量化可能导致非矩阵乘法操作成为性能瓶颈，尤其是在嵌入式系统中。

Method: 引入了SIRA技术，通过区间算术确定量化神经网络中张量的范围、比例和偏差，并利用这些信息优化位宽调整和操作转换。

Result: 实验结果显示，SIRA优化平均减少了17%的LUT资源、66%的DSP资源和22%的累加器位宽。

Conclusion: SIRA技术有效减少了量化神经网络的资源消耗，并开源以促进广泛研究和应用。

Abstract: While neural network quantization effectively reduces the cost of matrix
multiplications, aggressive quantization can expose non-matrix-multiply
operations as significant performance and resource bottlenecks on embedded
systems. Addressing such bottlenecks requires a comprehensive approach to
tailoring the precision across operations in the inference computation. To this
end, we introduce scaled-integer range analysis (SIRA), a static analysis
technique employing interval arithmetic to determine the range, scale, and bias
for tensors in quantized neural networks. We show how this information can be
exploited to reduce the resource footprint of FPGA dataflow neural network
accelerators via tailored bitwidth adaptation for accumulators and downstream
operations, aggregation of scales and biases, and conversion of consecutive
elementwise operations to thresholding operations. We integrate SIRA-driven
optimizations into the open-source FINN framework, then evaluate their
effectiveness across a range of quantized neural network workloads and compare
implementation alternatives for non-matrix-multiply operations. We demonstrate
an average reduction of 17% for LUTs, 66% for DSPs, and 22% for accumulator
bitwidths with SIRA optimizations, providing detailed benchmark analysis and
analytical models to guide the implementation style for non-matrix layers.
Finally, we open-source SIRA to facilitate community exploration of its
benefits across various applications and hardware platforms.

</details>


### [37] [Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators](https://arxiv.org/abs/2508.21524)
*Wenyong Zhou,Zhengwu Liu,Yuan Ren,Ngai Wong*

Main category: cs.AR

TL;DR: 提出了一种二进制权重多比特激活（BWMA）方法，用于CIM加速器上的CNN，兼顾硬件效率和模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在二进制权重量化下牺牲精度，而多比特量化则效率受限，BWMA旨在解决这一权衡问题。

Method: 通过闭式解优化每层权重量化，并提出可微的激活量化函数，避免繁琐的搜索过程。

Result: 在CIFAR-10和ImageNet上，BWMA分别提升精度1.44%-5.46%和0.35%-5.37%；4比特激活量化在硬件成本与性能间最优。

Conclusion: BWMA在CIM平台实现了精度与效率的双重提升，4比特激活量化是理想选择。

Abstract: Compute-in-memory (CIM) accelerators have emerged as a promising way for
enhancing the energy efficiency of convolutional neural networks (CNNs).
Deploying CNNs on CIM platforms generally requires quantization of network
weights and activations to meet hardware constraints. However, existing
approaches either prioritize hardware efficiency with binary weight and
activation quantization at the cost of accuracy, or utilize multi-bit weights
and activations for greater accuracy but limited efficiency. In this paper, we
introduce a novel binary weight multi-bit activation (BWMA) method for CNNs on
CIM-based accelerators. Our contributions include: deriving closed-form
solutions for weight quantization in each layer, significantly improving the
representational capabilities of binarized weights; and developing a
differentiable function for activation quantization, approximating the ideal
multi-bit function while bypassing the extensive search for optimal settings.
Through comprehensive experiments on CIFAR-10 and ImageNet datasets, we show
that BWMA achieves notable accuracy improvements over existing methods,
registering gains of 1.44\%-5.46\% and 0.35\%-5.37\% on respective datasets.
Moreover, hardware simulation results indicate that 4-bit activation
quantization strikes the optimal balance between hardware cost and model
performance.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [38] [An Optimistic Gradient Tracking Method for Distributed Minimax Optimization](https://arxiv.org/abs/2508.21431)
*Yan Huang,Jinming Xu,Jiming Chen,Karl Henrik Johansson*

Main category: math.OC

TL;DR: 本文研究了分布式网络中的极小极大优化问题，提出了一种分布式乐观梯度跟踪方法（DOGT），通过局部近似集中式乐观方法提升收敛性能，并在强凸-强凹函数下实现线性收敛。进一步提出加速版ADOGT算法，达到最优收敛率和通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 分布式优化中的异构性和收敛性能是重要挑战，需要提出高效且鲁棒的算法来解决极小极大问题。

Method: 提出了DOGT方法，通过构建代理函数捕捉局部目标函数的相似性，并结合加速共识协议开发了ADOGT算法。

Result: DOGT实现了强凸-强凹函数下的线性收敛，ADOGT达到了最优收敛率和通信复杂度。数值实验验证了算法的有效性。

Conclusion: DOGT和ADOGT为分布式极小极大优化提供了高效且鲁棒的解决方案，显著提升了收敛性能。

Abstract: This paper studies the distributed minimax optimization problem over
networks. To enhance convergence performance, we propose a distributed
optimistic gradient tracking method, termed DOGT, which solves a surrogate
function that captures the similarity between local objective functions to
approximate a centralized optimistic approach locally. Leveraging a
Lyapunov-based analysis, we prove that DOGT achieves linear convergence to the
optimal solution for strongly convex-strongly concave objective functions while
remaining robust to the heterogeneity among them. Moreover, by integrating an
accelerated consensus protocol, the accelerated DOGT (ADOGT) algorithm achieves
an optimal convergence rate of $\mathcal{O} \left( \kappa \log \left( \epsilon
^{-1} \right) \right)$ and communication complexity of $\mathcal{O} \left(
\kappa \log \left( \epsilon ^{-1} \right) /\sqrt{1-\sqrt{\rho _W}} \right)$ for
a suboptimality level of $\epsilon>0$, where $\kappa$ is the condition number
of the objective function and $\rho_W$ is the spectrum gap of the network.
Numerical experiments illustrate the effectiveness of the proposed algorithms.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [39] [Epsilon-saturation for stable graphs and Littlestone classes](https://arxiv.org/abs/2508.21807)
*Maryanthe Malliaris,Olga Medrano Martín del Campo,Shay Moran*

Main category: math.LO

TL;DR: 研究了Littlestone类或稳定图的ε饱和度，分析了其作为虚拟元素的闭包性质及其对Littlestone维度和VC维度的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过引入ε饱和度来扩展Littlestone类或稳定图，以理解虚拟元素的作用及其对学习模型的影响。

Method: 引入ε饱和度的概念，并在Littlestone类或稳定图中进行闭包操作，分析其性质及其对参数选择的依赖性。

Result: 证明了ε饱和度下的闭包在合理参数选择下仍保持Littlestone或稳定性，但其维度可能变化，揭示了ε与Littlestone/稳定性及VC维度的关系。

Conclusion: ε饱和度为Littlestone类或稳定图提供了新的分析视角，揭示了虚拟元素在学习和模型理论中的重要性，并展示了参数选择对其性质的影响。

Abstract: Any Littlestone class, or stable graph, has finite sets which function as
``virtual elements'': these can be seen from the learning side as representing
hypotheses which are expressible as weighted majority opinions of hypotheses in
the class, and from the model-theoretic side as an approximate finitary version
of realizing types. We introduce and study the epsilon-saturation of a
Littlestone class, or stable graph, which is essentially the closure of the
class under inductively adding all such virtual elements. We characterize this
closure and prove that under reasonable choices of parameters, it remains
Littlestone (or stable), though not always of the same Littlestone dimension.
This highlights some surprising phenomena having to do with regimes of epsilon
and the relation between Littlestone/stability and VC dimension.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [40] [The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation](https://arxiv.org/abs/2508.21219)
*A H M Nazmus Sakib,Mahsin Bin Akram,Joseph Spracklen,Sahan Kalutarage,Raveen Wijewickrama,Igor Bilogrevic,Murtuza Jadliwala*

Main category: cs.CR

TL;DR: 论文系统评估了WebAssembly(WASM)混淆对现代浏览器指纹防御的影响，发现学术研究与实际防御工具在效果上存在差距。


<details>
  <summary>Details</summary>
Motivation: 研究WASM混淆技术对现有指纹防御的潜在威胁，填补JavaScript防御研究的盲点。

Method: 开发自动化工具将JS指纹脚本转为WASM混淆版本，测试两类防御工具：学术检测器和商业浏览器工具。

Result: 学术检测器因依赖过时数据集或缺乏WASM兼容性而表现一般，商业工具因其API拦截机制仍完全有效。

Conclusion: WASM混淆揭示了学术与实际防御策略的差距，为未来防御和攻击技术提供了方向。

Abstract: Browser fingerprinting defenses have historically focused on detecting
JavaScript(JS)-based tracking techniques. However, the widespread adoption of
WebAssembly (WASM) introduces a potential blind spot, as adversaries can
convert JS to WASM's low-level binary format to obfuscate malicious logic. This
paper presents the first systematic evaluation of how such WASM-based
obfuscation impacts the robustness of modern fingerprinting defenses. We
develop an automated pipeline that translates real-world JS fingerprinting
scripts into functional WASM-obfuscated variants and test them against two
classes of defenses: state-of-the-art detectors in research literature and
commercial, in-browser tools. Our findings reveal a notable divergence:
detectors proposed in the research literature that rely on feature-based
analysis of source code show moderate vulnerability, stemming from outdated
datasets or a lack of WASM compatibility. In contrast, defenses such as browser
extensions and native browser features remained completely effective, as their
API-level interception is agnostic to the script's underlying implementation.
These results highlight a gap between academic and practical defense strategies
and offer insights into strengthening detection approaches against WASM-based
obfuscation, while also revealing opportunities for more evasive techniques in
future attacks.

</details>


### [41] [Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain](https://arxiv.org/abs/2508.21480)
*Narges Dadkhah,Khan Reaz,Gerhard Wunder*

Main category: cs.CR

TL;DR: 本文提出了一种基于区块链的去中心化智能家居设备注册框架，解决了传统集中式PKI模型的安全风险，通过联盟链技术提升透明度与安全性，并在原型中验证了其高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 智能家居设备的安全注册和信任建立面临挑战，传统方法依赖集中式PKI模型，存在安全风险且限制用户主权。

Method: 提出一种结合联盟区块链技术的去中心化注册框架，支持设备注册、密钥撤销、访问控制等功能，并使用Tamarin Prover进行形式化验证。

Result: 原型验证显示注册完成时间为0.34秒，区块链方法能高效处理负载，保持高吞吐量和低延迟。

Conclusion: 该框架为智能家居提供了一种安全、可扩展的去中心化解决方案，适用于资源受限设备。

Abstract: The increasing adoption of smart home devices and IoT-based security systems
presents significant opportunities to enhance convenience, safety, and risk
management for homeowners and service providers. However, secure
onboarding-provisioning credentials and establishing trust with cloud
platforms-remains a considerable challenge. Traditional onboarding methods
often rely on centralized Public Key Infrastructure (PKI) models and
manufacturer-controlled keys, which introduce security risks and limit the
user's digital sovereignty. These limitations hinder the widespread deployment
of scalable IoT solutions. This paper presents a novel onboarding framework
that builds upon existing network-layer onboarding techniques and extends them
to the application layer to address these challenges. By integrating consortium
blockchain technology, we propose a decentralized onboarding mechanism that
enhances transparency, security, and monitoring for smart home architectures.
The architecture supports device registration, key revocation, access control
management, and risk detection through event-driven alerts across dedicated
blockchain channels and smart contracts. To evaluate the framework, we formally
model the protocol using the Tamarin Prover under the Dolev-Yao adversary
model. The analysis focuses on authentication, token integrity, key
confidentiality, and resilience over public channels. A prototype
implementation demonstrates the system's viability in smart home settings, with
verification completing in 0.34 seconds, highlighting its scalability and
suitability for constrained devices and diverse stakeholders. Additionally,
performance evaluation shows that the blockchain-based approach effectively
handles varying workloads, maintains high throughput and low latency, and
supports near real-time IoT data processing.

</details>


### [42] [Generalized Encrypted Traffic Classification Using Inter-Flow Signals](https://arxiv.org/abs/2508.21558)
*Federica Bianchi,Edoardo Di Paolo,Angelo Spognardi*

Main category: cs.CR

TL;DR: 本文提出了一种新颖的加密流量分类模型，直接处理原始PCAP数据，无需预定义流量类型，利用创新的跨流信号表示方法，在多任务分类和大多数数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有加密流量分类方法通常需要预定义流量类型，且缺乏跨任务通用性，因此需要一种更通用、无需假设的新模型。

Method: 提出一种直接处理原始PCAP数据的模型，利用创新的跨流信号（涵盖时间相关性和数据包分布）进行特征表示。

Result: 模型在多个分类任务和数据集上表现优异，部分情况下准确率达99%，显著优于现有方法。

Conclusion: 该模型具有强通用性和适应性，为加密流量分类提供了新思路。

Abstract: In this paper, we present a novel encrypted traffic classification model that
operates directly on raw PCAP data without requiring prior assumptions about
traffic type. Unlike existing methods, it is generalizable across multiple
classification tasks and leverages inter-flow signals - an innovative
representation that captures temporal correlations and packet volume
distributions across flows. Experimental results show that our model
outperforms well-established methods in nearly every classification task and
across most datasets, achieving up to 99% accuracy in some cases, demonstrating
its robustness and adaptability.

</details>


### [43] [Locus: Agentic Predicate Synthesis for Directed Fuzzing](https://arxiv.org/abs/2508.21302)
*Jie Zhu,Chihao Shen,Ziyang Li,Jiahao Yu,Yizheng Chen,Kexin Pei*

Main category: cs.CR

TL;DR: Locus框架通过合成谓词来提升定向模糊测试的效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决定向模糊测试中目标状态难以精确引导的问题，克服现有方法的局限性。

Method: 利用程序分析工具合成和迭代优化谓词，作为语义中间状态指导模糊测试。

Result: Locus在发现真实漏洞时平均提速41.6倍，并发现8个未修复漏洞。

Conclusion: Locus通过语义中间状态有效提升定向模糊测试的效率和通用性。

Abstract: Directed fuzzing aims to find program inputs that lead to specified target
program states. It has broad applications, such as debugging system crashes,
confirming reported bugs, and generating exploits for potential
vulnerabilities. This task is inherently challenging because target states are
often deeply nested in the program, while the search space manifested by
numerous possible program inputs is prohibitively large. Existing approaches
rely on branch distances or manually-specified constraints to guide the search;
however, the branches alone are often insufficient to precisely characterize
progress toward reaching the target states, while the manually specified
constraints are often tailored for specific bug types and thus difficult to
generalize to diverse target states and programs.
  We present Locus, a novel framework to improve the efficiency of directed
fuzzing. Our key insight is to synthesize predicates to capture fuzzing
progress as semantically meaningful intermediate states, serving as milestones
towards reaching the target states. When used to instrument the program under
fuzzing, they can reject executions unlikely to reach the target states, while
providing additional coverage guidance. To automate this task and generalize to
diverse programs, Locus features an agentic framework with program analysis
tools to synthesize and iteratively refine the candidate predicates, while
ensuring the predicates strictly relax the target states to prevent false
rejections via symbolic execution. Our evaluation shows that Locus
substantially improves the efficiency of eight state-of-the-art fuzzers in
discovering real-world vulnerabilities, achieving an average speedup of 41.6x.
So far, Locus has found eight previously unpatched bugs, with one already
acknowledged with a draft patch.

</details>


### [44] [Risks and Compliance with the EU's Core Cyber Security Legislation](https://arxiv.org/abs/2508.21386)
*Jukka Ruohonen,Jesper Løffler Nielsen,Jakub Skórczynski*

Main category: cs.CR

TL;DR: 该论文研究了欧盟五项核心网络安全立法中风险概念的框架，分析了这些框架是否趋同或分歧，并探讨了描述风险的法律术语。研究发现法案涵盖了广泛的风险类型，但也存在一些概念上的空白。


<details>
  <summary>Details</summary>
Motivation: 欧盟采用基于风险的方法进行监管，网络安全立法也不例外。论文旨在分析这些立法中风险概念的框架，以理解其一致性和差异性。

Method: 论文采用定性法律解释和分类学构建的方法，分析了五项核心网络安全立法中的风险框架和术语。

Result: 研究发现这些立法涵盖了技术、组织、人为安全等多方面的风险，但也存在关于可接受风险、非概率风险和剩余风险的空白。

Conclusion: 欧盟的网络安全立法扩展了基于风险的监管方法，但也增加了复杂性。论文提出了一些应对合规和研究的实践建议。

Abstract: The European Union (EU) has long favored a risk-based approach to regulation.
Such an approach is also used in recent cyber security legislation enacted in
the EU. Risks are also inherently related to compliance with the new
legislation. Objective: The paper investigates how risks are framed in the EU's
five core cyber security legislative acts, whether the framings indicate
convergence or divergence between the acts and their risk concepts, and what
qualifying words and terms are used when describing the legal notions of risks.
Method : The paper's methodology is based on qualitative legal interpretation
and taxonomy-building. Results: The five acts have an encompassing coverage of
different cyber security risks, including but not limited to risks related to
technical, organizational, and human security as well as those not originating
from man-made actions. Both technical aspects and assets are used to frame the
legal risk notions in many of the legislative acts. A threat-centric viewpoint
is also present in one of the acts. Notable gaps are related to acceptable
risks, non-probabilistic risks, and residual risks. Conclusion: The EU's new
cyber security legislation has significantly extended the risk-based approach
to regulations. At the same time, complexity and compliance burden have
increased. With this point in mind, the paper concludes with a few practical
takeaways about means to deal with compliance and research it.

</details>


### [45] [An Empirical Study of Vulnerable Package Dependencies in LLM Repositories](https://arxiv.org/abs/2508.21417)
*Shuhan Liu,Xing Hu,Xin Xia,David Lo,Xiaohu Yang*

Main category: cs.CR

TL;DR: 论文分析了大型语言模型（LLM）依赖供应链中的安全漏洞，发现半数漏洞未被披露超过56.2个月，且75.8%的LLMs配置文件中存在漏洞依赖。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在各领域取得成功，但其依赖外部代码的供应链漏洞被忽视，可能带来安全风险。

Method: 对52个开源LLMs进行实证分析，研究其第三方依赖及漏洞，并与Python生态系统对比。

Result: LLM生态系统中漏洞披露时间更长，且多数LLMs存在漏洞依赖。

Conclusion: 研究揭示了LLM供应链风险，为实践者提供见解，并提出改进安全的方向。

Abstract: Large language models (LLMs) have developed rapidly in recent years,
revolutionizing various fields. Despite their widespread success, LLMs heavily
rely on external code dependencies from package management systems, creating a
complex and interconnected LLM dependency supply chain. Vulnerabilities in
dependencies can expose LLMs to security risks. While existing research
predominantly focuses on model-level security threats, vulnerabilities within
the LLM dependency supply chain have been overlooked. To fill this gap, we
conducted an empirical analysis of 52 open-source LLMs, examining their
third-party dependencies and associated vulnerabilities. We then explored
activities within the LLM repositories to understand how maintainers manage
third-party vulnerabilities in practice. Finally, we compared third-party
dependency vulnerabilities in the LLM ecosystem to those in the Python
ecosystem. Our results show that half of the vulnerabilities in the LLM
ecosystem remain undisclosed for more than 56.2 months, significantly longer
than those in the Python ecosystem. Additionally, 75.8% of LLMs include
vulnerable dependencies in their configuration files. This study advances the
understanding of LLM supply chain risks, provides insights for practitioners,
and highlights potential directions for improving the security of the LLM
supply chain.

</details>


### [46] [RepoMark: A Code Usage Auditing Framework for Code Large Language Models](https://arxiv.org/abs/2508.21432)
*Wenjie Qu,Yuguang Zhou,Bo Wang,Wengrui Zheng,Yuexin Li,Jinyuan Jia,Jiaheng Zhang*

Main category: cs.CR

TL;DR: RepoMark是一种新型数据标记框架，用于审核代码LLM的数据使用，能高效检测代码是否被用于训练，并保证语义完整性和低误检率。


<details>
  <summary>Details</summary>
Motivation: 解决代码LLM训练中数据授权和开源许可证合规性的法律与伦理问题，增强数据使用的透明度。

Method: 提出RepoMark框架，通过生成语义等效的代码变体并嵌入数据标记，利用基于排序的假设检验检测模型记忆。

Result: 在5%的误检率保证下，小代码库的检测成功率超过90%，显著优于现有方法。

Conclusion: RepoMark为代码LLM训练提供了透明且可靠的解决方案，有效保护代码库所有者的权益。

Abstract: The rapid development of Large Language Models (LLMs) for code generation has
transformed software development by automating coding tasks with unprecedented
efficiency.
  However, the training of these models on open-source code repositories (e.g.,
from GitHub) raises critical ethical and legal concerns, particularly regarding
data authorization and open-source license compliance. Developers are
increasingly questioning whether model trainers have obtained proper
authorization before using repositories for training, especially given the lack
of transparency in data collection.
  To address these concerns, we propose a novel data marking framework RepoMark
to audit the data usage of code LLMs. Our method enables repository owners to
verify whether their code has been used in training, while ensuring semantic
preservation, imperceptibility, and theoretical false detection rate (FDR)
guarantees. By generating multiple semantically equivalent code variants,
RepoMark introduces data marks into the code files, and during detection,
RepoMark leverages a novel ranking-based hypothesis test to detect memorization
within the model. Compared to prior data auditing approaches, RepoMark
significantly enhances sample efficiency, allowing effective auditing even when
the user's repository possesses only a small number of code files.
  Experiments demonstrate that RepoMark achieves a detection success rate over
90\% on small code repositories under a strict FDR guarantee of 5\%. This
represents a significant advancement over existing data marking techniques, all
of which only achieve accuracy below 55\% under identical settings. This
further validates RepoMark as a robust, theoretically sound, and promising
solution for enhancing transparency in code LLM training, which can safeguard
the rights of repository owners.

</details>


### [47] [Detecting Stealthy Data Poisoning Attacks in AI Code Generators](https://arxiv.org/abs/2508.21636)
*Cristina Improta*

Main category: cs.CR

TL;DR: 论文研究了现有数据投毒检测方法在无触发器威胁模型下的有效性，发现现有方法难以检测这种隐蔽攻击。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在代码生成中容易受到隐蔽的数据投毒攻击，需要研究其检测方法的有效性。

Method: 对三种深度学习模型（CodeBERT、CodeT5+、AST-T5）进行了针对性投毒，并评估了光谱签名分析、激活聚类和静态分析作为防御手段的效果。

Result: 所有方法均难以检测无触发器的投毒攻击，表现不佳的原因是表示方法无法隔离有毒样本，静态分析存在误报和漏报。

Conclusion: 需要开发更鲁棒的、不依赖触发器的防御方法以保护AI辅助代码生成。

Abstract: Deep learning (DL) models for natural language-to-code generation have become
integral to modern software development pipelines. However, their heavy
reliance on large amounts of data, often collected from unsanitized online
sources, exposes them to data poisoning attacks, where adversaries inject
malicious samples to subtly bias model behavior. Recent targeted attacks
silently replace secure code with semantically equivalent but vulnerable
implementations without relying on explicit triggers to launch the attack,
making it especially hard for detection methods to distinguish clean from
poisoned samples. We present a systematic study on the effectiveness of
existing poisoning detection methods under this stealthy threat model.
Specifically, we perform targeted poisoning on three DL models (CodeBERT,
CodeT5+, AST-T5), and evaluate spectral signatures analysis, activation
clustering, and static analysis as defenses. Our results show that all methods
struggle to detect triggerless poisoning, with representation-based approaches
failing to isolate poisoned samples and static analysis suffering false
positives and false negatives, highlighting the need for more robust,
trigger-independent defenses for AI-assisted code generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [48] [GLENDA: Gynecologic Laparoscopy Endometriosis Dataset](https://arxiv.org/abs/2508.21398)
*Andreas Leibetseder,Sabrina Kletz,Klaus Schoeffmann,Simon Keckstein,Jörg Keckstein*

Main category: cs.CV

TL;DR: 论文介绍了GLENDA数据集，这是一个关于子宫内膜异位症的腹腔镜手术图像数据集，旨在通过计算机视觉和机器学习技术解决手动分析手术记录的耗时问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决传统手动分析腹腔镜手术视频的耗时问题，并填补医学领域相关数据集的空白。

Method: 研究团队与医学专家合作，创建了首个包含子宫内膜异位症区域标注的图像数据集GLENDA。

Result: 发布了GLENDA数据集，为计算机视觉和机器学习技术在手术视频分析中的应用提供了支持。

Conclusion: GLENDA数据集的开源将促进医学图像分析技术的发展，并为手术视频的自动化处理奠定基础。

Abstract: Gynecologic laparoscopy as a type of minimally invasive surgery (MIS) is
performed via a live feed of a patient's abdomen surveying the insertion and
handling of various instruments for conducting treatment. Adopting this kind of
surgical intervention not only facilitates a great variety of treatments, the
possibility of recording said video streams is as well essential for numerous
post-surgical activities, such as treatment planning, case documentation and
education. Nonetheless, the process of manually analyzing surgical recordings,
as it is carried out in current practice, usually proves tediously
time-consuming. In order to improve upon this situation, more sophisticated
computer vision as well as machine learning approaches are actively developed.
Since most of such approaches heavily rely on sample data, which especially in
the medical field is only sparsely available, with this work we publish the
Gynecologic Laparoscopy ENdometriosis DAtaset (GLENDA) - an image dataset
containing region-based annotations of a common medical condition named
endometriosis, i.e. the dislocation of uterine-like tissue. The dataset is the
first of its kind and it has been created in collaboration with leading medical
experts in the field.

</details>


### [49] [Identifying Surgical Instruments in Laparoscopy Using Deep Learning Instance Segmentation](https://arxiv.org/abs/2508.21399)
*Sabrina Kletz,Klaus Schoeffmann,Jenny Benois-Pineau,Heinrich Husslein*

Main category: cs.CV

TL;DR: 论文研究了通过区域全卷积网络在腹腔镜妇科手术视频中分割和识别手术器械的性能，显示高精度的器械分割效果，但器械类型识别仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 手术视频内容索引的自动化是医学内窥镜领域的重大挑战，研究旨在通过分割与识别手术器械提升视频检索能力。

Method: 采用区域全卷积网络进行实例感知的（1）器械分割（二元分割）和（2）器械识别（多类识别）。

Result: 实验显示即使在训练样本较少的情况下，器械区域的分割定位精度较高，但器械类型识别因器械高度相似性而难度大。

Conclusion: 研究证明了手术器械分割的高可行性，但器械类型识别仍需进一步改进以应对实际应用中的高相似性挑战。

Abstract: Recorded videos from surgeries have become an increasingly important
information source for the field of medical endoscopy, since the recorded
footage shows every single detail of the surgery. However, while video
recording is straightforward these days, automatic content indexing - the basis
for content-based search in a medical video archive - is still a great
challenge due to the very special video content. In this work, we investigate
segmentation and recognition of surgical instruments in videos recorded from
laparoscopic gynecology. More precisely, we evaluate the achievable performance
of segmenting surgical instruments from their background by using a
region-based fully convolutional network for instance-aware (1) instrument
segmentation as well as (2) instrument recognition. While the first part
addresses only binary segmentation of instances (i.e., distinguishing between
instrument or background) we also investigate multi-class instrument
recognition (i.e., identifying the type of instrument). Our evaluation results
show that even with a moderately low number of training examples, we are able
to localize and segment instrument regions with a pretty high accuracy.
However, the results also reveal that determining the particular instrument is
still very challenging, due to the inherently high similarity of surgical
instruments.

</details>


### [50] [Learning from Silence and Noise for Visual Sound Source Localization](https://arxiv.org/abs/2508.21761)
*Xavier Juanola,Giovana Morais,Magdalena Fuentes,Gloria Haro*

Main category: cs.CV

TL;DR: 论文提出了新的训练策略和指标，提升了视觉声源定位的性能，尤其在处理低音频-视觉语义对应情况时表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理低音频-视觉语义对应（如静音、噪声、屏外声音）时表现不佳，且评估多限于单一可见声源的场景。

Method: 提出了新的训练策略（SSL-SaN）、新指标以量化特征对齐性和可分性的权衡，并扩展了数据集IS3+。

Result: SSL-SaN模型在自监督模型中达到最先进性能，且在声源定位和跨模态检索中表现优异。

Conclusion: 新方法提升了模型的鲁棒性和性能，并提供了更全面的评估工具和数据。

Abstract: Visual sound source localization is a fundamental perception task that aims
to detect the location of sounding sources in a video given its audio. Despite
recent progress, we identify two shortcomings in current methods: 1) most
approaches perform poorly in cases with low audio-visual semantic
correspondence such as silence, noise, and offscreen sounds, i.e. in the
presence of negative audio; and 2) most prior evaluations are limited to
positive cases, where both datasets and metrics convey scenarios with a single
visible sound source in the scene. To address this, we introduce three key
contributions. First, we propose a new training strategy that incorporates
silence and noise, which improves performance in positive cases, while being
more robust against negative sounds. Our resulting self-supervised model,
SSL-SaN, achieves state-of-the-art performance compared to other
self-supervised models, both in sound localization and cross-modal retrieval.
Second, we propose a new metric that quantifies the trade-off between alignment
and separability of auditory and visual features across positive and negative
audio-visual pairs. Third, we present IS3+, an extended and improved version of
the IS3 synthetic dataset with negative audio.
  Our data, metrics and code are available on the
https://xavijuanola.github.io/SSL-SaN/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL](https://arxiv.org/abs/2508.21739)
*Hamza Ezzaoui Rahali,Abhilasha Dave,Larry Ruckman,Mohammad Mehdi Rahimifar,Audrey C. Therrien,James J. Russel,Ryan T. Herbst*

Main category: cs.LG

TL;DR: SLAC实验室开发的SNL框架和Auto-SNL工具通过FPGA实现实时机器学习推断，显著降低X射线实验中的数据处理延迟和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 高频X射线实验数据量巨大，传统机器学习方法延迟过高，需要高效实时的数据缩减方案。

Method: 开发SNL框架和Auto-SNL工具，支持动态更新模型权重，无需FPGA重新综合，并将Python模型转换为FPGA兼容代码。

Result: SNL在多个神经网络架构中表现优于或相当于现有工具hls4ml，且在某些情况下节省FPGA资源。

Conclusion: SNL为高频实验数据实时处理提供灵活高效的解决方案，适用于多种领域。

Abstract: The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline
experiments at rates of up to 1~MHz, with detectors producing data throughputs
exceeding 1 TB/s. Managing such massive data streams presents significant
challenges, as transmission and storage infrastructures become prohibitively
expensive. Machine learning (ML) offers a promising solution for real-time data
reduction, but conventional implementations introduce excessive latency, making
them unsuitable for high-speed experimental environments. To address these
challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized
framework designed to deploy real-time ML inference models on
Field-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to
dynamically update model weights without requiring FPGA resynthesis, enhancing
flexibility for adaptive learning applications. To further enhance usability
and accessibility, we introduce Auto-SNL, a Python extension that streamlines
the process of converting Python-based neural network models into
SNL-compatible high-level synthesis code. This paper presents a benchmark
comparison against hls4ml, the current state-of-the-art tool, across multiple
neural network architectures, fixed-point precisions, and synthesis
configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL
achieves competitive or superior latency in most tested architectures, while in
some cases also offering FPGA resource savings. This adaptation demonstrates
SNL's versatility, opening new opportunities for researchers and academics in
fields such as high-energy physics, medical imaging, robotics, and many more.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [52] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文提出了Misviz和Misviz-synth两个数据集，用于检测和分类误导性可视化图表，并评估了多种AI模型在此任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 误导性可视化图表在社交媒体和网页中广泛传播，导致读者得出错误结论。缺乏大型、多样化、公开可用的数据集限制了AI模型的训练和评估。

Method: 发布了Misviz（2604个真实图表）和Misviz-synth（81814个合成图表）数据集，并评估了多模态大语言模型、规则系统和微调分类器的性能。

Result: 任务极具挑战性，现有模型表现有待提升。

Conclusion: 发布的数据集和代码为未来研究提供了资源，有助于减少误导性信息的传播。

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [53] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
*Sarfaroz Yunusov,Kaige Chen,Kazi Nishat Anwar,Ali Emami*

Main category: cs.CL

TL;DR: 研究探讨不同性格特质用户是否对特定大语言模型有偏好，发现理性型偏好GPT-4，理想型偏好Claude 3.5。


<details>
  <summary>Details</summary>
Motivation: 探究用户性格与语言模型选择的关联性。

Method: 32名参与者分四种性格类型，评估与GPT-4和Claude 3.5在多任务中的互动。

Result: 性格驱动明显偏好：理性型爱用GPT-4，理想型倾向Claude 3.5；其他类型偏好因任务而异。

Conclusion: 性格分析可揭示传统评测遗漏的模型差异。

Abstract: As Large Language Models (LLMs) increasingly integrate into everyday
workflows, where users shape outcomes through multi-turn collaboration, a
critical question emerges: do users with different personality traits
systematically prefer certain LLMs over others? We conducted a study with 32
participants evenly distributed across four Keirsey personality types,
evaluating their interactions with GPT-4 and Claude 3.5 across four
collaborative tasks: data analysis, creative writing, information retrieval,
and writing assistance. Results revealed significant personality-driven
preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented
tasks, while idealists favored Claude 3.5, especially for creative and
analytical tasks. Other personality types showed task-dependent preferences.
Sentiment analysis of qualitative feedback confirmed these patterns. Notably,
aggregate helpfulness ratings were similar across models, showing how
personality-based analysis reveals LLM differences that traditional evaluations
miss.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [54] [Zero-Shot KWS for Children's Speech using Layer-Wise Features from SSL Models](https://arxiv.org/abs/2508.21248)
*Subham Kutum,Abhijit Sinha,Hemant Kumar Kathania,Sudarsana Reddy Kadiri,Mahesh Chandra Govil*

Main category: eess.AS

TL;DR: 论文提出了一种基于自监督学习（SSL）的零样本关键词识别（KWS）方法，针对儿童语音的特点，取得了优异的效果。


<details>
  <summary>Details</summary>
Motivation: 儿童语音因其独特的声学和语言特征，对KWS系统提出了挑战，现有方法主要针对成人语音。

Method: 利用Wav2Vec2、HuBERT和Data2Vec等SSL模型逐层提取特征，训练Kaldi的KWS系统，并在儿童语音数据集上进行零样本测试。

Result: Wav2Vec2的22层表现最佳，ATWV得分为0.691，显著优于传统MFCC基线，且在不同年龄组和噪声条件下均表现出色。

Conclusion: SSL特征显著提升了儿童语音的零样本KWS性能，解决了儿童语音的独特挑战。

Abstract: Numerous methods have been proposed to enhance Keyword Spotting (KWS) in
adult speech, but children's speech presents unique challenges for KWS systems
due to its distinct acoustic and linguistic characteristics. This paper
introduces a zero-shot KWS approach that leverages state-of-the-art
self-supervised learning (SSL) models, including Wav2Vec2, HuBERT and Data2Vec.
Features are extracted layer-wise from these SSL models and used to train a
Kaldi-based DNN KWS system. The WSJCAM0 adult speech dataset was used for
training, while the PFSTAR children's speech dataset was used for testing,
demonstrating the zero-shot capability of our method. Our approach achieved
state-of-the-art results across all keyword sets for children's speech.
Notably, the Wav2Vec2 model, particularly layer 22, performed the best,
delivering an ATWV score of 0.691, a MTWV score of 0.7003 and probability of
false alarm and probability of miss of 0.0164 and 0.0547 respectively, for a
set of 30 keywords. Furthermore, age-specific performance evaluation confirmed
the system's effectiveness across different age groups of children. To assess
the system's robustness against noise, additional experiments were conducted
using the best-performing layer of the best-performing Wav2Vec2 model. The
results demonstrated a significant improvement over traditional MFCC-based
baseline, emphasizing the potential of SSL embeddings even in noisy conditions.
To further generalize the KWS framework, the experiments were repeated for an
additional CMU dataset. Overall the results highlight the significant
contribution of SSL features in enhancing Zero-Shot KWS performance for
children's speech, effectively addressing the challenges associated with the
distinct characteristics of child speakers.

</details>
