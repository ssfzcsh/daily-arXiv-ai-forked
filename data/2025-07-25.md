<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 27]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.MS](#cs.MS) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations](https://arxiv.org/abs/2507.17930)
*Vahid Garousi,Zafar Jafarov*

Main category: cs.SE

TL;DR: 论文探讨AI如何辅助软件工程，提出过程模型和决策框架，帮助开发者权衡效率与输出质量。


<details>
  <summary>Details</summary>
Motivation: 研究开发者如何在实际任务中与AI工具互动，尤其是信任、优化或拒绝AI生成输出的决策过程。

Method: 基于实践者报告和三个行业的直接观察，提出过程模型和2D决策框架。

Result: 模型提供了结构化指导，支持更有效使用AI工具，促进人机协作。

Conclusion: 工作为AI在软件工程中的实际应用提供了结构化且轻量级的解决方案。

Abstract: Artificial Intelligence (AI) has the potential to transform Software
Engineering (SE) by enhancing productivity, efficiency, and decision support.
Tools like GitHub Copilot and ChatGPT have given rise to "vibe coding"-an
exploratory, prompt-driven development style. Yet, how software engineers
engage with these tools in daily tasks, especially in deciding whether to
trust, refine, or reject AI-generated outputs, remains underexplored. This
paper presents two complementary contributions. First, a pragmatic process
model capturing real-world AI-assisted SE activities, including prompt design,
inspection, fallback, and refinement. Second, a 2D decision framework that
could help developers reason about trade-offs between effort saved and output
quality. Grounded in practitioner reports and direct observations in three
industry settings across Turkiye and Azerbaijan, our work illustrates how
engineers navigate AI use with human oversight. These models offer structured,
lightweight guidance to support more deliberate and effective use of AI tools
in SE, contributing to ongoing discussions on practical human-AI collaboration.

</details>


### [2] [Use as Directed? A Comparison of Software Tools Intended to Check Rigor and Transparency of Published Work](https://arxiv.org/abs/2507.17991)
*Peter Eckmann,Adrian Barnett,Alexandra Bannach-Brown,Elisa Pilar Bascunan Atria,Guillaume Cabanac,Louise Delwen Owen Franzen,Małgorzata Anna Gazda,Kaitlyn Hair,James Howison,Halil Kilicoglu,Cyril Labbe,Sarah McCann,Vladislav Nachev,Martijn Roelandse,Maia Salholz-Hillel,Robert Schulz,Gerben ter Riet,Colby Vorland,Anita Bandrowski,Tracey Weissgerber*

Main category: cs.SE

TL;DR: 摘要探讨了科学报告中可重复性危机的原因，比较了11种自动化工具在9项严谨性标准上的表现，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提高科学报告的透明度和标准化，解决可重复性危机。

Method: 方法包括对11种自动化工具在9项严谨性标准上进行广泛比较。

Result: 结果显示某些标准下存在明显优胜工具，而其他标准下工具组合表现更优。

Conclusion: 结论为提出对工具开发者和利益相关者的建议，以改进严谨性和透明度检测工具。

Abstract: The causes of the reproducibility crisis include lack of standardization and
transparency in scientific reporting. Checklists such as ARRIVE and CONSORT
seek to improve transparency, but they are not always followed by authors and
peer review often fails to identify missing items. To address these issues,
there are several automated tools that have been designed to check different
rigor criteria. We have conducted a broad comparison of 11 automated tools
across 9 different rigor criteria from the ScreenIT group. We found some
criteria, including detecting open data, where the combination of tools showed
a clear winner, a tool which performed much better than other tools. In other
cases, including detection of inclusion and exclusion criteria, the combination
of tools exceeded the performance of any one tool. We also identified key areas
where tool developers should focus their effort to make their tool maximally
useful. We conclude with a set of insights and recommendations for stakeholders
in the development of rigor and transparency detection tools. The code and data
for the study is available at https://github.com/PeterEckmann1/tool-comparison.

</details>


### [3] [An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges](https://arxiv.org/abs/2507.18029)
*Xiang Echo Chen,Wenhan Zhu,Guoshuai Albert Shi,Michael W. Godfrey*

Main category: cs.SE

TL;DR: 研究探讨了生成式AI（GenAI）在开源游戏开发中的应用，通过GitHub讨论分析其使用模式、工具、任务及挑战，并与传统AI（TradAI）和非AI方法进行对比。


<details>
  <summary>Details</summary>
Motivation: 当前对GenAI在实际开发中的应用缺乏实证研究，尤其是在开源社区中。

Method: 通过构建开源游戏项目数据集，对GitHub问题进行分层抽样，采用开放卡片分类和主题分析，标注类型和内容。

Result: 对比分析了GenAI、TradAI和非AI在游戏开发中的使用模式、开发者关注点和集成实践。

Conclusion: 研究揭示了GenAI如何影响开源游戏开发的工作流程和痛点，为未来研究提供了基础。

Abstract: The growing capabilities of generative AI (GenAI) have begun to reshape how
games are designed and developed, offering new tools for content creation,
gameplay simulation, and design ideation. While prior research has explored
traditional uses of AI in games, such as controlling agents or generating
procedural content. There is limited empirical understanding of how GenAI is
adopted by developers in real-world contexts, especially within the open-source
community. This study aims to explore how GenAI technologies are discussed,
adopted, and integrated into open-source game development by analyzing issue
discussions on GitHub. We investigate the tools, tasks, and challenges
associated with GenAI by comparing GenAI-related issues to those involving
traditional AI (TradAI) and NonAI topics. Our goal is to uncover how GenAI
differs from other approaches in terms of usage patterns, developer concerns,
and integration practices. To address this objective, we construct a dataset of
open-source game repositories that discuss AI-related topics. We apply open
card sorting and thematic analysis to a stratified sample of GitHub issues,
labelling each by type and content. These annotations enable comparative
analysis across GenAI, TradAI, and NonAI groups, and provide insight into how
GenAI is shaping the workflows and pain points of open-source game developers.

</details>


### [4] [Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping](https://arxiv.org/abs/2507.18037)
*Sivana Hamer,Jacob Bowen,Md Nazmul Haque,Chris Madden,Laurie Williams*

Main category: cs.SE

TL;DR: 本文描述了MITRE ATT&CK攻击技术与P-SSCRM框架任务的映射，帮助软件组织识别任务如何缓解软件供应链攻击。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过映射技术帮助软件组织更好地理解和应对软件供应链攻击。

Method: 通过四种独立策略创建MITRE ATT&CK攻击技术与P-SSCRM任务的映射关系，并与10个其他框架进行对比。

Result: 提供了MITRE ATT&CK与其他政府和行业框架之间的映射关系，为组织提供更全面的风险管理工具。

Conclusion: 该映射为软件供应链风险管理提供了实用参考，增强了攻击缓解的能力。

Abstract: The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)
Attack Technique to Proactive Software Supply Chain Risk Management Framework
(P-SSCRM) Task mapping described in this document helps software organizations
to determine how different tasks mitigate the attack techniques of software
supply chain attacks. The mapping was created through four independent
strategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to
one or more tasks from the 10 frameworks, the mapping we provide is also a
mapping between MITRE ATT&CK and other prominent government and industry
frameworks.

</details>


### [5] [Factors Impacting Faculty Adoption of Project-Based Learning in Computing Education: a Survey](https://arxiv.org/abs/2507.18039)
*Ahmad D. Suleiman,Yiming Tang,Daqing Hou*

Main category: cs.SE

TL;DR: 该研究探讨了影响计算机教育工作者在软件工程和计算课程中采用项目制学习（PjBL）的因素，揭示了障碍与促进因素，并强调了系统性支持的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管项目制学习（PjBL）能提升学生的多项能力，但其采用率参差不齐，研究旨在探索障碍并找到促进成功采用的策略和资源。

Method: 采用混合方法，通过在线调查收集80名计算机教师的定量与定性数据，并分别进行统计分析和主题分析。

Result: 结果显示，PjBL虽受重视，但选择性采用现象普遍，障碍包括规划与管理困难、项目设计难度和缺乏制度支持；成功因素包括同行协作、专业发展和制度激励。

Conclusion: 研究强调需建立系统性支持结构，以帮助教师尝试和扩展PjBL实践。

Abstract: This research full paper investigates the factors influencing computing
educators' adoption of project-based learning (PjBL) in software engineering
and computing curricula. Recognized as a student-centered pedagogical approach,
PjBL has the potential to enhance student motivation, engagement, critical
thinking, collaboration, and problem-solving skills. Despite these benefits,
faculty adoption remains inconsistent due to challenges such as insufficient
institutional support, time constraints, limited training opportunities,
designing or sourcing projects, and aligning them with course objectives. This
research explores these barriers and investigates the strategies and resources
that facilitate a successful adoption. Using a mixed-methods approach, data
from 80 computing faculty were collected through an online survey comprising
closed-ended questions to quantify barriers, enablers, and resource needs,
along with an open-ended question to gather qualitative insights. Quantitative
data were analyzed using statistical methods, while qualitative responses
underwent thematic analysis. Results reveal that while PjBL is widely valued,
its adoption is often selective and impacted by challenges in planning and
managing the learning process, designing suitable projects, and a lack of
institutional support, such as time, funding, and teaching assistants. Faculty
are more likely to adopt or sustain PjBL when they have access to peer
collaboration, professional development, and institutional incentives. In
addition, sourcing projects from research, industry partnerships, and borrowing
from peers emerged as key facilitators for new projects. These findings
underscore the need for systemic support structures to empower faculty to
experiment with and scale PjBL practices.

</details>


### [6] [An Empirical Study of Complexity, Heterogeneity, and Compliance of GitHub Actions Workflows](https://arxiv.org/abs/2507.18062)
*Edward Abrokwah,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 该研究分析了GitHub Actions（GHA）工作流在开源项目中的结构、复杂性和合规性，旨在揭示其与最佳实践的差距，并为CI服务提供改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管GHA有官方文档和社区最佳实践，但缺乏对开源项目中实际工作流是否符合这些实践的实证研究。许多工作流可能过于复杂，未达到CI简单化目标。

Method: 通过分析Java、Python和C++仓库的大量GHA工作流数据，研究其复杂性、结构模式、合规性及跨语言差异。

Result: 预计发现对最佳实践的遵循情况，以及需要改进的领域。

Conclusion: 研究结果将强调CI文档中需要更清晰的指南和全面示例，对CI服务有重要启示。

Abstract: Continuous Integration (CI) has evolved from a tooling strategy to a
fundamental mindset in modern CI engineering. It enables teams to develop,
test, and deliver software rapidly and collaboratively. Among CI services,
GitHub Actions (GHA) has emerged as a dominant service due to its deep
integration with GitHub and a vast ecosystem of reusable workflow actions.
Although GHA provides official documentation and community-supported best
practices, there appears to be limited empirical understanding of how
open-source real-world CI workflows align with such practices. Many workflows
might be unnecessarily complex and not aligned with the simplicity goals of CI
practices. This study will investigate the structure, complexity,
heterogeneity, and compliance of GHA workflows in open-source software
repositories. Using a large dataset of GHA workflows from Java, Python, and C++
repositories, our goal is to (a) identify workflow complexities, (b) analyze
recurring and heterogeneous structuring patterns, (c) assess compliance with
GHA best practices, and (d) uncover differences in CI pipeline design across
programming languages. Our findings are expected to reveal both areas of strong
adherence to best practices and areas for improvement where needed. These
insights will also have implications for CI services, as they will highlight
the need for clearer guidelines and comprehensive examples in CI documentation.

</details>


### [7] [Identifier Name Similarities: An Exploratory Study](https://arxiv.org/abs/2507.18081)
*Carol Wong,Mai Abe,Silvia De Benedictis,Marissa Halim,Anthony Peruma*

Main category: cs.SE

TL;DR: 该研究探讨了标识符名称相似性对代码理解和协作的影响，并通过开发分类法初步分类了相似性的不同形式。


<details>
  <summary>Details</summary>
Motivation: 标识符名称是程序理解的核心，但名称相似性可能导致误解，增加认知负担，影响协作。

Method: 通过开发分类法，对软件项目中标识符名称的相似性形式进行分类。

Result: 提出了一个初步的分类法，用于分析名称相似性对代码理解、可维护性和协作的影响。

Conclusion: 该分类法为研究者提供了平台，未来可进一步扩展和细化。

Abstract: Identifier names, which comprise a significant portion of the codebase, are
the cornerstone of effective program comprehension. However, research has shown
that poorly chosen names can significantly increase cognitive load and hinder
collaboration. Even names that appear readable in isolation may lead to
misunderstandings in contexts when they closely resemble other names in either
structure or functionality. In this exploratory study, we present our
preliminary findings on the occurrence of identifier name similarity in
software projects through the development of a taxonomy that categorizes
different forms of identifier name similarity. We envision our initial taxonomy
providing researchers with a platform to analyze and evaluate the impact of
identifier name similarity on code comprehension, maintainability, and
collaboration among developers, while also allowing for further refinement and
expansion of the taxonomy.

</details>


### [8] [Understanding the Supply Chain and Risks of Large Language Model Applications](https://arxiv.org/abs/2507.18105)
*Yujie Ma,Lili Quan,Xiaofei Xie,Qiang Hu,Jiongchi Yu,Yao Zhang,Sen Chen*

Main category: cs.SE

TL;DR: 论文介绍了首个全面分析大型语言模型（LLM）供应链安全的数据集，揭示了深度嵌套的依赖关系和供应链中的严重漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着LLM系统的广泛应用，理解其复杂供应链的风险变得至关重要，但目前缺乏系统性研究的基准。

Method: 收集了3,859个真实世界的LLM应用，分析了109,211个模型、2,474个数据集和9,862个库的依赖关系，并从公开漏洞数据库中提取了1,555个风险问题。

Result: 研究发现LLM应用中存在深度嵌套的依赖关系和供应链中的显著漏洞。

Conclusion: 论文提出了实践建议，以帮助研究人员和开发者构建更安全、更可信的LLM系统。

Abstract: The rise of Large Language Models (LLMs) has led to the widespread deployment
of LLM-based systems across diverse domains. As these systems proliferate,
understanding the risks associated with their complex supply chains is
increasingly important. LLM-based systems are not standalone as they rely on
interconnected supply chains involving pretrained models, third-party
libraries, datasets, and infrastructure. Yet, most risk assessments narrowly
focus on model or data level, overlooking broader supply chain vulnerabilities.
While recent studies have begun to address LLM supply chain risks, there
remains a lack of benchmarks for systematic research.
  To address this gap, we introduce the first comprehensive dataset for
analyzing and benchmarking LLM supply chain security. We collect 3,859
real-world LLM applications and perform interdependency analysis, identifying
109,211 models, 2,474 datasets, and 9,862 libraries. We extract model
fine-tuning paths, dataset reuse, and library reliance, mapping the ecosystem's
structure. To evaluate security, we gather 1,555 risk-related issues-50 for
applications, 325 for models, 18 for datasets, and 1,229 for libraries from
public vulnerability databases.
  Using this dataset, we empirically analyze component dependencies and risks.
Our findings reveal deeply nested dependencies in LLM applications and
significant vulnerabilities across the supply chain, underscoring the need for
comprehensive security analysis. We conclude with practical recommendations to
guide researchers and developers toward safer, more trustworthy LLM-enabled
systems.

</details>


### [9] [NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition](https://arxiv.org/abs/2507.18130)
*Le Deng,Zhonghao Jiang,Jialun Cao,Michael Pradel,Zhongxin Liu*

Main category: cs.SE

TL;DR: 本文介绍了NoCode-bench基准，用于评估大型语言模型在自然语言驱动的无代码开发中的表现，发现当前模型在任务成功率上仍有较大挑战。


<details>
  <summary>Details</summary>
Motivation: 自然语言驱动的无代码开发能提高生产力和普及开发，但现有大型语言模型在此领域的表现尚未成熟，需要一个标准化的评估工具。

Method: 作者构建了NoCode-bench基准，包含634个任务，涉及10个项目和114k代码更改，结合文档更新与代码实现，并通过开发者编写的测试用例验证。

Result: 实验显示，最佳大型语言模型的任务成功率仅为15.79%，且在跨文件编辑、代码库理解和工具调用方面表现不足。

Conclusion: 研究表明大型语言模型尚未完全适用于自然语言驱动的无代码开发，但NoCode-bench为未来的改进奠定了基础。

Abstract: Natural language-driven no-code development allows users to specify software
functionality using natural language (NL) instead of editing source code,
promising increased productivity and democratized development. Large language
models (LLMs) show potential in enabling this paradigm. In this context,
software documentation acts as an NL specification for functionality. This work
introduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world
NL-driven feature addition tasks, consisting of 634 tasks across 10 projects
and 114k code changes. Each task pairs documentation updates with corresponding
code implementations, validated by developer-written test cases. A subset of
114 high-quality, human-verified instances, NoCode-bench Verified, ensures
reliable evaluation. Our experiments reveal that, despite high token usage, the
best LLMs achieve a task success rate of only 15.79%, highlighting challenges
in cross-file editing, codebase understanding, and tool calling. These findings
indicate that LLMs are not yet ready for fully NL-driven no-code development.
NoCode-bench lays the foundation for future advances in this area.

</details>


### [10] [SMECS: A Software Metadata Extraction and Curation Software](https://arxiv.org/abs/2507.18159)
*Stephan Ferenz,Aida Jafarbigloo,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: SMECS是一种软件元数据提取与整理工具，通过从GitHub等在线仓库提取元数据并提供交互界面简化元数据创建，支持FAIR原则。


<details>
  <summary>Details</summary>
Motivation: 研究人员和软件工程师创建高质量元数据资源消耗大，SMECS旨在解决这一问题。

Method: 开发SMECS工具，从在线仓库提取元数据并提供用户友好的交互界面进行整理。

Result: 通过可用性实验验证SMECS提供良好的用户体验。

Conclusion: SMECS简化了元数据创建，支持研究软件的FAIR化。

Abstract: Metadata play a crucial role in adopting the FAIR principles for research
software and enables findability and reusability. However, creating
high-quality metadata can be resource-intensive for researchers and research
software engineers. To address this challenge, we developed the Software
Metadata Extraction and Curation Software (SMECS) which integrates the
extraction of metadata from existing sources together with a user-friendly
interface for metadata curation. SMECS extracts metadata from online
repositories such as GitHub and presents it to researchers through an
interactive interface for further curation and export as a CodeMeta file. The
usability of SMECS was evaluated through usability experiments which confirmed
that SMECS provides a satisfactory user experience. SMECS supports the
FAIRification of research software by simplifying metadata creation.

</details>


### [11] [GenAI for Automotive Software Development: From Requirements to Wheels](https://arxiv.org/abs/2507.18223)
*Nenad Petrovic,Fengjunjie Pan,Vahid Zolfaghari,Krzysztof Lebioda,Andre Schamschurko,Alois Knoll*

Main category: cs.SE

TL;DR: 论文提出了一种基于GenAI的自动化汽车软件开发方法，重点针对自动驾驶和高级驾驶辅助系统（ADAS）。通过输入需求，生成仿真环境的测试场景代码和ADAS功能的实现代码，并结合模型驱动工程（MDE）检查需求一致性。利用大语言模型（LLMs）进行需求总结、测试场景生成和代码生成，同时采用检索增强生成（RAG）从法规文档中提升测试场景生成。目标是缩短合规性周期和开发测试时间。


<details>
  <summary>Details</summary>
Motivation: 为了缩短汽车软件开发中的合规性周期、开发时间和测试时间，尤其是在自动驾驶和ADAS功能的开发中，传统方法效率较低。论文提出利用GenAI和LLMs来优化这一流程。

Method: 结合模型驱动工程（MDE）和大语言模型（LLMs），从输入的需求生成测试场景代码和ADAS功能实现代码。使用LLMs进行需求总结、测试场景生成、仿真代码（Python）和目标平台代码（C++）生成，并通过RAG从法规文档中增强测试场景生成。

Result: 提出的方法缩短了合规性和重新设计周期，同时减少了ADAS相关功能的开发和测试时间。

Conclusion: GenAI和LLMs的结合为汽车软件的自动化开发提供了高效解决方案，特别是在ADAS和自动驾驶领域，显著提升了开发效率和合规性。

Abstract: This paper introduces a GenAI-empowered approach to automated development of
automotive software, with emphasis on autonomous and Advanced Driver Assistance
Systems (ADAS) capabilities. The process starts with requirements as input,
while the main generated outputs are test scenario code for simulation
environment, together with implementation of desired ADAS capabilities
targeting hardware platform of the vehicle connected to testbench. Moreover, we
introduce additional steps for requirements consistency checking leveraging
Model-Driven Engineering (MDE). In the proposed workflow, Large Language Models
(LLMs) are used for model-based summarization of requirements (Ecore metamodel,
XMI model instance and OCL constraint creation), test scenario generation,
simulation code (Python) and target platform code generation (C++).
Additionally, Retrieval Augmented Generation (RAG) is adopted to enhance test
scenario generation from autonomous driving regulations-related documents. Our
approach aims shorter compliance and re-engineering cycles, as well as reduced
development and testing time when it comes to ADAS-related capabilities.

</details>


### [12] [An Empirical Study on Embodied Artificial Intelligence Robot (EAIR) Software Bugs](https://arxiv.org/abs/2507.18267)
*Zeqin Liao,Zibin Zheng,Peifan Reng,Henglong Liang,Zixu Gao,Zhixiang Chen,Wei Li,Yuhong Nan*

Main category: cs.SE

TL;DR: 该研究首次对885个EAIR系统错误进行了系统性分析，揭示了18种根本原因、15种症状和13个受影响模块，并提出了8种EAIR特有症状和8种特有原因。


<details>
  <summary>Details</summary>
Motivation: 由于对EAIR系统错误的普遍认识不足，阻碍了相关技术和实践的发展，因此需要深入研究。

Method: 通过分析80个EAIR系统项目中的885个错误，研究其症状、根本原因和模块分布。

Result: 发现8种EAIR特有症状和8种特有原因，主要集中在AI代理的推理和决策问题，并构建了错误原因与模块的映射关系。

Conclusion: 研究结果有助于指导未来EAIR系统错误的修复和预防，为研究者提供了诊断工具。

Abstract: Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly
evolving technological domain. Ensuring their program correctness is
fundamental to their successful deployment. However, a general and in-depth
understanding of EAIR system bugs remains lacking, which hinders the
development of practices and techniques to tackle EAIR system bugs.
  To bridge this gap, we conducted the first systematic study of 885 EAIR
system bugs collected from 80 EAIR system projects to investigate their
symptoms, underlying causes, and module distribution. Our analysis takes
considerable effort, which classifies these bugs into 18 underlying causes, 15
distinct symptoms, and identifies 13 affected modules. It reveals several new
interesting findings and implications which help shed light on future research
on tackling or repairing EAIR system bugs. First, among the 15 identified
symptoms, our findings highlight 8 symptoms specific to EAIR systems, which is
characterized by severe functional failures and potential physical hazards.
Second, within the 18 underlying causes, we define 8 EAIR-specific causes, the
majority of which stem from the intricate issues of AI- agent reasoning and
decision making. Finally, to facilitate precise and efficient bug prediction,
detection, and repair, we constructed a mapping between underlying causes and
the modules in which they most frequently occur, which enables researchers to
focus diagnostic efforts on the modules most susceptible to specific bug types.

</details>


### [13] [Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling](https://arxiv.org/abs/2507.18289)
*Yan Li,Wenzhang Yang,Yuekun Wang,Jian Gao,Shaohua Wang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: Scheduzz是一种基于LLM的自动化库模糊测试技术，通过提取API组合约束和双调度框架优化资源利用，显著提高了覆盖率并减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试技术生成模糊驱动质量低，浪费计算资源，Scheduzz旨在解决这些问题。

Method: 利用LLM理解库的合理用法并提取API组合约束；采用双调度框架管理API组合和模糊驱动。

Result: 在33个真实库中测试，Scheduzz覆盖率比现有技术高1.5-1.89倍，并发现33个未知漏洞。

Conclusion: Scheduzz高效、可靠，优于现有技术，适用于大规模库模糊测试。

Abstract: Fuzzing a library requires experts to understand the library usage well and
craft high-quality fuzz drivers, which is tricky and tedious. Therefore, many
techniques have been proposed to automatically generate fuzz drivers. However,
they fail to generate rational fuzz drivers due to the lack of adherence to
proper library usage conventions, such as ensuring a resource is closed after
being opened. To make things worse, existing library fuzzing techniques
unconditionally execute each driver, resulting in numerous irrational drivers
that waste computational resources while contributing little coverage and
generating false positive bug reports.
  To tackle these challenges, we propose a novel automatic library fuzzing
technique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs
to understand rational usage of libraries and extract API combination
constraints. To optimize computational resource utilization, a dual scheduling
framework is implemented to efficiently manage API combinations and fuzz
drivers. The framework models driver generation and the corresponding fuzzing
campaign as an online optimization problem. Within the scheduling loop,
multiple API combinations are selected to generate fuzz drivers, while
simultaneously, various optimized fuzz drivers are scheduled for execution or
suspension.
  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared
to baseline approaches, Scheduzz significantly reduces computational overhead
and outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and
1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,
Promptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,
Scheduzz discovered 33 previously unknown bugs in these well-tested libraries,
3 of which have been assigned CVEs.

</details>


### [14] [YATE: The Role of Test Repair in LLM-Based Unit Test Generation](https://arxiv.org/abs/2507.18316)
*Michael Konstantinou,Renzo Degiovanni,Jie M. Zhang,Mark Harman,Mike Papadakis*

Main category: cs.SE

TL;DR: 提出一种名为YATE的技术，通过静态分析和重新提示修复语言模型生成的错误测试，显著提升了测试覆盖率和突变杀死率。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成的测试虽有效但常含错误，修复这些错误测试可提升测试价值并为生成更多测试提供种子。

Method: 结合基于规则的静态分析和重新提示技术，修复不正确测试。

Result: YATE在6个开源项目中平均提升32.06%的行覆盖率和21.77%的突变杀死率，优于其他LLM方法。

Conclusion: YATE技术能以较低成本显著提升测试效果，优于现有方法。

Abstract: Recent advances in automated test generation utilises language models to
produce unit tests. While effective, language models tend to generate many
incorrect tests with respect to both syntax and semantics. Although such
incorrect tests can be easily detected and discarded, they constitute a "missed
opportunity" -- if fixed, they are often valuable as they directly add testing
value (they effectively target the underlying program logic to be tested) and
indirectly form good seeds for generating additional tests. To this end, we
propose a simple technique for repairing some of these incorrect tests through
a combination of rule-based static analysis and re-prompting. We evaluate this
simple approach, named YATE, on a set of 6 open-source projects and show that
it can effectively produce tests that cover on average 32.06% more lines and
kill 21.77% more mutants than a plain LLM-based method. We also compare YATE
with four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and
COVERUP and show that it produces tests that cover substantially more code.
YATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%
more mutants at a comparable cost (number of calls to LLMs).

</details>


### [15] [Gotta catch 'em all! Towards File Localisation from Issues at Large](https://arxiv.org/abs/2507.18319)
*Jesse Maarleveld,Jiapan Guo,Daniel Feitosa*

Main category: cs.SE

TL;DR: 本文提出了一种适用于所有类型问题的文件定位数据管道和基线评估，发现基于bug特定启发式的方法在通用问题上表现不佳，需研究通用模型。


<details>
  <summary>Details</summary>
Motivation: 研究目标是从所有类型的问题中定位需要修改的文件，而非仅限于bug，克服现有研究的局限性。

Method: 提出了一种数据管道，用于创建适用于任意分支和合并实践的issue文件定位数据集，并使用传统信息检索方法进行基线评估。

Result: 结果表明，基于bug启发式的方法在通用问题上表现不佳，不同问题类型间存在显著性能差异，且许多结果依赖于具体项目。

Conclusion: 需要开发通用模型，并针对项目特定特性调整方法。

Abstract: Bug localisation, the study of developing methods to localise the files
requiring changes to resolve bugs, has been researched for a long time to
develop methods capable of saving developers' time. Recently, researchers are
starting to consider issues outside of bugs. Nevertheless, most existing
research into file localisation from issues focusses on bugs or uses other
selection methods to ensure only certain types of issues are considered as part
of the focus of the work. Our goal is to work on all issues at large, without
any specific selection.
  In this work, we provide a data pipeline for the creation of issue file
localisation datasets, capable of dealing with arbitrary branching and merging
practices. We provide a baseline performance evaluation for the file
localisation problem using traditional information retrieval approaches.
Finally, we use statistical analysis to investigate the influence of biases
known in the bug localisation community on our dataset.
  Our results show that methods designed using bug-specific heuristics perform
poorly on general issue types, indicating a need for research into general
purpose models. Furthermore, we find that there are small, but statistically
significant differences in performance between different issue types. Finally,
we find that the presence of identifiers have a small effect on performance for
most issue types. Many results are project-dependent, encouraging the
development of methods which can be tuned to project-specific characteristics.

</details>


### [16] [FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping](https://arxiv.org/abs/2507.18339)
*Nils Bosbach,Meik Schmidt,Lukas Jünger,Matthias Berthold,Rainer Leupers*

Main category: cs.SE

TL;DR: 论文提出了一种新框架，通过FMI标准控制SystemC虚拟平台，实现未修改软件在虚拟环境中的测试，并接收外部工具的真实数据输入。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂度增加，测试和虚拟原型需求增长，但SystemC缺乏原生FMI支持，限制了其在协同仿真中的集成。

Method: 提出一个框架，利用FMI连接SystemC虚拟平台，并通过案例研究展示如何从外部工具获取数据。

Result: 实现了目标软件在虚拟平台中运行并接收真实环境数据，支持软件测试和验证，可提前完成认证。

Conclusion: 该框架扩展了SystemC在协同仿真中的应用，提高了测试效率并加速认证流程。

Abstract: As systems become more complex, the demand for thorough testing and virtual
prototyping grows. To simulate whole systems, multiple tools are usually needed
to cover different parts. These parts include the hardware of a system and the
environment with which the system interacts. The Functional Mock-up Interface
(FMI) standard for co-simulation can be used to connect these tools.
  The control part of modern systems is usually a computing unit, such as a
System-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software
from a connected memory and interacts with peripherals. To develop software
without requiring access to physical hardware, full-system simulators, the
so-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized
framework for VP development is SystemC TLM. SystemC provides interfaces and
concepts that enable modular design and model exchange. However, SystemC lacks
native FMI support, which limits the integration into broader co-simulation
environments.
  This paper presents a novel framework to control and interact with
SystemC-based VPs using the FMI. We present a case study showing how a
simulated temperature sensor in a SystemC simulation can obtain temperature
values from an external tool via FMI. This approach allows the unmodified
target software to run on the VP and receive realistic environmental input data
such as temperature, velocity, or acceleration values from other tools. Thus,
extensive software testing and verification is enabled. By having tests ready
and the software pre-tested using a VP once the physical hardware is available,
certifications like ISO 26262 can be done earlier.

</details>


### [17] [Automated Code Review Using Large Language Models with Symbolic Reasoning](https://arxiv.org/abs/2507.18476)
*Busra Icoz,Goksel Biricik*

Main category: cs.SE

TL;DR: 该论文提出了一种结合符号推理和大语言模型（LLM）的混合方法，以自动化代码审查过程，提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 手动代码审查主观性强且耗时，自动化是理想选择，但现有LLM缺乏逻辑推理能力。

Method: 采用符号推理技术结合LLM，并在CodexGlue数据集上测试了多种模型（如CodeT5、CodeBERT等）。

Result: 实验证明该混合方法显著提升了自动化代码审查的准确性和效率。

Conclusion: 结合符号推理与LLM的混合方法是自动化代码审查的有效途径。

Abstract: Code review is one of the key processes in the software development lifecycle
and is essential to maintain code quality. However, manual code review is
subjective and time consuming. Given its rule-based nature, code review is well
suited for automation. In recent years, significant efforts have been made to
automate this process with the help of artificial intelligence. Recent
developments in Large Language Models (LLMs) have also emerged as a promising
tool in this area, but these models often lack the logical reasoning
capabilities needed to fully understand and evaluate code. To overcome this
limitation, this study proposes a hybrid approach that integrates symbolic
reasoning techniques with LLMs to automate the code review process. We tested
our approach using the CodexGlue dataset, comparing several models, including
CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining
symbolic reasoning and prompting techniques with LLMs. Our results show that
this approach improves the accuracy and efficiency of automated code review.

</details>


### [18] [A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat](https://arxiv.org/abs/2507.18515)
*Zezhou Yang,Ting Peng,Cuiyun Gao,Chaozheng Wang,Hailiang Huang,Yuetang Deng*

Main category: cs.SE

TL;DR: 研究了检索增强生成（RAG）方法在工业级闭源代码库（如微信）中的作用，发现相似性RAG效果更佳，且结合词法和语义检索技术效果最优。


<details>
  <summary>Details</summary>
Motivation: 探索检索增强生成（RAG）在闭源代码库中的性能，填补现有研究仅关注开源代码库的空白。

Method: 使用26个开源LLM（参数从0.5B到671B）测试两种RAG方法（基于标识符和基于相似性），并结合不同检索技术分析性能。

Result: 相似性RAG表现更好，BM25（词法）和GTE-Qwen（语义）技术效果显著，两者结合最优。开发者也验证了RAG的实际效用。

Conclusion: RAG在闭源代码库中有效，结合多种检索技术可提升性能，适合实际开发环境。

Abstract: Code completion, a crucial task in software engineering that enhances
developer productivity, has seen substantial improvements with the rapid
advancement of large language models (LLMs). In recent years,
retrieval-augmented generation (RAG) has emerged as a promising method to
enhance the code completion capabilities of LLMs, which leverages relevant
context from codebases without requiring model retraining. While existing
studies have demonstrated the effectiveness of RAG on public repositories and
benchmarks, the potential distribution shift between open-source and
closed-source codebases presents unique challenges that remain unexplored. To
mitigate the gap, we conduct an empirical study to investigate the performance
of widely-used RAG methods for code completion in the industrial-scale codebase
of WeChat, one of the largest proprietary software systems. Specifically, we
extensively explore two main types of RAG methods, namely identifier-based RAG
and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B
parameters. For a more comprehensive analysis, we employ different retrieval
techniques for similarity-based RAG, including lexical and semantic retrieval.
Based on 1,669 internal repositories, we achieve several key findings: (1) both
RAG methods demonstrate effectiveness in closed-source repositories, with
similarity-based RAG showing superior performance, (2) the effectiveness of
similarity-based RAG improves with more advanced retrieval techniques, where
BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior
performance, and (3) the combination of lexical and semantic retrieval
techniques yields optimal results, demonstrating complementary strengths.
Furthermore, we conduct a developer survey to validate the practical utility of
RAG methods in real-world development environments.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Higher-Order Behavioural Conformances via Fibrations](https://arxiv.org/abs/2507.18509)
*Henning Urbat*

Main category: cs.PL

TL;DR: 提出了一种统一的范畴化Howe方法，用于验证高阶语言中的行为一致性，适用于概率等高阶语言。


<details>
  <summary>Details</summary>
Motivation: 由于定量特征语言的兴起，需要扩展共归纳方法以支持更精细的行为一致性，如行为距离。

Method: 使用抽象的范畴化方法（AHOS）建模高阶语言，并通过纤维化建模行为一致性。

Result: 证明了在自然条件下，行为一致性的最大（双）一致性形成同余关系。

Conclusion: 该方法适用于概率高阶语言，验证了共似性和行为伪度量的一致性。

Abstract: Coinduction is a widely used technique for establishing behavioural
equivalence of programs in higher-order languages. In recent years, the rise of
languages with quantitative (e.g.~probabilistic) features has led to extensions
of coinductive methods to more refined types of behavioural conformances, most
notably notions of behavioural distance. To guarantee soundness of coinductive
reasoning, one needs to show that the behavioural conformance at hand forms a
program congruence, i.e. it is suitably compatible with the operations of the
language. This is usually achieved by a complex proof technique known as
\emph{Howe's method}, which needs to be carefully adapted to both the specific
language and the targeted notion of behavioural conformance. We develop a
uniform categorical approach to Howe's method that features two orthogonal
dimensions of abstraction: (1) the underlying higher-order language is modelled
by an \emph{abstract higher-order specification} (AHOS), a novel and very
general categorical account of operational semantics, and (2) notions of
behavioural conformance (such as relations or metrics) are modelled via
fibrations over the base category of an AHOS. Our main result is a fundamental
congruence theorem at this level of generality: Under natural conditions on the
categorical ingredients and the operational rules of a language modelled by an
AHOS, the greatest behavioural (bi)conformance on its operational model forms a
congruence. We illustrate our theory by deriving congruence of bisimilarity and
behavioural pseudometrics for probabilistic higher-order languages.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Frame-Based Zero-Shot Semantic Channel Equalization for AI-Native Communications](https://arxiv.org/abs/2507.17835)
*Simone Fiorellino,Claudio Battiloro,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.NI

TL;DR: 论文提出了一种零射、基于框架的语义信道均衡器（PFE），用于对齐异构编码器的潜在空间，无需重新训练系统，同时优化通信、计算和学习资源。


<details>
  <summary>Details</summary>
Motivation: 在AI原生无线网络中，异构深度神经网络编码器之间的潜在空间不匹配会导致语义信道噪声，降低系统性能。

Method: 提出了Parseval Frame Equalizer（PFE），通过动态信号压缩和扩展对齐潜在空间，并引入动态优化策略协调资源。

Result: 实验证明该方法在多样化且时变的网络条件下能保持语义一致性，满足延迟和准确性约束。

Conclusion: PFE有效解决了潜在空间不匹配问题，提升了多代理语义通信的性能和资源效率。

Abstract: In future AI-native wireless networks, the presence of mismatches between the
latent spaces of independently designed and trained deep neural network (DNN)
encoders may impede mutual understanding due to the emergence of semantic
channel noise. This undermines the receiver's ability to interpret transmitted
representations, thereby reducing overall system performance. To address this
issue, we propose the Parseval Frame Equalizer (PFE), a zero-shot, frame-based
semantic channel equalizer that aligns latent spaces of heterogeneous encoders
without requiring system retraining. PFE enables dynamic signal compression and
expansion, mitigating semantic noise while preserving performance on downstream
tasks. Building on this capability, we introduce a dynamic optimization
strategy that coordinates communication, computation, and learning resources to
balance energy consumption, end-to-end (E2E) latency, and task performance in
multi-agent semantic communication scenarios. Extensive simulations confirm the
effectiveness of our approach in maintaining semantic consistency and meeting
long-term constraints on latency and accuracy under diverse and time-varying
network conditions.

</details>


### [21] [ARCADE: A RAN Diagnosis Methodology in a Hybrid AI Environment for 6G Networks](https://arxiv.org/abs/2507.17861)
*Daniel Ricardo Cunha Oliveira,Rodrigo Moreira,Flávio de Oliveira Silva*

Main category: cs.NI

TL;DR: 提出了ARCADE方法，用于检测和评估蜂窝接入网络中的异常，并展示了如何通过混合网络分析架构在6G中更广泛地应用AI。


<details>
  <summary>Details</summary>
Motivation: 为了解决5G中尚未充分自动化的网络段问题，并探索AI在6G网络中的更广泛应用。

Method: 介绍了ARCADE方法，结合混合网络分析架构，用于异常检测和诊断。

Result: ARCADE成功展示了在6G网络中如何增强AI应用的实例。

Conclusion: ARCADE为6G网络中更全面的AI应用提供了可行方案。

Abstract: Artificial Intelligence (AI) plays a key role in developing 6G networks.
While current specifications already include Network Data Analytics Function
(NWDAF) as a network element responsible for providing information about the
core, a more comprehensive approach will be needed to enable automation of
network segments that are not yet fully explored in the context of 5G. In this
paper, we present Automated Radio Coverage Anomalies Detection and Evaluation
(ARCADE), a methodology for identifying and diagnosing anomalies in the
cellular access network. Furthermore, we demonstrate how a hybrid architecture
of network analytics functions in the evolution toward 6G can enhance the
application of AI in a broader network context, using ARCADE as a practical
example of this approach.

</details>


### [22] [Talk with the Things: Integrating LLMs into IoT Networks](https://arxiv.org/abs/2507.17865)
*Alakesh Kalita*

Main category: cs.NI

TL;DR: 提出了一个边缘计算框架，将大型语言模型（LLM）集成到物联网（IoT）架构中，实现自然语言控制、上下文感知决策和自动化。


<details>
  <summary>Details</summary>
Motivation: 利用LLM和IoT的融合，构建智能、响应迅速且用户友好的系统。

Method: 采用模块化、轻量级的检索增强生成（RAG）LLM，部署在边缘设备上，本地处理用户命令和传感器数据。

Result: 通过智能家居原型验证，展示模型精度与推理时间的权衡。

Conclusion: 讨论了LLM-IoT系统的潜在应用及关键挑战。

Abstract: The convergence of Large Language Models (LLMs) and Internet of Things (IoT)
networks open new opportunities for building intelligent, responsive, and
user-friendly systems. This work presents an edge-centric framework that
integrates LLMs into IoT architectures to enable natural language-based
control, context-aware decision-making, and enhanced automation. The proposed
modular and lightweight Retrieval Augmented Generation (RAG)-based LLMs are
deployed on edge computing devices connected to IoT gateways, enabling local
processing of user commands and sensor data for reduced latency, improved
privacy, and enhanced inference quality. We validate the framework through a
smart home prototype using LLaMA 3 and Gemma 2B models for controlling smart
devices. Experimental results highlight the trade-offs between model accuracy
and inference time with respect to models size. At last, we also discuss the
potential applications that can use LLM-based IoT systems, and a few key
challenges associated with such systems.

</details>


### [23] [Enabling Scalability in Asynchronous and Bidirectional Communication in LPWAN](https://arxiv.org/abs/2507.17905)
*Mahbubur Rahman*

Main category: cs.NI

TL;DR: 该论文通过改进LPWAN技术SNOW，实现了大规模并行数据传输，提升可扩展性约9倍。


<details>
  <summary>Details</summary>
Motivation: 解决LPWAN在物联网和CPS应用中大规模传感器数据传输的低延迟和高效率挑战。

Method: 利用D-OFDM子载波和Gold码PN序列，实现异步传感器的并行数据传输。

Result: 实验表明，SNOW的可扩展性提升约9倍，同时保持低能耗和及时数据传输。

Conclusion: 该技术为需要高可扩展性和低延迟的物联网和CPS应用提供了可行方案。

Abstract: LPWANs have become ubiquitous due to their ability to connect sensors over
large geographic areas in a single hop. It is, however, very challenging to
achieve massive scalability in LPWANs, where numerous sensors can transmit data
efficiently and with low latency, which emerging IoT and CPS applications may
require. In this paper, we address the above challenges by significantly
advancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal
frequency division multiplexing, D-OFDM, subcarriers to enable parallel
reception of data to a BS from multiple asynchronous sensors, each using a
different subcarrier. In this paper, we achieve massive scalability in SNOW by
enabling the BS to decode concurrent data from numerous asynchronous sensors on
the same subcarrier while parallelly decoding from other subcarriers as well.
Additionally, we enable numerous asynchronous sensors to receive distinct data
from the BS on the same subcarrier while other sensors also receive data
parallelly on other subcarriers. To do this, we develop a set of Gold
code-based pseudorandom noise or PN sequences that are mutually non-interfering
within and across the subcarriers. Each sensor uses its PN sequence from the
set for encoding or decoding data on its subcarriers, enabling massive
concurrency. Our evaluation results demonstrate that we can achieve
approximately 9x more scalability in SNOW while being timely in data collection
at the BS and energy efficient at the sensors. This may enable emerging IoT and
CPS applications requiring tens of thousands of sensors with longer battery
life and making data-driven, time-sensitive decisions.

</details>


### [24] [Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of Information Optimization in Vehicular Networks](https://arxiv.org/abs/2507.18328)
*Xiao Xu,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 论文研究了5G NR V2I Mode 2下车联网中的公平接入问题和信息年龄（AoI），通过联合优化框架平衡公平性和AoI，确保数据的及时性和相关性。


<details>
  <summary>Details</summary>
Motivation: 车辆在相邻车道行驶时速度不同，导致RSU驻留时间和通信时长差异，引起网络资源访问不公平，影响驾驶安全。

Method: 定义了公平性指数，利用SHS建模AoI，并自适应调整SPS选择窗口；采用LLM-Based MOEA/D算法解决优化问题。

Result: 仿真结果表明，所提方案能有效平衡公平接入和最小化AoI。

Conclusion: 联合优化框架解决了5G V2I Mode 2下车联网中的公平性和数据新鲜性问题。

Abstract: In this paper, we consider the fair access problem and the Age of Information
(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in
vehicular networks. Specifically, vehicles follow Mode 2 to communicate with
Roadside Units (RSUs) to obtain accurate data for driving
assistance.Nevertheless, vehicles often have different velocity when they are
moving in adjacent lanes, leading to difference in RSU dwelltime and
communication duration. This results in unfair access to network resources,
potentially influencing driving safety. To ensure the freshness of received
data, the AoI should be analyzed. Mode 2 introduces a novel preemption
mechanism, necessitating simultaneous optimization of fair access and AoI to
guarantee timely and relevant data delivery. We propose a joint optimization
framework for vehicular network, defining a fairness index and employing
Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By
adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)
in Mode 2, we address the optimization of fairness and AoI. We apply a large
language model (LLM)-Based Multi-objective Evolutionary Algorithm Based on
Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate
the effectiveness of our scheme in balancing fair access and minimizing AoI.

</details>


### [25] [Improving Wi-Fi 8 Latency with Coordinated Spatial Reuse](https://arxiv.org/abs/2507.18480)
*David Nunez,Francesc Wilhelmi,Lorenzo Galati-Giordano,Giovanni Geraci,Boris Bellalta*

Main category: cs.NI

TL;DR: 该论文探讨了协调空间复用(Co-SR)在Wi-Fi 8网络中的性能，通过模拟实验展示了其显著降低延迟的效果。


<details>
  <summary>Details</summary>
Motivation: 满足云游戏、XR和视频流等新兴应用对高吞吐量、低延迟和高可靠性的需求。

Method: 提出了一种与Wi-Fi 8标准化工作一致的Co-SR实现，并通过Wi-Fi模拟器评估其性能。

Result: 在由四个AP组成的WLAN中，Co-SR相比DCF将延迟降低了31%至95%。

Conclusion: Co-SR能有效优化频谱资源利用，提升密集环境中的网络性能。

Abstract: IEEE 802.11 networks continuously adapt to meet the stringent requirements of
emerging applications like cloud gaming, eXtended Reality (XR), and video
streaming services, which require high throughput, low latency, and high
reliability. To address these challenges, Coordinated Spatial Reuse (Co-SR) can
potentially contribute to optimizing spectrum resource utilization. This
mechanism is expected to enable simultaneous transmissions, thereby boosting
spectral efficiency in dense environments and increasing the overall network
performance. In this paper, we shed light on the performance of Co-SR for Wi-Fi
8 networks. For that, we propose an implementation of Co-SR aligned with
ongoing Wi-Fi 8 standardization efforts. The evaluation is done on a Wi-Fi
simulator, which allows us to study the performance of the proposed Co-SR
mechanisms in relevant scenarios. The results obtained in a Wireless Local Area
Network (WLAN) consisting of four APs show delay reduction with Co-SR ranging
from 31% to 95% when compared to Distributed Coordination Function (DCF).

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [26] [Program Logics via Distributive Monoidal Categories](https://arxiv.org/abs/2507.18238)
*Filippo Bonchi,Elena Di Lavore,Mario Román,Sam Staton*

Main category: cs.LO

TL;DR: 从公理化范畴论角度，通过统一追踪分布式拷贝-丢弃范畴，推导出多种程序逻辑（如正确性、不正确性和关系Hoare逻辑）。


<details>
  <summary>Details</summary>
Motivation: 探索程序逻辑的范畴论基础，提供统一的推导框架。

Method: 引入命令式多范畴的内部语言，并基于此推导Dijkstra守卫命令语言的组合子。

Result: 成功导出程序逻辑的规则。

Conclusion: 范畴论方法为程序逻辑提供了统一且严谨的基础。

Abstract: We derive multiple program logics, including correctness, incorrectness, and
relational Hoare logic, from the axioms of imperative categories: uniformly
traced distributive copy-discard categories. We introduce an internal language
for imperative multicategories, on top of which we derive combinators for an
adaptation of Dijkstra's guarded command language. Rules of program logics are
derived from this internal language.

</details>


### [27] [Resourceful Traces for Commuting Processes](https://arxiv.org/abs/2507.18246)
*Matthew Earnshaw,Chad Nester,Mario Román*

Main category: cs.LO

TL;DR: 该论文提出了一种新的效果范畴（effectful categories）展示方法，从输入的指定类型到输出的指定类型的转换角度看待Mazurkiewicz迹，并以此构建了图形演算。


<details>
  <summary>Details</summary>
Motivation: 探索效果范畴的更直观展示方式，以及如何通过这些展示捕捉计算中的副作用行为。

Method: 将Mazurkiewicz迹视为输入到输出的转换，而非简单的原子操作，从而构建新的展示方法。

Result: 提出了效果范畴的图形演算，并展示了如何用于构建自由效果范畴的交换张量积。

Conclusion: 新的展示方法不仅适用于效果范畴的语义分析，还能支持资源交换等实际应用场景。

Abstract: We show that, when the actions of a Mazurkiewicz trace are considered not
merely as atomic (i.e., mere names) but transformations from a specified type
of inputs to a specified type of outputs, we obtain a novel notion of
presentation for effectful categories (also known as generalised Freyd
categories), a well-known algebraic structure in the semantics of
side-effecting computation. Like the usual representation of traces as graphs,
our notion of presentation gives rise to a graphical calculus for effectful
categories. We use our presentations to give a construction of the commuting
tensor product of free effectful categories, capturing the combination of
systems in which the actions of each must commute with one another, while still
permitting exchange of resources

</details>


### [28] [Distributing Retractions, Weak Distributive Laws and Applications to Monads of Hyperspaces, Continuous Valuations and Measures](https://arxiv.org/abs/2507.18418)
*Jean Goubault-Larrecq*

Main category: cs.LO

TL;DR: 论文研究了在范畴中通过弱分配律结合两个单子S和T来构建联合单子U的方法，并提出了分配回缩的概念来验证U的正确性。


<details>
  <summary>Details</summary>
Motivation: 解决在已知单子S和T的情况下，如何构建并验证联合单子U的问题，以及探索其在数学中的应用。

Method: 通过弱分配律和分配回缩的概念，建立S和T的联合单子U，并在2-范畴中对两者进行一一对应。

Result: 证明了分配回缩与弱分配律的对应关系，并举例说明了S和T在不同应用中的联合单子U。

Conclusion: 分配回缩是验证联合单子U的有效工具，且在多个数学领域（如概率估值和拓扑空间）中具有实际应用价值。

Abstract: Given two monads $S$, $T$ on a category where idempotents split, and a weak
distributive law between them, one can build a combined monad $U$. Making
explicit what this monad $U$ is requires some effort. When we already have an
idea what $U$ should be, we show how to recognize that $U$ is indeed the
combined monad obtained from $S$ and $T$: it suffices to exhibit what we call a
distributing retraction of $ST$ onto $U$. We show that distributing retractions
and weak distributive laws are in one-to-one correspondence, in a 2-categorical
setting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin
hyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad
of previsions or of forks, depending on the case. As a byproduct, this allows
us to describe the algebras of monads of superlinear, resp. sublinear
previsions. In the category of compact Hausdorff spaces, the Plotkin hyperspace
monad is sometimes known as the Vietoris monad, the monad of probability
valuations coincides with the Radon monad, and we infer that the associated
combined monad is the monad of normalized forks.

</details>


### [29] [Well-Founded Coalgebras Meet König's Lemma](https://arxiv.org/abs/2507.18539)
*Henning Urbat,Thorsten Wißmann*

Main category: cs.LO

TL;DR: 该论文对König引理进行了共代数的推广，从有限分支树扩展到finitary endofunctor H的共代数，并从集合基类扩展到局部有限可呈现的类别C。


<details>
  <summary>Details</summary>
Motivation: 为了在不同数学和计算机科学领域中扩展König引理的应用范围。

Method: 使用共代数方法和局部有限可呈现的类别C，提出一个广义König引理的共代数版本。

Result: 证明了在温和条件下，每个良好基础的共代数是其有限生成子共代数的定向连接，并推导出König引理在图和过渡系统中的新版本。

Conclusion: 研究不仅扩展了König引理的应用，还为初始代数的构造提供了新的方法和简洁的证明。

Abstract: K\"onig's lemma is a fundamental result about trees with countless
applications in mathematics and computer science. In contrapositive form, it
states that if a tree is finitely branching and well-founded (i.e. has no
infinite paths), then it is finite. We present a coalgebraic version of
K\"onig's lemma featuring two dimensions of generalization: from finitely
branching trees to coalgebras for a finitary endofunctor H, and from the base
category of sets to a locally finitely presentable category C, such as the
category of posets, nominal sets, or convex sets. Our coalgebraic K\"onig's
lemma states that, under mild assumptions on C and H, every well-founded
coalgebra for H is the directed join of its well-founded subcoalgebras with
finitely generated state space -- in particular, the category of well-founded
coalgebras is locally presentable. As applications, we derive versions of
K\"onig's lemma for graphs in a topos as well as for nominal and convex
transition systems. Additionally, we show that the key construction underlying
the proof gives rise to two simple constructions of the initial algebra
(equivalently, the final recursive coalgebra) for the functor H: The initial
algebra is both the colimit of all well-founded and of all recursive coalgebras
with finitely presentable state space. Remarkably, this result holds even in
settings where well-founded coalgebras form a proper subclass of recursive
ones. The first construction of the initial algebra is entirely new, while for
the second one our approach yields a short and transparent new correctness
proof.

</details>


### [30] [Proceedings 19th International Workshop on the ACL2 Theorem Prover and Its Applications](https://arxiv.org/abs/2507.18567)
*Ruben Gamboa,Panagiotis Manolios*

Main category: cs.LO

TL;DR: 总结：ACL2研讨会是ACL2定理证明系统用户展示研究的主要论坛。


<details>
  <summary>Details</summary>
Motivation: ACL2作为一个工业级自动化推理系统，致力于推动定理证明技术及其应用。

Method: 研讨会汇集研究者和用户，分享ACL2及其相关技术的最新进展。

Result: ACL2及其Boyer-Moore家族因其贡献获得2005年ACM软件系统奖。

Conclusion: ACL2系列研讨会和系统在自动化推理领域具有重要影响。

Abstract: The ACL2 Workshop series is the major technical forum for users of the ACL2
theorem proving system to present research related to the ACL2 theorem prover
and its applications. ACL2 is an industrial-strength automated reasoning
system, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM
Software System Award was awarded to Boyer, Kaufmann, and Moore for their work
on ACL2 and the other theorem provers in the Boyer-Moore family.

</details>


### [31] [Approximate SMT Counting Beyond Discrete Domains](https://arxiv.org/abs/2507.18612)
*Arijit Shaw,Kuldeep S. Meel*

Main category: cs.LO

TL;DR: 论文介绍了pact，一种基于哈希的近似模型计数方法，用于混合SMT公式，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法（如bit-blasting）仅限于离散变量，无法有效处理混合公式的模型计数问题。

Method: pact使用基于哈希的近似模型计数方法，通过优化的哈希函数和对数次SMT求解器调用，估计解的个数。

Result: pact在14,202个实例中成功完成了603个，而基线方法仅完成了13个，性能提升显著。

Conclusion: pact是一种高效的混合SMT公式模型计数工具，具有理论保证和实际性能优势。

Abstract: Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning,
solving complex formulas across discrete and continuous domains. Recent
progress in propositional model counting motivates extending SMT capabilities
toward model counting, especially for hybrid SMT formulas. Existing approaches,
like bit-blasting, are limited to discrete variables, highlighting the
challenge of counting solutions projected onto the discrete domain in hybrid
formulas.
  We introduce pact, an SMT model counter for hybrid formulas that uses
hashing-based approximate model counting to estimate solutions with theoretical
guarantees. pact makes a logarithmic number of SMT solver calls relative to the
projection variables, leveraging optimized hash functions. pact achieves
significant performance improvements over baselines on a large suite of
benchmarks. In particular, out of 14,202 instances, pact successfully finished
on 603 instances, while Baseline could only finish on 13 instances.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [32] [Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving](https://arxiv.org/abs/2507.17753)
*Liang Zhang,Xiaoming Zhai,Jionghao Lin,Jionghao Lin,Jennifer Kleiman,Diego Zapata-Rivera,Carol Forsyth,Yang Jiang,Xiangen Hu,Arthur C. Graesser*

Main category: cs.HC

TL;DR: 研究评估了四种LLM代理沟通策略在数学问题解决中的作用，发现双代理优于单代理，其中“点对点协作”表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探究LLM代理在AI辅助教育中如何通过有效沟通提高问题解决效率。

Method: 基于GPT-4o模型，在双代理数学问题解决环境中测试四种沟通模式。

Result: 双代理优于单代理，“点对点协作”准确率最高，对话行为对协作影响显著。

Conclusion: 多代理框架需结合有效沟通策略以应对复杂教育问题。

Abstract: Large Language Model (LLM) agents are increasingly utilized in AI-aided
education to support tutoring and learning. Effective communication strategies
among LLM agents improve collaborative problem-solving efficiency and
facilitate cost-effective adoption in education. However, little research has
systematically evaluated the impact of different communication strategies on
agents' problem-solving. Our study examines four communication modes,
\textit{teacher-student interaction}, \textit{peer-to-peer collaboration},
\textit{reciprocal peer teaching}, and \textit{critical debate}, in a
dual-agent, chat-based mathematical problem-solving environment using the
OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that
dual-agent setups outperform single agents, with \textit{peer-to-peer
collaboration} achieving the highest accuracy. Dialogue acts like statements,
acknowledgment, and hints play a key role in collaborative problem-solving.
While multi-agent frameworks enhance computational tasks, effective
communication strategies are essential for tackling complex problems in AI
education.

</details>


### [33] [A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians](https://arxiv.org/abs/2507.17754)
*Justin Morse,Kurt Gilbert,Kyle Shin,Rick Cooke,Peyton Rose,Jack Sullivan,Angelo Sisante*

Main category: cs.HC

TL;DR: AI驱动的医疗记录应用减轻了临床医生的负担，提高了文档质量和效率。


<details>
  <summary>Details</summary>
Motivation: 减少医生的工作负担和认知负荷，提升医疗服务的质量和效率。

Method: 集成了Whisper转录和GPT-4o的模块化上下文学习流水线，自动生成SOAP笔记和患者指导。使用微调的BART模型优化简洁性。

Result: 应用生成的笔记质量超过专家笔记，被540多名临床医生使用，94%的医生报告认知负荷减少，97%报告文档负担减轻。

Conclusion: AI系统具备减轻行政负担、支持高效高质量医疗服务的潜力。

Abstract: Clinician burnout has motivated the growing adoption of ambient medical
scribes in the clinic. In this work, we introduce a custom-built ambient scribe
application integrated into the EHR system at Included Health, a personalized
all-in-one healthcare company offering telehealth services. The application
uses Whisper for transcription and a modular in-context learning pipeline with
GPT-4o to automatically generate SOAP notes and patient instructions. Testing
on mock visit data shows that the notes generated by the application exceed the
quality of expert-written notes as determined by an LLM-as-a-judge. The
application has been widely adopted by the clinical practice, with over 540
clinicians at Included Health using the application at least once. 94% (n = 63)
of surveyed clinicians report reduced cognitive load during visits and 97% (n =
66) report less documentation burden when using the application. Additionally,
we show that post-processing notes with a fine-tuned BART model improves
conciseness. These findings highlight the potential for AI systems to ease
administrative burdens and support clinicians in delivering efficient,
high-quality care.

</details>


### [34] [Effects of variation in system responsiveness on user performance in virtual environments](https://arxiv.org/abs/2507.18085)
*Benjamin Watson,Neff Walker,William Ribarsky,Victoria Spaulding*

Main category: cs.HC

TL;DR: 系统响应时间（SR）在虚拟环境中的统计影响：标准差超过82毫秒时影响任务表现，且放置任务更敏感。


<details>
  <summary>Details</summary>
Motivation: 研究虚拟环境（VEs）中系统响应时间（SR）的波动及其对任务表现的影响，以优化人机交互。

Method: 通过三个被试内设计实验（11、12和10名参与者），测量和操纵SR的均值（MSR）和标准差（SDSR）。

Result: SDSR超过82毫秒才影响表现，放置任务对SR更敏感且需要更多视觉反馈。

Conclusion: 虚拟环境设计无需严格控制SDSR，但应根据视觉反馈需求调整SR控制，可广泛应用于交互图形应用。

Abstract: System responsiveness (SR) is defined as the elapsed time until a system
responds to user control. SR fluctuates over time, so it must be described
statistically with mean (MSR) and standard deviation (SDSR). In this paper, we
examine SR in virtual environments (VEs), outlining its components and methods
of experimental measurement and manipulation. Three studies of MSR and SDSR
effects on performance of grasp and placement tasks are then presented. The
studies used within-subjects designs with 11, 12, and 10 participants,
respectively. Results showed that SDSR affected performance only if it was
above 82 ms. Placement required more frequent visual feedback and was more
sensitive to SR. We infer that VE designers need not tightly control SDSR and
may wish to vary SR control based on required visual feedback frequency. These
results may be used to improve the human-computer interface in a wide range of
interactive graphical applications, including scientific visualization,
training, mental health, and entertainment.

</details>


### [35] [Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image](https://arxiv.org/abs/2507.17755)
*Jianfeng Lan,Yingjia Huang*

Main category: cs.HC

TL;DR: 研究发现，抖音（Douyin）的使用与青少年男性的外貌评价和身体区域满意度显著相关，而微信（WeChat）则无明显影响。抖音的视频算法加剧了对理想身材标准的接触，影响认知层面。


<details>
  <summary>Details</summary>
Motivation: 探讨中国社交媒体平台（抖音和微信）如何影响青少年男性的身体形象认知，特别是在算法驱动的视频环境中。

Method: 采用多维身体自我关系问卷（MBSRQ-AS）对395名10至24岁的中国男性青少年进行调查，评估其自我评价和身体满意度。

Result: 抖音使用与外貌评价及身体区域满意度显著相关；微信使用则无显著关联。

Conclusion: 社交媒体的技术设计和内容形式对心理结果具有中介作用，应重视平台特性以解决青少年身体形象问题。

Abstract: In the digital era, social media platforms play a pivotal role in shaping
adolescents' body image perceptions. This study examines how Douyin and WeChat,
two contrasting Chinese social media platforms, influence body image among
Chinese male adolescents. Employing a platformization perspective, we surveyed
395 male adolescents aged 10 to 24 using the Multidimensional Body-Self
Relations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation
and body satisfaction. Our findings reveal that Douyin usage is significantly
correlated with appearance evaluation and body area satisfaction, while WeChat
usage shows no significant correlation with any body image dimensions. These
results suggest that Douyin's algorithm-driven, video-centric environment
intensifies exposure to idealized body standards, impacting users at a
cognitive level. This study underscores the importance of considering
platform-specific characteristics in understanding social media's impact on
body image. It contributes to the broader discourse on how technological design
and content modalities mediate psychological outcomes, offering insights for
addressing body image concerns among male adolescents in China.

</details>


### [36] [Insights from Railway Professionals: Rethinking Railway assumptions regarding safety and autonomy](https://arxiv.org/abs/2507.17756)
*Josh Hunter,John McDermid,Simon Burton*

Main category: cs.HC

TL;DR: 研究探讨铁路专业人士对安全的认知，以指导未来技术发展。通过访谈司机、路线规划者和管理人员，发现对自动化的谨慎态度、偏好辅助技术，以及对安全的多维度理解。


<details>
  <summary>Details</summary>
Motivation: 理解铁路专业人士的安全认知，为未来技术开发和提升铁路安全提供依据。

Method: 通过访谈司机、路线规划者和行政人员，分析当前安全实践、自动化潜力及铁路系统的复杂性。

Result: 发现对自动化的谨慎态度、偏好辅助技术，以及对安全的多维度理解（人、系统、技术因素）。

Conclusion: 需铁路专用因果模型评估安全，研究填补学术与实践间的鸿沟，助力开发更有效的安全指标。

Abstract: This study investigates how railway professionals perceive safety as a
concept within rail, with the intention to help inform future technological
developments within the industry. Through a series of interviews with drivers,
route planners,and administrative personnel, the research explores the
currentstate of safety practices, the potential for automation and the
understanding of the railway as a system of systems. Key findings highlight a
cautious attitude towards automation, a preference for assistive technologies,
and a complex understanding of safety that integrates human, systematic and
technological factors. The study also addresses the limitations of transferring
automotive automation technologies to railways and the need for a
railway-specific causation model to better evaluate and enhance safety in an
evolving technological landscape. This study aims to bridge thegap between
contemporary research and practical applications, contributing to the
development of more effective safety metrics.

</details>


### [37] [BrisT1D Dataset: Young Adults with Type 1 Diabetes in the UK using Smartwatches](https://arxiv.org/abs/2507.17757)
*Sam Gordon James,Miranda Elaine Glynis Armstrong,Aisling Ann O'Kane,Harry Emerson,Zahraa S. Abdallah*

Main category: cs.HC

TL;DR: BrisT1D数据集结合了1型糖尿病管理设备和智能手表数据，以及访谈记录，为血糖预测和用户体验研究提供了资源。


<details>
  <summary>Details</summary>
Motivation: 探索1型糖尿病管理技术的实际应用及附加数据流的潜力，以推动未来慢性病管理技术的发展。

Method: 通过纵向研究24名英国年轻成年人，收集其使用的糖尿病管理设备和智能手表数据，并进行月度访谈。

Result: 数据集包含处理后的设备数据（便于快速分析）和原始数据（深入研究），以及访谈记录。

Conclusion: 数据集可用于血糖预测、低血糖预测、闭环算法开发及用户体验研究，支持混合方法研究。

Abstract: Background: Type 1 diabetes (T1D) has seen a rapid evolution in management
technology and forms a useful case study for the future management of other
chronic conditions. Further development of this management technology requires
an exploration of its real-world use and the potential of additional data
streams. To facilitate this, we contribute the BrisT1D Dataset to the growing
number of public T1D management datasets. The dataset was developed from a
longitudinal study of 24 young adults in the UK who used a smartwatch alongside
their usual T1D management. Findings: The BrisT1D dataset features both device
data from the T1D management systems and smartwatches used by participants, as
well as transcripts of monthly interviews and focus groups conducted during the
study. The device data is provided in a processed state, for usability and more
rapid analysis, and in a raw state, for in-depth exploration of novel insights
captured in the study. Conclusions: This dataset has a range of potential
applications. The quantitative elements can support blood glucose prediction,
hypoglycaemia prediction, and closed-loop algorithm development. The
qualitative elements enable the exploration of user experiences and opinions,
as well as broader mixed-methods research into the role of smartwatches in T1D
management.

</details>


### [38] [DHMS: A Digital Hostel Management System Integrating Campus ChatBot, Predictive Intelligence, and Real-Time Automation](https://arxiv.org/abs/2507.17759)
*Riddhi Heda,Sidhant Singh,Umair Yasir,Tanmay Jaiswal,Anil Mokhade*

Main category: cs.HC

TL;DR: 本文介绍了DHMS（数字化宿舍管理系统），一个旨在数字化和简化宿舍管理的模块化集成平台，利用现代技术提升效率和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 传统宿舍管理效率低下且沟通不畅，无法满足数字原生学生的需求，DHMS旨在解决这些问题。

Method: DHMS采用现代网页技术、人工智能和云计算，自动化房间分配、投诉处理、通行证物流和自然语言聊天机器人沟通。

Result: 在模拟测试中，DHMS实现了92%的学生满意度，聊天机器人响应时间低于1秒，并具备预测性维护和情感分析功能。

Conclusion: 尽管成果显著，系统仍需在多宿舍楼集成、用户接受度、负载扩展性和ERP兼容性方面进一步测试。

Abstract: Traditional hostel management practices in academic institutions often suffer
from inefficiencies, delays, and fragmented communication. These systems fail
to meet the expectations of digitally native students and place a significant
operational burden on hostel staff. This paper introduces DHMS (Digital Hostel
Management System), a modular and integrated platform designed to digitize and
streamline essential hostel management functions. DHMS leverages modern web
technologies, artificial intelligence, and cloud infrastructure to automate
room allotment, grievance redressal, gate pass logistics, and communication via
a natural language chatbot. In simulation tests, DHMS achieved a 92% student
satisfaction rate in room allocation and maintained an average chatbot response
time below one second. Additional features include predictive analytics for
proactive maintenance planning and sentiment analysis for feedback processing.
While promising, the system requires further testing for integration across
multiple hostel blocks, user acceptance, scalability under load, and ERP
compatibility before campus-wide deployment. This work discusses the system
architecture, implementation approach, and factors critical to improving user
experience, administrative efficiency, and decision-making processes.

</details>


### [39] [Co-constructing Explanations for AI Systems using Provenance](https://arxiv.org/abs/2507.17761)
*Jan-Christoph Kalo,Fina Polat,Shubha Guha,Paul Groth*

Main category: cs.HC

TL;DR: 提出了一个交互式代理，帮助用户与AI系统协同构建基于数据溯源的个性化解释。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统复杂，溯源数据虽能提供解释，但过于细节且缺乏用户背景。

Method: 开发了一个原型代理，并设计了一个基于用户模拟和大语言模型评审的可扩展评估框架。

Result: 通过代理和用户协同工作，实现了更接地气、有用的AI解释。

Conclusion: 交互式代理能够有效提升用户对AI系统的理解，同时具备可扩展性。

Abstract: Modern AI systems are complex workflows containing multiple components and
data sources. Data provenance provides the ability to interrogate and
potentially explain the outputs of these systems. However, provenance is often
too detailed and not contextualized for the user trying to understand the AI
system. In this work, we present our vision for an interactive agent that works
together with the user to co-construct an explanation that is simultaneously
useful to the user as well as grounded in data provenance. To illustrate this
vision, we present: 1) an initial prototype of such an agent; and 2) a scalable
evaluation framework based on user simulations and a large language model as a
judge approach.

</details>


### [40] [Human-AI Co-Creation: A Framework for Collaborative Design in Intelligent Systems](https://arxiv.org/abs/2507.17774)
*Zhangqi Liu*

Main category: cs.HC

TL;DR: 探讨AI作为创意伙伴在早期设计过程中与人类共同创作的新范式，超越自动化，融入构思、视觉化和决策。


<details>
  <summary>Details</summary>
Motivation: 随着AI从后端工具发展为交互式生成伙伴，需重新思考以人为中心的设计工作流。

Method: 研究大型语言模型（如GPT-4）和多模态扩散模型（如Stable Diffusion）作为创意代理，与设计师进行提案、批评和修订的迭代循环。

Result: 提出人机共创新框架，强调AI在设计和决策中的主动参与。

Conclusion: 人机共创为设计领域带来变革，AI不仅是工具，更是设计过程中的合作者。

Abstract: As artificial intelligence (AI) continues to evolve from a back-end
computational tool into an interactive, generative collaborator, its
integration into early-stage design processes demands a rethinking of
traditional workflows in human-centered design. This paper explores the
emergent paradigm of human-AI co-creation, where AI is not merely used for
automation or efficiency gains, but actively participates in ideation, visual
conceptualization, and decision-making. Specifically, we investigate the use of
large language models (LLMs) like GPT-4 and multimodal diffusion models such as
Stable Diffusion as creative agents that engage designers in iterative cycles
of proposal, critique, and revision.

</details>


### [41] [Same Data, Different Audiences: Using Personas to Scope a Supercomputing Job Queue Visualization](https://arxiv.org/abs/2507.17898)
*Connor Scully-Allison,Kevin Menear,Kristin Potter,Andrew McNutt,Katherine E. Isaacs,Dmitry Duplyakin*

Main category: cs.HC

TL;DR: 该论文探讨如何通过设计支持多用户群体的可视化工具，解决领域特定可视化工具的局限性，并通过设计研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 领域特定可视化工具通常仅针对单一用户群体，限制了其适用范围。论文旨在开发支持多群体的可视化工具，以提升其整体效用。

Method: 通过设计研究开发了Guidepost工具，结合笔记本嵌入式可视化技术，支持科学家、机器学习研究人员和系统管理员的不同任务。采用人物角色模型分类任务，共享任务通过交互式可视化支持，独特任务则通过脚本实现。

Result: 实验证明，Guidepost成功支持三个用户群体的共享任务，并通过程序化分析工作流满足特定需求。

Conclusion: 通过交互式可视化和程序化分析的结合，多用户群体可视化工具能够兼顾广度和深度需求，提升整体实用性。

Abstract: Domain-specific visualizations sometimes focus on narrow, albeit important,
tasks for one group of users. This focus limits the utility of a visualization
to other groups working with the same data. While tasks elicited from other
groups can present a design pitfall if not disambiguated, they also present a
design opportunity -- development of visualizations that support multiple
groups. This development choice presents a trade off of broadening the scope
but limiting support for the more narrow tasks of any one group, which in some
cases can enhance the overall utility of the visualization. We investigate this
scenario through a design study where we develop \textit{Guidepost}, a
notebook-embedded visualization of supercomputer queue data that helps
scientists assess supercomputer queue wait times, machine learning researchers
understand prediction accuracy, and system maintainers analyze usage trends. We
adapt the use of personas for visualization design from existing literature in
the HCI and software engineering domains and apply them in categorizing tasks
based on their uniqueness across the stakeholder personas. Under this model,
tasks shared between all groups should be supported by interactive
visualizations and tasks unique to each group can be deferred to scripting with
notebook-embedded visualization design. We evaluate our visualization with nine
expert analysts organized into two groups: a "research analyst" group that uses
supercomputer queue data in their research (representing the Machine Learning
researchers and Jobs Data Analyst personas) and a "supercomputer user" group
that uses this data conditionally (representing the HPC User persona). We find
that our visualization serves our three stakeholder groups by enabling users to
successfully execute shared tasks with point-and-click interaction while
facilitating case-specific programmatic analysis workflows.

</details>


### [42] [Automated Brake Onset Detection in Naturalistic Driving Data](https://arxiv.org/abs/2507.17943)
*Shu-Yuan Liu,Johan Engström,Gustav Markkula*

Main category: cs.HC

TL;DR: 论文提出了一种基于分段线性加速度模型的算法，用于自动估计刹车启动时间，适用于大规模数据且无需车辆控制信号，验证了其高效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于手动标注或车辆控制信号（如刹车踏板动作），无法处理大规模数据尤其是自动驾驶系统（ADS）日志数据，因此需要一种新的自动化方法。

Method: 开发了一种基于分段线性加速度模型的简单高效算法，自动估计刹车启动时间；同时提出了一种手动标注方法作为验证依据。

Result: 使用R2作为置信度指标验证算法准确性，并在自然避撞数据中与人工标注对比，表明算法高效、通用且可配置。

Conclusion: 尽管存在局限，但算法适用于各种道路用户和场景，且在无车辆控制信号情况下仍能有效工作。

Abstract: Response timing measures play a crucial role in the assessment of automated
driving systems (ADS) in collision avoidance scenarios, including but not
limited to establishing human benchmarks and comparing ADS to human driver
response performance. For example, measuring the response time (of a human
driver or ADS) to a conflict requires the determination of a stimulus onset and
a response onset. In existing studies, response onset relies on manual
annotation or vehicle control signals such as accelerator and brake pedal
movements. These methods are not applicable when analyzing large scale data
where vehicle control signals are not available. This holds in particular for
the rapidly expanding sets of ADS log data where the behavior of surrounding
road users is observed via onboard sensors. To advance evaluation techniques
for ADS and enable measuring response timing when vehicle control signals are
not available, we developed a simple and efficient algorithm, based on a
piecewise linear acceleration model, to automatically estimate brake onset that
can be applied to any type of driving data that includes vehicle longitudinal
time series data. We also proposed a manual annotation method to identify brake
onset and used it as ground truth for validation. R2 was used as a confidence
metric to measure the accuracy of the algorithm, and its classification
performance was analyzed using naturalistic collision avoidance data of both
ADS and humans, where our method was validated against human manual annotation.
Although our algorithm is subject to certain limitations, it is efficient,
generalizable, applicable to any road user and scenario types, and is highly
configurable.

</details>


### [43] [Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale](https://arxiv.org/abs/2507.17985)
*Alex Liu,Lief Esbenshade,Shawon Sarkar,Victor Tian,Zachary Zhang,Kevin He,Min Sun*

Main category: cs.HC

TL;DR: 论文提出了一种人机协作方法，通过大规模定性分析教师与AI的互动信息，探讨了教师在教育实践中如何使用AI工具，并验证了LLM在定性编码任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究教师如何利用大型语言模型（LLM）进行教学规划、支持多样化学习者及专业反思，并填补关于AI在教育实践中大规模应用研究的空白。

Method: 采用四阶段编码流程（包括归纳主题发现、代码本开发、结构化标注和模型基准测试），分析14万条来自K-12教师与AI的对话信息。

Result: 发现LLM（尤其是Claude 3.5 Haiku）能可靠支持主题识别，在复杂场景中表现优于开源模型；揭示了教师在教学中使用AI的主要模式。

Conclusion: 研究为AI增强定性研究提供了可扩展的透明模型，并为生成式AI在教育实践中的角色提供了基础性见解。

Abstract: The integration of large language models (LLMs) into educational tools has
the potential to substantially impact how teachers plan instruction, support
diverse learners, and engage in professional reflection. Yet little is known
about how educators actually use these tools in practice and how their
interactions with AI can be meaningfully studied at scale. This paper presents
a human-AI collaborative methodology for large-scale qualitative analysis of
over 140,000 educator-AI messages drawn from a generative AI platform used by
K-12 teachers. Through a four-phase coding pipeline, we combined inductive
theme discovery, codebook development, structured annotation, and model
benchmarking to examine patterns of educator engagement and evaluate the
performance of LLMs in qualitative coding tasks. We developed a hierarchical
codebook aligned with established teacher evaluation frameworks, capturing
educators' instructional goals, contextual needs, and pedagogical strategies.
Our findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably
support theme identification, extend human recognition in complex scenarios,
and outperform open-weight models in both accuracy and structural reliability.
The analysis also reveals substantive patterns in how educators inquire AI to
enhance instructional practices (79.7 percent of total conversations), create
or adapt content (76.1 percent), support assessment and feedback loop (46.9
percent), attend to student needs for tailored instruction (43.3 percent), and
assist other professional responsibilities (34.2 percent), highlighting
emerging AI-related competencies that have direct implications for teacher
preparation and professional development. This study offers a scalable,
transparent model for AI-augmented qualitative research and provides
foundational insights into the evolving role of generative AI in educational
practice.

</details>


### [44] [Evaluating judgment of spatial correlation in visual displays of scalar field distributions](https://arxiv.org/abs/2507.17997)
*Yayan Zhao,Matthew Berger*

Main category: cs.HC

TL;DR: 研究了2D标量场分布中空间相关性的识别，比较了动画和并排视图两种可视化设计，探讨了分布特性和显示方式对判断的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨人类如何在不同视觉显示中识别标量场的空间相关性，以及不同可视化设计和颜色尺度选择的效果。

Method: 通过实验设计，比较动画和并排视图的显示方式，并控制空间相关性和可辨别性的水平。

Result: 结果展示了分布特性对判断的影响，并揭示了不同视觉显示对空间相关性评估的作用。

Conclusion: 不同可视化设计显著影响人类对空间相关性的判断，分布特性也是重要因素。

Abstract: In this work we study the identification of spatial correlation in
distributions of 2D scalar fields, presented across different forms of visual
displays. We study simple visual displays that directly show color-mapped
scalar fields, namely those drawn from a distribution, and whether humans can
identify strongly correlated spatial regions in these displays. In this
setting, the recognition of correlation requires making judgments on a set of
fields, rather than just one field. Thus, in our experimental design we compare
two basic visualization designs: animation-based displays against juxtaposed
views of scalar fields, along different choices of color scales. Moreover, we
investigate the impacts of the distribution itself, controlling for the level
of spatial correlation and discriminability in spatial scales. Our study's
results illustrate the impacts of these distribution characteristics, while
also highlighting how different visual displays impact the types of judgments
made in assessing spatial correlation. Supplemental material is available at
https://osf.io/zn4qy

</details>


### [45] ["I Would Not Be This Version of Myself Today": Elaborating on the Effects of Eudaimonic Gaming Experiences](https://arxiv.org/abs/2507.18084)
*Nisha Devasia,Georgia Kenderova,Michele Newman,Julie Kientz,Jin Ha Lee*

Main category: cs.HC

TL;DR: 该研究探讨了游戏中的“幸福体验”如何影响玩家的个人成长和生活质量，填补了现有文献中关于此类体验效果的空白。


<details>
  <summary>Details</summary>
Motivation: 近年来，游戏研究逐渐从单一的愉悦体验转向关注具有个人意义的混合情感体验，但对其长期影响的研究不足。

Method: 采用混合方法，对166名受访者进行问卷调查，分析他们从游戏中获得的深刻体验及其对生活的影响。

Result: 研究发现，游戏的幸福体验能带来反思、学习、社交、健康和职业方面的积极影响，并识别了这些体验形成的关键子成分。

Conclusion: 研究扩展了游戏幸福体验的理论模型，为研究人员和实践者如何利用这些发现促进玩家积极发展提供了启示。

Abstract: While much of the research in digital games has emphasized hedonic
experiences, such as flow, enjoyment, and positive affect, recent years have
seen increased interest in eudaimonic gaming experiences, typically
mixed-affect and associated with personal meaningfulness and growth. The
formation of such experiences in games is theorized to have four constituent
elements: motivation, game use, experience, and effects. However, while the
first three elements have been relatively well explored in the literature, the
effects - and how they may influence positive individual outcomes - have been
underexplored thus far. To this end, in this work, we investigate the perceived
outcomes of eudaimonic gaming and how different components of the experience
influence these effects. We conducted a survey (n = 166) in which respondents
recounted meaningful gaming experiences and how they affected their present
lives. We used a mixed-methods approach to classify effects and identify
significant subcomponents of their formation. We contribute an empirical
understanding of how meaningful gaming experiences can lead to positive
reflective, learning, social, health, and career effects, extending current
theoretical models of eudaimonic gaming experiences and offering implications
for how researchers and practitioners might use these findings to promote
positive outcomes for players.

</details>


### [46] [Understood: Real-Time Communication Support for Adults with ADHD Using Mixed Reality](https://arxiv.org/abs/2507.18151)
*Shizhen Zhang,Shengxin Li,Quan Li*

Main category: cs.HC

TL;DR: Understood是一种基于混合现实（MR）的系统，旨在通过实时对话总结、上下文感知的单词建议和话题转移检测帮助成人ADHD患者改善沟通。


<details>
  <summary>Details</summary>
Motivation: 成人ADHD患者由于执行功能障碍和情绪失调面临沟通挑战，但目前缺乏适用于成人的非侵入性工具。

Method: 通过半结构化访谈和设计研讨会识别沟通障碍，开发了基于HoloLens 2的MR系统，并结合三项核心功能。

Result: 用户研究和专家访谈表明，Understood在支持沟通方面效果显著且易用性高。

Conclusion: Understood为成人ADHD患者提供了一种有效的沟通辅助工具，可作为治疗干预的补充。

Abstract: Adults with Attention Deficit Hyperactivity Disorder (ADHD) often experience
communication challenges, primarily due to executive dysfunction and emotional
dysregulation, even after years of social integration. While existing
interventions predominantly target children through structured or intrusive
methods, adults lack tools that translate clinical strategies into daily
communication support. To address this gap, we present Understood, a Mixed
Reality (MR) system implemented on Microsoft HoloLens 2, designed to assist
adults with ADHD in real-world communication. Through formative semi-structured
interviews and a design workshop, we identified critical communication barriers
and derived design goals for the system. Understood combines three key
features: (1) real-time conversation summarization to reduce cognitive load,
(2) context-aware subsequent word suggestions during moments of disfluency, and
(3) topic shifting detection and reminding to mitigate off-topic transitions. A
within-subjects user study and expert interviews demonstrate that Understood
effectively supports communication with high usability, offering a complement
to therapist-mediated interventions.

</details>


### [47] [ProactiveVA: Proactive Visual Analytics with LLM-Based UI Agent](https://arxiv.org/abs/2507.18165)
*Yuheng Zhao,Xueli Shu,Liwen Fan,Lin Gao,Yu Zhang,Siming Chen*

Main category: cs.HC

TL;DR: 提出了一个名为 ProactiveVA 的框架，通过 LLM 驱动的 UI 代理监控用户交互并主动提供上下文感知帮助。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 辅助 VA 系统仅在用户明确请求时提供帮助，无法在用户最需要时主动提供建议。

Method: 首先通过用户交互日志分析帮助需求，设计了意图识别、解决方案生成等关键功能，并开发了一个包括感知、推理和行动的三阶段 UI 代理流程。

Result: 在两种代表性的 VA 系统中实现并验证了框架的通用性和有效性。

Conclusion: ProactiveVA 框架能够有效提供主动帮助，但需要在设计和探索上进一步优化。

Abstract: Visual analytics (VA) is typically applied to complex data, thus requiring
complex tools. While visual analytics empowers analysts in data analysis,
analysts may get lost in the complexity occasionally. This highlights the need
for intelligent assistance mechanisms. However, even the latest LLM-assisted VA
systems only provide help when explicitly requested by the user, making them
insufficiently intelligent to offer suggestions when analysts need them the
most. We propose a ProactiveVA framework in which LLM-powered UI agent monitors
user interactions and delivers context-aware assistance proactively. To design
effective proactive assistance, we first conducted a formative study analyzing
help-seeking behaviors in user interaction logs, identifying when users need
proactive help, what assistance they require, and how the agent should
intervene. Based on this analysis, we distilled key design requirements in
terms of intent recognition, solution generation, interpretability and
controllability. Guided by these requirements, we develop a three-stage UI
agent pipeline including perception, reasoning, and acting. The agent
autonomously perceives users' needs from VA interaction logs, providing
tailored suggestions and intuitive guidance through interactive exploration of
the system. We implemented the framework in two representative types of VA
systems, demonstrating its generalizability, and evaluated the effectiveness
through an algorithm evaluation, case and expert study and a user study. We
also discuss current design trade-offs of proactive VA and areas for further
exploration.

</details>


### [48] [Recommender systems, representativeness, and online music: A psychosocial analysis of Italian listeners](https://arxiv.org/abs/2507.18169)
*Lorenzo Porcaro,Chiara Monaldi*

Main category: cs.HC

TL;DR: 论文探讨了音乐推荐系统可能导致的代表性伤害，特别是从心理社会和文化角度分析意大利听众的反馈，发现听众对算法的理解不足，尤其是性别差异问题未被充分关注。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于对推荐系统可能引发的代表性伤害的关注，尤其是在音乐推荐领域，现有研究多从认知行为学角度出发，缺乏心理社会和文化视角的探讨。

Method: 研究方法包括对意大利音乐听众的访谈，并通过情感文本分析法（Emotional Textual Analysis）分析其叙述。

Result: 研究发现听众对推荐系统的理解不足，且对性别差异等代表性问题的认知有限。

Conclusion: 结论强调需要跨学科研究来解决代表性伤害问题，并提升算法意识和数字素养，以开发更可信的推荐系统。

Abstract: Recommender systems shape music listening worldwide due to their widespread
adoption in online platforms. Growing concerns about representational harms
that these systems may cause are nowadays part of the scientific and public
debate, wherein music listener perspectives are oftentimes reported and
discussed from a cognitive-behaviorism perspective, but rarely contextualised
under a psychosocial and cultural lens. We proceed in this direction, by
interviewing a group of Italian music listeners and analysing their narratives
through Emotional Textual Analysis. Thanks to this, we identify shared cultural
repertoires that reveal people's complex relationship with listening practices:
even when familiar with online platforms, listeners may still lack a critical
understanding of recommender systems. Moreover, representational issues,
particularly gender disparities, seem not yet fully grasped in the context of
online music listening. This study underscores the need for interdisciplinary
research to address representational harms, and the role of algorithmic
awareness and digital literacy in developing trustworthy recommender systems.

</details>


### [49] [Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning](https://arxiv.org/abs/2507.18252)
*Dongyang Guo,Yasmeen Abdrabou,Enkeleda Thaqi,Enkelejda Kasneci*

Main category: cs.HC

TL;DR: 该论文提出了一种多模态人机协作框架，用于提升眼动追踪数据的认知模式提取能力。通过分阶段流水线、专家模型联合评分和混合异常检测模块，显著提高了数据分析的一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 眼动追踪数据具有结构化和非语言特性，分析难度大，而现有大型语言模型（LLMs）在处理时空和数值数据时表现不足，因此需要一种更高效的解决方案。

Method: 框架包括多阶段流水线（水平与垂直分割结合LLM推理）、专家模型联合评分模块（整合专家判断与LLM输出生成信任分数），以及混合异常检测模块（LSTM时空建模与LLM语义分析结合）。

Result: 实验结果表明，该方法在多个LLM和提示策略下表现优异，预测任务准确性提升达50%，同时增强了数据分析的一致性和可解释性。

Conclusion: 该框架为认知建模提供了可扩展且可解释的解决方案，在自适应学习、人机交互和教育分析等领域具有广泛应用潜力。

Abstract: Eye-tracking data reveals valuable insights into users' cognitive states but
is difficult to analyze due to its structured, non-linguistic nature. While
large language models (LLMs) excel at reasoning over text, they struggle with
temporal and numerical data. This paper presents a multimodal human-AI
collaborative framework designed to enhance cognitive pattern extraction from
eye-tracking signals. The framework includes: (1) a multi-stage pipeline using
horizontal and vertical segmentation alongside LLM reasoning to uncover latent
gaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert
judgment with LLM output to generate trust scores for behavioral
interpretations; and (3) a hybrid anomaly detection module combining LSTM-based
temporal modeling with LLM-driven semantic analysis. Our results across several
LLMs and prompt strategies show improvements in consistency, interpretability,
and performance, with up to 50% accuracy in difficulty prediction tasks. This
approach offers a scalable, interpretable solution for cognitive modeling and
has broad potential in adaptive learning, human-computer interaction, and
educational analytics.

</details>


### [50] [Talking to...uh...um...Machines: The Impact of Disfluent Speech Agents on Partner Models and Perspective Taking](https://arxiv.org/abs/2507.18315)
*Rhys Jacka,Paola R. Peña,Sophie Leonard,Éva Székely,Benjamin R. Cowan*

Main category: cs.HC

TL;DR: 研究探讨了语流中断（disfluencies）在人机对话（HMD）中对参与者感知和语言生成的影响，发现语流中断的语音代理被认为更可靠，但未显著影响对话灵活性或拟人化。同时，与语流中断代理互动增加了自我中心语言使用。


<details>
  <summary>Details</summary>
Motivation: 探索语流中断在人机对话中的作用，特别是在参与者对代理的感知和语言生成行为中的影响。

Method: 采用在线Namer-Matcher任务，61名参与者与语流中断或流畅的语音代理互动，并在任务前后完成伙伴建模问卷（PMQ）。

Result: 参与者认为语流中断代理更可靠，但对对话灵活性和拟人化无显著差异。语流中断代理互动增加了自我中心语言使用。

Conclusion: 语流中断在人机对话中影响参与者对代理的感知和语言生成行为，但其具体机制尚未明确，未来研究需进一步探讨。

Abstract: Speech disfluencies play a role in perspective-taking and audience design in
human-human communication (HHC), but little is known about their impact in
human-machine dialogue (HMD). In an online Namer-Matcher task, sixty-one
participants interacted with a speech agent using either fluent or disfluent
speech. Participants completed a partner-modelling questionnaire (PMQ) both
before and after the task. Post-interaction evaluations indicated that
participants perceived the disfluent agent as more competent, despite no
significant differences in pre-task ratings. However, no notable differences
were observed in assessments of conversational flexibility or human-likeness.
Our findings also reveal evidence of egocentric and allocentric language
production when participants interact with speech agents. Interaction with
disfluent speech agents appears to increase egocentric communication in
comparison to fluent agents. Although the wide credibility intervals mean this
effect is not clear-cut. We discuss potential interpretations of this finding,
focusing on how disfluencies may impact partner models and language production
in HMD.

</details>


### [51] [PALM: PAnoramic Learning Map Integrating Learning Analytics and Curriculum Map for Scalable Insights Across Courses](https://arxiv.org/abs/2507.18393)
*Mahiro Ozaki,Li Chen,Shotaro Naganuma,Valdemar Švábenský,Fumiya Okubo,Atsushi Shimada*

Main category: cs.HC

TL;DR: PALM是一种学习分析仪表盘，旨在通过整合课程级信息解决学习分析的扩展性问题，提升学生对学习行为和学业进展的认识。


<details>
  <summary>Details</summary>
Motivation: 传统学习分析多关注单门课程或学生，缺乏对课程间关系和学习长期轨迹的考虑，PALM填补了这一空白。

Method: PALM将多层教育数据整合到课程地图中，通过可视化学习历史和统计趋势来支持学生规划与反思。

Result: PALM显著提升了学生的学习行为意识和系统可用性，尤其在视觉吸引力和实用性上优于现有系统。

Conclusion: PALM是一种突破传统学习分析的综合且可扩展的方法，尽管仍需改进，但其促进了学生的自主学习与参与。

Abstract: This study proposes and evaluates the PAnoramic Learning Map (PALM), a
learning analytics (LA) dashboard designed to address the scalability
challenges of LA by integrating curriculum-level information. Traditional LA
research has predominantly focused on individual courses or learners and often
lacks a framework that considers the relationships between courses and the
long-term trajectory of learning. To bridge this gap, PALM was developed to
integrate multilayered educational data into a curriculum map, enabling
learners to intuitively understand their learning records and academic
progression. We conducted a system evaluation to assess PALM's effectiveness in
two key areas: (1) its impact on students' awareness of their learning
behaviors, and (2) its comparative performance against existing systems. The
results indicate that PALM enhances learners' awareness of study planning and
reflection, particularly by improving perceived behavioral control through the
visual presentation of individual learning histories and statistical trends,
which clarify the links between learning actions and outcomes. Although PALM
requires ongoing refinement as a system, it received significantly higher
evaluations than existing systems in terms of visual appeal and usability. By
serving as an information resource with previously inaccessible insights, PALM
enhances self-regulated learning and engagement, representing a significant
step beyond conventional LA toward a comprehensive and scalable approach.

</details>


### [52] [Multisensory Integration and Sensory Substitution Across Vision, Audition, and Haptics: Answering the What, Which, and When in Study Protocols](https://arxiv.org/abs/2507.18401)
*Andrew Jeyathasan,Swati Banerjee*

Main category: cs.HC

TL;DR: 研究探讨多感官整合（MSI）的关键因素及其在研究设计中的应用。


<details>
  <summary>Details</summary>
Motivation: 理解多感官整合（MSI）的复杂性和在多模态（三种及以上）情况下的交互作用。

Method: 分析跨模态对应、一致性、认知负荷和刺激时机等因素。

Result: 提出如何将这些因素应用于设计有效的MSI研究协议。

Conclusion: 强调多感官整合研究在多模态条件下的重要性及其潜在应用。

Abstract: We experience the world through multiple senses that work together to create
a cohesive perception, whether in daily life or immersive technologies.
Understanding this multisensory integration (MSI) requires examining the
interactions between sensory modalities, each with unique temporal dynamics and
characteristics. While most research focuses on unimodal or bimodal cues, the
integration of three or more modalities remains underexplored. MSI studies must
account for factors like cross-modal correspondence, congruence, cognitive
load, and stimulus timing, which become increasingly complex as modalities
multiply. This article examines these key factors and how they can be applied
to 8 design effective MSI study protocols.

</details>


### [53] [Towards Understanding Decision Problems As a Goal of Visualization Design](https://arxiv.org/abs/2507.18428)
*Lena Cibulski,Stefan Bruckner*

Main category: cs.HC

TL;DR: 本文提出了一种描述决策问题的框架，以改进可视化研究中的决策支持任务。


<details>
  <summary>Details</summary>
Motivation: 现有的任务模型往往忽略决策的环境条件，本文旨在填补这一空白，帮助更精确地描述决策支持任务。

Method: 作者提出了一种基于数据、用户和任务上下文的关键属性的描述方案。

Result: 该方案被应用于现有设计研究中的决策任务，展示了其有效性并指出了未来研究的机会。

Conclusion: 该框架为可视化研究中更精确地支持决策任务提供了基础，并为未来研究方向提供了启示。

Abstract: Decision-making is a central yet under-defined goal in visualization
research. While existing task models address decision processes, they often
neglect the conditions framing a decision. To better support decision-making
tasks, we propose a characterization scheme that describes decision problems
through key properties of the data, users, and task context. This scheme helps
visualization researchers specify decision-support claims more precisely and
informs the design of appropriate visual encodings and interactions. We
demonstrate the utility of our approach by applying it to characterize decision
tasks targeted by existing design studies, highlighting opportunities for
future research in decision-centric visualization.

</details>


### [54] [High-Dimensional Data Classification in Concentric Coordinates](https://arxiv.org/abs/2507.18450)
*Alice Williams,Boris Kovalerchuk*

Main category: cs.HC

TL;DR: 提出了一种支持低维到高维数据的无损可视化框架，基于同心坐标，克服了传统方法的遮挡问题并提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据可视化中遮挡问题和计算效率的限制，支持机器学习和人机交互。

Method: 提出基于同心坐标的无损可视化框架，作为平行坐标和圆形坐标的紧凑扩展。

Result: 实现了高维数据的无损可视化，支持机器学习和交互。

Conclusion: 同心坐标框架为解决高维数据可视化问题提供了有效方法。

Abstract: The visualization of multi-dimensional data with interpretable methods
remains limited by capabilities for both high-dimensional lossless
visualizations that do not suffer from occlusion and that are computationally
capable by parameterized visualization. This paper proposes a low to high
dimensional data supporting framework using lossless Concentric Coordinates
that are a more compact generalization of Parallel Coordinates along with
former Circular Coordinates. These are forms of the General Line Coordinate
visualizations that can directly support machine learning algorithm
visualization and facilitate human interaction.

</details>


### [55] [ForcePinch: Force-Responsive Spatial Interaction for Tracking Speed Control in XR](https://arxiv.org/abs/2507.18510)
*Chenyang Zhang,Tiffany S Ma,John Andrews,Eric J Gonzalez,Mar Gonzalez-Franco,Yalong Yang*

Main category: cs.HC

TL;DR: ForcePinch是一种新型的力响应空间交互方法，通过捏力调节跟踪速度，提升3D环境中的交互灵活性和精确性。


<details>
  <summary>Details</summary>
Motivation: 现有技术将跟踪速度调整直接与手部运动耦合，限制了交互的灵活性，因此需要一种更自然、灵活的方法。

Method: 开发了一种硬件原型，集成压力传感器和可定制映射功能，将捏力转化为跟踪速度调整。通过用户研究比较ForcePinch与Go-Go和PRISM技术。

Result: 研究发现力响应方法在不同交互情境中表现独特，验证了其灵活性和精确性。

Conclusion: ForcePinch展示了力响应交互的多样性和潜力，为未来空间交互设计提供了新思路。

Abstract: Spatial interaction in 3D environments requires balancing efficiency and
precision, which requires dynamic tracking speed adjustments. However, existing
techniques often couple tracking speed adjustments directly with hand
movements, reducing interaction flexibility. Inspired by the natural friction
control inherent in the physical world, we introduce ForcePinch, a novel
force-responsive spatial interaction method that enables users to intuitively
modulate pointer tracking speed and smoothly transition between rapid and
precise movements by varying their pinching force. To implement this concept,
we developed a hardware prototype integrating a pressure sensor with a
customizable mapping function that translates pinching force into tracking
speed adjustments. We conducted a user study with 20 participants performing
well-established 1D, 2D, and 3D object manipulation tasks, comparing ForcePinch
against the distance-responsive technique Go-Go and speed-responsive technique
PRISM. Results highlight distinctive characteristics of the force-responsive
approach across different interaction contexts. Drawing on these findings, we
highlight the contextual meaning and versatility of force-responsive
interactions through four illustrative examples, aiming to inform and inspire
future spatial interaction design.

</details>


### [56] [PosterMate: Audience-driven Collaborative Persona Agents for Poster Design](https://arxiv.org/abs/2507.18572)
*Donghoon Shin,Daniel Lee,Gary Hsieh,Gromit Yeuk-Yin Chan*

Main category: cs.HC

TL;DR: PosterMate是一个海报设计助手，通过创建基于营销文档的受众角色代理，模拟多样化反馈，并整合意见以改进设计。


<details>
  <summary>Details</summary>
Motivation: 解决在海报设计中获取多样化受众反馈的挑战，并探讨生成式AI在反馈过程中的作用。

Method: 引入PosterMate，通过角色代理收集反馈，并借助调解器促进讨论以达成共识。

Result: 用户研究表明PosterMate能捕捉被忽视的观点，并有效综合不同角色的反馈。

Conclusion: PosterMate展示了AI在优化设计反馈过程中的潜力。

Abstract: Poster designing can benefit from synchronous feedback from target audiences.
However, gathering audiences with diverse perspectives and reconciling them on
design edits can be challenging. Recent generative AI models present
opportunities to simulate human-like interactions, but it is unclear how they
may be used for feedback processes in design. We introduce PosterMate, a poster
design assistant that facilitates collaboration by creating audience-driven
persona agents constructed from marketing documents. PosterMate gathers
feedback from each persona agent regarding poster components, and stimulates
discussion with the help of a moderator to reach a conclusion. These
agreed-upon edits can then be directly integrated into the poster design.
Through our user study (N=12), we identified the potential of PosterMate to
capture overlooked viewpoints, while serving as an effective prototyping tool.
Additionally, our controlled online evaluation (N=100) revealed that the
feedback from an individual persona agent is appropriate given its persona
identity, and the discussion effectively synthesizes the different persona
agents' perspectives.

</details>


### [57] [MeloKids: Multisensory VR System to Enhance Speech and Motor Coordination in Children with Hearing Loss](https://arxiv.org/abs/2507.18619)
*Yichen Yu,Qiaoran Wang*

Main category: cs.HC

TL;DR: 研究探讨基于虚拟现实的多感官反馈技术如何改善听力障碍儿童的语言和运动康复效果，利用fNIRS技术评估不同互动模式下的皮质激活模式，为个性化康复系统设计提供依据。


<details>
  <summary>Details</summary>
Motivation: 听力障碍儿童在语言和运动发展上持续面临挑战，多感官反馈技术有望改善康复效果。

Method: 结合虚拟现实技术（听觉、视觉、触觉刺激）和fNIRS技术，评估儿童在不同互动模式下的皮质激活。

Result: 为设计个性化互动康复系统提供了科学依据。

Conclusion: 多感官反馈技术可提升听力障碍儿童的认知参与和运动控制能力。

Abstract: Children with hearing impairments face ongoing challenges in language and
motor development. This study explores how multi-sensory feedback technology
based on virtual reality (VR), integrating auditory, visual, and tactile
stimuli, can enhance rehabilitation outcomes. Using functional near-infrared
spectroscopy (fNIRS) technology, we assessed cortical activation patterns in
children during pitch-matching tasks across different interaction modes. Our
findings aim to provide evidence for designing personalized, interactive
rehabilitation systems that enhance cognitive engagement and motor control in
children with hearing impairments.

</details>


### [58] [Evaluation of a Provenance Management Tool for Immersive Virtual Fieldwork](https://arxiv.org/abs/2507.18622)
*Armin Bernstetter,Tom Kwasnitschka,Isabella Peters*

Main category: cs.HC

TL;DR: 论文探讨了通过记录研究流程的起源信息来确保研究可重复性，并评估了一种用于地质科学虚拟实地工作的起源管理工具。用户研究表明，该工具在沉浸式和非沉浸式环境下均被视为有用且易用。


<details>
  <summary>Details</summary>
Motivation: 确保研究的可重复性是良好科学实践的关键部分，尤其是在地质科学等领域，研究人员使用软件进行交互式和沉浸式的地理空间数据可视化。

Method: 通过用户研究评估了一种起源管理工具（Digital Lab Book，DLB），记录用户与虚拟实地工作工具的交互，并标注可视化的不同状态。

Result: 参与者认为DLB既有用又易于使用，沉浸式环境下感知的易用性更高，但使用模式未显示显著组间差异。

Conclusion: DLB在支持研究流程记录方面表现出潜力，尤其是在沉浸式环境中可能提升用户体验。

Abstract: Ensuring reproducibility of research is an integral part of good scientific
practice. One way to support this is through provenance: information about
research workflows from data gathering to researchers' sensemaking processes
leading to published results. This is highly important in disciplines such as
geosciences, where researchers use software for interactive and immersive
visualizations of geospatial data, doing virtual measurements in simulated
fieldwork on 3D models. We evaluated a provenance management tool, which allows
recording of interactions with a virtual fieldwork tool and annotating
different states of the visualization. The user study investigated how
researchers used this Digital Lab Book (DLB) and whether perceived ease of use
and perceived usefulness differed between groups in immersive or non-immersive
settings. Participants perceived the DLB as both useful and easy to use. While
there were indications of differences in perceived ease of use (higher for
immersive setting), usage patterns showed no significant group differences.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [59] [Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA](https://arxiv.org/abs/2507.17963)
*Rameen Abdal,Or Patashnik,Ekaterina Deyneka,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov,Daniel Cohen-Or,Kfir Aberman*

Main category: cs.GR

TL;DR: 论文提出了一种零样本框架，用于文本到视频模型中的动态概念个性化，无需实例微调，通过结构化视频网格和轻量级Grid-LoRA适配器实现高效编辑与合成。


<details>
  <summary>Details</summary>
Motivation: 现有的动态概念个性化方法通常需要针对每个实例进行微调，限制了可扩展性，因此研究团队旨在开发一种完全零样本的解决方案。

Method: 利用结构化2x2视频网格组织输入和输出对，训练轻量级Grid-LoRA适配器，并结合Grid Fill模块完成部分观察的布局，实现时序一致性和身份保持的输出。

Result: 实验表明，该方法能够在无需测试时优化的前提下，广泛适用于未训练的动态概念和编辑场景，生成高质量且一致的结果。

Conclusion: 提出的框架有效解决了动态概念个性化的可扩展性问题，为文本到视频生成提供了高效的零样本解决方案。

Abstract: Recent advances in text-to-video generation have enabled high-quality
synthesis from text and image prompts. While the personalization of dynamic
concepts, which capture subject-specific appearance and motion from a single
video, is now feasible, most existing methods require per-instance fine-tuning,
limiting scalability. We introduce a fully zero-shot framework for dynamic
concept personalization in text-to-video models. Our method leverages
structured 2x2 video grids that spatially organize input and output pairs,
enabling the training of lightweight Grid-LoRA adapters for editing and
composition within these grids. At inference, a dedicated Grid Fill module
completes partially observed layouts, producing temporally coherent and
identity preserving outputs. Once trained, the entire system operates in a
single forward pass, generalizing to previously unseen dynamic concepts without
any test-time optimization. Extensive experiments demonstrate high-quality and
consistent results across a wide range of subjects beyond trained concepts and
editing scenarios.

</details>


### [60] [DanceGraph: A Complementary Architecture for Synchronous Dancing Online](https://arxiv.org/abs/2507.18052)
*David Sinclair,Ademyemi Ademola,Babis Koniaris,Kenny Mitchell*

Main category: cs.GR

TL;DR: DanceGraph是一种用于在线同步跳舞的架构，旨在解决网络延迟问题，通过高效的实时带宽设计和运动预测技术实现与音乐节奏的同步。


<details>
  <summary>Details</summary>
Motivation: 解决在线跳舞中因网络延迟导致的姿势共享不同步问题。

Method: 开发实时带宽高效架构，减少延迟；引入参数化舞蹈动作风格化方法。

Result: 成功减少了运动预测时间，实现了与音乐节奏的同步。

Conclusion: DanceGraph为在线同步跳舞提供了一种有效的技术方案。

Abstract: DanceGraph is an architecture for synchronized online dancing overcoming the
latency of networked body pose sharing. We break down this challenge by
developing a real-time bandwidth-efficient architecture to minimize lag and
reduce the timeframe of required motion prediction for synchronization with the
music's rhythm. In addition, we show an interactive method for the
parameterized stylization of dance motions for rhythmic dance using online
dance correctives.

</details>


### [61] [Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation](https://arxiv.org/abs/2507.18352)
*Zhen Han,Mattias Teye,Derek Yadgaroff,Judith Bütepage*

Main category: cs.GR

TL;DR: 论文提出了一种用于游戏开发的实时面部动画模型，通过混合知识蒸馏和小型学生模型降低计算需求。


<details>
  <summary>Details</summary>
Motivation: 缺乏高质量、多样化的音频-动画数据集，且现有模型过大，无法实时运行。

Method: 使用混合知识蒸馏和伪标签技术，训练小型学生模型，减少了模型的复杂性和计算需求。

Result: 内存占用降至3.4 MB，所需未来音频上下文减少至81 ms，同时保持高质量动画效果。

Conclusion: 该方法为实现设备端实时推理提供了可行方案，推动了模型驱动的数字角色发展。

Abstract: The training of high-quality, robust machine learning models for
speech-driven 3D facial animation requires a large, diverse dataset of
high-quality audio-animation pairs. To overcome the lack of such a dataset,
recent work has introduced large pre-trained speech encoders that are robust to
variations in the input audio and, therefore, enable the facial animation model
to generalize across speakers, audio quality, and languages. However, the
resulting facial animation models are prohibitively large and lend themselves
only to offline inference on a dedicated machine. In this work, we explore
on-device, real-time facial animation models in the context of game
development. We overcome the lack of large datasets by using hybrid knowledge
distillation with pseudo-labeling. Given a large audio dataset, we employ a
high-performing teacher model to train very small student models. In contrast
to the pre-trained speech encoders, our student models only consist of
convolutional and fully-connected layers, removing the need for attention
context or recurrent updates. In our experiments, we demonstrate that we can
reduce the memory footprint to up to 3.4 MB and required future audio context
to up to 81 ms while maintaining high-quality animations. This paves the way
for on-device inference, an important step towards realistic, model-driven
digital characters.

</details>


### [62] [GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar](https://arxiv.org/abs/2507.18155)
*SeungJun Moon,Hah Min Lew,Seungeun Lee,Ji-Su Kang,Gyeong-Moon Park*

Main category: cs.GR

TL;DR: GeoAvatar通过自适应几何高斯泼溅技术，解决了3D头部avatar在身份保持与动画新姿势表达之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理面部区域几何偏差时表现不佳，导致质量不理想。

Method: 提出GeoAvatar框架，包括自适应预分配阶段（APS）、基于口腔结构的变形策略及正则化损失。

Result: 实验表明GeoAvatar在重建和新动画场景中优于现有技术。

Conclusion: GeoAvatar通过创新的几何处理方法提升了3D头部avatar的质量和动画效果。

Abstract: Despite recent progress in 3D head avatar generation, balancing identity
preservation, i.e., reconstruction, with novel poses and expressions, i.e.,
animation, remains a challenge. Existing methods struggle to adapt Gaussians to
varying geometrical deviations across facial regions, resulting in suboptimal
quality. To address this, we propose GeoAvatar, a framework for adaptive
geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation
Stage (APS), an unsupervised method that segments Gaussians into rigid and
flexible sets for adaptive offset regularization. Then, based on mouth anatomy
and dynamics, we introduce a novel mouth structure and the part-wise
deformation strategy to enhance the animation fidelity of the mouth. Finally,
we propose a regularization loss for precise rigging between Gaussians and 3DMM
faces. Moreover, we release DynamicFace, a video dataset with highly expressive
facial motions. Extensive experiments show the superiority of GeoAvatar
compared to state-of-the-art methods in reconstruction and novel animation
scenarios.

</details>


### [63] [PS-GS: Gaussian Splatting for Multi-View Photometric Stereo](https://arxiv.org/abs/2507.18231)
*Yixiao Chen,Bin Liang,Hanzhi Guo,Yongqing Cheng,Jiayi Zhao,Dongdong Weng*

Main category: cs.GR

TL;DR: 该论文提出了一种名为PS-GS的方法，通过结合高斯喷涂技术和多视角光度立体学（MVPS），高效地联合估计物体的几何、材质和光照，解决了现有逆向渲染方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的逆向渲染方法依赖于固定的环境光照，重建精度较低。而将逆向渲染与多视角光度立体学（MVPS）结合可以提升精度，但高效实现仍具挑战性。

Method: PS-GS方法首先构建2D高斯喷涂模型作为初始几何，随后基于该模型进行逆渲染优化，结合光照计算的多层感知机和未校准光度立体法估计的法线图，并提出2D高斯光线追踪以优化光照。

Result: 实验结果表明，该方法在合成和真实数据集上均优于现有方法，提高了重建精度和计算效率。

Conclusion: PS-GS方法能够高效地完成逆向渲染任务，支持新视角合成、重新光照以及材質和形状编辑，展现出广泛的应用潜力。

Abstract: Integrating inverse rendering with multi-view photometric stereo (MVPS)
yields more accurate 3D reconstructions than the inverse rendering approaches
that rely on fixed environment illumination. However, efficient inverse
rendering with MVPS remains challenging. To fill this gap, we introduce the
Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently
and jointly estimates the geometry, materials, and lighting of the object that
is illuminated by diverse directional lights (multi-light). Our method first
reconstructs a standard 2D Gaussian splatting model as the initial geometry.
Based on the initialization model, it then proceeds with the deferred inverse
rendering by the full rendering equation containing a lighting-computing
multi-layer perceptron. During the whole optimization, we regularize the
rendered normal maps by the uncalibrated photometric stereo estimated normals.
We also propose the 2D Gaussian ray-tracing for single directional light to
refine the incident lighting. The regularizations and the use of multi-view and
multi-light images mitigate the ill-posed problem of inverse rendering. After
optimization, the reconstructed object can be used for novel-view synthesis,
relighting, and material and shape editing. Experiments on both synthetic and
real datasets demonstrate that our method outperforms prior works in terms of
reconstruction accuracy and computational efficiency.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [64] [Low-power switching of memristors exhibiting fractional-order dynamics](https://arxiv.org/abs/2507.18487)
*Nathan Astin,Yuriy V. Pershin*

Main category: cs.ET

TL;DR: 研究探讨了具有分数阶行为的忆阻器在电流脉冲下的开关策略，发现最优策略取决于分数阶导数的阶数和运动方程中的幂指数。当分数阶导数阶数超过幂指数一半时，宽脉冲效果最佳；否则，零电流加窄高幅脉冲更优。这为节能神经形态计算架构提供了设计基础。


<details>
  <summary>Details</summary>
Motivation: 旨在探索使用电流脉冲控制具有分数阶行为的忆阻器的最优开关策略，以减少焦耳损耗，从而为更高效、仿生的神经形态计算架构提供支持。

Method: 采用分数阶微分方程（Caputo型导数）建模忆阻器的状态变量演化，并通过研究焦耳损耗来分析不同脉冲策略的影响。

Result: 最优开关策略取决于分数阶导数的阶数与幂指数的关系：若阶数超过幂指数一半，则宽脉冲更优；否则，零电流加窄高幅脉冲更能减少焦耳损耗。

Conclusion: 研究结果为设计下一代节能神经形态计算架构提供了理论基础，特别是通过优化开关策略以更接近生物系统的效能。

Abstract: In this conference contribution, we present some initial results on switching
memristive devices exhibiting fractional-order behavior using current pulses.
In our model, it is assumed that the evolution of a state variable follows a
fractional-order differential equation involving a Caputo-type derivative. A
study of Joule losses demonstrates that the best switching strategy minimizing
these losses depends on the fractional derivative's order and the power
exponent in the equation of motion. It is found that when the order of the
fractional derivative exceeds half of the power exponent, the best approach is
to employ a wide pulse. Conversely, when this condition is not met, Joule
losses are minimized by applying a zero current followed by a narrow current
pulse of the highest allowable amplitude. These findings are explored further
in the context of multi-pulse control. Our research lays the foundation for the
advancement of the next generation of energy-efficient neuromorphic computing
architectures that more closely mimic their biological counterparts.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [65] [Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release](https://arxiv.org/abs/2507.17766)
*Felix Quinque,Alan Aboudib,Szymon Fonau,Rodrigo Lopez Portillo Alcocer,Brian McCrindle,Steffen Cruz*

Main category: cs.DC

TL;DR: 论文提出了一种名为IOTA的分布式训练架构，解决了Bittensor SN9中模型存储和激励机制的问题，通过协同训练和公平激励机制实现了大规模语言模型的分布式预训练。


<details>
  <summary>Details</summary>
Motivation: 解决分布式网络中模型存储和'赢家通吃'奖励机制的问题，提升模型训练的效率和公平性。

Method: 采用数据并行和流水线并行的SWARM架构，结合连续激励机制、激活压缩、Butterfly All-Reduce和CLASP公平分配机制。

Result: 实现了模型规模随参与者数量线性扩展，通信带宽压缩128倍，训练速度显著提升，并确保贡献者公平获得奖励。

Conclusion: IOTA架构为分布式训练提供了一种高效、公平和可扩展的解决方案，解决了传统方法的局限性。

Abstract: In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed
network of incentivized, permissionless actors could each pretrain large
language models (LLMs) ranging from 700 million to 14 billion parameters, while
surpassing established baselines. While that work validated blockchain-based
decentralized pretraining as viable, it contained core issues: (i) every miner
had to fit an entire model locally, and (ii) "winner-takes-all" rewards
encouraged model hoarding.
  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an
architecture that addresses these limitations by transforming SN9's previously
isolated competitors into a single cooperating unit that can scale arbitrarily
while still rewarding each contributor fairly.
  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -
An orchestrator distributes model layers across heterogeneous miners and
streams activations between them, enabling model sizes to scale with the number
of participants rather than being constrained by the VRAM of a single machine;
(2) Granular, continuous incentives - Validators measure each miner's
contribution and allocate token emissions proportionally; (3) Activation
compression - We used model-bottlenecks to cut communication bandwidths of
activations by up to 128x, vastly improving training speed; (4) Butterfly
All-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,
offering linear scalability, redundancy and built-in collusion detection; (5)
CLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair
attribution scheme assigns credit to miners proportional to their marginal
utility and detects exploits, even when contributions are interdependent across
the pipeline.

</details>


### [66] [PolyServe: Efficient Multi-SLO Serving at Scale](https://arxiv.org/abs/2507.17769)
*Kan Zhu,Haiyang Shi,Le Xu,Jiaxin Shan,Arvind Krishnamurthy,Baris Kasikci,Liguang Xie*

Main category: cs.DC

TL;DR: PolyServe是一种新型多SLO调度策略，高效处理大语言模型请求，提升吞吐量和SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 现有系统对延迟敏感的LLM请求分类简单，忽视了SLO多样性，导致调度效果不佳。

Method: PolyServe通过请求分组、梯度负载调度和共享实例，结合动态分块和预测优化调度。

Result: 相比现有策略，PolyServe实现了1.23倍的吞吐量提升，达到最优吞吐量的92.5%。

Conclusion: PolyServe有效解决了多SLO调度问题，显著提升了用户体验和资源利用率。

Abstract: Advances in Large Language Models (LLMs) have led to a surge of LLM-powered
applications. These applications have diverse token-generation latency
requirements. As a result, simply classifying workloads as latency-sensitive
(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive
category and results in suboptimal user experiences and scheduling
opportunities. However, efficiently serving requests with multiple SLO
requirements poses significant challenges. First, all requests within a batch
generate new tokens simultaneously, which can misalign them with their distinct
SLO requirements. Moreover, while existing systems focus on auto-scaling for
handling various overall request rates, the diversity of SLOs necessitates
fine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE
scenarios, where BE requests can be aborted at any time to ensure the SLO
attainment of LS requests, those with different latency-sensitive SLOs cannot
tolerate prolonged delays, and tail latency must be controlled.
  To tackle these challenges, we propose PolyServe, a novel multi-SLO
scheduling policy at scale that maintains high SLO attainment while maximizing
throughput. PolyServe first groups requests into multiple bins based on their
per-token latency requirement, then schedules each bin to a subset of the
server fleet. PolyServe routes requests to the highest-load but still
SLO-attainable server to create a load gradient that facilitates auto-scaling.
To increase utilization, PolyServe permits looser-SLO requests to share
tighter-SLO instances when their own servers are saturated. PolyServe uses
profiling data to guide scheduling decisions and manage tail latency through
request-wait-time-aware scheduling, dynamic chunking, and continuous chunked
prefill prediction. PolyServe achieves 1.23x goodput gain compared to existing
policies, achieving up to 92.5% of optimal goodput.

</details>


### [67] [Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale](https://arxiv.org/abs/2507.17770)
*Pei-Kun Yang*

Main category: cs.DC

TL;DR: 研究比较了五种基于软件的QUBO求解器（Neal、PyTorch CPU/GPU、JAX、SciPy）在不同规模和收敛阈值下的性能，发现PyTorch在可扩展性和计算效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为大规模组合优化问题选择高效且可扩展的QUBO求解器提供参考。

Method: 在随机生成的QUBO矩阵（1000x1000至45000x45000）上测试五种求解器，评估能量和时间性能。

Result: Neal能量最低但不可扩展；PyTorch能量略高但可处理45000变量，GPU加速性能佳；JAX表现中等；SciPy最受限。

Conclusion: PyTorch是资源允许时大规模QUBO问题的平衡选择，兼顾质量与效率。

Abstract: Quadratic Unconstrained Binary Optimization (QUBO) is a versatile framework
for modeling combinatorial optimization problems. This study benchmarks five
software-based QUBO solvers: Neal, PyTorch (CPU), PyTorch (GPU), JAX, and
SciPy, on randomly generated QUBO matrices ranging from 1000x1000 to
45000x45000, under six convergence thresholds from 10^-1 to 10^-6. We evaluate
their performance in terms of solution quality (energy) and computational time.
Among the solvers tested, Neal achieved the lowest energy values but was
limited to problems with up to 6000 variables due to high memory consumption.
PyTorch produced slightly higher energy results than Neal but demonstrated
superior scalability, solving instances with up to 45000 variables. Its support
for GPU acceleration and CPU multi-threading also resulted in significantly
shorter runtimes. JAX yielded energy values slightly above those of PyTorch and
was limited to 25000 variables, with runtimes comparable to PyTorch on GPU.
SciPy was the most constrained solver, handling only up to 6000 variables and
consistently producing the highest energy values with the longest computation
times. These findings highlight trade-offs between solution quality,
scalability, and runtime efficiency, and suggest that PyTorch is the most
balanced choice for large-scale QUBO problems when computational resources
permit.

</details>


### [68] [Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN Inference Acceleration](https://arxiv.org/abs/2507.17771)
*Dmitri Lyalikov*

Main category: cs.DC

TL;DR: 论文探讨了在资源受限的嵌入式平台上部署现代CNN的挑战，并提出了利用RISC-V Vector扩展优化异构架构的系统整合和编译/执行模型，实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现代CNN在嵌入式平台的部署受到资源限制和功率/性能权衡的制约，异构架构和定制硬件（如DLAs和NPUs）提供了潜在解决方案，但其高效利用需要系统整合和优化的执行模型。

Method: 通过实验验证CNN执行中的性能瓶颈，利用RISC-V Vector 1.0扩展作为灵活目标，结合缓存层次方案，减少预处理瓶颈和CPU回退过程。

Result: 实验结果显示预处理速度提升9倍，YOLOv3回退层执行速度提升3倍，同时功耗低于传统并行执行平台。

Conclusion: RISC-V Vector 1.0能有效支持异构嵌入式SoC的计算和内存需求，为现代深度学习数据流提供高效、低功耗的解决方案。

Abstract: The emergence of heterogeneity and domain-specific architectures targeting
deep learning inference show great potential for enabling the deployment of
modern CNNs on resource-constrained embedded platforms. A significant
development is the diversification of custom hardware solely targeting the most
expensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural
processing units), among others, can overcome the approaching limits of
traditional silicon scaling and provide a solution to the power/performance
tradeoff within embedded SoCs. Efficient DSA utilization requires proper system
integration and a compilation/execution model for balanced execution in these
heterogeneous architectures. There is a critical need for proper system
integration and an efficient compilation/execution model for balanced execution
in these heterogeneous architectures. This work highlights the hardware
integration challenges for efficiently placing these units within the memory
hierarchy and correct proximity to other execution blocks. We experimentally
verify performance bottlenecks in CNN execution and pre/post-processing at
runtime, where previous attention has generally been given to accelerator
speedup alone. This work takes advantage of the ratification of the RISC-V
Vector 1.0 extension and demonstrates its potential as a flexible target within
a well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and
CPU fallback processes. Our results show up to a 9x speedup of image
pre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.
We demonstrate RVV-1.0 in exposing a flexible programming model that can enable
a balanced computation and memory footprint on accelerator-rich embedded SoCs
supporting modern deep-learning dataflows while consuming less power than
traditional parallel execution platforms.

</details>


### [69] [Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments](https://arxiv.org/abs/2507.17772)
*Ahmad Alhonainy,Praveen Rao*

Main category: cs.DC

TL;DR: 提出了一种基于缓存策略（如FIFO、LRU和优先级缓存）的联邦学习方法，以减少不必要的模型更新传输，从而降低带宽消耗并保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中通信成本高的问题，特别是在资源受限的环境下。

Method: 采用FIFO、LRU和基于优先级的缓存策略，选择性转发重要模型更新。

Result: 在CIFAR-10和医疗数据集上的实验表明，通信量减少且准确性损失极小。

Conclusion: 智能缓存提高了联邦学习的可扩展性和内存效率，适用于边缘物联网网络中的智能城市和医疗等延迟敏感应用。

Abstract: Federated Learning (FL) allows multiple distributed devices to jointly train
a shared model without centralizing data, but communication cost remains a
major bottleneck, especially in resource-constrained environments. This paper
introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce
unnecessary model update transmissions. By selectively forwarding significant
updates, our approach lowers bandwidth usage while maintaining model accuracy.
Experiments on CIFAR-10 and medical datasets show reduced communication with
minimal accuracy loss. Results confirm that intelligent caching improves
scalability, memory efficiency, and supports reliable FL in edge IoT networks,
making it practical for deployment in smart cities, healthcare, and other
latency-sensitive applications.

</details>


### [70] [MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation](https://arxiv.org/abs/2507.17773)
*Zhongzhen Wen,Yinghui Zhang,Zhong Li,Zhongxin Liu,Linna Xie,Tian Zhang*

Main category: cs.DC

TL;DR: MultiKernelBench是一个综合性多平台基准测试，用于评估大语言模型（LLMs）在深度学习内核生成任务中的表现，解决了现有基准测试的硬件支持不足、分类粗糙和任务覆盖不均衡等问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在深度学习内核生成领域的评估基准存在硬件支持有限、分类不精确和任务覆盖不均衡的问题，亟需一个更全面的解决方案。

Method: 引入MultiKernelBench，支持285个任务和14个内核类别，涵盖Nvidia GPU、Huawei NPU和Google TPU三大硬件平台，并提出了一种类别感知的单次提示方法以提高生成质量。

Result: 通过对七个先进LLMs的系统评估，揭示了任务难度差异显著、对训练较少硬件平台的泛化能力差，以及针对性提示策略的有效性。

Conclusion: MultiKernelBench为LLMs在深度学习内核生成领域的评估提供了一个标准化、可扩展的平台，推动了该领域的进一步发展。

Abstract: The automatic generation of deep learning (DL) kernels using large language
models (LLMs) has emerged as a promising approach to reduce the manual effort
and hardware-specific expertise required for writing high-performance operator
implementations. However, existing benchmarks for evaluating LLMs in this
domain suffer from limited hardware support, coarse-grained kernel
categorization, and imbalanced task coverage. To address these limitations, we
introduce MultiKernelBench, the first comprehensive, multi-platform benchmark
for LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14
well-defined kernel categories and supports three major hardware platforms:
Nvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we
design a modular backend abstraction layer that decouples platform-specific
logic from the core benchmarking infrastructure, allowing easy integration of
new hardware platforms. We further propose a simple yet effective
category-aware one-shot prompting method that improves generation quality by
providing in-category exemplars. Through systematic evaluations of seven
state-of-the-art LLMs, we reveal significant variation in task difficulty, poor
generalization to platforms with less training exposure, and the effectiveness
of targeted prompting strategies. MultiKernelBench is publicly available at
https://github.com/wzzll123/MultiKernelBench.

</details>


### [71] [CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks](https://arxiv.org/abs/2507.17793)
*Joel Brogan,Matthew Yohe,David Cornett*

Main category: cs.DC

TL;DR: CHAMP是一种模块化边缘计算平台，支持动态更换AI功能模块，适用于生物识别和监控等任务。


<details>
  <summary>Details</summary>
Motivation: 为需要灵活、高性能边缘AI系统的现场操作员提供定制化解决方案。

Method: 采用基于FPGA的加速器和定制操作系统VDiSK，实现即插即用AI流水线和安全数据集。

Result: 实验显示1至5个神经计算加速器的吞吐量接近线性增长，但也揭示了总线限制。

Conclusion: CHAMP在生物识别和灾害响应中有广泛应用前景，未来可改进总线协议和模块功能。

Abstract: What if you could piece together your own custom biometrics and AI analysis
system, a bit like LEGO blocks? We aim to bring that technology to field
operators in the field who require flexible, high-performance edge AI system
that can be adapted on a moment's notice. This paper introduces CHAMP
(Configurable Hot-swappable Architecture for Machine Perception), a modular
edge computing platform that allows operators to dynamically swap in
specialized AI "capability cartridges" for tasks like face recognition, object
tracking, and document analysis. CHAMP leverages low-power FPGA-based
accelerators on a high-throughput bus, orchestrated by a custom operating
system (VDiSK) to enable plug-and-play AI pipelines and cryptographically
secured biometric datasets. In this paper we describe the CHAMP design,
including its modular scaling with multiple accelerators and the VDiSK
operating system for runtime reconfiguration, along with its cryptographic
capabilities to keep data stored on modules safe and private. Experiments
demonstrate near-linear throughput scaling from 1 to 5 neural compute
accelerators, highlighting both the performance gains and saturation limits of
the USB3-based bus. Finally, we discuss applications of CHAMP in field
biometrics, surveillance, and disaster response, and outline future
improvements in bus protocols, cartridge capabilities, and system software.

</details>


### [72] [Optimizing Edge Gaming Slices through an Enhanced User Plane Function and Analytics in Beyond-5G Networks](https://arxiv.org/abs/2507.17843)
*Bruno Marques da Silva,Larissa Ferreira Rodrigues Moreira,Flávio de Oliveira Silva,Rodrigo Moreira*

Main category: cs.DC

TL;DR: 论文提出了一种闭环架构，结合NWDAF和UPF以估计用户延迟并增强5G控制平面，从而提升移动边缘游戏的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决移动边缘游戏中服务管理和SLA合规性的挑战，尤其是非侵入式用户延迟测量的需求。

Method: 提出了一种闭环架构，通过集成NWDAF和UPF来估计用户延迟，并利用人工智能模型进行分类。

Result: 结果表明，嵌入式AI模型能够实现游戏分类，为移动边缘游戏研究开辟了新途径。

Conclusion: 该架构显著提升了5G控制平面的延迟感知能力，为移动边缘游戏的发展提供了技术支持。

Abstract: The latest generation of games and pervasive communication technologies poses
challenges in service management and Service-Level Agreement compliance for
mobile users. State-of-the-art edge-gaming techniques enhance throughput,
reduce latency, and leverage cloud computing. However, further development of
core functions such as the User Plane Function (UPF) is needed for
non-intrusive user latency measurement. This paper proposes a closed-loop
architecture integrating the Network Data Analytics Function (NWDAF) and UPF to
estimate user latency and enhance the 5G control plane by making it
latency-aware. The results show that embedding an artificial intelligence model
within NWDAF enables game classification and opens new avenues for mobile edge
gaming research.

</details>


### [73] [PowerTrip: Exploiting Federated Heterogeneous Datacenter Power for Distributed ML Training](https://arxiv.org/abs/2507.17904)
*Talha Mehboob,Luanzheng Guo,Nathan Tallent,Michael Zink,David Irwin*

Main category: cs.DC

TL;DR: PowerTrip 是一个动态选择分布式站点的系统，用于优化大规模 AI 模型训练中的电力与通信权衡，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式训练中电力供应不均和网络延迟带来的性能损失问题。

Method: PowerTrip 采用动态贪心算法，基于电力与成本的启发式规则选择站点，优化训练效率。

Result: PowerTrip 能够将训练时间缩短 50%，优于现有基准策略。

Conclusion: PowerTrip 有效解决了电力受限和地理分布式环境中的模型训练问题，显著提升效率。

Abstract: The exponential growth of large-scale AI models has led to computational and
power demands that can exceed the capacity of a single data center. This is due
to the limited power supplied by regional grids that leads to limited regional
computational power. Consequently, distributing training workloads across
geographically distributed sites has become essential. However, this approach
introduces a significant challenge in the form of communication overhead,
creating a fundamental trade-off between the performance gains from accessing
greater aggregate power and the performance losses from increased network
latency. Although prior work has focused on reducing communication volume or
using heuristics for distribution, these methods assume constant homogeneous
power supplies and ignore the challenge of heterogeneous power availability
between sites.
  To address the challenge of training large models in power-constrained,
geo-distributed environments, we introduce PowerTrip, a system that dynamically
selects a subset of sites during runtime to optimize the power-communication
trade-off. Specifically, PowerTrip selects sites based on a power-to-cost
heuristic, prioritizing those with high power availability and low network
latency. PowerTrip employs a dynamic greedy approach and uses the marginal gain
in training efficiency, i.e., accuracy improvement per unit of time, to
optimize for the number of sites where the performance penalty from network
overhead negates the benefit of adding more computational power. Our
evaluation, which uses real-world Google power traces to model realistic power
capacity constraints, demonstrates that PowerTrip can reduce time-to-accuracy
by up to 50% compared to existing baseline policies.

</details>


### [74] [C-Koordinator: Interference-aware Management for Large-scale and Co-located Microservice Clusters](https://arxiv.org/abs/2507.18005)
*Shengye Song,Minxian Xu,Zuowei Zhang,Chengxi Gao,Fansong Zeng,Yu Ding,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 论文研究了大规模共址微服务集群中的资源干扰问题，提出基于CPI的干扰预测模型，并设计了C-Koordinator平台来优化资源利用和减少干扰。


<details>
  <summary>Details</summary>
Motivation: 微服务共址虽提升资源利用率，却引发资源竞争和干扰，尤其是在大规模、多样化和异构环境中，亟需高效干扰管理策略。

Method: 通过分析阿里巴巴大规模微服务集群特性，选择CPI作为干扰度量指标，开发多维度干扰预测模型，并构建C-Koordinator平台实施共址与干扰缓解策略。

Result: 干扰预测模型精度超过90.3%，应用延迟在各百分位（P50-P99）显著降低16.7%-36.1%，优于现有系统。

Conclusion: C-Koordinator平台能有效预测和缓解干扰，稳定微服务性能，适用于大规模共址环境。

Abstract: Microservices transform traditional monolithic applications into lightweight,
loosely coupled application components and have been widely adopted in many
enterprises. Cloud platform infrastructure providers enhance the resource
utilization efficiency of microservices systems by co-locating different
microservices. However, this approach also introduces resource competition and
interference among microservices. Designing interference-aware strategies for
large-scale, co-located microservice clusters is crucial for enhancing resource
utilization and mitigating competition-induced interference. These challenges
are further exacerbated by unreliable metrics, application diversity, and node
heterogeneity.
  In this paper, we first analyze the characteristics of large-scale and
co-located microservices clusters at Alibaba and further discuss why cycle per
instruction (CPI) is adopted as a metric for interference measurement in
large-scale production clusters, as well as how to achieve accurate prediction
of CPI through multi-dimensional metrics. Based on CPI interference prediction
and analysis, we also present the design of the C-Koordinator platform, an
open-source solution utilized in Alibaba cluster, which incorporates
co-location and interference mitigation strategies. The interference prediction
models consistently achieve over 90.3% accuracy, enabling precise prediction
and rapid mitigation of interference in operational environments. As a result,
application latency is reduced and stabilized across all percentiles (P50, P90,
P99) response time (RT), achieving improvements ranging from 16.7% to 36.1%
under various system loads compared with state-of-the-art system. These results
demonstrate the system's ability to maintain smooth application performance in
co-located environments.

</details>


### [75] [Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling](https://arxiv.org/abs/2507.18006)
*Jingfeng Wu,Yiyuan He,Minxian Xu,Xitong Gao,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: CoCoServe是一个弹性系统，通过模块级操作实现动态和细粒度的扩展，显著优化大型语言模型（LLM）的资源利用和性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM服务系统中资源管理与动态流量适配的挑战。

Method: 提出模块级的复制和迁移操作，并开发自动扩展机制，动态调整资源分配。

Result: 降低成本46%，延迟减少14%-75%，吞吐量提升1.16x-4x。

Conclusion: CoCoServe在资源利用和性能方面优于现有系统。

Abstract: The rise of large language models (LLMs) has created new opportunities across
various fields but has also introduced significant challenges in resource
management. Current LLM serving systems face a fundamental tension: balancing
serving demands with limited resources while adapting to unpredictable traffic
patterns. Static deployments lead to suboptimal resource utilization and
performance degradation under dynamic workloads. Furthermore, the high cost of
adjusting instances hinders dynamic scaling, limiting the true potential of
efficient LLM serving.
  To address this, we propose CoCoServe, an elastic system that facilitates
dynamic and fine-grained scaling. Its key innovation lies in the module-level
operations for the replication and migration of LLM modules, such as decoder
layers and projections. Through a comprehensive analysis of the trade-offs
associated with these operations, we develop an auto-scaling mechanism that
dynamically regulates module-level resource allocation and performance
optimization, enabling a more cost-effective deployment of LLMs. Our evaluation
demonstrates that the scaling operations employed by CoCoServe exhibit
excellent scalability and can reduce costs by 46% while maintaining
availability. Compared to state-of-the-art LLM serving systems (e.g., Hugging
Face Transformers and vLLM), our approach reduces latency by 14%-75% and
achieves 1.16x-4x throughput on average across different model sizes and
workloads.

</details>


### [76] [Cloud Native System for LLM Inference Serving](https://arxiv.org/abs/2507.18007)
*Minxian Xu,Junhan Liao,Jingfeng Wu,Yiyuan He,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 利用云原生技术优化大型语言模型推理服务，提升资源效率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的高计算需求在云端部署中带来效率挑战，传统方法存在资源浪费和扩展性问题。

Method: 采用容器化、微服务和动态调度等云原生技术，结合Kubernetes自动扩展进行优化。

Result: 实现更高效的资源分配、降低延迟并提升吞吐量，有效应对高负载场景。

Conclusion: 云原生框架有望重塑可扩展的LLM推理服务未来，为研究与实践提供重要见解。

Abstract: Large Language Models (LLMs) are revolutionizing numerous industries, but
their substantial computational demands create challenges for efficient
deployment, particularly in cloud environments. Traditional approaches to
inference serving often struggle with resource inefficiencies, leading to high
operational costs, latency issues, and limited scalability. This article
explores how Cloud Native technologies, such as containerization,
microservices, and dynamic scheduling, can fundamentally improve LLM inference
serving. By leveraging these technologies, we demonstrate how a Cloud Native
system enables more efficient resource allocation, reduces latency, and
enhances throughput in high-demand scenarios. Through real-world evaluations
using Kubernetes-based autoscaling, we show that Cloud Native architectures can
dynamically adapt to workload fluctuations, mitigating performance bottlenecks
while optimizing LLM inference serving performance. This discussion provides a
broader perspective on how Cloud Native frameworks could reshape the future of
scalable LLM inference serving, offering key insights for researchers,
practitioners, and industry leaders in cloud computing and artificial
intelligence.

</details>


### [77] [FCPO: Federated Continual Policy Optimization for Real-Time High-Throughput Edge Video Analytics](https://arxiv.org/abs/2507.18047)
*Lucas Liebe,Thanh-Tung Nguyen,Dongman Lee*

Main category: cs.DC

TL;DR: FCPO结合持续强化学习(CRL)和联邦强化学习(FRL)，优化边缘视频分析(EVA)的实时推理服务，显著提升吞吐量、降低延迟并减少内存消耗。


<details>
  <summary>Details</summary>
Motivation: 边缘视频分析的复杂性增加，现有调度系统在动态边缘环境中效率不足，需快速调整以适应变化。

Method: FCPO整合CRL和FRL，动态调整推理批处理大小、分辨率及多线程处理，通过智能体验缓冲和多样化学习提升性能。

Result: 实验显示FCPO在吞吐量上提升5倍，延迟降低60%，收敛速度加快20%，内存消耗减少10倍。

Conclusion: FCPO有效解决了边缘环境中的实时推理挑战，为智能应用提供了高效解决方案。

Abstract: The growing complexity of Edge Video Analytics (EVA) facilitates new kind of
intelligent applications, but creates challenges in real-time inference serving
systems. State-of-the-art (SOTA) scheduling systems optimize global workload
distributions for heterogeneous devices but often suffer from extended
scheduling cycles, leading to sub-optimal processing in rapidly changing Edge
environments. Local Reinforcement Learning (RL) enables quick adjustments
between cycles but faces scalability, knowledge integration, and adaptability
issues. Thus, we propose FCPO, which combines Continual RL (CRL) with Federated
RL (FRL) to address these challenges. This integration dynamically adjusts
inference batch sizes, input resolutions, and multi-threading during pre- and
post-processing. CRL allows agents to learn from changing Markov Decision
Processes, capturing dynamic environmental variations, while FRL improves
generalization and convergence speed by integrating experiences across
inference models. FCPO combines these via an agent-specific aggregation scheme
and a diversity-aware experience buffer. Experiments on a real-world EVA
testbed showed over 5 times improvement in effective throughput, 60% reduced
latency, and 20% faster convergence with up to 10 times less memory consumption
compared to SOTA RL-based approaches.

</details>


### [78] [A large-scale distributed parallel discrete event simulation engines based on Warped2 for Wargaming simulation](https://arxiv.org/abs/2507.18050)
*Xiaoning Jia,Ruilin Kong,Guangya Si,Bilong Shen,Zhe Ji*

Main category: cs.DC

TL;DR: 论文提出了一个优化的并行离散事件仿真框架，解决了Warped2引擎在资源分配和实体交互中的局限性，通过四项改进显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统并行离散事件仿真引擎（如Warped2）在大规模仿真中存在资源分配低效和复杂实体交互模式未解决的问题。

Method: 引入四项协同优化：异步监听线程、METIS负载均衡策略、实体交互求解器和空间哈希算法。

Result: 实验验证显示，框架性能提升显著，基准测试中实现了16倍加速和58.18%的同步开销减少。

Conclusion: 优化后的框架为大规模仿真提供了高效解决方案。

Abstract: Rising demand for complex simulations highlights conventional
engines'scalability limits, spurring Parallel Discrete Event Simulation (PDES)
adoption.Warped2, a PDES engine leveraging Time Warp synchronization with
Pending Event Set optimization, delivers strong performance, it struggles with
inherent wargaming limitations: inefficient LP resource allocation during
synchronization and unaddressed complex entity interaction patterns. To address
these challenges, we present an optimized framework featuring four synergistic
improvements: (1) Asynchronous listener threads are introduced to address event
monitoring latency in large-scale scenarios, instead of synchronous polling
mechanisms, (2) METIS-based load rebalancing strategy is incorporated to
address the issue of dynamic event allocation during real-world simulation, (3)
Entity interaction solver with constraint satisfaction mechanisms is designed
to mitigate state conflicts, and (4) Spatial hashing algorithm to overcome
O(n^2) complexity bottlenecks in large-scale nearest-neighbor searches.
Experimental validation through a GridWorld demo demonstrates significant
enhancements in temporal fidelity and computational efficiency. Benchmark
results show our framework achieves 16x acceleration over baseline
implementations and maintains 8x speedup over 1-thread configuration across MPI
and Pthreads implementations.The combined load balancing and LP migration
strategy reduces synchronization overhead by 58.18%, with load balancing
accounting for 57% of the total improvement as the dominant optimization
factor. These improvements provide an enhanced solution for PDES implementation
in large-scale simulation scenarios.

</details>


### [79] [Towards Designing an Energy Aware Data Replication Strategy for Cloud Systems Using Reinforcement Learning](https://arxiv.org/abs/2507.18459)
*Amir Najjar,Riad Mokadem,Jean-Marc Pierson*

Main category: cs.DC

TL;DR: 提出了一种基于强化学习的新型数据复制策略，用于云系统以动态适应工作负载变化和系统特性，旨在优化服务质量和成本效益权衡。


<details>
  <summary>Details</summary>
Motivation: 全球数据量的快速增长需要可扩展的分布式系统来保持高服务质量，传统基于阈值的复制策略需要人工调整，难以适应变化。

Method: 采用强化学习方法，自动学习系统特性并动态调整数据复制策略，定义状态、动作和奖励以优化目标。

Result: 提出的策略能够在满足服务质量的同时，优化云服务提供商的利润与环境影响的平衡。

Conclusion: 强化学习为数据复制策略的动态调整提供了有效方法，适用于现代云系统的工作负载变化需求。

Abstract: The rapid growth of global data volumes has created a demand for scalable
distributed systems that can maintain a high quality of service. Data
replication is a widely used technique that provides fault tolerance, improved
performance and higher availability. Traditional implementations often rely on
threshold-based activation mechanisms, which can vary depending on workload
changes and system architecture. System administrators typically bear the
responsibility of adjusting these thresholds. To address this challenge,
reinforcement learning can be used to dynamically adapt to workload changes and
different architectures. In this paper, we propose a novel data replication
strategy for cloud systems that employs reinforcement learning to automatically
learn system characteristics and adapt to workload changes. The strategy's aim
is to provide satisfactory Quality of Service while optimizing a trade-off
between provider profit and environmental impact. We present the architecture
behind our solution and describe the reinforcement learning model by defining
the states, actions and rewards.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [80] [An advanced AI driven database system](https://arxiv.org/abs/2507.17778)
*M. Tedeschi,S. Rizwan,C. Shringi,V. Devram Chandgir,S. Belich*

Main category: cs.DB

TL;DR: 提出一种基于AI的新型数据库系统，通过自然语言处理和LLMs简化数据库管理，降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 解决传统数据库系统对非技术用户复杂且难以使用的问题。

Method: 结合NLP和LLMs，实现自然语言交互、自动查询生成和数据结构设计。

Result: 系统支持自动数据建模、查询理解和性能优化，减少技术需求和人为错误。

Conclusion: AI数据库系统显著提升了易用性和效率，适用于非技术用户。

Abstract: Contemporary database systems, while effective, suffer severe issues related
to complexity and usability, especially among individuals who lack technical
expertise but are unfamiliar with query languages like Structured Query
Language (SQL). This paper presents a new database system supported by
Artificial Intelligence (AI), which is intended to improve the management of
data using natural language processing (NLP) - based intuitive interfaces, and
automatic creation of structured queries and semi-structured data formats like
yet another markup language (YAML), java script object notation (JSON), and
application program interface (API) documentation. The system is intended to
strengthen the potential of databases through the integration of Large Language
Models (LLMs) and advanced machine learning algorithms. The integration is
purposed to allow the automation of fundamental tasks such as data modeling,
schema creation, query comprehension, and performance optimization. We present
in this paper a system that aims to alleviate the main problems with current
database technologies. It is meant to reduce the need for technical skills,
manual tuning for better performance, and the potential for human error. The AI
database employs generative schema inference and format selection to build its
schema models and execution formats.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [81] [Designing High-Performance and Thermally Feasible Multi-Chiplet Architectures enabled by Non-bendable Glass Interposer](https://arxiv.org/abs/2507.18040)
*Harsh Sharma,Janardhan Rao Doppa,Umit Y. Ogras,Partha Pratim Pande*

Main category: cs.AR

TL;DR: 玻璃中介层多芯片架构通过架构与封装协同优化，实现了性能提升和能耗降低，解决了系统增大时的翘曲问题。


<details>
  <summary>Details</summary>
Motivation: 解决玻璃中介层系统因尺寸增大导致的翘曲问题及可靠性挑战。

Method: 提出热、翘曲和性能感知的设计框架，通过表面和嵌入式芯片的协同优化平衡设计目标。

Result: 优化后的架构在深度神经网络任务中实现了64.7%的性能提升和40%的能耗降低。

Conclusion: 该框架为多芯片系统提供了可扩展的性能与可靠性解决方案。

Abstract: Multi-chiplet architectures enabled by glass interposer offer superior
electrical performance, enable higher bus widths due to reduced crosstalk, and
have lower capacitance in the redistribution layer than current silicon
interposer-based systems. These advantages result in lower energy per bit,
higher communication frequencies, and extended interconnect range. However,
deformation of the package (warpage) in glass interposer-based systems becomes
a critical challenge as system size increases, leading to severe mechanical
stress and reliability concerns. Beyond a certain size, conventional packaging
techniques fail to manage warpage effectively, necessitating new approaches to
mitigate warpage induced bending with scalable performance for glass interposer
based multi-chiplet systems. To address these inter-twined challenges, we
propose a thermal-, warpage-, and performance-aware design framework that
employs architecture and packaging co-optimization. The proposed framework
disintegrates the surface and embedded chiplets to balance conflicting design
objectives, ensuring optimal trade-offs between performance, power, and
structural reliability. Our experiments demonstrate that optimized
multi-chiplet architectures from our design framework achieve up to 64.7%
performance improvement and 40% power reduction compared to traditional 2.5D
systems to execute deep neural network workloads with lower fabrication costs.

</details>


### [82] [Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving](https://arxiv.org/abs/2507.18454)
*Juntao Zhao,Jiuru Li,Chuan Wu*

Main category: cs.AR

TL;DR: Sandwich是一个针对CPU优化的大型语言模型（LLM）服务引擎，通过区分预填充和解码阶段的执行计划并分别优化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有CPU解决方案忽略了预填充和解码阶段的工作负载差异，采用静态分区和供应商库，效率低下。

Method: 提出Sandwich引擎，针对预填充和解码阶段设计不同执行计划，并分别优化。

Result: 在多种CPU平台上，Sandwich平均吞吐量提升2.01倍，减少了延迟和资源需求，并且在连续批处理服务中表现优异。

Conclusion: Sandwich通过动态优化和高效内核生成，显著提升了CPU服务的效率和性能。

Abstract: Utilizing CPUs to serve large language models (LLMs) is a resource-friendly
alternative to GPU serving. Existing CPU-based solutions ignore workload
differences between the prefill and the decode phases of LLM inference,
applying a static per-NUMA (Non-Uniform Memory Access) node model partition and
utilizing vendor libraries for operator-level execution, which is suboptimal.
We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses
different execution plans for the prefill and decode phases and optimizes them
separately.
  We evaluate Sandwich across diverse baselines and datasets on five CPU
platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.
Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory
time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up
to 3.40x lower requirements in single sequence serving, and significant
improvement in Goodput in continuous-batching serving. The GEMM kernels
generated by Sandwich outperform representative vendor kernels and other
dynamic shape solutions, achieving performance comparable to static compilers
with three orders of magnitude less kernel tuning costs.

</details>


### [83] [PRACtical: Subarray-Level Counter Update and Bank-Level Recovery Isolation for Efficient PRAC Rowhammer Mitigation](https://arxiv.org/abs/2507.18581)
*Ravan Nazaraliyev,Saber Ganjisaffar,Nurlan Nazaraliyev,Nael Abu-Ghazaleh*

Main category: cs.AR

TL;DR: PRACtical优化了DDR5中的PRAC+ABO方案，通过集中计数器更新和银行级缓解措施，提升性能8%，降低能耗19%，同时保证安全。


<details>
  <summary>Details</summary>
Motivation: 随着DRAM密度增加，Rowhammer问题因电荷泄漏加剧而更严重，现行DDR5的PRAC+ABO方案存在性能开销和整体通道暂停的问题。

Method: PRACtical引入集中计数电路以减少延迟，并通过DRAM寄存器实现银行级缓解，仅暂停受攻击的银行。

Result: 性能平均提升8%（最高20%），能耗降低19%，且性能攻击的退化控制在6%以内。

Conclusion: PRACtical在保持Rowhammer防护的同时显著优化了性能和能耗。

Abstract: As DRAM density increases, Rowhammer becomes more severe due to heightened
charge leakage, reducing the number of activations needed to induce bit flips.
The DDR5 standard addresses this threat with in-DRAM per-row activation
counters (PRAC) and the Alert Back-Off (ABO) signal to trigger mitigation.
However, PRAC adds performance overhead by incrementing counters during the
precharge phase, and recovery refreshes stalls the entire memory channel, even
if only one bank is under attack.
  We propose PRACtical, a performance-optimized approach to PRAC+ABO that
maintains the same security guarantees. First, we reduce counter update latency
by introducing a centralized increment circuit, enabling overlap between
counter updates and subsequent row activations in other subarrays. Second, we
enhance the $RFM_{ab}$ mitigation by enabling bank-level granularity: instead
of stalling the entire channel, only affected banks are paused. This is
achieved through a DRAM-resident register that identifies attacked banks.
  PRACtical improves performance by 8% on average (up to 20%) over the
state-of-the-art, reduces energy by 19%, and limits performance degradation
from aggressive performance attacks to less than 6%, all while preserving
Rowhammer protection.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [84] [Building an Accelerated OpenFOAM Proof-of-Concept Application using Modern C++](https://arxiv.org/abs/2507.18268)
*Giulio Malenza,Giovanni Stabile,Filippo Spiga,Robert Birke,Marco Aldinucci*

Main category: cs.MS

TL;DR: 论文探讨了利用GPU和CPU加速高性能计算的方法，通过现代ISO C++并行构造优化OpenFOAM的性能，证明了GPU卸载计算的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前高性能计算中使用GPU等加速器已成为趋势，但如何优化软件以充分利用这些硬件创新仍需研究。本文旨在探索OpenFOAM通过C++并行构造实现GPU加速的可行性。

Method: 采用现代ISO C++并行构造，结合NVIDIA HPC SDK编译器运行时堆栈，实现多核执行和GPU卸载的统一代码库。

Result: 通过GPU卸载计算，成功提升了OpenFOAM中laplacianFoam应用的性能。

Conclusion: 研究表明，通过C++并行构造和GPU加速，可以显著提升OpenFOAM的计算性能，为高性能计算提供了新的优化方向。

Abstract: The modern trend in High-Performance Computing (HPC) involves the use of
accelerators such as Graphics Processing Units (GPUs) alongside Central
Processing Units (CPUs) to speed up numerical operations in various
applications. Leading manufacturers such as NVIDIA, Intel, and AMD are
constantly advancing these architectures, augmenting them with features such as
mixed precision, enhanced memory hierarchies, and specialised accelerator
silicon blocks (e.g., Tensor Cores on GPU or AMX/SME engines on CPU) to enhance
compute performance. At the same time, significant efforts in software
development are aimed at optimizing the use of these innovations, seeking to
improve usability and accessibility. This work contributes to the
state-of-the-art of OpenFOAM development by presenting a working
Proof-Of-Concept application built using modern ISO C++ parallel constructs.
This approach, combined with an appropriate compiler runtime stack, like the
one provided by the NVIDIA HPC SDK, makes it possible to accelerate
well-defined kernels, allowing multi-core execution and GPU offloading using a
single codebase. The study demonstrates that it is possible to increase the
performance of the OpenFOAM laplacianFoam application by offloading the
computations on NVIDIA GPUs using the C++ parallel construct.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [85] [Topology-Preserving Coupling of Compressible Fluids and Thin Deformables](https://arxiv.org/abs/2507.18460)
*Jonathan Panuelos,Eitan Grinspun,David Levin*

Main category: physics.comp-ph

TL;DR: 提出了一种新型的离散化方法，用于耦合可压缩流体与薄变形结构，确保流体域的路径连通性，防止泄漏。


<details>
  <summary>Details</summary>
Motivation: 解决流体与固体耦合中的泄漏问题，特别是在薄变形结构情况下，确保物理模拟的准确性。

Method: 结合约束Voronoi空间划分与Godunov风格有限体积时间积分，精确离散流体域至符合流体-固体界面的单元。

Result: 验证了方法在多种复杂场景下的有效性，如气球充气、香槟瓶塞弹出和超音速小行星，展示了双向能量传递。

Conclusion: 该方法成功解决了泄漏问题，实现了高精度的流体-固体耦合模拟。

Abstract: We present a novel discretization of coupled compressible fluid and thin
deformable structures that provides sufficient and necessary leakproofness by
preserving the path connectedness of the fluid domain. Our method employs a
constrained Voronoi-based spatial partitioning combined with Godunov-style
finite-volume time integration. The fluid domain is discretized into cells that
conform exactly to the fluid-solid interface, allowing boundary conditions to
be sharply resolved exactly at the interface. This enables direct force
exchange between the fluid and solid while ensuring that no fluid leaks through
the solid, even when arbitrarily thin. We validate our approach on a series of
challenging scenarios -- including a balloon propelled by internal compressed
air, a champagne cork ejecting after overcoming friction, and a supersonic
asteroid -- demonstrating bidirectional energy transfer between fluid and
solid.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [86] [Fagin's Theorem for Semiring Turing Machines](https://arxiv.org/abs/2507.18375)
*Guillermo Badia,Manfred Droste,Thomas Eiter,Rafael Kiesel,Carles Noguera,Erik Paul*

Main category: cs.CC

TL;DR: 该论文提出了一个新的定量复杂性类 NPnewinf{R}，并使用加权逻辑形式化方法通过 Fagin 风格的定理对其进行了刻画。


<details>
  <summary>Details</summary>
Motivation: 连接计算复杂性与逻辑表达能力，改进现有的半环图灵机模型。

Method: 引入加权存在二阶逻辑来描述 NPnewinf{R} 类，并与 Eiter & Kiesel 的模型进行比较。

Result: 成功建立了 NPnewinf{R} 的逻辑特征化，并重新验证了 Eiter & Kiesel 的复杂度结果。

Conclusion: 新模型不仅提供了对定量 NP 类的逻辑描述，还能适用于多种计数复杂度类。

Abstract: In recent years, quantitative complexity over semirings has been intensively
investigated. An important problem in this context is to connect computational
complexity with logical expressiveness. In this paper we improve on the model
of \emph{Semiring Turing Machines} (distinct from so called weighted Turing
machines) introduced by Eiter \& Kiesel (Semiring Reasoning Frameworks in AI
and Their Computational Complexity, \emph{J. Artif. Intell. Res.}, 2023). Our
central result is a Fagin-style theorem for a new quantitative complexity class
using a suitable weighted logical formalism. We show that the quantitative
complexity class that we call \NPnewinf{$\mathcal{R}$}, where $\mathcal{R}$ is
a commutative semiring, can be captured using a version of weighted existential
second-order logic that allows for predicates interpreted as semiring-annotated
relations. This result provides a precise logical characterization of the power
series that form the class \NPnewinf{$\mathcal{R}$}. We also give the exact
relation between Eiter \& Kiesel's version of NP, called
\NPoldinf{$\mathcal{R}$}, and the class \NPnewinf{$\mathcal{R}$}. Incidentally,
we are able to recapture all the complexity results by Eiter \& Kiesel (2023)
in our new model, connecting a quantitative version of NP to various counting
complexity classes.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [87] [On the Role of Age and Semantics of Information in Remote Estimation of Markov Sources](https://arxiv.org/abs/2507.18514)
*Jiping Luo,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 研究了语义感知的有限状态马尔可夫链远程估计，结合MAP估计器和AoCE、AoI指标优化传输策略，提出一种混合策略和高效算法Insec-SPI。


<details>
  <summary>Details</summary>
Motivation: 旨在通过优化传输策略提升估计性能，解决传输频率约束下的语义感知问题。

Method: 采用MAP估计器，结合AoCE和AoI指标，将问题建模为CMDP，提出混合策略和Insec-SPI算法。

Result: 混合策略和算法Insec-SPI显著提升了估计性能。

Conclusion: 结合AoI和AoCE能显著优于单独使用任一指标。

Abstract: This paper investigates the semantics-aware remote estimation of a
finite-state Markov chain. We employ the maximum a posteriori (MAP) estimator
and aim to devise a transmission policy to optimize estimation performance
subject to a transmission frequency constraint. We leverage two metrics, namely
the Age of Consecutive Error (AoCE) and the Age of Information (AoI), to
quantify, respectively, the significance of estimation error at the transmitter
and the predictability of outdated information at the receiver. The optimal
transmission problem is formulated as a constrained Markov decision process
(CMDP) with unbounded costs. We show the existence of an optimal simple mixture
policy, which randomly selects between two deterministic switching policies
with a fixed probability. Notably, each switching policy triggers a
transmission only when the AoCE exceeds a threshold value that depends on both
the AoI and the instantaneous estimation error. We further derive sufficient
conditions under which the switching policy reduces to a simple threshold
policy; that is, it admits identical thresholds for all estimation errors.
Leveraging these results, we develop an efficient structure-aware algorithm,
Insec-SPI, that computes the optimal policy with reduced computation overhead.
Our results demonstrate that incorporating both AoI and AoCE yields
significantly improved estimation quality compared to using either metric
alone.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [88] [WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection](https://arxiv.org/abs/2507.18173)
*Haodong Zhu,Wenhao Dong,Linlin Yang,Hong Li,Yuguang Yang,Yangyang Ren,Qingcheng Zhu,Zichao Feng,Changbai Li,Shaohui Lin,Runqi Wang,Xiaoyan Luo,Baochang Zhang*

Main category: cs.CV

TL;DR: 该论文提出了WaveMamba，一种通过离散小波变换分解RGB和红外图像互补特征的跨模态融合方法，结合改进的检测头减少信息损失，显著提升了目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 利用RGB和红外图像的互补特性提升目标检测性能。

Method: 提出WaveMamba融合块，通过低频Mamba融合块和通道交换进行初步融合，再通过门控注意力机制和"绝对最大"融合策略增强高频特征。

Result: 在四个基准测试中平均mAP提升4.5%，超越现有方法。

Conclusion: WaveMamba能有效融合跨模态特征，显著提升目标检测性能。

Abstract: Leveraging the complementary characteristics of visible (RGB) and infrared
(IR) imagery offers significant potential for improving object detection. In
this paper, we propose WaveMamba, a cross-modality fusion method that
efficiently integrates the unique and complementary frequency features of RGB
and IR decomposed by Discrete Wavelet Transform (DWT). An improved detection
head incorporating the Inverse Discrete Wavelet Transform (IDWT) is also
proposed to reduce information loss and produce the final detection results.
The core of our approach is the introduction of WaveMamba Fusion Block (WMFB),
which facilitates comprehensive fusion across low-/high-frequency sub-bands.
Within WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba
framework, first performs initial low-frequency feature fusion with channel
swapping, followed by deep fusion with an advanced gated attention mechanism
for enhanced integration. High-frequency features are enhanced using a strategy
that applies an ``absolute maximum" fusion approach. These advancements lead to
significant performance gains, with our method surpassing state-of-the-art
approaches and achieving average mAP improvements of 4.5% on four benchmarks.

</details>


### [89] [3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation](https://arxiv.org/abs/2507.18625)
*Shuqing Li,Anson Y. Lam,Yun Peng,Wenxuan Wang,Michael R. Lyu*

Main category: cs.CV

TL;DR: 论文提出Scenethesis，一种需求敏感的3D软件合成方法，通过领域专用语言ScenethesisLang桥接自然语言需求和可执行3D软件，实现细粒度修改和复杂约束满足。


<details>
  <summary>Details</summary>
Motivation: 探索3D软件生成领域，解决现有方法无法修改特定元素及处理复杂空间与语义约束的问题。

Method: 采用ScenethesisLang作为中间表示，分解成多阶段操作实现独立验证、目标修改和系统约束满足。

Result: Scenethesis准确捕捉80%以上用户需求，满足90%以上硬约束，同时处理100多个约束，视觉评估得分提升42.8%。

Conclusion: Scenethesis为3D软件合成提供了高效解决方案，支持细粒度控制和复杂约束表达。

Abstract: Graphical user interface (UI) software has undergone a fundamental
transformation from traditional two-dimensional (2D) desktop/web/mobile
interfaces to spatial three-dimensional (3D) environments. While existing work
has made remarkable success in automated 2D software generation, such as
HTML/CSS and mobile app interface code synthesis, the generation of 3D software
still remains under-explored. Current methods for 3D software generation
usually generate the 3D environments as a whole and cannot modify or control
specific elements in the software. Furthermore, these methods struggle to
handle the complex spatial and semantic constraints inherent in the real world.
To address the challenges, we present Scenethesis, a novel
requirement-sensitive 3D software synthesis approach that maintains formal
traceability between user specifications and generated 3D software. Scenethesis
is built upon ScenethesisLang, a domain-specific language that serves as a
granular constraint-aware intermediate representation (IR) to bridge natural
language requirements and executable 3D software. It serves both as a
comprehensive scene description language enabling fine-grained modification of
3D software elements and as a formal constraint-expressive specification
language capable of expressing complex spatial constraints. By decomposing 3D
software synthesis into stages operating on ScenethesisLang, Scenethesis
enables independent verification, targeted modification, and systematic
constraint satisfaction. Our evaluation demonstrates that Scenethesis
accurately captures over 80% of user requirements and satisfies more than 90%
of hard constraints while handling over 100 constraints simultaneously.
Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual
evaluation scores compared to the state-of-the-art method.

</details>


### [90] [SIDA: Synthetic Image Driven Zero-shot Domain Adaptation](https://arxiv.org/abs/2507.18632)
*Ye-Chan Kim,SeungJu Cha,Si-Woo Kim,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: 提出了SIDA方法，利用合成图像进行零样本域适应，显著提高了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本描述的零样本域适应方法难以捕捉复杂真实世界变化且耗时较长，因此探索利用图像数据改进。

Method: 通过生成合成图像（源图像+目标域风格转换）作为目标域代理，并引入Domain Mix和Patch Style Transfer模块建模变化。

Result: 在多样零样本适应场景中表现最优，尤其在挑战性领域，并大幅减少适应时间。

Conclusion: SIDA提供了一种高效且高性能的零样本域适应解决方案。

Abstract: Zero-shot domain adaptation is a method for adapting a model to a target
domain without utilizing target domain image data. To enable adaptation without
target images, existing studies utilize CLIP's embedding space and text
description to simulate target-like style features. Despite the previous
achievements in zero-shot domain adaptation, we observe that these text-driven
methods struggle to capture complex real-world variations and significantly
increase adaptation time due to their alignment process. Instead of relying on
text descriptions, we explore solutions leveraging image data, which provides
diverse and more fine-grained style cues. In this work, we propose SIDA, a
novel and efficient zero-shot domain adaptation method leveraging synthetic
images. To generate synthetic images, we first create detailed, source-like
images and apply image translation to reflect the style of the target domain.
We then utilize the style features of these synthetic images as a proxy for the
target domain. Based on these features, we introduce Domain Mix and Patch Style
Transfer modules, which enable effective modeling of real-world variations. In
particular, Domain Mix blends multiple styles to expand the intra-domain
representations, and Patch Style Transfer assigns different styles to
individual patches. We demonstrate the effectiveness of our method by showing
state-of-the-art performance in diverse zero-shot adaptation scenarios,
particularly in challenging domains. Moreover, our approach achieves high
efficiency by significantly reducing the overall adaptation time.

</details>


### [91] [Real-Time Object Detection and Classification using YOLO for Edge FPGAs](https://arxiv.org/abs/2507.18174)
*Rashed Al Amin,Roman Obermaisser*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv5优化的资源高效实时目标检测分类系统，适用于FPGA边缘计算，实现了99%的准确率和3.5W的低功耗。


<details>
  <summary>Details</summary>
Motivation: 现有YOLO方法在资源效率上仍难以满足边缘FPGA平台的需求，因此需优化以实现高效实时检测。

Method: 采用YOLOv5优化模型，在COCO和GTSRD数据集上训练，并在Xilinx Kria KV260 FPGA板上部署。

Result: 实验结果显示99%的分类准确率，功耗3.5W，处理速度9 FPS。

Conclusion: 该方法在边缘计算中实现了高效实时的目标检测分类，具备资源效率和低功耗优势。

Abstract: Object detection and classification are crucial tasks across various
application domains, particularly in the development of safe and reliable
Advanced Driver Assistance Systems (ADAS). Existing deep learning-based methods
such as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and
You Only Look Once (YOLO) have demonstrated high performance in terms of
accuracy and computational speed when deployed on Field-Programmable Gate
Arrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based
object detection and classification systems continue to face challenges in
achieving resource efficiency suitable for edge FPGA platforms. To address this
limitation, this paper presents a resource-efficient real-time object detection
and classification system based on YOLOv5 optimized for FPGA deployment. The
proposed system is trained on the COCO and GTSRD datasets and implemented on
the Xilinx Kria KV260 FPGA board. Experimental results demonstrate a
classification accuracy of 99%, with a power consumption of 3.5W and a
processing speed of 9 frames per second (FPS). These findings highlight the
effectiveness of the proposed approach in enabling real-time,
resource-efficient object detection and classification for edge computing
applications.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [92] [Quantum Machine Learning Playground](https://arxiv.org/abs/2507.17931)
*Pascal Debus,Sebastian Issel,Kilian Tscharke*

Main category: quant-ph

TL;DR: 本文介绍了一种创新的交互式可视化工具，旨在简化量子机器学习(QML)算法的理解。


<details>
  <summary>Details</summary>
Motivation: 受经典机器学习可视化工具(如TensorFlow Playground)成功的启发，本文旨在填补QML领域可视化工具的空白。

Method: 通过结合量子计算和经典机器学习的可视化隐喻，开发了一个算法可视化概念，并设计了一个交互式网页应用的具体实现。

Result: 该工具以数据重新上传通用量子分类器为代表，降低了量子计算的入门门槛，并提出了首个QML学习探索平台的初步版本。

Conclusion: 该工具为QML领域的进一步创新提供了支持。

Abstract: This article introduces an innovative interactive visualization tool designed
to demystify quantum machine learning (QML) algorithms. Our work is inspired by
the success of classical machine learning visualization tools, such as
TensorFlow Playground, and aims to bridge the gap in visualization resources
specifically for the field of QML. The article includes a comprehensive
overview of relevant visualization metaphors from both quantum computing and
classical machine learning, the development of an algorithm visualization
concept, and the design of a concrete implementation as an interactive web
application. By combining common visualization metaphors for the so-called data
re-uploading universal quantum classifier as a representative QML model, this
article aims to lower the entry barrier to quantum computing and encourage
further innovation in the field. The accompanying interactive application is a
proposal for the first version of a quantum machine learning playground for
learning and exploring QML models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [93] [Neuromorphic Computing: A Theoretical Framework for Time, Space, and Energy Scaling](https://arxiv.org/abs/2507.17886)
*James B Aimone*

Main category: cs.NE

TL;DR: 该论文探讨了神经形态计算（NMC）作为传统冯·诺依曼架构的低功耗替代方案，并分析了其时间和空间缩放特性及能量效率的独特性。


<details>
  <summary>Details</summary>
Motivation: 研究NMC作为一种通用且可编程的计算架构，尽管其与传统存储程序架构差异显著，但具有独特的优势。

Method: 通过对比NMC与传统架构在时间、空间和能量缩放的差异，论证其适用性。

Result: NMC在能量效率上优于传统系统，特别适合稀疏算法和迭代优化等任务。

Conclusion: NMC因其独特特性，在某些算法类别中比传统多核系统更具优势，尤其适合处理稀疏和可扩展的问题。

Abstract: Neuromorphic computing (NMC) is increasingly viewed as a low-power
alternative to conventional von Neumann architectures such as central
processing units (CPUs) and graphics processing units (GPUs), however the
computational value proposition has been difficult to define precisely.
  Here, we explain how NMC should be seen as general-purpose and programmable
even though it differs considerably from a conventional stored-program
architecture. We show that the time and space scaling of NMC is equivalent to
that of a theoretically infinite processor conventional system, however the
energy scaling is significantly different. Specifically, the energy of
conventional systems scales with absolute algorithm work, whereas the energy of
neuromorphic systems scales with the derivative of algorithm state. The unique
characteristics of NMC architectures make it well suited for different classes
of algorithms than conventional multi-core systems like GPUs that have been
optimized for dense numerical applications such as linear algebra. In contrast,
the unique characteristics of NMC make it ideally suited for scalable and
sparse algorithms whose activity is proportional to an objective function, such
as iterative optimization and large-scale sampling (e.g., Monte Carlo).

</details>


### [94] [Explicit Sign-Magnitude Encoders Enable Power-Efficient Multipliers](https://arxiv.org/abs/2507.18179)
*Felix Arnold,Maxence Bouvier,Ryan Amaudruz,Renzo Andri,Lukas Cavigelli*

Main category: cs.NE

TL;DR: 提出一种通过将定点乘法器分解为子组件来最大化其能效的方法，利用符号幅度编码提高乘法效率，优化后功率显著降低。


<details>
  <summary>Details</summary>
Motivation: AI工作负载中常见输入值围绕零分布，因此需要优化定点乘法器的功率效率。

Method: 将乘法器分解为编码器和乘法模块，分别优化，利用符号幅度编码的优势。

Result: 在4位乘法器中，功率降低可达12.9%（标准输入分布）或33%（受限输入范围），并展示进一步优化潜力。

Conclusion: 该方法有效提升功率效率且保持逻辑等效性，适合生产系统应用。

Abstract: This work presents a method to maximize power-efficiency of fixed point
multiplier units by decomposing them into sub-components. First, an encoder
block converts the operands from a two's complement to a sign magnitude
representation, followed by a multiplier module which performs the compute
operation and outputs the resulting value in the original format. This allows
to leverage the power-efficiency of the Sign Magnitude encoding for the
multiplication. To ensure the computing format is not altered, those two
components are synthesized and optimized separately. Our method leads to
significant power savings for input values centered around zero, as commonly
encountered in AI workloads. Under a realistic input stream with values
normally distributed with a standard deviation of 3.0, post-synthesis
simulations of the 4-bit multiplier design show up to 12.9% lower switching
activity compared to synthesis without decomposition. Those gains are achieved
while ensuring compliance into any production-ready system as the overall
circuit stays logic-equivalent. With the compliance lifted and a slightly
smaller input range of -7 to +7, switching activity reductions can reach up to
33%. Additionally, we demonstrate that synthesis optimization methods based on
switching-activity-driven design space exploration can yield a further 5-10%
improvement in power-efficiency compared to a power agnostic approach.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [95] [Formal Verification of the Safegcd Implementation](https://arxiv.org/abs/2507.17956)
*Russell O'Connor,Andrew Poelstra*

Main category: cs.CR

TL;DR: 本文介绍了对比特币中使用的libsecp256k1库的模逆算法正确性的计算机验证证明。


<details>
  <summary>Details</summary>
Motivation: 由于Bernstein和Yang提出的新算法可能引入错误，需要确保其正确性以保证数字签名的安全性。

Method: 使用Coq证明助手和可验证C的分离逻辑实现，完成了对libsecp256k1模逆算法正确性的验证。

Result: 成功验证了算法的正确性。

Conclusion: 通过形式化验证，确保了模逆算法的可靠性，增加了比特币等应用的安全性。

Abstract: The modular inverse is an essential piece of computation required for
elliptic curve operations used for digital signatures in Bitcoin and other
applications. A novel approach to the extended Euclidean algorithm has been
developed by Bernstein and Yang within the last few years and incorporated into
the libsecp256k1 cryptographic library used by Bitcoin. However, novel
algorithms introduce new risks of errors. To address this we have completed a
computer verified proof of the correctness of (one of) libsecp256k1's modular
inverse implementations with the Coq proof assistant using the Verifiable C's
implementation of separation logic.

</details>


### [96] [MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection](https://arxiv.org/abs/2507.17978)
*Paulo Mendes,Eva Maia,Isabel Praça*

Main category: cs.CR

TL;DR: 提出了MeAJOR Corpus数据集，整合多源钓鱼邮件样本，通过多样化的特征设计和分类模型实验，验证其在钓鱼检测中的高效性，F1达到98.34%。


<details>
  <summary>Details</summary>
Motivation: 现有钓鱼邮件数据集在质量和多样性上存在不足，限制了机器学习模型的检测性能，需要更全面的数据集以提升研究效果。

Method: 整合来自多源的135894个钓鱼和正常邮件样本，设计多样化特征，并通过RF、XGB、MLP和CNN四种分类模型验证数据集的性能。

Result: 实验结果显示XGB模型表现最佳，F1得分达98.34%，数据集解决了类别不平衡、泛化性和可重现性问题。

Conclusion: MeAJOR Corpus是一个高效且可复用的钓鱼检测数据集，显著提升了研究的质量和可靠性。

Abstract: Phishing emails continue to pose a significant threat to cybersecurity by
exploiting human vulnerabilities through deceptive content and malicious
payloads. While Machine Learning (ML) models are effective at detecting
phishing threats, their performance largely relies on the quality and diversity
of the training data. This paper presents MeAJOR (Merged email Assets from
Joint Open-source Repositories) Corpus, a novel, multi-source phishing email
dataset designed to overcome critical limitations in existing resources. It
integrates 135894 samples representing a broad number of phishing tactics and
legitimate emails, with a wide spectrum of engineered features. We evaluated
the dataset's utility for phishing detection research through systematic
experiments with four classification models (RF, XGB, MLP, and CNN) across
multiple feature configurations. Results highlight the dataset's effectiveness,
achieving 98.34% F1 with XGB. By integrating broad features from multiple
categories, our dataset provides a reusable and consistent resource, while
addressing common challenges like class imbalance, generalisability and
reproducibility.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [97] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 提出一种自动化医疗数据处理和模型部署的AI框架，通过任务专属代理减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 解决医疗领域机器学习部署效率低、成本高的问题。

Method: 采用模块化代理系统，实现从数据输入到推理的全流程自动化，包括特征提取、模型选择和预处理。

Result: 在老年病、姑息治疗和结肠镜图像等数据集上验证了框架的有效性。

Conclusion: 该框架显著提高AI在临床应用的可扩展性和成本效益。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [98] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: 研究使用均值聚合函数的图神经网络(GNNs)的表达能力，发现其在非均匀设置下与比率模态逻辑等价，而均匀设置下低于max和sum聚合GNNs。


<details>
  <summary>Details</summary>
Motivation: 探索均值聚合GNNs的表达能力，并与sum和max聚合进行比较，以理解其在不同设置下的性能差异。

Method: 通过模态逻辑和比率模态逻辑的理论分析，对比不同聚合函数在不同设置下的表达能力。

Result: 均值GNNs在非均匀设置下与比率模态逻辑等价，但在均匀设置下表达能力低于sum和max聚合GNNs。

Conclusion: 均值GNNs的表达能力取决于设置和参数假设，为进一步优化提供了理论依据。

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [99] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: 研究探讨了图表是否对AI系统有帮助，实验表明视觉语言模型在图表辅助下能更精确分析数据。


<details>
  <summary>Details</summary>
Motivation: 探索图表是否能像帮助人类一样对AI系统分析数据起到积极作用。

Method: 使用GPT 4.1和Claude 3.5两种商业视觉语言模型，在三项分析任务中对比图表辅助的效果。

Result: 图表（尤其是散点图）能显著提升AI系统对复杂数据集的分析精确性。

Conclusion: AI系统可以像人类一样从可视化中受益，图表内容对其性能提升起关键作用。

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [100] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: VeriMinder是一个交互式系统，用于检测和减轻自然语言数据库查询中的认知偏差，通过上下文语义映射和优化的LLM技术支持用户进行无偏数据分析。


<details>
  <summary>Details</summary>
Motivation: 帮助没有统计背景的用户在使用自然语言数据库查询系统时避免认知偏差，提升数据分析的质量。

Method: 引入上下文语义映射框架、基于Hard-to-Vary原则的分析框架，以及优化LLM生成任务特定提示的系统。

Result: 用户测试显示82.5%的参与者认为系统提升了分析质量，VeriMinder在具体性、全面性和准确性上比替代方法高20%。

Conclusion: VeriMinder有效避免了数据分析中的“错误问题”风险，并开源以促进社区研究和采用。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [101] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: 研究了Wikipedia多语言版本中表格数据的不一致性，并提出了一种方法来收集、对齐和分析这些数据，以评估多语言对齐情况。


<details>
  <summary>Details</summary>
Motivation: Wikipedia不同语言版本的内容独立编写和更新，导致事实不一致，可能影响其中立性和可靠性，尤其是AI系统常将其作为训练数据。

Method: 开发了一种方法来收集、对齐和分析Wikipedia多语言文章中的表格数据，定义不一致类别，并应用定量和定性指标评估多语言对齐。

Result: 通过样本数据集的分析，揭示了多语言表格数据的不一致性，并提供了相关见解。

Conclusion: 研究结果对事实验证、多语言知识交互以及设计可靠的AI系统具有重要参考价值。

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [102] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 比较先进LLM与微调模型在道德推理任务上的表现，发现LLM在道德内容检测上表现不佳，微调模型仍占优。


<details>
  <summary>Details</summary>
Motivation: 分析LLM在道德推理任务上的表现，为伦理对齐AI系统提供依据。

Method: 使用ROC、PR和DET曲线对比LLM与微调模型在Twitter和Reddit数据集上的表现。

Result: LLM在道德内容检测上表现较差，假阴性率高，系统性地低估道德内容。

Conclusion: 在道德推理任务中，微调模型仍优于提示工程LLM。

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [103] [Weaving the Future: Generative AI and the Reimagining of Fashion Design](https://arxiv.org/abs/2507.17758)
*Pierre-Marie Chauvin,Angèle Merlin,Xavier Fresquet,Hugo Caselles-Dupré,Benjamin Simmenauer,Mathieu de Fayet*

Main category: cs.CY

TL;DR: 探讨生成式AI在时装设计中的应用及其对创意流程、伦理和文化的影响


<details>
  <summary>Details</summary>
Motivation: 研究AI如何重塑时装设计的创意流程，并探讨其伦理、美学和劳工问题

Method: 基于2025年1月研讨会‘Tisser le futur’的见解进行分析

Result: 揭示了人类与机器的协同创造潜力、美学创新可能性及算法设计的环境与文化挑战

Conclusion: 生成式AI在时装设计中展现出创新潜力，但也需关注伦理和文化挑战

Abstract: This paper explores the integration of generative AI into the fashion design
process. Drawing on insights from the January 2025 seminar ``Tisser le futur,''
it investigates how AI reshapes creative workflows, from ideation to
prototyping, while interrogating the ethical, aesthetic, and labor
implications. The paper highlights co-creative dynamics between humans and
machines, the potential for aesthetic innovation, and the environmental and
cultural challenges of algorithmic design.

</details>


### [104] [How Instructional Sequence and Personalized Support Impact Diagnostic Strategy Learning](https://arxiv.org/abs/2507.17760)
*Fatma Betül Güreş,Tanya Nazaretsky,Bahar Radmehr,Martina Rau,Tanja Käser*

Main category: cs.CY

TL;DR: 研究探讨如何在药学技术员学徒的在线场景学习中优化教学顺序，发现解决问题后提供策略指导（PS-I）比之前提供（I-PS）更能提升迁移任务表现。


<details>
  <summary>Details</summary>
Motivation: 学生在诊断推理中易受认知偏差影响，场景学习虽有效，但教学顺序的最佳实践尚不明确。

Method: 在PharmaSim环境中采用组间设计，比较策略指导前（I-PS）与后（PS-I）的效果。

Result: 两种教学顺序均有益，但PS-I在迁移任务中显著更优。

Conclusion: 教学顺序对学习迁移有重要影响，PS-I更适合提升实际应用能力。

Abstract: Supporting students in developing effective diagnostic reasoning is a key
challenge in various educational domains. Novices often struggle with cognitive
biases such as premature closure and over-reliance on heuristics.
Scenario-based learning (SBL) can address these challenges by offering
realistic case experiences and iterative practice, but the optimal sequencing
of instruction and problem-solving activities remains unclear. This study
examines how personalized support can be incorporated into different
instructional sequences and whether providing explicit diagnostic strategy
instruction before (I-PS) or after problem-solving (PS-I) improves learning and
its transfer. We employ a between-groups design in an online SBL environment
called PharmaSim, which simulates real-world client interactions for pharmacy
technician apprentices. Results indicate that while both instruction types are
beneficial, PS-I leads to significantly higher performance in transfer tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [105] [ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation](https://arxiv.org/abs/2507.18262)
*Chenyu Su,Weiwei Shang,Chen Qian,Fei Zhang,Shuang Cong*

Main category: cs.RO

TL;DR: 该论文提出了一种名为ReSem3D的框架，通过结合视觉基础模型和大型语言模型，实现了细粒度的视觉定位和实时操作。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语义建模、实时规划和环境适应性方面存在不足，ReSem3D旨在解决这些问题。

Method: 采用分层递归推理，将自然语言指令和RGB-D观测转化为3D空间约束，并将其编码为实时优化目标。

Result: 在模拟和真实环境中，ReSem3D展示了强大的适应性和泛化能力。

Conclusion: ReSem3D在语义多样性环境中表现出色，为机器人操作提供了新思路。

Abstract: Semantics-driven 3D spatial constraints align highlevel semantic
representations with low-level action spaces, facilitating the unification of
task understanding and execution in robotic manipulation. The synergistic
reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation
Models (VFMs) enables cross-modal 3D spatial constraint construction.
Nevertheless, existing methods have three key limitations: (1) coarse semantic
granularity in constraint modeling, (2) lack of real-time closed-loop planning,
(3) compromised robustness in semantically diverse environments. To address
these challenges, we propose ReSem3D, a unified manipulation framework for
semantically diverse environments, leveraging the synergy between VFMs and
MLLMs to achieve fine-grained visual grounding and dynamically constructs
hierarchical 3D spatial constraints for real-time manipulation. Specifically,
the framework is driven by hierarchical recursive reasoning in MLLMs, which
interact with VFMs to automatically construct 3D spatial constraints from
natural language instructions and RGB-D observations in two stages: part-level
extraction and region-level refinement. Subsequently, these constraints are
encoded as real-time optimization objectives in joint space, enabling reactive
behavior to dynamic disturbances. Extensive simulation and real-world
experiments are conducted in semantically rich household and sparse chemical
lab environments. The results demonstrate that ReSem3D performs diverse
manipulation tasks under zero-shot conditions, exhibiting strong adaptability
and generalization. Code and videos at https://resem3d.github.io.

</details>
