<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 13]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.DS](#cs.DS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Survey of AIOps in the Era of Large Language Models](https://arxiv.org/abs/2507.12472)
*Lingzhe Zhang,Tong Jia,Mengxi Jia,Yifan Wu,Aiwei Liu,Yong Yang,Zhonghai Wu,Xuming Hu,Philip S. Yu,Ying Li*

Main category: cs.SE

TL;DR: 论文综述了LLMs在AIOps中的应用，分析183篇论文回答4个关键问题，总结了现状、差距和未来方向。


<details>
  <summary>Details</summary>
Motivation: 填补对LLMs在AIOps中影响、潜力和局限性理解的空白。

Method: 调查183篇2020-2024年的论文，聚焦四个研究问题（数据源、任务演化、方法和评估）。

Result: 总结了LLM4AIOps的最新技术、趋势和研究不足。

Conclusion: 提出未来研究方向，推动LLMs在AIOps中的进一步应用。

Abstract: As large language models (LLMs) grow increasingly sophisticated and
pervasive, their application to various Artificial Intelligence for IT
Operations (AIOps) tasks has garnered significant attention. However, a
comprehensive understanding of the impact, potential, and limitations of LLMs
in AIOps remains in its infancy. To address this gap, we conducted a detailed
survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve
outcomes in this domain. We analyzed 183 research papers published between
January 2020 and December 2024 to answer four key research questions (RQs). In
RQ1, we examine the diverse failure data sources utilized, including advanced
LLM-based processing techniques for legacy data and the incorporation of new
data sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,
highlighting the emergence of novel tasks and the publication trends across
these tasks. RQ3 investigates the various LLM-based methods applied to address
AIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to
assess LLM-integrated AIOps approaches. Based on our findings, we discuss the
state-of-the-art advancements and trends, identify gaps in existing research,
and propose promising directions for future exploration.

</details>


### [2] [LLM-Powered Quantum Code Transpilation](https://arxiv.org/abs/2507.12480)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 利用大型语言模型（LLMs）作为灵活的跨平台量子软件转换工具


<details>
  <summary>Details</summary>
Motivation: 现有量子软件开发工具包（QSDKs）间的多样性导致互操作性和跨平台开发困难

Method: 利用LLMs的预训练知识和上下文推理能力，实现无编程语言依赖的代码转换

Result: LLMs可作为通用转换工具，无需手动定义规则，实现功能等效的代码转换

Conclusion: 这一工作推动了量子计算生态系统中智能通用转换技术的发展

Abstract: There exist various Software Development Kits (SDKs) tailored to different
quantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples
include but are not limited to Qiskit, Cirq, and PennyLane. However, this
diversity presents significant challenges for interoperability and
cross-platform development of hybrid quantum-classical software systems.
Traditional rule-based transpilers for translating code between QSDKs are
time-consuming to design and maintain, requiring deep expertise and rigid
mappings in the source and destination code. In this study, we explore the use
of Large Language Models (LLMs) as a flexible and automated solution.
Leveraging their pretrained knowledge and contextual reasoning capabilities, we
position LLMs as programming language-agnostic transpilers capable of
converting quantum programs from one QSDK to another while preserving
functional equivalence. Our approach eliminates the need for manually defined
transformation rules and offers a scalable solution to quantum software
portability. This work represents a step toward enabling intelligent,
general-purpose transpilation in the quantum computing ecosystem.

</details>


### [3] [Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding](https://arxiv.org/abs/2507.12482)
*Ishraq Khan,Assad Chowdary,Sharoz Haseeb,Urvish Patel*

Main category: cs.SE

TL;DR: Kodezi Chronos是一种新型架构，专为超长代码上下文设计，支持高效的代码理解、调试和维护，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在代码生成和自动化中的上下文限制和结构推理不足问题。

Method: 采用多级嵌入内存引擎，结合向量和图索引的检索方法。

Result: 在真实世界的bug检测中表现优于现有模型，调试周期减少40%。

Conclusion: Kodezi Chronos为自持、持续优化的软件生态系统提供了关键进展。

Abstract: Large Language Models (LLMs) have advanced code generation and software
automation, but are fundamentally constrained by limited inference-time context
and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a
next-generation architecture for autonomous code understanding, debugging, and
maintenance, designed to operate across ultra-long contexts comprising entire
codebases, histories, and documentation, all without fixed window limits.
Kodezi Chronos leverages a multi-level embedding memory engine, combining
vector and graph-based indexing with continuous code-aware retrieval. This
enables efficient and accurate reasoning over millions of lines of code,
supporting repository-scale comprehension, multi-file refactoring, and
real-time self-healing actions. Our evaluation introduces a novel Multi Random
Retrieval benchmark, specifically tailored to the software engineering domain.
Unlike classical retrieval benchmarks, this method requires the model to
resolve arbitrarily distant and obfuscated associations across code artifacts,
simulating realistic tasks such as variable tracing, dependency migration, and
semantic bug localization. Chronos outperforms prior LLMs and code models,
demonstrating a 23% improvement in real-world bug detection and reducing
debugging cycles by up to 40% compared to traditional sequence-based
approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos
enables seamless, autonomous software maintenance, elevating code reliability
and productivity while reducing manual effort. These results mark a critical
advance toward self-sustaining, continuously optimized software ecosystems.

</details>


### [4] [A Survey of Reinforcement Learning for Software Engineering](https://arxiv.org/abs/2507.12483)
*Dong Wang,Hanmo You,Lingwei Zhu,Kaiwei Lin,Zheng Chen,Chen Yang,Junji Yu,Zan Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 本文对强化学习（RL）在软件工程（SE）领域的应用进行了首次系统性调查，回顾了115项研究，分析了趋势、算法分类及关键因素，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域日益复杂且自动化需求增加，促使研究者探索强化学习的应用，但缺乏系统性调查。

Method: 综述了115项同行评审研究，分析发表趋势、SE主题与RL算法分类，以及数据集使用、模型设计和评估实践。

Result: 提供了RL在SE领域的系统性图谱，总结了当前应用现状，并提出了未来研究挑战和方向。

Conclusion: 该调查填补了RL-for-SE领域的系统性空白，旨在支持研究者和从业者推动该领域发展。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential
decision-making and has attracted growing interest across various domains,
particularly following the advent of Deep Reinforcement Learning (DRL) in 2015.
Simultaneously, the rapid advancement of Large Language Models (LLMs) has
further fueled interest in integrating RL with LLMs to enable more adaptive and
intelligent systems. In the field of software engineering (SE), the increasing
complexity of systems and the rising demand for automation have motivated
researchers to apply RL to a broad range of tasks, from software design and
development to quality assurance and maintenance. Despite growing research in
RL-for-SE, there remains a lack of a comprehensive and systematic survey of
this evolving field. To address this gap, we reviewed 115 peer-reviewed studies
published across 22 premier SE venues since the introduction of DRL. We
conducted a comprehensive analysis of publication trends, categorized SE topics
and RL algorithms, and examined key factors such as dataset usage, model design
and optimization, and evaluation practices. Furthermore, we identified open
challenges and proposed future research directions to guide and inspire ongoing
work in this evolving area. To summarize, this survey offers the first
systematic mapping of RL applications in software engineering, aiming to
support both researchers and practitioners in navigating the current landscape
and advancing the field. Our artifacts are publicly available:
https://github.com/KaiWei-Lin-lanina/RL4SE.

</details>


### [5] [When Retriever Meets Generator: A Joint Model for Code Comment Generation](https://arxiv.org/abs/2507.12558)
*Tien P. T. Le,Anh M. T. Bui,Huy N. D. Pham,Alessio Bucaioni,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: RAGSum是一种结合检索与生成的代码注释生成方法，通过CodeT5框架实现高效推荐，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 自动化生成代码注释可以减少文档编写工作量并加速程序理解，现有检索增强方法中检索与生成分离导致噪声传播。

Method: 提出RAGSum方法，基于CodeT5框架，通过对比预训练和端到端训练结合检索与生成，并引入轻量级自优化循环。

Result: 在Java、Python、C三种语言的基准测试中，RAGSum在BLEU、METEOR和ROUTE-L指标上显著优于基线方法。

Conclusion: 紧密耦合检索与生成可以提升注释自动化的性能，未来需要进一步复现和开发者定性研究。

Abstract: Automatically generating concise, informative comments for source code can
lighten documentation effort and accelerate program comprehension.
Retrieval-augmented approaches first fetch code snippets with existing comments
and then synthesize a new comment, yet retrieval and generation are typically
optimized in isolation, allowing irrelevant neighbors topropagate noise
downstream. To tackle the issue, we propose a novel approach named RAGSum with
the aim of both effectiveness and efficiency in recommendations. RAGSum is
built on top offuse retrieval and generation using a single CodeT5 backbone. We
report preliminary results on a unified retrieval-generation framework built on
CodeT5. A contrastive pre-training phase shapes code embeddings for
nearest-neighbor search; these weights then seed end-to-end training with a
composite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes
comment-generation error. More importantly, a lightweight self-refinement loop
is deployed to polish the final output. We evaluated theframework on three
cross-language benchmarks (Java, Python, C), and compared it with three
well-established baselines. The results show that our approach substantially
outperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These
findings indicate that tightly coupling retrieval and generationcan raise the
ceiling for comment automation and motivateforthcoming replications and
qualitative developer studies.

</details>


### [6] [ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells](https://arxiv.org/abs/2507.12561)
*Samal Nursapa,Anastassiya Samuilova,Alessio Bucaioni. Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 论文探讨了使用预训练变换器模型（如CodeBERT和CodeT5）为检测到的架构异味推荐合适重构方法的有效性。CodeT5表现优于CodeBERT和传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具能检测架构异味（如God Class、Cyclic Dependency等），但很少提供修复建议，影响了软件质量和可维护性。

Method: 将任务建模为三分类问题，并在11,149个开源Java项目的200多万条重构实例上微调CodeBERT和CodeT5。

Result: CodeT5达到96.9%准确率和95.2% F1分数，优于CodeBERT和传统方法。

Conclusion: 变换器模型能有效填补异味检测和具体修复建议之间的空白，为未来重构推荐系统奠定基础。所有资源和数据均已开源。

Abstract: Architectural smells such as God Class, Cyclic Dependency, and Hub-like
Dependency degrade software quality and maintainability. Existing tools detect
such smells but rarely suggest how to fix them. This paper explores the use of
pre-trained transformer models--CodeBERT and CodeT5--for recommending suitable
refactorings based on detected smells. We frame the task as a three-class
classification problem and fine-tune both models on over 2 million refactoring
instances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%
accuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our
results show that transformer-based models can effectively bridge the gap
between smell detection and actionable repair, laying the foundation for future
refactoring recommendation systems. We release all code, models, and data under
an open license to support reproducibility and further research.

</details>


### [7] [QSpark: Towards Reliable Qiskit Code Generation](https://arxiv.org/abs/2507.12642)
*Kiana Kheiri,Aamna Aamir,Andriy Miranskyy,Chen Ding*

Main category: cs.SE

TL;DR: 该论文研究了如何通过强化学习方法（GRPO和ORPO）优化量子电路的代码生成，显著提升了模型在Qiskit HumanEval基准上的表现，但仍未解决高级任务。


<details>
  <summary>Details</summary>
Motivation: 解决量子电路代码生成的错误问题，提升AI在量子编程中的辅助能力。

Method: 使用GRPO和ORPO两种强化学习方法对32 B模型进行微调，并利用标注丰富的合成数据集。

Result: ORPO在Qiskit HumanEval上达到56.29% Pass@1，GRPO为49%，均超过通用基线；在原始HumanEval上分别为65.90%和63.00%。GRPO擅长基础任务，ORPO在中级任务上表现更好，但两者均未解决高级任务。

Conclusion: 尽管在AI辅助量子编程方面取得了显著进展，但在解决高级任务上仍有提升空间。

Abstract: Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and
StarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two
RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference
Optimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit
HumanEval benchmark, ORPO reaches 56.29\% Pass@1 ($\approx+10$ pp over
Granite-8B-QK) and GRPO hits 49\%, both beating all general-purpose baselines;
on the original HumanEval they score 65.90\% and 63.00\%. GRPO excels on basic
tasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five
advanced tasks, highlighting clear gains yet room for progress in AI-assisted
quantum programming.

</details>


### [8] [A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain](https://arxiv.org/abs/2507.12649)
*Christine van Stiphoudt,Sergio Potenciano Menci,Gilbert Fridgen*

Main category: cs.SE

TL;DR: 摘要讨论了智能电网数字化进程中新兴信息与数据模型的评估方法，提出了一种结合显式和隐式评估的三阶段方法，填补了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 智能电网数字化导致信息交换增加，现有模型不足需要新评估方法以防止潜在问题。现有评估方法要么过于笼统，要么仅针对已使用模型的测试。

Method: 设计了一种结合显式和隐式评估的三阶段方法，基于设计科学研究，并在工业灵活性描述的信息和数据模型开发中进行了应用和优化。

Result: 提出了一种适用于新模型开发的评估方法，并通过案例展示了其有效性，同时总结了经验教训。

Conclusion: 该方法填补了智能电网领域新模型评估的空白，为未来研究提供了实践指导。

Abstract: The ongoing digitalisation of the smart grid is resulting in an increase in
automated information exchanges across distributed energy systems. This process
has led to the development of new information and data models when the existing
ones fall short. To prevent potential disruptions caused by flaws in the newly
designed information and data models, it is essential to evaluate them during
the design process before they are implemented in operation.
  Currently, general explicit evaluation approaches outside the smart grid
domain stay at a high level without defining clear steps. Meanwhile, implicit
evaluation approaches in the smart grid domain focus on testing systems that
utilise information and data models already in use for functionality in terms
of conformance and interoperability. Notably, no combination of explicit and
implicit evaluation approaches for newly designed information and data models
offers a clearly defined set of steps during their design process in the smart
grid context.
  Consequently, we design a three-phase evaluation approach using design
science research to address this gap. Our evaluation approach combines explicit
and implicit evaluation methods and is applicable when developing new
information and data models. We use the development of an information model and
data model focused on industrial flexibility descriptions to refine our
evaluation approach. Additionally, we provide lessons learned from our
experience.

</details>


### [9] [A Fuzzy Approach to Project Success: Measuring What Matters](https://arxiv.org/abs/2507.12653)
*João Granja-Correia,Remedios Hernández-Linares,Luca Ferranti,Arménio Rego*

Main category: cs.SE

TL;DR: 提出了一种基于模糊逻辑的新型项目成功评估方法，动态考虑终端用户的持续积极影响。


<details>
  <summary>Details</summary>
Motivation: 传统Likert量表忽视了项目成功的多层次和上下文依赖性，需要更精准的评估工具。

Method: 采用分层Type-1 Mamdani模糊系统，突出终端用户的持续积极影响。

Result: 该方法可能更准确衡量项目成功，并适用于复杂评估场景。

Conclusion: 未来研究将进行实证测试并拓展模糊逻辑在社会科学中的应用。

Abstract: This paper introduces a novel approach to project success evaluation by
integrating fuzzy logic into an existing construct. Traditional Likert-scale
measures often overlook the context-dependent and multifaceted nature of
project success. The proposed hierarchical Type-1 Mamdani fuzzy system
prioritizes sustained positive impact for end-users, reducing emphasis on
secondary outcomes like stakeholder satisfaction and internal project success.
This dynamic approach may provide a more accurate measure of project success
and could be adaptable to complex evaluations. Future research will focus on
empirical testing and broader applications of fuzzy logic in social science.

</details>


### [10] [Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development](https://arxiv.org/abs/2507.12665)
*Salvador D. Escobedo*

Main category: cs.SE

TL;DR: 提出了单对话方法论（SCM），一种使用大语言模型（LLM）的实用软件开发方法，强调结构化、持久化的开发对话。


<details>
  <summary>Details</summary>
Motivation: 当前实践中对大语言模型的被动依赖需要修正，重新确立开发者作为智能工具架构者和监督者的角色。

Method: SCM基于认知清晰性、可追溯性、模块化和文档化原则，定义开发阶段、最佳实践和哲学立场。

Result: SCM提供了一种从需求到架构再到实现的单对话开发框架。

Conclusion: SCM是对当前LLM使用实践的改进，强调了开发者的主动性和结构化开发的重要性。

Abstract: We propose the Single Conversation Methodology (SCM), a novel and pragmatic
approach to software development using large language models (LLMs). In
contrast to ad hoc interactions with generative AI, SCM emphasizes a structured
and persistent development dialogue, where all stages of a project - from
requirements to architecture and implementation - unfold within a single,
long-context conversation. The methodology is grounded on principles of
cognitive clarity, traceability, modularity, and documentation. We define its
phases, best practices, and philosophical stance, while arguing that SCM offers
a necessary correction to the passive reliance on LLMs prevalent in current
practices. We aim to reassert the active role of the developer as architect and
supervisor of the intelligent tool.

</details>


### [11] [Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases](https://arxiv.org/abs/2507.13035)
*Keila Lucas,Rohit Gheyi,Márcio Ribeiro,Fabio Palomba,Luana Martins,Elvys Soares*

Main category: cs.SE

TL;DR: 小型语言模型（SLMs）如Phi-4、Gemma3和Llama3.2在自动检测测试异味方面表现出色，无需依赖大量手动规则定义，并能自主解释问题和提出改进建议，提升测试质量和效率。


<details>
  <summary>Details</summary>
Motivation: 自然语言测试用例中常见的测试异味（如模糊性、冗余等）影响测试的可靠性和可维护性，现有检测工具依赖手动规则且缺乏扩展性，研究探讨SLMs的潜在应用价值。

Method: 评估Gemma3、Llama3.2和Phi-4在143个真实Ubuntu测试用例上的表现，覆盖七种测试异味类型。

Result: Phi-4表现最佳，检测测试异味句子的pass@2达97%，Gemma3和Llama3.2约91%。SLMs还能自主解释问题并提出改进建议。

Conclusion: SLMs是一种高效、低成本且保护数据隐私的测试异味检测工具，能够提升实际测试场景中的质量。

Abstract: Manual testing, in which testers follow natural language instructions to
validate system behavior, remains crucial for uncovering issues not easily
captured by automation. However, these test cases often suffer from test
smells, quality issues such as ambiguity, redundancy, or missing checks that
reduce test reliability and maintainability. While detection tools exist, they
typically require manual rule definition and lack scalability. This study
investigates the potential of Small Language Models (SLMs) for automatically
detecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143
real-world Ubuntu test cases, covering seven types of test smells. Phi-4
achieved the best results, reaching a pass@2 of 97% in detecting sentences with
test smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond
detection, SLMs autonomously explained issues and suggested improvements, even
without explicit prompt instructions. They enabled low-cost, concept-driven
identification of diverse test smells without relying on extensive rule
definitions or syntactic analysis. These findings highlight the potential of
SLMs as efficient tools that preserve data privacy and can improve test quality
in real-world scenarios.

</details>


### [12] [iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development](https://arxiv.org/abs/2507.13081)
*Dongming Jin,Weisong Sun,Jiangping Huang,Peng Liang,Jifeng Xuan,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 论文提出了一种名为iReDev的知识驱动多智能体框架，用于智能需求开发，通过集成人类知识和支持人-智能体协作，显著提升了需求开发的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 需求开发是一个关键但耗时的阶段，现有研究对需求开发的支持有限，且忽视了人类知识的注入和人-智能体协作。

Method: iReDev框架由六个知识驱动的智能体组成，采用基于事件驱动的通信机制和人工干预机制（human-in-the-loop），支持全周期需求开发。

Result: 评估表明，iReDev在多项指标上优于现有基线方法，能够快速处理新需求并生成符合利益相关者期望的文档。

Conclusion: iReDev为智能需求开发提供了新方向，未来可进一步探索其三个关键发展方向。

Abstract: Requirements development is a critical phase as it is responsible for
providing a clear understanding of what stakeholders need. It involves
collaboration among stakeholders to extract explicit requirements and address
potential conflicts, which is time-consuming and labor-intensive. Recently,
multi-agent systems for software development have attracted much attention.
However, existing research provides limited support for requirements
development and overlooks the injection of human knowledge into agents and the
human-agent collaboration. % To address these issues, this paper proposes a
knowledge-driven multi-agent framework for intelligent requirement development,
named iReDev. iReDev features: iReDev consists of six knowledge-driven agents
to support the entire requirements development. They collaboratively perform
various tasks to produce a software requirements specification. iReDev focuses
on integrating human knowledge for agents, enabling them to simulate real-world
stakeholders. iReDev uses an event-driven communication mechanism based on an
artifact pool. Agents continuously monitor the pool and autonomously trigger
the next action based on its changes, enabling iReDev to handle new
requirements quickly. iReDev introduces a human-in-the-loop mechanism to
support human-agent collaboration, ensuring that the generated artifacts align
with the expectations of stakeholders. We evaluated the generated artifacts and
results show that iReDev outperforms existing baselines in multiple aspects. We
further envision three key directions and hope this work can facilitate the
development of intelligent requirements development.

</details>


### [13] [A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems](https://arxiv.org/abs/2507.13095)
*Dongming Jin,Zhi Jin,Linyu Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: 大型预训练模型在软件系统中的广泛应用挑战了传统需求工程方法，本文提出一个针对此类系统的概念框架和研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究预训练模型作为核心组件的软件系统对传统需求工程假设的挑战，如功能可分解性和行为可预测性。

Method: 提出一个专门针对预训练模型软件系统的需求工程概念框架，并探讨相关研究方向。

Result: 为研究人员和从业者应对预训练模型系统的需求工程挑战提供了指导。

Conclusion: 需要重新思考需求工程方法以适应预训练模型系统的独特性和持续演进特性。

Abstract: Recent advances in large pretrained models have led to their widespread
integration as core components in modern software systems. The trend is
expected to continue in the foreseeable future. Unlike traditional software
systems governed by deterministic logic, systems powered by pretrained models
exhibit distinctive and emergent characteristics, such as ambiguous capability
boundaries, context-dependent behavior, and continuous evolution. These
properties fundamentally challenge long-standing assumptions in requirements
engineering, including functional decomposability and behavioral
predictability. This paper investigates this problem and advocates for a
rethinking of existing requirements engineering methodologies. We propose a
conceptual framework tailored to requirements engineering of
pretrained-model-enabled software systems and outline several promising
research directions within this framework. This vision helps provide a guide
for researchers and practitioners to tackle the emerging challenges in
requirements engineering of pretrained-model-enabled systems.

</details>


### [14] [Inferring Attributed Grammars from Parser Implementations](https://arxiv.org/abs/2507.13117)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.SE

TL;DR: 论文提出了一种新方法，通过动态分析递归下降解析器的实现，从语法中推断出属性文法，从而恢复输入处理的语义规范。


<details>
  <summary>Details</summary>
Motivation: 现有语法挖掘技术主要关注语法结构恢复，而对输入处理的语义规范缺乏探索。

Method: 动态分析递归下降解析器的实现，观察程序执行并映射运行时行为到语法，嵌入语义动作到文法规则中。

Result: 通过初始程序集验证，该方法能准确生成属性文法，重现程序行为。

Conclusion: 该方法成功地实现了全面规范恢复，为输入处理的语义规范填补了空白。

Abstract: Software systems that process structured inputs often lack complete and
up-to-date specifications, which specify the input syntax and the semantics of
input processing. While grammar mining techniques have focused on recovering
syntactic structures, the semantics of input processing remains largely
unexplored. In this work, we introduce a novel approach for inferring
attributed grammars from parser implementations. Given an input grammar, our
technique dynamically analyzes the implementation of recursive descent parsers
to reconstruct the semantic aspects of input handling, resulting in
specifications in the form of attributed grammars. By observing program
executions and mapping the program's runtime behavior to the grammar, we
systematically extract and embed semantic actions into the grammar rules. This
enables comprehensive specification recovery. We demonstrate the feasibility of
our approach using an initial set of programs, showing that it can accurately
reproduce program behavior through the generated attributed grammars.

</details>


### [15] [Detecting LLM-generated Code with Subtle Modification by Adversarial Training](https://arxiv.org/abs/2507.13123)
*Xin Yin,Xinrui Li,Chao Ni,Xiaodan Xu,Xiaohu Yang*

Main category: cs.SE

TL;DR: 论文探讨了如何有效检测并增强对经过修改的LLM生成代码的识别鲁棒性，提出CodeGPTSensor+方法，通过对抗训练提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的广泛应用，其带来的代码溯源、版权和质量问题日益突出，且现有检测方法对修改后的代码鲁棒性不足。

Method: 提出CodeGPTSensor+，采用对抗训练和MIST模块生成高质量对抗样本，提升模型对输入扰动的鲁棒性。

Result: 在HMCorp数据集上，CodeGPTSensor+在对抗测试集上显著提升检测准确率，同时在原始测试集上保持高准确率。

Conclusion: CodeGPTSensor+有效解决了修改后LLM生成代码的检测问题，展示了卓越的鲁棒性。

Abstract: With the rapid development of Large Language Models (LLMs), their powerful
code-generation capabilities have been widely applied in tasks like code
completion and automated development, demonstrating the value of improving
coding efficiency. However, the extensive use of LLM-generated code also raises
several new challenges. On the one hand, issues such as the regulation of code
provenance, copyright disputes, and code quality have become increasingly
concerning. How to effectively detect LLM-generated code and ensure its
compliant and responsible use has become a critical and urgent issue. On the
other hand, in practical applications, LLM-generated code is often subject to
manual modifications, such as variable renaming or structural adjustments.
Although some recent studies have proposed training-based and zero-shot methods
for detecting LLM-generated code, these approaches show insufficient robustness
when facing modified LLM-generated code, and there is a lack of an effective
solution. To address the real-world scenario where LLM-generated code may
undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of
CodeGPTSensor, which employs adversarial training to improve robustness against
input perturbations. CodeGPTSensor+ integrates an adversarial sample generation
module, Multi-objective Identifier and Structure Transformation (MIST), which
systematically generates both high-quality and representative adversarial
samples. This module effectively enhances the model's resistance against
diverse adversarial attacks. Experimental results on the HMCorp dataset
demonstrate that CodeGPTSensor+ significantly improves detection accuracy on
the adversarial test set while maintaining high accuracy on the original test
set, showcasing superior robustness compared to CodeGPTSensor.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Dual-Numbers Reverse AD for Functional Array Languages](https://arxiv.org/abs/2507.12640)
*Tom Smeding,Mikołaj Konarski,Simon Peyton Jones,Andrew Fitzgibbon*

Main category: cs.PL

TL;DR: 该论文提出了一种在双数反向模式自动微分（AD）中支持多维数组的方法，通过三个松散耦合的组件提升了性能，但牺牲了高阶代码的支持。


<details>
  <summary>Details</summary>
Motivation: 现有的双数反向模式AD在处理数组程序时性能不足，难以满足实践需求。

Method: 通过语义保持的向量化代码转换（BOT）、将基本双数反向AD算法提升到一阶数组语言，以及符号解释实现端到端编译。

Result: 实现了多维数组的高效支持，性能开销极小，但丧失了对高阶代码的通用支持。

Conclusion: 提出的方法显著提升了双数反向模式AD在数组程序中的性能，尽管牺牲了部分通用性。

Abstract: The standard dual-numbers construction works well for forward-mode automatic
differentiation (AD) and is attractive due to its simplicity; recently, it also
has been adapted to reverse-mode AD, but practical performance, especially on
array programs, leaves a lot to be desired. In this paper we introduce
first-class support for multidimensional arrays in dual-numbers reverse-mode AD
with little to no performance overhead. The algorithm consists of three
loosely-coupled components: a semantics-preserving vectorisation code
transformation (the bulk-operation transform or BOT), a fairly straightforward
lifting of the basic dual-numbers reverse AD algorithm to a mostly first-order
array language, and symbolic interpretation to achieve an end-to-end
compilation pipeline. Unfortunately, we lose some of the nice generalisable
aspects of dual-numbers AD in the process, most importantly support for
higher-order code.
  We do support some higher-order array combinators, but only a
carefully-chosen set: 'build' (elementwise array construction), 'gather' and
'scatter'. In return, the BOT can eliminate the essential (for AD)
higher-orderness of the input program, meaning that AD gets essentially
presented with a first-order program. This allows the naive trick of lifting
dual numbers to "dual arrays" to work without much modification.

</details>


### [17] [Formal Verification for JavaScript Regular Expressions: a Proven Semantics and its Applications](https://arxiv.org/abs/2507.13091)
*Aurèle Barrière,Victor Deng,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: 该论文提出了一种现代正则表达式语言的机械化、简洁、实用且完整的语义，并证明了其忠实性，展示了两种实际应用。


<details>
  <summary>Details</summary>
Motivation: 填补现代正则表达式语言缺乏机械化、忠实语义的空白。

Method: 通过证明其与ECMAScript规范的等价性，确保语义的忠实性，并应用实际案例验证实用性。

Result: 实现了两种实际应用：上下文等价性验证和PikeVM算法的形式化证明。

Conclusion: 成功定义并机械化了一种现代正则表达式语言的完整语义，为实际应用提供了理论基础。

Abstract: We present the first mechanized, succinct, practical, complete, and
proven-faithful semantics for a modern regular expression language with
backtracking semantics. We ensure its faithfulness by proving it equivalent to
a preexisting line-by-line embedding of the official ECMAScript specification
of JavaScript regular expressions. We demonstrate its practicality by
presenting two real-world applications. First, a new notion of contextual
equivalence for modern regular expressions, which we use to prove or disprove
rewrites drawn from previous work. Second, the first formal proof of the PikeVM
algorithm used in many real-world engines. In contrast with the specification
and other formalization work, our semantics captures not only the top-priority
match, but a full backtracking tree recording all possible matches and their
respective priority. All our definitions and results have been mechanized in
the Rocq proof assistant.

</details>


### [18] [Towards Formal Verification of LLM-Generated Code from Natural Language Prompts](https://arxiv.org/abs/2507.13290)
*Aaron Councilman,David Fu,Aryan Gupta,Chengxiao Wang,David Grove,Yu-Xiong Wang,Vikram Adve*

Main category: cs.PL

TL;DR: 本文提出一种方法，通过结合形式化查询语言，验证LLM生成的代码是否正确匹配用户意图，并在Ansible编程语言中实现该系统，验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 为解决LLM生成代码常含错误且用户难以检测的问题，提供形式化正确性保证，提升AI代码助手体验，甚至支持无编程知识的用户进行自然语言编程。

Method: 引入形式化查询语言表达用户意图，并验证LLM生成的代码是否符合该意图。在Ansible中实现系统Astrogator，包含查询语言、行为演算和符号解释器。

Result: 在21个代码生成任务中，验证器能正确验证83%的正确代码，识别92%的错误代码。

Conclusion: 形式化验证方法有效提升LLM生成代码的可靠性和用户体验，为自然语言编程提供可能。

Abstract: In the past few years LLMs have emerged as a tool that can aid programmers by
taking natural language descriptions and generating code based on it. However,
LLMs often generate incorrect code that users need to fix and the literature
suggests users often struggle to detect these errors. In this work we seek to
offer formal guarantees of correctness to LLM generated code; such guarantees
could improve the experience of using AI Code Assistants and potentially enable
natural language programming for users with little or no programming knowledge.
To address this challenge we propose to incorporate a formal query language
that can represent a user's intent in a formally defined but natural
language-like manner that a user can confirm matches their intent. Then, using
such a query we propose to verify LLM generated code to ensure it matches the
user's intent. We implement these ideas in our system, Astrogator, for the
Ansible programming language which includes such a formal query language, a
calculus for representing the behavior of Ansible programs, and a symbolic
interpreter which is used for the verification. On a benchmark suite of 21
code-generation tasks, our verifier is able to verify correct code in 83% of
cases and identify incorrect code in 92%.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [19] [Design and Reliability of a User Space Write-Ahead Log in Rust](https://arxiv.org/abs/2507.13062)
*Vitor K. F. Pellegatti,Gustavo M. D. Vieira*

Main category: cs.OS

TL;DR: 本文讨论了使用Rust开发用户空间预写日志（WAL）原型的经验，强调了Rust的易用性、库丰富性以及高性能。


<details>
  <summary>Details</summary>
Motivation: 预写日志（WAL）是计算机科学中重要的容错技术，要求高可靠性和高性能。本文旨在探索使用Rust实现WAL的可行性。

Method: 通过开发一个Rust语言的用户空间WAL原型，评估其性能和可靠性。

Result: Rust易于使用且性能优异，WAL原型的性能接近稳定存储设备的理论性能。

Conclusion: Rust是实现高性能WAL的有效工具，适合类似需求的项目。

Abstract: Write-ahead logs (WALs) are a fundamental fault-tolerance technique found in
many areas of computer science. WALs must be reliable while maintaining high
performance, because all operations will be written to the WAL to ensure their
stability. Without reliability a WAL is useless, because its utility is tied to
its ability to recover data after a failure. In this paper we describe our
experience creating a prototype user space WAL in Rust. We observed that Rust
is easy to use, compact and has a very rich set of libraries. More importantly,
we have found that the overhead is minimal, with the WAL prototype operating at
basically the expected performance of the stable memory device.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Energy-Efficient RSMA-enabled Low-altitude MEC Optimization Via Generative AI-enhanced Deep Reinforcement Learning](https://arxiv.org/abs/2507.12910)
*Xudong Wang,Hongyang Du,Lei Feng,Kaibin Huang*

Main category: cs.NI

TL;DR: 论文提出了一种基于生成AI的深度强化学习框架，用于优化6G中无人机低空移动边缘计算系统的能效。


<details>
  <summary>Details</summary>
Motivation: 6G对低延迟计算的需求推动了无人机低空移动边缘计算系统的应用，但地面终端之间的上行干扰问题严重。

Method: 结合生成AI和深度强化学习，优化无人机的3D轨迹、RSMA解码顺序、任务卸载决策和资源分配。

Result: 仿真结果表明，该方法在能效表现上优于基准方法。

Conclusion: 生成AI与RSMA的结合显著提升了低空移动边缘计算系统的能效性能。

Abstract: The growing demand for low-latency computing in 6G is driving the use of
UAV-based low-altitude mobile edge computing (MEC) systems. However, limited
spectrum often leads to severe uplink interference among ground terminals
(GTs). In this paper, we investigate a rate-splitting multiple access
(RSMA)-enabled low-altitude MEC system, where a UAV-based edge server assists
multiple GTs in concurrently offloading their tasks over a shared uplink. We
formulate a joint optimization problem involving the UAV 3D trajectory, RSMA
decoding order, task offloading decisions, and resource allocation, aiming to
mitigate multi-user interference and maximize energy efficiency. Given the high
dimensionality, non-convex nature, and dynamic characteristics of this
optimization problem, we propose a generative AI-enhanced deep reinforcement
learning (DRL) framework to solve it efficiently. Specifically, we embed a
diffusion model into the actor network to generate high-quality action samples,
improving exploration in hybrid action spaces and avoiding local optima. In
addition, a priority-based RSMA decoding strategy is designed to facilitate
efficient successive interference cancellation with low complexity. Simulation
results demonstrate that the proposed method for low-altitude MEC systems
outperforms baseline methods, and that integrating GDM with RSMA can achieve
significantly improved energy efficiency performance.

</details>


### [21] [RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents](https://arxiv.org/abs/2507.13140)
*Kuiyuan Ding,Caili Guo,Yang Yang,Jianzhang Guo*

Main category: cs.NI

TL;DR: RIDAS是一种多智能体框架，用于6G网络中高效资源分配，通过结合表示驱动代理和意图驱动代理，显著提升了用户支持数量和资源效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有解决方案在用户意图与低层参数化配置之间的差距，满足6G网络对AI集成的高要求和严格的QoS需求。

Method: 提出RIDAS框架，包含表示驱动代理（RDAs）和意图驱动代理（IDA）。RDAs提供可调参数接口，IDA通过两阶段规划方案（带宽预分配和再分配）结合LLM，将用户意图和系统状态映射为最优配置。

Result: 实验显示，RIDAS在相同QoS约束下支持的用户数量比WirelessAgent多44.71%。

Conclusion: RIDAS能有效捕捉用户意图并在AI RAN环境中更高效地分配资源。

Abstract: Sixth generation (6G) networks demand tight integration of artificial
intelligence (AI) into radio access networks (RANs) to meet stringent quality
of service (QoS) and resource efficiency requirements. Existing solutions
struggle to bridge the gap between high level user intents and the low level,
parameterized configurations required for optimal performance. To address this
challenge, we propose RIDAS, a multi agent framework composed of representation
driven agents (RDAs) and an intention driven agent (IDA). RDAs expose open
interface with tunable control parameters (rank and quantization bits, enabling
explicit trade) offs between distortion and transmission rate. The IDA employs
a two stage planning scheme (bandwidth pre allocation and reallocation) driven
by a large language model (LLM) to map user intents and system state into
optimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\%
more users than WirelessAgent under equivalent QoS constraints. These results
validate ability of RIDAS to capture user intent and allocate resources more
efficiently in AI RAN environments.

</details>


### [22] [Predictability-Aware Motion Prediction for Edge XR via High-Order Error-State Kalman Filtering](https://arxiv.org/abs/2507.13179)
*Ziyu Zhong,Hector A Caltenco,Björn Landfeldt,Günter Alce*

Main category: cs.NI

TL;DR: 6G网络将为XR应用提供边缘计算卸载的可能，降低用户设备电池需求并支持更小型设备设计。


<details>
  <summary>Details</summary>
Motivation: 探索6G低延迟与边缘计算结合如何实现XR应用的高效卸载。

Method: 利用6G的低延迟特性和边缘计算基础设施，将计算密集型功能（如渲染）从用户设备迁移到网络。

Result: 降低了用户设备的电池需求，并为设备小型化设计提供可能。

Conclusion: 6G网络的低延迟和边缘计算能力将为XR应用提供高效的卸载方案，推动新设备设计。

Abstract: As 6G networks are developed and defined, offloading of XR applications is
emerging as one of the strong new use cases. The reduced 6G latency coupled
with edge processing infrastructure will for the first time provide a realistic
offloading scenario in cellular networks where several computationally
intensive functions, including rendering, can migrate from the user device and
into the network. A key advantage of doing so is the lowering of the battery
needs in the user devices and the possibility to design new devices with
smaller form factors.

</details>


### [23] [Bidirectional Age of Incorrect Information: A Performance Metric for Status Updates in Virtual Dynamic Environments](https://arxiv.org/abs/2507.13312)
*Chiara Schiavo,Manuele Favero,Alessandro Buratto,Leonardo Badia*

Main category: cs.NI

TL;DR: 论文提出双向错误信息年龄（BAoII）来量化虚拟动态环境中因信息过时或错误导致的时间依赖性惩罚，并通过马尔可夫链模型找到最优更新策略。


<details>
  <summary>Details</summary>
Motivation: 虚拟动态环境中实体表征的准确性和实时性对系统可靠性至关重要，但现有方法未充分解决双向信息交换的时效性问题。

Method: 提出BAoII概念，利用连续时间马尔可夫链模型推导长期BAoII的闭式表达式，并确定最优更新策略的传输成本阈值。

Result: 模型验证了通信成本与信息新鲜度之间的权衡，数值模拟显示BAoII对评估系统性能有重要影响。

Conclusion: BAoII对元宇宙和数字孪生中的实时协作具有实际意义，可优化系统性能。

Abstract: Virtual dynamic environments (VDEs) such as the Metaverse and digital twins
(DTs) require proper representation of the interacting entities to map their
characteristics within the simulated or augmented space. Keeping these
representations accurate and up-to-date is crucial for seamless interaction and
system reliability. In this paper, we propose bidirectional age of incorrect
information (BAoII) to address this aspect. BAoII quantifies the time-dependent
penalty paid by an entity in a VDE due to incorrect or outdated knowledge about
itself and the overall dynamically changing space. This extends the concept of
age of incorrect information for a bidirectional information exchange,
capturing that a VDE requires mutual awareness of the entity's own
representation, measured in the virtual space, and what the other entities
share about their representations. Using a continuous-time Markov chain model,
we derive a closed-form expression for long-term BAoII and identify a
transmission cost threshold for optimal update strategies. We describe a
trade-off between communication cost and information freshness and validate our
model through numerical simulations, demonstrating the impact of BAoII on
evaluating system performance and highlighting its relevance for real-time
collaboration in the Metaverse and DTs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [24] [Dependency Pairs for Expected Innermost Runtime Complexity and Strong Almost-Sure Termination of Probabilistic Term Rewriting](https://arxiv.org/abs/2507.12918)
*Jan-Christoph Kassing,Leon Spitzer,Jürgen Giesl*

Main category: cs.LO

TL;DR: 该论文提出了首个用于分析概率项重写系统（PTRS）期望复杂度的依赖对（DP）框架，并证明了其在SAST方面的优势。


<details>
  <summary>Details</summary>
Motivation: 虽然依赖对框架在确定性系统终止性分析中表现出色，但概率系统的自动复杂度分析仍未被充分探索，因此需要扩展DP框架以填补这一空白。

Method: 论文扩展了依赖对框架，使其能够分析PTRS的期望复杂度，并通过工具AProVE实现了这一方法。

Result: 与现有技术相比，该方法在证明强几乎必然终止（SAST）方面表现出更强的能力。

Conclusion: 该研究为PTRS的终止性和复杂度分析提供了首个有效的DP框架，并通过实验验证了其优越性。

Abstract: The dependency pair (DP) framework is one of the most powerful techniques for
automatic termination and complexity analysis of term rewrite systems. While
DPs were extended to prove almost-sure termination of probabilistic term
rewrite systems (PTRSs), automatic complexity analysis for PTRSs is largely
unexplored. We introduce the first DP framework for analyzing expected
complexity and for proving positive or strong almost-sure termination (SAST) of
innermost rewriting with PTRSs, i.e., finite expected runtime. We implemented
our framework in the tool AProVE and demonstrate its power compared to existing
techniques for proving SAST.

</details>


### [25] [Cyclic proof theory of positive inductive definitions](https://arxiv.org/abs/2507.13057)
*Gianluca Curzi,Lukas Melgaard*

Main category: cs.LO

TL;DR: 本文研究了μPA的循环证明系统，发现循环和归纳μPA具有相同的证明论强度，并通过翻译和形式化方法验证了这一结果。


<details>
  <summary>Details</summary>
Motivation: 探讨循环证明系统与归纳证明系统在μPA中的等价性，以扩展对非良基证明论的分析。

Method: 将循环证明翻译为带注释的变体，利用更强的有效性条件简化一致性证明，并在Π₁²-CA₀中形式化这一论证。

Result: 证明了循环和归纳μPA的证明论强度相同，且带注释与普通循环证明在μPA中证明相同的定理。

Conclusion: 本研究通过非良基证明论方法进一步验证了循环证明系统的强度，为算术理论的证明论分析提供了新思路。

Abstract: We study cyclic proof systems for $\mu\mathsf{PA}$, an extension of Peano
arithmetic by positive inductive definitions that is arithmetically equivalent
to the (impredicative) subsystem of second-order arithmetic
$\Pi^1_2$-$\mathsf{CA}_0$ by M\"{o}llefeld. The main result of this paper is
that cyclic and inductive $\mu\mathsf{PA}$ have the same proof-theoretic
strength. First, we translate cyclic proofs into an annotated variant based on
Sprenger and Dam's systems for first-order $\mu$-calculus, whose stronger
validity condition allows for a simpler proof of soundness. We then formalise
this argument within $\Pi^1_2$-$\mathsf{CA}_0$, leveraging M\"{o}llerfeld's
conservativity properties. To this end, we build on prior work by Curzi and Das
on the reverse mathematics of the Knaster-Tarski theorem. As a byproduct of our
proof methods we show that, despite the stronger validity condition, annotated
and "plain" cyclic proofs for $\mu\mathsf{PA}$ prove the same theorems. This
work represents a further step in the non-wellfounded proof-theoretic analysis
of theories of arithmetic via impredicative fragments of second-order
arithmetic, an approach initiated by Simpson's Cyclic Arithmetic, and continued
by Das and Melgaard in the context of arithmetical inductive definitions.

</details>


### [26] [Monotone weak distributive laws over the lifted powerset monad in categories of algebras](https://arxiv.org/abs/2507.13058)
*Quentin Aristote*

Main category: cs.LO

TL;DR: 研究了单调弱分配律在集合和紧致Hausdorff空间中的相似性，部分实现了自动弱提升，但不能推广到其他代数范畴。论文进一步刻画了在代数范畴中幂集单子上单调弱分配律的存在条件。


<details>
  <summary>Details</summary>
Motivation: 探讨不同范畴中单调弱分配律的通用性，特别是针对幂集单子的情况。

Method: 通过比较集合和紧致Hausdorff空间中的单调弱分配律，尝试自动弱提升，并在其他代数范畴中验证其可行性。

Result: 发现在紧致Hausdorff空间中可以结合概率与非确定性，但在许多其他情况下此类分配律不存在。

Conclusion: 单调弱分配律的存在性具有特定性，仅在特定范畴中成立。

Abstract: Noticing the similarity between the monotone weak distributive laws combining
two layers of nondeterminism in sets and in compact Hausdorff spaces, we study
whether the latter law can be obtained automatically as a weak lifting of the
former. This holds partially, but does not generalize to other categories of
algebras: we then characterize when exactly monotone weak distributive laws
over powerset monads in categories of algebras exist, exhibiting a law
combining probabilities and non-determinism in compact Hausdorff spaces and
showing on the other hand that such laws do not exist in a lot of other cases.

</details>


### [27] [Impact and Performance of Randomized Test-Generation using Prolog](https://arxiv.org/abs/2507.13178)
*Marcus Gelderie,Maximilian Luff,Maximilian Peltzer*

Main category: cs.LO

TL;DR: 论文探讨了如何用Prolog随机生成测试序列，研究了随机化和SLD解析对测试性能的影响，并提出了两种随机策略。


<details>
  <summary>Details</summary>
Motivation: 为了解决测试输入序列生成的复杂逻辑依赖问题，并应对大量或无限可能测试的挑战，随机化成为一个自然选择。

Method: 提出了两种随机化策略：一种基于标准Prolog语义，另一种修改SLD选择函数。通过马尔可夫链分析平均时间和生成测试用例的数量。

Result: 提供了两种方法的实证评估和比较。

Conclusion: 论文表明，随机化与SLD解析结合能有效提升测试生成性能。

Abstract: We study randomized generation of sequences of test-inputs to a system using
Prolog. Prolog is a natural fit to generate test-sequences that have complex
logical inter-dependent structure. To counter the problems posed by a large (or
infinite) set of possible tests, randomization is a natural choice. We study
the impact that randomization in conjunction with SLD resolution have on the
test performance. To this end, this paper proposes two strategies to add
randomization to a test-generating program. One strategy works on top of
standard Prolog semantics, whereas the other alters the SLD selection function.
We analyze the mean time to reach a test-case, and the mean number of generated
test-cases in the framework of Markov chains. Finally, we provide an additional
empirical evaluation and comparison between both approaches. Under
consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [28] [Just Verification of Mutual Exclusion Algorithms](https://arxiv.org/abs/2507.13198)
*Rob van Glabbeek,Bas Luttik,Myrthe Spronck*

Main category: cs.LO

TL;DR: 通过模型检查验证多种互斥算法的正确性，考虑了共享寄存器的原子性和非原子性。


<details>
  <summary>Details</summary>
Motivation: 验证互斥算法在共享读写寄存器下的正确性，排除虚假反例以确保活性。

Method: 使用公正性作为完备性标准，考虑不同并发关系建模共享寄存器的工作假设。

Result: 展示了一些算法违反正确性的执行案例，并提出改进建议。

Conclusion: 公正性标准对不同并发关系的建模是验证互斥算法正确性的有效方法。

Abstract: We verify the correctness of a variety of mutual exclusion algorithms through
model checking. We look at algorithms where communication is via shared
read/write registers, where those registers can be atomic or non-atomic. For
the verification of liveness properties, it is necessary to assume a
completeness criterion to eliminate spurious counterexamples. We use justness
as completeness criterion. Justness depends on a concurrency relation; we
consider several such relations, modelling different assumptions on the working
of the shared registers. We present executions demonstrating the violation of
correctness properties by several algorithms, and in some cases suggest
improvements.

</details>


### [29] [Solving SAT By Computing A Stable Set Of Points In Clusters](https://arxiv.org/abs/2507.13282)
*Eugene Goldberg*

Main category: cs.LO

TL;DR: 本文介绍了如何通过集群计算稳定点集（SSP），以解决直接逐点计算SSP对实际CNF公式不可行的问题。


<details>
  <summary>Details</summary>
Motivation: 研究发现实际CNF公式的SSP规模巨大，逐点计算不可行，需要更高效的方法。

Method: 提出通过同时处理大量点的集群来计算SSP。

Result: 该方法不仅能更好地利用公式结构设计更高效的SAT算法，还便于并行计算。

Conclusion: 集群计算SSP为解决大规模CNF公式提供了一种可行且高效的方法。

Abstract: Earlier we introduced the notion of a stable set of points (SSP). We proved
that a CNF formula is unsatisfiable iff there is a set of points (i.e. complete
assignments) that is stable with respect to this formula. Experiments showed
that SSPs for CNF formulas of practical interest are very large. So computing
an SSP for a CNF formula point by point is, in general, infeasible. In this
report, we show how an SSP can be computed in clusters, each cluster being a
large set of points that are processed simultaneously. The appeal of computing
SSPs is twofold. First, it allows one to better take into account formula
structure and hence, arguably, design more efficient SAT algorithms. Second,
SAT solving by SSPs facilitates parallel computing.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [30] ["How to Explore Biases in Speech Emotion AI with Users?" A Speech-Emotion-Acting Study Exploring Age and Language Biases](https://arxiv.org/abs/2507.12580)
*Josephine Beatrice Skovbo Borre,Malene Gorm Wold,Sara Kjær Rasmussen,Ilhan Aslan*

Main category: cs.HC

TL;DR: 研究探讨年龄和语言如何影响情感的有意声音表达，对比青少年和55岁以上成年人在语音情感识别中的表现，发现模型在不同语言和年龄组中的表现相似。


<details>
  <summary>Details</summary>
Motivation: 探索语音情感识别（SER）中年龄和语言的影响，填补对青少年和老年人的研究空白。

Method: 开发新实验范式，结合自定义用户界面和实时SER预测后端，参与者通过表达特定情感来完成任务。

Result: 结果显示语言和年龄组间无显著差异，模型表现具有跨语言和年龄的稳健性，但高唤醒情感识别存在局限。

Conclusion: 需超越系统为中心的准确度指标，采用更包容的、以人为中心的SER模型。

Abstract: This study explores how age and language shape the deliberate vocal
expression of emotion, addressing underexplored user groups, Teenagers (N = 12)
and Adults 55+ (N = 12), within speech emotion recognition (SER). While most
SER systems are trained on spontaneous, monolingual English data, our research
evaluates how such models interpret intentionally performed emotional speech
across age groups and languages (Danish and English). To support this, we
developed a novel experimental paradigm combining a custom user interface with
a backend for real-time SER prediction and data logging. Participants were
prompted to hit visual targets in valence-arousal space by deliberately
expressing four emotion targets. While limitations include some reliance on
self-managed voice recordings and inconsistent task execution, the results
suggest contrary to expectations, no significant differences between language
or age groups, and a degree of cross-linguistic and age robustness in model
interpretation. Though some limitations in high-arousal emotion recognition
were evident. Our qualitative findings highlight the need to move beyond
system-centered accuracy metrics and embrace more inclusive, human-centered SER
models. By framing emotional expression as a goal-directed act and logging the
real-time gap between human intent and machine interpretation, we expose the
risks of affective misalignment.

</details>


### [31] [NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting](https://arxiv.org/abs/2507.12621)
*Kuangshi Ai,Kaiyuan Tang,Chaoli Wang*

Main category: cs.HC

TL;DR: 本文提出NLI4VolVis系统，通过自然语言交互实现体数据探索、查询和编辑，解决了传统体可视化方法在交互性和计算效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统体可视化方法存在交互设计僵硬和计算成本高的问题，新兴方法则需要额外学习成本且缺乏语义级交互支持。

Method: NLI4VolVis整合多视角语义分割和视觉语言模型，利用多代理大语言模型架构解析用户意图并执行可视化任务。

Result: 系统支持开放词汇查询、实时场景编辑、最佳视角选择和2D风格化，用户研究验证了其可用性和易用性提升。

Conclusion: NLI4VolVis通过自然语言交互显著提升了体数据探索的可访问性和效率。

Abstract: Traditional volume visualization (VolVis) methods, like direct volume
rendering, suffer from rigid transfer function designs and high computational
costs. Although novel view synthesis approaches enhance rendering efficiency,
they require additional learning effort for non-experts and lack support for
semantic-level interaction. To bridge this gap, we propose NLI4VolVis, an
interactive system that enables users to explore, query, and edit volumetric
scenes using natural language. NLI4VolVis integrates multi-view semantic
segmentation and vision-language models to extract and understand semantic
components in a scene. We introduce a multi-agent large language model
architecture equipped with extensive function-calling tools to interpret user
intents and execute visualization tasks. The agents leverage external tools and
declarative VolVis commands to interact with the VolVis engine powered by 3D
editable Gaussians, enabling open-vocabulary object querying, real-time scene
editing, best-view selection, and 2D stylization. We validate our system
through case studies and a user study, highlighting its improved accessibility
and usability in volumetric data exploration. We strongly recommend readers
check our case studies, demo video, and source code at
https://nli4volvis.github.io/.

</details>


### [32] [Design Patterns of Human-AI Interfaces in Healthcare](https://arxiv.org/abs/2507.12721)
*Rui Sheng,Chuhan Shi,Sobhan Lotfi,Shiyi Liu,Adam Perer,Huamin Qu,Furui Cheng*

Main category: cs.HC

TL;DR: 该论文提出了在医疗健康领域中设计人机交互界面的系统性指导，总结了12种设计模式，并通过采访和研讨会验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 设计医疗健康领域中的人机交互界面具有挑战性，需系统性指导。

Method: 通过采访12名医疗专业人员了解使用场景和注意事项，并在线招募14名参与者进行设计模式的研讨会评估。

Result: 提出了12种设计模式，并讨论了其普适性和局限性。

Conclusion: 设计模式可为医疗健康领域的人机交互提供指导，未来需进一步扩展和优化。

Abstract: Human-AI interfaces play a crucial role in advancing practices and research
within the healthcare domain. However, designing such interfaces presents a
substantial challenge for designers. In this paper, we propose systematic
guidance for designing human-AI interfaces in typical healthcare scenarios by
summarizing the design patterns for presenting and interacting with common
information entities. To deepen our understanding of these 12 design patterns,
we interviewed 12 healthcare professionals to explore potential usage scenarios
and important considerations. Furthermore, we conducted workshops with 14
participants recruited online to evaluate our design patterns. Finally, we
discussed the generalizability of the design patterns to other application
domains, the limitations, and the future work.

</details>


### [33] [An Age-based Study into Interactive Narrative Visualization Engagement](https://arxiv.org/abs/2507.12734)
*Nina Errey,Yi Chen,Yu Dong,Quang Vinh Nguyen,Xiaoru Yuan,Tuck Wah Leong,Christy Jie Liang*

Main category: cs.HC

TL;DR: 研究显示，受众年龄影响其对数字媒体的参与度。交互式叙事可视化结合了数据可视化和讲故事，但年龄因素常被忽视。实验发现，年轻群体比年长群体更投入，理解更好，并提出了包容性设计建议。


<details>
  <summary>Details</summary>
Motivation: 探讨受众年龄如何影响对交互式叙事可视化的参与度，填补年龄因素在这一领域的研究空白。

Method: 采用已建立的可视化参与度问卷，进行实证实验，比较不同年龄群体的参与度差异，并结合定性分析。

Result: 年长群体的参与度略低于年轻群体；年轻群体对交互式叙事模式的理解更明显。

Conclusion: 针对不同年龄群体，提出了交互式叙事可视化的包容性设计建议。

Abstract: Research has shown that an audiences' age impacts their engagement in digital
media. Interactive narrative visualization is an increasingly popular form of
digital media that combines data visualization and storytelling to convey
important information. However, audience age is often overlooked by interactive
narrative visualization authors. Using an established visualization engagement
questionnaire, we ran an empirical experiment where we compared end-user
engagement to audience age. We found a small difference in engagement scores
where older age cohorts were less engaged than the youngest age cohort. Our
qualitative analysis revealed that the terminology and overall understanding of
interactive narrative patterns integrated into narrative visualization was more
apparent in the feedback from younger age cohorts relative to the older age
cohorts. We conclude this paper with a series of recommendations for authors of
interactive narrative visualization on how to design inclusively for audiences
according to their age.

</details>


### [34] [Public Evaluation on Potential Social Impacts of Fully Autonomous Cybernetic Avatars for Physical Support in Daily-Life Environments: Large-Scale Demonstration and Survey at Avatar Land](https://arxiv.org/abs/2507.12741)
*Lotfi El Hafi,Kazuma Onishi,Shoichi Hasegawa,Akira Oyama,Tomochika Ishikawa,Masashi Osada,Carl Tornberg,Ryoma Kado,Kento Murata,Saki Hashimoto,Sebastian Carrera Villalobos,Akira Taniguchi,Gustavo Alfonso Garcia Ricardez,Yoshinobu Hagiwara,Tatsuya Aoki,Kensuke Iwata,Takato Horii,Yukiko Horikawa,Takahiro Miyashita,Tadahiro Taniguchi,Hiroshi Ishiguro*

Main category: cs.HC

TL;DR: 研究评估了公众对完全自主的网络化身（CAs）在日常生活物理支持中的看法及其潜在社会影响。


<details>
  <summary>Details</summary>
Motivation: 探索完全自主CAs的部署挑战及其社会接受度。

Method: 通过大型公开活动（Avatar Land）展示并调查参与者对自主及半自主CAs的看法，收集了2,285份问卷，分析了333名与完全自主CAs互动者的反馈。

Result: 公众对CAs在生活及工作中的物理支持感兴趣，但对其任务执行可靠性存有担忧，成本和拟人交互非主要问题。

Conclusion: 完全自主CAs在实用中需解决可靠性问题，以提高公众接受度。

Abstract: Cybernetic avatars (CAs) are key components of an avatar-symbiotic society,
enabling individuals to overcome physical limitations through virtual agents
and robotic assistants. While semi-autonomous CAs intermittently require human
teleoperation and supervision, the deployment of fully autonomous CAs remains a
challenge. This study evaluates public perception and potential social impacts
of fully autonomous CAs for physical support in daily life. To this end, we
conducted a large-scale demonstration and survey during Avatar Land, a 19-day
public event in Osaka, Japan, where fully autonomous robotic CAs, alongside
semi-autonomous CAs, performed daily object retrieval tasks. Specifically, we
analyzed responses from 2,285 visitors who engaged with various CAs, including
a subset of 333 participants who interacted with fully autonomous CAs and
shared their perceptions and concerns through a survey questionnaire. The
survey results indicate interest in CAs for physical support in daily life and
at work. However, concerns were raised regarding task execution reliability. In
contrast, cost and human-like interaction were not dominant concerns. Project
page: https://lotfielhafi.github.io/FACA-Survey/.

</details>


### [35] [PatternSight: A Perceptual Grouping Effectiveness Assessment Approach for Graphical Patterns in Charts](https://arxiv.org/abs/2507.12749)
*Xumeng Wang,Xiangxuan Zhang,Zhiqi Gao,Shuangcheng Jiao,Yuxin Ma*

Main category: cs.HC

TL;DR: 该论文提出了一种感知模拟模型，帮助图表作者评估图表设计的感知效果，并通过原型工具PatternSight优化设计。


<details>
  <summary>Details</summary>
Motivation: 图表工具普及后，作者缺乏感知理论导致难以评估图表效果，需改进设计流程。

Method: 结合感知理论提取图表视觉特征，构建感知模拟模型预测观众可能的注意力分布。

Result: 模型能模拟观众感知行为，PatternSight工具有效辅助优化设计。

Conclusion: 感知模拟模型及工具提升了图表设计的感知效果评估与优化能力。

Abstract: The boom in visualization generation tools has significantly lowered the
threshold for chart authoring. Nevertheless, chart authors with an insufficient
understanding of perceptual theories may encounter difficulties in evaluating
the effectiveness of chart representations, thereby struggling to identify the
appropriate chart design to convey the intended data patterns. To address this
issue, we propose a perception simulation model that can assess the perceptual
effectiveness of charts by predicting graphical patterns that chart viewers are
likely to notice. The perception simulation model integrates perceptual theory
into visual feature extraction of chart elements to provide interpretable model
outcomes. Human perceptual results proved that the outcome of our model can
simulate the perceptual grouping behaviors of most chart viewers and cover
diverse perceptual results. We also embed the model into a prototype interface
called PatternSight to facilitate chart authors in assessing whether the chart
design can satisfy their pattern representation requirements as expected and
determining feasible improvements of visual design. According to the results of
a user experiment, PatternSight can effectively assist chart authors in
optimizing chart design for representing data patterns.

</details>


### [36] [Autonomy for Older Adult-Agent Interaction](https://arxiv.org/abs/2507.12767)
*Jiaxin An*

Main category: cs.HC

TL;DR: 本文探讨AI代理在老年人护理中的自主性需求，提出了四个关键维度，并建议未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着全球人口老龄化，AI代理成为老年人护理的潜在工具，但如何使其与老年人的自主性需求保持一致仍是挑战。

Method: 本文基于跨学科的自主性概念，分析了老年人自主性的四个维度，并提出了三个研究方向。

Result: 提出了四个自主性维度和三个研究方向，旨在解决AI代理在老年人护理中的自主性问题。

Conclusion: 未来研究应关注社会责任感、任务视角的操作化和自主性测量，以确保AI代理更好地满足老年人需求。

Abstract: As the global population ages, artificial intelligence (AI)-powered agents
have emerged as potential tools to support older adults' caregiving. Prior
research has explored agent autonomy by identifying key interaction stages in
task processes and defining the agent's role at each stage. However, ensuring
that agents align with older adults' autonomy preferences remains a critical
challenge. Drawing on interdisciplinary conceptualizations of autonomy, this
paper examines four key dimensions of autonomy for older adults:
decision-making autonomy, goal-oriented autonomy, control autonomy, and social
responsibility autonomy. This paper then proposes the following research
directions: (1) Addressing social responsibility autonomy, which concerns the
ethical and social implications of agent use in communal settings; (2)
Operationalizing agent autonomy from the task perspective; and (3) Developing
autonomy measures.

</details>


### [37] [Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication](https://arxiv.org/abs/2507.13052)
*Tianyu Song,Feng Li,Yuan Bi,Angelos Karlas,Amir Yousefi,Daniela Branzan,Zhongliang Jiang,Ulrich Eck,Nassir Navab*

Main category: cs.HC

TL;DR: 该论文探讨了利用大型语言模型（LLMs）和机器人技术开发智能虚拟超声医师（IVS），以弥合医生-机器人-患者之间的沟通鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在患者-机器人或医生-机器人交互上，而智能虚拟超声医师在医生-机器人-患者沟通中的角色尚未得到充分探索。

Method: 论文提出了一种基于扩展现实（XR）的对话虚拟代理，通过整合LLM驱动的对话、语音转文本、文本转语音和机器人控制，实现医生、机器人超声系统（RUS）和患者之间的实时交互。

Result: 该系统提高了机器人超声采集的效率、清晰度和可访问性，同时增强了医生对机器人交互的控制和信任，改善了患者的体验和接受度。

Conclusion: 该研究为理解IVS如何弥合医生-机器人-患者交互中的沟通鸿沟迈出了第一步，为未来的相关研究提供了基础。

Abstract: The advancement and maturity of large language models (LLMs) and robotics
have unlocked vast potential for human-computer interaction, particularly in
the field of robotic ultrasound. While existing research primarily focuses on
either patient-robot or physician-robot interaction, the role of an intelligent
virtual sonographer (IVS) bridging physician-robot-patient communication
remains underexplored. This work introduces a conversational virtual agent in
Extended Reality (XR) that facilitates real-time interaction between
physicians, a robotic ultrasound system(RUS), and patients. The IVS agent
communicates with physicians in a professional manner while offering empathetic
explanations and reassurance to patients. Furthermore, it actively controls the
RUS by executing physician commands and transparently relays these actions to
the patient. By integrating LLM-powered dialogue with speech-to-text,
text-to-speech, and robotic control, our system enhances the efficiency,
clarity, and accessibility of robotic ultrasound acquisition. This work
constitutes a first step toward understanding how IVS can bridge communication
gaps in physician-robot-patient interaction, providing more control and
therefore trust into physician-robot interaction while improving patient
experience and acceptance of robotic ultrasound.

</details>


### [38] ["What do you expect? You're part of the internet": Analyzing Celebrities' Experiences as Usees of Deepfake Technology](https://arxiv.org/abs/2507.13065)
*John Twomey,Sarah Foley,Sarah Robinson,Michael Quayle,Matthew Peter Aylett,Conor Linehan,Gillian Murphy*

Main category: cs.HC

TL;DR: 分析名人女性和非二元性别者如何应对非自愿合成亲密影像（NSII）的困扰及社会障碍，并提出技术和社会干预方向。


<details>
  <summary>Details</summary>
Motivation: 探讨名人如何应对NSII的危害及社会结构性障碍，呼吁关注技术滥用问题。

Method: 采用批判性话语心理分析法，研究八位名人女性和一位非二元性别者的公开声明。

Result: 名人描述了被NSII非自愿针对的危害及社会/结构性障碍如何阻碍维权。

Conclusion: 提出需要技术和社会干预，挑战NSII背后的价值观和错误信息。

Abstract: Deepfake technology is often used to create non-consensual synthetic intimate
imagery (NSII), mainly of celebrity women. Through Critical Discursive
Psychological analysis we ask; i) how celebrities construct being targeted by
deepfakes and ii) how they navigate infrastructural and social obstacles when
seeking recourse. In this paper, we adopt Baumers concept of Usees
(stakeholders who are non-consenting, unaware and directly targeted by
technology), to understand public statements made by eight celebrity women and
one non-binary individual targeted with NSII. Celebrities describe harms of
being non-consensually targeted by deepfakes and the distress of becoming aware
of these videos. They describe various infrastructural/social factors (e.g.
blaming/ silencing narratives and the industry behind deepfake abuse) which
hinder activism and recourse. This work has implications in recognizing the
roles of various stakeholders in the infrastructures underlying deepfake abuse
and the potential of human-computer interaction to improve existing recourses
for NSII. We also contribute to understanding how false beliefs online
facilitate deepfake abuse. Future work should involve interventions which
challenge the values and false beliefs which motivate NSII
creation/dissemination.

</details>


### [39] [On tangible user interfaces, humans and spatiality](https://arxiv.org/abs/2507.13167)
*Ehud Sharlin,Benjamin Watson,Yoshifumi Kitamura,Fumio Kishino,Yuichi Itoh*

Main category: cs.HC

TL;DR: 论文探讨了如何在可触摸用户界面（TUI）设计中利用人类的空间直觉能力，提出了一套启发式方法，并通过分析现有空间TUI验证了这些方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过利用人类与物理对象的空间关系，提升可触摸用户界面的设计效果，这是TUI成功的关键。

Method: 通过研究人类与物理对象的关系及相关文献，提炼出一套启发式方法，并将其应用于设计空间TUI中。

Result: 提出了空间TUI的概念，并通过分析现有TUI验证了启发式方法的实用性和有效性。

Conclusion: 空间性是TUI设计的重要基石，通过合理应用启发式方法，可以显著提升TUI的用户体验和成功率。

Abstract: Like the prehistoric twig and stone, tangible user interfaces (TUIs) are
objects manipulated by humans. TUI success will depend on how well they exploit
spatiality, the intuitive spatial skills humans have with the objects they use.
In this paper we carefully examine the relationship between humans and physical
objects, and related previous research. From this examination we distill a set
of observations, and turn these into heuristics for incorporation of spatiality
into TUI application design, a cornerstone for their success. Following this
line of thought, we identify spatial TUIs, the subset of TUIs that mediate
interaction with shape, space and structure. We then examine several existing
spatial TUIs using our heuristics.

</details>


### [40] [Difficulty as a Proxy for Measuring Intrinsic Cognitive Load Item](https://arxiv.org/abs/2507.13235)
*Minghao Cai,Guher Gorgun,Carrie Demmans Epp*

Main category: cs.HC

TL;DR: 研究探讨了使用项目难度参数作为在线学习平台中认知负荷代理测量的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统认知负荷测量依赖主观性强的自我报告，研究寻求更客观的替代方法。

Method: 利用项目反应理论推导的项目难度值，分析其对内在和外在负荷的贡献。

Result: 发现项目难度与认知负荷理论一致，可作为内在负荷的代理。

Conclusion: 在建模学习游戏中的认知负荷时，项目难度可作为内在负荷的有效代表。

Abstract: Cognitive load is key to ensuring an optimal learning experience. However,
measuring the cognitive load of educational tasks typically relies on
self-report measures which has been criticized by researchers for being
subjective. In this study, we investigated the feasibility of using item
difficulty parameters as a proxy for measuring cognitive load in an online
learning platform. Difficulty values that were derived using item-response
theory were consistent with theories of how intrinsic and extraneous load
contribute to cognitive load. This finding suggests that we can use item
difficulty to represent intrinsic load when modelling cognitive load in
learning games.

</details>


### [41] [RemVerse: Supporting Reminiscence Activities for Older Adults through AI-Assisted Virtual Reality](https://arxiv.org/abs/2507.13247)
*Ruohao Li,Jiawei Li,Jia Sun,Zhiqing Wu,Zisu Li,Ziyan Wang,Ge Lin Kan,Mingming Fan*

Main category: cs.HC

TL;DR: 论文提出RemVerse，一个结合VR和AI的工具，通过生成视觉提示和互动对话帮助老年人回忆过去，有效提升认知功能和参与感。


<details>
  <summary>Details</summary>
Motivation: 城市化导致熟悉环境消失，传统照片无法完全支持回忆，而VR和AI能动态重建沉浸式环境以辅助回忆。

Method: 设计RemVerse原型，整合生成模型和AI代理到VR中，提供AI生成的视觉提示和互动对话。

Result: 用户研究显示，RemVerse能有效触发、具象化和深化老年人记忆，增强参与感和自主性。

Conclusion: 研究提出设计建议，使AI辅助的VR回忆活动对老年人更易用和吸引。

Abstract: Reminiscence activities, which involve recalling and sharing past
experiences, have proven beneficial for improving cognitive function, mood, and
overall well-being. However, urbanization has led to the disappearance of
familiar environments, removing visual and audio cues for effective
reminiscence. While old photos can serve as visual cues to aid reminiscence, it
is challenging for people to reconstruct the reminisced content and environment
that are not in the photos. Virtual reality (VR) and artificial intelligence
(AI) offer the ability to reconstruct an immersive environment with dynamic
content and to converse with people to help them gradually reminisce. We
designed RemVerse, an AI-empowered VR prototype aimed to support reminiscence
activities. Integrating generative models and AI agent into a VR environment,
RemVerse helps older adults reminisce with AI-generated visual cues and
interactive dialogues. Our user study with 14 older adults showed that RemVerse
effectively supported reminiscence activities by triggering, concretizing, and
deepening personal memories, while fostering increased engagement and autonomy
among older adults. Based on our findings, we proposed design implications to
make reminiscence activities in AI-assisted VR more accessible and engaging for
older adults.

</details>


### [42] [FocusView: Understanding and Customizing Informational Video Watching Experiences for Viewers with ADHD](https://arxiv.org/abs/2507.13309)
*Hanxiu 'Hazel' Zhu,Ruijia Chen,Yuhang Zhao*

Main category: cs.HC

TL;DR: 研究设计了一个名为FocusView的视频定制界面，帮助ADHD患者减少视频中的注意力分散，显著提升了观看体验。


<details>
  <summary>Details</summary>
Motivation: ADHD患者在观看信息性视频时因多媒体元素的干扰而面临注意力问题，需要一种定制化解决方案来提升观看效果。

Method: 设计并评估了FocusView视频定制界面，允许用户从多个方面自定义视频内容，以减少干扰。

Result: FocusView显著提升了视频的可观看性，同时揭示了ADHD用户对干扰感知的多样性和定制偏好。

Conclusion: 研究为未来的ADHD视频定制系统提供了设计建议，强调减少选项以避免定制本身带来的分心。

Abstract: While videos have become increasingly prevalent in delivering information
across different educational and professional contexts, individuals with ADHD
often face attention challenges when watching informational videos due to the
dynamic, multimodal, yet potentially distracting video elements. To understand
and address this critical challenge, we designed \textit{FocusView}, a video
customization interface that allows viewers with ADHD to customize
informational videos from different aspects. We evaluated FocusView with 12
participants with ADHD and found that FocusView significantly improved the
viewability of videos by reducing distractions. Through the study, we uncovered
participants' diverse perceptions of video distractions (e.g., background music
as a distraction vs. stimulation boost) and their customization preferences,
highlighting unique ADHD-relevant needs in designing video customization
interfaces (e.g., reducing the number of options to avoid distraction caused by
customization itself). We further derived design considerations for future
video customization systems for the ADHD community.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [43] [WaFusion: A Wavelet-Enhanced Diffusion Framework for Face Morph Generation](https://arxiv.org/abs/2507.12493)
*Seyed Rasoul Hosseini,Omid Ahmadieh,Jeremy Dawson,Nasser Nasrabadi*

Main category: cs.GR

TL;DR: WaFusion结合小波分解和扩散模型，生成高质量人脸融合图像，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决生物特征人脸融合技术对身份验证系统的安全性和鲁棒性的挑战。

Method: 使用小波变换捕捉结构细节，结合扩散模型的生成能力，高效生成低伪影的人脸融合图像。

Result: 在多个数据集上表现优异，APCER、BPCER和EER等关键指标优于现有方法。

Conclusion: WaFusion为生物特征安全系统提供了高效、前沿的解决方案，设定了新的技术标准。

Abstract: Biometric face morphing poses a critical challenge to identity verification
systems, undermining their security and robustness. To address this issue, we
propose WaFusion, a novel framework combining wavelet decomposition and
diffusion models to generate high-quality, realistic morphed face images
efficiently. WaFusion leverages the structural details captured by wavelet
transforms and the generative capabilities of diffusion models, producing face
morphs with minimal artifacts. Experiments conducted on FERET, FRGC, FRLL, and
WVU Twin datasets demonstrate WaFusion's superiority over state-of-the-art
methods, producing high-resolution morphs with fewer artifacts. Our framework
excels across key biometric metrics, including the Attack Presentation
Classification Error Rate (APCER), Bona Fide Presentation Classification Error
Rate (BPCER), and Equal Error Rate (EER). This work sets a new benchmark in
biometric morph generation, offering a cutting-edge and efficient solution to
enhance biometric security systems.

</details>


### [44] [Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition](https://arxiv.org/abs/2507.12498)
*Beizhen Zhao,Yifan Zhou,Sicheng Yu,Zijian Wang,Hao Wang*

Main category: cs.GR

TL;DR: 该论文提出了一种基于小波分解的解耦优化框架，改进了3D高斯泼溅技术，解决了复杂场景重建中的结构完整性和局部光照问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在复杂场景重建中存在结构不完整和局部光照效果不清晰的问题，需要一种更优的解决方案。

Method: 通过3D小波分解将点云分为高频和低频成分，低频成分优化全局结构和高斯分布，高频成分恢复细节并加入光照模块；同时利用2D小波分解模拟辐射变化，指导细节重建。

Result: 实验表明，该方法在多个数据集上达到最先进性能，优于现有方法。

Conclusion: 该框架显著提升了3D场景重建的质量与效率，推动了该领域的发展。

Abstract: 3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction,
which effectively balances rendering quality, efficiency, and speed. However,
existing 3DGS approaches usually generate plausible outputs and face
significant challenges in complex scene reconstruction, manifesting as
incomplete holistic structural outlines and unclear local lighting effects. To
address these issues simultaneously, we propose a novel decoupled optimization
framework, which integrates wavelet decomposition into 3D Gaussian Splatting
and 2D sampling. Technically, through 3D wavelet decomposition, our approach
divides point clouds into high-frequency and low-frequency components, enabling
targeted optimization for each. The low-frequency component captures global
structural outlines and manages the distribution of Gaussians through
voxelization. In contrast, the high-frequency component restores intricate
geometric and textural details while incorporating a relight module to mitigate
lighting artifacts and enhance photorealistic rendering. Additionally, a 2D
wavelet decomposition is applied to the training images, simulating radiance
variations. This provides critical guidance for high-frequency detail
reconstruction, ensuring seamless integration of details with the global
structure. Extensive experiments on challenging datasets demonstrate our method
achieves state-of-the-art performance across various metrics, surpassing
existing approaches and advancing the field of 3D scene reconstruction.

</details>


### [45] [HairFormer: Transformer-Based Dynamic Neural Hair Simulation](https://arxiv.org/abs/2507.12600)
*Joy Xiaoji Zhang,Jingsen Zhu,Hanyu Chen,Steve Marschner*

Main category: cs.GR

TL;DR: 提出了一种基于Transformer的两阶段神经网络方法，用于模拟任意发型、体型和运动下的头发动态效果。


<details>
  <summary>Details</summary>
Motivation: 模拟不同发型、体型和运动下的头发动态效果是一个重要挑战，此前方法泛化能力不足。

Method: 使用静态网络预测静态发型，解决头发与身体穿透问题；动态网络通过跨注意力机制结合静态特征与运动输入，生成动态效果。同时支持高效微调。

Result: 方法在多种发型下实现高保真动态效果，实时推理能力强，并能处理复杂未见过的长发场景。

Conclusion: 该方法在泛化性和保真度上表现出色，适用于广泛的头发动态模拟场景。

Abstract: Simulating hair dynamics that generalize across arbitrary hairstyles, body
shapes, and motions is a critical challenge. Our novel two-stage neural
solution is the first to leverage Transformer-based architectures for such a
broad generalization. We propose a Transformer-powered static network that
predicts static draped shapes for any hairstyle, effectively resolving
hair-body penetrations and preserving hair fidelity. Subsequently, a dynamic
network with a novel cross-attention mechanism fuses static hair features with
kinematic input to generate expressive dynamics and complex secondary motions.
This dynamic network also allows for efficient fine-tuning of challenging
motion sequences, such as abrupt head movements. Our method offers real-time
inference for both static single-frame drapes and dynamic drapes over pose
sequences. Our method demonstrates high-fidelity and generalizable dynamic hair
across various styles, guided by physics-informed losses, and can resolve
penetrations even for complex, unseen long hairstyles, highlighting its broad
generalization.

</details>


### [46] [VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians](https://arxiv.org/abs/2507.12667)
*Siyuan Yao,Chaoli Wang*

Main category: cs.GR

TL;DR: VolSegGS是一个基于高斯喷射的新框架，支持动态体积场景中的交互式分割和跟踪，为探索性分析和可视化提供实时新视图合成功能。


<details>
  <summary>Details</summary>
Motivation: 大规模时变模拟数据的可视化需要大量资源，现有的视图合成技术虽能生成高质量结果，但缺乏交互性，作者旨在解决这一问题。

Method: 使用可变形3D高斯表示动态场景，结合视图无关颜色和亲和力场网络实现精确分割，并将分割结果嵌入高斯中以实现连续跟踪。

Result: 实验表明，VolSegGS能够实时交互，并提供灵活的分割和跟踪功能，在低计算需求下表现优异。

Conclusion: VolSegGS为时变体积数据的分析和可视化提供了高效、灵活的新解决方案。

Abstract: Visualization of large-scale time-dependent simulation data is crucial for
domain scientists to analyze complex phenomena, but it demands significant I/O
bandwidth, storage, and computational resources. To enable effective
visualization on local, low-end machines, recent advances in view synthesis
techniques, such as neural radiance fields, utilize neural networks to generate
novel visualizations for volumetric scenes. However, these methods focus on
reconstruction quality rather than facilitating interactive visualization
exploration, such as feature extraction and tracking. We introduce VolSegGS, a
novel Gaussian splatting framework that supports interactive segmentation and
tracking in dynamic volumetric scenes for exploratory visualization and
analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic
volumetric scene, allowing for real-time novel view synthesis. For accurate
segmentation, we leverage the view-independent colors of Gaussians for
coarse-level segmentation and refine the results with an affinity field network
for fine-level segmentation. Additionally, by embedding segmentation results
within the Gaussians, we ensure that their deformation enables continuous
tracking of segmented regions over time. We demonstrate the effectiveness of
VolSegGS with several time-varying datasets and compare our solutions against
state-of-the-art methods. With the ability to interact with a dynamic scene in
real time and provide flexible segmentation and tracking capabilities, VolSegGS
offers a powerful solution under low computational demands. This framework
unlocks exciting new possibilities for time-varying volumetric data analysis
and visualization.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [47] [Geometric Theory of Ising Machines](https://arxiv.org/abs/2507.12626)
*Andrew G. Moore,Zachary Richey,Isaac K. Martin*

Main category: cs.ET

TL;DR: 该论文提出了一种用于低温Ising机器的数学设计理论，包括可视化决策边界的图解工具，并证明Ising电路是1-NN分类器的推广，以及局部极小值消除可转化为线性规划问题。


<details>
  <summary>Details</summary>
Motivation: 研究低温Ising机器的数学设计理论，以解决能量函数设计难题，实现高效分布式计算。

Method: 引入一种图解工具可视化Ising电路的决策边界，并应用于两个理论证明。

Result: 证明Ising电路是1-NN分类器的推广，且局部极小值消除问题可转化为线性规划。

Conclusion: 图解工具和理论结果为Ising机器的设计提供了新的数学基础和优化方法。

Abstract: We contribute to the mathematical theory of the design of low temperature
Ising machines, a type of experimental probabilistic computing device
implementing the Ising model. Encoding the output of a function in the ground
state of a physical system allows efficient and distributed computation, but
the design of the energy function is a difficult puzzle. We introduce a
diagrammatic device that allows us to visualize the decision boundaries for
Ising circuits. It is then used to prove two results: (1) Ising circuits are a
generalization of 1-NN classifiers with a certain special structure, and (2)
Elimination of local minima in the energy landscape can be formulated as a
linear programming problem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [48] [Building State Machine Replication Using Practical Network Synchrony](https://arxiv.org/abs/2507.12792)
*Yiliang Wan,Nitin Shivaraman,Akshaye Shenoi,Xiang Liu,Tao Luo,Jialin Li*

Main category: cs.DC

TL;DR: 论文提出了一种基于现代数据中心强同步性的设计，通过优化网络和架构，开发了新复制协议Chora，显著提升了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代分布式协议通常假设网络是部分同步或完全异步的，但作者认为数据中心可通过设计实现强同步性，从而提高性能。

Method: 结合内核旁路网络、多线程架构和宽松的轮次长度，设计出同步性强的网络，并开发了新的复制协议Chora。

Result: Chora在实验中比单领导者和多领导者协议的吞吐量分别提升了255%和109%。

Conclusion: 现代数据中心可通过强同步性设计显著提升分布式协议的效率。

Abstract: Distributed systems, such as state machine replication, are critical
infrastructures for modern applications. Practical distributed protocols make
minimum assumptions about the underlying network: They typically assume a
partially synchronous or fully asynchronous network model. In this work, we
argue that modern data center systems can be designed to provide strong
synchrony properties in the common case, where servers move in synchronous
lock-step rounds. We prove this hypothesis by engineering a practical design
that uses a combination of kernel-bypass network, multithreaded architecture,
and loosened round length, achieving a tight round bound under 2us. Leveraging
our engineered networks with strong synchrony, we co-design a new replication
protocol, Chora. Chora exploits the network synchrony property to efficiently
pipeline multiple replication instances, while allowing all replicas to propose
in parallel without extra coordination. Through experiments, we show that Chora
achieves 255% and 109% improvement in throughput over state-of-the-art
single-leader and multi-leader protocols, respectively.

</details>


### [49] [Autonomous Resource Management in Microservice Systems via Reinforcement Learning](https://arxiv.org/abs/2507.12879)
*Yujun Zou,Nia Qi,Yingnan Deng,Zhihao Xue,Ming Gong,Wuyang Zhang*

Main category: cs.DC

TL;DR: 本文提出了一种基于强化学习的微服务资源调度与优化方法，旨在解决传统微服务架构中资源分配不均、高延迟和吞吐量不足等问题。


<details>
  <summary>Details</summary>
Motivation: 随着微服务系统中服务数量和负载的增加，高效调度和分配计算、内存和存储等资源成为关键研究挑战。

Method: 采用基于强化学习的智能调度算法，通过代理与环境的交互不断优化资源分配策略。

Result: 实验结果表明，该方法在低负载和高并发条件下显著提高了系统响应速度和吞吐量，同时优化了资源利用并降低能耗。

Conclusion: 与传统静态资源分配方法相比，强化学习模型展现出更强的适应性和优化能力，能够在动态变化的负载和资源环境中实时调整策略，保持良好性能。

Abstract: This paper proposes a reinforcement learning-based method for microservice
resource scheduling and optimization, aiming to address issues such as uneven
resource allocation, high latency, and insufficient throughput in traditional
microservice architectures. In microservice systems, as the number of services
and the load increase, efficiently scheduling and allocating resources such as
computing power, memory, and storage becomes a critical research challenge. To
address this, the paper employs an intelligent scheduling algorithm based on
reinforcement learning. Through the interaction between the agent and the
environment, the resource allocation strategy is continuously optimized. In the
experiments, the paper considers different resource conditions and load
scenarios, evaluating the proposed method across multiple dimensions, including
response time, throughput, resource utilization, and cost efficiency. The
experimental results show that the reinforcement learning-based scheduling
method significantly improves system response speed and throughput under low
load and high concurrency conditions, while also optimizing resource
utilization and reducing energy consumption. Under multi-dimensional resource
conditions, the proposed method can consider multiple objectives and achieve
optimized resource scheduling. Compared to traditional static resource
allocation methods, the reinforcement learning model demonstrates stronger
adaptability and optimization capability. It can adjust resource allocation
strategies in real time, thereby maintaining good system performance in
dynamically changing load and resource environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [50] [Transforming Football Data into Object-centric Event Logs with Spatial Context Information](https://arxiv.org/abs/2507.12504)
*Vito Chan,Lennart Ebert,Paul-Julius Hillmann,Christoffer Rubensson,Stephan A. Fahrenkrog-Petersen,Jan Mendling*

Main category: cs.DB

TL;DR: 该论文提出了一种将足球数据转化为对象中心事件日志的框架，并展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的单案例事件日志无法分析复杂和真实的流程行为，且现实中此类日志数量有限，需要更多研究验证其用途。团队运动数据的可用性为对象中心流程挖掘提供了机会。

Method: 提出了一种框架，将足球数据转化为对象中心事件日志，并增加了空间维度。基于真实足球数据生成日志，并讨论了不同流程表示的结果。

Result: 框架成功生成了对象中心事件日志，并展示了其在足球分析中的应用价值。

Conclusion: 这是足球分析中首次使用对象中心事件日志的示例，未来工作应关注变体分析和过滤技术以处理变异性。

Abstract: Object-centric event logs expand the conventional single-case notion event
log by considering multiple objects, allowing for the analysis of more complex
and realistic process behavior. However, the number of real-world
object-centric event logs remains limited, and further studies are needed to
test their usefulness. The increasing availability of data from team sports can
facilitate object-centric process mining, leveraging both real-world data and
suitable use cases. In this paper, we present a framework for transforming
football (soccer) data into an object-centric event log, further enhanced with
a spatial dimension. We demonstrate the effectiveness of our framework by
generating object-centric event logs based on real-world football data and
discuss the results for varying process representations. With our paper, we
provide the first example for object-centric event logs in football analytics.
Future work should consider variant analysis and filtering techniques to better
handle variability

</details>


### [51] [Rel-HNN: Split Parallel Hypergraph Neural Network for Learning on Relational Databases](https://arxiv.org/abs/2507.12562)
*Md. Tanvir Alam,Md. Ahasanul Alam,Md Mahmudur Rahman,Md. Mosaddek Khan*

Main category: cs.DB

TL;DR: 本文提出了一种名为rel-HNN的新型超图框架，用于从关系数据库中捕获细粒度的关联关系，并通过多级表示学习和并行训练算法提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络在处理关系数据库时简化了关系结构，无法捕获内部元组关联，导致性能受限。

Method: 提出rel-HNN框架，将属性-值对作为节点，元组作为超边，实现细粒度关系建模，并引入多GPU并行训练算法以提升效率。

Result: 实验表明，rel-HNN在分类和回归任务上显著优于现有方法，并行训练速度提升最高达3.18倍。

Conclusion: rel-HNN通过超图建模和并行训练有效解决了关系数据库中的语义捕获和可扩展性问题，性能优越。

Abstract: Relational databases (RDBs) are ubiquitous in enterprise and real-world
applications. Flattening the database poses challenges for deep learning models
that rely on fixed-size input representations to capture relational semantics
from the structured nature of relational data. Graph neural networks (GNNs)
have been proposed to address this, but they often oversimplify relational
structures by modeling all the tuples as monolithic nodes and ignoring
intra-tuple associations. In this work, we propose a novel hypergraph-based
framework, that we call rel-HNN, which models each unique attribute-value pair
as a node and each tuple as a hyperedge, enabling the capture of fine-grained
intra-tuple relationships. Our approach learns explicit multi-level
representations across attribute-value, tuple, and table levels. To address the
scalability challenges posed by large RDBs, we further introduce a
split-parallel training algorithm that leverages multi-GPU execution for
efficient hypergraph learning. Extensive experiments on real-world and
benchmark datasets demonstrate that rel-HNN significantly outperforms existing
methods in both classification and regression tasks. Moreover, our
split-parallel training achieves substantial speedups -- up to 3.18x for
learning on relational data and up to 2.94x for hypergraph learning -- compared
to conventional single-GPU execution.

</details>


### [52] [Targeted Mining of Time-Interval Related Patterns](https://arxiv.org/abs/2507.12668)
*Shuang Liang,Lili Chen,Wensheng Gan,Philip S. Yu,Shengjie Zhao*

Main category: cs.DB

TL;DR: 提出了TaTIRP算法，用于高效挖掘基于特定标准的时间间隔相关模式，并结合剪枝策略优化性能，实验验证了其准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的序列模式挖掘忽视事件持续时间，时间间隔模式（TIRP）挖掘为解决这一问题而生，但全模式挖掘效率低且资源消耗大。

Method: 提出TaTIRP算法，结合多种剪枝策略减少冗余操作，针对特定标准挖掘时间间隔相关模式。

Result: 在多种真实和合成数据集上验证了算法的准确性和高效性。

Conclusion: TaTIRP算法通过剪枝策略显著提升时间间隔模式挖掘效率，适用于大规模数据分析。

Abstract: Compared to frequent pattern mining, sequential pattern mining emphasizes the
temporal aspect and finds broad applications across various fields. However,
numerous studies treat temporal events as single time points, neglecting their
durations. Time-interval-related pattern (TIRP) mining is introduced to address
this issue and has been applied to healthcare analytics, stock prediction, etc.
Typically, mining all patterns is not only computationally challenging for
accurate forecasting but also resource-intensive in terms of time and memory.
Targeting the extraction of time-interval-related patterns based on specific
criteria can improve data analysis efficiency and better align with customer
preferences. Therefore, this paper proposes a novel algorithm called TaTIRP to
discover Targeted Time-Interval Related Patterns. Additionally, we develop
multiple pruning strategies to eliminate redundant extension operations,
thereby enhancing performance on large-scale datasets. Finally, we conduct
experiments on various real-world and synthetic datasets to validate the
accuracy and efficiency of the proposed algorithm.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [53] [Modular SAIL: dream or reality?](https://arxiv.org/abs/2507.12471)
*Petr Kourzanov,Anmol*

Main category: cs.AR

TL;DR: 论文研究了如何在RISC-V开发流程中实现模块化，并通过实验验证了模块化SAIL的可行性。


<details>
  <summary>Details</summary>
Motivation: 为了解决RISC-V ISA模块化的组合性问题，覆盖更广泛的开发生态，如仿真、模拟和验证。

Method: 引入了模块化SAIL，改造SAIL-RISCV黄金模型以支持模块化，并在模拟器层面实现静态和动态绑定。

Result: 实验表明，模块化SAIL模拟器的功能行为与原始整体式模拟器一致，且性能表现相近。

Conclusion: 模块化SAIL在技术上可行，为RISC-V生态的进一步发展提供了可能。

Abstract: In order to truly benefit from RISC-V ISA modularity, the community has to
address the issue of compositionality, going beyond modules at the
specification level covering larger subsets of the RISC-V development flow
including emulation, simulation and verification. In this paper we introduce
modular SAIL, an experiment to inject compositionality into the SAIL-RISCV
golden model. We show that it is, in principle, not difficult to adapt the
SAIL-RISCV flow (and ideally the SAIL compiler itself) to support modules at
the emulator level. We back our findings by a comparative study of the
resulting pluggable emulator's performance using both static and dynamic
binding, which both exhibit same functional behavior as the original monolithic
emulator (aka RISC-V ISS).

</details>


### [54] [An ultra-low-power CGRA for accelerating Transformers at the edge](https://arxiv.org/abs/2507.12904)
*Rohit Prasad*

Main category: cs.AR

TL;DR: 本文提出了一种专为边缘设备设计的低功耗CGRA架构，用于加速Transformer模型中的GEMM操作。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在边缘设备上的高计算需求限制了其部署，因此需要一种低功耗的解决方案。

Method: 采用4x4 PE阵列和4x2 MOB模块，结合无开关网络设计，优化并行计算和内存操作。

Result: 该架构显著降低了内存带宽需求并提升了数据重用效率。

Conclusion: 这种CGRA架构为在边缘设备上部署复杂的机器学习模型提供了可行的解决方案。

Abstract: Transformers have revolutionized deep learning with applications in natural
language processing, computer vision, and beyond. However, their computational
demands make it challenging to deploy them on low-power edge devices. This
paper introduces an ultra-low-power, Coarse-Grained Reconfigurable Array (CGRA)
architecture specifically designed to accelerate General Matrix Multiplication
(GEMM) operations in transformer models tailored for the energy and resource
constraints of edge applications. The proposed architecture integrates a 4 x 4
array of Processing Elements (PEs) for efficient parallel computation and
dedicated 4 x 2 Memory Operation Blocks (MOBs) for optimized LOAD/STORE
operations, reducing memory bandwidth demands and enhancing data reuse. A
switchless mesh torus interconnect network further minimizes power and latency
by enabling direct communication between PEs and MOBs, eliminating the need for
centralized switching. Through its heterogeneous array design and efficient
dataflow, this CGRA architecture addresses the unique computational needs of
transformers, offering a scalable pathway to deploy sophisticated machine
learning models on edge devices.

</details>


### [55] [WIP: Turning Fake Chips into Learning Opportunities](https://arxiv.org/abs/2507.13281)
*Haniye Mehraban,Saad Azmeen-ur-Rahman,John Hu*

Main category: cs.AR

TL;DR: 本文通过一个案例研究，将课程中发现的假冒TL074运算放大器转化为实践学习机会，帮助学生深入理解模拟电路和供应链安全。


<details>
  <summary>Details</summary>
Motivation: 假冒集成电路在本科电子实验室中越来越常见，威胁实验的完整性。论文旨在通过实际问题提升学生的实践能力。

Method: 将假冒组件作为教学工具，让学生通过测量电流、分析波形和故障排查等实践操作学习。

Result: 学生通过接触假冒芯片组件，更深入地理解了模拟电路、供应链安全和实际工程问题。

Conclusion: 利用实际问题作为教学资源，可以有效提升学生的实践能力和工程洞察力。

Abstract: This work-in-progress paper presents a case study in which counterfeit TL074
operational amplifiers, discovered in a junior level electronics course, became
the basis for a hands on learning experience. Counterfeit integrated circuits
(IC) are increasingly common, posing a significant threat to the integrity of
undergraduate electronics laboratories. Instead of simply replacing the
counterfeit components, we turned the issue into a teaching moment. Students
engaged in hands-on diagnostics measuring current, analyzing waveforms, and
troubleshooting. By working with fake chip components, they gained deeper
insight into analog circuits, supply chain security, and practical engineering.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [56] [Learning-Based Interface for Semantic Communication with Bit Importance Awareness](https://arxiv.org/abs/2507.12850)
*Wenzheng Kong,Wenyi Zhang*

Main category: cs.IT

TL;DR: 提出一种基于学习的比特级接口设计，改进Split DeepJSCC的端到端性能，并设计重要性感知网络以动态适应信道条件。


<details>
  <summary>Details</summary>
Motivation: 解决现有JSCC方法难以与通信网络架构兼容的问题，同时保留语义保真和信道适应性优势。

Method: 采用可训练的比特级接口设计，并引入重要性感知网络动态适应信道带宽比和时变条件。

Result: 实验表明方法在无线图像传输任务中提升了性能。

Conclusion: 为现有无线网络中实现语义通信提供了潜在解决方案。

Abstract: Joint source-channel coding (JSCC) is an effective approach for semantic
communication. However, current JSCC methods are difficult to integrate with
existing communication network architectures, where application and network
providers are typically different entities. Recently, a novel paradigm termed
Split DeepJSCC has been under consideration to address this challenge. Split
DeepJSCC employs a bit-level interface that enables separate design of source
and channel codes, ensuring compatibility with existing communication networks
while preserving the advantages of JSCC in terms of semantic fidelity and
channel adaptability. In this paper, we propose a learning-based interface
design by treating its parameters as trainable, achieving improved end-to-end
performance compared to Split DeepJSCC. In particular, the interface enables
specification of bit-level importance at the output of the source code.
Furthermore, we propose an Importance-Aware Net that utilizes the
interface-derived bit importance information, enabling dynamical adaptation to
diverse channel bandwidth ratios and time-varying channel conditions.
Experimental results show that our method improves performance in wireless
image transmission tasks. This work provides a potential solution for realizing
semantic communications in existing wireless networks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/abs/2507.12714)
*Yang Yang,Dongni Mao,Hiroaki Santo,Yasuyuki Matsushita,Fumio Okura*

Main category: cs.CV

TL;DR: 提出了一种名为NeuraLeaf的神经参数模型，用于3D叶子的建模和重建，解决了植物叶子多样形状和灵活变形的挑战。


<details>
  <summary>Details</summary>
Motivation: 农业和计算机图形学中需要精确的3D植物建模，尤其是叶子的多样性形状和变形问题。现有神经参数模型主要针对人类和动物，植物叶子建模面临独特挑战。

Method: NeuraLeaf将叶子几何解耦为2D基础形状和3D变形，利用丰富的2D叶子图像数据集学习基础形状，并提出无骨架的蒙皮模型处理3D变形。数据集DeformLeaf支持学习。

Result: NeuraLeaf能够生成多样化的叶子形状并精确拟合3D观测数据（如深度图和点云）。

Conclusion: NeuraLeaf在农业和计算机图形领域具有应用潜力，其实现和数据集已公开。

Abstract: We develop a neural parametric model for 3D leaves for plant modeling and
reconstruction that are essential for agriculture and computer graphics. While
neural parametric models are actively studied for humans and animals, plant
leaves present unique challenges due to their diverse shapes and flexible
deformation. To this problem, we introduce a neural parametric model for
leaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be
approximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into
their 2D base shapes and 3D deformations. This representation allows learning
from rich sources of 2D leaf image datasets for the base shapes, and also has
the advantage of simultaneously learning textures aligned with the geometry. To
model the 3D deformation, we propose a novel skeleton-free skinning model and
create a newly captured 3D leaf dataset called DeformLeaf. We show that
NeuraLeaf successfully generates a wide range of leaf shapes with deformation,
resulting in accurate model fitting to 3D observations like depth maps and
point clouds. Our implementation and dataset are available at
https://neuraleaf-yang.github.io/.

</details>


### [58] [Predicting 3D Rigid Body Dynamics with Deep Residual Network](https://arxiv.org/abs/2407.18798)
*Abiodun Finbarrs Oketunji*

Main category: cs.CV

TL;DR: 该研究探索了深度残差网络在预测三维刚体相互作用动态中的应用，结合C++物理模拟器和PyTorch深度学习模型，实现了对复杂3D动力学的高效建模。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用深度残差网络捕捉三维刚体间的复杂物理交互，填补物理信息机器学习在此领域的空白。

Method: 方法包括通过C++物理模拟器生成训练数据（涵盖线性/角运动、碰撞等），并使用PyTorch构建深度残差网络进行预测。

Result: 模型在10,000个模拟场景中表现优异，位置和方向预测误差分别为0.015和0.022，较基线方法提升25%。

Conclusion: 结论表明深度残差网络对复杂3D物理系统建模潜力巨大，未来可扩展至更多物体形状和材料的研究。

Abstract: This study investigates the application of deep residual networks for
predicting the dynamics of interacting three-dimensional rigid bodies. We
present a framework combining a 3D physics simulator implemented in C++ with a
deep learning model constructed using PyTorch. The simulator generates training
data encompassing linear and angular motion, elastic collisions, fluid
friction, gravitational effects, and damping. Our deep residual network,
consisting of an input layer, multiple residual blocks, and an output layer, is
designed to handle the complexities of 3D dynamics. We evaluate the network's
performance using a datasetof 10,000 simulated scenarios, each involving 3-5
interacting rigid bodies. The model achieves a mean squared error of 0.015 for
position predictions and 0.022 for orientation predictions, representing a 25%
improvement over baseline methods. Our results demonstrate the network's
ability to capture intricate physical interactions, with particular success in
predicting elastic collisions and rotational dynamics. This work significantly
contributes to physics-informed machine learning by showcasing the immense
potential of deep residual networks in modeling complex 3D physical systems. We
discuss our approach's limitations and propose future directions for improving
generalization to more diverse object shapes and materials.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [59] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 研究者提出了一种结合分布式和符号表征的模型合成架构（MSA），用于模拟人类在新情境下的推理能力，并通过实验验证其优于纯语言模型的基线。


<details>
  <summary>Details</summary>
Motivation: 探索人类在面对新情境时如何整合广泛的背景知识进行推理，并提出一种计算模型来模拟这一能力。

Method: 提出MSA架构，结合语言模型实现全局相关性检索和模型合成，使用概率程序构建定制化的世界模型，并通过一个新颖的推理数据集（Model Olympics）评估其表现。

Result: MSA在模拟人类判断方面优于纯语言模型基线，特别是在处理新因果结构和背景知识时。

Conclusion: MSA为理解和复制人类在开放领域中的推理能力提供了一种可行路径。

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [60] [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)
*Lyucheng Wu,Mengru Wang,Ziwen Xu,Tri Cao,Nay Oo,Bryan Hooi,Shumin Deng*

Main category: cs.CL

TL;DR: AutoSteer是一种无需调整模型的模块化干预技术，通过安全性评分、自适应探测器和轻量级拒绝模块，显著降低多模态模型的对抗攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在跨模态推理方面表现出强大能力，但也面临对抗性多模态输入的安全隐患。为提高推理安全性，提出了AutoSteer。

Method: AutoSteer包含三个核心组件：安全性感知评分（SAS）、自适应安全探测器和轻量级拒绝模块，用于识别安全风险并选择性干预生成。

Result: 在LLaVA-OV和Chameleon等基准测试中，AutoSteer显著降低文本、视觉和跨模态威胁的攻击成功率，同时不损害模型的通用能力。

Conclusion: AutoSteer为多模态AI系统的安全部署提供了一种实用、可解释且有效的框架。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has unlocked
powerful cross-modal reasoning abilities, but also raised new safety concerns,
particularly when faced with adversarial multimodal inputs. To improve the
safety of MLLMs during inference, we introduce a modular and adaptive
inference-time intervention technology, AutoSteer, without requiring any
fine-tuning of the underlying model. AutoSteer incorporates three core
components: (1) a novel Safety Awareness Score (SAS) that automatically
identifies the most safety-relevant distinctions among the model's internal
layers; (2) an adaptive safety prober trained to estimate the likelihood of
toxic outputs from intermediate representations; and (3) a lightweight Refusal
Head that selectively intervenes to modulate generation when safety risks are
detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical
benchmarks demonstrate that AutoSteer significantly reduces the Attack Success
Rate (ASR) for textual, visual, and cross-modal threats, while maintaining
general abilities. These findings position AutoSteer as a practical,
interpretable, and effective framework for safer deployment of multimodal AI
systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [61] [Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries](https://arxiv.org/abs/2507.12723)
*Minyoung Kim,Sehwan Park,Sungmin Cha,Paul Hongsuck Seo*

Main category: cs.SD

TL;DR: 论文提出了一种针对合成视听伪造（SAVF）的跨模态水印框架，用于恢复真实音频（AAR）和定位篡改（TLA），以抵御虚假信息的传播。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅能检测或定位篡改，无法恢复传递消息语义的真实音频，限制了其对抗视听虚假信息的有效性。

Method: 提出了一种跨模态水印框架，在篡改前将真实音频嵌入到视觉信息中。

Result: 方法在恢复真实音频和定位篡改方面表现优异，能有效应对多种篡改手段。

Conclusion: 该框架为SAVF问题提供了新的解决方案，显著提升了对抗虚假信息的能力。

Abstract: Recent advances in voice cloning and lip synchronization models have enabled
Synthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are
manipulated to mimic a target speaker. This significantly increases the risk of
misinformation by making fake content seem real. To address this issue,
existing methods detect or localize manipulations but cannot recover the
authentic audio that conveys the semantic content of the message. This
limitation reduces their effectiveness in combating audiovisual misinformation.
In this work, we introduce the task of Authentic Audio Recovery (AAR) and
Tamper Localization in Audio (TLA) from SAVFs and propose a cross-modal
watermarking framework to embed authentic audio into visuals before
manipulation. This enables AAR, TLA, and a robust defense against
misinformation. Extensive experiments demonstrate the strong performance of our
method in AAR and TLA against various manipulations, including voice cloning
and lip synchronization.

</details>


### [62] [Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy Protection against Voice Deepfakes](https://arxiv.org/abs/2507.12932)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.SD

TL;DR: Enkidu是一个新型的面向用户的隐私保护框架，通过黑盒知识和少量用户数据训练生成通用频域扰动，实时轻量地防御个性化语音深度伪造威胁。


<details>
  <summary>Details</summary>
Motivation: 语音深度伪造技术的发展严重威胁用户音频隐私，现有防御方法存在适应性差、可扩展性不足、依赖白盒知识和高计算成本等问题。

Method: 利用黑盒知识和少量用户数据训练生成通用频域噪声补丁，实现实时、轻量的保护。

Result: Enkidu在处理内存效率（低至0.004GB）和运行时效率（实时系数低至0.004）上显著优于现有方法，且在六种主流TTS模型和五种ASV模型上表现出强防御能力。

Conclusion: Enkidu在防御普通和自适应语音深度伪造攻击中表现出高效性、可转移性和实用性。

Abstract: The rapid advancement of voice deepfake technologies has raised serious
concerns about user audio privacy, as attackers increasingly exploit publicly
available voice data to generate convincing fake audio for malicious purposes
such as identity theft, financial fraud, and misinformation campaigns. While
existing defense methods offer partial protection, they face critical
limitations, including weak adaptability to unseen user data, poor scalability
to long audio, rigid reliance on white-box knowledge, and high computational
and temporal costs during the encryption process. To address these challenges
and defend against personalized voice deepfake threats, we propose Enkidu, a
novel user-oriented privacy-preserving framework that leverages universal
frequential perturbations generated through black-box knowledge and few-shot
training on a small amount of user data. These highly malleable
frequency-domain noise patches enable real-time, lightweight protection with
strong generalization across variable-length audio and robust resistance to
voice deepfake attacks, all while preserving perceptual quality and speech
intelligibility. Notably, Enkidu achieves over 50 to 200 times processing
memory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime
efficiency (real-time coefficient as low as 0.004) compared to six
state-of-the-art countermeasures. Extensive experiments across six mainstream
text-to-speech models and five cutting-edge automated speaker verification
models demonstrate the effectiveness, transferability, and practicality of
Enkidu in defending against both vanilla and adaptive voice deepfake attacks.

</details>


### [63] [Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features](https://arxiv.org/abs/2507.12793)
*J. M. Chan Sri Manukalpa,H. S. Bopage,W. A. M. Jayawardena,P. K. P. G. Panduwawala*

Main category: cs.SD

TL;DR: 该研究提出了一种基于深度学习的非侵入式声学分类框架，用于早期白蚁检测，结合卷积神经网络和长短期记忆网络，显著提升了检测准确率和时效性。


<details>
  <summary>Details</summary>
Motivation: 白蚁等结构害虫对木制建筑造成隐蔽且渐进性的损害，传统检测方法侵入性强、耗时且难以早期发现。

Method: 采用混合卷积神经网络-长短期记忆网络（CNN-LSTM）架构，提取梅尔频率倒谱系数（MFCC）特征，对白蚁声学信号进行分类。

Result: 模型表现优异，准确率达94.5%，精确率和召回率分别为93.2%和95.8%，误报率低。

Conclusion: 该研究为非侵入式白蚁早期检测提供了自动化解决方案，未来可结合物联网技术拓展应用。

Abstract: Structural pests, such as termites, pose a serious threat to wooden
buildings, resulting in significant economic losses due to their hidden and
progressive damage. Traditional detection methods, such as visual inspections
and chemical treatments, are invasive, labor intensive, and ineffective for
early stage infestations. To bridge this gap, this study proposes a non
invasive deep learning based acoustic classification framework for early
termite detection. We aim to develop a robust, scalable model that
distinguishes termite generated acoustic signals from background noise. We
introduce a hybrid Convolutional Neural Network Long Short Term Memory
architecture that captures both spatial and temporal features of termite
activity. Audio data were collected from termite infested and clean wooden
samples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN
LSTM model to classify the signals. Experimental results show high performance,
with 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis
reveals that the hybrid model outperforms standalone CNN and LSTM
architectures, underscoring its combined strength. Notably, the model yields
low false-negative rates, which is essential for enabling timely intervention.
This research contributes a non invasive, automated solution for early termite
detection, with practical implications for improved pest monitoring, minimized
structural damage, and better decision making by homeowners and pest control
professionals. Future work may integrate IoT for real time alerts and extend
detection to other structural pests.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [64] [Physically Based Neural LiDAR Resimulation](https://arxiv.org/abs/2507.12489)
*Richard Marcus,Marc Stamminger*

Main category: cs.RO

TL;DR: 本文提出了一种新型LiDAR模拟方法，通过明确建模传感器特性（如滚动快门、激光功率变化等），实现了比现有技术更准确的LiDAR模拟。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR模拟在传感器特定效果（如滚动快门）方面表现不足，需要更精确的模拟方法。

Method: 通过建模传感器特性（如滚动快门、激光功率变化）来改进LiDAR模拟。

Result: 定量和定性比较表明，该方法优于现有技术，并能生成高分辨率LiDAR扫描。

Conclusion: 该研究不仅提升了LiDAR模拟的准确性，还展示了高效的重模拟能力。

Abstract: Methods for Novel View Synthesis (NVS) have recently found traction in the
field of LiDAR simulation and large-scale 3D scene reconstruction. While
solutions for faster rendering or handling dynamic scenes have been proposed,
LiDAR specific effects remain insufficiently addressed. By explicitly modeling
sensor characteristics such as rolling shutter, laser power variations, and
intensity falloff, our method achieves more accurate LiDAR simulation compared
to existing techniques. We demonstrate the effectiveness of our approach
through quantitative and qualitative comparisons with state-of-the-art methods,
as well as ablation studies that highlight the importance of each sensor model
component. Beyond that, we show that our approach exhibits advanced
resimulation capabilities, such as generating high resolution LiDAR scans in
the camera perspective.
  Our code and the resulting dataset are available at
https://github.com/richardmarcus/PBNLiDAR.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [65] [Catching Dark Signals in Algorithms: Unveiling Audiovisual and Thematic Markers of Unsafe Content Recommended for Children and Teenagers](https://arxiv.org/abs/2507.12571)
*Haoning Xue,Brian Nishimine,Martin Hilbert,Drew Cingel,Samantha Vigil,Jane Shawcroft,Arti Thakur,Zubair Shafiq,Jingwen Zhang*

Main category: cs.CY

TL;DR: 短期视频平台对儿童和青少年的潜在危害受到关注，研究通过分析多平台推荐内容，揭示了显性和隐性风险，提出了保护措施。


<details>
  <summary>Details</summary>
Motivation: 由于年龄验证机制无效，儿童和青少年在算法主导的在线环境中面临潜在危害，研究旨在揭示这些风险。

Method: 通过对Instagram Reels、TikTok和YouTube Shorts上的4,492条短视频进行多模态特征分析和主题建模，进行算法审计实验。

Result: 不安全的视频具有较暗的视觉特征，并包含显性的有害内容和隐性的焦虑诱导内容。提出了显性、隐性和意外危害的框架。

Conclusion: 研究呼吁采取针对性的内容审核、年龄验证和平台监管措施，以保护儿童和青少年免受社交媒体上的多维度风险。

Abstract: The prevalence of short form video platforms, combined with the
ineffectiveness of age verification mechanisms, raises concerns about the
potential harms facing children and teenagers in an algorithm-moderated online
environment. We conducted multimodal feature analysis and thematic topic
modeling of 4,492 short videos recommended to children and teenagers on
Instagram Reels, TikTok, and YouTube Shorts, collected as a part of an
algorithm auditing experiment. This feature-level and content-level analysis
revealed that unsafe (i.e., problematic, mentally distressing) short videos (a)
possess darker visual features and (b) contain explicitly harmful content and
implicit harm from anxiety-inducing ordinary content. We introduce a useful
framework of online harm (i.e., explicit, implicit, unintended), providing a
unique lens for understanding the dynamic, multifaceted online risks facing
children and teenagers. The findings highlight the importance of protecting
younger audiences in critical developmental stages from both explicit and
implicit risks on social media, calling for nuanced content moderation, age
verification, and platform regulation.

</details>


### [66] [Rookie Mistakes: Measuring Software Quality in Student Projects to Guide Educational Enhancement](https://arxiv.org/abs/2507.12488)
*Marco De Luca,Sergio Di Martino,Sergio Di Meglio,Anna Rita Fasolino,Luigi Libero Lucio Starace,Porfirio Tramontana*

Main category: cs.CY

TL;DR: 论文研究了在本科编程课程中如何更有效地教授软件质量，分析了学生团队项目的质量问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决本科编程教学中软件质量教育不足的问题，研究填补了现有文献对中级水平学生在复杂项目中的质量问题的研究空白。

Method: 分析了83个由172名学生开发的对象导向团队项目，使用SonarQube和ArchUnit进行静态分析，检测代码异味和架构反模式。

Result: 研究发现学生团队项目中存在重复的质量问题，为课程改进提供了具体依据。

Conclusion: 研究结果为教育者改进软件工程课程和推广质量导向的开发实践提供了有价值的指导。

Abstract: When teaching Programming and Software Engineering in Bachelor's Degree
programs, the emphasis on creating functional software projects often
overshadows the focus on software quality, a trend that aligns with ACM
curricula recommendations. Software Engineering courses are typically
introduced later in the curriculum, and can generally allocate only limited
time to quality-related topics, leaving educators with the challenge of
deciding which quality aspects to prioritize. In this decision, the literature
offers limited guidance, as most existing studies focus on code written by
novice students and small code units, making it unclear whether those findings
extend to intermediate-level students with foundational object-oriented
programming skills working on more complex software projects. To address this
gap, we analyze 83 object-oriented team projects developed by 172 university
students across 4 different editions of the Object-Oriented Programming course.
We apply a static analysis pipeline used in prior research to assess software
quality, combining SonarQube and ArchUnit to detect code smells and
architectural anti-patterns. Our findings highlight recurring quality issues
and offer concrete evidence of the challenges students face at this stage,
providing valuable guidance for educators aiming to continuously improve
Software Engineering curricula and promote quality-oriented development
practices.

</details>


### [67] [ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle](https://arxiv.org/abs/2507.12674)
*Mihran Miroyan,Rose Niousha,Joseph E. Gonzalez,Gireeja Ranade,Narges Norouzi*

Main category: cs.CY

TL;DR: 该论文研究了大型语言模型（LLM）能否生成类似真实学生的代码，并提出了一种方法“ParaStudent”来分析其在编程课程中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否能够生成与真实学生代码相似的不完美、迭代且风格多样的代码，以模拟学生的学习过程。

Method: 利用多学期的学生提交数据设计实验，从语义、功能、和风格维度评估代码输出，并进行微调以提高模型表现。

Result: 研究表明，微调能显著提升模型与真实学生学习轨迹的一致性，并能更好地捕捉错误模式、增量改进和风格变化。

Conclusion: 模拟真实学生代码需要采用上下文感知生成、时间建模和多维评估等方法。

Abstract: Large Language Models (LLMs) have shown strong performance on programming
tasks, but can they generate student-like code like real students - imperfect,
iterative, and stylistically diverse? We present ParaStudent, a systematic
study of LLM-based "student-like" code generation in an introductory
programming course setting. Using a dataset of timestamped student submissions
across multiple semesters, we design low- and high-resolution experiments to
model student progress and evaluate code outputs along semantic, functional,
and stylistic dimensions. Our results show that fine-tuning significantly
improves alignment with real student trajectories and captures error patterns,
incremental improvements, and stylistic variations more faithfully. This study
shows that modeling realistic student code requires capturing learning dynamics
through context-aware generation, temporal modeling, and multi-dimensional
evaluation. Code for experiments and evaluation is available at
\href{https://github.com/mmiroyan/ParaStudent}{\texttt{github.com/mmiroyan/ParaStudent}}.

</details>


### [68] [The Case for Contextual Copyleft: Licensing Open Source Training Data and Generative AI](https://arxiv.org/abs/2507.12713)
*Grant Shanklin,Emmie Hine,Claudio Novelli,Tyler Schroder,Luciano Floridi*

Main category: cs.CY

TL;DR: CCAI许可是一种新型开源许可机制，将传统copyleft原则扩展到AI模型，以保护开源训练数据，促进开源AI发展，但需结合监管平衡风险。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI对FOSS社区的挑战，特别是开源代码用于训练AI模型时的版权问题。

Method: 提出CCAI许可，并通过三部分评估框架（法律可行性、政策合理性、跨情境分析）验证其有效性。

Result: CCAI许可能增强开发者控制、激励开源AI开发，但需监管辅助以平衡风险。

Conclusion: 在负责任的监管环境下，CCAI许可是将FOSS原则适配生成式AI发展的可行方案。

Abstract: The proliferation of generative AI systems has created new challenges for the
Free and Open Source Software (FOSS) community, particularly regarding how
traditional copyleft principles should apply when open source code is used to
train AI models. This article introduces the Contextual Copyleft AI (CCAI)
license, a novel licensing mechanism that extends copyleft requirements from
training data to the resulting generative AI models. The CCAI license offers
significant advantages, including enhanced developer control, incentivization
of open source AI development, and mitigation of openwashing practices. This is
demonstrated through a structured three-part evaluation framework that examines
(1) legal feasibility under current copyright law, (2) policy justification
comparing traditional software and AI contexts, and (3) synthesis of
cross-contextual benefits and risks. However, the increased risk profile of
open source AI, particularly the potential for direct misuse, necessitates
complementary regulatory approaches to achieve an appropriate risk-benefit
balance. The paper concludes that when implemented within a robust regulatory
environment focused on responsible AI usage, the CCAI license provides a viable
mechanism for preserving and adapting core FOSS principles to the evolving
landscape of generative AI development.

</details>


### [69] [Bridging Boundaries: How to Foster Effective Research Collaborations Across Affiliations in the Field of Trust and Safety](https://arxiv.org/abs/2507.13008)
*Amanda Menking,Mona Elswah,David J. Grüning,Lasse H. Hansen,Irene Huang,Julia Kamin,Catrine Normann*

Main category: cs.CY

TL;DR: 该论文探讨了如何在数字空间的信任与安全领域开展跨部门研究合作，提出了一个实用框架以确保合作的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着数字空间中信任与安全领域的发展，跨学术、行业、政府和非政府部门的合作变得日益重要且复杂，需要克服激励、时间线和约束的不匹配问题。

Method: 论文提出了一个实用的逐步框架，包括建立信任、目标对齐和角色分配的策略，强调了合作中沟通与协调的重要性。

Result: 论文通过实际经验总结了不同类型部门的差异，并展示了如何通过跨部门合作开展更符合伦理、公平且具有实际影响力的研究。

Conclusion: 倡导优先考虑包容性、透明度和现实相关性的合作模式，以应对这一新兴领域的跨学科需求。

Abstract: As the field of Trust and Safety in digital spaces continues to grow, it has
become increasingly necessary - but also increasingly complex - to collaborate
on research across the academic, industry, governmental and non-governmental
sectors. This paper examines how cross-affiliation research partnerships can be
structured to overcome misaligned incentives, timelines and constraints while
delivering on the unique strengths of each stakeholder. Drawing on our own
experience of cross-sector collaboration, we define the main types of
affiliation and highlight the common differences in research priorities,
operational pressures and evaluation metrics across sectors. We then propose a
practical, step-by-step framework for initiating and managing effective
collaborations, including strategies for building trust, aligning goals, and
distributing roles. We emphasize the critical yet often invisible work of
articulation and argue that cross-sector partnerships are essential for
developing more ethical, equitable and impactful research in trust and safety.
Ultimately, we advocate collaborative models that prioritize inclusivity,
transparency and real-world relevance in order to meet the interdisciplinary
demands of this emerging field.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [70] [Efficiently Constructing Sparse Navigable Graphs](https://arxiv.org/abs/2507.13296)
*Alex Conway,Laxman Dhulipala,Martin Farach-Colton,Rob Johnson,Ben Landrum,Christopher Musco,Yarin Shechter,Torsten Suel,Richard Wen*

Main category: cs.DS

TL;DR: 该论文研究了快速构建稀疏可导航搜索图的算法，通过改进贪心算法和利用流式与次线性集合覆盖技术，提出了一种近似最优的构建方法。


<details>
  <summary>Details</summary>
Motivation: 目前图基最近邻搜索方法构建稀疏可导航图的计算成本高昂，且依赖启发式方法，缺乏理论保证，因此需要高效的算法来解决这一问题。

Method: 通过将稀疏可导航图构建问题建模为多个相关的最小集合覆盖实例，结合流式与次线性集合覆盖算法，提出了一种时间复杂度为$	ilde{O}(n^2)$的近似算法。

Result: 提出的算法能够在$	ilde{O}(n^2)$时间内构建稀疏度为$O(\log n)$近似最优的可导航图，并可扩展解决其他相关图构建问题。

Conclusion: 该工作不仅提供了理论保证的高效算法，还证明了改进近似比的困难性，填补了图基最近邻搜索方法中图构建的理论空白。

Abstract: Graph-based nearest neighbor search methods have seen a surge of popularity
in recent years, offering state-of-the-art performance across a wide variety of
applications. Central to these methods is the task of constructing a sparse
navigable search graph for a given dataset endowed with a distance function.
Unfortunately, doing so is computationally expensive, so heuristics are
universally used in practice.
  In this work, we initiate the study of fast algorithms with provable
guarantees for search graph construction. For a dataset with $n$ data points,
the problem of constructing an optimally sparse navigable graph can be framed
as $n$ separate but highly correlated minimum set cover instances. This yields
a naive $O(n^3)$ time greedy algorithm that returns a navigable graph whose
sparsity is at most $O(\log n)$ higher than optimal. We improve significantly
on this baseline, taking advantage of correlation between the set cover
instances to leverage techniques from streaming and sublinear-time set cover
algorithms. Combined with problem-specific pre-processing techniques, we
present an $\tilde{O}(n^2)$ time algorithm for constructing an $O(\log
n)$-approximate sparsest navigable graph under any distance function.
  The runtime of our method is optimal up to logarithmic factors under the
Strong Exponential Time Hypothesis via a reduction from Monochromatic Closest
Pair. Moreover, we prove that, as with general set cover, obtaining better than
an $O(\log n)$-approximation is NP-hard, despite the significant additional
structure present in the navigable graph problem. Finally, we show that our
techniques can also beat cubic time for the closely related and practically
important problems of constructing $\alpha$-shortcut reachable and
$\tau$-monotonic graphs, which are also used for nearest neighbor search. For
such graphs, we obtain $\tilde{O}(n^{2.5})$ time or better algorithms.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [71] [Efficient Classical-Processing of Constant-Depth Time Evolution Circuits in Control Hardware](https://arxiv.org/abs/2507.12765)
*Akhil Francis,Abhi D. Rajagopala,Norm M. Tubman,Katherine Klymko,Kasra Nowrouzi*

Main category: quant-ph

TL;DR: 通过硬件辅助参数化电路执行（PCE）优化量子算法运行时间，减少经典处理和编译时间，在自旋模型中实现高达50%的速度提升。


<details>
  <summary>Details</summary>
Motivation: 降低经典处理和编译时间，以提升量子算法在近期量子设备中的运行效率。

Method: 采用硬件辅助参数化电路执行（PCE）技术，利用结构等效的时间演化电路计算量子多体系统的动力学性质。

Result: 在横向场XY和Heisenberg自旋模型中，运行时间减少了50%。

Conclusion: 硬件辅助PCE技术能有效缓解近期量子算法中的经典瓶颈问题。

Abstract: Improving quantum algorithms run-time performance involves several strategies
such as reducing the quantum gate counts, decreasing the number of
measurements, advancement in QPU technology for faster gate operations, or
optimizing the classical processing. This work focuses on the latter,
specifically reducing classical processing and compilation time via
hardware-assisted parameterized circuit execution (PCE) for computing dynamical
properties of quantum systems. PCE was previously validated for QCVV protocols,
which leverages structural circuit equivalencies. We demonstrate the
applicability of this approach to computing dynamical properties of quantum
many-body systems using structurally equivalent time evolution circuits,
specifically calculating correlation functions of spin models using
constant-depth circuits generated via Cartan decomposition. Implementing this
for spin-spin correlation functions in Transverse field XY (up to 6-sites) and
Heisenberg spin models (up to 3-sites), we observed a run-time reduction of up
to 50\% compared to standard compilation methods. This highlights the
adaptability of time-evolution circuit with hardware-assisted PCE to
potentially mitigate the classical bottlenecks in near-term quantum algorithms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [72] [BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training](https://arxiv.org/abs/2507.12619)
*Rui Li,Xiaoyun Zhi,Jinxin Chi,Menghan Yu,Lixin Huang,Jia Zhu,Weilun Zhang,Xing Ma,Wenjia Liu,Zhicheng Zhu,Daowen Luo,Zuquan Song,Xin Yin,Chao Xiang,Shuguang Wang,Wencong Xiao,Gene Cooperman*

Main category: cs.LG

TL;DR: 本文首次深入研究了基于实际生产数据的LLM训练启动开销问题，提出了Bootseer优化框架，成功减少了50%的启动开销。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在工业级规模的普及，训练启动开销（如延迟和资源浪费）成为一个关键问题。研究表明，仅启动开销就占用了3.5%的GPU时间。

Method: 通过分析启动开销的组成部分，设计了一个名为Bootseer的系统级优化框架，采用热块记录与预取、依赖快照和HDFS-FUSE条纹化等技术。

Result: Bootseer在真实LLM训练负载中部署后，将启动开销减少了50%。

Conclusion: 启动开销是LLM训练中的重要问题，通过系统级优化可以显著提升效率，未来可进一步扩展到其他场景。

Abstract: Large Language Models (LLMs) have become a cornerstone of modern AI, driving
breakthroughs in natural language processing and expanding into multimodal jobs
involving images, audio, and video. As with most computational software, it is
important to distinguish between ordinary runtime performance and startup
overhead. Prior research has focused on runtime performance: improving training
efficiency and stability. This work focuses instead on the increasingly
critical issue of startup overhead in training: the delay before training jobs
begin execution. Startup overhead is particularly important in large,
industrial-scale LLMs, where failures occur more frequently and multiple teams
operate in iterative update-debug cycles. In one of our training clusters, more
than 3.5% of GPU time is wasted due to startup overhead alone.
  In this work, we present the first in-depth characterization of LLM training
startup overhead based on real production data. We analyze the components of
startup cost, quantify its direct impact, and examine how it scales with job
size. These insights motivate the design of Bootseer, a system-level
optimization framework that addresses three primary startup bottlenecks: (a)
container image loading, (b) runtime dependency installation, and (c) model
checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three
techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and
(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment
and evaluated on real LLM training workloads, demonstrating a 50% reduction in
startup overhead.

</details>


### [73] [MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration](https://arxiv.org/abs/2507.12935)
*Shirui Zhao,Jun Yin,Lingyun Yao,Martin Andraud,Wannes Meert,Marian Verhelst*

Main category: cs.LG

TL;DR: 提出了一种名为MC²A的算法-硬件协同设计框架，用于高效和灵活地加速MCMC算法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有MCMC加速方案在硬件灵活性或系统效率上存在不足，限制了其在大规模问题中的应用。

Method: 通过扩展处理器性能模型为3维，设计可参数化的硬件加速器架构，并提出新型Gumbel采样器。

Result: MC²A在CPU、GPU、TPU和现有MCMC加速器上分别实现了307.6倍、1.4倍、2.0倍和84.2倍的加速。

Conclusion: 证明了通用硬件加速的可行性，可推动MCMC在多样化应用中的普及。

Abstract: An increasing number of applications are exploiting sampling-based algorithms
for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)
algorithms form the computational backbone of this emerging branch of machine
learning. Unfortunately, the high computational cost limits their feasibility
for large-scale problems and real-world applications, and the existing MCMC
acceleration solutions are either limited in hardware flexibility or fail to
maintain efficiency at the system level across a variety of end-to-end
applications. This paper introduces \textbf{MC$^2$A}, an algorithm-hardware
co-design framework, enabling efficient and flexible optimization for MCMC
acceleration. Firstly, \textbf{MC$^2$A} analyzes the MCMC workload diversity
through an extension of the processor performance roofline model with a 3rd
dimension to derive the optimal balance between the compute, sampling and
memory parameters. Secondly, \textbf{MC$^2$A} proposes a parametrized hardware
accelerator architecture with flexible and efficient support of MCMC kernels
with a pipeline of ISA-programmable tree-structured processing units,
reconfigurable samplers and a crossbar interconnect to support irregular
access. Thirdly, the core of \textbf{MC$^2$A} is powered by a novel Gumbel
sampler that eliminates exponential and normalization operations. In the
end-to-end case study, \textbf{MC$^2$A} achieves an overall {$307.6\times$,
$1.4\times$, $2.0\times$, $84.2\times$} speedup compared to the CPU, GPU, TPU
and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC
workloads, this work demonstrates and exploits the feasibility of general
hardware acceleration to popularize MCMC-based solutions in diverse application
domains.

</details>


### [74] [PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database](https://arxiv.org/abs/2507.12805)
*Hui Sun,Yanfeng Ding,Liping Yi,Huidong Ma,Gang Wang,Xiaoguang Liu,Cheng Zhong,Wentong Cai*

Main category: cs.LG

TL;DR: 提出了一种基于并行多知识学习的新型压缩器（PMKLC），解决了现有学习型无损压缩器在压缩比、吞吐量和鲁棒性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的学习型无损压缩器在大规模基因组数据库应用中存在压缩比不足、吞吐量低和鲁棒性差的问题。

Method: PMKLC采用自动化多知识学习框架、GPU加速的(s,k)-mer编码、数据块分区和逐步模型传递机制，支持单GPU和多GPU模式。

Result: 在15个真实数据集上测试，PMKLC-S/M平均压缩比提升73%以上，吞吐量提升3-10倍，并表现出最佳鲁棒性和内存效率。

Conclusion: PMKLC在压缩性能、吞吐量和鲁棒性上均显著优于现有方法，适合复杂应用场景。

Abstract: Learning-based lossless compressors play a crucial role in large-scale
genomic database backup, storage, transmission, and management. However, their
1) inadequate compression ratio, 2) low compression \& decompression
throughput, and 3) poor compression robustness limit their widespread adoption
and application in both industry and academia. To solve those challenges, we
propose a novel \underline{P}arallel \underline{M}ulti-\underline{K}nowledge
\underline{L}earning-based \underline{C}ompressor (PMKLC) with four crucial
designs: 1) We propose an automated multi-knowledge learning-based compression
framework as compressors' backbone to enhance compression ratio and robustness;
2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression
throughput and computing resource usage; 3) we introduce data block
partitioning and Step-wise Model Passing (SMP) mechanisms for parallel
acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet
the complex application scenarios, where the former runs on a
resource-constrained single GPU and the latter is multi-GPU accelerated. We
benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15
real-world datasets with different species and data sizes. Compared to
baselines on the testing datasets, PMKLC-S/M achieve the average compression
ratio improvement up to 73.609\% and 73.480\%, the average throughput
improvement up to 3.036$\times$ and 10.710$\times$, respectively. Besides,
PMKLC-S/M also achieve the best robustness and competitive memory cost,
indicating its greater stability against datasets with different probability
distribution perturbations, and its strong ability to run on memory-constrained
devices.

</details>


### [75] [FedGA: A Fair Federated Learning Framework Based on the Gini Coefficient](https://arxiv.org/abs/2507.12983)
*ShanBin Liu*

Main category: cs.LG

TL;DR: 联邦学习中的公平性问题日益突出，FedGA算法通过动态调整聚合权重提升公平性。


<details>
  <summary>Details</summary>
Motivation: 数据异构性导致客户端间性能差异，需解决公平性问题。

Method: 使用基尼系数衡量性能差异，动态调整全局模型更新时机和聚合权重。

Result: 实验表明FedGA显著提升公平性指标，同时保持整体性能。

Conclusion: FedGA有效解决了联邦学习中的公平性问题。

Abstract: Fairness has emerged as one of the key challenges in federated learning. In
horizontal federated settings, data heterogeneity often leads to substantial
performance disparities across clients, raising concerns about equitable model
behavior. To address this issue, we propose FedGA, a fairness-aware federated
learning algorithm. We first employ the Gini coefficient to measure the
performance disparity among clients. Based on this, we establish a relationship
between the Gini coefficient $G$ and the update scale of the global model
${U_s}$, and use this relationship to adaptively determine the timing of
fairness intervention. Subsequently, we dynamically adjust the aggregation
weights according to the system's real-time fairness status, enabling the
global model to better incorporate information from clients with relatively
poor performance.We conduct extensive experiments on the Office-Caltech-10,
CIFAR-10, and Synthetic datasets. The results show that FedGA effectively
improves fairness metrics such as variance and the Gini coefficient, while
maintaining strong overall performance, demonstrating the effectiveness of our
approach.

</details>


### [76] [Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective](https://arxiv.org/abs/2507.12652)
*Kai Malcolm,César Uribe,Momona Yamagami*

Main category: cs.LG

TL;DR: 联邦学习在神经接口解码中表现出高性能与隐私保护的潜力，但在实时闭环应用中需适应性调整，存在性能与隐私的权衡。


<details>
  <summary>Details</summary>
Motivation: 神经信号包含敏感个人信息，数据共享训练解码器面临隐私挑战，探索联邦学习在闭环自适应神经接口中的应用。

Method: 引入基于联邦学习的神经解码，使用高维肌电信号在开环和闭环场景中系统评估性能与隐私。

Result: 开环中联邦学习显著优于本地学习；闭环中需调整方法，本地学习表现更优但隐私风险更高。

Conclusion: 实时自适应应用中存在性能与隐私的权衡，需设计专为单用户协同适应优化的联邦学习方法。

Abstract: Invasive and non-invasive neural interfaces hold promise as high-bandwidth
input devices for next-generation technologies. However, neural signals
inherently encode sensitive information about an individual's identity and
health, making data sharing for decoder training a critical privacy challenge.
Federated learning (FL), a distributed, privacy-preserving learning framework,
presents a promising solution, but it remains unexplored in closed-loop
adaptive neural interfaces. Here, we introduce FL-based neural decoding and
systematically evaluate its performance and privacy using high-dimensional
electromyography signals in both open- and closed-loop scenarios. In open-loop
simulations, FL significantly outperformed local learning baselines,
demonstrating its potential for high-performance, privacy-conscious neural
decoding. In contrast, closed-loop user studies required adapting FL methods to
accommodate single-user, real-time interactions, a scenario not supported by
standard FL. This modification resulted in local learning decoders surpassing
the adapted FL approach in closed-loop performance, yet local learning still
carried higher privacy risks. Our findings highlight a critical
performance-privacy tradeoff in real-time adaptive applications and indicate
the need for FL methods specifically designed for co-adaptive, single-user
applications.

</details>


### [77] [Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces](https://arxiv.org/abs/2507.13092)
*Hyo-Jeong Jang,Hye-Bin Shin,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 该论文提出了一种新的跨模态知识蒸馏框架，通过原型相似性模块和任务特定蒸馏头解决EEG学习中的模态差异和标签不一致问题，显著提升了EEG情感分类和回归性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受噪声和标签误差影响，而现有多模态知识蒸馏方法因模态差异和标签不一致而受限，亟需解决这些问题以提升模型性能。

Method: 提出包含原型相似性模块和任务特定蒸馏头的跨模态知识蒸馏框架，以对齐特征语义并解决标签不一致问题。

Result: 实验表明该方法在公开多模态数据集上优于单模态和多模态基线，显著提升了EEG情感分类和回归性能。

Conclusion: 提出的框架有效解决了模态和标签不一致问题，为BCI应用提供了潜力。

Abstract: Electroencephalography (EEG) is a fundamental modality for cognitive state
monitoring in brain-computer interfaces (BCIs). However, it is highly
susceptible to intrinsic signal errors and human-induced labeling errors, which
lead to label noise and ultimately degrade model performance. To enhance EEG
learning, multimodal knowledge distillation (KD) has been explored to transfer
knowledge from visual models with rich representations to EEG-based models.
Nevertheless, KD faces two key challenges: modality gap and soft label
misalignment. The former arises from the heterogeneous nature of EEG and visual
feature spaces, while the latter stems from label inconsistencies that create
discrepancies between ground truth labels and distillation targets. This paper
addresses semantic uncertainty caused by ambiguous features and weakly defined
labels. We propose a novel cross-modal knowledge distillation framework that
mitigates both modality and label inconsistencies. It aligns feature semantics
through a prototype-based similarity module and introduces a task-specific
distillation head to resolve label-induced inconsistency in supervision.
Experimental results demonstrate that our approach improves EEG-based emotion
regression and classification performance, outperforming both unimodal and
multimodal baselines on a public multimodal dataset. These findings highlight
the potential of our framework for BCI applications.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [78] [UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets](https://arxiv.org/abs/2507.12951)
*Zhichao Sheng,Shilin Zhou,Chen Gong,Zhenghua Li*

Main category: eess.AS

TL;DR: UniSLU是一个统一框架，用于联合建模多个SLU任务，通过统一的表示和生成方法提升任务交互和性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖单独模型架构导致的系统复杂性和跨任务交互不足问题。

Method: 提出统一的表示和生成方法，联合建模ASR、spoken NER和SA任务。

Result: 在公开SLU数据集上表现优于基准方法，适合实际语音多媒体场景。

Conclusion: UniSLU有效提升了SLU任务的性能和跨任务交互，代码和模型将公开。

Abstract: Spoken Language Understanding (SLU) plays a crucial role in speech-centric
multimedia applications, enabling machines to comprehend spoken language in
scenarios such as meetings, interviews, and customer service interactions. SLU
encompasses multiple tasks, including Automatic Speech Recognition (ASR),
spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).
However, existing methods often rely on separate model architectures for
individual tasks such as spoken NER and SA, which increases system complexity,
limits cross-task interaction, and fails to fully exploit heterogeneous
datasets available across tasks. To address these limitations, we propose
UniSLU, a unified framework that jointly models multiple SLU tasks within a
single architecture. Specifically, we propose a unified representation for
diverse SLU tasks, enabling full utilization of heterogeneous datasets across
multiple tasks. Built upon this representation, we propose a unified generative
method that jointly models ASR, spoken NER, and SA tasks, enhancing task
interactions and enabling seamless integration with large language models to
harness their powerful generative capabilities. Extensive experiments on public
SLU datasets demonstrate the effectiveness of our approach, achieving superior
SLU performance compared to several benchmark methods, making it well-suited
for real-world speech-based multimedia scenarios. We will release all code and
models at github to facilitate future research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [A Translation of Probabilistic Event Calculus into Markov Decision Processes](https://arxiv.org/abs/2507.12989)
*Lyris Xu,Fabio Aurelio D'Asaro,Luke Dickens*

Main category: cs.AI

TL;DR: PEC是一种用于在不确定环境中进行逻辑推理的框架，但缺乏目标导向推理机制。本文通过将PEC转换为MDP来填补这一空白，支持时间推理和目标规划，同时保持PEC的可解释性。


<details>
  <summary>Details</summary>
Motivation: PEC在叙事情景推理中具有强大的表达能力和可解释性，但无法处理目标导向推理问题。为了结合PEC的叙事优势与MDP的决策能力，提出一种新的形式化转换方法。

Method: 通过引入“动作执行情境”概念，将PEC领域形式化转换为MDP，保留PEC的灵活动作语义，并利用MDP的算法和工具扩展PEC功能。

Result: PEC-MDP形式化支持时间推理和目标驱动规划，同时能将学习的策略映射回人类可读的PEC表示，保持可解释性。

Conclusion: PEC-MDP成功结合了两者的优势，为复杂决策问题提供了兼具叙事表达能力和目标导向推理能力的统一框架。

Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.

</details>


### [80] [Higher-Order Pattern Unification Modulo Similarity Relations](https://arxiv.org/abs/2507.13208)
*Besik Dundua,Temur Kutsia*

Main category: cs.AI

TL;DR: 论文提出了一种结合高阶模式和模糊等价关系的统一算法，证明了其终止性、可靠性和完整性。


<details>
  <summary>Details</summary>
Motivation: 目的是解决高阶理论和模糊逻辑结合形式下的推理与计算问题，尤其是在需要处理抽象函数和谓词的决策任务中。

Method: 通过整合高阶模式和基于最小T-范数的模糊等价关系，提出了一种统一算法。

Result: 算法在可统一时能计算出一个具有最高近似度的最一般统一子，且问题为单元性。

Conclusion: 该方法为高阶模糊逻辑推理提供了一种高效且可靠的计算工具。

Abstract: The combination of higher-order theories and fuzzy logic can be useful in
decision-making tasks that involve reasoning across abstract functions and
predicates, where exact matches are often rare or unnecessary. Developing
efficient reasoning and computational techniques for such a combined formalism
presents a significant challenge. In this paper, we adopt a more
straightforward approach aiming at integrating two well-established and
computationally well-behaved components: higher-order patterns on one side and
fuzzy equivalences expressed through similarity relations based on minimum
T-norm on the other. We propose a unification algorithm for higher-order
patterns modulo these similarity relations and prove its termination,
soundness, and completeness. This unification problem, like its crisp
counterpart, is unitary. The algorithm computes a most general unifier with the
highest degree of approximation when the given terms are unifiable.

</details>


### [81] [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872)
*Rishane Dassanayake,Mario Demetroudi,James Walpole,Lindley Lentati,Jason R. Brown,Edward James Young*

Main category: cs.AI

TL;DR: 论文首次系统化地将操纵风险纳入AI安全治理，为AI公司提供了一个评估和缓解这些威胁的具体框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统已展现出特定情境下人类级别的说服和策略性欺骗能力，但操纵攻击的风险未得到足够重视，缺乏系统化评估和缓解方法。

Method: 提出了一个围绕三个核心论点（能力、控制、可信度）的安全性框架，包括证据要求、评估方法和实施考虑。

Result: 提供了一个系统化方法论，帮助AI公司在部署前评估和缓解操纵风险。

Conclusion: 论文填补了AI操纵风险系统性研究的空白，为AI安全治理提供了实用的工具。

Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade,
deceive, and influence human behaviour, with current models already
demonstrating human-level persuasion and strategic deception in specific
contexts. Humans are often the weakest link in cybersecurity systems, and a
misaligned AI system deployed internally within a frontier company may seek to
undermine human oversight by manipulating employees. Despite this growing
threat, manipulation attacks have received little attention, and no systematic
framework exists for assessing and mitigating these risks. To address this, we
provide a detailed explanation of why manipulation attacks are a significant
threat and could lead to catastrophic outcomes. Additionally, we present a
safety case framework for manipulation risk, structured around three core lines
of argument: inability, control, and trustworthiness. For each argument, we
specify evidence requirements, evaluation methodologies, and implementation
considerations for direct application by AI companies. This paper provides the
first systematic methodology for integrating manipulation risk into AI safety
governance, offering AI companies a concrete foundation to assess and mitigate
these threats before deployment.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [82] [Mapping Emotions in the Brain: A Bi-Hemispheric Neural Model with Explainable Deep Learning](https://arxiv.org/abs/2507.12625)
*David Freire-Obregón,Agnieszka Dubiel,Prasoon Kumar Vinodkumar,Gholamreza Anbarjafari,Dorota Kamińska,Modesto Castrillón-Santana*

Main category: q-bio.NC

TL;DR: 提出了一个针对双流脑电分类器的后验可解释性框架，扩展了LIME方法以处理双半球结构输入，揭示了与神经生理现象一致的激活模式。


<details>
  <summary>Details</summary>
Motivation: 尽管双半球神经网络在脑电情绪识别中表现良好，但其可解释性限制了在敏感领域的应用，因此需要改进。

Method: 该方法扩展了LIME，支持双半球结构输入，分解预测相关的通道贡献，并结合对称电极分析模型的侧化行为。

Result: 框架揭示了情绪特定的半球激活模式，如快乐时的额叶侧化和悲伤时的后部不对称，且与已知神经生理现象一致。

Conclusion: 该框架为双流脑电分类器提供了神经生理学基础的可解释性，支持情感神经科学中的功能不对称性。

Abstract: Recent advances have shown promise in emotion recognition from
electroencephalogram (EEG) signals by employing bi-hemispheric neural
architectures that incorporate neuroscientific priors into deep learning
models. However, interpretability remains a significant limitation for their
application in sensitive fields such as affective computing and cognitive
modeling. In this work, we introduce a post-hoc interpretability framework
tailored to dual-stream EEG classifiers, extending the Local Interpretable
Model-Agnostic Explanations (LIME) approach to accommodate structured,
bi-hemispheric inputs. Our method adapts LIME to handle structured two-branch
inputs corresponding to left and right-hemisphere EEG channel groups. It
decomposes prediction relevance into per-channel contributions across
hemispheres and emotional classes. We apply this framework to a previously
validated dual-branch recurrent neural network trained on EmoNeuroDB, a dataset
of EEG recordings captured during a VR-based emotion elicitation task. The
resulting explanations reveal emotion-specific hemispheric activation patterns
consistent with known neurophysiological phenomena, such as frontal
lateralization in joy and posterior asymmetry in sadness. Furthermore, we
aggregate local explanations across samples to derive global channel importance
profiles, enabling a neurophysiologically grounded interpretation of the
model's decisions. Correlation analysis between symmetric electrodes further
highlights the model's emotion-dependent lateralization behavior, supporting
the functional asymmetries reported in affective neuroscience.

</details>
