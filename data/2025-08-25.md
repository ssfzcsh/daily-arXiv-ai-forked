<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.PL](#cs.PL) [Total: 5]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 4]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Systematic Literature Review of Machine Learning Approaches for Migrating Monolithic Systems to Microservices](https://arxiv.org/abs/2508.15941)
*Imen Trabelsi,Brahim Mahmoudi,Jean Baptiste Minani,Naouel Moha,Yann-Gaël Guéhéneuc*

Main category: cs.SE

TL;DR: 论文综述了微服务迁移中机器学习方法的应用，指出当前研究的不足与挑战，如数据有限性和工具支持不足。


<details>
  <summary>Details</summary>
Motivation: 解决从单体系统迁移到微服务的复杂性问题，并探索机器学习在自动化迁移中的应用潜力。

Method: 通过系统性文献综述（SLR），分析81篇2015至2024年的研究，使用PRISMA框架报告结果。

Result: 发现部分迁移阶段（如服务识别）研究较多，而其他阶段（如打包微服务）仍有空缺；主要挑战包括数据不足和工具支持缺乏。

Conclusion: 需开发更全面的解决方案以应对微服务迁移中的挑战，填补研究空白。

Abstract: Scalability and maintainability challenges in monolithic systems have led to
the adoption of microservices, which divide systems into smaller, independent
services. However, migrating existing monolithic systems to microservices is a
complex and resource-intensive task, which can benefit from machine learning
(ML) to automate some of its phases. Choosing the right ML approach for
migration remains challenging for practitioners. Previous works studied
separately the objectives, artifacts, techniques, tools, and benefits and
challenges of migrating monolithic systems to microservices. No work has yet
investigated systematically existing ML approaches for this migration to
understand the \revised{automated migration phases}, inputs used, ML techniques
applied, evaluation processes followed, and challenges encountered. We present
a systematic literature review (SLR) that aggregates, synthesises, and
discusses the approaches and results of 81 primary studies (PSs) published
between 2015 and 2024. We followed the Preferred Reporting Items for Systematic
Review and Meta-Analysis (PRISMA) statement to report our findings and answer
our research questions (RQs). We extract and analyse data from these PSs to
answer our RQs. We synthesise the findings in the form of a classification that
shows the usage of ML techniques in migrating monolithic systems to
microservices. The findings reveal that some phases of the migration process,
such as monitoring and service identification, are well-studied, while others,
like packaging microservices, remain unexplored. Additionally, the findings
highlight key challenges, including limited data availability, scalability and
complexity constraints, insufficient tool support, and the absence of
standardized benchmarking, emphasizing the need for more holistic solutions.

</details>


### [2] [Breaking Barriers in Software Testing: The Power of AI-Driven Automation](https://arxiv.org/abs/2508.16025)
*Saba Naqvi,Mohammad Baqar*

Main category: cs.SE

TL;DR: 本文提出了一种基于AI的自动化测试框架，利用NLP、RL和预测模型优化测试生成与验证，显著提升缺陷检测效率和软件可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统软件测试方法效率低、成本高且覆盖率不足，无法满足现代复杂软件需求，需要更智能的解决方案。

Method: 采用NLP将自然语言需求转化为可执行测试案例，通过RL持续优化测试策略，并结合实时分析验证结果，同时引入信任与公平模型减少偏差。

Result: 案例研究表明，该方法显著提高了缺陷检测率、减少了测试工作量并加速了发布周期，证明了AI在测试中的有效性。

Conclusion: 该框架成功将测试从被动、人工过程转变为主动、自适应的系统，为复杂环境下的软件质量提供了更高效的解决方案，同时解决了集成与扩展性问题。

Abstract: Software testing remains critical for ensuring reliability, yet traditional
approaches are slow, costly, and prone to gaps in coverage. This paper presents
an AI-driven framework that automates test case generation and validation using
natural language processing (NLP), reinforcement learning (RL), and predictive
models, embedded within a policy-driven trust and fairness model. The approach
translates natural language requirements into executable tests, continuously
optimizes them through learning, and validates outcomes with real-time analysis
while mitigating bias. Case studies demonstrate measurable gains in defect
detection, reduced testing effort, and faster release cycles, showing that
AI-enhanced testing improves both efficiency and reliability. By addressing
integration and scalability challenges, the framework illustrates how AI can
shift testing from a reactive, manual process to a proactive, adaptive system
that strengthens software quality in increasingly complex environments.

</details>


### [3] [Measuring the effectiveness of code review comments in GitHub repositories: A machine learning approach](https://arxiv.org/abs/2508.16053)
*Shadikur Rahman,Umme Ayman Koana,Hasibul Karim Shanto,Mahmuda Akter,Chitra Roy,Aras M. Ismael*

Main category: cs.SE

TL;DR: 该论文通过实证研究比较了七种机器学习算法在分类代码审查文本情感极性上的效果，发现线性支持向量分类器（SVC）表现最佳。


<details>
  <summary>Details</summary>
Motivation: 代码审查中的情感分类有助于程序员避免误解和错误，提高开发效率。

Method: 从GitHub三个开源项目中提取并手动标记了13,557条代码审查评论，使用七种机器学习算法进行分类比较。

Result: 线性SVC分类器在情感极性分类中准确率最高。

Conclusion: 研究表明线性SVC能有效分类代码审查情感，帮助程序员基于评论做出更准确的决策。

Abstract: This paper illustrates an empirical study of the working efficiency of
machine learning techniques in classifying code review text by semantic
meaning. The code review comments from the source control repository in GitHub
were extracted for development activity from the existing year for three
open-source projects. Apart from that, programmers need to be aware of their
code and point out their errors. In that case, it is a must to classify the
sentiment polarity of the code review comments to avoid an error. We manually
labelled 13557 code review comments generated by three open source projects in
GitHub during the existing year. In order to recognize the sentiment polarity
(or sentiment orientation) of code reviews, we use seven machine learning
algorithms and compare those results to find the better ones. Among those
Linear Support Vector Classifier(SVC) classifier technique achieves higher
accuracy than others. This study will help programmers to make any solution
based on code reviews by avoiding misconceptions.

</details>


### [4] [From Benchmark Data To Applicable Program Repair: An Experience Report](https://arxiv.org/abs/2508.16071)
*Mahinthan Chandramohan,Jovan Jancic,Yuntong Zhang,Padmanabhan Krishnan*

Main category: cs.SE

TL;DR: 本文探讨了自动化程序修复的方法，结合多种技术，在标准基准测试上表现优异，但对实际工业缺陷效果有限，需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决自动化程序修复在工业实践中效果不佳的问题，尤其是针对复杂生产代码的修复。

Method: 结合文献中的多种技术，并通过形式化规范增强代码，利用LLM生成高质量单元测试。

Result: 实验显示形式化规范对逻辑和字符串错误有效，但对简单错误无显著提升；真实场景采用受限。

Conclusion: 需改进规范语言的表达力，结合人类反馈，弥合学术基准与工业需求间的差距。

Abstract: This paper describes our approach to automated program repair. We combine
various techniques from the literature to achieve this. Our experiments show
that our approach performs better than other techniques on standard benchmarks.
However, on closer inspection, none of these techniques work on realistic
defects that we see in industry.
  We find that augmenting code with formal specifications enables LLMs to
generate higher-quality unit tests, especially for complex production code with
improved coverage of edge cases and exception handling. However, specifications
add little value for well-understood errors (e.g., null pointer, index out of
bounds), but are beneficial for logic and string manipulation errors. Despite
encouraging benchmark results, real-world adoption is limited since passing
tests do not guarantee correct patches. Current challenges include insufficient
expressiveness of the JML specification language, necessitating advanced
verification tools and richer predicates. Our ongoing work is exploring
contract automata, programming by example, and testcase repair, with a focus on
integrating human feedback and measuring productivity gains - highlighting the
gap between academic benchmarks and practical industry needs

</details>


### [5] [Validating Terrain Models in Digital Twins for Trustworthy sUAS Operations](https://arxiv.org/abs/2508.16104)
*Arturo Miguel Russell Bernal,Maureen Petterson,Pedro Antonio Alarcon Granadeno,Michael Murphy,James Mason,Jane Cleland-Huang*

Main category: cs.SE

TL;DR: 本文提出了一种基于软件工程原则的三维验证流程，用于验证小无人机系统（sUAS）环境数字孪生（EDT）中的地形模型，以支持真实世界任务。


<details>
  <summary>Details</summary>
Motivation: 随着小无人机系统（sUAS）在陌生复杂环境中的部署增加，环境数字孪生（EDT）对飞行安全和任务执行至关重要，但实际部署中面临诸多不确定因素，需要验证其关键组件如地形模型。

Method: 通过融合美国地质调查局（USGS）数据集和卫星图像构建高分辨率地形模型，并采用基于软件工程的三维验证流程，测试从仿真到真实环境的不同粒度和条件。

Result: 验证了地形模型及其在小无人机系统中的实际应用，解决了数据粒度不足、传感器误差等挑战。

Conclusion: 提出的三维验证流程为小无人机系统的地形模型提供了有效的验证方法，支持其在实际任务中的可靠使用。

Abstract: With the increasing deployment of small Unmanned Aircraft Systems (sUAS) in
unfamiliar and complex environments, Environmental Digital Twins (EDT) that
comprise weather, airspace, and terrain data are critical for safe flight
planning and for maintaining appropriate altitudes during search and
surveillance operations. With the expansion of sUAS capabilities through edge
and cloud computing, accurate EDT are also vital for advanced sUAS
capabilities, like geolocation. However, real-world sUAS deployment introduces
significant sources of uncertainty, necessitating a robust validation process
for EDT components. This paper focuses on the validation of terrain models, one
of the key components of an EDT, for real-world sUAS tasks. These models are
constructed by fusing U.S. Geological Survey (USGS) datasets and satellite
imagery, incorporating high-resolution environmental data to support mission
tasks. Validating both the terrain models and their operational use by sUAS
under real-world conditions presents significant challenges, including limited
data granularity, terrain discontinuities, GPS and sensor inaccuracies, visual
detection uncertainties, as well as onboard resources and timing constraints.
We propose a 3-Dimensions validation process grounded in software engineering
principles, following a workflow across granularity of tests, simulation to
real world, and the analysis of simple to edge conditions. We demonstrate our
approach using a multi-sUAS platform equipped with a Terrain-Aware Digital
Shadow.

</details>


### [6] [The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion](https://arxiv.org/abs/2508.16131)
*Zoe Kotti,Konstantina Dritsa,Diomidis Spinellis,Panos Louridas*

Main category: cs.SE

TL;DR: 论文研究了代码补全任务，通过测量代码困惑度评估LLM生成代码时的置信度，发现强类型语言比动态类型语言困惑度低，且模型选择对困惑度有影响。


<details>
  <summary>Details</summary>
Motivation: 代码补全可以提升开发效率，但现有评估指标复杂且不可靠，因此作者希望通过内在指标（如困惑度）来简单、通用地评估模型置信度。

Method: 使用多种LLM和657个GitHub项目的1008个文件，测量不同编程语言的代码困惑度，分析语言类型、模型选择和代码特性的影响。

Result: 强类型语言（如Java）困惑度较低，动态类型语言（如Perl）较高；模型选择影响困惑度，而代码数据集无显著影响；代码注释会增加困惑度，但不改变语言排名。

Conclusion: 研究表明，通过困惑度可以评估LLM代码补全的适用性，为开发者选择模型和语言提供参考。

Abstract: Code completion entails the task of providing missing tokens given a
surrounding context. It can boost developer productivity while providing a
powerful code discovery tool. Following the Large Language Model (LLM) wave,
code completion has been approached with diverse LLMs fine-tuned on code (code
LLMs). The performance of code LLMs can be assessed with downstream and
intrinsic metrics. Downstream metrics are usually employed to evaluate the
practical utility of a model, but can be unreliable and require complex
calculations and domain-specific knowledge. In contrast, intrinsic metrics such
as perplexity, entropy, and mutual information, which measure model confidence
or uncertainty, are simple, versatile, and universal across LLMs and tasks, and
can serve as proxies for functional correctness and hallucination risk in
LLM-generated code. Motivated by this, we evaluate the confidence of LLMs when
generating code by measuring code perplexity across programming languages,
models, and datasets using various LLMs, and a sample of 1008 files from 657
GitHub projects. We find that strongly-typed languages exhibit lower perplexity
than dynamically typed languages. Scripting languages also demonstrate higher
perplexity. Perl appears universally high in perplexity, whereas Java appears
low. Code perplexity depends on the employed LLM, but not on the code dataset.
Although code comments often increase perplexity, the language ranking based on
perplexity is barely affected by their presence. LLM researchers, developers,
and users can employ our findings to assess the benefits and suitability of
LLM-based code completion in specific software projects based on how language,
model choice, and code characteristics impact model confidence.

</details>


### [7] [Towards Recommending Usability Improvements with Multimodal Large Language Models](https://arxiv.org/abs/2508.16165)
*Sebastian Lubos,Alexander Felfernig,Gerhard Leitner,Julian Schwazer*

Main category: cs.SE

TL;DR: 论文探讨了利用多模态大语言模型（LLM）自动化评估用户界面可用性的潜力，以解决传统方法资源密集、依赖专家的问题。


<details>
  <summary>Details</summary>
Motivation: 传统可用性评估方法（如测试和检查）虽然有效，但对资源要求高且需专家参与，限制了小型组织的使用。多模态LLM的出现为部分自动化评估提供了新思路。

Method: 研究将可用性评估转化为推荐任务，由多模态LLM根据严重性对可用性问题进行排序，并与专家评估结果对比。

Result: 初步研究表明，LLM能够生成与专家相近的可用性改进建议，有望实现更快、更经济的评估。

Conclusion: 多模态LLM在资源受限的场景下，可作为实用性替代方案用于可用性评估。

Abstract: Usability describes a set of essential quality attributes of user interfaces
(UI) that influence human-computer interaction. Common evaluation methods, such
as usability testing and inspection, are effective but resource-intensive and
require expert involvement. This makes them less accessible for smaller
organizations. Recent advances in multimodal LLMs offer promising opportunities
to automate usability evaluation processes partly by analyzing textual, visual,
and structural aspects of software interfaces. To investigate this possibility,
we formulate usability evaluation as a recommendation task, where multimodal
LLMs rank usability issues by severity. We conducted an initial
proof-of-concept study to compare LLM-generated usability improvement
recommendations with usability expert assessments. Our findings indicate the
potential of LLMs to enable faster and more cost-effective usability
evaluation, which makes it a practical alternative in contexts with limited
expert resources.

</details>


### [8] [LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2](https://arxiv.org/abs/2508.16181)
*Zirui Li,Stephan Husung,Haoze Wang*

Main category: cs.SE

TL;DR: 提出了一种基于GPT的LLM辅助方法，用于SysML v2模型的语义对齐，通过提示驱动的方法实现可追溯的软对齐集成。


<details>
  <summary>Details</summary>
Motivation: 解决跨组织MBSE协作中独立开发的系统模型间语义对齐的挑战。

Method: 使用SysML v2的结构模块化和形式语义，结合LLM能力，开发迭代式对齐方法和交互提示，包括模型提取、语义匹配和验证。

Result: 通过测量系统示例验证了方法的有效性，讨论了其优势和局限性。

Conclusion: 该方法为SysML v2模型的语义对齐提供了新解决方案，但仍有改进空间。

Abstract: Cross-organizational collaboration in Model-Based Systems Engineering (MBSE)
faces many challenges in achieving semantic alignment across independently
developed system models. SysML v2 introduces enhanced structural modularity and
formal semantics, offering a stronger foundation for interoperable modeling.
Meanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for
assisting model understanding and integration. This paper proposes a
structured, prompt-driven approach for LLM-assisted semantic alignment of SysML
v2 models. The core contribution lies in the iterative development of an
alignment approach and interaction prompts, incorporating model extraction,
semantic matching, and verification. The approach leverages SysML v2 constructs
such as alias, import, and metadata extensions to support traceable, soft
alignment integration. It is demonstrated with a GPT-based LLM through an
example of a measurement system. Benefits and limitations are discussed.

</details>


### [9] [A Systematic Mapping Study on Smart Cities Modeling Approaches](https://arxiv.org/abs/2508.16273)
*Maria Teresa Rossi,Martina De Sanctis,Ludovico Iovino,Manuel Wimmer*

Main category: cs.SE

TL;DR: 本文通过系统映射研究分析了智慧城市建模方法，总结了现有研究趋势、常用建模方式及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 了解智慧城市设计与建模的现状，识别研究趋势及未来方向，为跨学科研究提供基础。

Method: 采用Petersen等人的系统映射研究指南，分析智慧城市的建模相关文献。

Result: 发现智能治理是最受关注的维度，常用建模方法包括业务、架构和本体建模，但多数技术尚未在操作环境中验证。

Conclusion: 研究结果为理解智慧城市建模的现状提供了基础，同时对模型驱动工程社区具有参考价值。

Abstract: The Smart City concept was introduced to define an idealized city
characterized by automation and connection. It then evolved rapidly by
including further aspects, such as economy, environment. Since then, many
publications have explored various aspects of Smart Cities across different
application domains and research communities, acknowledging the
interdisciplinary nature of this subject. In particular, our interest focuses
on how smart cities are designed and modeled, as a whole or as regards with
their subsystems, when dealing with the accomplishment of the research goals in
this complex and heterogeneous domain. To this aim, we performed a systematic
mapping study on smart cities modeling approaches identifying the relevant
contributions (i) to get an overview of existing research approaches, (ii) to
identify whether there are any publication trends, and (iii) to identify
possible future research directions. We followed the guidelines for conducting
systematic mapping studies by Petersen et al. to analyze smart cities modeling
publications. Our analysis revealed the following main findings: (i) smart
governance is the most investigated and modeled smart city dimension; (ii) the
most used modeling approaches are business, architectural, and ontological
modeling approaches, spanning multiple application fields; (iii) the great
majority of existing technologies for modeling smart cities are not yet proven
in operational environments; (iv) diverse research communities publish their
results in a multitude of different venues which further motivates the
presented literature study. Researchers can use our results for better
understanding the state-of-the-art in modeling smart cities, and as a
foundation for further analysis of specific approaches about smart cities
modeling. Lastly, we also discuss the impact of our analysis for the
Model-Driven Engineering community.

</details>


### [10] [ARSP: Automated Repair of Verilog Designs via Semantic Partitioning](https://arxiv.org/abs/2508.16517)
*Bingkun Yao,Ning Wang,Xiangfeng Liu,Yuxin Du,Yuchen Hu,Hong Gao,Zhe Jiang,Nan Guan*

Main category: cs.SE

TL;DR: ARSP是一种解决Verilog调试中信号稀释问题的两阶段系统，通过语义分割和修复LLM实现高效调试。


<details>
  <summary>Details</summary>
Motivation: 由于传统LLM在调试大规模Verilog模块时因信号稀释而表现不佳，需改进调试方法。

Method: ARSP使用语义分割将模块分成紧密的片段，再由修复LLM修补，并通过合成数据框架训练两模型。

Result: ARSP在pass@1和pass@5上分别达到77.92%和83.88%，优于主流商业LLM和现有工具。

Conclusion: 语义分割显著提升了LLM在Verilog调试中的效果，验证了片段级范围缩减的有效性。

Abstract: Debugging functional Verilog bugs consumes a significant portion of front-end
design time. While Large Language Models (LLMs) have demonstrated great
potential in mitigating this effort, existing LLM-based automated debugging
methods underperform on industrial-scale modules. A major reason for this is
bug signal dilution in long contexts, where a few bug-relevant tokens are
overwhelmed by hundreds of unrelated lines, diffusing the model's attention. To
address this issue, we introduce ARSP, a two-stage system that mitigates
dilution via semantics-guided fragmentation. A Partition LLM splits a module
into semantically tight fragments; a Repair LLM patches each fragment; edits
are merged without altering unrelated logic. A synthetic data framework
generates fragment-level training pairs spanning bug types, design styles, and
scales to supervise both models. Experiments show that ARSP achieves 77.92%
pass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including
Claude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,
semantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over
whole-module debugging, validating the effectiveness of fragment-level scope
reduction in LLM-based Verilog debugging.

</details>


### [11] [Metamorphic Coverage](https://arxiv.org/abs/2508.16307)
*Jinsheng Ba,Yuancheng Jiang,Manuel Rigger*

Main category: cs.SE

TL;DR: 提出了名为‘Metamorphic Coverage’（MC）的新覆盖率度量标准，用于评估变形测试方法的有效性，证明其比传统行覆盖率和变异测试更高效。


<details>
  <summary>Details</summary>
Motivation: 传统代码覆盖率和变异测试在评估变形测试方法时存在不足，前者无法准确衡量代码验证程度，后者计算成本高。

Method: 提出MC指标，通过分析变形测试中测试输入对执行的差异代码来评估测试效果。

Result: MC与64个发现的错误中的50个修复位置重叠，且比行覆盖率更能区分测试方法的有效性，计算时间比变异测试少359倍。

Conclusion: MC是一种高效且低成本的方法，可用于评估和改进变形测试方法，显著提高测试用例生成的效果。

Abstract: Metamorphic testing is a widely used methodology that examines an expected
relation between pairs of executions to automatically find bugs, such as
correctness bugs. We found that code coverage cannot accurately measure the
extent to which code is validated and mutation testing is computationally
expensive for evaluating metamorphic testing methods. In this work, we propose
Metamorphic Coverage (MC), a coverage metric that examines the distinct code
executed by pairs of test inputs within metamorphic testing. Our intuition is
that, typically, a bug can be observed if the corresponding code is executed
when executing either test input but not the other one, so covering more
differential code covered by pairs of test inputs might be more likely to
expose bugs. While most metamorphic testing methods have been based on this
general intuition, our work defines and systematically evaluates MC on five
widely used metamorphic testing methods for testing database engines,
compilers, and constraint solvers. The code measured by MC overlaps with the
bug-fix locations of 50 of 64 bugs found by metamorphic testing methods, and MC
has a stronger positive correlation with bug numbers than line coverage. MC is
4x more sensitive than line coverage in distinguishing testing methods'
effectiveness, and the average value of MC is 6x smaller than line coverage
while still capturing the part of the program that is being tested. MC required
359x less time than mutation testing. Based on a case study for an automated
database system testing approach, we demonstrate that when used for feedback
guidance, MC significantly outperforms code coverage, by finding 41\% more
bugs. Consequently, this work might have broad applications for assessing
metamorphic testing methods and improving test-case generation.

</details>


### [12] [SATORI: Static Test Oracle Generation for REST APIs](https://arxiv.org/abs/2508.16318)
*Juan C. Alonso,Alberto Martin-Lopez,Sergio Segura,Gabriele Bavota,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: SATORI是一种基于静态分析的REST API测试预言生成工具，利用大语言模型分析OpenAPI规范，显著提升了测试预言生成的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有REST API测试工具在预言生成方面存在局限性，通常仅支持有限的预言类型（如崩溃、回归和规范合规性）。

Method: SATORI通过分析OpenAPI规范中的响应字段属性（如名称和描述），利用大语言模型推断API的预期行为，并将其转换为可执行断言。

Result: 在17个工业API操作上，SATORI能生成数百个有效预言，F1分数达74.3%，优于动态方法AGORA+（69.3%），两者结合可覆盖90%的预言。

Conclusion: SATORI填补了静态预言生成的空白，并与动态方法互补，成功发现多个流行API的文档缺陷。

Abstract: REST API test case generation tools are evolving rapidly, with growing
capabilities for the automated generation of complex tests. However, despite
their strengths in test data generation, these tools are constrained by the
types of test oracles they support, often limited to crashes, regressions, and
noncompliance with API specifications or design standards. This paper
introduces SATORI (Static API Test ORacle Inference), a black-box approach for
generating test oracles for REST APIs by analyzing their OpenAPI Specification.
SATORI uses large language models to infer the expected behavior of an API by
analyzing the properties of the response fields of its operations, such as
their name and descriptions. To foster its adoption, we extended the
PostmanAssertify tool to automatically convert the test oracles reported by
SATORI into executable assertions. Evaluation results on 17 operations from 12
industrial APIs show that SATORI can automatically generate up to hundreds of
valid test oracles per operation. SATORI achieved an F1-score of 74.3%,
outperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which
requires executing the API-when generating comparable oracle types. Moreover,
our findings show that static and dynamic oracle inference methods are
complementary: together, SATORI and AGORA+ found 90% of the oracles in our
annotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular
APIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)
leading to documentation updates by the API maintainers.

</details>


### [13] [The (C)omprehensive (A)rchitecture (P)attern (I)ntegration method: Navigating the sea of technology](https://arxiv.org/abs/2508.16341)
*Sebastian Copei,Oliver Hohlfeld,Jens Kosiol*

Main category: cs.SE

TL;DR: 提出了一种名为CAPI的综合架构模式集成方法，通过诊断决策树为用户需求推荐架构模式，降低工具选择复杂性。


<details>
  <summary>Details</summary>
Motivation: 技术变化快速，单个开发者难以掌握所有工具和趋势，工具选择和架构设计成为复杂问题。

Method: 开发CAPI方法，利用决策树推荐架构模式，迭代评估其可理解性和可用性。

Result: 技术选择多依赖试错，CAPI被认为普遍有用，并能复现参与者的生产架构环境。

Conclusion: CAPI通过推荐架构模式有效简化了工具选择和架构设计问题。

Abstract: The technological landscape changes daily, making it nearly impossible for a
single person to be aware of all trends or available tools that may or may not
be suitable for their software project. This makes tool selection and
architectural design decisions a complex problem, especially for large-scale
software systems. To tackle this issue, we introduce CAPI, the Comprehensive
Architecture Pattern Integration method that uses a diagnostic decision tree to
suggest architectural patterns depending on user needs. By suggesting patterns
instead of tools, the overall complexity for further decisions is lower as
there are fewer architectural patterns than tools due to the abstract nature of
patterns. Moreover, since tools implement patterns, each non-proposed pattern
reduces the number of tools to choose from, reducing complexity. We iteratively
developed CAPI, evaluating its understandability and usability in small studies
with academic participants. When satisfied with the outcome, we performed a
user-study with industry representatives to investigate the state-of-the-art in
technology selection and the effectiveness of our proposed method. We find that
technology selection is largely performed via trial and error, that CAPI is
uniformly perceived as helpful, and that CAPI is able to reproduce the
productive architectural environments of our participants.

</details>


### [14] [AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions](https://arxiv.org/abs/2508.16402)
*Zihan Wang,Jiaze Chen,Zhicheng Liu,Markus Mak,Yidi Du,Geonsik Moon,Luoqi Xu,Aaron Tua,Kunshuo Peng,Jiayi Lu,Mingfei Xia,Boqian Zou,Chenyang Ran,Guang Tian,Shoutai Zhu,Yeheng Duan,Zhenghui Kang,Zhenxing Lin,Shangshu Li,Qiang Luo,Qingshen Long,Zhiyong Chen,Yihan Xiao,Yurong Wu,Daoguang Zan,Yuyi Fu,Mingxuan Wang,Ming Ding*

Main category: cs.SE

TL;DR: 提出了新的基准测试AetherCode，用于更准确地评估大型语言模型的编程能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试低估了人类编程高手与LLMs之间的差距，需要更难的测试问题。

Method: 从顶级编程竞赛中选择问题，结合自动生成和人工验证测试用例。

Result: AetherCode能更可靠地评估LLMs的编程能力。

Conclusion: AetherCode为解决现有基准测试的不足提供了新标准。

Abstract: Competitive programming has emerged as a critical benchmark for evaluating
the reasoning and coding capabilities of Large Language Models (LLMs). Despite
impressive progress on existing benchmarks, we argue that current evaluations
overstate model proficiency, masking a substantial gap between LLMs and elite
human programmers. This gap arises from two key limitations: insufficient
difficulty and scope of benchmark problems, and evaluation bias from
low-quality test cases. To address these shortcomings, we present AetherCode, a
new benchmark that draws problems from premier programming competitions such as
IOI and ICPC, offering broader coverage and higher difficulty. AetherCode
further incorporates comprehensive, expert-validated test suites built through
a hybrid of automated generation and human curation, ensuring rigorous and
reliable assessment. By combining challenging problem design with robust
evaluation, AetherCode provides a more faithful measure of LLM capabilities and
sets a new standard for future research in code reasoning.

</details>


### [15] [LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python](https://arxiv.org/abs/2508.16419)
*Akshay Mhatre,Noujoud Nader,Patrick Diehl,Deepti Gupta*

Main category: cs.SE

TL;DR: 这篇论文系统评估了ChatGPT-4、Claude 3和LLaMA 4在检测多种软件bug中的表现，发现它们在简单代码问题上表现优异，但处理复杂安全漏洞时性能下降。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）在检测真实世界软件bug中的实际有效性，尤其是在复杂安全漏洞方面的表现。

Method: 使用包含基础编程错误、经典安全漏洞和生产级bug的基准数据集，采用多阶段、上下文感知的提示协议模拟真实调试场景。

Result: 模型在简单代码问题中表现出色，但在复杂安全漏洞和大规模生产代码中性能下降，ChatGPT-4和Claude 3的分析更细致。

Conclusion: LLMs在代码分析中有潜力，但在复杂场景中仍有局限性。

Abstract: Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are
increasingly embedded in software/application development, supporting tasks
from code generation to debugging. Yet, their real-world effectiveness in
detecting diverse software bugs, particularly complex, security-relevant
vulnerabilities, remains underexplored. This study presents a systematic,
empirical evaluation of these three leading LLMs using a benchmark of
foundational programming errors, classic security flaws, and advanced,
production-grade bugs in C++ and Python. The dataset integrates real code from
SEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated
through local compilation and testing pipelines. A novel multi-stage,
context-aware prompting protocol simulates realistic debugging scenarios, while
a graded rubric measures detection accuracy, reasoning depth, and remediation
quality. Our results show that all models excel at identifying syntactic and
semantic issues in well-scoped code, making them promising for educational use
and as first-pass reviewers in automated code auditing. Performance diminishes
in scenarios involving complex security vulnerabilities and large-scale
production code, with ChatGPT-4 and Claude 3 generally providing more nuanced
contextual analyses than LLaMA 4. This highlights both the promise and the
present constraints of LLMs in serving as reliable code analysis tools.

</details>


### [16] [Using LLMs and Essence to Support Software Practice Adoption](https://arxiv.org/abs/2508.16445)
*Sonia Nicoletti,Paolo Ciancarini*

Main category: cs.SE

TL;DR: 该研究探索将Essence框架与大型语言模型结合，开发了一个专门聊天机器人，利用检索增强生成技术，提高了软件工程领域任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前NLP和AI研究多关注代码生成，而忽略了自动化支持最佳实践采纳、工作方式演进和流程健康监控。本研究旨在填补这一空白。

Method: 通过开发一个基于Essence框架和大型语言模型的聊天机器人，采用检索增强生成（RAG）系统从知识库中检索信息。

Result: 该系统在领域特定任务中表现优于通用大型语言模型，为软件工程知识的学习和应用提供了有效支持。

Conclusion: 尽管还需进一步用户验证，但研究表明基于LLM的自动化工具在提升软件工程学习和决策方面具有潜力。

Abstract: Recent advancements in natural language processing (NLP) have enabled the
development of automated tools that support various domains, including software
engineering. However, while NLP and artificial intelligence (AI) research has
extensively focused on tasks such as code generation, less attention has been
given to automating support for the adoption of best practices, the evolution
of ways of working, and the monitoring of process health. This study addresses
this gap by exploring the integration of Essence, a standard and thinking
framework for managing software engineering practices, with large language
models (LLMs). To this end, a specialised chatbot was developed to assist
students and professionals in understanding and applying Essence. The chatbot
employs a retrieval-augmented generation (RAG) system to retrieve relevant
contextual information from a curated knowledge base. Four different LLMs were
used to create multiple chatbot configurations, each evaluated both as a base
model and augmented with the RAG system. The system performance was evaluated
through both the relevance of retrieved context and the quality of generated
responses. Comparative analysis against the general-purpose LLMs demonstrated
that the proposed system consistently outperforms its baseline counterpart in
domain-specific tasks. By facilitating access to structured software
engineering knowledge, this work contributes to bridging the gap between
theoretical frameworks and practical application, potentially improving process
management and the adoption of software development practices. While further
validation through user studies is required, these findings highlight the
potential of LLM-based automation to enhance learning and decision-making in
software engineering.

</details>


### [17] [How Small is Enough? Empirical Evidence of Quantized Small Language Models for Automated Program Repair](https://arxiv.org/abs/2508.16499)
*Kazuki Kusama,Honglin Shu,Masanari Kondo,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 小型语言模型（SLMs）在自动化程序修复（APR）任务中表现出与大型语言模型（LLMs）相当的准确性，且计算资源需求更低，量化技术进一步提升了其效率。


<details>
  <summary>Details</summary>
Motivation: 研究SLMs是否能够在APR任务中替代高资源消耗的LLMs，以降低计算成本。

Method: 在QuixBugs基准上比较SLMs和LLMs的修复准确性，并分析int8量化的影响。

Result: SLMs与LLMs修复准确性相当甚至更优，量化对其准确性影响极小但显著降低内存需求。

Conclusion: SLMs是LLMs在APR中的可行替代方案，兼具高准确性和低计算成本，量化技术进一步优化效率。

Abstract: Background: Large language models (LLMs) have greatly improved the accuracy
of automated program repair (APR) methods. However, LLMs are constrained by
high computational resource requirements. Aims: We focus on small language
models (SLMs), which perform well even with limited computational resources
compared to LLMs. We aim to evaluate whether SLMs can achieve competitive
performance in APR tasks. Method: We conducted experiments on the QuixBugs
benchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed
the impact of int8 quantization on APR performance. Results: The latest SLMs
can fix bugs as accurately as--or even more accurately than--LLMs. Also, int8
quantization had minimal effect on APR accuracy while significantly reducing
memory requirements. Conclusions: SLMs present a viable alternative to LLMs for
APR, offering competitive accuracy with lower computational costs, and
quantization can further enhance their efficiency without compromising
effectiveness.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [18] [Correctness-Guaranteed Code Generation via Constrained Decoding](https://arxiv.org/abs/2508.15866)
*Lingxiao Li,Salar Rahili,Yiwei Zhao*

Main category: cs.PL

TL;DR: 提出了一个约束解码算法，通过上下文敏感解析器生成语义正确的程序，确保一次性正确性，适用于游戏和机器人等关键领域。


<details>
  <summary>Details</summary>
Motivation: 在代码生成领域，确保生成程序的正确性是一大挑战，尤其在需要一次性正确性的关键领域（如视频游戏和机器人）。

Method: 开发了一种结合上下文敏感解析器的约束解码算法，动态生成满足非扩展属性的正则表达式，使用动态解析器树（ToP）处理上下文信息。

Result: 通过sLua展示了该方法能生成语义正确的程序，并验证了其在游戏机制生成中的应用，确保运行时正确性。

Conclusion: 提出的方法能有效生成一次性正确的程序，适用于需高可靠性的关键领域。

Abstract: Language Models (LMs) are increasingly being used for code generation, but
ensuring the correctness of generated programs remains a significant challenge.
Although imperfect code may be acceptable during software development with
human oversight, domains such as video games and robotics require one-shot
correctness for runtime-critical components. We present a constrained decoding
algorithm for generating semantically correct programs that incorporates a
context-sensitive parser, which, at each step, outputs a regular expression
that satisfies a critical non-extensible property to guide the generation of
the next token sequence that can continue to a correct program. To build such a
context-sensitive parser, we propose a framework of a dynamic tree of parsers
(ToP) during parsing, where each parser corresponds to a modular context-free
grammar enriched with contextual information such as variable scopes and type
constraints, with tree branches representing ambiguity in the future code
segment. We demonstrate our approach through sLua, a strongly typed variant of
Lua, showing that our method can generate semantically correct programs
conforming to any prescribed scripting API. We further show that, with careful
design, our semantic guarantees extend to runtime correctness, as validated in
the application of generating game mechanics for a roguelike video game.

</details>


### [19] [Automated Formal Verification of a Software Fault Isolation System](https://arxiv.org/abs/2508.15898)
*Matthew Sotoudeh,Zachary Yedidia*

Main category: cs.PL

TL;DR: 本文介绍了对轻量级故障隔离（LFI）系统进行自动化形式验证的方法，以确保其验证器无漏洞，保障软件故障隔离（SFI）的安全性。


<details>
  <summary>Details</summary>
Motivation: SFI验证器的潜在漏洞可能破坏其安全模型，允许沙盒代码访问受保护内存，因此需要形式化验证以确保其可靠性。

Method: 通过自动化形式验证技术，对LFI验证器进行检查，验证其是否能确保程序仅访问指定的沙盒内存区域。

Result: 成功验证了LFI系统的正确性，即其验证器确实能防止程序访问沙盒外的内存。

Conclusion: 形式验证是确保SFI系统安全性的有效方法，LFI验证器的可靠性得到了证明。

Abstract: Software fault isolation (SFI) is a popular way to sandbox untrusted
software. A key component of SFI is the verifier that checks the untrusted code
is written in a subset of the machine language that guarantees it never reads
or writes outside of a region of memory dedicated to the sandbox. Soundness
bugs in the SFI verifier would break the SFI security model and allow the
supposedly sandboxed code to read protected memory. In this paper, we address
the concern of SFI verifier bugs by performing an automated formal verification
of a recent SFI system called Lightweight Fault Isolation (LFI). In particular,
we formally verify that programs accepted by the LFI verifier never read or
write to memory outside of a designated sandbox region.

</details>


### [20] [Synthesizing DSLs for Few-Shot Learning](https://arxiv.org/abs/2508.16063)
*Paul Krogmeier,P. Madhusudan*

Main category: cs.PL

TL;DR: 研究在符号领域为少样本学习合成领域特定语言（DSL）的问题，证明其可解性。


<details>
  <summary>Details</summary>
Motivation: 解决在符号域中为少样本学习设计DSL的挑战，确保训练样本的解决方案适用于测试样本。

Method: 使用树自动机评估语义，分析语法与表达式大小的关系，并探讨正则树集的解。

Result: 证明了在特定条件下DSL合成问题是可解的，并扩展了宏语法的可解性。

Conclusion: DSL合成在符号域少样本学习中具有理论支持，适用于多种语法变体。

Abstract: We study the problem of synthesizing domain-specific languages (DSLs) for
few-shot learning in symbolic domains. Given a base language and instances of
few-shot learning problems, where each instance is split into training and
testing samples, the DSL synthesis problem asks for a grammar over the base
language that guarantees that small expressions solving training samples also
solve corresponding testing samples. We prove that the problem is decidable for
a class of languages whose semantics over fixed structures can be evaluated by
tree automata and when expression size corresponds to parse tree depth in the
grammar, and, furthermore, the grammars solving the problem correspond to a
regular set of trees. We also prove decidability results for variants of the
problem where DSLs are only required to express solutions for input learning
problems and where DSLs are defined using macro grammars.

</details>


### [21] [Leveraging Large Language Models to Detect Missed Peephole Optimizations](https://arxiv.org/abs/2508.16125)
*Zhenyang Xu,Hongxu Xu,Yongqiang Tian,Xintong Zhou,Chengnian Sun*

Main category: cs.PL

TL;DR: 论文提出Lampo框架，利用大型语言模型(LLMs)结合验证工具，高效发现并验证编译器中的漏洞优化机会，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于指令集的复杂性和多样性，传统方法在发现有效的漏洞优化方面存在局限，Lampo旨在通过LLMs的创新能力和严格验证填补这一空白。

Method: Lampo结合LLMs的代码优化能力与翻译验证工具，通过反馈驱动的迭代流程发现并验证优化机会。

Result: 在LLVM生态中，Lampo平均检测到17/25的已知优化漏洞，并发现26个新优化，其中15个已确认，6个已修复。

Conclusion: Lampo展示了在持续检测漏洞优化方面的强大潜力，为编译器优化提供了新思路。

Abstract: By replacing small, suboptimal instruction sequences within programs with a
more efficient equivalent, peephole optimization can not only directly optimize
code size and performance, but also potentially enables further transformations
in the subsequent optimization pipeline. Although peephole optimization is a
critical class of compiler optimizations, discovering new and effective
peephole optimizations is challenging as the instruction sets can be extremely
complex and diverse. Previous methods either do not scale well or can only
capture a limited subset of peephole optimizations. In this work, we leverage
Large Language Models (LLMs) to detect missed peephole optimizations. We
propose Lampo, a novel automated framework that synergistically combines the
creative but unreliable code optimization ability of LLMs with rigorous
correctness verification performed by translation validation tools, integrated
in a feedback-driven iterative process. Through a comprehensive evaluation
within LLVM ecosystems, we show that Lampo can successfully detect up to 17 out
of 25 previously reported missed optimizations in LLVM on average, and that 22
out of 25 can potentially be found by Lampo with different LLMs. For
comparison, the state-of-the-art superoptimizer for LLVM, Souper, identified 15
of them. Moreover, within seven months of development and intermittent
experiments, Lampo found 26 missed peephole optimizations, 15 of which have
been confirmed and 6 already fixed. These results demonstrate Lampo's strong
potential in continuously detecting missed peephole optimizations.

</details>


### [22] [On the Duality of Task and Actor Programming Models](https://arxiv.org/abs/2508.16522)
*Rohan Yadav,Joseph Guman,Sean Treichler,Michael Garland,Alex Aiken,Fredrik Kjolstad,Michael Bauer*

Main category: cs.PL

TL;DR: 论文探讨了任务模型和行动者模型的对偶性，并提出技术使任务模型在不牺牲生产力的情况下实现接近行动者模型的性能。


<details>
  <summary>Details</summary>
Motivation: 分布式和异构机器上的编程模型需求增长，任务模型和行动者模型各有优劣，但两者实际上是对偶的。

Method: 展示了任务模型与行动者模型的对偶性，并提出了减少任务模型开销的技术，应用于Realm和Legion运行时。

Result: 技术在Realm中降低1.7-5.3倍开销，接近Charm++和MPI的性能；Legion应用强扩展性提升1.3-5.0倍。

Conclusion: 任务模型可通过优化接近行动者模型的性能，同时保持开发效率。

Abstract: Programming models for distributed and heterogeneous machines are rapidly
growing in popularity to meet the demands of modern workloads. Task and actor
models are common choices that offer different trade-offs between development
productivity and achieved performance. Task-based models offer better
productivity and composition of software, whereas actor-based models routinely
deliver better peak performance due to lower overheads. While task-based and
actor-based models appear to be different superficially, we demonstrate these
programming models are duals of each other. Importantly, we show that this
duality extends beyond functionality to performance, and elucidate techniques
that let task-based systems deliver performance competitive with actor-based
systems without compromising productivity. We apply these techniques to both
Realm, an explicitly parallel task-based runtime, as well as Legion, an
implicitly parallel task-based runtime. We show these techniques reduce Realm's
overheads by between 1.7-5.3x, coming within a factor of two of the overheads
imposed by heavily optimized actor-based systems like Charm++ and MPI. We
further show that our techniques enable between 1.3-5.0x improved strong
scaling of unmodified Legion applications.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [23] [Two-Timescale Dynamic Service Deployment and Task Scheduling with Spatiotemporal Collaboration in Mobile Edge Networks](https://arxiv.org/abs/2508.16293)
*Yang Li,Xing Zhang,Yunji Zhao,Wenbo Wang*

Main category: cs.PF

TL;DR: 本文提出了一种两时间尺度的在线优化框架，通过联合优化服务部署和任务调度，解决了边缘计算中资源分配的时空动态性问题，显著降低了任务处理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的边缘计算优化方法未能充分考虑时空维度的协作，限制了其对用户需求和系统状态动态变化的适应性。为了充分利用协作边缘计算的优势，需要进行联合优化。

Method: 采用凸优化技术解决任务调度问题，多智能体深度强化学习技术解决服务部署问题，通过两时间尺度的交替优化方法进行时空协同优化。

Result: 与基线算法相比，提出的方案在延迟性能上表现更优，同时具有较低的运行时间和良好的收敛性。

Conclusion: 该框架为边缘计算中的资源分配和任务调度提供了一种高效的时空协同优化方法，显著提升了系统性能。

Abstract: Collaborative edge computing addresses the resource constraints of individual
edge nodes by enabling resource sharing and task co-processing across multiple
nodes. To fully leverage the advantages of collaborative edge computing, joint
optimization of service deployment and task scheduling is necessary. Existing
optimization methods insufficiently address the collaboration across spatial
and temporal dimensions, which hinders their adaptability to the
spatiotemporally varying nature of user demands and system states. This paper
focuses on optimizing the expected task processing delay in edge networks. We
propose a two-timescale online optimization framework to jointly determine: i)
service deployment decisions at each large timescale; and ii) task scheduling
decisions at each small timescale. Specifically, the convex optimization
technique is used to solve the task scheduling problem, while a multi-agent
deep reinforcement learning technique is employed for the service deployment
problem. These two methods are combined for spatiotemporal co-optimization
through a two-timescale alternating optimization approach. Compared to the
baseline algorithms, the proposed scheme achieves better delay performance,
while also exhibiting low running time and favorable convergence behavior.

</details>


### [24] [GreenLLM: SLO-Aware Dynamic Frequency Scaling for Energy-Efficient LLM Serving](https://arxiv.org/abs/2508.16449)
*Qunyou Liu,Darong Huang,Marina Zapater,David Atienza*

Main category: cs.PF

TL;DR: GreenLLM是一种SLO感知的服务框架，通过分离预填充和解码控制，优化GPU能源使用，减少能量消耗34%。


<details>
  <summary>Details</summary>
Motivation: 现有的GPU功率控制器忽视了LLM推理的两个阶段（预填充和解码）的不对称性，导致能源浪费和性能下降。

Method: GreenLLM通过基于长度的队列路由请求，为预填充阶段构建紧凑的延迟-功耗模型，并为解码阶段设计双环控制器。

Result: 在阿里云和Azure的跟踪重放中，GreenLLM比默认DVFS基节省了34%的能源，且吞吐量无损失，SLO违规增加少于3.5%。

Conclusion: GreenLLM显著减少了LLM推理的能源消耗，同时保持了性能和SLO合规性。

Abstract: Large Language Models (LLMs) are becoming the backbone of modern cloud
services, yet their inference costs are dominated by GPU energy. Unlike
traditional GPU workloads, LLM inference has two stages with different
characteristics: the prefill phase, which is latency sensitive and scales
quadratically with prompt length, and the decode phase, which progresses token
by token with unpredictable length. Current GPU power governors (for example,
NVIDIA's default) overlook this asymmetry and treat both stages uniformly. The
result is mismatched voltage and frequency settings, head-of-line blocking, and
excessive energy use.
  We introduce GreenLLM, an SLO-aware serving framework that minimizes GPU
energy by explicitly separating prefill and decode control. At ingress,
requests are routed into length-based queues so short prompts avoid
head-of-line blocking and TTFT improves. For prefill, GreenLLM collects short
traces on a GPU node, fits compact latency-power models over SM frequency, and
solves a queueing-aware optimization to select energy-minimal clocks per class.
During decode, a lightweight dual-loop controller tracks throughput (tokens per
second) and adjusts frequency with hysteretic, fine-grained steps to hold tail
TBT within target bounds. Across Alibaba and Azure trace replays, GreenLLM
reduces total energy by up to 34 percent versus the default DVFS baseline, with
no loss of throughput and with less than 3.5 percent additional SLO violations.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [25] [CXLAimPod: CXL Memory is all you need in AI era](https://arxiv.org/abs/2508.15980)
*Yiwei Yang,Yusheng Zheng,Yiqi Chen,Zheng Liang,Kexin Chu,Zhe Zhou,Andi Quinn,Wei Zhang*

Main category: cs.OS

TL;DR: CXLAimPod 是一种自适应调度框架，利用 CXL 的全双工通道提升内存系统性能，对比 DDR5 在混合读写模式下带宽提升 55-61%，并在多种工作负载中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统 DDR5 架构在混合读写模式下因总线转向惩罚而带宽受限，CXL 的全双工通道潜力未被现有软件栈充分利用。

Method: 提出 CXLAimPod 框架，结合 cgroup 提示和多种调度策略，通过 Linux 内核 eBPF 实现高效优化。

Result: 在 Redis、LLM 文本生成和向量数据库中分别获得 7.4%、71.6% 和 9.1% 的性能提升。

Conclusion: CXLAimPod 能有效挖掘 CXL 架构潜力，显著提升内存系统性能。

Abstract: The proliferation of data-intensive applications, ranging from large language
models to key-value stores, increasingly stresses memory systems with mixed
read-write access patterns. Traditional half-duplex architectures such as DDR5
are ill-suited for such workloads, suffering bus turnaround penalties that
reduce their effective bandwidth under mixed read-write patterns. Compute
Express Link (CXL) offers a breakthrough with its full-duplex channels, yet
this architectural potential remains untapped as existing software stacks are
oblivious to this capability. This paper introduces CXLAimPod, an adaptive
scheduling framework designed to bridge this software-hardware gap through
system support, including cgroup-based hints for application-aware
optimization. Our characterization quantifies the opportunity, revealing that
CXL systems achieve 55-61% bandwidth improvement at balanced read-write ratios
compared to flat DDR5 performance, demonstrating the benefits of full-duplex
architecture. To realize this potential, the CXLAimPod framework integrates
multiple scheduling strategies with a cgroup-based hint mechanism to navigate
the trade-offs between throughput, latency, and overhead. Implemented
efficiently within the Linux kernel via eBPF, CXLAimPod delivers significant
performance improvements over default CXL configurations. Evaluation on diverse
workloads shows 7.4% average improvement for Redis (with up to 150% for
specific sequential patterns), 71.6% improvement for LLM text generation, and
9.1% for vector databases, demon-strating that duplex-aware scheduling can
effectively exploit CXL's architectural advantages.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [26] [Task Offloading and Resource Allocation for MEC-assisted Consumer Internet of Vehicle Systems](https://arxiv.org/abs/2508.15795)
*Yanheng Liu,Dalin Li,Hao Wu,Zemin Sun,Weihong Qin,Jun Li,Hongyang Du,Geng Sun*

Main category: cs.NI

TL;DR: 论文提出了一种AI驱动的任务卸载和资源分配方法（JTOCRA），用于MEC辅助的车联网系统，以解决资源有限与高计算需求之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算（MEC）辅助的车联网（IoV）需要满足车辆的高计算需求，但资源有限、动态环境复杂，亟需高效解决方案。

Method: 设计了基于多智能体深度确定性策略梯度（MADDPG）的联合任务卸载与资源分配方法（JTOCRA）。

Result: 仿真结果表明，JTOCRA在系统性能和扩展性上优于其他方法。

Conclusion: JTOCRA能有效优化MEC辅助车联网系统的服务延时和能耗，具有实际应用潜力。

Abstract: Mobile edge computing (MEC)-assisted internet of vehicle (IoV) is emerging as
a promising paradigm to provide computing services for vehicles. However,
meeting the computing-sensitive and computation-intensive demands of vehicles
poses several challenges, including the discrepancy between the limited
resource provision and stringent computing requirement, the difficulty in
capturing and integrating the intricate features of the MEC-assisted IoV system
into the problem formulation, and the need for real-time processing and
efficient resource management in the dynamic environment. In this work, we
explore the AI-enabled task offloading and resource allocation for MEC-assisted
consumer IoV systems. Specifically, we first present a multi-MEC-assisted
consumer IoV architecture that leverages the computational resources of MEC
servers to provide offloading services close to vehicles. Subsequently, we
formulate a system cost minimization optimization problem (SCMOP) by
integrating the service delay and energy consumption. To efficiently solve this
problem, we design a joint task offloading and computing resource allocation
approach (JTOCRA) by applying the multi-agent deep deterministic policy
gradient (MADDPG) algorithm. Finally, simulation results demonstrate that the
proposed JTOCRA can achieve superior system performances and exhibits better
scalability compared to other alternative approaches.

</details>


### [27] [Better Together: Leveraging Multiple Digital Twins for Deployment Optimization of Airborne Base Stations](https://arxiv.org/abs/2508.15816)
*Mauro Belgiovine,Chris Dick,Kaushik Chowdhury*

Main category: cs.NI

TL;DR: 本文提出了一种基于数字孪生（DT）的方法，通过结合NVIDIA的Sionna和Aerial Omniverse Digital Twin（AODT）平台，优化无人机基站（ABS）的位置、天线方向和发射功率，以实现高效覆盖。并通过数值评估和提出弹性机制，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 无人机基站（ABS）因其灵活的资源和快速部署能力，在动态负载和自然灾害恢复中具有潜力。然而，有限的飞行时间和资源要求优化ABS位置，避免耗时的实地试验。

Method: 1. 实现两个开源数字孪生平台（Sionna和AODT）的交互式软件桥，评估场景高保真度。2. 在Sionna中设计反向传播算法，快速优化无人机位置、天线方向和发射功率。3. 在AODT中进行大规模场景（50用户，10 ABS）数值评估。4. 提出弹性机制，确保关键设备覆盖。

Result: 数值评估展示了不同环境条件下两个数字孪生平台性能的一致性和分歧，验证了方法的可行性。弹性机制通过双向信息流实现了关键设备的稳定覆盖。

Conclusion: 数字孪生技术为无人机基站的优化和性能评估提供了高效工具，尤其是在复杂环境和大规模网络中。弹性机制进一步增强了系统的可靠性和适应性。

Abstract: Airborne Base Stations (ABSs) allow for flexible geographical allocation of
network resources with dynamically changing load as well as rapid deployment of
alternate connectivity solutions during natural disasters. Since the radio
infrastructure is carried by unmanned aerial vehicles (UAVs) with limited
flight time, it is important to establish the best location for the ABS without
exhaustive field trials. This paper proposes a digital twin (DT)-guided
approach to achieve this through the following key contributions: (i)
Implementation of an interactive software bridge between two open-source DTs
such that the same scene is evaluated with high fidelity across NVIDIA's Sionna
and Aerial Omniverse Digital Twin (AODT), highlighting the unique features of
each of these platforms for this allocation problem, (ii) Design of a
back-propagation-based algorithm in Sionna for rapidly converging on the
physical location of the UAVs, orientation of the antennas and transmit power
to ensure efficient coverage across the swarm of the UAVs, and (iii) numerical
evaluation in AODT for large network scenarios (50 UEs, 10 ABS) that identifies
the environmental conditions in which there is agreement or divergence of
performance results between these twins. Finally, (iv) we propose a resilience
mechanism to provide consistent coverage to mission-critical devices and
demonstrate a use case for bi-directional flow of information between the two
DTs.

</details>


### [28] [Towards Integrated Energy-Communication-Transportation Hub: A Base-Station-Centric Design in 5G and Beyond](https://arxiv.org/abs/2508.15833)
*Linfeng Shen,Guanzhen Wu,Cong Zhang,Xiaoyi Fan,Jiangchuan Liu*

Main category: cs.NI

TL;DR: 论文探讨了5G基站作为能源-通信-交通（ECT）枢纽的潜力，通过优化电池调度和EV充电价格激励机制，提高能源利用效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 5G基站的广泛部署带来高能耗问题，工业界对储能系统兴趣浓厚，需有效利用基站电池减少运营成本并参与需求响应。

Method: 结合基站分布与EV充电设施，提出激励机制设定充电价格，采用深度强化学习进行电池调度。

Result: 实验证明ECT-Hub能优化剩余能源利用，通过EV充电创收降低运营成本。

Conclusion: 基站作为ECT枢纽具备潜力，结合激励机制和深度学习可高效实现能源与交通整合。

Abstract: The rise of 5G communication has transformed the telecom industry for
critical applications. With the widespread deployment of 5G base stations comes
a significant concern about energy consumption. Key industrial players have
recently shown strong interest in incorporating energy storage systems to store
excess energy during off-peak hours, reducing costs and participating in demand
response. The fast development of batteries opens up new possibilities, such as
the transportation area. An effective method is needed to maximize base station
battery utilization and reduce operating costs. In this trend towards
next-generation smart and integrated energy-communication-transportation (ECT)
infrastructure, base stations are believed to play a key role as service hubs.
By exploring the overlap between base station distribution and electric vehicle
charging infrastructure, we demonstrate the feasibility of efficiently charging
EVs using base station batteries and renewable power plants at the Hub. Our
model considers various factors, including base station traffic conditions,
weather, and EV charging behavior. This paper introduces an incentive mechanism
for setting charging prices and employs a deep reinforcement learning-based
method for battery scheduling. Experimental results demonstrate the
effectiveness of our proposed ECT-Hub in optimizing surplus energy utilization
and reducing operating costs, particularly through revenue-generating EV
charging.

</details>


### [29] [Agent Communications toward Agentic AI at Edge -- A Case Study of the Agent2Agent Protocol](https://arxiv.org/abs/2508.15819)
*Qiang Duan,Zhihui Lu*

Main category: cs.NI

TL;DR: 本文探讨了代理通信技术在边缘计算中的适用性，以A2A协议为例，分析了其在边缘环境中的有效性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着边缘智能的兴起，现有代理通信协议未充分考虑边缘计算的特殊挑战，其有效性有待验证。

Method: 评估A2A协议在边缘计算中的表现，分析其关键技术，提出未来研究方向。

Result: A2A协议在边缘环境中存在不足，需进一步优化以适应边缘计算需求。

Conclusion: 未来研究应聚焦于解决当前代理通信技术的开放问题，以适应边缘计算的需求。

Abstract: The current evolution of artificial intelligence introduces a paradigm shift
toward agentic AI built upon multi-agent systems (MAS). Agent communications
serve as a key to effective agent interactions in MAS and thus have a
significant impact on the performance of agentic AI applications. The recent
research on agent communications has made exciting rapid progress that leads to
a variety of protocol designs, among which the Agent2Agent (A2A) protocol is
considered the most representative one. Simultaneously, the rise of edge
intelligence is expected to enable agentic AI at the network edge. However, the
current agent communication protocols are designed without sufficient
consideration of the special challenges of edge computing, and their
effectiveness in the edge environment is largely unexamined. In this paper, we
attempt to assess the abilities of agent communication technologies to face the
challenges of edge computing using the A2A protocol as a representative case.
We first discuss the core functionalities of agent communications, present a
landscape of agent communication protocols, and identify the main challenges
introduced by edge computing. Then, we conduct a case study on the A2A protocol
to examine the key technologies leveraged in the protocol for their
effectiveness in meeting the requirements of agent communications in edge
computing. Based on the insights obtained from this assessment, we identify
open issues in the current agent communication technologies and discuss
directions for future research to address these issues.

</details>


### [30] [Self-Healing Network of Interconnected Edge Devices Empowered by Infrastructure-as-Code and LoRa Communication](https://arxiv.org/abs/2508.16268)
*Rob Carson,Mohamed Chahine Ghanem,Feriel Bouakkaz*

Main category: cs.NI

TL;DR: 提出了一种基于Raspberry Pi的自愈自动网络，利用LoRa协议和IaC方法解决传统网络不可用场景下的挑战，但存在吞吐量和数据包碰撞问题。


<details>
  <summary>Details</summary>
Motivation: 解决在传统网络不可用场景下的通信问题，利用LoRa和IaC实现低功耗、长距离的自愈网络。

Method: 采用容器化架构部署Raspberry Pi集群，结合LoRa协议和IaC原则，通过数据包分片和重传解决吞吐量问题，集成自动故障转移机制。

Result: 实验表明分片和重传可缓解LoRa的吞吐量限制，自动故障转移机制能在1秒内恢复服务，但碰撞和视线干扰问题仍存在。

Conclusion: 研究展示了自愈网络的可行性，未来需探索网状网络、高级调度算法和LPWAN技术以进一步提升性能。

Abstract: This Paper proposes a self-healing, automated network of Raspberry Pi devices
designed for deployment in scenarios where traditional networking is
unavailable. Leveraging the low-power, long-range capabilities of the LoRa
(Long Range) protocol alongside Infrastructure as Code (IaC) methodologies, the
research addresses challenges such as limited bandwidth, data collisions, and
node failures. Given that LoRa's packet-based system is incompatible with
conventional IaC tools like Ansible and Terraform, which rely on TCP/IP
networking, the research adapts IaC principles within a containerised
architecture deployed across a Raspberry Pi cluster. Evaluation experiments
indicate that fragmenting data packets and retransmitting any missed fragments
can mitigate LoRa's inherent throughput and packet size limitations, although
issues such as collisions and line-of-sight interference persist. An automated
failover mechanism was integrated into the architecture, enabling unresponsive
services to be redeployed to alternative nodes within one second, demonstrating
the system's resilience in maintaining operational continuity despite node or
service failures. The paper also identifies practical challenges, including the
necessity for time-slotting transmissions to prevent data packet overlap and
collisions. Future research should explore the integration of mesh networking
to enhance range, develop more advanced scheduling algorithms, and adopt
cutting-edge low-power wide-area network (LPWAN) techniques.

</details>


### [31] [Safeguarding ISAC Performance in Low-Altitude Wireless Networks Under Channel Access Attack](https://arxiv.org/abs/2508.15838)
*Jiacheng Wang,Jialing He,Geng Sun,Zehui Xiong,Dusit Niyato,Shiwen Mao,Dong In Kim,Tao Xiang*

Main category: cs.NI

TL;DR: 论文提出了一种基于博弈论的框架，用于减轻低空无线网络中的恶意攻击对集成感知与通信性能的影响，并通过仿真验证其优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 随着地面资源日益饱和，低空应用（如空中出租车）需求增长，但低空无线网络的开放性使其易受恶意攻击，影响性能。

Method: 首先推导通信数据的信噪比和感知数据的信息年龄作为服务质量指标，然后将优化问题建模为Stackelberg博弈，并设计逆向归纳算法实现均衡。

Result: 仿真结果表明，所提算法优于现有基准，证明其能有效减轻攻击对性能的退化。

Conclusion: 提出的博弈框架和算法能确保低空无线网络为低空应用提供可靠服务，并在理论上证明了均衡的存在性和唯一性。

Abstract: The increasing saturation of terrestrial resources has driven the exploration
of low-altitude applications such as air taxis. Low altitude wireless networks
(LAWNs) serve as the foundation for these applications, and integrated sensing
and communication (ISAC) constitutes one of the core technologies within LAWNs.
However, the openness nature of low-altitude airspace makes LAWNs vulnerable to
malicious channel access attacks, which degrade the ISAC performance.
Therefore, this paper develops a game-based framework to mitigate the influence
of the attacks on LAWNs. Concretely, we first derive expressions of
communication data's signal-to-interference-plus-noise ratio and the age of
information of sensing data under attack conditions, which serve as quality of
service metrics. Then, we formulate the ISAC performance optimization problem
as a Stackelberg game, where the attacker acts as the leader, and the
legitimate drone and the ground ISAC base station act as second and first
followers, respectively. On this basis, we design a backward induction
algorithm that achieves the Stackelberg equilibrium while maximizing the
utilities of all participants, thereby mitigating the attack-induced
degradation of ISAC performance in LAWNs. We further prove the existence and
uniqueness of the equilibrium. Simulation results show that the proposed
algorithm outperforms existing baselines and a static Nash equilibrium
benchmark, ensuring that LAWNs can provide reliable service for low-altitude
applications.

</details>


### [32] [xDiff: Online Diffusion Model for Collaborative Inter-Cell Interference Management in 5G O-RAN](https://arxiv.org/abs/2508.15843)
*Peihao Yan,Huacheng Zeng,Y. Thomas Hou*

Main category: cs.NI

TL;DR: 提出了xDiff，一种基于扩散模型的强化学习框架，用于解决O-RAN中的小区间干扰管理问题，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: O-RAN是5G及未来网络的关键架构，扩散模型在图像和视频生成中表现优异，适用于网络优化任务。

Method: 将ICIM问题建模为资源分配优化问题，并开发了结合扩散模型的强化学习框架，引入新指标‘偏好值’指导资源分配。

Result: 在5G测试床上的实验结果显示xDiff优于现有ICIM方法。

Conclusion: 扩散模型在O-RAN在线优化中具有潜力，xDiff为ICIM问题提供了高效解决方案。

Abstract: Open Radio Access Network (O-RAN) is a key architectural paradigm for 5G and
beyond cellular networks, enabling the adoption of intelligent and efficient
resource management solutions. Meanwhile, diffusion models have demonstrated
remarkable capabilities in image and video generation, making them attractive
for network optimization tasks. In this paper, we propose xDiff, a
diffusion-based reinforcement learning(RL) framework for inter-cell
interference management (ICIM) in O-RAN. We first formulate ICIM as a resource
allocation optimization problem aimed at maximizing a user-defined reward
function and then develop an online learning solution by integrating a
diffusion model into an RL framework for near-real-time policy generation.
Particularly, we introduce a novel metric, preference values, as the policy
representation to enable efficient policy-guided resource allocation within
O-RAN distributed units (DUs). We implement xDiff on a 5G testbed consisting of
three cells and a set of smartphones in two small-cell scenarios. Experimental
results demonstrate that xDiff outperforms state-of-the-art ICIM approaches,
highlighting the potential of diffusion models for online optimization of
O-RAN. Source code is available on GitHub [1].

</details>


### [33] [Time Series Based Network Intrusion Detection using MTF-Aided Transformer](https://arxiv.org/abs/2508.16035)
*Poorvi Joshi,Mohan Gurusamy*

Main category: cs.NI

TL;DR: 该论文提出了一种基于马尔可夫转移场(MTF)和Transformer的新型时间序列分类方法，用于软件定义网络(SDN)，在数据有限的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决SDN中时间序列分类问题，特别是数据稀疏环境的挑战。

Method: 结合MTF的时间依赖建模能力和Transformer的模式识别能力，提出MTF-aided Transformer模型。

Result: 在InSDN数据集上表现优于基线模型，训练和推理效率高。

Conclusion: MTF-aided Transformer为SDN中数据稀缺场景提供了一种可靠且可扩展的解决方案。

Abstract: This paper introduces a novel approach to time series classification using a
Markov Transition Field (MTF)-aided Transformer model, specifically designed
for Software-Defined Networks (SDNs). The proposed model integrates the
temporal dependency modeling strengths of MTFs with the sophisticated pattern
recognition capabilities of Transformer architectures. We evaluate the model's
performance using the InSDN dataset, demonstrating that our model outperforms
baseline classification models, particularly in data-constrained environments
commonly encountered in SDN applications. We also highlight the relationship
between the MTF and Transformer components, which leads to better performance,
even with limited data. Furthermore, our approach achieves competitive training
and inference times, making it an efficient solution for real-world SDN
applications. These findings establish the potential of MTF-aided Transformers
to address the challenges of time series classification in SDNs, offering a
promising path for reliable and scalable analysis in scenarios with sparse
data.

</details>


### [34] [Congestion Control System Optimization with Large Language Models](https://arxiv.org/abs/2508.16074)
*Zhiyuan He,Aashish Gottipati,Lili Qiu,Yuqing Yang,Francis Y. Yan*

Main category: cs.NI

TL;DR: 提出了一种利用大型语言模型（LLMs）自动优化拥塞控制算法的新方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有拥塞控制算法在不同网络环境中表现不佳，缺乏普适性优化方案。

Method: 通过结构化算法生成、广泛的网络条件仿真评估及统计指导方法，减少评估时间。

Result: 实验验证了方法的有效性，新算法在QUIC实现中比原BBR算法性能提升27%。

Conclusion: LLMs在网络算法设计中具有潜力，可加速高性能网络系统的开发。

Abstract: Congestion control is a fundamental component of Internet infrastructure, and
researchers have dedicated considerable effort to developing improved
congestion control algorithms. However, despite extensive study, existing
algorithms continue to exhibit suboptimal performance across diverse network
environments. In this paper, we introduce a novel approach that automatically
optimizes congestion control algorithms using large language models (LLMs). Our
framework consists of a structured algorithm generation process, an
emulation-based evaluation pipeline covering a broad range of network
conditions, and a statistically guided method to substantially reduce
evaluation time. Empirical results from four distinct LLMs validate the
effectiveness of our approach. We successfully identify algorithms that achieve
up to 27% performance improvements over the original BBR algorithm in a
production QUIC implementation. Our work demonstrates the potential of LLMs to
accelerate the design of high-performance network algorithms and paves the way
for broader applications in networking systems.

</details>


### [35] [ANSC: Probabilistic Capacity Health Scoring for Datacenter-Scale Reliability](https://arxiv.org/abs/2508.16119)
*Madhava Gaikwad,Abhishek Gandhi*

Main category: cs.NI

TL;DR: ANSC是一种用于超大规模数据中心网络的概率性容量健康评分框架，能够评估潜在的级联容量不足风险，帮助运维人员优先处理最关键的问题。


<details>
  <summary>Details</summary>
Motivation: 现有警报系统仅能检测单个设备或链路故障，无法捕捉级联容量不足的聚合风险，因此需要一种更全面的风险评估工具。

Method: ANSC采用颜色编码评分系统，结合当前剩余容量和额外故障概率，在数据中心和区域级别进行归一化评估。

Result: ANSC在400多个数据中心和60个区域中成功减少了噪音，并帮助SRE团队专注于最关键的风险。

Conclusion: ANSC通过概率性容量评分，显著提升了超大规模数据中心网络的风险管理效率。

Abstract: We present ANSC, a probabilistic capacity health scoring framework for
hyperscale datacenter fabrics. While existing alerting systems detect
individual device or link failures, they do not capture the aggregate risk of
cascading capacity shortfalls. ANSC provides a color-coded scoring system that
indicates the urgency of issues \emph{not solely by current impact, but by the
probability of imminent capacity violations}. Our system accounts for both
current residual capacity and the probability of additional failures,
normalized at datacenter and regional level. We demonstrate that ANSC enables
operators to prioritize remediation across more than 400 datacenters and 60
regions, reducing noise and aligning SRE focus on the most critical risks.

</details>


### [36] [Joint Cache Placement and Routing in Satellite-Terrestrial Edge Computing Network: A GNN-Enabled DRL Approach](https://arxiv.org/abs/2508.16184)
*Yuhao Zheng,Ting You,Kejia Peng,Chang Liu*

Main category: cs.NI

TL;DR: 提出了一种基于学习和动态图的方法，用于卫星-地面边缘计算网络中的联合内容缓存和路由优化。


<details>
  <summary>Details</summary>
Motivation: 解决动态LEO卫星拓扑和异构内容需求带来的挑战，提升分布式用户的缓存服务。

Method: 整合图神经网络（GNN）与深度强化学习（DRL），将卫星网络表示为动态图，并应用软Actor-Critic（SAC）算法优化缓存策略。

Result: 显著提高了交付成功率并降低了通信流量成本。

Conclusion: 提出的学习框架在动态和异构环境下有效优化了缓存和路由问题。

Abstract: In this letter, we investigate the problem of joint content caching and
routing in satellite-terrestrial edge computing networks (STECNs) to improve
caching service for geographically distributed users. To handle the challenges
arising from dynamic low Earth orbit (LEO) satellite topologies and
heterogeneous content demands, we propose a learning-based framework that
integrates graph neural networks (GNNs) with deep reinforcement learning (DRL).
The satellite network is represented as a dynamic graph, where GNNs are
embedded within the DRL agent to capture spatial and topological dependencies
and support routing-aware decision-making. The caching strategy is optimized by
formulating the problem as a Markov decision process (MDP) and applying soft
actor-critic (SAC) algorithm. Simulation results demonstrate that our approach
significantly improves the delivery success rate and reduces communication
traffic cost.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [37] [Beyond Interpretability: Exploring the Comprehensibility of Adaptive Video Streaming through Large Language Models](https://arxiv.org/abs/2508.16448)
*Lianchen Jia,Chaoyang Li,Ziqi Yuan,Jiahui Chen,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.MM

TL;DR: 本文提出了一种名为ComTree的比特率自适应算法生成框架，强调开发者理解性，通过结合决策树和大型语言模型评估，显著提升了算法的可理解性。


<details>
  <summary>Details</summary>
Motivation: 深度学习的黑盒特性使开发者难以理解其决策过程，现有研究通过决策树转换提高可解释性，但这并不直接等同于开发者的主观理解性。

Method: ComTree框架首先生成满足性能要求的决策树全集，然后利用大型语言模型评估这些树的开发者理解性，最终选择最易于人类理解和优化的解决方案。

Result: 实验证明，ComTree在保持竞争力的同时显著提高了算法的可理解性，展现出进一步改进的潜力。

Conclusion: ComTree通过结合决策树和大型语言模型，首次在比特率自适应算法中实现了开发者理解性的提升，为未来研究提供了新方向。

Abstract: Over the past decade, adaptive video streaming technology has witnessed
significant advancements, particularly driven by the rapid evolution of deep
learning techniques. However, the black-box nature of deep learning algorithms
presents challenges for developers in understanding decision-making processes
and optimizing for specific application scenarios. Although existing research
has enhanced algorithm interpretability through decision tree conversion,
interpretability does not directly equate to developers' subjective
comprehensibility. To address this challenge, we introduce \texttt{ComTree},
the first bitrate adaptation algorithm generation framework that considers
comprehensibility. The framework initially generates the complete set of
decision trees that meet performance requirements, then leverages large
language models to evaluate these trees for developer comprehensibility,
ultimately selecting solutions that best facilitate human understanding and
enhancement. Experimental results demonstrate that \texttt{ComTree}
significantly improves comprehensibility while maintaining competitive
performance, showing potential for further advancement. The source code is
available at https://github.com/thu-media/ComTree.

</details>


### [38] [Towards User-level QoE: Large-scale Practice in Personalized Optimization of Adaptive Video Streaming](https://arxiv.org/abs/2508.16454)
*Lianchen Jia,Chao Zhou,Chaoyang Li,Jiangchuan Liu,Lifeng Sun*

Main category: cs.MM

TL;DR: LingXi 是大规模部署的个性化自适应视频流系统，通过用户参与度动态优化算法目标，显著提升了用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统基于系统级 QoS 指标的优化方法在现代大规模流媒体系统中已接近性能极限，如何将用户级 QoE 与算法优化目标对齐仍是一个未解决的挑战。

Method: 利用退出率作为关键指标，分析 QoS 指标与退出率的关系，开发个性化退出率预测器，并通过蒙特卡洛采样和在线贝叶斯优化迭代确定最优参数。

Result: 在大规模 A/B 测试中，LingXi 实现了总观看时间增加 0.15%，比特率提升 0.1%，卡顿时间减少 1.3%，低带宽用户卡顿时间减少 15%。

Conclusion: LingXi 成功将用户级 QoE 作为优化目标，显著提升流媒体系统的整体性能和用户体验。

Abstract: Traditional optimization methods based on system-wide Quality of Service
(QoS) metrics have approached their performance limitations in modern
large-scale streaming systems. However, aligning user-level Quality of
Experience~(QoE) with algorithmic optimization objectives remains an unresolved
challenge. Therefore, we propose \texttt{LingXi}, the first large-scale
deployed system for personalized adaptive video streaming based on user-level
experience. \texttt{LingXi} dynamically optimizes the objectives of adaptive
video streaming algorithms by analyzing user engagement. Utilizing exit rate as
a key metric, we investigate the correlation between QoS indicators and exit
rates based on production environment logs, subsequently developing a
personalized exit rate predictor. Through Monte Carlo sampling and online
Bayesian optimization, we iteratively determine optimal parameters. Large-scale
A/B testing utilizing 8\% of traffic on Kuaishou, one of the largest short
video platforms, demonstrates \texttt{LingXi}'s superior performance.
\texttt{LingXi} achieves a 0.15\% increase in total viewing time, a 0.1\%
improvement in bitrate, and a 1.3\% reduction in stall time across all users,
with particularly significant improvements for low-bandwidth users who
experience a 15\% reduction in stall time.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [39] [Experimental Results for Vampire on the Equational Theories Project](https://arxiv.org/abs/2508.15856)
*Mikoláš Janota*

Main category: cs.LO

TL;DR: 《等式理论项目》通过自动定理证明工具Vampire验证一阶逻辑蕴含的有效性，证明其不仅能验证所有成立的蕴含，还能反驳大部分不成立的蕴含。


<details>
  <summary>Details</summary>
Motivation: 探索自动定理证明工具在一阶逻辑蕴含验证中的应用潜力。

Method: 使用Vampire自动定理证明工具对特定类型的蕴含进行验证和反驳。

Result: Vampire能验证所有成立的蕴含，并能反驳大部分不成立的蕴含。

Conclusion: 自动定理证明工具在逻辑验证中表现出高效且全面的能力。

Abstract: Equational Theories Project is a collaborative effort, which explores the
validity of certain first-order logic implications of certain kind. The project
has been completed but triggered further research. This report investigates how
much can be automatically proven and disproven by the automated theorem prover
Vampire. An interesting conclusion is that Vampire can prove all the considered
implications that hold and also is able to refute a vast majority of those that
do not hold.

</details>


### [40] [Lean Meets Theoretical Computer Science: Scalable Synthesis of Theorem Proving Challenges in Formal-Informal Pairs](https://arxiv.org/abs/2508.15878)
*Terry Jingchen Zhang,Wenyuan Jiang,Rongchuan Liu,Yisong Wang,Junran Yang,Ning Wang,Nicole Ni,Yinya Huang,Mrinmaya Sachan*

Main category: cs.LO

TL;DR: 论文提出利用理论计算机科学（TCS）作为可扩展的严格证明问题来源，通过自动生成定理-证明对来解决形式定理证明（FTP）数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有形式定理证明数据集因人工标注成本高和挑战性问题稀缺而受限，需要一种可扩展的方法生成验证过的定理-证明对。

Method: 采用TCS作为问题来源，自动生成形式（Lean4）和非形式（Markdown）规范的定理-证明对，并以Busy Beaver问题和Mixed Boolean Arithmetic问题为例展示了该方法。

Result: 测试显示，即使是前沿模型DeepSeekProver-V2-671B，在Busy Beaver问题上成功率为57.5%，而在Mixed Boolean Arithmetic问题上仅为12%，突显了长证明生成的难度。

Conclusion: TCS领域为推进自动推理研究提供了有价值的挑战性问题，展示了其在形式定理证明中的潜力。

Abstract: Formal theorem proving (FTP) has emerged as a critical foundation for
evaluating the reasoning capabilities of large language models, enabling
automated verification of mathematical proofs at scale. However, progress has
been constrained by limited datasets due to the high cost of manual curation
and the scarcity of challenging problems with verified formal-informal
correspondences. We propose leveraging theoretical computer science (TCS) as a
scalable source of rigorous proof problems, where algorithmic definitions
enable automated generation of arbitrarily many challenging theorem-proof
pairs. We demonstrate this approach on two TCS domains: Busy Beaver problems,
which involve proving bounds on Turing machine halting behavior, and Mixed
Boolean Arithmetic problems, which combine logical and arithmetic reasoning.
Our framework automatically synthesizes problems with parallel formal (Lean4)
and informal (Markdown) specifications, creating a scalable pipeline for
generating verified proof challenges. Evaluation on frontier models reveals
substantial gaps in automated theorem proving: while DeepSeekProver-V2-671B
achieves 57.5\% success on Busy Beaver problems, it manages only 12\% on Mixed
Boolean Arithmetic problems. These results highlight the difficulty of
long-form proof generation even for problems that are computationally easy to
verify, demonstrating the value of TCS domains for advancing automated
reasoning research.

</details>


### [41] [Disjunctions of Two Dependence Atoms](https://arxiv.org/abs/2508.16146)
*Nicolas Fröhlich,Phokion G. Kolaitis,Arne Meier*

Main category: cs.LO

TL;DR: 文章研究了依赖逻辑中两个一元依赖原子析取模型的复杂性，建立了三分法定理，并分类了其复杂性。


<details>
  <summary>Details</summary>
Motivation: 受数据库理论启发，研究依赖逻辑中两个一元依赖原子析取模型的复杂性。

Method: 分析依赖逻辑中两个一元依赖原子析取模型的复杂性，建立三分法定理。

Result: 模型检查问题的复杂性分为三种：NL-complete、LOGSPACE-complete或一阶可定义。

Conclusion: 研究结果为依赖逻辑模型的复杂性提供了分类和理论依据。

Abstract: Dependence logic is a formalism that augments the syntax of first-order logic
with dependence atoms asserting that the value of a variable is determined by
the values of some other variables, i.e., dependence atoms express functional
dependencies in relational databases. On finite structures, dependence logic
captures NP, hence there are sentences of dependence logic whose model-checking
problem is NP-complete. In fact, it is known that there are disjunctions of
three dependence atoms whose model-checking problem is NP-complete. Motivated
from considerations in database theory, we study the model-checking problem for
disjunctions of two unary dependence atoms and establish a trichotomy theorem,
namely, for every such formula, one of the following is true for the
model-checking problem: (i) it is NL-complete; (ii) it is LOGSPACE-complete;
(iii) it is first-order definable (hence, in AC[0]). Furthermore, we classify
the complexity of the model-checking problem for disjunctions of two arbitrary
dependence atoms, and also characterize when such a disjunction is coherent,
i.e., when it satisfies a certain small-model property. Along the way, we
identify a new class of 2CNF-formulas whose satisfiability problem is
LOGSPACE-complete.

</details>


### [42] [A Reduction of Input/Output Logics to SAT](https://arxiv.org/abs/2508.16242)
*Alexander Steen*

Main category: cs.LO

TL;DR: 论文提出了一种基于命题可满足性问题归约的自动化方法，用于处理I/O逻辑，并通过原型实现rio进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为I/O逻辑提供一种自动化推理方法，弥补其在形式化处理中的不足。

Method: 通过将I/O逻辑问题归约到命题可满足性问题序列中，并开发了原型工具rio。

Result: 成功实现了I/O逻辑的自动化推理，并通过示例验证了方法的有效性。

Conclusion: 该方法为I/O逻辑的自动化处理提供了可行方案，未来可进一步优化和扩展。

Abstract: Deontic logics are formalisms for reasoning over norms, obligations,
permissions and prohibitions. Input/Output (I/O) Logics are a particular family
of so-called norm-based deontic logics that formalize conditional norms outside
of the underlying object logic language, where conditional norms do not carry a
truth-value themselves. In this paper, an automation approach for I/O logics is
presented that makes use of suitable reductions to (sequences of) propositional
satisfiability problems. A prototypical implementation, named rio (reasoner for
input/output logics), of the proposed procedures is presented and applied to
illustrative examples.

</details>


### [43] [Uppaal Coshy: Automatic Synthesis of Compact Shields for Hybrid Systems](https://arxiv.org/abs/2508.16345)
*Asger Horn Brorholt,Andreas Holck Høeg-Petersen,Peter Gjøl Jensen,Kim Guldstrand Larsen,Marius Mikučionis,Christian Schilling,Andrzej Wąsowski*

Main category: cs.LO

TL;DR: Uppaal Coshy是一个自动合成马尔可夫决策过程安全策略的工具，通过分区状态空间和模拟解决复杂问题，采用Caap算法高效存储。


<details>
  <summary>Details</summary>
Motivation: 为连续状态空间和复杂混合动力学的马尔可夫决策过程提供自动安全策略合成工具。

Method: 分区状态空间，解决双玩家安全游戏，采用模拟近似解，并用Caap算法高效存储决策树。

Result: 实现了完全自动化的Uppaal模型支持，显著减少了存储需求。

Conclusion: Uppaal Coshy通过分区和高效算法，解决了复杂系统安全策略合成的难题。

Abstract: We present Uppaal Coshy, a tool for automatic synthesis of a safety strategy
-- or shield -- for Markov decision processes over continuous state spaces and
complex hybrid dynamics. The general methodology is to partition the state
space and then solve a two-player safety game, which entails a number of
algorithmically hard problems such as reachability for hybrid systems. The
general philosophy of Uppaal Coshy is to approximate hard-to-obtain solutions
using simulations. Our implementation is fully automatic and supports the
expressive formalism of Uppaal models, which encompass stochastic hybrid
automata. The precision of our partition-based approach benefits from using
finer grids, which however are not efficient to store. We include an algorithm
called Caap to efficiently compute a compact representation of a shield in the
form of a decision tree, which yields significant reductions.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [44] [Harmonious Color Pairings: Insights from Human Preference and Natural Hue Statistics](https://arxiv.org/abs/2508.15777)
*Ortensia Forni,Alexandre Darmon,Michael Benzaquen*

Main category: cs.HC

TL;DR: 本文通过定量研究揭示了颜色配对偏好的高度依赖性，并提出了与自然景观颜色分布一致的和谐模式。


<details>
  <summary>Details</summary>
Motivation: 研究目的是通过数据驱动方法解决色彩和谐研究中定性模型和有限数据集的局限性。

Method: 使用HSL色彩空间中的色调基础调色板，参与者评估了13种不同色调的组合，构建偏好矩阵和可组合性指数。

Result: 颜色偏好高度依赖于色调，但平均后显现出和谐模式，与自然景观中的色调分布一致。

Conclusion: 研究为色彩和谐及其感知和生态基础提供了定量框架。

Abstract: While color harmony has long been studied in art and design, a clear
consensus remains elusive, as most models are grounded in qualitative insights
or limited datasets. In this work, we present a quantitative, data-driven study
of color pairing preferences using controlled hue-based palettes in the HSL
color space. Participants evaluated combinations of thirteen distinct hues,
enabling us to construct a preference matrix and define a combinability index
for each color. Our results reveal that preferences are highly hue dependent,
challenging the assumption of universal harmony rules proposed in the
literature. Yet, when averaged over hues, statistically meaningful patterns of
aesthetic preference emerge, with certain hue separations perceived as more
harmonious. Strikingly, these patterns align with hue distributions found in
natural landscapes, pointing to a statistical correspondence between human
color preferences and the structure of color in nature. Together, these
findings offer a quantitative framework for studying color harmony and its
potential perceptual and ecological underpinnings.

</details>


### [45] [VR Fire safety training application](https://arxiv.org/abs/2508.15788)
*Ujwal M R*

Main category: cs.HC

TL;DR: 介绍了一款VR消防安全培训应用，通过虚拟现实技术安全、逼真地训练用户应对火灾的技能。


<details>
  <summary>Details</summary>
Motivation: 传统消防演练成本高、效果差且难以模拟真实紧急情况的压力，需要更高效、安全的培训方式。

Method: 使用VR头显和动作控制器构建3D虚拟环境，模拟火灾场景、烟雾和逃生路线，提供互动式培训。

Result: 应用能根据用户技能水平调整难度并跟踪进度，提升信心、记忆效果，同时学习更安全、有趣。

Conclusion: VR消防培训作为一种互动体验，有效解决了传统方法的不足，适用于不同经验水平的用户。

Abstract: Fire emergencies can happen without warning and knowing how to respond
quickly can save lives Unfortunately traditional fire drills can be disruptive
costly and often fail to recreate the pressure of a real emergency This project
introduces a Virtual Reality VR Fire Safety Training Application that gives
people a safe yet realistic way to practice life saving skills Using a VR
headset and motion controllers trainees step into a 3D world where fire hazards
smoke and evacuation routes are brought to life They can learn how to use a
fire extinguisher find safe exits and make decisions under pressure without any
real danger The training adapts to the users skill level and tracks progress
making it useful for beginners and experienced personnel alike By turning fire
safety into an interactive experience this VR approach boosts confidence
improves retention and makes learning both safer and more engaging

</details>


### [46] [Kokatsuji: A Visualization Approach for Typographic Forensics of Early Japanese Movable Type](https://arxiv.org/abs/2508.15995)
*Ignacio Perez-Messina,Asanobu Kitamoto*

Main category: cs.HC

TL;DR: 论文介绍了一个可视化系统，用于支持日本可动木活字印刷研究的字体鉴定。


<details>
  <summary>Details</summary>
Motivation: 通过结合机器学习和专家知识，提升对早期印刷书籍生产流程的研究效率与准确性。

Method: 系统基于四个概念对象（展开、段、块和字符）构建，提供交互式工具，支持不同阅读实践。

Result: 初步测试表明，系统能发现分割错误、聚类不一致及不可见的块重用模式。

Conclusion: 该系统为专家用户提供了一个有效的工具，有助于深入理解印刷工艺。

Abstract: We present a visualization system designed to support typographic forensics
in the study of Kokatsuji, the short-lived tradition of Japanese movable wooden
type printing. Building on recent advances in machine learning for block
identification, our system provides expert users with an interactive tool for
exploring, validating hypothesis, and integrating expert knowledge into
model-generated results about the production process of early printed books.
The system is structured around an ontology of four conceptual objects
(spreads, segments, blocks, and characters) each corresponding to a dedicated
view in the system. These coordinated views enable scholars to navigate between
material evidence and computational abstractions, supporting close, near-by,
and distant reading practices. Preliminary results from expert use of the
system demonstrate its ability to reveal errors in segmentation,
inconsistencies in clustering, and previously inaccessible patterns of block
reuse.

</details>


### [47] [Prompting with Sign Parameters for Low-resource Sign Language Instruction Generation](https://arxiv.org/abs/2508.16076)
*Md Tariquzzaman,Md Farhan Ishmam,Saiyma Sittul Muna,Md Kamrul Hasan,Hasan Mahmud*

Main category: cs.HC

TL;DR: 介绍了第一个孟加拉手语指令生成数据集BdSLIG，并提出Sign Parameter-Infused (SPI)提示方法提升零样本性能，促进资源匮乏社区的手语学习。


<details>
  <summary>Details</summary>
Motivation: 为解决资源匮乏手语在AI领域的研究不足问题，推动听障社区的双向交流与学习。

Method: 引入BdSLIG数据集，提出SPI提示法，将标准手语参数（如手形、动作、方向）融入文本提示中。

Result: SPI提示法使指令更结构化、可复现，优于自由文本提示，提升了零样本性能。

Conclusion: 该工作促进了资源匮乏社区的手语学习系统的包容性与技术进步。

Abstract: Sign Language (SL) enables two-way communication for the deaf and
hard-of-hearing community, yet many sign languages remain under-resourced in
the AI space. Sign Language Instruction Generation (SLIG) produces step-by-step
textual instructions that enable non-SL users to imitate and learn SL gestures,
promoting two-way interaction. We introduce BdSLIG, the first Bengali SLIG
dataset, used to evaluate Vision Language Models (VLMs) (i) on under-resourced
SLIG tasks, and (ii) on long-tail visual concepts, as Bengali SL is unlikely to
appear in the VLM pre-training data. To enhance zero-shot performance, we
introduce Sign Parameter-Infused (SPI) prompting, which integrates standard SL
parameters, like hand shape, motion, and orientation, directly into the textual
prompts. Subsuming standard sign parameters into the prompt makes the
instructions more structured and reproducible than free-form natural text from
vanilla prompting. We envision that our work would promote inclusivity and
advancement in SL learning systems for the under-resourced communities.

</details>


### [48] [Cooperative Design Optimization through Natural Language Interaction](https://arxiv.org/abs/2508.16077)
*Ryogo Niwa,Shigeo Yoshida,Yuki Koyama,Yoshitaka Ushiku*

Main category: cs.HC

TL;DR: 该论文提出了一种结合系统优化方法和大型语言模型的设计优化框架，通过自然语言交互提升设计师的参与度。


<details>
  <summary>Details</summary>
Motivation: 传统的系统优化方法限制了设计师在优化过程中的干预能力，影响了设计体验。

Method: 通过结合贝叶斯优化和大语言模型，实现设计师与优化系统的自然语言交互。

Result: 实验表明，该方法提高了用户控制权，优化性能优于手动设计，且认知负荷低于现有协作方法。

Conclusion: 该框架有效平衡了系统优化与设计师干预的需求，提升了设计效率和体验。

Abstract: Designing successful interactions requires identifying optimal design
parameters. To do so, designers often conduct iterative user testing and
exploratory trial-and-error. This involves balancing multiple objectives in a
high-dimensional space, making the process time-consuming and cognitively
demanding. System-led optimization methods, such as those based on Bayesian
optimization, can determine for designers which parameters to test next.
However, they offer limited opportunities for designers to intervene in the
optimization process, negatively impacting the designer's experience. We
propose a design optimization framework that enables natural language
interactions between designers and the optimization system, facilitating
cooperative design optimization. This is achieved by integrating system-led
optimization methods with Large Language Models (LLMs), allowing designers to
intervene in the optimization process and better understand the system's
reasoning. Experimental results show that our method provides higher user
agency than a system-led method and shows promising optimization performance
compared to manual design. It also matches the performance of an existing
cooperative method with lower cognitive load.

</details>


### [49] [Designing Doable and Locally-adapted Action Cards for an Interactive Tabletop Game To Support Bottom-Up Flood Resilience](https://arxiv.org/abs/2508.16480)
*Linda Hirsch,James Fey,Katherine Isbister*

Main category: cs.HC

TL;DR: 论文研究了如何通过卡片游戏提升社区的洪灾韧性，采用参与式设计方法，最终开发了包含20张行动卡片的桌游。


<details>
  <summary>Details</summary>
Motivation: 研究动机是如何通过游戏化方法帮助社区提升洪灾韧性，尤其是在识别和整合本地可行行动方面。

Method: 方法包括与社区教育中心合作，采用迭代参与式设计流程（实地观察、专家焦点小组和在线调查），识别和定义行动。

Result: 研究识别出27项与洪灾韧性相关的行动，并转化为20张游戏卡片，目前正在开发一款互动桌游。

Conclusion: 结论是卡片游戏能有效教育非专家提升洪灾韧性，并展示了从本地需求到游戏化赋权的实践路径。

Abstract: Serious games can support communities in becoming more flood resilient.
However, the process of identifying and integrating locally relevant and doable
actions into gameplay is complex and underresearched. We approached the
challenge by collaborating with a community-led education center and applying
an iterative and participatory design process of identifying and defining
actions that may increase local applicability and relevance. The process
comprised a field observation, two expert focus groups (n=4), and an online
survey (n=13). Our findings identified 27 actions related to increasing or
maintaining individuals' and communities' flood resilience, which we turned
into 20 playing cards. These action cards are a part of a larger interactive
tabletop game, which we are currently developing. Our work discusses the
potential of card games to educate non-experts to increase flood resilience,
and contributes to our process of identifying local needs and conditions, and
turning them into engaging game artifacts for bottom-up empowerment.

</details>


### [50] [SafeSpace: An Integrated Web Application for Digital Safety and Emotional Well-being](https://arxiv.org/abs/2508.16488)
*Kayenat Fatmi,Mohammad Abbas*

Main category: cs.HC

TL;DR: SafeSpace 是一个集成毒性检测、安全警报和情感评估的统一网络应用，旨在提升数字安全和情感健康。


<details>
  <summary>Details</summary>
Motivation: 当前数字时代中，人们在线上面临毒性内容、操纵和骚扰等风险，现有系统未能将数字安全与情感健康结合。

Method: SafeSpace 包含三个模块：基于 NLP 和 Google Perspective API 的毒性检测，可配置的安全警报系统（含实时位置），以及关系健康和情感韧性的自评问卷。

Result: 实验显示毒性检测准确率 93%，安全警报可靠性 100%（模拟测试），问卷自动与手动评分一致率达 92%。

Conclusion: SafeSpace 证明了集成检测、保护和反思的可行性，未来将扩展为移动应用以提升可访问性。

Abstract: In the digital era, individuals are increasingly exposed to online harms such
as toxicity, manipulation, and grooming, which often pose emotional and safety
risks. Existing systems for detecting abusive content or issuing safety alerts
operate in isolation and rarely combine digital safety with emotional
well-being. In this paper, we present SafeSpace, a unified web application that
integrates three modules: (1) toxicity detection in chats and screenshots using
NLP models and Google's Perspective API, (2) a configurable safety ping system
that issues emergency alerts with the user's live location (longitude and
latitude) via SMTP-based emails when check-ins are missed or SOS alerts are
manually triggered, and (3) a reflective questionnaire that evaluates
relationship health and emotional resilience. The system employs Firebase for
alert management and a modular architecture designed for usability, privacy,
and scalability. The experimental evaluation shows 93% precision in toxicity
detection, 100% reliability in safety alerts under emulator tests, and 92%
alignment between automated and manual questionnaire scoring. SafeSpace,
implemented as a web application, demonstrates the feasibility of integrating
detection, protection, and reflection within a single platform, with future
deployment envisioned as a mobile application for broader accessibility.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [51] [Wavelet-Space Super-Resolution for Real-Time Rendering](https://arxiv.org/abs/2508.16024)
*Prateek Poudel,Prashant Aryal,Kirtan Kunwar,Navin Nepal,Dinesh Bania Kshatri*

Main category: cs.GR

TL;DR: 该论文提出了一种基于小波空间特征分解的神经超分辨率方法，通过在重建前分离低频和高频细节，提升纹理保留和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统RGB空间回归在超分辨率中难以同时保留精细纹理和结构一致性的问题，利用小波变换优化神经渲染。

Method: 采用静止小波变换（SWT）避免空间下采样，通过预测小波系数并合成恢复图像，结合空间G缓冲和时间历史帧。

Result: 实验表明，SWT方法平均提升1.5 dB的PSNR，降低17%的LPIPS，计算开销增加约24ms。

Conclusion: 小波域表示是一种提升图形应用中神经超分辨率感知质量的有效方法。

Abstract: We investigate the use of wavelet-space feature decomposition in neural
super-resolution for rendering pipelines. Building on the DFASR framework, we
introduce a wavelet-domain representation that separates low- and
high-frequency details before reconstruction, enabling the network to better
preserve fine textures while maintaining structural consistency. Unlike
RGB-space regression, our approach leverages the stationary wavelet transform
(SWT) to avoid spatial down-sampling, ensuring alignment across subbands and
preserving shift invariance. The model predicts wavelet coefficients
conditioned on spatial G-buffers and temporally warped history frames, which
are then recombined through inverse wavelet synthesis. We conduct a
comprehensive ablation study across wavelet families, transform types, and
architectural variants, showing that incorporating SWT improves PSNR by up to
1.5 dB and reduces LPIPS by 17% on average, at a computational overhead of
roughly +24 ms compared to out DFASR baseline. While absolute runtimes on our
RTX 3050 mobile GPU are higher ( 141ms) than the original DFASR report on RTX
4090( 11ms), the relative overhead remains modest, suggesting that on
higher-end GPUs our method would also remain real-time capable. Taken together,
our results suggest that wavelet-domain representations are a principled and
effective way to enhance perceptual quality in neural upscaling for graphics
applications.

</details>


### [52] [Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars](https://arxiv.org/abs/2508.16401)
*NVIDIA,:,Chaeyeon Chung,Ilya Fedorov,Michael Huang,Aleksey Karmanov,Dmitry Korobchenko,Roger Ribera,Yeongho Seol*

Main category: cs.GR

TL;DR: 论文介绍了NVIDIA Audio2Face-3D技术，旨在通过音频驱动生成3D面部动画，支持实时人机交互，并开源了相关工具和数据集。


<details>
  <summary>Details</summary>
Motivation: 解决数字虚拟角色的面部动画制作问题，提升实时互动的真实性和效率，服务于游戏开发和数字虚拟角色创作。

Method: 包括数据采集、网络架构设计、动作重定向方法、评估指标和使用案例的介绍。

Result: 实现了音频驱动的实时3D面部动画生成，并提供了开源工具和数据集。

Conclusion: Audio2Face-3D为开发者提供了一个高效的工具，推动了数字虚拟角色面部动画技术的发展。

Abstract: Audio-driven facial animation presents an effective solution for animating
digital avatars. In this paper, we detail the technical aspects of NVIDIA
Audio2Face-3D, including data acquisition, network architecture, retargeting
methodology, evaluation metrics, and use cases. Audio2Face-3D system enables
real-time interaction between human users and interactive avatars, facilitating
facial animation authoring for game characters. To assist digital avatar
creators and game developers in generating realistic facial animations, we have
open-sourced Audio2Face-3D networks, SDK, training framework, and example
dataset.

</details>


### [53] [Real-time 3D Light-field Viewing with Eye-tracking on Conventional Displays](https://arxiv.org/abs/2508.16535)
*Trung Hieu Pham,Chanh Minh Tran,Eiji Kamioka,Xuan Tan Phan*

Main category: cs.GR

TL;DR: 提出了一种低成本系统，利用普通2D显示器、RGB摄像头和红青立体眼镜实现实时3D光场观看，无需昂贵硬件。


<details>
  <summary>Details</summary>
Motivation: 解决3D可视化技术因硬件成本高而难以普及的问题，提升资源受限环境下的可访问性。

Method: 集成实时眼球追踪和轻量级渲染管线，动态调整光场图像以适应头部位置。

Result: 系统在CPU上实现30 FPS稳定帧率，证实了其在消费级硬件上的可行性。

Conclusion: 该系统为教育、数字媒体等领域的交互式3D应用提供了可访问的平台。

Abstract: Creating immersive 3D visual experiences typically requires expensive and
specialized hardware such as VR headsets, autostereoscopic displays, or active
shutter glasses. These constraints limit the accessibility and everyday use of
3D visualization technologies in resource-constrained settings. To address
this, we propose a low-cost system that enables real-time 3D light-field
viewing using only a standard 2D monitor, a conventional RGB webcam, and
red-cyan anaglyph glasses. The system integrates real-time eye-tracking to
dynamically adapt the displayed light-field image to the user's head position
with a lightweight rendering pipeline that selects and composites stereoscopic
views from pre-captured light-field data. The resulting anaglyph image is
updated in real-time, creating a more immersive and responsive 3D experience.
The system operates entirely on CPU and maintains a stable frame rate of 30
FPS, confirming its feasibility on typical consumer-grade hardware. All of
these highlight the potential of our approach as an accessible platform for
interactive 3D applications in education, digital media, and beyond.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [54] [HePGA: A Heterogeneous Processing-in-Memory based GNN Training Accelerator](https://arxiv.org/abs/2508.16011)
*Chukwufumnanya Ogbogu,Gaurav Narang,Biresh Kumar Joardar,Janardhan Rao Doppa,Krishnendu Chakrabarty,Partha Pratim Pande*

Main category: cs.ET

TL;DR: 本文提出了一种基于3D异构内存计算（PIM）架构的加速器HePGA，用于高效节能地加速图神经网络（GNN）训练，并在性能和能效上显著优于现有PIM架构。


<details>
  <summary>Details</summary>
Motivation: 现有PIM设备（如ReRAM、FeFET等）在功耗、延迟和面积等方面存在不同的权衡，通过3D集成技术将多种PIM设备结合到一个平台上，可以优化GNN训练的性能和能效。

Method: 通过利用GNN层及其计算核的独特特性，将任务优化映射到不同的PIM设备和平面层级，设计了一种名为HePGA的异构PIM加速器。

Result: 实验表明，HePGA在能效（TOPS/W）和计算效率（TOPS/mm2）上分别比现有PIM架构提升了3.8倍和6.8倍，且不影响GNN预测准确性。

Conclusion: HePGA不仅适用于GNN训练，还能加速新兴Transformer模型的推理任务，展示了其广泛的应用潜力。

Abstract: Processing-In-Memory (PIM) architectures offer a promising approach to
accelerate Graph Neural Network (GNN) training and inference. However, various
PIM devices such as ReRAM, FeFET, PCM, MRAM, and SRAM exist, with each device
offering unique trade-offs in terms of power, latency, area, and
non-idealities. A heterogeneous manycore architecture enabled by 3D integration
can combine multiple PIM devices on a single platform, to enable
energy-efficient and high-performance GNN training. In this work, we propose a
3D heterogeneous PIM-based accelerator for GNN training referred to as HePGA.
We leverage the unique characteristics of GNN layers and associated computing
kernels to optimize their mapping on to different PIM devices as well as planar
tiers. Our experimental analysis shows that HePGA outperforms existing
PIM-based architectures by up to 3.8x and 6.8x in energy-efficiency (TOPS/W)
and compute efficiency (TOPS/mm2) respectively, without sacrificing the GNN
prediction accuracy. Finally, we demonstrate the applicability of HePGA to
accelerate inferencing of emerging transformer models.

</details>


### [55] [Set Transformer Architectures and Synthetic Data Generation for Flow-Guided Nanoscale Localization](https://arxiv.org/abs/2508.16200)
*Mika Leo Hube,Filip Lemic,Ethungshan Shitiri,Gerard Calvo Bartra,Sergi Abadal,Xavier Costa Pérez*

Main category: cs.ET

TL;DR: FGL提出了一种利用无序集和生成模型增强的Set Transformer架构，用于改进基于纳米设备的体内定位方法。


<details>
  <summary>Details</summary>
Motivation: 现有FGL方法依赖固定拓扑或手工特征，难以适应解剖学变异，且扩展性受限。

Method: 采用Set Transformer处理无序输入，结合生成模型（如CGAN、WGAN等）生成合成数据以增强训练。

Result: Set Transformer在分类准确度上与GNN相当，且能更好地适应解剖学变异。

Conclusion: 实验说明了置换不变模型和合成数据增强在纳米级定位中的潜力。

Abstract: Flow-guided Localization (FGL) enables the identification of spatial regions
within the human body that contain an event of diagnostic interest. FGL does
that by leveraging the passive movement of energy-constrained nanodevices
circulating through the bloodstream. Existing FGL solutions rely on graph
models with fixed topologies or handcrafted features, which limit their
adaptability to anatomical variability and hinder scalability. In this work, we
explore the use of Set Transformer architectures to address these limitations.
Our formulation treats nanodevices' circulation time reports as unordered sets,
enabling permutation-invariant, variable-length input processing without
relying on spatial priors. To improve robustness under data scarcity and class
imbalance, we integrate synthetic data generation via deep generative models,
including CGAN, WGAN, WGAN-GP, and CVAE. These models are trained to replicate
realistic circulation time distributions conditioned on vascular region labels,
and are used to augment the training data. Our results show that the Set
Transformer achieves comparable classification accuracy compared to Graph
Neural Networks (GNN) baselines, while simultaneously providing by-design
improved generalization to anatomical variability. The findings highlight the
potential of permutation-invariant models and synthetic augmentation for robust
and scalable nanoscale localization.

</details>


### [56] [Energy-Information Trade-Off in Self-Directed Channel Memristors](https://arxiv.org/abs/2508.16236)
*Waleed El-Geresy,Dániel Hajtó,György Cserey,Deniz Gündüz*

Main category: cs.ET

TL;DR: 研究了SDC忆阻器的能量-信息权衡，通过实验建模能量需求，并利用cGAN生成模型评估存储条件分布。


<details>
  <summary>Details</summary>
Motivation: 探讨忆阻器在数据存储和神经形态应用中的信息存储能力及其能耗关系。

Method: 实验测量SDC忆阻器状态设定能耗及稳定性，并使用cGAN生成模型分析存储条件分布。

Result: 揭示了能量消耗与设备有效容量之间的优雅权衡。

Conclusion: SDC忆阻器的能量-信息权衡为优化存储设备和神经形态应用提供了新视角。

Abstract: Understanding the nature of information storage on memristors is vital to
enable their use in novel data storage and neuromorphic applications. One key
consideration in information storage is the energy cost of storage and what
impact the available energy has on the information capacity of the devices. In
this paper, we propose and study an energy-information trade-off for a
particular kind of memristive device - Self-Directed Channel (SDC) memristors.
We perform experiments to model the energy required to set the devices into
various states, as well as assessing the stability of these states over time.
Based on these results, we employ a generative modelling approach, using a
conditional Generative Adversarial Network (cGAN) to characterise the storage
conditional distribution, allowing us to estimate energy-information curves for
a range of storage delays, showing the graceful trade-off between energy
consumed and the effective capacity of the devices.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [57] [HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO Serving and Fast Scaling](https://arxiv.org/abs/2508.15919)
*Zahra Yousefijamarani,Xinglu Wang,Qian Wang,Morgan Lindsay Heisler,Taha Shabani,Niloofar Gholipour,Parham Yassini,Hong Chang,Kan Chen,Qiantao Zhang,Xiaolong Bai,Jiannan Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: HyperFlexis是一个统一的LLM服务系统，通过算法和系统级创新优化调度和扩展，支持多SLO调度、高效扩展和KV缓存传输，显著提升性能和降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决现代LLM服务系统面临的请求多样性、优先级和服务级目标（SLO）多变性的挑战。

Method: 设计多SLO感知调度器、成本效益扩展决策、预填充-解码实例链接，以及D2D权重传输机制。

Result: 系统实现最高4.44倍的SLO达成率提升，65.82%的延迟降低，并保持与现有技术成本持平。

Conclusion: HyperFlexis通过高效调度和扩展优化，显著提升了LLM服务系统的性能和成本效益。

Abstract: Modern large language model (LLM) serving systems face challenges from highly
variable requests with diverse lengths, priorities, and stage-specific
service-level objectives (SLOs). Meeting these requires real-time scheduling,
rapid and cost-effective scaling, and support for both collocated and
disaggregated Prefill/Decode (P/D) architectures.
  We present \textbf{HyperFlexis}, a unified LLM serving system that integrates
algorithmic and system-level innovations to jointly optimize scheduling and
scaling under multiple SLOs. It features a multi-SLO-aware scheduler that
leverages budget estimation and request prioritization to ensure proactive SLO
compliance for both new and ongoing requests. The system supports prefill- and
decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV
cache transfers. It also enables cost-effective scaling decisions,
prefill-decode instance linking during scaling, and rapid P/D role transitions.
To accelerate scaling and reduce cold-start latency, a device-to-device (D2D)
weight transfer mechanism is proposed that lowers weight loading overhead by up
to \textbf{19.39$\times$}. These optimizations allow the system to achieve up
to \textbf{4.44$\times$} higher SLO attainment, \textbf{65.82\%} lower request
latency, and cost parity with state-of-the-art baselines. The code will be
released soon.

</details>


### [58] [Generalizing Brooks' theorem via Partial Coloring is Hard Classically and Locally](https://arxiv.org/abs/2508.16308)
*Jan Bok,Avinandan Das,Anna Gujgiczer,Nikola Jedličková*

Main category: cs.DC

TL;DR: 论文研究了$k$-partial $k$-coloring问题的经典和分布式复杂性，发现当颜色数量从$k+1$减少到$k$时，问题复杂度显著增加。经典情况下该问题是NP完全的，分布式情况下存在$\Omega(n)$轮下界。


<details>
  <summary>Details</summary>
Motivation: 探讨$k$-partial $c$-coloring问题在$c=k$时的复杂性，以扩展对Brooks定理的理解，并回答之前研究中提出的开放性问题。

Method: 通过构造复杂图结构和不可区分性论证，证明经典NP完全性和分布式下界。

Result: 经典情况下$k$-partial $k$-coloring对$k\geq3$是NP完全的；分布式情况下存在$\Omega(n)$轮下界。

Conclusion: 颜色数量减少至$k$时，问题复杂度显著增加，揭示了与$k+1$情况的关键差异。

Abstract: We investigate the classical and distributed complexity of \emph{$k$-partial
$c$-coloring} where $c=k$, a natural generalization of Brooks' theorem where
each vertex should be colored from the palette $\{1,\ldots,c\} =
\{1,\ldots,k\}$ such that it must have at least $\min\{k, \deg(v)\}$ neighbors
colored differently. Das, Fraigniaud, and Ros{\'{e}}n~[OPODIS 2023] showed that
the problem of $k$-partial $(k+1)$-coloring admits efficient centralized and
distributed algorithms and posed an open problem about the status of the
distributed complexity of $k$-partial $k$-coloring. We show that the problem
becomes significantly harder when the number of colors is reduced from $k+1$ to
$k$ for every constant $k\geq 3$.
  In the classical setting, we prove that deciding whether a graph admits a
$k$-partial $k$-coloring is NP-complete for every constant $k \geq 3$,
revealing a sharp contrast with the linear-time solvable $(k+1)$-color case.
For the distributed LOCAL model, we establish an $\Omega(n)$-round lower bound
for computing $k$-partial $k$-colorings, even when the graph is guaranteed to
be $k$-partial $k$-colorable. This demonstrates an exponential separation from
the $O(\log^2 k \cdot \log n)$-round algorithms known for $(k+1)$-colorings.
  Our results leverage novel structural characterizations of ``hard instances''
where partial coloring reduces to proper coloring, and we construct intricate
graph gadgets to prove lower bounds via indistinguishability arguments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [59] [Combined Approximations for Uniform Operational Consistent Query Answering](https://arxiv.org/abs/2508.15814)
*Marco Calautti,Ester Livshits,Andreas Pieris,Markus Schneider*

Main category: cs.DB

TL;DR: 该论文研究了在操作一致性查询回答（CQA）框架下，对于自连接自由且广义超树宽度有界的联合查询，是否存在高效的查询近似计算方案。


<details>
  <summary>Details</summary>
Motivation: 探讨在查询作为输入的一部分（即组合复杂度）的情况下，如何高效近似计算满足给定查询的修复百分比。

Method: 引入计数复杂性类 $\\mathsf{SpanTL}$，并利用树自动机的近似性结果，将问题置于此类中。

Result: 证明了在上述限制条件下（自连接自由且有界广义超树宽度）存在高效的近似方案；若放弃任一限制，则不可能存在高效近似。

Conclusion: 该研究为操作CQA框架下的高效查询近似提供了理论支持，同时明确了限制条件的重要性。

Abstract: Operational consistent query answering (CQA) is a recent framework for CQA
based on revised definitions of repairs, which are built by applying a sequence
of operations (e.g., fact deletions) starting from an inconsistent database
until we reach a database that is consistent w.r.t. the given set of
constraints. It has been recently shown that there is an efficient
approximation for computing the percentage of repairs that entail a given query
when we focus on primary keys, conjunctive queries, and assuming the query is
fixed (i.e., in data complexity). However, it has been left open whether such
an approximation exists when the query is part of the input (i.e., in combined
complexity). We show that this is the case when we focus on self-join-free
conjunctive queries of bounded generelized hypertreewidth. We also show that it
is unlikely that efficient approximation schemes exist once we give up one of
the adopted syntactic restrictions, i.e., self-join-freeness or bounding the
generelized hypertreewidth. Towards the desired approximation, we introduce a
counting complexity class, called $\mathsf{SpanTL}$, show that each problem in
it admits an efficient approximation scheme by using a recent approximability
result about tree automata, and then place the problem of interest in
$\mathsf{SpanTL}$.

</details>


### [60] [MAAdvisor: Zero-Shot Index Advisor using Multi-Agent LLMs](https://arxiv.org/abs/2508.16044)
*Zhaodonghui Li,Haitao Yuan,Jiachen Shi,Hao Zhang,Yu Rong,Gao Cong*

Main category: cs.DB

TL;DR: 提出了一种基于多智能体框架的零样本LLM索引顾问MAAdvisor，通过分解索引推荐问题为子步骤并设计专门的智能体，实现了更高的效率和零样本推理能力，超越了传统启发式和学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统索引推荐方法计算成本高、泛化能力差以及提示调优方法资源密集且效果不佳的问题。

Method: 将索引推荐分解为规划、选择、组合、修订和反思等子步骤，设计全局和局部智能体分别控制流程和具体操作。

Result: MAAdvisor在实验中达到了最佳性能，且效率和零样本推理能力优于现有方法。

Conclusion: MAAdvisor为索引推荐提供了一种高效且泛化能力强的解决方案。

Abstract: Index recommendation is one of the most important problems in database
management system (DBMS) optimization. Given queries and certain index-related
constraints, traditional methods rely on heuristic optimization or
learning-based models to select effective indexes and improve query
performance. However, heuristic optimization suffers from high computation
time, and learning-based models lose generalisability due to training for
different workloads and database schemas. With the recent rapid development of
large language models (LLMs), methods using prompt tuning have been proposed to
enhance the efficiency of index selection. However, such methods still can not
achieve the state-of-the-art (SOTA) results, and preparing the index selection
demonstrations is also resource-intensive. To address these issues, we propose
MAAdvisor, a zero-shot LLM-based index advisor with a multi-agent framework. We
decompose the index recommendation problem into sub-steps, including planning,
selection, combination, revision, and reflection. A set of LLM-embedded agents
is designed to handle each one of the different sub-steps. Our method utilizes
global agents to control the index selection process and local agents to select
and revise indexes. Through extensive experiments, we show that our proposed
MAAdvisor not only achieves the SOTA performance compared to the heuristic
methods, but also outperforms learning-based and prompt-based methods with
higher efficiency and better zero-shot inference ability.

</details>


### [61] [Attribute Filtering in Approximate Nearest Neighbor Search: An In-depth Experimental Study](https://arxiv.org/abs/2508.16263)
*Mocheng Li,Xiao Yan,Baotong Lu,Yue Zhang,James Cheng,Chenhao Ma*

Main category: cs.DB

TL;DR: 本文提出并评估了一个统一的过滤近似最近邻搜索接口，对现有算法进行了分类和分析，并提供了性能比较和实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着结构化和非结构化数据的融合，需要统一分析过滤近似最近邻搜索算法，以便比较和优化。

Method: 提出分类法，分析算法关键组件（如索引结构、剪枝策略等），并在多个数据集上进行实验评估。

Result: 实验揭示了剪枝、入口点选择和边缘过滤对性能的影响，总结了各方法的优缺点。

Conclusion: 总结了每种方法的适用场景，并提出了未来研究方向，代码已开源。

Abstract: With the growing integration of structured and unstructured data, new methods
have emerged for performing similarity searches on vectors while honoring
structured attribute constraints, i.e., a process known as Filtering
Approximate Nearest Neighbor (Filtering ANN) search. Since many of these
algorithms have only appeared in recent years and are designed to work with a
variety of base indexing methods and filtering strategies, there is a pressing
need for a unified analysis that identifies their core techniques and enables
meaningful comparisons.
  In this work, we present a unified Filtering ANN search interface that
encompasses the latest algorithms and evaluate them extensively from multiple
perspectives. First, we propose a comprehensive taxonomy of existing Filtering
ANN algorithms based on attribute types and filtering strategies. Next, we
analyze their key components, i.e., index structures, pruning strategies, and
entry point selection, to elucidate design differences and tradeoffs. We then
conduct a broad experimental evaluation on 10 algorithms and 12 methods across
4 datasets (each with up to 10 million items), incorporating both synthetic and
real attributes and covering selectivity levels from 0.1% to 100%. Finally, an
in-depth component analysis reveals the influence of pruning, entry point
selection, and edge filtering costs on overall performance. Based on our
findings, we summarize the strengths and limitations of each approach, provide
practical guidelines for selecting appropriate methods, and suggest promising
directions for future research. Our code is available at:
https://github.com/lmccccc/FANNBench.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [62] [ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with Benchmark Evaluation](https://arxiv.org/abs/2508.15940)
*Ahmed Allam,Youssef Mansour,Mohamed Shalan*

Main category: cs.AR

TL;DR: ASIC-Agent是一个为数字ASIC设计任务设计的自主系统，通过多代理架构增强基础LLM，解决其在硬件设计中的局限性，并展示了显著加速ASIC设计流程的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在RTL设计方面表现出色，但在实际硬件设计流程中存在代码执行、调试能力和长期记忆的不足，ASIC-Agent旨在解决这些问题。

Method: ASIC-Agent采用多代理架构，包含RTL生成、验证、OpenLane硬化和Caravel芯片集成等子代理，并利用包含社区知识和工具的向量数据库。

Result: ASIC-Agent与Claude 4 Sonnet结合，能自动化完成多种复杂度的ASIC设计任务，展示了加速设计流程的潜力。

Conclusion: ASIC-Agent通过多代理架构和社区资源整合，成功解决了LLM在硬件设计中的局限性，并提供了高效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
Register Transfer Level (RTL) design, enabling high-quality code generation
from natural language descriptions. However, LLMs alone face significant
limitations in real-world hardware design workflows, including the inability to
execute code, lack of debugging capabilities, and absence of long-term memory.
To address these challenges, we present ASIC-Agent, an autonomous system
designed specifically for digital ASIC design tasks. ASIC-Agent enhances base
LLMs with a multi-agent architecture incorporating specialized sub-agents for
RTL generation, verification, OpenLane hardening, and Caravel chip integration,
all operating within a comprehensive sandbox environment with access to
essential hardware design tools. The system leverages a vector database
containing documentation, API references, error knowledge, and curated insights
from the open-source silicon community. To evaluate ASIC-Agent's performance,
we introduce ASIC-Agent-Bench, the first benchmark specifically designed to
assess agentic systems in hardware design tasks. We evaluate ASIC-Agent with
various base LLMs, providing quantitative comparisons and qualitative insights
into agent behavior across different design scenarios. Our results demonstrate
that ASIC-Agent, when powered by Claude 4 Sonnet, successfully automates a
broad range of ASIC design tasks spanning varying levels of complexity, showing
the potential of significantly accelerating the ASIC design workflow.

</details>


### [63] [Bare-Metal RISC-V + NVDLA SoC for Efficient Deep Learning Inference](https://arxiv.org/abs/2508.16095)
*Vineet Kumar,Ajay Kumar M,Yike Li,Shreejith Shanker,Deepu John*

Main category: cs.AR

TL;DR: 提出了一种新型SoC架构，通过硬件与软件优化加速边缘计算中的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决边缘计算中深度学习模型的加速问题，克服现有方案的操作系统开销。

Method: 结合NVDLA与RISC-V核心，生成裸机应用代码以减少开销。

Result: 在100 MHz时钟频率下，LeNet-5、ResNet-18和ResNet-50的推理时间分别为4.8 ms、16.2 ms和1.1 s。

Conclusion: 该架构显著提升了执行速度和存储效率，适用于边缘计算。

Abstract: This paper presents a novel System-on-Chip (SoC) architecture for
accelerating complex deep learning models for edge computing applications
through a combination of hardware and software optimisations. The hardware
architecture tightly couples the open-source NVIDIA Deep Learning Accelerator
(NVDLA) to a 32-bit, 4-stage pipelined RISC-V core from Codasip called uRISC_V.
To offload the model acceleration in software, our toolflow generates
bare-metal application code (in assembly), overcoming complex OS overheads of
previous works that have explored similar architectures. This tightly coupled
architecture and bare-metal flow leads to improvements in execution speed and
storage efficiency, making it suitable for edge computing solutions. We
evaluate the architecture on AMD's ZCU102 FPGA board using NVDLA-small
configuration and test the flow using LeNet-5, ResNet-18 and ResNet-50 models.
Our results show that these models can perform inference in 4.8 ms, 16.2 ms and
1.1 s respectively, at a system clock frequency of 100 MHz.

</details>


### [64] [Hardwired-Neurons Language Processing Units as General-Purpose Cognitive Substrates](https://arxiv.org/abs/2508.16151)
*Yang Liu,Yi Chen,Yongwei Zhao,Yifan Hao,Zifu Zheng,Weihao Kong,Zhangmai Li,Dongchen Jiang,Ruiyang Xia,Zhihong Ma,Zisheng Liu,Zhaoyong Wan,Yunqi Lu,Ximing Liu,Hongrui Guo,Zhihao Yang,Zhe Wang,Tianrui Ma,Mo Zou,Rui Zhang,Ling Li,Xing Hu,Zidong Du,Zhiwei Xu,Qi Guo,Tianshi Chen,Yunji Chen*

Main category: cs.AR

TL;DR: 论文提出了一种专为大型语言模型（LLM）推理设计的硬连线神经元语言处理单元（HNLPU），并通过金属嵌入方法显著降低了制造成本，提升了计算效率和能源效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，其推理任务对能源的需求急剧增加，迫切需要一种高效且经济的专用硬件解决方案。

Method: 采用硬连线技术，将LLM权重参数直接嵌入计算架构中，并通过创新的金属嵌入方法提升密度和降低制造成本。

Result: 实验结果显示，HNLPU在计算效率、能源效率和制造成本方面均有显著提升，优于现有GPU和WSE技术。

Conclusion: HNLPU通过硬连线和金属嵌入技术，为LLM推理提供了一种高效、经济且环保的硬件解决方案。

Abstract: The rapid advancement of Large Language Models (LLMs) has established
language as a core general-purpose cognitive substrate, driving the demand for
specialized Language Processing Units (LPUs) tailored for LLM inference. To
overcome the growing energy consumption of LLM inference systems, this paper
proposes a Hardwired-Neurons Language Processing Unit (HNLPU), which physically
hardwires LLM weight parameters into the computational fabric, achieving
several orders of magnitude computational efficiency improvement by extreme
specialization. However, a significant challenge still lies in the scale of
modern LLMs. An ideal estimation on hardwiring gpt-oss 120 B requires
fabricating at least 6 billion dollars of photomask sets, rendering the
straightforward solution economically impractical. Addressing this challenge,
we propose the novel Metal-Embedding methodology. Instead of embedding weights
in a 2D grid of silicon device cells, Metal-Embedding embeds weight parameters
into the 3D topology of metal wires. This brings two benefits: (1) a 15x
increase in density, and (2) 60 out of 70 layers of photomasks are made
homogeneous across chips, including all EUV photomasks. In total,
Metal-Embedding reduced the photomask cost by 112x, bringing the Non-Recurring
Engineering (NRE) cost of HNLPU into an economically viable range. Experimental
results show that HNLPU achieved 249,960 tokens/s (5,555x/85x of GPU/WSE), 36
tokens/J (1,047x/283x of GPU/WSE), 13,232 mm2 total die area (29% inscribed
rectangular area in a 300 mm wafer), \$184M estimated NRE at 5 nm technology.
Analysis shows that HNLPU achieved 8.57x cost-effectiveness and 230x carbon
footprint reduction compared to H100 clusters, under an annual weight updating
assumption.

</details>


### [65] [RIROS: A Parallel RTL Fault SImulation FRamework with TwO-Dimensional Parallelism and Unified Schedule](https://arxiv.org/abs/2508.16376)
*Jiaping Tang,Jianan Mu,Zizhen Liu,Ge Yu,Tenghui Hua,Bin Sun,Silin Liu,Jing Ye,Huawei Li*

Main category: cs.AR

TL;DR: 提出了一种二维并行方法（RIROS）来加速RTL故障模拟，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶等安全关键应用的发展，芯片功能安全的验证需求增加，但传统并行方法效率不足。

Method: 结合结构级和故障级并行，引入工作窃取机制和统一调度方法。

Result: 实验显示性能分别提升7倍和11倍。

Conclusion: RIROS能高效减少RTL故障模拟中的空闲时间。

Abstract: With the rapid development of safety-critical applications such as autonomous
driving and embodied intelligence, the functional safety of the corresponding
electronic chips becomes more critical. Ensuring chip functional safety
requires performing a large number of time-consuming RTL fault simulations
during the design phase, significantly increasing the verification cycle. To
meet time-to-market demands while ensuring thorough chip verification, parallel
acceleration of RTL fault simulation is necessary. Due to the dynamic nature of
fault propagation paths and varying fault propagation capabilities, task loads
in RTL fault simulation are highly imbalanced, making traditional
singledimension parallel methods, such as structural-level parallelism,
ineffective. Through an analysis of fault propagation paths and task loads, we
identify two types of tasks in RTL fault simulation: tasks that are few in
number but high in load, and tasks that are numerous but low in load. Based on
this insight, we propose a two-dimensional parallel approach that combines
structurallevel and fault-level parallelism to minimize bubbles in RTL fault
simulation. Structural-level parallelism combining with workstealing mechanism
is used to handle the numerous low-load tasks, while fault-level parallelism is
applied to split the high-load tasks. Besides, we deviate from the traditional
serial execution model of computation and global synchronization in RTL
simulation by proposing a unified computation/global synchronization scheduling
approach, which further eliminates bubbles. Finally, we implemented a parallel
RTL fault simulation framework, RIROS. Experimental results show a performance
improvement of 7.0 times and 11.0 times compared to the state-of-the-art RTL
fault simulation and a commercial tool.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [66] [Hybrid Classical-Quantum Supercomputing: A demonstration of a multi-user, multi-QPU and multi-GPU environment](https://arxiv.org/abs/2508.16297)
*Mateusz Slysz,Piotr Rydlichowski,Krzysztof Kurowski,Omar Bacarezza,Esperanza Cuenca Gomez,Zohim Chandani,Bettina Heim,Pradnya Khalate,William R. Clements,James Fletcher*

Main category: quant-ph

TL;DR: 本文介绍了世界上首个在HPC中心实现的多用户混合经典-量子计算环境，用于支持混合算法在多个量子处理单元和GPU上的执行。


<details>
  <summary>Details</summary>
Motivation: 为了将量子计算的实际优势带给用户，需要在HPC中心提供支持混合经典-量子算法的软硬件基础设施。

Method: 在Poznan超级计算与网络中心（PCSS）部署了符合当前HPC规范的设备，使用Slurm进行工作负载管理，并结合NVIDIA CUDA-Q扩展API实现经典-量子交互。

Result: 该系统成功应用于混合经典-量子机器学习和优化任务。

Conclusion: 本文为量子计算如何实际增强和扩展HPC能力提供了一个实验性范例，推动社区进一步研究和发展。

Abstract: Achieving a practical quantum advantage for near-term applications is widely
expected to rely on hybrid classical-quantum algorithms. To deliver this
practical advantage to users, high performance computing (HPC) centers need to
provide a suitable software and hardware stack that supports algorithms of this
type. In this paper, we describe the world's first implementation of a
classical-quantum environment in an HPC center that allows multiple users to
execute hybrid algorithms on multiple quantum processing units (QPUs) and GPUs.
Our setup at the Poznan Supercomputing and Networking Center (PCSS) aligns with
current HPC norms: the computing hardware including QPUs is installed in an
active data center room with standard facilities; there are no special
considerations for networking, power, and cooling; we use Slurm for workload
management as well as the NVIDIA CUDA-Q extension API for classical-quantum
interactions. We demonstrate applications of this environment for hybrid
classical-quantum machine learning and optimisation. The aim of this work is to
provide the community with an experimental example for further research and
development on how quantum computing can practically enhance and extend HPC
capabilities.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [67] [A Survey of Post-Quantum Cryptography Support in Cryptographic Libraries](https://arxiv.org/abs/2508.16078)
*Nadeem Ahmed,Lei Zhang,Aryya Gangopadhyay*

Main category: cs.CR

TL;DR: 该研究评估了九个开源加密库对NIST选定的后量子密码（PQC）算法的支持情况，揭示了各库在应对量子计算威胁方面的不同准备状态。


<details>
  <summary>Details</summary>
Motivation: 量子计算的快速发展对现代加密系统构成重大威胁，亟需向后量子密码过渡。

Method: 研究基于最新文档和行业报告，分析了九个开源加密库（如OpenSSL、Bouncy Castle等）对NIST PQC候选算法的支持情况。

Result: 部分库已集成PQC支持或有明确路线图，而另一些则落后，带来潜在安全风险。

Conclusion: 需加紧研究、标准化和协同推进后量子密码的采用，以确保安全过渡。

Abstract: The rapid advancement of quantum computing poses a significant threat to
modern cryptographic systems, necessitating the transition to Post-Quantum
Cryptography (PQC). This study evaluates the support for PQC algorithms within
nine widely used open-source cryptographic libraries -- OpenSSL, wolfSSL,
BoringSSL, LibreSSL, Bouncy Castle, libsodium, Crypto++, Botan, and MbedTLS --
focusing on their implementation of the NIST-selected PQC finalists:
CRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, and SPHINCS+. Our analysis, based
on the latest available documentation, release notes, and industry reports as
of early 2025, reveals a varied state of readiness across these libraries.
While some libraries have integrated PQC support or have clear implementation
roadmaps, others lag behind, creating potential security risks as quantum
threats become more imminent. We discuss key challenges, including performance
trade-offs, implementation security, and adoption hurdles in real-world
cryptographic applications. Our findings highlight the urgent need for
continued research, standardization efforts, and coordinated adoption
strategies to ensure a secure transition to the quantum-resistant cryptographic
landscape.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [68] [Abmax: A JAX-based Agent-based Modeling Framework](https://arxiv.org/abs/2508.16508)
*Siddharth Chaturvedi,Ahmed El-Gazzar,Marcel van Gerven*

Main category: cs.MA

TL;DR: Abmax是基于JAX的代理建模框架，通过JIT编译和向量化技术，实现高效且灵活的代理操作，适用于大规模复杂系统的模拟。


<details>
  <summary>Details</summary>
Motivation: 解决使用JAX进行代理建模时，由于数组形状不可变导致的代理操作灵活性受限问题。

Method: 开发Abmax框架，提供多种JIT编译算法，支持动态代理更新和并行运行多个模型。

Result: 在经典捕食模型上表现与最优实现相当，并能高效运行交通流和金融市场模型。

Conclusion: Abmax为代理建模提供了兼具性能和灵活性的解决方案。

Abstract: Agent-based modeling (ABM) is a principal approach for studying complex
systems. By decomposing a system into simpler, interacting agents, agent-based
modeling (ABM) allows researchers to observe the emergence of complex
phenomena. High-performance array computing libraries like JAX can help scale
such computational models to a large number of agents by using automatic
vectorization and just-in-time (JIT) compilation. One of the caveats of using
JAX to achieve such scaling is that the shapes of arrays used in the
computational model should remain immutable throughout the simulation. In the
context of agent-based modeling (ABM), this can pose constraints on certain
agent manipulation operations that require flexible data structures. A subset
of which is represented by the ability to update a dynamically selected number
of agents by applying distinct changes to them during a simulation. To this
effect, we introduce Abmax, an ABM framework based on JAX that implements
multiple just-in-time (JIT) compilable algorithms to provide this
functionality. On the canonical predation model benchmark, Abmax achieves
runtime performance comparable to state-of-the-art implementations. Further, we
show that this functionality can also be vectorized, making it possible to run
many similar agent-based models in parallel. We also present two examples in
the form of a traffic-flow model and a financial market model to show the use
case of Abmax.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [69] [CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars](https://arxiv.org/abs/2508.16030)
*Jinyue Song,Hansol Ku,Jayneel Vora,Nelson Lee,Ahmad Kamari,Prasant Mohapatra,Parth Pathak*

Main category: cs.CV

TL;DR: 本文提出了一个名为CoVeRaP的多车协同数据集和一个统一的协同感知框架，通过融合雷达、摄像头和GPS数据，显著提高了3D物体检测的精度。


<details>
  <summary>Details</summary>
Motivation: 汽车FMCW雷达在恶劣天气下表现可靠，但其稀疏且嘈杂的点云数据限制了3D物体检测效果。因此，需要一个多车协同的数据集和框架来提升检测性能。

Method: 作者发布了CoVeRaP数据集，并提出了一种基于中间和后期融合选项的统一协同感知框架。基准网络采用多分支PointNet风格编码器，结合自注意力机制，融合空间、多普勒和强度信息到一个共同的潜在空间，解码器将其转换为3D边界框和每点深度置信度。

Result: 实验表明，结合强度编码的中间融合方法将平均精度提高了9倍（IoU 0.9），且持续优于单车基线。

Conclusion: CoVeRaP首次为多车FMCW雷达感知建立了可复现的基准，并证明经济实惠的雷达共享显著提升了检测鲁棒性。数据集和代码已公开以促进进一步研究。

Abstract: Automotive FMCW radars remain reliable in rain and glare, yet their sparse,
noisy point clouds constrain 3-D object detection. We therefore release
CoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and
GPS streams from multiple vehicles across diverse manoeuvres. Built on this
data, we propose a unified cooperative-perception framework with middle- and
late-fusion options. Its baseline network employs a multi-branch PointNet-style
encoder enhanced with self-attention to fuse spatial, Doppler, and intensity
cues into a common latent space, which a decoder converts into 3-D bounding
boxes and per-point depth confidence. Experiments show that middle fusion with
intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and
consistently outperforms single-vehicle baselines. CoVeRaP thus establishes the
first reproducible benchmark for multi-vehicle FMCW-radar perception and
demonstrates that affordable radar sharing markedly improves detection
robustness. Dataset and code are publicly available to encourage further
research.

</details>


### [70] [HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images](https://arxiv.org/abs/2508.16465)
*Anilkumar Swamy,Vincent Leroy,Philippe Weinzaepfel,Jean-Sébastien Franco,Grégory Rogez*

Main category: cs.CV

TL;DR: 提出了一种无需关键点检测的鲁棒方法HOSt3R，用于从单目视频中估计手-物体的3D变换，并结合多视角重建恢复形状，避免了现有方法对多样物体几何和遮挡的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖关键点检测，难以应对复杂物体几何和遮挡，限制了通用性和扩展性。

Method: 提出无需关键点检测的方法，结合多视角重建，无需预扫描模板或相机内参。

Result: 在SHOWMe基准测试中达到SOTA性能，并在HO3D数据集上展示了对未见物体类别的泛化能力。

Conclusion: HOSt3R是一种通用性强、非侵入性的方法，适用于手-物体3D重建任务。

Abstract: Hand-object 3D reconstruction has become increasingly important for
applications in human-robot interaction and immersive AR/VR experiences. A
common approach for object-agnostic hand-object reconstruction from RGB
sequences involves a two-stage pipeline: hand-object 3D tracking followed by
multi-view 3D reconstruction. However, existing methods rely on keypoint
detection techniques, such as Structure from Motion (SfM) and hand-keypoint
optimization, which struggle with diverse object geometries, weak textures, and
mutual hand-object occlusions, limiting scalability and generalization. As a
key enabler to generic and seamless, non-intrusive applicability, we propose in
this work a robust, keypoint detector-free approach to estimating hand-object
3D transformations from monocular motion video/images. We further integrate
this with a multi-view reconstruction pipeline to accurately recover
hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely
on pre-scanned object templates or camera intrinsics, and reaches
state-of-the-art performance for the tasks of object-agnostic hand-object 3D
transformation and shape estimation on the SHOWMe benchmark. We also experiment
on sequences from the HO3D dataset, demonstrating generalization to unseen
object categories.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [71] [Automata Learning -- Expect Delays!](https://arxiv.org/abs/2508.16384)
*Gabriel Dengler,Sven Apel,Holger Hermanns*

Main category: cs.FL

TL;DR: 本文研究了在存在随机延迟的情况下主动自动机学习（AAL）的问题，提出了一种分离学习和延迟采样的方法，以高效估计Mealy机器，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在随机延迟的环境中，传统AAL方法（如$L^*$）因在状态空间根部过度采样而效率低下，需要更高效的延迟采样策略。

Method: 将行为学习和延迟采样概念上分离，利用逻辑行为学习的信息设计高效的输入序列以收集延迟样本，并处理相同输入/输出行为可能由不同延迟特性引起的情况。

Result: 实验表明，该方法在多种基准测试中优于传统方法，并在关系数据库的连接顺序研究中展现出实用潜力。

Conclusion: 分离行为学习和延迟采样的策略显著提高了AAL在随机延迟环境中的效率，适用于实际应用场景。

Abstract: This paper studies active automata learning (AAL) in the presence of
stochastic delays. We consider Mealy machines that have stochastic delays
associated with each transition and explore how the learner can efficiently
arrive at faithful estimates of those machines, the precision of which
crucially relies on repetitive sampling of transition delays. While it is
possible to na\"ively integrate the delay sampling into AAL algorithms such as
$L^*$, this leads to considerable oversampling near the root of the state
space. We address this problem by separating conceptually the learning of
behavior and delays such that the learner uses the information gained while
learning the logical behavior to arrive at efficient input sequences for
collecting the needed delay samples. We put emphasis on treating cases in which
identical input/output behaviors might stem from distinct delay
characteristics. Finally, we provide empirical evidence that our method
outperforms the na\"ive baseline across a wide range of benchmarks and
investigate its applicability in a realistic setting by studying the join order
in a relational database.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [72] [Hierarchical Vision-Language Reasoning for Multimodal Multiple-Choice Question Answering](https://arxiv.org/abs/2508.16148)
*Ao Zhou,Zebo Gu,Tenghao Sun,Jiawen Chen,Mingsheng Tu,Zifeng Cheng,Yafeng Yin,Zhiwei Jiang,Qing Gu*

Main category: cs.IR

TL;DR: 提出了一种结合多模态分层推理和优化的日本PDF文档理解框架，显著提升了复杂文档的语义解析能力和实际应用中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有MLLM在处理复杂布局和长篇PDF文档时的局限性，尤其是针对日语等非英语语言的性能不足问题。

Method: 结合多模态分层推理机制、Colqwen优化的检索方法，创新性地引入通过子问题分解的语义验证策略。

Result: 实验结果表明框架显著提升了模型的深度语义解析能力，并在实际应用场景中表现出更强的鲁棒性。

Conclusion: 新框架有效解决了复杂PDF文档的理解问题，尤其适用于日语等非英语语言场景。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
multimodal understanding capabilities in Visual Question Answering (VQA) tasks
by integrating visual and textual features. However, under the challenging
ten-choice question evaluation paradigm, existing methods still exhibit
significant limitations when processing PDF documents with complex layouts and
lengthy content. Notably, current mainstream models suffer from a strong bias
toward English training data, resulting in suboptimal performance for Japanese
and other language scenarios. To address these challenges, this paper proposes
a novel Japanese PDF document understanding framework that combines multimodal
hierarchical reasoning mechanisms with Colqwen-optimized retrieval methods,
while innovatively introducing a semantic verification strategy through
sub-question decomposition. Experimental results demonstrate that our framework
not only significantly enhances the model's deep semantic parsing capability
for complex documents, but also exhibits superior robustness in practical
application scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [Physically Plausible Data Augmentations for Wearable IMU-based Human Activity Recognition Using Physics Simulation](https://arxiv.org/abs/2508.13284)
*Nobuyuki Oishi,Philip Birch,Daniel Roggen,Paula Lago*

Main category: cs.LG

TL;DR: 针对传感器基人体活动识别中高质量标注数据稀缺问题，研究提出物理合理性数据增强（PPDA）方法，通过物理模拟生成多样化训练数据，显著提升模型性能并减少对初始数据收集的需求。


<details>
  <summary>Details</summary>
Motivation: 传感器基人体活动识别（HAR）因高质量标注数据稀缺而受限，传统信号变换数据增强（STDA）方法存在物理不合理性，可能破坏活动标签的原意。

Method: 提出PPDA方法，利用物理模拟整合人体运动数据和多种实际变异性（如身体动作、传感器位置和硬件效应），并与传统STDA进行对比实验。

Result: 实验表明，PPDA平均提升宏F1分数3.7 pp（最高13 pp），且使用60%更少训练样本即可达到与STDA相当的性能。

Conclusion: PPDA通过物理合理性显著提升数据增强效果，为HAR中的标注数据稀缺问题提供了一种成本效益高且可扩展的解决方案。

Abstract: The scarcity of high-quality labeled data in sensor-based Human Activity
Recognition (HAR) hinders model performance and limits generalization across
real-world scenarios. Data augmentation is a key strategy to mitigate this
issue by enhancing the diversity of training datasets. Signal
Transformation-based Data Augmentation (STDA) techniques have been widely used
in HAR. However, these methods are often physically implausible, potentially
resulting in augmented data that fails to preserve the original meaning of the
activity labels. In this study, we introduce and systematically characterize
Physically Plausible Data Augmentation (PPDA) enabled by physics simulation.
PPDA leverages human body movement data from motion capture or video-based pose
estimation and incorporates various realistic variabilities through physics
simulation, including modifying body movements, sensor placements, and
hardware-related effects. We compare the performance of PPDAs with traditional
STDAs on three public datasets of daily activities and fitness workouts. First,
we evaluate each augmentation method individually, directly comparing PPDAs to
their STDA counterparts. Next, we assess how combining multiple PPDAs can
reduce the need for initial data collection by varying the number of subjects
used for training. Experiments show consistent benefits of PPDAs, improving
macro F1 scores by an average of 3.7 pp (up to 13 pp) and achieving competitive
performance with up to 60% fewer training subjects than STDAs. As the first
systematic study of PPDA in sensor-based HAR, these results highlight the
advantages of pursuing physical plausibility in data augmentation and the
potential of physics simulation for generating synthetic Inertial Measurement
Unit data for training deep learning HAR models. This cost-effective and
scalable approach therefore helps address the annotation scarcity challenge in
HAR.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [74] [The next question after Turing's question: Introducing the Grow-AI test](https://arxiv.org/abs/2508.16277)
*Alexandru Tugui*

Main category: cs.AI

TL;DR: 本研究扩展了GROW-AI框架，旨在通过六项标准和四类游戏评估AI的“成长”水平，结果展示了一种通用的评估方法。


<details>
  <summary>Details</summary>
Motivation: 提出GROW-AI框架，以回答“机器能否成长”的问题，并超越图灵测试。

Method: 使用六项标准（C1-C6）和四类游戏进行评估，记录在AI日志中，通过专家方法计算成长指数。

Result: 方法能统一评估各类AI的“成长”水平，并揭示其优劣势，日志保证了评估的可追溯性。

Conclusion: GROW-AI通过多学科整合，不仅测量性能，还能捕捉AI的成熟路径。

Abstract: This study aims to extend the framework for assessing artificial
intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),
designed to answer the question "Can machines grow up?" -- a natural successor
to the Turing Test. The methodology applied is based on a system of six primary
criteria (C1-C6), each assessed through a specific "game", divided into four
arenas that explore both the human dimension and its transposition into AI. All
decisions and actions of the entity are recorded in a standardized AI Journal,
the primary source for calculating composite scores. The assessment uses the
prior expert method to establish initial weights, and the global score -- Grow
Up Index -- is calculated as the arithmetic mean of the six scores, with
interpretation on maturity thresholds. The results show that the methodology
allows for a coherent and comparable assessment of the level of "growth" of AI
entities, regardless of their type (robots, software agents, LLMs). The
multi-game structure highlights strengths and vulnerable areas, and the use of
a unified journal guarantees traceability and replicability in the evaluation.
The originality of the work lies in the conceptual transposition of the process
of "growing" from the human world to that of artificial intelligence, in an
integrated testing format that combines perspectives from psychology, robotics,
computer science, and ethics. Through this approach, GROW-AI not only measures
performance but also captures the evolutionary path of an AI entity towards
maturity.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [75] [EEG Study of the Influence of Imagined Temperature Sensations on Neuronal Activity in the Sensorimotor Cortex](https://arxiv.org/abs/2508.16274)
*Anton Belichenko,Daria Trinitatova,Aigul Nasibullina,Lev Yakovlev,Dzmitry Tsetserukou*

Main category: q-bio.NC

TL;DR: 研究探讨了想象温度感觉对神经活动的影响，发现其激活感觉运动皮层的机制与实际温度感知相似，对脑机接口和神经康复技术有潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 理解感觉想象的神经关联对认知神经科学和脑机接口技术的发展至关重要。

Method: 通过EEG记录真实热刺激（40°C和20°C）与想象温度感觉时的神经活动，分析感觉运动皮层mu节律的事件相关去同步化（ERD）。

Result: 想象温度感觉和实际热刺激都能诱发中央头皮区域的mu-ERD，差异不显著（p>.05），但均显著不同于基线（p<.001）。

Conclusion: 研究证实想象温度感觉可以类似实际感知激活感觉运动皮层，为脑机接口和神经康复技术提供了新的可能性。

Abstract: Understanding the neural correlates of sensory imagery is crucial for
advancing cognitive neuroscience and developing novel Brain-Computer Interface
(BCI) paradigms. This study investigated the influence of imagined temperature
sensations (ITS) on neural activity within the sensorimotor cortex. The
experimental study involved the evaluation of neural activity using
electroencephalography (EEG) during both real thermal stimulation (TS:
40{\deg}C Hot, 20{\deg}C Cold) applied to the participants' hand, and the
mental temperature imagination (ITS) of the corresponding hot and cold
sensations. The analysis focused on quantifying the event-related
desynchronization (ERD) of the sensorimotor mu-rhythm (8-13 Hz). The
experimental results revealed a characteristic mu-ERD localized over central
scalp regions (e.g., C3) during both TS and ITS conditions. Although the
magnitude of mu-ERD during ITS was slightly lower than during TS, this
difference was not statistically significant (p>.05). However, ERD during both
ITS and TS was statistically significantly different from the resting baseline
(p<.001). These findings demonstrate that imagining temperature sensations
engages sensorimotor cortical mechanisms in a manner comparable to actual
thermal perception. This insight expands our understanding of the
neurophysiological basis of sensory imagery and suggests the potential utility
of ITS for non-motor BCI control and neurorehabilitation technologies.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [76] [Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration](https://arxiv.org/abs/2508.15809)
*Songyuan Sui,Hongyi Liu,Serena Liu,Li Li,Soo-Hyun Choi,Rui Chen,Xia Hu*

Main category: cs.CL

TL;DR: CoQ是一种新型多代理框架，通过自然语言式表格表示和分句SQL生成策略，显著提高了表理解的准确性和SQL有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在表格结构理解、错误传播和过度依赖执行正确性方面的局限性。

Method: 采用自然语言式表格表示，分句生成SQL，并分离SQL机械推理与LLM逻辑推理。

Result: 在五个基准测试中，准确率从61.11%提升至74.77%，无效SQL率从9.48%降至3.34%。

Conclusion: CoQ在表理解任务中表现出色，代码已开源。

Abstract: Table understanding requires structured, multi-step reasoning. Large Language
Models (LLMs) struggle with it due to the structural complexity of tabular
data. Recently, multi-agent frameworks for SQL generation have shown promise in
tackling the challenges of understanding tabular data, but existing approaches
often suffer from limitations such as the inability to comprehend table
structure for reliable SQL generation, error propagation that results in
invalid queries, and over-reliance on execution correctness. To address these
issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for
SQL-aided table understanding. CoQ adopts natural-language-style
representations of table schemas to abstract away structural noise and enhance
understanding. It employs a clause-by-clause SQL generation strategy to improve
query quality and introduces a hybrid reasoning division that separates
SQL-based mechanical reasoning from LLM-based logical inference, thereby
reducing reliance on execution outcomes. Experiments with four models (both
closed- and open-source) across five widely used benchmarks show that
Chain-of-Query significantly improves accuracy from 61.11% to 74.77% and
reduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior
effectiveness in table understanding. The code is available at
https://github.com/SongyuanSui/ChainofQuery.

</details>


### [77] [LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions](https://arxiv.org/abs/2508.15801)
*Seyedali Mohammadi,Manas Paldhe,Amit Chhabra*

Main category: cs.CL

TL;DR: 论文介绍了LingVarBench，一种通过合成数据生成和自动化验证解决电话通话转录标注高成本问题的管道，显著提升了结构化信息提取的准确性。


<details>
  <summary>Details</summary>
Motivation: 电话通话转录标注因隐私法规、许可要求和手动标注的高成本（每分钟约2美元）而昂贵，现有方法在包含不流畅、打断和说话者重叠的对话语音上表现不佳。

Method: 利用LLM生成合成数据，包括结构化字段值和自然对话语句，并通过LLM验证器确保合成数据的有效性。使用DSPy的SIMBA优化器自动化合成提取提示，消除手动提示工程。

Result: 优化提示在真实客户通话转录中取得显著提升：数字字段准确率95%（零样本88-89%），姓名90%（零样本47-79%），日期80%（零样本72-77%）。

Conclusion: LingVarBench通过合成数据学习对话模式，有效泛化至真实电话通话，为商业环境中大规模通话分析提供了成本效益高的解决方案。

Abstract: Phone call transcript labeling is prohibitively expensive (approximately 2
USD per minute) due to privacy regulations, consent requirements, and manual
annotation costs requiring 3 hours of expert time per hour of audio. Existing
extraction methods fail on conversational speech containing disfluencies,
interruptions, and speaker overlap. We introduce LingVarBench, a synthetic data
generation pipeline that addresses these constraints through automated
validation. First, we prompt an LLM to generate realistic structured field
values across multiple use cases. Second, we recursively prompt the model to
transform these values into thousands of natural conversational utterances
containing typical phone call characteristics. Third, we validate each
synthetic utterance by testing whether a separate LLM-based extractor can
recover the original structured information. We employ DSPy's SIMBA optimizer
to automatically synthesize extraction prompts from validated synthetic
transcripts, eliminating manual prompt engineering. Our optimized prompts
achieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent
zero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for
dates (vs. 72-77 percent) on real customer transcripts, demonstrating
substantial gains over zero-shot prompting. The synthetic-to-real transfer
demonstrates that conversational patterns learned from generated data
generalize effectively to authentic phone calls containing background noise and
domain-specific terminology. LingVarBench provides the first systematic
benchmark for structured extraction from synthetic conversational data,
demonstrating that automated prompt optimization overcomes cost and privacy
barriers preventing large-scale phone call analysis in commercial settings.

</details>


### [78] [Embarrassed to observe: The effects of directive language in brand conversation](https://arxiv.org/abs/2508.15826)
*Andria Andriuzzi,Géraldine Michel*

Main category: cs.CL

TL;DR: 研究探讨了社交媒体中品牌使用指令性语言对观察消费者的负面影响，发现这种语言会引发间接尴尬并降低参与度，尤其在非产品中心对话中效果更显著，但品牌关系强度可以缓解这种影响。


<details>
  <summary>Details</summary>
Motivation: 虽然广告中的指令性语言效果不一，但社交媒体中品牌使用指令性语言对观察消费者的影响尚未被充分研究。

Method: 通过一项实地研究和三个在线实验，结合Goffman的面子理论分析消费者反应。

Result: 指令性语言会降低观察消费者的参与度，非产品中心对话中负面影响更强，品牌关系强度可缓解此效应。

Conclusion: 研究强调了互动沟通中情境的重要性，对社交媒体和品牌管理具有直接启示。

Abstract: In social media, marketers attempt to influence consumers by using directive
language, that is, expressions designed to get consumers to take action. While
the literature has shown that directive messages in advertising have mixed
results for recipients, we know little about the effects of directive brand
language on consumers who see brands interacting with other consumers in social
media conversations. On the basis of a field study and three online
experiments, this study shows that directive language in brand conversation has
a detrimental downstream effect on engagement of consumers who observe such
exchanges. Specifically, in line with Goffman's facework theory, because a
brand that encourages consumers to react could be perceived as
face-threatening, consumers who see a brand interacting with others in a
directive way may feel vicarious embarrassment and engage less (compared with a
conversation without directive language). In addition, we find that when the
conversation is nonproduct-centered (vs. product-centered), consumers expect
more freedom, as in mundane conversations, even for others; therefore,
directive language has a stronger negative effect. However, in this context,
the strength of the brand relationship mitigates this effect. Thus, this study
contributes to the literature on directive language and brand-consumer
interactions by highlighting the importance of context in interactive
communication, with direct relevance for social media and brand management.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [79] [Prover-Adversary games for systems over (non-deterministic) branching programs](https://arxiv.org/abs/2508.16014)
*Anupam Das,Avgerinos Delkos*

Main category: cs.CC

TL;DR: 通过Pudlak-Buss风格的证明者-对手游戏，研究了确定性（BPs）和非确定性（NBPs）分支程序的证明系统，并证明了与eLDT和eLNDT系统的多项式等价性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过游戏理论方法，深入理解确定性及非确定性分支程序的证明系统及其能力。

Method: 引入了Pudlak-Buss风格的证明者-对手游戏，并与Buss等人提出的eLDT和eLNDT系统进行对比分析。

Result: 证明了两类游戏与eLDT、eLNDT系统间的多项式等价性，并扩展了Immerman-Szelepcsenyi定理的非均匀版本。

Conclusion: 通过技术发展，进一步验证了eLNDT与有限交替分支程序系统的多项式等价性。

Abstract: We introduce Pudlak-Buss style Prover-Adversary games to characterise proof
systems reasoning over deterministic branching programs (BPs) and
non-deterministic branching programs (NBPs). Our starting points are the proof
systems eLDT and eLNDT, for BPs and NBPs respectively, previously introduced by
Buss, Das and Knop. We prove polynomial equivalences between these proof
systems and the corresponding games we introduce. This crucially requires
access to a form of negation of branching programs which, for NBPs, requires us
to formalise a non-uniform version of the Immerman-Szelepcsenyi theorem that
coNL = NL. Thanks to the techniques developed, we further obtain a proof
complexity theoretic version of Immerman-Szelepcsenyi, showing that eLNDT is
polynomially equivalent to systems over boundedly alternating branching
programs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [80] [Straggler-Resilient Federated Learning over A Hybrid Conventional and Pinching Antenna Network](https://arxiv.org/abs/2508.15821)
*Bibo Wu,Fang Fang,Ming Zeng,Xianbin Wang*

Main category: cs.IT

TL;DR: 论文提出了一种混合传统和捏缩天线的网络(HCPAN)，通过动态建立强视距链接来缓解联邦学习中的“拖后腿”问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)中常见的“拖后腿”问题影响了通信效率，论文旨在通过优化天线部署和资源分配来改善这一状况。

Method: 1. 提出了一种基于模糊逻辑的客户端分类方案。2. 通过深度强化学习(DRL)联合优化天线放置和资源分配。

Result: 仿真结果表明，该方案通过优化捏缩天线部署显著提升了FL性能。

Conclusion: HCPAN和DRL算法的结合有效解决了FL中的通信效率问题，为实际应用提供了可行方案。

Abstract: Leveraging pinching antennas in wireless network enabled federated learning
(FL) can effectively mitigate the common "straggler" issue in FL by dynamically
establishing strong line-of-sight (LoS) links on demand. This letter proposes a
hybrid conventional and pinching antenna network (HCPAN) to significantly
improve communication efficiency in the non-orthogonal multiple access
(NOMA)-enabled FL system. Within this framework, a fuzzy logic-based client
classification scheme is first proposed to effectively balance clients' data
contributions and communication conditions. Given this classification, we
formulate a total time minimization problem to jointly optimize pinching
antenna placement and resource allocation. Due to the complexity of variable
coupling and non-convexity, a deep reinforcement learning (DRL)-based algorithm
is developed to effectively address this problem. Simulation results validate
the superiority of the proposed scheme in enhancing FL performance via the
optimized deployment of pinching antenna.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [81] [PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark](https://arxiv.org/abs/2508.16439)
*Adil Bahaj,Mounir Ghogho*

Main category: cs.CY

TL;DR: 论文提出了一个多模态儿科问答基准PediatricsMQA，旨在解决医学语言和视觉模型中存在的年龄偏见问题。研究发现模型在儿科任务上表现较差，需要开发年龄感知方法以实现公平的AI支持。


<details>
  <summary>Details</summary>
Motivation: 医学语言和视觉模型存在系统性年龄偏见，尤其在儿科任务上表现不佳，这反映了医学研究中儿科领域的资金和代表性不足问题。

Method: 构建了一个包含3,417个文本多选题和2,067个视觉多选题的综合儿科数据集，采用混合手动-自动流程，整合了多个权威资源。

Result: 评估显示现有模型在年轻群体上性能显著下降，突显了年龄偏见的严重性。

Conclusion: 需要开发年龄感知方法，以确保AI在儿科护理中的公平性。

Abstract: Large language models (LLMs) and vision-augmented LLMs (VLMs) have
significantly advanced medical informatics, diagnostics, and decision support.
However, these models exhibit systematic biases, particularly age bias,
compromising their reliability and equity. This is evident in their poorer
performance on pediatric-focused text and visual question-answering tasks. This
bias reflects a broader imbalance in medical research, where pediatric studies
receive less funding and representation despite the significant disease burden
in children. To address these issues, a new comprehensive multi-modal pediatric
question-answering benchmark, PediatricsMQA, has been introduced. It consists
of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric
topics across seven developmental stages (prenatal to adolescent) and 2,067
vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256
anatomical regions. The dataset was developed using a hybrid manual-automatic
pipeline, incorporating peer-reviewed pediatric literature, validated question
banks, existing benchmarks, and existing QA resources. Evaluating
state-of-the-art open models, we find dramatic performance drops in younger
cohorts, highlighting the need for age-aware methods to ensure equitable AI
support in pediatric care.

</details>
