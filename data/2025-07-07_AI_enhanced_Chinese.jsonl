{"id": "2507.02068", "pdf": "https://arxiv.org/pdf/2507.02068", "abs": "https://arxiv.org/abs/2507.02068", "authors": ["Brian Bell", "Teresa Thomas", "Sang Won Lee", "Chris Brown"], "title": "How do Software Engineering Candidates Prepare for Technical Interviews?", "categories": ["cs.SE"], "comment": null, "summary": "To obtain employment, aspiring software engineers must complete technical\ninterviews -- a hiring process which involves candidates writing code while\ncommunicating to an audience. However, the complexities of tech interviews are\ndifficult to prepare for and seldom faced in computing curricula. To this end,\nwe seek to understand how candidates prepare for technical interviews,\ninvestigating the effects of preparation methods and the role of education. We\ndistributed a survey to candidates (n = 131) actively preparing for technical\ninterviews. Our results suggest candidates rarely train in authentic settings\nand courses fail to support preparation efforts -- leading to stress and\nunpreparedness. Based on our findings, we provide implications for stakeholders\nto enhance tech interview preparation for candidates pursuing software\nengineering roles.", "AI": {"tldr": "\u603b\u7ed3\uff1a\u7814\u7a76\u63a2\u8ba8\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5019\u9009\u4eba\u5982\u4f55\u51c6\u5907\u6280\u672f\u9762\u8bd5\uff0c\u53d1\u73b0\u7f3a\u4e4f\u771f\u5b9e\u8bad\u7ec3\u548c\u8bfe\u7a0b\u652f\u6301\u5bfc\u81f4\u538b\u529b\u548c\u51c6\u5907\u4e0d\u8db3\u3002", "motivation": "\u52a8\u673a\uff1a\u6280\u672f\u9762\u8bd5\u7684\u590d\u6742\u6027\u96be\u4ee5\u51c6\u5907\u4e14\u8ba1\u7b97\u8bfe\u7a0b\u4e2d\u8f83\u5c11\u6d89\u53ca\uff0c\u56e0\u6b64\u9700\u8981\u4e86\u89e3\u5019\u9009\u4eba\u5982\u4f55\u51c6\u5907\u4ee5\u53ca\u6559\u80b2\u7684\u4f5c\u7528\u3002", "method": "\u65b9\u6cd5\uff1a\u5bf9131\u540d\u6b63\u5728\u51c6\u5907\u6280\u672f\u9762\u8bd5\u7684\u5019\u9009\u4eba\u8fdb\u884c\u4e86\u8c03\u67e5\u3002", "result": "\u7ed3\u679c\uff1a\u5019\u9009\u4eba\u5f88\u5c11\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u8bfe\u7a0b\u4e5f\u672a\u80fd\u652f\u6301\u51c6\u5907\uff0c\u5bfc\u81f4\u538b\u529b\u548c\u51c6\u5907\u4e0d\u8db3\u3002", "conclusion": "\u7ed3\u8bba\uff1a\u7814\u7a76\u4e3a\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u4e86\u589e\u5f3a\u6280\u672f\u9762\u8bd5\u51c6\u5907\u7684\u5efa\u8bae\u3002"}}
{"id": "2507.02107", "pdf": "https://arxiv.org/pdf/2507.02107", "abs": "https://arxiv.org/abs/2507.02107", "authors": ["Ben Limpanukorn", "Yanjun Wang", "Zach Patterson", "Pranav Garg", "Murali Krishna Ramanathan", "Xiaofei Ma", "Anoop Deoras", "Miryung Kim"], "title": "Structural Code Search using Natural Language Queries", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Searching code is a common task that developers perform to understand APIs,\nlearn common code patterns, and navigate code. Currently, developers most\ncommonly search using keywords and regular expressions that are easy to use and\nwidely available. Beyond keywords and regular expressions, structural code\nsearch tools allow developers to search for code based on its syntactic\nstructure. This has numerous applications ranging from bug finding to\nsystematically refactoring code. However, these structural code search tools\noperate on queries expressed in domain-specific languages (DSL) that can be\ndifficult to learn and write. We propose to allow developers to use natural\nlanguage to search for code structurally. Expressing queries in natural\nlanguage provides an intuitive way to search for code and lowers the barrier to\nentry.\n  In this work, we develop a novel general approach that combines the reasoning\ncapabilities of an LLM to interpret natural language search queries with the\npower of structural search tools to efficiently and accurately retrieve\nrelevant code. We then instantiate this approach for two structural code search\nDSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for\nstructural code search consisting of 400 queries over 10 Java projects. We show\nthat our approach for structural code search based on translating NL queries to\nDSL queries using an LLM is effective and robust, achieving a high precision\nand recall ranging from 55% - 70%. Further, our approach significantly\noutperforms baselines based on semantic code search and LLM retrievals by up to\n57% and 14% on F1 scores.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u7ed3\u6784\u5316\u4ee3\u7801\u641c\u7d22\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u548c\u73b0\u6709\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u641c\u7d22\u6548\u679c\u3002", "motivation": "\u5f00\u53d1\u8005\u5e38\u7528\u5173\u952e\u8bcd\u548c\u6b63\u5219\u8868\u8fbe\u5f0f\u641c\u7d22\u4ee3\u7801\uff0c\u4f46\u7ed3\u6784\u5316\u641c\u7d22\u5de5\u5177\u4f7f\u7528\u95e8\u69db\u9ad8\u3002", "method": "\u7ed3\u5408LLM\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e0e\u7ed3\u6784\u5316\u641c\u7d22\u5de5\u5177\uff0c\u5b9e\u4f8b\u5316\u4e8eSemgrep\u548cGQL\u3002", "result": "\u5728400\u4e2a\u67e5\u8be2\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u8fbe55%-70%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u7ed3\u6784\u5316\u4ee3\u7801\u641c\u7d22\u65b9\u6cd5\u6709\u6548\u4e14\u9c81\u68d2\uff0c\u663e\u8457\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\u5e76\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.02110", "pdf": "https://arxiv.org/pdf/2507.02110", "abs": "https://arxiv.org/abs/2507.02110", "authors": ["Md Nahidul Islam Opu", "Fatima Islam Mouri", "Rick Kazman", "Yuanfang Cai", "Shaiful Chowdhury"], "title": "Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!", "categories": ["cs.SE"], "comment": null, "summary": "Predicting mobile app popularity before release can provide developers with a\nstrategic advantage in a competitive marketplace, yet it remains a challenging\nproblem. This study explores whether internal software metrics, measurable from\nsource code before deployment, can predict an app's popularity, defined by user\nratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).\nUsing a dataset of 446 open-source Android apps from F-Droid, we extract a wide\narray of features, including system-, class-, and method-level code metrics,\ncode smells, and app metadata. Additional information, such as user reviews,\ndownload counts, and uses-permission, was collected from the Google Play Store.\nWe evaluate regression and classification models across three feature sets: a\nminimal Size-only baseline, a domain-informed Handpicked set, and a Voting set\nderived via feature selection algorithms. Regression models perform poorly due\nto skewed data, with low $R^2$ scores. However, when reframed as binary\nclassification (Popular vs. Unpopular), results improve significantly. The best\nmodel, a Multilayer Perceptron using the Voting set, achieves F1-scores of\n0.72. These results suggest that internal code metrics, although limited in\ntheir explanatory power, can serve as useful indicators of app popularity. This\nchallenges earlier findings that dismissed internal metrics as predictors of\nsoftware quality.", "AI": {"tldr": "\u9884\u6d4b\u79fb\u52a8\u5e94\u7528\u53d1\u5e03\u524d\u7684\u53d7\u6b22\u8fce\u7a0b\u5ea6\u5bf9\u5f00\u53d1\u8005\u5728\u7ade\u4e89\u6fc0\u70c8\u7684\u5e02\u573a\u4e2d\u5177\u6709\u6218\u7565\u610f\u4e49\uff0c\u4f46\u4ecd\u662f\u6311\u6218\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u5f00\u6e90Android\u5e94\u7528\u7684\u4ee3\u7801\u6307\u6807\uff0c\u53d1\u73b0\u5185\u90e8\u4ee3\u7801\u6307\u6807\u53ef\u4f5c\u4e3a\u53d7\u6b22\u8fce\u7a0b\u5ea6\u7684\u6307\u793a\u5668\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u5e02\u573a\u7684\u7ade\u4e89\u6fc0\u70c8\uff0c\u9884\u6d4b\u5e94\u7528\u53d1\u5e03\u524d\u7684\u53d7\u6b22\u8fce\u7a0b\u5ea6\u6709\u52a9\u4e8e\u5f00\u53d1\u8005\u4f18\u5316\u7b56\u7565\uff0c\u4f46\u76ee\u524d\u5c1a\u65e0\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528446\u4e2a\u5f00\u6e90Android\u5e94\u7528\u7684\u4ee3\u7801\u6307\u6807\u3001\u7528\u6237\u8bc4\u5206\u548c\u4e0b\u8f7d\u6570\u636e\uff0c\u901a\u8fc7\u56de\u5f52\u548c\u5206\u7c7b\u6a21\u578b\uff08\u5982\u591a\u5c42\u611f\u77e5\u673a\uff09\u8fdb\u884c\u5206\u6790\uff0c\u6bd4\u8f83\u4e0d\u540c\u7279\u5f81\u96c6\u7684\u6548\u679c\u3002", "result": "\u56de\u5f52\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff08R\u00b2\u4f4e\uff09\uff0c\u4f46\u5206\u7c7b\u6a21\u578b\uff08\u5c24\u5176\u662f\u591a\u5c42\u611f\u77e5\u673a\uff09\u8868\u73b0\u8f83\u597d\uff08F1=0.72\uff09\uff0c\u8868\u660e\u5185\u90e8\u4ee3\u7801\u6307\u6807\u53ef\u4f5c\u4e3a\u53d7\u6b22\u8fce\u7a0b\u5ea6\u7684\u6307\u793a\u5668\u3002", "conclusion": "\u5185\u90e8\u4ee3\u7801\u6307\u6807\u867d\u89e3\u91ca\u529b\u6709\u9650\uff0c\u4f46\u53ef\u4f5c\u4e3a\u9884\u6d4b\u5e94\u7528\u53d7\u6b22\u8fce\u7a0b\u5ea6\u7684\u6709\u6548\u5de5\u5177\uff0c\u6311\u6218\u4e86\u6b64\u524d\u5426\u5b9a\u5176\u9884\u6d4b\u4ef7\u503c\u7684\u7ed3\u8bba\u3002"}}
{"id": "2507.02118", "pdf": "https://arxiv.org/pdf/2507.02118", "abs": "https://arxiv.org/abs/2507.02118", "authors": ["Cristina Martinez Montes", "Daniela Grassi", "Nicole Novielli", "Birgit Penzenstadle"], "title": "A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights", "categories": ["cs.SE"], "comment": null, "summary": "The study of well-being, stress and other human factors has traditionally\nrelied on self-report instruments to assess key variables. However, concerns\nabout potential biases in these instruments, even when thoroughly validated and\nstandardised, have driven growing interest in alternatives in combining these\nmeasures with more objective methods, such as physiological measures.\n  We aimed to (i) compare psychometric stress measures and biometric indicators\nand (ii) identify stress-related patterns in biometric data during software\nengineering tasks.\n  We conducted an experiment where participants completed a pre-survey, then\nprogrammed two tasks wearing biometric sensors, answered brief post-surveys for\neach, and finally went through a short exit interview.\n  Our results showed diverse outcomes; we found no stress in the psychometric\ninstruments. Participants in the interviews reported a mix of feeling no stress\nand experiencing time pressure. Finally, the biometrics showed a significant\ndifference only in EDA phasic peaks.\n  We conclude that our chosen way of inducing stress by imposing a stricter\ntime limit was insufficient. We offer methodological insights for future\nstudies working with stress, biometrics, and psychometric instruments.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5fc3\u7406\u6d4b\u91cf\u538b\u529b\u6307\u6807\u4e0e\u751f\u7269\u7279\u5f81\u6307\u6807\uff0c\u5e76\u8bd5\u56fe\u901a\u8fc7\u7f16\u7a0b\u4efb\u52a1\u8bc6\u522b\u751f\u7269\u6570\u636e\u4e2d\u7684\u538b\u529b\u6a21\u5f0f\uff0c\u4f46\u672a\u663e\u8457\u8bf1\u5bfc\u51fa\u538b\u529b\u3002", "motivation": "\u4f20\u7edf\u81ea\u6211\u62a5\u544a\u5de5\u5177\u53ef\u80fd\u5b58\u5728\u504f\u5dee\uff0c\u7814\u7a76\u63a2\u7d22\u7ed3\u5408\u751f\u7269\u7279\u5f81\u4e0e\u5fc3\u7406\u6d4b\u91cf\u65b9\u6cd5\u4ee5\u66f4\u5ba2\u89c2\u8bc4\u4f30\u538b\u529b\u3002", "method": "\u5b9e\u9a8c\u8bbe\u8ba1\u5305\u542b\u9884\u8c03\u67e5\u3001\u7a7f\u6234\u751f\u7269\u4f20\u611f\u5668\u7f16\u7a0b\u4efb\u52a1\u3001\u540e\u8c03\u67e5\u53ca\u7b80\u77ed\u9000\u51fa\u8bbf\u8c08\u3002", "result": "\u5fc3\u7406\u6d4b\u91cf\u672a\u663e\u793a\u538b\u529b\uff1b\u8bbf\u8c08\u53cd\u9988\u6df7\u5408\u611f\u53d7\uff1b\u751f\u7269\u6570\u636e\u4e2d\u4ec5EDA\u9636\u6bb5\u6027\u5cf0\u503c\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u65f6\u95f4\u9650\u5236\u8bf1\u5bfc\u538b\u529b\u4e0d\u8db3\uff0c\u63d0\u4f9b\u672a\u6765\u7814\u7a76\u65b9\u6cd5\u53c2\u8003\u3002"}}
{"id": "2507.02080", "pdf": "https://arxiv.org/pdf/2507.02080", "abs": "https://arxiv.org/abs/2507.02080", "authors": ["Yubeen Lee", "Sangeun Lee", "Chaewon Park", "Junyeop Cha", "Eunil Park"], "title": "TAGF: Time-aware Gated Fusion for Multimodal Valence-Arousal Estimation", "categories": ["cs.MM", "cs.SD"], "comment": "9 pages, 2 figures, 2 tables", "summary": "Multimodal emotion recognition often suffers from performance degradation in\nvalence-arousal estimation due to noise and misalignment between audio and\nvisual modalities. To address this challenge, we introduce TAGF, a Time-aware\nGated Fusion framework for multimodal emotion recognition. The TAGF adaptively\nmodulates the contribution of recursive attention outputs based on temporal\ndynamics. Specifically, the TAGF incorporates a BiLSTM-based temporal gating\nmechanism to learn the relative importance of each recursive step and\neffectively integrates multistep cross-modal features. By embedding temporal\nawareness into the recursive fusion process, the TAGF effectively captures the\nsequential evolution of emotional expressions and the complex interplay between\nmodalities. Experimental results on the Aff-Wild2 dataset demonstrate that TAGF\nachieves competitive performance compared with existing recursive\nattention-based models. Furthermore, TAGF exhibits strong robustness to\ncross-modal misalignment and reliably models dynamic emotional transitions in\nreal-world conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u611f\u77e5\u95e8\u63a7\u878d\u5408\u6846\u67b6\uff08TAGF\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7531\u4e8e\u566a\u58f0\u548c\u6a21\u6001\u4e0d\u5bf9\u9f50\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u8282\u9012\u5f52\u6ce8\u610f\u529b\u8f93\u51fa\u6765\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\uff0c\u7531\u4e8e\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u7684\u566a\u58f0\u548c\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u6548\u4ef7-\u5524\u9192\u5ea6\u4f30\u8ba1\u4e0a\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f15\u5165TAGF\u6846\u67b6\uff0c\u5229\u7528BiLSTM\u7684\u65f6\u95f4\u95e8\u63a7\u673a\u5236\u52a8\u6001\u8c03\u8282\u9012\u5f52\u6ce8\u610f\u529b\u8f93\u51fa\u7684\u8d21\u732e\uff0c\u6709\u6548\u6574\u5408\u591a\u6b65\u8de8\u6a21\u6001\u7279\u5f81\u3002", "result": "\u5728Aff-Wild2\u6570\u636e\u96c6\u4e0a\uff0cTAGF\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u9c81\u68d2\u6027\uff0c\u80fd\u53ef\u9760\u5730\u5efa\u6a21\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u52a8\u6001\u60c5\u611f\u8f6c\u6362\u3002", "conclusion": "TAGF\u901a\u8fc7\u65f6\u95f4\u52a8\u6001\u5efa\u6a21\u548c\u8de8\u6a21\u6001\u878d\u5408\uff0c\u5728\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.02226", "pdf": "https://arxiv.org/pdf/2507.02226", "abs": "https://arxiv.org/abs/2507.02226", "authors": ["Mohammad Akyash", "Kimia Azar", "Hadi Kamali"], "title": "DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs", "categories": ["cs.PL", "cs.AR", "cs.LG"], "comment": "Accepted to the International Conference on Computer-Aided Design\n  (ICCAD 2025)", "summary": "As one of their many applications, large language models (LLMs) have recently\nshown promise in automating register transfer level (RTL) code generation.\nHowever, conventional LLM decoding strategies, originally designed for natural\nlanguage, often fail to meet the structural and semantic demands of RTL,\nleading to hallucinated, repetitive, or invalid code outputs. In this paper, we\nfirst investigate the root causes of these decoding failures through an\nempirical analysis of token-level entropy during RTL generation. Our findings\nreveal that LLMs exhibit low confidence in regions of structural ambiguity or\nsemantic complexity, showing that standard decoding strategies fail to\ndifferentiate between regions requiring determinism (syntax-critical regions)\nand those that benefit from creative exploratory variability (design-critical\nregions). Then, to overcome this, we introduce DecoRTL, a novel run-time\ndecoding strategy, that is both syntax-aware and contrastive for RTL code\ngeneration. DecoRTL integrates two complementary components: (i)\nself-consistency sampling, which generates multiple candidates and re-ranks\nthem based on token-level agreement to promote correctness while maintaining\ndiversity; and (ii) syntax-aware temperature adaptation, which classifies\ntokens by their syntactical and functional roles and adjusts the sampling\ntemperature accordingly, enforcing low temperature for syntax-critical tokens\nand higher temperature for exploratory ones. Our approach operates entirely at\ninference time without requiring any additional model fine-tuning. Through\nevaluations on multiple open-source LLMs using the VerilogEval benchmark, we\ndemonstrate significant improvements in syntactic validity, functional\ncorrectness, and output diversity, while the execution overhead (performance\noverhead) is imperceptible.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DecoRTL\uff0c\u4e00\u79cd\u9488\u5bf9RTL\u4ee3\u7801\u751f\u6210\u7684\u65b0\u578b\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u4e00\u81f4\u6027\u91c7\u6837\u548c\u8bed\u6cd5\u611f\u77e5\u6e29\u5ea6\u8c03\u6574\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u7684\u8bed\u6cd5\u6709\u6548\u6027\u548c\u529f\u80fd\u6027\u3002", "motivation": "\u4f20\u7edfLLM\u89e3\u7801\u7b56\u7565\u5728RTL\u4ee3\u7801\u751f\u6210\u4e2d\u5e38\u56e0\u7ed3\u6784\u6216\u8bed\u4e49\u9700\u6c42\u4e0d\u6ee1\u8db3\u800c\u5931\u8d25\uff0c\u5bfc\u81f4\u751f\u6210\u65e0\u6548\u4ee3\u7801\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DecoRTL\u7ed3\u5408\u81ea\u4e00\u81f4\u6027\u91c7\u6837\uff08\u751f\u6210\u5e76\u91cd\u6392\u5019\u9009\uff09\u548c\u8bed\u6cd5\u611f\u77e5\u6e29\u5ea6\u8c03\u6574\uff08\u6309\u8bed\u6cd5\u89d2\u8272\u8c03\u6574\u91c7\u6837\u6e29\u5ea6\uff09\u3002", "result": "\u5728VerilogEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDecoRTL\u663e\u8457\u63d0\u5347\u4e86\u8bed\u6cd5\u6709\u6548\u6027\u3001\u529f\u80fd\u6b63\u786e\u6027\u548c\u8f93\u51fa\u591a\u6837\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u8fd0\u884c\u6548\u7387\u3002", "conclusion": "DecoRTL\u6709\u6548\u89e3\u51b3\u4e86RTL\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u89e3\u7801\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\uff0c\u9002\u7528\u4e8e\u591a\u79cdLLM\u3002"}}
{"id": "2507.02135", "pdf": "https://arxiv.org/pdf/2507.02135", "abs": "https://arxiv.org/abs/2507.02135", "authors": ["Zongpu Zhang", "Pranab Dash", "Y. Charlie Hu", "Qiang Xu", "Jian Li", "Haibing Guan"], "title": "Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency", "categories": ["cs.OS", "cs.CL"], "comment": "equal contribution between Zhang and Dash", "summary": "Large Language Models (LLMs) are increasingly being integrated into various\napplications and services running on billions of mobile devices. However,\ndeploying LLMs on resource-limited mobile devices faces a significant challenge\ndue to their high demand for computation, memory, and ultimately energy. While\ncurrent LLM frameworks for mobile use three power-hungry components-CPU, GPU,\nand Memory-even when running primarily-GPU LLM models, optimized DVFS governors\nfor CPU, GPU, and memory featured in modern mobile devices operate\nindependently and are oblivious of each other. Motivated by the above\nobservation, in this work, we first measure the energy-efficiency of a SOTA LLM\nframework consisting of various LLM models on mobile phones which showed the\ntriplet mobile governors result in up to 40.4% longer prefilling and decoding\nlatency compared to optimal combinations of CPU, GPU, and memory frequencies\nwith the same energy consumption for sampled prefill and decode lengths.\nSecond, we conduct an in-depth measurement study to uncover how the intricate\ninterplay (or lack of) among the mobile governors cause the above inefficiency\nin LLM inference. Finally, based on these insights, we design FUSE - a unified\nenergy-aware governor for optimizing the energy efficiency of LLM inference on\nmobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the\ntime-to-first-token and time-per-output-token latencies by 7.0%-16.9% and\n25.4%-36.8% on average with the same energy-per-token for various mobile LLM\nmodels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u6548\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u80fd\u6e90\u611f\u77e5\u8c03\u63a7\u5668FUSE\uff0c\u663e\u8457\u4f18\u5316\u4e86LLM\u63a8\u7406\u7684\u80fd\u6548\u3002", "motivation": "\u7531\u4e8eLLM\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u65f6\u5bf9\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u80fd\u6e90\u7684\u9ad8\u9700\u6c42\uff0c\u5f53\u524d\u72ec\u7acb\u7684CPU\u3001GPU\u548c\u5185\u5b58\u8c03\u63a7\u5668\u5bfc\u81f4\u80fd\u6548\u4f4e\u4e0b\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u73b0\u6709LLM\u6846\u67b6\u7684\u80fd\u6548\uff0c\u5206\u6790\u8c03\u63a7\u5668\u95f4\u7684\u4ea4\u4e92\u95ee\u9898\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u73b0FUSE\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u80fd\u6e90\u611f\u77e5\u8c03\u63a7\u5668\u3002", "result": "FUSE\u663e\u8457\u51cf\u5c11\u4e86\u9996\u8bcd\u751f\u6210\u65f6\u95f4\u548c\u6bcf\u8bcd\u751f\u6210\u65f6\u95f4\u7684\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u80fd\u6e90\u6d88\u8017\u3002", "conclusion": "FUSE\u6210\u529f\u4f18\u5316\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0aLLM\u63a8\u7406\u7684\u80fd\u6548\uff0c\u4e3a\u672a\u6765\u79fb\u52a8\u7aefLLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02257", "pdf": "https://arxiv.org/pdf/2507.02257", "abs": "https://arxiv.org/abs/2507.02257", "authors": ["Stephen Pasch", "Joel K. Salzman", "Changxi Zheng"], "title": "Gbake: Baking 3D Gaussian Splats into Reflection Probes", "categories": ["cs.GR"], "comment": "SIGGRAPH 2025 Posters", "summary": "The growing popularity of 3D Gaussian Splatting has created the need to\nintegrate traditional computer graphics techniques and assets in splatted\nenvironments. Since 3D Gaussian primitives encode lighting and geometry jointly\nas appearance, meshes are relit improperly when inserted directly in a mixture\nof 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a\nspecialized tool for baking reflection probes from Gaussian-splatted scenes\nthat enables realistic reflection mapping of traditional 3D meshes in the Unity\ngame engine.", "AI": {"tldr": "GBake\u5de5\u5177\u901a\u8fc7\u4ece\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u4e2d\u70d8\u7119\u53cd\u5c04\u63a2\u5934\uff0c\u5b9e\u73b0\u4e86\u5728Unity\u4e2d\u5bf9\u4f20\u7edf3D\u7f51\u683c\u8fdb\u884c\u771f\u5b9e\u53cd\u5c04\u6620\u5c04\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u63d2\u5165\u65f6\u53d1\u5149\u548c\u51e0\u4f55\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e3D\u9ad8\u65af\u539f\u59cb\u6570\u636e\u540c\u65f6\u7f16\u7801\u7167\u660e\u548c\u51e0\u4f55\u5f62\u72b6\u4e3a\u5916\u89c2\uff0c\u76f4\u63a5\u63d2\u51653D\u9ad8\u65af\u6df7\u5408\u73af\u5883\u4e2d\u7684\u7f51\u683c\u4f1a\u51fa\u73b0\u5149\u7167\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4\u660e\u663e\u7684\u4e0d\u534f\u8c03\u3002", "method": "\u5f15\u5165\u4e86GBake\u5de5\u5177\uff0c\u4e13\u95e8\u7528\u4e8e\u4ece\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u4e2d\u70d8\u7119\u53cd\u5c04\u63a2\u5934\uff0c\u4ece\u800c\u5728Unity\u6e38\u620f\u5f15\u64ce\u4e2d\u5b9e\u73b0\u4f20\u7edf3D\u7f51\u683c\u7684\u771f\u5b9e\u53cd\u5c04\u6620\u5c04\u3002", "result": "GBake\u80fd\u591f\u4f7f\u4f20\u7edf3D\u7f51\u683c\u5728\u9ad8\u65af\u6cfc\u6e85\u73af\u5883\u4e2d\u5b9e\u73b0\u4e0e\u5468\u56f4\u573a\u666f\u4e00\u81f4\u7684\u53cd\u5c04\u6548\u679c\uff0c\u63d0\u9ad8\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "conclusion": "GBake\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\u73af\u5883\u4e2d\u4f20\u7edf\u7f51\u683c\u5149\u7167\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e3a\u6e38\u620f\u548c3D\u573a\u666f\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.02008", "pdf": "https://arxiv.org/pdf/2507.02008", "abs": "https://arxiv.org/abs/2507.02008", "authors": ["Ziyi Yang", "Guangyu Hu", "Mingkai Miao", "Changyuan Yu", "Hongce Zhang"], "title": "SMT-Sweep: Word-Level Representation Unification for Hardware Verification", "categories": ["cs.LO"], "comment": null, "summary": "SAT sweeping has long been a cornerstone technique in logic simplification\nand equivalence checking at the bit level, leveraging structural hashing,\nsimulation and SAT solving to prune redundant logic. However, with the growing\nadoption of word-level constructs in hardware verification, such as bit-vector\noperations, arithmetics and arrays, there lacks a counterpart of SAT sweeping\nat the word level. In this paper, we introduce SMT-Sweep, a novel extension of\nSAT sweeping into the word level, grounded in Satisfiability Modulo Theories\n(SMT). SMT-Sweep takes advantage of simulation and equivalence detection to\nhandle SMT terms with rich bit-vector operations and array semantics. Our\nframework incorporates both randomized and constraint-driven word-level\nsimulation tailored to symbolic expressions and operator semantics beyond pure\nBoolean logic. Experimental results show that SMT-Sweep achieves significant\nspeed-up compared to state-of-the-art bit-level SAT sweeping and word-level\nmonolithic SMT solving (averaging around 44x and 69x, respectively).To the best\nof our knowledge, this is the first work that brings sweeping techniques to\nSMT-based hardware verification. The implementation is open-sourced at:\nhttps://github.com/yangziyiiii/SMT-Sweep.", "AI": {"tldr": "SMT-Sweep\u662f\u4e00\u79cd\u5c06SAT sweeping\u6280\u672f\u6269\u5c55\u5230\u5b57\u7ea7\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u57fa\u4e8eSMT\u7406\u8bba\uff0c\u663e\u8457\u63d0\u5347\u4e86\u786c\u4ef6\u9a8c\u8bc1\u7684\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5b57\u7ea7\u7ed3\u6784\u5728\u786c\u4ef6\u9a8c\u8bc1\u4e2d\u7684\u666e\u53ca\uff0c\u7f3a\u4e4f\u9488\u5bf9\u5b57\u7ea7\u7684\u7b49\u6548\u68c0\u67e5\u6280\u672f\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "SMT-Sweep\u5229\u7528SMT\u7406\u8bba\uff0c\u7ed3\u5408\u968f\u673a\u548c\u7ea6\u675f\u9a71\u52a8\u7684\u5b57\u7ea7\u6a21\u62df\uff0c\u5904\u7406\u5bcc\u8bed\u4e49\u7684\u4f4d\u5411\u91cf\u64cd\u4f5c\u548c\u6570\u7ec4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSMT-Sweep\u6bd4\u73b0\u6709\u7684\u4f4d\u7ea7SAT sweeping\u548c\u5b57\u7ea7SMT\u6c42\u89e3\u5feb44\u500d\u548c69\u500d\u3002", "conclusion": "SMT-Sweep\u662f\u9996\u4e2a\u5c06sweeping\u6280\u672f\u5f15\u5165\u57fa\u4e8eSMT\u7684\u786c\u4ef6\u9a8c\u8bc1\u7684\u5de5\u4f5c\uff0c\u5176\u5b9e\u73b0\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01976", "pdf": "https://arxiv.org/pdf/2507.01976", "abs": "https://arxiv.org/abs/2507.01976", "authors": ["Nirhoshan Sivaroopan", "Kaushitha Silva", "Chamara Madarasingha", "Thilini Dahanayaka", "Guillaume Jourjon", "Anura Jayasumana", "Kanchana Thilakarathna"], "title": "A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "Synthetic network traffic generation has emerged as a promising alternative\nfor various data-driven applications in the networking domain. It enables the\ncreation of synthetic data that preserves real-world characteristics while\naddressing key challenges such as data scarcity, privacy concerns, and purity\nconstraints associated with real data. In this survey, we provide a\ncomprehensive review of synthetic network traffic generation approaches,\ncovering essential aspects such as data types, generation models, and\nevaluation methods. With the rapid advancements in AI and machine learning, we\nfocus particularly on deep learning-based techniques while also providing a\ndetailed discussion of statistical methods and their extensions, including\ncommercially available tools. Furthermore, we highlight open challenges in this\ndomain and discuss potential future directions for further research and\ndevelopment. This survey serves as a foundational resource for researchers and\npractitioners, offering a structured analysis of existing methods, challenges,\nand opportunities in synthetic network traffic generation.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u5408\u6210\u7f51\u7edc\u6d41\u91cf\u751f\u6210\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e86\u6570\u636e\u7c7b\u578b\u3001\u751f\u6210\u6a21\u578b\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u6df1\u5ea6\u5b66\u4e60\u548c\u7edf\u8ba1\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u6570\u636e\u7a00\u7f3a\u3001\u9690\u79c1\u548c\u7eaf\u5ea6\u95ee\u9898\uff0c\u63d0\u4f9b\u5408\u6210\u6570\u636e\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7efc\u5408\u8bc4\u8ff0\u6df1\u5ea6\u5b66\u4e60\u548c\u7edf\u8ba1\u65b9\u6cd5\uff0c\u5305\u62ec\u5546\u4e1a\u5de5\u5177\u3002", "result": "\u7cfb\u7edf\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u3001\u6311\u6218\u548c\u673a\u4f1a\u3002", "conclusion": "\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5173\u4e8e\u5408\u6210\u7f51\u7edc\u6d41\u91cf\u751f\u6210\u7684\u5168\u9762\u8d44\u6e90\u3002"}}
{"id": "2507.02067", "pdf": "https://arxiv.org/pdf/2507.02067", "abs": "https://arxiv.org/abs/2507.02067", "authors": ["Nikolaos Papanikolaou", "Doha Touhafi", "Jurgen Vandendriessche", "Danial Karimi", "Sohail Fatimi", "Gianluca Cornetta", "Abdellah Touhafi"], "title": "Advanced Printed Sensors for Environmental Applications: A Path Towards Sustainable Monitoring Solutions", "categories": ["cs.AR"], "comment": null, "summary": "Printed sensors represent a transformative advancement in sensor technology,\nutilizing innovative printing techniques to create flexible, cost-effective,\nand highly customizable sensing devices. Their versatility allows integration\ninto numerous applications across diverse fields such as monitoring a wide\nrange of environmental factors e.g. air and water quality, soil conditions, and\natmospheric changes among others. These sensors demonstrate high sensitivity\nand accuracy in detecting pollutants, temperature variations, humidity levels,\nand other critical parameters essential for environmental assessment and\nprotection.", "AI": {"tldr": "\u5370\u5237\u4f20\u611f\u5668\u901a\u8fc7\u521b\u65b0\u5370\u5237\u6280\u672f\u5b9e\u73b0\u7075\u6d3b\u3001\u4f4e\u6210\u672c\u548c\u9ad8\u5b9a\u5236\u5316\u7684\u4f20\u611f\u8bbe\u5907\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u73af\u5883\u76d1\u6d4b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7075\u6d3b\u3001\u4f4e\u6210\u672c\u4e14\u591a\u529f\u80fd\u7684\u4f20\u611f\u5668\uff0c\u4ee5\u6ee1\u8db3\u73af\u5883\u76d1\u6d4b\u7684\u591a\u6837\u5316\u9700\u6c42\u3002", "method": "\u91c7\u7528\u521b\u65b0\u7684\u5370\u5237\u6280\u672f\u5236\u9020\u4f20\u611f\u5668\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u7075\u654f\u5ea6\u548c\u9ad8\u7cbe\u5ea6\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u4f20\u611f\u5668\u80fd\u6709\u6548\u68c0\u6d4b\u6c61\u67d3\u7269\u3001\u6e29\u6e7f\u5ea6\u53d8\u5316\u7b49\u5173\u952e\u73af\u5883\u53c2\u6570\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5370\u5237\u4f20\u611f\u5668\u5728\u73af\u5883\u76d1\u6d4b\u9886\u57df\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u548c\u6f5c\u529b\u3002"}}
{"id": "2507.02156", "pdf": "https://arxiv.org/pdf/2507.02156", "abs": "https://arxiv.org/abs/2507.02156", "authors": ["Benjamin Watson", "Janet Kim", "Tim McEneany", "Tom Moher", "Claudia Hindo", "Louis Gomez", "Stephen Fransen"], "title": "StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative", "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "The StorySpace project studies the role new interface technologies might play\nin high school education. With this approach in mind, StorySpace is\nspecifically designed to support and enhance classroom narrative, an already\nwell-established classroom activity. StorySpace strives to achieve this through\nadherence to three design goals. The first is to trigger student reflection and\ninterpretation. The narrative medium created by StorySpace should represent the\ntopic of classroom discussion and learning in all its complexity. In building\ntheir representation, the students will then be confronted with that same\ncomplexity. The medium should also itself be exciting and compelling, making\nclassroom narrative interesting and fun.", "AI": {"tldr": "StorySpace\u9879\u76ee\u63a2\u8ba8\u65b0\u754c\u9762\u6280\u672f\u5982\u4f55\u652f\u6301\u9ad8\u4e2d\u8bfe\u5802\u53d9\u4e8b\u6d3b\u52a8\uff0c\u901a\u8fc7\u4e09\u4e2a\u8bbe\u8ba1\u76ee\u6807\u6fc0\u53d1\u5b66\u751f\u53cd\u601d\u4e0e\u5174\u8da3\u3002", "motivation": "\u7814\u7a76\u65b0\u754c\u9762\u6280\u672f\u5728\u9ad8\u4e2d\u7684\u5e94\u7528\uff0c\u589e\u5f3a\u8bfe\u5802\u53d9\u4e8b\u7684\u6548\u679c\u3002", "method": "\u9075\u5faa\u4e09\u4e2a\u8bbe\u8ba1\u76ee\u6807\uff1a\u6fc0\u53d1\u5b66\u751f\u53cd\u601d\u3001\u5448\u73b0\u590d\u6742\u4e3b\u9898\u3001\u6253\u9020\u6709\u8da3\u5a92\u4ecb\u3002", "result": "\u76ee\u6807\u662f\u8ba9\u5b66\u751f\u901a\u8fc7\u6784\u5efa\u590d\u6742\u8868\u8fbe\u63d0\u5347\u5b66\u4e60\u5174\u8da3\u3002", "conclusion": "StorySpace\u901a\u8fc7\u8bbe\u8ba1\u521b\u65b0\u63d0\u5347\u8bfe\u5802\u53d9\u4e8b\u7684\u5438\u5f15\u529b\u548c\u6559\u80b2\u6548\u679c\u3002"}}
{"id": "2507.02122", "pdf": "https://arxiv.org/pdf/2507.02122", "abs": "https://arxiv.org/abs/2507.02122", "authors": ["Neil K. R. Sehgal", "Hita Kambhamettu", "Allen Chang", "Andrew Zhu", "Lyle Ungar", "Sharath Chandra Guntuku"], "title": "PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Effective communication in serious illness and palliative care is essential\nbut often under-taught due to limited access to training resources like\nstandardized patients. We present PAL (Palliative Assisted Learning-bot), a\nconversational system that simulates emotionally nuanced patient interactions\nand delivers structured feedback grounded in an existing empathy-based\nframework. PAL supports text and voice modalities and is designed to scaffold\nclinical skill-building through repeated, low-cost practice. Through a\nmixed-methods study with 17 U.S. medical trainees and clinicians, we explore\nuser engagement with PAL, evaluate usability, and examine design tensions\naround modalities, emotional realism, and feedback delivery. Participants found\nPAL helpful for reflection and skill refinement, though some noted limitations\nin emotional authenticity and the adaptability of feedback. We contribute: (1)\nempirical evidence that large language models can support palliative\ncommunication training; (2) design insights for modality-aware, emotionally\nsensitive simulation tools; and (3) implications for systems that support\nemotional labor, cooperative learning, and AI-augmented training in high-stakes\ncare settings.", "AI": {"tldr": "PAL\u662f\u4e00\u79cd\u8f85\u52a9\u7f13\u548c\u533b\u7597\u6c9f\u901a\u8bad\u7ec3\u7684\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u60c5\u611f\u5316\u60a3\u8005\u4e92\u52a8\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u652f\u6301\u6587\u672c\u548c\u8bed\u97f3\u6a21\u5f0f\u3002\u7814\u7a76\u53d1\u73b0\u5176\u6709\u52a9\u4e8e\u6280\u80fd\u63d0\u5347\uff0c\u4f46\u5728\u60c5\u611f\u771f\u5b9e\u6027\u548c\u53cd\u9988\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u7f13\u548c\u533b\u7597\u6c9f\u901a\u8bad\u7ec3\u8d44\u6e90\u6709\u9650\uff0c\u6807\u51c6\u5316\u60a3\u8005\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64\u5f00\u53d1\u4e86PAL\u7cfb\u7edf\uff0c\u4ee5\u4f4e\u6210\u672c\u3001\u53ef\u91cd\u590d\u7684\u65b9\u5f0f\u652f\u6301\u4e34\u5e8a\u6280\u80fd\u57f9\u8bad\u3002", "method": "PAL\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u652f\u6301\u6587\u672c\u548c\u8bed\u97f3\u4e92\u52a8\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0817\u540d\u7f8e\u56fd\u533b\u5b66\u751f\u548c\u533b\u751f\u53c2\u4e0e\uff09\u8bc4\u4f30\u5176\u53ef\u7528\u6027\u548c\u8bbe\u8ba1\u95ee\u9898\u3002", "result": "\u53c2\u4e0e\u8005\u8ba4\u4e3aPAL\u5bf9\u53cd\u601d\u548c\u6280\u80fd\u63d0\u5347\u6709\u5e2e\u52a9\uff0c\u4f46\u4e5f\u6307\u51fa\u60c5\u611f\u771f\u5b9e\u6027\u548c\u53cd\u9988\u9002\u5e94\u6027\u4e0d\u8db3\u3002\u7814\u7a76\u8bc1\u660e\u8bed\u8a00\u6a21\u578b\u53ef\u7528\u4e8e\u7f13\u548c\u533b\u7597\u57f9\u8bad\u3002", "conclusion": "PAL\u4e3a\u7f13\u548c\u533b\u7597\u6c9f\u901a\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u60c5\u611f\u654f\u611f\u3001\u6a21\u5f0f\u611f\u77e5\u7684\u6a21\u62df\u5de5\u5177\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86AI\u5728\u9ad8\u538b\u62a4\u7406\u73af\u5883\u4e2d\u7684\u57f9\u8bad\u6f5c\u529b\u3002"}}
{"id": "2507.02020", "pdf": "https://arxiv.org/pdf/2507.02020", "abs": "https://arxiv.org/abs/2507.02020", "authors": ["Tim Uilkema", "Yao Ma", "Seyed Sahand Mohammadi Ziabari", "Joep van Vliet"], "title": "Template-Based Schema Matching of Multi-Layout Tenancy Schedules:A Comparative Study of a Template-Based Hybrid Matcher and the ALITE Full Disjunction Model", "categories": ["cs.DB"], "comment": null, "summary": "The lack of standardized tabular formats for tenancy schedules across real\nestate firms creates significant inefficiencies in data integration. Existing\nautomated integration methods, such as Full Disjunction (FD)-based models like\nALITE, prioritize completeness but result in schema bloat, sparse attributes\nand limited business usability. We propose a novel hybrid, template-based\nschema matcher that aligns multi-layout tenancy schedules to a predefined\ntarget schema. The matcher combines schema (Jaccard, Levenshtein) and\ninstance-based metrics (data types, distributions) with globally optimal\nassignments determined via the Hungarian Algorithm. Evaluation against a\nmanually labeled ground truth demonstrates substantial improvements, with grid\nsearch optimization yielding a peak F1-score of 0.881 and an overall null\npercentage of 45.7%. On a separate ground truth of 20 semantically similar\ncolumn sets, ALITE achieves an F1-score of 0.712 and 75.6% nulls. These results\nsuggest that combining structured business knowledge with hybrid matching can\nyield more usable and business-aligned schema mappings. The approach assumes\ncleanly extracted tabular input, future work could explore extending the\nmatcher to support complex, composite tables.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6a21\u677f\u7684\u6df7\u5408\u6a21\u5f0f\u5339\u914d\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u623f\u5730\u4ea7\u516c\u53f8\u79df\u8d41\u65e5\u7a0b\u6570\u636e\u683c\u5f0f\u4e0d\u7edf\u4e00\u5bfc\u81f4\u7684\u96c6\u6210\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5339\u914d\u6548\u679c\u3002", "motivation": "\u623f\u5730\u4ea7\u516c\u53f8\u79df\u8d41\u65e5\u7a0b\u6570\u636e\u683c\u5f0f\u4e0d\u7edf\u4e00\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u6210\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u6ce8\u91cd\u5b8c\u6574\u6027\u4f46\u5b58\u5728\u6a21\u5f0f\u81a8\u80c0\u548c\u4e1a\u52a1\u53ef\u7528\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u5f0f\uff08Jaccard\u3001Levenshtein\uff09\u548c\u5b9e\u4f8b\uff08\u6570\u636e\u7c7b\u578b\u3001\u5206\u5e03\uff09\u6307\u6807\u7684\u6df7\u5408\u5339\u914d\u5668\uff0c\u901a\u8fc7\u5308\u7259\u5229\u7b97\u6cd5\u786e\u5b9a\u5168\u5c40\u6700\u4f18\u5339\u914d\u3002", "result": "\u4f18\u5316\u540e\u7684\u5339\u914d\u5668F1\u5206\u6570\u8fbe0.881\uff0c\u7a7a\u503c\u7387\u4e3a45.7%\uff0c\u4f18\u4e8eALITE\u76840.712\u548c75.6%\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u5316\u4e1a\u52a1\u77e5\u8bc6\u548c\u6df7\u5408\u5339\u914d\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u66f4\u5177\u4e1a\u52a1\u53ef\u7528\u6027\u7684\u6a21\u5f0f\u6620\u5c04\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u590d\u6742\u8868\u683c\u3002"}}
{"id": "2507.02124", "pdf": "https://arxiv.org/pdf/2507.02124", "abs": "https://arxiv.org/abs/2507.02124", "authors": ["Fumikazu Konishi"], "title": "SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan", "categories": ["cs.DC", "cs.NI", "C.5.5; B.8.2"], "comment": "13 pages, 2 Figures, 10 tables", "summary": "SAKURAONE is a managed high performance computing (HPC) cluster developed and\noperated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU\nPHY'' configuration of bare-metal GPU servers and is designed as a cluster\ncomputing resource optimized for advanced workloads, including large language\nmodel (LLM) training.\n  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked\n\\textbf{49th} in the world based on its High Performance Linpack (HPL) score,\ndemonstrating its global competitiveness. In particular, it is the \\textbf{only\nsystem within the top 100} that employs a fully open networking stack based on\n\\textbf{800~GbE (Gigabit Ethernet)} and the \\textbf{SONiC (Software for Open\nNetworking in the Cloud)} operating system, highlighting the viability of open\nand vendor-neutral technologies in large-scale HPC infrastructure.\n  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL\nbenchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate\nGradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets\nlow-precision workloads representative of AI applications, SAKURAONE delivered\nan impressive 339.86~PFLOP/s using FP8 precision.\n  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100\nGPUs. It is supported by an all-flash Lustre storage subsystem with a total\nphysical capacity of 2~petabytes, providing high-throughput and low-latency\ndata access. Internode communication is enabled by a full-bisection bandwidth\ninterconnect based on a Rail-Optimized topology, where the Leaf and Spine\nlayers are interconnected via 800~GbE links. This topology, in combination with\nRoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless\ndata transfers and mitigates communication bottlenecks in large-scale parallel\nworkloads.", "AI": {"tldr": "SAKURAONE\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\uff0c\u4e13\u4e3aLLM\u8bad\u7ec3\u7b49\u9ad8\u7ea7\u5de5\u4f5c\u8d1f\u8f7d\u8bbe\u8ba1\uff0c\u91c7\u7528\u5168\u5f00\u653e\u7f51\u7edc\u6808\uff0c\u6027\u80fd\u5353\u8d8a\u3002", "motivation": "\u5c55\u793a\u5f00\u653e\u548c\u4f9b\u5e94\u5546\u4e2d\u7acb\u6280\u672f\u5728\u5927\u89c4\u6a21HPC\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4f18\u5316\u9ad8\u6027\u80fd\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u914d\u7f6e\u88f8\u91d1\u5c5eGPU\u670d\u52a1\u5668\uff0c\u91c7\u7528800 GbE\u548cSONiC\u5f00\u653e\u7f51\u7edc\u6808\uff0c\u7ed3\u5408RoCEv2\u548cRail-Optimized\u62d3\u6251\u3002", "result": "HPL\u5f97\u5206\u6392\u540d\u5168\u740349\uff0cHPL-MxP\u5728FP8\u7cbe\u5ea6\u4e0b\u8fbe\u5230339.86 PFLOP/s\uff0c\u5177\u5907\u9ad8\u541e\u5410\u548c\u4f4e\u5ef6\u8fdf\u5b58\u50a8\u3002", "conclusion": "SAKURAONE\u8bc1\u660e\u4e86\u5f00\u653e\u6280\u672f\u5728\u5927\u89c4\u6a21HPC\u4e2d\u7684\u53ef\u884c\u6027\u548c\u7ade\u4e89\u529b\u3002"}}
{"id": "2507.02137", "pdf": "https://arxiv.org/pdf/2507.02137", "abs": "https://arxiv.org/abs/2507.02137", "authors": ["Martin Obaidi", "Marc Herrmann", "Jil Kl\u00fcnder", "Kurt Schneider"], "title": "Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection", "categories": ["cs.SE"], "comment": "This paper has been accepted at the RETRAI workshop of the 33rd IEEE\n  International Requirements Engineering Conference (REW 2025)", "summary": "Software development relies heavily on text-based communication, making\nsentiment analysis a valuable tool for understanding team dynamics and\nsupporting trustworthy AI-driven analytics in requirements engineering.\nHowever, existing sentiment analysis tools often perform inconsistently across\ndatasets from different platforms, due to variations in communication style and\ncontent.\n  In this study, we analyze linguistic and statistical features of 10 developer\ncommunication datasets from five platforms and evaluate the performance of 14\nsentiment analysis tools. Based on these results, we propose a mapping approach\nand questionnaire that recommends suitable sentiment analysis tools for new\ndatasets, using their characteristic features as input.\n  Our results show that dataset characteristics can be leveraged to improve\ntool selection, as platforms differ substantially in both linguistic and\nstatistical properties. While transformer-based models such as SetFit and\nRoBERTa consistently achieve strong results, tool effectiveness remains\ncontext-dependent. Our approach supports researchers and practitioners in\nselecting trustworthy tools for sentiment analysis in software engineering,\nwhile highlighting the need for ongoing evaluation as communication contexts\nevolve.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e8610\u4e2a\u5f00\u53d1\u8005\u6c9f\u901a\u6570\u636e\u96c6\u7684\u7279\u5f81\uff0c\u8bc4\u4f30\u4e8614\u79cd\u60c5\u611f\u5206\u6790\u5de5\u5177\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u96c6\u7279\u5f81\u7684\u6620\u5c04\u65b9\u6cd5\u548c\u95ee\u5377\uff0c\u4ee5\u63a8\u8350\u9002\u5408\u7684\u5de5\u5177\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6570\u636e\u96c6\u7279\u5f81\u5bf9\u5de5\u5177\u9009\u62e9\u6709\u663e\u8457\u5f71\u54cd\uff0c\u800cTransformer\u6a21\u578b\u8868\u73b0\u7a33\u5b9a\u4f46\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u60c5\u611f\u5206\u6790\u5de5\u5177\u5728\u4e0d\u540c\u5e73\u53f0\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u652f\u6301\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u53ef\u4fe1\u7684AI\u9a71\u52a8\u5206\u6790\u3002", "method": "\u5206\u67905\u4e2a\u5e73\u53f0\u768410\u4e2a\u5f00\u53d1\u8005\u6c9f\u901a\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u8bc4\u4f3014\u79cd\u60c5\u611f\u5206\u6790\u5de5\u5177\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u6620\u5c04\u65b9\u6cd5\u548c\u63a8\u8350\u95ee\u5377\u3002", "result": "\u6570\u636e\u96c6\u7279\u5f81\u663e\u8457\u5f71\u54cd\u5de5\u5177\u9009\u62e9\uff0cTransformer\u6a21\u578b\uff08\u5982SetFit\u548cRoBERTa\uff09\u8868\u73b0\u7a33\u5b9a\u4f46\u4f9d\u8d56\u4e0a\u4e0b\u6587\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63a8\u8350\u5de5\u5177\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u60c5\u611f\u5206\u6790\u5de5\u5177\u7684\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\uff0c\u540c\u65f6\u5f3a\u8c03\u9700\u6301\u7eed\u8bc4\u4f30\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u6c9f\u901a\u73af\u5883\u3002"}}
{"id": "2507.02626", "pdf": "https://arxiv.org/pdf/2507.02626", "abs": "https://arxiv.org/abs/2507.02626", "authors": ["Siran Chen", "Boyu Chen", "Chenyun Yu", "Yuxiao Luo", "Ouyang Yi", "Lei Cheng", "Chengxiang Zhuo", "Zang Li", "Yali Wang"], "title": "VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning", "categories": ["cs.MM"], "comment": null, "summary": "Owing to powerful natural language processing and generative capabilities,\nlarge language model (LLM) agents have emerged as a promising solution for\nenhancing recommendation systems via user simulation. However, in the realm of\nvideo recommendation, existing studies predominantly resort to prompt-based\nsimulation using frozen LLMs and encounter the intricate challenge of\nmultimodal content understanding. This frequently results in suboptimal item\nmodeling and user preference learning, thereby ultimately constraining\nrecommendation performance. To address these challenges, we introduce\nVRAgent-R1, a novel agent-based paradigm that incorporates human-like\nintelligence in user simulation. Specifically, VRAgent-R1 comprises two\ndistinct agents: the Item Perception (IP) Agent and the User Simulation (US)\nAgent, designed for interactive user-item modeling. Firstly, the IP Agent\nemulates human-like progressive thinking based on MLLMs, effectively capturing\nhidden recommendation semantics in videos. With a more comprehensive multimodal\ncontent understanding provided by the IP Agent, the video recommendation system\nis equipped to provide higher-quality candidate items. Subsequently, the US\nAgent refines the recommended video sets based on in-depth chain-of-thought\n(CoT) reasoning and achieves better alignment with real user preferences\nthrough reinforcement learning. Experimental results on a large-scale video\nrecommendation benchmark have demonstrated the effectiveness of our proposed\nVRAgent-R1 method, e.g., the IP Agent achieves a 6.0\\% improvement in NDCG@10\non the MicroLens-100k dataset, while the US Agent shows approximately 45.0\\%\nhigher accuracy in user decision simulation compared to state-of-the-art\nbaselines.", "AI": {"tldr": "VRAgent-R1\u662f\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u65b0\u578b\u8303\u5f0f\uff0c\u901a\u8fc7\u4e24\u4e2a\u4ee3\u7406\uff08IP\u548cUS\uff09\u63d0\u5347\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5185\u5bb9\u7406\u89e3\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u4f7f\u7528\u51bb\u7ed3\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u57fa\u4e8e\u63d0\u793a\u7684\u7528\u6237\u6a21\u62df\uff0c\u5b58\u5728\u591a\u6a21\u6001\u5185\u5bb9\u7406\u89e3\u548c\u7528\u6237\u504f\u597d\u5b66\u4e60\u7684\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u63a8\u8350\u6027\u80fd\u3002", "method": "VRAgent-R1\u5305\u542bIP\u4ee3\u7406\uff08\u57fa\u4e8eMLLM\u6a21\u62df\u4eba\u7c7b\u6e10\u8fdb\u5f0f\u601d\u7ef4\uff09\u548cUS\u4ee3\u7406\uff08\u901a\u8fc7\u6df1\u5ea6\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63a8\u8350\u96c6\uff09\u3002", "result": "IP\u4ee3\u7406\u5728MicroLens-100k\u6570\u636e\u96c6\u4e0aNDCG@10\u63d0\u53476.0%\uff0cUS\u4ee3\u7406\u5728\u7528\u6237\u51b3\u7b56\u6a21\u62df\u4e2d\u51c6\u786e\u7387\u63d0\u9ad845%\u3002", "conclusion": "VRAgent-R1\u901a\u8fc7\u591a\u6a21\u6001\u7406\u89e3\u548c\u7528\u6237\u6a21\u62df\u7684\u534f\u540c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2507.02622", "pdf": "https://arxiv.org/pdf/2507.02622", "abs": "https://arxiv.org/abs/2507.02622", "authors": ["Zhicheng Zhang", "Mingsheng Ying"], "title": "Access Control Threatened by Quantum Entanglement", "categories": ["quant-ph", "cs.CR", "cs.OS"], "comment": "23 pages, 10 figures", "summary": "Access control is a cornerstone of computer security that prevents\nunauthorised access to resources. In this paper, we study access control in\nquantum computer systems. We present the first explicit scenario of a security\nbreach when a classically secure access control system is straightforwardly\nadapted to the quantum setting. The breach is ultimately due to that quantum\nmechanics allows the phenomenon of entanglement and violates Mermin inequality,\na multi-party variant of the celebrated Bell inequality. This reveals a threat\nfrom quantum entanglement to access control if existing computer systems\nintegrate with quantum computing. To protect against such threat, we propose\nseveral new models of quantum access control, and rigorously analyse their\nsecurity, flexibility and efficiency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u8ba1\u7b97\u673a\u7cfb\u7edf\u4e2d\u7684\u8bbf\u95ee\u63a7\u5236\u95ee\u9898\uff0c\u9996\u6b21\u5c55\u793a\u4e86\u7ecf\u5178\u5b89\u5168\u8bbf\u95ee\u63a7\u5236\u7cfb\u7edf\u76f4\u63a5\u5e94\u7528\u4e8e\u91cf\u5b50\u73af\u5883\u65f6\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u91cf\u5b50\u8bbf\u95ee\u63a7\u5236\u6a21\u578b\u4ee5\u5e94\u5bf9\u5a01\u80c1\u3002", "motivation": "\u91cf\u5b50\u529b\u5b66\u7684\u7279\u6027\uff08\u5982\u7ea0\u7f20\u548c\u8fdd\u53cdMermin\u4e0d\u7b49\u5f0f\uff09\u53ef\u80fd\u5bfc\u81f4\u7ecf\u5178\u8bbf\u95ee\u63a7\u5236\u7cfb\u7edf\u5728\u91cf\u5b50\u73af\u5883\u4e2d\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u65b0\u7684\u8bbf\u95ee\u63a7\u5236\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u591a\u79cd\u65b0\u7684\u91cf\u5b50\u8bbf\u95ee\u63a7\u5236\u6a21\u578b\uff0c\u5e76\u5bf9\u5176\u5b89\u5168\u6027\u3001\u7075\u6d3b\u6027\u548c\u6548\u7387\u8fdb\u884c\u4e86\u4e25\u683c\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u91cf\u5b50\u7ea0\u7f20\u5bf9\u8bbf\u95ee\u63a7\u5236\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u4fdd\u62a4\u65b9\u6848\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u73af\u5883\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u8bbf\u95ee\u63a7\u5236\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u91cf\u5b50\u529b\u5b66\u5e26\u6765\u7684\u65b0\u5b89\u5168\u6311\u6218\u3002"}}
{"id": "2507.02674", "pdf": "https://arxiv.org/pdf/2507.02674", "abs": "https://arxiv.org/abs/2507.02674", "authors": ["Tom Kneiphof", "Reinhard Klein"], "title": "Real-time Image-based Lighting of Glints", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Image-based lighting is a widely used technique to reproduce shading under\nreal-world lighting conditions, especially in real-time rendering applications.\nA particularly challenging scenario involves materials exhibiting a sparkling\nor glittering appearance, caused by discrete microfacets scattered across their\nsurface. In this paper, we propose an efficient approximation for image-based\nlighting of glints, enabling fully dynamic material properties and environment\nmaps. Our novel approach is grounded in real-time glint rendering under area\nlight illumination and employs standard environment map filtering techniques.\nCrucially, our environment map filtering process is sufficiently fast to be\nexecuted on a per-frame basis. Our method assumes that the environment map is\npartitioned into few homogeneous regions of constant radiance. By filtering the\ncorresponding indicator functions with the normal distribution function, we\nobtain the probabilities for individual microfacets to reflect light from each\nregion. During shading, these probabilities are utilized to hierarchically\nsample a multinomial distribution, facilitated by our novel dual-gated Gaussian\napproximation of binomial distributions. We validate that our real-time\napproximation is close to ground-truth renderings for a range of material\nproperties and lighting conditions, and demonstrate robust and stable\nperformance, with little overhead over rendering glints from a single\ndirectional light. Compared to rendering smooth materials without glints, our\napproach requires twice as much memory to store the prefiltered environment\nmap.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u56fe\u50cf\u57fa\u5149\u7167\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u6e32\u67d3\u5177\u6709\u95ea\u70c1\u6216\u95ea\u5149\u5916\u89c2\u7684\u6750\u6599\uff0c\u652f\u6301\u52a8\u6001\u6750\u8d28\u5c5e\u6027\u4e0e\u73af\u5883\u8d34\u56fe\u3002", "motivation": "\u89e3\u51b3\u5b9e\u65f6\u6e32\u67d3\u4e2d\u6750\u6599\u95ea\u70c1\u6548\u679c\u5728\u56fe\u50cf\u57fa\u5149\u7167\u4e0b\u7684\u9ad8\u6548\u8ba1\u7b97\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5b9e\u65f6\u533a\u57df\u5149\u95ea\u6e32\u67d3\u548c\u6807\u51c6\u73af\u5883\u8d34\u56fe\u6ee4\u6ce2\u6280\u672f\uff0c\u901a\u8fc7\u5206\u533a\u73af\u5883\u8d34\u56fe\u5e76\u7ed3\u5408\u6b63\u6001\u5206\u5e03\u51fd\u6570\u8ba1\u7b97\u5fae\u9762\u53cd\u5c04\u6982\u7387\uff0c\u4f7f\u7528\u53cc\u95e8\u63a7\u9ad8\u65af\u8fd1\u4f3c\u5b9e\u73b0\u5206\u5c42\u91c7\u6837\u3002", "result": "\u65b9\u6cd5\u63a5\u8fd1\u771f\u5b9e\u6e32\u67d3\u6548\u679c\uff0c\u6027\u80fd\u7a33\u5b9a\uff0c\u4ec5\u9700\u4e24\u500d\u5185\u5b58\u5b58\u50a8\u9884\u5904\u7406\u73af\u5883\u8d34\u56fe\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u95ea\u5149\u6750\u6599\u6e32\u67d3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u5149\u7167\u4e0e\u6750\u8d28\u3002"}}
{"id": "2507.02742", "pdf": "https://arxiv.org/pdf/2507.02742", "abs": "https://arxiv.org/abs/2507.02742", "authors": ["G. Buriola", "D. Cantone", "G. Cincotti", "E. G. Omodeo", "G. T. Spart\u00e0"], "title": "Decision algorithms for fragments of real analysis. III: A theory of differentiable functions with (semi-)open intervals", "categories": ["cs.LO", "03B25, 26A06"], "comment": null, "summary": "This paper enriches preexisting satisfiability tests for unquantified\nlanguages, which in turn augment a fragment of Tarski's elementary algebra with\nunary real functions possessing a continuous first derivative.\n  Two sorts of individual variables are available, one ranging over real\nnumbers and the other one ranging over the functions of interest. Numerical\nterms are built from real variables through constructs designating the four\nbasic arithmetic operations and through the function-application constructs\n$f(t)$ and $D[\\,f\\,](t)$, where $f$ stands for a function variable, $t$ for a\nnumerical term, and $D[\\,\\sqdot\\,]$ designates the differentiation operator.\nComparison relators can be placed between numerical terms. An array of\npredicate symbols are also available, designating various relationships between\nfunctions, as well as function properties, that may hold over intervals of the\nreal line; those are: (pointwise) function comparisons, strict and nonstrict\nmonotonicity~/~convexity~/~concavity properties, comparisons between the\nderivative of a function and a real term--here, w.r.t.\\ earlier research, they\nare extended to (semi)-open intervals.\n  The decision method we propose consists in preprocessing the given formula\ninto an equisatisfiable quantifier-free formula of the elementary algebra of\nreal numbers, whose satisfiability can then be checked by means of Tarski's\ndecision method. No direct reference to functions will appear in the target\nformula, each function variable having been superseded by a collection of stub\nreal variables; hence, in order to prove that the proposed translation is\nsatisfiability-preserving, we must figure out a sufficiently flexible family of\ninterpolating $C^1$ functions that can accommodate a model for the source\nformula whenever the target formula turns out to be satisfiable.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u672a\u91cf\u5316\u8bed\u8a00\u7684\u6ee1\u8db3\u6027\u6d4b\u8bd5\uff0c\u5f15\u5165\u4e86\u4e00\u7c7b\u5177\u6709\u8fde\u7eed\u4e00\u9636\u5bfc\u6570\u7684\u5b9e\u51fd\u6570\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u51b3\u7b56\u65b9\u6cd5\u5c06\u516c\u5f0f\u8f6c\u6362\u4e3a\u5b9e\u6570\u4ee3\u6570\u5f62\u5f0f\uff0c\u4ece\u800c\u9a8c\u8bc1\u5176\u6ee1\u8db3\u6027\u3002", "motivation": "\u52a8\u673a\u662f\u4e30\u5bcc\u672a\u91cf\u5316\u8bed\u8a00\u7684\u6ee1\u8db3\u6027\u6d4b\u8bd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5177\u6709\u8fde\u7eed\u4e00\u9636\u5bfc\u6570\u7684\u5b9e\u51fd\u6570\uff0c\u5e76\u6269\u5c55\u8fd9\u4e9b\u6d4b\u8bd5\u5230\u66f4\u5e7f\u6cdb\u7684\u60c5\u666f\uff0c\u5982\u534a\u5f00\u533a\u95f4\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5c06\u7ed9\u5b9a\u516c\u5f0f\u9884\u5904\u7406\u4e3a\u7b49\u6ee1\u8db3\u6027\u7684\u65e0\u91cf\u5316\u5b9e\u6570\u4ee3\u6570\u516c\u5f0f\uff0c\u5229\u7528Tarski\u7684\u51b3\u7b56\u65b9\u6cd5\u9a8c\u8bc1\u5176\u6ee1\u8db3\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u63d2\u503c\u51fd\u6570\u8bc1\u660e\u8f6c\u6362\u7684\u5408\u7406\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5177\u6709\u8fde\u7eed\u5bfc\u6570\u7684\u5b9e\u51fd\u6570\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u65e0\u91cf\u5316\u5f62\u5f0f\uff0c\u4ece\u800c\u9a8c\u8bc1\u5176\u6ee1\u8db3\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u6269\u5c55\u4e86\u6ee1\u8db3\u6027\u6d4b\u8bd5\u7684\u8303\u56f4\uff0c\u9002\u7528\u4e8e\u66f4\u590d\u6742\u7684\u51fd\u6570\u548c\u533a\u95f4\u60c5\u666f\uff0c\u5e76\u4fdd\u6301\u4e86\u6ee1\u8db3\u6027\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.01988", "pdf": "https://arxiv.org/pdf/2507.01988", "abs": "https://arxiv.org/abs/2507.01988", "authors": ["Giyong Jung", "Saeid Gorgin", "John Kim", "Jungrae Kim"], "title": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers", "categories": ["cs.NI"], "comment": "12 pages, 8 figures. This paper is accepted for [2025 The\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis (SC)]", "summary": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Implicit Sequence Number (ISN)\u673a\u5236\u548cReliability Extended Link (RXL)\u6269\u5c55\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u8282\u70b9\u82af\u7247\u4e92\u8054\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u5bf9\u5904\u7406\u5668\u6027\u80fd\u9700\u6c42\u7684\u589e\u957f\uff0c\u82af\u7247\u95f4\u4e92\u8054\u7684\u9ad8\u5e26\u5bbd\u548c\u5c0f\u8d1f\u8f7d\u534f\u8bae\uff08\u5982CXL\u3001NVLink\uff09\u5728\u9ad8\u901f\u4f20\u8f93\u4e2d\u6613\u53d7\u9519\u8bef\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u591a\u8282\u70b9\u914d\u7f6e\u4e0b\uff0c\u4f20\u7edf\u53ef\u9760\u6027\u673a\u5236\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86ISN\u673a\u5236\u7528\u4e8e\u7cbe\u786e\u68c0\u6d4b\u6570\u636e\u5305\u4e22\u5931\u548c\u6709\u5e8f\u4f20\u8f93\uff0c\u4e0d\u589e\u52a0\u989d\u5916\u5934\u90e8\u5f00\u9500\uff1bRXL\u6269\u5c55\u5219\u5c06CRC\u63d0\u5347\u4e3a\u4f20\u8f93\u5c42\u673a\u5236\uff0c\u7ed3\u5408FEC\u786e\u4fdd\u7aef\u5230\u7aef\u7684\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "RXL\u5728\u4e0d\u727a\u7272\u5e26\u5bbd\u6548\u7387\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u517c\u5bb9\u73b0\u6709\u6570\u636e\u5305\u7ed3\u6784\u3002", "conclusion": "ISN\u548cRXL\u662f\u89e3\u51b3\u9ad8\u901f\u4e92\u8054\u534f\u8bae\u5728\u591a\u8282\u70b9\u73af\u5883\u4e2d\u53ef\u9760\u6027\u95ee\u9898\u7684\u6709\u6548\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u6570\u636e\u4f20\u8f93\u3002"}}
{"id": "2507.02164", "pdf": "https://arxiv.org/pdf/2507.02164", "abs": "https://arxiv.org/abs/2507.02164", "authors": ["Ruibai Tang", "Chengbin Quan"], "title": "Hardware-Accelerated Algorithm for Complex Function Roots Density Graph Plotting", "categories": ["cs.AR"], "comment": null, "summary": "Solving and visualizing the potential roots of complex functions is essential\nin both theoretical and applied domains, yet often computationally intensive.\nWe present a hardware-accelerated algorithm for complex function roots density\ngraph plotting by approximating functions with polynomials and solving their\nroots using single-shift QR iteration. By leveraging the Hessenberg structure\nof companion matrices and optimizing QR decomposition with Givens rotations, we\ndesign a pipelined FPGA architecture capable of processing a large amount of\npolynomials with high throughput. Our implementation achieves up to 65x higher\nenergy efficiency than CPU-based approaches, and while it trails modern GPUs in\nperformance due to differences in fabrication technique.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u786c\u4ef6\u52a0\u901f\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u9879\u5f0f\u8fd1\u4f3c\u548c\u5355\u4f4d\u79fbQR\u8fed\u4ee3\u9ad8\u6548\u8ba1\u7b97\u5e76\u53ef\u89c6\u5316\u590d\u51fd\u6570\u6839\u7684\u5bc6\u5ea6\u56fe\u3002", "motivation": "\u89e3\u51b3\u590d\u51fd\u6570\u6839\u7684\u6c42\u89e3\u548c\u53ef\u89c6\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u3002", "method": "\u5229\u7528\u4f34\u968f\u77e9\u9635\u7684Hessenberg\u7ed3\u6784\uff0c\u901a\u8fc7Givens\u65cb\u8f6c\u4f18\u5316QR\u5206\u89e3\uff0c\u8bbe\u8ba1\u6d41\u6c34\u7ebfFPGA\u67b6\u6784\uff0c\u5904\u7406\u5927\u91cf\u591a\u9879\u5f0f\u3002", "result": "\u6bd4\u57fa\u4e8eCPU\u7684\u65b9\u6cd5\u80fd\u6548\u63d0\u9ad865\u500d\uff0c\u4f46\u6027\u80fd\u4ecd\u843d\u540e\u4e8e\u73b0\u4ee3GPU\u3002", "conclusion": "FPGA\u67b6\u6784\u5728\u590d\u51fd\u6570\u6839\u5bc6\u5ea6\u56fe\u8ba1\u7b97\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u80fd\uff0c\u4f46\u5236\u9020\u5de5\u827a\u9650\u5236\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.02517", "pdf": "https://arxiv.org/pdf/2507.02517", "abs": "https://arxiv.org/abs/2507.02517", "authors": ["Vivek Yadav", "Anugrah Jain"], "title": "Detecting Multiple Diseases in Multiple Crops Using Deep Learning", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "India, as a predominantly agrarian economy, faces significant challenges in\nagriculture, including substantial crop losses caused by diseases, pests, and\nenvironmental stress. Early detection and accurate identification of diseases\nacross different crops are critical for improving yield and ensuring food\nsecurity. This paper proposes a deep learning based solution for detecting\nmultiple diseases in multiple crops, aimed to cover India's diverse\nagricultural landscape. We first create a unified dataset encompassing images\nof 17 different crops and 34 different diseases from various available\nrepositories. Proposed deep learning model is trained on this dataset and\noutperforms the state-of-the-art in terms of accuracy and the number of crops,\ndiseases covered. We achieve a significant detection accuracy, i.e., 99 percent\nfor our unified dataset which is 7 percent more when compared to\nstate-of-the-art handling 14 crops and 26 different diseases only. By improving\nthe number of crops and types of diseases that can be detected, proposed\nsolution aims to provide a better product for Indian farmers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u79cd\u4f5c\u7269\u4e2d\u7684\u591a\u79cd\u75be\u75c5\uff0c\u65e8\u5728\u8986\u76d6\u5370\u5ea6\u591a\u6837\u5316\u7684\u519c\u4e1a\u666f\u89c2\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u5370\u5ea6\u4f5c\u4e3a\u4e3b\u8981\u519c\u4e1a\u7ecf\u6d4e\u56fd\u5bb6\uff0c\u4f5c\u7269\u56e0\u75c5\u5bb3\u3001\u5bb3\u866b\u548c\u73af\u5883\u538b\u529b\u906d\u53d7\u91cd\u5927\u635f\u5931\uff0c\u65e9\u671f\u51c6\u786e\u68c0\u6d4b\u5bf9\u63d0\u9ad8\u4ea7\u91cf\u548c\u786e\u4fdd\u98df\u54c1\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u9996\u5148\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b17\u79cd\u4f5c\u7269\u548c34\u79cd\u75be\u75c5\u7684\u7edf\u4e00\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u8be5\u6a21\u578b\u5728\u7edf\u4e00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8699%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u6bd4\u73b0\u6709\u6280\u672f\uff08\u8986\u76d614\u79cd\u4f5c\u7269\u548c26\u79cd\u75be\u75c5\uff09\u63d0\u9ad8\u4e867%\u3002", "conclusion": "\u901a\u8fc7\u589e\u52a0\u53ef\u68c0\u6d4b\u7684\u4f5c\u7269\u548c\u75be\u75c5\u7c7b\u578b\uff0c\u8be5\u89e3\u51b3\u65b9\u6848\u65e8\u5728\u4e3a\u5370\u5ea6\u519c\u6c11\u63d0\u4f9b\u66f4\u4f18\u7684\u4ea7\u54c1\u3002"}}
{"id": "2507.02138", "pdf": "https://arxiv.org/pdf/2507.02138", "abs": "https://arxiv.org/abs/2507.02138", "authors": ["Shan Li", "Guozhu Ding"], "title": "A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy", "categories": ["cs.HC"], "comment": null, "summary": "This study introduces and evaluates Healthy Choice, an innovative\ntheory-driven and AI-enhanced simulation platform designed to cultivate\nnutrition literacy through interactive scenario-based learning experiences. We\ncollected feedback from 114 university students with diverse backgrounds who\ncompleted simulated product selection scenarios. Quantitative ratings of\nusefulness and ease of use demonstrated high user satisfaction.", "AI": {"tldr": "Healthy Choice\u662f\u4e00\u4e2aAI\u589e\u5f3a\u7684\u4e92\u52a8\u5b66\u4e60\u5e73\u53f0\uff0c\u65e8\u5728\u63d0\u5347\u8425\u517b\u7d20\u517b\u3002\u7814\u7a76\u8868\u660e\uff0c114\u540d\u5927\u5b66\u751f\u5bf9\u5176\u6548\u679c\u548c\u6613\u7528\u6027\u8bc4\u4ef7\u5f88\u9ad8\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7406\u8bba\u9a71\u52a8\u4e14AI\u589e\u5f3a\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u4e92\u52a8\u573a\u666f\u57f9\u517b\u8425\u517b\u7d20\u517b\u3002", "method": "\u6536\u96c6114\u540d\u5927\u5b66\u5b66\u751f\u7684\u53cd\u9988\uff0c\u5b8c\u6210\u6a21\u62df\u4ea7\u54c1\u9009\u62e9\u573a\u666f\u3002", "result": "\u5b9a\u91cf\u8bc4\u5206\u663e\u793a\u9ad8\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u6613\u7528\u6027\u3002", "conclusion": "Healthy Choice\u5e73\u53f0\u5728\u63d0\u5347\u8425\u517b\u7d20\u517b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.02635", "pdf": "https://arxiv.org/pdf/2507.02635", "abs": "https://arxiv.org/abs/2507.02635", "authors": ["Mao Luo", "Zhi Wang", "Yiwen Huang", "Qingyun Zhang", "Zhouxing Su", "Zhipeng Lv", "Wen Hu", "Jianguo Li"], "title": "SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection", "categories": ["cs.CR", "cs.DB"], "comment": null, "summary": "Electronic payment platforms are estimated to process billions oftransactions\ndaily, with the cumulative value of these transactionspotentially reaching into\nthe trillions. Even a minor error within thishigh-volume environment could\nprecipitate substantial financiallosses. To mitigate this risk, manually\nconstructed verification rules,developed by domain experts, are typically\nemployed to identifyand scrutinize transactions in production environments.\nHowever,due to the absence of a systematic approach to ensure the robust-ness\nof these verification rules against vulnerabilities, they remainsusceptible to\nexploitation.To mitigate this risk, manually constructed verification rules,\nde-veloped by domain experts, are typically employed to identify andscrutinize\ntransactions in production environments. However, dueto the absence of a\nsystematic approach to ensure the robustness ofthese verification rules against\nvulnerabilities, they remain suscep-tible to exploitation. To ensure data\nsecurity, database maintainersusually compose complex verification rules to\ncheck whether aquery/update request is valid. However, the rules written by\nex-perts are usually imperfect, and malicious requests may bypassthese rules.\nAs a result, the demand for identifying the defects ofthe rules systematically\nemerges.", "AI": {"tldr": "\u7535\u5b50\u652f\u4ed8\u5e73\u53f0\u7684\u9ad8\u4ea4\u6613\u91cf\u4e2d\uff0c\u9a8c\u8bc1\u89c4\u5219\u7684\u6f0f\u6d1e\u53ef\u80fd\u5f15\u53d1\u91cd\u5927\u635f\u5931\uff0c\u9700\u7cfb\u7edf\u5316\u65b9\u6cd5\u786e\u4fdd\u5176\u7a33\u5065\u6027\u3002", "motivation": "\u9ad8\u4ea4\u6613\u91cf\u4e0b\uff0c\u624b\u5de5\u6784\u5efa\u7684\u9a8c\u8bc1\u89c4\u5219\u6613\u53d7\u6f0f\u6d1e\u5229\u7528\uff0c\u53ef\u80fd\u5bfc\u81f4\u8d22\u52a1\u635f\u5931\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u6539\u8fdb\u3002", "method": "\u624b\u5de5\u6784\u5efa\u9a8c\u8bc1\u89c4\u5219\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u65b9\u6cd5\u786e\u4fdd\u5176\u6297\u6f0f\u6d1e\u80fd\u529b\u3002", "result": "\u5f53\u524d\u89c4\u5219\u6613\u88ab\u6076\u610f\u8bf7\u6c42\u7ed5\u8fc7\uff0c\u9700\u7cfb\u7edf\u6027\u8bc6\u522b\u89c4\u5219\u7f3a\u9677\u3002", "conclusion": "\u9700\u5f00\u53d1\u7cfb\u7edf\u5316\u65b9\u6cd5\u63d0\u5347\u9a8c\u8bc1\u89c4\u5219\u7684\u7a33\u5065\u6027\uff0c\u4ee5\u51cf\u5c11\u6f0f\u6d1e\u5229\u7528\u98ce\u9669\u3002"}}
{"id": "2507.02158", "pdf": "https://arxiv.org/pdf/2507.02158", "abs": "https://arxiv.org/abs/2507.02158", "authors": ["Jacob Roberts", "Blair Archibald", "Phil Trinder"], "title": "Signalling Health for Improved Kubernetes Microservice Availability", "categories": ["cs.DC"], "comment": "10 pages, 7 figures", "summary": "Microservices are often deployed and managed by a container orchestrator that\ncan detect and fix failures to maintain the service availability critical in\nmany applications. In Poll-based Container Monitoring (PCM), the orchestrator\nperiodically checks container health. While a common approach, PCM requires\ncareful tuning, may degrade service availability, and can be slow to detect\ncontainer health changes. An alternative is Signal-based Container Monitoring\n(SCM), where the container signals the orchestrator when its status changes. We\npresent the design, implementation, and evaluation of an SCM approach for\nKubernetes and empirically show that it has benefits over PCM, as predicted by\na new mathematical model. We compare the service availability of SCM and PCM\nover six experiments using the SockShop benchmark. SCM does not require that\npolling intervals are tuned, and yet detects container failure 86\\% faster than\nPCM and container readiness in a comparable time with limited resource\noverheads. We find PCM can erroneously detect failures, and this reduces\nservice availability by 4\\%. We propose that orchestrators offer SCM features\nfor faster failure detection than PCM without erroneous detections or careful\ntuning.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u57fa\u4e8e\u4fe1\u53f7\uff08SCM\uff09\u548c\u57fa\u4e8e\u8f6e\u8be2\uff08PCM\uff09\u7684\u5bb9\u5668\u76d1\u63a7\u65b9\u6cd5\uff0c\u53d1\u73b0SCM\u5728\u6545\u969c\u68c0\u6d4b\u901f\u5ea6\u548c\u53ef\u7528\u6027\u4e0a\u4f18\u4e8ePCM\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u63d0\u5347\u5bb9\u5668\u76d1\u63a7\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4ee5\u51cf\u5c11\u670d\u52a1\u4e2d\u65ad\u548c\u8bef\u68c0\u3002", "method": "\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u53f7\u7684\u5bb9\u5668\u76d1\u63a7\uff08SCM\uff09\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u8f6e\u8be2\u65b9\u6cd5\uff08PCM\uff09\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "SCM\u6bd4PCM\u6545\u969c\u68c0\u6d4b\u901f\u5ea6\u5feb86%\uff0c\u4e14\u4e0d\u4f1a\u8bef\u68c0\uff0c\u670d\u52a1\u53ef\u7528\u6027\u63d0\u53474%\u3002", "conclusion": "\u5efa\u8bae\u5728\u5bb9\u5668\u7f16\u6392\u5668\u4e2d\u91c7\u7528SCM\u4ee5\u63d0\u5347\u76d1\u63a7\u6548\u7387\u548c\u53ef\u7528\u6027\u3002"}}
{"id": "2507.02182", "pdf": "https://arxiv.org/pdf/2507.02182", "abs": "https://arxiv.org/abs/2507.02182", "authors": ["Fangjian Lei", "Jiawen Liu", "Shayan Noei", "Ying Zou", "Derek Truong", "William Alexander"], "title": "Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Common Business Oriented Language (COBOL) is a programming language used to\ndevelop business applications that are widely adopted by financial, business,\nand government agencies. Due to its age, complexity, and declining number of\nCOBOL developers, maintaining COBOL codebases is becoming increasingly\nchallenging. In particular, the lack of documentation makes it difficult for\nnew developers to effectively understand and maintain COBOL systems. Existing\nresearch utilizes large language models (LLMs) to explain the functionality of\ncode snippets. However, COBOL presents unique challenges due to its\narchitectural and syntactical differences, which often cause its code to exceed\nthe token window size of LLMs. In this work, we propose a multi-agent approach\nthat leverages two LLM-based agents working collaboratively to generate\nexplanations for functions, files, and the overall project. These agents\nincorporate together by utilizing contextual information from the codebase into\nthe code explanation prompts. We evaluate the effectiveness of our approach\nusing 14 open-source, real-world COBOL projects. Our results indicate that our\napproach performs significantly better than the baseline in function code\nexplanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,\nchrF, and SentenceBERT scores, respectively. At the file level, our approach\neffectively explains both short and long COBOL files that exceed the token\nwindow size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in\nexplaining the purpose, functionality, and clarity of the generated\nexplanation. At the project level, our approach generates explanations that\nconvey the functionality and purpose of 82% of the selected projects.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5229\u7528\u4e24\u4e2aLLM\u534f\u4f5c\u751f\u6210COBOL\u4ee3\u7801\u89e3\u91ca\uff0c\u89e3\u51b3\u4e86LLM\u4ee4\u724c\u7a97\u53e3\u9650\u5236\u95ee\u9898\uff0c\u5e76\u5728\u529f\u80fd\u3001\u6587\u4ef6\u548c\u9879\u76ee\u7ea7\u522b\u53d6\u5f97\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "COBOL\u4ee3\u7801\u7ef4\u62a4\u56f0\u96be\uff0c\u7f3a\u4e4f\u6587\u6863\u4e14\u5f00\u53d1\u8005\u51cf\u5c11\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u56e0COBOL\u7279\u6b8a\u6027\u53ca\u4ee4\u724c\u9650\u5236\u6548\u679c\u6709\u9650\u3002", "method": "\u91c7\u7528\u53ccLLM\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u7ed3\u5408\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u751f\u6210\u529f\u80fd\u3001\u6587\u4ef6\u548c\u9879\u76ee\u7ea7\u522b\u7684\u89e3\u91ca\u3002", "result": "\u572814\u4e2aCOBOL\u9879\u76ee\u4e0a\u9a8c\u8bc1\uff0c\u529f\u80fd\u7ea7\u522bMETEOR\u3001chrF\u3001SentenceBERT\u5206\u522b\u63d0\u534712.67%\u300118.59%\u548c0.62%\uff1b\u6587\u4ef6\u7ea7\u522b\u89e3\u91ca\u5728\u76ee\u7684\u3001\u529f\u80fd\u548c\u6e05\u6670\u5ea6\u4e0a\u5206\u522b\u8d85\u8d8a\u57fa\u7ebf4.21%\u300110.72%\u548c14.68%\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86COBOL\u4ee3\u7801\u89e3\u91ca\u7684\u6311\u6218\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21COBOL\u7cfb\u7edf\u3002"}}
{"id": "2507.02000", "pdf": "https://arxiv.org/pdf/2507.02000", "abs": "https://arxiv.org/abs/2507.02000", "authors": ["Yongsen Zheng", "Zongxuan Xie", "Guohua Wang", "Ziyao Liu", "Liang Lin", "Kwok-Yan Lam"], "title": "Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System", "categories": ["cs.IR", "cs.CL", "cs.MM"], "comment": null, "summary": "Unfairness is a well-known challenge in Recommender Systems (RSs), often\nresulting in biased outcomes that disadvantage users or items based on\nattributes such as gender, race, age, or popularity. Although some approaches\nhave started to improve fairness recommendation in offline or static contexts,\nthe issue of unfairness often exacerbates over time, leading to significant\nproblems like the Matthew effect, filter bubbles, and echo chambers. To address\nthese challenges, we proposed a novel framework, Hypergraph Contrastive\nMulti-Interest Learning for Fair Conversational Recommender System (HyFairCRS),\naiming to promote multi-interest diversity fairness in dynamic and interactive\nConversational Recommender Systems (CRSs). HyFairCRS first captures a wide\nrange of user interests by establishing diverse hypergraphs through contrastive\nlearning. These interests are then utilized in conversations to generate\ninformative responses and ensure fair item predictions within the dynamic\nuser-system feedback loop. Experiments on two CRS-based datasets show that\nHyFairCRS achieves a new state-of-the-art performance while effectively\nalleviating unfairness. Our code is available at\nhttps://github.com/zysensmile/HyFairCRS.", "AI": {"tldr": "HyFairCRS\u662f\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8d85\u56fe\u5bf9\u6bd4\u591a\u5174\u8da3\u5b66\u4e60\u89e3\u51b3\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4e0d\u516c\u5e73\u6027\u53ef\u80fd\u5bfc\u81f4\u57fa\u4e8e\u6027\u522b\u3001\u79cd\u65cf\u3001\u5e74\u9f84\u6216\u6d41\u884c\u5ea6\u7b49\u56e0\u7d20\u7684\u504f\u89c1\u7ed3\u679c\uff0c\u8fd9\u4e00\u95ee\u9898\u5728\u52a8\u6001\u4ea4\u4e92\u73af\u5883\u4e2d\u5c24\u4e3a\u4e25\u91cd\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5efa\u7acb\u591a\u6837\u5316\u8d85\u56fe\uff0c\u6355\u6349\u7528\u6237\u591a\u5174\u8da3\uff0c\u5e76\u5728\u5bf9\u8bdd\u4e2d\u751f\u6210\u516c\u5e73\u7684\u63a8\u8350\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cHyFairCRS\u5c55\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u5e76\u6709\u6548\u51cf\u5c11\u4e0d\u516c\u5e73\u6027\u3002", "conclusion": "HyFairCRS\u4e3a\u52a8\u6001\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02477", "pdf": "https://arxiv.org/pdf/2507.02477", "abs": "https://arxiv.org/abs/2507.02477", "authors": ["Gaochao Song", "Zibo Zhao", "Haohan Weng", "Jingbo Zeng", "Rongfei Jia", "Shenghua Gao"], "title": "Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk", "categories": ["cs.CV", "cs.GR"], "comment": "9 pages main text, 14 pages appendix, 23 figures", "summary": "We introduce Mesh Silksong, a compact and efficient mesh representation\ntailored to generate the polygon mesh in an auto-regressive manner akin to silk\nweaving. Existing mesh tokenization methods always produce token sequences with\nrepeated vertex tokens, wasting the network capability. Therefore, our approach\ntokenizes mesh vertices by accessing each mesh vertice only once, reduces the\ntoken sequence's redundancy by 50\\%, and achieves a state-of-the-art\ncompression rate of approximately 22\\%. Furthermore, Mesh Silksong produces\npolygon meshes with superior geometric properties, including manifold topology,\nwatertight detection, and consistent face normals, which are critical for\npractical applications. Experimental results demonstrate the effectiveness of\nour approach, showcasing not only intricate mesh generation but also\nsignificantly improved geometric integrity.", "AI": {"tldr": "Mesh Silksong\u662f\u4e00\u79cd\u7d27\u51d1\u9ad8\u6548\u7684\u7f51\u683c\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u65b9\u5f0f\u751f\u6210\u591a\u8fb9\u5f62\u7f51\u683c\uff0c\u51cf\u5c11\u9876\u70b9\u6807\u8bb0\u5197\u4f5950%\uff0c\u5e76\u5b9e\u73b0\u7ea622%\u7684\u6700\u5148\u8fdb\u538b\u7f29\u7387\u3002", "motivation": "\u73b0\u6709\u7f51\u683c\u6807\u8bb0\u5316\u65b9\u6cd5\u5b58\u5728\u9876\u70b9\u6807\u8bb0\u91cd\u590d\u7684\u95ee\u9898\uff0c\u6d6a\u8d39\u7f51\u7edc\u80fd\u529b\uff0cMesh Silksong\u901a\u8fc7\u6539\u8fdb\u6807\u8bb0\u5316\u65b9\u5f0f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Mesh Silksong\u901a\u8fc7\u81ea\u56de\u5f52\u7f51\u683c\u8868\u793a\u65b9\u5f0f\uff0c\u6bcf\u4e2a\u9876\u70b9\u4ec5\u6807\u8bb0\u4e00\u6b21\uff0c\u51cf\u5c11\u6807\u8bb0\u5e8f\u5217\u5197\u4f59\uff0c\u5e76\u4f18\u5316\u51e0\u4f55\u5c5e\u6027\u5982\u6d41\u5f62\u62d3\u6251\u3001\u6c34\u5bc6\u68c0\u6d4b\u548c\u4e00\u81f4\u9762\u6cd5\u7ebf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMesh Silksong\u4e0d\u4ec5\u80fd\u751f\u6210\u7cbe\u7ec6\u7f51\u683c\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u51e0\u4f55\u5b8c\u6574\u6027\u3002", "conclusion": "Mesh Silksong\u5728\u7f51\u683c\u538b\u7f29\u548c\u51e0\u4f55\u5c5e\u6027\u4f18\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.02767", "pdf": "https://arxiv.org/pdf/2507.02767", "abs": "https://arxiv.org/abs/2507.02767", "authors": ["Tiziano Dalmonte", "Marianna Girlando"], "title": "A Proof-Theoretic View of Basic Intuitionistic Conditional Logic (Extended Version)", "categories": ["cs.LO"], "comment": "Draft. A shorter version of this paper was accepted for presentation\n  at TABLEAUX 2025", "summary": "Intuitionistic conditional logic, studied by Weiss, Ciardelli and Liu, and\nOlkhovikov, aims at providing a constructive analysis of conditional reasoning.\nIn this framework, the would and the might conditional operators are no longer\ninterdefinable. The intuitionistic conditional logics considered in the\nliterature are defined by setting Chellas' conditional logic CK, whose\nsemantics is defined using selection functions, within the constructive and\nintuitionistic framework introduced for intuitionistic modal logics. This\noperation gives rise to a constructive and an intuitionistic variant of\n(might-free-) CK, which we call CCKbox and IntCK respectively. Building on the\nproof systems defined for CK and for intuitionistic modal logics, in this paper\nwe introduce a nested calculus for IntCK and a sequent calculus for CCKbox.\nBased on the sequent calculus, we define CCK, a conservative extension of\nWeiss' logic CCKbox with the might operator. We introduce a class of models and\nan axiomatization for CCK, and extend these result to several extensions of\nCCK.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u76f4\u89c9\u4e3b\u4e49\u6761\u4ef6\u903b\u8f91\u7684\u6784\u9020\u6027\u6846\u67b6\uff0c\u4ecb\u7ecd\u4e86\u4e24\u79cd\u903b\u8f91\u7cfb\u7edfIntCK\u548cCCKbox\uff0c\u5e76\u6269\u5c55\u4e86Weiss\u7684\u903b\u8f91CCKbox\uff0c\u63d0\u51fa\u6a21\u578b\u548c\u516c\u7406\u5316\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u76f4\u89c9\u4e3b\u4e49\u6761\u4ef6\u903b\u8f91\u7684\u6784\u9020\u6027\u6846\u67b6\uff0c\u63a2\u7d22\u6761\u4ef6\u63a8\u7406\u4e2d\u7684would\u548cmight\u7b97\u5b50\u7684\u72ec\u7acb\u6027\u3002", "method": "\u901a\u8fc7\u5d4c\u5957\u6f14\u7b97\u548c\u5e8f\u5217\u6f14\u7b97\u5206\u522b\u5b9a\u4e49IntCK\u548cCCKbox\uff0c\u6269\u5c55Weiss\u7684\u903b\u8f91CCKbox\uff0c\u5e76\u5f15\u5165\u6a21\u578b\u548c\u516c\u7406\u5316\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86CCK\u903b\u8f91\u7cfb\u7edf\u53ca\u5176\u6269\u5c55\u7684\u6a21\u578b\u548c\u516c\u7406\u5316\u65b9\u6cd5\u3002", "conclusion": "\u76f4\u89c9\u4e3b\u4e49\u6761\u4ef6\u903b\u8f91\u7684\u6784\u9020\u6027\u6846\u67b6\u4e3a\u6761\u4ef6\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.01994", "pdf": "https://arxiv.org/pdf/2507.01994", "abs": "https://arxiv.org/abs/2507.01994", "authors": ["Sardar Jaffar Ali", "Syed M. Raza", "Duc-Tai Le", "Rajesh Challa", "Min Young Chung", "Ness Shroff", "Hyunseung Choo"], "title": "Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks", "categories": ["cs.NI", "cs.MA"], "comment": null, "summary": "Despite advancements, Radio Access Networks (RAN) still account for over 50\\%\nof the total power consumption in 5G networks. Existing RAN split options do\nnot fully harness data potential, presenting an opportunity to reduce\noperational expenditures. This paper addresses this opportunity through a\ntwofold approach. First, highly accurate network traffic and user predictions\nare achieved using the proposed Curated Collaborative Learning (CCL) framework,\nwhich selectively collaborates with relevant correlated data for traffic\nforecasting. CCL optimally determines whom, when, and what to collaborate with,\nsignificantly outperforming state-of-the-art approaches, including global,\nfederated, personalized federated, and cyclic institutional incremental\nlearnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the\nDistributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep\nreinforcement learning and prediction inferences from CCL to reduce the number\nof active DU servers efficiently. DUPS dynamically redirects traffic from\nunderutilized DU servers to optimize resource use, improving energy efficiency\nby up to 89% over conventional strategies, translating into substantial\nmonetary benefits for operators. By integrating CCL-driven predictions with\nDUPS, this paper demonstrates a transformative approach for minimizing energy\nconsumption and operational costs in 5G RANs, significantly enhancing\nefficiency and cost-effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7Curated Collaborative Learning\uff08CCL\uff09\u6846\u67b6\u548cDistributed Unit Pooling Scheme\uff08DUPS\uff09\u6765\u964d\u4f4e5G RAN\u80fd\u8017\u548c\u8fd0\u8425\u6210\u672c\u7684\u53cc\u91cd\u65b9\u6cd5\u3002", "motivation": "5G\u7f51\u7edc\u4e2dRAN\u7684\u80fd\u8017\u5360\u603b\u80fd\u8017\u768450%\u4ee5\u4e0a\uff0c\u73b0\u6709RAN\u5206\u5272\u65b9\u6848\u672a\u80fd\u5145\u5206\u5229\u7528\u6570\u636e\u6f5c\u529b\uff0c\u4e9f\u9700\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002", "method": "\u91c7\u7528CCL\u6846\u67b6\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u7f51\u7edc\u6d41\u91cf\u548c\u7528\u6237\u9884\u6d4b\uff0c\u5e76\u7ed3\u5408DUPS\u65b9\u6848\uff0c\u5229\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548cCCL\u9884\u6d4b\u6765\u4f18\u5316DU\u670d\u52a1\u5668\u8d44\u6e90\u3002", "result": "CCL\u5728\u6d41\u91cf\u9884\u6d4b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd543.9%-31.35%\uff0cDUPS\u5c06\u80fd\u6548\u63d0\u534789%\u3002", "conclusion": "\u7ed3\u5408CCL\u548cDUPS\u53ef\u663e\u8457\u964d\u4f4e5G RAN\u7684\u80fd\u8017\u548c\u8fd0\u8425\u6210\u672c\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2507.02456", "pdf": "https://arxiv.org/pdf/2507.02456", "abs": "https://arxiv.org/abs/2507.02456", "authors": ["Wenzhe Guo", "Joyjit Kundu", "Uras Tos", "Weijiang Kong", "Giuliano Sisto", "Timon Evenblij", "Manu Perumkunnil"], "title": "System-performance and cost modeling of Large Language Model training and inference", "categories": ["cs.AR"], "comment": null, "summary": "Large language models (LLMs), based on transformer architectures, have\nrevolutionized numerous domains within artificial intelligence, science, and\nengineering due to their exceptional scalability and adaptability. However, the\nexponential growth in LLM size and complexity has outpaced advancements in\ncompute capacity, memory bandwidth, network performance, and cost efficiency,\nposing significant challenges to their scalability on distributed systems. To\naddress these limitations, alternative model architectures, optimization\nstrategies, communication-aware network topologies, and novel system design\napproaches have been proposed in literature. This paper introduces a\nperformance-cost modeling methodology for LLM training and inference that\nintegrates state-of-the-art compute techniques with memory optimizations, and\nlatest communication techniques. Building on an analytical performance model,\nour approach incorporates recent innovations such as the flash attention\ntechnique and mixture of experts models to address the memory bandwidth and\ncompute bottlenecks. It also considers the impact of different network\ntopologies and topology-specific communication algorithms with 5D parallellism.\nThe framework also integrates a chiplet cost model. The proposed modeling\nmethodology provides valuable insights to guide future compute system design\nand facilitates hardware-software co-development, in particular due to its\nability to analyze performance-cost trade-offs for various system architectural\nconfigurations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u6027\u80fd-\u6210\u672c\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5148\u8fdb\u7684\u8ba1\u7b97\u3001\u5185\u5b58\u4f18\u5316\u548c\u901a\u4fe1\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u7684\u6307\u6570\u589e\u957f\uff0c\u8ba1\u7b97\u80fd\u529b\u3001\u5185\u5b58\u5e26\u5bbd\u3001\u7f51\u7edc\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\u7684\u63d0\u5347\u672a\u80fd\u8ddf\u4e0a\uff0c\u8fd9\u5bf9\u5176\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u57fa\u4e8e\u5206\u6790\u6027\u80fd\u6a21\u578b\uff0c\u6574\u5408\u4e86\u6700\u65b0\u7684\u8ba1\u7b97\u6280\u672f\uff08\u5982\u95ea\u5b58\u6ce8\u610f\u529b\u6280\u672f\u548c\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff09\u3001\u5185\u5b58\u4f18\u5316\u4ee5\u53ca\u7f51\u7edc\u62d3\u6251\u548c\u901a\u4fe1\u7b97\u6cd5\uff08\u59825D\u5e76\u884c\uff09\u3002\u8fd8\u5305\u62ec\u4e00\u4e2a\u82af\u7247\u6210\u672c\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7684\u5efa\u6a21\u65b9\u6cd5\u80fd\u591f\u5206\u6790\u4e0d\u540c\u7cfb\u7edf\u67b6\u6784\u914d\u7f6e\u7684\u6027\u80fd-\u6210\u672c\u6743\u8861\uff0c\u4e3a\u672a\u6765\u7684\u8ba1\u7b97\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u5f00\u53d1\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u5c24\u5176\u662f\u5176\u5206\u6790\u6027\u80fd-\u6210\u672c\u6743\u8861\u7684\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u4f18\u5316LLM\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.02536", "pdf": "https://arxiv.org/pdf/2507.02536", "abs": "https://arxiv.org/abs/2507.02536", "authors": ["Azmat Ullah", "Maria Ilaria Lunesu", "Lodovica Marchesi", "Roberto Tonelli"], "title": "Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain", "categories": ["cs.CR", "cs.ET"], "comment": "2 pages", "summary": "This paper presents a blockchain-based Internet of Things (IoT) system for\nmonitoring pizza production in restaurants. IoT devices track temperature and\nhumidity in real-time, while blockchain ensures secure and tamper-proof data. A\nRaspberry Pi processes sensor data, captures images, triggers alerts, and\ninteracts with smart contracts. The system detects abnormal conditions,\nenabling quick responses. Blockchain adds transparency and traceability,\nsupporting compliance and audits. Experiments show improved ingredient\nmanagement, reduced waste, and increased kitchen efficiency.", "AI": {"tldr": "\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u7269\u8054\u7f51\u7cfb\u7edf\u7528\u4e8e\u76d1\u63a7\u9910\u5385\u62ab\u8428\u751f\u4ea7\uff0c\u5b9e\u73b0\u6570\u636e\u5b89\u5168\u548c\u900f\u660e\u5316\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u533a\u5757\u94fe\u548c\u7269\u8054\u7f51\u6280\u672f\uff0c\u63d0\u5347\u62ab\u8428\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u7684\u6570\u636e\u5b89\u5168\u6027\u548c\u7ba1\u7406\u6548\u7387\u3002", "method": "\u4f7f\u7528\u7269\u8054\u7f51\u8bbe\u5907\u5b9e\u65f6\u76d1\u6d4b\u6e29\u6e7f\u5ea6\uff0c\u901a\u8fc7\u6811\u8393\u6d3e\u5904\u7406\u6570\u636e\u5e76\u4e0e\u667a\u80fd\u5408\u7ea6\u4ea4\u4e92\uff0c\u533a\u5757\u94fe\u4fdd\u969c\u6570\u636e\u4e0d\u53ef\u7be1\u6539\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u7ba1\u7406\u539f\u6599\u3001\u51cf\u5c11\u6d6a\u8d39\u5e76\u63d0\u9ad8\u53a8\u623f\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u98df\u54c1\u751f\u4ea7\u884c\u4e1a\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u900f\u660e\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02233", "pdf": "https://arxiv.org/pdf/2507.02233", "abs": "https://arxiv.org/abs/2507.02233", "authors": ["Bruce Fang", "Danyi Gao"], "title": "Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems", "categories": ["cs.DC"], "comment": null, "summary": "This paper addresses the challenge of fault root cause identification in\ncloud computing environments. The difficulty arises from complex system\nstructures, dense service coupling, and limited fault information. To solve\nthis problem, an intelligent identification algorithm based on transfer\nlearning is proposed. The method introduces a shared feature extraction module\nand a domain adversarial mechanism to enable effective knowledge transfer from\nthe source domain to the target domain. This improves the model's\ndiscriminative ability and generalization performance in the target domain. The\nmodel incorporates a pseudo-label selection strategy. When labeled samples are\nlacking in the target domain, high-confidence predictions are used in training.\nThis enhances the model's ability to recognize minority classes. To evaluate\nthe stability and adaptability of the method in real-world scenarios,\nexperiments are designed under three conditions: label scarcity, class\nimbalance, and heterogeneous node environments. Experimental results show that\nthe proposed method outperforms existing mainstream approaches in several key\nmetrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger\ndiscriminative power and robustness. Notably, under extreme class imbalance and\nsignificant structural differences in the target domain, the model still\nmaintains high performance. This validates the effectiveness and practical\nvalue of the proposed mechanisms in complex cloud computing systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u667a\u80fd\u6545\u969c\u6839\u56e0\u8bc6\u522b\u7b97\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u548c\u57df\u5bf9\u6297\u673a\u5236\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u4e91\u8ba1\u7b97\u73af\u5883\u4e2d\u56e0\u590d\u6742\u7cfb\u7edf\u7ed3\u6784\u3001\u5bc6\u96c6\u670d\u52a1\u8026\u5408\u548c\u6709\u9650\u6545\u969c\u4fe1\u606f\u5bfc\u81f4\u7684\u6545\u969c\u6839\u56e0\u8bc6\u522b\u96be\u9898\u3002", "method": "\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f15\u5165\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u57df\u5bf9\u6297\u673a\u5236\uff0c\u5e76\u5229\u7528\u4f2a\u6807\u7b7e\u9009\u62e9\u7b56\u7565\u589e\u5f3a\u6a21\u578b\u5bf9\u5c11\u6570\u7c7b\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5728\u51c6\u786e\u6027\u3001F1-Score\u548cAUC\u7b49\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u76ee\u6807\u57df\u7ed3\u6784\u5dee\u5f02\u5927\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u63d0\u51fa\u7684\u673a\u5236\u5728\u590d\u6742\u4e91\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u6545\u969c\u6839\u56e0\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.02318", "pdf": "https://arxiv.org/pdf/2507.02318", "abs": "https://arxiv.org/abs/2507.02318", "authors": ["Chen Yang", "Ziqi Wang", "Yanjie Jiang", "Lin Yang", "Yuteng Zheng", "Jianyi Zhou", "Junjie Chen"], "title": "Precisely Detecting Python Type Errors via LLM-based Unit Test Generation", "categories": ["cs.SE"], "comment": null, "summary": "Type errors in Python often lead to runtime failures, posing significant\nchallenges to software reliability and developer productivity. Existing static\nanalysis tools aim to detect such errors without execution but frequently\nsuffer from high false positive rates. Recently, unit test generation\ntechniques offer great promise in achieving high test coverage, but they often\nstruggle to produce bug-revealing tests without tailored guidance. To address\nthese limitations, we present RTED, a novel type-aware test generation\ntechnique for automatically detecting Python type errors. Specifically, RTED\ncombines step-by-step type constraint analysis with reflective validation to\nguide the test generation process and effectively suppress false positives. We\nevaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.\nExperimental results show that RTED can detect 22-29 more benchmarked type\nerrors than four state-of-the-art techniques. RTED is also capable of producing\nfewer false positives, achieving an improvement of 173.9%-245.9% in precision.\nFurthermore, RTED successfully discovered 12 previously unknown type errors\nfrom six real-world open-source Python projects.", "AI": {"tldr": "RTED\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7c7b\u578b\u611f\u77e5\u6d4b\u8bd5\u751f\u6210\u6280\u672f\uff0c\u901a\u8fc7\u7ed3\u5408\u9010\u6b65\u7c7b\u578b\u7ea6\u675f\u5206\u6790\u548c\u53cd\u5c04\u9a8c\u8bc1\uff0c\u6709\u6548\u51cf\u5c11Python\u7c7b\u578b\u9519\u8bef\u7684\u8bef\u62a5\u7387\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "Python\u4e2d\u7684\u7c7b\u578b\u9519\u8bef\u5e38\u5bfc\u81f4\u8fd0\u884c\u65f6\u6545\u969c\uff0c\u5f71\u54cd\u8f6f\u4ef6\u53ef\u9760\u6027\u548c\u5f00\u53d1\u6548\u7387\u3002\u73b0\u6709\u9759\u6001\u5206\u6790\u5de5\u5177\u8bef\u62a5\u7387\u9ad8\uff0c\u800c\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u6280\u672f\u7f3a\u4e4f\u9488\u5bf9\u6027\u6307\u5bfc\u3002", "method": "RTED\u901a\u8fc7\u9010\u6b65\u7c7b\u578b\u7ea6\u675f\u5206\u6790\u4e0e\u53cd\u5c04\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u6307\u5bfc\u6d4b\u8bd5\u751f\u6210\uff0c\u964d\u4f4e\u8bef\u62a5\u7387\u3002", "result": "\u5728BugsInPy\u548cTypeBugs\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRTED\u68c0\u6d4b\u5230\u6bd4\u73b0\u6709\u6280\u672f\u591a22-29\u4e2a\u7c7b\u578b\u9519\u8bef\uff0c\u8bef\u62a5\u7387\u964d\u4f4e173.9%-245.9%\uff0c\u5e76\u53d1\u73b012\u4e2a\u672a\u77e5\u9519\u8bef\u3002", "conclusion": "RTED\u5728Python\u7c7b\u578b\u9519\u8bef\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u5b9e\u9645\u5e94\u7528\u4e8e\u5f00\u6e90\u9879\u76ee\u3002"}}
{"id": "2507.02271", "pdf": "https://arxiv.org/pdf/2507.02271", "abs": "https://arxiv.org/abs/2507.02271", "authors": ["Feizhen Huang", "Yu Wu", "Yutian Lin", "Bo Du"], "title": "Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by IJCAI 2025", "summary": "Video-to-Audio (V2A) Generation achieves significant progress and plays a\ncrucial role in film and video post-production. However, current methods\noverlook the cinematic language, a critical component of artistic expression in\nfilmmaking. As a result, their performance deteriorates in scenarios where\nFoley targets are only partially visible. To address this challenge, we propose\na simple self-distillation approach to extend V2A models to cinematic language\nscenarios. By simulating the cinematic language variations, the student model\nlearns to align the video features of training pairs with the same audio-visual\ncorrespondences, enabling it to effectively capture the associations between\nsounds and partial visual information. Our method not only achieves impressive\nimprovements under partial visibility across all evaluation metrics, but also\nenhances performance on the large-scale V2A dataset, VGGSound.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u89c6\u9891\u5230\u97f3\u9891\uff08V2A\uff09\u751f\u6210\u6a21\u578b\u5728\u7535\u5f71\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u90e8\u5206\u53ef\u89c1\u6027\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dV2A\u65b9\u6cd5\u5ffd\u7565\u4e86\u7535\u5f71\u8bed\u8a00\uff0c\u5bfc\u81f4\u5728Foley\u76ee\u6807\u4ec5\u90e8\u5206\u53ef\u89c1\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u7535\u5f71\u8bed\u8a00\u53d8\u5316\uff0c\u81ea\u84b8\u998f\u6a21\u578b\u5b66\u4e60\u5bf9\u9f50\u76f8\u540c\u89c6\u542c\u5bf9\u5e94\u7684\u89c6\u9891\u7279\u5f81\uff0c\u6709\u6548\u6355\u6349\u58f0\u97f3\u4e0e\u90e8\u5206\u89c6\u89c9\u4fe1\u606f\u7684\u5173\u8054\u3002", "result": "\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u90e8\u5206\u53ef\u89c1\u6027\u573a\u666f\u7684\u6027\u80fd\uff0c\u5e76\u5728VGGSound\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u81ea\u84b8\u998f\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86V2A\u751f\u6210\u4e2d\u7535\u5f71\u8bed\u8a00\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u9002\u7528\u6027\u3002"}}
{"id": "2507.02803", "pdf": "https://arxiv.org/pdf/2507.02803", "abs": "https://arxiv.org/abs/2507.02803", "authors": ["Gent Serifi", "Marcel C. B\u00fchler"], "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://gserifi.github.io/HyperGaussians", "summary": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for\nhigh-quality animatable face avatars. Creating such detailed face avatars from\nvideos is a challenging problem and has numerous applications in augmented and\nvirtual reality. While tremendous successes have been achieved for static\nfaces, animatable avatars from monocular videos still fall in the uncanny\nvalley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face\nthrough a collection of 3D Gaussian primitives. 3DGS excels at rendering static\nfaces, but the state-of-the-art still struggles with nonlinear deformations,\ncomplex lighting effects, and fine details. While most related works focus on\npredicting better Gaussian parameters from expression codes, we rethink the 3D\nGaussian representation itself and how to make it more expressive. Our insights\nlead to a novel extension of 3D Gaussians to high-dimensional multivariate\nGaussians, dubbed 'HyperGaussians'. The higher dimensionality increases\nexpressivity through conditioning on a learnable local embedding. However,\nsplatting HyperGaussians is computationally expensive because it requires\ninverting a high-dimensional covariance matrix. We solve this by\nreparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.\nThis trick boosts the efficiency so that HyperGaussians can be seamlessly\nintegrated into existing models. To demonstrate this, we plug in HyperGaussians\ninto the state-of-the-art in fast monocular face avatars: FlashAvatar. Our\nevaluation on 19 subjects from 4 face datasets shows that HyperGaussians\noutperform 3DGS numerically and visually, particularly for high-frequency\ndetails like eyeglass frames, teeth, complex facial movements, and specular\nreflections.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHyperGaussians\u7684\u65b0\u65b9\u6cd5\uff0c\u6269\u5c55\u4e863D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf\u53ef\u52a8\u753b\u5316\u9762\u90e8\u5934\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u53d8\u5f62\u548c\u590d\u6742\u5149\u7167\u6548\u679c\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u9759\u6001\u9762\u90e8\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u53ef\u52a8\u753b\u5316\u5934\u50cf\u548c\u590d\u6742\u7ec6\u8282\u5904\u7406\u4e0a\u4ecd\u6709\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u9ad8\u7ef4\u591a\u53d8\u91cf\u9ad8\u65af\uff08HyperGaussians\uff09\u5e76\u5229\u7528\u53ef\u5b66\u4e60\u7684\u5c40\u90e8\u5d4c\u5165\u589e\u52a0\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u901a\u8fc7\u2018\u9006\u534f\u65b9\u5dee\u6280\u5de7\u2019\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u57284\u4e2a\u9762\u90e8\u6570\u636e\u96c6\u4e0a\u768419\u540d\u53d7\u8bd5\u8005\u4e2d\uff0cHyperGaussians\u5728\u6570\u503c\u548c\u89c6\u89c9\u6548\u679c\u4e0a\u5747\u4f18\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5c24\u5176\u5728\u7ec6\u8282\u5904\u7406\u5982\u773c\u955c\u6846\u3001\u7259\u9f7f\u548c\u590d\u6742\u9762\u90e8\u52a8\u4f5c\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "HyperGaussians\u4e3a\u53ef\u52a8\u753b\u5316\u9762\u90e8\u5934\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\u548c\u7ec6\u8282\u8868\u73b0\u3002"}}
{"id": "2507.02855", "pdf": "https://arxiv.org/pdf/2507.02855", "abs": "https://arxiv.org/abs/2507.02855", "authors": ["Colin Rothgang", "Florian Rabe"], "title": "Subtyping in DHOL -- Extended preprint", "categories": ["cs.LO", "cs.AI", "cs.FL"], "comment": "16 pages main document, 44 pages of appendices, to be published in\n  FroCoS 2025", "summary": "The recently introduced dependent typed higher-order logic (DHOL) offers an\ninteresting compromise between expressiveness and automation support. It\nsacrifices the decidability of its type system in order to significantly extend\nits expressiveness over standard HOL. Yet it retains strong automated theorem\nproving support via a sound and complete translation to HOL.\n  We leverage this design to extend DHOL with refinement and quotient types.\nBoth of these are commonly requested by practitioners but rarely provided by\nautomated theorem provers. This is because they inherently require undecidable\ntyping and thus are very difficult to retrofit to decidable type systems. But\nwith DHOL already doing the heavy lifting, adding them is not only possible but\nelegant and simple.\n  Concretely, we add refinement and quotient types as special cases of\nsubtyping. This turns the associated canonical inclusion resp. projection maps\ninto identity maps and thus avoids costly changes in representation. We present\nthe syntax, semantics, and translation to HOL for the extended language,\nincluding the proofs of soundness and completeness.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u5728\u4f9d\u8d56\u7c7b\u578b\u9ad8\u9636\u903b\u8f91\uff08DHOL\uff09\u4e2d\u6dfb\u52a0\u7cbe\u70bc\u7c7b\u578b\u548c\u5546\u7c7b\u578b\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u8868\u8fbe\u80fd\u529b\u800c\u4e0d\u5f71\u54cd\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\u7684\u652f\u6301\u3002", "motivation": "\u7cbe\u70bc\u7c7b\u578b\u548c\u5546\u7c7b\u578b\u5728\u5b9e\u8df5\u4e2d\u5e38\u88ab\u9700\u6c42\uff0c\u4f46\u7531\u4e8e\u9700\u8981\u4e0d\u53ef\u5224\u5b9a\u7684\u7c7b\u578b\u7cfb\u7edf\uff0c\u5f88\u5c11\u80fd\u5728\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u4e2d\u5b9e\u73b0\u3002DHOL\u7684\u8bbe\u8ba1\u4f7f\u5f97\u8fd9\u79cd\u6269\u5c55\u6210\u4e3a\u53ef\u80fd\u3002", "method": "\u5c06\u7cbe\u70bc\u7c7b\u578b\u548c\u5546\u7c7b\u578b\u4f5c\u4e3a\u5b50\u7c7b\u578b\u7684\u7279\u4f8b\u6dfb\u52a0\uff0c\u907f\u514d\u4e86\u8868\u793a\u4e0a\u7684\u9ad8\u6210\u672c\u53d8\u5316\uff0c\u5e76\u4fdd\u6301\u4e86\u8bed\u6cd5\u3001\u8bed\u4e49\u548cHOL\u8f6c\u6362\u7684\u5b8c\u6574\u6027\u3002", "result": "\u6210\u529f\u6269\u5c55\u4e86DHOL\uff0c\u8bc1\u660e\u4e86\u5176\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u8f6c\u6362\u5230HOL\u7684\u6b63\u786e\u6027\u3002", "conclusion": "DHOL\u7684\u6269\u5c55\u4e0d\u4ec5\u53ef\u884c\uff0c\u800c\u4e14\u4f18\u96c5\u7b80\u6d01\uff0c\u4e3a\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2507.01997", "pdf": "https://arxiv.org/pdf/2507.01997", "abs": "https://arxiv.org/abs/2507.01997", "authors": ["Zhihao Wang", "Alessandro Cornacchia", "Franco Galante", "Carlo Centofanti", "Alessio Sacco", "Dingde Jiang"], "title": "Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting", "categories": ["cs.NI", "cs.AI", "cs.MA"], "comment": "Accepted at ACM SIGCOMM 1st Workshop on Next-Generation Network\n  Observability (NGNO)", "summary": "Recent research has demonstrated the effectiveness of Artificial Intelligence\n(AI), and more specifically, Large Language Models (LLMs), in supporting\nnetwork configuration synthesis and automating network diagnosis tasks, among\nothers. In this preliminary work, we restrict our focus to the application of\nAI agents to network troubleshooting and elaborate on the need for a\nstandardized, reproducible, and open benchmarking platform, where to build and\nevaluate AI agents with low operational effort.", "AI": {"tldr": "AI\uff08\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5728\u7f51\u7edc\u914d\u7f6e\u548c\u8bca\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u672c\u6587\u63a2\u8ba8\u4e86AI\u5728\u7f51\u7edc\u6545\u969c\u6392\u9664\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u5f00\u653e\u57fa\u51c6\u5e73\u53f0\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u7814\u7a76AI\u5728\u7f51\u7edc\u6545\u969c\u6392\u9664\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u6807\u51c6\u5316\u5e73\u53f0\u7684\u9700\u6c42\uff0c\u4ee5\u51cf\u5c11\u64cd\u4f5c\u6210\u672c\u3002", "method": "\u63a2\u8ba8\u4e86AI\u4ee3\u7406\u5728\u7f51\u7edc\u6545\u969c\u6392\u9664\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u6784\u5efa\u5f00\u653e\u57fa\u51c6\u5e73\u53f0\u7684\u521d\u6b65\u8bbe\u60f3\u3002", "result": "\u5c1a\u672a\u63d0\u51fa\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u5f3a\u8c03\u4e86\u6807\u51c6\u5316\u5e73\u53f0\u5bf9AI\u4ee3\u7406\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u5f00\u653e\u57fa\u51c6\u5e73\u53f0\u662f\u63a8\u52a8AI\u5728\u7f51\u7edc\u6545\u969c\u6392\u9664\u4e2d\u5e94\u7528\u7684\u5173\u952e\u3002"}}
{"id": "2507.02598", "pdf": "https://arxiv.org/pdf/2507.02598", "abs": "https://arxiv.org/abs/2507.02598", "authors": ["Chenhao Xue", "Kezhi Li", "Jiaxing Zhang", "Yi Ren", "Zhengyuan Shi", "Chen Zhang", "Yibo Lin", "Lining Zhang", "Qiang Xu", "Guangyu Sun"], "title": "AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models", "categories": ["cs.AR", "cs.AI"], "comment": "8 pages, 12 figures", "summary": "Arithmetic circuits, such as adders and multipliers, are fundamental\ncomponents of digital systems, directly impacting the performance, power\nefficiency, and area footprint. However, optimizing these circuits remains\nchallenging due to the vast design space and complex physical constraints.\nWhile recent deep learning-based approaches have shown promise, they struggle\nto consistently explore high-potential design variants, limiting their\noptimization efficiency. To address this challenge, we propose AC-Refiner, a\nnovel arithmetic circuit optimization framework leveraging conditional\ndiffusion models. Our key insight is to reframe arithmetic circuit synthesis as\na conditional image generation task. By carefully conditioning the denoising\ndiffusion process on target quality-of-results (QoRs), AC-Refiner consistently\nproduces high-quality circuit designs. Furthermore, the explored designs are\nused to fine-tune the diffusion model, which focuses the exploration near the\nPareto frontier. Experimental results demonstrate that AC-Refiner generates\ndesigns with superior Pareto optimality, outperforming state-of-the-art\nbaselines. The performance gain is further validated by integrating AC-Refiner\ninto practical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAC-Refiner\u6846\u67b6\uff0c\u5229\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4f18\u5316\u7b97\u672f\u7535\u8def\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5c06\u7535\u8def\u5408\u6210\u8f6c\u5316\u4e3a\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u8bbe\u8ba1\u8d28\u91cf\u548c\u5e15\u7d2f\u6258\u6700\u4f18\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7b97\u672f\u7535\u8def\uff08\u5982\u52a0\u6cd5\u5668\u548c\u4e58\u6cd5\u5668\uff09\u662f\u6570\u5b57\u7cfb\u7edf\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5176\u4f18\u5316\u9762\u4e34\u8bbe\u8ba1\u7a7a\u95f4\u5927\u548c\u7269\u7406\u7ea6\u675f\u590d\u6742\u7684\u6311\u6218\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u63a2\u7d22\u9ad8\u6f5c\u529b\u8bbe\u8ba1\u53d8\u4f53\u3002", "method": "\u63d0\u51faAC-Refiner\u6846\u67b6\uff0c\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u5c06\u7535\u8def\u5408\u6210\u4efb\u52a1\u8f6c\u5316\u4e3a\u6761\u4ef6\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u7ed3\u5408\u76ee\u6807\u6027\u80fd\u6307\u6807\uff08QoRs\uff09\u6307\u5bfc\u53bb\u566a\u8fc7\u7a0b\uff0c\u540c\u65f6\u5229\u7528\u63a2\u7d22\u7684\u8bbe\u8ba1\u5fae\u8c03\u6a21\u578b\u4ee5\u805a\u7126\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAC-Refiner\u751f\u6210\u7684\u7535\u8def\u8bbe\u8ba1\u5728\u5e15\u7d2f\u6258\u6700\u4f18\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AC-Refiner\u901a\u8fc7\u521b\u65b0\u5730\u5c06\u7535\u8def\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u6761\u4ef6\u751f\u6210\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u8ba1\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u7b97\u672f\u7535\u8def\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.02682", "pdf": "https://arxiv.org/pdf/2507.02682", "abs": "https://arxiv.org/abs/2507.02682", "authors": ["Ehud Sharlin", "Pablo Figueroa", "Mark Green", "Benjamin Watson"], "title": "A wireless, inexpensive optical tracker for the CAVE", "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "CAVE displays offer many advantages over other virtual reality (VR) displays,\nincluding a large, unencumbering viewing space. Unfortunately, the typical\ntracking subsystems used with CAVE displays tether the user and lessen this\nadvantage. We have designed a simple, low-cost feet tracker that is wireless,\nleaving the user free to move. The tracker can be assembled for less than $200\nUS, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested\nthe prototype with two applications: a visualization supporting close visual\ninspection, and a walkthrough of the campus. Although the tracking was\nconvincing, it was clear that the tracker's limitations make it less than ideal\nfor applications requiring precise visual inspection. However, the freedom of\nmotion allowed by the tracker was a compelling supplement to our campus\nwalkthrough, allowing users to stroll and look around corners.", "AI": {"tldr": "\u4e00\u7bc7\u5173\u4e8e\u4f4e\u6210\u672c\u65e0\u7ebf\u811a\u90e8\u8ffd\u8e2a\u5668\u7684\u7814\u7a76\uff0c\u7528\u4e8eCAVE\u663e\u793a\u7cfb\u7edf\uff0c\u63d0\u4f9b\u7528\u6237\u65e0\u675f\u7f1a\u7684\u8fd0\u52a8\u81ea\u7531\uff0c\u9002\u7528\u4e8e\u6821\u56ed\u6f2b\u6e38\u4f46\u4e0d\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u89c6\u89c9\u68c0\u67e5\u3002", "motivation": "\u4f20\u7edfCAVE\u663e\u793a\u7cfb\u7edf\u7684\u8ffd\u8e2a\u5b50\u7cfb\u7edf\u4f1a\u9650\u5236\u7528\u6237\u79fb\u52a8\uff0c\u524a\u5f31\u5176\u4f18\u52bf\uff0c\u56e0\u6b64\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u65e0\u7ebf\u7684\u811a\u90e8\u8ffd\u8e2a\u5668\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6210\u672c\u4f4e\u4e8e200\u7f8e\u5143\u7684\u65e0\u7ebf\u811a\u90e8\u8ffd\u8e2a\u5668\uff0c\u91c7\u6837\u7387\u4e3a20 Hz\uff0c\u7cbe\u5ea6\u4e3a10 cm\uff0c\u5e76\u5728\u53ef\u89c6\u5316\u68c0\u67e5\u4e0e\u6821\u56ed\u6f2b\u6e38\u5e94\u7528\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u8ffd\u8e2a\u5668\u5728\u6821\u56ed\u6f2b\u6e38\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u7528\u6237\u53ef\u81ea\u7531\u79fb\u52a8\uff0c\u4f46\u5728\u9700\u8981\u9ad8\u7cbe\u5ea6\u89c6\u89c9\u68c0\u67e5\u7684\u5e94\u7528\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u65e0\u7ebf\u811a\u90e8\u8ffd\u8e2a\u5668\u4e3aCAVE\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65e0\u675f\u7f1a\u7684\u8fd0\u52a8\u81ea\u7531\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u5e94\u7528\uff0c\u5982\u6821\u56ed\u6f2b\u6e38\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u9700\u6c42\u3002"}}
{"id": "2507.02180", "pdf": "https://arxiv.org/pdf/2507.02180", "abs": "https://arxiv.org/abs/2507.02180", "authors": ["Russell Beale"], "title": "The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future", "categories": ["cs.HC", "cs.CY", "H.5.0; K.3.1; K.3.2"], "comment": null, "summary": "Large language Models have only been widely available since 2022 and yet in\nless than three years have had a significant impact on approaches to education\nand educational technology. Here we review the domains in which they have been\nused, and discuss a variety of use cases, their successes and failures. We then\nprogress to discussing how this is changing the dynamic for learners and\neducators, consider the main design challenges facing LLMs if they are to\nbecome truly helpful and effective as educational systems, and reflect on the\nlearning paradigms they support. We make clear that the new interaction\nparadigms they bring are significant and argue that this approach will become\nso ubiquitous it will become the default way in which we interact with\ntechnologies, and revolutionise what people expect from computer systems in\ngeneral. This leads us to present some specific and significant considerations\nfor the design of educational technology in the future that are likely to be\nneeded to ensure acceptance by the changing expectations of learners and users.", "AI": {"tldr": "\u8bba\u6587\u6982\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea2022\u5e74\u5e7f\u6cdb\u53ef\u7528\u4ee5\u6765\u5bf9\u6559\u80b2\u548c\u6559\u80b2\u6280\u672f\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u8ba8\u8bba\u4e86\u5176\u5e94\u7528\u9886\u57df\u3001\u6210\u529f\u4e0e\u5931\u8d25\u7684\u6848\u4f8b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5bf9\u5b66\u4e60\u8005\u548c\u6559\u80b2\u8005\u52a8\u6001\u7684\u6539\u53d8\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86LLMs\u5728\u8bbe\u8ba1\u4e0a\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u9884\u6d4b\u5176\u5c06\u6210\u4e3a\u4e0e\u6280\u672f\u4ea4\u4e92\u7684\u9ed8\u8ba4\u65b9\u5f0f\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u672a\u6765\u6559\u80b2\u6280\u672f\u8bbe\u8ba1\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u9886\u57df\u7684\u5feb\u901f\u666e\u53ca\u53ca\u5176\u5bf9\u6559\u80b2\u6280\u672f\u548c\u4e92\u52a8\u65b9\u5f0f\u7684\u6df1\u8fdc\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0LLMs\u7684\u5e94\u7528\u9886\u57df\u548c\u4f7f\u7528\u6848\u4f8b\uff0c\u5206\u6790\u5176\u5bf9\u6559\u80b2\u52a8\u6001\u7684\u6539\u53d8\uff0c\u5e76\u8ba8\u8bba\u5176\u8bbe\u8ba1\u6311\u6218\u548c\u652f\u6301\u7684\u5b66\u4e60\u8303\u5f0f\u3002", "result": "LLMs\u5df2\u663e\u8457\u6539\u53d8\u6559\u80b2\u548c\u6559\u80b2\u6280\u672f\uff0c\u4f46\u5176\u8bbe\u8ba1\u4ecd\u9700\u6539\u8fdb\u4ee5\u6ee1\u8db3\u5b66\u4e60\u8005\u548c\u7528\u6237\u7684\u671f\u671b\u3002", "conclusion": "LLMs\u5c06\u6210\u4e3a\u4e0e\u6280\u672f\u4ea4\u4e92\u7684\u9ed8\u8ba4\u65b9\u5f0f\uff0c\u672a\u6765\u6559\u80b2\u6280\u672f\u8bbe\u8ba1\u9700\u8003\u8651\u5176\u5e26\u6765\u7684\u65b0\u4e92\u52a8\u8303\u5f0f\u548c\u7528\u6237\u671f\u671b\u3002"}}
{"id": "2507.02295", "pdf": "https://arxiv.org/pdf/2507.02295", "abs": "https://arxiv.org/abs/2507.02295", "authors": ["Roopkatha Banerjee", "Prince Modi", "Jinal Vyas", "Chunduru Sri Abhijit", "Tejus Chandrashekar", "Harsha Varun Marisetty", "Manik Gupta", "Yogesh Simmhan"], "title": "Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources", "categories": ["cs.DC"], "comment": null, "summary": "With the recent improvements in mobile and edge computing and rising concerns\nof data privacy, Federated Learning(FL) has rapidly gained popularity as a\nprivacy-preserving, distributed machine learning methodology. Several FL\nframeworks have been built for testing novel FL strategies. However, most focus\non validating the learning aspects of FL through pseudo-distributed simulation\nbut not for deploying on real edge hardware in a distributed manner to\nmeaningfully evaluate the federated aspects from a systems perspective. Current\nframeworks are also inherently not designed to support asynchronous\naggregation, which is gaining popularity, and have limited resilience to client\nand server failures. We introduce Flotilla, a scalable and lightweight FL\nframework. It adopts a ``user-first'' modular design to help rapidly compose\nvarious synchronous and asynchronous FL strategies while being agnostic to the\nDNN architecture. It uses stateless clients and a server design that separates\nout the session state, which are periodically or incrementally checkpointed. We\ndemonstrate the modularity of Flotilla by evaluating five different FL\nstrategies for training five DNN models. We also evaluate the client and\nserver-side fault tolerance on 200+ clients, and showcase its ability to\nrapidly failover within seconds. Finally, we show that Flotilla's resource\nusage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or\nbetter than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It\nalso scales significantly better compared to Flower for 1000+ clients. This\npositions Flotilla as a competitive candidate to build novel FL strategies on,\ncompare them uniformly, rapidly deploy them, and perform systems research and\noptimizations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86Flotilla\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u53ef\u6269\u5c55\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u5206\u5e03\u5f0f\u90e8\u7f72\u3001\u5f02\u6b65\u805a\u5408\u548c\u5bb9\u9519\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u548c\u8fb9\u7f18\u8ba1\u7b97\u7684\u8fdb\u6b65\u4ee5\u53ca\u5bf9\u6570\u636e\u9690\u79c1\u7684\u5173\u6ce8\uff0c\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4f5c\u4e3a\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53d7\u5230\u5e7f\u6cdb\u6b22\u8fce\u3002\u7136\u800c\uff0c\u73b0\u6709\u6846\u67b6\u591a\u7528\u4e8e\u4f2a\u5206\u5e03\u5f0f\u6a21\u62df\uff0c\u7f3a\u4e4f\u5bf9\u5b9e\u9645\u8fb9\u7f18\u786c\u4ef6\u90e8\u7f72\u7684\u652f\u6301\uff0c\u4e14\u4e0d\u652f\u6301\u5f02\u6b65\u805a\u5408\u548c\u5bb9\u9519\u6027\u3002", "method": "Flotilla\u91c7\u7528\u201c\u7528\u6237\u4f18\u5148\u201d\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u540c\u6b65\u548c\u5f02\u6b65FL\u7b56\u7565\uff0c\u4e14\u5bf9DNN\u67b6\u6784\u65e0\u5173\u3002\u5b83\u4f7f\u7528\u65e0\u72b6\u6001\u5ba2\u6237\u7aef\u548c\u5206\u79bb\u4f1a\u8bdd\u72b6\u6001\u7684\u670d\u52a1\u5668\u8bbe\u8ba1\uff0c\u5b9a\u671f\u6216\u589e\u91cf\u68c0\u67e5\u70b9\u3002", "result": "Flotilla\u5c55\u793a\u4e86\u5176\u6a21\u5757\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u4e94\u79cd\u4e0d\u540c\u7684FL\u7b56\u7565\u548c\u4e94\u79cdDNN\u6a21\u578b\u9a8c\u8bc1\u3002\u5728200\u591a\u4e2a\u5ba2\u6237\u7aef\u4e0a\u5c55\u793a\u5bb9\u9519\u6027\uff0c\u5e76\u5728\u8d44\u6e90\u4f7f\u7528\u4e0a\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u5982Flower\u3001OpenFL\u548cFedML\u3002", "conclusion": "Flotilla\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684FL\u6846\u67b6\uff0c\u9002\u5408\u6784\u5efa\u65b0\u578bFL\u7b56\u7565\u3001\u5feb\u901f\u90e8\u7f72\u548c\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\u4e0e\u4f18\u5316\u3002"}}
{"id": "2507.02376", "pdf": "https://arxiv.org/pdf/2507.02376", "abs": "https://arxiv.org/abs/2507.02376", "authors": ["Chung-ju Huang", "Ziqi Zhang", "Yinggui Wang", "Binghui Wang", "Tao Wei", "Leye Wang"], "title": "VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software", "categories": ["cs.SE", "cs.AI", "cs.DC"], "comment": null, "summary": "Vertical Federated Learning (VFL) is a distributed AI software deployment\nmechanism for cross-silo collaboration without accessing participants' data.\nHowever, existing VFL work lacks a mechanism to audit the execution correctness\nof the inference software of the data party. To address this problem, we design\na Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task\nparty to audit whether the data party's inference software is executed as\nexpected during large-scale inference without leaking the data privacy of the\ndata party or introducing additional latency to the inference system. The core\nof VeFIA is that the task party can use the inference results from a framework\nwith Trusted Execution Environments (TEE) and the coordinator to validate the\ncorrectness of the data party's computation results. VeFIA guarantees that, as\nlong as the abnormal inference exceeds 5.4%, the task party can detect\nexecution anomalies in the inference software with a probability of 99.99%,\nwithout incurring any additional online inference latency. VeFIA's random\nsampling validation achieves 100% positive predictive value, negative\npredictive value, and true positive rate in detecting abnormal inference. To\nthe best of our knowledge, this is the first paper to discuss the correctness\nof inference software execution in VFL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVeFIA\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5ba1\u6838\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u4e2d\u6570\u636e\u65b9\u7684\u63a8\u7406\u8f6f\u4ef6\u6267\u884c\u6b63\u786e\u6027\uff0c\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u4e14\u4e0d\u589e\u52a0\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709VFL\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6570\u636e\u65b9\u63a8\u7406\u8f6f\u4ef6\u6267\u884c\u6b63\u786e\u6027\u7684\u5ba1\u6838\u673a\u5236\u3002", "method": "\u8bbe\u8ba1VeFIA\u6846\u67b6\uff0c\u5229\u7528\u53ef\u4fe1\u6267\u884c\u73af\u5883\uff08TEE\uff09\u548c\u534f\u8c03\u8005\u9a8c\u8bc1\u6570\u636e\u65b9\u7684\u8ba1\u7b97\u7ed3\u679c\u3002", "result": "VeFIA\u80fd\u68c0\u6d4b5.4%\u4ee5\u4e0a\u7684\u5f02\u5e38\u63a8\u7406\uff0c\u51c6\u786e\u7387\u8fbe99.99%\uff0c\u4e14\u4e0d\u5f71\u54cd\u5728\u7ebf\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "VeFIA\u662f\u9996\u4e2a\u8ba8\u8bbaVFL\u4e2d\u63a8\u7406\u8f6f\u4ef6\u6267\u884c\u6b63\u786e\u6027\u7684\u65b9\u6848\uff0c\u9a8c\u8bc1\u6548\u679c\u4f18\u5f02\u3002"}}
{"id": "2507.02861", "pdf": "https://arxiv.org/pdf/2507.02861", "abs": "https://arxiv.org/abs/2507.02861", "authors": ["Zhening Huang", "Xiaoyang Wu", "Fangcheng Zhong", "Hengshuang Zhao", "Matthias Nie\u00dfner", "Joan Lasenby"], "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "Project Page: https://litereality.github.io; Video:\n  https://www.youtube.com/watch?v=ecK9m3LXg2c&feature=youtu.be", "summary": "We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor\nenvironments into compact, realistic, and interactive 3D virtual replicas.\nLiteReality not only reconstructs scenes that visually resemble reality but\nalso supports key features essential for graphics pipelines -- such as object\nindividuality, articulation, high-quality physically based rendering materials,\nand physically based interaction. At its core, LiteReality first performs scene\nunderstanding and parses the results into a coherent 3D layout and objects with\nthe help of a structured scene graph. It then reconstructs the scene by\nretrieving the most visually similar 3D artist-crafted models from a curated\nasset database. Next, the Material Painting module enhances realism by\nrecovering high-quality, spatially varying materials. Finally, the\nreconstructed scene is integrated into a simulation engine with basic physical\nproperties to enable interactive behavior. The resulting scenes are compact,\neditable, and fully compatible with standard graphics pipelines, making them\nsuitable for applications in AR/VR, gaming, robotics, and digital twins. In\naddition, LiteReality introduces a training-free object retrieval module that\nachieves state-of-the-art similarity performance on the Scan2CAD benchmark,\nalong with a robust material painting module capable of transferring\nappearances from images of any style to 3D assets -- even under severe\nmisalignment, occlusion, and poor lighting. We demonstrate the effectiveness of\nLiteReality on both real-life scans and public datasets. Project page:\nhttps://litereality.github.io; Video:\nhttps://www.youtube.com/watch?v=ecK9m3LXg2c", "AI": {"tldr": "LiteReality\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u53ef\u5c06\u5ba4\u5185\u73af\u5883\u7684RGB-D\u626b\u63cf\u8f6c\u6362\u4e3a\u7d27\u51d1\u3001\u903c\u771f\u4e14\u53ef\u4ea4\u4e92\u76843D\u865a\u62df\u590d\u5236\u54c1\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u548c\u7269\u7406\u4ea4\u4e92\u3002", "motivation": "\u5c06\u5ba4\u5185\u73af\u5883\u7684\u626b\u63cf\u6570\u636e\u8f6c\u6362\u4e3a\u903c\u771f\u4e14\u529f\u80fd\u9f50\u5168\u76843D\u865a\u62df\u573a\u666f\uff0c\u4ee5\u6ee1\u8db3AR/VR\u3001\u6e38\u620f\u3001\u673a\u5668\u4eba\u5b66\u548c\u6570\u5b57\u5b6a\u751f\u7b49\u5e94\u7528\u9700\u6c42\u3002", "method": "1. \u573a\u666f\u89e3\u6790\u4e3a3D\u5e03\u5c40\u548c\u5bf9\u8c61\u56fe\uff1b2. \u4ece\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u89c6\u89c9\u76f8\u4f3c\u76843D\u6a21\u578b\uff1b3. \u4f7f\u7528\u6750\u6599\u7ed8\u753b\u6a21\u5757\u589e\u5f3a\u771f\u5b9e\u611f\uff1b4. \u6574\u5408\u5230\u4eff\u771f\u5f15\u64ce\u4ee5\u652f\u6301\u4ea4\u4e92\u3002", "result": "\u751f\u6210\u7684\u573a\u666f\u7d27\u51d1\u3001\u53ef\u7f16\u8f91\u4e14\u5b8c\u5168\u517c\u5bb9\u6807\u51c6\u56fe\u5f62\u7ba1\u7ebf\uff0c\u5728Scan2CAD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u76f8\u4f3c\u6027\u6027\u80fd\u3002", "conclusion": "LiteReality\u4e0d\u4ec5\u63d0\u4f9b\u9ad8\u8d28\u91cf\u76843D\u91cd\u5efa\uff0c\u8fd8\u652f\u6301\u4ea4\u4e92\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.02319", "pdf": "https://arxiv.org/pdf/2507.02319", "abs": "https://arxiv.org/abs/2507.02319", "authors": ["Paolo Liberatore"], "title": "Iterated belief revision: from postulates to abilities", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "The belief revision field is opulent in new proposals and indigent in\nanalyses of existing approaches. Much work hinge on postulates, employed as\nsyntactic characterizations: some revision mechanism is equivalent to some\nproperties. Postulates constraint specific revision instances: certain\nrevisions update certain beliefs in a certain way. As an example, if the\nrevision is consistent with the current beliefs, it is incorporated with no\nother change. A postulate like this tells what revisions must do and neglect\nwhat they can do. Can they reach a certain state of beliefs? Can they reach all\npossible states of beliefs? Can they reach all possible states of beliefs from\nno previous belief? Can they reach a dogmatic state of beliefs, where\neverything not believed is impossible? Can they make two conditions equally\nbelieved? An application where every possible state of beliefs is sensible\nrequires each state of beliefs to be reachable. An application where conditions\nmay be equally believed requires such a belief state to be reachable. An\napplication where beliefs may become dogmatic requires a way to make them\ndogmatic. Such doxastic states need to be reached in a way or another. Not in\nspecific way, as dictated by a typical belief revision postulate. This is an\nability, not a constraint: the ability of being plastic, equating, dogmatic.\nAmnesic, correcting, believer, damascan, learnable are other abilities. Each\nrevision mechanism owns some of these abilities and lacks the others:\nlexicographic, natural, restrained, very radical, full meet, radical, severe,\nmoderate severe, deep severe, plain severe and deep severe revisions, each of\nthese revisions is proved to possess certain abilities.", "AI": {"tldr": "\u8bba\u6587\u6279\u8bc4\u4e86\u73b0\u6709\u4fe1\u5ff5\u4fee\u6b63\u7814\u7a76\u8fc7\u4e8e\u4f9d\u8d56\u540e\u9a8c\u5047\u8bbe\u800c\u5ffd\u89c6\u4e86\u4fee\u6b63\u673a\u5236\u7684\u7075\u6d3b\u6027\u5206\u6790\uff0c\u63d0\u51fa\u4fee\u6b63\u673a\u5236\u5e94\u5177\u5907\u591a\u79cd\u80fd\u529b\uff08\u5982\u53ef\u5851\u6027\u3001\u53ef\u5e73\u7b49\u5316\u3001\u6559\u6761\u5316\u7b49\uff09\uff0c\u5e76\u5217\u4e3e\u4e86\u4e00\u4e9b\u4fee\u6b63\u673a\u5236\u6240\u5177\u5907\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4fe1\u5ff5\u4fee\u6b63\u7814\u7a76\u8fc7\u4e8e\u4f9d\u8d56\u540e\u9a8c\u5047\u8bbe\uff0c\u800c\u5ffd\u89c6\u4e86\u4fee\u6b63\u673a\u5236\u7684\u7075\u6d3b\u6027\u548c\u80fd\u529b\u5206\u6790\u3002\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u4fee\u6b63\u673a\u5236\u5e94\u5177\u5907\u7684\u591a\u79cd\u80fd\u529b\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u5e94\u7528\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5217\u4e3e\u591a\u79cd\u4fee\u6b63\u673a\u5236\uff08\u5982\u8bcd\u5178\u5e8f\u3001\u81ea\u7136\u4fee\u6b63\u7b49\uff09\uff0c\u5206\u6790\u5b83\u4eec\u5404\u81ea\u5177\u5907\u6216\u7f3a\u4e4f\u7684\u80fd\u529b\uff08\u5982\u53ef\u5851\u6027\u3001\u6559\u6761\u5316\u7b49\uff09\uff0c\u4ece\u800c\u5c55\u793a\u4e0d\u540c\u4fee\u6b63\u673a\u5236\u7684\u7279\u70b9\u3002", "result": "\u7814\u7a76\u8868\u660e\u4e0d\u540c\u4fee\u6b63\u673a\u5236\u5177\u5907\u4e0d\u540c\u7684\u80fd\u529b\uff0c\u4f8b\u5982\u67d0\u4e9b\u673a\u5236\u53ef\u4ee5\u5b9e\u73b0\u6559\u6761\u5316\u6216\u5e73\u7b49\u5316\u4fe1\u5ff5\u72b6\u6001\uff0c\u800c\u5176\u4ed6\u673a\u5236\u5219\u7f3a\u4e4f\u8fd9\u4e9b\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u547c\u5401\u5728\u4fe1\u5ff5\u4fee\u6b63\u7814\u7a76\u4e2d\u4e0d\u4ec5\u8981\u5173\u6ce8\u540e\u9a8c\u5047\u8bbe\u7684\u7ea6\u675f\u6027\uff0c\u8fd8\u5e94\u91cd\u89c6\u4fee\u6b63\u673a\u5236\u7684\u7075\u6d3b\u6027\u80fd\u529b\uff0c\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2507.02013", "pdf": "https://arxiv.org/pdf/2507.02013", "abs": "https://arxiv.org/abs/2507.02013", "authors": ["Hao Liu", "Bo Yang", "Zhiwen Yu", "Xuelin Cao", "George C. Alexandropoulos", "Yan Zhang", "Chau Yuen"], "title": "AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "The Internet of Vehicles (IoV) transforms the transportation ecosystem\npromising pervasive connectivity and data-driven approaches. Deep learning and\ngenerative Artificial Intelligence (AI) have the potential to significantly\nenhance the operation of applications within IoV by facilitating efficient\ndecision-making and predictive capabilities, including intelligent navigation,\nvehicle safety monitoring, accident prevention, and intelligent traffic\nmanagement. Nevertheless, efficiently transmitting and processing the massive\nvolumes of data generated by the IoV in real-time remains a significant\nchallenge, particularly in dynamic and unpredictable wireless channel\nconditions. To address these challenges, this paper proposes a semantic\ncommunication framework based on channel perception to improve the accuracy and\nefficiency of data transmission. The semantic communication model extracts and\ncompresses the information to be transmitted. In addition, the wireless channel\nis estimated by using a generative diffusion model, which is employed to\npredict the dynamic channel states, thereby improving the quality of IoV\nservice. In dynamic scenarios, however, the channel estimation performance may\nbe degraded when substantially new scenarios take place, which will adversely\naffect user experience. To mitigate this limitation, we employ a large model to\nfine-tune the channel generation model to enhance its adaptability for varying\nscenarios. The performance and reliability of the proposed framework are\nevaluated on the two public datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u9053\u611f\u77e5\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u8f66\u8054\u7f51\uff08IoV\uff09\u4e2d\u6570\u636e\u4f20\u8f93\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u5229\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\u548c\u5927\u6a21\u578b\u4f18\u5316\u52a8\u6001\u4fe1\u9053\u4f30\u8ba1\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u8f66\u8054\u7f51\uff08IoV\uff09\u9700\u8981\u5b9e\u65f6\u5904\u7406\u6d77\u91cf\u6570\u636e\uff0c\u4f46\u5728\u52a8\u6001\u65e0\u7ebf\u4fe1\u9053\u6761\u4ef6\u4e0b\u5b58\u5728\u4f20\u8f93\u548c\u5904\u7406\u7684\u6311\u6218\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u63d0\u53d6\u5e76\u538b\u7f29\u4f20\u8f93\u4fe1\u606f\uff1b\u5229\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\u9884\u6d4b\u52a8\u6001\u4fe1\u9053\u72b6\u6001\uff0c\u5e76\u7ed3\u5408\u5927\u6a21\u578b\u8c03\u6574\u4ee5\u9002\u5e94\u65b0\u573a\u666f\u3002", "result": "\u6846\u67b6\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u6570\u636e\u4f20\u8f93\u548c\u4fe1\u9053\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f66\u8054\u7f51\u670d\u52a1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2507.02654", "pdf": "https://arxiv.org/pdf/2507.02654", "abs": "https://arxiv.org/abs/2507.02654", "authors": ["Rui Xie", "Asad Ul Haq", "Yunhua Fang", "Linsen Ma", "Sanchari Sen", "Swagath Venkataramani", "Liu Liu", "Tong Zhang"], "title": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure", "categories": ["cs.AR"], "comment": null, "summary": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6545\u969c\u7ba1\u7406\u8f6c\u79fb\u5230\u5185\u5b58\u63a7\u5236\u5668\uff0c\u5e76\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7684ECC\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4eHBM\u6210\u672c\uff0c\u540c\u65f6\u5728\u9ad8\u8bef\u7801\u7387\u4e0b\u4fdd\u6301\u6027\u80fd\u548c\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "HBM\u7684\u9ad8\u6210\u672c\u9650\u5236\u4e86\u5176\u5728AI\u9886\u57df\u7684\u53ef\u6269\u5c55\u90e8\u7f72\uff0c\u7279\u522b\u662f\u7531\u4e8e\u4e25\u683c\u7684\u7247\u4e0a\u53ef\u9760\u6027\u8981\u6c42\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u964d\u4f4e\u6210\u672c\u3002", "method": "\u91c7\u7528\u9886\u57df\u7279\u5b9a\u7684ECC\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u7801\u5b57Reed-Solomon\u7ea0\u9519\u3001\u8f7b\u91cf\u7ea7\u7ec6\u7c92\u5ea6CRC\u68c0\u6d4b\u3001\u5dee\u5f02\u5947\u5076\u66f4\u65b0\u4ee5\u51cf\u5c11\u5199\u5165\u653e\u5927\uff0c\u5e76\u6839\u636e\u6570\u636e\u91cd\u8981\u6027\u8c03\u6574\u4fdd\u62a4\u7ea7\u522b\u3002", "result": "\u5728\u9ad8\u8bef\u7801\u7387\uff08$10^{-3}$\uff09\u4e0b\uff0c\u7cfb\u7edf\u4ecd\u80fd\u4fdd\u755978%\u7684\u541e\u5410\u91cf\u548c97%\u7684\u6a21\u578b\u7cbe\u5ea6\uff0c\u63a5\u8fd1\u7406\u60f3\u65e0\u9519HBM\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53ef\u9760\u6027\u4f5c\u4e3a\u53ef\u8c03\u7cfb\u7edf\u53c2\u6570\u800c\u975e\u56fa\u5b9a\u786c\u4ef6\u7ea6\u675f\uff0c\u672c\u6587\u8bbe\u8ba1\u4e3a\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fdHBM\u5728AI\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.02186", "pdf": "https://arxiv.org/pdf/2507.02186", "abs": "https://arxiv.org/abs/2507.02186", "authors": ["Zahra Ashktorab", "Elizabeth M. Daly", "Erik Miehling", "Werner Geyer", "Martin Santillan Cooper", "Tejaswini Pedapati", "Michael Desmond", "Qian Pan", "Hyo Jin Do"], "title": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge", "categories": ["cs.HC"], "comment": null, "summary": "With the broad availability of large language models and their ability to\ngenerate vast outputs using varied prompts and configurations, determining the\nbest output for a given task requires an intensive evaluation process, one\nwhere machine learning practitioners must decide how to assess the outputs and\nthen carefully carry out the evaluation. This process is both time-consuming\nand costly. As practitioners work with an increasing number of models, they\nmust now evaluate outputs to determine which model and prompt performs best for\na given task. LLMs are increasingly used as evaluators to filter training data,\nevaluate model performance, assess harms and risks, or assist human evaluators\nwith detailed assessments. We present EvalAssist, a framework that simplifies\nthe LLM-as-a-judge workflow. The system provides an online criteria development\nenvironment, where users can interactively build, test, and share custom\nevaluation criteria in a structured and portable format. We support a set of\nLLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a\nprompt-chaining approach we developed and contributed to the UNITXT open-source\nlibrary. Additionally, our system also includes specially trained evaluators to\ndetect harms and risks in LLM outputs. We have deployed the system internally\nin our organization with several hundreds of users.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86EvalAssist\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u6784\u5efa\u548c\u6d4b\u8bd5\u8bc4\u4f30\u6807\u51c6\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc4\u4f30\u5176\u8f93\u51fa\u7684\u6700\u4f73\u65b9\u6cd5\u53d8\u5f97\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u7684\u5de5\u5177\u3002", "method": "EvalAssist\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728\u7ebf\u6807\u51c6\u5f00\u53d1\u73af\u5883\uff0c\u652f\u6301\u7528\u6237\u4ea4\u4e92\u5f0f\u6784\u5efa\u3001\u6d4b\u8bd5\u548c\u5171\u4eab\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u57fa\u4e8e\u5f00\u6e90\u5e93UNITXT\u5b9e\u73b0\u63d0\u793a\u94fe\u65b9\u6cd5\u3002", "result": "\u7cfb\u7edf\u5df2\u5728\u7ec4\u7ec7\u5185\u90e8\u90e8\u7f72\uff0c\u62e5\u6709\u6570\u767e\u540d\u7528\u6237\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "EvalAssist\u901a\u8fc7\u7b80\u5316\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u6d41\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u8bc4\u4f30\u6548\u7387\u3002"}}
{"id": "2507.02404", "pdf": "https://arxiv.org/pdf/2507.02404", "abs": "https://arxiv.org/abs/2507.02404", "authors": ["Maxime Martinasso", "Mark Klein", "Thomas C. Schulthess"], "title": "Alps, a versatile research infrastructure", "categories": ["cs.DC"], "comment": "10 pages, 6 figures, Cray User Group(CUG) 2025 Best Paper Award", "summary": "The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition\nof delivering top-tier high-performance computing systems, exemplified by the\nPiz Daint supercomputer. However, the increasing diversity of scientific needs\nhas exposed limitations in traditional vertically integrated HPC architectures,\nwhich often lack flexibility and composability. To address these challenges,\nCSCS developed Alps, a next-generation HPC infrastructure designed with a\ntransformative principle: resources operate as independent endpoints within a\nhigh-speed network. This architecture enables the creation of independent\ntenant-specific and platform-specific services, tailored to diverse scientific\nrequirements.\n  Alps incorporates heterogeneous hardware, including CPUs and GPUs,\ninterconnected by a high-performance Slingshot network, and offers a modular\nstorage system. A key innovation is the versatile software-defined cluster\n(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting\ninfrastructure, service management, and user environments into distinct layers,\nvClusters allow for customized platforms that support diverse workloads.\nCurrent platforms on Alps serve various scientific domains, including numerical\nweather prediction, and AI research.", "AI": {"tldr": "\u745e\u58eb\u56fd\u5bb6\u8d85\u7ea7\u8ba1\u7b97\u4e2d\u5fc3\uff08CSCS\uff09\u5f00\u53d1\u4e86\u65b0\u4e00\u4ee3\u9ad8\u6027\u80fd\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bdAlps\uff0c\u901a\u8fc7\u8d44\u6e90\u72ec\u7acb\u7aef\u70b9\u548c\u9ad8\u901f\u7f51\u7edc\u7684\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfHPC\u7f3a\u4e4f\u7075\u6d3b\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfHPC\u67b6\u6784\u5728\u79d1\u5b66\u9700\u6c42\u591a\u6837\u5316\u4e0b\u9762\u4e34\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u4fc3\u4f7fCSCS\u5f00\u53d1\u66f4\u9002\u5e94\u9700\u6c42\u7684Alps\u7cfb\u7edf\u3002", "method": "Alps\u91c7\u7528\u5f02\u6784\u786c\u4ef6\uff08CPU\u3001GPU\uff09\u548c\u9ad8\u901fSlingshot\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u8f6f\u4ef6\u5b9a\u4e49\u96c6\u7fa4\u6280\u672f\uff08vCluster\uff09\u5b9e\u73b0\u5206\u5c42\u7ba1\u7406\u3002", "result": "Alps\u6210\u529f\u652f\u6301\u4e86\u5305\u62ec\u6570\u503c\u5929\u6c14\u9884\u62a5\u548cAI\u7814\u7a76\u5728\u5185\u7684\u591a\u9886\u57df\u79d1\u5b66\u9700\u6c42\u3002", "conclusion": "Alps\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u7075\u6d3b\u67b6\u6784\u4e3a\u591a\u6837\u5316\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02533", "pdf": "https://arxiv.org/pdf/2507.02533", "abs": "https://arxiv.org/abs/2507.02533", "authors": ["Miguel Romero-Arjona", "Jos\u00e9 A. Parejo", "Juan C. Alonso", "Ana B. S\u00e1nchez", "Aitor Arrieta", "Sergio Segura"], "title": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Fairness--the absence of unjustified bias--is a core principle in the\ndevelopment of Artificial Intelligence (AI) systems, yet it remains difficult\nto assess and enforce. Current approaches to fairness testing in large language\nmodels (LLMs) often rely on manual evaluation, fixed templates, deterministic\nheuristics, and curated datasets, making them resource-intensive and difficult\nto scale. This work aims to lay the groundwork for a novel, automated method\nfor testing fairness in LLMs, reducing the dependence on domain-specific\nresources and broadening the applicability of current approaches. Our approach,\nMeta-Fair, is based on two key ideas. First, we adopt metamorphic testing to\nuncover bias by examining how model outputs vary in response to controlled\nmodifications of input prompts, defined by metamorphic relations (MRs). Second,\nwe propose exploiting the potential of LLMs for both test case generation and\noutput evaluation, leveraging their capability to generate diverse inputs and\nclassify outputs effectively. The proposal is complemented by three open-source\ntools supporting LLM-driven generation, execution, and evaluation of test\ncases. We report the findings of several experiments involving 12 pre-trained\nLLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.\nThe results show that Meta-Fair is effective in uncovering bias in LLMs,\nachieving an average precision of 92% and revealing biased behaviour in 29% of\nexecutions. Additionally, LLMs prove to be reliable and consistent evaluators,\nwith the best-performing models achieving F1-scores of up to 0.79. Although\nnon-determinism affects consistency, these effects can be mitigated through\ncareful MR design. While challenges remain to ensure broader applicability, the\nresults indicate a promising path towards an unprecedented level of automation\nin LLM testing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMeta-Fair\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u516c\u5e73\u6027\uff0c\u901a\u8fc7\u53d8\u5f62\u6d4b\u8bd5\u548cLLM\u7684\u81ea\u751f\u6210\u4e0e\u8bc4\u4f30\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\u5e76\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f53\u524dLLM\u516c\u5e73\u6027\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bc4\u4f30\u548c\u56fa\u5b9a\u6a21\u677f\uff0c\u8d44\u6e90\u5bc6\u96c6\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u9886\u57df\u7279\u5b9a\u8d44\u6e90\u7684\u4f9d\u8d56\u3002", "method": "Meta-Fair\u7ed3\u5408\u53d8\u5f62\u6d4b\u8bd5\uff08\u901a\u8fc7\u8f93\u5165\u63d0\u793a\u7684\u53d7\u63a7\u4fee\u6539\u53d1\u73b0\u504f\u5dee\uff09\u548cLLM\u7684\u751f\u6210\u4e0e\u8bc4\u4f30\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u652f\u6301\u6d4b\u8bd5\u6848\u4f8b\u7684\u751f\u6210\u4e0e\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u6d89\u53ca12\u4e2a\u9884\u8bad\u7ec3LLM\u548c14\u4e2a\u53d8\u5f62\u5173\u7cfb\uff0c\u663e\u793aMeta-Fair\u5728\u53d1\u73b0\u504f\u5dee\u65b9\u9762\u5e73\u5747\u7cbe\u5ea6\u8fbe92%\uff0c\u5e76\u63ed\u793a29%\u7684\u6267\u884c\u5b58\u5728\u504f\u5dee\u884c\u4e3a\u3002LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u8868\u73b0\u53ef\u9760\u3002", "conclusion": "Meta-Fair\u4e3aLLM\u516c\u5e73\u6027\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u81ea\u52a8\u5316\u9014\u5f84\uff0c\u5c3d\u7ba1\u5b58\u5728\u975e\u786e\u5b9a\u6027\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u53ef\u7f13\u89e3\u3002\u7ed3\u679c\u663e\u793a\u4e86\u81ea\u52a8\u5316\u6d4b\u8bd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.02585", "pdf": "https://arxiv.org/pdf/2507.02585", "abs": "https://arxiv.org/abs/2507.02585", "authors": ["Fabian Kresse", "Emily Yu", "Christoph H. Lampert"], "title": "Scalable Interconnect Learning in Boolean Networks", "categories": ["cs.LG", "cs.LO"], "comment": "12 pages, 8 Figures", "summary": "Learned Differentiable Boolean Logic Networks (DBNs) already deliver\nefficient inference on resource-constrained hardware. We extend them with a\ntrainable, differentiable interconnect whose parameter count remains constant\nas input width grows, allowing DBNs to scale to far wider layers than earlier\nlearnable-interconnect designs while preserving their advantageous accuracy. To\nfurther reduce model size, we propose two complementary pruning stages: an\nSAT-based logic equivalence pass that removes redundant gates without affecting\nperformance, and a similarity-based, data-driven pass that outperforms a\nmagnitude-style greedy baseline and offers a superior compression-accuracy\ntrade-off.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u53ef\u5fae\u5206\u5e03\u5c14\u903b\u8f91\u7f51\u7edc\uff08DBNs\uff09\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u4e14\u53c2\u6570\u56fa\u5b9a\u7684\u4e92\u8054\u7ed3\u6784\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u66f4\u5bbd\u7684\u8f93\u5165\u5c42\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u4e92\u8865\u7684\u526a\u679d\u65b9\u6cd5\u8fdb\u4e00\u6b65\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u3002", "motivation": "\u73b0\u6709\u7684DBNs\u5728\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u4e0a\u5df2\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u6269\u5c55\u5176\u9002\u7528\u8303\u56f4\u5e76\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u3002", "method": "\u6269\u5c55DBNs\u7684\u53ef\u8bad\u7ec3\u4e92\u8054\u7ed3\u6784\uff0c\u4f7f\u5176\u53c2\u6570\u56fa\u5b9a\u4e14\u9002\u7528\u4e8e\u66f4\u5bbd\u7684\u8f93\u5165\u5c42\uff1b\u63d0\u51fa\u4e24\u79cd\u526a\u679d\u65b9\u6cd5\uff1a\u57fa\u4e8e\u903b\u8f91\u7b49\u4ef7\u6027\u7684SAT\u526a\u679d\u548c\u6570\u636e\u9a71\u52a8\u7684\u76f8\u4f3c\u6027\u526a\u679d\u3002", "result": "\u6539\u8fdb\u540e\u7684DBNs\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u80fd\u591f\u6269\u5c55\u5230\u66f4\u5bbd\u7684\u8f93\u5165\u5c42\uff0c\u5e76\u901a\u8fc7\u526a\u679d\u65b9\u6cd5\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aDBNs\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u538b\u7f29\u7684\u65b0\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2507.02021", "pdf": "https://arxiv.org/pdf/2507.02021", "abs": "https://arxiv.org/abs/2507.02021", "authors": ["Eyad Gad", "Gad Gad", "Mostafa M. Fouda", "Mohamed I. Ibrahem", "Muhammad Ismail", "Zubair Md Fadlullah"], "title": "REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks", "categories": ["cs.NI"], "comment": "2025 International Conference on Communications", "summary": "With the rise of Software-Defined Networking (SDN) for managing traffic and\nensuring seamless operations across interconnected devices, challenges arise\nwhen SDN controllers share infrastructure with deep learning (DL) workloads.\nResource contention between DL training and SDN operations, especially in\nlatency-sensitive IoT environments, can degrade SDN's responsiveness and\ncompromise network performance. Federated Learning (FL) helps address some of\nthese concerns by decentralizing DL training to edge devices, thus reducing\ndata transmission costs and enhancing privacy. Yet, the computational demands\nof DL training can still interfere with SDN's performance, especially under the\ncontinuous data streams characteristic of IoT systems. To mitigate this issue,\nwe propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),\na resampling technique that optimizes DL training by prioritizing misclassified\nsamples and excluding redundant data, inspired by AdaBoost. REDUS reduces the\nnumber of training samples per epoch, thereby conserving computational\nresources, reducing energy consumption, and accelerating convergence without\nsignificantly impacting accuracy. Applied within an FL setup, REDUS enhances\nthe efficiency of model training on resource-limited edge devices while\nmaintaining network performance. In this paper, REDUS is evaluated on the\nCICIoT2023 dataset for IoT attack detection, showing a training time reduction\nof up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable\nand practical solution for intelligent networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREDUS\u7684\u91cd\u91c7\u6837\u6280\u672f\uff0c\u7528\u4e8e\u5728\u667a\u80fd\u7f51\u7edc\u4e2d\u4f18\u5316\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\uff0c\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u548c\u80fd\u91cf\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740SDN\u548c\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u5171\u4eab\u57fa\u7840\u8bbe\u65bd\uff0c\u8d44\u6e90\u7ade\u4e89\u5bfc\u81f4\u7f51\u7edc\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u5ef6\u8fdf\u654f\u611f\u7684\u7269\u8054\u7f51\u73af\u5883\u4e2d\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u8bc1\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u4e0d\u5e72\u6270SDN\u7684\u8fd0\u884c\u3002", "method": "\u63d0\u51faREDUS\u6280\u672f\uff0c\u901a\u8fc7\u4f18\u5148\u5904\u7406\u8bef\u5206\u7c7b\u6837\u672c\u548c\u6392\u9664\u5197\u4f59\u6570\u636e\u6765\u4f18\u5316\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\uff0c\u57fa\u4e8eAdaBoost\u542f\u53d1\u3002", "result": "\u5728CICIoT2023\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cREDUS\u51cf\u5c11\u4e8672.6%\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u4ec5\u635f\u59311.62%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "REDUS\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2507.02187", "pdf": "https://arxiv.org/pdf/2507.02187", "abs": "https://arxiv.org/abs/2507.02187", "authors": ["Xiyuxing Zhang", "Duc Vu", "Chengyi Shen", "Yuntao Wang", "Yuanchun Shi", "Justin Chan"], "title": "VergeIO: Depth-Aware Eye Interaction on Glasses", "categories": ["cs.HC"], "comment": null, "summary": "There is growing industry interest in creating unobtrusive designs for\nelectrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and\nApple eyewear). We present VergeIO, the first EOG-based glasses that enables\ndepth-aware eye interaction using vergence with an optimized electrode layout\nand novel smart glass prototype. It can distinguish between four and six\ndepth-based eye gestures with 83-98% accuracy using personalized models in a\nuser study across 11 users and 1,320 gesture instances. It generalizes to\nunseen users with an accuracy of 80-98% without any calibration. To reduce\nfalse detections, we incorporate a motion artifact detection pipeline and a\npreamble-based activation scheme. The system uses dry sensors without any\nadhesives or gel, and operates in real time with 3 mW power consumption by the\nsensing front-end, making it suitable for always-on sensing.", "AI": {"tldr": "VergeIO\u662f\u4e00\u79cd\u57fa\u4e8eEOG\u6280\u672f\u7684\u773c\u955c\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u7535\u6781\u5e03\u5c40\u548c\u65b0\u578b\u667a\u80fd\u773c\u955c\u539f\u578b\u5b9e\u73b0\u6df1\u5ea6\u611f\u77e5\u773c\u52a8\u4ea4\u4e92\uff0c\u652f\u6301\u9ad8\u7cbe\u5ea6\u624b\u52bf\u8bc6\u522b\u3002", "motivation": "\u884c\u4e1a\u5bf9\u773c\u955c\u4e0a\u65e0\u5e72\u6270EOG\u4f20\u611f\u8bbe\u8ba1\u7684\u9700\u6c42\u589e\u957f\uff0cVergeIO\u65e8\u5728\u5b9e\u73b0\u6df1\u5ea6\u611f\u77e5\u773c\u52a8\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u4f18\u5316\u7684\u7535\u6781\u5e03\u5c40\u548c\u65b0\u578b\u667a\u80fd\u773c\u955c\u539f\u578b\uff0c\u7ed3\u5408\u4e2a\u6027\u5316\u6a21\u578b\u548c\u8fd0\u52a8\u4f2a\u5f71\u68c0\u6d4b\u6280\u672f\u3002", "result": "\u572811\u540d\u7528\u6237\u548c1,320\u4e2a\u624b\u52bf\u5b9e\u4f8b\u4e2d\uff0c\u8bc6\u522b\u51c6\u786e\u7387\u8fbe83-98%\uff1b\u65b0\u7528\u6237\u65e0\u6821\u51c6\u4e0b\u51c6\u786e\u7387\u4e3a80-98%\u3002", "conclusion": "VergeIO\u4ee5\u4f4e\u529f\u8017\u5b9e\u73b0\u5b9e\u65f6\u65e0\u5e72\u6270\u773c\u52a8\u4ea4\u4e92\uff0c\u9002\u5408\u6301\u7eed\u4f20\u611f\u5e94\u7528\u3002"}}
{"id": "2507.02620", "pdf": "https://arxiv.org/pdf/2507.02620", "abs": "https://arxiv.org/abs/2507.02620", "authors": ["Xing Liu", "Lizhuo Luo", "Ming Tang", "Chao Huang"], "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference", "categories": ["cs.DC", "cs.AI"], "comment": "16 pages, and the last 3 are appendix", "summary": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.36$\\times$-1.77$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}", "AI": {"tldr": "FlowSpec\u662f\u4e00\u79cd\u57fa\u4e8e\u7ba1\u9053\u5e76\u884c\u548c\u6811\u72b6\u63a8\u6d4b\u89e3\u7801\u7684\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5206\u5e03\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u5728\u8fb9\u7f18\u7f51\u7edc\u7684\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5728\u8fb9\u7f18\u7f51\u7edc\u4e2d\u63a8\u65ad\u8bf7\u6c42\u7a00\u758f\u65f6\u73b0\u6709\u7ba1\u9053\u5e76\u884c\u65b9\u6cd5\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc71)\u57fa\u4e8e\u5206\u6570\u7684\u9010\u6b65\u9a8c\u8bc1\u4f18\u5148\u91cd\u8981\u8349\u6848\u6807\u8bb0\uff1b2)\u6709\u6548\u7ba1\u7406\u8349\u6848\u4ee5\u4fee\u526a\u65e0\u6548\u6807\u8bb0\uff1b3)\u52a8\u6001\u6269\u5c55\u7b56\u7565\u63d0\u4f9b\u9ad8\u8d28\u91cf\u63a8\u6d4b\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlowSpec\u5728\u591a\u6837\u6a21\u578b\u548c\u914d\u7f6e\u4e0b\u663e\u8457\u63d0\u5347\u63a8\u65ad\u901f\u5ea6\uff0c\u6bd4\u57fa\u7ebf\u5feb1.36\u00d7-1.77\u00d7\u3002", "conclusion": "FlowSpec\u6709\u6548\u63d0\u5347\u7ba1\u9053\u5229\u7528\u7387\u548c\u63a8\u6d4b\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0fLLM\u63a8\u65ad\u3002"}}
{"id": "2507.02564", "pdf": "https://arxiv.org/pdf/2507.02564", "abs": "https://arxiv.org/abs/2507.02564", "authors": ["Alexander Korn", "Samuel Gorsch", "Andreas Vogelsang"], "title": "LLMREI: Automating Requirements Elicitation Interviews with LLMs", "categories": ["cs.SE"], "comment": null, "summary": "Requirements elicitation interviews are crucial for gathering system\nrequirements but heavily depend on skilled analysts, making them\nresource-intensive, susceptible to human biases, and prone to miscommunication.\nRecent advancements in Large Language Models present new opportunities for\nautomating parts of this process. This study introduces LLMREI, a chat bot\ndesigned to conduct requirements elicitation interviews with minimal human\nintervention, aiming to reduce common interviewer errors and improve the\nscalability of requirements elicitation. We explored two main approaches,\nzero-shot prompting and least-to-most prompting, to optimize LLMREI for\nrequirements elicitation and evaluated its performance in 33 simulated\nstakeholder interviews. A third approach, fine-tuning, was initially considered\nbut abandoned due to poor performance in preliminary trials. Our study assesses\nthe chat bot's effectiveness in three key areas: minimizing common interview\nerrors, extracting relevant requirements, and adapting its questioning based on\ninterview context and user responses. Our findings indicate that LLMREI makes a\nsimilar number of errors compared to human interviewers, is capable of\nextracting a large portion of requirements, and demonstrates a notable ability\nto generate highly context-dependent questions. We envision the greatest\nbenefit of LLMREI in automating interviews with a large number of stakeholders.", "AI": {"tldr": "LLMREI\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u9700\u6c42\u83b7\u53d6\u9762\u8bd5\uff0c\u51cf\u5c11\u4eba\u4e3a\u9519\u8bef\u5e76\u63d0\u5347\u6548\u7387\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u4e24\u79cd\u63d0\u793a\u65b9\u6cd5\uff0c\u5e76\u5728\u6a21\u62df\u9762\u8bd5\u4e2d\u8bc4\u4f30\u4e86\u5176\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u9700\u6c42\u83b7\u53d6\u9762\u8bd5\u4f9d\u8d56\u5206\u6790\u5e08\uff0c\u6210\u672c\u9ad8\u4e14\u6613\u51fa\u9519\u3002\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u53ef\u4ee5\u63d0\u5347\u6548\u7387\u548c\u89c4\u6a21\u3002", "method": "\u8bbe\u8ba1\u4e86LLMREI\u673a\u5668\u4eba\uff0c\u6d4b\u8bd5\u4e86\u96f6\u6837\u672c\u63d0\u793a\u548c\u9010\u6b65\u63d0\u793a\u4e24\u79cd\u65b9\u6cd5\uff0c\u653e\u5f03\u4e86\u5fae\u8c03\u65b9\u6cd5\u3002\u572833\u6b21\u6a21\u62df\u9762\u8bd5\u4e2d\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "result": "LLMREI\u7684\u9519\u8bef\u7387\u4e0e\u4eba\u7c7b\u5206\u6790\u5e08\u76f8\u5f53\uff0c\u80fd\u9ad8\u6548\u63d0\u53d6\u9700\u6c42\uff0c\u5e76\u80fd\u6839\u636e\u4e0a\u4e0b\u6587\u751f\u6210\u76f8\u5173\u95ee\u9898\u3002", "conclusion": "LLMREI\u9002\u7528\u4e8e\u5927\u91cf\u5229\u76ca\u76f8\u5173\u8005\u7684\u81ea\u52a8\u5316\u9700\u6c42\u83b7\u53d6\uff0c\u63d0\u5347\u6548\u7387\u5e76\u51cf\u5c11\u4eba\u4e3a\u9519\u8bef\u3002"}}
{"id": "2507.02721", "pdf": "https://arxiv.org/pdf/2507.02721", "abs": "https://arxiv.org/abs/2507.02721", "authors": ["Jan Friso Groote", "Matthias Volk"], "title": "A formal specification of the desired software behaviour of the Princess Marijke lock complex", "categories": ["eess.SY", "cs.LO", "cs.SY"], "comment": null, "summary": "The Princess Marijke lock complex is a large lock and water-protection\ninstallation in the Netherlands between the river Rhine and the\nAmsterdam-Rijnkanaal -- a large waterway connecting the Rhine to the port of\nAmsterdam. The lock complex consists of two independent locks and a moveable\nflood-protection barrier. Ensuring safe control of the lock complex is of\nutmost importance to guarantee both flood-protection and reliable ship\noperations. This paper gives a precise, formal description of the software\ncontrol of the lock complex in less than 400 lines of mCRL2 code. This\ndescription can act as a blueprint on how the software of this lock complex\nneeds to be constructed. Moreover, using model checking, 53 software\nrequirements are shown to be valid, ensuring that the formal description of the\nbehaviour is correct with regard to these properties and is unlikely to contain\nmistakes and oversights.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5c11\u4e8e400\u884c\u7684mCRL2\u4ee3\u7801\uff0c\u7cbe\u786e\u63cf\u8ff0\u4e86\u8377\u5170Princess Marijke\u9501\u590d\u5408\u4f53\u7684\u8f6f\u4ef6\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u68c0\u67e5\u9a8c\u8bc1\u4e8653\u4e2a\u8f6f\u4ef6\u9700\u6c42\u7684\u6b63\u786e\u6027\uff0c\u786e\u4fdd\u5176\u884c\u4e3a\u63cf\u8ff0\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u786e\u4fdd\u8fd9\u4e00\u91cd\u8981\u6c34\u5229\u8bbe\u65bd\u7684\u8f6f\u4ef6\u63a7\u5236\u5b89\u5168\u53ef\u9760\uff0c\u4ee5\u4fdd\u969c\u9632\u6d2a\u548c\u8239\u8236\u64cd\u4f5c\u7684\u6b63\u5e38\u8fd0\u884c\u3002", "method": "\u4f7f\u7528mCRL2\u5f62\u5f0f\u5316\u63cf\u8ff0\u8bed\u8a00\uff0c\u7f16\u5199\u5c11\u4e8e400\u884c\u7684\u4ee3\u7801\u6765\u5b9a\u4e49\u9501\u590d\u5408\u4f53\u7684\u63a7\u5236\u903b\u8f91\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u68c0\u67e5\u9a8c\u8bc1\u5176\u6b63\u786e\u6027\u3002", "result": "\u9a8c\u8bc1\u4e8653\u4e2a\u8f6f\u4ef6\u9700\u6c42\u7684\u6b63\u786e\u6027\uff0c\u8bc1\u660e\u5f62\u5f0f\u5316\u63cf\u8ff0\u7684\u884c\u4e3a\u7b26\u5408\u9884\u671f\u3002", "conclusion": "\u8be5\u5f62\u5f0f\u5316\u63cf\u8ff0\u53ef\u4f5c\u4e3a\u9501\u590d\u5408\u4f53\u8f6f\u4ef6\u6784\u5efa\u7684\u84dd\u56fe\uff0c\u786e\u4fdd\u5176\u8bbe\u8ba1\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.02613", "pdf": "https://arxiv.org/pdf/2507.02613", "abs": "https://arxiv.org/abs/2507.02613", "authors": ["Yalin E. Sagduyu", "Kemal Davaslioglu", "Tugba Erpek", "Sastry Kompella", "Gustave Anderson", "Jonathan Ashdown"], "title": "MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking", "categories": ["cs.NI", "cs.DC", "eess.SP"], "comment": null, "summary": "This paper presents a complete signal-processing chain for multistatic\nintegrated sensing and communications (ISAC) using 5G Positioning Reference\nSignal (PRS). We consider a distributed architecture in which one gNB transmits\na periodic OFDM-PRS waveform while multiple spatially separated receivers\nexploit the same signal for target detection, parameter estimation and\ntracking. A coherent cross-ambiguity function (CAF) is evaluated to form a\nrange-Doppler map from which the bistatic delay and radial velocity are\nextracted for every target. For a single target, the resulting bistatic delays\nare fused through nonlinear least-squares trilateration, yielding a geometric\nposition estimate, and a regularized linear inversion of the radial-speed\nequations yields a two-dimensional velocity vector, where speed and heading are\nobtained. The approach is applied to 2D and 3D settings, extended to account\nfor time synchronization bias, and generalized to multiple targets by resolving\ntarget association. The sequence of position-velocity estimates is then fed to\nstandard and extended Kalman filters to obtain smoothed tracks. Our results\nshow high-fidelity moving-target detection, positioning, and tracking using 5G\nPRS signals for multistatic ISAC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75285G\u5b9a\u4f4d\u53c2\u8003\u4fe1\u53f7\uff08PRS\uff09\u7684\u591a\u9759\u6001\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u5b8c\u6574\u4fe1\u53f7\u5904\u7406\u94fe\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u67b6\u6784\u5b9e\u73b0\u76ee\u6807\u68c0\u6d4b\u3001\u53c2\u6570\u4f30\u8ba1\u548c\u8ddf\u8e2a\u3002", "motivation": "\u901a\u8fc75G PRS\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7684\u79fb\u52a8\u76ee\u6807\u68c0\u6d4b\u3001\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\uff0c\u4ee5\u652f\u6301\u591a\u9759\u6001ISAC\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u901a\u8fc7\u76f8\u5e72\u4ea4\u53c9\u6a21\u7cca\u51fd\u6570\uff08CAF\uff09\u751f\u6210\u8ddd\u79bb-\u591a\u666e\u52d2\u56fe\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u4e09\u8fb9\u6d4b\u91cf\u548c\u6b63\u5219\u5316\u7ebf\u6027\u53cd\u6f14\u65b9\u6cd5\u8fdb\u884c\u76ee\u6807\u5b9a\u4f4d\u4e0e\u901f\u5ea6\u4f30\u8ba1\uff0c\u5e76\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5e73\u6ed1\u8ddf\u8e2a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u79fb\u52a8\u76ee\u6807\u68c0\u6d4b\u3001\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u9759\u6001ISAC\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u4fe1\u53f7\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.02660", "pdf": "https://arxiv.org/pdf/2507.02660", "abs": "https://arxiv.org/abs/2507.02660", "authors": ["Deepak Narayan Gadde", "Keerthan Kopparam Radhakrishna", "Vaisakh Naduvodi Viswambharan", "Aman Kumar", "Djones Lettnin", "Wolfgang Kunz", "Sebastian Simon"], "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification", "categories": ["cs.AI", "cs.AR"], "comment": "To appear at the 38th SBC/SBMicro/IEEE Symposium on Integrated\n  Circuits and Systems Design (SBCCI), August 25-29, 2025, Manaus, BRAZIL", "summary": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u4ee3\u7406\u7684\u786c\u4ef6\u8bbe\u8ba1\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u5e72\u9884\uff08HITL\uff09\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u8fed\u4ee3\u7684\u7aef\u5230\u7aef\u9a8c\u8bc1\uff0c\u9a8c\u8bc1\u5f00\u6e90\u8bbe\u8ba1\u65f6\u8986\u76d6\u7387\u8d85\u8fc795%\uff0c\u4e14\u65f6\u95f4\u7f29\u77ed\u3002", "motivation": "\u73b0\u4ee3\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u590d\u6742\u5ea6\u9ad8\uff0c\u9a8c\u8bc1\u8fc7\u7a0b\u8017\u65f6\u4e14\u9700\u4e25\u8c28\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u786c\u4ef6\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u91c7\u7528AI\u4ee3\u7406\u4e0e\u4eba\u7c7b\u534f\u4f5c\uff08HITL\uff09\u7684\u52a8\u6001\u8fed\u4ee3\u65b9\u6cd5\uff0c\u8fdb\u884c\u7aef\u5230\u7aef\u786c\u4ef6\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\u3002", "result": "\u5728\u4e94\u79cd\u5f00\u6e90\u8bbe\u8ba1\u4e2d\uff0c\u8986\u76d6\u7387\u8fbe95%\u4ee5\u4e0a\uff0c\u540c\u65f6\u51cf\u5c11\u9a8c\u8bc1\u65f6\u95f4\uff0c\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\u3001\u9002\u5e94\u6027\u548c\u53ef\u914d\u7f6e\u6027\u3002", "conclusion": "AI\u4ee3\u7406\u7ed3\u5408\u4eba\u7c7b\u5e72\u9884\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u786c\u4ef6\u9a8c\u8bc1\u6548\u7387\u4e0e\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.02229", "pdf": "https://arxiv.org/pdf/2507.02229", "abs": "https://arxiv.org/abs/2507.02229", "authors": ["Sifatul Anindho", "Videep Venkatesha", "Mariah Bradford", "Anne M. Cleary", "Nathaniel Blanchard"], "title": "An Exploration of Internal States in Collaborative Problem Solving", "categories": ["cs.HC"], "comment": "Accepted to the International Conference on Human-Computer\n  Interaction (HCII) 2025", "summary": "Collaborative problem solving (CPS) is a complex cognitive, social, and\nemotional process that is increasingly prevalent in educational and\nprofessional settings. This study investigates the emotional states of\nindividuals during CPS using a mixed-methods approach. Teams of four first\ncompleted a novel CPS task. Immediately after, each individual was placed in an\nisolated room where they reviewed the video of their group performing the task\nand self-reported their internal experiences throughout the task. We performed\na linguistic analysis of these internal monologues, providing insights into the\nrange of emotions individuals experience during CPS. Our analysis showed\ndistinct patterns in language use, including characteristic unigrams and\nbigrams, key words and phrases, emotion labels, and semantic similarity between\nemotion-related words.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u63a2\u8ba8\u4e86\u534f\u4f5c\u95ee\u9898\u89e3\u51b3(CPS)\u8fc7\u7a0b\u4e2d\u4e2a\u4f53\u7684\u60c5\u7eea\u72b6\u6001\uff0c\u901a\u8fc7\u89c6\u9891\u56de\u987e\u548c\u81ea\u6211\u62a5\u544a\u5206\u6790\u4e86\u60c5\u7eea\u7684\u8bed\u8a00\u8868\u8fbe\u7279\u5f81\u3002", "motivation": "\u7814\u7a76CPS\u4e2d\u60c5\u7eea\u72b6\u6001\u7684\u91cd\u8981\u6027\uff0c\u56e0\u4e3aCPS\u5728\u6559\u80b2\u548c\u804c\u4e1a\u73af\u5883\u4e2d\u8d8a\u6765\u8d8a\u666e\u904d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u56e2\u961f\u5b8c\u6210CPS\u4efb\u52a1\u540e\uff0c\u4e2a\u4f53\u901a\u8fc7\u89c6\u9891\u56de\u987e\u548c\u81ea\u6211\u62a5\u544a\u5176\u5185\u90e8\u4f53\u9a8c\uff0c\u5e76\u8fdb\u884c\u8bed\u8a00\u5206\u6790\u3002", "result": "\u8bed\u8a00\u5206\u6790\u63ed\u793a\u4e86\u60c5\u7eea\u76f8\u5173\u7684\u7279\u5b9a\u7528\u8bcd\u6a21\u5f0f\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86CPS\u8fc7\u7a0b\u4e2d\u4e2a\u4f53\u60c5\u7eea\u7684\u8bed\u8a00\u8868\u8fbe\u7279\u5f81\uff0c\u4e3a\u7406\u89e3\u590d\u6742\u534f\u4f5c\u4e2d\u7684\u60c5\u7eea\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.02578", "pdf": "https://arxiv.org/pdf/2507.02578", "abs": "https://arxiv.org/abs/2507.02578", "authors": ["Zoe Pfister"], "title": "Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems", "categories": ["cs.SE", "cs.HC", "D.2.1"], "comment": "Copyright 2025 IEEE. Accepted for publication in: 2025 IEEE 33nd\n  International Requirements Engineering Conference (RE), Doctor Symposium\n  Paper, 5 pages", "summary": "Adaptive Cyber-Physical Systems (CPS) are systems that integrate both\nphysical and computational capabilities, which can adjust in response to\nchanging parameters. Furthermore, they increasingly incorporate human-machine\ncollaboration, allowing them to benefit from the individual strengths of humans\nand machines. Human-Machine Teaming (HMT) represents the most advanced paradigm\nof human-machine collaboration, envisioning seamless teamwork between humans\nand machines. However, achieving effective and seamless HMT in adaptive CPS is\nchallenging. While adaptive CPS already benefit from feedback loops such as\nMAPE-K, there is still a gap in integrating humans into these feedback loops\ndue to different operational cadences of humans and machines. Further, HMT\nrequires constant monitoring of human operators, collecting potentially\nsensitive information about their actions and behavior. Respecting the privacy\nand human values of the actors of the CPS is crucial for the success of\nhuman-machine teams. This research addresses these challenges by: (1)\ndeveloping novel methods and processes for integrating HMT into adaptive CPS,\nfocusing on human-machine interaction principles and their incorporation into\nadaptive feedback loops found in CPS, and (2) creating frameworks for\nintegrating, verifying, and validating ethics and human values throughout the\nsystem lifecycle, starting from requirements engineering.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u9002\u5e94\u6027\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\uff08HMT\uff09\uff0c\u63d0\u51fa\u4e86\u6574\u5408HMT\u7684\u65b0\u65b9\u6cd5\u548c\u4f26\u7406\u6846\u67b6\u3002", "motivation": "\u9002\u5e94\u6027CPS\u4e2d\u5b9e\u73b0\u65e0\u7f1d\u4eba\u673a\u534f\u4f5c\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5982\u4f55\u5c06\u4eba\u7c7b\u64cd\u4f5c\u8005\u6574\u5408\u5230\u53cd\u9988\u5faa\u73af\u4e2d\u5e76\u4fdd\u62a4\u5176\u9690\u79c1\u548c\u4ef7\u503c\u89c2\u3002", "method": "\u5f00\u53d1\u4e86\u6574\u5408HMT\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4ece\u9700\u6c42\u5de5\u7a0b\u5f00\u59cb\u7684\u4f26\u7406\u548c\u4eba\u7c7b\u4ef7\u503c\u89c2\u9a8c\u8bc1\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u89e3\u51b3CPS\u4e2d\u4eba\u673a\u534f\u4f5c\u969c\u788d\u548c\u4f26\u7406\u95ee\u9898\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u9002\u5e94\u6027CPS\u4e2d\u7684HMT\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u652f\u6301\uff0c\u5f3a\u8c03\u4f26\u7406\u548c\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.02680", "pdf": "https://arxiv.org/pdf/2507.02680", "abs": "https://arxiv.org/abs/2507.02680", "authors": ["Jorge Baranda", "Marius Caus", "Luis Blanco", "Cristian J. Vaca-Rubio", "Engin Zeydan", "Kapal Dev", "Zheng Li", "Tomaso DeCola"], "title": "On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks", "categories": ["cs.NI"], "comment": "20 pages, 7 figures", "summary": "The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks\n(NTNs) poses unique architectural and functional challenges due to\nheterogeneous propagation conditions, dynamic topologies and limited on-board\nprocessing capabilities. This paper examines architectural and functional split\nstrategies that are consistent with O-RAN principles for future integrated\nTN-NTN systems. A taxonomy of split options is proposed that distributes RAN\nand core functions between satellites and ground nodes, and trade-offs in terms\nof performance, latency, autonomy and deployment are analysed. In particular,\nwe evaluate configurations ranging from pure on-board DU deployments to full\ngNB and UPF integration into satellites, including variations based on intra-\nand inter-satellite processing. In addition, the placement of Near-RT and\nNon-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible\nsplit strategies between space and ground to optimise the performance and\nscalability of the control loop. A comprehensive mapping between architectural\nsplits and RIC placement options is provided, emphasising implementation\nconstraints and interoperability considerations. The paper concludes by\nidentifying key challenges and outlining future directions to enable\nstandardised, modular and efficient TN-NTN convergence in the context of the\nO-RAN.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8eO-RAN\u539f\u5219\u7684\u672a\u6765\u5730\u9762\u7f51\u7edc\uff08TN\uff09\u4e0e\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u878d\u5408\u7684\u67b6\u6784\u548c\u529f\u80fd\u5206\u5272\u7b56\u7565\uff0c\u5206\u6790\u4e86\u6027\u80fd\u548c\u90e8\u7f72\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u548c\u6a21\u5757\u5316\u878d\u5408\u7684\u5173\u952e\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u5f02\u6784\u4f20\u64ad\u6761\u4ef6\u3001\u52a8\u6001\u62d3\u6251\u548c\u6709\u9650\u7684\u661f\u4e0a\u5904\u7406\u80fd\u529b\uff0cTN\u4e0eNTN\u7684\u96c6\u6210\u4e3a\u67b6\u6784\u548c\u529f\u80fd\u8bbe\u8ba1\u5e26\u6765\u4e86\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5272\u9009\u9879\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06RAN\u548c\u6838\u5fc3\u529f\u80fd\u5206\u5e03\u5728\u536b\u661f\u548c\u5730\u9762\u8282\u70b9\u4e4b\u95f4\uff0c\u5e76\u8bc4\u4f30\u4e86\u4ece\u7eaf\u661f\u4e0aDU\u90e8\u7f72\u5230\u5b8c\u5168\u96c6\u6210gNB\u548cUPF\u7684\u591a\u79cd\u914d\u7f6e\u3002", "result": "\u5206\u6790\u4e86\u6027\u80fd\u3001\u5ef6\u8fdf\u3001\u81ea\u4e3b\u6027\u548c\u90e8\u7f72\u7684\u6743\u8861\uff0c\u5e76\u63d0\u4f9b\u4e86\u67b6\u6784\u5206\u5272\u4e0eRIC\u653e\u7f6e\u7684\u5168\u9762\u6620\u5c04\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u6807\u51c6\u5316\u3001\u6a21\u5757\u5316\u548c\u9ad8\u6548TN-NTN\u878d\u5408\u7684\u5173\u952e\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2507.02254", "pdf": "https://arxiv.org/pdf/2507.02254", "abs": "https://arxiv.org/abs/2507.02254", "authors": ["Pablo Figueroa", "Mark Green", "Benjamin Watson"], "title": "A framework for 3D interaction techniques", "categories": ["cs.HC"], "comment": null, "summary": "This paper presents a software architecture for 3D interaction techniques\n(ITs) and an object oriented, toolkit-independent framework that implements\nsuch architecture. ITs are composed of basic filters connected in a dataflow,\nwhere virtual input devices and objects in the scene are sources of\ninformation. An execution model defines the general flow of information between\nfilters. This framework has been designed to be extensible: new information\ntypes, new input devices, new execution models, or new interaction techniques\ncan easily be added. Application specific code and application specific ITs are\nseamlessly integrated into this architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e3D\u4ea4\u4e92\u6280\u672f\u7684\u8f6f\u4ef6\u67b6\u6784\u53ca\u72ec\u7acb\u4e8e\u5de5\u5177\u5305\u7684\u9762\u5411\u5bf9\u8c61\u6846\u67b6\uff0c\u652f\u6301\u6269\u5c55\u6027\u548c\u65e0\u7f1d\u96c6\u6210\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b33D\u4ea4\u4e92\u6280\u672f\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u901a\u7528\u7684\u67b6\u6784\u548c\u6846\u67b6\u3002", "method": "\u91c7\u7528\u6570\u636e\u6d41\u4e2d\u7684\u57fa\u7840\u8fc7\u6ee4\u5668\u7ec4\u5408\uff0c\u865a\u62df\u8f93\u5165\u8bbe\u5907\u548c\u573a\u666f\u5bf9\u8c61\u4f5c\u4e3a\u4fe1\u606f\u6e90\uff0c\u5b9a\u4e49\u4e00\u4e2a\u901a\u7528\u7684\u4fe1\u606f\u6d41\u6267\u884c\u6a21\u578b\u3002", "result": "\u8be5\u6846\u67b6\u652f\u6301\u65b0\u4fe1\u606f\u7c7b\u578b\u3001\u8f93\u5165\u8bbe\u5907\u3001\u6267\u884c\u6a21\u578b\u6216\u4ea4\u4e92\u6280\u672f\u7684\u8f7b\u677e\u6dfb\u52a0\uff0c\u5e76\u80fd\u4e0e\u5e94\u7528\u7279\u5b9a\u7684\u4ee3\u7801\u548c\u6280\u672f\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "\u8be5\u67b6\u6784\u548c\u6846\u67b6\u4e3a3D\u4ea4\u4e92\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02443", "pdf": "https://arxiv.org/pdf/2507.02443", "abs": "https://arxiv.org/abs/2507.02443", "authors": ["Sandro Costa Magalh\u00e3es", "Marco Almeida", "Filipe Neves dos Santos", "Ant\u00f3nio Paulo Moreira", "Jorge Dias"], "title": "Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic", "categories": ["cs.CV", "cs.AI", "cs.DC", "cs.LG", "cs.RO"], "comment": "Submitted to ROBOT'2025", "summary": "Robots usually slow down for canning to detect objects while moving.\nAdditionally, the robot's camera is configured with a low framerate to track\nthe velocity of the detection algorithms. This would be constrained while\nexecuting tasks and exploring, making robots increase the task execution time.\nAMD has developed the Vitis-AI framework to deploy detection algorithms into\nFPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we\nuse the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit\nquantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation\n(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This\nis a self-acquired dataset released in open access. MobileNet v1 performed\nbetter, reaching a success rate of 98 % and an inference speed of 6611 FPS. In\nthis work, we proved that we can use FPGAs to speed up ANNs and make them\nsuitable for attention mechanisms.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u5229\u7528FPGA\u52a0\u901fANN\uff0c\u901a\u8fc7FINN\u67b6\u6784\u90e8\u7f72\u4e0d\u540c\u91cf\u5316\u6a21\u578b\uff0c\u63d0\u5347\u673a\u5668\u4eba\u68c0\u6d4b\u901f\u5ea6\u3002", "motivation": "\u673a\u5668\u4eba\u56e0\u68c0\u6d4b\u901f\u5ea6\u548c\u6444\u50cf\u5934\u7684\u4f4e\u5e27\u7387\u9650\u5236\u4efb\u52a1\u6267\u884c\u6548\u7387\uff0c\u73b0\u6709\u5de5\u5177\u672a\u80fd\u5145\u5206\u5229\u7528FPGA\u8d44\u6e90\u3002", "method": "\u4f7f\u7528FINN\u67b6\u6784\u5728FPGA\u4e2d\u90e8\u7f723\u79cd\u91cf\u5316ANN\u6a21\u578b\uff08MobileNet v1\u3001CNV\uff09\uff0c\u8bad\u7ec3\u57fa\u4e8eRG2C\u6570\u636e\u96c6\u3002", "result": "MobileNet v1\u8868\u73b0\u6700\u4f73\uff0c\u6210\u529f\u7387\u8fbe98%\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe6611 FPS\u3002", "conclusion": "\u9a8c\u8bc1\u4e86FPGA\u52a0\u901fANN\u7684\u53ef\u884c\u6027\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u3002"}}
{"id": "2507.02665", "pdf": "https://arxiv.org/pdf/2507.02665", "abs": "https://arxiv.org/abs/2507.02665", "authors": ["Timo Kehrer", "Robert Haines", "Guido Juckeland", "Shurui Zhou", "David E. Bernholdt"], "title": "Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?", "categories": ["cs.SE"], "comment": "Early access journal version: T. Kehrer, R. Haines, G. Juckeland, S.\n  Zhou and D. E. Bernholdt, \"Do Research Software Engineers and Software\n  Engineering Researchers Speak the Same Language?,\" in Computing in Science &\n  Engineering, doi: 10.1109/MCSE.2025.3557236", "summary": "Anecdotal evidence suggests that Research Software Engineers (RSEs) and\nSoftware Engineering Researchers (SERs) often use different terminologies for\nsimilar concepts, creating communication challenges. To better understand these\ndivergences, we have started investigating how SE fundamentals from the SER\ncommunity are interpreted within the RSE community, identifying aligned\nconcepts, knowledge gaps, and areas for potential adaptation. Our preliminary\nfindings reveal opportunities for mutual learning and collaboration, and our\nsystematic methodology for terminology mapping provides a foundation for a\ncrowd-sourced extension and validation in the future.", "AI": {"tldr": "\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff08RSEs\uff09\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u8005\uff08SERs\uff09\u5728\u672f\u8bed\u4f7f\u7528\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4\u6c9f\u901a\u56f0\u96be\u3002\u672c\u7814\u7a76\u901a\u8fc7\u8c03\u67e5SE\u57fa\u7840\u77e5\u8bc6\u5728RSE\u793e\u533a\u4e2d\u7684\u89e3\u8bfb\uff0c\u53d1\u73b0\u53cc\u65b9\u53ef\u4ee5\u4e92\u76f8\u5b66\u4e60\u4e0e\u5408\u4f5c\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u672f\u8bed\u6620\u5c04\u65b9\u6cd5\u3002", "motivation": "RSEs\u548cSERs\u5728\u672f\u8bed\u4f7f\u7528\u4e0a\u5b58\u5728\u5206\u6b67\uff0c\u5f71\u54cd\u4e86\u53cc\u65b9\u7684\u6c9f\u901a\u4e0e\u5408\u4f5c\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u63a2\u7a76\u8fd9\u4e9b\u5dee\u5f02\u5e76\u5bfb\u627e\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8c03\u67e5SE\u57fa\u7840\u77e5\u8bc6\u5728RSE\u793e\u533a\u4e2d\u7684\u89e3\u8bfb\uff0c\u8bc6\u522b\u6982\u5ff5\u5bf9\u9f50\u3001\u77e5\u8bc6\u5dee\u8ddd\u548c\u6f5c\u5728\u9002\u5e94\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u7cfb\u7edf\u5316\u7684\u672f\u8bed\u6620\u5c04\u65b9\u6cd5\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u8868\u660e\u53cc\u65b9\u5b58\u5728\u4e92\u76f8\u5b66\u4e60\u548c\u5408\u4f5c\u7684\u673a\u4f1a\uff0c\u672f\u8bed\u6620\u5c04\u65b9\u6cd5\u4e3a\u672a\u6765\u7684\u4f17\u5305\u6269\u5c55\u548c\u9a8c\u8bc1\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cRSEs\u4e0eSERs\u4e4b\u95f4\u7684\u672f\u8bed\u5dee\u5f02\u53ef\u4ee5\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u5f25\u5408\uff0c\u672a\u6765\u53ef\u4ee5\u901a\u8fc7\u4f17\u5305\u8fdb\u4e00\u6b65\u6269\u5c55\u4e0e\u9a8c\u8bc1\u8fd9\u4e9b\u53d1\u73b0\u3002"}}
{"id": "2507.02283", "pdf": "https://arxiv.org/pdf/2507.02283", "abs": "https://arxiv.org/abs/2507.02283", "authors": ["Tim Rogers", "Ben Teehankee"], "title": "Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness", "categories": ["cs.HC", "I.2.6; H.1.2"], "comment": "21 pages", "summary": "This paper examines a critical yet unexplored dimension of the AI alignment\nproblem: the potential for Large Language Models (LLMs) to inherit and amplify\nexisting misalignments between human espoused theories and theories-in-use.\nDrawing on action science research, we argue that LLMs trained on\nhuman-generated text likely absorb and reproduce Model 1 theories-in-use - a\ndefensive reasoning pattern that both inhibits learning and creates ongoing\nanti-learning dynamics at the dyad, group, and organisational levels. Through a\ndetailed case study of an LLM acting as an HR consultant, we show how its\nadvice, while superficially professional, systematically reinforces\nunproductive problem-solving approaches and blocks pathways to deeper\norganisational learning. This represents a specific instance of the alignment\nproblem where the AI system successfully mirrors human behaviour but inherits\nour cognitive blind spots. This poses particular risks if LLMs are integrated\ninto organisational decision-making processes, potentially entrenching\nanti-learning practices while lending authority to them. The paper concludes by\nexploring the possibility of developing LLMs capable of facilitating Model 2\nlearning - a more productive theory-in-use - and suggests this effort could\nadvance both AI alignment research and action science practice. This analysis\nreveals an unexpected symmetry in the alignment challenge: the process of\ndeveloping AI systems properly aligned with human values could yield tools that\nhelp humans themselves better embody those same values.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u80fd\u7ee7\u627f\u5e76\u653e\u5927\u4eba\u7c7b\u7406\u8bba\u4e0e\u5b9e\u9645\u884c\u4e3a\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u63ed\u793a\u4e86AI\u5bf9\u9f50\u95ee\u9898\u5728\u7ec4\u7ec7\u5b66\u4e60\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLM\u5982\u4f55\u6a21\u4eff\u4eba\u7c7b\u7684\u9632\u5fa1\u6027\u63a8\u7406\u6a21\u5f0f\uff08Model 1\u7406\u8bba\uff09\uff0c\u4ece\u800c\u963b\u788d\u7ec4\u7ec7\u5b66\u4e60\uff0c\u8fdb\u4e00\u6b65\u52a0\u5267AI\u5bf9\u9f50\u95ee\u9898\u7684\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7HR\u987e\u95ee\u89d2\u8272\u7684LLM\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u5176\u5efa\u8bae\u5982\u4f55\u5f3a\u5316\u65e0\u6548\u95ee\u9898\u89e3\u51b3\u65b9\u5f0f\u5e76\u963b\u788d\u6df1\u5ea6\u5b66\u4e60\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u867d\u80fd\u6a21\u4eff\u4eba\u7c7b\u884c\u4e3a\uff0c\u4f46\u4e5f\u7ee7\u627f\u4e86\u8ba4\u77e5\u76f2\u70b9\uff0c\u53ef\u80fd\u5f3a\u5316\u7ec4\u7ec7\u4e2d\u7684\u53cd\u5b66\u4e60\u5b9e\u8df5\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u5f00\u53d1\u652f\u6301Model 2\u5b66\u4e60\u7684LLM\u7684\u53ef\u80fd\u6027\uff0c\u8ba4\u4e3aAI\u5bf9\u9f50\u7814\u7a76\u53ef\u4ee5\u4fc3\u8fdb\u4eba\u7c7b\u4ef7\u503c\u7684\u5185\u5316\uff0c\u5b9e\u73b0\u53cc\u5411\u6539\u8fdb\u3002"}}
{"id": "2507.02464", "pdf": "https://arxiv.org/pdf/2507.02464", "abs": "https://arxiv.org/abs/2507.02464", "authors": ["Craig S Wright"], "title": "Resolving CAP Through Automata-Theoretic Economic Design: A Unified Mathematical Framework for Real-Time Partition-Tolerant Systems", "categories": ["cs.GT", "cs.DC", "cs.FL", "cs.IR", "econ.GN", "q-fin.EC", "68M14, 91A05, 68Q85", "C.2.4; D.2.4; F.1.1"], "comment": "51 pages 4 tables, includes formal proofs, automata construction, and\n  case study on Bitcoin Script", "summary": "The CAP theorem asserts a trilemma between consistency, availability, and\npartition tolerance. This paper introduces a rigorous automata-theoretic and\neconomically grounded framework that reframes the CAP trade-off as a constraint\noptimization problem. We model distributed systems as partition-aware state\nmachines and embed economic incentive layers to stabilize consensus behavior\nacross adversarially partitioned networks. By incorporating game-theoretic\nmechanisms into the global transition semantics, we define provable bounds on\nconvergence, liveness, and correctness. Our results demonstrate that\navailability and consistency can be simultaneously preserved within bounded\nepsilon margins, effectively extending the classical CAP limits through formal\neconomic control.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u81ea\u52a8\u673a\u7406\u8bba\u548c\u7ecf\u6d4e\u5b66\u6846\u67b6\u91cd\u6784\u4e86CAP\u7406\u8bba\u4e2d\u7684\u6743\u8861\u95ee\u9898\uff0c\u8bc1\u660e\u5728\u6709\u9650\u8bef\u5dee\u8303\u56f4\u5185\u53ef\u540c\u65f6\u4fdd\u6301\u53ef\u7528\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "CAP\u7406\u8bba\u7684\u7ecf\u5178\u4e09\u96be\u95ee\u9898\u9650\u5236\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ecf\u6d4e\u6fc0\u52b1\u673a\u5236\u548c\u5f62\u5f0f\u5316\u65b9\u6cd5\u6269\u5c55CAP\u7684\u6781\u9650\u3002", "method": "\u91c7\u7528\u5206\u533a\u611f\u77e5\u7684\u72b6\u6001\u673a\u5efa\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5e76\u5d4c\u5165\u6e38\u620f\u8bba\u673a\u5236\u4ee5\u7a33\u5b9a\u5206\u533a\u7f51\u7edc\u4e0b\u7684\u5171\u8bc6\u884c\u4e3a\u3002", "result": "\u5728\u6709\u9650\u8bef\u5dee\u8303\u56f4\u5185\uff0c\u53ef\u7528\u6027\u548c\u4e00\u81f4\u6027\u53ef\u540c\u65f6\u5b9e\u73b0\uff0c\u6269\u5c55\u4e86CAP\u7406\u8bba\u7684\u7ecf\u5178\u9650\u5236\u3002", "conclusion": "\u901a\u8fc7\u7ecf\u6d4e\u63a7\u5236\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u7ba1\u7406CAP\u6743\u8861\uff0c\u63d0\u5347\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.02690", "pdf": "https://arxiv.org/pdf/2507.02690", "abs": "https://arxiv.org/abs/2507.02690", "authors": ["Jiaxing Wang", "Yifeng Yu", "Jiahan Song", "Bin Cao", "Jing Fan", "Ji Zhang"], "title": "RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes", "categories": ["cs.SE", "cs.LG"], "comment": "15 pages, 7 figures. Business process prediction using reinforcement\n  learning and heterogeneous graph neural networks", "summary": "Next activity prediction represents a fundamental challenge for optimizing\nbusiness processes in service-oriented architectures such as microservices\nenvironments, distributed enterprise systems, and cloud-native platforms, which\nenables proactive resource allocation and dynamic service composition. Despite\nthe prevalence of sequence-based methods, these approaches fail to capture\nnon-sequential relationships that arise from parallel executions and\nconditional dependencies. Even though graph-based approaches address structural\npreservation, they suffer from homogeneous representations and static\nstructures that apply uniform modeling strategies regardless of individual\nprocess complexity characteristics. To address these limitations, we introduce\nRLHGNN, a novel framework that transforms event logs into heterogeneous process\ngraphs with three distinct edge types grounded in established process mining\ntheory. Our approach creates four flexible graph structures by selectively\ncombining these edges to accommodate different process complexities, and\nemploys reinforcement learning formulated as a Markov Decision Process to\nautomatically determine the optimal graph structure for each specific process\ninstance. RLHGNN then applies heterogeneous graph convolution with\nrelation-specific aggregation strategies to effectively predict the next\nactivity. This adaptive methodology enables precise modeling of both sequential\nand non-sequential relationships in service interactions. Comprehensive\nevaluation on six real-world datasets demonstrates that RLHGNN consistently\noutperforms state-of-the-art approaches. Furthermore, it maintains an inference\nlatency of approximately 1 ms per prediction, representing a highly practical\nsolution suitable for real-time business process monitoring applications. The\nsource code is available at https://github.com/Joker3993/RLHGNN.", "AI": {"tldr": "RLHGNN\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e8b\u4ef6\u65e5\u5fd7\u8f6c\u5316\u4e3a\u5f02\u6784\u56fe\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6765\u81ea\u9002\u5e94\u5efa\u6a21\u4e1a\u52a1\u6d41\u7a0b\u4e2d\u7684\u987a\u5e8f\u548c\u975e\u987a\u5e8f\u5173\u7cfb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5e76\u884c\u6267\u884c\u548c\u6761\u4ef6\u4f9d\u8d56\u7b49\u975e\u987a\u5e8f\u5173\u7cfb\uff0c\u4e14\u56fe\u57fa\u65b9\u6cd5\u5b58\u5728\u540c\u8d28\u8868\u793a\u548c\u9759\u6001\u7ed3\u6784\u7684\u5c40\u9650\u6027\u3002", "method": "RLHGNN\u5c06\u4e8b\u4ef6\u65e5\u5fd7\u8f6c\u5316\u4e3a\u5f02\u6784\u56fe\uff0c\u5305\u542b\u4e09\u79cd\u8fb9\u7c7b\u578b\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u56fe\u7ed3\u6784\uff0c\u518d\u901a\u8fc7\u5f02\u6784\u56fe\u5377\u79ef\u9884\u6d4b\u4e0b\u4e00\u6d3b\u52a8\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cRLHGNN\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u63a8\u7406\u5ef6\u8fdf\u4ec5\u7ea61\u6beb\u79d2\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u76d1\u63a7\u3002", "conclusion": "RLHGNN\u901a\u8fc7\u81ea\u9002\u5e94\u5efa\u6a21\u590d\u6742\u5173\u7cfb\uff0c\u4e3a\u4e1a\u52a1\u6d41\u7a0b\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02300", "pdf": "https://arxiv.org/pdf/2507.02300", "abs": "https://arxiv.org/abs/2507.02300", "authors": ["Yuhao Zhang", "Jiaxin An", "Ben Wang", "Yan Zhang", "Jiqun Liu"], "title": "Human-Centered Explainability in Interactive Information Systems: A Survey", "categories": ["cs.HC"], "comment": null, "summary": "Human-centered explainability has become a critical foundation for the\nresponsible development of interactive information systems, where users must be\nable to understand, interpret, and scrutinize AI-driven outputs to make\ninformed decisions. This systematic survey of literature aims to characterize\nrecent progress in user studies on explainability in interactive information\nsystems by reviewing how explainability has been conceptualized, designed, and\nevaluated in practice. Following PRISMA guidelines, eight academic databases\nwere searched, and 100 relevant articles were identified. A structural encoding\napproach was then utilized to extract and synthesize insights from these\narticles. The main contributions include 1) five dimensions that researchers\nhave used to conceptualize explainability; 2) a classification scheme of\nexplanation designs; 3) a categorization of explainability measurements into\nsix user-centered dimensions. The review concludes by reflecting on ongoing\nchallenges and providing recommendations for future exploration of related\nissues. The findings shed light on the theoretical foundations of\nhuman-centered explainability, informing the design of interactive information\nsystems that better align with diverse user needs and promoting the development\nof systems that are transparent, trustworthy, and accountable.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u8c03\u67e5\uff0c\u603b\u7ed3\u4e86\u4eba\u673a\u4ea4\u4e92\u4fe1\u606f\u7cfb\u7edf\u4e2d\u53ef\u89e3\u91ca\u6027\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5305\u62ec\u6982\u5ff5\u5316\u3001\u8bbe\u8ba1\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5efa\u8bae\u3002", "motivation": "\u7531\u4e8e\u53ef\u89e3\u91ca\u6027\u5bf9\u4e8e\u7528\u6237\u7406\u89e3\u548c\u76d1\u7763AI\u9a71\u52a8\u7684\u8f93\u51fa\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u5176\u5728\u5b9e\u9645\u4e2d\u7684\u6982\u5ff5\u3001\u8bbe\u8ba1\u548c\u8bc4\u4f30\u65b9\u6cd5\u6709\u52a9\u4e8e\u63a8\u52a8\u900f\u660e\u3001\u53ef\u4fe1\u548c\u8d1f\u8d23\u4efb\u7684\u4fe1\u606f\u7cfb\u7edf\u53d1\u5c55\u3002", "method": "\u9075\u5faaPRISMA\u6307\u5357\uff0c\u641c\u7d22\u4e868\u4e2a\u5b66\u672f\u6570\u636e\u5e93\uff0c\u7b5b\u9009\u51fa100\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u7f16\u7801\u65b9\u6cd5\u63d0\u53d6\u548c\u7efc\u5408\u4fe1\u606f\u3002", "result": "\u603b\u7ed3\u4e86\u53ef\u89e3\u91ca\u6027\u7684\u4e94\u4e2a\u6982\u5ff5\u7ef4\u5ea6\u3001\u89e3\u91ca\u8bbe\u8ba1\u7684\u5206\u7c7b\u65b9\u6848\u4ee5\u53ca\u516d\u79cd\u7528\u6237\u4e2d\u5fc3\u5316\u7684\u6d4b\u91cf\u7ef4\u5ea6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u673a\u4ea4\u4e92\u53ef\u89e3\u91ca\u6027\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4e3a\u6ee1\u8db3\u591a\u6837\u5316\u7528\u6237\u9700\u6c42\u7684\u4fe1\u606f\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.02695", "pdf": "https://arxiv.org/pdf/2507.02695", "abs": "https://arxiv.org/abs/2507.02695", "authors": ["Sahar Ahmadisakha", "Lech Bialek", "Mohamed Soliman", "Vasilios Andrikopoulos"], "title": "Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms", "categories": ["cs.SE"], "comment": null, "summary": "In recent years, sustainability in software systems has gained significant\nattention, especially with the rise of cloud computing and the shift towards\ncloud-based architectures. This shift has intensified the need to identify\nsustainability in architectural discussions to take informed architectural\ndecisions. One source to see these decisions is in online Q&A forums among\npractitioners' discussions. However, recognizing sustainability concepts within\nsoftware practitioners' discussions remains challenging due to the lack of\nclear and distinct guidelines for this task. To address this issue, we\nintroduce the notion of sustainability flags as pointers in relevant\ndiscussions, developed through thematic analysis of multiple sustainability\nbest practices from cloud providers. This study further evaluates the\neffectiveness of these flags in identifying sustainability within cloud\narchitecture posts, using a controlled experiment. Preliminary results suggest\nthat the use of flags results in classifying fewer posts as\nsustainability-related compared to a control group, with moderately higher\ncertainty and significantly improved performance. Moreover, sustainability\nflags are perceived as more useful and understandable than relying solely on\ndefinitions for identifying sustainability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u53ef\u6301\u7eed\u6027\u6807\u5fd7\u2019\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u8f6f\u4ef6\u67b6\u6784\u8ba8\u8bba\u4e2d\u7684\u53ef\u6301\u7eed\u6027\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u4e91\u8ba1\u7b97\u548c\u4e91\u67b6\u6784\u7684\u5174\u8d77\uff0c\u8f6f\u4ef6\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u6027\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u660e\u786e\u7684\u6307\u5bfc\u539f\u5219\u6765\u8bc6\u522b\u76f8\u5173\u8ba8\u8bba\u3002", "method": "\u901a\u8fc7\u4e3b\u9898\u5206\u6790\u5f00\u53d1\u2018\u53ef\u6301\u7eed\u6027\u6807\u5fd7\u2019\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc4\u4f30\u5176\u5728\u8bc6\u522b\u4e91\u67b6\u6784\u5e16\u5b50\u4e2d\u53ef\u6301\u7eed\u6027\u5185\u5bb9\u7684\u6548\u679c\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u6807\u5fd7\u80fd\u51cf\u5c11\u8bef\u5206\u7c7b\uff0c\u63d0\u9ad8\u786e\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u4e14\u6bd4\u4ec5\u4f9d\u8d56\u5b9a\u4e49\u66f4\u5b9e\u7528\u6613\u61c2\u3002", "conclusion": "\u53ef\u6301\u7eed\u6027\u6807\u5fd7\u4e3a\u8bc6\u522b\u8f6f\u4ef6\u67b6\u6784\u8ba8\u8bba\u4e2d\u7684\u53ef\u6301\u7eed\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6539\u5584\u4e86\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2507.02306", "pdf": "https://arxiv.org/pdf/2507.02306", "abs": "https://arxiv.org/abs/2507.02306", "authors": ["Ruican Zhong", "David W. McDonald", "Gary Hsieh"], "title": "Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Usability evaluation is crucial in human-centered design but can be costly,\nrequiring expert time and user compensation. In this work, we developed a\nmethod for synthetic heuristic evaluation using multimodal LLMs' ability to\nanalyze images and provide design feedback. Comparing our synthetic evaluations\nto those by experienced UX practitioners across two apps, we found our\nevaluation identified 73% and 77% of usability issues, which exceeded the\nperformance of 5 experienced human evaluators (57% and 63%). Compared to human\nevaluators, the synthetic evaluation's performance maintained consistent\nperformance across tasks and excelled in detecting layout issues, highlighting\npotential attentional and perceptual strengths of synthetic evaluation.\nHowever, synthetic evaluation struggled with recognizing some UI components and\ndesign conventions, as well as identifying across screen violations.\nAdditionally, testing synthetic evaluations over time and accounts revealed\nstable performance. Overall, our work highlights the performance differences\nbetween human and LLM-driven evaluations, informing the design of synthetic\nheuristic evaluations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6a21\u6001LLM\u5206\u6790\u56fe\u50cf\u5e76\u63d0\u4f9b\u8bbe\u8ba1\u53cd\u9988\u7684\u5408\u6210\u542f\u53d1\u5f0f\u8bc4\u4f30\u65b9\u6cd5\u3002\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u76f8\u6bd4\uff0c\u5408\u6210\u65b9\u6cd5\u5728\u8bc6\u522b\u53ef\u7528\u6027\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u53ef\u7528\u6027\u8bc4\u4f30\u5728\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e13\u5bb6\u65f6\u95f4\u548c\u7528\u6237\u8865\u507f\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528\u591a\u6a21\u6001LLM\u8fdb\u884c\u4f4e\u6210\u672c\u9ad8\u6548\u8bc4\u4f30\u7684\u53ef\u80fd\u6027\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001LLM\u5206\u6790\u56fe\u50cf\u5e76\u63d0\u4f9b\u8bbe\u8ba1\u53cd\u9988\uff0c\u5f00\u53d1\u4e86\u5408\u6210\u542f\u53d1\u5f0f\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u5728\u4e24\u4e2a\u5e94\u7528\u4e2d\u4e0e5\u540d\u7ecf\u9a8c\u4e30\u5bcc\u7684\u7528\u6237\u4f53\u9a8c\u4ece\u4e1a\u8005\u7684\u8bc4\u4f30\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5408\u6210\u8bc4\u4f30\u65b9\u6cd5\u5728\u4e24\u4e2a\u5e94\u7528\u4e2d\u5206\u522b\u8bc6\u522b\u51fa73%\u548c77%\u7684\u53ef\u7528\u6027\u95ee\u9898\uff0c\u4f18\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\uff0857%\u548c63%\uff09\u3002\u5408\u6210\u65b9\u6cd5\u5728\u68c0\u6d4b\u5e03\u5c40\u95ee\u9898\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u5408\u6210\u8bc4\u4f30\u65b9\u6cd5\u5728\u53ef\u7528\u6027\u95ee\u9898\u8bc6\u522b\u4e0a\u4f18\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\uff0c\u4f46\u5728\u8bc6\u522b\u90e8\u5206UI\u7ec4\u4ef6\u548c\u8bbe\u8ba1\u60ef\u4f8b\u4e0a\u5b58\u5728\u5c40\u9650\u3002\u7814\u7a76\u4e3a\u5408\u6210\u542f\u53d1\u5f0f\u8bc4\u4f30\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2507.02846", "pdf": "https://arxiv.org/pdf/2507.02846", "abs": "https://arxiv.org/abs/2507.02846", "authors": ["Anmol Singhal", "Travis Breaux"], "title": "Legal Requirements Translation from Law", "categories": ["cs.SE", "cs.CL"], "comment": "13 pages, 7 figures, Accepted at the 33rd IEEE International\n  Requirements Engineering 2025", "summary": "Software systems must comply with legal regulations, which is a\nresource-intensive task, particularly for small organizations and startups\nlacking dedicated legal expertise. Extracting metadata from regulations to\nelicit legal requirements for software is a critical step to ensure compliance.\nHowever, it is a cumbersome task due to the length and complex nature of legal\ntext. Although prior work has pursued automated methods for extracting\nstructural and semantic metadata from legal text, key limitations remain: they\ndo not consider the interplay and interrelationships among attributes\nassociated with these metadata types, and they rely on manual labeling or\nheuristic-driven machine learning, which does not generalize well to new\ndocuments. In this paper, we introduce an approach based on textual entailment\nand in-context learning for automatically generating a canonical representation\nof legal text, encodable and executable as Python code. Our representation is\ninstantiated from a manually designed Python class structure that serves as a\ndomain-specific metamodel, capturing both structural and semantic legal\nmetadata and their interrelationships. This design choice reduces the need for\nlarge, manually labeled datasets and enhances applicability to unseen\nlegislation. We evaluate our approach on 13 U.S. state data breach notification\nlaws, demonstrating that our generated representations pass approximately 89.4%\nof test cases and achieve a precision and recall of 82.2 and 88.7,\nrespectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u8574\u542b\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u81ea\u52a8\u751f\u6210\u6cd5\u5f8b\u6587\u672c\u7684\u89c4\u8303\u8868\u793a\uff0c\u53ef\u7f16\u7801\u4e3aPython\u4ee3\u7801\uff0c\u4ee5\u89e3\u51b3\u6cd5\u5f8b\u5408\u89c4\u6027\u4e2d\u7684\u5143\u6570\u636e\u63d0\u53d6\u95ee\u9898\u3002", "motivation": "\u5c0f\u578b\u4f01\u4e1a\u548c\u521d\u521b\u516c\u53f8\u7f3a\u4e4f\u6cd5\u5f8b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u6cd5\u5f8b\u6587\u672c\u590d\u6742\u5197\u957f\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5143\u6570\u636e\u95f4\u7684\u5173\u8054\u6027\u4e14\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3002", "method": "\u4f7f\u7528\u6587\u672c\u8574\u542b\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u751f\u6210\u6cd5\u5f8b\u6587\u672c\u7684\u89c4\u8303\u8868\u793a\uff0c\u57fa\u4e8ePython\u7c7b\u7684\u9886\u57df\u7279\u5b9a\u5143\u6a21\u578b\uff0c\u6355\u83b7\u7ed3\u6784\u548c\u8bed\u4e49\u5143\u6570\u636e\u53ca\u5176\u5173\u8054\u3002", "result": "\u572813\u4e2a\u7f8e\u56fd\u5dde\u6570\u636e\u6cc4\u9732\u901a\u77e5\u6cd5\u5f8b\u4e0a\u6d4b\u8bd5\uff0c\u751f\u6210\u7684\u8868\u793a\u901a\u8fc789.4%\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u5206\u522b\u4e3a82.2%\u548c88.7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u65b0\u6cd5\u89c4\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.02350", "pdf": "https://arxiv.org/pdf/2507.02350", "abs": "https://arxiv.org/abs/2507.02350", "authors": ["Hao Tang", "Songyun Xie", "Xinzhou Xie", "Can Liao", "Xin Zhang", "Bohan Li", "Zhongyu Tian", "Dalu Zheng"], "title": "From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance", "categories": ["cs.HC"], "comment": null, "summary": "Traditional video-induced emotion physiological datasets often use\nwhole-trial annotation, assigning a single emotion label to all data collected\nduring an entire trial. This coarse-grained annotation approach misaligns with\nthe dynamic and temporally localized nature of emotional responses as they\nunfold with video narratives, introducing label noise that limits emotion\nrecognition algorithm evaluation and performance. To solve the label noise\nproblem caused by coarse-grained annotation, we propose a fine-grained\nannotation method through an immediate recall paradigm. This paradigm\nintegrates an immediate video replay phase after the initial stimulus viewing,\nallowing participants to precisely mark the onset timestamp, emotion label, and\nintensity based on their immediate recall. We validate this paradigm through\nphysiological evidence and recognition performance. Physiological validation of\nmultimodal signals within participant-marked windows revealed rhythm-specific\nEEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of\nhigh-arousal versus 6% of low-arousal emotion windows. These objective\nphysiological data changes strongly aligned with subjective annotations,\nconfirming annotation precision. For recognition performance, classification\nexperiments showed that models trained on fine-grained annotations achieved\n9.7% higher accuracy than traditional whole-trial labeling, despite using less\ndata. This work not only addresses label noise through fine-grained annotation\nbut also demonstrates that annotation precision outweighs data scale in\ndetermining emotion recognition performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u6807\u6ce8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5373\u65f6\u56de\u5fc6\u8303\u5f0f\u89e3\u51b3\u4f20\u7edf\u89c6\u9891\u8bf1\u5bfc\u60c5\u611f\u751f\u7406\u6570\u636e\u96c6\u4e2d\u7c97\u7c92\u5ea6\u6807\u6ce8\u5bfc\u81f4\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u60c5\u611f\u8bc6\u522b\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u60c5\u611f\u6570\u636e\u96c6\u4f7f\u7528\u7c97\u7c92\u5ea6\u7684\u5168\u8bd5\u6b21\u6807\u6ce8\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u60c5\u611f\u53cd\u5e94\u7684\u52a8\u6001\u6027\u548c\u5c40\u90e8\u6027\uff0c\u5bfc\u81f4\u6807\u7b7e\u566a\u58f0\uff0c\u9650\u5236\u4e86\u60c5\u611f\u8bc6\u522b\u7b97\u6cd5\u7684\u8bc4\u4f30\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5373\u65f6\u56de\u5fc6\u8303\u5f0f\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\u65b9\u6cd5\uff0c\u5728\u521d\u59cb\u523a\u6fc0\u89c2\u770b\u540e\u52a0\u5165\u5373\u65f6\u89c6\u9891\u56de\u653e\uff0c\u8ba9\u53c2\u4e0e\u8005\u7cbe\u786e\u6807\u6ce8\u60c5\u611f\u53d1\u751f\u7684\u65f6\u95f4\u6233\u3001\u6807\u7b7e\u548c\u5f3a\u5ea6\u3002", "result": "\u751f\u7406\u9a8c\u8bc1\u663e\u793a\uff0c\u88ab\u8bd5\u6807\u6ce8\u7684\u7a97\u53e3\u4e2d\u51fa\u73b0\u4e86\u8282\u5f8b\u7279\u5f02\u6027\u7684EEG\u6a21\u5f0f\u548c\u4e0e\u4e3b\u89c2\u6807\u6ce8\u9ad8\u5ea6\u4e00\u81f4\u7684GSR\u53cd\u5e94\uff1b\u60c5\u611f\u8bc6\u522b\u5b9e\u9a8c\u4e2d\uff0c\u7ec6\u7c92\u5ea6\u6807\u6ce8\u8bad\u7ec3\u7684\u6a21\u578b\u51c6\u786e\u7387\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9ad89.7%\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u6807\u6ce8\u4e0d\u4ec5\u89e3\u51b3\u4e86\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u8fd8\u8868\u660e\u6807\u6ce8\u7cbe\u786e\u5ea6\u6bd4\u6570\u636e\u89c4\u6a21\u5bf9\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u7684\u5f71\u54cd\u66f4\u5927\u3002"}}
{"id": "2507.02858", "pdf": "https://arxiv.org/pdf/2507.02858", "abs": "https://arxiv.org/abs/2507.02858", "authors": ["Yuchen Shen", "Anmol Singhal", "Travis Breaux"], "title": "Requirements Elicitation Follow-Up Question Generation", "categories": ["cs.SE", "cs.CL"], "comment": "13 pages, 2 figures, accepted at the 33rd IEEE International\n  Requirements Engineering 2025", "summary": "Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528GPT-4\u751f\u6210\u9700\u6c42\u8bbf\u8c08\u4e2d\u7684\u540e\u7eed\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u8bbf\u8c08\u8d28\u91cf\u548c\u6548\u7387\u3002\u5b9e\u9a8c\u663e\u793a\uff0cLLM\u751f\u6210\u7684\u95ee\u9898\u4e0d\u4e9a\u4e8e\u4eba\u5de5\u95ee\u9898\uff0c\u4e14\u5728\u6307\u5bfc\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u9700\u6c42\u8bbf\u8c08\u4e2d\uff0c\u8bbf\u8c08\u8005\u9762\u4e34\u8ba4\u77e5\u8d1f\u62c5\u548c\u4fe1\u606f\u8fc7\u8f7d\u7b49\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528LLM\uff08\u5982GPT-4\uff09\u5b9e\u65f6\u751f\u6210\u9ad8\u8d28\u91cf\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u8bbf\u8c08\u8005\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8e\u5e38\u89c1\u8bbf\u8c08\u9519\u8bef\u7684\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u4ece\u53d7\u8bbf\u8005\u8bdd\u8bed\u751f\u6210\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5bf9\u7167\u5b9e\u9a8c\u6bd4\u8f83LLM\u751f\u6210\u548c\u4eba\u5de5\u95ee\u9898\u7684\u8868\u73b0\u3002", "result": "LLM\u751f\u6210\u7684\u95ee\u9898\u5728\u6e05\u6670\u5ea6\u3001\u76f8\u5173\u6027\u548c\u4fe1\u606f\u91cf\u4e0a\u4e0d\u900a\u4e8e\u4eba\u5de5\u95ee\u9898\uff0c\u4e14\u5728\u9519\u8bef\u7c7b\u578b\u6307\u5bfc\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "LLM\u5728\u9700\u6c42\u8bbf\u8c08\u4e2d\u6709\u6f5c\u529b\u5b9e\u65f6\u63d0\u5347\u95ee\u9898\u8d28\u91cf\u548c\u8bbf\u8c08\u6548\u7387\u3002"}}
{"id": "2507.02432", "pdf": "https://arxiv.org/pdf/2507.02432", "abs": "https://arxiv.org/abs/2507.02432", "authors": ["Jueun Lee", "Dennis Moschina", "Supraja Ramesh", "Tobias R\u00f6ddiger", "Kai Kunze", "Michael Beigl"], "title": "Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset", "categories": ["cs.HC"], "comment": "8 pages, 6 figures. Submitted to the International Symposium on\n  Wearable Computers (ISWC)", "summary": "We investigate the use of musically structured, closed-loop vibration\npatterns as a passive biofeedback intervention for relaxation and sleep\ninitiation. By encoding rhythmic meter structures into smartwatch vibrations\nand adapting their frequency to be slightly slower than the user's real-time\nheart rate, our system aims to reduce arousal through tactile entrainment,\noffering a non-invasive alternative to auditory or open-loop approaches\npreviously used in sleep and anxiety contexts. In the first study (N=20), we\ncompared five adaptive vibration rhythms for their effects on heart rate and\nsubjective perceptions of relaxation in a resting context. In the second study\n(N=28), we evaluated the most promising pattern from Study 1 in a prolonged\nsleep initiation setting. Results showed increased parasympathetic activity and\nperceived relaxation during short-term stimulation, but no significant effects\non sleep-related measures during the sleep onset phase. This work contributes\nto the understanding of how wearable haptic feedback can support relaxation and\nsleep, offering design insights and identifying methodological considerations\nfor effectively integrating haptic interaction into self-directed\ninterventions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u97f3\u4e50\u8282\u594f\u632f\u52a8\u4f5c\u4e3a\u88ab\u52a8\u751f\u7269\u53cd\u9988\u5e72\u9884\u624b\u6bb5\uff0c\u7528\u4e8e\u653e\u677e\u548c\u4fc3\u8fdb\u5165\u7761\u3002\u7cfb\u7edf\u901a\u8fc7\u667a\u80fd\u624b\u8868\u632f\u52a8\uff0c\u4ee5\u7565\u4f4e\u4e8e\u7528\u6237\u5b9e\u65f6\u5fc3\u7387\u7684\u65b9\u5f0f\u8c03\u6574\u632f\u9891\uff0c\u65e8\u5728\u901a\u8fc7\u89e6\u89c9\u540c\u6b65\u964d\u4f4e\u89c9\u9192\u6c34\u5e73\u3002\u77ed\u671f\u5e72\u9884\u663e\u793a\u526f\u4ea4\u611f\u795e\u7ecf\u6d3b\u52a8\u589e\u5f3a\u548c\u653e\u677e\u611f\u63d0\u5347\uff0c\u4f46\u5165\u7761\u9636\u6bb5\u65e0\u663e\u8457\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u7684\u89e6\u89c9\u53cd\u9988\u65b9\u6cd5\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u542c\u89c9\u6216\u5f00\u73af\u5e72\u9884\u624b\u6bb5\uff0c\u7528\u4e8e\u653e\u677e\u548c\u4fc3\u8fdb\u5165\u7761\u3002", "method": "\u7814\u7a76\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u7b2c\u4e00\u90e8\u5206\u6bd4\u8f83\u4e94\u79cd\u81ea\u9002\u5e94\u632f\u52a8\u8282\u594f\u5bf9\u5fc3\u7387\u548c\u4e3b\u89c2\u653e\u677e\u611f\u7684\u5f71\u54cd\uff1b\u7b2c\u4e8c\u90e8\u5206\u5728\u5165\u7761\u73af\u5883\u4e2d\u8bc4\u4f30\u6700\u6709\u6548\u7684\u632f\u52a8\u6a21\u5f0f\u3002", "result": "\u77ed\u671f\u632f\u52a8\u5e72\u9884\u589e\u52a0\u4e86\u526f\u4ea4\u611f\u795e\u7ecf\u6d3b\u52a8\u548c\u4e3b\u89c2\u653e\u677e\u611f\uff0c\u4f46\u5bf9\u5165\u7761\u9636\u6bb5\u7684\u7761\u7720\u6307\u6807\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u7a7f\u6234\u89e6\u89c9\u53cd\u9988\u5728\u653e\u677e\u548c\u7761\u7720\u5e72\u9884\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u548c\u65b9\u6cd5\u5b66\u53c2\u8003\uff0c\u660e\u786e\u4e86\u5176\u77ed\u671f\u6548\u679c\u548c\u5165\u7761\u9636\u6bb5\u7684\u6709\u6548\u6027\u9650\u5236\u3002"}}
{"id": "2507.02607", "pdf": "https://arxiv.org/pdf/2507.02607", "abs": "https://arxiv.org/abs/2507.02607", "authors": ["Frida Sundfeldt", "Bianca Widstam", "Mahshid Helali Moghadam", "Kuo-Yun Liang", "Anders Vesterberg"], "title": "Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures", "categories": ["cs.CR", "cs.LG", "cs.SE"], "comment": null, "summary": "The digital evolution of connected vehicles and the subsequent security risks\nemphasize the critical need for implementing in-vehicle cyber security measures\nsuch as intrusion detection and response systems. The continuous advancement of\nattack scenarios further highlights the need for adaptive detection mechanisms\nthat can detect evolving, unknown, and complex threats. The effective use of\nML-driven techniques can help address this challenge. However, constraints on\nimplementing diverse attack scenarios on test vehicles due to safety, cost, and\nethical considerations result in a scarcity of data representing attack\nscenarios. This limitation necessitates alternative efficient and effective\nmethods for generating high-quality attack-representing data. This paper\npresents a context-aware attack data generator that generates attack inputs and\ncorresponding in-vehicle network log, i.e., controller area network (CAN) log,\nrepresenting various types of attack including denial of service (DoS), fuzzy,\nspoofing, suspension, and replay attacks. It utilizes parameterized attack\nmodels augmented with CAN message decoding and attack intensity adjustments to\nconfigure the attack scenarios with high similarity to real-world scenarios and\npromote variability. We evaluate the practicality of the generated\nattack-representing data within an intrusion detection system (IDS) case study,\nin which we develop and perform an empirical evaluation of two deep neural\nnetwork IDS models using the generated data. In addition to the efficiency and\nscalability of the approach, the performance results of IDS models, high\ndetection and classification capabilities, validate the consistency and\neffectiveness of the generated data as well. In this experience study, we also\nelaborate on the aspects influencing the fidelity of the data to real-world\nscenarios and provide insights into its application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u9ad8\u8d28\u91cf\u653b\u51fb\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u653b\u51fb\u6570\u636e\u751f\u6210\u5668\uff0c\u6a21\u62df\u591a\u79cd\u653b\u51fb\u573a\u666f\uff0c\u7528\u4e8e\u8f66\u8f86\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u8054\u7f51\u8f66\u8f86\u7684\u6570\u5b57\u5316\u53d1\u5c55\uff0c\u5b89\u5168\u98ce\u9669\u589e\u52a0\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u5b89\u5168\u3001\u6210\u672c\u548c\u4f26\u7406\u9650\u5236\u96be\u4ee5\u83b7\u53d6\u653b\u51fb\u6570\u636e\uff0c\u4e9f\u9700\u9ad8\u6548\u7684\u65b9\u6cd5\u751f\u6210\u653b\u51fb\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u5316\u653b\u51fb\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u653b\u51fb\u6570\u636e\u751f\u6210\u5668\uff0c\u6a21\u62df\u591a\u79cd\u653b\u51fb\u7c7b\u578b\uff0c\u5e76\u7ed3\u5408CAN\u6d88\u606f\u89e3\u7801\u548c\u653b\u51fb\u5f3a\u5ea6\u8c03\u6574\uff0c\u63d0\u9ad8\u6570\u636e\u771f\u5b9e\u6027\u3002", "result": "\u751f\u6210\u7684\u653b\u51fb\u6570\u636e\u5728\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5c55\u73b0\u51fa\u9ad8\u68c0\u6d4b\u548c\u5206\u7c7b\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u653b\u51fb\u6570\u636e\uff0c\u652f\u6301\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2507.02453", "pdf": "https://arxiv.org/pdf/2507.02453", "abs": "https://arxiv.org/abs/2507.02453", "authors": ["Jueun Lee", "Martin Flipe", "Philipp Lepold", "Tobias R\u00f6ddiger", "Michael Beigl"], "title": "Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?", "categories": ["cs.HC"], "comment": "8 pages, 6 figures. Submitted to the International Symposium on\n  Wearable Computers (ISWC)", "summary": "Wearable haptic interventions offer promising support for relaxation through\nslow, vibrotactile biofeedback. Despite their potential, current applications\nfocus on stress-inducing procedures and fixed vibration patterns, with limited\nconsideration of body location and dynamic biofeedback during restful states.\nThis study investigates the effects of haptic biofeedback adjusted from\nreal-time heart rate during eyes-closed wakeful rest, comparing four wearable\nbody placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave\nactivity on the ear, subjective restfulness, and vibration experience were\nmeasured across these conditions. Results show that biofeedback reduced heart\nrate at the wrist, shoulder, and forearm, while alpha power measured at the ear\nremained unchanged. Subjective restfulness was rated highest at the shoulder\nand forearm, which were also the most preferred locations. In addition,\nparticipants reported greater comfort, relaxation, and further increased\nsleepiness at the forearm compared to the wrist, which was more easily\nrecognizable. These findings suggest that the forearm and shoulder are ideal\nfor unobtrusive relaxation feedback for wakeful rest, while the wrist may\nrequire design improvements for subjective experience.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5b9e\u65f6\u5fc3\u7387\u7684\u89e6\u89c9\u53cd\u9988\u5728\u56db\u79cd\u8eab\u4f53\u90e8\u4f4d\uff08\u624b\u8155\u3001\u624b\u3001\u524d\u81c2\u3001\u80a9\u8180\uff09\u5bf9\u653e\u677e\u6548\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u524d\u81c2\u548c\u80a9\u8180\u6700\u9002\u5408\u653e\u677e\u53cd\u9988\u3002", "motivation": "\u5f53\u524d\u89e6\u89c9\u53cd\u9988\u5e94\u7528\u591a\u5173\u6ce8\u538b\u529b\u573a\u666f\u548c\u56fa\u5b9a\u632f\u52a8\u6a21\u5f0f\uff0c\u7f3a\u4e4f\u5bf9\u4f11\u606f\u72b6\u6001\u4e0b\u52a8\u6001\u53cd\u9988\u548c\u8eab\u4f53\u4f4d\u7f6e\u7684\u8003\u8651\u3002", "method": "\u6bd4\u8f83\u56db\u79cd\u4f69\u6234\u4f4d\u7f6e\uff08\u624b\u8155\u3001\u624b\u3001\u524d\u81c2\u3001\u80a9\u8180\uff09\u5bf9\u5fc3\u7387\u3001\u03b1\u6ce2\u6d3b\u52a8\u3001\u4e3b\u89c2\u4f11\u606f\u611f\u548c\u632f\u52a8\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "result": "\u624b\u8155\u3001\u80a9\u8180\u548c\u524d\u81c2\u7684\u89e6\u89c9\u53cd\u9988\u964d\u4f4e\u5fc3\u7387\uff0c\u4f46\u03b1\u6ce2\u65e0\u53d8\u5316\uff1b\u524d\u81c2\u548c\u80a9\u8180\u7684\u4e3b\u89c2\u4f11\u606f\u611f\u6700\u9ad8\u4e14\u6700\u53d7\u6b22\u8fce\u3002", "conclusion": "\u524d\u81c2\u548c\u80a9\u8180\u9002\u5408\u4f5c\u4e3a\u653e\u677e\u53cd\u9988\u7684\u7406\u60f3\u4f4d\u7f6e\uff0c\u624b\u8155\u9700\u6539\u8fdb\u8bbe\u8ba1\u4ee5\u63d0\u5347\u4e3b\u89c2\u4f53\u9a8c\u3002"}}
{"id": "2507.02537", "pdf": "https://arxiv.org/pdf/2507.02537", "abs": "https://arxiv.org/abs/2507.02537", "authors": ["Paulo Ricardo Knob", "Leonardo Scholler", "Juliano Rigatti", "Soraia Raupp Musse"], "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u60c5\u611f\u4e30\u5bcc\u7684\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c3d\u7ba1\u5bf9\u8bdd\u7684\u60c5\u611f\u7ed3\u6784\u7b26\u5408\u9884\u671f\uff0c\u4f46\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u7684\u540c\u7406\u5fc3\u548c\u8fde\u8d2f\u6027\u4ecd\u6709\u5dee\u8ddd\u3002", "motivation": "\u968f\u7740\u5bf9\u8bdd\u4ee3\u7406\u5728\u65e5\u5e38\u4e92\u52a8\u4e2d\u7684\u666e\u53ca\uff0c\u60c5\u611f\u667a\u80fd\uff08\u5c24\u5176\u662f\u540c\u7406\u5fc3\u503e\u542c\uff09\u7684\u9700\u6c42\u65e5\u76ca\u663e\u8457\u3002", "method": "\u901a\u8fc7\u5c0f\u578b\u4e13\u5bb6\u6570\u636e\u96c6\u6269\u5c55\u5bf9\u8bdd\uff0c\u4f7f\u7528ChatGPT\u548cGemini\u751f\u6210\uff0c\u5e76\u7528\u60c5\u611f\u5206\u6790\uff08VADER\uff09\u548c\u4e13\u5bb6\u8bc4\u4f30\u5206\u6790\u60c5\u611f\u8fdb\u5c55\u3002", "result": "\u751f\u6210\u7684\u5bf9\u8bdd\u867d\u7136\u60c5\u611f\u7ed3\u6784\u63a5\u8fd1\u9884\u671f\uff0c\u4f46\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u5176\u540c\u7406\u5fc3\u548c\u8fde\u8d2f\u6027\u4e0d\u8db3\u3002", "conclusion": "\u60c5\u611f\u5bf9\u8bdd\u5efa\u6a21\u9700\u7ed3\u5408\u81ea\u52a8\u5316\u548c\u4eba\u7c7b\u8bc4\u4f30\uff0c\u4ee5\u63d0\u5347\u60c5\u611f\u6df1\u5ea6\u548c\u4ee3\u7406\u7684\u60c5\u611f\u80fd\u529b\u3002"}}
{"id": "2507.02745", "pdf": "https://arxiv.org/pdf/2507.02745", "abs": "https://arxiv.org/abs/2507.02745", "authors": ["Zahra Ashktorab", "Alessandra Buccella", "Jason D'Cruz", "Zoe Fowler", "Andrew Gill", "Kei Yan Leung", "P. D. Magnus", "John Richards", "Kush R. Varshney"], "title": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots", "categories": ["cs.HC"], "comment": null, "summary": "As chatbots driven by large language models (LLMs) are increasingly deployed\nin everyday contexts, their ability to recover from errors through effective\napologies is critical to maintaining user trust and satisfaction. In a\npreregistered study with Prolific workers (N=162), we examine user preferences\nfor three types of apologies (rote, explanatory, and empathic) issued in\nresponse to three categories of common LLM mistakes (bias, unfounded\nfabrication, and factual errors). We designed a pairwise experiment in which\nparticipants evaluated chatbot responses consisting of an initial error, a\nsubsequent apology, and a resolution. Explanatory apologies were generally\npreferred, but this varied by context and user. In the bias scenario, empathic\napologies were favored for acknowledging emotional impact, while\nhallucinations, though seen as serious, elicited no clear preference,\nreflecting user uncertainty. Our findings show the complexity of effective\napology in AI systems. We discuss key insights such as personalization and\ncalibration that future systems must navigate to meaningfully repair trust.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86LLM\u9a71\u52a8\u7684\u804a\u5929\u673a\u5668\u4eba\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u7c7b\u578b\u7684\u9053\u6b49\uff08\u4f8b\u884c\u3001\u89e3\u91ca\u6027\u3001\u5171\u60c5\u6027\uff09\u4fee\u590d\u7528\u6237\u4fe1\u4efb\uff0c\u53d1\u73b0\u89e3\u91ca\u6027\u9053\u6b49\u6700\u53d7\u6b22\u8fce\uff0c\u4f46\u573a\u666f\u548c\u7528\u6237\u5dee\u5f02\u5f71\u54cd\u504f\u597d\u3002", "motivation": "\u968f\u7740LLM\u804a\u5929\u673a\u5668\u4eba\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u901a\u8fc7\u6709\u6548\u9053\u6b49\u4fee\u590d\u9519\u8bef\u7684\u80fd\u529b\u5bf9\u7ef4\u6301\u7528\u6237\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u9884\u6ce8\u518c\u5b9e\u9a8c\uff08N=162\uff09\uff0c\u91c7\u7528\u6210\u5bf9\u8bbe\u8ba1\uff0c\u8bc4\u4f30\u4e09\u79cd\u9053\u6b49\u5728\u4e09\u79cd\u5e38\u89c1\u9519\u8bef\uff08\u504f\u89c1\u3001\u865a\u6784\u3001\u4e8b\u5b9e\u9519\u8bef\uff09\u4e2d\u7684\u6548\u679c\u3002", "result": "\u89e3\u91ca\u6027\u9053\u6b49\u666e\u904d\u66f4\u53d7\u9752\u7750\uff0c\u4f46\u5728\u504f\u89c1\u573a\u666f\u4e2d\uff0c\u5171\u60c5\u6027\u9053\u6b49\u56e0\u627f\u8ba4\u60c5\u611f\u5f71\u54cd\u66f4\u53d7\u6b22\u8fce\uff1b\u865a\u6784\u9519\u8bef\u5219\u65e0\u660e\u786e\u504f\u597d\u3002", "conclusion": "\u6709\u6548\u9053\u6b49\u7684\u590d\u6742\u6027\u8981\u6c42\u672a\u6765\u7cfb\u7edf\u9700\u4e2a\u6027\u5316\u4e0e\u6821\u51c6\uff0c\u4ee5\u771f\u6b63\u4fee\u590d\u4fe1\u4efb\u3002"}}
{"id": "2507.02800", "pdf": "https://arxiv.org/pdf/2507.02800", "abs": "https://arxiv.org/abs/2507.02800", "authors": ["Ebrahim Feghhi", "Shreyas Kaasyap", "Nima Hadidi", "Jonathan C. Kao"], "title": "Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding", "categories": ["cs.HC"], "comment": "10 pages, 4 figures", "summary": "Speech neuroprostheses aim to restore communication for people with severe\nparalysis by decoding speech directly from neural activity. To accelerate\nalgorithmic progress, a recent benchmark released intracranial recordings from\na paralyzed participant attempting to speak, along with a baseline decoding\nalgorithm. Prior work on the benchmark showed impressive accuracy gains.\nHowever, these gains increased computational costs and were not demonstrated in\na real-time decoding setting. Here, we make three contributions that pave the\nway towards accurate, efficient, and real-time neural speech decoding. First,\nwe incorporate large amounts of time masking during training. On average, over\n$50\\%$ of each trial is masked. Second, we replace the gated recurrent unit\n(GRU) architecture used in the baseline algorithm with a compact Transformer.\nThe Transformer architecture uses $77\\%$ fewer parameters, cuts peak GPU memory\nusage by $36\\%$ relative, and is significantly faster to calibrate relative to\nthe GRU. Third, we design a lightweight variant of an existing test-time\nadaptation method developed for decoding handwriting from neural activity. Our\nvariant adapts the model using multiple time masked augmentations of a single\ntrial and requires only one gradient step per trial. Together, these\ncontributions reduce word error rate by $19.5\\%$ and effectively mitigate\nperformance degradations across held-out days in a real-time decoding setting\nwhile substantially lowering computational costs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u6539\u8fdb\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u795e\u7ecf\u8bed\u97f3\u89e3\u7801\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u5b9e\u65f6\u6027\uff0c\u5305\u62ec\u5927\u91cf\u65f6\u95f4\u63a9\u7801\u8bad\u7ec3\u3001\u7d27\u51d1\u578bTransformer\u67b6\u6784\u548c\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u9519\u8bef\u7387\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u795e\u7ecf\u8bed\u97f3\u89e3\u7801\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u672a\u5728\u5b9e\u65f6\u73af\u5883\u4e0b\u9a8c\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u65b9\u6cd5\u3002", "method": "1. \u5f15\u5165\u5927\u91cf\u65f6\u95f4\u63a9\u7801\u8bad\u7ec3\uff08\u63a9\u76d6\u6bcf\u4e2a\u8bd5\u9a8c\u768450%\u4ee5\u4e0a\uff09\uff1b2. \u7528\u7d27\u51d1\u578bTransformer\u66ff\u4ee3GRU\u67b6\u6784\uff0c\u51cf\u5c11\u53c2\u6570\u548cGPU\u5185\u5b58\uff1b3. \u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u68af\u5ea6\u6b65\u9002\u5e94\u6a21\u578b\u3002", "result": "\u6539\u8fdb\u65b9\u6cd5\u5c06\u8bcd\u9519\u8bef\u7387\u964d\u4f4e19.5%\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u4e86\u8de8\u65e5\u6d4b\u8bd5\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\uff0c\u8bba\u6587\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u5b9e\u65f6\u795e\u7ecf\u8bed\u97f3\u89e3\u7801\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.02819", "pdf": "https://arxiv.org/pdf/2507.02819", "abs": "https://arxiv.org/abs/2507.02819", "authors": ["Luke Guerdan", "Devansh Saxena", "Stevie Chancellor", "Zhiwei Steven Wu", "Kenneth Holstein"], "title": "Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks", "categories": ["cs.HC", "cs.CY", "cs.LG"], "comment": null, "summary": "Data scientists often formulate predictive modeling tasks involving fuzzy,\nhard-to-define concepts, such as the \"authenticity\" of student writing or the\n\"healthcare need\" of a patient. Yet the process by which data scientists\ntranslate fuzzy concepts into a concrete, proxy target variable remains poorly\nunderstood. We interview fifteen data scientists in education (N=8) and\nhealthcare (N=7) to understand how they construct target variables for\npredictive modeling tasks. Our findings suggest that data scientists construct\ntarget variables through a bricolage process, involving iterative negotiation\nbetween high-level measurement objectives and low-level practical constraints.\nData scientists attempt to satisfy five major criteria for a target variable\nthrough bricolage: validity, simplicity, predictability, portability, and\nresource requirements. To achieve this, data scientists adaptively use problem\n(re)formulation strategies, such as swapping out one candidate target variable\nfor another when the first fails to meet certain criteria (e.g.,\npredictability), or composing multiple outcomes into a single target variable\nto capture a more holistic set of modeling objectives. Based on our findings,\nwe present opportunities for future HCI, CSCW, and ML research to better\nsupport the art and science of target variable construction.", "AI": {"tldr": "\u6570\u636e\u79d1\u5b66\u5bb6\u6784\u5efa\u9884\u6d4b\u6a21\u578b\u7684\u76ee\u6807\u53d8\u91cf\u662f\u4e00\u4e2a\u590d\u6742\u7684\u2018\u62fc\u51d1\u2019\u8fc7\u7a0b\uff0c\u6d89\u53ca\u9ad8\u9636\u6d4b\u91cf\u76ee\u6807\u548c\u4f4e\u9636\u5b9e\u9645\u7ea6\u675f\u4e4b\u95f4\u7684\u8fed\u4ee3\u534f\u5546\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u6570\u636e\u79d1\u5b66\u5bb6\u5982\u4f55\u5c06\u6a21\u7cca\u6982\u5ff5\u8f6c\u5316\u4e3a\u5177\u4f53\u7684\u4ee3\u7406\u76ee\u6807\u53d8\u91cf\uff0c\u4ee5\u586b\u8865\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8bbf\u8c0815\u540d\u6559\u80b2\u548c\u533b\u7597\u9886\u57df\u7684\u6570\u636e\u79d1\u5b66\u5bb6\uff08\u6559\u80b28\u4eba\uff0c\u533b\u75977\u4eba\uff09\uff0c\u5206\u6790\u4ed6\u4eec\u6784\u5efa\u76ee\u6807\u53d8\u91cf\u7684\u65b9\u6cd5\u3002", "result": "\u6570\u636e\u79d1\u5b66\u5bb6\u901a\u8fc7\u2018\u62fc\u51d1\u2019\u8fc7\u7a0b\u6ee1\u8db3\u76ee\u6807\u53d8\u91cf\u7684\u4e94\u4e2a\u4e3b\u8981\u6807\u51c6\uff1a\u6709\u6548\u6027\u3001\u7b80\u5355\u6027\u3001\u53ef\u9884\u6d4b\u6027\u3001\u53ef\u79fb\u690d\u6027\u548c\u8d44\u6e90\u9700\u6c42\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u672a\u6765HCI\u3001CSCW\u548cML\u7814\u7a76\u7684\u673a\u9047\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u76ee\u6807\u53d8\u91cf\u6784\u5efa\u7684\u827a\u672f\u4e0e\u79d1\u5b66\u3002"}}
{"id": "2507.01968", "pdf": "https://arxiv.org/pdf/2507.01968", "abs": "https://arxiv.org/abs/2507.01968", "authors": ["Chris Duckworth", "Zlatko Zlatev", "James Sciberras", "Peter Hallett", "Enrico Gerding"], "title": "Optimising task allocation to balance business goals and worker well-being for financial service workforces", "categories": ["q-fin.GN", "cs.HC"], "comment": "Accepted in Journal of Modelling in Management", "summary": "Purpose: Financial service companies manage huge volumes of data which\nrequires timely error identification and resolution. The associated tasks to\nresolve these errors frequently put financial analyst workforces under\nsignificant pressure leading to resourcing challenges and increased business\nrisk. To address this challenge, we introduce a formal task allocation model\nwhich considers both business orientated goals and analyst well-being.\n  Methodology: We use a Genetic Algorithm (GA) to optimise our formal model to\nallocate and schedule tasks to analysts. The proposed solution is able to\nallocate tasks to analysts with appropriate skills and experience, while taking\ninto account staff well-being objectives.\n  Findings: We demonstrate our GA model outperforms baseline heuristics,\ncurrent working practice, and is applicable to a range of single and\nmulti-objective real-world scenarios. We discuss the potential for\nmetaheuristics (such as GAs) to efficiently find sufficiently good allocations\nwhich can provide recommendations for financial service managers in-the-loop.\n  Originality: A key gap in existing allocation and scheduling models, is fully\nconsidering worker well-being. This paper presents an allocation model which\nexplicitly optimises for well-being while still improving on current working\npractice for efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e1a\u52a1\u76ee\u6807\u548c\u5458\u5de5\u798f\u7949\u7684\u6b63\u5f0f\u4efb\u52a1\u5206\u914d\u6a21\u578b\uff0c\u91c7\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u5206\u914d\u548c\u8c03\u5ea6\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u8003\u8651\u4e86\u5458\u5de5\u798f\u7949\u3002", "motivation": "\u91d1\u878d\u670d\u52a1\u4e1a\u5904\u7406\u5927\u91cf\u6570\u636e\uff0c\u9519\u8bef\u89e3\u51b3\u4efb\u52a1\u5bf9\u5206\u6790\u5e08\u9020\u6210\u538b\u529b\uff0c\u5bfc\u81f4\u8d44\u6e90\u6311\u6218\u548c\u4e1a\u52a1\u98ce\u9669\u589e\u52a0\uff0c\u9700\u540c\u65f6\u517c\u987e\u6548\u7387\u548c\u5458\u5de5\u798f\u7949\u3002", "method": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u4efb\u52a1\u5206\u914d\u6a21\u578b\uff0c\u8003\u8651\u6280\u80fd\u3001\u7ecf\u9a8c\u548c\u5458\u5de5\u798f\u7949\u76ee\u6807\u3002", "result": "\u9057\u4f20\u7b97\u6cd5\u6a21\u578b\u4f18\u4e8e\u57fa\u51c6\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u73b0\u6709\u5b9e\u8df5\uff0c\u9002\u7528\u4e8e\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u573a\u666f\uff0c\u53ef\u63d0\u4f9b\u9ad8\u6548\u5206\u914d\u5efa\u8bae\u3002", "conclusion": "\u6a21\u578b\u586b\u8865\u4e86\u73b0\u6709\u4efb\u52a1\u5206\u914d\u4e2d\u5ffd\u89c6\u5458\u5de5\u798f\u7949\u7684\u7a7a\u767d\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4e3a\u91d1\u878d\u670d\u52a1\u7ba1\u7406\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02183", "pdf": "https://arxiv.org/pdf/2507.02183", "abs": "https://arxiv.org/abs/2507.02183", "authors": ["Russell Beale"], "title": "Computer Science Education in the Age of Generative AI", "categories": ["cs.CY", "cs.HC", "H.5.0; K.3.1; K.3.2"], "comment": null, "summary": "Generative AI tools - most notably large language models (LLMs) like ChatGPT\nand Codex - are rapidly revolutionizing computer science education. These tools\ncan generate, debug, and explain code, thereby transforming the landscape of\nprogramming instruction. This paper examines the profound opportunities that AI\noffers for enhancing computer science education in general, from coding\nassistance to fostering innovative pedagogical practices and streamlining\nassessments. At the same time, it highlights challenges including academic\nintegrity concerns, the risk of over-reliance on AI, and difficulties in\nverifying originality. We discuss what computer science educators should teach\nin the AI era, how to best integrate these technologies into curricula, and the\nbest practices for assessing student learning in an environment where AI can\ngenerate code, prototypes and user feedback. Finally, we propose a set of\npolicy recommendations designed to harness the potential of generative AI while\npreserving the integrity and rigour of computer science education. Empirical\ndata and emerging studies are used throughout to support our arguments.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff08\u5982ChatGPT\u548cCodex\uff09\u6b63\u5728\u6539\u53d8\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\uff0c\u63d0\u4f9b\u7f16\u7801\u8f85\u52a9\u548c\u521b\u65b0\u6559\u5b66\u65b9\u6cd5\u7684\u673a\u4f1a\uff0c\u540c\u65f6\u4e5f\u5e26\u6765\u5b66\u672f\u8bda\u4fe1\u548c\u8fc7\u5ea6\u4f9d\u8d56\u7b49\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5982\u4f55\u4e3a\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u5e26\u6765\u53d8\u9769\uff0c\u540c\u65f6\u5e94\u5bf9\u76f8\u5173\u6311\u6218\u3002", "method": "\u5206\u6790AI\u5728\u7f16\u7a0b\u6559\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u8bfe\u7a0b\u6574\u5408\u548c\u8bc4\u4f30\u7b56\u7565\uff0c\u652f\u6301\u5b9e\u8bc1\u6570\u636e\u3002", "result": "\u63d0\u51fa\u4e86\u5728AI\u65f6\u4ee3\u4fdd\u6301\u6559\u80b2\u4e25\u8c28\u6027\u7684\u653f\u7b56\u5efa\u8bae\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u6709\u6f5c\u529b\u63d0\u5347\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\uff0c\u4f46\u9700\u5e73\u8861\u521b\u65b0\u4e0e\u5b66\u672f\u8bda\u4fe1\u3002"}}
{"id": "2507.02207", "pdf": "https://arxiv.org/pdf/2507.02207", "abs": "https://arxiv.org/abs/2507.02207", "authors": ["Nathan Kawamoto", "Daniel Hoover", "Jonathan Xie", "Jacob Walters", "Katie Snyder", "Aditi Verma"], "title": "Public perspectives on the design of fusion energy facilities", "categories": ["physics.soc-ph", "cs.HC", "physics.ed-ph", "physics.plasm-ph"], "comment": "33 pages", "summary": "As fusion energy technologies approach demonstration and commercial\ndeployment, understanding public perspectives on future fusion facilities will\nbe critical for achieving social license, especially because fusion energy\nfacilities, unlike large fission reactors, may be sited in closer proximity to\npeople and communities, due to distinct regulatory frameworks. In a departure\nfrom the 'decide-announce-defend' approach typically used to site energy\ninfrastructure, we develop a participatory design methodology for\ncollaboratively designing fusion energy facilities with prospective host\ncommunities. We present here our findings from a participatory design workshop\nthat brought together 22 community participants and 34 engineering students.\nOur analysis of the textual and visual data from this workshop shows a range of\ndesign values and decision-making criteria with 'integrity' and 'respect'\nranking highest among values and 'economic benefits' and 'environmental\nprotection/safety' ranking highest among decision-making criteria. Salient\ndesign themes that emerge across facility concepts include connecting the\nhistory and legacy of the community to the design of the facility, care for\nworkers, transparency and access to the facility, and health and safety of the\nhost community. Participants reported predominantly positive sentiments,\nexpressing joy and surprise as the workshop progressed from learning about\nfusion to designing the hypothetical facility. Our findings suggest that\ncarrying out participatory design in the early stages of technology development\ncan invite and make concrete public hopes and concerns, improve understanding\nof, and curiosity about, an emerging technology, build toward social license,\nand inform context-specific development of fusion energy facilities.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5728\u805a\u53d8\u80fd\u6e90\u8bbe\u65bd\u9009\u5740\u548c\u8bbe\u8ba1\u9636\u6bb5\u878d\u5165\u793e\u533a\u610f\u89c1\uff0c\u4ee5\u5b9e\u73b0\u793e\u4f1a\u8ba4\u53ef\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u793e\u533a\u548c\u5b66\u751f\u7684\u8bbe\u8ba1\u4ef7\u503c\u89c2\u5305\u62ec\u8bda\u4fe1\u548c\u5c0a\u91cd\uff0c\u51b3\u7b56\u6807\u51c6\u5219\u56f4\u7ed5\u7ecf\u6d4e\u5229\u76ca\u548c\u73af\u5883\u4fdd\u62a4\u3002", "motivation": "\u968f\u7740\u805a\u53d8\u80fd\u6e90\u6280\u672f\u63a5\u8fd1\u5546\u4e1a\u90e8\u7f72\uff0c\u516c\u4f17\u5bf9\u8bbe\u65bd\u7684\u63a5\u53d7\u5ea6\u5bf9\u793e\u4f1a\u8bb8\u53ef\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u56e0\u4e3a\u805a\u53d8\u8bbe\u65bd\u7684\u9009\u5740\u53ef\u80fd\u4e0e\u793e\u533a\u66f4\u8fd1\u3002\u4f20\u7edf\u51b3\u7b56\u65b9\u5f0f\uff08\u51b3\u5b9a-\u5ba3\u5e03-\u8fa9\u62a4\uff09\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u91c7\u7528\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7ec4\u7ec7\u793e\u533a\u53c2\u4e0e\u8005\u548c\u5de5\u7a0b\u5b66\u751f\u5171\u540c\u8bbe\u8ba1\u805a\u53d8\u80fd\u6e90\u8bbe\u65bd\uff0c\u5206\u6790\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\uff0c\u63d0\u53d6\u8bbe\u8ba1\u4ef7\u503c\u89c2\u548c\u51b3\u7b56\u6807\u51c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bda\u4fe1\u548c\u5c0a\u91cd\u662f\u6700\u91cd\u8981\u7684\u4ef7\u503c\u89c2\uff0c\u7ecf\u6d4e\u5229\u76ca\u548c\u73af\u5883\u4fdd\u62a4\u662f\u6700\u5173\u952e\u7684\u51b3\u7b56\u6807\u51c6\u3002\u8bbe\u8ba1\u4e3b\u9898\u5305\u62ec\u793e\u533a\u5386\u53f2\u4f20\u627f\u3001\u5de5\u4eba\u5173\u6000\u3001\u900f\u660e\u5ea6\u53ca\u5065\u5eb7\u5b89\u5168\u3002", "conclusion": "\u65e9\u671f\u5f15\u5165\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u53ef\u4ee5\u660e\u786e\u516c\u4f17\u671f\u671b\u4e0e\u62c5\u5fe7\uff0c\u63d0\u5347\u5bf9\u65b0\u5174\u6280\u672f\u7684\u7406\u89e3\u4e0e\u5174\u8da3\uff0c\u4fc3\u8fdb\u793e\u4f1a\u8ba4\u53ef\uff0c\u5e76\u4e3a\u805a\u53d8\u8bbe\u65bd\u5f00\u53d1\u63d0\u4f9b\u9488\u5bf9\u6027\u6307\u5bfc\u3002"}}
{"id": "2507.02320", "pdf": "https://arxiv.org/pdf/2507.02320", "abs": "https://arxiv.org/abs/2507.02320", "authors": ["Haodong Zhang", "Hongqi Li"], "title": "Transformer-based EEG Decoding: A Survey", "categories": ["cs.LG", "cs.HC"], "comment": "Submitted to IEEE Journals", "summary": "Electroencephalography (EEG) is one of the most common signals used to\ncapture the electrical activity of the brain, and the decoding of EEG, to\nacquire the user intents, has been at the forefront of brain-computer/machine\ninterfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods\nwith machine learning, the advent of deep learning approaches have gradually\nrevolutionized the field by providing an end-to-end long-cascaded architecture,\nwhich can learn more discriminative features automatically. Among these,\nTransformer is renowned for its strong handling capability of sequential data\nby the attention mechanism, and the application of Transformers in various EEG\nprocessing tasks is increasingly prevalent. This article delves into a relevant\nsurvey, summarizing the latest application of Transformer models in EEG\ndecoding since it appeared. The evolution of the model architecture is followed\nto sort and organize the related advances, in which we first elucidate the\nfundamentals of the Transformer that benefits EEG decoding and its direct\napplication. Then, the common hybrid architectures by integrating basic\nTransformer with other deep learning techniques\n(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial\nnetworks, diffusion models, etc.) is overviewed in detail. The research\nadvances of applying the modified intrinsic structures of customized\nTransformer have also been introduced. Finally, the current challenges and\nfuture development prospects in this rapidly evolving field are discussed. This\npaper aims to help readers gain a clear understanding of the current state of\nTransformer applications in EEG decoding and to provide valuable insights for\nfuture research endeavors.", "AI": {"tldr": "\u7efc\u8ff0\u8bba\u6587\u63a2\u8ba8\u4e86Transformer\u6a21\u578b\u5728\u8111\u7535\u56fe\uff08EEG\uff09\u89e3\u7801\u4e2d\u7684\u6700\u65b0\u5e94\u7528\uff0c\u5305\u62ec\u5176\u57fa\u7840\u539f\u7406\u3001\u6df7\u5408\u67b6\u6784\u53ca\u672a\u6765\u53d1\u5c55\u3002", "motivation": "\u63a8\u52a8EEG\u89e3\u7801\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982Transformer\uff09\u63d0\u9ad8\u8111\u673a\u63a5\u53e3\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u548c\u603b\u7ed3Transformer\u5728EEG\u89e3\u7801\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u5176\u67b6\u6784\u6f14\u53d8\u548c\u4e0e\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u5982\u5377\u79ef/\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u7ed3\u5408\u3002", "result": "\u6574\u7406\u5e76\u6982\u8ff0\u4e86Transformer\u5728EEG\u89e3\u7801\u4e2d\u7684\u8fdb\u5c55\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u548c\u6f5c\u529b\u3002", "conclusion": "Transformer\u5728EEG\u89e3\u7801\u4e2d\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u63a2\u7d22\u6df7\u5408\u67b6\u6784\u3002"}}
{"id": "2507.02400", "pdf": "https://arxiv.org/pdf/2507.02400", "abs": "https://arxiv.org/abs/2507.02400", "authors": ["Maximilian Zipfl", "Pascal Zwick", "Patrick Schulz", "Marc Rene Zofka", "Albert Schotschneider", "Helen Gremmelmaier", "Nikolai Polley", "Ferdinand M\u00fctsch", "Kevin Simon", "Fabian Gottselig", "Michael Frey", "Sergio Marschall", "Akim Stark", "Maximilian M\u00fcller", "Marek Wehmer", "Mihai Kocsis", "Dominic Waldenmayer", "Florian Schnepf", "Erik Heinrich", "Sabrina Pletz", "Matthias K\u00f6lle", "Karin Langbein-Euchner", "Alexander Viehl", "Raoul Z\u00f6llner", "J. Marius Z\u00f6llner"], "title": "DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the IEEE IAVVC 2025 Conference", "summary": "In the future, mobility will be strongly shaped by the increasing use of\ndigitalization. Not only will individual road users be highly interconnected,\nbut also the road and associated infrastructure. At that point, a Digital Twin\nbecomes particularly appealing because, unlike a basic simulation, it offers a\ncontinuous, bilateral connection linking the real and virtual environments.\nThis paper describes the digital reconstruction used to develop the Digital\nTwin of the Test Area Autonomous Driving-Baden-W\\\"urttemberg (TAF-BW), Germany.\nThe TAF-BW offers a variety of different road sections, from high-traffic urban\nintersections and tunnels to multilane motorways. The test area is equipped\nwith a comprehensive Vehicle-to-Everything (V2X) communication infrastructure\nand multiple intelligent intersections equipped with camera sensors to\nfacilitate real-time traffic flow monitoring. The generation of authentic data\nas input for the Digital Twin was achieved by extracting object lists at the\nintersections. This process was facilitated by the combined utilization of\ncamera images from the intelligent infrastructure and LiDAR sensors mounted on\na test vehicle. Using a unified interface, recordings from real-world\ndetections of traffic participants can be resimulated. Additionally, the\nsimulation framework's design and the reconstruction process is discussed. The\nresulting framework is made publicly available for download and utilization at:\nhttps://digit4taf-bw.fzi.de The demonstration uses two case studies to\nillustrate the application of the digital twin and its interfaces: the analysis\nof traffic signal systems to optimize traffic flow and the simulation of\nsecurity-related scenarios in the communications sector.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63cf\u8ff0\u4e86\u4e3a\u5fb7\u56fdTAF-BW\u6d4b\u8bd5\u533a\u5f00\u53d1\u6570\u5b57\u5b6a\u751f\u7684\u6570\u5b57\u91cd\u5efa\u8fc7\u7a0b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4ea4\u901a\u4f18\u5316\u548c\u5b89\u5168\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5316\u7684\u53d1\u5c55\uff0c\u4ea4\u901a\u7ba1\u7406\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u5b57\u5b6a\u751f\u4e3a\u6b64\u63d0\u4f9b\u4e86\u5b9e\u65f6\u7684\u865a\u5b9e\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u548c\u8f66\u8f7dLiDAR\u4f20\u611f\u5668\u63d0\u53d6\u5bf9\u8c61\u5217\u8868\uff0c\u751f\u6210\u771f\u5b9e\u6570\u636e\u8f93\u5165\u6570\u5b57\u5b6a\u751f\uff0c\u5e76\u8bbe\u8ba1\u7edf\u4e00\u7684\u6a21\u62df\u6846\u67b6\u3002", "result": "\u5f00\u53d1\u4e86\u516c\u5f00\u53ef\u7528\u7684\u6a21\u62df\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u4ea4\u901a\u4fe1\u53f7\u4f18\u5316\u548c\u901a\u4fe1\u5b89\u5168\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5728\u4ea4\u901a\u7ba1\u7406\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u901a\u8fc7\u865a\u5b9e\u7ed3\u5408\u4f18\u5316\u51b3\u7b56\u548c\u573a\u666f\u6a21\u62df\u3002"}}
{"id": "2507.02438", "pdf": "https://arxiv.org/pdf/2507.02438", "abs": "https://arxiv.org/abs/2507.02438", "authors": ["Shivam Chaubey", "Francesco Verdoja", "Shankar Deka", "Ville Kyrki"], "title": "MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Shared control combines human intention with autonomous decision-making, from\nlow-level safety overrides to high-level task guidance, enabling systems that\nadapt to users while ensuring safety and performance. This enhances task\neffectiveness and user experience across domains such as assistive robotics,\nteleoperation, and autonomous driving. However, existing shared control\nmethods, based on e.g. Model Predictive Control, Control Barrier Functions, or\nlearning-based control, struggle with feasibility, scalability, or safety\nguarantees, particularly since the user input is unpredictable.\n  To address these challenges, we propose an assistive controller framework\nbased on Constrained Optimal Control Problem that incorporates an\noffline-computed Control Invariant Set, enabling online computation of control\nactions that ensure feasibility, strict constraint satisfaction, and minimal\noverride of user intent. Moreover, the framework can accommodate structured\nclass of non-convex constraints, which are common in real-world scenarios. We\nvalidate the approach through a large-scale user study with 66\nparticipants--one of the most extensive in shared control research--using a\ncomputer game environment to assess task load, trust, and perceived control, in\naddition to performance. The results show consistent improvements across all\nthese aspects without compromising safety and user intent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u8f85\u52a9\u63a7\u5236\u5668\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u7ebf\u8ba1\u7b97\u7684\u63a7\u5236\u4e0d\u53d8\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\u5728\u53ef\u884c\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\u5728\u5904\u7406\u7528\u6237\u4e0d\u53ef\u9884\u6d4b\u8f93\u5165\u65f6\u7684\u53ef\u884c\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u7ed3\u5408\u79bb\u7ebf\u8ba1\u7b97\u7684\u63a7\u5236\u4e0d\u53d8\u96c6\uff0c\u5728\u7ebf\u751f\u6210\u63a7\u5236\u52a8\u4f5c\uff0c\u786e\u4fdd\u53ef\u884c\u6027\u3001\u4e25\u683c\u7ea6\u675f\u6ee1\u8db3\u548c\u6700\u5c0f\u5316\u7528\u6237\u610f\u56fe\u8986\u76d6\u3002", "result": "\u901a\u8fc766\u540d\u53c2\u4e0e\u8005\u7684\u5927\u89c4\u6a21\u7528\u6237\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u8d1f\u8377\u3001\u4fe1\u4efb\u5ea6\u3001\u611f\u77e5\u63a7\u5236\u548c\u6027\u80fd\u65b9\u9762\u7684\u5168\u9762\u63d0\u5347\uff0c\u4e14\u4e0d\u635f\u5bb3\u5b89\u5168\u6027\u548c\u7528\u6237\u610f\u56fe\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u6539\u5584\u4e86\u5171\u4eab\u63a7\u5236\u7cfb\u7edf\u7684\u6027\u80fd\u4e0e\u7528\u6237\u4f53\u9a8c\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u7684\u975e\u51f8\u7ea6\u675f\u573a\u666f\u3002"}}
{"id": "2507.02510", "pdf": "https://arxiv.org/pdf/2507.02510", "abs": "https://arxiv.org/abs/2507.02510", "authors": ["Ahmed G. Habashi", "Ahmed M. Azab", "Seif Eldawlatly", "Gamal M. Aly"], "title": "TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification", "categories": ["cs.LG", "cs.HC", "cs.NE"], "comment": null, "summary": "Cross-subject motor imagery (CS-MI) classification in brain-computer\ninterfaces (BCIs) is a challenging task due to the significant variability in\nElectroencephalography (EEG) patterns across different individuals. This\nvariability often results in lower classification accuracy compared to\nsubject-specific models, presenting a major barrier to developing\ncalibration-free BCIs suitable for real-world applications. In this paper, we\nintroduce a novel approach that significantly enhances cross-subject MI\nclassification performance through optimized preprocessing and deep learning\ntechniques. Our approach involves direct classification of Short-Time Fourier\nTransform (STFT)-transformed EEG data, optimized STFT parameters, and a\nbalanced batching strategy during training of a Convolutional Neural Network\n(CNN). This approach is uniquely validated across four different datasets,\nincluding three widely-used benchmark datasets leading to substantial\nimprovements in cross-subject classification, achieving 67.60% on the BCI\nCompetition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on\nDataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we\nsystematically investigate the classification performance using MI windows\nranging from the full 4-second window to 1-second windows. These results\nestablish a new benchmark for generalizable, calibration-free MI classification\nin addition to contributing a robust open-access dataset to advance research in\nthis domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u88ab\u8bd5\u8fd0\u52a8\u60f3\u8c61\uff08CS-MI\uff09\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u9884\u5904\u7406\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u8de8\u88ab\u8bd5\u8fd0\u52a8\u60f3\u8c61\u5206\u7c7b\u5728\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u4e2d\u7531\u4e8e\u4e2a\u4f53\u95f4\u8111\u7535\u56fe\uff08EEG\uff09\u6a21\u5f0f\u5dee\u5f02\u5927\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63a8\u52a8\u65e0\u9700\u6821\u51c6\u7684BCI\u5728\u5b9e\u9645\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u76f4\u63a5\u5206\u7c7b\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff08STFT\uff09\u5904\u7406\u7684EEG\u6570\u636e\u3001\u4f18\u5316STFT\u53c2\u6570\uff0c\u4ee5\u53ca\u5728\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u65f6\u91c7\u7528\u5e73\u8861\u6279\u5904\u7406\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff08\u5982BCI Competition IV\u6570\u636e\u96c6\u6700\u9ad880.22%\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u901a\u7528\u3001\u65e0\u9700\u6821\u51c6\u7684MI\u5206\u7c7b\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u540c\u65f6\u8d21\u732e\u4e86\u4e00\u4e2a\u5f00\u653e\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u9886\u57df\u7814\u7a76\u3002"}}
{"id": "2507.02593", "pdf": "https://arxiv.org/pdf/2507.02593", "abs": "https://arxiv.org/abs/2507.02593", "authors": ["Cornelia Gruber", "Helen Alber", "Bernd Bischl", "G\u00f6ran Kauermann", "Barbara Plank", "Matthias A\u00dfenmacher"], "title": "Revisiting Active Learning under (Human) Label Variation", "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.ML"], "comment": null, "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6807\u8bb0\u6570\u636e\u4e2d\u7684\u6807\u7b7e\u53d8\u5f02\uff08LV\uff09\u548c\u4eba\u7c7b\u6807\u7b7e\u53d8\u5f02\uff08HLV\uff09\u5982\u4f55\u5f71\u54cd\u76d1\u7763\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06HLV\u7eb3\u5165\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u5faa\u73af\u7684\u6982\u5ff5\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u6807\u8bb0\u6570\u636e\u5047\u8bbe\u5355\u4e00\u771f\u5b9e\u6807\u7b7e\uff0c\u5ffd\u7565\u4e86HLV\u4f5c\u4e3a\u4fe1\u606f\u4fe1\u53f7\u7684\u91cd\u8981\u6027\uff0c\u4e14\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7b80\u5316\u5047\u8bbe\u5728HLV\u5b58\u5728\u65f6\u96be\u4ee5\u6210\u7acb\u3002", "method": "\u5206\u6790\u4e86LV\u7684\u6765\u6e90\uff0c\u63d0\u51fa\u5c06\u5176\u5206\u89e3\u4e3a\u4fe1\u53f7\uff08HLV\uff09\u548c\u566a\u58f0\uff08\u5982\u6807\u8bb0\u9519\u8bef\uff09\uff0c\u5e76\u6784\u5efa\u4e86HLV\u611f\u77e5\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u6574\u5408HLV\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ec\u5b9e\u4f8b\u9009\u62e9\u3001\u6807\u6ce8\u8005\u9009\u62e9\u548c\u6807\u7b7e\u8868\u793a\uff0c\u5e76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u6807\u6ce8\u8005\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3aHLV\u611f\u77e5\u7684\u4e3b\u52a8\u5b66\u4e60\u5960\u5b9a\u4e86\u6982\u5ff5\u57fa\u7840\uff0c\u66f4\u771f\u5b9e\u5730\u53cd\u6620\u4e86\u5b9e\u9645\u6807\u6ce8\u7684\u590d\u6742\u6027\u3002"}}
