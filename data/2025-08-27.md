<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 16]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DB](#cs.DB) [Total: 8]
- [cs.AR](#cs.AR) [Total: 3]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 5]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CV](#cs.CV) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.SE

TL;DR: 提出的CTF-Dojo是一个可执行运行时环境，用于训练LLM，并通过自动化和可验证反馈实现高效训练，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有可扩展且通用的执行环境稀缺，限制了训练更强大ML代理的进展。

Method: 使用CTF-Dojo（658个CTF挑战的Docker容器环境）和自动化的CTF-Forge管道，快速构建训练环境。

Result: 仅用486个高质量轨迹训练LLM代理，在多个基准测试中取得高达11.6%的提升，32B模型达到31.9% Pass@1。

Conclusion: CTF-Dojo展示了基于执行的训练信号在提升ML代理性能中的关键作用，无需依赖昂贵的专有系统。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when
trained within executable runtime environments, notably excelling at software
engineering tasks through verified feedback loops. Yet, scalable and
generalizable execution-grounded environments remain scarce, limiting progress
in training more capable ML agents. We introduce CTF-Dojo, the first
large-scale executable runtime tailored for training LLMs with verifiable
feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style
challenges containerized in Docker with guaranteed reproducibility. To enable
rapid scaling without manual intervention, we develop CTF-Forge, an automated
pipeline that transforms publicly available artifacts into ready-to-use
execution environments in minutes, eliminating weeks of expert configuration
traditionally required. We trained LLM-based agents on just 486 high-quality,
execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute
gains over strong baselines across three competitive benchmarks: InterCode-CTF,
NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,
establishing a new open-weight state-of-the-art that rivals frontier models
like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a
benchmark for executable-agent learning, CTF-Dojo demonstrates that
execution-grounded training signals are not only effective but pivotal in
advancing high-performance ML agents without dependence on costly proprietary
systems.

</details>


### [2] [DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting](https://arxiv.org/abs/2508.18431)
*Kérian Fiter,Louis Malassigné-Onfroy,Bentley Oakes*

Main category: cs.SE

TL;DR: DTInsight是一种自动化工具和方法论，用于为数字孪生（DT）生成持续报告，提供可视化、特性摘要和集成功能。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生的构建和演化，需要一个工具来帮助利益相关者实时理解系统的特性和架构。

Method: DTInsight通过DT描述框架（DTDF），提供交互式架构可视化、本体数据摘要生成，并将这些功能集成到CI/CD管道中。

Result: DTInsight能够生成最新且详细的报告，提升利益相关者的理解。

Conclusion: DTInsight为数字孪生的持续报告提供了一种系统化的解决方案。

Abstract: With Digital Twin (DT) construction and evolution occurring over time,
stakeholders require tools to understand the current characteristics and
conceptual architecture of the system at any time. We introduce DTInsight, a
systematic and automated tool and methodology for producing continuous
reporting for DTs. DTInsight offers three key features: (a) an interactive
conceptual architecture visualization of DTs; (b) generation of summaries of DT
characteristics based on ontological data; and (c) integration of these outputs
into a reporting page within a continuous integration and continuous deployment
(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT
Description Framework (DTDF), DTInsight enables up-to-date and detailed reports
for enhanced stakeholder understanding.

</details>


### [3] [Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling](https://arxiv.org/abs/2508.18452)
*Pierre-Emmanuel Goffi,Raphaël Tremblay,Bentley Oakes*

Main category: cs.SE

TL;DR: 本文总结了在安全关键应用中开发双向控制数字孪生（DT）的系统方法，包括三阶段工程方法、多层安全协议及软硬件集成策略，成功将发酵监控手动采样时间减少91%。


<details>
  <summary>Details</summary>
Motivation: 工程化交互式工业DT是一项复杂任务，尤其是在实现超越被动监控的服务时。本文旨在提供一种系统的方法和实践解决方案，以实现工业环境中双向DT的部署。

Method: 采用三阶段工程方法，将被动监控系统升级为交互式Type 2 DT，实现7 bar压力系统的实时控制。涵盖多层安全协议、Arduino控制器与Unity可视化的软硬件集成策略及实时同步解决方案。

Result: 通过模拟驱动开发和渐进式实施策略，成功将啤酒发酵监控的手动采样时间减少91%，并验证了双向控制DT在安全关键应用中的可行性。

Conclusion: 本文为开发安全关键应用中双向控制DT的从业者提供了实用指导，强调了安全优先设计、跨领域协作和渐进式实施策略的重要性。

Abstract: Successfully engineering interactive industrial DTs is a complex task,
especially when implementing services beyond passive monitoring. We present
here an experience report on engineering a safety-critical digital twin (DT)
for beer fermentation monitoring, which provides continual sampling and reduces
manual sampling time by 91%. We document our systematic methodology and
practical solutions for implementing bidirectional DTs in industrial
environments. This includes our three-phase engineering approach that
transforms a passive monitoring system into an interactive Type 2 DT with
real-time control capabilities for pressurized systems operating at seven bar.
We contribute details of multi-layered safety protocols, hardware-software
integration strategies across Arduino controllers and Unity visualization, and
real-time synchronization solutions. We document specific engineering
challenges and solutions spanning interdisciplinary integration, demonstrating
how our use of the constellation reporting framework facilitates cross-domain
collaboration. Key findings include the critical importance of safety-first
design, simulation-driven development, and progressive implementation
strategies. Our work thus provides actionable guidance for practitioners
developing DTs requiring bidirectional control in safety-critical applications.

</details>


### [4] [How do Humans and LLMs Process Confusing Code?](https://arxiv.org/abs/2508.18547)
*Youssef Abdelsalam,Norman Peitek,Anna-Maria Maurer,Mariya Toneva,Sven Apel*

Main category: cs.SE

TL;DR: 论文研究发现，大型语言模型（LLMs）和人类程序员在理解代码时的困惑点相似，并提出了一种基于LLM的数据驱动方法来识别代码中的困惑区域。


<details>
  <summary>Details</summary>
Motivation: 探索人类和LLMs在理解代码时的困惑是否一致，以优化LLMs在软件工程工作流中的应用。

Method: 通过比较LLM的困惑度（perplexity）和人类程序员的神经生理反应（如EEG），分析两者在理解干净和困惑代码时的表现。

Result: LLM的困惑度峰值与人类神经生理反应的困惑信号在位置和幅度上相关，表明两者困惑点相似。

Conclusion: 基于发现，提出了一种数据驱动方法，利用LLM识别代码中可能引发人类困惑的区域。

Abstract: Already today, humans and programming assistants based on large language
models (LLMs) collaborate in everyday programming tasks. Clearly, a
misalignment between how LLMs and programmers comprehend code can lead to
misunderstandings, inefficiencies, low code quality, and bugs.
  A key question in this space is whether humans and LLMs are confused by the
same kind of code. This would not only guide our choices of integrating LLMs in
software engineering workflows, but also inform about possible improvements of
LLMs.
  To this end, we conducted an empirical study comparing an LLM to human
programmers comprehending clean and confusing code. We operationalized
comprehension for the LLM by using LLM perplexity, and for human programmers
using neurophysiological responses (in particular, EEG-based fixation-related
potentials).
  We found that LLM perplexity spikes correlate both in terms of location and
amplitude with human neurophysiological responses that indicate confusion. This
result suggests that LLMs and humans are similarly confused about the code.
Based on these findings, we devised a data-driven, LLM-based approach to
identify regions of confusion in code that elicit confusion in human
programmers.

</details>


### [5] [LaQual: A Novel Framework for Automated Evaluation of LLM App Quality](https://arxiv.org/abs/2508.18636)
*Yan Wang,Xinyi Hou,Yanjie Zhao,Weiguo Lin,Haoyu Wang,Junjun Si*

Main category: cs.SE

TL;DR: LaQual是一个自动化框架，用于评估LLM应用的质量，通过分层分类、静态筛选和动态评估三阶段方法，显著提升了推荐效果和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用商店的排名和推荐方法主要依赖静态指标，用户难以高效找到高质量应用，因此需要一种更有效的评估框架。

Method: LaQual采用三阶段方法：分层分类LLM应用、静态指标筛选低质量应用、动态生成场景适配的评估指标和任务进行质量评估。

Result: 实验表明LaQual的自动评分与人工判断高度一致（Spearman's rho 0.62-0.60），能减少候选应用66.7%-81.3%，用户研究显示其显著优于基线系统。

Conclusion: LaQual提供了一个可扩展、客观且以用户为中心的解决方案，有效提升了LLM应用的推荐质量。

Abstract: LLM app stores are quickly emerging as platforms that gather a wide range of
intelligent applications based on LLMs, giving users many choices for content
creation, coding support, education, and more. However, the current methods for
ranking and recommending apps in these stores mostly rely on static metrics
like user activity and favorites, which makes it hard for users to efficiently
find high-quality apps. To address these challenges, we propose LaQual, an
automated framework for evaluating the quality of LLM apps. LaQual consists of
three main stages: first, it labels and classifies LLM apps in a hierarchical
way to accurately match them to different scenarios; second, it uses static
indicators, such as time-weighted user engagement and functional capability
metrics, to filter out low-quality apps; and third, it conducts a dynamic,
scenario-adaptive evaluation, where the LLM itself generates scenario-specific
evaluation metrics, scoring rules, and tasks for a thorough quality assessment.
Experiments on a popular LLM app store show that LaQual is effective. Its
automated scores are highly consistent with human judgments (with Spearman's
rho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in
travel planning). By effectively screening, LaQual can reduce the pool of
candidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual
significantly outperforms baseline systems in decision confidence, comparison
efficiency (with average scores of 5.45 compared to 3.30), and the perceived
value of its evaluation reports (4.75 versus 2.25). Overall, these results
demonstrate that LaQual offers a scalable, objective, and user-centered
solution for finding and recommending high-quality LLM apps in real-world use
cases.

</details>


### [6] [Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision](https://arxiv.org/abs/2508.18675)
*Xu Lu,Weisong Sun,Yiran Zhang,Ming Hu,Cong Tian,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出了一种名为ReDeFo的多代理框架，通过需求开发和形式化方法，提升基于LLM的代码生成质量，弥补现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码生成方法在满足实际需求和质量上存在不足，缺乏系统化的需求开发策略。

Method: 设计了一个包含三个代理的多代理框架ReDeFo，融合形式化方法的知识与技术，用于需求到代码的生成流程。

Result: 通过形式化规范桥接自然语言需求与精确代码，增强了质量保证，支持正确性推理和缺陷发现。

Conclusion: ReDeFo为实现可靠自动生成软件的目标迈出了重要一步。

Abstract: Automated code generation has long been considered the holy grail of software
engineering. The emergence of Large Language Models (LLMs) has catalyzed a
revolutionary breakthrough in this area. However, existing methods that only
rely on LLMs remain inadequate in the quality of generated code, offering no
guarantees of satisfying practical requirements. They lack a systematic
strategy for requirements development and modeling. Recently, LLM-based agents
typically possess powerful abilities and play an essential role in facilitating
the alignment of LLM outputs with user requirements. In this paper, we envision
the first multi-agent framework for reliable code generation based on
\textsc{re}quirements \textsc{de}velopment and \textsc{fo}rmalization, named
\textsc{ReDeFo}. This framework incorporates three agents, highlighting their
augmentation with knowledge and techniques of formal methods, into the
requirements-to-code generation pipeline to strengthen quality assurance. The
core of \textsc{ReDeFo} is the use of formal specifications to bridge the gap
between potentially ambiguous natural language requirements and precise
executable code. \textsc{ReDeFo} enables rigorous reasoning about correctness,
uncovering hidden bugs, and enforcing critical properties throughout the
development process. In general, our framework aims to take a promising step
toward realizing the long-standing vision of reliable, auto-generated software.

</details>


### [7] [LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging](https://arxiv.org/abs/2508.18721)
*Yunrui Pei,Hongshu Wang,Wenjie Zhang,Yun Lin,Weiyu Kong,Jin song Dong*

Main category: cs.SE

TL;DR: RecovSlicing是一种动态数据依赖分析方法，通过单次运行和部分插桩解决变量定义追踪问题，利用LLM推断程序行为，显著提高了准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要全面插桩或重复运行，成本高且对非确定性程序不可行，因此需要更高效的方法。

Method: RecovSlicing结合部分记录的执行轨迹和代码上下文，使用LLM推断缺失执行，支持显式和隐式变量。

Result: 在三个基准测试中，RecovSlicing的准确率和召回率显著优于基线方法，并帮助定位更多回归错误。

Conclusion: RecovSlicing为动态数据依赖分析提供了高效且准确的解决方案，适用于复杂程序调试。

Abstract: Dynamic data dependency, answering "why a variable has this value?", is
critical for debugging. Given a program step `s` reading a variable `v`,
finding the dynamic definition of `v` is challenging. Traditional methods
require either (1) exhaustive instrumentation of all possible definitions of
`v` in one run or (2) replicating the run to re-examine reads/writes - both
costly. If `v` is defined in a library, instrumentation becomes expensive; for
non-deterministic programs, replication is infeasible.
  We propose RecovSlicing, which computes dynamic data dependency in a single
run with partial instrumentation. We leverage LLMs to infer program behavior
from a partially recorded trace and code context. Given a trace and a slicing
criterion (step `s` and variable `v`), RecovSlicing estimates the runtime
definition of `v` by recovering the missing execution.It also supports implicit
variables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:
(1) recovering runtime values and structures, and (2) aligning recovered
variables with recorded memory to analyze definitions.
  We evaluate RecovSlicing on 8,300 data dependencies across three slicing
benchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution
Slicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,
outperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall
(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug
localizer, it enables finding 16% more regressions.

</details>


### [8] [Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions](https://arxiv.org/abs/2508.18771)
*Kexin Sun,Hongyu Kuang,Sebastian Baltes,Xin Zhou,He Zhang,Xiaoxing Ma,Guoping Rong,Dong Shao,Christoph Treude*

Main category: cs.SE

TL;DR: 该论文研究了16种流行的基于AI的代码审查工具在GitHub工作流程中的实际影响，分析了超过22,000条评论。研究发现，工具的有效性因设计而异，简短、含代码片段且手动触发的评论更容易导致代码变更。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解基于AI的代码审查工具在实际使用中的效果和影响因素，填补当前对该领域了解不足的空白。

Method: 采用大规模实证研究方法，开发了一个两阶段的LLM辅助框架来分析评论是否被采纳，并使用可解释的机器学习识别影响因素。

Result: 研究结果显示，工具的采用率在增长，但有效性差异较大；简短、含代码片段且手动触发的评论更容易导致代码变更。

Conclusion: 结论强调了工具设计的重要性，并提出了改进基于AI的代码审查系统的方向。

Abstract: AI-based code review tools automatically review and comment on pull requests
to improve code quality. Despite their growing presence, little is known about
their actual impact. We present a large-scale empirical study of 16 popular
AI-based code review actions for GitHub workflows, analyzing more than 22,000
review comments in 178 repositories. We investigate (1) how these tools are
adopted and configured, (2) whether their comments lead to code changes, and
(3) which factors influence their effectiveness. We develop a two-stage
LLM-assisted framework to determine whether review comments are addressed, and
use interpretable machine learning to identify influencing factors. Our
findings show that, while adoption is growing, effectiveness varies widely.
Comments that are concise, contain code snippets, and are manually triggered,
particularly those from hunk-level review tools, are more likely to result in
code changes. These results highlight the importance of careful tool design and
suggest directions for improving AI-based code review systems.

</details>


### [9] [Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study](https://arxiv.org/abs/2508.18816)
*Sabato Nocera,Davide Fucci,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: 论文研究了GitHub项目如何使用和定制SonarQube Cloud这一静态代码分析工具，发现多数项目使用默认配置，但也有显著部分进行自定义以达成特定质量目标。


<details>
  <summary>Details</summary>
Motivation: 研究开源项目如何使用和定制SCA工具，以了解实际使用情况和配置效果。

Method: 通过挖掘GitHub项目与SonarQube Cloud的链接数据，分析配置和使用情况。

Result: 81%的项目正确连接到SonarQube Cloud，75%使用默认质量门，45%自定义质量门。常见条件符合“Clean as You Code”原则。

Conclusion: 项目多依赖预定义配置，但自定义配置也很普遍。未来研究可探索配置与实际软件效果的关系，以提供基于证据的SCA工具配置建议。

Abstract: Background: Static Code Analysis (SCA) tools are widely adopted to enforce
code quality standards. However, little is known about how open-source projects
use and customize these tools. Aims: This paper investigates how GitHub
projects use and customize a popular SCA tool, namely SonarQube Cloud. Method:
We conducted a mining study of GitHub projects that are linked through GitHub
Actions to SonarQube Cloud projects. Results: Among 321 GitHub projects using
SonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud
projects, while others exhibit misconfigurations or restricted access. Among
265 accessible SonarQube Cloud projects, 75% use the organization's default
quality gate, i.e., a set of conditions that deployed source code must meet to
pass automated checks. While 55% of the projects use the built-in quality gate
provided by SonarQube Cloud, 45% of them customize their quality gate with
different conditions. Overall, the most common quality conditions align with
SonarQube Cloud's "Clean as You Code" principle and enforce security,
maintainability, reliability, coverage, and a few duplicates on newly added or
modified source code. Conclusions: Many projects rely on predefined
configurations, yet a significant portion customize their configurations to
meet specific quality goals. Building on our initial results, we envision a
future research agenda linking quality gate configurations to actual software
outcomes (e.g., improvement of software security). This would enable
evidence-based recommendations for configuring SCA tools like SonarQube Cloud
in various contexts.

</details>


### [10] [Interleaving Large Language Models for Compiler Testing](https://arxiv.org/abs/2508.18955)
*Yunbo Ni,Shaohua Li*

Main category: cs.SE

TL;DR: 论文提出了一种新的编译器测试框架LegoFuzz，通过离线生成和在线组合代码片段来高效测试C编译器，发现了GCC和LLVM中的66个严重错误。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI模型测试编译器方法中存在生成的程序过于简单且计算成本高的问题。

Method: 分离线阶段（用LLM生成小而功能丰富的代码片段）和在线阶段（组合这些片段构建高质量测试程序）的两阶段测试框架。

Result: 在GCC和LLVM中发现了66个错误，近半数是现有LLM工具无法发现的严重错误（miscompilation bugs）。

Conclusion: 该设计高效，为AI模型在软件测试中的应用提供了新思路，不仅限于C编译器。

Abstract: Testing compilers with AI models, especially large language models (LLMs),
has shown great promise. However, current approaches struggle with two key
problems: The generated programs for testing compilers are often too simple,
and extensive testing with the LLMs is computationally expensive. In this
paper, we propose a novel compiler testing framework that decouples the testing
process into two distinct phases: an offline phase and an online phase. In the
offline phase, we use LLMs to generate a collection of small but feature-rich
code pieces. In the online phase, we reuse these code pieces by strategically
combining them to build high-quality and valid test programs, which are then
used to test compilers.
  We implement this idea in a tool, LegoFuzz, for testing C compilers. The
results are striking: we found 66 bugs in GCC and LLVM, the most widely used C
compilers. Almost half of the bugs are miscompilation bugs, which are serious
and hard-to-find bugs that none of the existing LLM-based tools could find. We
believe this efficient design opens up new possibilities for using AI models in
software testing beyond just C compilers.

</details>


### [11] [GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging](https://arxiv.org/abs/2508.18993)
*Ziyi Ni,Huacan Wang,Shuo Zhang,Shuo Lu,Ziyang He,Wang You,Zhenheng Tang,Yuntao Du,Bill Sun,Hongzhang Liu,Sen Hu,Ronghao Chen,Bo Li,Xin Li,Chen Hu,Binxing Jiao,Daxin Jiang,Pin Lyu*

Main category: cs.SE

TL;DR: 本文介绍了GitTaskBench，一个用于评估代码代理在真实、工作流驱动的代码任务中表现的基准测试。实验表明，当前系统在解决复杂任务时仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少评估代码代理在真实的工作流驱动场景中的表现，而这是实际软件开发中的重要能力。

Method: 通过引入54个跨7种模态和领域的任务，每个任务配有自动化的人为评估工具，衡量执行和任务成功率，并提出alpha-value指标量化经济收益。

Result: 最佳系统仅完成48.15%的任务，超过一半的失败源于环境设置和依赖解决等基础步骤。

Conclusion: GitTaskBench旨在推动对代码库感知能力的关注，推动代理解决复杂端到端现实任务的能力发展。

Abstract: Beyond scratch coding, exploiting large-scale code repositories (e.g.,
GitHub) for practical tasks is vital in real-world software development, yet
current benchmarks rarely evaluate code agents in such authentic,
workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a
benchmark designed to systematically assess this capability via 54 realistic
tasks across 7 modalities and 7 domains. Each task pairs a relevant repository
with an automated, human-curated evaluation harness specifying practical
success criteria. Beyond measuring execution and task success, we also propose
the alpha-value metric to quantify the economic benefit of agent performance,
which integrates task success rates, token cost, and average developer
salaries. Experiments across three state-of-the-art agent frameworks with
multiple advanced LLMs show that leveraging code repositories for complex task
solving remains challenging: even the best-performing system, OpenHands+Claude
3.7, solves only 48.15% of tasks. Error analysis attributes over half of
failures to seemingly mundane yet critical steps like environment setup and
dependency resolution, highlighting the need for more robust workflow
management and increased timeout preparedness. By releasing GitTaskBench, we
aim to drive progress and attention toward repository-aware code reasoning,
execution, and deployment -- moving agents closer to solving complex,
end-to-end real-world tasks. The benchmark and code are open-sourced at
https://github.com/QuantaAlpha/GitTaskBench.

</details>


### [12] [A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs](https://arxiv.org/abs/2508.19056)
*S. Panda,D. Munjal,D. P. Mohapatra*

Main category: cs.SE

TL;DR: 论文提出一种静态方法，通过计算面向对象程序中受影响部分的耦合度（ACC）来优先执行测试用例，提高早期错误检测效率。


<details>
  <summary>Details</summary>
Motivation: 测试用例优先执行的目标是优化测试顺序以早期检测错误，节省重新测试的时间和成本。

Method: 构建受影响切片图（ASG）表示受影响的程序部分，通过计算节点的ACC值确定其错误倾向性，优先执行覆盖高ACC值节点的测试用例。

Result: 实验表明，该方法能有效提高早期错误检测率，与现有技术相比具有可行性。

Conclusion: 基于ACC的静态优先执行方法在测试用例优化中表现良好。

Abstract: Test case prioritization focuses on finding a suitable order of execution of
the test cases in a test suite to meet some performance goals like detecting
faults early. It is likely that some test cases execute the program parts that
are more prone to errors and will detect more errors if executed early during
the testing process. Finding an optimal order of execution for the selected
regression test cases saves time and cost of retesting. This paper presents a
static approach to prioritizing the test cases by computing the affected
component coupling (ACC) of the affected parts of object-oriented programs. We
construct a graph named affected slice graph (ASG) to represent these affected
program parts.We determine the fault-proneness of the nodes of ASG by computing
their respective ACC values. We assign higher priority to those test cases that
cover the nodes with higher ACC values. Our analysis with mutation faults shows
that the test cases executing the fault-prone program parts have a higher
chance to reveal faults earlier than other test cases in the test suite. The
result obtained from seven case studies justifies that our approach is feasible
and gives acceptable performance in comparison to some existing techniques.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants](https://arxiv.org/abs/2508.18587)
*Barış Bayazıt,Yao Li,Xujie Si*

Main category: cs.PL

TL;DR: LLMs在证明助手中的应用效果研究，发现其在小证明上表现优异，但受外部依赖和项目差异影响。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在自动化证明生成中的有效性，特别是在成熟项目hs-to-coq和Verdi中的应用。

Method: 通过定量和定性分析，评估LLMs在生成证明时的表现，包括依赖、项目差异和错误类型。

Result: LLMs在小证明中表现优秀，能生成简洁证明，但受外部依赖和项目类型影响，且可能犯奇怪错误。

Conclusion: LLMs在自动化证明中具有潜力，但需注意项目差异和依赖性问题。

Abstract: Large language models (LLMs) can potentially help with verification using
proof assistants by automating proofs. However, it is unclear how effective
LLMs are in this task. In this paper, we perform a case study based on two
mature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the
effectiveness of LLMs in generating proofs by both quantitative and qualitative
analysis. Our study finds that: (1) external dependencies and context in the
same source file can significantly help proof generation; (2) LLMs perform
great on small proofs but can also generate large proofs; (3) LLMs perform
differently on different verification projects; and (4) LLMs can generate
concise and smart proofs, apply classical techniques to new definitions, but
can also make odd mistakes.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [14] [Exact Persistent Stochastic Non-Interference](https://arxiv.org/abs/2508.19110)
*Carla Piazza,Riccardo Romanello,Sabina Rossi*

Main category: cs.PF

TL;DR: 本文提出了Exact PSNI（EPSNI），基于弱精确等价的新安全特性，用于分析随机系统中的非干扰性。


<details>
  <summary>Details</summary>
Motivation: 重新审视PSNI，从性能角度提出更精确的行为关系，以控制量化可观察性。

Method: 引入弱精确等价（weak-exact equivalence），扩展精确等价并放松内部动作处理，定义EPSNI。

Result: EPSNI保持与PSNI相同的特性（如双模拟和展开形式），并具有类似的组合性。

Conclusion: 弱精确等价为随机系统非干扰性分析提供了坚实的基础。

Abstract: Persistent Stochastic Non-Interference (PSNI) was introduced to capture a
quantitative security property in stochastic process algebras, ensuring that a
high-level process does not influence the observable behaviour of a low-level
component, as formalised via lumpable bisimulation. In this work, we revisit
PSNI from a performance-oriented perspective and propose a new characterisation
based on a refined behavioural relation. We introduce \emph{weak-exact
equivalence}, which extends exact equivalence with a relaxed treatment of
internal (\(\tau\)) actions, enabling precise control over quantitative
observables while accommodating unobservable transitions. Based on this, we
define \emph{Exact PSNI} (EPSNI), a variant of PSNI characterised via
weak-exact equivalence. We show that EPSNI admits the same bisimulation-based
and unwinding-style characterisations as PSNI, and enjoys analogous
compositionality properties. These results confirm weak-exact equivalence as a
robust foundation for reasoning about non-interference in stochastic systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [15] [Digital Twin-Guided Energy Management over Real-Time Pub/Sub Protocol in 6G Smart Cities](https://arxiv.org/abs/2508.18516)
*Kubra Duran,Lal Verda Cakir,Sana Ullah Jan,Kerem Gursu,Berk Canberk*

Main category: cs.NI

TL;DR: 论文提出了一种基于数字孪生（DT）的能源管理框架，用于解决6G物联网中的低延迟和能源效率问题，通过强化学习优化设备数据更新时间，显著降低了延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 6G物联网的资源限制与低延迟需求之间的矛盾，以及现有管理策略缺乏实时性和连续性，促使研究团队开发新的解决方案。

Method: 采用数字孪生框架，通过分布式覆盖网络提供孪生模型，利用RTPS协议处理动态更新；设计了基于强化学习的引擎，结合DDPG算法优化设备数据更新时间。

Result: 仿真结果显示，该框架在95%百分位延迟上提升了37%，能耗降低了30%，优于现有方法。

Conclusion: 提出的数字孪生框架有效解决了6G物联网的低延迟和能源效率问题，验证了其在实践中的优越性。

Abstract: Although the emergence of 6G IoT networks has accelerated the deployment of
enhanced smart city services, the resource limitations of IoT devices remain as
a significant problem. Given this limitation, meeting the low-latency service
requirement of 6G networks becomes even more challenging. However, existing 6G
IoT management strategies lack real-time operation and mostly rely on discrete
actions, which are insufficient to optimise energy consumption. To address
these, in this study, we propose a Digital Twin (DT)-guided energy management
framework to jointly handle the low latency and energy efficiency challenges in
6G IoT networks. In this framework, we provide the twin models through a
distributed overlay network and handle the dynamic updates between the data
layer and the upper layers of the DT over the Real-Time Publish Subscribe
(RTPS) protocol. We also design a Reinforcement Learning (RL) engine with a
novel formulated reward function to provide optimal data update times for each
of the IoT devices. The RL engine receives a diverse set of environment states
from the What-if engine and runs Deep Deterministic Policy Gradient (DDPG) to
output continuous actions to the IoT devices. Based on our simulation results,
we observe that the proposed framework achieves a 37% improvement in 95th
percentile latency and a 30% reduction in energy consumption compared to the
existing literature.

</details>


### [16] [Dynamic Trajectory Optimization and Power Control for Hierarchical UAV Swarms in 6G Aerial Access Network](https://arxiv.org/abs/2508.18702)
*Ziye Jia,Jia He,Lijun He,Min Sheng,Junyu Liu,Qihui Wu,Zhu Han*

Main category: cs.NI

TL;DR: 论文提出了一种用于6G空中接入网络的分层无人机群结构，通过优化部署和轨迹以减少能耗和延迟，并使用改进的算法实现50%的复杂性降低。


<details>
  <summary>Details</summary>
Motivation: 为了解决在大规模偏远区域协同部署多无人机群的挑战，文章提出了分层无人机群结构，以扩展6G时代地面用户的连接性。

Method: 采用联合优化无人机群的动态部署和轨迹，使用K-means和Voronoi图进行区域划分，并利用改进的非支配排序鲸鱼优化算法求解多目标优化问题。

Result: 通过仿真验证，提出的算法在与基准机制比较时，实现了50%的复杂性降低。

Conclusion: 分层无人机群结构和优化算法显著提升了6G空中接入网络的性能，适用于大规模偏远区域的连接扩展。

Abstract: Unmanned aerial vehicles (UAVs) can serve as aerial base stations (BSs) to
extend the ubiquitous connectivity for ground users (GUs) in the
sixth-generation (6G) era. However, it is challenging to cooperatively deploy
multiple UAV swarms in large-scale remote areas. Hence, in this paper, we
propose a hierarchical UAV swarms structure for 6G aerial access networks,
where the head UAVs serve as aerial BSs, and tail UAVs (T-UAVs) are responsible
for relay. In detail, we jointly optimize the dynamic deployment and trajectory
of UAV swarms, which is formulated as a multi-objective optimization problem
(MOP) to concurrently minimize the energy consumption of UAV swarms and GUs, as
well as the delay of GUs. However, the proposed MOP is a mixed integer
nonlinear programming and NP-hard to solve. Therefore, we develop a K-means and
Voronoi diagram based area division method, and construct Fermat points to
establish connections between GUs and T-UAVs. Then, an improved non-dominated
sorting whale optimization algorithm is proposed to seek Pareto optimal
solutions for the transformed MOP. Finally, extensive simulations are conducted
to verify the performance of proposed algorithms by comparing with baseline
mechanisms, resulting in a 50% complexity reduction.

</details>


### [17] [Toward Edge General Intelligence with Agentic AI and Agentification: Concepts, Technologies, and Future Directions](https://arxiv.org/abs/2508.18725)
*Ruichen Zhang,Guangyuan Liu,Yinqiu Liu,Changyuan Zhao,Jiacheng Wang,Yunting Xu,Dusit Niyato,Jiawen Kang,Yonghui Li,Shiwen Mao,Sumei Sun,Xuemin Shen,Dong In Kim*

Main category: cs.NI

TL;DR: 本文综述了代理人工智能（Agentic AI）在边缘通用智能中的应用，探讨了其与传统边缘智能的区别、关键技术、案例研究，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着6G和IoT的快速发展，传统边缘智能方法无法应对动态、异构和资源受限的场景，代理AI提供了变革性解决方案。

Method: 论文首先介绍基础概念和与传统边缘智能的区别，随后分析关键技术（如模型压缩、能效计算等），并通过案例研究展示应用效果。

Result: 代理AI在低空经济网络、车辆网络等领域展示了强大能力，并通过数值评估验证了其有效性。

Conclusion: 代理AI是边缘智能的范式转变，但仍需解决研究挑战以实现规模化、可信赖的部署。

Abstract: The rapid expansion of sixth-generation (6G) wireless networks and the
Internet of Things (IoT) has catalyzed the evolution from centralized cloud
intelligence towards decentralized edge general intelligence. However,
traditional edge intelligence methods, characterized by static models and
limited cognitive autonomy, fail to address the dynamic, heterogeneous, and
resource-constrained scenarios inherent to emerging edge networks. Agentic
artificial intelligence (Agentic AI) emerges as a transformative solution,
enabling edge systems to autonomously perceive multimodal environments, reason
contextually, and adapt proactively through continuous
perception-reasoning-action loops. In this context, the agentification of edge
intelligence serves as a key paradigm shift, where distributed entities evolve
into autonomous agents capable of collaboration and continual adaptation. This
paper presents a comprehensive survey dedicated to Agentic AI and
agentification frameworks tailored explicitly for edge general intelligence.
First, we systematically introduce foundational concepts and clarify
distinctions from traditional edge intelligence paradigms. Second, we analyze
important enabling technologies, including compact model compression,
energy-aware computing strategies, robust connectivity frameworks, and advanced
knowledge representation and reasoning mechanisms. Third, we provide
representative case studies demonstrating Agentic AI's capabilities in
low-altitude economy networks, intent-driven networking, vehicular networks,
and human-centric service provisioning, supported by numerical evaluations.
Furthermore, we identify current research challenges, review emerging
open-source platforms, and highlight promising future research directions to
guide robust, scalable, and trustworthy Agentic AI deployments for
next-generation edge environments.

</details>


### [18] [A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks](https://arxiv.org/abs/2508.18803)
*Jiaqi Wu,Jing Liu,Yang Liu,Lixu Wang,Zehua Wang,Wei Chen,Zijian Tian,Richard Yu,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 该论文是一篇关于云边端协同智能（CETCI）在AIoT中应用的综述，介绍了其基础架构、关键技术及应用场景，并探讨了未来挑战和趋势。


<details>
  <summary>Details</summary>
Motivation: 随着IoT设备和AI服务的快速增长，高效分布式计算和网络架构需求增加，推动了CETCI在AIoT中的发展。

Method: 系统分析云、边、端三层架构，探讨网络虚拟化、容器编排等技术，并分类协作范式如任务卸载和资源分配。

Result: 综述了CETCI的架构、技术和协作模式，提出了智能协作学习框架及分布式计算的未来方向。

Conclusion: CETCI为AIoT提供了高效协作方案，但需解决可扩展性、异构性等挑战，未来可结合6G+、量子计算等技术发展。

Abstract: The proliferation of Internet of things (IoT) devices in smart cities,
transportation, healthcare, and industrial applications, coupled with the
explosive growth of AI-driven services, has increased demands for efficient
distributed computing architectures and networks, driving cloud-edge-terminal
collaborative intelligence (CETCI) as a fundamental paradigm within the
artificial intelligence of things (AIoT) community. With advancements in deep
learning, large language models (LLMs), and edge computing, CETCI has made
significant progress with emerging AIoT applications, moving beyond isolated
layer optimization to deployable collaborative intelligence systems for AIoT
(CISAIOT), a practical research focus in AI, distributed computing, and
communications. This survey describes foundational architectures, enabling
technologies, and scenarios of CETCI paradigms, offering a tutorial-style
review for CISAIOT beginners. We systematically analyze architectural
components spanning cloud, edge, and terminal layers, examining core
technologies including network virtualization, container orchestration, and
software-defined networking, while presenting categorizations of collaboration
paradigms that cover task offloading, resource allocation, and optimization
across heterogeneous infrastructures. Furthermore, we explain intelligent
collaboration learning frameworks by reviewing advances in federated learning,
distributed deep learning, edge-cloud model evolution, and reinforcement
learning-based methods. Finally, we discuss challenges (e.g., scalability,
heterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum
computing, digital twin), highlighting how integration of distributed computing
and communication can address open issues and guide development of robust,
efficient, and secure collaborative AIoT systems.

</details>


### [19] [Network Calculus Results for TSN: An Introduction](https://arxiv.org/abs/2508.18855)
*Lisa Maile,Kai-Steffen Hielscher,Reinhard German*

Main category: cs.NI

TL;DR: 该论文综述了时间敏感网络（TSN）中网络演算（NC）的应用，整理并统一了不同分析方法的主要结果，提出了一种改进模型以描述发送端设备的输出，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: TSN为工业网络提供了实时通信保障，而NC可用于计算网络中的延迟和缓冲区上限。然而，现有的NC分析方法分散且缺乏统一性，因此论文旨在整合这些方法并提供一致的研究框架。

Method: 论文通过文献综述，整理并对比了不同的NC分析方法和结果，引入了一致的符号系统以展示结果之间的依赖关系，并提出了一种改进的发送端设备输出模型。

Result: 研究统一了NC在TSN中的应用，明确了各种方法的异同和假设条件，为工业网络的分析提供了全面参考，并指出了模型改进的潜力。

Conclusion: 论文为TSN中的NC研究提供了系统化的综述和统一框架，识别了未来研究的可能方向，有助于推动该领域的进一步发展。

Abstract: Time-Sensitive Networking (TSN) is a set of standards that enables the
industry to provide real-time guarantees for time-critical communications with
Ethernet hardware. TSN supports various queuing and scheduling mechanisms and
allows the integration of multiple traffic types in a single network. Network
Calculus (NC) can be used to calculate upper bounds for latencies and buffer
sizes within these networks, for example, for safety or real-time traffic. We
explain the relevance of NC for TSN-based computer communications and potential
areas of application. Different NC analysis approaches have been published to
examine different parts of TSN and this paper provides a survey of these
publications and presents their main results, dependencies, and differences. We
present a consistent presentation of the most important results and suggest an
improvement to model the output of sending end-devices. To ease access to the
current research status, we introduce a common notation to show how all results
depend on each other and also identify common assumptions. Thus, we offer a
comprehensive overview of NC for industrial networks and identify possible
areas for future work.

</details>


### [20] [Saving Energy with Relaxed Latency Constraints: A Study on Data Compression and Communication](https://arxiv.org/abs/2508.18863)
*Pietro Talli,Anup Mishra,Federico Chiariotti,Israel Leyva-Mayorga,Andrea Zanella,Petar Popovski*

Main category: cs.NI

TL;DR: 研究边缘计算中预处理（如数据压缩）和传输之间的多维优化问题，分析能量、延迟和可靠性的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着边缘计算的发展，预处理的能量和延迟与传输的需求之间的平衡成为关键问题。

Method: 引入一个简单模型，考察压缩和通信操作的权衡，研究Pareto前沿。

Result: 结果显示能量成本随延迟减少呈指数增长，放松延迟要求可显著节能。

Conclusion: 挑战传统的固定延迟目标，建议采用基于应用的具体延迟预算。

Abstract: With the advent of edge computing, data generated by end devices can be
pre-processed before transmission, possibly saving transmission time and
energy. On the other hand, data processing itself incurs latency and energy
consumption, depending on the complexity of the computing operations and the
speed of the processor. The energy-latency-reliability profile resulting from
the concatenation of pre-processing operations (specifically, data compression)
and data transmission is particularly relevant in wireless communication
services, whose requirements may change dramatically with the application
domain. In this paper, we study this multi-dimensional optimization problem,
introducing a simple model to investigate the tradeoff among end-to-end
latency, reliability, and energy consumption when considering compression and
communication operations in a constrained wireless device. We then study the
Pareto fronts of the energy-latency trade-off, considering data compression
ratio and device processing speed as key design variables. Our results show
that the energy costs grows exponentially with the reduction of the end-to-end
latency, so that considerable energy saving can be obtained by slightly
relaxing the latency requirements of applications. These findings challenge
conventional rigid communication latency targets, advocating instead for
application-specific end-to-end latency budgets that account for computational
and transmission overhead.

</details>


### [21] [Combining Static and Dynamic Traffic with Delay Guarantees in Time-Sensitive Networking](https://arxiv.org/abs/2508.18883)
*Lisa Maile,Kai-Steffen Hielscher,Reinhard German*

Main category: cs.NI

TL;DR: 研究了时间敏感网络中支持静态和动态流量的资源分配算法，结合离线和在线优化，提升了网络延迟和灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决时间敏感网络中资源分配算法的标准化空白，确保低延迟和可靠性。

Method: 结合离线网络优化启发式和在线准入控制，使用Network Calculus框架验证Credit-Based Shaper网络。

Result: 与直觉和暴力算法相比，显著提高了质量和运行时性能，保证最大端到端延迟。

Conclusion: 该方案通过最小用户输入，提升了网络灵活性并确保延迟保障。

Abstract: To support reliable and low-latency communication, Time-Sensitive Networking
introduced protocols and interfaces for resource allocation in Ethernet.
However, the implementation of these allocation algorithms has not yet been
covered by the standards. Our work focuses on deadline-guaranteeing resource
allocation for networks with static and dynamic traffic. To achieve this, we
combine offline network optimization heuristics with online admission control
and, thus, allow for new flow registrations while the network is running. We
demonstrate our solution on Credit-Based Shaper networks by using the delay
analysis framework Network Calculus. We compare our approach with an intuitive
and a brute-force algorithm, where we can achieve significant improvements,
both, in terms of quality and runtime. Thereby, our results show that we can
guarantee maximum end-to-end delays and also increase the flexibility of the
network while requiring only minimal user input.

</details>


### [22] [Adaptive 6G Networks-in-Network Management for Industrial Applications](https://arxiv.org/abs/2508.18902)
*Daniel Lindenschmitt,Paul Seehofer,Marius Schmitz,Jan Mertes,Roland Bless,Martina Zitterbart,Jan C. Aurich,Hans D. Schotten*

Main category: cs.NI

TL;DR: 本文提出了一种动态频谱管理（DSM）方案，用于6G工业网络中的网络嵌套（NiN）概念，通过集中式频谱管理器实现高效频谱分配，为工业物联网提供灵活、可扩展的连接。


<details>
  <summary>Details</summary>
Motivation: 为解决未来6G工业网络中异构子网络（SNs）的动态频谱需求和实时服务质量（QoS）保障问题，提出DSM框架。

Method: 采用集中式频谱管理器（SM）和自组织KIRA路由协议，支持静态和移动子网络的动态频谱分配与无缝重新配置。

Result: 系统展示了可扩展的零接触连接能力，支持模块化工业物联网场景和关键任务控制。

Conclusion: DSM和NiNs框架在可重构制造环境中展现出灵活、密集和异构无线部署的潜力。

Abstract: This paper presents the application of Dynamic Spectrum Management (DSM) for
future 6G industrial networks, establishing an efficient controller for the
Networks-in-Network (NiN) concept. The proposed architecture integrates nomadic
as well as static sub-networks (SNs with diverse Quality of Service (QoS)
requirements within the coverage area of an overlayer network, managed by a
centralized spectrum manager (SM). Control plane connectivity between the SNs
and the DSM is ensured by the self-organizing KIRA routing protocol. The
demonstrated system enables scalable, zero-touch connectivity and supports
nomadic SNs through seamless discovery and reconfiguration. SNs are implemented
for modular Industrial Internet of Things (IIoT) scenarios, as well as for
mission-critical control loops and for logistics or nomadic behavior. The DSM
framework dynamically adapts spectrum allocation to meet real-time demands
while ensuring reliable operation. The demonstration highlights the potential
of DSM and NiNs to support flexible, dense, and heterogeneous wireless
deployments in reconfigurable manufacturing environments.

</details>


### [23] [LeoTCP: Low-Latency and High-Throughput Data Transport for LEO Satellite Networks](https://arxiv.org/abs/2508.19067)
*Aiden Valentine,George Parisis*

Main category: cs.NI

TL;DR: 论文介绍了LeoTCP，一种专为低地球轨道（LEO）卫星网络设计的新型数据传输协议，解决了动态网络中的延迟变化、热点和频繁切换等挑战。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络由于其动态特性（如卫星移动、链路质量波动和频繁切换）带来了独特的挑战，现有协议无法有效应对。

Method: LeoTCP利用网络内遥测技术（INT）逐跳收集拥塞信息，以最小化缓冲占用和延迟，最大化吞吐量和网络利用率，并快速应对网络热点。

Result: 通过与现有协议的比较，LeoTCP在仿真和微基准测试中显著提高了吞吐量，同时降低了延迟。

Conclusion: LeoTCP是解决LEO卫星网络动态特性引起的挑战的有效方案，显著优于现有方法。

Abstract: Low-Earth Orbit (LEO) satellite networks consist of thousands of satellites
orbiting the Earth, enabling low-latency and high-throughput communications
across the globe. Such networks present unprecedented challenges due to their
dynamic nature, which state-of-the-art data transport protocols do not address.
These challenges include: (1) non-congestive latency variation and loss, caused
by continuous satellite movement and fluctuating link quality due to weather
effects; (2) transient hotspots leading to buffer build-up, latency inflation,
and potential packet loss; and (3) frequent handovers, which may result in
temporary connectivity loss and re-routing through paths with unknown
congestion and delay characteristics. In this paper, we introduce LeoTCP, a
novel data transport protocol designed specifically to address these
challenges. LeoTCP leverages in-network telemetry (INT) to gather congestion
information on a per-hop basis. Using this information, LeoTCP (1) minimises
both buffer occupancy and latency for end users, (2) maximises application
throughput and network utilisation, and (3) swiftly reacts to network hotspots.
We compare LeoTCP to state-of-the-art data transport protocols using a LEO
satellite simulation model and targeted micro-benchmarks, both based on
OMNeT++/INET. The simulation model captures RTT dynamics in a simulated LEO
satellite constellation, while the micro-benchmarks isolate key LEO-specific
characteristics, including non-congestive latency variation and loss, path
changes, and congestion hotspots. Our results demonstrate that LeoTCP
significantly increases goodput compared to existing state-of-the-art
approaches, while simultaneously minimising latency.

</details>


### [24] [Sharing is Caring: Analysis of Hybrid Network Sharing Strategies for Energy Efficient Multi-Operator Cellular Systems](https://arxiv.org/abs/2508.19130)
*Laura Finarelli,Maoquan Ni,Michela Meo,Falko Dressler,Gianluca Rizzo*

Main category: cs.NI

TL;DR: 提出了一个用于评估蜂窝网络中能效且QoS感知的网络共享策略的新分析框架，结合随机几何学和多运营商协作，展示了高达35%的节能潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在系统评估不同网络共享策略的性能，特别是在多运营商协作的背景下，以提升能效和服务质量。

Method: 采用随机几何学框架，结合用户密度、速率需求和能耗模型，分析单运营商和多运营商混合共享策略。

Result: 应用于法国移动运营商数据集时，混合共享策略可实现高达35%的节能，同时保持服务质量，且效益受部署区域特性影响。

Conclusion: 协作共享策略能显著提升下一代蜂窝网络的运营效率和可持续性。

Abstract: This paper introduces a novel analytical framework for evaluating
energy-efficient, QoS-aware network-sharing strategies in cellular networks.
Leveraging stochastic geometry, our framework enables the systematic assessment
of network performance across a range of sharing paradigms, including both
conventional single-operator scenarios and advanced hybrid strategies that
enable full integration and cooperation among multiple mobile network
operators. Our framework incorporates diverse user densities, rate
requirements, and energy consumption models to ensure comprehensive analysis.
Applying our results to real-world datasets from French mobile network
operators, we demonstrate that hybrid network sharing can yield substantial
energy savings, up to $35\%$, while maintaining quality of service.
Furthermore, our results allow us to characterizing how the benefits of network
sharing vary as a function of the geographical and functional characteristics
of the deployment area. These findings highlight the potential of collaborative
sharing strategies to enhance operational efficiency and sustainability in
next-generation cellular networks.

</details>


### [25] [A Theory of Goal-Oriented Medium Access: Protocol Design and Distributed Bandit Learning](https://arxiv.org/abs/2508.19141)
*Federico Chiariotti,Andrea Zanella*

Main category: cs.NI

TL;DR: 该论文研究了多节点分布式场景下的目标导向多址接入问题，提出了一种理论框架和分析方法，证明了问题的非凸性和存在多个纳什均衡解的可能性，并通过分布式学习算法提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索多节点分布式场景下的目标导向通信问题，解决现有研究中集中式调度方法的局限，提升通信效率和能量利用率。

Method: 提出了一个理论框架，用于分析和优化分布式目标导向多址接入问题；设计了一种分布式学习算法，能够在有限反馈和无先验知识的情况下运行。

Result: 证明了问题的非凸性和多纳什均衡解的存在；提出的优化方法比集中式方法性能提升高达100%，同时降低了能耗。

Conclusion: 该研究为目标导向多址接入问题的分布式解决方案提供了理论基础，并通过算法实践验证了其优越性。

Abstract: The Goal-oriented Communication (GoC) paradigm breaks the separation between
communication and the content of the data, tailoring communication decisions to
the specific needs of the receiver and targeting application performance. While
recent studies show impressive encoding performance in point-to-point
scenarios, the multi-node distributed scenario is still almost unexplored.
Moreover, the few studies to investigate this consider a centralized
collision-free approach, where a central scheduler decides the transmission
order of the nodes. In this work, we address the Goal-oriented Multiple Access
(GoMA) problem, in which multiple intelligent agents must coordinate to share a
wireless channel and avoid mutual interference. We propose a theoretical
framework for the analysis and optimization of distributed GoMA, serving as a
first step towards its complete characterization. We prove that the problem is
non-convex and may admit multiple Nash Equilibrium (NE) solutions. We provide a
characterization of each node's best response to others' strategies and propose
an optimization approach that provably reaches one such NE, outperforming
centralized approaches by up to 100% while also reducing energy consumption. We
also design a distributed learning algorithm that operates with limited
feedback and no prior knowledge.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [26] [Decidability of Extensions of Presburger Arithmetic by Hardy Field Functions](https://arxiv.org/abs/2508.19206)
*Hera Brown,Jakub Konieczny*

Main category: cs.LO

TL;DR: 研究了Presburger算术通过亚多项式Hardy场函数的扩展，发现大多数情况下这些扩展是不可判定的。


<details>
  <summary>Details</summary>
Motivation: 探索Hardy场函数在Presburger算术中的扩展是否可判定，特别是当函数的增长速率介于多项式和线性之间时。

Method: 通过分析$
\mathrm{Th}(\mathbb{Z}; <, +, \lfloor f \rceil)$理论的可判定性，其中$f$是Hardy场函数，$
\lfloor \cdot \rceil$是最近整数算子。

Result: 当$f$的增长速率比线性快（多项式级别）或介于多项式与线性之间（亚线性）时，理论不可判定。

Conclusion: Hardy场函数在特定增长速率下的扩展会导致Presburger算术的不可判定性。

Abstract: We study the extension of Presburger arithmetic by the class of
sub-polynomial Hardy field functions, and show the majority of these extensions
to be undecidable. More precisely, we show that the theory
$\mathrm{Th}(\mathbb{Z}; <, +, \lfloor f \rceil)$, where $f$ is a Hardy field
function and $\lfloor \cdot \rceil$ the nearest integer operator, is
undecidable when $f$ grows polynomially faster than $x$. Further, we show that
when $f$ grows sub-linearly quickly, but still as fast as some polynomial, the
theory $\mathrm{Th}(\mathbb{Z}; <, +, \lfloor f \rceil)$ is undecidable.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [27] [Technology-assisted Personalized Yoga for Better Health -- Challenges and Outlook](https://arxiv.org/abs/2508.18283)
*Vivek Kumar,Himanshu Sahu,Hari Prabhat Gupta,Biplav Srivastava*

Main category: cs.HC

TL;DR: 本文探讨了瑜伽个性化问题及其多学科计算解决方案的初步方法。


<details>
  <summary>Details</summary>
Motivation: 瑜伽作为一种全球流行的身心锻炼方式，其复杂性使得个性化推荐成为挑战，需满足用户需求、兴趣和适应性。

Method: 提出了一个初步的多学科计算视角方法，结合案例研究说明瑜伽姿势传感和纠正推荐。

Result: 首次全面研究了瑜伽个性化决策支持问题，并通过案例展示解决方案。

Conclusion: 瑜伽个性化是一个复杂但可行的问题，需结合多学科技术解决，未来有望进一步优化。

Abstract: Yoga is a discipline of physical postures, breathing techniques, and
meditative practices rooted in ancient Indian traditions, now embraced
worldwide for promoting overall well-being and inner balance. The practices are
a large set of items, our term for executable actions like physical poses or
breath exercises, to offer for a person's well-being. However, to get benefits
of Yoga tailored to a person's unique needs, a person needs to (a) discover
their subset from the large and seemingly complex set with inter-dependencies,
(b) continue to follow them with interest adjusted to their changing abilities
and near-term objectives, and (c) as appropriate, adapt to alternative items
based on changing environment and the person's health conditions. In this
vision paper, we describe the challenges for the Yoga personalization problem.
Next, we sketch a preliminary approach and use the experience to provide an
outlook on solving the challenging problem using existing and novel techniques
from a multidisciplinary computing perspective. To the best of our knowledge,
this is the first paper that comprehensively examines decision support issues
around Yoga personalization, from pose sensing to recommendation of corrections
for a complete regimen, and illustrates with a case study of Surya Namaskar --
a set of 12 choreographed poses.

</details>


### [28] [Does Calibration Affect Human Actions?](https://arxiv.org/abs/2508.18317)
*Meir Nizri,Amos Azaria,Chirag Gupta,Noam Hazon*

Main category: cs.HC

TL;DR: 论文研究了校准分类模型对非专家决策的影响，发现校准不足以保证信任，而结合前景理论修正能提高决策与预测的相关性。


<details>
  <summary>Details</summary>
Motivation: 探讨校准如何影响非专家对机器学习分类模型预测的信任和决策相关性，以增强模型的可靠性。

Method: 通过HCI实验评估校准对信任和决策的影响，并提出基于前景理论的修正方法。

Result: 校准本身不足，前景理论修正显著提高了决策与预测的相关性，但对直接信任问题的回答无影响。

Conclusion: 校准需结合行为经济学理论（如前景理论）才能真正提升模型的决策相关性，而直接信任问题可能需要其他干预。

Abstract: Calibration has been proposed as a way to enhance the reliability and
adoption of machine learning classifiers. We study a particular aspect of this
proposal: how does calibrating a classification model affect the decisions made
by non-expert humans consuming the model's predictions? We perform a
Human-Computer-Interaction (HCI) experiment to ascertain the effect of
calibration on (i) trust in the model, and (ii) the correlation between
decisions and predictions. We also propose further corrections to the reported
calibrated scores based on Kahneman and Tversky's prospect theory from
behavioral economics, and study the effect of these corrections on trust and
decision-making. We find that calibration is not sufficient on its own; the
prospect theory correction is crucial for increasing the correlation between
human decisions and the model's predictions. While this increased correlation
suggests higher trust in the model, responses to ``Do you trust the model
more?" are unaffected by the method used.

</details>


### [29] [Impact of Target and Tool Visualization on Depth Perception and Usability in Optical See-Through AR](https://arxiv.org/abs/2508.18481)
*Yue Yang,Xue Xie,Xinkai Wang,Hui Zhang,Chiming Yu,Xiaoxian Xiong,Lifeng Zhu,Yuanyi Zheng,Jue Cen,Bruce Daniel,Fred Baik*

Main category: cs.HC

TL;DR: 研究了不同透明度和工具追踪方式对光透视增强现实（OST-AR）系统中深度感知和系统可用性的影响，发现不透明的目标和实时工具追踪能显著提高精度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决光透视增强现实系统中深度感知和实际工具遮挡的问题，以提升在手术等近距离任务中的指导和精度。

Method: 通过两个实验，分别比较不同透明度的目标渲染和三种工具追踪方式（虚拟工具、实际工具、无追踪）对深度匹配和模拟手术任务的影响。

Result: 不透明的目标显著降低深度估计误差，实时工具追踪的遮挡效果最佳，而透明目标显示虽允许工具可见，却轻微损害深度感知且未改善可用性。

Conclusion: 在OST-AR系统中，正确的遮挡线索、不透明的虚拟内容及实时工具遮挡是深度感知和精度的关键，设计应优先考虑工具追踪和遮挡处理。

Abstract: Optical see-through augmented reality (OST-AR) systems like Microsoft
HoloLens 2 hold promise for arm's distance guidance (e.g., surgery), but depth
perception of the hologram and occlusion of real instruments remain
challenging. We present an evaluation of how visualizing the target object with
different transparencies and visualizing a tracked tool (virtual proxy vs. real
tool vs. no tool tracking) affects depth perception and system usability. Ten
participants performed two experiments on HoloLens 2. In Experiment 1, we
compared high-transparency vs. low-transparency target rendering in a depth
matching task at arm's length. In Experiment 2, participants performed a
simulated surgical pinpoint task on a frontal bone target under six
visualization conditions ($2 \times 3$: two target transparencies and three
tool visualization modes: virtual tool hologram, real tool, or no tool
tracking). We collected data on depth matching error, target localization
error, system usability, task workload, and qualitative feedback. Results show
that a more opaque target yields significantly lower depth estimation error
than a highly transparent target at arm's distance. Moreover, showing the real
tool (occluding the virtual target) led to the highest accuracy and usability
with the lowest workload, while not tracking the tool yielded the worst
performance and user ratings. However, making the target highly transparent,
while allowing the real tool to remain visible, slightly impaired depth cues
and did not improve usability. Our findings underscore that correct occlusion
cues, rendering virtual content opaque and occluding it with real tools in real
time, are critical for depth perception and precision in OST-AR. Designers of
arm-distance AR systems should prioritize robust tool tracking and occlusion
handling; if unavailable, cautiously use transparency to balance depth
perception and tool visibility.

</details>


### [30] [Skeptik: A Hybrid Framework for Combating Potential Misinformation in Journalism](https://arxiv.org/abs/2508.18499)
*Arlen Fan,Fan Lei,Steven R. Corman,Ross Maciejewski*

Main category: cs.HC

TL;DR: Skeptik是一个结合大型语言模型和启发式方法的混合框架，用于检测新闻文章中的逻辑谬误，帮助读者批判性评估内容。


<details>
  <summary>Details</summary>
Motivation: 传统事实核查方法难以捕捉新闻中的逻辑问题，Skeptik旨在填补这一空白，提高读者的批判性思维和对媒体的信任。

Method: Skeptik作为浏览器扩展，自动标注逻辑谬误句子，提供解释和多层次干预。系统可扩展，适应新的谬误类型。

Result: 通过案例研究、定量分析和专家评估，证明Skeptik能有效提升读者对新闻的批判性分析能力。

Conclusion: Skeptik通过关注逻辑完整性而非仅事实准确性，为打击新闻中的误导信息提供了综合解决方案。

Abstract: The proliferation of misinformation in journalism, often stemming from flawed
reasoning and logical fallacies, poses significant challenges to public
understanding and trust in news media. Traditional fact-checking methods, while
valuable, are insufficient for detecting the subtle logical inconsistencies
that can mislead readers within seemingly factual content. To address this gap,
we introduce Skeptik, a hybrid framework that integrates Large Language Models
(LLMs) with heuristic approaches to analyze and annotate potential logical
fallacies and reasoning errors in online news articles. Operating as a web
browser extension, Skeptik automatically highlights sentences that may contain
logical fallacies, provides detailed explanations, and offers multi-layered
interventions to help readers critically assess the information presented. The
system is designed to be extensible, accommodating a wide range of fallacy
types and adapting to evolving misinformation tactics. Through comprehensive
case studies, quantitative analyses, usability experiments, and expert
evaluations, we demonstrate the effectiveness of Skeptik in enhancing readers'
critical examination of news content and promoting media literacy. Our
contributions include the development of an expandable classification system
for logical fallacies, the innovative integration of LLMs for real-time
analysis and annotation, and the creation of an interactive user interface that
fosters user engagement and close reading. By emphasizing the logical integrity
of textual content rather than relying solely on factual accuracy, Skeptik
offers a comprehensive solution to combat potential misinformation in
journalism. Ultimately, our framework aims to improve critical reading and
protect the public from deceptive information online and enhance the overall
credibility of news media.

</details>


### [31] [Beyond prior knowledge: The predictive role of knowledge-building in Tutor Learning](https://arxiv.org/abs/2508.18545)
*Tasmia Shahriar,Mia Ameen,Aditi Mallavarapu,Shiyan Jiang,Noboru Matsuda*

Main category: cs.HC

TL;DR: 研究表明，学生在教学中通过知识构建活动（如解释和纠错）能更有效地提升概念性和程序性知识，知识构建在两者之间起关键中介作用。


<details>
  <summary>Details</summary>
Motivation: 学生在教学环境中往往倾向于知识讲述而非知识构建，限制了学习效果，研究旨在探讨知识构建如何促进概念性与程序性知识的双向提升。

Method: 通过可教代理（能提出持续跟进问题的虚拟学生）引导学生进行知识构建活动，分析其对概念性和程序性知识的影响。

Result: 知识构建能稳定中介概念性与程序性知识的双向关系，且参与知识构建的学生在后测中表现更好。

Conclusion: 知识构建是连接低先验知识与高学习收益的关键机制，教学设计中应注重激发学生的知识构建行为。

Abstract: When adopting the role of a teacher in learning-by-teaching environments,
students often struggle to engage in knowledge-building activities, such as
providing explanations and addressing misconceptions. Instead, they frequently
default to knowledge-telling behaviors, where they simply dictate what they
already know or what to do without deeper reflection, thereby limiting
learning. Teachable agents, particularly those capable of posing persistent
follow-up questions, have been shown to encourage students (tutors) to shift
from knowledge-telling to knowledge-building and enhance tutor learning. Tutor
learning encompasses two interrelated types of knowledge: conceptual and
procedural knowledge. Research has established a bidirectional relationship
between these knowledge types, where improvements in one reinforce the other.
This study investigates the role of knowledge-building in mediating the
bidirectional relationship between procedural and conceptual learning. Our
findings revealed a stable bidirectional relationship between procedural and
conceptual knowledge, with higher post-test scores observed among students who
engaged in knowledge-building, regardless of their procedural and conceptual
pre-test performance. This suggests that knowledge-building serves as a crucial
mechanism bridging the gap between students with low prior knowledge and higher
conceptual and procedural learning gain.

</details>


### [32] [Gamification of Immersive Cervical Rehabilitation Exercises in VR: An Exploratory Study on Chin Tuck and Range of Motion Exercises](https://arxiv.org/abs/2508.18580)
*Haitham Abdelsalam,Chanelle Montpetit,Arash Harirpoush,Maryse Fortin,Yiming Xiao*

Main category: cs.HC

TL;DR: VR结合游戏化策略用于慢性颈部疼痛康复的研究，探索了新颖的康复游戏，初步研究表明其具有良好的可用性、参与感和健康价值。


<details>
  <summary>Details</summary>
Motivation: 传统的一对一康复治疗成本高、依从性差且可及性不足，VR技术结合游戏化为解决这些问题提供了潜力。

Method: 研究通过设计两种VR游戏（生存与等级进步策略用于肌肉强化练习，环境奖励用于颈部活动范围练习），并进行了初步用户研究。

Result: 初步用户研究表明，提出的VR颈部康复游戏在可用性、参与感和健康价值方面表现优异。

Conclusion: VR游戏化策略为慢性颈部疼痛康复提供了新的有效途径，但其仍需进一步研究和完善。

Abstract: Chronic neck pain is a prevalent condition that affects millions of
individuals worldwide, causing significant individual suffering and
socioeconomic burdens. Although exercise rehabilitation is a staple in
relieving pain and improving muscle function for the condition, traditional
one-on-one rehabilitation sessions are costly and suffer from poor adherence
and accessibility for the patients. Thanks to the increasing accessibility and
recent advancements in sensing and display technology, virtual reality (VR)
offers the potential to tackle the challenges in traditional exercise
rehabilitation, particularly through gamification. However, still in its
infancy, VR-based neck exercise rehabilitation lacks exploration in effective
gamification strategies and existing prototypes. To address the knowledge gap,
we conduct an exploratory study on the gamification strategies for VR-based
cervical rehabilitation exercises by using chin tuck and neck range of motion
exercises as examples. Specifically, with different game themes, we investigate
a survival and level progression strategy for muscle strengthening (chin tuck)
exercise for the first time, and the suitability of ambient reward for a neck
range of motion exercise. Through a preliminary user study, we assess the
proposed novel VR neck rehabilitation games and they demonstrate excellent
usability, engagement, and perceived health value.

</details>


### [33] [Portable Silent Room: Exploring VR Design for Anxiety and Emotion Regulation for Neurodivergent Women and Non-Binary Individuals](https://arxiv.org/abs/2508.18591)
*Kinga Skiers,Yun Suen Pai,Marina Nakagawa,Kouta Minamizawa,Giulia Barbareschi*

Main category: cs.HC

TL;DR: 研究探讨虚拟现实（VR）作为神经多样性个体的便携式安全空间，帮助情绪调节，尤其针对自闭症和ADHD女性及非二元性别者。


<details>
  <summary>Details</summary>
Motivation: 神经多样性个体常因社会压力和缺乏适当支持而经历焦虑和情绪失调，女性及非二元性别者面临更多障碍。

Method: 采用混合方法，包括在线调查（N=223）和设计研讨会（N=32），开发并测试VR原型（N=12），最终评估（N=25）。

Result: 开发的VR原型适应个体感官需求，可作为个性化便携式安静空间，提供随时随地感官调节。

Conclusion: VR环境为神经多样性个体提供包容性和适应性支持，提升情绪调节能力。

Abstract: Neurodivergent individuals, particularly those with Autism and Attention
Deficit Hyperactivity Disorder (ADHD), frequently experience anxiety, panic
attacks, meltdowns, and emotional dysregulation due to societal pressures and
inadequate accommodations. These challenges are especially pronounced for
neurodivergent women and non-binary individuals navigating intersecting
barriers of neurological differences and gender expectations. This research
investigates virtual reality (VR) as a portable safe space for emotional
regulation, addressing challenges of sensory overload and motion sickness while
enhancing relaxation capabilities. Our mixed-methods approach included an
online survey (N=223) and an ideation workshop (N=32), which provided key
design elements for creating effective calming VR environments. Based on these
findings, we developed and iteratively tested VR prototypes with neurodivergent
women and non-binary participants (N=12), leading to a final version offering
enhanced adaptability to individual sensory needs. This final prototype
underwent a comprehensive evaluation with 25 neurodivergent participants to
assess its effectiveness as a regulatory tool. This research contributes to the
development of inclusive, adaptive VR environments that function as
personalized "portable silent rooms" offering neurodivergent individuals
on-demand access to sensory regulation regardless of physical location.

</details>


### [34] [Enhancing XAI Interpretation through a Reverse Mapping from Insights to Visualizations](https://arxiv.org/abs/2508.18640)
*Aniket Nuthalapati,Nicholas Hinds,Brian Y. Lim,Qianwen Wang*

Main category: cs.HC

TL;DR: 论文提出了一种名为Reverse Mapping的新方法，通过将用户对AI解释的反馈整合到可视化解释中，帮助用户更好地理解和验证AI决策。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在高风险领域的广泛应用，用户需要更有效地理解和验证AI解释，但现有的解释方法往往难以满足用户需求。

Method: 提出Reverse Mapping方法，利用大语言模型从用户反馈中提取结构化见解，并通过交互式注释和多视图可视化将其映射回视觉解释中。

Result: 通过原型系统和用户定性反馈验证了该方法在提升用户与AI解释交互中的反思性和有效性的潜力。

Conclusion: Reverse Mapping为增强用户对AI解释的理解和验证提供了一种新途径，具有重要的实际应用价值。

Abstract: As AI systems become increasingly integrated into high-stakes domains,
enabling users to accurately interpret model behavior is critical. While AI
explanations can be provided, users often struggle to reason effectively with
these explanations, limiting their ability to validate or learn from AI
decisions. To address this gap, we introduce Reverse Mapping, a novel approach
that enhances visual explanations by incorporating user-derived insights back
into the explanation workflow. Our system extracts structured insights from
free-form user interpretations using a large language model and maps them back
onto visual explanations through interactive annotations and coordinated
multi-view visualizations. Inspired by the verification loop in the
visualization knowledge generation model, this design aims to foster more
deliberate, reflective interaction with AI explanations. We demonstrate our
approach in a prototype system with two use cases and qualitative user
feedback.

</details>


### [35] [RÉCITKIT: A Spatial Toolkit for Designing and Evaluating Human-Centered Immersive Data Narratives](https://arxiv.org/abs/2508.18670)
*Vidya Setlur,Samuel Ridet*

Main category: cs.HC

TL;DR: 论文介绍了RÉCITKIT工具包，用于在头戴式显示器（HMD）环境中构建空间数据叙事，并通过开发拿破仑1812年战役的沉浸式地图演示了其实用性。初步用户评估表明，空间交互和引导叙事有助于增强数据洞察力。


<details>
  <summary>Details</summary>
Motivation: 空间计算为沉浸式数据叙事提供了新机会，但目前缺乏相关工具和指导。

Method: 开发了RÉCITKIT工具包，支持创建交互式仪表盘、空间资产标记、动态过滤和数据探索。通过开发拿破仑战役的沉浸式应用进行了演示和初步用户评估。

Result: 用户反馈显示，空间交互和物理操作（如凝视、捏合）对理解时空数据特别有效。同时提出改进建议，如提升交互可见性和自定义叙事逻辑。

Conclusion: 该工具包为沉浸式数据叙事提供了新的工具支持，未来可扩展至教育、决策支持等领域。

Abstract: Spatial computing presents new opportunities for immersive data storytelling,
yet there is limited guidance on how to build such experiences or adapt
traditional narrative visualizations to this medium. We introduce a toolkit,
R\'ECITKIT for supporting spatial data narratives in head-mounted display (HMD)
environments. The toolkit allows developers to create interactive dashboards,
tag data attributes as spatial assets to 3D models and immersive scenes,
generate text and audio narratives, enabling dynamic filtering, and
hierarchical drill-down data discoverability. To demonstrate the utility of the
toolkit, we developed Charles Minard's historical flow map of Napoleon's 1812
campaign in Russia as an immersive experience on Apple Vision Pro. We conducted
a preliminary evaluation with 21 participants that comprised two groups:
developers, who evaluated the toolkit by authoring spatial stories and
consumers, who provided feedback on the Minard app's narrative clarity,
interaction design, and engagement. Feedback highlighted how spatial
interactions and guided narration enhanced insight formation, with participants
emphasizing the benefits of physical manipulation (e.g., gaze, pinch,
navigation) for understanding temporal and geographic data. Participants also
identified opportunities for future enhancement, including improved interaction
affordance visibility, customizable storytelling logic, and integration of
contextual assets to support user orientation. These findings contribute to the
broader discourse on toolkit-driven approaches to immersive data storytelling
across domains such as education, decision support, and exploratory analytics.

</details>


### [36] [Long-Term Variability in Physiological-Arousal Relationships for Robust Emotion Estimation](https://arxiv.org/abs/2508.18782)
*Hiroto Sakimura,Takayuki Nagaya,Tomoki Nishi,Tetsuo Kurahashi,Katsunori Kohda,Nobuhiko Muramoto*

Main category: cs.HC

TL;DR: 研究发现，生理信号与情绪状态的关系存在长期变异性，需定期更新情绪估计模型以保持性能。


<details>
  <summary>Details</summary>
Motivation: 验证生理特征与主观情绪之间的稳定关系是否能在数月内保持一致，以提升情绪估计系统的可靠性。

Method: 利用定制测量系统收集24名参与者自然工作环境中的生理信号（如心率、EDA等）及自我报告情绪，使用EBMs分析其变化。

Result: 模型在第二期数据上的准确性下降5%，显示生理-唤醒关系的长期变异性；心率稳定，但EDA存在个体波动。

Conclusion: 生理-唤醒关系具时间变异性，需定期（如每五个月）更新情绪估计模型以应对变化。

Abstract: Estimating emotional states from physiological signals is a central topic in
affective computing and psychophysiology. While many emotion estimation systems
implicitly assume a stable relationship between physiological features and
subjective affect, this assumption has rarely been tested over long timeframes.
This study investigates whether such relationships remain consistent across
several months within individuals. We developed a custom measurement system and
constructed a longitudinal dataset by collecting physiological signals --
including blood volume pulse, electrodermal activity (EDA), skin temperature,
and acceleration--along with self-reported emotional states from 24
participants over two three-month periods. Data were collected in naturalistic
working environments, allowing analysis of the relationship between
physiological features and subjective arousal in everyday contexts. We examined
how physiological-arousal relationships evolve over time by using Explainable
Boosting Machines (EBMs) to ensure model interpretability. A model trained on
1st-period data showed a 5\% decrease in accuracy when tested on 2nd-period
data, indicating long-term variability in physiological-arousal associations.
EBM-based comparisons further revealed that while heart rate remained a
relatively stable predictor, minimum EDA exhibited substantial individual-level
fluctuations between periods. While the number of participants is limited,
these findings highlight the need to account for temporal variability in
physiological-arousal relationships and suggest that emotion estimation models
should be periodically updated -- e.g., every five months -- based on observed
shift trends to maintain robust performance over time.

</details>


### [37] [Insights into User Interface Innovations from a Design Thinking Workshop at deRSE25](https://arxiv.org/abs/2508.18784)
*Maximilian Frank,Simon Lund*

Main category: cs.HC

TL;DR: 论文总结了通过设计思维工作坊开发的创新LLM用户界面概念，强调用户中心设计。


<details>
  <summary>Details</summary>
Motivation: 由于现有LLM界面过于线性化，缺乏灵活性，作者希望通过用户反馈开发更灵活的交互方式。

Method: 通过设计思维工作坊收集参与者意见，提出新界面概念，并采用以用户为中心的设计方法迭代开发。

Result: 提出了支持灵活上下文管理、动态对话分支和增强用户控制的界面概念。

Conclusion: 未来LLM界面开发应更注重用户中心设计原则。

Abstract: Large Language Models have become widely adopted tools due to their versatile
capabilities, yet their user interfaces remain limited, often following rigid,
linear interaction paradigms. In this paper, we present insights from a design
thinking workshop held at the deRSE25 conference aiming at collaboratively
developing innovative user interface concepts for LLMs. During the workshop,
participants identified common use cases, evaluated the strengths and
shortcomings of current LLM interfaces, and created visualizations of new
interaction concepts emphasizing flexible context management, dynamic
conversation branching, and enhanced mechanisms for user control. We describe
how these participant-generated ideas advanced our own whiteboard-based UI
approach. The ongoing development of this interface is guided by the
human-centered design process - an iterative, user-focused methodology that
emphasizes continuous refinement through user feedback. Broader implications
for future LLM interface development are discussed, advocating for increased
attention to UI innovation grounded in user-centered design principles.

</details>


### [38] [PRIMMDebug: A Debugging Teaching Aid For Secondary Students](https://arxiv.org/abs/2508.18875)
*Laurie Gale,Sue Sentance*

Main category: cs.HC

TL;DR: 论文介绍了PRIMMDebug，一种帮助中学生学习文本编程的调试辅助工具，旨在通过系统化和反思性方法改善调试效果，但学生对此方法的接受度较低。


<details>
  <summary>Details</summary>
Motivation: 中学生在学习文本编程时常因调试困难而产生情感困扰，现有工具缺乏系统性，促使开发PRIMMDebug以提升调试效率。

Method: PRIMMDebug是一个基于PRIMM框架的在线工具，通过限制代码编辑和运行，鼓励学生系统性、反思性地调试。

Result: 实验表明，学生对工具倡导的系统性和反思性方法接受度不高，需要进一步优化方法以提高实用性。

Conclusion: PRIMMDebug虽提出有效框架，但需调整以适应学生需求，以扩大其应用效果。

Abstract: Debugging is often a challenging and infuriating experience for secondary
school students learning their first text-based programming language. Many
students resort to ineffective debugging strategies, making success with
solving errors unlikely and emotional distress common. Developing tools that
encourage students to adopt a more systematic and reflective approach to
debugging is therefore an important, but lacking, area of research. This paper
presents PRIMMDebug, a debugging teaching aid for secondary school students
learning text-based programming. The aid consists of an online tool that takes
students through the steps of a systematic debugging process based on PRIMM, a
framework for teaching programming. The tool promotes a reflective approach to
debugging by heavily encouraging students to articulate their thoughts
throughout the PRIMMDebug process while simultaneously limiting their ability
to run and edit code. To evaluate the tool, a set of students from four
secondary schools were taught with PRIMMDebug over several lessons. Survey
results and log data analysis show that students were generally reluctant to
engage with the systematicity and reflection that the tool encourages. Given
that related work on systematic debugging has reported similar challenges, we
end by considering how these approaches could be refined to help more students
benefit from them.

</details>


### [39] [DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM with Audio Modality](https://arxiv.org/abs/2508.18918)
*Youngwon Choi,Donghyuk Jung,Hwayeon Kim*

Main category: cs.HC

TL;DR: DESAMO是一个基于音频LLM的智能家居系统，专为老年人设计，支持自然且私密的交互。


<details>
  <summary>Details</summary>
Motivation: 解决传统语音助手在处理老年人不清晰语音及非语音音频时的不足。

Method: 利用音频LLM直接处理原始音频输入，无需依赖ASR或ASR-LLM级联。

Result: 能更鲁棒地理解用户意图及关键事件（如摔倒或呼救）。

Conclusion: DESAMO为老年人提供了更有效的智能家居交互解决方案。

Abstract: We present DESAMO, an on-device smart home system for elder-friendly use
powered by Audio LLM, that supports natural and private interactions. While
conventional voice assistants rely on ASR-based pipelines or ASR-LLM cascades,
often struggling with the unclear speech common among elderly users and unable
to handle non-speech audio, DESAMO leverages an Audio LLM to process raw audio
input directly, enabling a robust understanding of user intent and critical
events, such as falls or calls for help.

</details>


### [40] [Impact Assessment Card: Communicating Risks and Benefits of AI Uses](https://arxiv.org/abs/2508.18919)
*Edyta Bogucka,Marios Constantinides,Sanja Šćepanović,Daniele Quercia*

Main category: cs.HC

TL;DR: 开发了一种名为‘影响评估卡’的工具，用于更清晰地向公众传达AI的风险和收益，比传统技术报告更高效和易理解。


<details>
  <summary>Details</summary>
Motivation: 当前的技术报告往往排斥非技术背景的公众，需要一种更易理解的沟通方式。

Method: 通过与12名参与者的焦点小组会议设计了初版卡片，随后在235名多样化参与者的在线研究中测试了优化版。

Result: 使用卡片比传统报告更快完成任务，且生成的邮件质量更优。

Conclusion: 设计选择可以提升信息的可理解性，支持AI治理。

Abstract: Communicating the risks and benefits of AI is important for regulation and
public understanding. Yet current methods such as technical reports often
exclude people without technical expertise. Drawing on HCI research, we
developed an Impact Assessment Card to present this information more clearly.
We held three focus groups with a total of 12 participants who helped identify
design requirements and create early versions of the card. We then tested a
refined version in an online study with 235 participants, including AI
developers, compliance experts, and members of the public selected to reflect
the U.S. population by age, sex, and race. Participants used either the card or
a full impact assessment report to write an email supporting or opposing a
proposed AI system. The card led to faster task completion and higher-quality
emails across all groups. We discuss how design choices can improve
accessibility and support AI governance. Examples of cards are available at:
https://social-dynamics.net/ai-risks/impact-card/.

</details>


### [41] [Reading minds on the road: decoding perceived risk in automated vehicles through 140K+ ratings](https://arxiv.org/abs/2508.19121)
*Xiaolin He,Zirui Li,Xinwei Wang,Riender Happee,Meng Wang*

Main category: cs.HC

TL;DR: 论文提出了一种实时测量和解码自动化车辆(AVs)中感知风险的新方法，通过重构离散安全评分获得了迄今为止最大的感知风险数据集，并利用深度学习预测实时感知风险。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆中的感知风险可能导致危险行为，但目前实时测量这种心理状态仍然具有挑战性。

Method: 通过高保真视频实验收集参与者的离散安全评分，重构为连续信号，并使用深度神经网络从车辆运动学预测感知风险。

Result: 成功构建了236小时的连续感知风险数据集，预测模型的平均相对误差低于3%。可解释性分析揭示了实时感知风险的关键因素。

Conclusion: 该方法为实时量化乘客心理状态提供了新范式，有助于优化自动驾驶和其他机器的设计，以实现自动化技术的潜在优势。

Abstract: Perceived risk in automated vehicles (AVs) can create the very danger that
automation is meant to prevent: a frightened rider may hesitate when seconds
matter, misjudge hazards, or disengage. However, measuring how perceived risk
evolves in real time during driving remains challenging, leaving a gap in
decoding such hidden psychological states. Here, we present a novel method to
time-continuously measure and decode perceived risk. We conducted a controlled
experiment where 2,164 participants viewed high-fidelity videos of common
highway driving scenes and provided 141,628 discrete safety ratings. Through
continuous-signal reconstruction of the discrete ratings, we obtained 236 hours
of time-continuous perceived risk data - the largest perceived risk dataset to
date. Leveraging this dataset, we trained deep neural networks that predict
moment-by-moment perceived risk from vehicle kinematics with a mean relative
error below $3\%$. Explainable AI analysis uncovers which factors determine
perceived risk in real time. Our findings demonstrate a new paradigm for
quantifying dynamic passenger experience and psychological constructs in real
time. These findings can guide the design of AVs and other machines that
operate in close proximity to people, adjusting behaviour before trust erodes,
and help realise automation's benefits in transport, healthcare, and service
robotics.

</details>


### [42] [Beyond Competitive Gaming: How Casual Players Evaluate and Respond to Teammate Performance](https://arxiv.org/abs/2508.19230)
*Kaushall Senthil Nathan,Jieun Lee,Derrick M. Wang,Geneva M. Smith,Eugene Kukshinov,Daniel Harley,Lennart E. Nacke*

Main category: cs.HC

TL;DR: 该研究探讨了在休闲合作游戏中玩家如何评估队友表现，发现与竞争性游戏不同，休闲玩家更倾向于相对比较而非绝对指标。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证竞争性游戏中的表现评估机制和行为反应是否适用于休闲合作游戏。

Method: 通过操控《Overcooked 2》中队友表现，结合观察、NASA TLX自评和访谈，进行受试者间实验（N=23）。

Result: 研究显示：(1)观察数据揭示了自评中未体现的挫败行为；(2)玩家通过相对比较而非绝对指标评估队友表现。

Conclusion: 结论表明竞争性游戏的表现评估框架不适用于休闲合作游戏，休闲游戏需要基于比较的操作化方法。

Abstract: Teammate performance evaluation fundamentally shapes intervention design in
video games. However, our current understanding stems primarily from
competitive E-Sports contexts where individual performance directly impacts
outcomes. This research addresses whether performance evaluation mechanisms and
behavioural responses identified in competitive games generalize to casual
cooperative games. We investigated how casual players evaluate teammate
competence and respond behaviourally in a controlled between-subjects
experiment (N=23). We manipulated confederate performance in Overcooked 2,
combining observations, NASA TLX self-reports, and interviews. We present two
key findings. (1) Observations revealed frustration behaviours completely
absent in self-report data. Thus, these instruments assess fundamentally
distinct constructs. (2) Participants consistently evaluated teammate
performance through relative comparison rather than absolute metrics. This
contradicts task-performance operationalizations dominant in competitive gaming
research. Hence, performance evaluation frameworks from competitive contexts
cannot be directly applied to casual cooperative games. We provide empirical
evidence that performance evaluation in casual games requires a comparative
operationalization.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [43] [Controllable Single-shot Animation Blending with Temporal Conditioning](https://arxiv.org/abs/2508.18525)
*Eleni Tselepi,Spyridon Thermos,Gerasimos Potamianos*

Main category: cs.GR

TL;DR: 该论文提出了一种单次运动生成模型，能够在单一生成过程中无缝混合两种或更多运动，解决了现有方法无法显式控制运动混合的问题。


<details>
  <summary>Details</summary>
Motivation: 动画社区需要一种能够在单一运动序列上训练生成模型的方法，并能可控地生成运动变体，无需额外数据或大量重新训练。同时，现有方法缺乏在单一生成过程中显式控制运动混合的能力。

Method: 论文提出了一种骨架感知的归一化机制，通过时间条件化的生成过程来引导运动之间的过渡，实现平滑、数据驱动的混合控制。

Result: 通过多种动画风格和不同骨架结构的广泛定性和定量评估，证明了该方法能够以统一且高效的方式生成合理、平滑且可控的运动混合。

Conclusion: 提出的框架是首个实现单次运动混合的方法，解决了现有技术的局限性，为动画创作提供了更灵活和高效的工具。

Abstract: Training a generative model on a single human skeletal motion sequence
without being bound to a specific kinematic tree has drawn significant
attention from the animation community. Unlike text-to-motion generation,
single-shot models allow animators to controllably generate variations of
existing motion patterns without requiring additional data or extensive
retraining. However, existing single-shot methods do not explicitly offer a
controllable framework for blending two or more motions within a single
generative pass. In this paper, we present the first single-shot motion
blending framework that enables seamless blending by temporally conditioning
the generation process. Our method introduces a skeleton-aware normalization
mechanism to guide the transition between motions, allowing smooth, data-driven
control over when and how motions blend. We perform extensive quantitative and
qualitative evaluations across various animation styles and different kinematic
skeletons, demonstrating that our approach produces plausible, smooth, and
controllable motion blends in a unified and efficient manner.

</details>


### [44] [Real-time 3D Visualization of Radiance Fields on Light Field Displays](https://arxiv.org/abs/2508.18540)
*Jonghyun Kim,Cheng Sun,Michael Stengel,Matthew Chan,Andrew Russell,Jaehyun Jung,Wil Braithwaite,Shalini De Mello,David Luebke*

Main category: cs.GR

TL;DR: 提出了一种统一高效的框架，用于在光场显示器上实时渲染辐射场，支持多种辐射场表示，并在多种场景中实现高帧率。


<details>
  <summary>Details</summary>
Motivation: 结合光场显示器和辐射场技术面临计算量大、实时性差的挑战，研究目标是实现高效实时的渲染。

Method: 采用单遍平面扫描策略和共享非方向性组件的缓存技术，支持NeRFs、3D高斯溅射等多种表示，避免视图间冗余计算。

Result: 在Looking Glass显示器上实现200+ FPS（512p，45视图），基准测试中速度提升22倍且保持图像质量。

Conclusion: 该框架在多种辐射场表示中实现了高效实时渲染，为沉浸式3D交互提供了可行方案。

Abstract: Radiance fields have revolutionized photo-realistic 3D scene visualization by
enabling high-fidelity reconstruction of complex environments, making them an
ideal match for light field displays. However, integrating these technologies
presents significant computational challenges, as light field displays require
multiple high-resolution renderings from slightly shifted viewpoints, while
radiance fields rely on computationally intensive volume rendering. In this
paper, we propose a unified and efficient framework for real-time radiance
field rendering on light field displays. Our method supports a wide range of
radiance field representations, including NeRFs, 3D Gaussian Splatting, and
Sparse Voxels, within a shared architecture based on a single-pass plane
sweeping strategy and caching of shared, non-directional components. The
framework generalizes across different scene formats without retraining, and
avoids redundant computation across views. We further demonstrate a real-time
interactive application on a Looking Glass display, achieving 200+ FPS at 512p
across 45 views, enabling seamless, immersive 3D interaction. On standard
benchmarks, our method achieves up to 22x speedup compared to independently
rendering each view, while preserving image quality.

</details>


### [45] [SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis](https://arxiv.org/abs/2508.18597)
*Xiaohao Sun,Divyam Goel,Angle X. Chang*

Main category: cs.GR

TL;DR: 提出了SemLayoutDiff，一种统一模型，用于生成多样化的3D室内场景布局，结合语义地图与物体属性，并通过扩散模型和注意力机制实现空间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法基于建筑约束生成场景的问题，强调空间一致性和实用性。

Method: 结合语义地图与扩散模型，分步生成语义布局和家具布置，利用注意力机制保持家具与布局的协调。

Result: 在3D-FRONT数据集上验证，生成场景更一致、真实且多样，优于现有方法。

Conclusion: SemLayoutDiff通过显式建筑约束和分层生成策略，显著提升了3D场景合成的质量与多样性。

Abstract: We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor
scenes across multiple room types. The model introduces a scene layout
representation combining a top-down semantic map and attributes for each
object. Unlike prior approaches, which cannot condition on architectural
constraints, SemLayoutDiff employs a categorical diffusion model capable of
conditioning scene synthesis explicitly on room masks. It first generates a
coherent semantic map, followed by a cross-attention-based network to predict
furniture placements that respect the synthesized layout. Our method also
accounts for architectural elements such as doors and windows, ensuring that
generated furniture arrangements remain practical and unobstructed. Experiments
on the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent,
realistic, and varied scenes, outperforming previous methods.

</details>


### [46] [PanoHair: Detailed Hair Strand Synthesis on Volumetric Heads](https://arxiv.org/abs/2508.18944)
*Shashikant Verma,Shanmuganathan Raman*

Main category: cs.GR

TL;DR: PanoHair是一种生成模型，通过预训练教师模型的知识蒸馏，快速生成高保真头发几何结构，显著提升头发合成效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要复杂的数据采集设置且效率低下，PanoHair旨在简化流程并提高头发合成的速度和多样性。

Method: 利用知识蒸馏从预训练模型估计头部几何，并预测头发区域的语义分割和3D方向，支持潜在空间操作生成多样化发型。

Result: PanoHair能在5秒内生成干净的头发网格及语义和方向图，实验证明了其优于现有方法的性能。

Conclusion: PanoHair为头发合成提供了一种高效、多样化的解决方案，克服了现有方法的局限性。

Abstract: Achieving realistic hair strand synthesis is essential for creating lifelike
digital humans, but producing high-fidelity hair strand geometry remains a
significant challenge. Existing methods require a complex setup for data
acquisition, involving multi-view images captured in constrained studio
environments. Additionally, these methods have longer hair volume estimation
and strand synthesis times, which hinder efficiency. We introduce PanoHair, a
model that estimates head geometry as signed distance fields using knowledge
distillation from a pre-trained generative teacher model for head synthesis.
Our approach enables the prediction of semantic segmentation masks and 3D
orientations specifically for the hair region of the estimated geometry. Our
method is generative and can generate diverse hairstyles with latent space
manipulations. For real images, our approach involves an inversion process to
infer latent codes and produces visually appealing hair strands, offering a
streamlined alternative to complex multi-view data acquisition setups. Given
the latent code, PanoHair generates a clean manifold mesh for the hair region
in under 5 seconds, along with semantic and orientation maps, marking a
significant improvement over existing methods, as demonstrated in our
experiments.

</details>


### [47] [A Bag of Tricks for Efficient Implicit Neural Point Clouds](https://arxiv.org/abs/2508.19140)
*Florian Hahlbohm,Linus Franke,Leon Overkämping,Paula Wespe,Susana Castillo,Martin Eisemann,Marcus Magnor*

Main category: cs.GR

TL;DR: INPC是一种结合神经场和点云渲染的高效表示方法，但在渲染速度上受限。本研究通过优化算法实现了更快的训练和推理速度，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 尽管INPC在图像质量上表现优异，但其较慢的渲染速度限制了实际应用。

Method: 改进光栅化实现、高效采样技术、预训练卷积神经网络，并将点建模为高斯分布。

Result: 优化后的INPC实现了训练速度提升25%，渲染速度翻倍，显存使用减少20%，且图像质量略有提升。

Conclusion: 本研究提出的优化方法显著提升了INPC的性能，同时保持了其高质量渲染能力。

Abstract: Implicit Neural Point Cloud (INPC) is a recent hybrid representation that
combines the expressiveness of neural fields with the efficiency of point-based
rendering, achieving state-of-the-art image quality in novel view synthesis.
However, as with other high-quality approaches that query neural networks
during rendering, the practical usability of INPC is limited by comparatively
slow rendering. In this work, we present a collection of optimizations that
significantly improve both the training and inference performance of INPC
without sacrificing visual fidelity. The most significant modifications are an
improved rasterizer implementation, more effective sampling techniques, and the
incorporation of pre-training for the convolutional neural network used for
hole-filling. Furthermore, we demonstrate that points can be modeled as small
Gaussians during inference to further improve quality in extrapolated, e.g.,
close-up views of the scene. We design our implementations to be broadly
applicable beyond INPC and systematically evaluate each modification in a
series of experiments. Our optimized INPC pipeline achieves up to 25% faster
training, 2x faster rendering, and 20% reduced VRAM usage paired with slight
image quality improvements.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [48] [Experiences with Model Context Protocol Servers for Science and High Performance Computing](https://arxiv.org/abs/2508.18489)
*Haochen Pan,Ryan Chard,Reid Mello,Christopher Grams,Tanjin He,Alexander Brace,Owen Price Skelly,Will Engler,Hayden Holbrook,Song Young Oh,Maxime Gonthier,Michael Papka,Ben Blaiszik,Kyle Chard,Ian Foster*

Main category: cs.DC

TL;DR: 本文探讨了如何使用MCP协议作为统一接口，使大型语言模型（LLM）代理能够更有效地发现、调用和组合研究基础设施的异构API。通过多领域案例研究，展示了MCP的实际应用，并总结了经验教训和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 研究基础设施（CI）的异构API和安全模型限制了LLM代理的部署。作者希望通过MCP协议为代理提供一个统一的接口，以简化工作流程。

Method: 采用MCP协议作为中间层，将其实现为现有成熟服务的薄服务器（如Globus、Octopus等），并通过多个领域的案例研究验证其可行性。

Result: 实践证明，MCP能够有效地统一异构API，使代理能够发现、调用和组合研究能力。案例研究涵盖了计算化学、生物信息学等多个领域。

Conclusion: MCP架构在代理主导的科学研究中具有潜力，但仍需解决评估和信任等开放性问题。

Abstract: Large language model (LLM)-powered agents are increasingly used to plan and
execute scientific workflows, yet most research cyberinfrastructure (CI)
exposes heterogeneous APIs and implements security models that present barriers
for use by agents. We report on our experience using the Model Context Protocol
(MCP) as a unifying interface that makes research capabilities discoverable,
invokable, and composable. Our approach is pragmatic: we implement thin MCP
servers over mature services, including Globus Transfer, Compute, and Search;
status APIs exposed by computing facilities; Octopus event fabric; and
domain-specific tools such as Garden and Galaxy. We use case studies in
computational chemistry, bioinformatics, quantum chemistry, and filesystem
monitoring to illustrate how this MCP-oriented architecture can be used in
practice. We distill lessons learned and outline open challenges in evaluation
and trust for agent-led science.

</details>


### [49] [CARMA: Collocation-Aware Resource Manager with GPU Memory Estimator](https://arxiv.org/abs/2508.19073)
*Ehsan Yousefzadeh-Asl-Miandoab,Reza Karimzadeh,Bulat Ibragimov,Florina M. Ciorba,Pınar Tözün*

Main category: cs.DC

TL;DR: 论文提出CARMA系统，通过优化GPU资源分配和任务调度，解决深度学习任务在GPU上的内存不足和性能干扰问题，显著提升GPU利用率和能效。


<details>
  <summary>Details</summary>
Motivation: 企业级基础设施中GPU利用率普遍较低，深度学习任务在GPU上的共存可能导致内存不足和性能干扰问题，影响系统稳健性和能效。

Method: 提出了CARMA系统，包括GPU内存估计框架GPUMemNet和资源管理策略，以最小化内存错误和性能干扰，并提供任务恢复机制。

Result: CARMA将GPU利用率提升了39.3%，任务执行时间减少了约26.7%，GPU能耗降低了约14.2%。

Conclusion: CARMA通过智能资源管理显著优化了GPU的利用效率和能效，解决了深度学习任务共存的挑战。

Abstract: Studies conducted on enterprise-scale infrastructure have shown that GPUs --
the core computational resource for deep learning (DL) training -- are often
significantly underutilized. DL task collocation on GPUs is an opportunity to
address this challenge. However, it may result in (1) out-of-memory crashes for
the subsequently arriving task and (2) slowdowns for all tasks sharing the GPU
due to resource interference. The former challenge poses a threat to
robustness, while the latter affects the quality of service and energy
efficiency.
  We propose CARMA, a server-scale task-level collocation-aware resource
management system that handles both collocation challenges. CARMA encompasses
GPUMemNet, a novel ML-based GPU memory estimator framework for DL training
tasks, to minimize out-of-memory errors and introduces collocation policies
that cap GPU utilization to minimize interference. Furthermore, CARMA
introduces a recovery method to ensure robust restart of tasks that crash. Our
evaluation on traces modeled after real-world DL training task traces shows
that CARMA increases the GPU utilization over time by 39.3\%, decreases the
end-to-end execution time by $\sim$26.7\%, and reduces the GPU energy use by
$\sim$14.2\%.

</details>


### [50] [Managing Multi Instance GPUs for High Throughput and Energy Savings](https://arxiv.org/abs/2508.18556)
*Abhijeet Saraha,Yuanbo Li,Chris Porter,Santosh Pande*

Main category: cs.DC

TL;DR: 该论文提出了针对现代GPU（如Ampere和Hopper系列）的分区与调度方案，显著提升了通用及机器学习工作负载的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 利用现代GPU的性能和安全性隔离特性，解决由于复杂分区约束导致的并发利用难题。

Method: 开发了动态内存估计、分区融合与分裂等方案，并支持进程重启以优化内存错误恢复。

Result: 在A100 GPU上实现了通用工作负载最高6.20倍吞吐和5.93倍能效提升，机器学习工作负载最高1.59倍吞吐和1.12倍能效提升。

Conclusion: 所提方案能显著提升GPU在科学和机器学习工作负载中的性能和能效表现。

Abstract: Modern GPUs such as the Ampere series (A30, A100) as well as the Hopper
series (H100, H200) offer performance as well as security isolation features.
They also support a good amount of concurrency, but taking advantage of it can
be quite challenging due to the complex constraints on partitioning the chip.
  In this work, we develop partitioning and scheduling schemes for a variety of
workloads, ranging from scientific to modern ML workloads, including LLMs. We
develop several schemes involving dynamic memory estimation, partition fusion
and partition fission. We also support process restart to recover from
out-of-memory errors for workloads and early restart as an optimization. This
approach yields up to 6.20x throughput and 5.93x energy improvements for
general workloads; and we see 1.59x and 1.12x improvement to throughput and
energy, respectively, for ML workloads on an A100 GPU. We leverage this
technique on LLM workloads and show good improvements, including up to 1.43x
throughput improvement and 1.11x energy savings.

</details>


### [51] [Strata: Hierarchical Context Caching for Long Context Language Model Serving](https://arxiv.org/abs/2508.18572)
*Zhiqiang Xie,Ziyi Xu,Mark Zhao,Yuwei An,Vikram Sharma Mailthody,Scott Mahlke,Michael Garland,Christos Kozyrakis*

Main category: cs.DC

TL;DR: Strata框架通过GPU辅助I/O和缓存感知调度，解决了长上下文LLM服务中的性能瓶颈，显著提升了响应速度。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM的性能瓶颈在于KV缓存存储和加载的问题，现有系统无法充分利用带宽且调度效率低。

Method: 提出Strata框架，采用GPU辅助I/O解决缓存碎片化，并通过缓存感知调度平衡计算与I/O延迟。

Result: Strata在生产环境中实现了比vLLM + LMCache更低的TTFT（5倍）和比NVIDIA TensorRT-LLM更快的速度（3.75倍）。

Conclusion: Strata以高效的方式解决了长上下文LLM服务的性能问题，且不影响短上下文性能。

Abstract: Large Language Models (LLMs) with expanding context windows face significant
performance hurdles. While caching key-value (KV) states is critical for
avoiding redundant computation, the storage footprint of long-context caches
quickly exceeds GPU memory capacity, forcing production systems to adopt
hierarchical caching across memory hierarchies. However, transferring large
cached contexts back to the GPU introduces severe performance bottlenecks:
fragmented I/O from paged layouts prevents full bandwidth utilization, and
existing schedulers fail to account for cache-loading delays, leaving systems
loading-bound rather than compute-bound. We present Strata, a hierarchical
context caching framework designed for efficient long context LLM serving.
Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling
GPU and CPU memory layouts and employs cache-aware request scheduling to
balance compute with I/O latency and overlapping unavoidable stalls with
complementary tasks. Built on SGLang and deployed in production, Strata
achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache
and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without
degrading short-context performance.

</details>


### [52] [Examining MPI and its Extensions for Asynchronous Multithreaded Communication](https://arxiv.org/abs/2508.18667)
*Jiakun Yan,Marc Snir,Yanfei Guo*

Main category: cs.DC

TL;DR: 该论文评估了MPI的两个扩展（VCI和Continuation）在AMT系统HPX中的性能，发现它们虽有改进但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算架构的复杂化和不规则科学算法的普及，异步多线程通信需求增加，MPI的原始设计未考虑此需求，需评估其新扩展的有效性。

Method: 通过微基准测试和HPX集成，评估VCI和Continuation扩展的峰值性能和实际应用效果。

Result: 新扩展相比标准MPI提升了性能，但Continuation限制了多VCI的多线程消息速率，单VCI模式因注意力问题效果不佳。

Conclusion: 需提高VCI内线程效率以实现可扩展的多线程通信，充分发挥MPI扩展的潜力。

Abstract: The increasing complexity of HPC architectures and the growing adoption of
irregular scientific algorithms demand efficient support for asynchronous,
multithreaded communication. This need is especially pronounced with
Asynchronous Many-Task (AMT) systems. This communication pattern was not a
consideration during the design of the original MPI specification. The MPI
community has recently introduced several extensions to address these evolving
requirements. This work evaluates two such extensions, the Virtual
Communication Interface (VCI) and the Continuation extensions, in the context
of an established AMT runtime HPX. We begin by using an MPI-level
microbenchmark, modeled from HPX's low-level communication mechanism, to
measure the peak performance potential of these extensions. We then integrate
them into HPX to evaluate their effectiveness in real-world scenarios. Our
results show that while these extensions can enhance performance compared to
standard MPI, areas for improvement remain. The current continuation proposal
limits the maximum multithreaded message rate achievable in the multi-VCI
setting. Furthermore, the recommended one-VCI-per-thread mode proves
ineffective in real-world systems due to the attentiveness problem. These
findings underscore the importance of improving intra-VCI threading efficiency
to achieve scalable multithreaded communication and fully realize the benefits
of recent MPI extensions.

</details>


### [53] [ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive](https://arxiv.org/abs/2508.18850)
*Xinhao Luo,Zihan Liu,Yangjie Zhou,Shihan Fang,Ziyu Huang,Yu Feng,Chen Zhang,Shixuan Sun,Zhenzhe Zheng,Jingwen Leng,Minyi Guo*

Main category: cs.DC

TL;DR: 论文提出了一种名为ClusterFusion的执行框架，通过集群级通信原语（ClusterReduce和ClusterGather）优化LLM解码的延迟问题，实现了1.61倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）解码过程中由于操作碎片化和对片外内存依赖导致的高延迟问题。

Method: 引入了ClusterReduce和ClusterGather两个集群级通信原语，设计ClusterFusion框架，通过联合调度通信和计算，扩展算子融合范围。

Result: 在H100 GPU上的评估显示，ClusterFusion平均端到端延迟比现有框架提升1.61倍。

Conclusion: ClusterFusion通过高效的数据交换和算子融合，显著降低了LLM解码的延迟。

Abstract: Large language model (LLM) decoding suffers from high latency due to
fragmented execution across operators and heavy reliance on off-chip memory for
data exchange and reduction. This execution model limits opportunities for
fusion and incurs significant memory traffic and kernel launch overhead. While
modern architectures such as NVIDIA Hopper provide distributed shared memory
and low-latency intra-cluster interconnects, they expose only low-level data
movement instructions, lacking structured abstractions for collective on-chip
communication. To bridge this software-hardware gap, we introduce two
cluster-level communication primitives, ClusterReduce and ClusterGather, which
abstract common communication patterns and enable structured, high-speed data
exchange and reduction between thread blocks within a cluster, allowing
intermediate results to be on-chip without involving off-chip memory. Building
on these abstractions, we design ClusterFusion, an execution framework that
schedules communication and computation jointly to expand operator fusion scope
by composing decoding stages such as QKV Projection, Attention, and Output
Projection into a single fused kernels. Evaluations on H100 GPUs show that
ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on
average in end-to-end latency across different models and configurations. The
source code is available at https://github.com/xinhao-luo/ClusterFusion.

</details>


### [54] [SIREN: Software Identification and Recognition in HPC Systems](https://arxiv.org/abs/2508.18950)
*Thomas Jakobsche,Fredrik Robertsén,Jessica R. Jones,Utz-Uwe Haus,Florina M. Ciorba*

Main category: cs.DC

TL;DR: SIREN是一个用于HPC系统软件识别和重复执行识别的框架，通过模糊哈希等技术克服传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: HPC系统需要更可靠的方法来识别软件和重复执行，传统方法因依赖文件名而不可靠。

Method: 采用模糊哈希技术分析进程元数据、环境信息和可执行文件哈希，开发SIREN框架。

Result: 在LUMI上的初步部署表明，SIREN能有效识别软件使用情况和未知应用的相似性。

Conclusion: SIREN提高了HPC系统的可观测性，为系统优化和安全改进提供了新方法。

Abstract: HPC systems use monitoring and operational data analytics to ensure
efficiency, performance, and orderly operations. Application-specific insights
are crucial for analyzing the increasing complexity and diversity of HPC
workloads, particularly through the identification of unknown software and
recognition of repeated executions, which facilitate system optimization and
security improvements. However, traditional identification methods using job or
file names are unreliable for arbitrary user-provided names (a.out). Fuzzy
hashing of executables detects similarities despite changes in executable
version or compilation approach while preserving privacy and file integrity,
overcoming these limitations. We introduce SIREN, a process-level data
collection framework for software identification and recognition. SIREN
improves observability in HPC by enabling analysis of process metadata,
environment information, and executable fuzzy hashes. Findings from a first
opt-in deployment campaign on LUMI show SIREN's ability to provide insights
into software usage, recognition of repeated executions of known applications,
and similarity-based identification of unknown applications.

</details>


### [55] [Deep Learning-Enabled Supercritical Flame Simulation at Detailed Chemistry and Real-Fluid Accuracy Towards Trillion-Cell Scale](https://arxiv.org/abs/2508.18969)
*Zhuoqiang Guo,Runze Mao,Lijun Liu,Guangming Tan,Weile Jia,Zhi X. Chen*

Main category: cs.DC

TL;DR: 通过优化DeepFlame软件，实现了超临界火焰模拟的大规模计算，达到前所未有的性能，为火箭发动机设计提供了高精度工具。


<details>
  <summary>Details</summary>
Motivation: 超临界火焰模拟因计算资源限制，长期局限于百万级网格。优化软件以突破这一限制，支持火箭发动机等复杂系统的设计。

Method: 从并行计算、计算效率和I/O性能三个方面优化DeepFlame软件，利用深度神经网络保持真实流体力学和化学精度。

Result: 在Sunway和Fugaku超算上实现了高达6180亿和1540亿网格的模拟，性能显著提升，为火箭发动机燃烧模拟提供可能。

Conclusion: 优化的DeepFlame将高精度超临界火焰模拟提升为新设计工具，支持下一代火箭推进系统开发。

Abstract: For decades, supercritical flame simulations incorporating detailed chemistry
and real-fluid transport have been limited to millions of cells, constraining
the resolved spatial and temporal scales of the physical system. We optimize
the supercritical flame simulation software DeepFlame -- which incorporates
deep neural networks while retaining the real-fluid mechanical and chemical
accuracy -- from three perspectives: parallel computing, computational
efficiency, and I/O performance. Our highly optimized DeepFlame achieves
supercritical liquid oxygen/methane (LOX/\ce{CH4}) turbulent combustion
simulation of up to 618 and 154 billion cells with unprecedented
time-to-solution, attaining 439/1186 and 187/316 PFlop/s (32.3\%/21.8\% and
37.4\%/31.8\% of the peak) in FP32/mixed-FP16 precision on Sunway (98,304
nodes) and Fugaku (73,728 nodes) supercomputers, respectively. This
computational capability surpasses existing capacities by three orders of
magnitude, enabling the first practical simulation of rocket engine combustion
with >100 LOX/\ce{CH4} injectors. This breakthrough establishes high-fidelity
supercritical flame modeling as a critical design tool for next-generation
rocket propulsion and ultra-high energy density systems.

</details>


### [56] [Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices](https://arxiv.org/abs/2508.19078)
*Fahao Chen,Jie Wan,Peng Li,Zhou Su,Dongxiao Yu*

Main category: cs.DC

TL;DR: FLUX系统通过量化分析、自适应合并和动态角色分配，实现了资源受限设备上的联邦微调Mixture-of-Experts大语言模型，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限设备上联邦微调MoE大语言模型的挑战，弥补现有方法的不足。

Method: 引入量化分析、自适应合并和动态角色分配三种创新方法。

Result: 在多个数据集上验证，FLUX显著优于现有方法，提速达4.75倍。

Conclusion: FLUX为资源受限设备上的联邦微调提供了高效解决方案。

Abstract: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models
(LLMs) is challenging due to their massive computational requirements and the
resource constraints of participants. Existing working attempts to fill this
gap through model quantization, computation offloading, or expert pruning.
However, they cannot achieve desired performance due to impractical system
assumptions and a lack of consideration for MoE-specific characteristics. In
this paper, we propose FLUX, a system designed to enable federated fine-tuning
of MoE-based LLMs across participants with constrained computing resources
(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX
introduces three key innovations: (1) quantization-based local profiling to
estimate expert activation with minimal overhead, (2) adaptive layer-aware
expert merging to reduce resource consumption while preserving accuracy, and
(3) dynamic expert role assignment using an exploration-exploitation strategy
to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE
and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX
significantly outperforms existing methods, achieving up to 4.75X speedup in
time-to-accuracy.

</details>


### [57] [Ab-initio Quantum Transport with the GW Approximation, 42,240 Atoms, and Sustained Exascale Performance](https://arxiv.org/abs/2508.19138)
*Nicolas Vetsch,Alexander Maeder,Vincent Maillou,Anders Winka,Jiang Cao,Grzegorz Kwasniewski,Leonard Deuschle,Torsten Hoefler,Alexandros Nikolaos Ziogas,Mathieu Luisier*

Main category: cs.DC

TL;DR: 论文提出了一种名为QuaTrEx的NEGF+GW实现方案，用于处理纳米尺度电子器件中的强电子-电子相互作用，并展示了其在大规模计算中的高效性能。


<details>
  <summary>Details</summary>
Motivation: 纳米尺度电子器件（如NRFETs）的建模需要捕捉量子力学效应，尤其是强电子-电子相互作用。现有DFT+NEGF方法需扩展GW近似。

Method: 提出了NEGF+GW方案的首次实现，使用新的空间域分解方案，能够处理实验规模（84,480原子）的器件。

Result: QuaTrEx在超级计算机上表现优异（>80%弱扩展效率），1.15 Eflop/s的exascale性能。

Conclusion: QuaTrEx为纳米器件建模提供了高效工具，能够处理实验规模的强相互作用问题。

Abstract: Designing nanoscale electronic devices such as the currently manufactured
nanoribbon field-effect transistors (NRFETs) requires advanced modeling tools
capturing all relevant quantum mechanical effects. State-of-the-art approaches
combine the non-equilibrium Green's function (NEGF) formalism and density
functional theory (DFT). However, as device dimensions do not exceed a few
nanometers anymore, electrons are confined in ultra-small volumes, giving rise
to strong electron-electron interactions. To account for these critical
effects, DFT+NEGF solvers should be extended with the GW approximation, which
massively increases their computational intensity. Here, we present the first
implementation of the NEGF+GW scheme capable of handling NRFET geometries with
dimensions comparable to experiments. This package, called QuaTrEx, makes use
of a novel spatial domain decomposition scheme, can treat devices made of up to
84,480 atoms, scales very well on the Alps and Frontier supercomputers (>80%
weak scaling efficiency), and sustains an exascale FP64 performance on 42,240
atoms (1.15 Eflop/s).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [58] [Metrics, KPIs, and Taxonomy for Data Valuation and Monetisation -- A Systematic Literature Review](https://arxiv.org/abs/2508.18331)
*Eduardo Vyhmeister,Bastien Pietropaoli,Alejando Martinez Molina,Montserrat Gonzalez-Ferreiro,Gabriel Gonzalez-Castane,Jordi Arjona Aroca,Andrea Visentin*

Main category: cs.DB

TL;DR: 本文通过文献综述和数据分类，探讨了数据估值和货币化的关键指标，并提出了基于平衡计分卡的分类法，帮助组织理解该领域的复杂性。


<details>
  <summary>Details</summary>
Motivation: 数据估值和货币化在现代组织中至关重要，但缺乏标准框架和程序。本文旨在填补这一空白，通过系统梳理文献和指标，帮助组织更好地理解和应用相关概念。

Method: 通过系统文献综述，收集并分析了162篇文献中的指标和KPI。采用平衡计分卡方法对这些指标进行分类和子聚类，以覆盖组织的所有业务层面。

Result: 提出了一个全面的指标分类法，涵盖数据估值和货币化的各个方面，为组织的数据管理提供了清晰的框架。

Conclusion: 尽管提出了分类法，但数据估值和货币化仍面临标准化框架的挑战，未来需要进一步研究和解决这些问题。

Abstract: Data valuation and data monetisation are complex subjects but essential to
most organisations today. Unfortunately, they still lack standard procedures
and frameworks for organisations to follow. In this survey, we introduce the
reader to the concepts by providing the definitions and the background required
to better understand data, monetisation strategies, and finally metrics and
KPIs used in these strategies. We have conducted a systematic literature review
on metrics and KPIs used in data valuation and monetisation, in every aspect of
an organisation's business, and by a variety of stakeholders. We provide an
expansive list of such metrics and KPIs with 162 references. We then categorise
all the metrics and KPIs found into a large taxonomy, following the Balanced
Scorecard (BSC) approach with further subclustering to cover every aspect of an
organisation's business. This taxonomy will help every level of data management
understand the complex landscape of the domain. We also discuss the difficulty
in creating a standard framework for data valuation and data monetisation and
the major challenges the domain is currently facing.

</details>


### [59] [DiskJoin: Large-scale Vector Similarity Join with SSD](https://arxiv.org/abs/2508.18494)
*Yanqi Chen,Xiao Yan,Alexandra Meliou,Eric Lo*

Main category: cs.DB

TL;DR: DiskJoin是一种基于磁盘的高效相似性连接算法，适用于单机处理十亿级向量数据集，通过优化数据访问模式和动态缓存管理显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决分布式计算的通信开销问题和传统磁盘解决方案的I/O瓶颈，提供一种高效的单机处理方案。

Method: 优化数据访问模式以避免重复读取和读放大，利用主存作为动态缓存，并结合概率性剪枝技术减少计算量。

Result: 在真实大规模数据集上，DiskJoin比现有方法快50至1000倍。

Conclusion: DiskJoin为大规模相似性连接提供了一种高效的单机解决方案，显著提升了处理速度和资源利用率。

Abstract: Similarity join--a widely used operation in data science--finds all pairs of
items that have distance smaller than a threshold. Prior work has explored
distributed computation methods to scale similarity join to large data volumes
but these methods require a cluster deployment, and efficiency suffers from
expensive inter-machine communication. On the other hand, disk-based solutions
are more cost-effective by using a single machine and storing the large dataset
on high-performance external storage, such as NVMe SSDs, but in these methods
the disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,
the first disk-based similarity join algorithm that can process billion-scale
vector datasets efficiently on a single machine. DiskJoin improves disk I/O by
tailoring the data access patterns to avoid repetitive accesses and read
amplification. It also uses main memory as a dynamic cache and carefully
manages cache eviction to improve cache hit rate and reduce disk retrieval
time. For further acceleration, we adopt a probabilistic pruning technique that
can effectively prune a large number of vector pairs from computation. Our
evaluation on real-world, large-scale datasets shows that DiskJoin
significantly outperforms alternatives, achieving speedups from 50x to 1000x.

</details>


### [60] [Brook-2PL: Tolerating High Contention Workloads with A Deadlock-Free Two-Phase Locking Protocol](https://arxiv.org/abs/2508.18576)
*Farzad Habibi,Juncheng Fang,Tania Lorido-Botran,Faisal Nawab*

Main category: cs.DB

TL;DR: Brook-2PL是一种新型两阶段锁协议，通过SLW-Graph和部分事务分割解决高争用下的热点问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统并发控制协议在高争用下表现不佳，导致大量事务中止和死锁，Brook-2PL旨在解决这一问题。

Method: 引入SLW-Graph实现无死锁事务执行，并通过部分事务分割实现早期锁释放。

Result: 在TPC-C基准测试中，Brook-2PL平均提速2.86倍，尾部延迟降低48%。

Conclusion: Brook-2PL通过创新方法显著提升了高争用场景下的性能，优于现有协议。

Abstract: The problem of hotspots remains a critical challenge in high-contention
workloads for concurrency control (CC) protocols. Traditional concurrency
control approaches encounter significant difficulties under high contention,
resulting in excessive transaction aborts and deadlocks. In this paper, we
propose Brook-2PL, a novel two-phase locking (2PL) protocol that (1) introduces
SLW-Graph for deadlock-free transaction execution, and (2) proposes partial
transaction chopping for early lock release. Previous methods suffer from
transaction aborts that lead to wasted work and can further burden the system
due to their cascading effects. Brook-2PL addresses this limitation by
statically analyzing a new graph-based dependency structure called SLW-Graph,
enabling deadlock-free two-phase locking through predetermined lock
acquisition. Brook-2PL also reduces contention by enabling early lock release
using partial transaction chopping and static transaction analysis. We overcome
the inherent limitations of traditional transaction chopping by providing a
more flexible chopping method. Evaluation using both our synthetic online game
store workload and the TPC-C benchmark shows that Brook-2PL significantly
outperforms state-of-the-art CC protocols. Brook-2PL achieves an average
speed-up of 2.86x while reducing tail latency (p95) by 48% in the TPC-C
benchmark.

</details>


### [61] [Optimal $(α,β)$-Dense Subgraph Search in Bipartite Graphs](https://arxiv.org/abs/2508.18616)
*Yalong Zhang,Rong-Hua Li,Qi Zhang,Guoren Wang*

Main category: cs.DB

TL;DR: 本文提出了一种针对二分图中密集子图搜索的高效索引BD-Index，支持最优查询时间和线性空间占用，并提供了两种动态维护策略以平衡更新效率与内存使用。


<details>
  <summary>Details</summary>
Motivation: 当前(α,β)-密集子图模型虽能有效捕捉二分图的密度结构，但缺乏高效的查询处理和动态更新支持，限制了其在大规模应用中的实用性。

Method: 设计了BD-Index，一种支持最优查询时间的线性空间索引，并开发了两种动态维护策略：空间高效策略（较低更新复杂度但保持低空间成本）和时间高效策略（显著减少更新时间但增加内存使用）。

Result: 在10个大规模真实数据集上的实验证明，BD-Index及其维护策略具有高效性和可扩展性。

Conclusion: BD-Index及其动态维护策略能够灵活适应不同应用需求，解决了现有模型的局限性。

Abstract: Dense subgraph search in bipartite graphs is a fundamental problem in graph
analysis, with wide-ranging applications in fraud detection, recommendation
systems, and social network analysis. The recently proposed $(\alpha,
\beta)$-dense subgraph model has demonstrated superior capability in capturing
the intrinsic density structure of bipartite graphs compared to existing
alternatives. However, despite its modeling advantages, the $(\alpha,
\beta)$-dense subgraph model lacks efficient support for query processing and
dynamic updates, limiting its practical utility in large-scale applications. To
address these limitations, we propose BD-Index, a novel index that answers
$(\alpha, \beta)$-dense subgraph queries in optimal time while using only
linear space $O(|E|)$, making it well-suited for real-world applications
requiring both fast query processing and low memory consumption. We further
develop two complementary maintenance strategies for dynamic bipartite graphs
to support efficient updates to the BD-Index. The space-efficient strategy
updates the index in time complexity of $O(p \cdot |E|^{1.5})$ per edge
insertion or deletion, while maintaining a low space cost of $O(|E|)$ (the same
as the index itself), where $p$ is typically a small constant in real-world
graphs. In contrast, the time-efficient strategy significantly reduces the
update time to $O(p \cdot |E|)$ per edge update by maintaining auxiliary
orientation structures, at the cost of increased memory usage up to $O(p \cdot
|E|)$. These two strategies provide flexible trade-offs between maintenance
efficiency and memory usage, enabling BD-Index to adapt to diverse application
requirements. Extensive experiments on 10 large-scale real-world datasets
demonstrate high efficiency and scalability of our proposed solutions.

</details>


### [62] [WoW: A Window-to-Window Incremental Index for Range-Filtering Approximate Nearest Neighbor Search](https://arxiv.org/abs/2508.18617)
*Ziqi Wang,Jingzhe Zhang,Wei Hu*

Main category: cs.DB

TL;DR: 该论文提出了一种基于窗口图的RFANNS索引方法，解决了增量构建和任意范围过滤的挑战，实验表明其构建时间和查询效率均优于现有索引。


<details>
  <summary>Details</summary>
Motivation: RFANNS在向量数据库和智能系统中是基础功能，但现有索引难以增量构建且无法应对任意范围过滤，需改进。

Method: 提出窗口图索引，增量构建算法用于动态添加数据，并通过范围选择性优化窗口搜索以处理任意范围过滤。

Result: 实验显示，构建时间与最快速索引相当，查询效率比最优动态索引快4倍，且接近静态索引性能。

Conclusion: 窗口图索引在构建和查询效率上均表现优异，适用于实际应用中增量数据和高性能需求。

Abstract: Given a hybrid dataset where every data object consists of a vector and an
attribute value, for each query with a target vector and a range filter,
range-filtering approximate nearest neighbor search (RFANNS) aims to retrieve
the most similar vectors from the dataset and the corresponding attribute
values fall in the query range. It is a fundamental function in vector database
management systems and intelligent systems with embedding abilities. Dedicated
indices for RFANNS accelerate query speed with an acceptable accuracy loss on
nearest neighbors. However, they are still facing the challenges to be
constructed incrementally and generalized to achieve superior query performance
for arbitrary range filters. In this paper, we introduce a window graph-based
RFANNS index. For incremental construction, we propose an insertion algorithm
to add new vector-attribute pairs into hierarchical window graphs with varying
window size. To handle arbitrary range filters, we optimize relevant window
search for attribute filter checks and vector distance computations by range
selectivity. Extensive experiments on real-world datasets show that for index
construction, the indexing time is on par with the most building-efficient
index, and 4.9x faster than the most query-efficient index with 0.4-0.5x
smaller size; For RFANNS query, it is 4x faster than the most efficient
incremental index, and matches the performance of the best statically-built
index.

</details>


### [63] [Rethinking Caching for LLM Serving Systems: Beyond Traditional Heuristics](https://arxiv.org/abs/2508.18736)
*Jungwoo Kim,Minsang Kim,Jaeheon Lee,Chanwoo Moon,Heejin Kim,Taeho Hwang,Woosuk Chung,Yeseong Kim,Sungjin Lee*

Main category: cs.DB

TL;DR: SISO是一种专为大型语言模型（LLM）服务设计的语义缓存系统，通过创新方法显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统缓存策略无法满足LLM服务的高效需求，需要新的解决方案以应对计算和内存限制。

Method: SISO采用基于聚类的缓存（centroid-based caching）、局部感知替换（locality-aware replacement）和动态阈值（dynamic thresholding）来优化性能。

Result: 在多样化数据集中，SISO的命中率提升了1.71倍，服务等级目标（SLO）达成率更高。

Conclusion: SISO通过创新的语义缓存策略，为LLM服务提供了高效、可扩展的解决方案。

Abstract: Serving Large Language Models (LLMs) at scale requires meeting strict Service
Level Objectives (SLOs) under severe computational and memory constraints.
Nevertheless, traditional caching strategies fall short: exact-matching and
prefix caches neglect query semantics, while state-of-the-art semantic caches
remain confined to traditional intuitions, offering little conceptual
departure. Building on this, we present SISO, a semantic caching system that
redefines efficiency for LLM serving. SISO introduces centroid-based caching to
maximize coverage with minimal memory, locality-aware replacement to preserve
high-value entries, and dynamic thresholding to balance accuracy and latency
under varying workloads. Across diverse datasets, SISO delivers up to
1.71$\times$ higher hit ratios and consistently stronger SLO attainment
compared to state-of-the-art systems.

</details>


### [64] [Text to Query Plans for Question Answering on Large Tables](https://arxiv.org/abs/2508.18758)
*Yipeng Zhang,Chen Wang,Yuzhe Zhang,Jacky Jiang*

Main category: cs.DB

TL;DR: 提出了一种将自然语言查询转换为查询计划的新框架，绕过SQL的局限性并支持复杂分析功能。


<details>
  <summary>Details</summary>
Motivation: 解决大型表格数据集查询和分析的挑战，特别是为非编程用户提供更高效和灵活的工具。

Method: 利用LLM逐步解释查询并构建操作序列，直接在数据上执行操作。

Result: 实验验证了该框架在标准数据库和大型科学表格上的有效性。

Conclusion: 该框架克服了SQL的不足，实现了更灵活和高效的数据分析。

Abstract: Efficient querying and analysis of large tabular datasets remain significant
challenges, especially for users without expertise in programming languages
like SQL. Text-to-SQL approaches have shown promising performance on benchmark
data; however, they inherit SQL's drawbacks, including inefficiency with large
datasets and limited support for complex data analyses beyond basic querying.
We propose a novel framework that transforms natural language queries into
query plans. Our solution is implemented outside traditional databases,
allowing us to support classical SQL commands while avoiding SQL's inherent
limitations. Additionally, we enable complex analytical functions, such as
principal component analysis and anomaly detection, providing greater
flexibility and extensibility than traditional SQL capabilities. We leverage
LLMs to iteratively interpret queries and construct operation sequences,
addressing computational complexity by incrementally building solutions. By
executing operations directly on the data, we overcome context length
limitations without requiring the entire dataset to be processed by the model.
We validate our framework through experiments on both standard databases and
large scientific tables, demonstrating its effectiveness in handling extensive
datasets and performing sophisticated data analyses.

</details>


### [65] [Enriching Object-Centric Event Data with Process Scopes: A Framework for Aggregation and Analysis](https://arxiv.org/abs/2508.18830)
*Shahrzad Khayatbashi,Majid Rafiei,Jiayuan Chen,Timotheus Kampik,Gregor Berg,Amin Jalali*

Main category: cs.DB

TL;DR: 论文提出了一种方法，将分析师定义的过程范围嵌入到面向对象的事件日志（OCEL）中，以解决现有格式缺乏明确过程范围定义的问题，支持多过程共存和不同抽象级别的分析。


<details>
  <summary>Details</summary>
Motivation: 现有面向对象事件数据（OCED）格式缺乏明确的过程范围定义，限制了分析层次和洞察力。实践中，OCED常涉及多个相互关联的过程，但因过程边界不明确而难以解释。

Method: 提出了一种方法，将分析师定义的过程范围嵌入OCEL，支持多过程的结构化表示和跨范围的聚合分析。

Result: 通过公开可用的OCEL日志验证了方法的适用性，并提供工具支持范围定义和分析。

Conclusion: 该方法解决了过程边界不明确的问题，支持更灵活和多层次的分析，提升了面向对象过程挖掘的实用性。

Abstract: Object-Centric Process Mining enables the analysis of complex operational
behavior by capturing interactions among multiple business objects (e.g.,
orders, items, deliveries). These interactions are recorded using
Object-Centric Event Data (OCED) formats, such as the Object-Centric Event Log
(OCEL). However, existing formats lack explicit definitions of process scopes,
which restricts analysis to individual processes and limits insights to a low
level of granularity. In practice, OCED often spans multiple interrelated
processes, as shared objects connect events across organizational functions.
This structure reflects how value is created along the organizational value
chain, but introduces challenges for interpretation when process boundaries are
not clearly defined. Moreover, process definitions are typically subjective and
context-dependent; they vary across organizations, roles, and analytical goals,
and cannot always be discovered automatically. To address these challenges, we
propose a method for embedding analyst-defined process scopes into OCEL. This
enables the structured representation of multiple coexisting processes,
supports the aggregation of event data across scopes, and facilitates analysis
at varying levels of abstraction. We demonstrate the applicability of our
approach using a publicly available OCEL log and provide supporting tools for
scope definition and analysis.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [66] [SeDA: Secure and Efficient DNN Accelerators with Hardware/Software Synergy](https://arxiv.org/abs/2508.18924)
*Wei Xuan,Zhongrui Wang,Lang Feng,Ning Lin,Zihao Xuan,Rongliang Fu,Tsung-Yi Ho,Yuzhong Jiao,Luhong Liang*

Main category: cs.AR

TL;DR: 论文提出SeDA，通过带宽感知加密、优化块粒度和多级完整性验证，显著减少DNN加速器的性能和内存访问开销。


<details>
  <summary>Details</summary>
Motivation: 确保DNN加速器的机密性和完整性在自动驾驶、医疗和金融等领域至关重要，但现有安全方案资源消耗大且内存访问开销高。

Method: SeDA采用带宽感知加密、优化块粒度（通过层内和层间分块模式）和多级完整性验证机制。

Result: 实验显示，SeDA将服务器和边缘NPU的性能开销降低超过12%，同时保持强可扩展性。

Conclusion: SeDA在减少资源消耗和内存访问开销方面表现优异，适用于多样化的应用场景。

Abstract: Ensuring the confidentiality and integrity of DNN accelerators is paramount
across various scenarios spanning autonomous driving, healthcare, and finance.
However, current security approaches typically require extensive hardware
resources, and incur significant off-chip memory access overheads. This paper
introduces SeDA, which utilizes 1) a bandwidth-aware encryption mechanism to
improve hardware resource efficiency, 2) optimal block granularity through
intra-layer and inter-layer tiling patterns, and 3) a multi-level integrity
verification mechanism that minimizes, or even eliminates, memory access
overheads. Experimental results show that SeDA decreases performance overhead
by over 12% for both server and edge neural processing units (NPUs), while
ensuring robust scalability.

</details>


### [67] [TaiBai: A fully programmable brain-inspired processor with topology-aware efficiency](https://arxiv.org/abs/2508.18961)
*Qianpeng Li,Yu Song,Xin Liu,Wenna Song,Boshi Zhao,Zhichao Wang,Aoxin Chen,Tielin Zhang,Liang Chen*

Main category: cs.AR

TL;DR: TaiBai是一种事件驱动的可编程多核脑启发处理器，通过利用时空峰值稀疏性优化带宽和计算开销，具有灵活的拓扑编码、多粒度指令集和优化的编译器堆栈，能效比传统GPU高200倍以上。


<details>
  <summary>Details</summary>
Motivation: 传统脑启发芯片因网络拓扑刚性限制和神经元可编程性不足，阻碍了其适应性。TaiBai旨在解决这些问题，提供更灵活和高效的计算方案。

Method: 设计了一种层次化拓扑编码方案以支持任意网络架构，开发了多粒度指令集实现神经元和突触的可编程性，并通过协同设计的编译器堆栈优化任务映射和资源分配。

Result: 在多种任务测试中，TaiBai芯片上的脉冲神经网络能效比NVIDIA RTX 3090 GPU高200倍以上，且精度相当。

Conclusion: TaiBai展示了其在多尺度脑模拟和脑启发计算中的高度潜能，是一种可扩展、可编程且超高效的解决方案。

Abstract: Brain-inspired computing has emerged as a promising paradigm to overcome the
energy-efficiency limitations of conventional intelligent systems by emulating
the brain's partitioned architecture and event-driven sparse computation.
However, existing brain-inspired chips often suffer from rigid network topology
constraints and limited neuronal programmability, hindering their adaptability.
To address these challenges, we present TaiBai, an event-driven, programmable
many-core brain-inspired processor that leverages temporal and spatial spike
sparsity to minimize bandwidth and computational overhead. TaiBai chip contains
three key features: First, a brain-inspired hierarchical topology encoding
scheme is designed to flexibly support arbitrary network architectures while
slashing storage overhead for large-scale networks; Second, a multi-granularity
instruction set enables programmability of brain-like spiking neuron or
synapses with various dynamics and on-chip learning rules; Third, a co-designed
compiler stack optimizes task mapping and resource allocation. After evaluating
across various tasks, such as speech recognition, ECG classification, and
cross-day brain-computer interface decoding, we found spiking neural networks
embedded on the TaiBai chip could achieve more than 200 times higher energy
efficiency than a standard NVIDIA RTX 3090 GPU at a comparable accuracy. These
results demonstrated its high potentiation as a scalable, programmable, and
ultra-efficient solution for both multi-scale brain simulation and
brain-inspired computation.

</details>


### [68] [Building an Open CGRA Ecosystem for Agile Innovation](https://arxiv.org/abs/2508.19090)
*Rohan Juneja,Pranav Dangi,Thilini Kaushalya Bandara,Zhaoying Li,Dhananjaya Wijerathne,Li-Shiuan Peh,Tulika Mitra*

Main category: cs.AR

TL;DR: 介绍了一个开源的CGRA生态系统，包括HyCUBE、PACE和Morpher，旨在降低创新门槛并推动空间计算的发展。


<details>
  <summary>Details</summary>
Motivation: 现代AI和边缘计算的硬件-软件协同设计需要开放的敏捷平台，CGRA因其灵活性和高效性非常适合这一需求。

Method: 开发了HyCUBE（具有可重构单周期多跳互连的CGRA）、PACE（集成在RISC-V SoC中的低功耗CGRA）和Morpher（开源CGRA设计框架）。

Result: 通过这些工具，实现了架构探索、跨层优化和实际部署，推动了敏捷硬件开发的创新。

Conclusion: 呼吁制定统一的CGRA抽象层，以解耦硬件特化与软件开发，促进空间计算的开放与可扩展性。

Abstract: Modern computing workloads, particularly in AI and edge applications, demand
hardware-software co-design to meet aggressive performance and energy targets.
Such co-design benefits from open and agile platforms that replace closed,
vertically integrated development with modular, community-driven ecosystems.
Coarse-Grained Reconfigurable Architectures (CGRAs), with their unique balance
of flexibility and efficiency are particularly well-suited for this paradigm.
When built on open-source hardware generators and software toolchains, CGRAs
provide a compelling foundation for architectural exploration, cross-layer
optimization, and real-world deployment. In this paper, we will present an open
CGRA ecosystem that we have developed to support agile innovation across the
stack. Our contributions include HyCUBE, a CGRA with a reconfigurable
single-cycle multi-hop interconnect for efficient data movement; PACE, which
embeds a power-efficient HyCUBE within a RISC-V SoC targeting edge computing;
and Morpher, a fully open-source, architecture-adaptive CGRA design framework
that supports design space exploration, compilation, simulation, and
validation. By embracing openness at every layer, we aim to lower barriers to
innovation, enable reproducible research, and demonstrate how CGRAs can anchor
the next wave of agile hardware development. We will conclude with a call for a
unified abstraction layer for CGRAs and spatial accelerators, one that
decouples hardware specialization from software development. Such a
representation would unlock architectural portability, compiler innovation, and
a scalable, open foundation for spatial computing.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [69] [Architecting Distributed Quantum Computers: Design Insights from Resource Estimation](https://arxiv.org/abs/2508.19160)
*Dmitry Filippov,Peter Yang,Prakash Murali*

Main category: quant-ph

TL;DR: 提出了一种分布式量子计算的资源估算框架，分析了其性能与资源需求，并与单片架构进行比较。


<details>
  <summary>Details</summary>
Motivation: 现有单片量子计算设备在规模上受限，分布式架构有望突破这一限制，但缺乏相关资源估算研究。

Method: 开发了一个新的资源估算框架，模拟分布式执行栈的关键组件，分析不同硬件配置下的量子算法性能。

Result: 分布式系统的资源需求（物理量子比特数量和执行时间）比单片架构高，但硬件实现前景更好。

Conclusion: 分布式量子计算资源估算的可行性和设计见解为未来系统设计提供了重要参考。

Abstract: To enable practically useful quantum computing, we require hundreds to
thousands of logical qubits (collections of physical qubits with error
correction). Current monolithic device architectures have scaling limits beyond
few tens of logical qubits. To scale up, we require architectures that
orchestrate several monolithic devices into a distributed quantum computing
system. Currently, resource estimation, which is crucial for determining
hardware needs and bottlenecks, focuses exclusively on monolithic systems. Our
work fills this gap and answers key architectural design questions about
distributed systems, including the impact of distribution on application
resource needs, the organization of qubits across nodes and the requirements of
entanglement distillation (quantum network). To answer these questions, we
develop a novel resource estimation framework that models the key components of
the distributed execution stack. We analyse the performance of practical
quantum algorithms on various hardware configurations, spanning different qubit
speeds, entanglement generation rates and distillation protocols. We show that
distributed architectures have practically feasible resource requirements; for
a node size of 45K qubits, distributed systems need on average 1.4X higher
number of physical qubits and 4X higher execution time compared to monolithic
architectures, but with more favourable hardware implementation prospects. Our
insights on entanglement generation rates, node sizes and architecture have the
potential to inform system designs in the coming years.

</details>


### [70] [Private Quantum Database](https://arxiv.org/abs/2508.19055)
*Giancarlo Gatti,Rihan Hai*

Main category: quant-ph

TL;DR: 量子数据库通过量子力学保护数据和用户隐私，提出了一种结合QRACs和MUBs的方法，无需依赖可信硬件或复杂加密技术。


<details>
  <summary>Details</summary>
Motivation: 传统数据库系统只能单独保护数据隐私或用户隐私，量子数据库旨在同时实现这两者。

Method: 将关系表编码为基于MUBs的QRACs序列，通过有限量子态传输和单次破坏性测量实现数据重构。

Result: 实现了数据隐私和用户隐私的双重保护，且兼容当前NISQ设备的限制。

Conclusion: 量子数据库为数据管理提供了新的前沿技术，同时适合早期部署的混合量子-经典架构。

Abstract: Quantum databases open an exciting new frontier in data management by
offering privacy guarantees that classical systems cannot match. Traditional
engines tackle user privacy, which hides the records being queried, or data
privacy, which prevents a user from learning more than she has queried. We
propose a quantum database that protects both by leveraging quantum mechanics:
when the user measures her chosen basis, the superposition collapses and the
unqueried rows become physically inaccessible. We encode relational tables as a
sequence of Quantum Random Access Codes (QRACs) over mutually unbiased bases
(MUBs), transmit a bounded number of quantum states, and let a single,
destructive measurement reconstruct only the selected tuple. This allows us to
preserve data privacy and user privacy at once without trusted hardware or
heavyweight cryptography. Moreover, we envision a novel hybrid
quantum-classical architecture ready for early deployment, which ensures
compatibility with the limitations of today's Noisy Intermediate-Scale Quantum
devices.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [71] [Murakkab: Resource-Efficient Agentic Workflow Orchestration in Cloud Platforms](https://arxiv.org/abs/2508.18298)
*Gohar Irfan Chaudhry,Esha Choukse,Haoran Qiu,Íñigo Goiri,Rodrigo Fonseca,Adam Belay,Ricardo Bianchini*

Main category: cs.MA

TL;DR: Murakkab系统通过声明式抽象和跨层优化，显著提升了代理工作流的资源效率，减少了GPU使用、能源消耗和成本。


<details>
  <summary>Details</summary>
Motivation: 当前代理工作流的资源利用效率低，逻辑与模型、硬件选择紧耦合，导致资源浪费和服务目标受损。

Method: 提出Murakkab系统，采用声明式抽象分离工作流规格与执行配置，结合优化器和运行时动态调整执行。

Result: 测试显示，Murakkab减少GPU使用2.8倍，能耗3.7倍，成本4.3倍，同时满足SLO。

Conclusion: Murakkab通过跨层优化有效解决了代理工作流资源浪费问题，显著提升了效率。

Abstract: Agentic workflows commonly coordinate multiple models and tools with complex
control logic. They are quickly becoming the dominant paradigm for AI
applications. However, serving them remains inefficient with today's
frameworks. The key problem is that they expose workflows as opaque sequences
of model and tool calls that tightly couple agent logic with model and hardware
choices. Often, these workflow components are fragmented across different
entities, preventing systems from reasoning about trade-offs across accuracy,
latency, energy, and cost. This leads to resource waste and degraded
service-level objectives (SLOs).
  We present Murakkab, a resource-efficient serving system for agentic
workflows. Murakkab introduces a declarative abstraction that decouples
workflow specification from execution configuration. A profile-guided optimizer
and adaptive runtime jointly manage the full stack: orchestrating workflow
components, mapping them to models and hardware, and dynamically reconfiguring
execution to satisfy user-defined SLOs. By exposing the internal structure of
agentic workflows, Murakkab enables cross-layer optimization that existing
frameworks and cloud schedulers cannot achieve.
  Our evaluation on diverse workflows shows that \sysname{} reduces GPU usage
by up to 2.8$\times$, energy consumption by 3.7$\times$, and cost by
4.3$\times$ while maintaining SLOs.

</details>


### [72] [Toward Generalized Autonomous Agents: A Neuro-Symbolic AI Framework for Integrating Social and Technical Support in Education](https://arxiv.org/abs/2508.18406)
*Ryan Hare,Ying Tang*

Main category: cs.MA

TL;DR: 该论文提出了一种多代理、神经符号框架，结合强化学习的‘导师’代理和基于大语言模型的‘同伴’代理，以解决学生自主学习的问题，并通过案例研究验证其跨领域的适应性。


<details>
  <summary>Details</summary>
Motivation: 教育中的核心挑战是如何让学生自主设定目标、跟踪进度并调整策略。人工智能（尤其是大语言模型和神经符号系统）的进步为数字化学习环境提供了新的支持方式。

Method: 设计了一个多代理、神经符号框架，包含一个基于强化学习的‘导师’代理和一个基于大语言模型的‘同伴’代理，通过中心教育本体统一它们的角色。

Result: 框架在大学生和中学生环境中成功展示了跨领域的适应性。

Conclusion: 论文总结了关键见解，并展望了人工智能驱动学习环境的未来发展方向。

Abstract: One of the enduring challenges in education is how to empower students to
take ownership of their learning by setting meaningful goals, tracking their
progress, and adapting their strategies when faced with setbacks. Research has
shown that this form of leaner-centered learning is best cultivated through
structured, supportive environments that promote guided practice, scaffolded
inquiry, and collaborative dialogue. In response, educational efforts have
increasingly embraced artificial-intelligence (AI)-powered digital learning
environments, ranging from educational apps and virtual labs to serious games.
Recent advances in large language models (LLMs) and neuro-symbolic systems,
meanwhile, offer a transformative opportunity to reimagine how support is
delivered in digital learning environments. LLMs are enabling socially
interactive learning experiences and scalable, cross-domain learning support
that can adapt instructional strategies across varied subjects and contexts. In
parallel, neuro-symbolic AI provides new avenues for designing these agents
that are not only adaptive but also scalable across domains. Based on these
remarks, this paper presents a multi-agent, neuro-symbolic framework designed
to resolve the aforementioned challenges. The framework assigns distinct
pedagogical roles to specialized agents: an RL-based 'tutor' agent provides
authoritative, non-verbal scaffolding, while a proactive, LLM-powered 'peer'
agent facilitates the social dimensions of learning. While prior work has
explored such agents in isolation, our framework's novelty lies in unifying
them through a central educational ontology. Through case studies in both
college-level and middle school settings, we demonstrate the framework's
adaptability across domains. We conclude by outlining key insights and future
directions for advancing AI-driven learning environments.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [73] [The Accessibility Paradox: How Blind and Low Vision Employees Experience and Negotiate Accessibility in the Technology Industry](https://arxiv.org/abs/2508.18492)
*Aparajita Marathe,Anne Marie Piper*

Main category: cs.CY

TL;DR: 科技公司试图通过产品可访问性和雇佣残疾员工改善包容性，但研究中发现组织努力与实际体验存在矛盾，称为‘可访问性悖论’。


<details>
  <summary>Details</summary>
Motivation: 探讨科技公司中残疾员工（尤其是视觉障碍者）的可访问性体验与组织努力之间的不对等现象。

Method: 通过对20名视觉障碍员工的访谈，分析他们在职场中的日常体验。

Result: 揭示了可访问性悖论，即公司追求生产力与利润的目标与雇佣残疾员工的矛盾，表现在数字基础设施、政策、能力假设等方面。

Conclusion: 提出了未来研究和实践中改善职场可访问性与包容性的建议。

Abstract: Many technology companies aim to improve access and inclusion not only by
making their products accessible but also by bringing people with disabilities
into the tech workforce. We know less about how accessibility is experienced
and negotiated by disabled workers within these organizations. Through
interviews with 20 BLV workers across various tech companies, we uncover a
persistent misalignment between organizational attempts at accessibility and
the current realities of these employees. We introduce the concept of the
accessibility paradox, which we define as the inherent tension between the
productivity- and profit-driven nature of tech companies and their desire to
hire and retain disabled workers. Focusing on the experiences of BLV workers,
we show how the accessibility paradox manifests in their everyday workplace
interactions, including digital infrastructure, accommodations processes and
policies, ability assumptions, and competing priorities. We offer
recommendations for future research and practice to understand and improve
workplace accessibility and inclusion.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [74] [A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs](https://arxiv.org/abs/2508.18439)
*Anders Mølmen Høst,Pierre Lison,Leon Moonen*

Main category: cs.CR

TL;DR: TRIAGE使用大语言模型自动将CVE映射到ATT&CK技术，结合规则推理和数据驱动方法，提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞数据库缺乏漏洞实际影响信息，人工映射耗时且效率低，需自动化支持。

Method: 采用混合方法，结合基于规则和上下文学习的LLM模块，预测CVE相关攻击技术。

Result: 上下文学习表现优于单一方法，混合方法提升技术召回率，GPT-4o-mini优于Llama3.3-70B。

Conclusion: LLM可自动化预测漏洞影响，TRIAGE提升CVE到ATT&CK映射效率。

Abstract: Vulnerability databases, such as the National Vulnerability Database (NVD),
offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but
often lack information on their real-world impact, such as the tactics,
techniques, and procedures (TTPs) that adversaries may use to exploit the
vulnerability. However, manually linking CVEs to their corresponding TTPs is a
challenging and time-consuming task, and the high volume of new vulnerabilities
published annually makes automated support desirable.
  This paper introduces TRIAGE, a two-pronged automated approach that uses
Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK
knowledge base. We first prompt an LLM with instructions based on MITRE's CVE
Mapping Methodology to predict an initial list of techniques. This list is then
combined with the results from a second LLM-based module that uses in-context
learning to map a CVE to relevant techniques. This hybrid approach
strategically combines rule-based reasoning with data-driven inference. Our
evaluation reveals that in-context learning outperforms the individual mapping
methods, and the hybrid approach improves recall of exploitation techniques. We
also find that GPT-4o-mini performs better than Llama3.3-70B on this task.
Overall, our results show that LLMs can be used to automatically predict the
impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping
CVEs to ATT&CK more efficient.
  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language
models, automated mapping.

</details>


### [75] [An 8- and 12-bit block AES cipher](https://arxiv.org/abs/2508.18485)
*Peter T. Breuer*

Main category: cs.CR

TL;DR: 本文记录了一种罕见的8位或12位块AES（Rijndael）密码，并提供了Java源代码。


<details>
  <summary>Details</summary>
Motivation: 由于这种微小块的AES密码非常罕见且难以找到，作者希望通过文档和代码的方式填补这一空白。

Method: 作者实现并详细描述了一种8位或12位块的AES（Rijndael）密码，并提供了Java源代码作为参考。

Result: 成功实现并展示了一种非标准的微小块AES密码，为相关研究提供了新的资源。

Conclusion: 本文为研究非标准块大小的AES密码提供了宝贵的资料，并通过开源代码促进了进一步的研究和应用。

Abstract: Because it is so unusual, or hard to find, or expository, a truly tiny 8- or
12-bit block AES (Rijndael) cipher is documented here, along with Java source
code.

</details>


### [76] [An Efficient Lightweight Blockchain for Decentralized IoT](https://arxiv.org/abs/2508.19219)
*Faezeh Dehghan Tarzjani,Mostafa Salehi*

Main category: cs.CR

TL;DR: 为了解决物联网（IoT）网络中的挑战，论文提出了一种基于区块链的去中心化架构，采用轻量级PoA共识算法（WBS方法）以优化能耗和性能。


<details>
  <summary>Details</summary>
Motivation: 物联网的集中式架构和大规模网络带来挑战，需要去中心化解决方案。传统区块链共识算法（如PoW、PoS）对资源有限的IoT设备不适用。

Method: 提出了一种基于权重的PoA共识算法（WBS），结合虚拟化和集群技术，提升生产力和可扩展性。

Result: 通过仿真比较，WBS方法比传统TBS方法显著降低了能耗和响应时间，并提高了吞吐量。

Conclusion: 利用WBS方法的轻量级区块链架构能有效解决IoT中的去中心化需求，提升系统性能。

Abstract: The Internet of Things (IoT) is applied in various fields, and the number of
physical devices connected to the IoT is increasingly growing. There are
significant challenges to the IoT's growth and development, mainly due to the
centralized nature and large-scale IoT networks. The emphasis on the
decentralization of IoT's architecture can overcome challenges to IoT's
capabilities. A promising decentralized platform for IoT is blockchain. Owing
to IoT devices' limited resources, traditional consensus algorithms such as PoW
and PoS in the blockchain are computationally expensive. Therefore, the PoA
consensus algorithm is proposed in the blockchain consensus network for IoT.
The PoA selects the validator as Turn-based selection (TBS) that needs
optimization and faces system reliability, energy consumption, latency, and low
scalability. We propose an efficient, lightweight blockchain for decentralizing
IoT architecture by using virtualization and clustering to increase
productivity and scalability to address these issues. We also introduce a novel
PoA based on the Weight-Based-Selection (WBS) method for validators to validate
transactions and add them to the blockchain. By simulation, we evaluated the
performance of our proposed WBS method as opposed to TBS. The results show
reduced energy consumption, and response time, and increased throughput.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [Quantifying The Limits of AI Reasoning: Systematic Neural Network Representations of Algorithms](https://arxiv.org/abs/2508.18526)
*Anastasis Kratsios,Dennis Zvigelsky,Bradd Hart*

Main category: cs.LG

TL;DR: 本文提出一种元算法，将任何电路转换为具有ReLU激活的前馈神经网络，证明神经网络可以精确模拟各种推理任务，且其规模和结构直接反映电路复杂性。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在完美训练条件下能执行何种形式推理的核心问题。

Method: 通过将推理任务解释为电路仿真，使用ReLU多层感知器逐步替换电路门，构建精确模拟的神经网络。

Result: 证明神经网络能精确模拟任何电路（如布尔逻辑、动态规划等），且规模和结构与电路复杂度直接相关。

Conclusion: 神经网络的算法时间复杂度可转化为空间复杂度，其能力超越经典的通用近似定理。

Abstract: A main open question in contemporary AI research is quantifying the forms of
reasoning neural networks can perform when perfectly trained. This paper
answers this by interpreting reasoning tasks as circuit emulation, where the
gates define the type of reasoning; e.g. Boolean gates for predicate logic,
tropical circuits for dynamic programming, arithmetic and analytic gates for
symbolic mathematical representation, and hybrids thereof for deeper reasoning;
e.g. higher-order logic.
  We present a systematic meta-algorithm that converts essentially any circuit
into a feedforward neural network (NN) with ReLU activations by iteratively
replacing each gate with a canonical ReLU MLP emulator. We show that, on any
digital computer, our construction emulates the circuit exactly--no
approximation, no rounding, modular overflow included--demonstrating that no
reasoning task lies beyond the reach of neural networks. The number of neurons
in the resulting network (parametric complexity) scales with the circuit's
complexity, and the network's computational graph (structure) mirrors that of
the emulated circuit. This formalizes the folklore that NNs networks trade
algorithmic run-time (circuit runtime) for space complexity (number of
neurons).
  We derive a range of applications of our main result, from emulating
shortest-path algorithms on graphs with cubic--size NNs, to simulating stopped
Turing machines with roughly quadratically--large NNs, and even the emulation
of randomized Boolean circuits. Lastly, we demonstrate that our result is
strictly more powerful than a classical universal approximation theorem: any
universal function approximator can be encoded as a circuit and directly
emulated by a NN.

</details>


### [78] [DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction](https://arxiv.org/abs/2508.18376)
*Weilin Cai,Le Qin,Shwai He,Junwei Cui,Ang Li,Jiayi Huang*

Main category: cs.LG

TL;DR: 本文提出DualSparse-MoE系统，通过结合动态张量级计算丢弃和静态神经元级重建，显著提高了MoE模型的效率，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然通过稀疏激活提高效率，但仍面临大规模计算和不可预测激活模式的挑战。本文旨在通过双稀疏性（张量和神经元级别）优化MoE部署。

Method: 引入训练后专家划分技术，在不重新训练的情况下诱导稀疏性。结合动态张量级计算丢弃和静态神经元级重建，构建DualSparse-MoE系统。

Result: 实验表明，丢弃25%的计算仅导致0.08%-0.28%的精度损失，而计算速度成比例提升。负载均衡优化进一步实现1.41倍的模块加速。

Conclusion: DualSparse-MoE在保证高精度的同时，显著提升MoE模型的效率，适用于大规模LLMs的实际部署。

Abstract: Mixture of Experts (MoE) has become a mainstream architecture for building
Large Language Models (LLMs) by reducing per-token computation while enabling
model scaling. It can be viewed as partitioning a large Feed-Forward Network
(FFN) at the tensor level into fine-grained sub-FFNs, or experts, and
activating only a sparse subset for each input. While this sparsity improves
efficiency, MoE still faces substantial challenges due to their massive
computational scale and unpredictable activation patterns.
  To enable efficient MoE deployment, we identify dual sparsity at the tensor
and neuron levels in pre-trained MoE modules as a key factor for both accuracy
and efficiency. Unlike prior work that increases tensor-level sparsity through
finer-grained expert design during pre-training, we introduce post-training
expert partitioning to induce such sparsity without retraining. This preserves
the mathematical consistency of model transformations and enhances both
efficiency and accuracy in subsequent fine-tuning and inference. Building upon
this, we propose DualSparse-MoE, an inference system that integrates dynamic
tensor-level computation dropping with static neuron-level reconstruction to
deliver significant efficiency gains with minimal accuracy loss.
  Experimental results show that enforcing an approximate 25% drop rate with
our approach reduces average accuracy by only 0.08%-0.28% across three
prevailing MoE models, while nearly all degrees of computation dropping
consistently yield proportional computational speedups. Furthermore,
incorporating load-imbalance awareness into expert parallelism achieves a 1.41x
MoE module speedup with just 0.5% average accuracy degradation.

</details>


### [79] [History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL](https://arxiv.org/abs/2508.18588)
*Jingkai He,Tianjian Li,Erhu Feng,Dong Du,Qian Liu,Tao Liu,Yubin Xia,Haibo Chen*

Main category: cs.LG

TL;DR: RhymeRL通过利用历史生成的相似性优化RL训练，提出了两项创新技术，显著提高了GPU利用率和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前RL系统在LLM训练中存在GPU利用率低的问题，主要原因是生成阶段占主导和批次长度不均导致的空闲。需要一种不影响准确性的高效解决方案。

Method: 提出了RhymeRL系统，包含HistoSpec（基于历史生成相似性的推测解码引擎）和HistoPipe（利用历史分布相似性的调度策略）。

Result: 实验证明RhymeRL在真实生产环境中可扩展至数千GPU，性能提升2.6倍，且不影响准确性。

Conclusion: RhymeRL通过高效利用历史数据的相似性，解决了RL训练中的GPU利用率问题，为LLM推理和训练提供了新的优化方向。

Abstract: With the rapid advancement of large language models (LLMs), reinforcement
learning (RL) has emerged as a pivotal methodology for enhancing the reasoning
capabilities of LLMs. Unlike traditional pre-training approaches, RL
encompasses multiple stages: rollout, reward, and training, which necessitates
collaboration among various worker types. However, current RL systems continue
to grapple with substantial GPU underutilization, due to two primary factors:
(1) The rollout stage dominates the overall RL process due to test-time
scaling; (2) Imbalances in rollout lengths (within the same batch) result in
GPU bubbles. While prior solutions like asynchronous execution and truncation
offer partial relief, they may compromise training accuracy for efficiency.
  Our key insight stems from a previously overlooked observation: rollout
responses exhibit remarkable similarity across adjacent training epochs. Based
on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate
RL training with two key innovations. First, to enhance rollout generation, we
present HistoSpec, a speculative decoding inference engine that utilizes the
similarity of historical rollout token sequences to obtain accurate drafts.
Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier
scheduling strategy that leverages the similarity of historical rollout
distributions to balance workload among rollout workers. We have evaluated
RhymeRL within a real production environment, demonstrating scalability from
dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL
achieves a 2.6x performance improvement over existing methods, without
compromising accuracy or modifying the RL paradigm.

</details>


### [80] [FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning](https://arxiv.org/abs/2508.19009)
*Md Anwar Hossen,Fatema Siddika,Wensheng Zhang,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: FedProtoKD 是一种基于双知识蒸馏机制的异构联邦学习方法，通过对比学习和可训练的服务器原型解决了原型缩小问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的异构联邦学习方法在原型聚合时使用加权平均，导致原型缩小，影响模型性能，尤其是在异构模型和极端非独立同分布数据场景下。

Method: 提出 FedProtoKD，利用双知识蒸馏机制（客户端 logits 和原型特征表示），结合对比学习和可训练的服务器原型，改进原型聚合效果。

Result: 在多种设定下，FedProtoKD 的平均准确率提升了 1.13% 到 34.13%，显著优于现有方法。

Conclusion: FedProtoKD 通过改进原型聚合和知识蒸馏机制，有效解决了异构联邦学习中的性能问题，为后续研究提供了新的方向。

Abstract: Heterogeneous Federated Learning (HFL) has gained attention for its ability
to accommodate diverse models and heterogeneous data across clients.
Prototype-based HFL methods emerge as a promising solution to address
statistical heterogeneity and privacy challenges, paving the way for new
advancements in HFL research. This method focuses on sharing only
class-representative prototypes among heterogeneous clients. However, these
prototypes are often aggregated on the server using weighted averaging, leading
to sub-optimal global knowledge; these cause the shrinking of aggregated
prototypes, which negatively affects the model performance in scenarios when
models are heterogeneous and data distributions are extremely non-IID. We
propose FedProtoKD in a Heterogeneous Federated Learning setting, using an
enhanced dual-knowledge distillation mechanism to improve the system
performance with clients' logits and prototype feature representation. We aim
to resolve the prototype margin-shrinking problem using a contrastive
learning-based trainable server prototype by leveraging a class-wise adaptive
prototype margin. Furthermore, we assess the importance of public samples using
the closeness of the sample's prototype to its class representative prototypes,
which enhances learning performance. FedProtoKD achieved average improvements
of 1.13% up to 34.13% accuracy across various settings and significantly
outperforms existing state-of-the-art HFL methods.

</details>


### [81] [A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach](https://arxiv.org/abs/2508.18301)
*Md Sabbir Ahmed,Nova Ahmed*

Main category: cs.LG

TL;DR: 开发了一个快速检测抑郁的系统，利用7天的应用使用数据，在1秒内完成分析，准确率高达82.4%。


<details>
  <summary>Details</summary>
Motivation: 现有系统需要长时间数据收集，无法满足早期抑郁检测的需求。

Method: 开发快速工具收集7天应用使用数据，使用多种机器学习模型和特征选择方法。

Result: 基于1秒内获取的数据，模型识别抑郁学生的准确率达82.4%。

Conclusion: 该系统的快速和简约特性使其适用于欠发达地区，并为开发低资源系统提供支持。

Abstract: Background: Existing robust, pervasive device-based systems developed in
recent years to detect depression require data collected over a long period and
may not be effective in cases where early detection is crucial.
  Objective: Our main objective was to develop a minimalistic system to
identify depression using data retrieved in the fastest possible time.
  Methods: We developed a fast tool that retrieves the past 7 days' app usage
data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from
Bangladesh participated in our study, and our tool collected their app usage
data. To identify depressed and nondepressed students, we developed a diverse
set of ML models. We selected important features using the stable approach,
along with 3 main types of feature selection (FS) approaches.
  Results: Leveraging only the app usage data retrieved in 1 second, our light
gradient boosting machine model used the important features selected by the
stable FS approach and correctly identified 82.4% (n=42) of depressed students
(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we
presented a parsimonious stacking model where around 5 features selected by the
all-relevant FS approach Boruta were used in each iteration of validation and
showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis
of our best models presented behavioral markers that were related to
depression.
  Conclusions: Due to our system's fast and minimalistic nature, it may make a
worthwhile contribution to identifying depression in underdeveloped and
developing regions. In addition, our detailed discussion about the implication
of our findings can facilitate the development of less resource-intensive
systems to better understand students who are depressed.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [82] [Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges](https://arxiv.org/abs/2508.18296)
*Edgar Rangel,Fabio Martinez*

Main category: eess.IV

TL;DR: 该论文提出了一种协作框架，通过共享深度中心无关表征知识来分割DWI序列中的缺血性卒中病变，显著提高了模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 卒中病变分析因患者、扫描设备和专家标注的差异而高度可变，现有计算策略局限于单一机构数据，缺乏泛化能力。

Method: 开发了一个协作框架，通过联邦平均（FedAvg）模型从14个医疗中心的2031项研究中共享知识。

Result: FedAvg模型在所有中心上的DSC为0.71±0.24，优于集中式和其他联邦规则，且在外部分布中心表现出强泛化能力。

Conclusion: 该协作框架解决了数据不足和泛化问题，为卒中病变分析提供了一种高效、可靠的方法。

Abstract: Stroke is the second leading cause of death and the third leading cause of
disability worldwide. Clinical guidelines establish diffusion resonance imaging
(DWI, ADC) as the standard for localizing, characterizing, and measuring
infarct volume, enabling treatment support and prognosis. Nonetheless, such
lesion analysis is highly variable due to different patient demographics,
scanner vendors, and expert annotations. Computational support approaches have
been key to helping with the localization and segmentation of lesions. However,
these strategies are dedicated solutions that learn patterns from only one
institution, lacking the variability to generalize geometrical lesions shape
models. Even worse, many clinical centers lack sufficient labeled samples to
adjust these dedicated solutions. This work developed a collaborative framework
for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge
from deep center-independent representations. From 14 emulated healthcare
centers with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \pm
0.24$, AVD of $5.29 \pm 22.74$, ALD of $2.16 \pm 3.60$ and LF1 of $0.70 \pm
0.26$ over all centers, outperforming both the centralized and other federated
rules. Interestingly, the model demonstrated strong generalization properties,
showing uniform performance across different lesion categories and reliable
performance in out-of-distribution centers (with DSC of $0.64 \pm 0.29$ and AVD
of $4.44 \pm 8.74$ without any additional training).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [83] [An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees](https://arxiv.org/abs/2508.19074)
*ZhenDong Chen,ZhanShang Nie,ShiXing Wan,JunYi Li,YongTian Cheng,Shuai Zhao*

Main category: cs.RO

TL;DR: 论文提出了一种自然语言与机器人语言的翻译框架（NRTrans），通过验证和反馈机制提高轻量级LLM生成机器人控制程序的正确性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法直接利用大型语言模型（LLM）从自然语言任务生成可执行程序，但由于LLM的不一致性和任务的高复杂性，生成的程序错误率高，尤其是在轻量级LLM中表现更差。

Method: 提出了Robot Skill Language（RSL）作为抽象层，连接自然语言任务和机器人技能；设计了RSL编译器和调试器，验证LLM生成的程序并提供错误反馈以优化输出。

Result: 实验表明NRTrans在多种LLM和任务中优于现有方法，显著提高了轻量级LLM的成功率。

Conclusion: 该框架通过验证和反馈机制，为LLM生成的程序提供了正确性保证，提升了机器人应用的效能。

Abstract: The Large Language Models (LLM) are increasingly being deployed in robotics
to generate robot control programs for specific user tasks, enabling embodied
intelligence. Existing methods primarily focus on LLM training and prompt
design that utilize LLMs to generate executable programs directly from user
tasks in natural language. However, due to the inconsistency of the LLMs and
the high complexity of the tasks, such best-effort approaches often lead to
tremendous programming errors in the generated code, which significantly
undermines the effectiveness especially when the light-weight LLMs are applied.
This paper introduces a natural-robotic language translation framework that (i)
provides correctness verification for generated control programs and (ii)
enhances the performance of LLMs in program generation via feedback-based
fine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is
proposed to abstract away from the intricate details of the control programs,
bridging the natural language tasks with the underlying robot skills. Then, the
RSL compiler and debugger are constructed to verify RSL programs generated by
the LLM and provide error feedback to the LLM for refining the outputs until
being verified by the compiler. This provides correctness guarantees for the
LLM-generated programs before being offloaded to the robots for execution,
significantly enhancing the effectiveness of LLM-powered robotic applications.
Experiments demonstrate NRTrans outperforms the existing method under a range
of LLMs and tasks, and achieves a high success rate for light-weight LLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 论文提出了一种新的框架，通过结合模型感知难度和样本内在复杂性来优化多模态思维链（MCoT）提示的选择，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决随机或人工选择提示示例时的不足，这些示例未能考虑模型特定知识分布和任务内在复杂性，导致性能不稳定。

Method: 提出一个基于难度平衡采样策略的框架，结合模型感知难度和内在样本复杂性，设计有序的提示课程。

Result: 在五个基准测试和多个多模态大语言模型上验证，表现显著提升，减少了随机采样带来的性能差异。

Conclusion: 该方法为增强多模态推理提供了一种原则性和稳健的途径。

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [85] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: 论文介绍了LangCrUX数据集，分析了多语言网页无障碍问题，并提出Kizuki作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 多语言网页对视觉障碍用户造成挑战，但缺乏大规模研究数据。

Method: 使用LangCrUX数据集（12种语言的12万个网站），系统分析无障碍提示的语言不一致性。

Result: 发现无障碍提示未反映内容语言多样性，降低了屏幕阅读器效果。

Conclusion: 提出Kizuki扩展以改善多语言网页无障碍测试。

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [86] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
*Jiaqi Chen,Yanzhe Zhang,Yutong Zhang,Yijia Shao,Diyi Yang*

Main category: cs.CL

TL;DR: 论文提出了一种生成式语言模型界面（Generative Interfaces for Language Models），通过主动生成用户界面来提升交互效率，优于传统聊天界面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型的线性请求-响应格式在复杂任务中效率低下，需要更灵活的交互方式。

Method: 利用结构化界面表示和迭代优化，将用户查询转化为任务特定界面。

Result: 生成式界面在70%以上的情况下优于传统聊天界面，用户更偏好此类交互方式。

Conclusion: 生成式界面为人类与AI交互的未来发展提供了新方向。

Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots,
and consultants, capable of supporting a wide range of tasks through natural
conversation. However, most systems remain constrained by a linear
request-response format that often makes interactions inefficient in
multi-turn, information-dense, and exploratory tasks. To address these
limitations, we propose Generative Interfaces for Language Models, a paradigm
in which LLMs respond to user queries by proactively generating user interfaces
(UIs) that enable more adaptive and interactive engagement. Our framework
leverages structured interface-specific representations and iterative
refinements to translate user queries into task-specific UIs. For systematic
evaluation, we introduce a multidimensional assessment framework that compares
generative interfaces with traditional chat-based ones across diverse tasks,
interaction patterns, and query types, capturing functional, interactive, and
emotional aspects of user experience. Results show that generative interfaces
consistently outperform conversational ones, with humans preferring them in
over 70% of cases. These findings clarify when and why users favor generative
interfaces, paving the way for future advancements in human-AI interaction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [AniME: Adaptive Multi-Agent Planning for Long Animation Generation](https://arxiv.org/abs/2508.18781)
*Lisai Zhang,Baohan Xu,Siqian Yang,Mingyu Yin,Jing Liu,Chao Xu,Siqi Wang,Yidi Wu,Yuxin Hong,Zihao Zhang,Yanzhang Liang,Yudong Jiang*

Main category: cs.AI

TL;DR: AniME是一个导演导向的多智能体系统，用于自动化长篇动画制作，涵盖从故事到最终视频的全流程。


<details>
  <summary>Details</summary>
Motivation: 解决长篇动画制作的自动化问题，提供一致性和同步性强的AI驱动解决方案。

Method: 通过导演代理维护全局记忆，协调下游专业代理，结合MCP协议自适应选择子任务控制条件。

Result: 生成具有一致性和同步性的电影级动画，适用于规模化AI动画创作。

Conclusion: AniME为AI驱动的动画制作提供了高效、可扩展的解决方案。

Abstract: We present AniME, a director-oriented multi-agent system for automated
long-form anime production, covering the full workflow from a story to the
final video. The director agent keeps a global memory for the whole workflow,
and coordinates several downstream specialized agents. By integrating
customized Model Context Protocol (MCP) with downstream model instruction, the
specialized agent adaptively selects control conditions for diverse sub-tasks.
AniME produces cinematic animation with consistent characters and synchronized
audio visual elements, offering a scalable solution for AI-driven anime
creation.

</details>


### [88] [MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation](https://arxiv.org/abs/2508.19163)
*Ernest Lim,Yajie Vera He,Jared Joselowitz,Kate Preston,Mohita Chowdhury,Louis Williams,Aisling Higham,Katrina Mason,Mariane Melo,Tom Lawton,Yan Jia,Ibrahim Habli*

Main category: cs.AI

TL;DR: MATRIX 是一个面向安全评估的临床对话系统框架，整合了安全工程方法和基于LLM的评估工具。


<details>
  <summary>Details</summary>
Motivation: 现有评估多关注任务完成度或流畅性，缺乏对安全关键系统行为的深入分析。

Method: MATRIX 结合了安全场景分类、基于LLM的评估器（BehvJudge）和模拟患者代理（PatBot）。

Result: 实验显示MATRIX在危险检测和患者模拟方面表现优异，优于人工评估，并成功应用于多个LLM代理的评估。

Conclusion: MATRIX 首次将安全工程与可扩展的对话AI评估结合，为安全审计提供了标准化工具。

Abstract: Despite the growing use of large language models (LLMs) in clinical dialogue
systems, existing evaluations focus on task completion or fluency, offering
little insight into the behavioral and risk management requirements essential
for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion
fRamework for safe Interactions and conteXtual clinical conversational
evaluation), a structured, extensible framework for safety-oriented evaluation
of clinical dialogue agents.
  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical
scenarios, expected system behaviors and failure modes derived through
structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator
for detecting safety-relevant dialogue failures, validated against expert
clinician annotations; and (3) PatBot, a simulated patient agent capable of
producing diverse, scenario-conditioned responses, evaluated for realism and
behavioral fidelity with human factors expertise, and a patient-preference
study.
  Across three experiments, we show that MATRIX enables systematic, scalable
safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard
detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded
assessment of 240 dialogues. We also conducted one of the first realism
analyses of LLM-based patient simulation, showing that PatBot reliably
simulates realistic patient behavior in quantitative and qualitative
evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking
five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios
and 10 clinical domains.
  MATRIX is the first framework to unify structured safety engineering with
scalable, validated conversational AI evaluation, enabling regulator-aligned
safety auditing. We release all evaluation tools, prompts, structured
scenarios, and datasets.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [89] [Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion](https://arxiv.org/abs/2508.18734)
*DongHoon Lim,YoungChae Kim,Dong-Hyun Kim,Da-Hee Yang,Joon-Hyuk Chang*

Main category: cs.CV

TL;DR: 提出了一种自适应调整音频和视觉特征权重的AVSR框架，以提升噪声环境下的语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有系统在噪声环境中难以估计音频可靠性并动态调整模态依赖性的问题。

Method: 采用基于路由器的跨模态特征融合方法，通过令牌级声学损坏评分动态调整音频和视觉特征权重，并在解码层中通过门控交叉注意力强化视觉线索。

Result: 在LRS3数据集上，相较于AV-HuBERT，模型实现了16.51-42.67%的词错误率相对降低。

Conclusion: 提出的路由器和门控机制显著提升了模型在真实噪声环境中的鲁棒性。

Abstract: Robust audio-visual speech recognition (AVSR) in noisy environments remains
challenging, as existing systems struggle to estimate audio reliability and
dynamically adjust modality reliance. We propose router-gated cross-modal
feature fusion, a novel AVSR framework that adaptively reweights audio and
visual features based on token-level acoustic corruption scores. Using an
audio-visual feature fusion-based router, our method down-weights unreliable
audio tokens and reinforces visual cues through gated cross-attention in each
decoder layer. This enables the model to pivot toward the visual modality when
audio quality deteriorates. Experiments on LRS3 demonstrate that our approach
achieves an 16.51-42.67% relative reduction in word error rate compared to
AV-HuBERT. Ablation studies confirm that both the router and gating mechanism
contribute to improved robustness under real-world acoustic noise.

</details>


### [90] [LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding](https://arxiv.org/abs/2508.19204)
*Julian Ost,Andrea Ramazzina,Amogh Joshi,Maximilian Bömer,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 提出了一种结合代理几何生成和2D图像先验的方法，直接生成具有精确几何的大规模3D驾驶场景，解决了现有技术中静态环境限制或缺乏几何基础的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模场景数据对机器人学习至关重要，但现有神经重建方法受限于静态环境且可控性低，而图像或视频扩散模型缺乏几何基础和因果关系。本文旨在填补这一空白。

Method: 结合代理几何生成与环境表示，利用学习到的2D图像先验进行分数提取，实现高可控性的大规模3D场景生成。

Result: 方法能够生成几何精确、可控性强的大规模3D驾驶场景，支持因果新视角合成和3D几何估计。

Conclusion: 提出的方法在几何一致性和可控性上优于现有技术，能够生成逼真且几何一致的复杂驾驶场景。

Abstract: Large-scale scene data is essential for training and testing in robot
learning. Neural reconstruction methods have promised the capability of
reconstructing large physically-grounded outdoor scenes from captured sensor
data. However, these methods have baked-in static environments and only allow
for limited scene control -- they are functionally constrained in scene and
trajectory diversity by the captures from which they are reconstructed. In
contrast, generating driving data with recent image or video diffusion models
offers control, however, at the cost of geometry grounding and causality. In
this work, we aim to bridge this gap and present a method that directly
generates large-scale 3D driving scenes with accurate geometry, allowing for
causal novel view synthesis with object permanence and explicit 3D geometry
estimation. The proposed method combines the generation of a proxy geometry and
environment representation with score distillation from learned 2D image
priors. We find that this approach allows for high controllability, enabling
the prompt-guided geometry and high-fidelity texture and structure that can be
conditioned on map layouts -- producing realistic and geometrically consistent
3D generations of complex driving scenes.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [91] [Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology](https://arxiv.org/abs/2508.18288)
*Jay L. Cunningham,Adinawa Adjagbodjou,Jeffrey Basoah,Jainaba Jawara,Kowe Kadoma,Aaleyah Lewis*

Main category: eess.AS

TL;DR: 该范围综述探讨了自动语音识别（ASR）及相关技术中公平性、偏见和公正性对非裔美国英语（AAE）使用者的影响，并提出治理为中心的框架。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决ASR技术对语言多样性的忽视及对AAE等语言的偏见问题。

Method: 通过44篇跨学科文献分析，识别了四个主要研究方向。

Result: 发现技术公平干预的不足，并提出治理为中心的ASR开发生命周期框架。

Conclusion: 需更多社区参与和语言学公正的治理方法以实现公平的ASR系统。

Abstract: This scoping literature review examines how fairness, bias, and equity are
conceptualized and operationalized in Automatic Speech Recognition (ASR) and
adjacent speech and language technologies (SLT) for African American English
(AAE) speakers and other linguistically diverse communities. Drawing from 44
peer-reviewed publications across Human-Computer Interaction (HCI), Machine
Learning/Natural Language Processing (ML/NLP), and Sociolinguistics, we
identify four major areas of inquiry: (1) how researchers understand
ASR-related harms; (2) inclusive data practices spanning collection, curation,
annotation, and model training; (3) methodological and theoretical approaches
to linguistic inclusion; and (4) emerging practices and design recommendations
for more equitable systems. While technical fairness interventions are growing,
our review highlights a critical gap in governance-centered approaches that
foreground community agency, linguistic justice, and participatory
accountability. We propose a governance-centered ASR lifecycle as an emergent
interdisciplinary framework for responsible ASR development and offer
implications for researchers, practitioners, and policymakers seeking to
address language marginalization in speech AI systems.

</details>
