{"id": "2507.17930", "pdf": "https://arxiv.org/pdf/2507.17930", "abs": "https://arxiv.org/abs/2507.17930", "authors": ["Vahid Garousi", "Zafar Jafarov"], "title": "How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations", "categories": ["cs.SE"], "comment": null, "summary": "Artificial Intelligence (AI) has the potential to transform Software\nEngineering (SE) by enhancing productivity, efficiency, and decision support.\nTools like GitHub Copilot and ChatGPT have given rise to \"vibe coding\"-an\nexploratory, prompt-driven development style. Yet, how software engineers\nengage with these tools in daily tasks, especially in deciding whether to\ntrust, refine, or reject AI-generated outputs, remains underexplored. This\npaper presents two complementary contributions. First, a pragmatic process\nmodel capturing real-world AI-assisted SE activities, including prompt design,\ninspection, fallback, and refinement. Second, a 2D decision framework that\ncould help developers reason about trade-offs between effort saved and output\nquality. Grounded in practitioner reports and direct observations in three\nindustry settings across Turkiye and Azerbaijan, our work illustrates how\nengineers navigate AI use with human oversight. These models offer structured,\nlightweight guidance to support more deliberate and effective use of AI tools\nin SE, contributing to ongoing discussions on practical human-AI collaboration."}
{"id": "2507.17991", "pdf": "https://arxiv.org/pdf/2507.17991", "abs": "https://arxiv.org/abs/2507.17991", "authors": ["Peter Eckmann", "Adrian Barnett", "Alexandra Bannach-Brown", "Elisa Pilar Bascunan Atria", "Guillaume Cabanac", "Louise Delwen Owen Franzen", "Małgorzata Anna Gazda", "Kaitlyn Hair", "James Howison", "Halil Kilicoglu", "Cyril Labbe", "Sarah McCann", "Vladislav Nachev", "Martijn Roelandse", "Maia Salholz-Hillel", "Robert Schulz", "Gerben ter Riet", "Colby Vorland", "Anita Bandrowski", "Tracey Weissgerber"], "title": "Use as Directed? A Comparison of Software Tools Intended to Check Rigor and Transparency of Published Work", "categories": ["cs.SE", "cs.IR"], "comment": null, "summary": "The causes of the reproducibility crisis include lack of standardization and\ntransparency in scientific reporting. Checklists such as ARRIVE and CONSORT\nseek to improve transparency, but they are not always followed by authors and\npeer review often fails to identify missing items. To address these issues,\nthere are several automated tools that have been designed to check different\nrigor criteria. We have conducted a broad comparison of 11 automated tools\nacross 9 different rigor criteria from the ScreenIT group. We found some\ncriteria, including detecting open data, where the combination of tools showed\na clear winner, a tool which performed much better than other tools. In other\ncases, including detection of inclusion and exclusion criteria, the combination\nof tools exceeded the performance of any one tool. We also identified key areas\nwhere tool developers should focus their effort to make their tool maximally\nuseful. We conclude with a set of insights and recommendations for stakeholders\nin the development of rigor and transparency detection tools. The code and data\nfor the study is available at https://github.com/PeterEckmann1/tool-comparison."}
{"id": "2507.18029", "pdf": "https://arxiv.org/pdf/2507.18029", "abs": "https://arxiv.org/abs/2507.18029", "authors": ["Xiang Echo Chen", "Wenhan Zhu", "Guoshuai Albert Shi", "Michael W. Godfrey"], "title": "An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges", "categories": ["cs.SE"], "comment": null, "summary": "The growing capabilities of generative AI (GenAI) have begun to reshape how\ngames are designed and developed, offering new tools for content creation,\ngameplay simulation, and design ideation. While prior research has explored\ntraditional uses of AI in games, such as controlling agents or generating\nprocedural content. There is limited empirical understanding of how GenAI is\nadopted by developers in real-world contexts, especially within the open-source\ncommunity. This study aims to explore how GenAI technologies are discussed,\nadopted, and integrated into open-source game development by analyzing issue\ndiscussions on GitHub. We investigate the tools, tasks, and challenges\nassociated with GenAI by comparing GenAI-related issues to those involving\ntraditional AI (TradAI) and NonAI topics. Our goal is to uncover how GenAI\ndiffers from other approaches in terms of usage patterns, developer concerns,\nand integration practices. To address this objective, we construct a dataset of\nopen-source game repositories that discuss AI-related topics. We apply open\ncard sorting and thematic analysis to a stratified sample of GitHub issues,\nlabelling each by type and content. These annotations enable comparative\nanalysis across GenAI, TradAI, and NonAI groups, and provide insight into how\nGenAI is shaping the workflows and pain points of open-source game developers."}
{"id": "2507.18037", "pdf": "https://arxiv.org/pdf/2507.18037", "abs": "https://arxiv.org/abs/2507.18037", "authors": ["Sivana Hamer", "Jacob Bowen", "Md Nazmul Haque", "Chris Madden", "Laurie Williams"], "title": "Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping", "categories": ["cs.SE", "cs.CR"], "comment": "Mapping generated from: arXiv:2503.12192", "summary": "The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)\nAttack Technique to Proactive Software Supply Chain Risk Management Framework\n(P-SSCRM) Task mapping described in this document helps software organizations\nto determine how different tasks mitigate the attack techniques of software\nsupply chain attacks. The mapping was created through four independent\nstrategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to\none or more tasks from the 10 frameworks, the mapping we provide is also a\nmapping between MITRE ATT&CK and other prominent government and industry\nframeworks."}
{"id": "2507.17963", "pdf": "https://arxiv.org/pdf/2507.17963", "abs": "https://arxiv.org/abs/2507.17963", "authors": ["Rameen Abdal", "Or Patashnik", "Ekaterina Deyneka", "Hao Chen", "Aliaksandr Siarohin", "Sergey Tulyakov", "Daniel Cohen-Or", "Kfir Aberman"], "title": "Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Project Page and Video :\n  https://snap-research.github.io/zero-shot-dynamic-concepts/", "summary": "Recent advances in text-to-video generation have enabled high-quality\nsynthesis from text and image prompts. While the personalization of dynamic\nconcepts, which capture subject-specific appearance and motion from a single\nvideo, is now feasible, most existing methods require per-instance fine-tuning,\nlimiting scalability. We introduce a fully zero-shot framework for dynamic\nconcept personalization in text-to-video models. Our method leverages\nstructured 2x2 video grids that spatially organize input and output pairs,\nenabling the training of lightweight Grid-LoRA adapters for editing and\ncomposition within these grids. At inference, a dedicated Grid Fill module\ncompletes partially observed layouts, producing temporally coherent and\nidentity preserving outputs. Once trained, the entire system operates in a\nsingle forward pass, generalizing to previously unseen dynamic concepts without\nany test-time optimization. Extensive experiments demonstrate high-quality and\nconsistent results across a wide range of subjects beyond trained concepts and\nediting scenarios."}
{"id": "2507.18509", "pdf": "https://arxiv.org/pdf/2507.18509", "abs": "https://arxiv.org/abs/2507.18509", "authors": ["Henning Urbat"], "title": "Higher-Order Behavioural Conformances via Fibrations", "categories": ["cs.PL"], "comment": null, "summary": "Coinduction is a widely used technique for establishing behavioural\nequivalence of programs in higher-order languages. In recent years, the rise of\nlanguages with quantitative (e.g.~probabilistic) features has led to extensions\nof coinductive methods to more refined types of behavioural conformances, most\nnotably notions of behavioural distance. To guarantee soundness of coinductive\nreasoning, one needs to show that the behavioural conformance at hand forms a\nprogram congruence, i.e. it is suitably compatible with the operations of the\nlanguage. This is usually achieved by a complex proof technique known as\n\\emph{Howe's method}, which needs to be carefully adapted to both the specific\nlanguage and the targeted notion of behavioural conformance. We develop a\nuniform categorical approach to Howe's method that features two orthogonal\ndimensions of abstraction: (1) the underlying higher-order language is modelled\nby an \\emph{abstract higher-order specification} (AHOS), a novel and very\ngeneral categorical account of operational semantics, and (2) notions of\nbehavioural conformance (such as relations or metrics) are modelled via\nfibrations over the base category of an AHOS. Our main result is a fundamental\ncongruence theorem at this level of generality: Under natural conditions on the\ncategorical ingredients and the operational rules of a language modelled by an\nAHOS, the greatest behavioural (bi)conformance on its operational model forms a\ncongruence. We illustrate our theory by deriving congruence of bisimilarity and\nbehavioural pseudometrics for probabilistic higher-order languages."}
{"id": "2507.18238", "pdf": "https://arxiv.org/pdf/2507.18238", "abs": "https://arxiv.org/abs/2507.18238", "authors": ["Filippo Bonchi", "Elena Di Lavore", "Mario Román", "Sam Staton"], "title": "Program Logics via Distributive Monoidal Categories", "categories": ["cs.LO", "18M50"], "comment": "52 pages, including appendix", "summary": "We derive multiple program logics, including correctness, incorrectness, and\nrelational Hoare logic, from the axioms of imperative categories: uniformly\ntraced distributive copy-discard categories. We introduce an internal language\nfor imperative multicategories, on top of which we derive combinators for an\nadaptation of Dijkstra's guarded command language. Rules of program logics are\nderived from this internal language."}
{"id": "2507.18173", "pdf": "https://arxiv.org/pdf/2507.18173", "abs": "https://arxiv.org/abs/2507.18173", "authors": ["Haodong Zhu", "Wenhao Dong", "Linlin Yang", "Hong Li", "Yuguang Yang", "Yangyang Ren", "Qingcheng Zhu", "Zichao Feng", "Changbai Li", "Shaohui Lin", "Runqi Wang", "Xiaoyan Luo", "Baochang Zhang"], "title": "WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Leveraging the complementary characteristics of visible (RGB) and infrared\n(IR) imagery offers significant potential for improving object detection. In\nthis paper, we propose WaveMamba, a cross-modality fusion method that\nefficiently integrates the unique and complementary frequency features of RGB\nand IR decomposed by Discrete Wavelet Transform (DWT). An improved detection\nhead incorporating the Inverse Discrete Wavelet Transform (IDWT) is also\nproposed to reduce information loss and produce the final detection results.\nThe core of our approach is the introduction of WaveMamba Fusion Block (WMFB),\nwhich facilitates comprehensive fusion across low-/high-frequency sub-bands.\nWithin WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba\nframework, first performs initial low-frequency feature fusion with channel\nswapping, followed by deep fusion with an advanced gated attention mechanism\nfor enhanced integration. High-frequency features are enhanced using a strategy\nthat applies an ``absolute maximum\" fusion approach. These advancements lead to\nsignificant performance gains, with our method surpassing state-of-the-art\napproaches and achieving average mAP improvements of 4.5% on four benchmarks."}
{"id": "2507.17835", "pdf": "https://arxiv.org/pdf/2507.17835", "abs": "https://arxiv.org/abs/2507.17835", "authors": ["Simone Fiorellino", "Claudio Battiloro", "Emilio Calvanese Strinati", "Paolo Di Lorenzo"], "title": "Frame-Based Zero-Shot Semantic Channel Equalization for AI-Native Communications", "categories": ["cs.NI"], "comment": null, "summary": "In future AI-native wireless networks, the presence of mismatches between the\nlatent spaces of independently designed and trained deep neural network (DNN)\nencoders may impede mutual understanding due to the emergence of semantic\nchannel noise. This undermines the receiver's ability to interpret transmitted\nrepresentations, thereby reducing overall system performance. To address this\nissue, we propose the Parseval Frame Equalizer (PFE), a zero-shot, frame-based\nsemantic channel equalizer that aligns latent spaces of heterogeneous encoders\nwithout requiring system retraining. PFE enables dynamic signal compression and\nexpansion, mitigating semantic noise while preserving performance on downstream\ntasks. Building on this capability, we introduce a dynamic optimization\nstrategy that coordinates communication, computation, and learning resources to\nbalance energy consumption, end-to-end (E2E) latency, and task performance in\nmulti-agent semantic communication scenarios. Extensive simulations confirm the\neffectiveness of our approach in maintaining semantic consistency and meeting\nlong-term constraints on latency and accuracy under diverse and time-varying\nnetwork conditions."}
{"id": "2507.17778", "pdf": "https://arxiv.org/pdf/2507.17778", "abs": "https://arxiv.org/abs/2507.17778", "authors": ["M. Tedeschi", "S. Rizwan", "C. Shringi", "V. Devram Chandgir", "S. Belich"], "title": "An advanced AI driven database system", "categories": ["cs.DB", "cs.AI", "cs.SE", "68P20", "H.2.4; I.2.7"], "comment": "10 pages, 5 figures, appears in EDULEARN25 Conference Proceedings", "summary": "Contemporary database systems, while effective, suffer severe issues related\nto complexity and usability, especially among individuals who lack technical\nexpertise but are unfamiliar with query languages like Structured Query\nLanguage (SQL). This paper presents a new database system supported by\nArtificial Intelligence (AI), which is intended to improve the management of\ndata using natural language processing (NLP) - based intuitive interfaces, and\nautomatic creation of structured queries and semi-structured data formats like\nyet another markup language (YAML), java script object notation (JSON), and\napplication program interface (API) documentation. The system is intended to\nstrengthen the potential of databases through the integration of Large Language\nModels (LLMs) and advanced machine learning algorithms. The integration is\npurposed to allow the automation of fundamental tasks such as data modeling,\nschema creation, query comprehension, and performance optimization. We present\nin this paper a system that aims to alleviate the main problems with current\ndatabase technologies. It is meant to reduce the need for technical skills,\nmanual tuning for better performance, and the potential for human error. The AI\ndatabase employs generative schema inference and format selection to build its\nschema models and execution formats."}
{"id": "2507.18487", "pdf": "https://arxiv.org/pdf/2507.18487", "abs": "https://arxiv.org/abs/2507.18487", "authors": ["Nathan Astin", "Yuriy V. Pershin"], "title": "Low-power switching of memristors exhibiting fractional-order dynamics", "categories": ["cs.ET", "cond-mat.mes-hall"], "comment": null, "summary": "In this conference contribution, we present some initial results on switching\nmemristive devices exhibiting fractional-order behavior using current pulses.\nIn our model, it is assumed that the evolution of a state variable follows a\nfractional-order differential equation involving a Caputo-type derivative. A\nstudy of Joule losses demonstrates that the best switching strategy minimizing\nthese losses depends on the fractional derivative's order and the power\nexponent in the equation of motion. It is found that when the order of the\nfractional derivative exceeds half of the power exponent, the best approach is\nto employ a wide pulse. Conversely, when this condition is not met, Joule\nlosses are minimized by applying a zero current followed by a narrow current\npulse of the highest allowable amplitude. These findings are explored further\nin the context of multi-pulse control. Our research lays the foundation for the\nadvancement of the next generation of energy-efficient neuromorphic computing\narchitectures that more closely mimic their biological counterparts."}
{"id": "2507.17753", "pdf": "https://arxiv.org/pdf/2507.17753", "abs": "https://arxiv.org/abs/2507.17753", "authors": ["Liang Zhang", "Xiaoming Zhai", "Jionghao Lin", "Jionghao Lin", "Jennifer Kleiman", "Diego Zapata-Rivera", "Carol Forsyth", "Yang Jiang", "Xiangen Hu", "Arthur C. Graesser"], "title": "Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Model (LLM) agents are increasingly utilized in AI-aided\neducation to support tutoring and learning. Effective communication strategies\namong LLM agents improve collaborative problem-solving efficiency and\nfacilitate cost-effective adoption in education. However, little research has\nsystematically evaluated the impact of different communication strategies on\nagents' problem-solving. Our study examines four communication modes,\n\\textit{teacher-student interaction}, \\textit{peer-to-peer collaboration},\n\\textit{reciprocal peer teaching}, and \\textit{critical debate}, in a\ndual-agent, chat-based mathematical problem-solving environment using the\nOpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that\ndual-agent setups outperform single agents, with \\textit{peer-to-peer\ncollaboration} achieving the highest accuracy. Dialogue acts like statements,\nacknowledgment, and hints play a key role in collaborative problem-solving.\nWhile multi-agent frameworks enhance computational tasks, effective\ncommunication strategies are essential for tackling complex problems in AI\neducation."}
{"id": "2507.18040", "pdf": "https://arxiv.org/pdf/2507.18040", "abs": "https://arxiv.org/abs/2507.18040", "authors": ["Harsh Sharma", "Janardhan Rao Doppa", "Umit Y. Ogras", "Partha Pratim Pande"], "title": "Designing High-Performance and Thermally Feasible Multi-Chiplet Architectures enabled by Non-bendable Glass Interposer", "categories": ["cs.AR"], "comment": "Paper accepted at ACM Transactions on Embedded Computing Systems. To\n  be presented in Taiwan, Sept. 2025", "summary": "Multi-chiplet architectures enabled by glass interposer offer superior\nelectrical performance, enable higher bus widths due to reduced crosstalk, and\nhave lower capacitance in the redistribution layer than current silicon\ninterposer-based systems. These advantages result in lower energy per bit,\nhigher communication frequencies, and extended interconnect range. However,\ndeformation of the package (warpage) in glass interposer-based systems becomes\na critical challenge as system size increases, leading to severe mechanical\nstress and reliability concerns. Beyond a certain size, conventional packaging\ntechniques fail to manage warpage effectively, necessitating new approaches to\nmitigate warpage induced bending with scalable performance for glass interposer\nbased multi-chiplet systems. To address these inter-twined challenges, we\npropose a thermal-, warpage-, and performance-aware design framework that\nemploys architecture and packaging co-optimization. The proposed framework\ndisintegrates the surface and embedded chiplets to balance conflicting design\nobjectives, ensuring optimal trade-offs between performance, power, and\nstructural reliability. Our experiments demonstrate that optimized\nmulti-chiplet architectures from our design framework achieve up to 64.7%\nperformance improvement and 40% power reduction compared to traditional 2.5D\nsystems to execute deep neural network workloads with lower fabrication costs."}
{"id": "2507.17766", "pdf": "https://arxiv.org/pdf/2507.17766", "abs": "https://arxiv.org/abs/2507.17766", "authors": ["Felix Quinque", "Alan Aboudib", "Szymon Fonau", "Rodrigo Lopez Portillo Alcocer", "Brian McCrindle", "Steffen Cruz"], "title": "Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed\nnetwork of incentivized, permissionless actors could each pretrain large\nlanguage models (LLMs) ranging from 700 million to 14 billion parameters, while\nsurpassing established baselines. While that work validated blockchain-based\ndecentralized pretraining as viable, it contained core issues: (i) every miner\nhad to fit an entire model locally, and (ii) \"winner-takes-all\" rewards\nencouraged model hoarding.\n  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an\narchitecture that addresses these limitations by transforming SN9's previously\nisolated competitors into a single cooperating unit that can scale arbitrarily\nwhile still rewarding each contributor fairly.\n  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -\nAn orchestrator distributes model layers across heterogeneous miners and\nstreams activations between them, enabling model sizes to scale with the number\nof participants rather than being constrained by the VRAM of a single machine;\n(2) Granular, continuous incentives - Validators measure each miner's\ncontribution and allocate token emissions proportionally; (3) Activation\ncompression - We used model-bottlenecks to cut communication bandwidths of\nactivations by up to 128x, vastly improving training speed; (4) Butterfly\nAll-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,\noffering linear scalability, redundancy and built-in collusion detection; (5)\nCLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair\nattribution scheme assigns credit to miners proportional to their marginal\nutility and detects exploits, even when contributions are interdependent across\nthe pipeline."}
{"id": "2507.18039", "pdf": "https://arxiv.org/pdf/2507.18039", "abs": "https://arxiv.org/abs/2507.18039", "authors": ["Ahmad D. Suleiman", "Yiming Tang", "Daqing Hou"], "title": "Factors Impacting Faculty Adoption of Project-Based Learning in Computing Education: a Survey", "categories": ["cs.SE"], "comment": "Accepted at IEEE Frontiers in Education (FIE) 2025. This work has\n  been submitted to the IEEE for possible publication", "summary": "This research full paper investigates the factors influencing computing\neducators' adoption of project-based learning (PjBL) in software engineering\nand computing curricula. Recognized as a student-centered pedagogical approach,\nPjBL has the potential to enhance student motivation, engagement, critical\nthinking, collaboration, and problem-solving skills. Despite these benefits,\nfaculty adoption remains inconsistent due to challenges such as insufficient\ninstitutional support, time constraints, limited training opportunities,\ndesigning or sourcing projects, and aligning them with course objectives. This\nresearch explores these barriers and investigates the strategies and resources\nthat facilitate a successful adoption. Using a mixed-methods approach, data\nfrom 80 computing faculty were collected through an online survey comprising\nclosed-ended questions to quantify barriers, enablers, and resource needs,\nalong with an open-ended question to gather qualitative insights. Quantitative\ndata were analyzed using statistical methods, while qualitative responses\nunderwent thematic analysis. Results reveal that while PjBL is widely valued,\nits adoption is often selective and impacted by challenges in planning and\nmanaging the learning process, designing suitable projects, and a lack of\ninstitutional support, such as time, funding, and teaching assistants. Faculty\nare more likely to adopt or sustain PjBL when they have access to peer\ncollaboration, professional development, and institutional incentives. In\naddition, sourcing projects from research, industry partnerships, and borrowing\nfrom peers emerged as key facilitators for new projects. These findings\nunderscore the need for systemic support structures to empower faculty to\nexperiment with and scale PjBL practices."}
{"id": "2507.18052", "pdf": "https://arxiv.org/pdf/2507.18052", "abs": "https://arxiv.org/abs/2507.18052", "authors": ["David Sinclair", "Ademyemi Ademola", "Babis Koniaris", "Kenny Mitchell"], "title": "DanceGraph: A Complementary Architecture for Synchronous Dancing Online", "categories": ["cs.GR", "I.3.2; C.2.1"], "comment": "36th International Conference on Computer Animation and Social Agents", "summary": "DanceGraph is an architecture for synchronized online dancing overcoming the\nlatency of networked body pose sharing. We break down this challenge by\ndeveloping a real-time bandwidth-efficient architecture to minimize lag and\nreduce the timeframe of required motion prediction for synchronization with the\nmusic's rhythm. In addition, we show an interactive method for the\nparameterized stylization of dance motions for rhythmic dance using online\ndance correctives."}
{"id": "2507.18268", "pdf": "https://arxiv.org/pdf/2507.18268", "abs": "https://arxiv.org/abs/2507.18268", "authors": ["Giulio Malenza", "Giovanni Stabile", "Filippo Spiga", "Robert Birke", "Marco Aldinucci"], "title": "Building an Accelerated OpenFOAM Proof-of-Concept Application using Modern C++", "categories": ["cs.MS", "cs.PF", "cs.PL"], "comment": null, "summary": "The modern trend in High-Performance Computing (HPC) involves the use of\naccelerators such as Graphics Processing Units (GPUs) alongside Central\nProcessing Units (CPUs) to speed up numerical operations in various\napplications. Leading manufacturers such as NVIDIA, Intel, and AMD are\nconstantly advancing these architectures, augmenting them with features such as\nmixed precision, enhanced memory hierarchies, and specialised accelerator\nsilicon blocks (e.g., Tensor Cores on GPU or AMX/SME engines on CPU) to enhance\ncompute performance. At the same time, significant efforts in software\ndevelopment are aimed at optimizing the use of these innovations, seeking to\nimprove usability and accessibility. This work contributes to the\nstate-of-the-art of OpenFOAM development by presenting a working\nProof-Of-Concept application built using modern ISO C++ parallel constructs.\nThis approach, combined with an appropriate compiler runtime stack, like the\none provided by the NVIDIA HPC SDK, makes it possible to accelerate\nwell-defined kernels, allowing multi-core execution and GPU offloading using a\nsingle codebase. The study demonstrates that it is possible to increase the\nperformance of the OpenFOAM laplacianFoam application by offloading the\ncomputations on NVIDIA GPUs using the C++ parallel construct."}
{"id": "2507.18246", "pdf": "https://arxiv.org/pdf/2507.18246", "abs": "https://arxiv.org/abs/2507.18246", "authors": ["Matthew Earnshaw", "Chad Nester", "Mario Román"], "title": "Resourceful Traces for Commuting Processes", "categories": ["cs.LO", "math.CT"], "comment": null, "summary": "We show that, when the actions of a Mazurkiewicz trace are considered not\nmerely as atomic (i.e., mere names) but transformations from a specified type\nof inputs to a specified type of outputs, we obtain a novel notion of\npresentation for effectful categories (also known as generalised Freyd\ncategories), a well-known algebraic structure in the semantics of\nside-effecting computation. Like the usual representation of traces as graphs,\nour notion of presentation gives rise to a graphical calculus for effectful\ncategories. We use our presentations to give a construction of the commuting\ntensor product of free effectful categories, capturing the combination of\nsystems in which the actions of each must commute with one another, while still\npermitting exchange of resources"}
{"id": "2507.18352", "pdf": "https://arxiv.org/pdf/2507.18352", "abs": "https://arxiv.org/abs/2507.18352", "authors": ["Zhen Han", "Mattias Teye", "Derek Yadgaroff", "Judith Bütepage"], "title": "Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation", "categories": ["cs.GR", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal\n  track)", "summary": "The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters."}
{"id": "2507.17861", "pdf": "https://arxiv.org/pdf/2507.17861", "abs": "https://arxiv.org/abs/2507.17861", "authors": ["Daniel Ricardo Cunha Oliveira", "Rodrigo Moreira", "Flávio de Oliveira Silva"], "title": "ARCADE: A RAN Diagnosis Methodology in a Hybrid AI Environment for 6G Networks", "categories": ["cs.NI", "cs.ET"], "comment": null, "summary": "Artificial Intelligence (AI) plays a key role in developing 6G networks.\nWhile current specifications already include Network Data Analytics Function\n(NWDAF) as a network element responsible for providing information about the\ncore, a more comprehensive approach will be needed to enable automation of\nnetwork segments that are not yet fully explored in the context of 5G. In this\npaper, we present Automated Radio Coverage Anomalies Detection and Evaluation\n(ARCADE), a methodology for identifying and diagnosing anomalies in the\ncellular access network. Furthermore, we demonstrate how a hybrid architecture\nof network analytics functions in the evolution toward 6G can enhance the\napplication of AI in a broader network context, using ARCADE as a practical\nexample of this approach."}
{"id": "2507.17896", "pdf": "https://arxiv.org/pdf/2507.17896", "abs": "https://arxiv.org/abs/2507.17896", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "title": "VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Application systems using natural language interfaces to databases (NLIDBs)\nhave democratized data analysis. This positive development has also brought\nforth an urgent challenge to help users who might use these systems without a\nbackground in statistical analysis to formulate bias-free analytical questions.\nAlthough significant research has focused on text-to-SQL generation accuracy,\naddressing cognitive biases in analytical questions remains underexplored. We\npresent VeriMinder, https://veriminder.ai, an interactive system for detecting\nand mitigating such analytical vulnerabilities. Our approach introduces three\nkey innovations: (1) a contextual semantic mapping framework for biases\nrelevant to specific analysis contexts (2) an analytical framework that\noperationalizes the Hard-to-Vary principle and guides users in systematic data\nanalysis (3) an optimized LLM-powered system that generates high-quality,\ntask-specific prompts using a structured process involving multiple candidates,\ncritic feedback, and self-reflection.\n  User testing confirms the merits of our approach. In direct user experience\nevaluation, 82.5% participants reported positively impacting the quality of the\nanalysis. In comparative evaluation, VeriMinder scored significantly higher\nthan alternative approaches, at least 20% better when considered for metrics of\nthe analysis's concreteness, comprehensiveness, and accuracy. Our system,\nimplemented as a web application, is set to help users avoid \"wrong question\"\nvulnerability during data analysis. VeriMinder code base with prompts,\nhttps://reproducibility.link/veriminder, is available as an MIT-licensed\nopen-source software to facilitate further research and adoption within the\ncommunity."}
{"id": "2507.17861", "pdf": "https://arxiv.org/pdf/2507.17861", "abs": "https://arxiv.org/abs/2507.17861", "authors": ["Daniel Ricardo Cunha Oliveira", "Rodrigo Moreira", "Flávio de Oliveira Silva"], "title": "ARCADE: A RAN Diagnosis Methodology in a Hybrid AI Environment for 6G Networks", "categories": ["cs.NI", "cs.ET"], "comment": null, "summary": "Artificial Intelligence (AI) plays a key role in developing 6G networks.\nWhile current specifications already include Network Data Analytics Function\n(NWDAF) as a network element responsible for providing information about the\ncore, a more comprehensive approach will be needed to enable automation of\nnetwork segments that are not yet fully explored in the context of 5G. In this\npaper, we present Automated Radio Coverage Anomalies Detection and Evaluation\n(ARCADE), a methodology for identifying and diagnosing anomalies in the\ncellular access network. Furthermore, we demonstrate how a hybrid architecture\nof network analytics functions in the evolution toward 6G can enhance the\napplication of AI in a broader network context, using ARCADE as a practical\nexample of this approach."}
{"id": "2507.17754", "pdf": "https://arxiv.org/pdf/2507.17754", "abs": "https://arxiv.org/abs/2507.17754", "authors": ["Justin Morse", "Kurt Gilbert", "Kyle Shin", "Rick Cooke", "Peyton Rose", "Jack Sullivan", "Angelo Sisante"], "title": "A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Clinician burnout has motivated the growing adoption of ambient medical\nscribes in the clinic. In this work, we introduce a custom-built ambient scribe\napplication integrated into the EHR system at Included Health, a personalized\nall-in-one healthcare company offering telehealth services. The application\nuses Whisper for transcription and a modular in-context learning pipeline with\nGPT-4o to automatically generate SOAP notes and patient instructions. Testing\non mock visit data shows that the notes generated by the application exceed the\nquality of expert-written notes as determined by an LLM-as-a-judge. The\napplication has been widely adopted by the clinical practice, with over 540\nclinicians at Included Health using the application at least once. 94% (n = 63)\nof surveyed clinicians report reduced cognitive load during visits and 97% (n =\n66) report less documentation burden when using the application. Additionally,\nwe show that post-processing notes with a fine-tuned BART model improves\nconciseness. These findings highlight the potential for AI systems to ease\nadministrative burdens and support clinicians in delivering efficient,\nhigh-quality care."}
{"id": "2507.18454", "pdf": "https://arxiv.org/pdf/2507.18454", "abs": "https://arxiv.org/abs/2507.18454", "authors": ["Juntao Zhao", "Jiuru Li", "Chuan Wu"], "title": "Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.PL"], "comment": null, "summary": "Utilizing CPUs to serve large language models (LLMs) is a resource-friendly\nalternative to GPU serving. Existing CPU-based solutions ignore workload\ndifferences between the prefill and the decode phases of LLM inference,\napplying a static per-NUMA (Non-Uniform Memory Access) node model partition and\nutilizing vendor libraries for operator-level execution, which is suboptimal.\nWe propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses\ndifferent execution plans for the prefill and decode phases and optimizes them\nseparately.\n  We evaluate Sandwich across diverse baselines and datasets on five CPU\nplatforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.\nSandwich achieves an average 2.01x throughput improvement and 90% satisfactory\ntime-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up\nto 3.40x lower requirements in single sequence serving, and significant\nimprovement in Goodput in continuous-batching serving. The GEMM kernels\ngenerated by Sandwich outperform representative vendor kernels and other\ndynamic shape solutions, achieving performance comparable to static compilers\nwith three orders of magnitude less kernel tuning costs."}
{"id": "2507.17769", "pdf": "https://arxiv.org/pdf/2507.17769", "abs": "https://arxiv.org/abs/2507.17769", "authors": ["Kan Zhu", "Haiyang Shi", "Le Xu", "Jiaxin Shan", "Arvind Krishnamurthy", "Baris Kasikci", "Liguang Xie"], "title": "PolyServe: Efficient Multi-SLO Serving at Scale", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Advances in Large Language Models (LLMs) have led to a surge of LLM-powered\napplications. These applications have diverse token-generation latency\nrequirements. As a result, simply classifying workloads as latency-sensitive\n(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive\ncategory and results in suboptimal user experiences and scheduling\nopportunities. However, efficiently serving requests with multiple SLO\nrequirements poses significant challenges. First, all requests within a batch\ngenerate new tokens simultaneously, which can misalign them with their distinct\nSLO requirements. Moreover, while existing systems focus on auto-scaling for\nhandling various overall request rates, the diversity of SLOs necessitates\nfine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE\nscenarios, where BE requests can be aborted at any time to ensure the SLO\nattainment of LS requests, those with different latency-sensitive SLOs cannot\ntolerate prolonged delays, and tail latency must be controlled.\n  To tackle these challenges, we propose PolyServe, a novel multi-SLO\nscheduling policy at scale that maintains high SLO attainment while maximizing\nthroughput. PolyServe first groups requests into multiple bins based on their\nper-token latency requirement, then schedules each bin to a subset of the\nserver fleet. PolyServe routes requests to the highest-load but still\nSLO-attainable server to create a load gradient that facilitates auto-scaling.\nTo increase utilization, PolyServe permits looser-SLO requests to share\ntighter-SLO instances when their own servers are saturated. PolyServe uses\nprofiling data to guide scheduling decisions and manage tail latency through\nrequest-wait-time-aware scheduling, dynamic chunking, and continuous chunked\nprefill prediction. PolyServe achieves 1.23x goodput gain compared to existing\npolicies, achieving up to 92.5% of optimal goodput."}
{"id": "2507.18062", "pdf": "https://arxiv.org/pdf/2507.18062", "abs": "https://arxiv.org/abs/2507.18062", "authors": ["Edward Abrokwah", "Taher A. Ghaleb"], "title": "An Empirical Study of Complexity, Heterogeneity, and Compliance of GitHub Actions Workflows", "categories": ["cs.SE"], "comment": "Registered Report Accepted at the 41st IEEE International Conference\n  on Software Maintenance and Evolution 2025 (ICSME'25)", "summary": "Continuous Integration (CI) has evolved from a tooling strategy to a\nfundamental mindset in modern CI engineering. It enables teams to develop,\ntest, and deliver software rapidly and collaboratively. Among CI services,\nGitHub Actions (GHA) has emerged as a dominant service due to its deep\nintegration with GitHub and a vast ecosystem of reusable workflow actions.\nAlthough GHA provides official documentation and community-supported best\npractices, there appears to be limited empirical understanding of how\nopen-source real-world CI workflows align with such practices. Many workflows\nmight be unnecessarily complex and not aligned with the simplicity goals of CI\npractices. This study will investigate the structure, complexity,\nheterogeneity, and compliance of GHA workflows in open-source software\nrepositories. Using a large dataset of GHA workflows from Java, Python, and C++\nrepositories, our goal is to (a) identify workflow complexities, (b) analyze\nrecurring and heterogeneous structuring patterns, (c) assess compliance with\nGHA best practices, and (d) uncover differences in CI pipeline design across\nprogramming languages. Our findings are expected to reveal both areas of strong\nadherence to best practices and areas for improvement where needed. These\ninsights will also have implications for CI services, as they will highlight\nthe need for clearer guidelines and comprehensive examples in CI documentation."}
{"id": "2507.18155", "pdf": "https://arxiv.org/pdf/2507.18155", "abs": "https://arxiv.org/abs/2507.18155", "authors": ["SeungJun Moon", "Hah Min Lew", "Seungeun Lee", "Ji-Su Kang", "Gyeong-Moon Park"], "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "ICCV 2025, Project page: https://hahminlew.github.io/geoavatar/", "summary": "Despite recent progress in 3D head avatar generation, balancing identity\npreservation, i.e., reconstruction, with novel poses and expressions, i.e.,\nanimation, remains a challenge. Existing methods struggle to adapt Gaussians to\nvarying geometrical deviations across facial regions, resulting in suboptimal\nquality. To address this, we propose GeoAvatar, a framework for adaptive\ngeometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation\nStage (APS), an unsupervised method that segments Gaussians into rigid and\nflexible sets for adaptive offset regularization. Then, based on mouth anatomy\nand dynamics, we introduce a novel mouth structure and the part-wise\ndeformation strategy to enhance the animation fidelity of the mouth. Finally,\nwe propose a regularization loss for precise rigging between Gaussians and 3DMM\nfaces. Moreover, we release DynamicFace, a video dataset with highly expressive\nfacial motions. Extensive experiments show the superiority of GeoAvatar\ncompared to state-of-the-art methods in reconstruction and novel animation\nscenarios."}
{"id": "2507.18454", "pdf": "https://arxiv.org/pdf/2507.18454", "abs": "https://arxiv.org/abs/2507.18454", "authors": ["Juntao Zhao", "Jiuru Li", "Chuan Wu"], "title": "Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.PL"], "comment": null, "summary": "Utilizing CPUs to serve large language models (LLMs) is a resource-friendly\nalternative to GPU serving. Existing CPU-based solutions ignore workload\ndifferences between the prefill and the decode phases of LLM inference,\napplying a static per-NUMA (Non-Uniform Memory Access) node model partition and\nutilizing vendor libraries for operator-level execution, which is suboptimal.\nWe propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses\ndifferent execution plans for the prefill and decode phases and optimizes them\nseparately.\n  We evaluate Sandwich across diverse baselines and datasets on five CPU\nplatforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.\nSandwich achieves an average 2.01x throughput improvement and 90% satisfactory\ntime-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up\nto 3.40x lower requirements in single sequence serving, and significant\nimprovement in Goodput in continuous-batching serving. The GEMM kernels\ngenerated by Sandwich outperform representative vendor kernels and other\ndynamic shape solutions, achieving performance comparable to static compilers\nwith three orders of magnitude less kernel tuning costs."}
{"id": "2507.18418", "pdf": "https://arxiv.org/pdf/2507.18418", "abs": "https://arxiv.org/abs/2507.18418", "authors": ["Jean Goubault-Larrecq"], "title": "Distributing Retractions, Weak Distributive Laws and Applications to Monads of Hyperspaces, Continuous Valuations and Measures", "categories": ["cs.LO", "math.CT", "18N15, 18C15 (Primary) 54B20, 28A33, 46E27 (Secondary)"], "comment": "46 pages", "summary": "Given two monads $S$, $T$ on a category where idempotents split, and a weak\ndistributive law between them, one can build a combined monad $U$. Making\nexplicit what this monad $U$ is requires some effort. When we already have an\nidea what $U$ should be, we show how to recognize that $U$ is indeed the\ncombined monad obtained from $S$ and $T$: it suffices to exhibit what we call a\ndistributing retraction of $ST$ onto $U$. We show that distributing retractions\nand weak distributive laws are in one-to-one correspondence, in a 2-categorical\nsetting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin\nhyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad\nof previsions or of forks, depending on the case. As a byproduct, this allows\nus to describe the algebras of monads of superlinear, resp. sublinear\nprevisions. In the category of compact Hausdorff spaces, the Plotkin hyperspace\nmonad is sometimes known as the Vietoris monad, the monad of probability\nvaluations coincides with the Radon monad, and we infer that the associated\ncombined monad is the monad of normalized forks."}
{"id": "2507.18625", "pdf": "https://arxiv.org/pdf/2507.18625", "abs": "https://arxiv.org/abs/2507.18625", "authors": ["Shuqing Li", "Anson Y. Lam", "Yun Peng", "Wenxuan Wang", "Michael R. Lyu"], "title": "3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "comment": null, "summary": "Graphical user interface (UI) software has undergone a fundamental\ntransformation from traditional two-dimensional (2D) desktop/web/mobile\ninterfaces to spatial three-dimensional (3D) environments. While existing work\nhas made remarkable success in automated 2D software generation, such as\nHTML/CSS and mobile app interface code synthesis, the generation of 3D software\nstill remains under-explored. Current methods for 3D software generation\nusually generate the 3D environments as a whole and cannot modify or control\nspecific elements in the software. Furthermore, these methods struggle to\nhandle the complex spatial and semantic constraints inherent in the real world.\nTo address the challenges, we present Scenethesis, a novel\nrequirement-sensitive 3D software synthesis approach that maintains formal\ntraceability between user specifications and generated 3D software. Scenethesis\nis built upon ScenethesisLang, a domain-specific language that serves as a\ngranular constraint-aware intermediate representation (IR) to bridge natural\nlanguage requirements and executable 3D software. It serves both as a\ncomprehensive scene description language enabling fine-grained modification of\n3D software elements and as a formal constraint-expressive specification\nlanguage capable of expressing complex spatial constraints. By decomposing 3D\nsoftware synthesis into stages operating on ScenethesisLang, Scenethesis\nenables independent verification, targeted modification, and systematic\nconstraint satisfaction. Our evaluation demonstrates that Scenethesis\naccurately captures over 80% of user requirements and satisfies more than 90%\nof hard constraints while handling over 100 constraints simultaneously.\nFurthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual\nevaluation scores compared to the state-of-the-art method."}
{"id": "2507.17865", "pdf": "https://arxiv.org/pdf/2507.17865", "abs": "https://arxiv.org/abs/2507.17865", "authors": ["Alakesh Kalita"], "title": "Talk with the Things: Integrating LLMs into IoT Networks", "categories": ["cs.NI"], "comment": "arXiv admin note: text overlap with arXiv:2407.20970", "summary": "The convergence of Large Language Models (LLMs) and Internet of Things (IoT)\nnetworks open new opportunities for building intelligent, responsive, and\nuser-friendly systems. This work presents an edge-centric framework that\nintegrates LLMs into IoT architectures to enable natural language-based\ncontrol, context-aware decision-making, and enhanced automation. The proposed\nmodular and lightweight Retrieval Augmented Generation (RAG)-based LLMs are\ndeployed on edge computing devices connected to IoT gateways, enabling local\nprocessing of user commands and sensor data for reduced latency, improved\nprivacy, and enhanced inference quality. We validate the framework through a\nsmart home prototype using LLaMA 3 and Gemma 2B models for controlling smart\ndevices. Experimental results highlight the trade-offs between model accuracy\nand inference time with respect to models size. At last, we also discuss the\npotential applications that can use LLM-based IoT systems, and a few key\nchallenges associated with such systems."}
{"id": "2507.18406", "pdf": "https://arxiv.org/pdf/2507.18406", "abs": "https://arxiv.org/abs/2507.18406", "authors": ["Silvia Cappa", "Lingxiao Kong", "Pille-Riin Peet", "Fanfu Wei", "Yuchen Zhou", "Jan-Christoph Kalo"], "title": "Factual Inconsistencies in Multilingual Wikipedia Tables", "categories": ["cs.CL", "cs.DB", "cs.DL", "cs.IR"], "comment": "11 pages, 7 figures, White Paper for RTF Work at ISWS Summer School\n  2025", "summary": "Wikipedia serves as a globally accessible knowledge source with content in\nover 300 languages. Despite covering the same topics, the different versions of\nWikipedia are written and updated independently. This leads to factual\ninconsistencies that can impact the neutrality and reliability of the\nencyclopedia and AI systems, which often rely on Wikipedia as a main training\nsource. This study investigates cross-lingual inconsistencies in Wikipedia's\nstructured content, with a focus on tabular data. We developed a methodology to\ncollect, align, and analyze tables from Wikipedia multilingual articles,\ndefining categories of inconsistency. We apply various quantitative and\nqualitative metrics to assess multilingual alignment using a sample dataset.\nThese insights have implications for factual verification, multilingual\nknowledge interaction, and design for reliable AI systems leveraging Wikipedia\ncontent."}
{"id": "2507.18085", "pdf": "https://arxiv.org/pdf/2507.18085", "abs": "https://arxiv.org/abs/2507.18085", "authors": ["Benjamin Watson", "Neff Walker", "William Ribarsky", "Victoria Spaulding"], "title": "Effects of variation in system responsiveness on user performance in virtual environments", "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "System responsiveness (SR) is defined as the elapsed time until a system\nresponds to user control. SR fluctuates over time, so it must be described\nstatistically with mean (MSR) and standard deviation (SDSR). In this paper, we\nexamine SR in virtual environments (VEs), outlining its components and methods\nof experimental measurement and manipulation. Three studies of MSR and SDSR\neffects on performance of grasp and placement tasks are then presented. The\nstudies used within-subjects designs with 11, 12, and 10 participants,\nrespectively. Results showed that SDSR affected performance only if it was\nabove 82 ms. Placement required more frequent visual feedback and was more\nsensitive to SR. We infer that VE designers need not tightly control SDSR and\nmay wish to vary SR control based on required visual feedback frequency. These\nresults may be used to improve the human-computer interface in a wide range of\ninteractive graphical applications, including scientific visualization,\ntraining, mental health, and entertainment."}
{"id": "2507.17755", "pdf": "https://arxiv.org/pdf/2507.17755", "abs": "https://arxiv.org/abs/2507.17755", "authors": ["Jianfeng Lan", "Yingjia Huang"], "title": "Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image", "categories": ["cs.HC", "cs.CY", "cs.SI"], "comment": null, "summary": "In the digital era, social media platforms play a pivotal role in shaping\nadolescents' body image perceptions. This study examines how Douyin and WeChat,\ntwo contrasting Chinese social media platforms, influence body image among\nChinese male adolescents. Employing a platformization perspective, we surveyed\n395 male adolescents aged 10 to 24 using the Multidimensional Body-Self\nRelations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation\nand body satisfaction. Our findings reveal that Douyin usage is significantly\ncorrelated with appearance evaluation and body area satisfaction, while WeChat\nusage shows no significant correlation with any body image dimensions. These\nresults suggest that Douyin's algorithm-driven, video-centric environment\nintensifies exposure to idealized body standards, impacting users at a\ncognitive level. This study underscores the importance of considering\nplatform-specific characteristics in understanding social media's impact on\nbody image. It contributes to the broader discourse on how technological design\nand content modalities mediate psychological outcomes, offering insights for\naddressing body image concerns among male adolescents in China."}
{"id": "2507.18581", "pdf": "https://arxiv.org/pdf/2507.18581", "abs": "https://arxiv.org/abs/2507.18581", "authors": ["Ravan Nazaraliyev", "Saber Ganjisaffar", "Nurlan Nazaraliyev", "Nael Abu-Ghazaleh"], "title": "PRACtical: Subarray-Level Counter Update and Bank-Level Recovery Isolation for Efficient PRAC Rowhammer Mitigation", "categories": ["cs.AR", "cs.ET"], "comment": null, "summary": "As DRAM density increases, Rowhammer becomes more severe due to heightened\ncharge leakage, reducing the number of activations needed to induce bit flips.\nThe DDR5 standard addresses this threat with in-DRAM per-row activation\ncounters (PRAC) and the Alert Back-Off (ABO) signal to trigger mitigation.\nHowever, PRAC adds performance overhead by incrementing counters during the\nprecharge phase, and recovery refreshes stalls the entire memory channel, even\nif only one bank is under attack.\n  We propose PRACtical, a performance-optimized approach to PRAC+ABO that\nmaintains the same security guarantees. First, we reduce counter update latency\nby introducing a centralized increment circuit, enabling overlap between\ncounter updates and subsequent row activations in other subarrays. Second, we\nenhance the $RFM_{ab}$ mitigation by enabling bank-level granularity: instead\nof stalling the entire channel, only affected banks are paused. This is\nachieved through a DRAM-resident register that identifies attacked banks.\n  PRACtical improves performance by 8% on average (up to 20%) over the\nstate-of-the-art, reduces energy by 19%, and limits performance degradation\nfrom aggressive performance attacks to less than 6%, all while preserving\nRowhammer protection."}
{"id": "2507.17770", "pdf": "https://arxiv.org/pdf/2507.17770", "abs": "https://arxiv.org/abs/2507.17770", "authors": ["Pei-Kun Yang"], "title": "Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale", "categories": ["cs.DC", "quant-ph"], "comment": "14 pages, 5 figures", "summary": "Quadratic Unconstrained Binary Optimization (QUBO) is a versatile framework\nfor modeling combinatorial optimization problems. This study benchmarks five\nsoftware-based QUBO solvers: Neal, PyTorch (CPU), PyTorch (GPU), JAX, and\nSciPy, on randomly generated QUBO matrices ranging from 1000x1000 to\n45000x45000, under six convergence thresholds from 10^-1 to 10^-6. We evaluate\ntheir performance in terms of solution quality (energy) and computational time.\nAmong the solvers tested, Neal achieved the lowest energy values but was\nlimited to problems with up to 6000 variables due to high memory consumption.\nPyTorch produced slightly higher energy results than Neal but demonstrated\nsuperior scalability, solving instances with up to 45000 variables. Its support\nfor GPU acceleration and CPU multi-threading also resulted in significantly\nshorter runtimes. JAX yielded energy values slightly above those of PyTorch and\nwas limited to 25000 variables, with runtimes comparable to PyTorch on GPU.\nSciPy was the most constrained solver, handling only up to 6000 variables and\nconsistently producing the highest energy values with the longest computation\ntimes. These findings highlight trade-offs between solution quality,\nscalability, and runtime efficiency, and suggest that PyTorch is the most\nbalanced choice for large-scale QUBO problems when computational resources\npermit."}
{"id": "2507.18081", "pdf": "https://arxiv.org/pdf/2507.18081", "abs": "https://arxiv.org/abs/2507.18081", "authors": ["Carol Wong", "Mai Abe", "Silvia De Benedictis", "Marissa Halim", "Anthony Peruma"], "title": "Identifier Name Similarities: An Exploratory Study", "categories": ["cs.SE"], "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement - Emerging Results and Vision Track", "summary": "Identifier names, which comprise a significant portion of the codebase, are\nthe cornerstone of effective program comprehension. However, research has shown\nthat poorly chosen names can significantly increase cognitive load and hinder\ncollaboration. Even names that appear readable in isolation may lead to\nmisunderstandings in contexts when they closely resemble other names in either\nstructure or functionality. In this exploratory study, we present our\npreliminary findings on the occurrence of identifier name similarity in\nsoftware projects through the development of a taxonomy that categorizes\ndifferent forms of identifier name similarity. We envision our initial taxonomy\nproviding researchers with a platform to analyze and evaluate the impact of\nidentifier name similarity on code comprehension, maintainability, and\ncollaboration among developers, while also allowing for further refinement and\nexpansion of the taxonomy."}
{"id": "2507.18231", "pdf": "https://arxiv.org/pdf/2507.18231", "abs": "https://arxiv.org/abs/2507.18231", "authors": ["Yixiao Chen", "Bin Liang", "Hanzhi Guo", "Yongqing Cheng", "Jiayi Zhao", "Dongdong Weng"], "title": "PS-GS: Gaussian Splatting for Multi-View Photometric Stereo", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Integrating inverse rendering with multi-view photometric stereo (MVPS)\nyields more accurate 3D reconstructions than the inverse rendering approaches\nthat rely on fixed environment illumination. However, efficient inverse\nrendering with MVPS remains challenging. To fill this gap, we introduce the\nGaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently\nand jointly estimates the geometry, materials, and lighting of the object that\nis illuminated by diverse directional lights (multi-light). Our method first\nreconstructs a standard 2D Gaussian splatting model as the initial geometry.\nBased on the initialization model, it then proceeds with the deferred inverse\nrendering by the full rendering equation containing a lighting-computing\nmulti-layer perceptron. During the whole optimization, we regularize the\nrendered normal maps by the uncalibrated photometric stereo estimated normals.\nWe also propose the 2D Gaussian ray-tracing for single directional light to\nrefine the incident lighting. The regularizations and the use of multi-view and\nmulti-light images mitigate the ill-posed problem of inverse rendering. After\noptimization, the reconstructed object can be used for novel-view synthesis,\nrelighting, and material and shape editing. Experiments on both synthetic and\nreal datasets demonstrate that our method outperforms prior works in terms of\nreconstruction accuracy and computational efficiency."}
{"id": "2507.18539", "pdf": "https://arxiv.org/pdf/2507.18539", "abs": "https://arxiv.org/abs/2507.18539", "authors": ["Henning Urbat", "Thorsten Wißmann"], "title": "Well-Founded Coalgebras Meet König's Lemma", "categories": ["cs.LO"], "comment": null, "summary": "K\\\"onig's lemma is a fundamental result about trees with countless\napplications in mathematics and computer science. In contrapositive form, it\nstates that if a tree is finitely branching and well-founded (i.e. has no\ninfinite paths), then it is finite. We present a coalgebraic version of\nK\\\"onig's lemma featuring two dimensions of generalization: from finitely\nbranching trees to coalgebras for a finitary endofunctor H, and from the base\ncategory of sets to a locally finitely presentable category C, such as the\ncategory of posets, nominal sets, or convex sets. Our coalgebraic K\\\"onig's\nlemma states that, under mild assumptions on C and H, every well-founded\ncoalgebra for H is the directed join of its well-founded subcoalgebras with\nfinitely generated state space -- in particular, the category of well-founded\ncoalgebras is locally presentable. As applications, we derive versions of\nK\\\"onig's lemma for graphs in a topos as well as for nominal and convex\ntransition systems. Additionally, we show that the key construction underlying\nthe proof gives rise to two simple constructions of the initial algebra\n(equivalently, the final recursive coalgebra) for the functor H: The initial\nalgebra is both the colimit of all well-founded and of all recursive coalgebras\nwith finitely presentable state space. Remarkably, this result holds even in\nsettings where well-founded coalgebras form a proper subclass of recursive\nones. The first construction of the initial algebra is entirely new, while for\nthe second one our approach yields a short and transparent new correctness\nproof."}
{"id": "2507.18632", "pdf": "https://arxiv.org/pdf/2507.18632", "abs": "https://arxiv.org/abs/2507.18632", "authors": ["Ye-Chan Kim", "SeungJu Cha", "Si-Woo Kim", "Taewhan Kim", "Dong-Jin Kim"], "title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted to ACM MM 2025", "summary": "Zero-shot domain adaptation is a method for adapting a model to a target\ndomain without utilizing target domain image data. To enable adaptation without\ntarget images, existing studies utilize CLIP's embedding space and text\ndescription to simulate target-like style features. Despite the previous\nachievements in zero-shot domain adaptation, we observe that these text-driven\nmethods struggle to capture complex real-world variations and significantly\nincrease adaptation time due to their alignment process. Instead of relying on\ntext descriptions, we explore solutions leveraging image data, which provides\ndiverse and more fine-grained style cues. In this work, we propose SIDA, a\nnovel and efficient zero-shot domain adaptation method leveraging synthetic\nimages. To generate synthetic images, we first create detailed, source-like\nimages and apply image translation to reflect the style of the target domain.\nWe then utilize the style features of these synthetic images as a proxy for the\ntarget domain. Based on these features, we introduce Domain Mix and Patch Style\nTransfer modules, which enable effective modeling of real-world variations. In\nparticular, Domain Mix blends multiple styles to expand the intra-domain\nrepresentations, and Patch Style Transfer assigns different styles to\nindividual patches. We demonstrate the effectiveness of our method by showing\nstate-of-the-art performance in diverse zero-shot adaptation scenarios,\nparticularly in challenging domains. Moreover, our approach achieves high\nefficiency by significantly reducing the overall adaptation time."}
{"id": "2507.17905", "pdf": "https://arxiv.org/pdf/2507.17905", "abs": "https://arxiv.org/abs/2507.17905", "authors": ["Mahbubur Rahman"], "title": "Enabling Scalability in Asynchronous and Bidirectional Communication in LPWAN", "categories": ["cs.NI", "cs.DC"], "comment": "12 pages", "summary": "LPWANs have become ubiquitous due to their ability to connect sensors over\nlarge geographic areas in a single hop. It is, however, very challenging to\nachieve massive scalability in LPWANs, where numerous sensors can transmit data\nefficiently and with low latency, which emerging IoT and CPS applications may\nrequire. In this paper, we address the above challenges by significantly\nadvancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal\nfrequency division multiplexing, D-OFDM, subcarriers to enable parallel\nreception of data to a BS from multiple asynchronous sensors, each using a\ndifferent subcarrier. In this paper, we achieve massive scalability in SNOW by\nenabling the BS to decode concurrent data from numerous asynchronous sensors on\nthe same subcarrier while parallelly decoding from other subcarriers as well.\nAdditionally, we enable numerous asynchronous sensors to receive distinct data\nfrom the BS on the same subcarrier while other sensors also receive data\nparallelly on other subcarriers. To do this, we develop a set of Gold\ncode-based pseudorandom noise or PN sequences that are mutually non-interfering\nwithin and across the subcarriers. Each sensor uses its PN sequence from the\nset for encoding or decoding data on its subcarriers, enabling massive\nconcurrency. Our evaluation results demonstrate that we can achieve\napproximately 9x more scalability in SNOW while being timely in data collection\nat the BS and energy efficient at the sensors. This may enable emerging IoT and\nCPS applications requiring tens of thousands of sensors with longer battery\nlife and making data-driven, time-sensitive decisions."}
{"id": "2507.18115", "pdf": "https://arxiv.org/pdf/2507.18115", "abs": "https://arxiv.org/abs/2507.18115", "authors": ["Soorya Ram Shimgekar", "Shayan Vassef", "Abhay Goyal", "Navin Kumar", "Koustuv Saha"], "title": "Agentic AI framework for End-to-End Medical Data Inference", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.ET", "cs.LG"], "comment": "10 pages, 5 figures, 2 tables, BIBM conference", "summary": "Building and deploying machine learning solutions in healthcare remains\nexpensive and labor-intensive due to fragmented preprocessing workflows, model\ncompatibility issues, and stringent data privacy constraints. In this work, we\nintroduce an Agentic AI framework that automates the entire clinical data\npipeline, from ingestion to inference, through a system of modular,\ntask-specific agents. These agents handle both structured and unstructured\ndata, enabling automatic feature selection, model selection, and preprocessing\nrecommendation without manual intervention. We evaluate the system on publicly\navailable datasets from geriatrics, palliative care, and colonoscopy imaging.\nFor example, in the case of structured data (anxiety data) and unstructured\ndata (colonoscopy polyps data), the pipeline begins with file-type detection by\nthe Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring\nprivacy compliance, where we first identify the data type and then anonymize\nit. The Feature Extraction Agent identifies features using an embedding-based\napproach for tabular data, extracting all column names, and a multi-stage\nMedGemma-based approach for image data, which infers modality and disease name.\nThese features guide the Model-Data Feature Matcher Agent in selecting the\nbest-fit model from a curated repository. The Preprocessing Recommender Agent\nand Preprocessing Implementor Agent then apply tailored preprocessing based on\ndata type and model requirements. Finally, the ``Model Inference Agent\" runs\nthe selected model on the uploaded data and generates interpretable outputs\nusing tools like SHAP, LIME, and DETR attention maps. By automating these\nhigh-friction stages of the ML lifecycle, the proposed framework reduces the\nneed for repeated expert intervention, offering a scalable, cost-efficient\npathway for operationalizing AI in clinical environments."}
{"id": "2507.17756", "pdf": "https://arxiv.org/pdf/2507.17756", "abs": "https://arxiv.org/abs/2507.17756", "authors": ["Josh Hunter", "John McDermid", "Simon Burton"], "title": "Insights from Railway Professionals: Rethinking Railway assumptions regarding safety and autonomy", "categories": ["cs.HC", "cs.AI"], "comment": "9 pages, 3 figures, published in European Dependable Computing\n  Conference 2025", "summary": "This study investigates how railway professionals perceive safety as a\nconcept within rail, with the intention to help inform future technological\ndevelopments within the industry. Through a series of interviews with drivers,\nroute planners,and administrative personnel, the research explores the\ncurrentstate of safety practices, the potential for automation and the\nunderstanding of the railway as a system of systems. Key findings highlight a\ncautious attitude towards automation, a preference for assistive technologies,\nand a complex understanding of safety that integrates human, systematic and\ntechnological factors. The study also addresses the limitations of transferring\nautomotive automation technologies to railways and the need for a\nrailway-specific causation model to better evaluate and enhance safety in an\nevolving technological landscape. This study aims to bridge thegap between\ncontemporary research and practical applications, contributing to the\ndevelopment of more effective safety metrics."}
{"id": "2507.17886", "pdf": "https://arxiv.org/pdf/2507.17886", "abs": "https://arxiv.org/abs/2507.17886", "authors": ["James B Aimone"], "title": "Neuromorphic Computing: A Theoretical Framework for Time, Space, and Energy Scaling", "categories": ["cs.NE", "cs.AR", "cs.DC"], "comment": "True pre-print; to be submitted at future date", "summary": "Neuromorphic computing (NMC) is increasingly viewed as a low-power\nalternative to conventional von Neumann architectures such as central\nprocessing units (CPUs) and graphics processing units (GPUs), however the\ncomputational value proposition has been difficult to define precisely.\n  Here, we explain how NMC should be seen as general-purpose and programmable\neven though it differs considerably from a conventional stored-program\narchitecture. We show that the time and space scaling of NMC is equivalent to\nthat of a theoretically infinite processor conventional system, however the\nenergy scaling is significantly different. Specifically, the energy of\nconventional systems scales with absolute algorithm work, whereas the energy of\nneuromorphic systems scales with the derivative of algorithm state. The unique\ncharacteristics of NMC architectures make it well suited for different classes\nof algorithms than conventional multi-core systems like GPUs that have been\noptimized for dense numerical applications such as linear algebra. In contrast,\nthe unique characteristics of NMC make it ideally suited for scalable and\nsparse algorithms whose activity is proportional to an objective function, such\nas iterative optimization and large-scale sampling (e.g., Monte Carlo)."}
{"id": "2507.17771", "pdf": "https://arxiv.org/pdf/2507.17771", "abs": "https://arxiv.org/abs/2507.17771", "authors": ["Dmitri Lyalikov"], "title": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN Inference Acceleration", "categories": ["cs.DC", "eess.IV"], "comment": null, "summary": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms."}
{"id": "2507.18105", "pdf": "https://arxiv.org/pdf/2507.18105", "abs": "https://arxiv.org/abs/2507.18105", "authors": ["Yujie Ma", "Lili Quan", "Xiaofei Xie", "Qiang Hu", "Jiongchi Yu", "Yao Zhang", "Sen Chen"], "title": "Understanding the Supply Chain and Risks of Large Language Model Applications", "categories": ["cs.SE", "cs.CR"], "comment": "26 pages", "summary": "The rise of Large Language Models (LLMs) has led to the widespread deployment\nof LLM-based systems across diverse domains. As these systems proliferate,\nunderstanding the risks associated with their complex supply chains is\nincreasingly important. LLM-based systems are not standalone as they rely on\ninterconnected supply chains involving pretrained models, third-party\nlibraries, datasets, and infrastructure. Yet, most risk assessments narrowly\nfocus on model or data level, overlooking broader supply chain vulnerabilities.\nWhile recent studies have begun to address LLM supply chain risks, there\nremains a lack of benchmarks for systematic research.\n  To address this gap, we introduce the first comprehensive dataset for\nanalyzing and benchmarking LLM supply chain security. We collect 3,859\nreal-world LLM applications and perform interdependency analysis, identifying\n109,211 models, 2,474 datasets, and 9,862 libraries. We extract model\nfine-tuning paths, dataset reuse, and library reliance, mapping the ecosystem's\nstructure. To evaluate security, we gather 1,555 risk-related issues-50 for\napplications, 325 for models, 18 for datasets, and 1,229 for libraries from\npublic vulnerability databases.\n  Using this dataset, we empirically analyze component dependencies and risks.\nOur findings reveal deeply nested dependencies in LLM applications and\nsignificant vulnerabilities across the supply chain, underscoring the need for\ncomprehensive security analysis. We conclude with practical recommendations to\nguide researchers and developers toward safer, more trustworthy LLM-enabled\nsystems."}
{"id": "2507.18352", "pdf": "https://arxiv.org/pdf/2507.18352", "abs": "https://arxiv.org/abs/2507.18352", "authors": ["Zhen Han", "Mattias Teye", "Derek Yadgaroff", "Judith Bütepage"], "title": "Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation", "categories": ["cs.GR", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal\n  track)", "summary": "The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters."}
{"id": "2507.18567", "pdf": "https://arxiv.org/pdf/2507.18567", "abs": "https://arxiv.org/abs/2507.18567", "authors": ["Ruben Gamboa", "Panagiotis Manolios"], "title": "Proceedings 19th International Workshop on the ACL2 Theorem Prover and Its Applications", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "The ACL2 Workshop series is the major technical forum for users of the ACL2\ntheorem proving system to present research related to the ACL2 theorem prover\nand its applications. ACL2 is an industrial-strength automated reasoning\nsystem, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM\nSoftware System Award was awarded to Boyer, Kaufmann, and Moore for their work\non ACL2 and the other theorem provers in the Boyer-Moore family."}
{"id": "2507.18328", "pdf": "https://arxiv.org/pdf/2507.18328", "abs": "https://arxiv.org/abs/2507.18328", "authors": ["Xiao Xu", "Qiong Wu", "Pingyi Fan", "Kezhi Wang", "Nan Cheng", "Wen Chen", "Khaled B. Letaief"], "title": "Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of Information Optimization in Vehicular Networks", "categories": ["cs.NI"], "comment": "This paper has been submitted to IEEE TMC", "summary": "In this paper, we consider the fair access problem and the Age of Information\n(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in\nvehicular networks. Specifically, vehicles follow Mode 2 to communicate with\nRoadside Units (RSUs) to obtain accurate data for driving\nassistance.Nevertheless, vehicles often have different velocity when they are\nmoving in adjacent lanes, leading to difference in RSU dwelltime and\ncommunication duration. This results in unfair access to network resources,\npotentially influencing driving safety. To ensure the freshness of received\ndata, the AoI should be analyzed. Mode 2 introduces a novel preemption\nmechanism, necessitating simultaneous optimization of fair access and AoI to\nguarantee timely and relevant data delivery. We propose a joint optimization\nframework for vehicular network, defining a fairness index and employing\nStochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By\nadaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)\nin Mode 2, we address the optimization of fairness and AoI. We apply a large\nlanguage model (LLM)-Based Multi-objective Evolutionary Algorithm Based on\nDecomposition (MOEA/D) to solve this problem. Simulation results demonstrate\nthe effectiveness of our scheme in balancing fair access and minimizing AoI."}
{"id": "2507.18581", "pdf": "https://arxiv.org/pdf/2507.18581", "abs": "https://arxiv.org/abs/2507.18581", "authors": ["Ravan Nazaraliyev", "Saber Ganjisaffar", "Nurlan Nazaraliyev", "Nael Abu-Ghazaleh"], "title": "PRACtical: Subarray-Level Counter Update and Bank-Level Recovery Isolation for Efficient PRAC Rowhammer Mitigation", "categories": ["cs.AR", "cs.ET"], "comment": null, "summary": "As DRAM density increases, Rowhammer becomes more severe due to heightened\ncharge leakage, reducing the number of activations needed to induce bit flips.\nThe DDR5 standard addresses this threat with in-DRAM per-row activation\ncounters (PRAC) and the Alert Back-Off (ABO) signal to trigger mitigation.\nHowever, PRAC adds performance overhead by incrementing counters during the\nprecharge phase, and recovery refreshes stalls the entire memory channel, even\nif only one bank is under attack.\n  We propose PRACtical, a performance-optimized approach to PRAC+ABO that\nmaintains the same security guarantees. First, we reduce counter update latency\nby introducing a centralized increment circuit, enabling overlap between\ncounter updates and subsequent row activations in other subarrays. Second, we\nenhance the $RFM_{ab}$ mitigation by enabling bank-level granularity: instead\nof stalling the entire channel, only affected banks are paused. This is\nachieved through a DRAM-resident register that identifies attacked banks.\n  PRACtical improves performance by 8% on average (up to 20%) over the\nstate-of-the-art, reduces energy by 19%, and limits performance degradation\nfrom aggressive performance attacks to less than 6%, all while preserving\nRowhammer protection."}
{"id": "2507.17757", "pdf": "https://arxiv.org/pdf/2507.17757", "abs": "https://arxiv.org/abs/2507.17757", "authors": ["Sam Gordon James", "Miranda Elaine Glynis Armstrong", "Aisling Ann O'Kane", "Harry Emerson", "Zahraa S. Abdallah"], "title": "BrisT1D Dataset: Young Adults with Type 1 Diabetes in the UK using Smartwatches", "categories": ["cs.HC", "cs.LG"], "comment": "13 pages, 14 figures", "summary": "Background: Type 1 diabetes (T1D) has seen a rapid evolution in management\ntechnology and forms a useful case study for the future management of other\nchronic conditions. Further development of this management technology requires\nan exploration of its real-world use and the potential of additional data\nstreams. To facilitate this, we contribute the BrisT1D Dataset to the growing\nnumber of public T1D management datasets. The dataset was developed from a\nlongitudinal study of 24 young adults in the UK who used a smartwatch alongside\ntheir usual T1D management. Findings: The BrisT1D dataset features both device\ndata from the T1D management systems and smartwatches used by participants, as\nwell as transcripts of monthly interviews and focus groups conducted during the\nstudy. The device data is provided in a processed state, for usability and more\nrapid analysis, and in a raw state, for in-depth exploration of novel insights\ncaptured in the study. Conclusions: This dataset has a range of potential\napplications. The quantitative elements can support blood glucose prediction,\nhypoglycaemia prediction, and closed-loop algorithm development. The\nqualitative elements enable the exploration of user experiences and opinions,\nas well as broader mixed-methods research into the role of smartwatches in T1D\nmanagement."}
{"id": "2507.18174", "pdf": "https://arxiv.org/pdf/2507.18174", "abs": "https://arxiv.org/abs/2507.18174", "authors": ["Rashed Al Amin", "Roman Obermaisser"], "title": "Real-Time Object Detection and Classification using YOLO for Edge FPGAs", "categories": ["cs.CV", "cs.AR"], "comment": "This paper has been accepted for the 67th International Symposium on\n  ELMAR 2025", "summary": "Object detection and classification are crucial tasks across various\napplication domains, particularly in the development of safe and reliable\nAdvanced Driver Assistance Systems (ADAS). Existing deep learning-based methods\nsuch as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and\nYou Only Look Once (YOLO) have demonstrated high performance in terms of\naccuracy and computational speed when deployed on Field-Programmable Gate\nArrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based\nobject detection and classification systems continue to face challenges in\nachieving resource efficiency suitable for edge FPGA platforms. To address this\nlimitation, this paper presents a resource-efficient real-time object detection\nand classification system based on YOLOv5 optimized for FPGA deployment. The\nproposed system is trained on the COCO and GTSRD datasets and implemented on\nthe Xilinx Kria KV260 FPGA board. Experimental results demonstrate a\nclassification accuracy of 99%, with a power consumption of 3.5W and a\nprocessing speed of 9 frames per second (FPS). These findings highlight the\neffectiveness of the proposed approach in enabling real-time,\nresource-efficient object detection and classification for edge computing\napplications."}
{"id": "2507.17772", "pdf": "https://arxiv.org/pdf/2507.17772", "abs": "https://arxiv.org/abs/2507.17772", "authors": ["Ahmad Alhonainy", "Praveen Rao"], "title": "Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments", "categories": ["cs.DC", "cs.AI", "cs.CV", "cs.LG"], "comment": "Journal", "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications."}
{"id": "2507.18130", "pdf": "https://arxiv.org/pdf/2507.18130", "abs": "https://arxiv.org/abs/2507.18130", "authors": ["Le Deng", "Zhonghao Jiang", "Jialun Cao", "Michael Pradel", "Zhongxin Liu"], "title": "NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition", "categories": ["cs.SE"], "comment": null, "summary": "Natural language-driven no-code development allows users to specify software\nfunctionality using natural language (NL) instead of editing source code,\npromising increased productivity and democratized development. Large language\nmodels (LLMs) show potential in enabling this paradigm. In this context,\nsoftware documentation acts as an NL specification for functionality. This work\nintroduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world\nNL-driven feature addition tasks, consisting of 634 tasks across 10 projects\nand 114k code changes. Each task pairs documentation updates with corresponding\ncode implementations, validated by developer-written test cases. A subset of\n114 high-quality, human-verified instances, NoCode-bench Verified, ensures\nreliable evaluation. Our experiments reveal that, despite high token usage, the\nbest LLMs achieve a task success rate of only 15.79%, highlighting challenges\nin cross-file editing, codebase understanding, and tool calling. These findings\nindicate that LLMs are not yet ready for fully NL-driven no-code development.\nNoCode-bench lays the foundation for future advances in this area."}
{"id": "2507.17931", "pdf": "https://arxiv.org/pdf/2507.17931", "abs": "https://arxiv.org/abs/2507.17931", "authors": ["Pascal Debus", "Sebastian Issel", "Kilian Tscharke"], "title": "Quantum Machine Learning Playground", "categories": ["quant-ph", "cs.GR", "cs.LG"], "comment": "Accepted to IEEE Computer Graphics and Applications. Final version:\n  https://doi.org/10.1109/MCG.2024.3456288", "summary": "This article introduces an innovative interactive visualization tool designed\nto demystify quantum machine learning (QML) algorithms. Our work is inspired by\nthe success of classical machine learning visualization tools, such as\nTensorFlow Playground, and aims to bridge the gap in visualization resources\nspecifically for the field of QML. The article includes a comprehensive\noverview of relevant visualization metaphors from both quantum computing and\nclassical machine learning, the development of an algorithm visualization\nconcept, and the design of a concrete implementation as an interactive web\napplication. By combining common visualization metaphors for the so-called data\nre-uploading universal quantum classifier as a representative QML model, this\narticle aims to lower the entry barrier to quantum computing and encourage\nfurther innovation in the field. The accompanying interactive application is a\nproposal for the first version of a quantum machine learning playground for\nlearning and exploring QML models."}
{"id": "2507.18612", "pdf": "https://arxiv.org/pdf/2507.18612", "abs": "https://arxiv.org/abs/2507.18612", "authors": ["Arijit Shaw", "Kuldeep S. Meel"], "title": "Approximate SMT Counting Beyond Discrete Domains", "categories": ["cs.LO", "cs.AI"], "comment": "To be published in the proceedings of Design Automation Conference\n  (DAC) 2025", "summary": "Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning,\nsolving complex formulas across discrete and continuous domains. Recent\nprogress in propositional model counting motivates extending SMT capabilities\ntoward model counting, especially for hybrid SMT formulas. Existing approaches,\nlike bit-blasting, are limited to discrete variables, highlighting the\nchallenge of counting solutions projected onto the discrete domain in hybrid\nformulas.\n  We introduce pact, an SMT model counter for hybrid formulas that uses\nhashing-based approximate model counting to estimate solutions with theoretical\nguarantees. pact makes a logarithmic number of SMT solver calls relative to the\nprojection variables, leveraging optimized hash functions. pact achieves\nsignificant performance improvements over baselines on a large suite of\nbenchmarks. In particular, out of 14,202 instances, pact successfully finished\non 603 instances, while Baseline could only finish on 13 instances."}
{"id": "2507.18480", "pdf": "https://arxiv.org/pdf/2507.18480", "abs": "https://arxiv.org/abs/2507.18480", "authors": ["David Nunez", "Francesc Wilhelmi", "Lorenzo Galati-Giordano", "Giovanni Geraci", "Boris Bellalta"], "title": "Improving Wi-Fi 8 Latency with Coordinated Spatial Reuse", "categories": ["cs.NI"], "comment": "Submitted to IEEE Communications Standards Magazine", "summary": "IEEE 802.11 networks continuously adapt to meet the stringent requirements of\nemerging applications like cloud gaming, eXtended Reality (XR), and video\nstreaming services, which require high throughput, low latency, and high\nreliability. To address these challenges, Coordinated Spatial Reuse (Co-SR) can\npotentially contribute to optimizing spectrum resource utilization. This\nmechanism is expected to enable simultaneous transmissions, thereby boosting\nspectral efficiency in dense environments and increasing the overall network\nperformance. In this paper, we shed light on the performance of Co-SR for Wi-Fi\n8 networks. For that, we propose an implementation of Co-SR aligned with\nongoing Wi-Fi 8 standardization efforts. The evaluation is done on a Wi-Fi\nsimulator, which allows us to study the performance of the proposed Co-SR\nmechanisms in relevant scenarios. The results obtained in a Wireless Local Area\nNetwork (WLAN) consisting of four APs show delay reduction with Co-SR ranging\nfrom 31% to 95% when compared to Distributed Coordination Function (DCF)."}
{"id": "2507.17759", "pdf": "https://arxiv.org/pdf/2507.17759", "abs": "https://arxiv.org/abs/2507.17759", "authors": ["Riddhi Heda", "Sidhant Singh", "Umair Yasir", "Tanmay Jaiswal", "Anil Mokhade"], "title": "DHMS: A Digital Hostel Management System Integrating Campus ChatBot, Predictive Intelligence, and Real-Time Automation", "categories": ["cs.HC"], "comment": null, "summary": "Traditional hostel management practices in academic institutions often suffer\nfrom inefficiencies, delays, and fragmented communication. These systems fail\nto meet the expectations of digitally native students and place a significant\noperational burden on hostel staff. This paper introduces DHMS (Digital Hostel\nManagement System), a modular and integrated platform designed to digitize and\nstreamline essential hostel management functions. DHMS leverages modern web\ntechnologies, artificial intelligence, and cloud infrastructure to automate\nroom allotment, grievance redressal, gate pass logistics, and communication via\na natural language chatbot. In simulation tests, DHMS achieved a 92% student\nsatisfaction rate in room allocation and maintained an average chatbot response\ntime below one second. Additional features include predictive analytics for\nproactive maintenance planning and sentiment analysis for feedback processing.\nWhile promising, the system requires further testing for integration across\nmultiple hostel blocks, user acceptance, scalability under load, and ERP\ncompatibility before campus-wide deployment. This work discusses the system\narchitecture, implementation approach, and factors critical to improving user\nexperience, administrative efficiency, and decision-making processes."}
{"id": "2507.18179", "pdf": "https://arxiv.org/pdf/2507.18179", "abs": "https://arxiv.org/abs/2507.18179", "authors": ["Felix Arnold", "Maxence Bouvier", "Ryan Amaudruz", "Renzo Andri", "Lukas Cavigelli"], "title": "Explicit Sign-Magnitude Encoders Enable Power-Efficient Multipliers", "categories": ["cs.NE", "cs.AR", "cs.PF"], "comment": "Accepted and presented at the 34th International Workshop on Logic &\n  Synthesis June 2025", "summary": "This work presents a method to maximize power-efficiency of fixed point\nmultiplier units by decomposing them into sub-components. First, an encoder\nblock converts the operands from a two's complement to a sign magnitude\nrepresentation, followed by a multiplier module which performs the compute\noperation and outputs the resulting value in the original format. This allows\nto leverage the power-efficiency of the Sign Magnitude encoding for the\nmultiplication. To ensure the computing format is not altered, those two\ncomponents are synthesized and optimized separately. Our method leads to\nsignificant power savings for input values centered around zero, as commonly\nencountered in AI workloads. Under a realistic input stream with values\nnormally distributed with a standard deviation of 3.0, post-synthesis\nsimulations of the 4-bit multiplier design show up to 12.9% lower switching\nactivity compared to synthesis without decomposition. Those gains are achieved\nwhile ensuring compliance into any production-ready system as the overall\ncircuit stays logic-equivalent. With the compliance lifted and a slightly\nsmaller input range of -7 to +7, switching activity reductions can reach up to\n33%. Additionally, we demonstrate that synthesis optimization methods based on\nswitching-activity-driven design space exploration can yield a further 5-10%\nimprovement in power-efficiency compared to a power agnostic approach."}
{"id": "2507.17773", "pdf": "https://arxiv.org/pdf/2507.17773", "abs": "https://arxiv.org/abs/2507.17773", "authors": ["Zhongzhen Wen", "Yinghui Zhang", "Zhong Li", "Zhongxin Liu", "Linna Xie", "Tian Zhang"], "title": "MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation", "categories": ["cs.DC", "cs.LG", "cs.PF", "cs.SE"], "comment": null, "summary": "The automatic generation of deep learning (DL) kernels using large language\nmodels (LLMs) has emerged as a promising approach to reduce the manual effort\nand hardware-specific expertise required for writing high-performance operator\nimplementations. However, existing benchmarks for evaluating LLMs in this\ndomain suffer from limited hardware support, coarse-grained kernel\ncategorization, and imbalanced task coverage. To address these limitations, we\nintroduce MultiKernelBench, the first comprehensive, multi-platform benchmark\nfor LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14\nwell-defined kernel categories and supports three major hardware platforms:\nNvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we\ndesign a modular backend abstraction layer that decouples platform-specific\nlogic from the core benchmarking infrastructure, allowing easy integration of\nnew hardware platforms. We further propose a simple yet effective\ncategory-aware one-shot prompting method that improves generation quality by\nproviding in-category exemplars. Through systematic evaluations of seven\nstate-of-the-art LLMs, we reveal significant variation in task difficulty, poor\ngeneralization to platforms with less training exposure, and the effectiveness\nof targeted prompting strategies. MultiKernelBench is publicly available at\nhttps://github.com/wzzll123/MultiKernelBench."}
{"id": "2507.18159", "pdf": "https://arxiv.org/pdf/2507.18159", "abs": "https://arxiv.org/abs/2507.18159", "authors": ["Stephan Ferenz", "Aida Jafarbigloo", "Oliver Werth", "Astrid Nieße"], "title": "SMECS: A Software Metadata Extraction and Curation Software", "categories": ["cs.SE", "cs.DL"], "comment": null, "summary": "Metadata play a crucial role in adopting the FAIR principles for research\nsoftware and enables findability and reusability. However, creating\nhigh-quality metadata can be resource-intensive for researchers and research\nsoftware engineers. To address this challenge, we developed the Software\nMetadata Extraction and Curation Software (SMECS) which integrates the\nextraction of metadata from existing sources together with a user-friendly\ninterface for metadata curation. SMECS extracts metadata from online\nrepositories such as GitHub and presents it to researchers through an\ninteractive interface for further curation and export as a CodeMeta file. The\nusability of SMECS was evaluated through usability experiments which confirmed\nthat SMECS provides a satisfactory user experience. SMECS supports the\nFAIRification of research software by simplifying metadata creation."}
{"id": "2507.18460", "pdf": "https://arxiv.org/pdf/2507.18460", "abs": "https://arxiv.org/abs/2507.18460", "authors": ["Jonathan Panuelos", "Eitan Grinspun", "David Levin"], "title": "Topology-Preserving Coupling of Compressible Fluids and Thin Deformables", "categories": ["physics.comp-ph", "cs.GR", "physics.flu-dyn"], "comment": null, "summary": "We present a novel discretization of coupled compressible fluid and thin\ndeformable structures that provides sufficient and necessary leakproofness by\npreserving the path connectedness of the fluid domain. Our method employs a\nconstrained Voronoi-based spatial partitioning combined with Godunov-style\nfinite-volume time integration. The fluid domain is discretized into cells that\nconform exactly to the fluid-solid interface, allowing boundary conditions to\nbe sharply resolved exactly at the interface. This enables direct force\nexchange between the fluid and solid while ensuring that no fluid leaks through\nthe solid, even when arbitrarily thin. We validate our approach on a series of\nchallenging scenarios -- including a balloon propelled by internal compressed\nair, a champagne cork ejecting after overcoming friction, and a supersonic\nasteroid -- demonstrating bidirectional energy transfer between fluid and\nsolid."}
{"id": "2507.17956", "pdf": "https://arxiv.org/pdf/2507.17956", "abs": "https://arxiv.org/abs/2507.17956", "authors": ["Russell O'Connor", "Andrew Poelstra"], "title": "Formal Verification of the Safegcd Implementation", "categories": ["cs.CR", "cs.LO"], "comment": "15 pages; Coq sources can be found at\n  https://github.com/BlockstreamResearch/simplicity/tree/c1dddedd553b403da877377e658f17f0d2184cc4/Coq/C/secp256k1\n  ; Alectryon preview can be viewed at e.g.\n  https://html-preview.github.io/?url=https://github.com/BlockstreamResearch/simplicity/blob/c1dddedd553b403da877377e658f17f0d2184cc4/alectryon/verif_modinv64_impl.v.html", "summary": "The modular inverse is an essential piece of computation required for\nelliptic curve operations used for digital signatures in Bitcoin and other\napplications. A novel approach to the extended Euclidean algorithm has been\ndeveloped by Bernstein and Yang within the last few years and incorporated into\nthe libsecp256k1 cryptographic library used by Bitcoin. However, novel\nalgorithms introduce new risks of errors. To address this we have completed a\ncomputer verified proof of the correctness of (one of) libsecp256k1's modular\ninverse implementations with the Coq proof assistant using the Verifiable C's\nimplementation of separation logic."}
{"id": "2507.18514", "pdf": "https://arxiv.org/pdf/2507.18514", "abs": "https://arxiv.org/abs/2507.18514", "authors": ["Jiping Luo", "Nikolaos Pappas"], "title": "On the Role of Age and Semantics of Information in Remote Estimation of Markov Sources", "categories": ["cs.IT", "cs.NI", "cs.SY", "eess.SY", "math.IT"], "comment": "Submitted for possible journal publication. A shorter version has\n  been accepted as invited paper at Asilomar 2025", "summary": "This paper investigates the semantics-aware remote estimation of a\nfinite-state Markov chain. We employ the maximum a posteriori (MAP) estimator\nand aim to devise a transmission policy to optimize estimation performance\nsubject to a transmission frequency constraint. We leverage two metrics, namely\nthe Age of Consecutive Error (AoCE) and the Age of Information (AoI), to\nquantify, respectively, the significance of estimation error at the transmitter\nand the predictability of outdated information at the receiver. The optimal\ntransmission problem is formulated as a constrained Markov decision process\n(CMDP) with unbounded costs. We show the existence of an optimal simple mixture\npolicy, which randomly selects between two deterministic switching policies\nwith a fixed probability. Notably, each switching policy triggers a\ntransmission only when the AoCE exceeds a threshold value that depends on both\nthe AoI and the instantaneous estimation error. We further derive sufficient\nconditions under which the switching policy reduces to a simple threshold\npolicy; that is, it admits identical thresholds for all estimation errors.\nLeveraging these results, we develop an efficient structure-aware algorithm,\nInsec-SPI, that computes the optimal policy with reduced computation overhead.\nOur results demonstrate that incorporating both AoI and AoCE yields\nsignificantly improved estimation quality compared to using either metric\nalone."}
{"id": "2507.17761", "pdf": "https://arxiv.org/pdf/2507.17761", "abs": "https://arxiv.org/abs/2507.17761", "authors": ["Jan-Christoph Kalo", "Fina Polat", "Shubha Guha", "Paul Groth"], "title": "Co-constructing Explanations for AI Systems using Provenance", "categories": ["cs.HC"], "comment": "5 pages", "summary": "Modern AI systems are complex workflows containing multiple components and\ndata sources. Data provenance provides the ability to interrogate and\npotentially explain the outputs of these systems. However, provenance is often\ntoo detailed and not contextualized for the user trying to understand the AI\nsystem. In this work, we present our vision for an interactive agent that works\ntogether with the user to co-construct an explanation that is simultaneously\nuseful to the user as well as grounded in data provenance. To illustrate this\nvision, we present: 1) an initial prototype of such an agent; and 2) a scalable\nevaluation framework based on user simulations and a large language model as a\njudge approach."}
{"id": "2507.17793", "pdf": "https://arxiv.org/pdf/2507.17793", "abs": "https://arxiv.org/abs/2507.17793", "authors": ["Joel Brogan", "Matthew Yohe", "David Cornett"], "title": "CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks", "categories": ["cs.DC"], "comment": null, "summary": "What if you could piece together your own custom biometrics and AI analysis\nsystem, a bit like LEGO blocks? We aim to bring that technology to field\noperators in the field who require flexible, high-performance edge AI system\nthat can be adapted on a moment's notice. This paper introduces CHAMP\n(Configurable Hot-swappable Architecture for Machine Perception), a modular\nedge computing platform that allows operators to dynamically swap in\nspecialized AI \"capability cartridges\" for tasks like face recognition, object\ntracking, and document analysis. CHAMP leverages low-power FPGA-based\naccelerators on a high-throughput bus, orchestrated by a custom operating\nsystem (VDiSK) to enable plug-and-play AI pipelines and cryptographically\nsecured biometric datasets. In this paper we describe the CHAMP design,\nincluding its modular scaling with multiple accelerators and the VDiSK\noperating system for runtime reconfiguration, along with its cryptographic\ncapabilities to keep data stored on modules safe and private. Experiments\ndemonstrate near-linear throughput scaling from 1 to 5 neural compute\naccelerators, highlighting both the performance gains and saturation limits of\nthe USB3-based bus. Finally, we discuss applications of CHAMP in field\nbiometrics, surveillance, and disaster response, and outline future\nimprovements in bus protocols, cartridge capabilities, and system software."}
{"id": "2507.18223", "pdf": "https://arxiv.org/pdf/2507.18223", "abs": "https://arxiv.org/abs/2507.18223", "authors": ["Nenad Petrovic", "Fengjunjie Pan", "Vahid Zolfaghari", "Krzysztof Lebioda", "Andre Schamschurko", "Alois Knoll"], "title": "GenAI for Automotive Software Development: From Requirements to Wheels", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper introduces a GenAI-empowered approach to automated development of\nautomotive software, with emphasis on autonomous and Advanced Driver Assistance\nSystems (ADAS) capabilities. The process starts with requirements as input,\nwhile the main generated outputs are test scenario code for simulation\nenvironment, together with implementation of desired ADAS capabilities\ntargeting hardware platform of the vehicle connected to testbench. Moreover, we\nintroduce additional steps for requirements consistency checking leveraging\nModel-Driven Engineering (MDE). In the proposed workflow, Large Language Models\n(LLMs) are used for model-based summarization of requirements (Ecore metamodel,\nXMI model instance and OCL constraint creation), test scenario generation,\nsimulation code (Python) and target platform code generation (C++).\nAdditionally, Retrieval Augmented Generation (RAG) is adopted to enhance test\nscenario generation from autonomous driving regulations-related documents. Our\napproach aims shorter compliance and re-engineering cycles, as well as reduced\ndevelopment and testing time when it comes to ADAS-related capabilities."}
{"id": "2507.18145", "pdf": "https://arxiv.org/pdf/2507.18145", "abs": "https://arxiv.org/abs/2507.18145", "authors": ["Moritz Schönherr", "Carsten Lutz"], "title": "Logical Characterizations of GNNs with Mean Aggregation", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "We study the expressive power of graph neural networks (GNNs) with mean as\nthe aggregation function. In the non-uniform setting, we show that such GNNs\nhave exactly the same expressive power as ratio modal logic, which has modal\noperators expressing that at least a certain ratio of the successors of a\nvertex satisfies a specified property. The non-uniform expressive power of mean\nGNNs is thus higher than that of GNNs with max aggregation, but lower than for\nsum aggregation--the latter are characterized by modal logic and graded modal\nlogic, respectively. In the uniform setting, we show that the expressive power\nrelative to MSO is exactly that of alternation-free modal logic, under the\nnatural assumptions that combination functions are continuous and\nclassification functions are thresholds. This implies that, relative to MSO and\nin the uniform setting, mean GNNs are strictly less expressive than sum GNNs\nand max GNNs. When any of the assumptions is dropped, the expressive power\nincreases."}
{"id": "2507.17774", "pdf": "https://arxiv.org/pdf/2507.17774", "abs": "https://arxiv.org/abs/2507.17774", "authors": ["Zhangqi Liu"], "title": "Human-AI Co-Creation: A Framework for Collaborative Design in Intelligent Systems", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As artificial intelligence (AI) continues to evolve from a back-end\ncomputational tool into an interactive, generative collaborator, its\nintegration into early-stage design processes demands a rethinking of\ntraditional workflows in human-centered design. This paper explores the\nemergent paradigm of human-AI co-creation, where AI is not merely used for\nautomation or efficiency gains, but actively participates in ideation, visual\nconceptualization, and decision-making. Specifically, we investigate the use of\nlarge language models (LLMs) like GPT-4 and multimodal diffusion models such as\nStable Diffusion as creative agents that engage designers in iterative cycles\nof proposal, critique, and revision."}
{"id": "2507.17843", "pdf": "https://arxiv.org/pdf/2507.17843", "abs": "https://arxiv.org/abs/2507.17843", "authors": ["Bruno Marques da Silva", "Larissa Ferreira Rodrigues Moreira", "Flávio de Oliveira Silva", "Rodrigo Moreira"], "title": "Optimizing Edge Gaming Slices through an Enhanced User Plane Function and Analytics in Beyond-5G Networks", "categories": ["cs.DC"], "comment": null, "summary": "The latest generation of games and pervasive communication technologies poses\nchallenges in service management and Service-Level Agreement compliance for\nmobile users. State-of-the-art edge-gaming techniques enhance throughput,\nreduce latency, and leverage cloud computing. However, further development of\ncore functions such as the User Plane Function (UPF) is needed for\nnon-intrusive user latency measurement. This paper proposes a closed-loop\narchitecture integrating the Network Data Analytics Function (NWDAF) and UPF to\nestimate user latency and enhance the 5G control plane by making it\nlatency-aware. The results show that embedding an artificial intelligence model\nwithin NWDAF enables game classification and opens new avenues for mobile edge\ngaming research."}
{"id": "2507.18267", "pdf": "https://arxiv.org/pdf/2507.18267", "abs": "https://arxiv.org/abs/2507.18267", "authors": ["Zeqin Liao", "Zibin Zheng", "Peifan Reng", "Henglong Liang", "Zixu Gao", "Zhixiang Chen", "Wei Li", "Yuhong Nan"], "title": "An Empirical Study on Embodied Artificial Intelligence Robot (EAIR) Software Bugs", "categories": ["cs.SE"], "comment": null, "summary": "Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly\nevolving technological domain. Ensuring their program correctness is\nfundamental to their successful deployment. However, a general and in-depth\nunderstanding of EAIR system bugs remains lacking, which hinders the\ndevelopment of practices and techniques to tackle EAIR system bugs.\n  To bridge this gap, we conducted the first systematic study of 885 EAIR\nsystem bugs collected from 80 EAIR system projects to investigate their\nsymptoms, underlying causes, and module distribution. Our analysis takes\nconsiderable effort, which classifies these bugs into 18 underlying causes, 15\ndistinct symptoms, and identifies 13 affected modules. It reveals several new\ninteresting findings and implications which help shed light on future research\non tackling or repairing EAIR system bugs. First, among the 15 identified\nsymptoms, our findings highlight 8 symptoms specific to EAIR systems, which is\ncharacterized by severe functional failures and potential physical hazards.\nSecond, within the 18 underlying causes, we define 8 EAIR-specific causes, the\nmajority of which stem from the intricate issues of AI- agent reasoning and\ndecision making. Finally, to facilitate precise and efficient bug prediction,\ndetection, and repair, we constructed a mapping between underlying causes and\nthe modules in which they most frequently occur, which enables researchers to\nfocus diagnostic efforts on the modules most susceptible to specific bug types."}
{"id": "2507.18375", "pdf": "https://arxiv.org/pdf/2507.18375", "abs": "https://arxiv.org/abs/2507.18375", "authors": ["Guillermo Badia", "Manfred Droste", "Thomas Eiter", "Rafael Kiesel", "Carles Noguera", "Erik Paul"], "title": "Fagin's Theorem for Semiring Turing Machines", "categories": ["cs.CC", "cs.LO"], "comment": null, "summary": "In recent years, quantitative complexity over semirings has been intensively\ninvestigated. An important problem in this context is to connect computational\ncomplexity with logical expressiveness. In this paper we improve on the model\nof \\emph{Semiring Turing Machines} (distinct from so called weighted Turing\nmachines) introduced by Eiter \\& Kiesel (Semiring Reasoning Frameworks in AI\nand Their Computational Complexity, \\emph{J. Artif. Intell. Res.}, 2023). Our\ncentral result is a Fagin-style theorem for a new quantitative complexity class\nusing a suitable weighted logical formalism. We show that the quantitative\ncomplexity class that we call \\NPnewinf{$\\mathcal{R}$}, where $\\mathcal{R}$ is\na commutative semiring, can be captured using a version of weighted existential\nsecond-order logic that allows for predicates interpreted as semiring-annotated\nrelations. This result provides a precise logical characterization of the power\nseries that form the class \\NPnewinf{$\\mathcal{R}$}. We also give the exact\nrelation between Eiter \\& Kiesel's version of NP, called\n\\NPoldinf{$\\mathcal{R}$}, and the class \\NPnewinf{$\\mathcal{R}$}. Incidentally,\nwe are able to recapture all the complexity results by Eiter \\& Kiesel (2023)\nin our new model, connecting a quantitative version of NP to various counting\ncomplexity classes."}
{"id": "2507.17898", "pdf": "https://arxiv.org/pdf/2507.17898", "abs": "https://arxiv.org/abs/2507.17898", "authors": ["Connor Scully-Allison", "Kevin Menear", "Kristin Potter", "Andrew McNutt", "Katherine E. Isaacs", "Dmitry Duplyakin"], "title": "Same Data, Different Audiences: Using Personas to Scope a Supercomputing Job Queue Visualization", "categories": ["cs.HC"], "comment": "11 Pages, 4 figures", "summary": "Domain-specific visualizations sometimes focus on narrow, albeit important,\ntasks for one group of users. This focus limits the utility of a visualization\nto other groups working with the same data. While tasks elicited from other\ngroups can present a design pitfall if not disambiguated, they also present a\ndesign opportunity -- development of visualizations that support multiple\ngroups. This development choice presents a trade off of broadening the scope\nbut limiting support for the more narrow tasks of any one group, which in some\ncases can enhance the overall utility of the visualization. We investigate this\nscenario through a design study where we develop \\textit{Guidepost}, a\nnotebook-embedded visualization of supercomputer queue data that helps\nscientists assess supercomputer queue wait times, machine learning researchers\nunderstand prediction accuracy, and system maintainers analyze usage trends. We\nadapt the use of personas for visualization design from existing literature in\nthe HCI and software engineering domains and apply them in categorizing tasks\nbased on their uniqueness across the stakeholder personas. Under this model,\ntasks shared between all groups should be supported by interactive\nvisualizations and tasks unique to each group can be deferred to scripting with\nnotebook-embedded visualization design. We evaluate our visualization with nine\nexpert analysts organized into two groups: a \"research analyst\" group that uses\nsupercomputer queue data in their research (representing the Machine Learning\nresearchers and Jobs Data Analyst personas) and a \"supercomputer user\" group\nthat uses this data conditionally (representing the HPC User persona). We find\nthat our visualization serves our three stakeholder groups by enabling users to\nsuccessfully execute shared tasks with point-and-click interaction while\nfacilitating case-specific programmatic analysis workflows."}
{"id": "2507.17904", "pdf": "https://arxiv.org/pdf/2507.17904", "abs": "https://arxiv.org/abs/2507.17904", "authors": ["Talha Mehboob", "Luanzheng Guo", "Nathan Tallent", "Michael Zink", "David Irwin"], "title": "PowerTrip: Exploiting Federated Heterogeneous Datacenter Power for Distributed ML Training", "categories": ["cs.DC"], "comment": null, "summary": "The exponential growth of large-scale AI models has led to computational and\npower demands that can exceed the capacity of a single data center. This is due\nto the limited power supplied by regional grids that leads to limited regional\ncomputational power. Consequently, distributing training workloads across\ngeographically distributed sites has become essential. However, this approach\nintroduces a significant challenge in the form of communication overhead,\ncreating a fundamental trade-off between the performance gains from accessing\ngreater aggregate power and the performance losses from increased network\nlatency. Although prior work has focused on reducing communication volume or\nusing heuristics for distribution, these methods assume constant homogeneous\npower supplies and ignore the challenge of heterogeneous power availability\nbetween sites.\n  To address the challenge of training large models in power-constrained,\ngeo-distributed environments, we introduce PowerTrip, a system that dynamically\nselects a subset of sites during runtime to optimize the power-communication\ntrade-off. Specifically, PowerTrip selects sites based on a power-to-cost\nheuristic, prioritizing those with high power availability and low network\nlatency. PowerTrip employs a dynamic greedy approach and uses the marginal gain\nin training efficiency, i.e., accuracy improvement per unit of time, to\noptimize for the number of sites where the performance penalty from network\noverhead negates the benefit of adding more computational power. Our\nevaluation, which uses real-world Google power traces to model realistic power\ncapacity constraints, demonstrates that PowerTrip can reduce time-to-accuracy\nby up to 50% compared to existing baseline policies."}
{"id": "2507.18289", "pdf": "https://arxiv.org/pdf/2507.18289", "abs": "https://arxiv.org/abs/2507.18289", "authors": ["Yan Li", "Wenzhang Yang", "Yuekun Wang", "Jian Gao", "Shaohua Wang", "Yinxing Xue", "Lijun Zhang"], "title": "Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling", "categories": ["cs.SE", "cs.CR"], "comment": "15 pages, 12 figures, 5 tables", "summary": "Fuzzing a library requires experts to understand the library usage well and\ncraft high-quality fuzz drivers, which is tricky and tedious. Therefore, many\ntechniques have been proposed to automatically generate fuzz drivers. However,\nthey fail to generate rational fuzz drivers due to the lack of adherence to\nproper library usage conventions, such as ensuring a resource is closed after\nbeing opened. To make things worse, existing library fuzzing techniques\nunconditionally execute each driver, resulting in numerous irrational drivers\nthat waste computational resources while contributing little coverage and\ngenerating false positive bug reports.\n  To tackle these challenges, we propose a novel automatic library fuzzing\ntechnique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs\nto understand rational usage of libraries and extract API combination\nconstraints. To optimize computational resource utilization, a dual scheduling\nframework is implemented to efficiently manage API combinations and fuzz\ndrivers. The framework models driver generation and the corresponding fuzzing\ncampaign as an online optimization problem. Within the scheduling loop,\nmultiple API combinations are selected to generate fuzz drivers, while\nsimultaneously, various optimized fuzz drivers are scheduled for execution or\nsuspension.\n  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared\nto baseline approaches, Scheduzz significantly reduces computational overhead\nand outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and\n1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,\nPromptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,\nScheduzz discovered 33 previously unknown bugs in these well-tested libraries,\n3 of which have been assigned CVEs."}
{"id": "2507.17943", "pdf": "https://arxiv.org/pdf/2507.17943", "abs": "https://arxiv.org/abs/2507.17943", "authors": ["Shu-Yuan Liu", "Johan Engström", "Gustav Markkula"], "title": "Automated Brake Onset Detection in Naturalistic Driving Data", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Response timing measures play a crucial role in the assessment of automated\ndriving systems (ADS) in collision avoidance scenarios, including but not\nlimited to establishing human benchmarks and comparing ADS to human driver\nresponse performance. For example, measuring the response time (of a human\ndriver or ADS) to a conflict requires the determination of a stimulus onset and\na response onset. In existing studies, response onset relies on manual\nannotation or vehicle control signals such as accelerator and brake pedal\nmovements. These methods are not applicable when analyzing large scale data\nwhere vehicle control signals are not available. This holds in particular for\nthe rapidly expanding sets of ADS log data where the behavior of surrounding\nroad users is observed via onboard sensors. To advance evaluation techniques\nfor ADS and enable measuring response timing when vehicle control signals are\nnot available, we developed a simple and efficient algorithm, based on a\npiecewise linear acceleration model, to automatically estimate brake onset that\ncan be applied to any type of driving data that includes vehicle longitudinal\ntime series data. We also proposed a manual annotation method to identify brake\nonset and used it as ground truth for validation. R2 was used as a confidence\nmetric to measure the accuracy of the algorithm, and its classification\nperformance was analyzed using naturalistic collision avoidance data of both\nADS and humans, where our method was validated against human manual annotation.\nAlthough our algorithm is subject to certain limitations, it is efficient,\ngeneralizable, applicable to any road user and scenario types, and is highly\nconfigurable."}
{"id": "2507.18005", "pdf": "https://arxiv.org/pdf/2507.18005", "abs": "https://arxiv.org/abs/2507.18005", "authors": ["Shengye Song", "Minxian Xu", "Zuowei Zhang", "Chengxi Gao", "Fansong Zeng", "Yu Ding", "Kejiang Ye", "Chengzhong Xu"], "title": "C-Koordinator: Interference-aware Management for Large-scale and Co-located Microservice Clusters", "categories": ["cs.DC"], "comment": "15 pages", "summary": "Microservices transform traditional monolithic applications into lightweight,\nloosely coupled application components and have been widely adopted in many\nenterprises. Cloud platform infrastructure providers enhance the resource\nutilization efficiency of microservices systems by co-locating different\nmicroservices. However, this approach also introduces resource competition and\ninterference among microservices. Designing interference-aware strategies for\nlarge-scale, co-located microservice clusters is crucial for enhancing resource\nutilization and mitigating competition-induced interference. These challenges\nare further exacerbated by unreliable metrics, application diversity, and node\nheterogeneity.\n  In this paper, we first analyze the characteristics of large-scale and\nco-located microservices clusters at Alibaba and further discuss why cycle per\ninstruction (CPI) is adopted as a metric for interference measurement in\nlarge-scale production clusters, as well as how to achieve accurate prediction\nof CPI through multi-dimensional metrics. Based on CPI interference prediction\nand analysis, we also present the design of the C-Koordinator platform, an\nopen-source solution utilized in Alibaba cluster, which incorporates\nco-location and interference mitigation strategies. The interference prediction\nmodels consistently achieve over 90.3% accuracy, enabling precise prediction\nand rapid mitigation of interference in operational environments. As a result,\napplication latency is reduced and stabilized across all percentiles (P50, P90,\nP99) response time (RT), achieving improvements ranging from 16.7% to 36.1%\nunder various system loads compared with state-of-the-art system. These results\ndemonstrate the system's ability to maintain smooth application performance in\nco-located environments."}
{"id": "2507.18316", "pdf": "https://arxiv.org/pdf/2507.18316", "abs": "https://arxiv.org/abs/2507.18316", "authors": ["Michael Konstantinou", "Renzo Degiovanni", "Jie M. Zhang", "Mark Harman", "Mike Papadakis"], "title": "YATE: The Role of Test Repair in LLM-Based Unit Test Generation", "categories": ["cs.SE"], "comment": "12 pages, 4 figures", "summary": "Recent advances in automated test generation utilises language models to\nproduce unit tests. While effective, language models tend to generate many\nincorrect tests with respect to both syntax and semantics. Although such\nincorrect tests can be easily detected and discarded, they constitute a \"missed\nopportunity\" -- if fixed, they are often valuable as they directly add testing\nvalue (they effectively target the underlying program logic to be tested) and\nindirectly form good seeds for generating additional tests. To this end, we\npropose a simple technique for repairing some of these incorrect tests through\na combination of rule-based static analysis and re-prompting. We evaluate this\nsimple approach, named YATE, on a set of 6 open-source projects and show that\nit can effectively produce tests that cover on average 32.06% more lines and\nkill 21.77% more mutants than a plain LLM-based method. We also compare YATE\nwith four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and\nCOVERUP and show that it produces tests that cover substantially more code.\nYATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%\nmore mutants at a comparable cost (number of calls to LLMs)."}
{"id": "2507.17985", "pdf": "https://arxiv.org/pdf/2507.17985", "abs": "https://arxiv.org/abs/2507.17985", "authors": ["Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Victor Tian", "Zachary Zhang", "Kevin He", "Min Sun"], "title": "Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The integration of large language models (LLMs) into educational tools has\nthe potential to substantially impact how teachers plan instruction, support\ndiverse learners, and engage in professional reflection. Yet little is known\nabout how educators actually use these tools in practice and how their\ninteractions with AI can be meaningfully studied at scale. This paper presents\na human-AI collaborative methodology for large-scale qualitative analysis of\nover 140,000 educator-AI messages drawn from a generative AI platform used by\nK-12 teachers. Through a four-phase coding pipeline, we combined inductive\ntheme discovery, codebook development, structured annotation, and model\nbenchmarking to examine patterns of educator engagement and evaluate the\nperformance of LLMs in qualitative coding tasks. We developed a hierarchical\ncodebook aligned with established teacher evaluation frameworks, capturing\neducators' instructional goals, contextual needs, and pedagogical strategies.\nOur findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably\nsupport theme identification, extend human recognition in complex scenarios,\nand outperform open-weight models in both accuracy and structural reliability.\nThe analysis also reveals substantive patterns in how educators inquire AI to\nenhance instructional practices (79.7 percent of total conversations), create\nor adapt content (76.1 percent), support assessment and feedback loop (46.9\npercent), attend to student needs for tailored instruction (43.3 percent), and\nassist other professional responsibilities (34.2 percent), highlighting\nemerging AI-related competencies that have direct implications for teacher\npreparation and professional development. This study offers a scalable,\ntransparent model for AI-augmented qualitative research and provides\nfoundational insights into the evolving role of generative AI in educational\npractice."}
{"id": "2507.18006", "pdf": "https://arxiv.org/pdf/2507.18006", "abs": "https://arxiv.org/abs/2507.18006", "authors": ["Jingfeng Wu", "Yiyuan He", "Minxian Xu", "Xitong Gao", "Kejiang Ye", "Chengzhong Xu"], "title": "Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling", "categories": ["cs.DC"], "comment": "15 pages", "summary": "The rise of large language models (LLMs) has created new opportunities across\nvarious fields but has also introduced significant challenges in resource\nmanagement. Current LLM serving systems face a fundamental tension: balancing\nserving demands with limited resources while adapting to unpredictable traffic\npatterns. Static deployments lead to suboptimal resource utilization and\nperformance degradation under dynamic workloads. Furthermore, the high cost of\nadjusting instances hinders dynamic scaling, limiting the true potential of\nefficient LLM serving.\n  To address this, we propose CoCoServe, an elastic system that facilitates\ndynamic and fine-grained scaling. Its key innovation lies in the module-level\noperations for the replication and migration of LLM modules, such as decoder\nlayers and projections. Through a comprehensive analysis of the trade-offs\nassociated with these operations, we develop an auto-scaling mechanism that\ndynamically regulates module-level resource allocation and performance\noptimization, enabling a more cost-effective deployment of LLMs. Our evaluation\ndemonstrates that the scaling operations employed by CoCoServe exhibit\nexcellent scalability and can reduce costs by 46% while maintaining\navailability. Compared to state-of-the-art LLM serving systems (e.g., Hugging\nFace Transformers and vLLM), our approach reduces latency by 14%-75% and\nachieves 1.16x-4x throughput on average across different model sizes and\nworkloads."}
{"id": "2507.18319", "pdf": "https://arxiv.org/pdf/2507.18319", "abs": "https://arxiv.org/abs/2507.18319", "authors": ["Jesse Maarleveld", "Jiapan Guo", "Daniel Feitosa"], "title": "Gotta catch 'em all! Towards File Localisation from Issues at Large", "categories": ["cs.SE"], "comment": "12 pages, 6 figures", "summary": "Bug localisation, the study of developing methods to localise the files\nrequiring changes to resolve bugs, has been researched for a long time to\ndevelop methods capable of saving developers' time. Recently, researchers are\nstarting to consider issues outside of bugs. Nevertheless, most existing\nresearch into file localisation from issues focusses on bugs or uses other\nselection methods to ensure only certain types of issues are considered as part\nof the focus of the work. Our goal is to work on all issues at large, without\nany specific selection.\n  In this work, we provide a data pipeline for the creation of issue file\nlocalisation datasets, capable of dealing with arbitrary branching and merging\npractices. We provide a baseline performance evaluation for the file\nlocalisation problem using traditional information retrieval approaches.\nFinally, we use statistical analysis to investigate the influence of biases\nknown in the bug localisation community on our dataset.\n  Our results show that methods designed using bug-specific heuristics perform\npoorly on general issue types, indicating a need for research into general\npurpose models. Furthermore, we find that there are small, but statistically\nsignificant differences in performance between different issue types. Finally,\nwe find that the presence of identifiers have a small effect on performance for\nmost issue types. Many results are project-dependent, encouraging the\ndevelopment of methods which can be tuned to project-specific characteristics."}
{"id": "2507.17997", "pdf": "https://arxiv.org/pdf/2507.17997", "abs": "https://arxiv.org/abs/2507.17997", "authors": ["Yayan Zhao", "Matthew Berger"], "title": "Evaluating judgment of spatial correlation in visual displays of scalar field distributions", "categories": ["cs.HC"], "comment": null, "summary": "In this work we study the identification of spatial correlation in\ndistributions of 2D scalar fields, presented across different forms of visual\ndisplays. We study simple visual displays that directly show color-mapped\nscalar fields, namely those drawn from a distribution, and whether humans can\nidentify strongly correlated spatial regions in these displays. In this\nsetting, the recognition of correlation requires making judgments on a set of\nfields, rather than just one field. Thus, in our experimental design we compare\ntwo basic visualization designs: animation-based displays against juxtaposed\nviews of scalar fields, along different choices of color scales. Moreover, we\ninvestigate the impacts of the distribution itself, controlling for the level\nof spatial correlation and discriminability in spatial scales. Our study's\nresults illustrate the impacts of these distribution characteristics, while\nalso highlighting how different visual displays impact the types of judgments\nmade in assessing spatial correlation. Supplemental material is available at\nhttps://osf.io/zn4qy"}
{"id": "2507.18007", "pdf": "https://arxiv.org/pdf/2507.18007", "abs": "https://arxiv.org/abs/2507.18007", "authors": ["Minxian Xu", "Junhan Liao", "Jingfeng Wu", "Yiyuan He", "Kejiang Ye", "Chengzhong Xu"], "title": "Cloud Native System for LLM Inference Serving", "categories": ["cs.DC"], "comment": "10 pages", "summary": "Large Language Models (LLMs) are revolutionizing numerous industries, but\ntheir substantial computational demands create challenges for efficient\ndeployment, particularly in cloud environments. Traditional approaches to\ninference serving often struggle with resource inefficiencies, leading to high\noperational costs, latency issues, and limited scalability. This article\nexplores how Cloud Native technologies, such as containerization,\nmicroservices, and dynamic scheduling, can fundamentally improve LLM inference\nserving. By leveraging these technologies, we demonstrate how a Cloud Native\nsystem enables more efficient resource allocation, reduces latency, and\nenhances throughput in high-demand scenarios. Through real-world evaluations\nusing Kubernetes-based autoscaling, we show that Cloud Native architectures can\ndynamically adapt to workload fluctuations, mitigating performance bottlenecks\nwhile optimizing LLM inference serving performance. This discussion provides a\nbroader perspective on how Cloud Native frameworks could reshape the future of\nscalable LLM inference serving, offering key insights for researchers,\npractitioners, and industry leaders in cloud computing and artificial\nintelligence."}
{"id": "2507.18339", "pdf": "https://arxiv.org/pdf/2507.18339", "abs": "https://arxiv.org/abs/2507.18339", "authors": ["Nils Bosbach", "Meik Schmidt", "Lukas Jünger", "Matthias Berthold", "Rainer Leupers"], "title": "FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping", "categories": ["cs.SE", "cs.DC"], "comment": "PREPRINT - accepted by the 16th International Modelica and FMI\n  Conference 2025", "summary": "As systems become more complex, the demand for thorough testing and virtual\nprototyping grows. To simulate whole systems, multiple tools are usually needed\nto cover different parts. These parts include the hardware of a system and the\nenvironment with which the system interacts. The Functional Mock-up Interface\n(FMI) standard for co-simulation can be used to connect these tools.\n  The control part of modern systems is usually a computing unit, such as a\nSystem-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software\nfrom a connected memory and interacts with peripherals. To develop software\nwithout requiring access to physical hardware, full-system simulators, the\nso-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized\nframework for VP development is SystemC TLM. SystemC provides interfaces and\nconcepts that enable modular design and model exchange. However, SystemC lacks\nnative FMI support, which limits the integration into broader co-simulation\nenvironments.\n  This paper presents a novel framework to control and interact with\nSystemC-based VPs using the FMI. We present a case study showing how a\nsimulated temperature sensor in a SystemC simulation can obtain temperature\nvalues from an external tool via FMI. This approach allows the unmodified\ntarget software to run on the VP and receive realistic environmental input data\nsuch as temperature, velocity, or acceleration values from other tools. Thus,\nextensive software testing and verification is enabled. By having tests ready\nand the software pre-tested using a VP once the physical hardware is available,\ncertifications like ISO 26262 can be done earlier."}
{"id": "2507.18084", "pdf": "https://arxiv.org/pdf/2507.18084", "abs": "https://arxiv.org/abs/2507.18084", "authors": ["Nisha Devasia", "Georgia Kenderova", "Michele Newman", "Julie Kientz", "Jin Ha Lee"], "title": "\"I Would Not Be This Version of Myself Today\": Elaborating on the Effects of Eudaimonic Gaming Experiences", "categories": ["cs.HC"], "comment": "Accepted to CHI PLAY 2025", "summary": "While much of the research in digital games has emphasized hedonic\nexperiences, such as flow, enjoyment, and positive affect, recent years have\nseen increased interest in eudaimonic gaming experiences, typically\nmixed-affect and associated with personal meaningfulness and growth. The\nformation of such experiences in games is theorized to have four constituent\nelements: motivation, game use, experience, and effects. However, while the\nfirst three elements have been relatively well explored in the literature, the\neffects - and how they may influence positive individual outcomes - have been\nunderexplored thus far. To this end, in this work, we investigate the perceived\noutcomes of eudaimonic gaming and how different components of the experience\ninfluence these effects. We conducted a survey (n = 166) in which respondents\nrecounted meaningful gaming experiences and how they affected their present\nlives. We used a mixed-methods approach to classify effects and identify\nsignificant subcomponents of their formation. We contribute an empirical\nunderstanding of how meaningful gaming experiences can lead to positive\nreflective, learning, social, health, and career effects, extending current\ntheoretical models of eudaimonic gaming experiences and offering implications\nfor how researchers and practitioners might use these findings to promote\npositive outcomes for players."}
{"id": "2507.18047", "pdf": "https://arxiv.org/pdf/2507.18047", "abs": "https://arxiv.org/abs/2507.18047", "authors": ["Lucas Liebe", "Thanh-Tung Nguyen", "Dongman Lee"], "title": "FCPO: Federated Continual Policy Optimization for Real-Time High-Throughput Edge Video Analytics", "categories": ["cs.DC"], "comment": "13 pages, 14 figures, 2 tables", "summary": "The growing complexity of Edge Video Analytics (EVA) facilitates new kind of\nintelligent applications, but creates challenges in real-time inference serving\nsystems. State-of-the-art (SOTA) scheduling systems optimize global workload\ndistributions for heterogeneous devices but often suffer from extended\nscheduling cycles, leading to sub-optimal processing in rapidly changing Edge\nenvironments. Local Reinforcement Learning (RL) enables quick adjustments\nbetween cycles but faces scalability, knowledge integration, and adaptability\nissues. Thus, we propose FCPO, which combines Continual RL (CRL) with Federated\nRL (FRL) to address these challenges. This integration dynamically adjusts\ninference batch sizes, input resolutions, and multi-threading during pre- and\npost-processing. CRL allows agents to learn from changing Markov Decision\nProcesses, capturing dynamic environmental variations, while FRL improves\ngeneralization and convergence speed by integrating experiences across\ninference models. FCPO combines these via an agent-specific aggregation scheme\nand a diversity-aware experience buffer. Experiments on a real-world EVA\ntestbed showed over 5 times improvement in effective throughput, 60% reduced\nlatency, and 20% faster convergence with up to 10 times less memory consumption\ncompared to SOTA RL-based approaches."}
{"id": "2507.18476", "pdf": "https://arxiv.org/pdf/2507.18476", "abs": "https://arxiv.org/abs/2507.18476", "authors": ["Busra Icoz", "Goksel Biricik"], "title": "Automated Code Review Using Large Language Models with Symbolic Reasoning", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review."}
{"id": "2507.18085", "pdf": "https://arxiv.org/pdf/2507.18085", "abs": "https://arxiv.org/abs/2507.18085", "authors": ["Benjamin Watson", "Neff Walker", "William Ribarsky", "Victoria Spaulding"], "title": "Effects of variation in system responsiveness on user performance in virtual environments", "categories": ["cs.HC", "cs.ET"], "comment": null, "summary": "System responsiveness (SR) is defined as the elapsed time until a system\nresponds to user control. SR fluctuates over time, so it must be described\nstatistically with mean (MSR) and standard deviation (SDSR). In this paper, we\nexamine SR in virtual environments (VEs), outlining its components and methods\nof experimental measurement and manipulation. Three studies of MSR and SDSR\neffects on performance of grasp and placement tasks are then presented. The\nstudies used within-subjects designs with 11, 12, and 10 participants,\nrespectively. Results showed that SDSR affected performance only if it was\nabove 82 ms. Placement required more frequent visual feedback and was more\nsensitive to SR. We infer that VE designers need not tightly control SDSR and\nmay wish to vary SR control based on required visual feedback frequency. These\nresults may be used to improve the human-computer interface in a wide range of\ninteractive graphical applications, including scientific visualization,\ntraining, mental health, and entertainment."}
{"id": "2507.18050", "pdf": "https://arxiv.org/pdf/2507.18050", "abs": "https://arxiv.org/abs/2507.18050", "authors": ["Xiaoning Jia", "Ruilin Kong", "Guangya Si", "Bilong Shen", "Zhe Ji"], "title": "A large-scale distributed parallel discrete event simulation engines based on Warped2 for Wargaming simulation", "categories": ["cs.DC"], "comment": null, "summary": "Rising demand for complex simulations highlights conventional\nengines'scalability limits, spurring Parallel Discrete Event Simulation (PDES)\nadoption.Warped2, a PDES engine leveraging Time Warp synchronization with\nPending Event Set optimization, delivers strong performance, it struggles with\ninherent wargaming limitations: inefficient LP resource allocation during\nsynchronization and unaddressed complex entity interaction patterns. To address\nthese challenges, we present an optimized framework featuring four synergistic\nimprovements: (1) Asynchronous listener threads are introduced to address event\nmonitoring latency in large-scale scenarios, instead of synchronous polling\nmechanisms, (2) METIS-based load rebalancing strategy is incorporated to\naddress the issue of dynamic event allocation during real-world simulation, (3)\nEntity interaction solver with constraint satisfaction mechanisms is designed\nto mitigate state conflicts, and (4) Spatial hashing algorithm to overcome\nO(n^2) complexity bottlenecks in large-scale nearest-neighbor searches.\nExperimental validation through a GridWorld demo demonstrates significant\nenhancements in temporal fidelity and computational efficiency. Benchmark\nresults show our framework achieves 16x acceleration over baseline\nimplementations and maintains 8x speedup over 1-thread configuration across MPI\nand Pthreads implementations.The combined load balancing and LP migration\nstrategy reduces synchronization overhead by 58.18%, with load balancing\naccounting for 57% of the total improvement as the dominant optimization\nfactor. These improvements provide an enhanced solution for PDES implementation\nin large-scale simulation scenarios."}
{"id": "2507.18515", "pdf": "https://arxiv.org/pdf/2507.18515", "abs": "https://arxiv.org/abs/2507.18515", "authors": ["Zezhou Yang", "Ting Peng", "Cuiyun Gao", "Chaozheng Wang", "Hailiang Huang", "Yuetang Deng"], "title": "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat", "categories": ["cs.SE"], "comment": "Accepted in ICSME 25 Industry Track", "summary": "Code completion, a crucial task in software engineering that enhances\ndeveloper productivity, has seen substantial improvements with the rapid\nadvancement of large language models (LLMs). In recent years,\nretrieval-augmented generation (RAG) has emerged as a promising method to\nenhance the code completion capabilities of LLMs, which leverages relevant\ncontext from codebases without requiring model retraining. While existing\nstudies have demonstrated the effectiveness of RAG on public repositories and\nbenchmarks, the potential distribution shift between open-source and\nclosed-source codebases presents unique challenges that remain unexplored. To\nmitigate the gap, we conduct an empirical study to investigate the performance\nof widely-used RAG methods for code completion in the industrial-scale codebase\nof WeChat, one of the largest proprietary software systems. Specifically, we\nextensively explore two main types of RAG methods, namely identifier-based RAG\nand similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\nparameters. For a more comprehensive analysis, we employ different retrieval\ntechniques for similarity-based RAG, including lexical and semantic retrieval.\nBased on 1,669 internal repositories, we achieve several key findings: (1) both\nRAG methods demonstrate effectiveness in closed-source repositories, with\nsimilarity-based RAG showing superior performance, (2) the effectiveness of\nsimilarity-based RAG improves with more advanced retrieval techniques, where\nBM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\nperformance, and (3) the combination of lexical and semantic retrieval\ntechniques yields optimal results, demonstrating complementary strengths.\nFurthermore, we conduct a developer survey to validate the practical utility of\nRAG methods in real-world development environments."}
{"id": "2507.18151", "pdf": "https://arxiv.org/pdf/2507.18151", "abs": "https://arxiv.org/abs/2507.18151", "authors": ["Shizhen Zhang", "Shengxin Li", "Quan Li"], "title": "Understood: Real-Time Communication Support for Adults with ADHD Using Mixed Reality", "categories": ["cs.HC"], "comment": "Appear UIST2025", "summary": "Adults with Attention Deficit Hyperactivity Disorder (ADHD) often experience\ncommunication challenges, primarily due to executive dysfunction and emotional\ndysregulation, even after years of social integration. While existing\ninterventions predominantly target children through structured or intrusive\nmethods, adults lack tools that translate clinical strategies into daily\ncommunication support. To address this gap, we present Understood, a Mixed\nReality (MR) system implemented on Microsoft HoloLens 2, designed to assist\nadults with ADHD in real-world communication. Through formative semi-structured\ninterviews and a design workshop, we identified critical communication barriers\nand derived design goals for the system. Understood combines three key\nfeatures: (1) real-time conversation summarization to reduce cognitive load,\n(2) context-aware subsequent word suggestions during moments of disfluency, and\n(3) topic shifting detection and reminding to mitigate off-topic transitions. A\nwithin-subjects user study and expert interviews demonstrate that Understood\neffectively supports communication with high usability, offering a complement\nto therapist-mediated interventions."}
{"id": "2507.18459", "pdf": "https://arxiv.org/pdf/2507.18459", "abs": "https://arxiv.org/abs/2507.18459", "authors": ["Amir Najjar", "Riad Mokadem", "Jean-Marc Pierson"], "title": "Towards Designing an Energy Aware Data Replication Strategy for Cloud Systems Using Reinforcement Learning", "categories": ["cs.DC"], "comment": null, "summary": "The rapid growth of global data volumes has created a demand for scalable\ndistributed systems that can maintain a high quality of service. Data\nreplication is a widely used technique that provides fault tolerance, improved\nperformance and higher availability. Traditional implementations often rely on\nthreshold-based activation mechanisms, which can vary depending on workload\nchanges and system architecture. System administrators typically bear the\nresponsibility of adjusting these thresholds. To address this challenge,\nreinforcement learning can be used to dynamically adapt to workload changes and\ndifferent architectures. In this paper, we propose a novel data replication\nstrategy for cloud systems that employs reinforcement learning to automatically\nlearn system characteristics and adapt to workload changes. The strategy's aim\nis to provide satisfactory Quality of Service while optimizing a trade-off\nbetween provider profit and environmental impact. We present the architecture\nbehind our solution and describe the reinforcement learning model by defining\nthe states, actions and rewards."}
{"id": "2507.17773", "pdf": "https://arxiv.org/pdf/2507.17773", "abs": "https://arxiv.org/abs/2507.17773", "authors": ["Zhongzhen Wen", "Yinghui Zhang", "Zhong Li", "Zhongxin Liu", "Linna Xie", "Tian Zhang"], "title": "MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation", "categories": ["cs.DC", "cs.LG", "cs.PF", "cs.SE"], "comment": null, "summary": "The automatic generation of deep learning (DL) kernels using large language\nmodels (LLMs) has emerged as a promising approach to reduce the manual effort\nand hardware-specific expertise required for writing high-performance operator\nimplementations. However, existing benchmarks for evaluating LLMs in this\ndomain suffer from limited hardware support, coarse-grained kernel\ncategorization, and imbalanced task coverage. To address these limitations, we\nintroduce MultiKernelBench, the first comprehensive, multi-platform benchmark\nfor LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14\nwell-defined kernel categories and supports three major hardware platforms:\nNvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we\ndesign a modular backend abstraction layer that decouples platform-specific\nlogic from the core benchmarking infrastructure, allowing easy integration of\nnew hardware platforms. We further propose a simple yet effective\ncategory-aware one-shot prompting method that improves generation quality by\nproviding in-category exemplars. Through systematic evaluations of seven\nstate-of-the-art LLMs, we reveal significant variation in task difficulty, poor\ngeneralization to platforms with less training exposure, and the effectiveness\nof targeted prompting strategies. MultiKernelBench is publicly available at\nhttps://github.com/wzzll123/MultiKernelBench."}
{"id": "2507.18165", "pdf": "https://arxiv.org/pdf/2507.18165", "abs": "https://arxiv.org/abs/2507.18165", "authors": ["Yuheng Zhao", "Xueli Shu", "Liwen Fan", "Lin Gao", "Yu Zhang", "Siming Chen"], "title": "ProactiveVA: Proactive Visual Analytics with LLM-Based UI Agent", "categories": ["cs.HC"], "comment": "11 pages, 8 figures", "summary": "Visual analytics (VA) is typically applied to complex data, thus requiring\ncomplex tools. While visual analytics empowers analysts in data analysis,\nanalysts may get lost in the complexity occasionally. This highlights the need\nfor intelligent assistance mechanisms. However, even the latest LLM-assisted VA\nsystems only provide help when explicitly requested by the user, making them\ninsufficiently intelligent to offer suggestions when analysts need them the\nmost. We propose a ProactiveVA framework in which LLM-powered UI agent monitors\nuser interactions and delivers context-aware assistance proactively. To design\neffective proactive assistance, we first conducted a formative study analyzing\nhelp-seeking behaviors in user interaction logs, identifying when users need\nproactive help, what assistance they require, and how the agent should\nintervene. Based on this analysis, we distilled key design requirements in\nterms of intent recognition, solution generation, interpretability and\ncontrollability. Guided by these requirements, we develop a three-stage UI\nagent pipeline including perception, reasoning, and acting. The agent\nautonomously perceives users' needs from VA interaction logs, providing\ntailored suggestions and intuitive guidance through interactive exploration of\nthe system. We implemented the framework in two representative types of VA\nsystems, demonstrating its generalizability, and evaluated the effectiveness\nthrough an algorithm evaluation, case and expert study and a user study. We\nalso discuss current design trade-offs of proactive VA and areas for further\nexploration."}
{"id": "2507.17886", "pdf": "https://arxiv.org/pdf/2507.17886", "abs": "https://arxiv.org/abs/2507.17886", "authors": ["James B Aimone"], "title": "Neuromorphic Computing: A Theoretical Framework for Time, Space, and Energy Scaling", "categories": ["cs.NE", "cs.AR", "cs.DC"], "comment": "True pre-print; to be submitted at future date", "summary": "Neuromorphic computing (NMC) is increasingly viewed as a low-power\nalternative to conventional von Neumann architectures such as central\nprocessing units (CPUs) and graphics processing units (GPUs), however the\ncomputational value proposition has been difficult to define precisely.\n  Here, we explain how NMC should be seen as general-purpose and programmable\neven though it differs considerably from a conventional stored-program\narchitecture. We show that the time and space scaling of NMC is equivalent to\nthat of a theoretically infinite processor conventional system, however the\nenergy scaling is significantly different. Specifically, the energy of\nconventional systems scales with absolute algorithm work, whereas the energy of\nneuromorphic systems scales with the derivative of algorithm state. The unique\ncharacteristics of NMC architectures make it well suited for different classes\nof algorithms than conventional multi-core systems like GPUs that have been\noptimized for dense numerical applications such as linear algebra. In contrast,\nthe unique characteristics of NMC make it ideally suited for scalable and\nsparse algorithms whose activity is proportional to an objective function, such\nas iterative optimization and large-scale sampling (e.g., Monte Carlo)."}
{"id": "2507.17778", "pdf": "https://arxiv.org/pdf/2507.17778", "abs": "https://arxiv.org/abs/2507.17778", "authors": ["M. Tedeschi", "S. Rizwan", "C. Shringi", "V. Devram Chandgir", "S. Belich"], "title": "An advanced AI driven database system", "categories": ["cs.DB", "cs.AI", "cs.SE", "68P20", "H.2.4; I.2.7"], "comment": "10 pages, 5 figures, appears in EDULEARN25 Conference Proceedings", "summary": "Contemporary database systems, while effective, suffer severe issues related\nto complexity and usability, especially among individuals who lack technical\nexpertise but are unfamiliar with query languages like Structured Query\nLanguage (SQL). This paper presents a new database system supported by\nArtificial Intelligence (AI), which is intended to improve the management of\ndata using natural language processing (NLP) - based intuitive interfaces, and\nautomatic creation of structured queries and semi-structured data formats like\nyet another markup language (YAML), java script object notation (JSON), and\napplication program interface (API) documentation. The system is intended to\nstrengthen the potential of databases through the integration of Large Language\nModels (LLMs) and advanced machine learning algorithms. The integration is\npurposed to allow the automation of fundamental tasks such as data modeling,\nschema creation, query comprehension, and performance optimization. We present\nin this paper a system that aims to alleviate the main problems with current\ndatabase technologies. It is meant to reduce the need for technical skills,\nmanual tuning for better performance, and the potential for human error. The AI\ndatabase employs generative schema inference and format selection to build its\nschema models and execution formats."}
{"id": "2507.18169", "pdf": "https://arxiv.org/pdf/2507.18169", "abs": "https://arxiv.org/abs/2507.18169", "authors": ["Lorenzo Porcaro", "Chiara Monaldi"], "title": "Recommender systems, representativeness, and online music: A psychosocial analysis of Italian listeners", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Recommender systems shape music listening worldwide due to their widespread\nadoption in online platforms. Growing concerns about representational harms\nthat these systems may cause are nowadays part of the scientific and public\ndebate, wherein music listener perspectives are oftentimes reported and\ndiscussed from a cognitive-behaviorism perspective, but rarely contextualised\nunder a psychosocial and cultural lens. We proceed in this direction, by\ninterviewing a group of Italian music listeners and analysing their narratives\nthrough Emotional Textual Analysis. Thanks to this, we identify shared cultural\nrepertoires that reveal people's complex relationship with listening practices:\neven when familiar with online platforms, listeners may still lack a critical\nunderstanding of recommender systems. Moreover, representational issues,\nparticularly gender disparities, seem not yet fully grasped in the context of\nonline music listening. This study underscores the need for interdisciplinary\nresearch to address representational harms, and the role of algorithmic\nawareness and digital literacy in developing trustworthy recommender systems."}
{"id": "2507.17905", "pdf": "https://arxiv.org/pdf/2507.17905", "abs": "https://arxiv.org/abs/2507.17905", "authors": ["Mahbubur Rahman"], "title": "Enabling Scalability in Asynchronous and Bidirectional Communication in LPWAN", "categories": ["cs.NI", "cs.DC"], "comment": "12 pages", "summary": "LPWANs have become ubiquitous due to their ability to connect sensors over\nlarge geographic areas in a single hop. It is, however, very challenging to\nachieve massive scalability in LPWANs, where numerous sensors can transmit data\nefficiently and with low latency, which emerging IoT and CPS applications may\nrequire. In this paper, we address the above challenges by significantly\nadvancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal\nfrequency division multiplexing, D-OFDM, subcarriers to enable parallel\nreception of data to a BS from multiple asynchronous sensors, each using a\ndifferent subcarrier. In this paper, we achieve massive scalability in SNOW by\nenabling the BS to decode concurrent data from numerous asynchronous sensors on\nthe same subcarrier while parallelly decoding from other subcarriers as well.\nAdditionally, we enable numerous asynchronous sensors to receive distinct data\nfrom the BS on the same subcarrier while other sensors also receive data\nparallelly on other subcarriers. To do this, we develop a set of Gold\ncode-based pseudorandom noise or PN sequences that are mutually non-interfering\nwithin and across the subcarriers. Each sensor uses its PN sequence from the\nset for encoding or decoding data on its subcarriers, enabling massive\nconcurrency. Our evaluation results demonstrate that we can achieve\napproximately 9x more scalability in SNOW while being timely in data collection\nat the BS and energy efficient at the sensors. This may enable emerging IoT and\nCPS applications requiring tens of thousands of sensors with longer battery\nlife and making data-driven, time-sensitive decisions."}
{"id": "2507.18625", "pdf": "https://arxiv.org/pdf/2507.18625", "abs": "https://arxiv.org/abs/2507.18625", "authors": ["Shuqing Li", "Anson Y. Lam", "Yun Peng", "Wenxuan Wang", "Michael R. Lyu"], "title": "3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "comment": null, "summary": "Graphical user interface (UI) software has undergone a fundamental\ntransformation from traditional two-dimensional (2D) desktop/web/mobile\ninterfaces to spatial three-dimensional (3D) environments. While existing work\nhas made remarkable success in automated 2D software generation, such as\nHTML/CSS and mobile app interface code synthesis, the generation of 3D software\nstill remains under-explored. Current methods for 3D software generation\nusually generate the 3D environments as a whole and cannot modify or control\nspecific elements in the software. Furthermore, these methods struggle to\nhandle the complex spatial and semantic constraints inherent in the real world.\nTo address the challenges, we present Scenethesis, a novel\nrequirement-sensitive 3D software synthesis approach that maintains formal\ntraceability between user specifications and generated 3D software. Scenethesis\nis built upon ScenethesisLang, a domain-specific language that serves as a\ngranular constraint-aware intermediate representation (IR) to bridge natural\nlanguage requirements and executable 3D software. It serves both as a\ncomprehensive scene description language enabling fine-grained modification of\n3D software elements and as a formal constraint-expressive specification\nlanguage capable of expressing complex spatial constraints. By decomposing 3D\nsoftware synthesis into stages operating on ScenethesisLang, Scenethesis\nenables independent verification, targeted modification, and systematic\nconstraint satisfaction. Our evaluation demonstrates that Scenethesis\naccurately captures over 80% of user requirements and satisfies more than 90%\nof hard constraints while handling over 100 constraints simultaneously.\nFurthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual\nevaluation scores compared to the state-of-the-art method."}
{"id": "2507.18252", "pdf": "https://arxiv.org/pdf/2507.18252", "abs": "https://arxiv.org/abs/2507.18252", "authors": ["Dongyang Guo", "Yasmeen Abdrabou", "Enkeleda Thaqi", "Enkelejda Kasneci"], "title": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics."}
{"id": "2507.18339", "pdf": "https://arxiv.org/pdf/2507.18339", "abs": "https://arxiv.org/abs/2507.18339", "authors": ["Nils Bosbach", "Meik Schmidt", "Lukas Jünger", "Matthias Berthold", "Rainer Leupers"], "title": "FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping", "categories": ["cs.SE", "cs.DC"], "comment": "PREPRINT - accepted by the 16th International Modelica and FMI\n  Conference 2025", "summary": "As systems become more complex, the demand for thorough testing and virtual\nprototyping grows. To simulate whole systems, multiple tools are usually needed\nto cover different parts. These parts include the hardware of a system and the\nenvironment with which the system interacts. The Functional Mock-up Interface\n(FMI) standard for co-simulation can be used to connect these tools.\n  The control part of modern systems is usually a computing unit, such as a\nSystem-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software\nfrom a connected memory and interacts with peripherals. To develop software\nwithout requiring access to physical hardware, full-system simulators, the\nso-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized\nframework for VP development is SystemC TLM. SystemC provides interfaces and\nconcepts that enable modular design and model exchange. However, SystemC lacks\nnative FMI support, which limits the integration into broader co-simulation\nenvironments.\n  This paper presents a novel framework to control and interact with\nSystemC-based VPs using the FMI. We present a case study showing how a\nsimulated temperature sensor in a SystemC simulation can obtain temperature\nvalues from an external tool via FMI. This approach allows the unmodified\ntarget software to run on the VP and receive realistic environmental input data\nsuch as temperature, velocity, or acceleration values from other tools. Thus,\nextensive software testing and verification is enabled. By having tests ready\nand the software pre-tested using a VP once the physical hardware is available,\ncertifications like ISO 26262 can be done earlier."}
{"id": "2507.18315", "pdf": "https://arxiv.org/pdf/2507.18315", "abs": "https://arxiv.org/abs/2507.18315", "authors": ["Rhys Jacka", "Paola R. Peña", "Sophie Leonard", "Éva Székely", "Benjamin R. Cowan"], "title": "Talking to...uh...um...Machines: The Impact of Disfluent Speech Agents on Partner Models and Perspective Taking", "categories": ["cs.HC"], "comment": "12 pages, 3 figures, in Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces", "summary": "Speech disfluencies play a role in perspective-taking and audience design in\nhuman-human communication (HHC), but little is known about their impact in\nhuman-machine dialogue (HMD). In an online Namer-Matcher task, sixty-one\nparticipants interacted with a speech agent using either fluent or disfluent\nspeech. Participants completed a partner-modelling questionnaire (PMQ) both\nbefore and after the task. Post-interaction evaluations indicated that\nparticipants perceived the disfluent agent as more competent, despite no\nsignificant differences in pre-task ratings. However, no notable differences\nwere observed in assessments of conversational flexibility or human-likeness.\nOur findings also reveal evidence of egocentric and allocentric language\nproduction when participants interact with speech agents. Interaction with\ndisfluent speech agents appears to increase egocentric communication in\ncomparison to fluent agents. Although the wide credibility intervals mean this\neffect is not clear-cut. We discuss potential interpretations of this finding,\nfocusing on how disfluencies may impact partner models and language production\nin HMD."}
{"id": "2507.18454", "pdf": "https://arxiv.org/pdf/2507.18454", "abs": "https://arxiv.org/abs/2507.18454", "authors": ["Juntao Zhao", "Jiuru Li", "Chuan Wu"], "title": "Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.PL"], "comment": null, "summary": "Utilizing CPUs to serve large language models (LLMs) is a resource-friendly\nalternative to GPU serving. Existing CPU-based solutions ignore workload\ndifferences between the prefill and the decode phases of LLM inference,\napplying a static per-NUMA (Non-Uniform Memory Access) node model partition and\nutilizing vendor libraries for operator-level execution, which is suboptimal.\nWe propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses\ndifferent execution plans for the prefill and decode phases and optimizes them\nseparately.\n  We evaluate Sandwich across diverse baselines and datasets on five CPU\nplatforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.\nSandwich achieves an average 2.01x throughput improvement and 90% satisfactory\ntime-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up\nto 3.40x lower requirements in single sequence serving, and significant\nimprovement in Goodput in continuous-batching serving. The GEMM kernels\ngenerated by Sandwich outperform representative vendor kernels and other\ndynamic shape solutions, achieving performance comparable to static compilers\nwith three orders of magnitude less kernel tuning costs."}
{"id": "2507.18393", "pdf": "https://arxiv.org/pdf/2507.18393", "abs": "https://arxiv.org/abs/2507.18393", "authors": ["Mahiro Ozaki", "Li Chen", "Shotaro Naganuma", "Valdemar Švábenský", "Fumiya Okubo", "Atsushi Shimada"], "title": "PALM: PAnoramic Learning Map Integrating Learning Analytics and Curriculum Map for Scalable Insights Across Courses", "categories": ["cs.HC", "cs.CY"], "comment": "To appear in the Proceedings of the IEEE SMC 2025 conference", "summary": "This study proposes and evaluates the PAnoramic Learning Map (PALM), a\nlearning analytics (LA) dashboard designed to address the scalability\nchallenges of LA by integrating curriculum-level information. Traditional LA\nresearch has predominantly focused on individual courses or learners and often\nlacks a framework that considers the relationships between courses and the\nlong-term trajectory of learning. To bridge this gap, PALM was developed to\nintegrate multilayered educational data into a curriculum map, enabling\nlearners to intuitively understand their learning records and academic\nprogression. We conducted a system evaluation to assess PALM's effectiveness in\ntwo key areas: (1) its impact on students' awareness of their learning\nbehaviors, and (2) its comparative performance against existing systems. The\nresults indicate that PALM enhances learners' awareness of study planning and\nreflection, particularly by improving perceived behavioral control through the\nvisual presentation of individual learning histories and statistical trends,\nwhich clarify the links between learning actions and outcomes. Although PALM\nrequires ongoing refinement as a system, it received significantly higher\nevaluations than existing systems in terms of visual appeal and usability. By\nserving as an information resource with previously inaccessible insights, PALM\nenhances self-regulated learning and engagement, representing a significant\nstep beyond conventional LA toward a comprehensive and scalable approach."}
{"id": "2507.18401", "pdf": "https://arxiv.org/pdf/2507.18401", "abs": "https://arxiv.org/abs/2507.18401", "authors": ["Andrew Jeyathasan", "Swati Banerjee"], "title": "Multisensory Integration and Sensory Substitution Across Vision, Audition, and Haptics: Answering the What, Which, and When in Study Protocols", "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "We experience the world through multiple senses that work together to create\na cohesive perception, whether in daily life or immersive technologies.\nUnderstanding this multisensory integration (MSI) requires examining the\ninteractions between sensory modalities, each with unique temporal dynamics and\ncharacteristics. While most research focuses on unimodal or bimodal cues, the\nintegration of three or more modalities remains underexplored. MSI studies must\naccount for factors like cross-modal correspondence, congruence, cognitive\nload, and stimulus timing, which become increasingly complex as modalities\nmultiply. This article examines these key factors and how they can be applied\nto 8 design effective MSI study protocols."}
{"id": "2507.18428", "pdf": "https://arxiv.org/pdf/2507.18428", "abs": "https://arxiv.org/abs/2507.18428", "authors": ["Lena Cibulski", "Stefan Bruckner"], "title": "Towards Understanding Decision Problems As a Goal of Visualization Design", "categories": ["cs.HC"], "comment": null, "summary": "Decision-making is a central yet under-defined goal in visualization\nresearch. While existing task models address decision processes, they often\nneglect the conditions framing a decision. To better support decision-making\ntasks, we propose a characterization scheme that describes decision problems\nthrough key properties of the data, users, and task context. This scheme helps\nvisualization researchers specify decision-support claims more precisely and\ninforms the design of appropriate visual encodings and interactions. We\ndemonstrate the utility of our approach by applying it to characterize decision\ntasks targeted by existing design studies, highlighting opportunities for\nfuture research in decision-centric visualization."}
{"id": "2507.18450", "pdf": "https://arxiv.org/pdf/2507.18450", "abs": "https://arxiv.org/abs/2507.18450", "authors": ["Alice Williams", "Boris Kovalerchuk"], "title": "High-Dimensional Data Classification in Concentric Coordinates", "categories": ["cs.HC", "cs.LG"], "comment": "8 pages, 21 figures", "summary": "The visualization of multi-dimensional data with interpretable methods\nremains limited by capabilities for both high-dimensional lossless\nvisualizations that do not suffer from occlusion and that are computationally\ncapable by parameterized visualization. This paper proposes a low to high\ndimensional data supporting framework using lossless Concentric Coordinates\nthat are a more compact generalization of Parallel Coordinates along with\nformer Circular Coordinates. These are forms of the General Line Coordinate\nvisualizations that can directly support machine learning algorithm\nvisualization and facilitate human interaction."}
{"id": "2507.18510", "pdf": "https://arxiv.org/pdf/2507.18510", "abs": "https://arxiv.org/abs/2507.18510", "authors": ["Chenyang Zhang", "Tiffany S Ma", "John Andrews", "Eric J Gonzalez", "Mar Gonzalez-Franco", "Yalong Yang"], "title": "ForcePinch: Force-Responsive Spatial Interaction for Tracking Speed Control in XR", "categories": ["cs.HC"], "comment": null, "summary": "Spatial interaction in 3D environments requires balancing efficiency and\nprecision, which requires dynamic tracking speed adjustments. However, existing\ntechniques often couple tracking speed adjustments directly with hand\nmovements, reducing interaction flexibility. Inspired by the natural friction\ncontrol inherent in the physical world, we introduce ForcePinch, a novel\nforce-responsive spatial interaction method that enables users to intuitively\nmodulate pointer tracking speed and smoothly transition between rapid and\nprecise movements by varying their pinching force. To implement this concept,\nwe developed a hardware prototype integrating a pressure sensor with a\ncustomizable mapping function that translates pinching force into tracking\nspeed adjustments. We conducted a user study with 20 participants performing\nwell-established 1D, 2D, and 3D object manipulation tasks, comparing ForcePinch\nagainst the distance-responsive technique Go-Go and speed-responsive technique\nPRISM. Results highlight distinctive characteristics of the force-responsive\napproach across different interaction contexts. Drawing on these findings, we\nhighlight the contextual meaning and versatility of force-responsive\ninteractions through four illustrative examples, aiming to inform and inspire\nfuture spatial interaction design."}
{"id": "2507.18572", "pdf": "https://arxiv.org/pdf/2507.18572", "abs": "https://arxiv.org/abs/2507.18572", "authors": ["Donghoon Shin", "Daniel Lee", "Gary Hsieh", "Gromit Yeuk-Yin Chan"], "title": "PosterMate: Audience-driven Collaborative Persona Agents for Poster Design", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "comment": null, "summary": "Poster designing can benefit from synchronous feedback from target audiences.\nHowever, gathering audiences with diverse perspectives and reconciling them on\ndesign edits can be challenging. Recent generative AI models present\nopportunities to simulate human-like interactions, but it is unclear how they\nmay be used for feedback processes in design. We introduce PosterMate, a poster\ndesign assistant that facilitates collaboration by creating audience-driven\npersona agents constructed from marketing documents. PosterMate gathers\nfeedback from each persona agent regarding poster components, and stimulates\ndiscussion with the help of a moderator to reach a conclusion. These\nagreed-upon edits can then be directly integrated into the poster design.\nThrough our user study (N=12), we identified the potential of PosterMate to\ncapture overlooked viewpoints, while serving as an effective prototyping tool.\nAdditionally, our controlled online evaluation (N=100) revealed that the\nfeedback from an individual persona agent is appropriate given its persona\nidentity, and the discussion effectively synthesizes the different persona\nagents' perspectives."}
{"id": "2507.18619", "pdf": "https://arxiv.org/pdf/2507.18619", "abs": "https://arxiv.org/abs/2507.18619", "authors": ["Yichen Yu", "Qiaoran Wang"], "title": "MeloKids: Multisensory VR System to Enhance Speech and Motor Coordination in Children with Hearing Loss", "categories": ["cs.HC"], "comment": null, "summary": "Children with hearing impairments face ongoing challenges in language and\nmotor development. This study explores how multi-sensory feedback technology\nbased on virtual reality (VR), integrating auditory, visual, and tactile\nstimuli, can enhance rehabilitation outcomes. Using functional near-infrared\nspectroscopy (fNIRS) technology, we assessed cortical activation patterns in\nchildren during pitch-matching tasks across different interaction modes. Our\nfindings aim to provide evidence for designing personalized, interactive\nrehabilitation systems that enhance cognitive engagement and motor control in\nchildren with hearing impairments."}
{"id": "2507.18622", "pdf": "https://arxiv.org/pdf/2507.18622", "abs": "https://arxiv.org/abs/2507.18622", "authors": ["Armin Bernstetter", "Tom Kwasnitschka", "Isabella Peters"], "title": "Evaluation of a Provenance Management Tool for Immersive Virtual Fieldwork", "categories": ["cs.HC"], "comment": "Accepted for Mensch und Computer 2025 Short Paper Track", "summary": "Ensuring reproducibility of research is an integral part of good scientific\npractice. One way to support this is through provenance: information about\nresearch workflows from data gathering to researchers' sensemaking processes\nleading to published results. This is highly important in disciplines such as\ngeosciences, where researchers use software for interactive and immersive\nvisualizations of geospatial data, doing virtual measurements in simulated\nfieldwork on 3D models. We evaluated a provenance management tool, which allows\nrecording of interactions with a virtual fieldwork tool and annotating\ndifferent states of the visualization. The user study investigated how\nresearchers used this Digital Lab Book (DLB) and whether perceived ease of use\nand perceived usefulness differed between groups in immersive or non-immersive\nsettings. Participants perceived the DLB as both useful and easy to use. While\nthere were indications of differences in perceived ease of use (higher for\nimmersive setting), usage patterns showed no significant group differences."}
{"id": "2507.17758", "pdf": "https://arxiv.org/pdf/2507.17758", "abs": "https://arxiv.org/abs/2507.17758", "authors": ["Pierre-Marie Chauvin", "Angèle Merlin", "Xavier Fresquet", "Hugo Caselles-Dupré", "Benjamin Simmenauer", "Mathieu de Fayet"], "title": "Weaving the Future: Generative AI and the Reimagining of Fashion Design", "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "This paper explores the integration of generative AI into the fashion design\nprocess. Drawing on insights from the January 2025 seminar ``Tisser le futur,''\nit investigates how AI reshapes creative workflows, from ideation to\nprototyping, while interrogating the ethical, aesthetic, and labor\nimplications. The paper highlights co-creative dynamics between humans and\nmachines, the potential for aesthetic innovation, and the environmental and\ncultural challenges of algorithmic design."}
{"id": "2507.17760", "pdf": "https://arxiv.org/pdf/2507.17760", "abs": "https://arxiv.org/abs/2507.17760", "authors": ["Fatma Betül Güreş", "Tanya Nazaretsky", "Bahar Radmehr", "Martina Rau", "Tanja Käser"], "title": "How Instructional Sequence and Personalized Support Impact Diagnostic Strategy Learning", "categories": ["cs.CY", "cs.AI", "cs.HC", "K.3.1"], "comment": "Submitted to AIED 2025 main track", "summary": "Supporting students in developing effective diagnostic reasoning is a key\nchallenge in various educational domains. Novices often struggle with cognitive\nbiases such as premature closure and over-reliance on heuristics.\nScenario-based learning (SBL) can address these challenges by offering\nrealistic case experiences and iterative practice, but the optimal sequencing\nof instruction and problem-solving activities remains unclear. This study\nexamines how personalized support can be incorporated into different\ninstructional sequences and whether providing explicit diagnostic strategy\ninstruction before (I-PS) or after problem-solving (PS-I) improves learning and\nits transfer. We employ a between-groups design in an online SBL environment\ncalled PharmaSim, which simulates real-world client interactions for pharmacy\ntechnician apprentices. Results indicate that while both instruction types are\nbeneficial, PS-I leads to significantly higher performance in transfer tasks."}
{"id": "2507.17978", "pdf": "https://arxiv.org/pdf/2507.17978", "abs": "https://arxiv.org/abs/2507.17978", "authors": ["Paulo Mendes", "Eva Maia", "Isabel Praça"], "title": "MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection", "categories": ["cs.CR", "cs.AI", "cs.HC", "68P20 (Primary) 68T05, 68T07, 68T10 (Secondary)", "K.6.5; I.2.6; I.2.7; C.2.0"], "comment": "8 pages, 2 tables, WI-IAT 2025 conference", "summary": "Phishing emails continue to pose a significant threat to cybersecurity by\nexploiting human vulnerabilities through deceptive content and malicious\npayloads. While Machine Learning (ML) models are effective at detecting\nphishing threats, their performance largely relies on the quality and diversity\nof the training data. This paper presents MeAJOR (Merged email Assets from\nJoint Open-source Repositories) Corpus, a novel, multi-source phishing email\ndataset designed to overcome critical limitations in existing resources. It\nintegrates 135894 samples representing a broad number of phishing tactics and\nlegitimate emails, with a wide spectrum of engineered features. We evaluated\nthe dataset's utility for phishing detection research through systematic\nexperiments with four classification models (RF, XGB, MLP, and CNN) across\nmultiple feature configurations. Results highlight the dataset's effectiveness,\nachieving 98.34% F1 with XGB. By integrating broad features from multiple\ncategories, our dataset provides a reusable and consistent resource, while\naddressing common challenges like class imbalance, generalisability and\nreproducibility."}
{"id": "2507.18022", "pdf": "https://arxiv.org/pdf/2507.18022", "abs": "https://arxiv.org/abs/2507.18022", "authors": ["Victoria R. Li", "Johnathan Sun", "Martin Wattenberg"], "title": "Does visualization help AI understand data?", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "5 pages, 6 figures", "summary": "Charts and graphs help people analyze data, but can they also be useful to AI\nsystems? To investigate this question, we perform a series of experiments with\ntwo commercial vision-language models: GPT 4.1 and Claude 3.5. Across three\nrepresentative analysis tasks, the two systems describe synthetic datasets more\nprecisely and accurately when raw data is accompanied by a scatterplot,\nespecially as datasets grow in complexity. Comparison with two baselines --\nproviding a blank chart and a chart with mismatched data -- shows that the\nimproved performance is due to the content of the charts. Our results are\ninitial evidence that AI systems, like humans, can benefit from visualization."}
{"id": "2507.18262", "pdf": "https://arxiv.org/pdf/2507.18262", "abs": "https://arxiv.org/abs/2507.18262", "authors": ["Chenyu Su", "Weiwei Shang", "Chen Qian", "Fei Zhang", "Shuang Cong"], "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "12 pages,9 figures", "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io."}
{"id": "2507.18523", "pdf": "https://arxiv.org/pdf/2507.18523", "abs": "https://arxiv.org/abs/2507.18523", "authors": ["Maciej Skorski", "Alina Landowska"], "title": "The Moral Gap of Large Language Models", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "comment": "preprint", "summary": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications."}
