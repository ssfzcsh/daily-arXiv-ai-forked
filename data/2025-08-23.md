<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 4]
- [cs.HC](#cs.HC) [Total: 11]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 5]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: 该论文提出了一个全面的自动化程序修复（APR）工具评估框架，并验证了Sorald工具在修复代码违规时的副作用，如引入新错误和降低代码质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究对APR工具的评估仅关注其清除违规的能力，而忽略了其可能导致的新违规、功能变化和代码结构退化。

Method: 研究开发了一个综合评估框架，并利用Sorald工具修复了2,393个Java代码片段中的3,529个SonarQube违规。

Result: Sorald修复了特定规则违规，但引入了2,120个新问题（32个错误、2088个代码异味），并导致24%的单元测试失败和代码结构退化。

Conclusion: 研究强调需要全面的APR工具评估方法，以捕捉其所有副作用，确保安全有效使用。

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [2] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: 本文提出将生成式AI的认知能力与传统软件工程原则结合，构建可靠、自适应且高效的GenAI原生系统。


<details>
  <summary>Details</summary>
Motivation: 生成式AI虽强大，但其不可预测性和低效性限制了系统的可靠性，未来系统需更稳健和自适应。

Method: 提出GenAI原生设计原则（可靠性、卓越性、可进化性、自依赖性和保障性）及架构模式（如GenAI原生单元、有机基底和可编程路由器）。

Result: 构建了GenAI原生软件栈的初步框架，为未来研究提供了技术、用户采纳、经济和法律视角的参考。

Conclusion: 呼吁进一步验证此框架，激励相关社区实施并完善GenAI原生系统的设计。

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>


### [3] [An Empirical Study of Knowledge Distillation for Code Understanding Tasks](https://arxiv.org/abs/2508.15423)
*Ruiqi Wang,Zezhou Yang,Cuiyun Gao,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 对预训练语言模型（PLM）的知识蒸馏（KD）应用于代码理解任务的研究表明，KD能显著提升学生模型的性能，尤其是特征蒸馏方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 研究知识蒸馏在代码理解任务中的潜力，以解决PLM在部署中的计算和延迟问题。

Method: 通过两种KD方法（基于logit和特征）在八种学生模型和两种教师PLM上进行实验，涵盖三个下游任务。

Result: KD显著提升学生模型性能，特征蒸馏方法表现最佳，学生模型参数仅5%即可保留98%教师性能。

Conclusion: 特征蒸馏方法在代码理解任务中表现优越，教师模型与学生模型架构相似性不直接影响性能，未来研究方向值得探索。

Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code
understanding. However, deploying these PLMs in large-scale applications faces
practical challenges due to their computational intensity and inference
latency. Knowledge distillation (KD), a promising model compression and
acceleration technique, addresses these limitations by transferring knowledge
from large teacher models to compact student models, enabling efficient
inference while preserving most of the teacher models' capabilities. While this
technique has shown remarkable success in natural language processing and
computer vision domains, its potential for code understanding tasks remains
largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of
KD in code understanding tasks. Our study encompasses two popular types of KD
methods, i.e., logit-based and feature-based KD methods, experimenting across
eight student models and two teacher PLMs from different domains on three
downstream tasks. The experimental results indicate that KD consistently offers
notable performance boosts across student models with different sizes compared
with standard fine-tuning. Notably, code-specific PLM demonstrates better
effectiveness as the teacher model. Among all KD methods, the latest
feature-based KD methods exhibit superior performance, enabling student models
to retain up to 98% teacher performance with merely 5% parameters. Regarding
student architecture, our experiments reveal that similarity with teacher
architecture does not necessarily lead to better performance. We further
discuss the efficiency and behaviors in the KD process and inference, summarize
the implications of findings, and identify promising future directions.

</details>


### [4] [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://arxiv.org/abs/2508.15495)
*Dongjun Yu,Xiao Yan,Zhenrui Li,Jipeng Xiao,Haochuan He,Yongda Yu,Hao Zhang,Guoping Rong,Xiaobo Huang*

Main category: cs.SE

TL;DR: SynthCoder模型通过结合AST节点提取、BM25算法和调用图等方法构建多样化训练集，并采用两阶段训练（课程学习和DPO），在代码补全任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码补全模型在优化时存在性能波动（seesaw effect），部分数据集或指标上的提升可能伴随其他方面下降。SynthCoder旨在通过综合优化方法解决这一问题。

Method: 1) 结合AST节点提取和启发式方法构建多样化训练集；2) 使用BM25算法和调用图增强跨文件上下文；3) 基于Seed-Coder-8B-Base进行课程学习和DPO两阶段训练。

Result: 在aiXcoder、ExecRepoBench等主流代码补全基准测试中表现优异，且有效缓解了模型重复现有代码的问题。

Conclusion: SynthCoder通过数据多样性、上下文增强和两阶段训练，显著提升代码补全性能，为实际应用提供了更可靠的解决方案。

Abstract: Code completion is a prominent application of Large Language Models (LLMs) in
software engineering. Due to the near real-time response requirements of this
task, base models with small to medium-sized parameters are typically employed,
supplemented by various optimization and post-training techniques. However,
these optimization methods often have trade-offs, leading to a seesaw effect
where performance improvements on certain datasets or metrics are accompanied
by degradations on others -- sometimes even falling below the baseline model's
performance. This paper proposes SynthCoder, a model that integrates leading
industry practices to achieve state-of-the-art performance on the
Fill-in-the-Middle (FIM) code completion task. In specific, we first construct
a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with
heuristics that simulate developer behavior. Then we enrich our training corpus
with cross-file contextual information using the BM25 algorithm and call
graphs, enhancing the model's ability to perform code completion in both
file-level and repository-level scenarios. As the last step, we employ a
two-stage training process using the Seed-Coder-8B-Base as the base model.
First, we fine-tune the model using Curriculum Learning technology. Following
this, we perform alignment using Direct Preference Optimization (DPO) with
preference pairs generated through Rejection Sampling. Experimental results
demonstrate that our final model excels on mainstream repository-level code
completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and
CoLT. Furthermore, our carefully curated training set effectively mitigates the
model's tendency to just repeat existing code, a common issue existing in
various code completion models.

</details>


### [5] [Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset](https://arxiv.org/abs/2508.15496)
*Elena Masserini,Diego Clerissi,Daniela Micucci,João R. Campos,Leonardo Mariani*

Main category: cs.SE

TL;DR: 论文提出了两个数据集（TOFU-R和BRASATO）及相关工具，用于评估任务型聊天机器人的可靠性、安全性和鲁棒性，弥补现有评估数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 任务型聊天机器人应用广泛，但其可靠性、安全性和鲁棒性的评估缺乏高质量、大规模的数据集。现有评估方法因数据不足或过时而受限。

Method: 作者创建了两个数据集：TOFU-R（来自GitHub的Rasa聊天机器人快照）和BRASATO（精选的高质量Rasa聊天机器人集合），并提供了相应的工具支持。

Result: TOFU-R代表了开源Rasa聊天机器人的现状，BRASATO则专注于对话复杂性、功能复杂性和实用性，为研究提供了可复现的数据基础。

Conclusion: 这些数据集和工具填补了聊天机器人评估领域的空白，有助于提升研究效率和可靠性评估的准确性。

Abstract: Task-based chatbots are increasingly being used to deliver real services, yet
assessing their reliability, security, and robustness remains underexplored,
also due to the lack of large-scale, high-quality datasets. The emerging
automated quality assessment techniques targeting chatbots often rely on
limited pools of subjects, such as custom-made toy examples, or outdated, no
longer available, or scarcely popular agents, complicating the evaluation of
such techniques. In this paper, we present two datasets and the tool support
necessary to create and maintain these datasets. The first dataset is RASA
TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa
chatbots available on GitHub, representing the state of the practice in
open-source chatbot development with Rasa. The second dataset is BOT RASA
COLLECTION (BRASATO), a curated selection of the most relevant chatbots for
dialogue complexity, functional complexity, and utility, whose goal is to ease
reproducibility and facilitate research on chatbot reliability.

</details>


### [6] [Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs](https://arxiv.org/abs/2508.15503)
*Sebastian Baltes,Florian Angermeir,Chetan Arora,Marvin Muñoz Barón,Chunyang Chen,Lukas Böhme,Fabio Calefato,Neil Ernst,Davide Falessi,Brian Fitzgerald,Davide Fucci,Marcos Kalinowski,Stefano Lambiase,Daniel Russo,Mircea Lungu,Lutz Prechelt,Paul Ralph,Christoph Treude,Stefan Wagner*

Main category: cs.SE

TL;DR: 该论文提出了一套分类法和八项指南，旨在帮助解决LLM在软件工程研究中透明度和可复现性的问题。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的非确定性、训练数据不透明和架构不断变化，导致其研究的复现和复制变得复杂，因此需要一套指南来提高研究的透明度。

Method: 通过社区协作，开发了LLM研究的分类法，并提出了八项设计原则，包括明确模型使用、报告详细信息、验证和开放基准等。

Result: 提出了一个动态在线资源（llm-guidelines.org），为社区提供分类法和指南，以支持LLM研究的透明化和标准化。

Conclusion: 论文的指南和分类法旨在克服LLM研究的挑战，促进开放科学和研究的可复现性。

Abstract: Large language models (LLMs) are increasingly being integrated into software
engineering (SE) research and practice, yet their non-determinism, opaque
training data, and evolving architectures complicate the reproduction and
replication of empirical studies. We present a community effort to scope this
space, introducing a taxonomy of LLM-based study types together with eight
guidelines for designing and reporting empirical studies involving LLMs. The
guidelines present essential (must) criteria as well as desired (should)
criteria and target transparency throughout the research process. Our
recommendations, contextualized by our study types, are: (1) to declare LLM
usage and role; (2) to report model versions, configurations, and fine-tuning;
(3) to document tool architectures; (4) to disclose prompts and interaction
logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)
to report suitable baselines, benchmarks, and metrics; and (8) to openly
articulate limitations and mitigations. Our goal is to enable reproducibility
and replicability despite LLM-specific barriers to open science. We maintain
the study types and guidelines online as a living resource for the community to
use and shape (llm-guidelines.org).

</details>


### [7] [QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements](https://arxiv.org/abs/2508.15512)
*Markus Borg,Martin Larsson,Philip Breid,Nadim Hagatulah*

Main category: cs.SE

TL;DR: 提出QUPER-MAn模型，通过需求工程提升代码可维护性管理。


<details>
  <summary>Details</summary>
Motivation: 可维护性在软件开发中常被忽视，需求工程可填补这一空白。

Method: 探索行业实践，提出QUPER-MAn模型，整合可维护性基准与目标设定。

Result: 可维护性仍被视为次要质量指标，工具多用于隐含需求。

Conclusion: QUPER-MAn有望将可维护性转化为主动管理目标。

Abstract: Maintainable source code is essential for sustainable development in any
software organization. Unfortunately, many studies show that maintainability
often receives less attention than its importance warrants. We argue that
requirements engineering can address this gap the problem by fostering
discussions and setting appropriate targets in a responsible manner. In this
preliminary work, we conducted an exploratory study of industry practices
related to requirements engineering for maintainability. Our findings confirm
previous studies: maintainability remains a second-class quality concern.
Explicit requirements often make sweeping references to coding conventions.
Tools providing maintainability proxies are common but typically only used in
implicit requirements related to engineering practices. To address this, we
propose QUPER-MAn, a maintainability adaption of the QUPER model, which was
originally developed to help organizations set targets for performance
requirements. Developed using a design science approach, QUPER-MAn, integrates
maintainability benchmarks and supports target setting. We posit that it can
shift maintainability from an overlooked development consequence to an actively
managed goal driven by informed and responsible engineering decisions.

</details>


### [8] [A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs](https://arxiv.org/abs/2508.15536)
*Yi Zhang,He Jiang,Xiaochen Li,Shikai Guo,Peiyu Zou,Zun Wang*

Main category: cs.SE

TL;DR: VERMEI是一种测试FPGA逻辑综合工具的新方法，通过预处理、等效变异和漏洞识别模块提高测试效果。


<details>
  <summary>Details</summary>
Motivation: 由于FPGA逻辑综合工具的缺陷可能导致意外行为和安全隐患，需通过测试加固现有工具。现有方法在测试程序中语义和逻辑复杂度不足，无法有效检测漏洞。

Method: VERMEI包含预处理、等效变异和漏洞识别三个模块。预处理通过模拟和覆盖率分析识别僵尸逻辑；等效变异利用贝叶斯采样生成复杂控制流和结构的等效变体；漏洞识别通过差分测试比较合成结果。

Result: 在Yosys、Vivado和Quartus上的实验表明，VERMEI优于现有方法。五个月内报告了15个漏洞，其中9个为新发现。

Conclusion: VERMEI能有效检测FPGA逻辑综合工具中的漏洞，提升测试语义和逻辑复杂度，具有实际应用价值。

Abstract: FPGA (Field-Programmable Gate Array) logic synthesis tools are key components
in the EDA (Electronic Design Automation) toolchain. They convert hardware
designs written in description languages such as Verilog into gate-level
representations for FPGAs. However, defects in these tools may lead to
unexpected behaviors and pose security risks. Therefore, it is crucial to
harden these tools through testing. Although several methods have been proposed
to automatically test FPGA logic synthesis tools, the challenge remains of
insufficient semantic and logical complexity in test programs. In this paper,
we propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI
consists of three modules: preprocessing, equivalent mutation, and bug
identification. The preprocessing module identifies zombie logic (inactive code
with no impact on the circuit output) in seed programs through simulation and
coverage analysis. The equivalent mutation module generates equivalent variants
of seed programs by pruning or inserting logic fragments in zombie areas. It
uses Bayesian sampling to extract logic fragments from historical Verilog
designs, making the generated variants have complex control flows and
structures. The bug identification module, based on differential testing,
compares the synthesized outputs of seed and variant programs to identify bugs.
Experiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms
the state-of-the-art methods. Within five months, VERMEI reported 15 bugs to
vendors, 9 of which were confirmed as new.

</details>


### [9] [Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study](https://arxiv.org/abs/2508.15570)
*Marion Wiese,Kamila Serwa,Anastasia Besier,Ariane S. Marion-Jetten,Eva Bittner*

Main category: cs.SE

TL;DR: 研究了基于工作坊的技术债管理（TDM）过程在企业中的实施效果，结果表明这种方法可行且能提升团队对技术债的意识。


<details>
  <summary>Details</summary>
Motivation: 技术债管理虽被广泛研究，但在实践中应用较少。本研究旨在通过工作坊方式在一个IT公司中建立TDM过程，并分析其长期效果。

Method: 采用行动研究方法（16个月内5个行动周期），通过问卷、团队会议观察、心理学方法（TD-SAGAT）和待办事项数据分析团队对技术债的认知变化。

Result: 团队偏好基于系统演化和成本计算的债务偿还与优先级排序，并通过待办事项中的提醒工具（如复选框）显著提升了技术债意识。

Conclusion: 工作坊方法可行且可持续，研究还提出了适用于其他团队的新TDM思路，如重新提交日期、讨论复选框和可视化优先级排序。

Abstract: Context. Technical debt (TD) items are constructs in a software system
providing short-term benefits but hindering future changes. TD management (TDM)
is frequently researched but rarely adopted in practice. Goal. This study aimed
to establish a TDM process in an IT company based on a predefined workshop
concept. We analyzed which research approaches practitioners adopted for each
TD activity and the TDM's long-term effect on TD awareness. Method. We used
action research (five action cycles in 16 months) with an IT team that creates
IT solutions for signal processing. To examine TD awareness, we (1) analyzed
questionnaires completed during each workshop, (2) observed team meetings, (3)
adopted a method from psychology for measuring awareness in decision-making
situations called TD-SAGAT, and (4) evaluated the backlog data. Results.
Practitioners preferred TD repayment and prioritization based on the system's
evolution and cost calculations, i.e., repayment of so-called low-hanging
fruits. Reminders in the backlog items, such as checkboxes or text templates,
led to a sustainable rise in TD awareness. Conclusions. We showed that a
workshop-based approach is feasible and leads to sustainable process changes.
New ideas for TDM applicable to other IT teams emerged, e.g., using a
re-submission date, using a Talked about TD checkbox, and using visualizations
for TD prioritization.

</details>


### [10] [From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems](https://arxiv.org/abs/2508.15584)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: 论文介绍了PREVENT和REACT方法在船舶系统中的异常检测与故障排除集成应用及其启示。


<details>
  <summary>Details</summary>
Motivation: 工业系统常因磨损、误用或故障而行为异常，需要及时检测、定位问题并实施对策。

Method: 采用PREVENT故障预测方法，并扩展了带故障排除模块的REACT方法，应用于Fincantieri的船舶系统。

Result: 成功将异常检测与故障排除集成验证于实际工业系统。

Conclusion: 总结了方法部署与扩展的实践经验，可为其他工业产品提供参考。

Abstract: Complex and large industrial systems often misbehave, for instance, due to
wear, misuse, or faults. To cope with these incidents, it is important to
timely detect their occurrences, localize the sources of the problems, and
implement the appropriate countermeasures. This paper reports our experience
with a state-of-the-art failure prediction method, PREVENT, and its extension
with a troubleshooting module, REACT, applied to naval systems developed by
Fincantieri. Our results show how to integrate anomaly detection with
troubleshooting procedures. We conclude by discussing a lesson learned, which
may help deploy and extend these analyses to other industrial products.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Homomorphism Calculus for User-Defined Aggregations](https://arxiv.org/abs/2508.15109)
*Ziteng Wang,Ruijie Fang,Linus Zheng,Dixin Tang,Isil Dillig*

Main category: cs.PL

TL;DR: 本文提出了一种新型的“同态演算”方法，用于验证或反驳用户定义的聚合函数（UDAF）是否满足同态性质。若满足，该方法还能构建对应的合并运算符，以支持增量计算和并行执行。实验表明，该方法优于现有的两种合成器。


<details>
  <summary>Details</summary>
Motivation: 当前的数据处理框架（如Spark和Flink）需要UDAF满足同态性质以实现高效执行，但缺乏有效的方法来验证或构建这种性质。

Method: 论文引入了“同态演算”方法，通过算法验证UDAF的同态性并构造合并运算符。

Result: 实验证明，该方法在真实世界的UDAF中表现优异，显著优于现有的两种合成器。

Conclusion: 提出的同态演算方法不仅能够验证UDAF的同态性质，还能构建高效的合并运算符，为增量计算和并行执行提供支持。

Abstract: Data processing frameworks like Apache Spark and Flink provide built-in
support for user-defined aggregation functions (UDAFs), enabling the
integration of domain-specific logic. However, for these frameworks to support
\emph{efficient} UDAF execution, the function needs to satisfy a
\emph{homomorphism property}, which ensures that partial results from
independent computations can be merged correctly. Motivated by this problem,
this paper introduces a novel \emph{homomorphism calculus} that can both verify
and refute whether a UDAF is a dataframe homomorphism. If so, our calculus also
enables the construction of a corresponding merge operator which can be used
for incremental computation and parallel execution. We have implemented an
algorithm based on our proposed calculus and evaluate it on real-world UDAFs,
demonstrating that our approach significantly outperforms two leading
synthesizers.

</details>


### [12] [Software Model Checking via Summary-Guided Search (Extended Version)](https://arxiv.org/abs/2508.15137)
*Ruijie Fang,Zachary Kincaid,Thomas Reps*

Main category: cs.PL

TL;DR: GPS是一种新的软件模型检查算法，结合静态分析和测试生成，高效发现程序中的安全证明和错误路径。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够同时处理安全证明和错误路径的模型检查算法，提高程序验证的效率和覆盖范围。

Method: GPS采用两层搜索策略，结合静态分析生成摘要以指导测试和路径探索，并引入技术确保完全性。

Result: GPS在基准测试中表现优于现有最先进的软件模型检查工具。

Conclusion: GPS为程序验证提供了一种高效且全面的解决方案。

Abstract: In this work, we describe a new software model-checking algorithm called GPS.
GPS treats the task of model checking a program as a directed search of the
program states, guided by a compositional, summary-based static analysis. The
summaries produced by static analysis are used both to prune away infeasible
paths and to drive test generation to reach new, unexplored program states. GPS
can find both proofs of safety and counter-examples to safety (i.e., inputs
that trigger bugs), and features a novel two-layered search strategy that
renders it particularly efficient at finding bugs in programs featuring long,
input-dependent error paths. To make GPS refutationally complete (in the sense
that it will find an error if one exists, if it is allotted enough time), we
introduce an instrumentation technique and show that it helps GPS achieve
refutation-completeness without sacrificing overall performance. We benchmarked
GPS on a suite of benchmarks including both programs from the Software
Verification Competition (SV-COMP) and from prior literature, and found that
our implementation of GPS outperforms state-of-the-art software model checkers
(including the top performers in SV-COMP ReachSafety-Loops category), both in
terms of the number of benchmarks solved and in terms of running time.

</details>


### [13] [Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment](https://arxiv.org/abs/2508.15157)
*David M Kahn,Jan Hoffmann,Runming Li*

Main category: cs.PL

TL;DR: 该论文提出了一种扩展的大步语义方法，通过引入归纳定义来捕捉发散计算，同时保持大步语义的简洁性。


<details>
  <summary>Details</summary>
Motivation: 传统的大步语义虽然简洁易用，但无法描述发散计算等行为。作者希望改进大步语义以解决这一问题。

Method: 作者扩展了大步语义的推理规则，通过少量额外规则定义了一个等价于小步语义的评价判断，避免了复杂的状态或共归纳原理。

Result: 该方法成功应用于类型化、无类型化和带效果的PCF语言，以及基于while循环的命令式语言。

Conclusion: 这种扩展的大步语义在保持简洁的同时，能够捕捉更多的程序行为，优于文献中的其他解决方案。

Abstract: As evident in the programming language literature, many practitioners favor
specifying dynamic program behavior using big-step over small-step semantics.
Unlike small-step semantics, which must dwell on every intermediate program
state, big-step semantics conveniently jump directly to the ever-important
result of the computation. Big-step semantics also typically involve fewer
inference rules than their small-step counterparts. However, in exchange for
ergonomics, big-step semantics give up power: Small-step semantics describes
program behaviors that are outside the grasp of big-step semantics, notably
divergence. This work presents a little-known extension of big-step semantics
with inductive definitions that captures diverging computations without
introducing error states. This big-stop semantics is illustrated for typed,
untyped, and effectful variants of PCF, as well as a while-loop-based
imperative language. Big-stop semantics extends the standard big-step inference
rules with a few additional rules to define an evaluation judgment that is
equivalent to the reflexive-transitive closure of small-step transitions. This
simple extension contrasts with other solutions in the literature which
sacrifice ergonomics by introducing many additional inference rules, global
state, and/or less-commonly-understood reasoning principles like coinduction.

</details>


### [14] [Probabilistic Inference for Datalog with Correlated Inputs](https://arxiv.org/abs/2508.15166)
*Jingbo Wang,Shashin Halalingaiah,Weiyi Chen,Chao Wang,Isil Dillig*

Main category: cs.PL

TL;DR: Praline是一种新的Datalog扩展，用于在输入事实存在统计相关性时进行精确概率推理。通过将推理问题建模为约束优化问题，并结合高效算法，实现了可扩展性和精确性。


<details>
  <summary>Details</summary>
Motivation: 现有的概率逻辑编程语言（如ProbLog）未考虑输入事实之间的统计相关性，影响了推理的精确性。

Method: 提出了Praline，将其设计为约束优化问题，并提出了一种基于约束求解、静态分析和迭代优化的δ-精确推理算法。

Result: 在真实世界基准测试中（如侧信道分析），Praline不仅具有可扩展性，还能提供紧密的概率边界。

Conclusion: Praline显著提升了在输入相关性问题中的精确性和可扩展性，适用于复杂场景。

Abstract: Probabilistic extensions of logic programming languages, such as ProbLog,
integrate logical reasoning with probabilistic inference to evaluate
probabilities of output relations; however, prior work does not account for
potential statistical correlations among input facts. This paper introduces
Praline, a new extension to Datalog designed for precise probabilistic
inference in the presence of (partially known) input correlations. We formulate
the inference task as a constrained optimization problem, where the solution
yields sound and precise probability bounds for output facts. However, due to
the complexity of the resulting optimization problem, this approach alone often
does not scale to large programs. To address scalability, we propose a more
efficient $\delta$-exact inference algorithm that leverages constraint solving,
static analysis, and iterative refinement. Our empirical evaluation on
challenging real-world benchmarks, including side-channel analysis,
demonstrates that our method not only scales effectively but also delivers
tight probability bounds.

</details>


### [15] [Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern](https://arxiv.org/abs/2508.15264)
*Patrick Redmond,Jonathan Castello,José Manuel Calderón Trilla,Lindsey Kuper*

Main category: cs.PL

TL;DR: 论文总结了ECS模式的优点（模块化、灵活性和性能），并提出了一个形式化模型Core ECS，揭示其本质，同时发现现有框架未充分利用确定性并发的潜力。


<details>
  <summary>Details</summary>
Motivation: ECS模式在少数领域外缺乏深入理解，现有解释多限于具体框架或模糊比喻，作者希望通过形式化模型揭示其本质。

Method: 设计了一个抽象的形式化模型Core ECS，并基于此分析了一类确定性程序，随后调查了多个实际ECS框架。

Result: 发现现有ECS框架未充分利用确定性并发的机会，提出了新的实现技术的空间。

Conclusion: Core ECS为ECS模式提供了严格理解，并指出了优化确定性并发的方向。

Abstract: The Entity-Component-System (ECS) software design pattern, long used in game
development, encourages a clean separation of identity (entities), data
properties (components), and computational behaviors (systems). Programs
written using the ECS pattern are naturally concurrent, and the pattern offers
modularity, flexibility, and performance benefits that have led to a
proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known
and not well understood outside of a few domains. Existing explanations of the
ECS pattern tend to be mired in the concrete details of particular ECS
frameworks, or they explain the pattern in terms of imperfect metaphors or in
terms of what it is not. We seek a rigorous understanding of the ECS pattern
via the design of a formal model, Core ECS, that abstracts away the details of
specific implementations to reveal the essence of software using the ECS
pattern. We identify a class of Core ECS programs that behave deterministically
regardless of scheduling, enabling use of the ECS pattern as a
deterministic-by-construction concurrent programming model. With Core ECS as a
point of comparison, we then survey several real-world ECS frameworks and find
that they all leave opportunities for deterministic concurrency unexploited.
Our findings point out a space for new ECS implementation techniques that
better leverage such opportunities.

</details>


### [16] [Fair Termination for Resource-Aware Active Objects](https://arxiv.org/abs/2508.15333)
*Francesco Dagnino,Paola Giannini,Violet Ka I Pun,Ulises Torrella*

Main category: cs.PL

TL;DR: 论文摘要介绍了一种资源感知的活动对象核心演算及其类型系统，确保良好类型的程序能够公平终止。


<details>
  <summary>Details</summary>
Motivation: 活动对象系统是用于建模分布式系统和业务流程的并发且资源感知的模型，因此需要开发资源感知的形式化方法。

Method: 结合了用于顺序程序的渐变语义和类型系统技术，以及用于同步会话的公平终止技术。

Result: 开发了资源感知的活动对象核心演算及其类型系统，确保程序能公平终止。

Conclusion: 该工作为资源感知的活动对象模型提供了形式化基础，并通过类型系统实现了公平终止目标。

Abstract: Active object systems are a model of distributed computation that has been
adopted for modelling distributed systems and business process workflows. This
field of modelling is, in essence, concurrent and resource-aware, motivating
the development of resource-aware formalisations on the active object model.
The contributions of this work are the development of a core calculus for
resource-aware active objects together with a type system ensuring that
well-typed programs are fairly terminating, i.e., they can always eventually
terminate. To achieve this, we combine techniques from graded semantics and
type systems, which are quite well understood for sequential programs, with
those for fair termination, which have been developed for synchronous~sessions.

</details>


### [17] [Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)](https://arxiv.org/abs/2508.15576)
*Andreas Lööw,Seung Hoon Park,Daniele Nantes-Sobrinho,Sacha-Élie Ayoun,Opale Sjöstedt,Philippa Gardner*

Main category: cs.PL

TL;DR: 该论文为基于内存模型参数化的组合符号执行（CSE）平台提供了新的正式基础，通过机械验证、支持多种内存模型、覆盖SL和ISL分析，并基于标准定义。


<details>
  <summary>Details</summary>
Motivation: 现有CSE工具虽能利用分离逻辑（SL）和不正确性分离逻辑（ISL）进行组合验证或错误查找，但缺乏对内存模型参数化的正式基础支持。

Method: 通过Rocq交互式定理证明器机械化基础，验证其对多种内存模型（如C和CHERI）的适用性，并扩展支持SL和ISL分析。

Result: 新框架在灵活性和性能上优于之前仅支持SL分析的参数化工作，且基于标准定义确保与其他工具的互操作性。

Conclusion: 该研究为内存模型参数化的CSE平台奠定了更全面的正式基础，提升了分析能力和工具兼容性。

Abstract: Multiple successful compositional symbolic execution (CSE) tools and
platforms exploit separation logic (SL) for compositional verification and/or
incorrectness separation logic (ISL) for compositional bug-finding, including
VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian
platform, the only CSE platform that is parametric on the memory model, meaning
that it can be instantiated to different memory models, suggests that the
ability to use custom memory models allows for more flexibility in supporting
analysis of a wide range of programming languages, for implementing custom
automation, and for improving performance. However, the literature lacks a
satisfactory formal foundation for memory-model-parametric CSE platforms.
  In this paper, inspired by Gillian, we provide a new formal foundation for
memory-model-parametric CSE platforms. Our foundation advances the state of the
art in four ways. First, we mechanise our foundation (in the interactive
theorem prover Rocq). Second, we validate our foundation by instantiating it to
a broad range of memory models, including models for C and CHERI. Third,
whereas previous memory-model-parametric work has only covered SL analyses, we
cover both SL and ISL analyses. Fourth, our foundation is based on standard
definitions of SL and ISL (including definitions of function specification
validity, to ensure sound interoperation with other tools and platforms also
based on standard definitions).

</details>


### [18] [Active Learning for Neurosymbolic Program Synthesis](https://arxiv.org/abs/2508.15750)
*Celeste Barnaby,Qiaochu Chen,Ramya Ramalingam,Osbert Bastani,Isil Dillig*

Main category: cs.PL

TL;DR: 本文提出了一种新型的主动学习技术，用于解决神经符号程序合成中神经网络误预测的问题，并开发了工具SmartLabel，实验显示其效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 神经符号程序合成中，神经网络误预测可能导致非预期程序的生成，现有主动学习技术无法有效解决这一问题。

Method: 提出基于约束顺应性评估（CCE）的新方法，通过用户反馈逐步提升CCE的精确性，确保最终生成程序与预期一致。

Result: 实验表明，SmartLabel在98%的基准测试中成功识别真实程序，平均交互轮次少于5次，显著优于现有技术（最高65%）。

Conclusion: CCE方法有效解决了神经符号程序合成中的误预测问题，显著提升了主动学习的效率和准确性。

Abstract: The goal of active learning for program synthesis is to synthesize the
desired program by asking targeted questions that minimize user interaction.
While prior work has explored active learning in the purely symbolic setting,
such techniques are inadequate for the increasingly popular paradigm of
neurosymbolic program synthesis, where the synthesized program incorporates
neural components. When applied to the neurosymbolic setting, such techniques
can -- and, in practice, do -- return an unintended program due to
mispredictions of neural components. This paper proposes a new active learning
technique that can handle the unique challenges posed by neural network
mispredictions. Our approach is based upon a new evaluation strategy called
constrained conformal evaluation (CCE), which accounts for neural
mispredictions while taking into account user-provided feedback. Our proposed
method iteratively makes CCE more precise until all remaining programs are
guaranteed to be observationally equivalent. We have implemented this method in
a tool called SmartLabel and experimentally evaluated it on three neurosymbolic
domains. Our results demonstrate that SmartLabel identifies the ground truth
program for 98% of the benchmarks, requiring under 5 rounds of user interaction
on average. In contrast, prior techniques for active learning are only able to
converge to the ground truth program for at most 65% of the benchmarks.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [19] [Mitigating context switching in densely packed Linux clusters with Latency-Aware Group Scheduling](https://arxiv.org/abs/2508.15703)
*Al Amjad Tawfiq Isstaif,Evangelia Kalyvianaki,Richard Mortier*

Main category: cs.OS

TL;DR: 论文研究了Kubernetes等集群编排工具中CPU上下文切换开销导致性能下降的问题，并提出改进Linux内核调度器的方案，减少28%的集群规模。


<details>
  <summary>Details</summary>
Motivation: 在高密度的无服务器应用中，CPU上下文切换的开销会导致节点性能显著下降，即使编排器理论上放置正确。

Method: 改进Linux内核调度器，优先完成任务而非低层级的任务公平性，以减少上下文切换时间。

Result: 改进后的调度器在相同性能下，可减少28%的集群规模。

Conclusion: 通过优化调度策略，可以有效降低上下文切换开销，提升集群资源利用率。

Abstract: Cluster orchestrators such as Kubernetes depend on accurate estimates of node
capacity and job requirements. Inaccuracies in either lead to poor placement
decisions and degraded cluster performance. In this paper, we show that in
densely packed workloads, such as serverless applications, CPU context
switching overheads can become so significant that a node's performance is
severely degraded, even when the orchestrator placement is theoretically sound.
In practice this issue is typically mitigated by over-provisioning the cluster,
leading to wasted resources.
  We show that these context switching overhead arise from both an increase in
the average cost of an individual context switch and a higher rate of context
switching, which together amplify overhead multiplicatively when managing large
numbers of concurrent cgroups, Linux's group scheduling mechanism for managing
multi-threaded colocated workloads. We propose and evaluate modifications to
the standard Linux kernel scheduler that mitigate these effects, achieving the
same effective performance with a 28% smaller cluster size. The key insight
behind our approach is to prioritise task completion over low-level per-task
fairness, enabling the scheduler to drain contended CPU run queues more rapidly
and thereby reduce time spent on context switching.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Toward Sustainable Subterranean mMTC: Space-Air-Ground-Underground Networks Powered by LoRaWAN and Wireless Energy Transfer](https://arxiv.org/abs/2508.15058)
*Kaiqiang Lin,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 论文提出了一种新型SAGUIN架构，整合了卫星、无人机、地面网络和地下通信，结合LoRaWAN和无线能量传输技术，解决了地下传感器网络资源匮乏和可持续通信的挑战。


<details>
  <summary>Details</summary>
Motivation: 地下无线传感器网络（WUSNs）在恶劣环境和资源限制下面临支持大规模机器通信（mMTC）的困难，尤其是在偏远和灾害地区。

Method: 提出SAGUIN架构，集成LoRaWAN和WET技术，通过建模和仿真评估其性能，优化时间和SF配置。

Result: 实验证明SAGUIN系统能有效延长设备寿命，支持可持续地下mMTC。

Conclusion: SAGUIN架构可行且有效，未来需进一步研究其挑战和改进方向。

Abstract: Wireless underground sensor networks (WUSNs), which enable real-time sensing
and monitoring of underground resources by underground devices (UDs), hold
great promise for delivering substantial social and economic benefits across
various verticals. However, due to the harsh subterranean environment, scarce
network resources, and restricted communication coverage, WUSNs face
significant challenges in supporting sustainable massive machine-type
communications (mMTC), particularly in remote, disaster-stricken, and
hard-to-reach areas. To complement this, we conceptualize in this study a novel
space-air-ground-underground integrated network (SAGUIN) architecture that
seamlessly incorporates satellite systems, aerial platforms, terrestrial
networks, and underground communications. On this basis, we integrate LoRaWAN
and wireless energy transfer (WET) technologies into SAGUIN to enable
sustainable subterranean mMTC. We begin by reviewing the relevant technical
background and presenting the architecture and implementation challenges of
SAGUIN. Then, we employ simulations to model a remote underground pipeline
monitoring scenario to evaluate the feasibility and performance of SAGUIN based
on LoRaWAN and WET technologies, focusing on the effects of parameters such as
underground conditions, time allocation, LoRaWAN spread factor (SF)
configurations, reporting periods, and harvested energy levels. Our results
evidence that the proposed SAGUIN system, when combined with the derived time
allocation strategy and an appropriate SF, can effectively extend the
operational lifetime of UDs, thereby facilitating sustainable subterranean
mMTC. Finally, we pinpoint key challenges and future research directions for
SAGUIN.

</details>


### [21] [From 5G RAN Queue Dynamics to Playback: A Performance Analysis for QUIC Video Streaming](https://arxiv.org/abs/2508.15087)
*Jashanjot Singh Sidhu,Jorge Ignacio Sandoval,Abdelhak Bentaleb,Sandra Cespedes*

Main category: cs.NI

TL;DR: 该研究分析了现代主动队列管理（AQM）策略（如RED和L4S）在不同QUIC实现中对5G网络下视频流传输质量体验（QoE）的影响，强调了跨层优化的重要性。


<details>
  <summary>Details</summary>
Motivation: 5G网络虽然支持超低延迟和高带宽，但移动网络中优化视频流QoE仍具挑战性，因为自适应比特率（ABR）方案、拥塞控制算法和链路层RLC队列之间复杂的交互作用。

Method: 通过综合分析方法，研究了AQM策略在不同QUIC实现中的交互作用，特别是它们与5G环境中RLC缓冲、拥塞控制算法及ABR方案的互动。

Result: 研究发现，AQM策略对视频流QoE的提升效果取决于其与QUIC实现、拥塞控制算法和ABR方案的动态交互，单一优化不足。

Conclusion: 为实现5G网络下高质量视频流传输，需要跨层自适应机制，实时协调网络、传输和应用层。

Abstract: The rapid adoption of QUIC as a transport protocol has transformed content
delivery by reducing latency, enhancing congestion control (CC), and enabling
more efficient multiplexing. With the advent of 5G networks, which support
ultra-low latency and high bandwidth, streaming high-resolution video at 4K and
beyond has become increasingly viable. However, optimizing Quality of
Experience (QoE) in mobile networks remains challenging due to the complex
interactions among Adaptive Bit Rate (ABR) schemes at the application layer, CC
algorithms at the transport layer, and Radio Link Control (RLC) queuing at the
link layer in the 5G network. While prior studies have largely examined these
components in isolation, this work presents a comprehensive analysis of the
impact of modern active queue management (AQM) strategies, such as RED and L4S,
on video streaming over diverse QUIC implementations--focusing particularly on
their interaction with the RLC buffer in 5G environments and the interplay
between CC algorithms and ABR schemes. Our findings demonstrate that the
effectiveness of AQM strategies in improving video streaming QoE is
intrinsically linked to their dynamic interaction with QUIC implementations, CC
algorithms and ABR schemes-highlighting that isolated optimizations are
insufficient. This intricate interdependence necessitates holistic, cross-layer
adaptive mechanisms capable of real-time coordination between network,
transport and application layers, which are crucial for fully leveraging the
capabilities of 5G networks to deliver robust, adaptive, and high-quality video
streaming.

</details>


### [22] [Toward Autonomous Digital Populations for Communication-Sensing-Computation Ecosystem](https://arxiv.org/abs/2508.15268)
*Gaosheng Zhao,Dong In Kim*

Main category: cs.NI

TL;DR: 本文提出了一种受自然启发的架构框架，利用数字孪生技术将边缘设备组织成功能数字群体，并通过云端多群体集成实现可演化数字生态系统。


<details>
  <summary>Details</summary>
Motivation: 当前通信网络依赖集中式控制、静态设计和人工干预，限制了功能和应用的多维发展，难以适应大规模、分层和复杂环境。

Method: 采用数字孪生技术组织边缘设备为数字群体，并通过云端多群体集成构建可演化生态系统。

Result: 该框架为下一代通信网络的动态协调、分布式决策、持续适应和进化能力奠定了理论基础。

Conclusion: 该结合工程方法与社会技术洞察的框架，有望推动未来通信网络的深度集成和自主运行。

Abstract: Future communication networks are expected to achieve deep integration of
communication, sensing, and computation, forming a tightly coupled and
autonomously operating infrastructure system. However, current reliance on
centralized control, static design, and human intervention continues to
constrain the multidimensional evolution of network functions and applications,
limiting adaptability and resilience in large-scale, layered, and complex
environments. To address these challenges, this paper proposes a
nature-inspired architectural framework that leverages digital twin technology
to organize connected devices at the edge into functional digital populations,
while enabling the emergence of an evolvable digital ecosystem through
multi-population integration at the cloud. We believe that this framework,
which combines engineering methodologies with sociotechnical insights, lays the
theoretical foundation for building next-generation communication networks with
dynamic coordination, distributed decision-making, continuous adaptation, and
evolutionary capabilities.

</details>


### [23] [Unlocking the Performance Potential of Mega-Constellation Networks: An Exploration of Structure-Building Paradigms](https://arxiv.org/abs/2508.15307)
*Xiangtong Wang,Wei Li,Menglong Yang,Songchen Han*

Main category: cs.NI

TL;DR: 论文提出了一种名为SML的新型网络结构设计范式，通过解耦局部模体设计和全局格子设计，解决了高可用性和低延迟的巨型星座网络设计问题，实验验证了其显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 巨型星座网络（MCN）需要设计稳定的网络控制结构，以确保未来空间无线通信网络的高可用性和低延迟传输。

Method: 采用SML（Structure = Motif + Lattice）范式，提出HALLMD问题，并开发SMLOP启发式算法，在多项式时间内优化网络结构。

Result: 实验表明，网络容量提升5~18%，吞吐量增加1~12%，路径拉伸减少12~23%，RTT降低8~77%。

Conclusion: SML范式能高效解决MCN的高可用性和低延迟设计问题，具有显著的实际应用价值。

Abstract: The network structure design plays a vital role in the mega-constellation
network (MSN) to coordinate massive network nodes to ensure the effectiveness
and reliability of operations and services for future space wireless
communications networks.
  One of the critical issues in MCN is how to design an optimal network control
structure by configuring the most stable inter-satellite link (ISL) to achieve
high available MCN within a limited average transmission delays.
  To address this problem, this paper introduces a novel MCN structure design
paradigm: Structure = Motif + Lattice (SML), which decouples MCN design into
local motifs design and global lattices design. Specifically, we formulate the
High-Availability and Low-Latency Mega-Constellation Design (HALLMD) problem,
aimed at maximizing ISL availability while minimizing the transmission latency.
To solve HALLMD, we propose SMLOP, a heuristic algorithm that efficiently finds
optimal network structures in polynomial time. Experimental validation on four
public state-of-the-art constellations demonstrates significant improvements,
including enhanced capacity by $5\sim 18\%$, increased throughput by $1\sim
12\%$, reduced path stretch by $12\sim 23\%$, and Round-Trip Time (RTT) by
$8\sim 77\%$.

</details>


### [24] [Interface on demand: Towards AI native Control interfaces for 6G](https://arxiv.org/abs/2508.15595)
*Abhishek Dandekar,Prashiddha D. Thapa,Ashrafur Rahman,Julius Schulz-Zander*

Main category: cs.NI

TL;DR: 提出基于LLM的多智能体框架，动态生成网络控制接口，解决传统标准化接口的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统网络接口存在厂商不兼容、设计僵化和缺乏适应性等问题，需要更灵活的解决方案。

Method: 使用匹配智能体对齐功能需求与能力，代码生成智能体创建API服务器，并在多厂商环境中验证。

Result: 性能评估展示了成本与延迟的权衡，验证了框架的可行性。

Conclusion: 该工作为AI原生动态控制接口生成奠定基础，提升未来移动网络的互操作性和适应性。

Abstract: Traditional standardized network interfaces face significant limitations,
including vendor-specific incompatibilities, rigid design assumptions, and lack
of adaptability for new functionalities. We propose a multi-agent framework
leveraging large language models (LLMs) to generate control interfaces on
demand between network functions (NFs). This includes a matching agent, which
aligns required control functionalities with NF capabilities, and a
code-generation agent, which generates the necessary API server for interface
realization. We validate our approach using simulated multi-vendor gNB and WLAN
AP environments. The performance evaluations highlight the trade-offs between
cost and latency across LLMs for interface generation tasks. Our work sets the
foundation for AI-native dynamic control interface generation, paving the way
for enhanced interoperability and adaptability in future mobile networks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [25] [Robust Symbolic Reasoning for Visual Narratives via Hierarchical and Semantically Normalized Knowledge Graphs](https://arxiv.org/abs/2508.14941)
*Yi-Chun Chen*

Main category: cs.MM

TL;DR: 论文提出了一个语义标准化框架，用于解决视觉叙事中符号叙事图的不一致和冗余问题，通过在多层次叙事图中整合语义相关动作和事件，提高了叙事的连贯性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉叙事（如漫画）需要结构化表示，但现有符号叙事图存在不一致和冗余问题，限制了推理和泛化能力。

Method: 基于认知基础模型，提出语义标准化框架，利用词相似性和嵌入聚类整合语义相关动作和事件。

Result: 在Manga109数据集上应用该框架，标准化后的叙事图在动作检索、角色定位和事件摘要等任务中表现更优。

Conclusion: 语义标准化是构建可扩展、认知启发的多模态叙事理解模型的关键步骤。

Abstract: Understanding visual narratives such as comics requires structured
representations that capture events, characters, and their relations across
multiple levels of story organization. However, symbolic narrative graphs often
suffer from inconsistency and redundancy, where similar actions or events are
labeled differently across annotations or contexts. Such variance limits the
effectiveness of reasoning and generalization.
  This paper introduces a semantic normalization framework for hierarchical
narrative knowledge graphs. Building on cognitively grounded models of
narrative comprehension, we propose methods that consolidate semantically
related actions and events using lexical similarity and embedding-based
clustering. The normalization process reduces annotation noise, aligns symbolic
categories across narrative levels, and preserves interpretability.
  We demonstrate the framework on annotated manga stories from the Manga109
dataset, applying normalization to panel-, event-, and story-level graphs.
Preliminary evaluations across narrative reasoning tasks, such as action
retrieval, character grounding, and event summarization, show that semantic
normalization improves coherence and robustness, while maintaining symbolic
transparency. These findings suggest that normalization is a key step toward
scalable, cognitively inspired graph models for multimodal narrative
understanding.

</details>


### [26] [Holo-Artisan: A Personalized Multi-User Holographic Experience for Virtual Museums on the Edge Intelligence](https://arxiv.org/abs/2508.14956)
*Nan-Hong Kuo,Hojjat Baghban*

Main category: cs.MM

TL;DR: Holo-Artisan是一个创新的系统架构，通过全息显示和边缘智能实现多人沉浸式虚拟博物馆体验，支持个性化互动和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 将静态博物馆展品转化为动态、互动的艺术作品，提升用户体验并保护隐私。

Method: 利用边缘计算和生成式AI模型实时处理用户数据，结合云协作平台和光追技术渲染个性化视图。

Result: 实现了多用户实时交互的沉浸式体验，同时通过联邦学习保护用户隐私。

Conclusion: Holo-Artisan为文化遗产互动开辟了新范式。

Abstract: We present Holo-Artisan, a novel system architecture enabling immersive
multi-user experiences in virtual museums through true holographic displays and
personalized edge intelligence. In our design, local edge computing nodes
process real-time user data -- including pose, facial expression, and voice --
for multiple visitors concurrently. Generative AI models then drive digital
artworks (e.g., a volumetric Mona Lisa) to respond uniquely to each viewer. For
instance, the Mona Lisa can return a smile to one visitor while engaging in a
spoken Q\&A with another, all in real time. A cloud-assisted collaboration
platform composes these interactions in a shared scene using a universal scene
description, and employs ray tracing to render high-fidelity, personalized
views with a direct pipeline to glasses-free holographic displays. To preserve
user privacy and continuously improve personalization, we integrate federated
learning (FL) -- edge devices locally fine-tune AI models and share only model
updates for aggregation. This edge-centric approach minimizes latency and
bandwidth usage, ensuring a synchronized shared experience with individual
customization. Through Holo-Artisan, static museum exhibits are transformed
into dynamic, living artworks that engage each visitor in a personal dialogue,
heralding a new paradigm of cultural heritage interaction.

</details>


### [27] [\textit{adder-viz}: Real-Time Visualization Software for Transcoding Event Video](https://arxiv.org/abs/2508.14996)
*Andrew C. Freeman,Luke Reinkensmeyer*

Main category: cs.MM

TL;DR: 该论文介绍了对事件视频可视化软件《adder-viz》的改进，以提升实时事件转码过程和应用的可视化效果。


<details>
  <summary>Details</summary>
Motivation: 针对神经形态事件视频研究中现有表示的局限性（如灵活性、速度和可压缩性），作者之前提出了统一的ADΔER表示方法，本文旨在进一步优化相关软件工具。

Method: 通过改进《adder-viz》软件，提供实时事件转码过程的可视化功能，并支持应用程序的循环调试。

Result: 改进后的软件已开源，可在GitHub中央仓库获取。

Conclusion: 优化后的工具提升了事件视频处理的实际应用能力，为研究和开发提供了便利。

Abstract: Recent years have brought about a surge in neuromorphic ``event'' video
research, primarily targeting computer vision applications. Event video eschews
video frames in favor of asynchronous, per-pixel intensity samples. While much
work has focused on a handful of representations for specific event cameras,
these representations have shown limitations in flexibility, speed, and
compressibility. We previously proposed the unified AD{\Delta}ER representation
to address these concerns. This paper introduces numerous improvements to the
\textit{adder-viz} software for visualizing real-time event transcode processes
and applications in-the-loop. The MIT-licensed software is available from a
centralized repository at
\href{https://github.com/ac-freeman/adder-codec-rs}{https://github.com/ac-freeman/adder-codec-rs}.

</details>


### [28] [A Low-Latency 3D Live Remote Visualization System for Tourist Sites Integrating Dynamic and Pre-captured Static Point Clouds](https://arxiv.org/abs/2508.15398)
*Takahiro Matsumoto,Masafumi Suzuki,Mariko Yamaguchi,Masakatsu Aoki,Shunsuke Konagai,Kazuhiko Murasaki*

Main category: cs.MM

TL;DR: 提出了一种结合LiDAR和摄像头的实时动态点云捕捉系统，用于户外旅游景点的3D可视化，解决了现有方法在户外应用中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在户外旅游景点应用时受限于传感器放置和光照变化，需要一种更灵活高效的解决方案。

Method: 结合多LiDAR和摄像头进行动态点云捕捉，并与预捕获的静态点云整合，自动调整静态点云颜色以适应光照变化。

Result: 系统在30 fps下实现宽场景实时捕捉，延迟低于100 ms，并在实际旅游景点中验证了有效性。

Conclusion: 该系统为户外景点提供了一种高效的实时3D可视化方案，克服了光照和传感器放置的挑战。

Abstract: Various real-time methods for capturing and transmitting dynamic 3D spaces
have been proposed, including those based on RGB-D cameras and volumetric
capture. However, applying existing methods to outdoor tourist sites remains
difficult because maintenance and aesthetic constraints limit sensor placement,
and daylight variability complicates processing. We propose a system that
combines multiple LiDARs and cameras for live dynamic point cloud capture, and
integrates them with pre-captured static point clouds for wide-area 3D
visualization. The system sustains 30 fps across wide-area scenes while keeping
latency below 100 ms. To mitigate lighting inconsistencies, static point-cloud
colors are automatically adjusted to current lighting. The effectiveness of our
system is demonstrated through real-world deployment in a tourist site.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [29] [LitForager: Exploring Multimodal Literature Foraging Strategies in Immersive Sensemaking](https://arxiv.org/abs/2508.15043)
*Haoyang Yang,Elliott H. Faa,Weijian Liu,Shunan Guo,Duen Horng Chau,Yalong Yang*

Main category: cs.HC

TL;DR: LitForager是一个沉浸式文献探索工具，旨在通过网络可视化和多模态交互支持文献的发现和整理，填补了沉浸式环境中信息搜集的空白。


<details>
  <summary>Details</summary>
Motivation: 研究文献的快速扩张使得文献探索和理解成为研究者的重要但具挑战性的任务。现有的沉浸式工具主要关注信息合成，而忽略了信息搜集（foraging）的重要性。

Method: 开发了LitForager工具，基于WebXR技术，采用网络可视化和多模态交互，支持文献的空间组织和流畅探索。

Result: 用户研究表明，LitForager通过多模态界面有效支持了文献的发现和空间理解。

Conclusion: LitForager填补了沉浸式环境中文献搜集的空白，为完整的信息理解流程提供了支持。

Abstract: Exploring and comprehending relevant academic literature is a vital yet
challenging task for researchers, especially given the rapid expansion in
research publications. This task fundamentally involves sensemaking -
interpreting complex, scattered information sources to build understanding.
While emerging immersive analytics tools have shown cognitive benefits like
enhanced spatial memory and reduced mental load, they predominantly focus on
information synthesis (e.g., organizing known documents). In contrast, the
equally important information foraging phase - discovering and gathering
relevant literature - remains underexplored within immersive environments,
hindering a complete sensemaking workflow. To bridge this gap, we introduce
LitForager, an interactive literature exploration tool designed to facilitate
information foraging of research literature within an immersive sensemaking
workflow using network-based visualizations and multimodal interactions.
Developed with WebXR and informed by a formative study with researchers,
LitForager supports exploration guidance, spatial organization, and seamless
transition through a 3D literature network. An observational user study with 15
researchers demonstrated LitForager's effectiveness in supporting fluid
foraging strategies and spatial sensemaking through its multimodal interface.

</details>


### [30] [Understanding Accessibility Needs of Blind Authors on CMS-Based Websites](https://arxiv.org/abs/2508.15045)
*Guillermo Vera-Amaro,José Rafael Rojano-Cáceres*

Main category: cs.HC

TL;DR: 论文探讨了盲人用户在内容管理系统（CMS）中作为内容创作者的困境，指出现有研究对盲人用户的可访问性关注不足，并提出了改进方案。


<details>
  <summary>Details</summary>
Motivation: 现有CMS平台对盲人用户的可访问性设计不足，尤其是盲人作为内容创作者时面临的界面交互问题。研究旨在填补这一空白。

Method: 研究采用自动化工具和手动可用性测试（包括三名盲人和一名正常人参与者），辅以基于Barrier Walkthrough方法的专家分析，评估盲人用户在CMS中的关键任务（如页面创建、菜单编辑等）的可访问性。

Result: 研究发现基于块的界面存在严重可用性问题，尽管自动化工具认为其可访问。使用文本编辑器、AI生成图像描述及针对屏幕阅读器工作流程的培训能显著提升盲人用户的可用性和独立性。

Conclusion: 研究揭示了自动化评估的局限性，强调了以用户为中心的设计实践的重要性。改进CMS可访问性需优化导航结构、减少对视觉交互模式的依赖，并整合AI工具以支持盲人创作者。

Abstract: This paper addresses the limited attention given to blind users as content
creators in Content Management Systems (CMS), a gap that remains under-explored
in web accessibility research. For blind authors, effective interaction with
CMS platforms requires more than technical compliance; it demands interfaces
designed with semantic clarity, predictable navigation, and meaningful feedback
for screen reader users. This study investigates the accessibility barriers
blind users face when performing key tasks, such as page creation, menu
editing, and image publishing, using CMS platforms. A two-fold evaluation was
conducted using automated tools and manual usability testing with three blind
and one sighted participant, complemented by expert analysis based on the
Barrier Walkthrough method. Results showed that block-based interfaces were
particularly challenging, often marked as accessible by automated tools but
resulting in critical usability issues during manual evaluation. The use of a
text-based editor, the integration of AI-generated image descriptions, and
training aligned with screen reader workflows, significantly improved usability
and autonomy. These findings underscore the limitations of automated
assessments and highlight the importance of user-centered design practices.
Enhancing CMS accessibility requires consistent navigation structures, reduced
reliance on visual interaction patterns, and the integration of AI tools that
support blind content authors throughout the content creation process.

</details>


### [31] [QueryGenie: Making LLM-Based Database Querying Transparent and Controllable](https://arxiv.org/abs/2508.15146)
*Longfei Chen,Shenghan Gao,Shiwei Wang,Ken Lin,Yun Wang,Quan Li*

Main category: cs.HC

TL;DR: QueryGenie是一个交互式系统，通过增量推理和实时验证解决LLM驱动查询中的误解和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具在用户意图误解、内容幻觉及反馈机制缺失方面存在问题，影响了可靠性和实用性。

Method: 提出QueryGenie系统，支持用户通过增量推理、实时验证和交互机制监控和指导查询生成。

Result: 用户能迭代优化查询逻辑，确保其与意图一致。

Conclusion: QueryGenie提升了LLM驱动查询的透明性和可控性。

Abstract: Conversational user interfaces powered by large language models (LLMs) have
significantly lowered the technical barriers to database querying. However,
existing tools still encounter several challenges, such as misinterpretation of
user intent, generation of hallucinated content, and the absence of effective
mechanisms for human feedback-all of which undermine their reliability and
practical utility. To address these issues and promote a more transparent and
controllable querying experience, we proposed QueryGenie, an interactive system
that enables users to monitor, understand, and guide the LLM-driven query
generation process. Through incremental reasoning, real-time validation, and
responsive interaction mechanisms, users can iteratively refine query logic and
ensure alignment with their intent.

</details>


### [32] [ReviseMate: Exploring Contextual Support for Digesting STEM Paper Reviews](https://arxiv.org/abs/2508.15148)
*Yuansong Xu,Shuhao Zhang,Yijie Fan,Shaohan Shi,Zhenhui Peng,Quan Li*

Main category: cs.HC

TL;DR: ReviseMate是一个交互式系统，旨在帮助研究者更高效地消化和整合审稿人的反馈。通过用户研究和实地部署，证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统审稿反馈消化过程存在耗时、疲劳和需要高分析能力等问题，现有工具缺乏针对性支持，因此需要更有效的解决方案。

Method: 通过访谈和故事板设计，开发了ReviseMate系统，并进行用户研究（N=31）和实地部署（N=6）验证其效果。

Result: ReviseMate在用户交互和实际应用场景中表现优异，显著提升了审稿反馈的消化和整合效率。

Conclusion: 交互式工具在提升审稿反馈处理效率方面具有重要潜力。

Abstract: Effectively assimilating and integrating reviewer feedback is crucial for
researchers seeking to refine their papers and handle potential rebuttal phases
in academic venues. However, traditional review digestion processes present
challenges such as time consumption, reading fatigue, and the requisite for
comprehensive analytical skills. Prior research on review analysis often
provides theoretical guidance with limited targeted support. Additionally,
general text comprehension tools overlook the intricate nature of
comprehensively understanding reviews and lack contextual assistance. To bridge
this gap, we formulated research questions to explore the authors' concerns and
methods for enhancing comprehension during the review digestion phase. Through
interviews and the creation of storyboards, we developed ReviseMate, an
interactive system designed to address the identified challenges. A controlled
user study (N=31) demonstrated the superiority of ReviseMate over baseline
methods, with positive feedback regarding user interaction. Subsequent field
deployment (N=6) further validated the effectiveness of ReviseMate in
real-world review digestion scenarios. These findings underscore the potential
of interactive tools to significantly enhance the assimilation and integration
of reviewer feedback during the manuscript review process.

</details>


### [33] [Evaluating an Immersive Analytics Application at an Enterprise Business Intelligence Customer Conference](https://arxiv.org/abs/2508.15152)
*Matthew Brehmer,Ginger Gloystein,Bailiang Zhou,Abby Gray,Sruthi Pillai,Ben Medina,Vidya Setlur*

Main category: cs.HC

TL;DR: 这篇论文评估了在企业BI会议上对Tableau for visionOS应用的沉浸式分析体验，强调了在评估实用性与新颖性之间的挑战，并提出了新的评估方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是在企业环境中评估沉浸式分析应用的可用性和潜在效用，探讨如何在真实场景中平衡新颖性与实用性。

Method: 采用形成性评估方法，对22名参与者进行了研究，收集了关于Tableau for visionOS的反馈以及对HMD在未来BI中潜力的看法。

Result: 研究结果表明，需要结合定性与定量方法，考虑HMD与3D交互模式的特点，并提出针对沉浸式分析的新评估框架。

Conclusion: 研究为企业视角下的沉浸式分析评估方法提供了贡献，强调了在实用性与新颖性之间找到平衡的重要性。

Abstract: We reflect on an evaluation of an immersive analytics application (Tableau
for visionOS) conducted at a large enterprise business intelligence (BI)
conference. Conducting a study in such a context offered an opportunistic
setting to gather diverse feedback. However, this setting also highlighted the
challenge of evaluating usability while also assessing potential utility, as
feedback straddled between the novelty of the experience and the practicality
of the application in participants' analytical workflows. This formative
evaluation with 22 participants allowed us to gather insights with respect to
the usability of Tableau for visionOS, along with broader perspectives on the
potential for head-mounted displays (HMDs) to promote new ways to engage with
BI data. Our experience suggests a need for new evaluation considerations that
integrate qualitative and quantitative measures and account for unique
interaction patterns with 3D representations and interfaces accessible via an
HMD. Overall, we contribute an enterprise perspective on evaluation
methodologies for immersive analytics.

</details>


### [34] [GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design](https://arxiv.org/abs/2508.15227)
*Wen-Fan Wang,Ting-Ying Lee,Chien-Ting Lu,Che-Wei Hsu,Nil Ponsa Campany,Yu Chen,Mike Y. Chen,Bing-Yu Chen*

Main category: cs.HC

TL;DR: GenTune通过改进AI生成的提示与图像内容的映射关系，增强了设计师在图像生成和编辑中的精确度和全局一致性。


<details>
  <summary>Details</summary>
Motivation: 设计师在使用生成式AI时面临提示冗长和全局一致性差的问题，需要更好的工具来优化工作流程。

Method: 提出GenTune系统，允许设计师通过选择和修改提示标签来精确控制图像内容，保持全局一致性。

Result: 在20名设计师的评估中，GenTune显著提高了提示理解、编辑质量和效率（p<0.01），并在实际应用中验证了其效果。

Conclusion: GenTune有效解决了设计师在使用生成式AI时的关键问题，提升了工作效率和满意度。

Abstract: Environment designers in the entertainment industry create imaginative 2D and
3D scenes for games, films, and television, requiring both fine-grained control
of specific details and consistent global coherence. Designers have
increasingly integrated generative AI into their workflows, often relying on
large language models (LLMs) to expand user prompts for text-to-image
generation, then iteratively refining those prompts and applying inpainting.
However, our formative study with 10 designers surfaced two key challenges: (1)
the lengthy LLM-generated prompts make it difficult to understand and isolate
the keywords that must be revised for specific visual elements; and (2) while
inpainting supports localized edits, it can struggle with global consistency
and correctness. Based on these insights, we present GenTune, an approach that
enhances human--AI collaboration by clarifying how AI-generated prompts map to
image content. Our GenTune system lets designers select any element in a
generated image, trace it back to the corresponding prompt labels, and revise
those labels to guide precise yet globally consistent image refinement. In a
summative study with 20 designers, GenTune significantly improved prompt--image
comprehension, refinement quality, and efficiency, and overall satisfaction
(all $p < .01$) compared to current practice. A follow-up field study with two
studios further demonstrated its effectiveness in real-world settings.

</details>


### [35] [Visualization on Smart Wristbands: Results from an In-situ Design Workshop with Four Scenarios](https://arxiv.org/abs/2508.15249)
*Alaul Islam,Fairouz Grioui,Raimund Dachselt,Petra Isenberg*

Main category: cs.HC

TL;DR: 论文研究了智能手环的数据可视化设计，针对不同使用场景（办公、散步、骑行、驾驶）中手臂姿势的差异，提出了自适应的可视化设计方案。


<details>
  <summary>Details</summary>
Motivation: 智能手环的可视化设计面临挑战，因为手腕不同区域的可见性随手臂姿势变化而变化。研究者希望通过设计适应不同姿势的可视化方案，提升用户体验。

Method: 通过纸面构思练习，模拟智能手环的可视化设计，探讨数据项目类型和手臂姿势对空间布局和设计的影响。

Result: 参与者更倾向于能够随手臂运动自适应的可视化设计。

Conclusion: 智能手环的可视化设计需要考虑动态姿势变化，未来技术实现时应支持自适应布局。

Abstract: We present the results of an in-situ ideation workshop for designing data
visualizations on smart wristbands that can show data around the entire wrist
of a wearer. Wristbands pose interesting challenges because the visibility of
different areas of the band depends on the wearer's arm posture. We focused on
four usage scenarios that lead to different postures: office work, leisurely
walks, cycling, and driving. As the technology for smart wristbands is not yet
commercially available, we conducted a paper-based ideation exercise that
showed how spatial layout and visualization design on smart wristbands may need
to vary depending on the types of data items of interest and arm postures.
Participants expressed a strong preference for responsive visualization designs
that could adapt to the movement of wearers' arms. Supplemental material from
the study is available here: https://osf.io/4hrca/.

</details>


### [36] [Spatio-Temporal Mixed and Augmented Reality Experience Description for Interactive Playback](https://arxiv.org/abs/2508.15258)
*Dooyoung Kim,Woontack Woo*

Main category: cs.HC

TL;DR: 提出了一种名为MAR-ED的新框架，用于标准化描述过去事件的时空混合现实体验，以支持互动和自适应回放。


<details>
  <summary>Details</summary>
Motivation: 当前的空间媒体技术主要关注静态内容的捕捉或回放，缺乏对体验的语义和互动结构的描述，因此需要一种新的框架来解决这一问题。

Method: 基于三个核心原语（事件原语、关键帧原语和回放原语），构建了MAR-ED框架，支持从记录到自适应回放的三阶段流程。

Result: MAR-ED框架能够将记录的体验转化为自适应的混合现实体验，动态适应新环境并支持用户实时输入。

Conclusion: MAR-ED为个人数字记忆和记录事件提供了新的范式，扩展了其在训练、文化遗产和互动叙事中的应用潜力。

Abstract: We propose the Spatio-Temporal Mixed and Augmented Reality Experience
Description (MAR-ED), a novel framework to standardize the representation of
past events for interactive and adaptive playback in a user's present physical
space. While current spatial media technologies have primarily focused on
capturing or replaying content as static assets, often disconnected from the
viewer's environment or offering limited interactivity, the means to describe
an experience's underlying semantic and interactive structure remains
underexplored. We propose a descriptive framework called MAR-ED based on three
core primitives: 1) Event Primitives for semantic scene graph representation,
2) Keyframe Primitives for efficient and meaningful data access, and 3)
Playback Primitives for user-driven adaptive interactive playback of recorded
MAR experience. The proposed flowchart of the three-stage process of the
proposed MAR-ED framework transforms a recorded experience into a unique
adaptive MAR experience during playback, where its spatio-temporal structure
dynamically conforms to a new environment and its narrative can be altered by
live user input. Drawing on this framework, personal digital memories and
recorded events can evolve beyond passive 2D/3D videos into immersive,
spatially-integrated group experiences, opening new paradigms for training,
cultural heritage, and interactive storytelling without requiring complex,
per-user adaptive rendering.

</details>


### [37] [Foundation Models for Cross-Domain EEG Analysis Application: A Survey](https://arxiv.org/abs/2508.15716)
*Hongqi Li,Yitong Chen,Yujuan Wang,Weihang Ni,Haodong Zhang*

Main category: cs.HC

TL;DR: 该论文提出了一种基于模态的分类法，系统整理并分析了EEG分析中的基础模型，旨在解决当前研究碎片化的问题，并推动其向可扩展和可解释的方向发展。


<details>
  <summary>Details</summary>
Motivation: 针对EEG分析中基础模型研究分散、架构不一致且缺乏系统分类的问题，本研究旨在填补这一空白。

Method: 研究提出了一种面向模态的分类法，根据EEG解码的输出模态（如EEG-文本、EEG-视觉等）系统梳理研究进展。

Result: 论文对各类别的研究思路、理论基础和创新架构进行了深入分析，并指出了模型可解释性、跨领域泛化等挑战。

Conclusion: 通过整合分散的研究领域，该研究为未来方法开发提供了参考框架，并促进了EEG基础模型向实际应用的转化。

Abstract: Electroencephalography (EEG) analysis stands at the forefront of neuroscience
and artificial intelligence research, where foundation models are reshaping the
traditional EEG analysis paradigm by leveraging their powerful representational
capacity and cross-modal generalization. However, the rapid proliferation of
these techniques has led to a fragmented research landscape, characterized by
diverse model roles, inconsistent architectures, and a lack of systematic
categorization. To bridge this gap, this study presents the first comprehensive
modality-oriented taxonomy for foundation models in EEG analysis,
systematically organizing research advances based on output modalities of the
native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal
frameworks. We rigorously analyze each category's research ideas, theoretical
foundations, and architectural innovations, while highlighting open challenges
such as model interpretability, cross-domain generalization, and real-world
applicability in EEG-based systems. By unifying this dispersed field, our work
not only provides a reference framework for future methodology development but
accelerates the translation of EEG foundation models into scalable,
interpretable, and online actionable solutions.

</details>


### [38] [Demystifying Reward Design in Reinforcement Learning for Upper Extremity Interaction: Practical Guidelines for Biomechanical Simulations in HCI](https://arxiv.org/abs/2508.15727)
*Hannah Selder,Florian Fischer,Per Ola Kristensson,Arthur Fleig*

Main category: cs.HC

TL;DR: 论文探讨了如何设计有效的奖励函数以优化基于强化学习的生物力学模拟，提出了三种奖励组件的影响，并提供了实用的设计指南。


<details>
  <summary>Details</summary>
Motivation: HCI研究人员在奖励函数设计上常需通过试错调整，计算资源浪费严重。本文旨在简化这一过程，提升模拟效率和实用性。

Method: 通过系统分析努力最小化、任务完成奖励和目标接近激励三种组件对典型HCI任务（如指向、跟踪和选择反应）的影响，研究其权重对任务成功和完成时间的敏感性。

Result: 研究发现接近激励对引导运动至关重要，完成奖励确保任务成功，努力项则能优化运动规律性。基于此，提供了无需强化学习专业知识的设计指南，并在实际任务中验证。

Conclusion: 本文提出的方法显著提升了生物力学模拟的效率和实用性，推动了HCI中基于模拟的交互设计与评估。

Abstract: Designing effective reward functions is critical for reinforcement
learning-based biomechanical simulations, yet HCI researchers and practitioners
often waste (computation) time with unintuitive trial-and-error tuning. This
paper demystifies reward function design by systematically analyzing the impact
of effort minimization, task completion bonuses, and target proximity
incentives on typical HCI tasks such as pointing, tracking, and choice
reaction. We show that proximity incentives are essential for guiding movement,
while completion bonuses ensure task success. Effort terms, though optional,
help refine motion regularity when appropriately scaled. We perform an
extensive analysis of how sensitive task success and completion time depend on
the weights of these three reward components. From these results we derive
practical guidelines to create plausible biomechanical simulations without the
need for reinforcement learning expertise, which we then validate on remote
control and keyboard typing tasks. This paper advances simulation-based
interaction design and evaluation in HCI by improving the efficiency and
applicability of biomechanical user modeling for real-world interface
development.

</details>


### [39] ["Does the cafe entrance look accessible? Where is the door?" Towards Geospatial AI Agents for Visual Inquiries](https://arxiv.org/abs/2508.15752)
*Jon E. Froehlich,Jared Hwang,Zeyu Wang,John S. O'Meara,Xia Su,William Huang,Yang Zhang,Alex Fiannaca,Philip Nelson,Shaun Kane*

Main category: cs.HC

TL;DR: 提出了Geo-Visual Agents的愿景，这是一种多模态AI代理，能够通过分析大规模地理空间图像和传统GIS数据来回答关于世界的视觉空间问题。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式数字地图依赖预结构化GIS数据，难以解决与视觉相关的地理问题。

Method: 结合多模态AI技术，分析街道景观、地标照片和航拍图像等地理空间数据源。

Result: 提出了Geo-Visual Agents的概念，并展示了三种示例。

Conclusion: Geo-Visual Agents有望解决传统地理信息系统的视觉局限，并提出了未来工作的挑战和机遇。

Abstract: Interactive digital maps have revolutionized how people travel and learn
about the world; however, they rely on pre-existing structured data in GIS
databases (e.g., road networks, POI indices), limiting their ability to address
geo-visual questions related to what the world looks like. We introduce our
vision for Geo-Visual Agents--multimodal AI agents capable of understanding and
responding to nuanced visual-spatial inquiries about the world by analyzing
large-scale repositories of geospatial images, including streetscapes (e.g.,
Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial
imagery (e.g., satellite photos) combined with traditional GIS data sources. We
define our vision, describe sensing and interaction approaches, provide three
exemplars, and enumerate key challenges and opportunities for future work.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [40] [Hybrelighter: Combining Deep Anisotropic Diffusion and Scene Reconstruction for On-device Real-time Relighting in Mixed Reality](https://arxiv.org/abs/2508.14930)
*Hanwen Zhao,John Akers,Baback Elmieh,Ira Kemelmacher-Shlizerman*

Main category: cs.GR

TL;DR: 提出了一种混合现实场景重照明方法，结合图像分割和各向异性扩散的光照传播，实现实时高帧率效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法无法实时运行，场景理解方法准确性不足，2D滤镜方法无法处理复杂几何。

Method: 集成图像分割、各向异性扩散光照传播与基础场景理解，结合滤镜技术。

Result: 在边缘设备上实现100 fps的实时重照明效果，校正扫描误差。

Conclusion: 该方法在真实场景（如房地产）中有效且优于行业标准。

Abstract: Mixed Reality scene relighting, where virtual changes to lighting conditions
realistically interact with physical objects, producing authentic illumination
and shadows, can be used in a variety of applications. One such application in
real estate could be visualizing a room at different times of day and placing
virtual light fixtures. Existing deep learning-based relighting techniques
typically exceed the real-time performance capabilities of current MR devices.
On the other hand, scene understanding methods, such as on-device scene
reconstruction, often yield inaccurate results due to scanning limitations, in
turn affecting relighting quality. Finally, simpler 2D image filter-based
approaches cannot represent complex geometry and shadows. We introduce a novel
method to integrate image segmentation, with lighting propagation via
anisotropic diffusion on top of basic scene understanding, and the
computational simplicity of filter-based techniques. Our approach corrects
on-device scanning inaccuracies, delivering visually appealing and accurate
relighting effects in real-time on edge devices, achieving speeds as high as
100 fps. We show a direct comparison between our method and the industry
standard, and present a practical demonstration of our method in the
aforementioned real estate example.

</details>


### [41] [Inference Time Debiasing Concepts in Diffusion Models](https://arxiv.org/abs/2508.14933)
*Lucas S. Kupssinskü,Marco N. Bochernitsan,Jordan Kopper,Otávio Parraga,Rodrigo C. Barros*

Main category: cs.GR

TL;DR: DeCoDi是一种用于文本到图像扩散模型的去偏方法，通过调整推理过程减少潜在维度的偏见概念，不影响图像质量且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 旨在解决扩散模型中存在的性别、种族和年龄等偏见问题，提供一种简单高效的去偏方法。

Method: 通过改变扩散过程避免偏见概念的潜在维度区域，仅调整推理过程，无需复杂或计算密集型干预。

Result: 在性别、种族和年龄方面的去偏实验中表现有效，人工评估1200张图像证实其效果，且与GPT4o的自动评估结果一致。

Conclusion: DeCoDi能显著提升扩散模型生成图像的多样性，是一种易于普及的去偏解决方案。

Abstract: We propose DeCoDi, a debiasing procedure for text-to-image diffusion-based
models that changes the inference procedure, does not significantly change
image quality, has negligible compute overhead, and can be applied in any
diffusion-based image generation model. DeCoDi changes the diffusion process to
avoid latent dimension regions of biased concepts. While most deep learning
debiasing methods require complex or compute-intensive interventions, our
method is designed to change only the inference procedure. Therefore, it is
more accessible to a wide range of practitioners. We show the effectiveness of
the method by debiasing for gender, ethnicity, and age for the concepts of
nurse, firefighter, and CEO. Two distinct human evaluators manually inspect
1,200 generated images. Their evaluation results provide evidence that our
method is effective in mitigating biases based on gender, ethnicity, and age.
We also show that an automatic bias evaluation performed by the GPT4o is not
significantly statistically distinct from a human evaluation. Our evaluation
shows promising results, with reliable levels of agreement between evaluators
and more coverage of protected attributes. Our method has the potential to
significantly improve the diversity of images it generates by diffusion-based
text-to-image generative models.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [42] [Money in Motion: Micro-Velocity and Usage of Ethereums Liquid Staking Tokens](https://arxiv.org/abs/2508.15391)
*Benjamin Kraner,Luca Pennella,Nicolò Vallarano,Claudio J. Tessone*

Main category: cs.ET

TL;DR: 该论文提出了一个用于分析流动性质押代币（stETH 和 wstETH）链上流通的微速度框架，揭示了其高速度和高集中度的特点，并观察到用户行为逐渐转向非复利形式的 wstETH。


<details>
  <summary>Details</summary>
Motivation: 研究流动性质押代币（LSTs）的微观货币动态，填补了这一领域的研究空白。

Method: 通过重构完整的转账和份额会计历史，计算地址级速度并分解其行为组成部分。

Result: 发现两种代币的流通速度持续高企，且活动高度集中于少数大型地址；用户行为逐渐转向非复利形式的 wstETH。

Conclusion: 研究提供了流动性质押代币流通的首个大规模实证分析，并为质押资产流动监测提供了可扩展的模板和开放资源。

Abstract: We introduce a micro-velocity framework for analysing the on-chain
circulation of Lidos liquid-staking tokens, stETH, and its wrapped ERC-20 form,
wstETH. By reconstructing full transfer and share-based accounting histories,
we compute address-level velocities and decompose them into behavioural
components. Despite their growing importance, the micro-level monetary dynamics
of LSTs remain largely unexplored. Our data reveal persistently high velocity
for both tokens, reflecting intensive reuse within DeFi. Yet activity is highly
concentrated: a small cohort of large addresses, likely institutional accounts,
are responsible for most turnover, while the rest of the users remain largely
passive. We also observe a gradual transition in user behavior, characterized
by a shift toward wstETH, the non-rebasing variant of stETH. This shift appears
to align with DeFi composability trends, as wstETH is more frequently deployed
across protocols such as AAVE, Spark, Balancer, and SkyMoney.
  To make the study fully reproducible, we release (i) an open-source pipeline
that indexes event logs and historical contract state, and (ii) two public
datasets containing every Transfer and TransferShares record for stETH and
wstETH through 2024-11-08. This is the first large-scale empirical
characterisation of liquid-staking token circulation. Our approach offers a
scalable template for monitoring staking asset flows and provides new,
open-access resources to the research community.

</details>


### [43] [Distributed Shared Layered Storage Quantum Simulator: A novel quantum simulation system for efficient scaling and cost optimization](https://arxiv.org/abs/2508.15542)
*Mingyang Yu,Haorui Yang,Donglin Wang,Desheng Kong,Ji Du,Yulong Fu,Wei Wang,Jing Xu*

Main category: cs.ET

TL;DR: 论文提出了一种新型分布式共享分层存储量子模拟器（DSLSQS），通过创新的分布式架构和去TCP/IP网络技术，解决了量子模拟器的单节点瓶颈问题，显著提升了性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有分布式技术无法满足量子模拟器的高频遍历需求，导致单节点瓶颈，限制了其性能和可扩展性。

Method: 采用分布式共享存储架构和De-TCP/IP网络技术，消除分布式系统中的东西向数据流，并结合分层存储技术降低高性能内存的使用和成本。

Result: 实验证实DSLSQS在27量子比特模拟中效率显著提升，性能比现有技术提高350%以上，同时有效降低了存储成本。

Conclusion: DSLSQS为量子计算的实用化提供了关键见解，并为分布式量子模拟集群的发展提供了有效框架。

Abstract: Quantum simulators are essential tools for developing and testing quantum
algorithms. However, the high-frequency traversal characteristic of quantum
simulators represents an unprecedented demand in the history of IT, and
existing distributed technologies is unable to meet this requirement, resulting
in a single-node bottleneck of quantum simulator. To overcome this limitation,
this paper introduces a novel Distributed Shared Layered Storage Quantum
Simulator (DSLSQS). By leveraging an innovative distributed architecture in
which multiple computational nodes share data storage directly, together with
De-TCP/IP networking technology, DSLSQS effectively eliminates East-West data
flow in distributed systems. This approach mitigates the bottleneck of
distributed quantum simulation clusters and enhances the scalability. Moreover,
the system employs layered storage technology, which reduces usage of expensive
high-performance memory and substantially lowers simulation costs. Furthermore,
this paper systematically analyzes the performance and cost constraints of
distributed quantum simulator cluster, identifying distributed networking as
the primary performance bottleneck and highlighting that minimizing storage
costs is crucial to reducing the total cost. Finally, experimental evaluations
with a 27-qubit simulation confirm the successful implementation of layered
storage within the quantum simulator. DSLSQS significantly enhances simulation
efficiency, yielding a performance improvement of over 350% compared to
existing distributed technologies. These results underscore the superior
performance and scalability of the proposed architecture in managing complex
quantum computing tasks. This paper provides crucial insights for the practical
deployment of quantum computing and presents an effective framework for the
development of distributed quantum simulation clusters.

</details>


### [44] [QVecOpt: An Efficient Storage and Computing Opti-mization Framework for Large-scale Quantum State Simulation](https://arxiv.org/abs/2508.15545)
*Mingyang Yu,Haorui Yang,Donglin Wang,Desheng Kong,Ji Du,Yulong Fu,Jing Xu*

Main category: cs.ET

TL;DR: 论文提出QVecOpt框架，通过四种策略优化量子态模拟，在大规模量子计算模拟中显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决经典计算平台上大规模量子态模拟的内存限制、频繁磁盘I/O和高计算复杂度等挑战。

Method: 结合振幅配对、缓存优化、块存储优化和并行优化四种策略，优化状态向量存储与计算调度。

Result: 在16-29量子比特模拟中效率提升近十倍，突破内存瓶颈，支持高比特量子电路模拟。

Conclusion: QVecOpt为大规模量子计算的经典模拟提供了高效、可扩展的解决方案，具有重要学术与实用价值。

Abstract: In response to the challenges in large-scale quantum state simulation on
classical computing platforms, including memory limits, frequent disk I/O, and
high computational complexity, this study builds upon a previously proposed
hierarchical storage-based quantum simulation system and introduces an
optimization framework, the Quantum Vector Optimization Framework (QVecOpt).
QVecOpt integrates four strategies: amplitude pairing, cache optimization,
block storage optimization, and parallel optimization. These collectively
enhance state vector storage and computational scheduling. The amplitude
pairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing
traversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache
optimization pre-allocates buffers and loads only required data, cutting disk
I/O. Block storage optimization partitions the state vector for on-demand
loading and local updates, reducing redundant access. Parallel optimization
distributes the state vector across nodes for collaborative computation,
achieving near-linear speedup. Complexity analysis shows that, compared with
hierarchical storage simulation, the method reduces state vector traversals for
single-qubit gates from $2^n$ to 1, removing the main bottleneck. It also
lowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and
$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,
breaking the memory bottleneck of existing tools and enabling high-bit quantum
circuit simulations beyond traditional methods. This work provides an
efficient, scalable solution for classical simulation of large-scale quantum
computation with significant academic and practical value.

</details>


### [45] [Low-Power Control of Resistance Switching Transitions in First-Order Memristors](https://arxiv.org/abs/2508.15620)
*Valeriy A. Slipko,Alon Ascoli,Fernando Corinto,Yuriy V. Pershin*

Main category: cs.ET

TL;DR: 研究探讨了如何通过单内部状态变量优化低功耗控制第一阶忆阻器，提出了一种通用的切换优化方法，并展示了不同电压脉冲方案的最优协议。


<details>
  <summary>Details</summary>
Motivation: 开发能量高效的忆阻器编程协议，解决电压-时间困境，具有重要的学术和工业应用价值。

Method: 采用通用优化方法，应用于电压控制忆阻器的微分代数方程集，比较不同电压脉冲方案。

Result: 研究发现，根据设备特性和约束条件，最优协议可能是单一电压脉冲或复杂的连续波形。

Conclusion: 研究为忆阻器的能量高效编程提供了实用方案，对社区和工业具有重要意义。

Abstract: In many cases, the behavior of physical memristive devices can be relatively
well captured by using a single internal state variable. This study
investigates the low-power control of first-order memristive devices to derive
the most energy-efficient protocols for programming their resistances. A unique
yet general approach to optimizing the switching transitions in devices of this
kind is introduced. For pedagogical purposes, without loss of generality, the
proposed control paradigm is applied to a couple of differential algebraic
equation sets for voltage-controlled devices, specifically Kvatinsky's Voltage
ThrEshold Adaptive Memristor mathematical description and Miranda's and Sune's
dynamic balance model. It is demonstrated that, depending upon intrinsic
physical properties of the device, captured in the model formulas and parameter
setting, and upon constraints on programming time and voltages, the optimal
protocol for either of the two switching scenarios may require the application
of a single square voltage pulse of height set to a certain level within the
admissible range across a fraction or entire given programming time interval,
or of some more involved voltage stimulus of unique polarity, including
analogue continuous waveforms that can be approximated by trains of square
voltage pulses of different heights, over the entire programming time interval.
The practical implications of these research findings are significant, as the
development of energy-efficient protocols to program memristive devices,
resolving the so-called voltage-time dilemma in the device physics community,
is a subject under intensive and extensive studies across the academic
community and industry.

</details>


### [46] [Exploration of Evolving Quantum Key Distribution Network Architecture Using Model-Based Systems Engineering](https://arxiv.org/abs/2508.15733)
*Hayato Ishida,Amal Elsokary,Maria Aslam,Catherine White,Michael J. de C. Henshaw,Siyuan Ji*

Main category: cs.ET

TL;DR: 本文探讨了量子密钥分发网络的架构演化，提出了一种基于变异性建模的系统工程方法，以应对量子通信网络快速发展的需求。


<details>
  <summary>Details</summary>
Motivation: 量子技术的进步需要将量子设备集成到现有经典基础设施中，同时量子计算的成熟对加密技术构成威胁，因此需要开发量子安全通信系统。

Method: 利用正交变异性建模和系统建模语言，研究建立了可追溯的模块化架构，并提出了一种变异性驱动的框架来管理快速演进的网络架构。

Result: 研究成果为量子密钥分发网络的系统开发提供了支持，并有助于解决量子系统工程中类似的集成挑战。

Conclusion: 提出的框架为量子通信网络的未来发展提供了可重用和模块化的解决方案，促进了量子系统工程的研究。

Abstract: Realisation of significant advances in capabilities of sensors, computing,
timing, and communication enabled by quantum technologies is dependent on
engineering highly complex systems that integrate quantum devices into existing
classical infrastructure. A systems engineering approach is considered to
address the growing need for quantum-secure telecommunications that overcome
the threat to encryption caused by maturing quantum computation. This work
explores a range of existing and future quantum communication networks,
specifically quantum key distribution network proposals, to model and
demonstrate the evolution of quantum key distribution network architectures.
Leveraging Orthogonal Variability Modelling and Systems Modelling Language as
candidate modelling languages, the study creates traceable artefacts to promote
modular architectures that are reusable for future studies. We propose a
variability-driven framework for managing fast-evolving network architectures
with respect to increasing stakeholder expectations. The result contributes to
the systematic development of viable quantum key distribution networks and
supports the investigation of similar integration challenges relevant to the
broader context of quantum systems engineering.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [47] [Declarative Data Pipeline for Large Scale ML Services](https://arxiv.org/abs/2508.15105)
*Yunzhao Yang,Runhui Wang,Xuanqing Liu,Adit Krishnan,Yefan Tao,Yuqian Deng,Kuangyou Yao,Peiyuan Sun,Henrik Johnson,Aditi sinha,Davor Golac,Gerald Friedland,Usman Shakeel,Daryl Cooke,Joe Sullivan,Chris Kong*

Main category: cs.DC

TL;DR: 本文提出了一种名为「声明式数据管道」的新型架构，用于解决分布式数据处理系统中性能与代码可维护性及开发效率的矛盾。通过模块化设计和标准化接口，显著提升了开发效率和系统性能。


<details>
  <summary>Details</summary>
Motivation: 现代分布式数据处理系统在集成机器学习功能时，面临性能与开发效率、可维护性的平衡问题，尤其在大规模协作环境中更为突出。

Method: 采用模块化框架「Declarative Data Pipeline」，结合逻辑计算单元（Pipes），替代传统的微服务方法，集成于Apache Spark。

Result: 企业案例显示开发效率提升50%，协作/解决时间从数周缩短至数天，性能提升500倍（扩展性）和10倍（吞吐量）；学术实验显示吞吐量至少提升5.7倍，CPU利用率达99%。

Conclusion: 该架构通过清晰的模块化设计和优化策略，实现了性能与开发效率的平衡，为构建可扩展、可维护的数据处理系统提供了实用方案。

Abstract: Modern distributed data processing systems face significant challenges in
balancing system performance with code maintainability and developer
productivity, particularly when integrating machine learning capabilities at
scale. In large collaborative environments, these challenges are amplified by
high communication overhead between teams and the complexity of coordinating
development across multiple groups. This paper presents a novel "Declarative
Data Pipeline" architecture that addresses these challenges while processing
billions of records with high accuracy and efficiency. Our architecture
introduces a modular framework that seamlessly integrates machine learning
capabilities within Apache Spark by combining logical computation units that we
refer as Pipes, departing from traditional microservice-based approaches. By
establishing clear component boundaries and standardized interfaces, we achieve
both modularity and system optimization without sacrificing maintainability.
The enterprise case study demonstrate substantial improvements in multiple
dimensions: development efficiency improved by 50%,
collaboration/troubleshooting efforts compressed from weeks to days,
performance improved by 500x in scalability and by 10x in throughput. The
academic experiment also proves at least 5.7x faster in throughput with 99% CPU
utilization than non-framework implementations. This paper details the
architectural decisions, implementation strategies, and performance
optimizations that enable these improvements, providing insights for building
scalable, maintainable data processing systems that effectively balance system
performance with development velocity.

</details>


### [48] [Databelt: A Continuous Data Path for Serverless Workflows in the 3D Compute Continuum](https://arxiv.org/abs/2508.15351)
*Cynthia Marcelino,Leonard Guelmino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Databelt是一个针对3D计算连续体中动态网络环境的服务器无状态工作流状态管理框架，通过SLO感知的状态传播和状态融合机制，显著降低了延迟并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在动态的3D计算连续体中，传统服务器无状态函数依赖远程存储服务导致高延迟和网络开销，亟需高效的状态管理解决方案。

Method: Databelt采用SLO感知的状态传播机制，主动将状态卸载到最适合的节点，并引入状态融合机制，减少冗余的网络和存储操作。

Result: 实验表明，Databelt将工作流执行时间减少66%，吞吐量提高50%，状态融合还减少了20%的存储操作延迟。

Conclusion: Databelt在动态网络环境中显著提升了服务器无状态工作流的效率，适用于3D计算连续体。

Abstract: Typically, serverless functions rely on remote storage services for managing
state, which can result in increased latency and network communication
overhead. In a dynamic environment such as the 3D (Edge-Cloud-Space) Compute
Continuum, serverless functions face additional challenges due to frequent
changes in network topology. As satellites move in and out of the range of
ground stations, functions must make multiple hops to access cloud services,
leading to high-latency state access and unnecessary data transfers. In this
paper, we present Databelt, a state management framework for serverless
workflows designed for the dynamic environment of the 3D Compute Continuum.
Databelt introduces an SLO-aware state propagation mechanism that enables the
function state to move continuously in orbit. Databelt proactively offloads
function states to the most suitable node, such that when functions execute,
the data is already present on the execution node or nearby, thus minimizing
state access latency and reducing the number of network hops. Additionally,
Databelt introduces a function state fusion mechanism that abstracts state
management for functions sharing the same serverless runtime. When functions
are fused, Databelt seamlessly retrieves their state as a group, reducing
redundant network and storage operations and improving overall workflow
efficiency. Our experimental results show that Databelt reduces workflow
execution time by up to 66% and increases throughput by 50% compared to the
baselines. Furthermore, our results show that Databelt function state fusion
reduces storage operations latency by up to 20%, by reducing repetitive storage
requests for functions within the same runtime, ensuring efficient execution of
serverless workflows in highly dynamic network environments such as the 3D
Continuum.

</details>


### [49] [Efficient Mixed-Precision Large Language Model Inference with TurboMind](https://arxiv.org/abs/2508.15601)
*Li Zhang,Youhe Jiang,Guoliang He,Xin Chen,Han Lv,Qian Yao,Fangcheng Fu,Kai Chen*

Main category: cs.DC

TL;DR: 该论文提出了一种混合精度LLM推理技术，通过优化存储和计算，显著降低了延迟并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 降低大型语言模型（LLM）的内存和计算需求，同时保持性能。

Method: 设计了两种混合精度流水线（GEMM和注意力），优化矩阵运算和注意力计算；包括硬件感知权重打包、自适应头部对齐等技术。

Result: 在16种LLM和4种GPU架构上测试，延迟降低最高61%（平均30%），吞吐量提高最高156%（平均58%）。

Conclusion: 该技术在多种配置和硬件上均表现优异，并已集成到开源项目TurboMind中。

Abstract: Mixed-precision inference techniques reduce the memory and computational
demands of Large Language Models (LLMs) by applying hybrid precision formats to
model weights, activations, and KV caches. This work introduces mixed-precision
LLM inference techniques that encompass (i) systematic memory and compute
optimization across hierarchical storage and tensor core architectures, and
(ii) comprehensive end-to-end mixed-precision optimization across diverse
precision formats and hardware configurations. Our approach features two novel
mixed-precision pipelines designed for optimal hardware utilization: a General
Matrix Multiply (GEMM) pipeline that optimizes matrix operations through
offline weight packing and online acceleration, and an attention pipeline that
enables efficient attention computation with arbitrary Query, Key, and Value
precision combinations. The key implementation of the pipelines includes (i)
hardware-aware weight packing for automatic format optimization, (ii) adaptive
head alignment for efficient attention computation, (iii) instruction-level
parallelism for memory hierarchy exploitation, and (iv) KV memory loading
pipeline for enhanced inference efficiency. We conduct comprehensive
evaluations across 16 popular LLMs and 4 representative GPU architectures.
Results demonstrate that our approach achieves up to 61% lower serving latency
(30% on average) and up to 156% higher throughput (58% on average) in
mixed-precision workloads compared to existing mixed-precision frameworks,
establishing consistent performance improvements across all tested
configurations and hardware types. This work is integrated into TurboMind, a
high-performance inference engine of the LMDeploy project, which is
open-sourced and publicly available at https://github.com/InternLM/lmdeploy.

</details>


### [50] [Universal Dancing by Luminous Robots under Sequential Schedulers](https://arxiv.org/abs/2508.15484)
*Caterina Feletti,Paola Flocchini,Debasish Pattanayak,Giuseppe Prencipe,Nicola Santoro*

Main category: cs.DC

TL;DR: 论文研究了机器人舞步问题，提出在LUMI模型下，利用顺序调度实现通用舞步问题，并通过分布式计数机制提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究对舞步问题和初始配置有严格限制，无法适应通用场景。本文旨在探索LUMI模型下的通用解决方案。

Method: 利用LUMI模型和顺序调度，提出分布式计数机制算法，解决非刚性移动下的舞步问题。

Result: 证明了通用舞步问题在有限颜色下的可行性，并提供算法确保空间一致性。

Conclusion: LUMI模型和顺序调度可放宽舞步问题的限制，通过分布式计数实现通用解决方案。

Abstract: The Dancing problem requires a swarm of $n$ autonomous mobile robots to form
a sequence of patterns, aka perform a choreography. Existing work has proven
that some crucial restrictions on choreographies and initial configurations
(e.g., on repetitions of patterns, periodicity, symmetries,
contractions/expansions) must hold so that the Dancing problem can be solved
under certain robot models. Here, we prove that these necessary constraints can
be dropped by considering the LUMI model (i.e., where robots are endowed with a
light whose color can be chosen from a constant-size palette) under the quite
unexplored sequential scheduler. We formalize the class of Universal Dancing
problems which require a swarm of $n$ robots starting from any initial
configuration to perform a (periodic or finite) sequence of arbitrary patterns,
only provided that each pattern consists of $n$ vertices (including
multiplicities). However, we prove that, to be solvable under LUMI, the length
of the feasible choreographies is bounded by the compositions of $n$ into the
number of colors available to the robots. We provide an algorithm solving the
Universal Dancing problem by exploiting the peculiar capability of sequential
robots to implement a distributed counter mechanism. Even assuming non-rigid
movements, our algorithm ensures spatial homogeneity of the performed
choreography.

</details>


### [51] [Lower Bounds for $k$-Set Agreement in Fault-Prone Networks](https://arxiv.org/abs/2508.15562)
*Pierre Fraigniaud,Minh Hang Nguyen,Ami Paz,Ulrich Schmid,Hugo Rincon Galeana*

Main category: cs.DC

TL;DR: 该论文提出了一种同步消息传递系统中k-set协议的新下限，适用于任意有向通信网络，并扩展了现有理论。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于扩展和统一现有关于k-set协议下限的理论，包括完全网络和特定网络模型中的结果。

Method: 方法基于拓扑证明，使用一系列可壳化的载波映射，结合Sperner引理和高连通性保持技术。

Result: 结果表明，在任意通信网络中，k-set协议的下限可以通过网络半径表达，并提出了更小的输入复合体。

Conclusion: 结论证明了新下限的广泛适用性，并简化了现有理论的复杂性。

Abstract: We develop a new lower bound for k-set agreement in synchronous
message-passing systems connected by an arbitrary directed communication
network, where up to t processes may crash. Our result thus generalizes the
t/k+1 lower bound for complete networks in the t-resilient model by Chaudhuri,
Herlihy, Lynch, and Tuttle [JACM'00]. Moreover, it generalizes two lower bounds
for oblivious algorithms in synchronous systems connected by an arbitrary
undirected communication network known to the processes, namely, the domination
number-based lower bound by Castaneda, Fraigniaud, Paz, Rajsbaum, Roy, and
Travers [TCS'21] for failure-free processes, and the radius-based lower bound
in the t-resilient model by Fraigniaud, Nguyen, and Paz [STACS'24].
  Our topological proof non-trivially generalizes and extends the
connectivity-based approach for the complete network, as presented in the book
by Herlihy, Kozlov, and Rajsbaum (2013). It is based on a sequence of shellable
carrier maps that, starting from a shellable input complex, determine the
evolution of the protocol complex: During the first t/k rounds, carrier maps
that crash exactly k processes per round are used, ensuring high connectivity
of their images. A Sperner's lemma style argument is used to prove that k-set
agreement is still impossible by that round. From round t/k+1 up to our lower
bound, we employ a novel carrier map that maintains high connectivity. Our
proof also provides a strikingly simple lower bound for k-set agreement in
synchronous systems with an arbitrary communication network with initial
crashes. We express the resulting additional agreement overhead via an
appropriately defined radius of the communication graphs. Finally, we prove
that the usual input pseudosphere complex for k-set agreement can be replaced
by an exponentially smaller input complex based on Kuhn triangulations, which
we prove to be also shellable.

</details>


### [52] [CausalMesh: A Formally Verified Causal Cache for Stateful Serverless Computing](https://arxiv.org/abs/2508.15647)
*Haoran Zhang,Zihao Zhang,Shuai Mu,Sebastian Angel,Vincent Liu*

Main category: cs.DC

TL;DR: 本文介绍了一种名为CausalMesh的新型缓存系统，旨在解决无服务器环境中因函数被调度到不同节点而引起的缓存一致性问题。


<details>
  <summary>Details</summary>
Motivation: 在无服务器环境中，工作流中的函数可能被调度到不同节点，导致访问不同缓存，引发非直观的异常。传统缓存方法无法有效解决这一问题。

Method: CausalMesh通过支持协调无关的无中止读写操作和读取事务，实现了因果一致性缓存。它还支持在客户端漫游时的读写事务一致性，但牺牲了无中止性。

Result: 通过Dafny形式化验证，CausalMesh在延迟和吞吐量上优于现有方案。

Conclusion: CausalMesh是首个在客户端漫游环境下实现无中止读写操作的缓存系统，显著提升了性能和一致性。

Abstract: Stateful serverless workflows consist of multiple serverless functions that
access state on a remote database. Developers sometimes add a cache layer
between the serverless runtime and the database to improve I/O latency.
However, in a serverless environment, functions in the same workflow may be
scheduled to different nodes with different caches, which can cause
non-intuitive anomalies. This paper presents CausalMesh, a novel approach to
causally consistent caching in environments where a computation may migrate
from one machine to another, such as in serverless computing. CausalMesh is the
first cache system that supports coordination-free and abort-free read/write
operations and read transactions when clients roam among multiple servers.
CausalMesh also supports read-write transactional causal consistency in the
presence of client roaming, but at the cost of abort-freedom.
  We have formally verified CausalMesh's protocol in Dafny, and our
experimental evaluation shows that CausalMesh has lower latency and higher
throughput than existing proposals

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [53] [Random Sampling over Spatial Range Joins](https://arxiv.org/abs/2508.15070)
*Daichi Amagata*

Main category: cs.DB

TL;DR: 该论文提出了一种高效的空间范围连接随机采样算法，旨在解决传统方法计算成本高和结果集过大的问题。


<details>
  <summary>Details</summary>
Motivation: 空间范围连接在地理信息系统、位置社交网络等领域有广泛应用，但其计算成本高且结果集庞大，随机采样成为实用解决方案。

Method: 论文首先设计两种基线算法，并提出一种新的数据结构，算法时间复杂度和空间复杂度分别为$	ilde{O}(n + m + t)$和$O(n+m)$。

Result: 实验表明，新算法在多数测试中明显优于基线方法。

Conclusion: 该算法首次高效解决了空间范围连接的随机采样问题，具有时间和空间效率优势。

Abstract: Spatial range joins have many applications, including geographic information
systems, location-based social networking services, neuroscience, and
visualization. However, joins incur not only expensive computational costs but
also too large result sets. A practical and reasonable approach to alleviating
these issues is to return random samples of the join results. Although this is
promising and sufficient for many applications involving spatial range joins,
efficiently computing random samples is not trivial. This is because we must
obtain random join samples without running spatial range joins. We address this
challenging problem for the first time and aim at designing a time- and
space-efficient algorithm. First, we design two baseline algorithms that employ
existing techniques for random sampling and show that they are not efficient.
Then, we propose a new data structure that can deal with our problem in
$\tilde{O}(n + m + t)$ expected time and $O(n+m)$ space, where $n$ and $m$ are
the sizes of two point sets and $t$ is the required number of samples. We
conduct extensive experiments using four real spatial datasets, and the results
demonstrate that our algorithm is significantly faster than the baselines in
most tests.

</details>


### [54] [Temporal $k$-Core Query, Revisited](https://arxiv.org/abs/2508.15238)
*Yinyu Liu,Kaiqiang Yu,Shengxin Liu,Cheng Long,Zhaoquan Gu*

Main category: cs.DB

TL;DR: 论文提出了一种名为CoreT的新算法，用于高效查询时序图中的k-core，解决了现有OTCD算法的冗余计算和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 理解时序图中的动态结构对社交网络、网页链接和通信模式等实际网络的分析至关重要，但现有方法存在冗余计算和可扩展性不足的局限。

Method: CoreT算法通过动态记录顶点或边首次进入k-core的时间戳，显著减少冗余计算，仅需单次遍历查询区间。

Result: 实验证明，CoreT在大型真实数据集上比OTCD快四个数量级，具备高效和可扩展性。

Conclusion: CoreT为时序k-core分析提供了一种高效的解决方案，适用于长期时序分析。

Abstract: Querying cohesive subgraphs in temporal graphs is essential for understanding
the dynamic structure of real-world networks, such as evolving communities in
social platforms, shifting hyperlink structures on the Web, and transient
communication patterns in call networks. Recently, research has focused on the
temporal $k$-core query, which aims to identify all $k$-cores across all
possible time sub-intervals within a given query interval. The state-of-the-art
algorithm OTCD mitigates redundant computations over overlapping sub-intervals
by exploiting inclusion relationships among $k$-cores in different time
intervals. Nevertheless, OTCD remains limited in scalability due to the
combinatorial growth in interval enumeration and repeated processing. In this
paper, we revisit the temporal $k$-core query problem and introduce a novel
algorithm CoreT, which dynamically records the earliest timestamp at which each
vertex or edge enters a $k$-core. This strategy enables substantial pruning of
redundant computations. As a result, CoreT requires only a single pass over the
query interval and achieves improved time complexity, which is linear in both
the number of temporal edges within the query interval and the duration of the
interval, making it highly scalable for long-term temporal analysis.
Experimental results on large real-world datasets show that CoreT achieves up
to four orders of magnitude speedup compared to the existing state-of-the-art
OTCD, demonstrating its effectiveness and scalability for temporal $k$-core
analysis.

</details>


### [55] [AmbiSQL: Interactive Ambiguity Detection and Resolution for Text-to-SQL](https://arxiv.org/abs/2508.15276)
*Zhongjun Ding,Yin Lin,Tianjing Zeng*

Main category: cs.DB

TL;DR: AmbiSQL是一个交互式系统，能自动检测查询歧义并通过多选问题指导用户澄清意图，显著提升Text-to-SQL系统的准确性。


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL系统中的查询歧义是LLM易错的主要原因，影响用户意图的正确解析和SQL生成的准确性。

Method: 提出细粒度歧义分类法，自动检测歧义并通过用户反馈重写模糊问题。

Result: AmbiSQL在歧义检测中达到87.2%的精确度，并使SQL完全匹配准确率提升50%。

Conclusion: AmbiSQL通过交互式澄清机制显著提升了Text-to-SQL系统的性能和实用性。

Abstract: Text-to-SQL systems translate natural language questions into SQL queries,
providing substantial value for non-expert users. While large language models
(LLMs) show promising results for this task, they remain error-prone. Query
ambiguity has been recognized as a major obstacle for LLM-based Text-to-SQL
systems, leading to misinterpretation of user intent and inaccurate SQL
generation. We demonstrate AmbiSQL, an interactive system that automatically
detects query ambiguities and guides users through intuitive multiple-choice
questions to clarify their intent. Our approach introduces a fine-grained
ambiguity taxonomy for identifying ambiguities that affect database element
mapping and LLM reasoning, then incorporates user feedback to rewrite ambiguous
questions. Evaluation on an ambiguous query dataset shows that AmbiSQL achieves
87.2% precision in ambiguity detection and improves SQL exact match accuracy by
50% when integrated with Text-to-SQL systems. Our demonstration showcases the
significant performance gains and highlights the system's practical usability.
Code repo and demonstration are available at:
https://github.com/JustinzjDing/AmbiSQL.

</details>


### [56] [Efficient Cloud-Edge-Device Query Execution Based on Collaborative Scan Operator](https://arxiv.org/abs/2508.15285)
*Chunyu Zhao,Hongzhi Wang,Kaixin Zhang,Hongliang Li,Yihan Zhang,Jiawei Zhang,Kunkai Gu,Yuan Tian,Xiangdong Huang,Jingyi Xu*

Main category: cs.DB

TL;DR: 论文提出了一种云-边-设备(CED)协作查询处理方法，通过协作扫描算子实现查询执行在云和边缘之间的灵活切换，以提升边缘资源瓶颈时的查询性能。


<details>
  <summary>Details</summary>
Motivation: 在CED协作查询处理中，充分利用云和边缘资源的优势是一个挑战，特别是在边缘资源达到瓶颈时，难以实现查询执行的无缝切换。

Method: 提出了一个基于协作扫描算子的CED协作框架，使得在边缘资源饱和时可以随时将查询执行转移到云端。

Result: 实验表明，在足够的网络下载带宽下，该方法有效缓解了高I/O负载和CPU等待时间对扫描算子的影响，并实现了云和边缘之间的资源平衡调度。

Conclusion: 该方法通过协作扫描算子显著提升了边缘资源瓶颈时的查询性能，并实现了资源的高效利用。

Abstract: In cloud-edge-device (CED) collaborative query (CQ) processing, by leveraging
CED collaboration, the advantages of both cloud computing and edge resources
can be fully integrated. However, it is difficult to implement collaborative
operators that can flexibly switch between the cloud and the edge during query
execution. Thus, in this paper, we aim to improve the query performance when
the edge resources reach a bottleneck. To achieve seamless switching of query
execution between the cloud and edge, we propose a CQ processing method by
establishing a CED collaborative framework based on the collaborative scan
operator, so that query execution can be transferred to the cloud at any time
when the edge resources are saturated. Extensive experiments show that, under
sufficient network download bandwidth, the CED collaborative scan operator can
effectively alleviate the performance degradation of scan operators caused by
high I/O load and CPU wait time at the edge. It also achieves balanced resource
scheduling between the cloud and edge.

</details>


### [57] [Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional Vector Search](https://arxiv.org/abs/2508.15290)
*Peiqi Yin,Xiao Yan,Qihui Zhou,Hui Li,Xiaolu Li,Lin Zhang,Meiling Wang,Xin Yao,James Cheng*

Main category: cs.DB

TL;DR: 该论文提出了Gorgeous系统，通过优化数据布局和优先级处理图结构，显著提升了基于SSD的向量搜索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于SSD的向量搜索系统因数据布局不佳和内存利用率低而导致性能下降，特别是对图结构的高频访问未被有效优化。

Method: 设计Gorgeous系统，优先处理图结构，包括内存缓存节点邻接表和磁盘块格式优化以增强数据局部性。

Result: Gorgeous平均查询吞吐量提升60%以上，查询延迟降低35%以上，优于现有系统。

Conclusion: 通过优化图结构访问和存储，Gorgeous显著提升了大规模向量搜索的效率和性能。

Abstract: Similarity-based vector search underpins many important applications, but a
key challenge is processing massive vector datasets (e.g., in TBs). To reduce
costs, some systems utilize SSDs as the primary data storage. They employ a
proximity graph, which connects similar vectors to form a graph and is the
state-of-the-art index for vector search. However, these systems are hindered
by sub-optimal data layouts that fail to effectively utilize valuable memory
space to reduce disk access and suffer from poor locality for accessing
disk-resident data. Through extensive profiling and analysis, we found that the
structure of the proximity graph index is accessed more frequently than the
vectors themselves, yet existing systems do not distinguish between the two. To
address this problem, we design the Gorgeous system with the principle of
prioritizing graph structure over vectors. Specifically, Gorgeous features a
memory cache that keeps the adjacency lists of graph nodes to improve cache
hits and a disk block format that explicitly stores neighbors' adjacency lists
along with a vector to enhance data locality. Experimental results show that
Gorgeous consistently outperforms two state-of-the-art disk-based systems for
vector search, boosting average query throughput by over 60% and reducing query
latency by over 35%.

</details>


### [58] [GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector Nearest Neighbor Search](https://arxiv.org/abs/2508.15694)
*Yijie Zhou,Shengyuan Lin,Shufeng Gong,Song Yu,Shuhao Fan,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: GoVector是一种针对磁盘图索引的I/O高效缓存策略，通过静态和动态缓存结合以及节点重排优化，显著减少了I/O操作并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于图的近似最近邻搜索在大规模数据下的高内存占用和I/O瓶颈问题。

Method: 结合静态缓存与动态缓存，通过节点重排优化存储布局。

Result: 在90%召回率下，平均减少46%的I/O操作，查询吞吐量提升1.73倍，延迟降低42%。

Conclusion: GoVector显著提升了磁盘图索引系统的性能，适用于大规模近似最近邻搜索场景。

Abstract: Graph-based high-dimensional vector indices have become a mainstream solution
for large-scale approximate nearest neighbor search (ANNS). However, their
substantial memory footprint often requires storage on secondary devices, where
frequent on-demand loading of graph and vector data leads to I/O becoming the
dominant bottleneck, accounting for over 90\% of query latency. Existing static
caching strategies mitigate this issue only in the initial navigation phase by
preloading entry points and multi-hop neighbors, but they fail in the second
phase where query-dependent nodes must be dynamically accessed to achieve high
recall. We propose GoVector, an I/O-efficient caching strategy tailored for
disk-based graph indices. GoVector combines (1) a static cache that stores
entry points and frequently accessed neighbors, and (2) a dynamic cache that
adaptively captures nodes with high spatial locality during the second search
phase. To further align storage layout with similarity-driven search patterns,
GoVector reorders nodes on disk so that similar vectors are colocated on the
same or adjacent pages, thereby improving locality and reducing I/O overhead.
Extensive experiments on multiple public datasets show that GoVector achieves
substantial performance improvements. At 90% recall, it reduces I/O operations
by 46% on average, increases query throughput by 1.73x, and lowers query
latency by 42% compared to state-of-the-art disk-based graph indexing systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [59] [Accelerating GenAI Workloads by Enabling RISC-V Microkernel Support in IREE](https://arxiv.org/abs/2508.14899)
*Adeel Ahmad,Ahmad Tameem Kamal,Nouman Amir,Bilal Zafar,Saad Bin Nasir*

Main category: cs.AR

TL;DR: 该项目在IREE中实现了RISC-V微内核支持，优化了MLIR linalg方言到RISC-V64目标的转换，并开发了优化的微内核，性能较上游IREE和Llama.cpp有所提升。


<details>
  <summary>Details</summary>
Motivation: 通过为RISC-V架构优化IREE编译器，提升机器学习模型（如Llama-3.2-1B-Instruct）在该平台上的性能。

Method: 首先将MLIR linalg方言的收缩操作转换为linalg.mmt4d操作，然后开发针对RISC-V的优化微内核。

Result: 与上游IREE和Llama.cpp相比，性能有所提升。

Conclusion: 该研究成功实现了RISC-V微内核支持，并为IREE在RISC-V平台上提供了性能优化。

Abstract: This project enables RISC-V microkernel support in IREE, an MLIR-based
machine learning compiler and runtime. The approach begins by enabling the
lowering of MLIR linalg dialect contraction ops to linalg.mmt4d op for the
RISC-V64 target within the IREE pass pipeline, followed by the development of
optimized microkernels for RISC-V. The performance gains are compared with
upstream IREE and Llama.cpp for the Llama-3.2-1B-Instruct model.

</details>


### [60] [Improving Chip Design Enablement for Universities in Europe -- A Position Paper](https://arxiv.org/abs/2508.14907)
*Lukas Krupp,Ian O'Connor,Luca Benini,Christoph Studer,Joachim Rodrigues,Norbert Wehn*

Main category: cs.AR

TL;DR: 本文探讨了欧洲大学和学术计划在提升芯片设计教育和研究中的作用，以应对欧洲在芯片设计能力上的短板。


<details>
  <summary>Details</summary>
Motivation: 欧洲半导体行业在工业和汽车领域至关重要，但面临芯片设计能力不足、人才短缺和设计价值链贡献滞后的问题。

Method: 通过综述当前的欧洲芯片设计计划，分析招聘、生产力、技术获取和设计支持方面的挑战，并提出战略机会。

Result: 提出一系列建议，强调需要协调努力和战略投资以应对这些挑战。

Conclusion: 通过学术机构的努力和战略投资，可以提升欧洲的芯片设计能力。

Abstract: The semiconductor industry is pivotal to Europe's economy, especially within
the industrial and automotive sectors. However, Europe faces a significant
shortfall in chip design capabilities, marked by a severe skilled labor
shortage and lagging contributions in the design value chain segment. This
paper explores the role of European universities and academic initiatives in
enhancing chip design education and research to address these deficits. We
provide a comprehensive overview of current European chip design initiatives,
analyze major challenges in recruitment, productivity, technology access, and
design enablement, and identify strategic opportunities to strengthen chip
design capabilities within academic institutions. Our analysis leads to a
series of recommendations that highlight the need for coordinated efforts and
strategic investments to overcome these challenges.

</details>


### [61] [Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis](https://arxiv.org/abs/2508.14917)
*Weichien Liao*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的实时去噪预处理流水线，适用于高通量成像工作流，如PRISM，通过HLS实现低延迟处理。


<details>
  <summary>Details</summary>
Motivation: 高通量成像工作流（如PRISM）的数据生成速度超过传统实时处理能力，需要一种高效解决方案。

Method: 采用FPGA架构，通过HLS实现，优化DRAM缓冲，利用AXI4接口减少延迟，直接对图像数据进行帧减法和平均。

Result: 该内核运行时低于帧间隔，实现实时去噪并减少数据集大小，适用于PRISM规模的数据采集。

Conclusion: 模块化FPGA框架为光谱学和显微镜学的延迟敏感工作流提供了实用解决方案。

Abstract: High-throughput imaging workflows, such as Parallel Rapid Imaging with
Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional
real-time processing capabilities. We present a scalable FPGA-based
preprocessing pipeline for real-time denoising, implemented via High-Level
Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture
performs frame subtraction and averaging directly on streamed image data,
minimizing latency through burst-mode AXI4 interfaces. The resulting kernel
operates below the inter-frame interval, enabling inline denoising and reducing
dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale
acquisition, this modular FPGA framework offers a practical solution for
latency-sensitive imaging workflows in spectroscopy and microscopy.

</details>


### [62] [Row-Column Hybrid Grouping for Fault-Resilient Multi-Bit Weight Representation on IMC Arrays](https://arxiv.org/abs/2508.15685)
*Kang Eun Jeon,Sangheum Yeon,Jinhee Kim,Hyeonsu Bang,Johnny Rhe,Jong Hwan Ko*

Main category: cs.AR

TL;DR: 提出了解决模拟内存计算系统中因卡顿故障（SAFs）导致的计算不可靠性和故障缓解算法编译高开销问题的新方法，包括行列混合分组技术和基于整数线性规划的编译器优化。


<details>
  <summary>Details</summary>
Motivation: 解决模拟内存计算（IMC）系统中因卡顿故障和编译开销高而限制其可扩展性和部署性的问题。

Method: 1. 引入行列混合分组技术，通过行列冗余增强容错能力；2. 设计基于整数线性规划的编译器管道，优化故障感知权值分解问题。

Result: 实验显示，在卷积网络和小型语言模型上，准确率提升8%，编译速度提高150倍，能效提升2倍。

Conclusion: 新方法显著提升了IMC系统的可靠性和效率，为大规模部署提供了可能。

Abstract: This paper addresses two critical challenges in analog In-Memory Computing
(IMC) systems that limit their scalability and deployability: the computational
unreliability caused by stuck-at faults (SAFs) and the high compilation
overhead of existing fault-mitigation algorithms, namely Fault-Free (FF). To
overcome these limitations, we first propose a novel multi-bit weight
representation technique, termed row-column hybrid grouping, which generalizes
conventional column grouping by introducing redundancy across both rows and
columns. This structural redundancy enhances fault tolerance and can be
effectively combined with existing fault-mitigation solutions. Second, we
design a compiler pipeline that reformulates the fault-aware weight
decomposition problem as an Integer Linear Programming (ILP) task, enabling
fast and scalable compilation through off-the-shelf solvers. Further
acceleration is achieved through theoretical insights that identify fault
patterns amenable to trivial solutions, significantly reducing computation.
Experimental results on convolutional networks and small language models
demonstrate the effectiveness of our approach, achieving up to 8%p improvement
in accuracy, 150x faster compilation, and 2x energy efficiency gain compared to
existing baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [63] [Locally Pareto-Optimal Interpretations for Black-Box Machine Learning Models](https://arxiv.org/abs/2508.15220)
*Aniruddha Joshi,Supratik Chakraborty,S Akshay,Shetal Shah,Hazem Torfah,Sanjit Seshia*

Main category: cs.LG

TL;DR: 论文提出了一种基于局部最优性保证的黑盒机器学习模型解释框架，解决了现有方法在全局最优性保证与可扩展性之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决黑盒模型解释中准确性与可解释性之间的权衡问题，尤其是在确保解释结果局部最优的同时提高方法的可扩展性。

Method: 方法结合多目标学习或搜索技术生成Pareto最优候选解释集，并通过SAT求解器验证每个候选解释的局部最优性。

Result: 在基准测试中，该方法生成的解释接近具有全局最优性保证的方法的结果，同时具备更高的可扩展性。

Conclusion: 结论是该框架为高效合成具有局部最优性保证的解释提供了一种可行路径，平衡了准确性与可解释性。

Abstract: Creating meaningful interpretations for black-box machine learning models
involves balancing two often conflicting objectives: accuracy and
explainability. Exploring the trade-off between these objectives is essential
for developing trustworthy interpretations. While many techniques for
multi-objective interpretation synthesis have been developed, they typically
lack formal guarantees on the Pareto-optimality of the results. Methods that do
provide such guarantees, on the other hand, often face severe scalability
limitations when exploring the Pareto-optimal space. To address this, we
develop a framework based on local optimality guarantees that enables more
scalable synthesis of interpretations. Specifically, we consider the problem of
synthesizing a set of Pareto-optimal interpretations with local optimality
guarantees, within the immediate neighborhood of each solution. Our approach
begins with a multi-objective learning or search technique, such as
Multi-Objective Monte Carlo Tree Search, to generate a best-effort set of
Pareto-optimal candidates with respect to accuracy and explainability. We then
verify local optimality for each candidate as a Boolean satisfiability problem,
which we solve using a SAT solver. We demonstrate the efficacy of our approach
on a set of benchmarks, comparing it against previous methods for exploring the
Pareto-optimal front of interpretations. In particular, we show that our
approach yields interpretations that closely match those synthesized by methods
offering global guarantees.

</details>


### [64] [Mini-Batch Robustness Verification of Deep Neural Networks](https://arxiv.org/abs/2508.15454)
*Saar Tzour-Shaday,Dana Drachsler Cohen*

Main category: cs.LG

TL;DR: 论文提出了一种名为BaVerLy的新型局部鲁棒性验证方法，通过动态构建和验证mini-batches来提高效率，实验显示其性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有的局部鲁棒性验证方法分析时间长或精度低，无法有效应对大规模输入。

Method: 提出一种基于mini-batches的分组局部鲁棒性验证方法BaVerLy，通过动态识别mini-batch大小并验证相似的ε-balls来提高效率。

Result: 在MNIST和CIFAR-10数据集上，BaVerLy将传统方法的时间从24小时缩短到6小时，性能提升2.3倍至4.1倍。

Conclusion: BaVerLy显著提升了局部鲁棒性验证的效率，为安全关键应用中的神经网络验证提供了新思路。

Abstract: Neural network image classifiers are ubiquitous in many safety-critical
applications. However, they are susceptible to adversarial attacks. To
understand their robustness to attacks, many local robustness verifiers have
been proposed to analyze $\epsilon$-balls of inputs. Yet, existing verifiers
introduce a long analysis time or lose too much precision, making them less
effective for a large set of inputs. In this work, we propose a new approach to
local robustness: group local robustness verification. The key idea is to
leverage the similarity of the network computations of certain $\epsilon$-balls
to reduce the overall analysis time. We propose BaVerLy, a sound and complete
verifier that boosts the local robustness verification of a set of
$\epsilon$-balls by dynamically constructing and verifying mini-batches.
BaVerLy adaptively identifies successful mini-batch sizes, accordingly
constructs mini-batches of $\epsilon$-balls that have similar network
computations, and verifies them jointly. If a mini-batch is verified, all
$\epsilon$-balls are proven robust. Otherwise, one $\epsilon$-ball is suspected
as not being robust, guiding the refinement. In the latter case, BaVerLy
leverages the analysis results to expedite the analysis of that $\epsilon$-ball
as well as the other $\epsilon$-balls in the batch. We evaluate BaVerLy on
fully connected and convolutional networks for MNIST and CIFAR-10. Results show
that BaVerLy scales the common one by one verification by 2.3x on average and
up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6
hours.

</details>


### [65] [Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications](https://arxiv.org/abs/2508.15008)
*Hamza A. Abushahla,Dara Varam,Ariel J. N. Panopio,Mohamed I. AlHajri*

Main category: cs.LG

TL;DR: 该论文探讨了量化神经网络（QNN）在资源受限设备上的部署挑战，介绍了TinyML的解决方案，并系统性地回顾了量化技术、框架和硬件平台，同时指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 资源受限设备（如微控制器）上部署QNN时，需平衡模型性能、计算复杂性和内存限制，TinyML为此提供了跨领域的优化方案。

Method: 通过对量化技术的系统性回顾，评估现有软件框架和硬件平台，分析模型性能与硬件能力的权衡。

Result: 总结了QNN部署中的关键技术和现有支持平台，突出了性能与硬件之间的权衡。

Conclusion: QNN在嵌入式系统中的部署仍面临挑战，但通过TinyML和量化技术的结合，未来有望进一步优化。

Abstract: The deployment of Quantized Neural Networks (QNNs) on resource-constrained
devices, such as microcontrollers, has introduced significant challenges in
balancing model performance, computational complexity and memory constraints.
Tiny Machine Learning (TinyML) addresses these issues by integrating
advancements across machine learning algorithms, hardware acceleration, and
software optimization to efficiently run deep neural networks on embedded
systems. This survey presents a hardware-centric introduction to quantization,
systematically reviewing essential quantization techniques employed to
accelerate deep learning models for embedded applications. In particular,
further emphasis is put on critical trade-offs among model performance and
hardware capabilities. The survey further evaluates existing software
frameworks and hardware platforms designed specifically for supporting QNN
execution on microcontrollers. Moreover, we provide an analysis of the current
challenges and an outline of promising future directions in the rapidly
evolving domain of QNN deployment.

</details>


### [66] [Quantum Long Short-term Memory with Differentiable Architecture Search](https://arxiv.org/abs/2508.14955)
*Samuel Yen-Chi Chen,Prayag Tiwari*

Main category: cs.LG

TL;DR: DiffQAS-QLSTM是一种端到端可微分框架，通过优化VQC参数和架构选择，提升量子序列学习性能，优于手工基线。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）中，变分量子电路（VQC）的设计通常需要针对特定任务，缺乏普适性。

Method: 提出DiffQAS-QLSTM框架，在训练过程中可微分地优化VQC参数和架构选择。

Result: DiffQAS-QLSTM在多样测试场景中表现优于手工设计的基准模型，损失更低。

Conclusion: 该方法为可扩展且自适应的量子序列学习提供了新途径。

Abstract: Recent advances in quantum computing and machine learning have given rise to
quantum machine learning (QML), with growing interest in learning from
sequential data. Quantum recurrent models like QLSTM are promising for
time-series prediction, NLP, and reinforcement learning. However, designing
effective variational quantum circuits (VQCs) remains challenging and often
task-specific. To address this, we propose DiffQAS-QLSTM, an end-to-end
differentiable framework that optimizes both VQC parameters and architecture
selection during training. Our results show that DiffQAS-QLSTM consistently
outperforms handcrafted baselines, achieving lower loss across diverse test
settings. This approach opens the door to scalable and adaptive quantum
sequence learning.

</details>


### [67] [TOAST: Fast and scalable auto-partitioning based on principled static analysis](https://arxiv.org/abs/2508.15010)
*Sami Alabed,Dominik Grewe,Norman Alexander Rink,Timur Sitdikov,Agnieszka Swietlik,Dimitrios Vytiniotis,Daniel Belov*

Main category: cs.LG

TL;DR: 提出了一种结合静态编译器分析和蒙特卡洛树搜索的系统，用于优化大型机器学习模型在分布式加速器上的分区问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自动分区方法因搜索空间庞大常导致内存不足或效率低下，甚至产生不可行或次优的解。

Method: 通过静态编译器分析构建高效决策空间，结合蒙特卡洛树搜索优化分区过程。

Result: 在多种硬件平台和模型架构上显著优于现有工业方法，发现更优的解决方案。

Conclusion: 该系统实现了完全自动化，适用于复杂和大规模的模型分区问题。

Abstract: Partitioning large machine learning models across distributed accelerator
systems is a complex process, requiring a series of interdependent decisions
that are further complicated by internal sharding ambiguities. Consequently,
existing auto-partitioners often suffer from out-of-memory errors or are
prohibitively slow when exploring the exponentially large space of possible
partitionings. To mitigate this, they artificially restrict the search space,
but this approach frequently yields infeasible solutions that violate device
memory constraints or lead to sub-optimal performance.
  We propose a system that combines a novel static compiler analysis with a
Monte Carlo Tree Search. Our analysis constructs an efficient decision space by
identifying (i) tensor dimensions requiring identical sharding, and (ii)
partitioning "conflicts" that require resolution.
  Our system significantly outperforms state-of-the-art industrial methods
across diverse hardware platforms and model architectures, discovering
previously unknown, superior solutions, and the process is fully automated even
for complex and large models.

</details>


### [68] [A Solvable Molecular Switch Model for Stable Temporal Information Processing](https://arxiv.org/abs/2508.15451)
*H. I. Nurdin,C. A. Nijhuis*

Main category: cs.LG

TL;DR: 本文研究了一种基于单状态微分方程的输入驱动模型，展示了其在动态分子开关中的应用，并具备生物启发的行为和稳定的数学特性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过动态分子开关模拟大脑突触行为，并探索其在非线性动力学系统中的稳定计算能力。

Method: 采用线性状态和非线性输入的模型，分析了其精确解、收敛性和衰减记忆特性。

Result: 结果表明，该模型兼具生物启发行为和数学稳定性，适用于神经形态计算中的多种架构。

Conclusion: 该研究为动态分子开关在神经形态计算中的应用提供了理论支持，并可能启发更通用的可解模型。

Abstract: This paper studies an input-driven one-state differential equation model
initially developed for an experimentally demonstrated dynamic molecular switch
that switches like synapses in the brain do. The linear-in-the-state and
nonlinear-in-the-input model is exactly solvable, and it is shown that it also
possesses mathematical properties of convergence and fading memory that enable
stable processing of time-varying inputs by nonlinear dynamical systems. Thus,
the model exhibits the co-existence of biologically-inspired behavior and
desirable mathematical properties for stable learning on sequential data. The
results give theoretical support for the use of the dynamic molecular switches
as computational units in deep cascaded/layered feedforward and recurrent
architectures as well as other more general structures for neuromorphic
computing. They could also inspire more general exactly solvable models that
can be fitted to emulate arbitrary physical devices which can mimic
brain-inspired behaviour and perform stable computation on input signals.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [69] [Human Feedback Driven Dynamic Speech Emotion Recognition](https://arxiv.org/abs/2508.14920)
*Ilya Fedorov,Dmitry Korobchenko*

Main category: cs.SD

TL;DR: 提出一种动态语音情感识别新方法，结合情感序列生成和人类反馈优化，并引入基于Dirichlet分布的情感混合模型，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别方法假设音频片段为单一情感，本文提出动态识别情感序列，尤其针对3D虚拟角色动画。

Method: 多阶段方法：训练经典情感识别模型，生成情感序列，通过人类反馈优化模型，引入Dirichlet分布建模情感混合。

Result: Dirichlet模型优于滑动窗口方法，加入人类反馈后模型质量提升且标注流程更简化。

Conclusion: 动态情感识别与Dirichlet分布结合效果显著，人类反馈进一步优化模型。

Abstract: This work proposes to explore a new area of dynamic speech emotion
recognition. Unlike traditional methods, we assume that each audio track is
associated with a sequence of emotions active at different moments in time. The
study particularly focuses on the animation of emotional 3D avatars. We propose
a multi-stage method that includes the training of a classical speech emotion
recognition model, synthetic generation of emotional sequences, and further
model improvement based on human feedback. Additionally, we introduce a novel
approach to modeling emotional mixtures based on the Dirichlet distribution.
The models are evaluated based on ground-truth emotions extracted from a
dataset of 3D facial animations. We compare our models against the sliding
window approach. Our experimental results show the effectiveness of
Dirichlet-based approach in modeling emotional mixtures. Incorporating human
feedback further improves the model quality while providing a simplified
annotation procedure.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [70] [HEAS: Hierarchical Evolutionary Agent Simulation Framework for Cross-Scale Modeling and Multi-Objective Search](https://arxiv.org/abs/2508.15555)
*Ruiyu Zhang,Lin Nie,Xin Zhao*

Main category: cs.MA

TL;DR: HEAS是一个Python框架，结合了分层代理建模、进化优化和锦标赛评估，提供统一且可重复的工作流。


<details>
  <summary>Details</summary>
Motivation: 提供一个统一的框架，解决跨学科、多层级研究中建模、优化和评估的复杂性问题，提高结果的可重复性和可比性。

Method: 采用分层轻量级流程（'streams'）和共享上下文机制，结合紧凑API和CLI，支持单目标和多目标进化、PyTorch策略集成，以及用户定义的评分规则。

Result: HEAS实现了机制与编排的分离，支持灵活组合和交换组件，为跨学科研究提供了可靠且可重复的基础。

Conclusion: HEAS是跨层级、多学科研究的实用工具，适用于生态和企业决策等多种场景。

Abstract: Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that
unifies layered agent-based modeling with evolutionary optimization and
tournament evaluation in a single, reproducible workflow. HEAS represents
models as hierarchies of lightweight processes ("streams") scheduled in
deterministic layers that read and write a shared context, making cross-scale
couplings explicit and auditable. A compact API and CLI-simulate, optimize,
evaluate-expose single- and multi-objective evolution, PyTorch policy
integration via parameter flattening/unflattening, and general tournament
tooling with user-defined scoring and voting rules. The framework standardizes
evaluation through uniform per-step and episode metrics, persists seeds,
logbooks, and hall-of-fame archives, and provides plotting helpers for traces,
Pareto fronts, and comparative outcomes, reducing glue code and improving
comparability across studies. HEAS emphasizes separation of mechanism from
orchestration, allowing exogenous drivers, endogenous agents, and aggregators
to be composed and swapped without refactoring, while the same model can be
used for forward simulation, optimization, or systematic comparison. We
illustrate usage with two compact examples-an ecological system and an
enterprise decision-making setting. HEAS offers a practical foundation for
cross-disciplinary, multi-level inquiry, yielding reliable, reproducible
results.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [71] [LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa](https://arxiv.org/abs/2508.15110)
*Graham Hill,JingYuan Gong,Thulani Babeli,Moseli Mots'oehli,James Gachomo Wanjiku*

Main category: cs.CE

TL;DR: 总结论文内容，重点关注AI在保险领域的潜力、机遇与挑战，以及非洲市场的现状和合作需求。


<details>
  <summary>Details</summary>
Motivation: 探讨AI（特别是LLMs和agentic AI）在保险行业的应用潜力，强调非洲市场的独特需求和合作机会。

Method: 分析了AI在保险领域的机会与挑战，识别了非洲市场的关键缺口，并提出了本地合作路径。

Result: 揭示了AI在非洲保险市场中的潜力，并呼吁多方合作以推动包容性和可持续性解决方案。

Conclusion: 论文呼吁各方共同努力，开发适合非洲的AI策略和解决方案。

Abstract: In this work, we highlight the transformative potential of Artificial
Intelligence (AI), particularly Large Language Models (LLMs) and agentic AI, in
the insurance sector. We consider and emphasize the unique opportunities,
challenges, and potential pathways in insurance amid rapid performance
improvements, increased open-source access, decreasing deployment costs, and
the complexity of LLM or agentic AI frameworks. To bring it closer to home, we
identify critical gaps in the African insurance market and highlight key local
efforts, players, and partnership opportunities. Finally, we call upon
actuaries, insurers, regulators, and tech leaders to a collaborative effort
aimed at creating inclusive, sustainable, and equitable AI strategies and
solutions: by and for Africans.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [72] [Image-Conditioned 3D Gaussian Splat Quantization](https://arxiv.org/abs/2508.15372)
*Xinshuang Liu,Runfa Blark Li,Keito Suzuki,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种基于图像条件的高斯溅射量化器（ICGS-Quantizer），显著提升压缩效率并适应存档后场景变化。


<details>
  <summary>Details</summary>
Motivation: 解决现有3DGS压缩方法在大型场景存储和不适应场景变化上的不足。

Method: 利用高斯间和属性间相关性及共享码本提升量化效率，图像条件解码适应场景变化。

Result: 将3DGS存储需求降至KB级并保持视觉保真度，实验显示其优于现有方法。

Conclusion: ICGS-Quantizer高效且灵活，适用于3D场景压缩与更新。

Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for
enabling high-quality real-time rendering. Although 3DGS compression methods
have been proposed for deployment on storage-constrained devices, two
limitations hinder archival use: (1) they compress medium-scale scenes only to
the megabyte range, which remains impractical for large-scale scenes or
extensive scene collections; and (2) they lack mechanisms to accommodate scene
changes after long-term archival. To address these limitations, we propose an
Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially
enhances compression efficiency and provides adaptability to scene changes
after archiving. ICGS-Quantizer improves quantization efficiency by jointly
exploiting inter-Gaussian and inter-attribute correlations and by using shared
codebooks across all training scenes, which are then fixed and applied to
previously unseen test scenes, eliminating the overhead of per-scene codebooks.
This approach effectively reduces the storage requirements for 3DGS to the
kilobyte range while preserving visual fidelity. To enable adaptability to
post-archival scene changes, ICGS-Quantizer conditions scene decoding on images
captured at decoding time. The encoding, quantization, and decoding processes
are trained jointly, ensuring that the codes, which are quantized
representations of the scene, are effective for conditional decoding. We
evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating.
Experimental results show that ICGS-Quantizer consistently outperforms
state-of-the-art methods in compression efficiency and adaptability to scene
changes. Our code, model, and data will be publicly available on GitHub.

</details>


### [73] [Scaling Group Inference for Diverse and High-Quality Generation](https://arxiv.org/abs/2508.15773)
*Gaurav Parmar,Or Patashnik,Daniil Ostashev,Kuan-Chieh Wang,Kfir Aberman,Srinivasa Narasimhan,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出一种可扩展的组推断方法，通过优化样本质量和多样性，解决生成模型中独立采样导致的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 现实中用户常需要一组多样化的输出，但独立采样容易导致冗余，限制了选择和探索。

Method: 将组推断建模为二次整数分配问题，利用图的节点表示候选输出，优化质量与多样性。通过渐进剪枝提升效率。

Result: 实验表明，该方法显著提升了组多样性和质量，适用于多种生成任务。

Conclusion: 该框架使生成模型能将多输出视为整体而非独立样本，提升了实用性。

Abstract: Generative models typically sample outputs independently, and recent
inference-time guidance and scaling algorithms focus on improving the quality
of individual samples. However, in real-world applications, users are often
presented with a set of multiple images (e.g., 4-8) for each prompt, where
independent sampling tends to lead to redundant results, limiting user choices
and hindering idea exploration. In this work, we introduce a scalable group
inference method that improves both the diversity and quality of a group of
samples. We formulate group inference as a quadratic integer assignment
problem: candidate outputs are modeled as graph nodes, and a subset is selected
to optimize sample quality (unary term) while maximizing group diversity
(binary term). To substantially improve runtime efficiency, we progressively
prune the candidate set using intermediate predictions, allowing our method to
scale up to large candidate sets. Extensive experiments show that our method
significantly improves group diversity and quality compared to independent
sampling baselines and recent inference algorithms. Our framework generalizes
across a wide range of tasks, including text-to-image, image-to-image, image
prompting, and video generation, enabling generative models to treat multiple
outputs as cohesive groups rather than independent samples.

</details>


### [74] [Reliable Multi-view 3D Reconstruction for `Just-in-time' Edge Environments](https://arxiv.org/abs/2508.15158)
*Md. Nurul Absur,Abhinav Kumar,Swastik Brahma,Saptarshi Debroy*

Main category: cs.CV

TL;DR: 该论文提出了一种基于投资组合理论的新型边缘资源管理策略，用于在多视图3D重建应用中应对系统中断问题，确保重建质量。


<details>
  <summary>Details</summary>
Motivation: 多视图3D重建在应急响应等场景中需要快速情境感知，但边缘环境的动态性和操作困难可能导致重建质量下降，因此需要可靠的资源管理策略。

Method: 提出了一种受投资组合理论启发的边缘资源管理方法，并使用遗传算法快速求解优化问题。

Result: 实验结果表明，与传统基线策略相比，该方法能在时空相关中断下保证重建质量的可靠性。

Conclusion: 该方法为动态边缘环境中的多视图3D重建提供了有效的资源管理解决方案。

Abstract: Multi-view 3D reconstruction applications are revolutionizing critical use
cases that require rapid situational-awareness, such as emergency response,
tactical scenarios, and public safety. In many cases, their near-real-time
latency requirements and ad-hoc needs for compute resources necessitate
adoption of `Just-in-time' edge environments where the system is set up on the
fly to support the applications during the mission lifetime. However,
reliability issues can arise from the inherent dynamism and operational
adversities of such edge environments, resulting in spatiotemporally correlated
disruptions that impact the camera operations, which can lead to sustained
degradation of reconstruction quality. In this paper, we propose a novel
portfolio theory inspired edge resource management strategy for reliable
multi-view 3D reconstruction against possible system disruptions. Our proposed
methodology can guarantee reconstruction quality satisfaction even when the
cameras are prone to spatiotemporally correlated disruptions. The portfolio
theoretic optimization problem is solved using a genetic algorithm that
converges quickly for realistic system settings. Using publicly available and
customized 3D datasets, we demonstrate the proposed camera selection strategy's
benefits in guaranteeing reliable 3D reconstruction against traditional
baseline strategies, under spatiotemporal disruptions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [75] [A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity](https://arxiv.org/abs/2508.15386)
*Sabine Houy,Bruno Kreyssig,Timothee Riom,Alexandre Bartel,Patrick McDaniel*

Main category: cs.CR

TL;DR: 本文总结了内存漏洞的危害，提出了基于LLVM的CFI分类方法，并通过实际漏洞案例评估其效果，为开发者在现有代码中逐步实施CFI提供指导。


<details>
  <summary>Details</summary>
Motivation: 内存漏洞是软件安全的重大威胁，但开发者缺乏在实际软件中应用CFI的具体指导。

Method: 建立LLVM前向边缘CFI变体的分类，针对四类高影响力漏洞进行评估。

Result: CFI在两种情况下能阻止漏洞利用，但在另外两种情况下失败。

Conclusion: 研究为CFI的实际部署提供了依据，并指出其潜力和局限性。

Abstract: Memory corruption vulnerabilities remain one of the most severe threats to
software security. They often allow attackers to achieve arbitrary code
execution by redirecting a vulnerable program's control flow. While Control
Flow Integrity (CFI) has gained traction to mitigate this exploitation path,
developers are not provided with any direction on how to apply CFI to
real-world software. In this work, we establish a taxonomy mapping LLVM's
forward-edge CFI variants to memory corruption vulnerability classes, offering
actionable guidance for developers seeking to deploy CFI incrementally in
existing codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)
list, we identify four high-impact vulnerability categories and select one
representative CVE for each. We evaluate LLVM's CFI against each CVE and
explain why CFI blocks exploitation in two cases while failing in the other
two, illustrating its potential and current limitations. Our findings support
informed deployment decisions and provide a foundation for improving the
practical use of CFI in production systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [76] [Optimizing Compilation for Distributed Quantum Computing via Clustering and Annealing](https://arxiv.org/abs/2508.15267)
*Ruilin Zhou,Jinglei Cheng,Yuhang Gan,Junyu Liu,Chen Qian*

Main category: quant-ph

TL;DR: 该论文提出了一种高效的量子程序编译框架，用于解决分布式量子计算中的异构量子处理单元映射问题。


<details>
  <summary>Details</summary>
Motivation: 分布式量子计算中，异构量子处理单元的高效映射是一个挑战性问题。

Method: 通过利用量子电路的结构模式、聚类初始量子比特放置，以及退火算法调整量子比特映射。

Result: 实验显示，该方法相比基线最多可减少88.40%的目标值。

Conclusion: 所提方法能有效处理复杂异构分布式量子系统。

Abstract: Efficiently mapping quantum programs onto Distributed quantum computing (DQC)
are challenging, particularly when considering the heterogeneous quantum
processing units (QPUs) with different structures. In this paper, we present a
comprehensive compilation framework that addresses these challenges with three
key insights: exploiting structural patterns within quantum circuits, using
clustering for initial qubit placement, and adjusting qubit mapping with
annealing algorithms. Experimental results demonstrate the effectiveness of our
methods and the capability to handle complex heterogeneous distributed quantum
systems. Our evaluation shows that our method reduces the objective value at
most 88.40\% compared to the baseline.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [77] [Neural Robot Dynamics](https://arxiv.org/abs/2508.15755)
*Jie Xu,Eric Heiden,Iretiayo Akinola,Dieter Fox,Miles Macklin,Yashraj Narang*

Main category: cs.RO

TL;DR: 神经网络模拟器NeRD解决了机器人动力学模拟的通用性问题，通过机器人中心和空间不变的模拟状态表示，能够泛化到新任务和环境。


<details>
  <summary>Details</summary>
Motivation: 现代机器人因高自由度和复杂机制，模拟面临挑战。现有神经模拟器需特定训练且泛化能力不足。

Method: 提出NeRD，替代传统低层动力学和接触求解器，采用机器人中心和空间不变的表示，并作为可互换后端求解器集成。

Result: 实验表明，NeRD在千步模拟中稳定准确，能泛化任务和环境，支持纯神经引擎策略学习，并可微调以缩小模拟与现实的差距。

Conclusion: NeRD为机器人动力学模拟提供了通用且高效的解决方案，能够适应新任务和环境，并通过真实数据微调提升性能。

Abstract: Accurate and efficient simulation of modern robots remains challenging due to
their high degrees of freedom and intricate mechanisms. Neural simulators have
emerged as a promising alternative to traditional analytical simulators,
capable of efficiently predicting complex dynamics and adapting to real-world
data; however, existing neural simulators typically require
application-specific training and fail to generalize to novel tasks and/or
environments, primarily due to inadequate representations of the global state.
In this work, we address the problem of learning generalizable neural
simulators for robots that are structured as articulated rigid bodies. We
propose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models
for predicting future states for articulated rigid bodies under contact
constraints. NeRD uniquely replaces the low-level dynamics and contact solvers
in an analytical simulator and employs a robot-centric and spatially-invariant
simulation state representation. We integrate the learned NeRD models as an
interchangeable backend solver within a state-of-the-art robotics simulator. We
conduct extensive experiments to show that the NeRD simulators are stable and
accurate over a thousand simulation steps; generalize across tasks and
environment configurations; enable policy learning exclusively in a neural
engine; and, unlike most classical simulators, can be fine-tuned from
real-world data to bridge the gap between simulation and reality.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [78] [JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs](https://arxiv.org/abs/2508.15468)
*Zhiqiang Que,Chang Sun,Sudarshan Paramesvaran,Emyr Clement,Katerina Karakoulaki,Christopher Brown,Lauri Laatu,Arianna Cox,Alexander Tapper,Wayne Luk,Maria Spiropulu*

Main category: hep-ex

TL;DR: JEDI-linear是一种新型GNN架构，通过共享变换和全局聚合实现线性计算复杂度，优化FPGA部署，显著降低延迟和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在硬件触发系统中因计算复杂性和内存访问模式导致部署困难，需满足严格延迟和资源限制。

Method: 引入JEDI-linear架构、细粒度量化感知训练和分布式算术的无乘法累加操作。

Result: 在FPGA上实现3.7至11.5倍延迟降低、150倍起始间隔优化和6.2倍LUT使用减少，且精度更高。

Conclusion: 该工作首次实现60纳秒以下延迟，满足HL-LHC触发系统需求，推动实时GNN推断应用。

Abstract: Graph Neural Networks (GNNs), particularly Interaction Networks (INs), have
shown exceptional performance for jet tagging at the CERN High-Luminosity Large
Hadron Collider (HL-LHC). However, their computational complexity and irregular
memory access patterns pose significant challenges for deployment on FPGAs in
hardware trigger systems, where strict latency and resource constraints apply.
In this work, we propose JEDI-linear, a novel GNN architecture with linear
computational complexity that eliminates explicit pairwise interactions by
leveraging shared transformations and global aggregation. To further enhance
hardware efficiency, we introduce fine-grained quantization-aware training with
per-parameter bitwidth optimization and employ multiplier-free
multiply-accumulate operations via distributed arithmetic. Evaluation results
show that our FPGA-based JEDI-linear achieves 3.7 to 11.5 times lower latency,
up to 150 times lower initiation interval, and up to 6.2 times lower LUT usage
compared to state-of-the-art designs while also delivering higher model
accuracy and eliminating the need for DSP blocks entirely. In contrast,
state-of-the-art solutions consume over 8,700 DSPs. This is the first
interaction-based GNN to achieve less than 60~ns latency and currently meets
the requirements for use in the HL-LHC CMS Level-1 trigger system. This work
advances the next-generation trigger systems by enabling accurate, scalable,
and resource-efficient GNN inference in real-time environments. Our
open-sourced templates will further support reproducibility and broader
adoption across scientific applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [79] [Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation Models in High-Resolution Medical Imaging](https://arxiv.org/abs/2508.14931)
*Zahra TehraniNasab,Amar Kumar,Tal Arbel*

Main category: eess.IV

TL;DR: 研究了多种微调技术对扩散基础模型在512x512高分辨率图像生成中的影响，评估了生成质量和下游分类任务表现。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像合成在医疗等领域需求增加，但现有研究多限于低分辨率，需探索微调技术在任务特定要求中的应用。

Method: 系统性地比较了全微调和参数高效微调（PEFT）等方法，分析了其对FID、Vendi分数和提示-图像对齐等指标的影响。

Result: 特定微调策略提升了生成图像的保真度，并在数据稀缺条件下增强了利用合成图像训练分类器的下游任务表现。

Conclusion: 微调技术在高分辨率图像生成中具有关键作用，能同时提升生成质量和下游任务的实用性。

Abstract: Advancements in diffusion-based foundation models have improved text-to-image
generation, yet most efforts have been limited to low-resolution settings. As
high-resolution image synthesis becomes increasingly essential for various
applications, particularly in medical imaging domains, fine-tuning emerges as a
crucial mechanism for adapting these powerful pre-trained models to
task-specific requirements and data distributions. In this work, we present a
systematic study, examining the impact of various fine-tuning techniques on
image generation quality when scaling to high resolution 512x512 pixels. We
benchmark a diverse set of fine-tuning methods, including full fine-tuning
strategies and parameter-efficient fine-tuning (PEFT). We dissect how different
fine-tuning methods influence key quality metrics, including Fr\'echet
Inception Distance (FID), Vendi score, and prompt-image alignment. We also
evaluate the utility of generated images in a downstream classification task
under data-scarce conditions, demonstrating that specific fine-tuning
strategies improve both generation fidelity and downstream performance when
synthetic images are used for classifier training and evaluation on real
images. Our code is accessible through the project website -
https://tehraninasab.github.io/PixelUPressure/.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [80] [Collaborative Filtering using Variational Quantum Hopfield Associative Memory](https://arxiv.org/abs/2508.14906)
*Amir Kermanshahani,Ebrahim Ardeshir-Larijani,Rakesh Saini,Saif Al-Kuwari*

Main category: cs.IR

TL;DR: 本文提出了一种结合量子Hopfield联想记忆（QHAM）和深度神经网络的混合推荐系统，在MovieLens 1M数据集上表现优异，同时在噪声环境下也保持稳定性能。


<details>
  <summary>Details</summary>
Motivation: 量子计算机在机器学习和推荐系统中的应用具有巨大潜力，但现有方法在噪声环境和计算资源利用上仍有改进空间。

Method: 结合QHAM和深度神经网络，使用K-Means聚类用户类型，并通过变分量子计算优化模型。

Result: 在理想环境下，ROC为0.9795，准确率为0.8841；在噪声环境下，ROC为0.9177，准确率为0.8013。

Conclusion: 该模型在真实数据集和噪声环境下均表现优异，为推荐系统提供了一种有前景的量子-经典混合方法。

Abstract: Quantum computing, with its ability to do exponentially faster computation
compared to classical systems, has found novel applications in various fields
such as machine learning and recommendation systems. Quantum Machine Learning
(QML), which integrates quantum computing with machine learning techniques,
presents powerful new tools for data processing and pattern recognition. This
paper proposes a hybrid recommendation system that combines Quantum Hopfield
Associative Memory (QHAM) with deep neural networks to improve the extraction
and classification on the MovieLens 1M dataset. User archetypes are clustered
into multiple unique groups using the K-Means algorithm and converted into
polar patterns through the encoder's activation function. These polar patterns
are then integrated into the variational QHAM-based hybrid recommendation
model. The system was trained using the MSE loss over 35 epochs in an ideal
environment, achieving an ROC value of 0.9795, an accuracy of 0.8841, and an
F-1 Score of 0.8786. Trained with the same number of epochs in a noisy
environment using a custom Qiskit AER noise model incorporating bit-flip and
readout errors with the same probabilities as in real quantum hardware, it
achieves an ROC of 0.9177, an accuracy of 0.8013, and an F-1 Score equal to
0.7866, demonstrating consistent performance.
  Additionally, we were able to optimize the qubit overhead present in previous
QHAM architectures by efficiently updating only one random targeted qubit. This
research presents a novel framework that combines variational quantum computing
with deep learning, capable of dealing with real-world datasets with comparable
performance compared to purely classical counterparts. Additionally, the model
can perform similarly well in noisy configurations, showcasing a steady
performance and proposing a promising direction for future usage in
recommendation systems.

</details>


### [81] [On the Effectiveness of Graph Reordering for Accelerating Approximate Nearest Neighbor Search on GPU](https://arxiv.org/abs/2508.15436)
*Yutaro Oguri,Mai Nishimura,Yusuke Matsui*

Main category: cs.IR

TL;DR: 该论文首次系统地研究了GPU上基于图的近似最近邻搜索（ANNS）中图重排序的效果，提出了一个统一评估框架，并通过优化内存布局提升了15%的查询速度。


<details>
  <summary>Details</summary>
Motivation: 虽然基于图的ANNS已成为现代AI应用的主导范式，但现有方法多关注算法创新，忽视了内存布局对执行时间的显著影响。

Method: 研究通过图适配器和GPU优化的图遍历引擎，评估了不同图索引的多种重排序策略，并引入了量化结构属性与内存布局效果关系的新指标。

Result: 针对GPU优化的重排序策略在保持搜索精度的同时，实现了高达15%的查询速度提升。

Conclusion: 内存布局优化与现有算法创新正交，未来研究可结合两者进一步提升性能。

Abstract: We present the first systematic investigation of graph reordering effects for
graph-based Approximate Nearest Neighbor Search (ANNS) on a GPU. While
graph-based ANNS has become the dominant paradigm for modern AI applications,
recent approaches focus on algorithmic innovations while neglecting memory
layout considerations that significantly affect execution time. Our unified
evaluation framework enables comprehensive evaluation of diverse reordering
strategies across different graph indices through a graph adapter that converts
arbitrary graph topologies into a common representation and a GPU-optimized
graph traversal engine. We conduct a comprehensive analysis across diverse
datasets and state-of-the-art graph indices, introducing analysis metrics that
quantify the relationship between structural properties and memory layout
effectiveness. Our GPU-targeted reordering achieves up to 15$\%$ QPS
improvements while preserving search accuracy, demonstrating that memory layout
optimization operates orthogonally to existing algorithmic innovations. We will
release all code upon publication to facilitate reproducibility and foster
further research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [82] [Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions](https://arxiv.org/abs/2508.15047)
*Yibo Liu,Liam Shatzel,Brandon Haworth,Teseo Schneider*

Main category: cs.AI

TL;DR: 提出了一种利用大型语言模型（LLMs）控制群体动画中代理导航和对话的新方法，以增强社交和环境的交互真实性。


<details>
  <summary>Details</summary>
Motivation: 现有群体动画中代理间交互多限于简单导航和固定目标，未能充分模拟社交和语言驱动的复杂行为。

Method: 结合对话系统和语言驱动导航，通过LLMs生成代理间对话并基于其个性、情感等状态控制导航行为。

Result: 在复杂场景中实现了自然的群体行为和分组动态，代理间的信息传递更为真实。

Conclusion: 该方法显著提升了群体动画的真实性和社交交互的自然性。

Abstract: Animating and simulating crowds using an agent-based approach is a
well-established area where every agent in the crowd is individually controlled
such that global human-like behaviour emerges. We observe that human navigation
and movement in crowds are often influenced by complex social and environmental
interactions, driven mainly by language and dialogue. However, most existing
work does not consider these dimensions and leads to animations where
agent-agent and agent-environment interactions are largely limited to steering
and fixed higher-level goal extrapolation.
  We propose a novel method that exploits large language models (LLMs) to
control agents' movement. Our method has two main components: a dialogue system
and language-driven navigation. We periodically query agent-centric LLMs
conditioned on character personalities, roles, desires, and relationships to
control the generation of inter-agent dialogue when necessitated by the spatial
and social relationships with neighbouring agents. We then use the conversation
and each agent's personality, emotional state, vision, and physical state to
control the navigation and steering of each agent. Our model thus enables
agents to make motion decisions based on both their perceptual inputs and the
ongoing dialogue.
  We validate our method in two complex scenarios that exemplify the interplay
between social interactions, steering, and crowding. In these scenarios, we
observe that grouping and ungrouping of agents automatically occur.
Additionally, our experiments show that our method serves as an
information-passing mechanism within the crowd. As a result, our framework
produces more realistic crowd simulations, with emergent group behaviours
arising naturally from any environmental setting.

</details>


### [83] [Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle](https://arxiv.org/abs/2508.15680)
*Mark Cote,Susana Aires*

Main category: cs.AI

TL;DR: 论文探讨了欧盟AI法案的技术哲学解读如何揭示AI系统中数据的长期动态，提出一种新概念工具分析AI生命周期，并强调政策制定中缺乏对AI动态演变的考量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补政策制定中对AI技术动态和经济逻辑忽视的空白，通过技术哲学视角揭示AI生命周期中的价值链问题。

Method: 采用跨学科方法，结合Simondon技术哲学，提出‘futurity’概念模型，分析AI从数据到部署的递归过程及监管盲点。

Result: 揭示了AI生命周期中的递归价值链及权力不对称问题，提出了包括审计、透明度和追责等监管措施。

Conclusion: 有效监管需关注AI的基础设施和时间动态，需实施生命周期审计等具体措施以应对技术寡头的权力集中。

Abstract: This paper argues that a techno-philosophical reading of the EU AI Act
provides insight into the long-term dynamics of data in AI systems,
specifically, how the lifecycle from ingestion to deployment generates
recursive value chains that challenge existing frameworks for Responsible AI.
We introduce a conceptual tool to frame the AI pipeline, spanning data,
training regimes, architectures, feature stores, and transfer learning. Using
cross-disciplinary methods, we develop a technically grounded and
philosophically coherent analysis of regulatory blind spots. Our central claim
is that what remains absent from policymaking is an account of the dynamic of
becoming that underpins both the technical operation and economic logic of AI.
To address this, we advance a formal reading of AI inspired by Simondonian
philosophy of technology, reworking his concept of individuation to model the
AI lifecycle, including the pre-individual milieu, individuation, and
individuated AI. To translate these ideas, we introduce futurity: the
self-reinforcing lifecycle of AI, where more data enhances performance, deepens
personalisation, and expands application domains. Futurity highlights the
recursively generative, non-rivalrous nature of data, underpinned by
infrastructures like feature stores that enable feedback, adaptation, and
temporal recursion. Our intervention foregrounds escalating power asymmetries,
particularly the tech oligarchy whose infrastructures of capture, training, and
deployment concentrate value and decision-making. We argue that effective
regulation must address these infrastructural and temporal dynamics, and
propose measures including lifecycle audits, temporal traceability, feedback
accountability, recursion transparency, and a right to contest recursive reuse.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models](https://arxiv.org/abs/2508.15357)
*Haji Gul,Abul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.CL

TL;DR: 提出了一种新的知识图谱补全评估方法EDAS，整合多数据集和指标提供统一评分。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在多数据集和指标上表现不一致，难以全面比较模型性能。

Method: 提出EDAS方法，通过距离平均解的综合评分统一评估模型表现。

Result: 在FB15k-237和WN18RR等数据集上验证了EDAS的一致性和鲁棒性。

Conclusion: EDAS为知识图谱补全模型提供了更可靠和可解释的评估框架。

Abstract: Knowledge Graphs (KGs) enable applications in various domains such as
semantic search, recommendation systems, and natural language processing. KGs
are often incomplete, missing entities and relations, an issue addressed by
Knowledge Graph Completion (KGC) methods that predict missing elements.
Different evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank
(MR), and Hit@k, are commonly used to assess the performance of such KGC
models. A major challenge in evaluating KGC models, however, lies in comparing
their performance across multiple datasets and metrics. A model may outperform
others on one dataset but underperform on another, making it difficult to
determine overall superiority. Moreover, even within a single dataset,
different metrics such as MRR and Hit@1 can yield conflicting rankings, where
one model excels in MRR while another performs better in Hit@1, further
complicating model selection for downstream tasks. These inconsistencies hinder
holistic comparisons and highlight the need for a unified meta-metric that
integrates performance across all metrics and datasets to enable a more
reliable and interpretable evaluation framework. To address this need, we
propose KG Evaluation based on Distance from Average Solution (EDAS), a robust
and interpretable meta-metric that synthesizes model performance across
multiple datasets and diverse evaluation criteria into a single normalized
score ($M_i \in [0,1]$). Unlike traditional metrics that focus on isolated
aspects of performance, EDAS offers a global perspective that supports more
informed model selection and promotes fairness in cross-dataset evaluation.
Experimental results on benchmark datasets such as FB15k-237 and WN18RR
demonstrate that EDAS effectively integrates multi-metric, multi-dataset
performance into a unified ranking, offering a consistent, robust, and
generalizable framework for evaluating KGC models.

</details>


### [85] [SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version](https://arxiv.org/abs/2508.15478)
*Nghiem Thanh Pham,Tung Kieu,Duc-Manh Nguyen,Son Ha Xuan,Nghia Duong-Trung,Danh Le-Phuoc*

Main category: cs.CL

TL;DR: SLM-Bench是首个专门评估小型语言模型（SLM）的基准测试，涵盖准确性、计算效率和可持续性等多维度。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对SLM性能和环境影响系统性的评估，SLM-Bench填补了这一空白。

Method: 在4种硬件配置下，对15个SLM在9个NLP任务的23个数据集中进行评估，包括11个指标的量化分析。

Result: 不同SLM在准确性和能源效率方面表现出显著差异。

Conclusion: SLM-Bench为SLM评估设定了新标准，平衡资源效率与实际应用。

Abstract: Small Language Models (SLMs) offer computational efficiency and
accessibility, yet a systematic evaluation of their performance and
environmental impact remains lacking. We introduce SLM-Bench, the first
benchmark specifically designed to assess SLMs across multiple dimensions,
including accuracy, computational efficiency, and sustainability metrics.
SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14
domains. The evaluation is conducted on 4 hardware configurations, providing a
rigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench
quantifies 11 metrics across correctness, computation, and consumption,
enabling a holistic assessment of efficiency trade-offs. Our evaluation
considers controlled hardware conditions, ensuring fair comparisons across
models. We develop an open-source benchmarking pipeline with standardized
evaluation protocols to facilitate reproducibility and further research. Our
findings highlight the diverse trade-offs among SLMs, where some models excel
in accuracy while others achieve superior energy efficiency. SLM-Bench sets a
new standard for SLM evaluation, bridging the gap between resource efficiency
and real-world applicability.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [86] [Transition-based vs stated-based acceptance for automata over infinite words](https://arxiv.org/abs/2508.15402)
*Antonio Casares*

Main category: cs.FL

TL;DR: 这篇论文探讨了从基于状态的接受条件转向基于转移的接受条件的原因及其在无限字自动机中的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的无限对象自动机使用基于状态的接受条件，但近年来逐渐转向基于转移的接受条件。本文旨在分析这一转变的原因及其在逻辑和形式验证中的影响。

Method: 通过调查和分析，作者比较了基于状态和基于转移的接受条件在无限字自动机中的差异，并探讨了在不同问题中的表现。

Result: 研究表明，在某些问题中，基于转移的接受条件比基于状态的条件更具优势，能够更高效地解决问题。

Conclusion: 作者认为基于转移的接受条件在无限字自动机中具有更广泛的应用前景，并建议进一步研究其潜力。

Abstract: Automata over infinite objects are a well-established model with applications
in logic and formal verification. Traditionally, acceptance in such automata is
defined based on the set of states visited infinitely often during a run.
However, there is a growing trend towards defining acceptance based on
transitions rather than states.
  In this survey, we analyse the reasons for this shift and advocate using
transition-based acceptance in the context of automata over infinite words. We
present a collection of problems where the choice of formalism has a major
impact and discuss the causes of these differences.

</details>
