{"id": "2508.03846", "pdf": "https://arxiv.org/pdf/2508.03846", "abs": "https://arxiv.org/abs/2508.03846", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "title": "Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices", "categories": ["cs.SE"], "comment": null, "summary": "Empathy is a powerful yet often overlooked element in software engineering\n(SE), supporting better teamwork, smoother communication, and effective\ndecision-making. In our previous study, we identified a range of practitioner\nstrategies for fostering empathy in SE contexts. Building on these insights,\nthis paper introduces 17 actionable empathy guidelines designed to support\npractitioners, teams, and organisations. We also explore how these guidelines\ncan be implemented in practice by examining real-world applications,\nchallenges, and strategies to overcome them shared by software practitioners.\nTo support adoption, we present a visual prioritisation framework that\ncategorises the guidelines based on perceived importance, ease of\nimplementation, and willingness to adopt. The findings offer practical and\nflexible suggestions for integrating empathy into everyday SE work, helping\nteams move from principles to sustainable action.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e8617\u6761\u53ef\u64cd\u4f5c\u7684\u5171\u60c5\u6307\u5357\uff0c\u5e2e\u52a9\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u63d0\u5347\u6c9f\u901a\u4e0e\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u53ef\u89c6\u5316\u4f18\u5148\u7ea7\u6846\u67b6\u4ee5\u4fc3\u8fdb\u5b9e\u65bd\u3002", "motivation": "\u5171\u60c5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u88ab\u5ffd\u89c6\uff0c\u4f46\u80fd\u6539\u5584\u56e2\u961f\u534f\u4f5c\u4e0e\u6c9f\u901a\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u5c06\u5171\u60c5\u878d\u5165\u5b9e\u8df5\u3002", "method": "\u57fa\u4e8e\u5148\u524d\u7814\u7a76\uff0c\u63d0\u51fa17\u6761\u5171\u60c5\u6307\u5357\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u63a2\u8ba8\u5b9e\u65bd\u7b56\u7565\u4e0e\u6311\u6218\u3002", "result": "\u5f00\u53d1\u4e86\u53ef\u89c6\u5316\u4f18\u5148\u7ea7\u6846\u67b6\uff0c\u5c06\u6307\u5357\u6309\u91cd\u8981\u6027\u3001\u5b9e\u65bd\u96be\u6613\u548c\u91c7\u7eb3\u610f\u613f\u5206\u7c7b\uff0c\u63d0\u4f9b\u7075\u6d3b\u5b9e\u8df5\u5efa\u8bae\u3002", "conclusion": "\u5171\u60c5\u6307\u5357\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u63d0\u4f9b\u4e86\u4ece\u7406\u8bba\u5230\u53ef\u6301\u7eed\u884c\u52a8\u7684\u5177\u4f53\u8def\u5f84\uff0c\u4fc3\u8fdb\u5171\u60c5\u5728\u65e5\u5e38\u5de5\u4f5c\u4e2d\u7684\u6574\u5408\u3002"}}
{"id": "2508.03856", "pdf": "https://arxiv.org/pdf/2508.03856", "abs": "https://arxiv.org/abs/2508.03856", "authors": ["Richard Hegewald", "Rebecca Beyer"], "title": "Evaluating Software Supply Chain Security in Research Software", "categories": ["cs.SE", "cs.CR"], "comment": "Accepted at conference GI SKILL 2025", "summary": "The security of research software is essential for ensuring the integrity and\nreproducibility of scientific results. However, research software security is\nstill largely unexplored. Due to its dependence on open source components and\ndistributed development practices, research software is particularly vulnerable\nto supply chain attacks. This study analyses 3,248 high-quality, largely\npeer-reviewed research software repositories using the OpenSSF Scorecard. We\nfind a generally weak security posture with an average score of 3.5/10.\nImportant practices, such as signed releases and branch protection, are rarely\nimplemented. Finally, we present actionable, low-effort recommendations that\ncan help research teams improve software security and mitigate potential\nthreats to scientific integrity.", "AI": {"tldr": "\u7814\u7a76\u8f6f\u4ef6\u5b89\u5168\u6027\u73b0\u72b6\u5206\u6790\u53ca\u6539\u8fdb\u5efa\u8bae", "motivation": "\u7814\u7a76\u8f6f\u4ef6\u7684\u5b89\u5168\u6027\u5bf9\u79d1\u5b66\u7ed3\u679c\u7684\u5b8c\u6574\u6027\u548c\u53ef\u91cd\u590d\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u76f8\u5173\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u4f7f\u7528OpenSSF Scorecard\u5206\u67903,248\u4e2a\u9ad8\u8d28\u91cf\u3001\u5927\u591a\u7ecf\u8fc7\u540c\u884c\u8bc4\u5ba1\u7684\u7814\u7a76\u8f6f\u4ef6\u4ed3\u5e93\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b89\u5168\u6001\u52bf\u666e\u904d\u8f83\u5f31\uff0c\u5e73\u5747\u5f97\u5206\u4ec5\u4e3a3.5/10\uff0c\u91cd\u8981\u5b89\u5168\u63aa\u65bd\u5982\u7b7e\u540d\u53d1\u5e03\u548c\u5206\u652f\u4fdd\u62a4\u5f88\u5c11\u5b9e\u65bd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u53ef\u884c\u7684\u4f4e\u5de5\u4f5c\u91cf\u5efa\u8bae\uff0c\u5e2e\u52a9\u7814\u7a76\u56e2\u961f\u63d0\u5347\u8f6f\u4ef6\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u5bf9\u79d1\u5b66\u5b8c\u6574\u6027\u7684\u6f5c\u5728\u5a01\u80c1\u3002"}}
{"id": "2508.03881", "pdf": "https://arxiv.org/pdf/2508.03881", "abs": "https://arxiv.org/abs/2508.03881", "authors": ["Martin Obaidi", "Kushtrim Qengaj", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl\u00fcnder", "Elisa Schmid", "Kurt Schneider"], "title": "From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential", "categories": ["cs.SE"], "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "In today's digitized world, software systems must support users in\nunderstanding both how to interact with a system and why certain behaviors\noccur. This study investigates whether explanation needs, classified from user\nreviews, can be predicted based on app properties, enabling early consideration\nduring development and large-scale requirements mining. We analyzed a gold\nstandard dataset of 4,495 app reviews enriched with metadata (e.g., app\nversion, ratings, age restriction, in-app purchases). Correlation analyses\nidentified mostly weak associations between app properties and explanation\nneeds, with moderate correlations only for specific features such as app\nversion, number of reviews, and star ratings. Linear regression models showed\nlimited predictive power, with no reliable forecasts across configurations.\nValidation on a manually labeled dataset of 495 reviews confirmed these\nfindings. Categories such as Security & Privacy and System Behavior showed\nslightly higher predictive potential, while Interaction and User Interface\nremained most difficult to predict. Overall, our results highlight that\nexplanation needs are highly context-dependent and cannot be precisely inferred\nfrom app metadata alone. Developers and requirements engineers should therefore\nsupplement metadata analysis with direct user feedback to effectively design\nexplainable and user-centered software systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u80fd\u5426\u901a\u8fc7\u5e94\u7528\u5c5e\u6027\u9884\u6d4b\u7528\u6237\u5bf9\u89e3\u91ca\u7684\u9700\u6c42\uff0c\u53d1\u73b0\u5173\u8054\u6027\u8f83\u5f31\uff0c\u4e14\u9884\u6d4b\u6a21\u578b\u6548\u679c\u6709\u9650\uff0c\u5efa\u8bae\u7ed3\u5408\u7528\u6237\u53cd\u9988\u4ee5\u66f4\u597d\u5730\u8bbe\u8ba1\u8f6f\u4ef6\u7cfb\u7edf\u3002", "motivation": "\u5728\u6570\u5b57\u5316\u65f6\u4ee3\uff0c\u8f6f\u4ef6\u7cfb\u7edf\u9700\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u4ea4\u4e92\u53ca\u884c\u4e3a\u539f\u56e0\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5e94\u7528\u5c5e\u6027\u9884\u6d4b\u7528\u6237\u89e3\u91ca\u9700\u6c42\uff0c\u4ee5\u652f\u6301\u5f00\u53d1\u548c\u9700\u6c42\u6316\u6398\u3002", "method": "\u5206\u6790\u4e864,495\u6761\u5305\u542b\u5143\u6570\u636e\u7684\u5e94\u7528\u8bc4\u8bba\uff0c\u8fdb\u884c\u76f8\u5173\u6027\u5206\u6790\u548c\u7ebf\u6027\u56de\u5f52\u5efa\u6a21\uff0c\u5e76\u5728495\u6761\u624b\u52a8\u6807\u8bb0\u6570\u636e\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5e94\u7528\u5c5e\u6027\u4e0e\u89e3\u91ca\u9700\u6c42\u95f4\u591a\u4e3a\u5f31\u5173\u8054\uff0c\u4ec5\u7279\u5b9a\u7279\u5f81\uff08\u5982\u7248\u672c\u3001\u8bc4\u8bba\u6570\u548c\u8bc4\u5206\uff09\u6709\u4e2d\u5ea6\u5173\u8054\uff0c\u9884\u6d4b\u6a21\u578b\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u89e3\u91ca\u9700\u6c42\u9ad8\u5ea6\u4f9d\u8d56\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u4ec5\u4ece\u5143\u6570\u636e\u4e2d\u7cbe\u786e\u63a8\u65ad\uff0c\u5efa\u8bae\u5f00\u53d1\u8005\u7ed3\u5408\u7528\u6237\u53cd\u9988\u8bbe\u8ba1\u66f4\u6613\u7406\u89e3\u7684\u7cfb\u7edf\u3002"}}
{"id": "2508.03922", "pdf": "https://arxiv.org/pdf/2508.03922", "abs": "https://arxiv.org/abs/2508.03922", "authors": ["Soroush Heydari"], "title": "A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output", "categories": ["cs.SE", "cs.HC", "D.2.1"], "comment": "8 pages", "summary": "The rapid adoption of Artificial Intelligence(AI) programming assistants such\nas GitHub Copilot introduces new challenges in how these software tools address\nhuman needs. Many existing evaluation frameworks address technical aspects such\nas code correctness and efficiency, but often overlook crucial human factors\nthat affect the successful integration of AI assistants in software development\nworkflows. In this study, I analyzed GitHub Copilot's interaction with users\nthrough its chat interface, measured Copilot's ability to adapt explanations\nand code generation to user expertise levels, and assessed its effectiveness in\nfacilitating collaborative programming experiences. I established a\nhuman-centered requirements framework with clear metrics to evaluate these\nqualities in GitHub Copilot chat. I discussed the test results and their\nimplications for future analysis of human requirements in automated\nprogramming.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86GitHub Copilot\u5982\u4f55\u9002\u5e94\u4e0d\u540c\u7528\u6237\u7684\u6280\u672f\u6c34\u5e73\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u9700\u6c42\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u5f80\u5f80\u5ffd\u7565\u4eba\u7c7b\u56e0\u7d20\uff0c\u800cAI\u7f16\u7a0b\u52a9\u624b\u9700\u66f4\u597d\u5730\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5206\u6790Copilot\u7684\u804a\u5929\u4ea4\u4e92\uff0c\u6d4b\u91cf\u5176\u9002\u5e94\u6027\u548c\u534f\u4f5c\u80fd\u529b\uff0c\u5efa\u7acb\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u660e\u786e\u7684\u6307\u6807\u8bc4\u4f30Copilot\u7684\u4eba\u6027\u5316\u8868\u73b0\uff0c\u5e76\u8ba8\u8bba\u4e86\u6d4b\u8bd5\u7ed3\u679c\u3002", "conclusion": "\u672a\u6765\u81ea\u52a8\u5316\u7f16\u7a0b\u9700\u66f4\u5173\u6ce8\u4eba\u7c7b\u9700\u6c42\uff0c\u7814\u7a76\u4e3a\u76f8\u5173\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.04078", "pdf": "https://arxiv.org/pdf/2508.04078", "abs": "https://arxiv.org/abs/2508.04078", "authors": ["Zhan Li", "Huangying Zhan", "Changyang Li", "Qingan Yan", "Yi Xu"], "title": "RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "14 pages, 9 figures", "summary": "Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive\nand expert-driven process, often resulting in inconsistent reconstructions and\nsuboptimal results. We propose RLGS, a plug-and-play reinforcement learning\nframework for adaptive hyperparameter tuning in 3DGS through lightweight policy\nmodules, dynamically adjusting critical hyperparameters such as learning rates\nand densification thresholds. The framework is model-agnostic and seamlessly\nintegrates into existing 3DGS pipelines without architectural modifications. We\ndemonstrate its generalization ability across multiple state-of-the-art 3DGS\nvariants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness\nacross diverse datasets. RLGS consistently enhances rendering quality. For\nexample, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT)\ndataset, under a fixed Gaussian budget, and continues to yield gains even when\nbaseline performance saturates. Our results suggest that RLGS provides an\neffective and general solution for automating hyperparameter tuning in 3DGS\ntraining, bridging a gap in applying reinforcement learning to 3DGS.", "AI": {"tldr": "RLGS\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u8d85\u53c2\u6570\u8c03\u4f18\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "3DGS\u4e2d\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u8fc7\u7a0b\u7e41\u7410\u4e14\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u4e0d\u4e00\u81f4\u4e14\u6548\u679c\u6b20\u4f73\u3002", "method": "\u63d0\u51faRLGS\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b56\u7565\u6a21\u5757\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u7387\u548c\u5bc6\u96c6\u5316\u9608\u503c\u7b49\u5173\u952e\u8d85\u53c2\u6570\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u67093DGS\u67b6\u6784\u3002", "result": "RLGS\u5728\u591a\u79cd3DGS\u53d8\u4f53\uff08\u5982Taming-3DGS\u548c3DGS-MCMC\uff09\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a33\u5065\uff0c\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\uff08\u5982PSNR\u63d0\u9ad80.7dB\uff09\u3002", "conclusion": "RLGS\u4e3a3DGS\u8bad\u7ec3\u4e2d\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u63d0\u4f9b\u4e86\u9ad8\u6548\u901a\u7528\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u5f3a\u5316\u5b66\u4e60\u57283DGS\u4e2d\u7684\u5e94\u7528\u7a7a\u767d\u3002"}}
{"id": "2508.03862", "pdf": "https://arxiv.org/pdf/2508.03862", "abs": "https://arxiv.org/abs/2508.03862", "authors": ["Abdul Saboor", "Zhuangzhuang Cui", "Achiel Colpaert", "Evgenii Vinogradov", "Sofie Pollin"], "title": "CASH: Context-Aware Smart Handover for Reliable UAV Connectivity on Aerial Corridors", "categories": ["cs.NI", "eess.SP"], "comment": "Accepted at IEEE Globecom 2025", "summary": "Urban Air Mobility (UAM) envisions aerial corridors for Unmanned Aerial\nVehicles (UAVs) to reduce ground traffic congestion by supporting 3D mobility,\nsuch as air taxis. A key challenge in these high-mobility aerial corridors is\nensuring reliable connectivity, where frequent handovers can degrade network\nperformance. To resolve this, we present a Context-Aware Smart Handover (CASH)\nprotocol that uses a forward-looking scoring mechanism based on UAV trajectory\nto make proactive handover decisions. We evaluate the performance of the\nproposed CASH against existing handover protocols in a custom-built simulator.\nResults show that CASH reduces handover frequency by up to 78% while\nmaintaining low outage probability. We then investigate the impact of base\nstation density and safety margin on handover performance, where their optimal\nsetups are empirically obtained to ensure reliable UAM communication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u667a\u80fd\u5207\u6362\u534f\u8bae\uff08CASH\uff09\uff0c\u901a\u8fc7\u9884\u6d4b\u65e0\u4eba\u673a\u8f68\u8ff9\u51cf\u5c11\u5207\u6362\u9891\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7684\u901a\u4fe1\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u4e2d\u9891\u7e41\u5207\u6362\u5bfc\u81f4\u7f51\u7edc\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u57283D\u7a7a\u4e2d\u8d70\u5eca\u7684\u901a\u4fe1\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faCASH\u534f\u8bae\uff0c\u5229\u7528\u65e0\u4eba\u673a\u8f68\u8ff9\u7684\u9884\u6d4b\u8bc4\u5206\u673a\u5236\uff0c\u4e3b\u52a8\u4f18\u5316\u5207\u6362\u51b3\u7b56\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u4e0e\u73b0\u6709\u534f\u8bae\u5bf9\u6bd4\u3002", "result": "CASH\u5c06\u5207\u6362\u9891\u7387\u964d\u4f4e78%\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u4e2d\u65ad\u6982\u7387\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u786e\u5b9a\u4e86\u57fa\u7ad9\u5bc6\u5ea6\u548c\u5b89\u5168\u88d5\u5ea6\u7684\u6700\u4f18\u914d\u7f6e\u3002", "conclusion": "CASH\u534f\u8bae\u663e\u8457\u63d0\u5347\u4e86\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u4e3a\u672a\u67653D\u7a7a\u4e2d\u4ea4\u901a\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2312.10925", "pdf": "https://arxiv.org/pdf/2312.10925", "abs": "https://arxiv.org/abs/2312.10925", "authors": ["Md Zesun Ahmed Mia", "Malyaban Bal", "Abhronil Sengupta"], "title": "Delving Deeper Into Astromorphic Transformers", "categories": ["cs.NE", "cs.AI", "cs.ET", "cs.LG"], "comment": null, "summary": "Preliminary attempts at incorporating the critical role of astrocytes - cells\nthat constitute more than 50\\% of human brain cells - in brain-inspired\nneuromorphic computing remain in infancy. This paper seeks to delve deeper into\nvarious key aspects of neuron-synapse-astrocyte interactions to mimic\nself-attention mechanisms in Transformers. The cross-layer perspective explored\nin this work involves bioplausible modeling of Hebbian and presynaptic\nplasticities in neuron-astrocyte networks, incorporating effects of\nnon-linearities and feedback along with algorithmic formulations to map the\nneuron-astrocyte computations to self-attention mechanism and evaluating the\nimpact of incorporating bio-realistic effects from the machine learning\napplication side. Our analysis on sentiment and image classification tasks\n(IMDB and CIFAR10 datasets) highlights the advantages of Astromorphic\nTransformers, offering improved accuracy and learning speed. Furthermore, the\nmodel demonstrates strong natural language generation capabilities on the\nWikiText-2 dataset, achieving better perplexity compared to conventional\nmodels, thus showcasing enhanced generalization and stability across diverse\nmachine learning tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u795e\u7ecf\u5143-\u7a81\u89e6-\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u76f8\u4e92\u4f5c\u7528\u7684\u751f\u7269\u5408\u7406\u6027\u5efa\u6a21\uff0c\u4ee5\u6a21\u62dfTransformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5c55\u793a\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u5728\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4ee5\u63d0\u5347\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u751f\u7269\u5408\u7406\u6027\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u5143-\u661f\u5f62\u80f6\u8d28\u7ec6\u80de\u7f51\u7edc\u7684Hebbian\u548c\u7a81\u89e6\u524d\u540e\u53ef\u5851\u6027\u5efa\u6a21\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u4e0e\u53cd\u9988\u6548\u5e94\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u3002", "result": "Astromorphic Transformers\u5728\u60c5\u611f\u548c\u56fe\u50cf\u5206\u7c7b\uff08IMDB\u548cCIFAR10\uff09\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u5b66\u4e60\u901f\u5ea6\uff0c\u5728WikiText-2\u4e0a\u8bed\u8a00\u751f\u6210\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u6a21\u578b\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u4f18\u5f02\u6cdb\u5316\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u751f\u7269\u542f\u53d1\u5f0f\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.03767", "pdf": "https://arxiv.org/pdf/2508.03767", "abs": "https://arxiv.org/abs/2508.03767", "authors": ["Sandeepa Kannangara", "Arman Abrahamyan", "Daniel Elias", "Thomas Kilby", "Nadav Dar", "Luiz Pizzato", "Anna Leontjeva", "Dan Jermyn"], "title": "A Robust and Efficient Pipeline for Enterprise-Level Large-Scale Entity Resolution", "categories": ["cs.DB", "cs.IR", "cs.LG", "H.3.3"], "comment": "10 pages, 5 figures", "summary": "Entity resolution (ER) remains a significant challenge in data management,\nespecially when dealing with large datasets. This paper introduces MERAI\n(Massive Entity Resolution using AI), a robust and efficient pipeline designed\nto address record deduplication and linkage issues in high-volume datasets at\nan enterprise level. The pipeline's resilience and accuracy have been validated\nthrough various large-scale record deduplication and linkage projects. To\nevaluate MERAI's performance, we compared it with two well-known entity\nresolution libraries, Dedupe and Splink. While Dedupe failed to scale beyond 2\nmillion records due to memory constraints, MERAI successfully processed\ndatasets of up to 15.7 million records and produced accurate results across all\nexperiments. Experimental data demonstrates that MERAI outperforms both\nbaseline systems in terms of matching accuracy, with consistently higher F1\nscores in both deduplication and record linkage tasks. MERAI offers a scalable\nand reliable solution for enterprise-level large-scale entity resolution,\nensuring data integrity and consistency in real-world applications.", "AI": {"tldr": "MERAI\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5b9e\u4f53\u89e3\u6790\uff08ER\uff09\u7ba1\u9053\uff0c\u4e13\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5176\u9ad8\u51c6\u786e\u6027\u548c\u6269\u5c55\u80fd\u529b\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8eDedupe\u548cSplink\u3002", "motivation": "\u5b9e\u4f53\u89e3\u6790\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u65f6\u4ecd\u5177\u6311\u6218\u6027\uff0cMERAI\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u6ee1\u8db3\u4f01\u4e1a\u7ea7\u9700\u6c42\u3002", "method": "MERAI\u91c7\u7528\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u7ba1\u9053\uff0c\u4e13\u6ce8\u4e8e\u8bb0\u5f55\u53bb\u91cd\u548c\u94fe\u63a5\uff0c\u5e76\u901a\u8fc7\u4e0eDedupe\u548cSplink\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "MERAI\u6210\u529f\u5904\u7406\u4e86\u9ad8\u8fbe1570\u4e07\u6761\u8bb0\u5f55\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u5339\u914d\u51c6\u786e\u6027\u548cF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "MERAI\u4e3a\u5927\u89c4\u6a21\u5b9e\u4f53\u89e3\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6570\u636e\u5b8c\u6574\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.03760", "pdf": "https://arxiv.org/pdf/2508.03760", "abs": "https://arxiv.org/abs/2508.03760", "authors": ["Qingyuan Li", "Bo Zhang", "Hui Kang", "Tianhao Xu", "Yulei Qian", "Yuchen Xie", "Lin Ma"], "title": "FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication", "categories": ["cs.DC", "cs.AI"], "comment": "9 pages, 8 figures", "summary": "Nowadays, communication bottlenecks have emerged as a critical challenge in\nthe distributed training and deployment of large language models (LLMs). This\npaper introduces FlashCommunication V2, a novel communication paradigm enabling\nefficient cross-GPU transmission at arbitrary bit widths. Its core innovations\nlie in the proposed bit splitting and spike reserving techniques, which address\nthe challenges of low-bit quantization. Bit splitting decomposes irregular bit\nwidths into basic units, ensuring compatibility with hardware capabilities and\nthus enabling transmission at any bit width. Spike reserving, on the other\nhand, retains numerical outliers (i.e., minima and maxima) as floating-point\nnumbers, which shrinks the dynamic numerical range and pushes the quantization\nlimits to 2-bit with acceptable losses. FlashCommunication V2 significantly\nenhances the flexibility and resource utilization of communication systems.\nThrough meticulous software-hardware co-design, it delivers robust performance\nand reduced overhead across both NVLink-based and PCIe-based architectures,\nachieving a maximum 3.2$\\times$ speedup in AllReduce and 2$\\times$ in All2All\ncommunication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFlashCommunication V2\uff0c\u901a\u8fc7\u6bd4\u7279\u5206\u5272\u548c\u5c16\u5cf0\u4fdd\u7559\u6280\u672f\uff0c\u5b9e\u73b0\u4efb\u610f\u6bd4\u7279\u5bbd\u5ea6\u7684\u8de8GPU\u9ad8\u6548\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347\u901a\u4fe1\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u5206\u5e03\u5f0f\u8bad\u7ec3\u548c\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u901a\u4fe1\u74f6\u9888\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u901a\u4fe1\u8303\u5f0f\u6765\u89e3\u51b3\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u6bd4\u7279\u5206\u5272\u6280\u672f\u5c06\u4e0d\u89c4\u5219\u6bd4\u7279\u5bbd\u5ea6\u5206\u89e3\u4e3a\u57fa\u7840\u5355\u5143\uff0c\u5c16\u5cf0\u4fdd\u7559\u6280\u672f\u4fdd\u7559\u6570\u503c\u5f02\u5e38\u70b9\u4e3a\u6d6e\u70b9\u6570\u4ee5\u51cf\u5c11\u52a8\u6001\u8303\u56f4\uff0c\u517c\u5bb9\u786c\u4ef6\u80fd\u529b\u3002", "result": "\u5728NVLink\u548cPCIe\u67b6\u6784\u4e0b\uff0c\u5b9e\u73b0AllReduce\u901a\u4fe1\u6700\u9ad83.2\u500d\u52a0\u901f\u548cAll2All\u901a\u4fe12\u500d\u52a0\u901f\u3002", "conclusion": "FlashCommunication V2\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u901a\u4fe1\u6548\u7387\u5e76\u964d\u4f4e\u5f00\u9500\u3002"}}
{"id": "2508.04417", "pdf": "https://arxiv.org/pdf/2508.04417", "abs": "https://arxiv.org/abs/2508.04417", "authors": ["Sujay Yadalam", "Konstantinos Kanellis", "Michael Swift", "Shivaram Venkataraman"], "title": "ARMS: Adaptive and Robust Memory Tiering System", "categories": ["cs.OS"], "comment": null, "summary": "Memory tiering systems seek cost-effective memory scaling by adding multiple\ntiers of memory. For maximum performance, frequently accessed (hot) data must\nbe placed close to the host in faster tiers and infrequently accessed (cold)\ndata can be placed in farther slower memory tiers. Existing tiering solutions\nsuch as HeMem, Memtis, and TPP use rigid policies with pre-configured\nthresholds to make data placement and migration decisions. We perform a\nthorough evaluation of the threshold choices and show that there is no single\nset of thresholds that perform well for all workloads and configurations, and\nthat tuning can provide substantial speedups. Our evaluation identified three\nprimary reasons why tuning helps: better hot/cold page identification, reduced\nwasteful migrations, and more timely migrations.\n  Based on this study, we designed ARMS - Adaptive and Robust Memory tiering\nSystem - to provide high performance without tunable thresholds. We develop a\nnovel hot/cold page identification mechanism relying on short-term and\nlong-term moving averages, an adaptive migration policy based on cost/benefit\nanalysis, and a bandwidth-aware batched migration scheduler. Combined, these\napproaches provide out-of-the-box performance within 3% the best tuned\nperformance of prior systems, and between 1.26x-2.3x better than prior systems\nwithout tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u5185\u5b58\u5206\u5c42\u7cfb\u7edfARMS\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u51b7\u70ed\u6570\u636e\u548c\u4f18\u5316\u8fc1\u79fb\u7b56\u7565\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u4f18\u5373\u53ef\u63a5\u8fd1\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5185\u5b58\u5206\u5c42\u7cfb\u7edf\u4f9d\u8d56\u56fa\u5b9a\u9608\u503c\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u8c03\u4f18\u53ef\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u77ed\u671f\u548c\u957f\u671f\u79fb\u52a8\u5e73\u5747\u7684\u70ed\u51b7\u6570\u636e\u8bc6\u522b\u673a\u5236\uff0c\u6210\u672c/\u6548\u76ca\u5206\u6790\u7684\u8fc1\u79fb\u7b56\u7565\uff0c\u4ee5\u53ca\u5e26\u5bbd\u611f\u77e5\u7684\u6279\u91cf\u8fc1\u79fb\u8c03\u5ea6\u3002", "result": "ARMS\u5728\u65e0\u9700\u8c03\u4f18\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u63a5\u8fd1\u73b0\u6709\u7cfb\u7edf\u7684\u6700\u4f73\u8c03\u4f18\u7ed3\u679c\uff08\u76f8\u5dee3%\uff09\uff0c\u4e14\u4f18\u4e8e\u672a\u8c03\u4f18\u7cfb\u7edf\u76841.26-2.3\u500d\u3002", "conclusion": "ARMS\u901a\u8fc7\u52a8\u6001\u81ea\u9002\u5e94\u7b56\u7565\u89e3\u51b3\u4e86\u56fa\u5b9a\u9608\u503c\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03837", "pdf": "https://arxiv.org/pdf/2508.03837", "abs": "https://arxiv.org/abs/2508.03837", "authors": ["Davide Zoni", "Andrea Galimberti", "Adriano Guarisco"], "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent Memory Subsystems", "categories": ["cs.AR"], "comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025", "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.", "AI": {"tldr": "Rhea\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u9a8c\u8bc1RTL\u7f13\u5b58\u4e00\u81f4\u6027\u5185\u5b58\u5b50\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408Verilator\u548cgem5\u5b9e\u73b0\u9ad8\u6548\u8bbe\u8ba1\u548c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u6838\u7cfb\u7edf\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u4ee3\u591a\u6838\u7cfb\u7edf\u82af\u7247\u67b6\u6784\u4e2d\uff0c\u8bbe\u8ba1\u548c\u9a8c\u8bc1\u9ad8\u6548\u7f13\u5b58\u4e00\u81f4\u6027\u5185\u5b58\u5b50\u7cfb\u7edf\u590d\u6742\u4e14\u91cd\u8981\uff0c\u9700\u8981\u7b80\u5316\u6d41\u7a0b\u5e76\u63d0\u9ad8\u9a8c\u8bc1\u51c6\u786e\u6027\u3002", "method": "Rhea\u6846\u67b6\u751f\u6210\u53ef\u7efc\u5408\u7684RTL\uff0c\u652f\u6301\u591a\u79cd\u67b6\u6784\u53c2\u6570\uff0c\u5e76\u96c6\u6210Verilator\u548cgem5\u7684\u4eff\u771f\u5de5\u5177\uff0c\u7528\u4e8e\u7cfb\u7edf\u7ea7\u9a8c\u8bc1\u3002", "result": "\u572816\u6838\u573a\u666f\u4e0b\uff0cRhea\u7684\u6027\u80fd\u4ecb\u4e8egem5\u7684MI\u548cMOESI\u6a21\u578b\u4e4b\u95f4\uff0c\u4eff\u771f\u5f00\u9500\u4e3a1.6\u500d\uff0c\u4f46\u4eff\u771f\u4fdd\u771f\u5ea6\u66f4\u9ad8\u3002", "conclusion": "Rhea\u80fd\u591f\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u5730\u52a0\u901fRTL\u7f13\u5b58\u4e00\u81f4\u6027\u5185\u5b58\u5b50\u7cfb\u7edf\u7684\u5f00\u53d1\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u786c\u4ef6\u4eff\u771f\u51c6\u786e\u6027\u3002"}}
{"id": "2508.03947", "pdf": "https://arxiv.org/pdf/2508.03947", "abs": "https://arxiv.org/abs/2508.03947", "authors": ["Vishnu Murali", "Mohammed Adib Oumer", "Majid Zamani"], "title": "Control Closure Certificates", "categories": ["cs.LO", "cs.SY", "eess.SY"], "comment": "28 pages, 4 figures, 6 Tables. To appear in International Symposium\n  on Automated Technology for Verification and Analysis (ATVA), 2025", "summary": "This paper introduces the notion of control closure certificates to\nsynthesize controllers for discrete-time control systems against\n$\\omega$-regular specifications. Typical functional approaches to synthesize\ncontrollers against $\\omega$-regular specifications rely on combining inductive\ninvariants (for example, via barrier certificates) with proofs of\nwell-foundedness (for example, via ranking functions). Transition invariants,\nprovide an alternative where instead of standard well-foundedness arguments one\nmay instead search for disjunctive well-foundedness arguments that together\nensure a well-foundedness argument. Closure certificates, functional analogs of\ntransition invariants, provide an effective, automated approach to verify\ndiscrete-time dynamical systems against linear temporal logic and\n$\\omega$-regular specifications. We build on this notion to synthesize\ncontrollers to ensure the satisfaction of $\\omega$-regular specifications. To\ndo so, we first illustrate how one may construct control closure certificates\nto visit a region infinitely often (or only finitely often) via disjunctive\nwell-founded arguments. We then combine these arguments to provide an argument\nfor parity specifications. Thus, finding an appropriate control closure\ncertificate over the product of the system and a parity automaton specifying a\ndesired $\\omega$-regular specification ensures that there exists a controller\n$\\kappa$ to enforce the $\\omega$-regular specification. We propose a\nsum-of-squares optimization approach to synthesize such certificates and\ndemonstrate their efficacy in designing controllers over some case studies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u95ed\u5305\u8bc1\u4e66\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5408\u6210\u79bb\u6563\u65f6\u95f4\u63a7\u5236\u7cfb\u7edf\u6ee1\u8db3\u03c9-regular\u89c4\u8303\u7684\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u7ec4\u5408\u4e0d\u53d8\u91cf\u548c\u6790\u53d6\u826f\u57fa\u6027\u8bc1\u660e\uff0c\u9a8c\u8bc1\u548c\u8bbe\u8ba1\u63a7\u5236\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u63a7\u5236\u5668\u5408\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5f52\u7eb3\u4e0d\u53d8\u5f0f\u548c\u826f\u57fa\u6027\u8bc1\u660e\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u548c\u9ad8\u6548\u6027\u3002\u672c\u6587\u63d0\u51fa\u63a7\u5236\u95ed\u5305\u8bc1\u4e66\u7684\u6982\u5ff5\uff0c\u4ee5\u66f4\u6709\u6548\u7684\u65b9\u5f0f\u9a8c\u8bc1\u548c\u8bbe\u8ba1\u6ee1\u8db3\u03c9-regular\u89c4\u8303\u7684\u63a7\u5236\u5668\u3002", "method": "\u7ed3\u5408\u4e0d\u53d8\u91cf\u548c\u6790\u53d6\u826f\u57fa\u6027\u8bba\u8bc1\uff0c\u6784\u5efa\u63a7\u5236\u95ed\u5305\u8bc1\u4e66\uff0c\u5e76\u901a\u8fc7\u4e58\u79ef\u7a7a\u95f4\u4e2d\u7684\u8bc1\u4e66\u8bbe\u8ba1\u63a7\u5236\u5668\u3002\u91c7\u7528\u5e73\u65b9\u548c\u4f18\u5316\u65b9\u6cd5\u5408\u6210\u8bc1\u4e66\u3002", "result": "\u5c55\u793a\u4e86\u63a7\u5236\u95ed\u5305\u8bc1\u4e66\u5728\u9a8c\u8bc1\u548c\u63a7\u5236\u5668\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "\u63a7\u5236\u95ed\u5305\u8bc1\u4e66\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5408\u6210\u6ee1\u8db3\u03c9-regular\u89c4\u8303\u7684\u63a7\u5236\u5668\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u6280\u672f\u7684\u80fd\u529b\u3002"}}
{"id": "2508.03830", "pdf": "https://arxiv.org/pdf/2508.03830", "abs": "https://arxiv.org/abs/2508.03830", "authors": ["Hanwen Guo", "Ben Greenman"], "title": "If-T: A Benchmark for Type Narrowing", "categories": ["cs.PL"], "comment": null, "summary": "**Context:** The design of static type systems that can validate\ndynamically-typed programs (**gradually**) is an ongoing challenge. A key\ndifficulty is that dynamic code rarely follows datatype-driven design. Programs\ninstead use runtime tests to narrow down the proper usage of incoming data.\nType systems for dynamic languages thus need a **type narrowing** mechanism\nthat refines the type environment along individual control paths based on\ndominating tests, a form of flow-sensitive typing. In order to express\nrefinements, the type system must have some notion of sets and subsets. Since\nset-theoretic types are computationally and ergonomically complex, the need for\ntype narrowing raises design questions about how to balance precision and\nperformance. **Inquiry:** To date, the design of type narrowing systems has\nbeen driven by intuition, past experience, and examples from users in various\nlanguage communities. There is no standard that captures desirable and\nundesirable behaviors. Prior formalizations of narrowing are also significantly\nmore complex than a standard type system, and it is unclear how the extra\ncomplexity pays off in terms of concrete examples. This paper addresses the\nproblems through If-T, a language-agnostic **design benchmark** for type\nnarrowing that characterizes the abilities of implementations using simple\nprograms that draw attention to fundamental questions. Unlike a traditional\nperformance-focused benchmark, If-T measures a narrowing system's ability to\nvalidate correct code and reject incorrect code. Unlike a test suite, systems\nare not required to fully conform to If-T. Deviations are acceptable provided\nthey are justified by well-reasoned design considerations, such as compile-time\nperformance. **Approach:** If-T is guided by the literature on type narrowing,\nthe documentation of gradual languages such as TypeScript, and experiments with\ntypechecker implementations. We have identified a set of core technical\ndimensions for type narrowing. For each dimension, the benchmark contains a set\nof topics and (at least) two characterizing programs per topic: one that should\ntypecheck and one that should not typecheck. **Knowledge:** If-T provides a\nbaseline to measure type narrowing systems. For researchers, it provides\ncriteria to categorize future designs via its collection of positive and\nnegative examples. For language designers, the benchmark demonstrates the\npayoff of typechecker complexity in terms of concrete examples. Designers can\nuse the examples to decide whether supporting a particular example is\nworthwhile. Both the benchmark and its implementations are freely available\nonline. **Grounding:** We have implemented the benchmark for five typecheckers:\nTypeScript, Flow, Typed Racket, mypy, and Pyright. The results highlight\nimportant differences, such as the ability to track logical implications among\nprogram variables and typechecking for user-defined narrowing predicates.\n**Importance:** Type narrowing is essential for gradual type systems, but the\ntradeoffs between systems with different complexity have been unclear. If-T\nclarifies these tradeoffs by illustrating the benefits and limitations of each\nlevel of complexity. With If-T as a way to assess implementations in a fair,\ncross-language manner, future type system designs can strive for a better\nbalance among precision, annotation burden, and performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86If-T\uff0c\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u7c7b\u578b\u7ec6\u5316\u7cfb\u7edf\u7684\u8bbe\u8ba1\u57fa\u51c6\uff0c\u5e2e\u52a9\u6743\u8861\u7cbe\u5ea6\u4e0e\u6027\u80fd\u3002", "motivation": "\u52a8\u6001\u7c7b\u578b\u8bed\u8a00\u4e2d\uff0c\u7c7b\u578b\u7ec6\u5316\u662f\u9a8c\u8bc1\u4ee3\u7801\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\uff0c\u590d\u6742\u6027\u6536\u76ca\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u6587\u732e\u3001\u8bed\u8a00\u6587\u6863\u548c\u5b9e\u9a8c\uff0c\u8bbe\u8ba1If-T\u57fa\u51c6\uff0c\u5305\u542b\u6b63\u8d1f\u6d4b\u8bd5\u7528\u4f8b\uff0c\u8bc4\u4f30\u4e94\u79cd\u7c7b\u578b\u68c0\u67e5\u5668\u3002", "result": "If-T\u63ed\u793a\u4e86\u4e0d\u540c\u7cfb\u7edf\u7684\u5dee\u5f02\uff0c\u5982\u53d8\u91cf\u903b\u8f91\u5173\u7cfb\u8ddf\u8e2a\u548c\u7528\u6237\u5b9a\u4e49\u8c13\u8bcd\u652f\u6301\u3002", "conclusion": "If-T\u4e3a\u672a\u6765\u7c7b\u578b\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u6743\u8861\u5de5\u5177\uff0c\u4fc3\u8fdb\u7cbe\u5ea6\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2508.04353", "pdf": "https://arxiv.org/pdf/2508.04353", "abs": "https://arxiv.org/abs/2508.04353", "authors": ["Anderson de Lima Luiz"], "title": "LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for Learned Thematic Significance Tracking in Multimedia Content", "categories": ["cs.MM", "cs.AI", "68T07"], "comment": "5 pages and 4 figures", "summary": "This paper introduces the Learned User Significance Tracker (LUST), a\nframework designed to analyze video content and quantify the thematic relevance\nof its segments in relation to a user-provided textual description of\nsignificance. LUST leverages a multi-modal analytical pipeline, integrating\nvisual cues from video frames with textual information extracted via Automatic\nSpeech Recognition (ASR) from the audio track. The core innovation lies in a\nhierarchical, two-stage relevance scoring mechanism employing Large Language\nModels (LLMs). An initial \"direct relevance\" score, $S_{d,i}$, assesses\nindividual segments based on immediate visual and auditory content against the\ntheme. This is followed by a \"contextual relevance\" score, $S_{c,i}$, that\nrefines the assessment by incorporating the temporal progression of preceding\nthematic scores, allowing the model to understand evolving narratives. The LUST\nframework aims to provide a nuanced, temporally-aware measure of user-defined\nsignificance, outputting an annotated video with visualized relevance scores\nand comprehensive analytical logs.", "AI": {"tldr": "LUST\u6846\u67b6\u901a\u8fc7\u89c6\u9891\u5185\u5bb9\u548c\u7528\u6237\u6587\u672c\u63cf\u8ff0\u5206\u6790\u4e3b\u9898\u76f8\u5173\u6027\uff0c\u91c7\u7528\u591a\u6a21\u6001\u5206\u6790\u548cLLM\u8bc4\u5206\u673a\u5236\u3002", "motivation": "\u76ee\u7684\u662f\u91cf\u5316\u89c6\u9891\u7247\u6bb5\u4e0e\u7528\u6237\u5b9a\u4e49\u7684\u6587\u672c\u4e3b\u9898\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u63d0\u4f9b\u52a8\u6001\u7684\u53d9\u4e8b\u7406\u89e3\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5206\u6790\u7ed3\u5408\u89c6\u89c9\u548c\u542c\u89c9\u4fe1\u606f\uff0c\u91c7\u7528\u4e24\u9636\u6bb5LLM\u8bc4\u5206\u673a\u5236\uff08\u76f4\u63a5\u76f8\u5173\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff09\u3002", "result": "\u8f93\u51fa\u5e26\u6ce8\u91ca\u7684\u89c6\u9891\u548c\u76f8\u5173\u5206\u6570\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u5206\u6790\u65e5\u5fd7\u3002", "conclusion": "LUST\u80fd\u52a8\u6001\u6355\u6349\u89c6\u9891\u53d9\u4e8b\u7684\u4e3b\u9898\u6f14\u53d8\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u8be6\u7ec6\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2508.03700", "pdf": "https://arxiv.org/pdf/2508.03700", "abs": "https://arxiv.org/abs/2508.03700", "authors": ["Liujian Tang", "Shaokang Dong", "Yijia Huang", "Minqi Xiang", "Hongtao Ruan", "Bin Wang", "Shuo Li", "Zhihui Cao", "Hailiang Pang", "Heng Kong", "He Yang", "Mingxu Chai", "Zhilin Gao", "Xingyu Liu", "Yingnan Fu", "Jiaming Liu", "Tao Gui", "Xuanjing Huang", "Yu-Gang Jiang", "Qi Zhang", "Kang Wang", "Yunke Zhang", "Yuran Wang"], "title": "MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This paper presents MagicGUI, a foundational mobile GUI agent designed to\naddress critical challenges in perception, grounding, and reasoning within\nreal-world mobile GUI environments. The framework is underpinned by following\nsix key components: (1) a comprehensive and accurate dataset, constructed via\nthe scalable GUI Data Pipeline, which aggregates the largest and most diverse\nGUI-centric multimodal data to date from open-source repositories, automated\ncrawling, and targeted manual annotation; (2) enhanced perception and grounding\ncapabilities, facilitating fine-grained multimodal alignment for UI element\nreferencing, grounding, and screen comprehension; (3) a comprehensive and\nunified action space, encompassing both fundamental UI operations and complex\ninteractive intents to support human-agent interactions; (4) planning-oriented\nreasoning mechanisms that enable the model to decompose complex user\ninstructions into sequential actions with explicit intermediate meta-paln\nreasoning; (5) an iterative two-stage training procedure, combining large-scale\ncontinue pre-training on 7.8M samples with reinforcement fine-tuning utilizing\na spatially enhanced composite reward and dual filtering strategy; and (6)\ncompetitive performance on both the proprietary Magic-RICH benchmark and over a\ndozen public benchmarks, achieving superior performance across GUI perception\nand agent tasks, while demonstrating robust generalization and real-world\ndeployment potential in practical mobile GUI scenarios, as detailed in Figure\n1.", "AI": {"tldr": "MagicGUI\u662f\u4e00\u79cd\u79fb\u52a8GUI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u516d\u9879\u5173\u952e\u6280\u672f\u89e3\u51b3\u611f\u77e5\u3001\u63a5\u5730\u548c\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8GUI\u73af\u5883\u4e2d\u611f\u77e5\u3001\u63a5\u5730\u548c\u63a8\u7406\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u3001\u589e\u5f3a\u611f\u77e5\u548c\u63a5\u5730\u80fd\u529b\u3001\u7edf\u4e00\u7684\u52a8\u4f5c\u7a7a\u95f4\u3001\u89c4\u5212\u5bfc\u5411\u7684\u63a8\u7406\u673a\u5236\u3001\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002", "conclusion": "MagicGUI\u4e3a\u79fb\u52a8GUI\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u7528\u6027\u548c\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2508.03931", "pdf": "https://arxiv.org/pdf/2508.03931", "abs": "https://arxiv.org/abs/2508.03931", "authors": ["Everton Guimaraes", "Nathalia Nascimento", "Chandan Shivalingaiah", "Asish Nelapati"], "title": "Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems", "categories": ["cs.SE"], "comment": "11 pages, 13 figures, 29th International Conference on Evaluation and\n  Assessment in Software Engineering (EASE)", "summary": "Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are\ntransforming software engineering by automating key tasks, including code\ngeneration, testing, and debugging. As these models become integral to\ndevelopment workflows, a systematic comparison of their performance is\nessential for optimizing their use in real world applications. This study\nbenchmarks these four prominent LLMs on one hundred and fifty LeetCode problems\nacross easy, medium, and hard difficulties, generating solutions in Java and\nPython. We evaluate each model based on execution time, memory usage, and\nalgorithmic complexity, revealing significant performance differences. ChatGPT\ndemonstrates consistent efficiency in execution time and memory usage, while\nCopilot and DeepSeek show variability as task complexity increases. Gemini,\nalthough effective on simpler tasks, requires more attempts as problem\ndifficulty rises. Our findings provide actionable insights into each model's\nstrengths and limitations, offering guidance for developers selecting LLMs for\nspecific coding tasks and providing insights on the performance and complexity\nof GPT-like generated solutions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u56db\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08ChatGPT\u3001Copilot\u3001Gemini\u548cDeepSeek\uff09\u5728150\u9053LeetCode\u9898\u76ee\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u7cfb\u7edf\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5176\u6267\u884c\u65f6\u95f4\u3001\u5185\u5b58\u4f7f\u7528\u548c\u7b97\u6cd5\u590d\u6742\u5ea6\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u5bf9\u5176\u6027\u80fd\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\uff0c\u4ee5\u4f18\u5316\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u7814\u7a76\u5bf9\u56db\u79cd\u6a21\u578b\u5728Java\u548cPython\u4e2d\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u6613\u3001\u4e2d\u3001\u96be\u4e09\u79cd\u96be\u5ea6\u7684\u9898\u76ee\u3002", "result": "ChatGPT\u5728\u6267\u884c\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u4e0a\u8868\u73b0\u4e00\u81f4\u9ad8\u6548\uff0cCopilot\u548cDeepSeek\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0cGemini\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u6709\u6548\u4f46\u96be\u5ea6\u589e\u52a0\u65f6\u9700\u8981\u66f4\u591a\u5c1d\u8bd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u6bcf\u79cd\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u5f00\u53d1\u8005\u9009\u62e9\u9002\u5408\u7279\u5b9a\u7f16\u7a0b\u4efb\u52a1\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.04326", "pdf": "https://arxiv.org/pdf/2508.04326", "abs": "https://arxiv.org/abs/2508.04326", "authors": ["Ke Li", "Mana Masuda", "Susanne Schmidt", "Shohei Mori"], "title": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research", "categories": ["cs.GR"], "comment": "This work has been submitted to the IEEE TVCG journal for possible\n  publication", "summary": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)\nand Neural Radiance Fields (NeRF), has revolutionized interactive\nphotorealistic view synthesis and presents enormous opportunities for XR\nresearch and applications. However, despite the exponential growth of RF\nresearch, RF-related contributions to the XR community remain sparse. To better\nunderstand this research gap, we performed a systematic survey of current RF\nliterature to analyze (i) how RF is envisioned for XR applications, (ii) how\nthey have already been implemented, and (iii) the remaining research gaps. We\ncollected 365 RF contributions related to XR from computer vision, computer\ngraphics, robotics, multimedia, human-computer interaction, and XR communities,\nseeking to answer the above research questions. Among the 365 papers, we\nperformed an analysis of 66 papers that already addressed a detailed aspect of\nRF research for XR. With this survey, we extended and positioned XR-specific RF\nresearch topics in the broader RF research field and provide a helpful resource\nfor the XR community to navigate within the rapid development of RF research.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u8f90\u5c04\u573a\uff08RF\uff09\u5728XR\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5305\u62ec\u613f\u666f\u3001\u5b9e\u9645\u5e94\u7528\u4e0e\u5269\u4f59\u7684\u7814\u7a76\u7f3a\u53e3\uff0c\u5e76\u901a\u8fc7\u5bf9365\u7bc7\u76f8\u5173\u6587\u732e\u7684\u7cfb\u7edf\u8c03\u67e5\uff0c\u4e3aXR\u793e\u533a\u63d0\u4f9b\u4e86\u5bfc\u822a\u8d44\u6e90\u3002", "motivation": "\u5c3d\u7ba1RF\u6280\u672f\uff08\u59823DGS\u548cNeRF\uff09\u5728\u4ea4\u4e92\u5f0f\u903c\u771f\u89c6\u56fe\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u7a81\u7834\uff0c\u4f46RF\u5728XR\u793e\u533a\u4e2d\u7684\u8d21\u732e\u4ecd\u7136\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76RF\u5728XR\u4e2d\u7684\u5e94\u7528\u524d\u666f\u4e0e\u5b9e\u9645\u8fdb\u5c55\u3002", "method": "\u7cfb\u7edf\u8c03\u67e5\u4e86365\u7bc7\u4e0eRF\u76f8\u5173\u7684XR\u6587\u732e\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u4e86\u5176\u4e2d66\u7bc7\uff0c\u4ee5\u56de\u7b54RF\u5728XR\u4e2d\u7684\u613f\u666f\u3001\u5b9e\u73b0\u65b9\u5f0f\u53ca\u7814\u7a76\u7f3a\u53e3\u3002", "result": "\u7814\u7a76\u6269\u5c55\u5e76\u5b9a\u4f4d\u4e86RF\u5728XR\u9886\u57df\u7684\u7814\u7a76\u4e3b\u9898\uff0c\u4e3aXR\u793e\u533a\u63d0\u4f9b\u4e86\u5feb\u901f\u53d1\u5c55\u7684RF\u7814\u7a76\u5bfc\u822a\u8d44\u6e90\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86RF\u5728XR\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u4e0e\u8d44\u6e90\u3002"}}
{"id": "2508.03891", "pdf": "https://arxiv.org/pdf/2508.03891", "abs": "https://arxiv.org/abs/2508.03891", "authors": ["Eun Hun Choi", "Jasleen Kaur", "Vladas Pipiras", "Nelson Gomes Rodrigues Antunes", "Brendan Massey"], "title": "Confidence Driven Classification of Application Types in the Presence of Background Network", "categories": ["cs.NI"], "comment": "10 pages", "summary": "Accurately classifying the application types of network traffic using deep\nlearning models has recently gained popularity. However, we find that these\nclassifiers do not perform well on real-world traffic data due to the presence\nof non-application-specific generic background traffic originating from\nadvertisements, analytics, shared APIs, and trackers. Unfortunately,\nstate-of-the-art application classifiers overlook such traffic in curated\ndatasets and only classify relevant application traffic. To address this issue,\nwhen we label and train using an additional class for background traffic, it\nleads to additional confusion between application and background traffic, as\nthe latter is heterogeneous and encompasses all traffic that is not relevant to\nthe application sessions. To avoid falsely classifying background traffic as\none of the relevant application types, a reliable confidence measure is\nwarranted, such that we can refrain from classifying uncertain samples.\nTherefore, we design a Gaussian Mixture Model-based classification framework\nthat improves the indication of the deep learning classifier's confidence to\nallow more reliable classification.", "AI": {"tldr": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u5206\u7c7b\u7f51\u7edc\u6d41\u91cf\u7c7b\u578b\u65f6\uff0c\u56e0\u80cc\u666f\u6d41\u91cf\u5e72\u6270\uff0c\u4f20\u7edf\u5206\u7c7b\u5668\u6548\u679c\u4e0d\u4f73\u3002\u901a\u8fc7\u5f15\u5165\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u7f6e\u4fe1\u5ea6\uff0c\u907f\u514d\u8bef\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u5668\u5728\u73b0\u5b9e\u6570\u636e\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5ffd\u7565\u4e86\u5e7f\u544a\u3001\u5206\u6790\u7b49\u975e\u5e94\u7528\u7279\u5b9a\u7684\u80cc\u666f\u6d41\u91cf\uff0c\u5bfc\u81f4\u5206\u7c7b\u51c6\u786e\u7387\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u53ef\u9760\u5730\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u7f6e\u4fe1\u5ea6\uff0c\u907f\u514d\u5bf9\u4e0d\u786e\u5b9a\u6837\u672c\u7684\u9519\u8bef\u5206\u7c7b\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u5668\u7684\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u4e86\u5bf9\u80cc\u666f\u6d41\u91cf\u7684\u8bef\u5206\u7c7b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u80cc\u666f\u6d41\u91cf\u7c7b\u522b\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u6709\u6548\u6539\u5584\u4e86\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.03858", "pdf": "https://arxiv.org/pdf/2508.03858", "abs": "https://arxiv.org/abs/2508.03858", "authors": ["Charles L. Wang", "Trisha Singhal", "Ameya Kelkar", "Jason Tuo"], "title": "MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems", "categories": ["cs.AI", "cs.ET", "cs.MA"], "comment": null, "summary": "Agentic AI systems capable of reasoning, planning, and executing actions\npresent fundamentally distinct governance challenges compared to traditional AI\nmodels. Unlike conventional AI, these systems exhibit emergent and unexpected\nbehaviors during runtime, introducing novel agent-related risks that cannot be\nfully anticipated through pre-deployment governance alone. To address this\ncritical gap, we introduce MI9, the first fully integrated runtime governance\nframework designed specifically for safety and alignment of agentic AI systems.\nMI9 introduces real-time controls through six integrated components:\nagency-risk index, agent-semantic telemetry capture, continuous authorization\nmonitoring, Finite-State-Machine (FSM)-based conformance engines,\ngoal-conditioned drift detection, and graduated containment strategies.\nOperating transparently across heterogeneous agent architectures, MI9 enables\nthe systematic, safe, and responsible deployment of agentic systems in\nproduction environments where conventional governance approaches fall short,\nproviding the foundational infrastructure for safe agentic AI deployment at\nscale. Detailed analysis through a diverse set of scenarios demonstrates MI9's\nsystematic coverage of governance challenges that existing approaches fail to\naddress, establishing the technical foundation for comprehensive agentic AI\noversight.", "AI": {"tldr": "MI9\u662f\u4e00\u4e2a\u4e13\u4e3a\u4ee3\u7406\u578bAI\u7cfb\u7edf\u8bbe\u8ba1\u7684\u8fd0\u884c\u65f6\u6cbb\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u516d\u4e2a\u7ec4\u4ef6\u89e3\u51b3\u4f20\u7edf\u6cbb\u7406\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u8fd0\u884c\u65f6\u98ce\u9669\u548c\u610f\u5916\u884c\u4e3a\u3002", "motivation": "\u4ee3\u7406\u578bAI\u7cfb\u7edf\u7684\u8fd0\u884c\u65f6\u884c\u4e3a\u548c\u98ce\u9669\u65e0\u6cd5\u5b8c\u5168\u901a\u8fc7\u90e8\u7f72\u524d\u6cbb\u7406\u9884\u6d4b\uff0c\u9700\u8981\u65b0\u7684\u8fd0\u884c\u65f6\u6cbb\u7406\u6846\u67b6\u3002", "method": "MI9\u901a\u8fc7\u673a\u6784\u98ce\u9669\u6307\u6570\u3001\u4ee3\u7406\u8bed\u4e49\u9065\u6d4b\u6355\u83b7\u3001\u8fde\u7eed\u6388\u6743\u76d1\u63a7\u7b49\u516d\u4e2a\u7ec4\u4ef6\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\u3002", "result": "MI9\u80fd\u591f\u7cfb\u7edf\u5730\u8986\u76d6\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u7684\u6cbb\u7406\u6311\u6218\uff0c\u4e3a\u4ee3\u7406\u578bAI\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u6280\u672f\u57fa\u7840\u3002", "conclusion": "MI9\u586b\u8865\u4e86\u4ee3\u7406\u578bAI\u8fd0\u884c\u65f6\u6cbb\u7406\u7684\u5173\u952e\u7a7a\u767d\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2508.03978", "pdf": "https://arxiv.org/pdf/2508.03978", "abs": "https://arxiv.org/abs/2508.03978", "authors": ["Amir Shaikhha", "Youning Xia", "Meisam Tarabkhah", "Jazal Saleem", "Anna Herlihy"], "title": "Raqlet: Cross-Paradigm Compilation for Recursive Queries", "categories": ["cs.DB", "cs.PL"], "comment": null, "summary": "We introduce Raqlet, a source-to-source compilation framework that addresses\nthe fragmentation of recursive querying engines spanning relational (recursive\nSQL), graph (Cypher, GQL), and deductive (Datalog) systems. Recent standards\nsuch as SQL:2023's SQL/PGQ and the GQL standard provide a common foundation for\nquerying graph data within relational and graph databases; however, real-world\nsupport remains inconsistent across systems. Raqlet bridges this gap by\ntranslating recursive queries across paradigms through leveraging intermediate\nrepresentations (IRs) grounded in well-defined semantics; it translates Cypher\nor SQL/PGQ to PGIR (inspired by Cypher), then into DLIR (inspired by Datalog),\nand finally to SQIR (inspired by recursive SQL). Raqlet provides a shared\nsemantic basis that can serve as a golden reference implementation for language\nstandards, while supporting static analysis and transformations (e.g.,\nmagic-set transformation) for performance tuning. Our vision is to make Raqlet\na robust platform that enables rapid cross-paradigm prototyping, portable\nrecursive queries, and formal reasoning about recursion even when targeting\ndiverse query execution engines.", "AI": {"tldr": "Raqlet\u662f\u4e00\u4e2a\u6e90\u4ee3\u7801\u5230\u6e90\u4ee3\u7801\u7684\u7f16\u8bd1\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u9012\u5f52\u67e5\u8be2\u5f15\u64ce\u5728\u5173\u7cfb\u578b\u3001\u56fe\u578b\u548c\u6f14\u7ece\u578b\u7cfb\u7edf\u4e2d\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u7531\u4e8eSQL:2023\u7684SQL/PGQ\u548cGQL\u6807\u51c6\u5728\u4e0d\u540c\u7cfb\u7edf\u4e2d\u7684\u5b9e\u73b0\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7edf\u4e00\u548c\u8de8\u8303\u5f0f\u7ffb\u8bd1\u9012\u5f52\u67e5\u8be2\u3002", "method": "Raqlet\u901a\u8fc7\u4e2d\u95f4\u8868\u793a\uff08IRs\uff09\u5c06\u9012\u5f52\u67e5\u8be2\u4eceCypher\u6216SQL/PGQ\u8f6c\u6362\u4e3aPGIR\u3001DLIR\uff0c\u518d\u5230SQIR\uff0c\u63d0\u4f9b\u8bed\u4e49\u57fa\u7840\u548c\u9759\u6001\u5206\u6790\u652f\u6301\u3002", "result": "Raqlet\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5171\u4eab\u8bed\u4e49\u57fa\u7840\uff0c\u53ef\u4f5c\u4e3a\u8bed\u8a00\u6807\u51c6\u7684\u53c2\u8003\u5b9e\u73b0\uff0c\u5e76\u652f\u6301\u6027\u80fd\u4f18\u5316\u3002", "conclusion": "\u76ee\u6807\u662f\u4f7fRaqlet\u6210\u4e3a\u652f\u6301\u8de8\u8303\u5f0f\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3001\u4fbf\u643a\u5f0f\u9012\u5f52\u67e5\u8be2\u548c\u5f62\u5f0f\u5316\u63a8\u7406\u7684\u5f3a\u5927\u5e73\u53f0\u3002"}}
{"id": "2508.03854", "pdf": "https://arxiv.org/pdf/2508.03854", "abs": "https://arxiv.org/abs/2508.03854", "authors": ["Xin Zhang", "Quanyu Zhu", "Liangbei Xu", "Zain Huda", "Wang Zhou", "Jin Fang", "Dennis van der Staay", "Yuxi Hu", "Jade Nie", "Jiyan Yang", "Chunzhi Yang"], "title": "Two-dimensional Sparse Parallelism for Large Scale Deep Learning Recommendation Model Training", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "The increasing complexity of deep learning recommendation models (DLRM) has\nled to a growing need for large-scale distributed systems that can efficiently\ntrain vast amounts of data. In DLRM, the sparse embedding table is a crucial\ncomponent for managing sparse categorical features. Typically, these tables in\nindustrial DLRMs contain trillions of parameters, necessitating model\nparallelism strategies to address memory constraints. However, as training\nsystems expand with massive GPUs, the traditional fully parallelism strategies\nfor embedding table post significant scalability challenges, including\nimbalance and straggler issues, intensive lookup communication, and heavy\nembedding activation memory. To overcome these limitations, we propose a novel\ntwo-dimensional sparse parallelism approach. Rather than fully sharding tables\nacross all GPUs, our solution introduces data parallelism on top of model\nparallelism. This enables efficient all-to-all communication and reduces peak\nmemory consumption. Additionally, we have developed the momentum-scaled\nrow-wise AdaGrad algorithm to mitigate performance losses associated with the\nshift in training paradigms. Our extensive experiments demonstrate that the\nproposed approach significantly enhances training efficiency while maintaining\nmodel performance parity. It achieves nearly linear training speed scaling up\nto 4K GPUs, setting a new state-of-the-art benchmark for recommendation model\ntraining.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e8c\u7ef4\u7a00\u758f\u5e76\u884c\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u5e76\u884c\u548c\u6a21\u578b\u5e76\u884c\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u63a8\u8350\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\u4e2d\u7684\u7a00\u758f\u5d4c\u5165\u8868\u5b58\u5728\u5185\u5b58\u9650\u5236\u548c\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u5e76\u884c\u7b56\u7565\u9762\u4e34\u4e0d\u5e73\u8861\u3001\u901a\u4fe1\u8d1f\u8f7d\u9ad8\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e8c\u7ef4\u7a00\u758f\u5e76\u884c\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u636e\u5e76\u884c\u548c\u6a21\u578b\u5e76\u884c\uff0c\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u5f00\u53d1\u4e86\u52a8\u91cf\u7f29\u653e\u7684\u884c\u7ea7AdaGrad\u7b97\u6cd5\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57284K GPU\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7ebf\u6027\u7684\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6a21\u578b\u8bad\u7ec3\u7684\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u8bbe\u5b9a\u4e86\u65b0\u7684\u6027\u80fd\u6807\u6746\u3002"}}
{"id": "2508.03866", "pdf": "https://arxiv.org/pdf/2508.03866", "abs": "https://arxiv.org/abs/2508.03866", "authors": ["Seock-Hwan Noh", "Hoyeon Lee", "Junkyum Kim", "Junsu Im", "Jay H. Park", "Sungjin Lee", "Sam H. Noh", "Yeseong Kim", "Jaeha Kung"], "title": "FlashVault: Versatile In-NAND Self-Encryption with Zero Area Overhead", "categories": ["cs.AR"], "comment": "15 pages, 14 figures, Under submission", "summary": "We present FlashVault, an in-NAND self-encryption architecture that embeds a\nreconfigurable cryptographic engine into the unused silicon area of a\nstate-of-the-art 4D V-NAND structure. FlashVault supports not only block\nciphers for data encryption but also public-key and post-quantum algorithms for\ndigital signatures, all within the NAND flash chip. This design enables each\nNAND chip to operate as a self-contained enclave without incurring area\noverhead, while eliminating the need for off-chip encryption. We implement\nFlashVault at the register-transfer level (RTL) and perform place-and-route\n(P&R) for accurate power/area evaluation. Our analysis shows that the power\nbudget determines the number of cryptographic engines per NAND chip. We\nintegrate this architectural choice into a full-system simulation and evaluate\nits performance on a wide range of cryptographic algorithms. Our results show\nthat FlashVault consistently outperforms both CPU-based encryption (1.46~3.45x)\nand near-core processing architecture (1.02~2.01x), demonstrating its\neffectiveness as a secure SSD architecture that meets diverse cryptographic\nrequirements imposed by regulatory standards and enterprise policies.", "AI": {"tldr": "FlashVault\u662f\u4e00\u79cd\u57284D V-NAND\u7ed3\u6784\u4e2d\u5d4c\u5165\u53ef\u91cd\u6784\u52a0\u5bc6\u5f15\u64ce\u7684\u81ea\u52a0\u5bc6\u67b6\u6784\uff0c\u652f\u6301\u591a\u79cd\u52a0\u5bc6\u7b97\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u52a0\u5bc6\u82af\u7247\uff0c\u6027\u80fd\u548c\u6548\u7387\u4f18\u4e8eCPU\u548c\u8fd1\u6838\u5fc3\u5904\u7406\u67b6\u6784\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edfNAND\u95ea\u5b58\u5728\u6570\u636e\u52a0\u5bc6\u4e0a\u4f9d\u8d56\u5916\u90e8\u82af\u7247\u7684\u95ee\u9898\uff0c\u5e76\u6ee1\u8db3\u591a\u6837\u5316\u7684\u52a0\u5bc6\u9700\u6c42\u3002", "method": "\u5728NAND\u82af\u7247\u7684\u672a\u4f7f\u7528\u7845\u9762\u79ef\u4e2d\u5d4c\u5165\u52a0\u5bc6\u5f15\u64ce\uff0c\u652f\u6301\u5757\u5bc6\u7801\u3001\u516c\u94a5\u548c\u540e\u91cf\u5b50\u7b97\u6cd5\uff0c\u5b9e\u73b0\u7247\u4e0a\u81ea\u52a0\u5bc6\u3002", "result": "\u6027\u80fd\u548c\u529f\u8017\u4f18\u4e8eCPU\u548c\u8fd1\u6838\u5fc3\u67b6\u6784\uff0c\u6ee1\u8db3\u4f01\u4e1a\u6807\u51c6\u548c\u76d1\u7ba1\u8981\u6c42\u3002", "conclusion": "FlashVault\u662f\u4e00\u79cd\u9ad8\u6548\u7684SSD\u5b89\u5168\u67b6\u6784\uff0c\u80fd\u591f\u6ee1\u8db3\u591a\u79cd\u52a0\u5bc6\u9700\u6c42\u4e14\u65e0\u9700\u989d\u5916\u82af\u7247\u3002"}}
{"id": "2508.04438", "pdf": "https://arxiv.org/pdf/2508.04438", "abs": "https://arxiv.org/abs/2508.04438", "authors": ["Mark Chevallier", "Filip Smola", "Richard Schmoetten", "Jacques D. Fleuriot"], "title": "GradSTL: Comprehensive Signal Temporal Logic for Neurosymbolic Reasoning and Learning", "categories": ["cs.LO"], "comment": "Accepted for presentation at TIME 2025", "summary": "We present GradSTL, the first fully comprehensive implementation of signal\ntemporal logic (STL) suitable for integration with neurosymbolic learning. In\nparticular, GradSTL can successfully evaluate any STL constraint over any\nsignal, regardless of how it is sampled. Our formally verified approach\nspecifies smooth STL semantics over tensors, with formal proofs of soundness\nand of correctness of its derivative function. Our implementation is generated\nautomatically from this formalisation, without manual coding, guaranteeing\ncorrectness by construction. We show via a case study that using our\nimplementation, a neurosymbolic process learns to satisfy a pre-specified STL\nconstraint. Our approach offers a highly rigorous foundation for integrating\nsignal temporal logic and learning by gradient descent.", "AI": {"tldr": "GradSTL\u662f\u9996\u4e2a\u5b8c\u5168\u652f\u6301\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u7684\u5f00\u6e90\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\uff0c\u80fd\u8bc4\u4f30\u4efb\u4f55\u4fe1\u53f7\u4e0a\u7684\u4efb\u4f55STL\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7684\u6b63\u5f0f\u9a8c\u8bc1\u4fdd\u8bc1\u6b63\u786e\u6027\u3002", "motivation": "\u4e3a\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u63d0\u4f9b\u4e00\u79cd\u53ef\u65e0\u7f1d\u96c6\u6210\u4e14\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u5b9e\u73b0\uff0c\u4ee5\u652f\u6301\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u5b66\u4e60\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u89c4\u8303\u5b9a\u4e49\u5e73\u6ed1STL\u8bed\u4e49\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u5b9e\u73b0\u4ee3\u7801\uff0c\u786e\u4fdd\u6784\u9020\u5373\u6b63\u786e\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u795e\u7ecf\u7b26\u53f7\u8fc7\u7a0b\u80fd\u591f\u901a\u8fc7\u5b66\u4e60\u6ee1\u8db3\u9884\u8bbe\u7684STL\u7ea6\u675f\u3002", "conclusion": "GradSTL\u4e3a\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u4e0e\u68af\u5ea6\u4e0b\u964d\u5b66\u4e60\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u57fa\u7840\u3002"}}
{"id": "2508.03831", "pdf": "https://arxiv.org/pdf/2508.03831", "abs": "https://arxiv.org/abs/2508.03831", "authors": ["Chinmayi Prabhu Baramashetru", "Paola Giannini", "Silvia Lizeth Tapia Tarifa", "Olaf Owe"], "title": "A Type System for Data Privacy Compliance in Active Object Languages", "categories": ["cs.PL"], "comment": null, "summary": "Data protection laws such as GDPR aim to give users unprecedented control\nover their personal data. Compliance with these regulations requires\nsystematically considering information flow and interactions among entities\nhandling sensitive data. Privacy-by-design principles advocate embedding data\nprotection into system architectures as a default. However, translating these\nabstract principles into concrete, explicit methods remains a significant\nchallenge. This paper addresses this gap by proposing a language-based approach\nto privacy integration, combining static and runtime techniques. By employing\ntype checking and type inference in an active object language, the framework\nenables the tracking of authorised data flows and the automatic generation of\nconstraints checked at runtime based on user consent. This ensures that\npersonal data is processed in compliance with GDPR constraints. The key\ncontribution of this work is a type system that gather the compliance checks\nand the changes to users consent and integrates data privacy compliance\nverification into system execution. The paper demonstrates the feasibility of\nthis approach through a soundness proof and several examples, illustrating how\nthe proposed language addresses common GDPR requirements, such as user consent,\npurpose limitation, and data subject rights. This work advances the state of\nthe art in privacy-aware system design by offering a systematic and automated\nmethod for integrating GDPR compliance into programming languages. This\ncapability has implications for building trustworthy systems in domains such as\nhealthcare or finance, where data privacy is crucial.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u7684\u9690\u79c1\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u9759\u6001\u548c\u8fd0\u884c\u65f6\u6280\u672f\u7ed3\u5408\uff0c\u786e\u4fdd\u6570\u636e\u6d41\u7b26\u5408GDPR\u8981\u6c42\u3002", "motivation": "\u6570\u636e\u4fdd\u62a4\u6cd5\u5f8b\uff08\u5982GDPR\uff09\u8981\u6c42\u7cfb\u7edf\u5728\u8bbe\u8ba1\u548c\u6267\u884c\u4e2d\u4fdd\u62a4\u4e2a\u4eba\u6570\u636e\uff0c\u4f46\u62bd\u8c61\u7684\u9690\u79c1\u8bbe\u8ba1\u539f\u5219\u96be\u4ee5\u8f6c\u5316\u4e3a\u5177\u4f53\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7c7b\u578b\u68c0\u67e5\u548c\u7c7b\u578b\u63a8\u65ad\u6280\u672f\uff0c\u5728\u6d3b\u52a8\u5bf9\u8c61\u8bed\u8a00\u4e2d\u8ddf\u8e2a\u6388\u6743\u6570\u636e\u6d41\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u57fa\u4e8e\u7528\u6237\u540c\u610f\u7684\u8fd0\u884c\u65f6\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u7c7b\u578b\u7cfb\u7edf\u96c6\u6210\u5408\u89c4\u68c0\u67e5\u548c\u7528\u6237\u540c\u610f\u53d8\u66f4\uff0c\u5728\u7cfb\u7edf\u6267\u884c\u4e2d\u9a8c\u8bc1GDPR\u5408\u89c4\u6027\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u548c\u8bc1\u660e\u5c55\u793a\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684GDPR\u5408\u89c4\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u533b\u7597\u3001\u91d1\u878d\u7b49\u6570\u636e\u9690\u79c1\u5173\u952e\u9886\u57df\u3002"}}
{"id": "2508.03699", "pdf": "https://arxiv.org/pdf/2508.03699", "abs": "https://arxiv.org/abs/2508.03699", "authors": ["Subin Raj Peter"], "title": "Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task", "categories": ["cs.CV", "cs.HC", "cs.MM"], "comment": "7 pages, 7 figures, conference", "summary": "Virtual Reality (VR) has emerged as a powerful tool for workforce training,\noffering immersive, interactive, and risk-free environments that enhance skill\nacquisition, decision-making, and confidence. Despite its advantages,\ndeveloping VR applications for training remains a significant challenge due to\nthe time, expertise, and resources required to create accurate and engaging\ninstructional content. To address these limitations, this paper proposes a\nnovel approach that leverages Large Language Models (LLMs) to automate the\ngeneration of virtual instructions from textual input. The system comprises two\ncore components: an LLM module that extracts task-relevant information from the\ntext, and an intelligent module that transforms this information into animated\ndemonstrations and visual cues within a VR environment. The intelligent module\nreceives input from the LLM module and interprets the extracted information.\nBased on this, an instruction generator creates training content using relevant\ndata from a database. The instruction generator generates the instruction by\nchanging the color of virtual objects and creating animations to illustrate\ntasks. This approach enhances training effectiveness and reduces development\noverhead, making VR-based training more scalable and adaptable to evolving\nindustrial needs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u4ece\u6587\u672c\u751f\u6210\u865a\u62df\u6307\u4ee4\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u7b80\u5316VR\u57f9\u8bad\u5e94\u7528\u7684\u5f00\u53d1\u3002", "motivation": "VR\u57f9\u8bad\u5e94\u7528\u5f00\u53d1\u590d\u6742\u4e14\u8017\u65f6\uff0c\u9700\u8981\u89e3\u51b3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408LLM\u6a21\u5757\u63d0\u53d6\u4efb\u52a1\u4fe1\u606f\uff0c\u667a\u80fd\u6a21\u5757\u5c06\u5176\u8f6c\u5316\u4e3aVR\u73af\u5883\u4e2d\u7684\u52a8\u6001\u6f14\u793a\u548c\u89c6\u89c9\u63d0\u793a\u3002", "result": "\u63d0\u5347\u4e86\u57f9\u8bad\u6548\u679c\uff0c\u964d\u4f4e\u4e86\u5f00\u53d1\u6210\u672c\uff0c\u4f7fVR\u57f9\u8bad\u66f4\u6613\u6269\u5c55\u548c\u9002\u5e94\u5de5\u4e1a\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aVR\u57f9\u8bad\u5e94\u7528\u7684\u81ea\u52a8\u5316\u548c\u9ad8\u6548\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.03705", "pdf": "https://arxiv.org/pdf/2508.03705", "abs": "https://arxiv.org/abs/2508.03705", "authors": ["Kanan Eldarov"], "title": "Screen Matters: Cognitive and Behavioral Divergence Between Smartphone-Native and Computer-Native Youth", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "This study explores how different modes of digital interaction -- namely,\ncomputers versus smartphones -- affect attention, frustration, and creative\nperformance in adolescents. Using a combination of digital task logs,\nwebcam-based gaze estimation, and expert evaluation of task outcomes, we\nanalyzed data from a diverse sample of 824 students aged 11-17. Participants\nwere assigned to device groups in a randomized and stratified design to control\nfor age, gender, and prior experience. Results suggest moderate but\nstatistically significant differences in sustained attention, perceived\nfrustration, and creative output. These findings indicate that the nature of\ndigital interaction -- beyond mere screen time -- may influence cognitive and\nbehavioral outcomes relevant to educational design. Practical implications for\nuser interface development and learning environments are discussed.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u7535\u8111\u4e0e\u667a\u80fd\u624b\u673a\u5bf9\u9752\u5c11\u5e74\u6ce8\u610f\u529b\u3001\u632b\u6298\u611f\u548c\u521b\u9020\u529b\u7684\u5f71\u54cd\uff0c\u663e\u793a\u4e0d\u540c\u8bbe\u5907\u4ea4\u4e92\u65b9\u5f0f\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22\u6570\u5b57\u4ea4\u4e92\u65b9\u5f0f\uff08\u7535\u8111vs\u667a\u80fd\u624b\u673a\uff09\u5982\u4f55\u5f71\u54cd\u9752\u5c11\u5e74\u7684\u8ba4\u77e5\u548c\u884c\u4e3a\u8868\u73b0\u3002", "method": "\u7ed3\u5408\u4efb\u52a1\u65e5\u5fd7\u3001\u89c6\u7ebf\u8ffd\u8e2a\u53ca\u4e13\u5bb6\u8bc4\u4f30\uff0c\u5bf9824\u540d11-17\u5c81\u5b66\u751f\u8fdb\u884c\u968f\u673a\u5206\u5c42\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u8bbe\u5907\u5bf9\u6301\u7eed\u6027\u6ce8\u610f\u529b\u3001\u632b\u6298\u611f\u548c\u521b\u9020\u6027\u4ea7\u51fa\u6709\u663e\u8457\u4f46\u4e2d\u7b49\u7a0b\u5ea6\u5f71\u54cd\u3002", "conclusion": "\u6570\u5b57\u4ea4\u4e92\u65b9\u5f0f\u4e0d\u4ec5\u5f71\u54cd\u5c4f\u5e55\u65f6\u95f4\uff0c\u8fd8\u53ef\u80fd\u5bf9\u6559\u80b2\u8bbe\u8ba1\u548c\u5b66\u4e60\u73af\u5883\u4ea7\u751f\u5b9e\u9645\u5f71\u54cd\u3002"}}
{"id": "2508.03949", "pdf": "https://arxiv.org/pdf/2508.03949", "abs": "https://arxiv.org/abs/2508.03949", "authors": ["Md. Abdul Awal", "Mrigank Rochan", "Chanchal K. Roy"], "title": "Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code", "categories": ["cs.SE"], "comment": null, "summary": "Transformer-based language models for code have shown remarkable performance\nin various software analytics tasks, but their adoption is hindered by high\ncomputational costs, slow inference speeds, and substantial environmental\nimpact. Model compression techniques such as pruning, quantization, and\nknowledge distillation have gained traction in addressing these challenges.\nHowever, the impact of these strategies on the robustness of compressed\nlanguage models for code in adversarial scenarios remains poorly understood.\nUnderstanding how these compressed models behave under adversarial attacks is\nessential for their safe and effective deployment in real-world applications.\nTo bridge this knowledge gap, we conduct a comprehensive evaluation of how\ncommon compression strategies affect the adversarial robustness of compressed\nmodels. We assess the robustness of compressed versions of three widely used\nlanguage models for code across three software analytics tasks, using six\nevaluation metrics and four commonly used classical adversarial attacks. Our\nfindings indicate that compressed models generally maintain comparable\nperformance to their uncompressed counterparts. However, when subjected to\nadversarial attacks, compressed models exhibit significantly reduced\nrobustness. These results reveal a trade-off between model size reduction and\nadversarial robustness, underscoring the need for careful consideration when\ndeploying compressed models in security-critical software applications. Our\nstudy highlights the need for further research into compression strategies that\nstrike a balance between computational efficiency and adversarial robustness,\nwhich is essential for deploying reliable language models for code in\nreal-world software applications.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u6280\u672f\uff08\u5982\u526a\u679d\u3001\u91cf\u5316\u548c\u77e5\u8bc6\u84b8\u998f\uff09\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u538b\u7f29\u6a21\u578b\u6027\u80fd\u7c7b\u4f3c\u672a\u538b\u7f29\u6a21\u578b\uff0c\u4f46\u5bf9\u6297\u653b\u51fb\u4e0b\u9c81\u68d2\u6027\u663e\u8457\u964d\u4f4e\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u6280\u672f\u5728\u5bf9\u6297\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u4fdd\u969c\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002", "method": "\u7814\u7a76\u5bf9\u4e09\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u79cd\u8f6f\u4ef6\u5206\u6790\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e86\u516d\u79cd\u8bc4\u4f30\u6307\u6807\u548c\u56db\u79cd\u7ecf\u5178\u5bf9\u6297\u653b\u51fb\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u538b\u7f29\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4e0e\u672a\u538b\u7f29\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u9c81\u68d2\u6027\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u538b\u7f29\u4e0e\u5bf9\u6297\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u6307\u51fa\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u9700\u8981\u8c28\u614e\u8003\u8651\u538b\u7f29\u6a21\u578b\u7684\u4f7f\u7528\u3002"}}
{"id": "2508.04508", "pdf": "https://arxiv.org/pdf/2508.04508", "abs": "https://arxiv.org/abs/2508.04508", "authors": ["Haodong Zhu", "Changbai Li", "Yangyang Ren", "Zichao Feng", "Xuhui Liu", "Hanlin Chen", "Xiantong Zhen", "Baochang Zhang"], "title": "Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Current multi-view 3D reconstruction methods rely on accurate camera\ncalibration and pose estimation, requiring complex and time-intensive\npre-processing that hinders their practical deployment. To address this\nchallenge, we introduce Surf3R, an end-to-end feedforward approach that\nreconstructs 3D surfaces from sparse views without estimating camera poses and\ncompletes an entire scene in under 10 seconds. Our method employs a\nmulti-branch and multi-view decoding architecture in which multiple reference\nviews jointly guide the reconstruction process. Through the proposed\nbranch-wise processing, cross-view attention, and inter-branch fusion, the\nmodel effectively captures complementary geometric cues without requiring\ncamera calibration. Moreover, we introduce a D-Normal regularizer based on an\nexplicit 3D Gaussian representation for surface reconstruction. It couples\nsurface normals with other geometric parameters to jointly optimize the 3D\ngeometry, significantly improving 3D consistency and surface detail accuracy.\nExperimental results demonstrate that Surf3R achieves state-of-the-art\nperformance on multiple surface reconstruction metrics on ScanNet++ and Replica\ndatasets, exhibiting excellent generalization and efficiency.", "AI": {"tldr": "Surf3R\u662f\u4e00\u79cd\u65e0\u9700\u76f8\u673a\u6821\u51c6\u7684\u591a\u89c6\u56fe3D\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5206\u652f\u548c\u89e3\u7801\u67b6\u6784\u572810\u79d2\u5185\u5b8c\u6210\u573a\u666f\u91cd\u5efa\uff0c\u6027\u80fd\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u591a\u89c6\u56fe3D\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u76f8\u673a\u6821\u51c6\u548c\u4f4d\u59ff\u4f30\u8ba1\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u591a\u5206\u652f\u548c\u591a\u89c6\u56fe\u89e3\u7801\u67b6\u6784\uff0c\u7ed3\u5408\u5206\u652f\u7ea7\u5904\u7406\u3001\u8de8\u89c6\u56fe\u6ce8\u610f\u529b\u548c\u5206\u652f\u95f4\u878d\u5408\uff0c\u65e0\u9700\u76f8\u673a\u6821\u51c6\uff1b\u5f15\u5165D-Normal\u6b63\u5219\u5316\u5668\u4f18\u53163D\u51e0\u4f55\u3002", "result": "\u5728ScanNet++\u548cReplica\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u5c55\u793a\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002", "conclusion": "Surf3R\u4e3a\u65e0\u9700\u76f8\u673a\u6821\u51c6\u7684\u5feb\u901f3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.04004", "pdf": "https://arxiv.org/pdf/2508.04004", "abs": "https://arxiv.org/abs/2508.04004", "authors": ["Tanguy Ropitault", "Matteo Bordin", "Paolo Testolina", "Michele Polese", "Pedram Johari", "Nada Golmie", "Tommaso Melodia"], "title": "Enabling Site-Specific Cellular Network Simulation Through Ray-Tracing-Driven ns-3", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "Evaluating cellular systems, from 5G New Radio (NR) and 5G-Advanced to 6G, is\nchallenging because the performance emerges from the tight coupling of\npropagation, beam management, scheduling, and higher-layer interactions.\nSystem-level simulation is therefore indispensable, yet the vast majority of\nstudies rely on the statistical 3GPP channel models. These are well suited to\ncapture average behavior across many statistical realizations, but cannot\nreproduce site-specific phenomena such as corner diffraction, street-canyon\nblockage, or deterministic line-of-sight conditions and\nangle-of-departure/arrival relationships that drive directional links. This\npaper extends 5G-LENA, an NR module for the system-level Network Simulator 3\n(ns-3), with a trace-based channel model that processes the Multipath\nComponents (MPCs) obtained from external ray-tracers (e.g., Sionna Ray Tracer\n(RT)) or measurement campaigns. Our module constructs frequency-domain channel\nmatrices and feeds them to the existing Physical (PHY)/Medium Access Control\n(MAC) stack without any further modifications. The result is a geometry-based\nchannel model that remains fully compatible with the standard 3GPP\nimplementation in 5G-LENA, while delivering site-specific geometric fidelity.\nThis new module provides a key building block toward Digital Twin (DT)\ncapabilities by offering realistic site-specific channel modeling, unlocking\nstudies that require site awareness, including beam management, blockage\nmitigation, and environment-aware sensing. We demonstrate its capabilities for\nprecise beam-steering validation and end-to-end metric analysis. In both cases,\nthe trace-driven engine exposes performance inflections that the statistical\nmodel does not exhibit, confirming its value for high-fidelity system-level\ncellular networks research and as a step toward DT applications.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u7cfb\u7edf\u7ea7\u4eff\u771f\u56685G-LENA\uff0c\u5f15\u5165\u57fa\u4e8e\u51e0\u4f55\u7684\u901a\u9053\u6a21\u578b\uff0c\u63d0\u5347\u5bf9\u7279\u5b9a\u573a\u666f\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6ce2\u675f\u7ba1\u7406\u548c\u7aef\u5230\u7aef\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u901a\u9053\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u7279\u5b9a\u573a\u666f\u7684\u51e0\u4f55\u73b0\u8c61\uff08\u5982\u884d\u5c04\u3001\u906e\u6321\u7b49\uff09\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7ea7\u4eff\u771f\u7684\u7cbe\u786e\u6027\u3002", "method": "\u6269\u5c555G-LENA\u6a21\u5757\uff0c\u6574\u5408\u57fa\u4e8e\u5c04\u7ebf\u8ffd\u8e2a\u6216\u5b9e\u6d4b\u7684\u591a\u5f84\u5206\u91cf\uff08MPCs\uff09\uff0c\u6784\u5efa\u9891\u57df\u901a\u9053\u77e9\u9635\u5e76\u4e0e\u73b0\u6709\u7269\u7406\u5c42/\u4ecb\u8d28\u8bbf\u95ee\u63a7\u5236\u5c42\u517c\u5bb9\u3002", "result": "\u65b0\u6a21\u578b\u5c55\u793a\u4e86\u7edf\u8ba1\u6a21\u578b\u65e0\u6cd5\u63ed\u793a\u7684\u6027\u80fd\u53d8\u5316\uff0c\u4e3a\u9ad8\u4fdd\u771f\u7cfb\u7edf\u7ea7\u7814\u7a76\u548c\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u57fa\u4e8e\u51e0\u4f55\u7684\u901a\u9053\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u7684\u573a\u666f\u7279\u5f02\u6027\u80fd\u529b\uff0c\u662f\u8fc8\u5411\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2508.03914", "pdf": "https://arxiv.org/pdf/2508.03914", "abs": "https://arxiv.org/abs/2508.03914", "authors": ["Sahil Khan", "Suhas Vittal", "Kenneth Brown", "Jonathan Baker"], "title": "Moveless: Minimizing Overhead on QCCDs via Versatile Execution and Low Excess Shuttling", "categories": ["quant-ph", "cs.ET", "cs.SY", "eess.SY"], "comment": "12 pages, 14 figures, Accepted at IEEE QCE 2025, Will be presented on\n  September 4, 2025", "summary": "One of the most promising paths towards large scale fault tolerant quantum\ncomputation is the use of quantum error correcting stabilizer codes. Just like\nevery other quantum circuit, these codes must be compiled to hardware in a way\nto minimize the total physical error introduced into the system, for example\neither due to high latency execution or excessive gates to meet connectivity\nlimitations of the target hardware. However, unlike arbitrary quantum circuits,\nall syndrome extraction circuits have several common properties, for example\nthey have a bipartite connectivity graph, consist only of commuting\nsubcircuits, among other properties. For the most part, compilation methods\nhave aimed at being generic, able to map any input circuit into executables on\nthe hardware, and therefore cannot appropriately exploit these properties and\nresult in executables which have higher physical error. In the case of modular\ntrapped ion systems, specifically QCCDs, this corresponds to the insertion of\nexcessive shuttling operations necessary to realize arbitrary qubit\ninteractions. We propose a compilation scheme explicitly tailored for the\nstructural regularity of QEC circuits based on several key observations: 1.\nonly ancilla or data (but not both) should be shuttled, 2. stabilizers can be\nexecuted in any order meaning we can dynamically modify circuit execution on a\nper-cycle basis 3. ancilla are indistinguishable meaning any can be selected to\nbegin a stabilizer measurement and retain a fixed-point mapping between cycles,\nand 4. QCCD hardware limits the number of parallel operations equal to the\nnumber traps in the system, meaning fewer ancilla are necessary and can be\nreused. Our resulting compiler, leads to QEC circuits which are on average\n3.38x faster to execute, and lead to up to two orders of magnitude of\nimprovement in logical error rates with realistic physical error rates.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u91cf\u5b50\u7ea0\u9519\uff08QEC\uff09\u7535\u8def\u7ed3\u6784\u7279\u70b9\u7684\u7f16\u8bd1\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u901f\u5ea6\u548c\u903b\u8f91\u9519\u8bef\u7387\u3002", "motivation": "\u91cf\u5b50\u7ea0\u9519\u7801\u5728\u7f16\u8bd1\u65f6\u82e5\u91c7\u7528\u901a\u7528\u65b9\u6cd5\uff0c\u4f1a\u5bfc\u81f4\u786c\u4ef6\u6267\u884c\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9488\u5bf9\u5176\u7ed3\u6784\u7279\u6027\u7684\u4e13\u95e8\u7f16\u8bd1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u89c2\u5bdfQEC\u7535\u8def\u7684\u7279\u70b9\uff08\u5982\u4ec5\u9700\u79fb\u52a8\u8f85\u52a9\u6bd4\u7279\u6216\u6570\u636e\u6bd4\u7279\u3001\u7a33\u5b9a\u5668\u53ef\u52a8\u6001\u6267\u884c\u7b49\uff09\uff0c\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u7f16\u8bd1\u5668\u3002", "result": "\u65b0\u7f16\u8bd1\u5668\u4f7fQEC\u7535\u8def\u6267\u884c\u901f\u5ea6\u5e73\u5747\u63d0\u53473.38\u500d\uff0c\u903b\u8f91\u9519\u8bef\u7387\u5728\u73b0\u5b9e\u7269\u7406\u9519\u8bef\u7387\u4e0b\u6700\u591a\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u9488\u5bf9QEC\u7535\u8def\u7684\u4e13\u95e8\u7f16\u8bd1\u65b9\u6848\u80fd\u663e\u8457\u4f18\u5316\u6267\u884c\u6548\u7387\u548c\u903b\u8f91\u9519\u8bef\u7387\u3002"}}
{"id": "2508.04031", "pdf": "https://arxiv.org/pdf/2508.04031", "abs": "https://arxiv.org/abs/2508.04031", "authors": ["Lianggui Weng", "Dandan Liu", "Rong Zhu", "Bolin Ding", "Jingren Zhou"], "title": "BridgeScope: A Universal Toolkit for Bridging Large Language Models and Databases", "categories": ["cs.DB"], "comment": "6 pages, 6 figures", "summary": "As large language models (LLMs) demonstrate increasingly powerful reasoning\nand orchestration capabilities, LLM-based agents are rapidly proliferating for\ncomplex data-related tasks. Despite this progress, the current design of how\nLLMs interact with databases exhibits critical limitations in usability,\nsecurity, privilege management, and data transmission efficiency. To resolve\nthese challenges, we introduce BridgeScope, a universal toolkit bridging LLMs\nand databases through three key innovations. First, it modularizes SQL\noperations into fine-grained tools for context retrieval, CRUD execution, and\nACID-compliant transaction management, enabling more precise and LLM-friendly\nfunctionality controls. Second, it aligns tool implementations with both\ndatabase privileges and user security policies to steer LLMs away from unsafe\nor unauthorized operations, improving task execution efficiency while\nsafeguarding database security. Third, it introduces a proxy mechanism for\nseamless inter-tool data transfer, bypassing LLM transmission bottlenecks. All\nof these designs are database-agnostic and can be transparently integrated with\nexisting agent architectures. We also release an open-source implementation of\nBridgeScope for PostgreSQL. Evaluations on two novel benchmarks demonstrate\nthat BridgeScope enables LLM agents to operate databases more effectively,\nreduces token usage by up to 80% through improved security awareness, and\nuniquely supports data-intensive workflows beyond existing toolkits,\nestablishing BridgeScope as a robust foundation for next-generation intelligent\ndata automation.", "AI": {"tldr": "BridgeScope\u662f\u4e00\u79cd\u8fde\u63a5\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u6570\u636e\u5e93\u7684\u901a\u7528\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u6a21\u5757\u5316SQL\u64cd\u4f5c\u3001\u5bf9\u9f50\u6743\u9650\u4e0e\u5b89\u5168\u7b56\u7565\u4ee5\u53ca\u5f15\u5165\u4ee3\u7406\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u4e0e\u6570\u636e\u5e93\u4ea4\u4e92\u4e2d\u7684\u5b9e\u7528\u6027\u3001\u5b89\u5168\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u4e0e\u6570\u636e\u5e93\u7684\u4ea4\u4e92\u8bbe\u8ba1\u5b58\u5728\u5b9e\u7528\u6027\u3001\u5b89\u5168\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u6765\u4f18\u5316\u8fd9\u4e9b\u65b9\u9762\u3002", "method": "BridgeScope\u901a\u8fc7\u6a21\u5757\u5316SQL\u64cd\u4f5c\u3001\u5bf9\u9f50\u6570\u636e\u5e93\u6743\u9650\u4e0e\u7528\u6237\u5b89\u5168\u7b56\u7565\u3001\u5f15\u5165\u4ee3\u7406\u673a\u5236\u6765\u5b9e\u73b0\u9ad8\u6548\u5b89\u5168\u7684LLM\u4e0e\u6570\u636e\u5e93\u4ea4\u4e92\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0cBridgeScope\u663e\u8457\u63d0\u5347\u4e86LLM\u64cd\u4f5c\u6570\u636e\u5e93\u7684\u6548\u7387\uff0c\u51cf\u5c11\u4e8680%\u7684token\u4f7f\u7528\uff0c\u5e76\u652f\u6301\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u6d41\u3002", "conclusion": "BridgeScope\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u6570\u636e\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2508.03981", "pdf": "https://arxiv.org/pdf/2508.03981", "abs": "https://arxiv.org/abs/2508.03981", "authors": ["Zhikui Chen", "Muhammad Zeeshan Haider", "Naiwen Luo", "Shuo Yu", "Xu Yuan", "Yaochen Zhang", "Tayyaba Noreen"], "title": "Reputation-based partition scheme for IoT security", "categories": ["cs.DC", "cs.CR", "cs.DB"], "comment": null, "summary": "With the popularity of smart terminals, such as the Internet of Things,\ncrowdsensing is an emerging data aggregation paradigm, which plays a pivotal\nrole in data-driven applications. There are some key issues in the development\nof crowdsensing such as platform security and privacy protection. As the\ncrowdsensing is usually managed by a centralized platform, centralized\nmanagement will bring various security vulnerabilities and scalability issues.\nTo solve these issues, an effective reputation-based partition scheme (RSPC) is\nproposed in this article. The partition scheme calculates the optimal partition\nsize by combining the node reputation value and divides the node into several\ndisjoint partitions according to the node reputation value. By selecting the\nappropriate partition size, RSPC provides a mechanism to ensure that each\npartition is valid, as long as themaximum permissible threshold for the failed\nnode is observed. At the same time, the RSPC reorganizes the network\nperiodically to avoid partition attacks. In addition, for cross-partition\ntransactions, this paper innovatively proposes a four-stage confirmation\nprotocol to ensure the efficient and safe completion of cross-partition\ntransactions. Finally, experiments show that RSPC improves scalability, low\nlatency, and high throughput for crowdsensing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u8a89\u7684\u5206\u533a\u65b9\u6848\uff08RSPC\uff09\uff0c\u4ee5\u89e3\u51b3\u4f17\u5305\u611f\u77e5\u4e2d\u7684\u5b89\u5168\u548c\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u7ec8\u7aef\u7684\u666e\u53ca\uff0c\u4f17\u5305\u611f\u77e5\u6210\u4e3a\u91cd\u8981\u6570\u636e\u805a\u5408\u8303\u5f0f\uff0c\u4f46\u96c6\u4e2d\u7ba1\u7406\u5e26\u6765\u5b89\u5168\u6f0f\u6d1e\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "RSPC\u7ed3\u5408\u8282\u70b9\u4fe1\u8a89\u503c\u8ba1\u7b97\u6700\u4f18\u5206\u533a\u5927\u5c0f\uff0c\u5e76\u5b9a\u671f\u91cd\u7ec4\u7f51\u7edc\u4ee5\u907f\u514d\u653b\u51fb\uff0c\u540c\u65f6\u63d0\u51fa\u56db\u9636\u6bb5\u786e\u8ba4\u534f\u8bae\u786e\u4fdd\u8de8\u5206\u533a\u4ea4\u6613\u5b89\u5168\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRSPC\u5728\u53ef\u6269\u5c55\u6027\u3001\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RSPC\u4e3a\u4f17\u5305\u611f\u77e5\u63d0\u4f9b\u4e86\u5b89\u5168\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03900", "pdf": "https://arxiv.org/pdf/2508.03900", "abs": "https://arxiv.org/abs/2508.03900", "authors": ["Navaneeth Kunhi Purayil", "Diyou Shen", "Matteo Perotti", "Luca Benini"], "title": "TROOP: At-the-Roofline Performance for Vector Processors on Low Operational Intensity Workloads", "categories": ["cs.AR"], "comment": "To be published in IEEE International Conference on Computer Design\n  (ICCD) 2025", "summary": "The fast evolution of Machine Learning (ML) models requires flexible and\nefficient hardware solutions as hardwired accelerators face rapid obsolescence.\nVector processors are fully programmable and achieve high energy efficiencies\nby exploiting data parallelism, amortizing instruction fetch and decoding\ncosts. Hence, a promising design choice is to build accelerators based on\nshared L1-memory clusters of streamlined Vector Processing Elements (VPEs).\nHowever, current state-of-the-art VPEs are limited in L1 memory bandwidth and\nachieve high efficiency only for computational kernels with high data reuse in\nthe Vector Register File (VRF), such as General Matrix Multiplication (GEMM).\nPerformance is suboptimal for workloads with lower data reuse like General\nMatrix-Vector Multiplication (GEMV). To fully exploit available bandwidth at\nthe L1 memory interface, the VPE micro-architecture must be optimized to\nachieve near-ideal utilization, i.e., to be as close as possible to the L1\nmemory roofline (at-the-roofline). In this work, we propose TROOP, a set of\nhardware optimizations that include decoupled load-store interfaces, improved\nvector chaining, shadow buffers to hide VRF conflicts, and address scrambling\ntechniques to achieve at-the-roofline performance for VPEs without compromising\ntheir area and energy efficiency. We implement TROOP on an open-source\nstreamlined vector processor in a 12nm FinFET technology. TROOP achieves\nsignificant speedups of 1.5x, 2.2x, and 2.6x, respectively, for key\nmemory-intensive kernels such as GEMV, DOTP and AXPY, achieving at-the-roofline\nperformance. Additionally, TROOP enhances the energy efficiency by up to 45%,\nreaching 38 DP-GFLOPs/W (1 GHz, TT, 0.8V) for DOTP while maintaining a high\nenergy efficiency of 61 DP-GFLOPs/W for GEMMs, incurring only a minor area\noverhead of less than 7%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTROOP\uff0c\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u63d0\u5347\u5411\u91cf\u5904\u7406\u5355\u5143\uff08VPEs\uff09\u7684L1\u5185\u5b58\u5e26\u5bbd\u5229\u7528\uff0c\u5b9e\u73b0\u63a5\u8fd1\u7406\u8bba\u6027\u80fd\uff08at-the-roofline\uff09\uff0c\u663e\u8457\u52a0\u901f\u5185\u5b58\u5bc6\u96c6\u578b\u4efb\u52a1\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5feb\u901f\u6f14\u8fdb\u9700\u8981\u7075\u6d3b\u9ad8\u6548\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u73b0\u6709\u5411\u91cf\u5904\u7406\u5355\u5143\u5728\u4f4e\u6570\u636e\u91cd\u7528\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0d\u8db3\u3002", "method": "TROOP\u5305\u62ec\u89e3\u8026\u7684\u52a0\u8f7d-\u5b58\u50a8\u63a5\u53e3\u3001\u6539\u8fdb\u7684\u5411\u91cf\u94fe\u63a5\u3001\u5f71\u5b50\u7f13\u51b2\u548c\u5730\u5740\u7f6e\u4e71\u7b49\u6280\u672f\u4f18\u5316VPE\u5fae\u67b6\u6784\u3002", "result": "\u572812nm\u5de5\u827a\u4e2d\u5b9e\u73b0TROOP\uff0cGEMV\u3001DOTP\u548cAXPY\u4efb\u52a1\u5206\u522b\u63d0\u901f1.5x\u30012.2x\u548c2.6x\uff0c\u80fd\u6548\u63d0\u534745%\uff0c\u9762\u79ef\u5f00\u9500\u4ec57%\u3002", "conclusion": "TROOP\u663e\u8457\u63d0\u9ad8\u4e86VPEs\u5728\u5185\u5b58\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u80fd\u8ba1\u7b97\u4efb\u52a1\u7684\u7ade\u4e89\u529b\u3002"}}
{"id": "2508.04163", "pdf": "https://arxiv.org/pdf/2508.04163", "abs": "https://arxiv.org/abs/2508.04163", "authors": ["Hasra Dodampegama", "Mohan Sridharan"], "title": "Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork", "categories": ["cs.AI", "cs.LO", "cs.MA"], "comment": "14 pages, 6 figures", "summary": "AI agents deployed in assistive roles often have to collaborate with other\nagents (humans, AI systems) without prior coordination. Methods considered\nstate of the art for such ad hoc teamwork often pursue a data-driven approach\nthat needs a large labeled dataset of prior observations, lacks transparency,\nand makes it difficult to rapidly revise existing knowledge in response to\nchanges. As the number of agents increases, the complexity of decision-making\nmakes it difficult to collaborate effectively. This paper advocates leveraging\nthe complementary strengths of knowledge-based and data-driven methods for\nreasoning and learning for ad hoc teamwork. For any given goal, our\narchitecture enables each ad hoc agent to determine its actions through\nnon-monotonic logical reasoning with: (a) prior commonsense domain-specific\nknowledge; (b) models learned and revised rapidly to predict the behavior of\nother agents; and (c) anticipated abstract future goals based on generic\nknowledge of similar situations in an existing foundation model. We\nexperimentally evaluate our architecture's capabilities in VirtualHome, a\nrealistic physics-based 3D simulation environment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdbAI\u4ee3\u7406\u5728\u975e\u9884\u5148\u534f\u8c03\u56e2\u961f\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3001\u7f3a\u4e4f\u900f\u660e\u5ea6\u4e14\u96be\u4ee5\u53ca\u65f6\u66f4\u65b0\uff0c\u968f\u7740\u4ee3\u7406\u6570\u91cf\u589e\u52a0\uff0c\u534f\u4f5c\u6548\u7387\u4e0b\u964d\u3002", "method": "\u7ed3\u5408\u975e\u5355\u8c03\u903b\u8f91\u63a8\u7406\uff0c\u5229\u7528\u5148\u9a8c\u5e38\u8bc6\u3001\u5feb\u901f\u5b66\u4e60\u7684\u9884\u6d4b\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u7684\u62bd\u8c61\u76ee\u6807\u63a8\u65ad\u3002", "result": "\u5728VirtualHome\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u77e5\u8bc6\u9a71\u52a8\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u7ed3\u5408\u80fd\u591f\u63d0\u5347\u975e\u534f\u8c03\u56e2\u961f\u7684\u534f\u4f5c\u6548\u7387\u3002"}}
{"id": "2508.03832", "pdf": "https://arxiv.org/pdf/2508.03832", "abs": "https://arxiv.org/abs/2508.03832", "authors": ["Andreas Pointner", "Josef Pichler", "Herbert Pr\u00e4hofer"], "title": "Generating Inputs for Grammar Mining using Dynamic Symbolic Execution", "categories": ["cs.PL"], "comment": null, "summary": "A vast number of software systems include components that parse and process\nstructured input. In addition to programming languages, which are analyzed by\ncompilers or interpreters, there are numerous components that process\nstandardized or proprietary data formats of varying complexity. Even if such\ncomponents were initially developed and tested based on a specification, such\nas a grammar, numerous modifications and adaptations over the course of\nsoftware evolution can make it impossible to precisely determine which inputs\nthey actually accept. In this situation, grammar mining can be used to\nreconstruct the specification in the form of a grammar. Established approaches\nalready produce useful results, provided that sufficient input data is\navailable to fully cover the input language. However, achieving this\ncompleteness is a major challenge. In practice, only input data recorded during\nthe operation of the software systems is available. If this data is used for\ngrammar mining, the resulting grammar reflects only the actual processed inputs\nbut not the complete grammar of the input language accepted by the software\ncomponent. As a result, edge cases or previously supported features that no\nlonger appear in the available input data are missing from the generated\ngrammar. This work addresses this challenge by introducing a novel approach for\nthe automatic generation of inputs for grammar mining. Although input\ngenerators have already been used for fuzz testing, it remains unclear whether\nthey are also suitable for grammar miners. Building on the grammar miner Mimid,\nthis work presents a fully automated approach to input generation. The approach\nleverages Dynamic Symbolic Execution (DSE) and extends it with two mechanisms\nto overcome the limitations of DSE regarding structured input parsers. First,\nthe search for new inputs is guided by an iterative expansion that starts with\na single-character input and gradually extends it. Second, input generation is\nstructured into a novel three-phase approach, which separates the generation of\ninputs for parser functions. The proposed method was evaluated against a\ndiverse set of eleven benchmark applications from the existing literature.\nResults demonstrate that the approach achieves precision and recall for\nextracted grammars close to those derived from state-of-the-art grammar miners\nsuch as Mimid. Notably, it successfully uncovers subtle features and edge cases\nin parsers that are typically missed by such grammar miners. The effectiveness\nof the method is supported by empirical evidence, showing that it can achieve\nhigh performance in various domains without requiring prior input samples. This\ncontribution is significant for researchers and practitioners in software\nengineering, offering an automated, scalable, and precise solution for grammar\nmining. By eliminating the need for manual input generation, the approach not\nonly reduces workload but also enhances the robustness and comprehensiveness of\nthe extracted grammars. Following this approach, software engineers can\nreconstruct specification from existing (legacy) parsers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u8f93\u5165\u7684\u8bed\u6cd5\u6316\u6398\u65b9\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u7b26\u53f7\u6267\u884c\uff08DSE\uff09\u548c\u65b0\u578b\u4e09\u9636\u6bb5\u8f93\u5165\u751f\u6210\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u8986\u76d6\u5b8c\u6574\u8f93\u5165\u8bed\u8a00\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u6cd5\u6316\u6398\u65b9\u6cd5\u4f9d\u8d56\u8f93\u5165\u6570\u636e\u8986\u76d6\u5b8c\u6574\u8f93\u5165\u8bed\u8a00\uff0c\u4f46\u5b9e\u8df5\u4e2d\u5f88\u96be\u83b7\u53d6\u5168\u9762\u6570\u636e\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8bed\u6cd5\u9057\u6f0f\u8fb9\u7f18\u60c5\u51b5\u6216\u65e7\u529f\u80fd\u3002", "method": "\u63d0\u51fa\u7ed3\u5408DSE\u7684\u81ea\u52a8\u5316\u8f93\u5165\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u6269\u5c55\u8f93\u5165\u548c\u65b0\u578b\u4e09\u9636\u6bb5\u751f\u6210\u7b56\u7565\uff0c\u4f18\u5316\u8bed\u6cd5\u6316\u6398\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u5e94\u7528\u4e2d\u9a8c\u8bc1\uff0c\u751f\u6210\u7684\u8bed\u6cd5\u63a5\u8fd1Mimid\u7b49\u73b0\u6709\u5de5\u5177\u7cbe\u5ea6\uff0c\u4e14\u80fd\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u9057\u6f0f\u7684\u7ec6\u5fae\u7279\u5f81\u548c\u8fb9\u7f18\u60c5\u51b5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u7cbe\u5ea6\u7684\u8bed\u6cd5\u6316\u6398\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u624b\u52a8\u751f\u6210\u8f93\u5165\uff0c\u63d0\u5347\u4e86\u8bed\u6cd5\u7684\u5168\u9762\u6027\u3002"}}
{"id": "2508.03729", "pdf": "https://arxiv.org/pdf/2508.03729", "abs": "https://arxiv.org/abs/2508.03729", "authors": ["Kosmas Pinitas", "Konstantinos Makantasis", "Georgios N. Yannakakis"], "title": "Privileged Contrastive Pretraining for Multimodal Affect Modelling", "categories": ["cs.LG", "cs.HC", "cs.MM"], "comment": null, "summary": "Affective Computing (AC) has made significant progress with the advent of\ndeep learning, yet a persistent challenge remains: the reliable transfer of\naffective models from controlled laboratory settings (in-vitro) to uncontrolled\nreal-world environments (in-vivo). To address this challenge we introduce the\nPrivileged Contrastive Pretraining (PriCon) framework according to which models\nare first pretrained via supervised contrastive learning (SCL) and then act as\nteacher models within a Learning Using Privileged Information (LUPI) framework.\nPriCon both leverages privileged information during training and enhances the\nrobustness of derived affect models via SCL. Experiments conducted on two\nbenchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained\nusing PriCon consistently outperform LUPI and end to end models. Remarkably, in\nmany cases, PriCon models achieve performance comparable to models trained with\naccess to all modalities during both training and testing. The findings\nunderscore the potential of PriCon as a paradigm towards further bridging the\ngap between in-vitro and in-vivo affective modelling, offering a scalable and\npractical solution for real-world applications.", "AI": {"tldr": "PriCon\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u7279\u6743\u4fe1\u606f\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u60c5\u611f\u8ba1\u7b97\u6a21\u578b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u60c5\u611f\u8ba1\u7b97\u6a21\u578b\u4ece\u5b9e\u9a8c\u5ba4\u5230\u73b0\u5b9e\u73af\u5883\u8fc1\u79fb\u65f6\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u7279\u6743\u4fe1\u606f\u5b66\u4e60\u7ed3\u5408\u7684PriCon\u6846\u67b6\u3002", "result": "\u5728RECOLA\u548cAGAIN\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfLUPI\u548c\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u63a5\u8fd1\u5168\u6a21\u6001\u8bad\u7ec3\u6548\u679c\u3002", "conclusion": "PriCon\u80fd\u6709\u6548\u7f29\u5c0f\u5b9e\u9a8c\u5ba4\u4e0e\u771f\u5b9e\u73af\u5883\u95f4\u7684\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\uff0c\u5177\u6709\u5b9e\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.03713", "pdf": "https://arxiv.org/pdf/2508.03713", "abs": "https://arxiv.org/abs/2508.03713", "authors": ["Minsuk Chang", "Yao Wang", "Huichen Will Wang", "Yuanhong Zhou", "Andreas Bulling", "Cindy Xiong Bearfield"], "title": "Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy and Visual Attention", "categories": ["cs.HC", "cs.CV"], "comment": "11 pages, 9 figures, Accepted to 2025 IEEE VIS (Visualization and\n  Visual Analytics)", "summary": "Accounting for individual differences can improve the effectiveness of\nvisualization design. While the role of visual attention in visualization\ninterpretation is well recognized, existing work often overlooks how this\nbehavior varies based on visual literacy levels. Based on data from a\n235-participant user study covering three visualization tests (mini-VLAT,\nCALVI, and SGL), we show that distinct attention patterns in visual data\nexploration can correlate with participants' literacy levels: While experts\n(high-scorers) generally show a strong attentional focus, novices (low-scorers)\nfocus less and explore more. We then propose two computational models\nleveraging these insights: Lit2Sal -- a novel visual saliency model that\npredicts observer attention given their visualization literacy level, and\nSal2Lit -- a model to predict visual literacy from human visual attention data.\nOur quantitative and qualitative evaluation demonstrates that Lit2Sal\noutperforms state-of-the-art saliency models with literacy-aware\nconsiderations. Sal2Lit predicts literacy with 86% accuracy using a single\nattention map, providing a time-efficient supplement to literacy assessment\nthat only takes less than a minute. Taken together, our unique approach to\nconsider individual differences in salience models and visual attention in\nliteracy assessments paves the way for new directions in personalized visual\ndata communication to enhance understanding.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0e\u89c6\u89c9\u7d20\u517b\u7684\u5173\u7cfb\uff0c\u63d0\u51faLit2Sal\u548cSal2Lit\u6a21\u578b\uff0c\u5206\u522b\u9884\u6d4b\u6ce8\u610f\u529b\u5206\u5e03\u548c\u89c6\u89c9\u7d20\u517b\uff0c\u63d0\u5347\u4e86\u53ef\u89c6\u5316\u8bbe\u8ba1\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002", "motivation": "\u63a2\u8ba8\u89c6\u89c9\u7d20\u517b\u5dee\u5f02\u5982\u4f55\u5f71\u54cd\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u4ee5\u4f18\u5316\u53ef\u89c6\u5316\u8bbe\u8ba1\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002", "method": "\u57fa\u4e8e235\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u6570\u636e\uff0c\u63d0\u51faLit2Sal\u548cSal2Lit\u4e24\u4e2a\u8ba1\u7b97\u6a21\u578b\uff0c\u5206\u522b\u9884\u6d4b\u6ce8\u610f\u529b\u5206\u5e03\u548c\u89c6\u89c9\u7d20\u517b\u3002", "result": "Lit2Sal\u4f18\u4e8e\u73b0\u6709\u663e\u8457\u6027\u6a21\u578b\uff0cSal2Lit\u9884\u6d4b\u89c6\u89c9\u7d20\u517b\u7684\u51c6\u786e\u7387\u8fbe86%\uff0c\u4e14\u8bc4\u4f30\u65f6\u95f4\u5c11\u4e8e1\u5206\u949f\u3002", "conclusion": "\u7ed3\u5408\u4e2a\u4f53\u5dee\u5f02\u7684\u663e\u8457\u6027\u6a21\u578b\u548c\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u6790\uff0c\u4e3a\u4e2a\u6027\u5316\u89c6\u89c9\u6570\u636e\u6c9f\u901a\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.04125", "pdf": "https://arxiv.org/pdf/2508.04125", "abs": "https://arxiv.org/abs/2508.04125", "authors": ["Sangwon Hyun", "Hyunjun Kim", "Jinhyuk Jang", "Hyojin Choi", "M. Ali Babar"], "title": "Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks", "categories": ["cs.SE", "cs.AI"], "comment": "The benchmark repository has not been publicly released yet due to\n  the IP policy in our institutions. If you would like to use the benchmark or\n  collaborate on extension, please contact \"dr.sangwon.hyun@gmail.com\"", "summary": "The application of Large Language Models (LLMs) is growing in the productive\ncompletion of Software Engineering tasks. Yet, studies investigating the\nproductive prompting techniques often employed a limited problem space,\nprimarily focusing on well-known prompting patterns and mainly targeting\nfunction-level SE practices. We identify significant gaps in real-world\nworkflows that involve complexities beyond class-level (e.g., multi-class\ndependencies) and different features that can impact Human-LLM Interactions\n(HLIs) processes in code generation. To address these issues, we designed an\nexperiment that comprehensively analyzed the HLI features regarding the code\ngeneration productivity. Our study presents two project-level benchmark tasks,\nextending beyond function-level evaluations. We conducted a user study with 36\nparticipants from diverse backgrounds, asking them to solve the assigned tasks\nby interacting with the GPT assistant using specific prompting patterns. We\nalso examined the participants' experience and their behavioral features during\ninteractions by analyzing screen recordings and GPT chat logs. Our statistical\nand empirical investigation revealed (1) that three out of 15 HLI features\nsignificantly impacted the productivity in code generation; (2) five primary\nguidelines for enhancing productivity for HLI processes; and (3) a taxonomy of\n29 runtime and logic errors that can occur during HLI processes, along with\nsuggested mitigation plans.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u9879\u76ee\u7ea7\u522b\u7684\u4ee3\u7801\u751f\u6210\u751f\u4ea7\u529b\u548c\u4eba\u673a\u4ea4\u4e92\uff08HLI\uff09\u7279\u5f81\uff0c\u63ed\u793a\u4e86\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u548c\u6539\u8fdb\u6307\u5357\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5f25\u8865\u73b0\u6709\u7814\u7a76\u5728\u9879\u76ee\u7ea7\u522b\u548cHLI\u7279\u5f81\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u5b9e\u9a8c\uff0c\u5206\u679036\u540d\u53c2\u4e0e\u8005\u5728\u9879\u76ee\u7ea7\u4efb\u52a1\u4e2d\u4f7f\u7528GPT\u52a9\u624b\u7684\u4ea4\u4e92\u884c\u4e3a\u548c\u4ee3\u7801\u751f\u6210\u6548\u679c\u3002", "result": "\u8bc6\u522b\u51fa15\u4e2aHLI\u7279\u5f81\u4e2d\u76843\u4e2a\u5bf9\u751f\u4ea7\u529b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u63d0\u51fa5\u6761\u6539\u8fdb\u6307\u5357\u548c29\u79cd\u9519\u8bef\u5206\u7c7b\u53ca\u7f13\u89e3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u751f\u4ea7\u529b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u548c\u9519\u8bef\u5904\u7406\u6846\u67b6\u3002"}}
{"id": "2508.04687", "pdf": "https://arxiv.org/pdf/2508.04687", "abs": "https://arxiv.org/abs/2508.04687", "authors": ["Ye Pan", "Ruisi Zhang", "Jingying Wang", "Nengfu Chen", "Yilin Qiu", "Yu Ding", "Kenny Mitchell"], "title": "MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics", "categories": ["cs.GR", "cs.CV", "I.3.2; I.4.10"], "comment": "IEEE VR extended authors version of the article published in 2022\n  IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and\n  Workshops (VRW). This work was supported by the European Union's Horizon 2020\n  research and innovation programme under Grant 101017779", "summary": "Our purpose is to improve performance-based animation which can drive\nbelievable 3D stylized characters that are truly perceptual. By combining\ntraditional blendshape animation techniques with multiple machine learning\nmodels, we present both non-real time and real time solutions which drive\ncharacter expressions in a geometrically consistent and perceptually valid way.\nFor the non-real time system, we propose a 3D emotion transfer network makes\nuse of a 2D human image to generate a stylized 3D rig parameters. For the real\ntime system, we propose a blendshape adaption network which generates the\ncharacter rig parameter motions with geometric consistency and temporally\nstability. We demonstrate the effectiveness of our system by comparing to a\ncommercial product Faceware. Results reveal that ratings of the recognition,\nintensity, and attractiveness of expressions depicted for animated characters\nvia our systems are statistically higher than Faceware. Our results may be\nimplemented into the animation pipeline, and provide animators with a system\nfor creating the expressions they wish to use more quickly and accurately.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u7ed3\u5408\u4f20\u7edfblendshape\u52a8\u753b\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u975e\u5b9e\u65f6\u548c\u5b9e\u65f6\u76843D\u98ce\u683c\u5316\u89d2\u8272\u52a8\u753b\u7cfb\u7edf\uff0c\u5728\u611f\u77e5\u6548\u679c\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u5546\u4e1a\u4ea7\u54c1Faceware\u3002", "motivation": "\u901a\u8fc7\u6539\u8fdb\u57fa\u4e8e\u6027\u80fd\u7684\u52a8\u753b\u6280\u672f\uff0c\u5b9e\u73b0\u66f4\u5177\u611f\u77e5\u6548\u679c\u76843D\u98ce\u683c\u5316\u89d2\u8272\u52a8\u753b\uff0c\u63d0\u5347\u52a8\u753b\u7684\u8868\u73b0\u529b\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408blendshape\u6280\u672f\u4e0e\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u63d0\u51fa\u975e\u5b9e\u65f6\uff083D\u60c5\u611f\u8fc1\u79fb\u7f51\u7edc\uff09\u548c\u5b9e\u65f6\uff08blendshape\u9002\u5e94\u7f51\u7edc\uff09\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7cfb\u7edf\u751f\u6210\u7684\u52a8\u753b\u5728\u8bc6\u522b\u5ea6\u3001\u5f3a\u5ea6\u548c\u5438\u5f15\u529b\u4e0a\u7684\u8bc4\u5206\u663e\u8457\u9ad8\u4e8eFaceware\u5546\u4e1a\u4ea7\u54c1\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u96c6\u6210\u5230\u52a8\u753b\u5236\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u5e2e\u52a9\u52a8\u753b\u5e08\u66f4\u5feb\u901f\u3001\u51c6\u786e\u5730\u751f\u6210\u6240\u9700\u8868\u60c5\uff0c\u63d0\u5347\u52a8\u753b\u5236\u4f5c\u6548\u7387\u3002"}}
{"id": "2508.04015", "pdf": "https://arxiv.org/pdf/2508.04015", "abs": "https://arxiv.org/abs/2508.04015", "authors": ["Haoxiang Luo", "Kun Yang", "Qi Huang", "Schahram Dustdar"], "title": "A Novel Hierarchical Co-Optimization Framework for Coordinated Task Scheduling and Power Dispatch in Computing Power Networks", "categories": ["cs.NI"], "comment": null, "summary": "The proliferation of large-scale artificial intelligence and data-intensive\napplications has spurred the development of Computing Power Networks (CPNs),\nwhich promise to deliver ubiquitous and on-demand computational resources.\nHowever, the immense energy consumption of these networks poses a significant\nsustainability challenge. Simultaneously, power grids are grappling with the\ninstability introduced by the high penetration of intermittent renewable energy\nsources (RES). This paper addresses these dual challenges through a novel\nTwo-Stage Co-Optimization (TSCO) framework that synergistically manages power\nsystem dispatch and CPN task scheduling to achieve low-carbon operations. The\nframework decomposes the complex, large-scale problem into a day-ahead\nstochastic unit commitment (SUC) stage and a real-time operational stage. The\nformer is solved using Benders decomposition for computational tractability,\nwhile in the latter, economic dispatch of generation assets is coupled with an\nadaptive CPN task scheduling managed by a Deep Reinforcement Learning (DRL)\nagent. This agent makes intelligent, carbon-aware decisions by responding to\ndynamic grid conditions, including real-time electricity prices and marginal\ncarbon intensity. Through extensive simulations on an IEEE 30-bus system\nintegrated with a CPN, the TSCO framework is shown to significantly outperform\nbaseline approaches. Results demonstrate that the proposed framework reduces\ntotal carbon emissions and operational costs, while simultaneously decreasing\nRES curtailment by more than 60% and maintaining stringent Quality of Service\n(QoS) for computational tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u534f\u540c\u4f18\u5316\u6846\u67b6\uff08TSCO\uff09\uff0c\u7528\u4e8e\u540c\u65f6\u7ba1\u7406\u7535\u529b\u7cfb\u7edf\u8c03\u5ea6\u548c\u8ba1\u7b97\u80fd\u529b\u7f51\u7edc\uff08CPN\uff09\u4efb\u52a1\u8c03\u5ea6\uff0c\u4ee5\u5b9e\u73b0\u4f4e\u78b3\u8fd0\u8425\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u4e3a\u65e5\u524d\u548c\u5b9e\u65f6\u4e24\u4e2a\u9636\u6bb5\uff0c\u7ed3\u5408Benders\u5206\u89e3\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u78b3\u6392\u653e\u3001\u8fd0\u8425\u6210\u672c\u53ca\u53ef\u518d\u751f\u80fd\u6e90\u5f03\u7535\u91cf\u3002", "motivation": "\u5e94\u5bf9\u5927\u89c4\u6a21\u4eba\u5de5\u667a\u80fd\u548c\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u5e26\u6765\u7684\u80fd\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u4ee5\u53ca\u9ad8\u6bd4\u4f8b\u53ef\u518d\u751f\u80fd\u6e90\u5f15\u5165\u7684\u7535\u7f51\u4e0d\u7a33\u5b9a\u6027\u6311\u6218\uff0c\u63d0\u51fa\u534f\u540c\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff08TSCO\uff09\uff0c\u7ed3\u5408Benders\u5206\u89e3\u548cDRL\u667a\u80fd\u8c03\u5ea6\uff0c\u5b9e\u73b0\u7535\u529b\u7cfb\u7edf\u4e0eCPN\u4efb\u52a1\u7684\u9ad8\u6548\u534f\u540c\u7ba1\u7406\u3002", "result": "\u5728IEEE 30-bus\u7cfb\u7edf\u7684\u6a21\u62df\u4e2d\uff0c\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u78b3\u6392\u653e\u548c\u8fd0\u8425\u6210\u672c\uff0c\u540c\u65f6\u51cf\u5c11\u4e8660%\u4ee5\u4e0a\u7684\u53ef\u518d\u751f\u80fd\u6e90\u5f03\u7535\u91cf\uff0c\u5e76\u786e\u4fdd\u4e86\u4efb\u52a1\u7684\u670d\u52a1\u8d28\u91cf\u3002", "conclusion": "TSCO\u6846\u67b6\u4e3a\u89e3\u51b3\u80fd\u6e90\u6d88\u8017\u548c\u7535\u7f51\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u78b3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.04124", "pdf": "https://arxiv.org/pdf/2508.04124", "abs": "https://arxiv.org/abs/2508.04124", "authors": ["Matthias Bartolo", "Konstantinos Makantasis", "Dylan Seychell"], "title": "Learning Using Privileged Information for Litter Detection", "categories": ["cs.CV", "cs.ET", "cs.LG", "cs.PF"], "comment": "This paper was accepted at the 13th European Workshop on Visual\n  Information Processing (EUVIP 2025)", "summary": "As litter pollution continues to rise globally, developing automated tools\ncapable of detecting litter effectively remains a significant challenge. This\nstudy presents a novel approach that combines, for the first time, privileged\ninformation with deep learning object detection to improve litter detection\nwhile maintaining model efficiency. We evaluate our method across five widely\nused object detection models, addressing challenges such as detecting small\nlitter and objects partially obscured by grass or stones. In addition to this,\na key contribution of our work can also be attributed to formulating a means of\nencoding bounding box information as a binary mask, which can be fed to the\ndetection model to refine detection guidance. Through experiments on both\nwithin-dataset evaluation on the renowned SODA dataset and cross-dataset\nevaluation on the BDW and UAVVaste litter detection datasets, we demonstrate\nconsistent performance improvements across all models. Our approach not only\nbolsters detection accuracy within the training sets but also generalises well\nto other litter detection contexts. Crucially, these improvements are achieved\nwithout increasing model complexity or adding extra layers, ensuring\ncomputational efficiency and scalability. Our results suggest that this\nmethodology offers a practical solution for litter detection, balancing\naccuracy and efficiency in real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u7279\u6743\u4fe1\u606f\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u63d0\u9ad8\u5783\u573e\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u5168\u7403\u5783\u573e\u6c61\u67d3\u65e5\u76ca\u4e25\u91cd\uff0c\u5f00\u53d1\u9ad8\u6548\u81ea\u52a8\u68c0\u6d4b\u5de5\u5177\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u9996\u6b21\u7ed3\u5408\u7279\u6743\u4fe1\u606f\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\uff0c\u5f15\u5165\u4e8c\u8fdb\u5236\u63a9\u7801\u7f16\u7801\u8fb9\u754c\u6846\u4fe1\u606f\u4f18\u5316\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u4fdd\u6301\u6a21\u578b\u8ba1\u7b97\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5783\u573e\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u6548\u7387\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04701", "pdf": "https://arxiv.org/pdf/2508.04701", "abs": "https://arxiv.org/abs/2508.04701", "authors": ["Bobbi Yogatama", "Yifei Yang", "Kevin Kristensen", "Devesh Sarda", "Abigale Kim", "Adrian Cockcroft", "Yu Teng", "Joshua Patterson", "Gregory Kimball", "Wes McKinney", "Weiwei Gong", "Xiangyao Yu"], "title": "Rethinking Analytical Processing in the GPU Era", "categories": ["cs.DB"], "comment": null, "summary": "The era of GPU-powered data analytics has arrived. In this paper, we argue\nthat recent advances in hardware (e.g., larger GPU memory, faster interconnect\nand IO, and declining cost) and software (e.g., composable data systems and\nmature libraries) have removed the key barriers that have limited the wider\nadoption of GPU data analytics. We present Sirius, a prototype open-source\nGPU-native SQL engine that offers drop-in acceleration for diverse data\nsystems. Sirius treats GPU as the primary engine and leverages libraries like\nlibcudf for high-performance relational operators. It provides drop-in\nacceleration for existing databases by leveraging the standard Substrait query\nrepresentation, replacing the CPU engine without changing the user-facing\ninterface. On TPC-H, Sirius achieves 7x speedup when integrated with DuckDB in\na single node at the same hardware rental cost, and up to 12.5x speedup when\nintegrated with Apache Doris in a distributed setting.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aSirius\u7684\u539f\u578b\u5f00\u6e90GPU\u539f\u751fSQL\u5f15\u64ce\uff0c\u901a\u8fc7\u5229\u7528GPU\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5b9e\u73b0\u4e86\u5bf9\u73b0\u6709\u6570\u636e\u7cfb\u7edf\u7684\u65e0\u7f1d\u52a0\u901f\u3002", "motivation": "\u968f\u7740GPU\u5185\u5b58\u589e\u5927\u3001\u4e92\u8054\u548cIO\u901f\u5ea6\u63d0\u5347\u4ee5\u53ca\u6210\u672c\u4e0b\u964d\uff0c\u52a0\u4e0a\u8f6f\u4ef6\u751f\u6001\u7684\u6210\u719f\uff0cGPU\u6570\u636e\u5206\u6790\u7684\u5e7f\u6cdb\u91c7\u7528\u6210\u4e3a\u53ef\u80fd\u3002", "method": "Sirius\u5229\u7528libcudf\u7b49\u5e93\u5b9e\u73b0\u9ad8\u6027\u80fd\u5173\u7cfb\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7Substrait\u67e5\u8be2\u8868\u793a\u6807\u51c6\u65e0\u7f1d\u66ff\u6362CPU\u5f15\u64ce\u3002", "result": "\u5728TPC-H\u6d4b\u8bd5\u4e2d\uff0cSirius\u5728\u5355\u8282\u70b9\u4e0a\u4e0eDuckDB\u96c6\u6210\u65f6\u5b9e\u73b0\u4e867\u500d\u52a0\u901f\uff0c\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\u4e0eApache Doris\u96c6\u6210\u65f6\u52a0\u901f\u9ad8\u8fbe12.5\u500d\u3002", "conclusion": "Sirius\u5c55\u793a\u4e86GPU\u4f5c\u4e3a\u6570\u636e\u5206\u6790\u4e3b\u8981\u5f15\u64ce\u7684\u6f5c\u529b\uff0c\u4e3a\u73b0\u6709\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u65e0\u7f1d\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2508.03984", "pdf": "https://arxiv.org/pdf/2508.03984", "abs": "https://arxiv.org/abs/2508.03984", "authors": ["Yuki Uchino", "Katsuhisa Ozaki", "Toshiyuki Imamura"], "title": "High-Performance and Power-Efficient Emulation of Matrix Multiplication using INT8 Matrix Engines", "categories": ["cs.DC"], "comment": "8 pages, 9 figures", "summary": "Recent architectures integrate high-performance and power-efficient matrix\nengines. These engines demonstrate remarkable performance in low-precision\nmatrix multiplication, which is crucial in deep learning. Several techniques\nhave been proposed to emulate single- and double-precision general\nmatrix-matrix multiplication (SGEMM and DGEMM, respectively) by leveraging such\nlow-precision matrix engines. In this study, we present emulation methods that\nsignificantly outperforms conventional approaches. On a GH200 Grace Hopper\nSuperchip, the proposed DGEMM emulation achieves a 1.4x speedup and a 43\\%\nimprovement in power efficiency compared to native DGEMM for sufficiently large\nproblems. The proposed SGEMM emulation achieves a 3.0x speedup and a 154\\%\nimprovement in power efficiency compared to native SGEMM for sufficiently large\nproblems. Furthermore, compared to conventional emulation methods, the proposed\nemulation achieves more than 2x higher performance and superior power\nefficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5229\u7528\u4f4e\u7cbe\u5ea6\u77e9\u9635\u5f15\u64ce\u6a21\u62df\u9ad8\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\uff08SGEMM\u548cDGEMM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u4f4e\u7cbe\u5ea6\u77e9\u9635\u5f15\u64ce\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u9ad8\u6027\u80fd\u8868\u73b0\u6fc0\u53d1\u4e86\u5bf9\u5176\u6a21\u62df\u9ad8\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\u7684\u7814\u7a76\u9700\u6c42\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u65b0\u578b\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u4f4e\u7cbe\u5ea6\u77e9\u9635\u5f15\u64ce\u7684\u4f7f\u7528\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\u7684\u52a0\u901f\u548c\u80fd\u6548\u63d0\u5347\u3002", "result": "\u5728GH200 Grace Hopper Superchip\u4e0a\uff0cDGEMM\u6a21\u62df\u6bd4\u539f\u751f\u65b9\u6cd5\u63d0\u901f1.4\u500d\u3001\u80fd\u6548\u63d0\u534743%\uff1bSGEMM\u6a21\u62df\u63d0\u901f3.0\u500d\u3001\u80fd\u6548\u63d0\u5347154%\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0e\u80fd\u6548\u4e0a\u8fdc\u8d85\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04106", "pdf": "https://arxiv.org/pdf/2508.04106", "abs": "https://arxiv.org/abs/2508.04106", "authors": ["Shan Shen", "Xingyang Li", "Zhuohua Liu", "Yikai Wang", "Yiheng Wu", "Junhao Ma", "Yuquan Sun", "Wei W. Xing"], "title": "OpenYield: An Open-Source SRAM Yield Analysis and Optimization Benchmark Suite", "categories": ["cs.AR"], "comment": "Accepted by The 43rd IEEE International Conference on Computer Design\n  (ICCD2025)", "summary": "Static Random-Access Memory (SRAM) yield analysis is essential for\nsemiconductor innovation, yet research progress faces a critical challenge: the\nsignificant disconnect between simplified academic models and complex\nindustrial realities. The absence of open, realistic benchmarks has created a\nreproducibility crisis, where promising academic techniques often fail to\ntranslate to industrial practice. We present \\textit{OpenYield}, a\ncomprehensive open-source ecosystem designed to address this critical gap\nthrough three core contributions: (1) A realistic SRAM circuit generator that\nuniquely incorporates critical second-order-effect parasitics, inter-cell\nleakage coupling, and peripheral circuit variations, which are typically\nomitted in academic studies but decisive in industrial designs. (2) A\nstandardized evaluation platform with a simple interface and implemented\nbaseline yield analysis algorithms, enabling fair comparisons and reproducible\nresearch. (3) A standardized SRAM optimization platform, demonstrating\nOpenYield's utility in enhancing SRAM design robustness and efficiency,\nproviding a comprehensive benchmark for optimization algorithms. OpenYield\ncreates a foundation for meaningful academia-industry collaboration,\naccelerating innovation in memory design. The framework is publicly available\non \\href{https://github.com/ShenShan123/OpenYield}{OpenYield:URL}", "AI": {"tldr": "OpenYield\u662f\u89e3\u51b3SRAM\u826f\u7387\u5206\u6790\u4e2d\u5b66\u672f\u4e0e\u5de5\u4e1a\u8131\u8282\u95ee\u9898\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\uff0c\u63d0\u4f9b\u771f\u5b9eSRAM\u7535\u8def\u751f\u6210\u5668\u3001\u6807\u51c6\u5316\u8bc4\u4f30\u4e0e\u4f18\u5316\u5e73\u53f0\u3002", "motivation": "\u5b66\u672f\u6a21\u578b\u8fc7\u4e8e\u7b80\u5316\uff0c\u4e0e\u5de5\u4e1a\u590d\u6742\u7684\u73b0\u5b9e\u8131\u8282\uff0c\u5bfc\u81f4\u7814\u7a76\u6210\u679c\u96be\u4ee5\u5728\u5de5\u4e1a\u5b9e\u8df5\u4e2d\u590d\u73b0\u4e0e\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5f00\u6e90\u5de5\u5177OpenYield\uff0c\u5f15\u5165\u5de5\u4e1a\u5173\u952e\u7684\u4e8c\u9636\u6548\u5e94\u3001\u6f0f\u7535\u8026\u5408\u7b49\uff0c\u5e76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u4e0e\u4f18\u5316\u5e73\u53f0\u3002", "result": "OpenYield\u4e3a\u5b66\u672f\u4e0e\u5de5\u4e1a\u5408\u4f5c\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63d0\u5347\u4e86SRAM\u8bbe\u8ba1\u7684\u7a33\u5065\u6027\u548c\u6548\u7387\u3002", "conclusion": "OpenYield\u586b\u8865\u4e86\u5b66\u672f\u4e0e\u5de5\u4e1a\u95f4\u7684\u9e3f\u6c9f\uff0c\u52a0\u901f\u4e86\u5185\u5b58\u8bbe\u8ba1\u7684\u521b\u65b0\u3002"}}
{"id": "2508.04296", "pdf": "https://arxiv.org/pdf/2508.04296", "abs": "https://arxiv.org/abs/2508.04296", "authors": ["Titouan Carette", "Daniela Cojocaru", "Renaud Vilmart"], "title": "The decohered ZX-calculus", "categories": ["quant-ph", "cs.LO"], "comment": null, "summary": "The discard ZX-calculus is known to be complete and universal for mixed-state\nquantum mechanics, allowing for both quantum and classical processes. However,\nif the quantum aspects of ZX-calculus have been explored in depth, little work\nhas been done on the classical side. In this paper, we investigate a fragment\nof discard ZX-calculus obtained by decohering the usual generators of\nZX-calculus. We show that this calculus is universal and complete for affinely\nsupported probability distributions over $\\mathbb{F}_{2}^{n}$. To do so, we\nexhibit a normal form, mixing ideas from the graphical linear algebra program\nand diagrammatic Fourier transforms. Our results both clarify how to handle\nhybrid classical-quantum processes in the discard ZX-calculus and pave the way\nto the picturing of more general random variables and probabilistic processes.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e22\u5f03ZX-\u6f14\u7b97\u4e2d\u7684\u7ecf\u5178\u7247\u6bb5\uff0c\u8bc1\u660e\u5176\u5bf9F2\u4e0a\u7684\u6982\u7387\u5206\u5e03\u5177\u6709\u666e\u9002\u6027\u548c\u5b8c\u5907\u6027\u3002", "motivation": "ZX-\u6f14\u7b97\u7684\u91cf\u5b50\u90e8\u5206\u5df2\u6709\u6df1\u5165\u7814\u7a76\uff0c\u4f46\u5176\u7ecf\u5178\u90e8\u5206\u5c1a\u672a\u5145\u5206\u63a2\u8ba8\u3002", "method": "\u901a\u8fc7\u9000\u76f8\u5e72\u5904\u7406ZX-\u6f14\u7b97\u7684\u751f\u6210\u5143\uff0c\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u56fe\u5f62\u7ebf\u6027\u4ee3\u6570\u4e0e\u56fe\u89e3\u5085\u91cc\u53f6\u53d8\u6362\u7684\u89c4\u8303\u5f62\u5f0f\u3002", "result": "\u8be5\u6f14\u7b97\u5bf9F2\u4e0a\u7684\u6982\u7387\u5206\u5e03\u5177\u6709\u666e\u9002\u6027\u548c\u5b8c\u5907\u6027\u3002", "conclusion": "\u4e3a\u5904\u7406\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50\u8fc7\u7a0b\u548c\u66f4\u4e00\u822c\u7684\u968f\u673a\u53d8\u91cf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.04115", "pdf": "https://arxiv.org/pdf/2508.04115", "abs": "https://arxiv.org/abs/2508.04115", "authors": ["Roger C. Su", "Robert J. Colvin"], "title": "Weak Memory Model Formalisms: Introduction and Survey", "categories": ["cs.PL", "A.1; C.1.2; D.3.1; F.3.1; F.3.2"], "comment": null, "summary": "Memory consistency models define the order in which accesses to shared memory\nin a concurrent system may be observed to occur. Such models are a necessity\nsince program order is not a reliable indicator of execution order, due to\nmicroarchitectural features or compiler transformations. Concurrent\nprogramming, already a challenging task, is thus made even harder when weak\nmemory effects must be addressed. A rigorous specification of weak memory\nmodels is therefore essential to make this problem tractable for developers of\nsafety- and security-critical, low-level software.\n  In this paper we survey the field of formalisations of weak memory models,\nincluding their specification, their effects on execution, and tools and\ninference systems for reasoning about code. To assist the discussion we also\nprovide an introduction to two styles of formal representation found commonly\nin the literature (using a much simplified version of Intel's x86 as the\nexample): a step-by-step construction of traces of the system (operational\nsemantics); and with respect to relations between memory events (axiomatic\nsemantics). The survey covers some long-standing hardware features that lead to\nobservable weak behaviours, a description of historical developments in\npractice and in theory, an overview of computability and complexity results,\nand outlines current and future directions in the field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5f31\u5185\u5b58\u6a21\u578b\u7684\u89c4\u8303\u5316\uff0c\u5305\u62ec\u5176\u89c4\u8303\u3001\u5bf9\u6267\u884c\u7684\u5f71\u54cd\u4ee5\u53ca\u63a8\u7406\u5de5\u5177\uff0c\u5e76\u4ecb\u7ecd\u4e86\u64cd\u4f5c\u8bed\u4e49\u548c\u516c\u7406\u8bed\u4e49\u4e24\u79cd\u5f62\u5f0f\u5316\u8868\u793a\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u5fae\u67b6\u6784\u7279\u6027\u6216\u7f16\u8bd1\u5668\u4f18\u5316\uff0c\u7a0b\u5e8f\u987a\u5e8f\u4e0d\u80fd\u53ef\u9760\u53cd\u6620\u6267\u884c\u987a\u5e8f\uff0c\u8fd9\u4f7f\u5f97\u5e76\u53d1\u7f16\u7a0b\u66f4\u5177\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u4e25\u683c\u89c4\u8303\u5f31\u5185\u5b58\u6a21\u578b\u5bf9\u5f00\u53d1\u5b89\u5168\u5173\u952e\u7684\u4f4e\u7ea7\u8f6f\u4ef6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u64cd\u4f5c\u8bed\u4e49\u548c\u516c\u7406\u8bed\u4e49\u4e24\u79cd\u5f62\u5f0f\u5316\u65b9\u6cd5\uff08\u4ee5\u7b80\u5316\u7684x86\u4e3a\u4f8b\uff09\u4ecb\u7ecd\u5f31\u5185\u5b58\u6a21\u578b\u7684\u89c4\u8303\uff0c\u5e76\u7efc\u8ff0\u8be5\u9886\u57df\u7684\u5de5\u5177\u548c\u63a8\u7406\u7cfb\u7edf\u3002", "result": "\u7efc\u8ff0\u4e86\u5bfc\u81f4\u5f31\u884c\u4e3a\u7684\u957f\u5b58\u786c\u4ef6\u7279\u6027\u3001\u5b9e\u8df5\u4e0e\u7406\u8bba\u7684\u53d1\u5c55\u5386\u53f2\uff0c\u4ee5\u53ca\u8ba1\u7b97\u6027\u548c\u590d\u6742\u6027\u7ed3\u679c\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u5f31\u5185\u5b58\u6a21\u578b\u7684\u4e25\u683c\u89c4\u8303\u662f\u89e3\u51b3\u5e76\u53d1\u7f16\u7a0b\u6311\u6218\u7684\u5173\u952e\uff0c\u672c\u6587\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u5c55\u671b\u3002"}}
{"id": "2508.04161", "pdf": "https://arxiv.org/pdf/2508.04161", "abs": "https://arxiv.org/abs/2508.04161", "authors": ["Yuqin Cao", "Yixuan Gao", "Wei Sun", "Xiaohong Liu", "Yulun Zhang", "Xiongkuo Min"], "title": "Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Face videos accompanied by audio have become integral to our daily lives,\nwhile they often suffer from complex degradations. Most face video restoration\nmethods neglect the intrinsic correlations between the visual and audio\nfeatures, especially in mouth regions. A few audio-aided face video restoration\nmethods have been proposed, but they only focus on compression artifact\nremoval. In this paper, we propose a General Audio-assisted face Video\nrestoration Network (GAVN) to address various types of streaming video\ndistortions via identity and temporal complementary learning. Specifically,\nGAVN first captures inter-frame temporal features in the low-resolution space\nto restore frames coarsely and save computational cost. Then, GAVN extracts\nintra-frame identity features in the high-resolution space with the assistance\nof audio signals and face landmarks to restore more facial details. Finally,\nthe reconstruction module integrates temporal features and identity features to\ngenerate high-quality face videos. Experimental results demonstrate that GAVN\noutperforms the existing state-of-the-art methods on face video compression\nartifact removal, deblurring, and super-resolution. Codes will be released upon\npublication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u97f3\u9891\u8f85\u52a9\u4eba\u8138\u89c6\u9891\u4fee\u590d\u7f51\u7edc\uff08GAVN\uff09\uff0c\u901a\u8fc7\u8eab\u4efd\u548c\u65f6\u95f4\u4e92\u8865\u5b66\u4e60\u89e3\u51b3\u591a\u79cd\u6d41\u5a92\u4f53\u89c6\u9891\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4e86\u89c6\u89c9\u4e0e\u97f3\u9891\u7279\u5f81\u7684\u5185\u5728\u5173\u8054\uff0c\u5c24\u5176\u662f\u5634\u90e8\u533a\u57df\u3002\u5c11\u6570\u97f3\u9891\u8f85\u52a9\u65b9\u6cd5\u4ec5\u5173\u6ce8\u538b\u7f29\u4f2a\u5f71\u53bb\u9664\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "GAVN\u5206\u4e09\u9636\u6bb5\uff1a1) \u4f4e\u5206\u8fa8\u7387\u7a7a\u95f4\u6355\u6349\u5e27\u95f4\u65f6\u95f4\u7279\u5f81\u4ee5\u7c97\u6062\u590d\uff0c\u8282\u7701\u8ba1\u7b97\u6210\u672c\uff1b2) \u9ad8\u5206\u8fa8\u7387\u7a7a\u95f4\u7ed3\u5408\u97f3\u9891\u548c\u9762\u90e8\u5173\u952e\u70b9\u63d0\u53d6\u5e27\u5185\u8eab\u4efd\u7279\u5f81\uff0c\u6062\u590d\u66f4\u4e30\u5bcc\u7684\u9762\u90e8\u7ec6\u8282\uff1b3) \u91cd\u5efa\u6a21\u5757\u6574\u5408\u65f6\u95f4\u4e0e\u8eab\u4efd\u7279\u5f81\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGAVN\u5728\u89c6\u9891\u538b\u7f29\u4f2a\u5f71\u53bb\u9664\u3001\u53bb\u6a21\u7cca\u548c\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GAVN\u901a\u8fc7\u7efc\u5408\u5229\u7528\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u7c7b\u89c6\u9891\u5931\u771f\u4fee\u590d\u7684\u6548\u679c\uff0c\u4ee3\u7801\u5c06\u5728\u53d1\u8868\u540e\u5f00\u6e90\u3002"}}
{"id": "2508.03714", "pdf": "https://arxiv.org/pdf/2508.03714", "abs": "https://arxiv.org/abs/2508.03714", "authors": ["Yuksel Aydin"], "title": "\"Think First, Verify Always\": Training Humans to Face AI Risks", "categories": ["cs.HC", "cs.AI", "cs.CR", "cs.CY"], "comment": null, "summary": "Artificial intelligence enables unprecedented attacks on human cognition, yet\ncybersecurity remains predominantly device-centric. This paper introduces the\n\"Think First, Verify Always\" (TFVA) protocol, which repositions humans as\n'Firewall Zero', the first line of defense against AI-enabled threats. The\nprotocol is grounded in five operational principles: Awareness, Integrity,\nJudgment, Ethical Responsibility, and Transparency (AIJET). A randomized\ncontrolled trial (n=151) demonstrated that a minimal 3-minute intervention\nproduced statistically significant improvements in cognitive security task\nperformance, with participants showing an absolute +7.87% gains compared to\ncontrols. These results suggest that brief, principles-based training can\nrapidly enhance human resilience against AI-driven cognitive manipulation. We\nrecommend that GenAI platforms embed \"Think First, Verify Always\" as a standard\nprompt, replacing passive warnings with actionable protocols to enhance\ntrustworthy and ethical AI use. By bridging the gap between technical\ncybersecurity and human factors, the TFVA protocol establishes human-empowered\nsecurity as a vital component of trustworthy AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u201cThink First, Verify Always\u201d\uff08TFVA\uff09\u534f\u8bae\uff0c\u5c06\u4eba\u7c7b\u4f5c\u4e3a\u62b5\u5fa1AI\u5a01\u80c1\u7684\u7b2c\u4e00\u9053\u9632\u7ebf\uff0c\u57fa\u4e8e\u4e94\u9879\u539f\u5219\uff08AIJET\uff09\u63d0\u5347\u8ba4\u77e5\u5b89\u5168\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u77ed\u77ed3\u5206\u949f\u7684\u5e72\u9884\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u8868\u73b0\u3002\u5efa\u8bae\u5c06TFVA\u5d4c\u5165GenAI\u5e73\u53f0\uff0c\u53d6\u4ee3\u88ab\u52a8\u8b66\u544a\u3002", "motivation": "\u5f53\u524d\u7f51\u7edc\u5b89\u5168\u4ee5\u8bbe\u5907\u4e3a\u4e2d\u5fc3\uff0c\u5ffd\u7565\u4e86AI\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u7684\u653b\u51fb\u5a01\u80c1\uff0c\u4e9f\u9700\u5c06\u4eba\u7c7b\u4f5c\u4e3a\u9632\u5fa1\u7684\u6838\u5fc3\u3002", "method": "\u63d0\u51faTFVA\u534f\u8bae\uff0c\u57fa\u4e8e\u4e94\u9879\u539f\u5219\uff08AIJET\uff09\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff08n=151\uff09\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "3\u5206\u949f\u5e72\u9884\u663e\u8457\u63d0\u5347\u8ba4\u77e5\u5b89\u5168\u4efb\u52a1\u8868\u73b0\uff0c\u7edd\u5bf9\u589e\u76ca7.87%\u3002", "conclusion": "TFVA\u534f\u8bae\u53ef\u5feb\u901f\u589e\u5f3a\u4eba\u7c7b\u5bf9AI\u9a71\u52a8\u7684\u8ba4\u77e5\u64cd\u7eb5\u7684\u62b5\u5fa1\u80fd\u529b\uff0c\u5efa\u8bae\u5c06\u5176\u4f5c\u4e3aGenAI\u7684\u6807\u51c6\u63d0\u793a\uff0c\u5f25\u5408\u6280\u672f\u4e0e\u4eba\u7c7b\u56e0\u7d20\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2508.04295", "pdf": "https://arxiv.org/pdf/2508.04295", "abs": "https://arxiv.org/abs/2508.04295", "authors": ["Chaofan Wang", "Tingrui Yu", "Jie Wang", "Dong Chen", "Wenrui Zhang", "Yuling Shi", "Xiaodong Gu", "Beijun Shen"], "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation", "categories": ["cs.SE"], "comment": null, "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.", "AI": {"tldr": "EvoC2Rust\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u6574\u4e2aC\u9879\u76ee\u8f6c\u6362\u4e3a\u7b49\u6548\u7684Rust\u9879\u76ee\uff0c\u901a\u8fc7\u9aa8\u67b6\u5f15\u5bfc\u7684\u7ffb\u8bd1\u7b56\u7565\u89e3\u51b3\u89c4\u5219\u548cLLM\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8C\u5230Rust\u8f6c\u6362\u7684\u5b89\u5168\u6027\u548c\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u7a0b\u5e8f\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8fdb\u5316\u7b56\u7565\uff1a\u6a21\u5757\u5206\u89e3\u548c\u9aa8\u67b6\u751f\u6210\u3001\u9010\u6b65\u51fd\u6570\u7ffb\u8bd1\u3001\u9519\u8bef\u4fee\u590d\u3002", "result": "\u5728\u8bed\u6cd5\u548c\u8bed\u4e49\u51c6\u786e\u6027\u4e0a\u5206\u522b\u63d0\u534717.24%\u548c14.32%\uff0c\u4ee3\u7801\u5b89\u5168\u6027\u63d0\u9ad896.79%\u3002", "conclusion": "EvoC2Rust\u5728\u9879\u76ee\u7ea7C\u5230Rust\u8f6c\u6362\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4ee3\u7801\u5e93\u3002"}}
{"id": "2508.04150", "pdf": "https://arxiv.org/pdf/2508.04150", "abs": "https://arxiv.org/abs/2508.04150", "authors": ["Ilias Chrysovergis", "Alexandros-Apostolos A. Boulogeorgos", "Theodoros A. Tsiftsis", "Dusit Niyato"], "title": "Metaverse Framework for Wireless Systems Management", "categories": ["cs.NI"], "comment": "9 pages, 5 figures, 1 algorithm", "summary": "This article introduces a comprehensive metaverse framework, which is\ndesigned for the simulation, emulation, and interaction with wireless systems.\nThe proposed framework integrates core metaverse technologies such as extended\nreality (XR), digital twins (DTs), artificial intelligence (AI), internet of\nthings (IoT), blockchain, and advanced 6G networking solutions to create a\ndynamic, immersive platform for both system development and management. By\nleveraging XR, users can visualize and engage with complex systems, while DTs\nenable real-time monitoring and optimization. AI generates the\nthree-dimensional (3D) content, enhances decision-making and system\nperformance, whereas IoT devices provide real-time sensor data for boosting the\nsimulation accuracy. Additionally, blockchain ensures secure, decentralized\ninteractions, and 5G/6G networks offer the necessary infrastructure for\nseamless, low-latency communication. This framework serves as a robust tool for\nexploring, developing, and optimizing wireless systems, aiming to provide\nvaluable insights into the future of networked environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5143\u5b87\u5b99\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u7ebf\u7cfb\u7edf\u7684\u6a21\u62df\u3001\u4eff\u771f\u548c\u4ea4\u4e92\uff0c\u6574\u5408\u4e86XR\u3001DT\u3001AI\u3001IoT\u3001\u533a\u5757\u94fe\u548c6G\u6280\u672f\u3002", "motivation": "\u76ee\u6807\u662f\u521b\u5efa\u4e00\u4e2a\u52a8\u6001\u3001\u6c89\u6d78\u5f0f\u7684\u5e73\u53f0\uff0c\u4ee5\u652f\u6301\u65e0\u7ebf\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u7ba1\u7406\uff0c\u5e76\u4e3a\u672a\u6765\u7f51\u7edc\u73af\u5883\u63d0\u4f9b\u6d1e\u5bdf\u3002", "method": "\u901a\u8fc7XR\u5b9e\u73b0\u53ef\u89c6\u5316\u4e0e\u4ea4\u4e92\uff0cDT\u7528\u4e8e\u5b9e\u65f6\u76d1\u63a7\u4f18\u5316\uff0cAI\u751f\u62103D\u5185\u5bb9\u5e76\u63d0\u5347\u51b3\u7b56\u80fd\u529b\uff0cIoT\u63d0\u4f9b\u5b9e\u65f6\u6570\u636e\uff0c\u533a\u5757\u94fe\u786e\u4fdd\u5b89\u5168\u6027\u3002", "result": "\u8be5\u6846\u67b6\u4e3a\u65e0\u7ebf\u7cfb\u7edf\u7684\u63a2\u7d22\u3001\u5f00\u53d1\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u672a\u6765\u7f51\u7edc\u7684\u53d1\u5c55\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5143\u5b87\u5b99\u6280\u672f\u5728\u65e0\u7ebf\u7cfb\u7edf\u9886\u57df\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.04171", "pdf": "https://arxiv.org/pdf/2508.04171", "abs": "https://arxiv.org/abs/2508.04171", "authors": ["Daigo Honda", "Yuta Nishiyama", "Junya Ishikawa", "Kenichi Matsuzaki", "Satoshi Miyata", "Tadahiro Chujo", "Yasuhisa Yamamoto", "Masahiko Kiminami", "Taro Kato", "Jun Towada", "Naoki Yoshioka", "Naoto Aoki", "Nobuyasu Ito"], "title": "Advantages of Co-locating Quantum-HPC Platforms: A Survey for Near-Future Industrial Applications", "categories": ["quant-ph", "cs.DC", "cs.ET"], "comment": "20 pages, 3 figures", "summary": "We conducted a systematic survey of emerging quantum-HPC platforms, which\nintegrate quantum computers and High-Performance Computing (HPC) systems\nthrough co-location. Currently, it remains unclear whether such platforms\nprovide tangible benefits for near-future industrial applications. To address\nthis, we examined the impact of co-location on latency reduction, bandwidth\nenhancement, and advanced job scheduling. Additionally, we assessed how\nHPC-level capabilities could enhance hybrid algorithm performance, support\nlarge-scale error mitigation, and facilitate complex quantum circuit\npartitioning and optimization. Our findings demonstrate that co-locating\nquantum and HPC systems can yield measurable improvements in overall hybrid job\nthroughput. We also observe that large-scale real-world problems can require\nHPC-level computational resources for executing hybrid algorithms.", "AI": {"tldr": "\u672c\u7814\u7a76\u5ba1\u67e5\u4e86\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u7cfb\u7edf\u5171\u7f6e\u5e73\u53f0\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u5bf9\u6df7\u5408\u4f5c\u4e1a\u541e\u5410\u91cf\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e14\u9700\u8981HPC\u8d44\u6e90\u652f\u6301\u5927\u89c4\u6a21\u5b9e\u9645\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u63a2\u8ba8\u91cf\u5b50-HPC\u5e73\u53f0\u662f\u5426\u80fd\u4e3a\u5de5\u4e1a\u5e94\u7528\u5e26\u6765\u5b9e\u9645\u6548\u76ca\u3002", "method": "\u7cfb\u7edf\u8c03\u67e5\u4e86\u5171\u7f6e\u5bf9\u5ef6\u8fdf\u3001\u5e26\u5bbd\u3001\u4f5c\u4e1a\u8c03\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u4e86HPC\u80fd\u529b\u5bf9\u6df7\u5408\u7b97\u6cd5\u6027\u80fd\u548c\u5927\u89c4\u6a21\u91cf\u5b50\u7535\u8def\u4f18\u5316\u7684\u652f\u6301\u3002", "result": "\u5171\u7f6e\u91cf\u5b50\u4e0eHPC\u7cfb\u7edf\u53ef\u63d0\u9ad8\u6df7\u5408\u4f5c\u4e1a\u541e\u5410\u91cf\uff0c\u4e14\u5927\u89c4\u6a21\u95ee\u9898\u9700HPC\u8d44\u6e90\u652f\u6301\u3002", "conclusion": "\u91cf\u5b50-HPC\u5171\u7f6e\u5e73\u53f0\u5bf9\u63d0\u5347\u6df7\u5408\u8ba1\u7b97\u6548\u7387\u5177\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u672a\u6765\u5de5\u4e1a\u5e94\u7528\u6f5c\u529b\u5de8\u5927\u3002"}}
{"id": "2508.04000", "pdf": "https://arxiv.org/pdf/2508.04000", "abs": "https://arxiv.org/abs/2508.04000", "authors": ["Tayyaba Noreen", "Qiufen Xia", "Muhammad Zeeshan Haider"], "title": "Advanced DAG-Based Ranking (ADR) Protocol for Blockchain Scalability", "categories": ["cs.DC", "cs.CR", "cs.DB"], "comment": null, "summary": "In the past decade, blockchain has emerged as a promising solution for\nbuilding secure distributed ledgers and has attracted significant attention.\nHowever, current blockchain systems suffer from limited throughput, poor\nscalability, and high latency. Due to limitations in consensus mechanisms,\nespecially in managing node identities, blockchain is often considered\nunsuitable for applications such as the Internet of Things (IoT). This paper\nproposes the Advanced DAG-based Ranking (ADR) protocol to enhance blockchain\nscalability and throughput. ADR employs a directed acyclic graph (DAG)\nstructure where nodes are positioned based on their rankings. Unlike\ntraditional chains, ADR allows honest nodes to write blocks and verify\ntransactions using a DAG-based topology. The protocol follows a three-step\napproach to secure the network against double-spending and enhance performance.\nFirst, it verifies nodes using their public and private keys before granting\nentry. Second, it builds an advanced DAG ledger enabling block production and\ntransaction validation. Third, a ranking algorithm filters out malicious nodes,\nranks the remaining nodes based on performance, and arranges them\ntopologically. This process increases throughput and ensures robust\nscalability. We evaluated ADR on Amazon EC2 clusters with over 100 nodes,\nincluding scenarios with injected malicious nodes. Simulation results\ndemonstrate that ADR significantly improves transaction throughput and network\nliveness compared to existing DAG-based blockchains such as IOTA and ByteBall,\nmaking it well-suited for IoT applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDAG\u7684ADR\u534f\u8bae\uff0c\u7528\u4e8e\u89e3\u51b3\u533a\u5757\u94fe\u7684\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u7279\u522b\u9002\u5408\u7269\u8054\u7f51\u5e94\u7528\u3002", "motivation": "\u533a\u5757\u94fe\u5f53\u524d\u7684\u541e\u5410\u91cf\u4f4e\u3001\u53ef\u6269\u5c55\u6027\u5dee\u548c\u9ad8\u5ef6\u8fdf\u95ee\u9898\u9650\u5236\u4e86\u5176\u5728\u7269\u8054\u7f51\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6539\u8fdb\u7684\u534f\u8bae\u3002", "method": "ADR\u534f\u8bae\u91c7\u7528DAG\u7ed3\u6784\uff0c\u901a\u8fc7\u8282\u70b9\u6392\u540d\u3001\u4e09\u6b65\u9aa4\u9a8c\u8bc1\uff08\u8282\u70b9\u8ba4\u8bc1\u3001DAG\u8d26\u672c\u6784\u5efa\u548c\u6076\u610f\u8282\u70b9\u8fc7\u6ee4\u6392\u540d\uff09\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728100\u591a\u4e2a\u8282\u70b9\u7684\u6a21\u62df\u73af\u5883\u4e2d\uff0cADR\u76f8\u6bd4\u73b0\u6709DAG\u533a\u5757\u94fe\uff08\u5982IOTA\u548cByteBall\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u548c\u7f51\u7edc\u6d3b\u8dc3\u5ea6\u3002", "conclusion": "ADR\u534f\u8bae\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u7684\u7269\u8054\u7f51\u5e94\u7528\u3002"}}
{"id": "2508.04516", "pdf": "https://arxiv.org/pdf/2508.04516", "abs": "https://arxiv.org/abs/2508.04516", "authors": ["Ishraq Tashdid", "Dewan Saiham", "Nafisa Anjum", "Tasnuva Farheen", "Sazadur Rahman"], "title": "ECOLogic: Enabling Circular, Obfuscated, and Adaptive Logic via eFPGA-Augmented SoCs", "categories": ["cs.AR", "cs.ET", "C.3; B.6; B.7.1"], "comment": "10 pages, 7 figures. Extended version of accepted short paper at IEEE\n  International Conference on Computer Design (ICCD) 2025, Dallas, TX, USA", "summary": "Traditional hardware platforms - ASICs and FPGAs - offer competing trade-offs\namong performance, flexibility, and sustainability. ASICs provide high\nefficiency but are inflexible post-fabrication, require costly re-spins for\nupdates, and expose IPs to piracy risks. FPGAs offer reconfigurability and\nreuse, yet suffer from substantial area, power, and performance overheads,\nresulting in higher carbon footprints. We present ECOLogic, a hybrid design\nparadigm that embeds lightweight eFPGA fabric within ASICs to enable secure,\nupdatable, and resource-aware computation. Central to this architecture is\nECOScore, a quantitative scoring framework that evaluates IPs based on\nadaptability, piracy threat, performance tolerance, and resource fit to guide\nRTL partitioning. Evaluated across six diverse SoC modules, ECOLogic retains an\naverage of 90 percent ASIC-level performance (up to 2 GHz), achieves 9.8 ns\ntiming slack (versus 5.1 ns in FPGA), and reduces power by 480 times on\naverage. Moreover, sustainability analysis shows a 99.7 percent reduction in\ndeployment carbon footprint and 300 to 500 times lower emissions relative to\nFPGA-only implementations. These results position ECOLogic as a\nhigh-performance, secure, and environmentally sustainable solution for\nnext-generation reconfigurable systems.", "AI": {"tldr": "ECOLogic\u662f\u4e00\u79cd\u6df7\u5408\u8bbe\u8ba1\u8303\u5f0f\uff0c\u901a\u8fc7\u5728ASIC\u4e2d\u5d4c\u5165\u8f7b\u91cf\u7ea7eFPGA\u7ed3\u6784\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u3001\u53ef\u66f4\u65b0\u548c\u73af\u4fdd\u7684\u8ba1\u7b97\u3002ECOScore\u6846\u67b6\u6307\u5bfcRTL\u5206\u533a\uff0c\u5728\u4fdd\u630190% ASIC\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u529f\u8017\u548c\u78b3\u6392\u653e\u3002", "motivation": "\u4f20\u7edf\u786c\u4ef6\u5e73\u53f0\uff08ASIC\u548cFPGA\uff09\u5728\u6027\u80fd\u3001\u7075\u6d3b\u6027\u548c\u53ef\u6301\u7eed\u6027\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\u3002ASIC\u9ad8\u6548\u4f46\u4e0d\u7075\u6d3b\uff0cFPGA\u53ef\u91cd\u6784\u4f46\u5b58\u5728\u9ad8\u529f\u8017\u548c\u6027\u80fd\u5f00\u9500\u3002ECOLogic\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u65e2\u9ad8\u6027\u80fd\u53c8\u53ef\u66f4\u65b0\u4e14\u73af\u4fdd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faECOLogic\u6df7\u5408\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5d4c\u5165eFPGA\u7ed3\u6784\u4e8eASIC\u4e2d\uff0c\u7ed3\u5408ECOScore\u6846\u67b6\u8bc4\u4f30IP\u9002\u5e94\u6027\u3001\u5b89\u5168\u6027\u3001\u6027\u80fd\u5bb9\u5fcd\u5ea6\u548c\u8d44\u6e90\u9002\u914d\u6027\uff0c\u6307\u5bfcRTL\u5206\u533a\u3002", "result": "\u5728\u516d\u79cdSoC\u6a21\u5757\u4e2d\uff0cECOLogic\u5e73\u5747\u4fdd\u755990% ASIC\u6027\u80fd\uff08\u6700\u9ad82 GHz\uff09\uff0c\u65f6\u5e8f\u88d5\u5ea69.8 ns\uff08FPGA\u4e3a5.1 ns\uff09\uff0c\u529f\u8017\u5e73\u5747\u964d\u4f4e480\u500d\u3002\u78b3\u6392\u653e\u6bd4FPGA\u5b9e\u73b0\u964d\u4f4e300\u81f3500\u500d\u3002", "conclusion": "ECOLogic\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u3001\u5b89\u5168\u4e14\u73af\u5883\u53ef\u6301\u7eed\u7684\u4e0b\u4e00\u4ee3\u53ef\u91cd\u6784\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04228", "pdf": "https://arxiv.org/pdf/2508.04228", "abs": "https://arxiv.org/abs/2508.04228", "authors": ["Kangrui Cen", "Baixuan Zhao", "Yi Xin", "Siqi Luo", "Guangtao Zhai", "Xiaohong Liu"], "title": "LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Project webpage: https://kr-panghu.github.io/LayerT2V/", "summary": "Controlling object motion trajectories in Text-to-Video (T2V) generation is a\nchallenging and relatively under-explored area, particularly in scenarios\ninvolving multiple moving objects. Most community models and datasets in the\nT2V domain are designed for single-object motion, limiting the performance of\ncurrent generative models in multi-object tasks. Additionally, existing motion\ncontrol methods in T2V either lack support for multi-object motion scenes or\nexperience severe performance degradation when object trajectories intersect,\nprimarily due to the semantic conflicts in colliding regions. To address these\nlimitations, we introduce LayerT2V, the first approach for generating video by\ncompositing background and foreground objects layer by layer. This layered\ngeneration enables flexible integration of multiple independent elements within\na video, positioning each element on a distinct \"layer\" and thus facilitating\ncoherent multi-object synthesis while enhancing control over the generation\nprocess. Extensive experiments demonstrate the superiority of LayerT2V in\ngenerating complex multi-object scenarios, showcasing 1.4x and 4.5x\nimprovements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods.\nProject page and code are available at https://kr-panghu.github.io/LayerT2V/ .", "AI": {"tldr": "LayerT2V\u901a\u8fc7\u5206\u5c42\u751f\u6210\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u591a\u5bf9\u8c61\u8fd0\u52a8\u8f68\u8ff9\u63a7\u5236\u7684\u6311\u6218\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dT2V\u751f\u6210\u6a21\u578b\u5728\u591a\u5bf9\u8c61\u8fd0\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u8c61\u8f68\u8ff9\u4ea4\u53c9\u65f6\u8bed\u4e49\u51b2\u7a81\u95ee\u9898\u4e25\u91cd\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86LayerT2V\uff0c\u901a\u8fc7\u5206\u5c42\u751f\u6210\u80cc\u666f\u548c\u524d\u666f\u5bf9\u8c61\uff0c\u72ec\u7acb\u63a7\u5236\u5404\u5bf9\u8c61\u5c42\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u591a\u5bf9\u8c61\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLayerT2V\u5728mIoU\u548cAP50\u6307\u6807\u4e0a\u5206\u522b\u6bd4SOTA\u65b9\u6cd5\u63d0\u5347\u4e861.4\u500d\u548c4.5\u500d\u3002", "conclusion": "LayerT2V\u5728\u591a\u5bf9\u8c61\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u63a7\u5236\u6027\u548c\u5408\u6210\u80fd\u529b\u3002"}}
{"id": "2508.03717", "pdf": "https://arxiv.org/pdf/2508.03717", "abs": "https://arxiv.org/abs/2508.03717", "authors": ["Muhammad Akmal Bin Mohammed Zaffir", "Daisuke Sakai", "Yuki Sato", "Takahiro Wada"], "title": "Relationship between Perceived Maneuverability and Involuntary Eye Movements under Systematically Varied Time Constants of Ride-on Machinery", "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Studies suggest that involuntary eye movements exhibit greater stability\nduring active motion compared to passive motion, and this effect may also apply\nto the operation of ride-on machinery. Moreover, a study suggested that\nexperimentally manipulating the sense of agency (SoA) by introducing delays may\ninfluence the stability of involuntary eye movements. Although a preliminary\ninvestigation examined involuntary eye movements and perceived maneuverability\nunder two distinct machine dynamics with preserved SoA, it remains unclear how\nsystematic variations in motion dynamics influence these factors. Therefore,\nthe purpose of the present research was to investigate whether systematic\nvariations in the dynamic properties of a ride-on machine, where the perceived\nmaneuverability is modulated, influence the accuracy of involuntary eye\nmovements in human operators. Participants rode a yaw-rotational platform whose\ntime constant from joystick input to motor torque of a rotational machine was\nsystematically manipulated. During the operation, eye movements were recorded\nwhile participants fixated on a visual target. After each condition,\nparticipants provided subjective ratings of maneuverability and cognitive load.\nAs the platform's time constant increased, the perceived maneuverability scores\ndecreased while the cognitive loads increased. Concurrently, involuntary eye\nmovement accuracy decreased. Moderate to weak positive correlations emerged\nbetween the perceived maneuverability scores and the eye movement gain and\naccuracy, while a weak negative correlation was found with cognitive load.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4e3b\u52a8\u8fd0\u52a8\u4e2d\u7684\u975e\u81ea\u613f\u773c\u7403\u8fd0\u52a8\u6bd4\u88ab\u52a8\u8fd0\u52a8\u66f4\u7a33\u5b9a\uff0c\u4e14\u64cd\u7eb5\u611f\uff08SoA\uff09\u53ef\u80fd\u5f71\u54cd\u8fd9\u79cd\u7a33\u5b9a\u6027\u3002\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u8c03\u8282\u9a91\u4e58\u673a\u5668\u7684\u52a8\u6001\u7279\u6027\uff0c\u63a2\u8ba8\u4e86\u5176\u5bf9\u975e\u81ea\u613f\u773c\u7403\u8fd0\u52a8\u51c6\u786e\u6027\u548c\u611f\u77e5\u64cd\u63a7\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u9a91\u4e58\u673a\u5668\u52a8\u6001\u7279\u6027\u7684\u7cfb\u7edf\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u975e\u81ea\u613f\u773c\u7403\u8fd0\u52a8\u7684\u51c6\u786e\u6027\u53ca\u611f\u77e5\u64cd\u63a7\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u53c2\u4e0e\u8005\u64cd\u4f5c\u4e00\u53f0\u65f6\u95f4\u5e38\u6570\u53ef\u8c03\u7684\u504f\u8f6c\u5e73\u53f0\uff0c\u8bb0\u5f55\u5176\u773c\u7403\u8fd0\u52a8\u5e76\u6536\u96c6\u611f\u77e5\u64cd\u63a7\u6027\u548c\u8ba4\u77e5\u8d1f\u8377\u7684\u4e3b\u89c2\u8bc4\u5206\u3002", "result": "\u65f6\u95f4\u5e38\u6570\u589e\u52a0\u5bfc\u81f4\u611f\u77e5\u64cd\u63a7\u6027\u4e0b\u964d\u3001\u8ba4\u77e5\u8d1f\u8377\u4e0a\u5347\uff0c\u540c\u65f6\u975e\u81ea\u613f\u773c\u7403\u8fd0\u52a8\u51c6\u786e\u6027\u964d\u4f4e\uff1b\u64cd\u63a7\u6027\u4e0e\u773c\u7403\u8fd0\u52a8\u8868\u73b0\u5448\u6b63\u76f8\u5173\uff0c\u4e0e\u8ba4\u77e5\u8d1f\u8377\u5448\u8d1f\u76f8\u5173\u3002", "conclusion": "\u9a91\u4e58\u673a\u5668\u7684\u52a8\u6001\u7279\u6027\u663e\u8457\u5f71\u54cd\u975e\u81ea\u613f\u773c\u7403\u8fd0\u52a8\u7684\u51c6\u786e\u6027\uff0c\u611f\u77e5\u64cd\u63a7\u6027\u548c\u8ba4\u77e5\u8d1f\u8377\u662f\u5173\u952e\u4e2d\u4ecb\u56e0\u7d20\u3002"}}
{"id": "2508.04352", "pdf": "https://arxiv.org/pdf/2508.04352", "abs": "https://arxiv.org/abs/2508.04352", "authors": ["Dragana Sunaric", "Charlotte Verbruggen", "Dominik Bork"], "title": "Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models", "categories": ["cs.SE"], "comment": null, "summary": "As organizations prepare for the end-of-life of Camunda 7, manual migration\nremains complex due to fundamental differences between the two platforms. We\npresent Vanilla-Converter, a command-line tool that facilitates the migration\nof BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the\ntransformation process, supports a wide range of BPMN elements, and produces a\ntransformed model and a detailed transformation log indicating automatic\nchanges and remaining manual conversion tasks. The tool's effectiveness is\ndemonstrated through three case studies with real industrially used Camunda 7\nmodels, confirming its ability to convert these models into valid and\nexecutable Camunda 8 models.", "AI": {"tldr": "Vanilla-Converter\u662f\u4e00\u79cd\u547d\u4ee4\u884c\u5de5\u5177\uff0c\u53ef\u81ea\u52a8\u5316\u5c06Camunda 7\u7684BPMN\u6a21\u578b\u8fc1\u79fb\u81f3Camunda 8\uff0c\u652f\u6301\u5e7f\u6cdb\u7684BPMN\u5143\u7d20\u5e76\u751f\u6210\u8f6c\u6362\u65e5\u5fd7\u3002", "motivation": "\u7531\u4e8eCamunda 7\u548c8\u5e73\u53f0\u95f4\u7684\u6839\u672c\u5dee\u5f02\uff0c\u624b\u52a8\u8fc1\u79fb\u590d\u6742\u4e14\u8017\u65f6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5f00\u53d1\u4e86Vanilla-Converter\u5de5\u5177\u3002", "method": "\u5de5\u5177\u81ea\u52a8\u5316\u8f6c\u6362BPMN\u6a21\u578b\uff0c\u652f\u6301\u591a\u79cd\u5143\u7d20\uff0c\u5e76\u751f\u6210\u8be6\u7ec6\u65e5\u5fd7\u8bb0\u5f55\u81ea\u52a8\u66f4\u6539\u548c\u5269\u4f59\u624b\u52a8\u4efb\u52a1\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9645\u5de5\u4e1a\u6848\u4f8b\u9a8c\u8bc1\uff0c\u5de5\u5177\u6210\u529f\u5c06Camunda 7\u6a21\u578b\u8f6c\u6362\u4e3a\u6709\u6548\u4e14\u53ef\u6267\u884c\u7684Camunda 8\u6a21\u578b\u3002", "conclusion": "Vanilla-Converter\u663e\u8457\u7b80\u5316\u4e86Camunda 7\u52308\u7684\u8fc1\u79fb\u8fc7\u7a0b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.04317", "pdf": "https://arxiv.org/pdf/2508.04317", "abs": "https://arxiv.org/abs/2508.04317", "authors": ["Joshua Smailes", "Filip Futera", "Sebastian K\u00f6hler", "Simon Birnbach", "Martin Strohmeier", "Ivan Martinovic"], "title": "DSNS: The Deep Space Network Simulator", "categories": ["cs.NI"], "comment": "12 pages, 8 figures, 3 tables", "summary": "Simulation tools are commonly used in the development and testing of new\nprotocols or new networks. However, as satellite networks start to grow to\nencompass thousands of nodes, and as companies and space agencies begin to\nrealize the interplanetary internet, existing satellite and network simulation\ntools have become impractical for use in this context.\n  We therefore present the Deep Space Network Simulator (DSNS): a new network\nsimulator with a focus on large-scale satellite networks. We demonstrate its\nimproved capabilities compared to existing offerings, showcase its flexibility\nand extensibility through an implementation of existing protocols and the DTN\nsimulation reference scenarios recommended by CCSDS, and evaluate its\nscalability, showing that it exceeds existing tools while providing better\nfidelity.\n  DSNS provides concrete usefulness to both standards bodies and satellite\noperators, enabling fast iteration on protocol development and testing of\nparameters under highly realistic conditions. By removing roadblocks to\nresearch and innovation, we can accelerate the development of upcoming\nsatellite networks and ensure that their communication is both fast and secure.", "AI": {"tldr": "DSNS\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u89c4\u6a21\u536b\u661f\u7f51\u7edc\u7684\u65b0\u578b\u7f51\u7edc\u6a21\u62df\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u62df\u5de5\u5177\u65e0\u6cd5\u5e94\u5bf9\u6570\u5343\u8282\u70b9\u536b\u661f\u7f51\u7edc\u7684\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740\u536b\u661f\u7f51\u7edc\u89c4\u6a21\u6269\u5927\u81f3\u6570\u5343\u8282\u70b9\u4ee5\u53ca\u661f\u9645\u4e92\u8054\u7f51\u6982\u5ff5\u7684\u5174\u8d77\uff0c\u73b0\u6709\u536b\u661f\u548c\u7f51\u7edc\u6a21\u62df\u5de5\u5177\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4e86Deep Space Network Simulator (DSNS)\uff0c\u5e76\u901a\u8fc7\u5b9e\u73b0\u73b0\u6709\u534f\u8bae\u548cCCSDS\u63a8\u8350\u7684DTN\u6a21\u62df\u53c2\u8003\u573a\u666f\uff0c\u5c55\u793a\u5176\u7075\u6d3b\u6027\u548c\u6269\u5c55\u6027\u3002", "result": "DSNS\u5728\u53ef\u6269\u5c55\u6027\u548c\u6a21\u62df\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u6807\u51c6\u5236\u5b9a\u673a\u6784\u548c\u536b\u661f\u8fd0\u8425\u5546\u3002", "conclusion": "DSNS\u901a\u8fc7\u53bb\u9664\u7814\u7a76\u548c\u521b\u65b0\u7684\u969c\u788d\uff0c\u52a0\u901f\u4e86\u672a\u6765\u536b\u661f\u7f51\u7edc\u7684\u5f00\u53d1\uff0c\u786e\u4fdd\u4e86\u901a\u4fe1\u7684\u9ad8\u901f\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2508.04013", "pdf": "https://arxiv.org/pdf/2508.04013", "abs": "https://arxiv.org/abs/2508.04013", "authors": ["Sameh Abdulah", "Mary Lai O. Salvana", "Ying Sun", "David E. Keyes", "Marc G. Genton"], "title": "High-Performance Statistical Computing (HPSC): Challenges, Opportunities, and Future Directions", "categories": ["cs.DC"], "comment": null, "summary": "We recognize the emergence of a statistical computing community focused on\nworking with large computing platforms and producing software and applications\nthat exemplify high-performance statistical computing (HPSC). The statistical\ncomputing (SC) community develops software that is widely used across\ndisciplines. However, it remains largely absent from the high-performance\ncomputing (HPC) landscape, particularly on platforms such as those featured on\nthe Top500 or Green500 lists. Many disciplines already participate in HPC,\nmostly centered around simulation science, although data-focused efforts under\nthe artificial intelligence (AI) label are gaining popularity. Bridging this\ngap requires both community adaptation and technical innovation to align\nstatistical methods with modern HPC technologies. We can accelerate progress in\nfast and scalable statistical applications by building strong connections\nbetween the SC and HPC communities. We present a brief history of SC, a vision\nfor how its strengths can contribute to statistical science in the HPC\nenvironment (such as HPSC), the challenges that remain, and the opportunities\ncurrently available, culminating in a possible roadmap toward a thriving HPSC\ncommunity.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7edf\u8ba1\u8ba1\u7b97\uff08SC\uff09\u793e\u533a\u5982\u4f55\u878d\u5165\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u73af\u5883\uff0c\u63d0\u51fa\u5efa\u7acb\u9ad8\u6027\u80fd\u7edf\u8ba1\u8ba1\u7b97\uff08HPSC\uff09\u793e\u533a\u7684\u613f\u666f\u3002", "motivation": "\u7edf\u8ba1\u8ba1\u7b97\u793e\u533a\u867d\u5f00\u53d1\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u8f6f\u4ef6\uff0c\u4f46\u5728HPC\u9886\u57df\uff08\u5982Top500\u6216Green500\u5e73\u53f0\uff09\u4e2d\u53c2\u4e0e\u8f83\u5c11\uff0c\u5e0c\u671b\u901a\u8fc7\u4e0eHPC\u793e\u533a\u5408\u4f5c\u63a8\u52a8\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u7edf\u8ba1\u5e94\u7528\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u68b3\u7406SC\u7684\u5386\u53f2\uff0c\u5206\u6790\u5176\u5728HPC\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3001\u73b0\u6709\u6311\u6218\u548c\u673a\u9047\uff0c\u63d0\u51fa\u5efa\u8bbeHPSC\u793e\u533a\u7684\u8def\u7ebf\u56fe\u3002", "result": "\u5c55\u793a\u4e86SC\u793e\u533a\u5982\u4f55\u901a\u8fc7\u6280\u672f\u548c\u793e\u533a\u521b\u65b0\uff0c\u5c06\u5176\u4f18\u52bf\u878d\u5165HPC\u73af\u5883\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u7edf\u8ba1\u8ba1\u7b97\u7684\u76ee\u6807\u3002", "conclusion": "\u5efa\u7acbHPSC\u793e\u533a\u662f\u53ef\u80fd\u7684\uff0c\u4f46\u9700SC\u4e0eHPC\u793e\u533a\u7684\u7d27\u5bc6\u5408\u4f5c\uff0c\u514b\u670d\u6280\u672f\u548c\u6587\u5316\u969c\u788d\uff0c\u4ee5\u5b9e\u73b0\u7edf\u8ba1\u8ba1\u7b97\u5728\u9ad8\u6027\u80fd\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2508.04609", "pdf": "https://arxiv.org/pdf/2508.04609", "abs": "https://arxiv.org/abs/2508.04609", "authors": ["Osama Abdelaleim", "Arun Prakash", "Ayhan Irfanoglu", "Veljko Milutinovic"], "title": "Near instantaneous O(1) Analog Solver Circuit for Linear Symmetric Positive-Definite Systems", "categories": ["cs.AR"], "comment": null, "summary": "Accelerating the solution of linear systems of equations is critical due to\ntheir central role in numerous applications, such as scientific simulations,\ndata analytics, and machine learning. This paper presents a general-purpose\nanalog direct solver circuit designed to accelerate the solution of positive\ndefinite symmetric linear systems of equations. The proposed design leverages\nnon-inverting operational amplifier configurations to create a negative\nresistance circuit, effectively modeling any symmetric system. The paper\ndetails the principles behind the design, optimizations of the system\narchitecture, and numerical results that demonstrate the robustness of the\ndesign. The findings reveal that the proposed system solves diagonally dominant\nsymmetric matrices with O(1) complexity, achieving the theoretical maximum\nspeed as the circuit relies solely on resistors. For non-diagonally dominant\nsymmetric positive-definite systems, the solution speed depends on matrix\nproperties such as eigenvalues and the maximum off-diagonal term, but remains\nindependent of matrix size.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u6a21\u62df\u76f4\u63a5\u6c42\u89e3\u7535\u8def\uff0c\u7528\u4e8e\u52a0\u901f\u6b63\u5b9a\u5bf9\u79f0\u7ebf\u6027\u65b9\u7a0b\u7ec4\u7684\u6c42\u89e3\uff0c\u5229\u7528\u8d1f\u7535\u963b\u7535\u8def\u5b9e\u73b0O(1)\u590d\u6742\u5ea6\u7684\u9ad8\u6548\u6c42\u89e3\u3002", "motivation": "\u7ebf\u6027\u65b9\u7a0b\u7ec4\u5728\u79d1\u5b66\u8ba1\u7b97\u3001\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u6c42\u89e3\u65b9\u6cd5\u901f\u5ea6\u6709\u9650\uff0c\u4e9f\u9700\u52a0\u901f\u65b9\u6848\u3002", "method": "\u91c7\u7528\u975e\u53cd\u76f8\u8fd0\u7b97\u653e\u5927\u5668\u914d\u7f6e\u8bbe\u8ba1\u8d1f\u7535\u963b\u7535\u8def\uff0c\u6a21\u62df\u5bf9\u79f0\u7cfb\u7edf\uff0c\u5e76\u4f18\u5316\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u8bbe\u8ba1\u80fd\u591f\u4ee5O(1)\u590d\u6742\u5ea6\u6c42\u89e3\u5bf9\u89d2\u5360\u4f18\u5bf9\u79f0\u77e9\u9635\uff0c\u5bf9\u975e\u5bf9\u89d2\u5360\u4f18\u7cfb\u7edf\u7684\u6c42\u89e3\u901f\u5ea6\u4f9d\u8d56\u4e8e\u77e9\u9635\u7279\u6027\u800c\u975e\u89c4\u6a21\u3002", "conclusion": "\u8be5\u7535\u8def\u8bbe\u8ba1\u4e3a\u5bf9\u79f0\u7ebf\u6027\u65b9\u7a0b\u7ec4\u7684\u5feb\u901f\u6c42\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u6a21\u62df\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04247", "pdf": "https://arxiv.org/pdf/2508.04247", "abs": "https://arxiv.org/abs/2508.04247", "authors": ["Huilin Chen", "Miaomiao Cai", "Fan Liu", "Zhiyong Cheng", "Richang Hong", "Meng Wang"], "title": "I$^3$-MRec: Invariant Learning with Information Bottleneck for Incomplete Modality Recommendation", "categories": ["cs.IR", "cs.MM", "H.3.3; H.5.1"], "comment": "ACM Multimedia 2025 Accepted", "summary": "Multimodal recommender systems (MRS) improve recommendation performance by\nintegrating diverse semantic information from multiple modalities. However, the\nassumption of the availability of all modalities rarely holds in practice due\nto missing images, incomplete descriptions, or inconsistent user content. These\nchallenges significantly degrade the robustness and generalization capabilities\nof current models. To address these challenges, we introduce a novel method\ncalled \\textbf{I$^3$-MRec}, which uses \\textbf{I}nvariant learning with\n\\textbf{I}nformation bottleneck principle for \\textbf{I}ncomplete\n\\textbf{M}odality \\textbf{Rec}ommendation. To achieve robust performance in\nmissing modality scenarios, I$^3$-MRec enforces two pivotal properties: (i)\ncross-modal preference invariance, which ensures consistent user preference\nmodeling across varying modality environments, and (ii) compact yet effective\nmodality representation, which filters out task-irrelevant modality information\nwhile maximally preserving essential features relevant to recommendation. By\ntreating each modality as a distinct semantic environment, I$^3$-MRec employs\ninvariant risk minimization (IRM) to learn modality-specific item\nrepresentations. In parallel, a missing-aware fusion module grounded in the\nInformation Bottleneck (IB) principle extracts compact and effective item\nembeddings by suppressing modality noise and preserving core user preference\nsignals. Extensive experiments conducted on three real-world datasets\ndemonstrate that I$^3$-MRec consistently outperforms existing state-of-the-art\nMRS methods across various modality-missing scenarios, highlighting its\neffectiveness and robustness in practical applications. The code and processed\ndatasets are released at https://github.com/HuilinChenJN/I3-MRec.", "AI": {"tldr": "\u63d0\u51fa\u4e86I3-MRec\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u53d8\u5b66\u4e60\u548c\u4fe1\u606f\u74f6\u9888\u539f\u5219\u5904\u7406\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\uff08MRS\uff09\u4f9d\u8d56\u4e8e\u591a\u79cd\u6a21\u6001\u7684\u4fe1\u606f\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6a21\u6001\u7f3a\u5931\uff08\u5982\u56fe\u50cf\u6216\u63cf\u8ff0\u4e0d\u5b8c\u6574\uff09\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "I3-MRec\u7ed3\u5408\u4e0d\u53d8\u5b66\u4e60\u548c\u4fe1\u606f\u74f6\u9888\u539f\u5219\uff0c\u786e\u4fdd\u7528\u6237\u504f\u597d\u7684\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u53d6\u7d27\u51d1\u6709\u6548\u7684\u6a21\u6001\u8868\u793a\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0cI3-MRec\u5728\u591a\u79cd\u6a21\u6001\u7f3a\u5931\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "I3-MRec\u901a\u8fc7\u9c81\u68d2\u7684\u6a21\u6001\u5904\u7406\u548c\u6709\u6548\u7684\u4fe1\u606f\u63d0\u53d6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.03792", "pdf": "https://arxiv.org/pdf/2508.03792", "abs": "https://arxiv.org/abs/2508.03792", "authors": ["Michael D. Ekstrand", "Afsaneh Razi", "Aleksandra Sarcevic", "Maria Soledad Pera", "Robin Burke", "Katherine Landau Wright"], "title": "Recommending With, Not For: Co-Designing Recommender Systems for Social Good", "categories": ["cs.HC", "cs.CY", "cs.IR"], "comment": "Accepted to ACM TORS Special Issue on Recommender Systems for Social\n  Good", "summary": "Recommender systems are usually designed by engineers, researchers,\ndesigners, and other members of development teams. These systems are then\nevaluated based on goals set by the aforementioned teams and other business\nunits of the platforms operating the recommender systems. This design approach\nemphasizes the designers' vision for how the system can best serve the\ninterests of users, providers, businesses, and other stakeholders. Although\ndesigners may be well-informed about user needs through user experience and\nmarket research, they are still the arbiters of the system's design and\nevaluation, with other stakeholders' interests less emphasized in user-centered\ndesign and evaluation. When extended to recommender systems for social good,\nthis approach results in systems that reflect the social objectives as\nenvisioned by the designers and evaluated as the designers understand them.\nInstead, social goals and operationalizations should be developed through\nparticipatory and democratic processes that are accountable to their\nstakeholders. We argue that recommender systems aimed at improving social good\nshould be designed *by* and *with*, not just *for*, the people who will\nexperience their benefits and harms. That is, they should be designed in\ncollaboration with their users, creators, and other stakeholders as full\nco-designers, not only as user study participants.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u63a8\u8350\u7cfb\u7edf\u7684\u8bbe\u8ba1\u5e94\u7531\u7528\u6237\u548c\u5176\u4ed6\u5229\u76ca\u76f8\u5173\u8005\u5171\u540c\u53c2\u4e0e\uff0c\u800c\u4e0d\u53ea\u662f\u7531\u8bbe\u8ba1\u5e08\u51b3\u5b9a\uff0c\u4ee5\u66f4\u597d\u5730\u5b9e\u73b0\u793e\u4f1a\u516c\u76ca\u3002", "motivation": "\u5f53\u524d\u63a8\u8350\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u4e3b\u8981\u7531\u5f00\u53d1\u56e2\u961f\u51b3\u5b9a\uff0c\u5176\u4ed6\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u672a\u5f97\u5230\u5145\u5206\u91cd\u89c6\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u793e\u4f1a\u516c\u76ca\u65f6\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u53c2\u4e0e\u5f0f\u548c\u6c11\u4e3b\u5316\u7684\u8fc7\u7a0b\u6765\u5b9a\u4e49\u793e\u4f1a\u76ee\u6807\u548c\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4f7f\u7528\u6237\u548c\u5176\u4ed6\u5229\u76ca\u76f8\u5173\u8005\u6210\u4e3a\u5171\u540c\u8bbe\u8ba1\u8005\u3002", "result": "\u5f3a\u8c03\u63a8\u8350\u7cfb\u7edf\u5e94\u4e3a\u793e\u4f1a\u516c\u76ca\u670d\u52a1\uff0c\u9700\u4ee5\u66f4\u591a\u5229\u76ca\u76f8\u5173\u8005\u7684\u53c2\u4e0e\u4e3a\u524d\u63d0\u3002", "conclusion": "\u63a8\u8350\u7cfb\u7edf\u7684\u8bbe\u8ba1\u5e94\u7531\u7528\u6237\u548c\u5229\u76ca\u76f8\u5173\u8005\u5171\u540c\u5b8c\u6210\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4e3a\u4ed6\u4eec\u8bbe\u8ba1\uff0c\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u793e\u4f1a\u516c\u76ca\u3002"}}
{"id": "2508.04408", "pdf": "https://arxiv.org/pdf/2508.04408", "abs": "https://arxiv.org/abs/2508.04408", "authors": ["Carlos Andr\u00e9s Ram\u00edrez Cata\u00f1o", "Makoto Itoh"], "title": "Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making", "categories": ["cs.SE", "cs.HC"], "comment": "16 pages, 2 figures, 2 formulas, 12 tables", "summary": "Software defect prediction using code metrics has been extensively researched\nover the past five decades. However, prediction harnessing non-software metrics\nis under-researched. Considering that the root cause of software defects is\noften attributed to human error, human factors theory might offer key\nforecasting metrics for actionable insights. This paper explores automated\nsoftware defect prediction at the method level based on the developers' coding\nhabits. First, we propose a framework for deciding the metrics to conduct\npredictions. Next, we compare the performance of our metrics to that of the\ncode and commit history metrics shown by research to achieve the highest\nperformance to date. Finally, we analyze the prediction importance of each\nmetric. As a result of our analyses of twenty-one critical infrastructure\nlarge-scale open-source software projects, we have presented: (1) a human\nerror-based framework with metrics useful for defect prediction at method\nlevel; (2) models using our proposed metrics achieve better average prediction\nperformance than the state-of-the-art code metrics and history measures; (3)\nthe prediction importance of all metrics distributes differently with each of\nthe novel metrics having better average importance than code and history\nmetrics; (4) the novel metrics dramatically enhance the explainability,\npracticality, and actionability of software defect prediction models,\nsignificantly advancing the field. We present a systematic approach to\nforecasting defect-prone software methods via a human error framework. This\nwork empowers practitioners to act on predictions, empirically demonstrating\nhow developer coding habits contribute to defects in software systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f00\u53d1\u8005\u7f16\u7801\u4e60\u60ef\u7684\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u4ee3\u7801\u6307\u6807\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u5b9e\u7528\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u4ee3\u7801\u548c\u5386\u53f2\u6307\u6807\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5229\u7528\u4eba\u7c7b\u56e0\u7d20\u7406\u8bba\u89e3\u51b3\u8f6f\u4ef6\u7f3a\u9677\u7684\u6839\u6e90\u95ee\u9898\uff0c\u63d0\u51fa\u5f00\u53d1\u8005\u7f16\u7801\u4e60\u60ef\u4f5c\u4e3a\u9884\u6d4b\u6307\u6807\uff0c\u586b\u8865\u975e\u8f6f\u4ef6\u6307\u6807\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bbe\u8ba1\u57fa\u4e8e\u4eba\u7c7b\u9519\u8bef\u7684\u9884\u6d4b\u6307\u6807\u6846\u67b6\uff0c\u5e76\u4e0e\u73b0\u6709\u6700\u4f73\u4ee3\u7801\u548c\u5386\u53f2\u6307\u6807\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u5404\u6307\u6807\u7684\u91cd\u8981\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u65b0\u6307\u6807\u5728\u5e73\u5747\u9884\u6d4b\u6027\u80fd\u548c\u91cd\u8981\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6307\u6807\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u901a\u8fc7\u5f00\u53d1\u8005\u7f16\u7801\u4e60\u60ef\u7684\u6846\u67b6\u663e\u8457\u63a8\u8fdb\u4e86\u7f3a\u9677\u9884\u6d4b\u9886\u57df\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u64cd\u4f5c\u7684\u4f9d\u636e\u3002"}}
{"id": "2508.04415", "pdf": "https://arxiv.org/pdf/2508.04415", "abs": "https://arxiv.org/abs/2508.04415", "authors": ["Xuan Chen", "Yu Huang", "Miaowen Wen", "Shahid Mumtaz", "Fatih Gulec", "Anwer Al-Dulaimi", "Andrew W. Eckford"], "title": "Empowering Nanoscale Connectivity through Molecular Communication: A Case Study of Virus Infection", "categories": ["cs.NI"], "comment": "Accepted for publication in IEEE Communications Magazine", "summary": "The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary\nhealthcare paradigm, shows promise for epidemic control. This paper explores\nthe potential of using molecular communication (MC) to address the challenges\nin constructing IoBNT for epidemic prevention, specifically focusing on\nmodeling viral transmission, detecting the virus/infected individuals, and\nidentifying virus mutations. First, the MC channels in macroscale and\nmicroscale scenarios are discussed to match viral transmission in both scales\nseparately. Besides, the detection methods for these two scales are also\nstudied, along with the localization mechanism designed for the virus/infected\nindividuals. Moreover, an identification strategy is proposed to determine\npotential virus mutations, which is validated through simulation using the\nORF3a protein as a benchmark. Finally, open research issues are discussed. In\nsummary, this paper aims to analyze viral transmission through MC and combat\nviral spread using signal processing techniques within MC.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5206\u5b50\u901a\u4fe1\uff08MC\uff09\u6784\u5efa\u751f\u7269\u7eb3\u7c73\u7269\u8054\u7f51\uff08IoBNT\uff09\u4ee5\u5e94\u5bf9\u6d41\u884c\u75c5\u63a7\u5236\u7684\u6311\u6218\uff0c\u5305\u62ec\u75c5\u6bd2\u4f20\u64ad\u5efa\u6a21\u3001\u68c0\u6d4b\u75c5\u6bd2/\u611f\u67d3\u8005\u53ca\u8bc6\u522b\u75c5\u6bd2\u7a81\u53d8\u3002", "motivation": "IoBNT\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u5177\u6709\u9769\u547d\u6027\u6f5c\u529b\uff0c\u4f46\u6784\u5efa\u5176\u7528\u4e8e\u6d41\u884c\u75c5\u9632\u63a7\u9762\u4e34\u6311\u6218\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7MC\u6280\u672f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86\u5b8f\u89c2\u548c\u5fae\u89c2\u5c3a\u5ea6\u4e0b\u7684MC\u901a\u9053\u4ee5\u5339\u914d\u75c5\u6bd2\u4f20\u64ad\uff0c\u7814\u7a76\u4e86\u68c0\u6d4b\u65b9\u6cd5\u53ca\u5b9a\u4f4d\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u75c5\u6bd2\u7a81\u53d8\u8bc6\u522b\u7b56\u7565\uff0c\u901a\u8fc7ORF3a\u86cb\u767d\u9a8c\u8bc1\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u5c3a\u5ea6\u68c0\u6d4b\u548c\u75c5\u6bd2\u7a81\u53d8\u8bc6\u522b\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7MC\u548c\u4fe1\u53f7\u5904\u7406\u6280\u672f\u5206\u6790\u4e86\u75c5\u6bd2\u4f20\u64ad\u5e76\u63d0\u51fa\u4e86\u9632\u63a7\u7b56\u7565\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.04610", "pdf": "https://arxiv.org/pdf/2508.04610", "abs": "https://arxiv.org/abs/2508.04610", "authors": ["Md Zesun Ahmed Mia", "Malyaban Bal", "Sen Lu", "George M. Nishibuchi", "Suhas Chelian", "Srini Vasan", "Abhronil Sengupta"], "title": "Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.NE"], "comment": null, "summary": "Inspired by the brain's hierarchical processing and energy efficiency, this\npaper presents a Spiking Neural Network (SNN) architecture for lifelong Network\nIntrusion Detection System (NIDS). The proposed system first employs an\nefficient static SNN to identify potential intrusions, which then activates an\nadaptive dynamic SNN responsible for classifying the specific attack type.\nMimicking biological adaptation, the dynamic classifier utilizes Grow When\nRequired (GWR)-inspired structural plasticity and a novel Adaptive\nSpike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible\nmechanisms enable the network to learn new threats incrementally while\npreserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual\nlearning setting, the architecture demonstrates robust adaptation, reduced\ncatastrophic forgetting, and achieves $85.3$\\% overall accuracy. Furthermore,\nsimulations using the Intel Lava framework confirm high operational sparsity,\nhighlighting the potential for low-power deployment on neuromorphic hardware.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u7ec8\u8eab\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u9759\u6001\u548c\u52a8\u6001SNN\u7ed3\u5408\uff0c\u5b9e\u73b0\u9ad8\u6548\u653b\u51fb\u68c0\u6d4b\u548c\u5206\u7c7b\uff0c\u5e76\u5f15\u5165\u751f\u7269\u542f\u53d1\u673a\u5236\u4ee5\u51cf\u5c11\u9057\u5fd8\u3002", "motivation": "\u53d7\u5230\u5927\u8111\u7684\u5c42\u6b21\u5904\u7406\u548c\u80fd\u91cf\u6548\u7387\u542f\u53d1\uff0c\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u65b0\u5a01\u80c1\u5e76\u4fdd\u6301\u4f4e\u80fd\u8017\u7684\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u9759\u6001SNN\u521d\u6b65\u8bc6\u522b\u6f5c\u5728\u5165\u4fb5\uff0c\u52a8\u6001SNN\u5206\u7c7b\u653b\u51fb\u7c7b\u578b\uff0c\u7ed3\u5408GWR\u7ed3\u6784\u53ef\u5851\u6027\u548c\u65b0\u578bAd-STDP\u5b66\u4e60\u89c4\u5219\u3002", "result": "\u5728UNSW-NB15\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u5f3a\u5927\u9002\u5e94\u6027\uff0c\u51cf\u5c11\u9057\u5fd8\uff0c\u6574\u4f53\u51c6\u786e\u7387\u8fbe85.3%\uff0c\u4e14\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u64cd\u4f5c\u7a00\u758f\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u5c55\u793a\u51fa\u5728\u7ec8\u8eab\u5b66\u4e60\u548c\u4f4e\u529f\u8017\u90e8\u7f72\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.04265", "pdf": "https://arxiv.org/pdf/2508.04265", "abs": "https://arxiv.org/abs/2508.04265", "authors": ["Borui Li", "Li Yan", "Jianmin Liu"], "title": "SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning", "categories": ["cs.DC", "cs.AI", "cs.CR"], "comment": "19 pages, 7 figures", "summary": "Federated Learning (FL) enables collaborative model training on decentralized\ndata but remains vulnerable to gradient leakage attacks that can reconstruct\nsensitive user information. Existing defense mechanisms, such as differential\nprivacy (DP) and homomorphic encryption (HE), often introduce a trade-off\nbetween privacy, model utility, and system overhead, a challenge that is\nexacerbated in heterogeneous environments with non-IID data and varying client\ncapabilities. To address these limitations, we propose SelectiveShield, a\nlightweight hybrid defense framework that adaptively integrates selective\nhomomorphic encryption and differential privacy. SelectiveShield leverages\nFisher information to quantify parameter sensitivity, allowing clients to\nidentify critical parameters locally. Through a collaborative negotiation\nprotocol, clients agree on a shared set of the most sensitive parameters for\nprotection via homomorphic encryption. Parameters that are uniquely important\nto individual clients are retained locally, fostering personalization, while\nnon-critical parameters are protected with adaptive differential privacy noise.\nExtensive experiments demonstrate that SelectiveShield maintains strong model\nutility while significantly mitigating gradient leakage risks, offering a\npractical and scalable defense mechanism for real-world federated learning\ndeployments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSelectiveShield\u7684\u8f7b\u91cf\u7ea7\u6df7\u5408\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u540c\u6001\u52a0\u5bc6\u548c\u5dee\u5206\u9690\u79c1\u7ed3\u5408\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u68af\u5ea6\u6cc4\u6f0f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u9632\u5fa1\u673a\u5236\uff08\u5982\u5dee\u5206\u9690\u79c1\u548c\u540c\u6001\u52a0\u5bc6\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u3001\u6a21\u578b\u6548\u7528\u548c\u7cfb\u7edf\u5f00\u9500\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u5c24\u5176\u662f\u5728\u5f02\u6784\u73af\u5883\u4e2d\uff0c\u95ee\u9898\u66f4\u52a0\u7a81\u51fa\u3002", "method": "SelectiveShield\u5229\u7528Fisher\u4fe1\u606f\u91cf\u5316\u53c2\u6570\u654f\u611f\u6027\uff0c\u901a\u8fc7\u534f\u4f5c\u534f\u5546\u534f\u8bae\u786e\u5b9a\u5173\u952e\u53c2\u6570\u8fdb\u884c\u540c\u6001\u52a0\u5bc6\u4fdd\u62a4\uff0c\u975e\u5173\u952e\u53c2\u6570\u5219\u91c7\u7528\u81ea\u9002\u5e94\u5dee\u5206\u9690\u79c1\u566a\u58f0\u4fdd\u62a4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSelectiveShield\u5728\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u68af\u5ea6\u6cc4\u6f0f\u98ce\u9669\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u3002", "conclusion": "SelectiveShield\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u9632\u5fa1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4e0e\u6548\u7528\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2508.04214", "pdf": "https://arxiv.org/pdf/2508.04214", "abs": "https://arxiv.org/abs/2508.04214", "authors": ["Yasaman Khorsandmanesh", "Emil Bj\u00f6rnson", "Joakim Jald\u00e9n", "Bengt Lindoff"], "title": "Channel-Coherence-Adaptive Two-Stage Fully Digital Combining for mmWave MIMO Systems", "categories": ["eess.SP", "cs.AR"], "comment": "This paper will be presented in PIMRC 2025", "summary": "This paper considers a millimeter-wave wideband point-to-point MIMO system\nwith fully digital transceivers at the base station and the user equipment\n(UE), focusing on mobile UE scenarios. A main challenge when building a digital\nUE combining is the large volume of baseband samples to handle. To mitigate\ncomputational and hardware complexity, we propose a novel two-stage digital\ncombining scheme at the UE. The first stage reduces the $N_{\\text{r}}$ received\nsignals to $N_{\\text{c}}$ streams before baseband processing, leveraging\nchannel geometry for dimension reduction and updating at the beam coherence\ntime, which is longer than the channel coherence time of the small-scale\nfading. By contrast, the second-stage combining is updated per fading\nrealization. We develop a pilot-based channel estimation framework for this\nhardware setup based on maximum likelihoodestimation in both uplink and\ndownlink. Digital precoding and combining designs are proposed, and a spectral\nefficiency expression that incorporates imperfect channel knowledge is derived.\nThe numerical results demonstrate that the proposed approach outperforms hybrid\nbeamforming, showcasing the attractiveness of using two-stage fully digital\ntransceivers in future systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6beb\u7c73\u6ce2\u5bbd\u5e26MIMO\u7cfb\u7edf\u4e2d\u7684\u4e24\u9636\u6bb5\u6570\u5b57\u7ec4\u5408\u65b9\u6848\uff0c\u964d\u4f4e\u786c\u4ef6\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u7528\u6237\u8bbe\u5907\u5728\u5904\u7406\u5927\u91cf\u57fa\u5e26\u6837\u672c\u65f6\u7684\u8ba1\u7b97\u548c\u786c\u4ef6\u590d\u6742\u5ea6\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6570\u5b57\u7ec4\u5408\u65b9\u6848\uff0c\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u4fe1\u9053\u51e0\u4f55\u7ed3\u6784\u964d\u7ef4\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u4fe1\u9053\u4f30\u8ba1\u66f4\u65b0\u7ec4\u5408\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6df7\u5408\u6ce2\u675f\u6210\u5f62\uff0c\u5c55\u73b0\u4e86\u5168\u6570\u5b57\u6536\u53d1\u5668\u7684\u4f18\u52bf\u3002", "conclusion": "\u4e24\u9636\u6bb5\u5168\u6570\u5b57\u6536\u53d1\u5668\u5728\u672a\u6765\u7cfb\u7edf\u4e2d\u5177\u6709\u5438\u5f15\u529b\uff0c\u5c24\u5176\u5728\u6beb\u7c73\u6ce2\u5bbd\u5e26MIMO\u573a\u666f\u3002"}}
{"id": "2508.04273", "pdf": "https://arxiv.org/pdf/2508.04273", "abs": "https://arxiv.org/abs/2508.04273", "authors": ["Junan Lin", "Daizong Liu", "Xianke Chen", "Xiaoye Qu", "Xun Yang", "Jixiang Zhu", "Sanyuan Zhang", "Jianfeng Dong"], "title": "Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video Moment Retrieval", "categories": ["cs.IR", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to ACM MM 2025", "summary": "Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically\nrelated to the given query. To tackle this task, most existing VMR methods\nsolely focus on the visual and textual modalities while neglecting the\ncomplementary but important audio modality. Although a few recent works try to\ntackle the joint audio-vision-text reasoning, they treat all modalities equally\nand simply embed them without fine-grained interaction for moment retrieval.\nThese designs are counter-practical as: Not all audios are helpful for video\nmoment retrieval, and the audio of some videos may be complete noise or\nbackground sound that is meaningless to the moment determination. To this end,\nwe propose a novel Importance-aware Multi-Granularity fusion model (IMG), which\nlearns to dynamically and selectively aggregate the audio-vision-text contexts\nfor VMR. Specifically, after integrating the textual guidance with vision and\naudio separately, we first design a pseudo-label-supervised audio importance\npredictor that predicts the importance score of the audio, and accordingly\nassigns weights to mitigate the interference caused by noisy audio. Then, we\ndesign a multi-granularity audio fusion module that adaptively fuses audio and\nvisual modalities at local-, event-, and global-level, fully capturing their\ncomplementary contexts. We further propose a cross-modal knowledge distillation\nstrategy to address the challenge of missing audio modality during inference.\nTo evaluate our method, we further construct a new VMR dataset, i.e.,\nCharades-AudioMatter, where audio-related samples are manually selected and\nre-organized from the original Charades-STA to validate the model's capability\nin utilizing audio modality. Extensive experiments validate the effectiveness\nof our method, achieving state-of-the-art with audio-video fusion in VMR\nmethods. Our code is available at https://github.com/HuiGuanLab/IMG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u91cd\u8981\u6027\u611f\u77e5\u591a\u7c92\u5ea6\u878d\u5408\u6a21\u578b\uff08IMG\uff09\uff0c\u7528\u4e8e\u89c6\u9891\u65f6\u523b\u68c0\u7d22\uff08VMR\uff09\uff0c\u52a8\u6001\u9009\u62e9\u6027\u5730\u878d\u5408\u97f3\u9891-\u89c6\u89c9-\u6587\u672c\u6a21\u6001\uff0c\u901a\u8fc7\u9884\u6d4b\u97f3\u9891\u91cd\u8981\u6027\u5206\u6570\u548c\u591a\u7c92\u5ea6\u878d\u5408\u6a21\u5757\uff0c\u4f18\u5316\u97f3\u9891\u5229\u7528\u3002", "motivation": "\u73b0\u6709VMR\u65b9\u6cd5\u5ffd\u89c6\u97f3\u9891\u6a21\u6001\u6216\u7b80\u5355\u5e73\u7b49\u5904\u7406\u6240\u6709\u6a21\u6001\uff0c\u800c\u5b9e\u9645\u4e2d\u97f3\u9891\u53ef\u80fd\u65e0\u610f\u4e49\u6216\u5e72\u6270\u68c0\u7d22\u3002\u672c\u6587\u65e8\u5728\u52a8\u6001\u5229\u7528\u97f3\u9891\u6a21\u6001\uff0c\u63d0\u5347VMR\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u97f3\u9891\u91cd\u8981\u6027\u9884\u6d4b\u5668\uff08\u4f2a\u6807\u7b7e\u76d1\u7763\uff09\u548c\u591a\u7c92\u5ea6\u97f3\u9891\u878d\u5408\u6a21\u5757\uff08\u5c40\u90e8\u3001\u4e8b\u4ef6\u3001\u5168\u5c40\u7ea7\uff09\uff0c\u5e76\u63d0\u51fa\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u4ee5\u5e94\u5bf9\u63a8\u7406\u4e2d\u97f3\u9891\u7f3a\u5931\u3002", "result": "\u6784\u5efa\u65b0\u6570\u636e\u96c6Charades-AudioMatter\uff0c\u5b9e\u9a8c\u8bc1\u660eIMG\u65b9\u6cd5\u5728\u97f3\u9891-\u89c6\u9891\u878d\u5408\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "IMG\u6a21\u578b\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u548c\u52a0\u6743\u97f3\u9891\u6a21\u6001\uff0c\u663e\u8457\u63d0\u5347VMR\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.03852", "pdf": "https://arxiv.org/pdf/2508.03852", "abs": "https://arxiv.org/abs/2508.03852", "authors": ["Zhuohao", "Zhang", "Haichang Li", "Chun Meng Yu", "Faraz Faruqi", "Junan Xie", "Gene S-H Kim", "Mingming Fan", "Angus G. Forbes", "Jacob O. Wobbrock", "Anhong Guo", "Liang He"], "title": "A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers", "categories": ["cs.HC"], "comment": "ASSETS 2025", "summary": "Building 3-D models is challenging for blind and low-vision (BLV) users due\nto the inherent complexity of 3-D models and the lack of support for non-visual\ninteraction in existing tools. To address this issue, we introduce A11yShape, a\nnovel system designed to help BLV users who possess basic programming skills\nunderstand, modify, and iterate on 3-D models. A11yShape leverages LLMs and\nintegrates with OpenSCAD, a popular open-source editor that generates 3-D\nmodels from code. Key functionalities of A11yShape include accessible\ndescriptions of 3-D models, version control to track changes in models and\ncode, and a hierarchical representation of model components. Most importantly,\nA11yShape employs a cross-representation highlighting mechanism to synchronize\nsemantic selections across all model representations -- code, semantic\nhierarchy, AI description, and 3-D rendering. We conducted a multi-session user\nstudy with four BLV programmers, where, after an initial tutorial session,\nparticipants independently completed 12 distinct models across two testing\nsessions, achieving results that aligned with their own satisfaction. The\nresult demonstrates that participants were able to comprehend provided 3-D\nmodels, as well as independently create and modify 3-D models -- tasks that\nwere previously impossible without assistance from sighted individuals.", "AI": {"tldr": "A11yShape\u662f\u4e00\u4e2a\u4e3a\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u8bbe\u8ba1\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7LLMs\u548cOpenSCAD\u7ed3\u5408\uff0c\u63d0\u4f9b\u975e\u89c6\u89c9\u4ea4\u4e92\u652f\u6301\uff0c\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u3001\u4fee\u6539\u548c\u8fed\u4ee33-D\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u5728\u6784\u5efa3-D\u6a21\u578b\u65f6\u9762\u4e34\u7684\u975e\u89c6\u89c9\u4ea4\u4e92\u652f\u6301\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "A11yShape\u7ed3\u5408LLMs\u548cOpenSCAD\uff0c\u63d0\u4f9b3-D\u6a21\u578b\u7684\u53ef\u8bbf\u95ee\u63cf\u8ff0\u3001\u7248\u672c\u63a7\u5236\u548c\u5c42\u6b21\u5316\u7ec4\u4ef6\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8de8\u8868\u793a\u9ad8\u4eae\u673a\u5236\u540c\u6b65\u6a21\u578b\u7684\u591a\u91cd\u8868\u793a\u3002", "result": "\u7528\u6237\u7814\u7a76\u4e2d\uff0c4\u540d\u76f2\u4eba\u7a0b\u5e8f\u5458\u72ec\u7acb\u5b8c\u621012\u4e2a\u6a21\u578b\u4efb\u52a1\uff0c\u6210\u529f\u7406\u89e3\u3001\u521b\u5efa\u548c\u4fee\u65393-D\u6a21\u578b\uff0c\u65e0\u9700\u89c6\u529b\u6b63\u5e38\u8005\u534f\u52a9\u3002", "conclusion": "A11yShape\u6709\u6548\u652f\u6301\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u72ec\u7acb\u64cd\u4f5c3-D\u6a21\u578b\uff0c\u586b\u8865\u73b0\u6709\u5de5\u5177\u7684\u975e\u89c6\u89c9\u4ea4\u4e92\u7a7a\u767d\u3002"}}
{"id": "2508.04448", "pdf": "https://arxiv.org/pdf/2508.04448", "abs": "https://arxiv.org/abs/2508.04448", "authors": ["Damian Gnieciak", "Tomasz Szandala"], "title": "Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection", "categories": ["cs.SE"], "comment": null, "summary": "Modern software relies on a multitude of automated testing and quality\nassurance tools to prevent errors, bugs and potential vulnerabilities. This\nstudy sets out to provide a head-to-head, quantitative and qualitative\nevaluation of six automated approaches: three industry-standard rule-based\nstatic code-analysis tools (SonarQube, CodeQL and Snyk Code) and three\nstate-of-the-art large language models hosted on the GitHub Models platform\n(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten\nreal-world C# projects that embed 63 vulnerabilities across common categories\nsuch as SQL injection, hard-coded secrets and outdated dependencies, we measure\nclassical detection accuracy (precision, recall, F-score), analysis latency,\nand the developer effort required to vet true positives. The language-based\nscanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their\nstatic counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'\nadvantage originates from superior recall, confirming an ability to reason\nacross broader code contexts. However, this benefit comes with substantial\ntrade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language\nmodels mislocate issues at line-or-column granularity due to tokenisation\nartefacts. Overall, language models successfully rival traditional static\nanalysers in finding real vulnerabilities. Still, their noisier output and\nimprecise localisation limit their standalone use in safety-critical audits. We\ntherefore recommend a hybrid pipeline: employ language models early in\ndevelopment for broad, context-aware triage, while reserving deterministic\nrule-based scanners for high-assurance verification. The open benchmark and\nJSON-based result harness released with this paper lay a foundation for\nreproducible, practitioner-centric research into next-generation automated code\nsecurity.", "AI": {"tldr": "\u672c\u6587\u5bf9\u516d\u79cd\u81ea\u52a8\u5316\u4ee3\u7801\u5206\u6790\u5de5\u5177\uff08\u4e09\u79cd\u9759\u6001\u5206\u6790\u5de5\u5177\u548c\u4e09\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff09\u8fdb\u884c\u4e86\u5b9a\u91cf\u4e0e\u5b9a\u6027\u8bc4\u4f30\uff0c\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ec\u56de\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u8bef\u62a5\u7387\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u8f83\u4f4e\uff0c\u63a8\u8350\u7ed3\u5408\u4f7f\u7528\u4e24\u79cd\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u8bc4\u4f30\u73b0\u4ee3\u81ea\u52a8\u5316\u6d4b\u8bd5\u5de5\u5177\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u4ee3\u7801\u5b89\u5168\u4fdd\u969c\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5305\u542b63\u4e2a\u6f0f\u6d1e\u768410\u4e2aC#\u9879\u76ee\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u5bf9\u6bd4\u4e86\u516d\u79cd\u5de5\u5177\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u5f00\u53d1\u8005\u5ba1\u6838\u5de5\u4f5c\u91cf\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e73\u5747F1\u5206\u6570\uff080.797\u30010.753\u30010.750\uff09\u9ad8\u4e8e\u9759\u6001\u5de5\u5177\uff080.260\u30010.386\u30010.546\uff09\uff0c\u4f46\u5176\u8bef\u62a5\u7387\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u8f83\u5dee\u3002", "conclusion": "\u5efa\u8bae\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u57df\u626b\u63cf\u548c\u9759\u6001\u5de5\u5177\u7684\u9ad8\u7cbe\u5ea6\u9a8c\u8bc1\uff0c\u4ee5\u4f18\u5316\u4ee3\u7801\u5b89\u5168\u5ba1\u8ba1\u6d41\u7a0b\u3002"}}
{"id": "2508.04526", "pdf": "https://arxiv.org/pdf/2508.04526", "abs": "https://arxiv.org/abs/2508.04526", "authors": ["Fannya R. Sandjaja", "Ayesha A. Majeed", "Abdullah Abdullah", "Gyan Wickremasinghe", "Karen Rafferty", "Vishal Sharma"], "title": "Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions", "categories": ["cs.NI", "cs.DC"], "comment": "10 pages, 5 Figures, 2 Tables", "summary": "Traditional security architectures are becoming more vulnerable to\ndistributed attacks due to significant dependence on trust. This will further\nescalate when implementing agentic AI within the systems, as more components\nmust be secured over a similar distributed space. These scenarios can be\nobserved in consumer technologies, such as the dense Internet of things (IoT).\nHere, zero-trust architecture (ZTA) can be seen as a potential solution, which\nrelies on a key principle of not giving users explicit trust, instead always\nverifying their privileges whenever a request is made. However, the overall\nsecurity in ZTA is managed through its policies, and unverified policies can\nlead to unauthorized access. Thus, this paper explores challenges and solutions\nfor ZTA policy design in the context of distributed networks, which is referred\nto as zero-trust distributed networks (ZTDN). This is followed by a case-study\non formal verification of policies using UPPAAL. Subsequently, the importance\nof accountability and responsibility in the system's security is discussed.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u96f6\u4fe1\u4efb\u67b6\u6784\uff08ZTA\uff09\u5728\u5206\u5e03\u5f0f\u7f51\u7edc\u4e2d\u7684\u7b56\u7565\u8bbe\u8ba1\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4f7f\u7528UPPAAL\u8fdb\u884c\u7b56\u7565\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u8d23\u4efb\u4e0e\u95ee\u8d23\u5728\u7cfb\u7edf\u5b89\u5168\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u67b6\u6784\u56e0\u8fc7\u5ea6\u4f9d\u8d56\u4fe1\u4efb\u800c\u5bb9\u6613\u53d7\u5230\u5206\u5e03\u5f0f\u653b\u51fb\uff0c\u5c24\u5176\u5728\u5f15\u5165\u667a\u80fd\u4ee3\u7406AI\u540e\uff0c\u95ee\u9898\u66f4\u52a0\u7a81\u51fa\u3002\u96f6\u4fe1\u4efb\u67b6\u6784\uff08ZTA\uff09\u867d\u7136\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u7b56\u7565\u8bbe\u8ba1\u4e0d\u5b8c\u5584\u53ef\u80fd\u5bfc\u81f4\u672a\u6388\u6743\u8bbf\u95ee\u3002", "method": "\u8bba\u6587\u7814\u7a76\u4e86\u96f6\u4fe1\u4efb\u5206\u5e03\u5f0f\u7f51\u7edc\uff08ZTDN\uff09\u7684\u7b56\u7565\u8bbe\u8ba1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7UPPAAL\u5de5\u5177\u5bf9\u7b56\u7565\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u7ed3\u5408\u6848\u4f8b\u7814\u7a76\u5206\u6790\u5176\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u5728\u5206\u5e03\u5f0f\u7f51\u7edc\u4e2d\u8bbe\u8ba1ZTA\u7b56\u7565\u7684\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u8bc1\u5b9e\u4e86\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u7cfb\u7edf\u5b89\u5168\u4e2d\u8d23\u4efb\u4e0e\u95ee\u8d23\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u96f6\u4fe1\u4efb\u67b6\u6784\u5728\u5206\u5e03\u5f0f\u7f51\u7edc\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7b56\u7565\u8bbe\u8ba1\u9700\u8981\u4e25\u683c\u9a8c\u8bc1\uff0c\u4e14\u8d23\u4efb\u4e0e\u95ee\u8d23\u673a\u5236\u662f\u786e\u4fdd\u5b89\u5168\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2508.04271", "pdf": "https://arxiv.org/pdf/2508.04271", "abs": "https://arxiv.org/abs/2508.04271", "authors": ["JinYi Yoon", "JiHo Lee", "Ting He", "Nakjung Choi", "Bo Ji"], "title": "S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task Inference on the Edge", "categories": ["cs.DC"], "comment": "Accepted at IEEE International Conference on Distributed Computing\n  Systems (ICDCS 2025)", "summary": "With the advancement of Artificial Intelligence (AI) towards multiple\nmodalities (language, vision, speech, etc.), multi-modal models have\nincreasingly been used across various applications (e.g., visual question\nanswering or image generation/captioning). Despite the success of AI as a\nservice for multi-modal applications, it relies heavily on clouds, which are\nconstrained by bandwidth, latency, privacy concerns, and unavailability under\nnetwork or server failures. While on-device AI becomes popular, supporting\nmultiple tasks on edge devices imposes significant resource challenges. To\naddress this, we introduce S2M3, a split-and-share multi-modal architecture for\nmulti-task inference on edge devices. Inspired by the general-purpose nature of\nmulti-modal models, which are composed of multiple modules (encoder, decoder,\nclassifier, etc.), we propose to split multi-modal models at functional-level\nmodules; and then share common modules to reuse them across tasks, thereby\nreducing resource usage. To address cross-model dependency arising from module\nsharing, we propose a greedy module-level placement with per-request parallel\nrouting by prioritizing compute-intensive modules. Through experiments on a\ntestbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks,\nwe demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in\nsingle-task and multi-task settings, respectively, without sacrificing\naccuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95\ninstances (93.7%) while reducing inference latency by up to 56.9% on\nresource-constrained devices, compared to cloud AI.", "AI": {"tldr": "S2M3\u662f\u4e00\u79cd\u591a\u6a21\u6001\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u62c6\u5206\u548c\u5171\u4eab\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u591a\u4efb\u52a1\u63a8\u7406\uff0c\u51cf\u5c11\u8d44\u6e90\u5360\u7528\u5e76\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u6a21\u6001AI\u670d\u52a1\u4f9d\u8d56\u4e91\u8ba1\u7b97\u5e26\u6765\u7684\u5e26\u5bbd\u3001\u5ef6\u8fdf\u3001\u9690\u79c1\u53ca\u53ef\u9760\u6027\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u8f7b\u8fb9\u7f18\u8bbe\u5907\u591a\u4efb\u52a1\u7684\u8d44\u6e90\u538b\u529b\u3002", "method": "\u63d0\u51faS2M3\u67b6\u6784\uff0c\u62c6\u5206\u591a\u6a21\u6001\u6a21\u578b\u7684\u529f\u80fd\u6a21\u5757\u5e76\u5171\u4eab\u901a\u7528\u6a21\u5757\uff1b\u91c7\u7528\u8d2a\u5fc3\u7b97\u6cd5\u8fdb\u884c\u6a21\u5757\u7ea7\u653e\u7f6e\u548c\u5e76\u884c\u8def\u7531\u3002", "result": "\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u5185\u5b58\u4f7f\u7528\u51cf\u5c1162%\uff0c\u5ef6\u8fdf\u964d\u4f4e56.9%\uff0c\u51c6\u786e\u6027\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "S2M3\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9ad8\u6548\u652f\u6301\u591a\u6a21\u6001\u591a\u4efb\u52a1\u63a8\u7406\uff0c\u4f18\u4e8e\u4e91AI\u65b9\u6848\u3002"}}
{"id": "2508.04325", "pdf": "https://arxiv.org/pdf/2508.04325", "abs": "https://arxiv.org/abs/2508.04325", "authors": ["Zizhan Ma", "Wenxuan Wang", "Guo Yu", "Yiu-Fai Cheung", "Meidan Ding", "Jie Liu", "Wenting Chen", "Linlin Shen"], "title": "Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Large language models (LLMs) show significant potential in healthcare,\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\npersist regarding the reliability of these benchmarks, which often lack\nclinical fidelity, robust data management, and safety-oriented evaluation\nmetrics. To address these shortcomings, we introduce MedCheck, the first\nlifecycle-oriented assessment framework specifically designed for medical\nbenchmarks. Our framework deconstructs a benchmark's development into five\ncontinuous stages, from design to governance, and provides a comprehensive\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\nuncovers widespread, systemic issues, including a profound disconnect from\nclinical practice, a crisis of data integrity due to unmitigated contamination\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\nserves as both a diagnostic tool for existing benchmarks and an actionable\nguideline to foster a more standardized, reliable, and transparent approach to\nevaluating AI in healthcare.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MedCheck\uff0c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u8bbe\u8ba1\u7684\u751f\u547d\u5468\u671f\u5bfc\u5411\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u7684\u666e\u904d\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u6807\u51c6\u548c\u900f\u660e\u7684\u6539\u8fdb\u6307\u5357\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u95ee\u9898\u7a81\u51fa\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faMedCheck\u6846\u67b6\uff0c\u5206\u4e94\u4e2a\u9636\u6bb5\u8bc4\u4f30\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b46\u9879\u533b\u5b66\u5b9a\u5236\u6807\u51c6\uff0c\u5e76\u5bf953\u4e2a\u533b\u7597LLM\u57fa\u51c6\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u53d1\u73b0\u666e\u904d\u95ee\u9898\uff0c\u5305\u62ec\u4e0e\u4e34\u5e8a\u5b9e\u8df5\u8131\u8282\u3001\u6570\u636e\u5b8c\u6574\u6027\u5371\u673a\u53ca\u5ffd\u89c6\u5b89\u5168\u6027\u8bc4\u4f30\u7ef4\u5ea6\u3002", "conclusion": "MedCheck\u53ef\u4f5c\u4e3a\u8bca\u65ad\u5de5\u5177\u548c\u884c\u52a8\u6307\u5357\uff0c\u63a8\u52a8\u533b\u7597AI\u8bc4\u4f30\u7684\u6807\u51c6\u5316\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2508.03876", "pdf": "https://arxiv.org/pdf/2508.03876", "abs": "https://arxiv.org/abs/2508.03876", "authors": ["Zach Cutler", "Jack Wilburn", "Hilson Shrestha", "Yiren Ding", "Brian Bollen", "Khandaker Abrar Nadib", "Tingying He", "Andrew McNutt", "Lane Harrison", "Alexander Lex"], "title": "ReVISit 2: A Full Experiment Life Cycle User Study Framework", "categories": ["cs.HC"], "comment": null, "summary": "Online user studies of visualizations, visual encodings, and interaction\ntechniques are ubiquitous in visualization research. Yet, designing,\nconducting, and analyzing studies effectively is still a major burden. Although\nvarious packages support such user studies, most solutions address only facets\nof the experiment life cycle, make reproducibility difficult, or do not cater\nto nuanced study designs or interactions. We introduce reVISit 2, a software\nframework that supports visualization researchers at all stages of designing\nand conducting browser-based user studies. ReVISit supports researchers in the\ndesign, debug & pilot, data collection, analysis, and dissemination experiment\nphases by providing both technical affordances (such as replay of participant\ninteractions) and sociotechnical aids (such as a mindfully maintained community\nof support). It is a proven system that can be (and has been) used in\npublication-quality studies -- which we demonstrate through a series of\nexperimental replications. We reflect on the design of the system via\ninterviews and an analysis of its technical dimensions. Through this work, we\nseek to elevate the ease with which studies are conducted, improve the\nreproducibility of studies within our community, and support the construction\nof advanced interactive studies.", "AI": {"tldr": "reVISit 2\u662f\u4e00\u4e2a\u652f\u6301\u53ef\u89c6\u5316\u7814\u7a76\u8005\u8bbe\u8ba1\u548c\u8fdb\u884c\u57fa\u4e8e\u6d4f\u89c8\u5668\u7684\u7528\u6237\u7814\u7a76\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u65e8\u5728\u7b80\u5316\u7814\u7a76\u6d41\u7a0b\u5e76\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u5728\u7ebf\u7528\u6237\u7814\u7a76\u5728\u53ef\u89c6\u5316\u7814\u7a76\u4e2d\u975e\u5e38\u666e\u904d\uff0c\u4f46\u8bbe\u8ba1\u548c\u5206\u6790\u8fd9\u4e9b\u7814\u7a76\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u8d1f\u62c5\uff0c\u73b0\u6709\u5de5\u5177\u96be\u4ee5\u652f\u6301\u590d\u6742\u7684\u7814\u7a76\u8bbe\u8ba1\u6216\u4ea4\u4e92\u3002", "method": "reVISit 2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u6db5\u76d6\u7814\u7a76\u7684\u8bbe\u8ba1\u3001\u8c03\u8bd5\u3001\u6570\u636e\u6536\u96c6\u3001\u5206\u6790\u548c\u4f20\u64ad\uff0c\u5e76\u63d0\u4f9b\u6280\u672f\u548c\u793e\u4f1a\u6280\u672f\u652f\u6301\u3002", "result": "\u8be5\u7cfb\u7edf\u5df2\u88ab\u7528\u4e8e\u9ad8\u8d28\u91cf\u7814\u7a76\u4e2d\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u590d\u73b0\u548c\u8bbf\u8c08\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "reVISit 2\u63d0\u5347\u4e86\u7814\u7a76\u7684\u4fbf\u6377\u6027\u3001\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u652f\u6301\u9ad8\u7ea7\u4ea4\u4e92\u7814\u7a76\u7684\u6784\u5efa\u3002"}}
{"id": "2508.04479", "pdf": "https://arxiv.org/pdf/2508.04479", "abs": "https://arxiv.org/abs/2508.04479", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "title": "Manifestations of Empathy in Software Engineering: How, Why, and When It Matters", "categories": ["cs.SE"], "comment": null, "summary": "Empathy plays a crucial role in software engineering (SE), influencing\ncollaboration, communication, and decision-making. While prior research has\nhighlighted the importance of empathy in SE, there is limited understanding of\nhow empathy manifests in SE practice, what motivates SE practitioners to\ndemonstrate empathy, and the factors that influence empathy in SE work. Our\nstudy explores these aspects through 22 interviews and a large scale survey\nwith 116 software practitioners. Our findings provide insights into the\nexpression of empathy in SE, the drivers behind empathetic practices, SE\nactivities where empathy is perceived as useful or not, and the other factors\nthat influence empathy. In addition, we offer practical implications for SE\npractitioners and researchers, offering a deeper understanding of how to\neffectively integrate empathy into SE processes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5171\u60c5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u8868\u73b0\u3001\u52a8\u673a\u53ca\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u8bbf\u8c08\u548c\u8c03\u67e5\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "\u63a2\u8ba8\u5171\u60c5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u7684\u5177\u4f53\u8868\u73b0\u3001\u52a8\u673a\u53ca\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u901a\u8fc722\u6b21\u8bbf\u8c08\u548c116\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\u7684\u5927\u89c4\u6a21\u8c03\u67e5\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u63ed\u793a\u4e86\u5171\u60c5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u8868\u8fbe\u65b9\u5f0f\u3001\u9a71\u52a8\u56e0\u7d20\u53ca\u5176\u5728\u4e0d\u540c\u6d3b\u52a8\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u5c06\u5171\u60c5\u6709\u6548\u6574\u5408\u5230\u8f6f\u4ef6\u5de5\u7a0b\u6d41\u7a0b\u4e2d\u7684\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2508.04556", "pdf": "https://arxiv.org/pdf/2508.04556", "abs": "https://arxiv.org/abs/2508.04556", "authors": ["Filipe B. Teixeira", "Carolina Sim\u00f5es", "Paulo Fidalgo", "Wagner Pedrosa", "Andr\u00e9 Coelho", "Manuel Ricardo", "Luis M. Pessoa"], "title": "CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps", "categories": ["cs.NI", "cs.CV"], "comment": "7 pages, 5 figures", "summary": "Telecommunications and computer vision have evolved independently. With the\nemergence of high-frequency wireless links operating mostly in line-of-sight,\nvisual data can help predict the channel dynamics by detecting obstacles and\nhelp overcoming them through beamforming or handover techniques.\n  This paper proposes a novel architecture for delivering real-time radio and\nvideo sensing information to O-RAN xApps through a multi-agent approach, and\nintroduces a new video function capable of generating blockage information for\nxApps, enabling Integrated Sensing and Communications. Experimental results\nshow that the delay of sensing information remains under 1\\,ms and that an xApp\ncan successfully use radio and video sensing information to control the 5G/6G\nRAN in real-time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u89c6\u89c9\u6570\u636e\u4e0e\u65e0\u7ebf\u901a\u4fe1\u7ed3\u5408\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u63d0\u4f9b\u65e0\u7ebf\u548c\u89c6\u9891\u611f\u77e5\u4fe1\u606f\uff0c\u4ee5\u652f\u6301\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u3002", "motivation": "\u9ad8\u9891\u65e0\u7ebf\u94fe\u8def\u9700\u8981\u89c6\u8ddd\u4f20\u8f93\uff0c\u89c6\u89c9\u6570\u636e\u53ef\u4ee5\u5e2e\u52a9\u9884\u6d4b\u4fe1\u9053\u52a8\u6001\u5e76\u4f18\u5316\u6280\u672f\uff08\u5982\u6ce2\u675f\u6210\u5f62\u6216\u5207\u6362\uff09\uff0c\u4ee5\u514b\u670d\u969c\u788d\u3002", "method": "\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5c06\u5b9e\u65f6\u7684\u65e0\u7ebf\u548c\u89c6\u9891\u611f\u77e5\u4fe1\u606f\u4f20\u9012\u7ed9O-RAN xApps\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u89c6\u9891\u529f\u80fd\u751f\u6210\u969c\u788d\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u611f\u77e5\u4fe1\u606f\u7684\u5ef6\u8fdf\u5c0f\u4e8e1\u6beb\u79d2\uff0cxApp\u80fd\u6210\u529f\u5229\u7528\u65e0\u7ebf\u548c\u89c6\u9891\u611f\u77e5\u4fe1\u606f\u5b9e\u65f6\u63a7\u52365G/6G RAN\u3002", "conclusion": "\u8be5\u67b6\u6784\u6709\u6548\u5b9e\u73b0\u4e86\u65e0\u7ebf\u901a\u4fe1\u4e0e\u89c6\u89c9\u611f\u77e5\u7684\u96c6\u6210\uff0c\u4e3a\u5b9e\u65f6RAN\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.04284", "pdf": "https://arxiv.org/pdf/2508.04284", "abs": "https://arxiv.org/abs/2508.04284", "authors": ["Julius Irion", "Philipp Wiesner", "Jonathan Bader", "Odej Kao"], "title": "Optimizing Microgrid Composition for Sustainable Data Centers", "categories": ["cs.DC", "cs.SY", "eess.SY"], "comment": null, "summary": "As computing energy demand continues to grow and electrical grid\ninfrastructure struggles to keep pace, an increasing number of data centers are\nbeing planned with colocated microgrids that integrate on-site renewable\ngeneration and energy storage. However, while existing research has examined\nthe tradeoffs between operational and embodied carbon emissions in the context\nof renewable energy certificates, there is a lack of tools to assess how the\nsizing and composition of microgrid components affects long-term sustainability\nand power reliability.\n  In this paper, we present a novel optimization framework that extends the\ncomputing and energy system co-simulator Vessim with detailed renewable energy\ngeneration models from the National Renewable Energy Laboratory's (NREL) System\nAdvisor Model (SAM). Our framework simulates the interaction between computing\nworkloads, on-site renewable production, and energy storage, capturing both\noperational and embodied emissions. We use a multi-horizon black-box\noptimization to explore efficient microgrid compositions and enable operators\nto make more informed decisions when planning energy systems for data centers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u4e2d\u5fc3\u5fae\u7535\u7f51\u7684\u957f\u671f\u53ef\u6301\u7eed\u6027\u548c\u7535\u529b\u53ef\u9760\u6027\uff0c\u7ed3\u5408\u8ba1\u7b97\u4e0e\u80fd\u6e90\u7cfb\u7edf\u7684\u534f\u540c\u6a21\u62df\u3002", "motivation": "\u968f\u7740\u6570\u636e\u4e2d\u5fc3\u80fd\u6e90\u9700\u6c42\u589e\u957f\u548c\u7535\u7f51\u57fa\u7840\u8bbe\u65bd\u6ede\u540e\uff0c\u5fae\u7535\u7f51\u7684\u53ef\u518d\u751f\u80fd\u6e90\u4e0e\u50a8\u80fd\u914d\u7f6e\u9700\u8981\u66f4\u6709\u6548\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u5176\u5bf9\u53ef\u6301\u7eed\u6027\u548c\u53ef\u9760\u6027\u7684\u5f71\u54cd\u3002", "method": "\u6269\u5c55\u4e86Vessim\u534f\u540c\u6a21\u62df\u5668\uff0c\u7ed3\u5408NREL\u7684SAM\u6a21\u578b\uff0c\u6a21\u62df\u8ba1\u7b97\u8d1f\u8f7d\u3001\u53ef\u518d\u751f\u80fd\u6e90\u751f\u4ea7\u4e0e\u50a8\u80fd\u7684\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u591a\u65f6\u95f4\u5c3a\u5ea6\u7684\u9ed1\u76d2\u4f18\u5316\u63a2\u7d22\u5fae\u7535\u7f51\u914d\u7f6e\u3002", "result": "\u6846\u67b6\u4e3a\u6570\u636e\u4e2d\u5fc3\u80fd\u6e90\u7cfb\u7edf\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u5e2e\u52a9\u4f18\u5316\u5fae\u7535\u7f51\u914d\u7f6e\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u5728\u5fae\u7535\u7f51\u53ef\u6301\u7eed\u6027\u548c\u53ef\u9760\u6027\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\uff0c\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u66f4\u79d1\u5b66\u7684\u89c4\u5212\u4f9d\u636e\u3002"}}
{"id": "2508.04418", "pdf": "https://arxiv.org/pdf/2508.04418", "abs": "https://arxiv.org/abs/2508.04418", "authors": ["Jinxing Zhou", "Yanghao Zhou", "Mingfei Han", "Tong Wang", "Xiaojun Chang", "Hisham Cholakkal", "Rao Muhammad Anwer"], "title": "Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation", "categories": ["cs.MM", "cs.CV", "cs.MA", "cs.SD", "eess.AS"], "comment": "Project page: https://github.com/jasongief/TGS-Agent", "summary": "Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects\nin audible videos based on given reference expressions. Prior works typically\nrely on learning latent embeddings via multimodal fusion to prompt a tunable\nSAM/SAM2 decoder for segmentation, which requires strong pixel-level\nsupervision and lacks interpretability. From a novel perspective of explicit\nreference understanding, we propose TGS-Agent, which decomposes the task into a\nThink-Ground-Segment process, mimicking the human reasoning procedure by first\nidentifying the referred object through multimodal analysis, followed by\ncoarse-grained grounding and precise segmentation. To this end, we first\npropose Ref-Thinker, a multimodal language model capable of reasoning over\ntextual, visual, and auditory cues. We construct an instruction-tuning dataset\nwith explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The\nobject description inferred by Ref-Thinker is used as an explicit prompt for\nGrounding-DINO and SAM2, which perform grounding and segmentation without\nrelying on pixel-level supervision. Additionally, we introduce\nR\\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and\nreasoning-intensive references for better evaluating model generalization. Our\napproach achieves state-of-the-art results on both standard Ref-AVSBench and\nproposed R\\textsuperscript{2}-AVSBench. Code will be available at\nhttps://github.com/jasongief/TGS-Agent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTGS-Agent\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c06\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u4efb\u52a1\u5206\u89e3\u4e3aThink-Ground-Segment\u4e09\u4e2a\u6b65\u9aa4\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u6a21\u6001\u878d\u5408\u5b66\u4e60\u6f5c\u5728\u5d4c\u5165\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\u4e14\u9700\u8981\u5f3a\u50cf\u7d20\u7ea7\u76d1\u7763\u3002\u672c\u6587\u4ece\u663e\u5f0f\u53c2\u8003\u7406\u89e3\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u63d0\u51fa\u66f4\u7b26\u5408\u4eba\u7c7b\u63a8\u7406\u7684\u65b9\u5f0f\u3002", "method": "\u63d0\u51faTGS-Agent\uff0c\u5305\u542bRef-Thinker\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u751f\u6210\u5bf9\u8c61\u63cf\u8ff0\u4f5c\u4e3aGrounding-DINO\u548cSAM2\u7684\u663e\u5f0f\u63d0\u793a\uff0c\u907f\u514d\u50cf\u7d20\u7ea7\u76d1\u7763\u3002", "result": "\u65b9\u6cd5\u5728\u6807\u51c6Ref-AVSBench\u548c\u65b0\u63d0\u51fa\u7684R\u00b2-AVSBench\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u548c\u591a\u6a21\u6001\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u89e3\u91ca\u6027\u66f4\u5f3a\u7684\u97f3\u9891-\u89c6\u89c9\u5206\u5272\u65b9\u6cd5\u3002"}}
{"id": "2508.03969", "pdf": "https://arxiv.org/pdf/2508.03969", "abs": "https://arxiv.org/abs/2508.03969", "authors": ["Wei Xu"], "title": "Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This chapter systematically promotes an emerging interdisciplinary field of\nhuman-artificial intelligence interaction (human-AI interaction, HAII) from a\nhuman-centered AI (HCAI) perspective. It introduces a framework of\nhuman-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII\nresearch and applications, emphasizing the importance of adopting a\nhuman-centered approach over a technology-centered one. The chapter presents\nthe HC-HAII methodology, including human-centered methods, process,\ninterdisciplinary teams, and multi-level design paradigms. It also highlights\nkey research challenges and future directions. As the first chapter, this\nchapter also provides a structural overview of this book, which brings together\ncontributions from an interdisciplinary community of researchers and\npractitioners to advance the theory, methodology, and applications of HCAI in\ndiverse domains of HAII. The purpose of this chapter is to provide a\nfundamental framework for this book, centered on HAII research and applications\nbased on the HCAI approach, which will pave the way for the content of\nsubsequent chapters.", "AI": {"tldr": "\u672c\u7ae0\u4ece\u4eba\u672c\u4eba\u5de5\u667a\u80fd\uff08HCAI\uff09\u7684\u89d2\u5ea6\u7cfb\u7edf\u63a8\u8fdb\u4e86\u4eba-\u4eba\u5de5\u667a\u80fd\u4ea4\u4e92\uff08HAII\uff09\u8fd9\u4e00\u65b0\u5174\u4ea4\u53c9\u9886\u57df\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4eba\u672cHAII\uff08HC-HAII\uff09\u6846\u67b6\u3002", "motivation": "\u5f3a\u8c03\u4ee5\u4eba\u4e3a\u6838\u5fc3\u7684\u7814\u7a76\u65b9\u6cd5\u4f18\u4e8e\u6280\u672f\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u4e3a\u540e\u7eed\u7ae0\u8282\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u4ecb\u7ecd\u4e86HC-HAII\u65b9\u6cd5\u8bba\uff0c\u5305\u62ec\u4eba\u672c\u65b9\u6cd5\u3001\u6d41\u7a0b\u3001\u8de8\u5b66\u79d1\u56e2\u961f\u548c\u591a\u5c42\u6b21\u8bbe\u8ba1\u8303\u5f0f\u3002", "result": "\u63d0\u51fa\u4e86HC-HAII\u6846\u67b6\uff0c\u5e76\u603b\u7ed3\u4e86\u5173\u952e\u7814\u7a76\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u672c\u7ae0\u4e3a\u672c\u4e66\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ee5HCAI\u65b9\u6cd5\u4e3a\u6838\u5fc3\u7684HAII\u7814\u7a76\u4e0e\u5e94\u7528\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2508.03936", "pdf": "https://arxiv.org/pdf/2508.03936", "abs": "https://arxiv.org/abs/2508.03936", "authors": ["Xiangzhe Xu", "Guangyu Shen", "Zian Su", "Siyuan Cheng", "Hanxi Guo", "Lu Yan", "Xuan Chen", "Jiasheng Jiang", "Xiaolong Jin", "Chengpeng Wang", "Zhuo Zhang", "Xiangyu Zhang"], "title": "ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants", "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "comment": "The first two authors (Xiangzhe Xu and Guangyu Shen) contributed\n  equally to this work", "summary": "AI coding assistants like GitHub Copilot are rapidly transforming software\ndevelopment, but their safety remains deeply uncertain-especially in\nhigh-stakes domains like cybersecurity. Current red-teaming tools often rely on\nfixed benchmarks or unrealistic prompts, missing many real-world\nvulnerabilities. We present ASTRA, an automated agent system designed to\nsystematically uncover safety flaws in AI-driven code generation and security\nguidance systems. ASTRA works in three stages: (1) it builds structured\ndomain-specific knowledge graphs that model complex software tasks and known\nweaknesses; (2) it performs online vulnerability exploration of each target\nmodel by adaptively probing both its input space, i.e., the spatial\nexploration, and its reasoning processes, i.e., the temporal exploration,\nguided by the knowledge graphs; and (3) it generates high-quality\nviolation-inducing cases to improve model alignment. Unlike prior methods,\nASTRA focuses on realistic inputs-requests that developers might actually\nask-and uses both offline abstraction guided domain modeling and online domain\nknowledge graph adaptation to surface corner-case vulnerabilities. Across two\nmajor evaluation domains, ASTRA finds 11-66% more issues than existing\ntechniques and produces test cases that lead to 17% more effective alignment\ntraining, showing its practical value for building safer AI systems.", "AI": {"tldr": "ASTRA\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u53d1\u73b0AI\u9a71\u52a8\u7684\u4ee3\u7801\u751f\u6210\u548c\u5b89\u5168\u6307\u5bfc\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u7f3a\u9677\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u81ea\u9002\u5e94\u63a2\u7d22\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f0f\u6d1e\u53d1\u73b0\u6548\u7387\u548c\u6a21\u578b\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u5f53\u524dAI\u7f16\u7801\u52a9\u624b\u7684\u5b89\u5168\u6027\u5728\u5173\u952e\u9886\u57df\uff08\u5982\u7f51\u7edc\u5b89\u5168\uff09\u4ecd\u4e0d\u786e\u5b9a\uff0c\u73b0\u6709\u5de5\u5177\u7684\u56fa\u5b9a\u57fa\u51c6\u548c\u4e0d\u73b0\u5b9e\u63d0\u793a\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u6f0f\u6d1e\u3002", "method": "ASTRA\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u6784\u5efa\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u3001\u81ea\u9002\u5e94\u63a2\u7d22\u8f93\u5165\u7a7a\u95f4\u548c\u63a8\u7406\u8fc7\u7a0b\u3001\u751f\u6210\u9ad8\u8d28\u91cf\u8fdd\u89c4\u6848\u4f8b\u3002", "result": "\u5728\u4e24\u9879\u4e3b\u8981\u8bc4\u4f30\u4e2d\uff0cASTRA\u6bd4\u73b0\u6709\u6280\u672f\u591a\u53d1\u73b011-66%\u7684\u95ee\u9898\uff0c\u6d4b\u8bd5\u6848\u4f8b\u4f7f\u5bf9\u9f50\u8bad\u7ec3\u6548\u679c\u63d0\u534717%\u3002", "conclusion": "ASTRA\u5728\u63d0\u5347AI\u7cfb\u7edf\u5b89\u5168\u6027\u65b9\u9762\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2508.03863", "pdf": "https://arxiv.org/pdf/2508.03863", "abs": "https://arxiv.org/abs/2508.03863", "authors": ["Amin Farajzadeh", "Hongzhao Zheng", "Sarah Dumoulin", "Trevor Ha", "Halim Yanikomeroglu", "Amir Ghasemi"], "title": "Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning", "categories": ["cs.LG", "cs.NI", "eess.SP"], "comment": "Accepted to be presented at IEEE PIMRC 2025", "summary": "Accurate spectrum demand prediction is crucial for informed spectrum\nallocation, effective regulatory planning, and fostering sustainable growth in\nmodern wireless communication networks. It supports governmental efforts,\nparticularly those led by the international telecommunication union (ITU), to\nestablish fair spectrum allocation policies, improve auction mechanisms, and\nmeet the requirements of emerging technologies such as advanced 5G, forthcoming\n6G, and the internet of things (IoT). This paper presents an effective\nspatio-temporal prediction framework that leverages crowdsourced user-side key\nperformance indicators (KPIs) and regulatory datasets to model and forecast\nspectrum demand. The proposed methodology achieves superior prediction accuracy\nand cross-regional generalizability by incorporating advanced feature\nengineering, comprehensive correlation analysis, and transfer learning\ntechniques. Unlike traditional ITU models, which are often constrained by\narbitrary inputs and unrealistic assumptions, this approach exploits granular,\ndata-driven insights to account for spatial and temporal variations in spectrum\nutilization. Comparative evaluations against ITU estimates, as the benchmark,\nunderscore our framework's capability to deliver more realistic and actionable\npredictions. Experimental results validate the efficacy of our methodology,\nhighlighting its potential as a robust approach for policymakers and regulatory\nbodies to enhance spectrum management and planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u9884\u6d4b\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4f17\u5305\u7528\u6237\u4fa7KPI\u548c\u76d1\u7ba1\u6570\u636e\u96c6\u9884\u6d4b\u9891\u8c31\u9700\u6c42\uff0c\u901a\u8fc7\u9ad8\u7ea7\u7279\u5f81\u5de5\u7a0b\u548c\u76f8\u5173\u5206\u6790\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8de8\u533a\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9891\u8c31\u9700\u6c42\u9884\u6d4b\u5bf9\u4e8e\u9891\u8c31\u5206\u914d\u3001\u76d1\u7ba1\u89c4\u5212\u548c\u65e0\u7ebf\u901a\u4fe1\u7f51\u7edc\u53ef\u6301\u7eed\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u652f\u6301ITU\u7b49\u673a\u6784\u5236\u5b9a\u516c\u5e73\u5206\u914d\u653f\u7b56\u548c\u6ee1\u8db35G\u30016G\u53caIoT\u7b49\u65b0\u5174\u6280\u672f\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u9ad8\u7ea7\u7279\u5f81\u5de5\u7a0b\u3001\u76f8\u5173\u6027\u5206\u6790\u548c\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u5229\u7528\u4f17\u5305KPI\u548c\u76d1\u7ba1\u6570\u636e\u5efa\u6a21\u9891\u8c31\u9700\u6c42\uff0c\u8003\u8651\u65f6\u7a7a\u53d8\u5316\u3002", "result": "\u76f8\u6bd4\u4f20\u7edfITU\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u73b0\u5b9e\u4e14\u53ef\u64cd\u4f5c\u7684\u9884\u6d4b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u548c\u76d1\u7ba1\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u7684\u9891\u8c31\u7ba1\u7406\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.04334", "pdf": "https://arxiv.org/pdf/2508.04334", "abs": "https://arxiv.org/abs/2508.04334", "authors": ["Noor Islam S. Mohammad"], "title": "Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in Cloud Computing", "categories": ["cs.DC"], "comment": null, "summary": "The rapid growth of Internet of Things (IoT) devices produces massive,\nheterogeneous data streams, demanding scalable and efficient scheduling in\ncloud environments to meet latency, energy, and Quality-of-Service (QoS)\nrequirements. Existing scheduling methods often lack adaptability to dynamic\nworkloads and network variability inherent in IoT-cloud systems. This paper\npresents a novel hybrid scheduling algorithm combining deep Reinforcement\nLearning (RL) and Ant Colony Optimization (ACO) to address these challenges.\nThe deep RL agent utilizes a model-free policy-gradient approach to learn\nadaptive task allocation policies responsive to real-time workload fluctuations\nand network states. Simultaneously, the ACO metaheuristic conducts a global\ncombinatorial search to optimize resource distribution, mitigate congestion,\nand balance load across distributed cloud nodes. Extensive experiments on\nlarge-scale synthetic IoT datasets, reflecting diverse workloads and QoS\nconstraints, demonstrate that the proposed method achieves up to 18.4%\nreduction in average response time, 12.7% improvement in resource utilization,\nand 9.3% decrease in energy consumption compared to leading heuristics and\nRL-only baselines. Moreover, the algorithm ensures strict Service Level\nAgreement (SLA) compliance through deadline-aware scheduling and dynamic\nprioritization. The results confirm the effectiveness of integrating model-free\nRL with swarm intelligence for scalable, energy-efficient IoT data scheduling,\noffering a promising approach for next-generation IoT-cloud platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u8681\u7fa4\u4f18\u5316\u7684\u6df7\u5408\u8c03\u5ea6\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u7269\u8054\u7f51-\u4e91\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u8d1f\u8f7d\u548c\u7f51\u7edc\u53d8\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u54cd\u5e94\u65f6\u95f4\u3001\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u7387\u548c\u51cf\u5c11\u4e86\u80fd\u8017\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u5927\u91cf\u5f02\u6784\u6570\u636e\u6d41\uff0c\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u8d1f\u8f7d\u548c\u7f51\u7edc\u53d8\u5316\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u4ee5\u6ee1\u8db3\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u670d\u52a1\u8d28\u91cf\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u8681\u7fa4\u4f18\u5316\uff0c\u524d\u8005\u5b66\u4e60\u52a8\u6001\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u540e\u8005\u5168\u5c40\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u8d1f\u8f7d\u5747\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7b97\u6cd5\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u964d\u4f4e18.4%\uff0c\u8d44\u6e90\u5229\u7528\u7387\u63d0\u534712.7%\uff0c\u80fd\u8017\u51cf\u5c119.3%\uff0c\u4e14\u4e25\u683c\u9075\u5b88SLA\u3002", "conclusion": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u7fa4\u4f53\u667a\u80fd\u7684\u7ed3\u5408\u4e3a\u7269\u8054\u7f51-\u4e91\u5e73\u53f0\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u8c03\u5ea6\u65b9\u6cd5\u3002"}}
{"id": "2508.04549", "pdf": "https://arxiv.org/pdf/2508.04549", "abs": "https://arxiv.org/abs/2508.04549", "authors": ["Quang-Trung Truong", "Yuk-Kwan Wong", "Vo Hoang Kim Tuyen Dang", "Rinaldi Gotama", "Duc Thanh Nguyen", "Sai-Kit Yeung"], "title": "MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Published at ACMMM2025 (Dataset track)", "summary": "Marine videos present significant challenges for video understanding due to\nthe dynamics of marine objects and the surrounding environment, camera motion,\nand the complexity of underwater scenes. Existing video captioning datasets,\ntypically focused on generic or human-centric domains, often fail to generalize\nto the complexities of the marine environment and gain insights about marine\nlife. To address these limitations, we propose a two-stage marine\nobject-oriented video captioning pipeline. We introduce a comprehensive video\nunderstanding benchmark that leverages the triplets of video, text, and\nsegmentation masks to facilitate visual grounding and captioning, leading to\nimproved marine video understanding and analysis, and marine video generation.\nAdditionally, we highlight the effectiveness of video splitting in order to\ndetect salient object transitions in scene changes, which significantly enrich\nthe semantics of captioning content. Our dataset and code have been released at\nhttps://msc.hkustvgd.com.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u6d77\u6d0b\u89c6\u9891\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u9891\u3001\u6587\u672c\u548c\u5206\u5272\u63a9\u7801\u7684\u4e09\u5143\u7ec4\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b57\u5e55\u6570\u636e\u96c6\u96be\u4ee5\u9002\u5e94\u6d77\u6d0b\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u7f3a\u4e4f\u5bf9\u6d77\u6d0b\u751f\u7269\u7684\u6df1\u5165\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u5904\u7406\u6d77\u6d0b\u89c6\u9891\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7684\u6d77\u6d0b\u89c6\u9891\u5b57\u5e55\u751f\u6210\u6d41\u7a0b\uff0c\u7ed3\u5408\u89c6\u9891\u3001\u6587\u672c\u548c\u5206\u5272\u63a9\u7801\u7684\u4e09\u5143\u7ec4\uff0c\u5e76\u901a\u8fc7\u89c6\u9891\u5206\u5272\u68c0\u6d4b\u663e\u8457\u5bf9\u8c61\u53d8\u5316\u4ee5\u4e30\u5bcc\u5b57\u5e55\u8bed\u4e49\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6d77\u6d0b\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u6d77\u6d0b\u89c6\u9891\u5b57\u5e55\u751f\u6210\u7684\u7a7a\u767d\uff0c\u4e3a\u6d77\u6d0b\u751f\u7269\u7814\u7a76\u548c\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2508.03974", "pdf": "https://arxiv.org/pdf/2508.03974", "abs": "https://arxiv.org/abs/2508.03974", "authors": ["Sayef Azad Sakin", "Katherine E. Isaacs"], "title": "Managing Data for Scalable and Interactive Event Sequence Visualization", "categories": ["cs.HC"], "comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization", "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86ESeMan\u7cfb\u7edf\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u89c4\u6a21\u5e76\u884c\u4e8b\u4ef6\u5e8f\u5217\u7684\u53ef\u89c6\u5316\u4ea4\u4e92\u6027\u80fd\uff0c\u901a\u8fc7\u5206\u5c42\u6570\u636e\u7ed3\u6784\u548c\u667a\u80fd\u7f13\u5b58\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6e32\u67d3\u3002", "motivation": "\u89e3\u51b3\u5e76\u884c\u4e8b\u4ef6\u5e8f\u5217\uff08\u5982\u7a0b\u5e8f\u6267\u884c\u8f68\u8ff9\uff09\u53ef\u89c6\u5316\u4e2d\u56e0\u6570\u636e\u91cf\u589e\u5927\u5bfc\u81f4\u7684\u4ea4\u4e92\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u56e0\u964d\u91c7\u6837\u800c\u727a\u7272\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faESeMan\u7cfb\u7edf\uff0c\u7ed3\u5408\u5206\u5c42\u6570\u636e\u7ed3\u6784\u548c\u667a\u80fd\u7f13\u5b58\u6280\u672f\uff0c\u52a8\u6001\u63d0\u4f9b\u6240\u9700\u6570\u636e\u4ee5\u5b9e\u73b0\u5feb\u901f\u4e14\u7cbe\u786e\u7684\u53ef\u89c6\u5316\u603b\u7ed3\u3002", "result": "ESeMan\u5728\u591a\u79cd\u7a0b\u5e8f\u6267\u884c\u8f68\u8ff9\u4e0a\u6d4b\u8bd5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6570\u636e\u83b7\u53d6\u65f6\u95f4\u4f4e\u4e8e100ms\u4e14\u4fdd\u6301\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u3002", "conclusion": "ESeMan\u4e3a\u4e8b\u4ef6\u5e8f\u5217\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u51c6\u6846\u67b6\u3002"}}
{"id": "2508.04652", "pdf": "https://arxiv.org/pdf/2508.04652", "abs": "https://arxiv.org/abs/2508.04652", "authors": ["Shuo Liu", "Zeyu Liang", "Xueguang Lyu", "Christopher Amato"], "title": "LLM Collaboration With Multi-Agent Reinforcement Learning", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2dLLM\u7684\u534f\u4f5c\u95ee\u9898\u901a\u8fc7MAGRPO\u7b97\u6cd5\u89e3\u51b3\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u80fd\u6709\u6548\u63d0\u5347\u534f\u4f5c\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u72ec\u7acb\u9884\u8bad\u7ec3\u4e14\u672a\u9488\u5bf9\u534f\u4f5c\u4f18\u5316\uff0c\u4e2a\u4f53\u5956\u52b1\u673a\u5236\u8bbe\u8ba1\u590d\u6742\uff0c\u9700\u65b0\u65b9\u6cd5\u3002", "method": "\u5c06LLM\u534f\u4f5c\u5efa\u6a21\u4e3aMARL\u95ee\u9898\uff0c\u63d0\u51faMAGRPO\u7b97\u6cd5\uff0c\u7ed3\u5408RL\u4e0eMARL\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMAGRPO\u80fd\u63d0\u5347LLM\u5728\u5199\u4f5c\u548c\u7f16\u7a0b\u534f\u4f5c\u4e2d\u7684\u54cd\u5e94\u8d28\u91cf\u548c\u6548\u7387\u3002", "conclusion": "MAGRPO\u4e3aLLM\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u63ed\u793a\u4e86MARL\u65b9\u6cd5\u7684\u5e94\u7528\u6f5c\u529b\u4e0e\u6311\u6218\u3002"}}
{"id": "2508.04536", "pdf": "https://arxiv.org/pdf/2508.04536", "abs": "https://arxiv.org/abs/2508.04536", "authors": ["Henrique Guerra", "Tailan S. Sarubi", "Rafael Chaves", "Jonas Maziero"], "title": "Entanglement distribution in quantum networks via swapping of partially entangled states", "categories": ["quant-ph", "cs.NI"], "comment": null, "summary": "The entanglement swapping protocol (ESP) is a fundamental primitive for\ndistributing quantum correlations across distant nodes in a quantum network.\nRecent studies have demonstrated that even when the involved qubit pairs are\nonly partially entangled, it is still possible to concentrate and transmit\nentanglement via Bell-basis measurements. In this work, we extend these ideas\nto quantum networks with various topologies - including linear, star, and\nhybrid configurations - by analyzing the application of the ESP to initially\npartially entangled states. We investigate how entanglement evolves under such\nprotocols by examining the transformations of the initial states and evaluating\nthe success probabilities for generating maximally entangled states at the\noutput. Our results offer new insights into the dynamics of the entanglement\ndistribution in quantum networks and provide practical guidelines for designing\nrobust quantum communication strategies under realistic conditions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u90e8\u5206\u7ea0\u7f20\u6001\u5728\u4e0d\u540c\u62d3\u6251\u7ed3\u6784\u91cf\u5b50\u7f51\u7edc\u4e2d\u7684\u7ea0\u7f20\u4ea4\u6362\u534f\u8bae\uff0c\u5206\u6790\u4e86\u521d\u59cb\u6001\u53d8\u6362\u53ca\u6210\u529f\u6982\u7387\uff0c\u4e3a\u5b9e\u9645\u91cf\u5b50\u901a\u4fe1\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u7814\u7a76\u90e8\u5206\u7ea0\u7f20\u6001\u5728\u91cf\u5b50\u7f51\u7edc\u4e2d\u7684\u7ea0\u7f20\u5206\u5e03\u52a8\u6001\uff0c\u6269\u5c55\u7ea0\u7f20\u4ea4\u6362\u534f\u8bae\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u901a\u8fc7Bell\u57fa\u6d4b\u91cf\u5206\u6790\u7ebf\u6027\u3001\u661f\u578b\u548c\u6df7\u5408\u62d3\u6251\u7f51\u7edc\u4e2d\u7684\u7ea0\u7f20\u4ea4\u6362\u534f\u8bae\u3002", "result": "\u63ed\u793a\u4e86\u7ea0\u7f20\u6001\u7684\u6f14\u5316\u89c4\u5f8b\u548c\u751f\u6210\u6700\u5927\u7ea0\u7f20\u6001\u7684\u6210\u529f\u6982\u7387\u3002", "conclusion": "\u4e3a\u91cf\u5b50\u7f51\u7edc\u4e2d\u7684\u9c81\u68d2\u901a\u4fe1\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u652f\u6301\u3002"}}
{"id": "2508.04596", "pdf": "https://arxiv.org/pdf/2508.04596", "abs": "https://arxiv.org/abs/2508.04596", "authors": ["Chuan-Chi Lai", "Yan-Lin Chen", "Bo-Xin Liu", "Chuan-Ming Liu"], "title": "Edge-assisted Parallel Uncertain Skyline Processing for Low-latency IoE Analysis", "categories": ["cs.DC", "cs.NI"], "comment": "18 pages, 15 figures, to appear in IEEE Internet of Things Journal", "summary": "Due to the Internet of Everything (IoE), data generated in our life become\nlarger. As a result, we need more effort to analyze the data and extract\nvaluable information. In the cloud computing environment, all data analysis is\ndone in the cloud, and the client only needs less computing power to handle\nsome simple tasks. However, with the rapid increase in data volume, sending all\ndata to the cloud via the Internet has become more expensive. The required\ncloud computing resources have also become larger. To solve this problem, edge\ncomputing is proposed. Edge is granted with more computation power to process\ndata before sending it to the cloud. Therefore, the data transmitted over the\nInternet and the computing resources required by the cloud can be effectively\nreduced. In this work, we proposed an Edge-assisted Parallel Uncertain Skyline\n(EPUS) algorithm for emerging low-latency IoE analytic applications. We use the\nconcept of skyline candidate set to prune data that are less likely to become\nthe skyline data on the parallel edge computing nodes. With the candidate\nskyline set, each edge computing node only sends the information required to\nthe server for updating the global skyline, which reduces the amount of data\nthat transfer over the internet. According to the simulation results, the\nproposed method is better than two comparative methods, which reduces the\nlatency of processing two-dimensional data by more than 50%. For\nhigh-dimensional data, the proposed EPUS method also outperforms the other\nexisting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u7684\u5e76\u884c\u4e0d\u786e\u5b9aSkyline\u7b97\u6cd5\uff08EPUS\uff09\uff0c\u7528\u4e8e\u5904\u7406IoE\u4e2d\u7684\u5927\u6570\u636e\u5206\u6790\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u548c\u4e91\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u968f\u7740IoE\u7684\u53d1\u5c55\uff0c\u6570\u636e\u91cf\u6fc0\u589e\uff0c\u4f20\u7edf\u7684\u4e91\u8ba1\u7b97\u6a21\u5f0f\u5728\u5904\u7406\u5927\u6570\u636e\u65f6\u9762\u4e34\u9ad8\u5ef6\u8fdf\u548c\u9ad8\u8d44\u6e90\u6d88\u8017\u7684\u95ee\u9898\u3002\u8fb9\u7f18\u8ba1\u7b97\u88ab\u63d0\u51fa\u4ee5\u5728\u6570\u636e\u4e0a\u4f20\u81f3\u4e91\u7aef\u524d\u8fdb\u884c\u9884\u5904\u7406\uff0c\u4ece\u800c\u51cf\u5c11\u7f51\u7edc\u4f20\u8f93\u548c\u4e91\u7aef\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u63d0\u51faEPUS\u7b97\u6cd5\uff0c\u5229\u7528Skyline\u5019\u9009\u96c6\u5728\u5e76\u884c\u8fb9\u7f18\u8ba1\u7b97\u8282\u70b9\u4e0a\u4fee\u526a\u4e0d\u592a\u53ef\u80fd\u6210\u4e3aSkyline\u7684\u6570\u636e\uff0c\u4ec5\u5c06\u5fc5\u8981\u4fe1\u606f\u4f20\u8f93\u81f3\u670d\u52a1\u5668\u66f4\u65b0\u5168\u5c40Skyline\uff0c\u51cf\u5c11\u7f51\u7edc\u6570\u636e\u4f20\u8f93\u91cf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cEPUS\u7b97\u6cd5\u5728\u4e8c\u7ef4\u6570\u636e\u4e0a\u5904\u7406\u5ef6\u8fdf\u964d\u4f4e\u8d85\u8fc750%\uff0c\u5728\u9ad8\u7ef4\u6570\u636e\u4e0a\u4e5f\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EPUS\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86IoE\u5927\u6570\u636e\u5206\u6790\u4e2d\u7684\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4f4e\u5ef6\u8fdf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04566", "pdf": "https://arxiv.org/pdf/2508.04566", "abs": "https://arxiv.org/abs/2508.04566", "authors": ["Jinxing Zhou", "Ziheng Zhou", "Yanghao Zhou", "Yuxin Mao", "Zhangling Duan", "Dan Guo"], "title": "CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally\nlocalize events in untrimmed videos that occur simultaneously in both the audio\nand visual modalities. This paper explores DAVEL under a new and more\nchallenging weakly-supervised setting (W-DAVEL task), where only video-level\nevent labels are provided and the temporal boundaries of each event are\nunknown. We address W-DAVEL by exploiting \\textit{cross-modal salient anchors},\nwhich are defined as reliable timestamps that are well predicted under weak\nsupervision and exhibit highly consistent event semantics across audio and\nvisual modalities. Specifically, we propose a \\textit{Mutual Event Agreement\nEvaluation} module, which generates an agreement score by measuring the\ndiscrepancy between the predicted audio and visual event classes. Then, the\nagreement score is utilized in a \\textit{Cross-modal Salient Anchor\nIdentification} module, which identifies the audio and visual anchor features\nthrough global-video and local temporal window identification mechanisms. The\nanchor features after multimodal integration are fed into an\n\\textit{Anchor-based Temporal Propagation} module to enhance event semantic\nencoding in the original temporal audio and visual features, facilitating\nbetter temporal localization under weak supervision. We establish benchmarks\nfor W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728\u5f31\u76d1\u7763\u6761\u4ef6\u4e0b\uff08W-DAVEL\u4efb\u52a1\uff09\u8fdb\u884c\u5bc6\u96c6\u97f3\u89c6\u9891\u4e8b\u4ef6\u5b9a\u4f4d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u663e\u8457\u951a\u70b9\u548c\u4e92\u4e8b\u4ef6\u4e00\u81f4\u6027\u8bc4\u4f30\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8\u5728\u4ec5\u63d0\u4f9b\u89c6\u9891\u7ea7\u4e8b\u4ef6\u6807\u7b7e\u4e14\u672a\u77e5\u65f6\u95f4\u8fb9\u754c\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u5229\u7528\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5b9e\u73b0\u97f3\u89c6\u9891\u4e8b\u4ef6\u5b9a\u4f4d\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u663e\u8457\u951a\u70b9\u8bc6\u522b\u548c\u4e92\u4e8b\u4ef6\u4e00\u81f4\u6027\u8bc4\u4f30\u6a21\u5757\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u673a\u5236\u8bc6\u522b\u951a\u70b9\u7279\u5f81\uff0c\u589e\u5f3a\u4e8b\u4ef6\u8bed\u4e49\u7f16\u7801\u3002", "result": "\u5728UnAV-100\u548cActivityNet1.3\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u97f3\u89c6\u9891\u4e8b\u4ef6\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.03980", "pdf": "https://arxiv.org/pdf/2508.03980", "abs": "https://arxiv.org/abs/2508.03980", "authors": ["Md Sabbir Ahmed", "Arafat Rahman", "Mark Rucker", "Laura E. Barnes"], "title": "SocialPulse: An On-Smartwatch System for Detecting Real-World Social Interactions", "categories": ["cs.HC"], "comment": null, "summary": "Social interactions are a fundamental part of daily life and play a critical\nrole in well-being. As emerging technologies offer opportunities to\nunobtrusively monitor behavior, there is growing interest in using them to\nbetter understand social experiences. However, automatically detecting\ninteractions, particularly via wearable devices, remains underexplored.\nExisting systems are often limited to controlled environments, constrained to\nin-person interactions, and rely on rigid assumptions such as the presence of\ntwo speakers within a fixed time window. These limitations reduce their\ngeneralizability to capture diverse real-world interactions. To address these\nchallenges, we developed a real-time, on-watch system capable of detecting both\nin-person and virtual interactions. The system leverages transfer learning to\ndetect foreground speech (FS) and infers interaction boundaries based upon FS\nand conversational cues like whispering. In a real-world evaluation involving\n11 participants over a total of 38 days (Mean = 3.45 days, SD = 2.73), the\nsystem achieved an interaction detection accuracy of 73.18%. Follow-up with six\nparticipants indicated perfect recall for detecting interactions. These\npreliminary findings demonstrate the potential of our system to capture\ninteractions in daily life, providing a foundation for applications such as\npersonalized interventions targeting social anxiety.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u5b9e\u65f6\u793e\u4ea4\u4e92\u52a8\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u8bed\u97f3\u7279\u5f81\u6355\u6349\u9762\u5bf9\u9762\u53ca\u865a\u62df\u4e92\u52a8\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u901a\u7528\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u6355\u6349\u591a\u6837\u5316\u7684\u793e\u4ea4\u4e92\u52a8\u3002", "method": "\u5f00\u53d1\u5b9e\u65f6\u8155\u6234\u7cfb\u7edf\uff0c\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u68c0\u6d4b\u524d\u666f\u8bed\u97f3\uff08FS\uff09\u5e76\u901a\u8fc7\u8bed\u97f3\u548c\u5bf9\u8bdd\u7ebf\u7d22\uff08\u5982\u4f4e\u8bed\uff09\u63a8\u65ad\u4e92\u52a8\u8fb9\u754c\u3002", "result": "\u572811\u540d\u53c2\u4e0e\u800538\u5929\u7684\u771f\u5b9e\u8bc4\u4f30\u4e2d\uff0c\u7cfb\u7edf\u4e92\u52a8\u68c0\u6d4b\u51c6\u786e\u7387\u4e3a73.18%\uff1b\u540e\u7eed6\u540d\u53c2\u4e0e\u8005\u53ec\u56de\u7387\u4e3a100%\u3002", "conclusion": "\u7cfb\u7edf\u521d\u6b65\u8bc1\u660e\u80fd\u6709\u6548\u6355\u6349\u65e5\u5e38\u4e92\u52a8\uff0c\u4e3a\u793e\u4ea4\u7126\u8651\u7b49\u4e2a\u6027\u5316\u5e72\u9884\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2508.03872", "pdf": "https://arxiv.org/pdf/2508.03872", "abs": "https://arxiv.org/abs/2508.03872", "authors": ["Wesley Brewer", "Murali Meena Gopalakrishnan", "Matthias Maiterth", "Aditya Kashi", "Jong Youl Choi", "Pei Zhang", "Stephen Nichols", "Riccardo Balin", "Miles Couchman", "Stephen de Bruyn Kops", "P. K. Yeung", "Daniel Dotson", "Rohini Uma-Vaideswaran", "Sarp Oral", "Feiyi Wang"], "title": "Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "14 pages, 8 figures, 2 tables", "summary": "With the end of Moore's law and Dennard scaling, efficient training\nincreasingly requires rethinking data volume. Can we train better models with\nsignificantly less data via intelligent subsampling? To explore this, we\ndevelop SICKLE, a sparse intelligent curation framework for efficient learning,\nfeaturing a novel maximum entropy (MaxEnt) sampling approach, scalable\ntraining, and energy benchmarking. We compare MaxEnt with random and\nphase-space sampling on large direct numerical simulation (DNS) datasets of\nturbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as\na preprocessing step can improve model accuracy and substantially lower energy\nconsumption, with reductions of up to 38x observed in certain cases.", "AI": {"tldr": "\u63d0\u51faSICKLE\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u5b50\u91c7\u6837\u51cf\u5c11\u6570\u636e\u91cf\uff0c\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u5e76\u964d\u4f4e38\u500d\u80fd\u8017\u3002", "motivation": "\u968f\u7740\u6469\u5c14\u5b9a\u5f8b\u548cDennard\u7f29\u653e\u7684\u7ec8\u7ed3\uff0c\u9ad8\u6548\u8bad\u7ec3\u9700\u51cf\u5c11\u6570\u636e\u91cf\uff0c\u63a2\u7d22\u667a\u80fd\u5b50\u91c7\u6837\u662f\u5426\u53ef\u884c\u3002", "method": "\u5f00\u53d1SICKLE\u6846\u67b6\uff0c\u91c7\u7528\u6700\u5927\u71b5\uff08MaxEnt\uff09\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u4e0e\u968f\u673a\u53ca\u76f8\u7a7a\u95f4\u91c7\u6837\u5bf9\u6bd4\u3002", "result": "\u5728Frontier\u4e0a\u9a8c\u8bc1\uff0c\u5b50\u91c7\u6837\u9884\u5904\u7406\u53ef\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u5e76\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff08\u90e8\u5206\u6848\u4f8b\u80fd\u8017\u51cf\u5c1138\u500d\uff09\u3002", "conclusion": "\u667a\u80fd\u5b50\u91c7\u6837\u662f\u9ad8\u6548\u8bad\u7ec3\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u5927\u5e45\u964d\u4f4e\u6570\u636e\u9700\u6c42\u548c\u80fd\u8017\u3002"}}
{"id": "2508.04700", "pdf": "https://arxiv.org/pdf/2508.04700", "abs": "https://arxiv.org/abs/2508.04700", "authors": ["Zeyi Sun", "Ziyu Liu", "Yuhang Zang", "Yuhang Cao", "Xiaoyi Dong", "Tong Wu", "Dahua Lin", "Jiaqi Wang"], "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MA", "cs.MM"], "comment": "Code at https://github.com/SunzeY/SEAgent", "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.", "AI": {"tldr": "SEAgent\u662f\u4e00\u4e2a\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u5e2e\u52a9\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u901a\u8fc7\u5b9e\u9a8c\u5b66\u4e60\u638c\u63e1\u65b0\u8f6f\u4ef6\uff0c\u63d0\u5347\u4e86\u5728\u6ca1\u6709\u4eba\u7c7b\u6807\u6ce8\u60c5\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65b0\u9896\u548c\u4e13\u7528\u8f6f\u4ef6\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\u7684\u573a\u666f\u3002", "method": "\u8bbe\u8ba1\u4e86World State Model\u548cCurriculum Generator\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5b66\u4e60\uff08\u5305\u62ec\u5931\u8d25\u884c\u4e3a\u7684\u5bf9\u6297\u6a21\u4eff\u548c\u6210\u529f\u884c\u4e3a\u7684GRPO\u4f18\u5316\uff09\u66f4\u65b0\u4ee3\u7406\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u4e13\u5bb6\u5230\u901a\u624d\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u4e94\u4e2a\u65b0\u8f6f\u4ef6\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0cSEAgent\u7684\u6210\u529f\u7387\u4ece11.3%\u63d0\u5347\u523034.5%\uff0c\u8d85\u8fc7\u7ade\u54c1UI-TARS 23.2%\u3002", "conclusion": "SEAgent\u901a\u8fc7\u81ea\u4e3b\u5b66\u4e60\u548c\u878d\u5408\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2508.04011", "pdf": "https://arxiv.org/pdf/2508.04011", "abs": "https://arxiv.org/abs/2508.04011", "authors": ["Hamza El Alaoui", "Atieh Taheri", "Yi-Hao Peng", "Jeffrey P. Bigham"], "title": "StepWrite: Adaptive Planning for Speech-Driven Text Generation", "categories": ["cs.HC", "cs.AI"], "comment": "This paper has been accepted to UIST 2025. For additional materials\n  and project details, please see:\n  https://www.cs.cmu.edu/~helalaou/publications/stepwrite", "summary": "People frequently use speech-to-text systems to compose short texts with\nvoice. However, current voice-based interfaces struggle to support composing\nmore detailed, contextually complex texts, especially in scenarios where users\nare on the move and cannot visually track progress. Longer-form communication,\nsuch as composing structured emails or thoughtful responses, requires\npersistent context tracking, structured guidance, and adaptability to evolving\nuser intentions--capabilities that conventional dictation tools and voice\nassistants do not support. We introduce StepWrite, a large language\nmodel-driven voice-based interaction system that augments human writing ability\nby enabling structured, hands-free and eyes-free composition of longer-form\ntexts while on the move. StepWrite decomposes the writing process into\nmanageable subtasks and sequentially guides users with contextually-aware\nnon-visual audio prompts. StepWrite reduces cognitive load by offloading the\ncontext-tracking and adaptive planning tasks to the models. Unlike baseline\nmethods like standard dictation features (e.g., Microsoft Word) and\nconversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite\ndynamically adapts its prompts based on the evolving context and user intent,\nand provides coherent guidance without compromising user autonomy. An empirical\nevaluation with 25 participants engaging in mobile or stationary hands-occupied\nactivities demonstrated that StepWrite significantly reduces cognitive load,\nimproves usability and user satisfaction compared to baseline methods.\nTechnical evaluations further confirmed StepWrite's capability in dynamic\ncontextual prompt generation, accurate tone alignment, and effective fact\nchecking. This work highlights the potential of structured, context-aware voice\ninteractions in enhancing hands-free and eye-free communication in everyday\nmultitasking scenarios.", "AI": {"tldr": "StepWrite\u662f\u4e00\u6b3e\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u4ea4\u4e92\u7cfb\u7edf\uff0c\u652f\u6301\u7528\u6237\u5728\u79fb\u52a8\u4e2d\u514d\u63d0\u3001\u514d\u89c6\u64b0\u5199\u957f\u6587\u672c\uff0c\u901a\u8fc7\u5206\u89e3\u5199\u4f5c\u4efb\u52a1\u548c\u63d0\u4f9b\u60c5\u5883\u611f\u77e5\u7684\u97f3\u9891\u63d0\u793a\uff0c\u663e\u8457\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u5e76\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u8bed\u97f3\u8f6c\u6587\u672c\u7cfb\u7edf\u548c\u8bed\u97f3\u52a9\u624b\u96be\u4ee5\u652f\u6301\u590d\u6742\u60c5\u5883\u4e0b\u7684\u957f\u6587\u672c\u64b0\u5199\uff0c\u5c24\u5176\u662f\u5728\u7528\u6237\u65e0\u6cd5\u89c6\u89c9\u8ddf\u8e2a\u8fdb\u5ea6\u7684\u79fb\u52a8\u573a\u666f\u4e2d\u3002", "method": "StepWrite\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u5199\u4f5c\u8fc7\u7a0b\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u60c5\u5883\u611f\u77e5\u7684\u975e\u89c6\u89c9\u97f3\u9891\u63d0\u793a\u9010\u6b65\u5f15\u5bfc\u7528\u6237\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cStepWrite\u663e\u8457\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\uff0c\u63d0\u5347\u53ef\u7528\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u6280\u672f\u8bc4\u4f30\u8bc1\u5b9e\u5176\u80fd\u52a8\u6001\u751f\u6210\u63d0\u793a\u3001\u51c6\u786e\u5bf9\u9f50\u8bed\u6c14\u5e76\u6709\u6548\u8fdb\u884c\u4e8b\u5b9e\u6838\u67e5\u3002", "conclusion": "StepWrite\u5c55\u793a\u4e86\u60c5\u5883\u611f\u77e5\u8bed\u97f3\u4ea4\u4e92\u5728\u514d\u63d0\u3001\u514d\u89c6\u7684\u591a\u4efb\u52a1\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.04077", "pdf": "https://arxiv.org/pdf/2508.04077", "abs": "https://arxiv.org/abs/2508.04077", "authors": ["Ayd\u0131n Bulu\u00e7"], "title": "The Ubiquitous Sparse Matrix-Matrix Products", "categories": ["math.NA", "cs.DC", "cs.LG", "cs.MS", "cs.NA", "math.CO"], "comment": null, "summary": "Multiplication of a sparse matrix with another (dense or sparse) matrix is a\nfundamental operation that captures the computational patterns of many data\nscience applications, including but not limited to graph algorithms, sparsely\nconnected neural networks, graph neural networks, clustering, and many-to-many\ncomparisons of biological sequencing data.\n  In many application scenarios, the matrix multiplication takes places on an\narbitrary algebraic semiring where the scalar operations are overloaded with\nuser-defined functions with certain properties or a more general heterogenous\nalgebra where even the domains of the input matrices can be different. Here, we\nprovide a unifying treatment of the sparse matrix-matrix operation and its rich\napplication space including machine learning, computational biology and\nchemistry, graph algorithms, and scientific computing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u5728\u4e0d\u540c\u4ee3\u6570\u534a\u73af\u6216\u5f02\u6784\u4ee3\u6570\u4e0b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6db5\u76d6\u4e86\u673a\u5668\u5b66\u4e60\u3001\u8ba1\u7b97\u751f\u7269\u5b66\u3001\u5316\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u3002", "motivation": "\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u662f\u6570\u636e\u79d1\u5b66\u4e2d\u8bb8\u591a\u5e94\u7528\u7684\u6838\u5fc3\u64cd\u4f5c\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u5e7f\u4e49\u4ee3\u6570\u6846\u67b6\u7684\u7edf\u4e00\u5904\u7406\u3002", "method": "\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u4ee3\u6570\u534a\u73af\u6216\u5f02\u6784\u4ee3\u6570\u4e2d\u7684\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u673a\u5668\u5b66\u4e60\u3001\u8ba1\u7b97\u751f\u7269\u5b66\u3001\u5316\u5b66\u3001\u56fe\u7b97\u6cd5\u548c\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u5904\u7406\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u53ca\u5176\u4e30\u5bcc\u7684\u5e94\u7528\u573a\u666f\uff0c\u8be5\u7814\u7a76\u4e3a\u8de8\u9886\u57df\u7684\u6570\u636e\u79d1\u5b66\u95ee\u9898\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04026", "pdf": "https://arxiv.org/pdf/2508.04026", "abs": "https://arxiv.org/abs/2508.04026", "authors": ["Shunyu Liu", "Minghao Liu", "Huichi Zhou", "Zhenyu Cui", "Yang Zhou", "Yuhao Zhou", "Wendong Fan", "Ge Zhang", "Jiajun Shi", "Weihao Xuan", "Jiaxing Huang", "Shuang Luo", "Fang Wu", "Heli Qi", "Qingcheng Zeng", "Ziqi Ren", "Jialiang Gao", "Jindi Lv", "Junjie Wang", "Aosong Feng", "Heng Zhou", "Wangchunshu Zhou", "Zhenfei Yin", "Wenlong Zhang", "Guohao Li", "Wenhao Yu", "Irene Li", "Lei Ma", "Lei Bai", "Qunshu Lin", "Mingli Song", "Dacheng Tao"], "title": "VeriGUI: Verifiable Long-Chain GUI Dataset", "categories": ["cs.HC"], "comment": null, "summary": "Recent studies have delved into constructing autonomous agents capable of\nperforming complex Graphical User Interface (GUI)-based computer tasks, with\nthe potential to revolutionize human-computer interaction. Despite encouraging\nresults, existing efforts mainly focus on short-term interactions and rely on\noutcome-only verification, thereby limiting their scalability in real-world GUI\napplications that demand long-horizon task decomposition and execution. In this\nwork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed\nto facilitate the development and evaluation of generalist GUI agents operating\nin realistic computer environments. Our dataset emphasizes two critical\ndimensions: (1) long-chain complexity, with tasks decomposed into a sequence of\ninterdependent subtasks spanning hundreds of steps, explicitly designed to\nallow any subtask to serve as a valid starting point; and (2) subtask-level\nverifiability, which enables diverse exploration strategies within each\nsubtask, while ensuring that each subtask-level goal remains verifiable and\nconsistent. The dataset consists of GUI task trajectories across both desktop\nand web, annotated by human experts. Extensive experiments on VeriGUI using\nvarious agents with different foundation models reveal significant performance\ngaps in handling long-horizon tasks, highlighting the need for more robust\nplanning and decision-making capabilities in GUI agents.", "AI": {"tldr": "VeriGUI\u662f\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u957f\u94feGUI\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u901a\u7528GUI\u4ee3\u7406\u5728\u771f\u5b9e\u8ba1\u7b97\u673a\u73af\u5883\u4e2d\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u7814\u7a76\u591a\u5173\u6ce8\u77ed\u671f\u4ea4\u4e92\u548c\u7ed3\u679c\u9a8c\u8bc1\uff0c\u96be\u4ee5\u5e94\u5bf9\u9700\u8981\u957f\u671f\u4efb\u52a1\u5206\u89e3\u548c\u6267\u884c\u7684\u73b0\u5b9e\u5e94\u7528\u3002", "method": "\u6784\u5efa\u5305\u542b\u957f\u94fe\u590d\u6742\u6027\u548c\u5b50\u4efb\u52a1\u53ef\u9a8c\u8bc1\u6027\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u684c\u9762\u548c\u7f51\u9875GUI\u4efb\u52a1\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u7684GUI\u4ee3\u7406\u5728\u957f\u94fe\u4efb\u52a1\u4e0a\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u8868\u660e\u9700\u8981\u66f4\u5f3a\u5927\u7684\u89c4\u5212\u548c\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "VeriGUI\u4e3aGUI\u4ee3\u7406\u7684\u957f\u94fe\u4efb\u52a1\u5904\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2508.04100", "pdf": "https://arxiv.org/pdf/2508.04100", "abs": "https://arxiv.org/abs/2508.04100", "authors": ["Borui Li", "Li Yan", "Junhao Han", "Jianmin Liu", "Lei Yu"], "title": "SenseCrypt: Sensitivity-guided Selective Homomorphic Encryption for Joint Federated Learning in Cross-Device Scenarios", "categories": ["cs.CR", "cs.AI", "cs.DC"], "comment": "17 pages, 19 figures", "summary": "Homomorphic Encryption (HE) prevails in securing Federated Learning (FL), but\nsuffers from high overhead and adaptation cost. Selective HE methods, which\npartially encrypt model parameters by a global mask, are expected to protect\nprivacy with reduced overhead and easy adaptation. However, in cross-device\nscenarios with heterogeneous data and system capabilities, traditional\nSelective HE methods deteriorate client straggling, and suffer from degraded HE\noverhead reduction performance. Accordingly, we propose SenseCrypt, a\nSensitivity-guided selective Homomorphic EnCryption framework, to adaptively\nbalance security and HE overhead per cross-device FL client. Given the\nobservation that model parameter sensitivity is effective for measuring\nclients' data distribution similarity, we first design a privacy-preserving\nmethod to respectively cluster the clients with similar data distributions.\nThen, we develop a scoring mechanism to deduce the straggler-free ratio of\nmodel parameters that can be encrypted by each client per cluster. Finally, for\neach client, we formulate and solve a multi-objective model parameter selection\noptimization problem, which minimizes HE overhead while maximizing model\nsecurity without causing straggling. Experiments demonstrate that SenseCrypt\nensures security against the state-of-the-art inversion attacks, while\nachieving normal model accuracy as on IID data, and reducing training time by\n58.4%-88.7% as compared to traditional HE methods.", "AI": {"tldr": "SenseCrypt\u662f\u4e00\u79cd\u57fa\u4e8e\u654f\u611f\u5ea6\u7684\u9009\u62e9\u6027\u540c\u6001\u52a0\u5bc6\u6846\u67b6\uff0c\u65e8\u5728\u5728\u8de8\u8bbe\u5907\u8054\u90a6\u5b66\u4e60\u4e2d\u5e73\u8861\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u5e76\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u9009\u62e9\u6027\u540c\u6001\u52a0\u5bc6\u65b9\u6cd5\u5728\u5f02\u6784\u6570\u636e\u548c\u7cfb\u7edf\u80fd\u529b\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u5ba2\u6237\u7aef\u6027\u80fd\u4e0b\u964d\u548c\u5f00\u9500\u589e\u52a0\u3002SenseCrypt\u901a\u8fc7\u654f\u611f\u5ea6\u5206\u6790\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SenseCrypt\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u805a\u7c7b\u5177\u6709\u76f8\u4f3c\u6570\u636e\u5206\u5e03\u7684\u5ba2\u6237\u7aef\uff0c\u5f00\u53d1\u8bc4\u5206\u673a\u5236\u786e\u5b9a\u52a0\u5bc6\u6bd4\u4f8b\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u9009\u62e9\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSenseCrypt\u80fd\u6709\u6548\u9632\u5fa1\u653b\u51fb\uff0c\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u5ea6\uff0c\u5e76\u5c06\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1158.4%-88.7%\u3002", "conclusion": "SenseCrypt\u5728\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u5408\u8de8\u8bbe\u5907\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2508.04108", "pdf": "https://arxiv.org/pdf/2508.04108", "abs": "https://arxiv.org/abs/2508.04108", "authors": ["Arthur Caetano", "Misha Sra"], "title": "XARP Tools: An Extended Reality Platform for Humans and AI Agents", "categories": ["cs.HC", "H.5"], "comment": null, "summary": "This technical report presents XARP Tools, an extended reality (XR) framework\ndesigned for human and AI developers alike. XARP comprises a server-side Python\nlibrary and platform-specific XR clients. The library offers high-level APIs\nand communicates with clients via a JSON-based protocol over WebSockets. XR\nclients encapsulate device and runtime specifics, providing responsive,\nlow-latency user interaction. XARP can be utilized in three ways: (i) as a\nlibrary that abstracts XR development for humans; (ii) as a set of callable\ntools that allow AI agents to drive on-the-fly interactions with users; and\n(iii) as a Model Context Protocol server that plugs XR devices into AI\necosystems. XARP code and working examples are released openly at\nhttps://github.com/HAL-UCSB/xarp.", "AI": {"tldr": "XARP Tools\u662f\u4e00\u4e2a\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u6846\u67b6\uff0c\u4e3a\u4eba\u7c7b\u548cAI\u5f00\u53d1\u8005\u63d0\u4f9b\u652f\u6301\uff0c\u5305\u542b\u670d\u52a1\u5668\u7aefPython\u5e93\u548c\u5e73\u53f0\u7279\u5b9a\u7684XR\u5ba2\u6237\u7aef\u3002", "motivation": "\u4e3a\u7b80\u5316XR\u5f00\u53d1\u5e76\u6574\u5408AI\u4e0eXR\u8bbe\u5907\u7684\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7Python\u5e93\u63d0\u4f9b\u9ad8\u7ea7API\uff0c\u4e0e\u5ba2\u6237\u7aef\u901a\u8fc7WebSockets\u7684JSON\u534f\u8bae\u901a\u4fe1\uff0c\u5c01\u88c5\u8bbe\u5907\u7ec6\u8282\u4ee5\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u4ea4\u4e92\u3002", "result": "\u652f\u6301\u4e09\u79cd\u4f7f\u7528\u65b9\u5f0f\uff1a\u7b80\u5316\u4eba\u7c7b\u5f00\u53d1\u7684\u5e93\u3001\u4f9bAI\u9a71\u52a8\u7684\u4ea4\u4e92\u5de5\u5177\uff0c\u4ee5\u53ca\u8fde\u63a5XR\u8bbe\u5907\u4e0eAI\u751f\u6001\u7684\u534f\u8bae\u670d\u52a1\u5668\u3002", "conclusion": "XARP Tools\u6210\u529f\u5b9e\u73b0\u4e86XR\u5f00\u53d1\u7684\u591a\u529f\u80fd\u652f\u6301\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2508.04160", "pdf": "https://arxiv.org/pdf/2508.04160", "abs": "https://arxiv.org/abs/2508.04160", "authors": ["Angela Locoro", "Silvia Golia", "Davide Falessi"], "title": "DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment", "categories": ["cs.HC", "cs.CV", "K.3; K.3.2"], "comment": null, "summary": "The underspecification of progressive levels of difficulty in measurement\nconstructs design and assessment tests for data visualization literacy may\nhinder the expressivity of measurements in both test design and test reuse. To\nmitigate this problem, this paper proposes DRIVE-T (Discriminating and\nRepresentative Items for Validating Expressive Tests), a methodology designed\nto drive the construction and evaluation of assessment items. Given a data\nvizualization, DRIVE-T supports the identification of task-based items\ndiscriminability and representativeness for measuring levels of data\nvisualization literacy. DRIVE-T consists of three steps: (1) tagging task-based\nitems associated with a set of data vizualizations; (2) rating them by\nindependent raters for their difficulty; (3) analysing raters' raw scores\nthrough a Many-Facet Rasch Measurement model. In this way, we can observe the\nemergence of difficulty levels of the measurement construct, derived from the\ndiscriminability and representativeness of task-based items for each data\nvizualization, ordered into Many-Facets construct levels. In this study, we\nshow and apply each step of the methodology to an item bank, which models the\ndifficulty levels of a measurement construct approximating a latent construct\nfor data visualization literacy. This measurement construct is drawn from\nsemiotics, i.e., based on the syntax, semantics and pragmatics knowledge that\neach data visualization may require to be mastered by people. The DRIVE-T\nmethodology operationalises an inductive approach, observable in a post-design\nphase of the items preparation, for formative-style and practice-based\nmeasurement construct emergence. A pilot study with items selected through the\napplication of DRIVE-T is also presented to test our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DRIVE-T\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u8bc4\u4f30\u6570\u636e\u53ef\u89c6\u5316\u7d20\u517b\u7684\u6d4b\u91cf\u5de5\u5177\uff0c\u901a\u8fc7\u4efb\u52a1\u9879\u7684\u533a\u5206\u5ea6\u548c\u4ee3\u8868\u6027\u6765\u786e\u5b9a\u96be\u5ea6\u7ea7\u522b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u53ef\u89c6\u5316\u7d20\u517b\u6d4b\u91cf\u5de5\u5177\u5728\u96be\u5ea6\u7ea7\u522b\u8868\u8fbe\u4e0a\u7684\u4e0d\u8db3\u9650\u5236\u4e86\u6d4b\u91cf\u6548\u679c\u7684\u51c6\u786e\u6027\u3002", "method": "DRIVE-T\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a(1) \u4e3a\u6570\u636e\u53ef\u89c6\u5316\u4efb\u52a1\u9879\u6253\u6807\u7b7e\uff0c(2) \u7531\u72ec\u7acb\u8bc4\u5206\u8005\u8bc4\u5b9a\u4efb\u52a1\u96be\u5ea6\uff0c(3) \u901a\u8fc7\u591a\u9762Rasch\u6a21\u578b\u5206\u6790\u8bc4\u5206\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u57fa\u4e8e\u533a\u5206\u5ea6\u548c\u4ee3\u8868\u6027\u6392\u5e8f\u7684\u96be\u5ea6\u7ea7\u522b\uff0c\u5e76\u901a\u8fc7\u8bd5\u70b9\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DRIVE-T\u4e3a\u6570\u636e\u53ef\u89c6\u5316\u7d20\u517b\u7684\u6d4b\u91cf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u6d4b\u91cf\u6784\u5ff5\u7684\u96be\u5ea6\u7ea7\u522b\u3002"}}
{"id": "2508.04217", "pdf": "https://arxiv.org/pdf/2508.04217", "abs": "https://arxiv.org/abs/2508.04217", "authors": ["Roberto Rocco", "Simone Rizzo", "Matteo Barbieri", "Gabriella Bettonte", "Elisabetta Boella", "Fulvio Ganz", "Sergio Iserte", "Antonio J. Pe\u00f1a", "Petter Sand\u00e5s", "Alberto Scionti", "Olivier Terzo", "Chiara Vercellino", "Giacomo Vitali", "Paolo Viviani", "Jonathan Frassineti", "Sara Marzella", "Daniele Ottaviani", "Iacopo Colonnelli", "Daniele Gregori"], "title": "Dynamic Solutions for Hybrid Quantum-HPC Resource Allocation", "categories": ["quant-ph", "cs.DC"], "comment": "7 pages, accepted to QCE25 WIHPQC Workshop (The Fifth International\n  Workshop on Integrating High-Performance and Quantum Computing)", "summary": "The integration of quantum computers within classical High-Performance\nComputing (HPC) infrastructures is receiving increasing attention, with the\nformer expected to serve as accelerators for specific computational tasks.\nHowever, combining HPC and quantum computers presents significant technical\nchallenges, including resource allocation. This paper presents a novel\nmalleability-based approach, alongside a workflow-based strategy, to optimize\nresource utilization in hybrid HPC-quantum workloads. With both these\napproaches, we can release classical resources when computations are offloaded\nto the quantum computer and reallocate them once quantum processing is\ncomplete. Our experiments with a hybrid HPC-quantum use case show the benefits\nof dynamic allocation, highlighting the potential of those solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5851\u6027\u548c\u5de5\u4f5c\u6d41\u7684\u7b56\u7565\uff0c\u4f18\u5316HPC-\u91cf\u5b50\u6df7\u5408\u8d1f\u8f7d\u4e2d\u7684\u8d44\u6e90\u5206\u914d\uff0c\u52a8\u6001\u91ca\u653e\u548c\u91cd\u65b0\u5206\u914d\u8d44\u6e90\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0e\u7ecf\u5178HPC\u57fa\u7840\u8bbe\u65bd\u7684\u96c6\u6210\u9700\u6c42\u589e\u52a0\uff0c\u8d44\u6e90\u5206\u914d\u6210\u4e3a\u5173\u952e\u6280\u672f\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5851\u6027\u7684\u65b9\u6cd5\u548c\u5de5\u4f5c\u6d41\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574HPC\u8d44\u6e90\uff0c\u4ee5\u9002\u5e94\u91cf\u5b50\u8ba1\u7b97\u4efb\u52a1\u7684\u8d1f\u8f7d\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u52a8\u6001\u8d44\u6e90\u5206\u914d\u80fd\u663e\u8457\u63d0\u5347\u6df7\u5408\u8d1f\u8f7d\u7684\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u8be5\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aHPC-\u91cf\u5b50\u6df7\u5408\u8d1f\u8f7d\u7684\u8d44\u6e90\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u52a8\u6001\u5206\u914d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.04202", "pdf": "https://arxiv.org/pdf/2508.04202", "abs": "https://arxiv.org/abs/2508.04202", "authors": ["Abdulrhman Alorini", "Yufeng Wu", "Abdullah Bin Sawad", "Mukesh Prasad", "A. Baki Kocaballi"], "title": "Unplug, Mute, Avoid Investigating smart speaker users' privacy protection behaviours in Saudi Homes", "categories": ["cs.HC"], "comment": null, "summary": "Smart speakers are increasingly integrated into domestic life worldwide, yet\ntheir privacy risks remain underexplored in non-Western cultural contexts. This\nstudy investigates how Saudi Arabian users of smart speakers navigate privacy\nconcerns within collectivist, gendered, and often multigenerational households.\nUsing cultural probes followed by semi-structured interviews with 16\nparticipants, we uncover everyday privacy-protective behaviours including\nunplugging devices, muting microphones, and avoiding voice interactions\naltogether. These practices are shaped not only by individual risk perceptions\nbut also by household norms, room configurations, and interpersonal dynamics.\nWe contribute empirical insights from an underrepresented region, theoretical\nextensions to contextual integrity frameworks, and design directions for\nculturally responsive voice interfaces. This work expands the global\nconversation on smart speaker privacy and informs more inclusive HCI practices\nin increasingly diverse smart home environments.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6c99\u7279\u963f\u62c9\u4f2f\u7528\u6237\u5982\u4f55\u5728\u4f7f\u7528\u667a\u80fd\u97f3\u7bb1\u65f6\u5e94\u5bf9\u9690\u79c1\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u4ed6\u4eec\u7684\u9690\u79c1\u4fdd\u62a4\u884c\u4e3a\u53ca\u5176\u80cc\u540e\u7684\u6587\u5316\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22\u975e\u897f\u65b9\u6587\u5316\u80cc\u666f\u4e0b\u667a\u80fd\u97f3\u7bb1\u7684\u9690\u79c1\u95ee\u9898\uff0c\u8865\u5145\u73b0\u6709\u7814\u7a76\u4e2d\u672a\u5145\u5206\u63a2\u8ba8\u7684\u5730\u533a\u548c\u6587\u5316\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u6587\u5316\u63a2\u9488\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u6536\u96c6\u4e8616\u4f4d\u53c2\u4e0e\u8005\u7684\u6570\u636e\uff0c\u5206\u6790\u5176\u9690\u79c1\u4fdd\u62a4\u884c\u4e3a\u53ca\u5176\u6210\u56e0\u3002", "result": "\u53d1\u73b0\u7528\u6237\u901a\u8fc7\u62d4\u6389\u8bbe\u5907\u3001\u9759\u97f3\u9ea6\u514b\u98ce\u6216\u907f\u514d\u8bed\u97f3\u4ea4\u4e92\u6765\u4fdd\u62a4\u9690\u79c1\uff0c\u8fd9\u4e9b\u884c\u4e3a\u53d7\u5bb6\u5ead\u89c4\u8303\u3001\u623f\u95f4\u914d\u7f6e\u548c\u4eba\u9645\u52a8\u6001\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u8865\u5145\u4e86\u5168\u7403\u667a\u80fd\u97f3\u7bb1\u9690\u79c1\u8ba8\u8bba\uff0c\u63d0\u51fa\u4e86\u6587\u5316\u54cd\u5e94\u7684\u8bbe\u8ba1\u65b9\u5411\uff0c\u4fc3\u8fdb\u66f4\u5305\u5bb9\u7684\u4eba\u673a\u4ea4\u4e92\u5b9e\u8df5\u3002"}}
{"id": "2508.04357", "pdf": "https://arxiv.org/pdf/2508.04357", "abs": "https://arxiv.org/abs/2508.04357", "authors": ["Gloria Fern\u00e1ndez-Nieto", "Vanessa Echeverria", "Yuheng Li", "Yi-Shan Tsai", "Lele Sha", "Guanliang Chen", "Dragan Gasevic", "Zachari Swiecki"], "title": "Capturing and Sharing Know-How through Visual Process Representations: A Human-Centred Approach to Teacher Workflows", "categories": ["cs.HC", "H.5.2"], "comment": "29 pages, 16 figures, 7 tables, submitted to Behaviours & Information\n  Technology", "summary": "Knowledge Management is crucial for capturing and transferring expertise\nwithin universities, especially in high staff turnover contexts where expertise\nloss disrupts teaching. Documenting teachers' workflows is time-intensive and\ndiverts experts from core responsibilities. Sequential Pattern Mining (SPM)\nleverages log data to identify expert workflows, offering an automated\nalternative to represent workflows but requiring transformation into intuitive\nformats for novice educators. This paper introduces Visual Process\nRepresentations (VPR), a design approach combining SPM, Knowledge Management\nprocesses, and storytelling techniques to convert expert log data into clear\nvisualisations. We detail the design phases and report a study evaluating\nvisual affordances (text lists vs. pictorial-style) and teachers' perceptions\nof four versions of the VPR with 160 higher teachers on Prolific. Results\nindicate improved task performance, usability, and engagement, particularly\nwith enriched visuals, though process memorability and task time improvements\nwere limited. The findings highlight VPR's potential to visualise workflows and\nsupport novice educators.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVPR\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86SPM\u3001\u77e5\u8bc6\u7ba1\u7406\u548c\u53d9\u4e8b\u6280\u672f\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u5c06\u4e13\u5bb6\u5de5\u4f5c\u6d41\u6570\u636e\u8f6c\u5316\u4e3a\u76f4\u89c2\u7684\u6d41\u7a0b\u8868\u793a\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u9ad8\u7b49\u6559\u80b2\u673a\u6784\u9762\u4e34\u9ad8\u5458\u5de5\u6d41\u52a8\u7387\u5bfc\u81f4\u7684\u77e5\u8bc6\u6d41\u5931\u95ee\u9898\uff0c\u4f20\u7edf\u7684\u5de5\u4f5c\u6d41\u8bb0\u5f55\u65b9\u5f0f\u8017\u65f6\u4e14\u5206\u6563\u4e13\u5bb6\u7cbe\u529b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528SPM\u6280\u672f\u5206\u6790\u65e5\u5fd7\u6570\u636e\uff0c\u7ed3\u5408VPR\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5c06\u4e13\u5bb6\u5de5\u4f5c\u6d41\u8f6c\u5316\u4e3a\u53ef\u89c6\u5316\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u89c6\u89c9\u8868\u73b0\u5f62\u5f0f\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0VPR\u80fd\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u3001\u53ef\u7528\u6027\u548c\u53c2\u4e0e\u5ea6\uff0c\u5c24\u5176\u662f\u5bcc\u89c6\u89c9\u5316\u7248\u672c\uff0c\u4f46\u5bf9\u6d41\u7a0b\u8bb0\u5fc6\u6027\u548c\u4efb\u52a1\u65f6\u95f4\u7684\u6539\u5584\u6709\u9650\u3002", "conclusion": "VPR\u5728\u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\u548c\u652f\u6301\u65b0\u624b\u6559\u5e08\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u589e\u5f3a\u5176\u6548\u679c\u3002"}}
{"id": "2508.04377", "pdf": "https://arxiv.org/pdf/2508.04377", "abs": "https://arxiv.org/abs/2508.04377", "authors": ["Gloria Fern\u00e1ndez-Nieto", "Lele Sha", "Yuheng Li", "Yi-Shan Tsai", "Guanliang Chen", "Yinwei Wei", "Weiqing Wang", "Jinchun Wen", "Shaveen Singh", "Ivan Silva", "Yuanfang Li", "Dragan Gas\u011bvi\u0107", "Zachari Swiecki"], "title": "GoldMind: A Teacher-Centered Knowledge Management System for Higher Education -- Lessons from Iterative Design", "categories": ["cs.HC"], "comment": "38 pages, 10 tables, 7 figures, submitted to TOCHI", "summary": "Designing Knowledge Management Systems (KMSs) for higher education requires\naddressing complex human-technology interactions, especially where staff\nturnover and changing roles create ongoing challenges for reusing knowledge.\nWhile advances in process mining and Generative AI enable new ways of designing\nfeatures to support knowledge management, existing KMSs often overlook the\nrealities of educators' workflows, leading to low adoption and limited impact.\nThis paper presents findings from a two-year human-centred design study with\n108 higher education teachers, focused on the iterative co-design and\nevaluation of GoldMind, a KMS supporting in-the-flow knowledge management\nduring digital teaching tasks. Through three design-evaluation cycles, we\nexamined how teachers interacted with the system and how their feedback\ninformed successive refinements. Insights are synthesised across three themes:\n(1) Technology Lessons from user interaction data, (2) Design Considerations\nshaped by co-design and usability testing, and (3) Human Factors, including\ncognitive load and knowledge behaviours, analysed using Epistemic Network\nAnalysis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GoldMind\u77e5\u8bc6\u7ba1\u7406\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\uff0c\u901a\u8fc7\u4e3a\u671f\u4e24\u5e74\u7684\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u6559\u5e08\u5728\u6570\u5b57\u6559\u5b66\u4efb\u52a1\u4e2d\u7684\u77e5\u8bc6\u7ba1\u7406\u9700\u6c42\uff0c\u5e76\u603b\u7ed3\u4e86\u6280\u672f\u3001\u8bbe\u8ba1\u548c\u4eba\u56e0\u4e09\u65b9\u9762\u7684\u5173\u952e\u53d1\u73b0\u3002", "motivation": "\u9ad8\u7b49\u6559\u80b2\u4e2d\u77e5\u8bc6\u7ba1\u7406\u7cfb\u7edf\uff08KMS\uff09\u7684\u8bbe\u8ba1\u9700\u89e3\u51b3\u590d\u6742\u7684\u4eba\u673a\u4ea4\u4e92\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5458\u5de5\u6d41\u52a8\u548c\u89d2\u8272\u53d8\u5316\u5bfc\u81f4\u77e5\u8bc6\u91cd\u7528\u6311\u6218\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u7cfb\u7edf\u5e38\u5ffd\u89c6\u6559\u80b2\u5de5\u4f5c\u8005\u7684\u5b9e\u9645\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5bfc\u81f4\u4f4e\u91c7\u7eb3\u7387\u548c\u6709\u9650\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e3a\u671f\u4e24\u5e74\u7684\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u7814\u7a76\uff0c\u4e0e108\u540d\u9ad8\u7b49\u6559\u80b2\u6559\u5e08\u5408\u4f5c\uff0c\u8fdb\u884c\u4e86GoldMind\u7cfb\u7edf\u7684\u8fed\u4ee3\u5171\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u5305\u62ec\u4e09\u4e2a\u8bbe\u8ba1-\u8bc4\u4f30\u5faa\u73af\u3002", "result": "\u7814\u7a76\u4ece\u4e09\u4e2a\u4e3b\u9898\u603b\u7ed3\u5173\u952e\u53d1\u73b0\uff1a(1)\u7528\u6237\u4ea4\u4e92\u6570\u636e\u7684\u6280\u672f\u6559\u8bad\uff0c(2)\u5171\u8bbe\u8ba1\u548c\u53ef\u7528\u6027\u6d4b\u8bd5\u5f62\u6210\u7684\u8bbe\u8ba1\u8003\u91cf\uff0c(3)\u901a\u8fc7\u8ba4\u77e5\u7f51\u7edc\u5206\u6790\u63ed\u793a\u7684\u8ba4\u77e5\u8d1f\u8377\u548c\u77e5\u8bc6\u884c\u4e3a\u7b49\u4eba\u56e0\u56e0\u7d20\u3002", "conclusion": "\u901a\u8fc7\u7814\u7a76\u5f3a\u8c03\u4e86KMS\u8bbe\u8ba1\u4e2d\u9700\u5173\u6ce8\u6559\u80b2\u5de5\u4f5c\u8005\u5b9e\u9645\u9700\u6c42\uff0c\u5e76\u4e3a\u672a\u6765\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5b9e\u8bc1\u7684\u6307\u5bfc\u3002"}}
{"id": "2508.04391", "pdf": "https://arxiv.org/pdf/2508.04391", "abs": "https://arxiv.org/abs/2508.04391", "authors": ["Ze Gao", "Mengyao Guo", "Zheng Wang", "Xiaolin Zhang", "Sihuang Man"], "title": "Plant-Centric Metaverse: A Biocentric-Creation Framework for Ecological Art and Digital Symbiosis", "categories": ["cs.HC"], "comment": null, "summary": "Digital ecological art represents an emergent frontier where biological media\nconverge with virtual environments. This study examines the paradigm shift from\nanthropocentric to plant-centered artistic narratives within the metaverse,\ncontextualizing how digital platforms transform ecological expression. However,\ncurrent frameworks fail to systematically guide artists in leveraging plant\nagency for digital symbiosis that transcends human-centered creation. We\npropose the Biocentric-Creation Transformation Ideology (BCTI) framework and\nvalidate it through multimodal case studies spanning bio-art, NFTs, and VR\necosystems (2013-2023). Our analysis reveals: (1) Metaverse ecosystems enable\nunprecedented plant-algorithm co-creation, with biological artworks increasing\nby 133% in premier archives (2020 vs 2013); (2) Digital symbiosis manifests\nthrough blockchain DAOs where plants govern human-plant collaborations; (3)\nAlgorithmic photosynthesis in VR environments reshapes ecological aesthetics\nthrough real-time biodata translation. The BCTI framework advances ecological\nart theory by systematizing the transition from representation to\nplant-centered agency, offering artists a blueprint for post-anthropocene\ncreation. This redefines environmental consciousness in virtual realms while\nestablishing new protocols for cross-species digital collaboration.", "AI": {"tldr": "\u6570\u5b57\u751f\u6001\u827a\u672f\u63a2\u8ba8\u4e86\u751f\u7269\u5a92\u4ecb\u4e0e\u865a\u62df\u73af\u5883\u7684\u878d\u5408\u3002\u7814\u7a76\u63d0\u51faBCTI\u6846\u67b6\uff0c\u4fc3\u8fdb\u690d\u7269\u4e3a\u4e2d\u5fc3\u7684\u521b\u4f5c\u6a21\u5f0f\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6848\u4f8b\u9a8c\u8bc1\u5176\u5728\u5143\u5b87\u5b99\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u827a\u672f\u6846\u67b6\u672a\u7cfb\u7edf\u6307\u5bfc\u827a\u672f\u5bb6\u5229\u7528\u690d\u7269\u4e3b\u4f53\u6027\u8fdb\u884c\u6570\u5b57\u5171\u751f\u521b\u4f5c\uff0c\u9700\u65b0\u65b9\u6cd5\u8d85\u8d8a\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\u3002", "method": "\u63d0\u51faBiocentric-Creation Transformation Ideology (BCTI)\u6846\u67b6\uff0c\u5e76\u901a\u8fc72013-2023\u5e74\u7684\u751f\u7269\u827a\u672f\u3001NFT\u548cVR\u751f\u6001\u7cfb\u7edf\u6848\u4f8b\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5143\u5b87\u5b99\u4e2d\u690d\u7269-\u7b97\u6cd5\u5171\u521b\u589e\u957f133%\uff0c\u533a\u5757\u94feDAO\u5b9e\u73b0\u690d\u7269\u4e3b\u5bfc\u534f\u4f5c\uff0cVR\u5b9e\u65f6\u751f\u7269\u6570\u636e\u91cd\u5851\u751f\u6001\u7f8e\u5b66\u3002", "conclusion": "BCTI\u6846\u67b6\u4e3a\u540e\u4eba\u7c7b\u4e16\u521b\u4f5c\u63d0\u4f9b\u84dd\u56fe\uff0c\u91cd\u65b0\u5b9a\u4e49\u865a\u62df\u73af\u5883\u610f\u8bc6\u5e76\u5efa\u7acb\u8de8\u7269\u79cd\u6570\u5b57\u534f\u4f5c\u65b0\u534f\u8bae\u3002"}}
{"id": "2508.04541", "pdf": "https://arxiv.org/pdf/2508.04541", "abs": "https://arxiv.org/abs/2508.04541", "authors": ["Zhu Yuting", "Cao Xinyu", "Su Yuzhuo", "Ma Yongbin"], "title": "Measuring Information Richness in Product Images: Implications for Online Sales", "categories": ["cs.HC"], "comment": null, "summary": "A common challenge for e-commerce sellers is to decide what product images to\ndisplay on online shopping sites. In this paper, we propose and validate a\nnovel metric, k-value, to quantify the information richness of an image set,\nand we further investigate its effect on consumers' purchase decisions. We\nleverage patch-level embeddings from Vision Transformers (ViT) and apply\nk-means clustering to identify distinct visual features, defining k-value as\nthe number of clusters. An online experiment demonstrates that k-value aligns\nwith human-perceived information richness, validating the metric. A simulated\nonline shopping experiment further reveals a significant yet counterintuitive\nresult: while an image set with a higher k-value (richer information) shortens\ndecision time, it paradoxically reduces purchase propensity. Our findings\nilluminate the complex relationship between visual information richness and\nconsumer behavior, providing sellers a quantifiable tool for image selection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u56fe\u50cf\u96c6\u4fe1\u606f\u4e30\u5bcc\u5ea6\u7684\u65b0\u6307\u6807k-value\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5bf9\u6d88\u8d39\u8005\u8d2d\u4e70\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "motivation": "\u7535\u5546\u5356\u5bb6\u5e38\u9762\u4e34\u9009\u62e9\u4ea7\u54c1\u56fe\u7247\u7684\u6311\u6218\uff0c\u9700\u8981\u91cf\u5316\u56fe\u7247\u96c6\u7684\u4fe1\u606f\u4e30\u5bcc\u5ea6\u5bf9\u8d2d\u4e70\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528Vision Transformers\u7684\u5d4c\u5165\u548ck-means\u805a\u7c7b\u5b9a\u4e49k-value\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u8d2d\u7269\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "k-value\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u4fe1\u606f\u4e30\u5bcc\u5ea6\u4e00\u81f4\uff0c\u4f46\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u56fe\u7247\u96c6\u867d\u7f29\u77ed\u51b3\u7b56\u65f6\u95f4\u5374\u964d\u4f4e\u8d2d\u4e70\u503e\u5411\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9\u4fe1\u606f\u4e30\u5bcc\u5ea6\u4e0e\u6d88\u8d39\u8005\u884c\u4e3a\u7684\u590d\u6742\u5173\u7cfb\uff0c\u4e3a\u5356\u5bb6\u63d0\u4f9b\u4e86\u91cf\u5316\u5de5\u5177\u3002"}}
{"id": "2508.04634", "pdf": "https://arxiv.org/pdf/2508.04634", "abs": "https://arxiv.org/abs/2508.04634", "authors": ["Mohammed Almutairi", "Charles Chiang", "Haoze Guo", "Matthew Belcher", "Nandini Banerjee", "Maria Milkowski", "Svitlana Volkova", "Daniel Nguyen", "Tim Weninger", "Michael Yankoski", "Trenton W. Ford", "Diego Gomez-Zara"], "title": "VirtLab: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations", "categories": ["cs.HC"], "comment": "5 pages, 2 figures, UIST 2025", "summary": "Simulating how team members collaborate within complex environments using\nAgentic AI is a promising approach to explore hypotheses grounded in social\nscience theories and study team behaviors. We introduce VirtLab, a\nuser-friendly, customizable, multi-agent, and scalable team simulation system\nthat enables testing teams with LLM-based agents in spatial and temporal\nsettings. This system addresses the current frameworks' design and technical\nlimitations that do not consider flexible simulation scenarios and spatial\nsettings. VirtLab contains a simulation engine and a web interface that enables\nboth technical and non-technical users to formulate, run, and analyze team\nsimulations without programming. We demonstrate the system's utility by\ncomparing ground truth data with simulated scenarios.", "AI": {"tldr": "VirtLab\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u56e2\u961f\u6a21\u62df\u7cfb\u7edf\uff0c\u7528\u4e8e\u7814\u7a76\u590d\u6742\u73af\u5883\u4e2d\u7684\u56e2\u961f\u534f\u4f5c\u884c\u4e3a\uff0c\u652f\u6301\u5b9a\u5236\u5316\u548c\u5927\u89c4\u6a21\u6a21\u62df\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u7075\u6d3b\u6027\u548c\u7a7a\u95f4\u8bbe\u7f6e\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u7814\u7a76\u56e2\u961f\u534f\u4f5c\u884c\u4e3a\u9700\u8981\u7075\u6d3b\u4e14\u53ef\u5b9a\u5236\u7684\u6a21\u62df\u5de5\u5177\uff0c\u800c\u73b0\u6709\u6846\u67b6\u5728\u8bbe\u8ba1\u548c\u6280\u672f\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u4e86VirtLab\u7cfb\u7edf\uff0c\u5305\u542b\u6a21\u62df\u5f15\u64ce\u548c\u7f51\u7edc\u754c\u9762\uff0c\u652f\u6301\u975e\u6280\u672f\u7528\u6237\u8fdb\u884c\u56e2\u961f\u6a21\u62df\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7\u5bf9\u6bd4\u771f\u5b9e\u6570\u636e\u4e0e\u6a21\u62df\u573a\u666f\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "VirtLab\u4e3a\u7814\u7a76\u56e2\u961f\u534f\u4f5c\u884c\u4e3a\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6613\u7528\u7684\u5de5\u5177\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u6280\u672f\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.04667", "pdf": "https://arxiv.org/pdf/2508.04667", "abs": "https://arxiv.org/abs/2508.04667", "authors": ["Natalia Echeverry", "Arun Lekshmi Narayanan"], "title": "How are CS students using resources and AI tools for coding tasks?", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "A survey of 26 CS students reveals that AI coding assistants are mainly used\nfor writing code (second to online searches) while AI chatbots are the top\nresource for debugging. Participants with different coding experience prefer\nonline help over direct human help from peers and instructors.", "AI": {"tldr": "\u9488\u5bf926\u540d\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u7684\u8c03\u67e5\u663e\u793a\uff0cAI\u7f16\u7a0b\u52a9\u624b\u4e3b\u8981\u7528\u4e8e\u5199\u4ee3\u7801\uff08\u4ec5\u6b21\u4e8e\u5728\u7ebf\u641c\u7d22\uff09\uff0c\u800cAI\u804a\u5929\u673a\u5668\u4eba\u662f\u8c03\u8bd5\u7684\u9996\u9009\u8d44\u6e90\u3002\u4e0d\u540c\u7f16\u7a0b\u7ecf\u9a8c\u7684\u5b66\u751f\u66f4\u503e\u5411\u4e8e\u5728\u7ebf\u5e2e\u52a9\u800c\u975e\u76f4\u63a5\u5bfb\u6c42\u540c\u884c\u6216\u6559\u5e08\u7684\u5e2e\u52a9\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5b66\u751f\u5728\u7f16\u7a0b\u8fc7\u7a0b\u4e2d\u5bf9\u4e0d\u540c\u8d44\u6e90\u7684\u504f\u597d\uff0c\u4ee5\u53caAI\u5de5\u5177\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u5bf926\u540d\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u5206\u6790\u4ed6\u4eec\u5728\u7f16\u7a0b\u548c\u8c03\u8bd5\u4efb\u52a1\u4e2d\u5bf9AI\u5de5\u5177\u548c\u5176\u4ed6\u8d44\u6e90\u7684\u504f\u597d\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cAI\u7f16\u7a0b\u52a9\u624b\u4e3b\u8981\u7528\u4e8e\u5199\u4ee3\u7801\uff0c\u800cAI\u804a\u5929\u673a\u5668\u4eba\u662f\u8c03\u8bd5\u7684\u4e3b\u8981\u5de5\u5177\uff1b\u5b66\u751f\u666e\u904d\u66f4\u503e\u5411\u4e8e\u4f7f\u7528\u5728\u7ebf\u8d44\u6e90\u800c\u975e\u76f4\u63a5\u5411\u4ed6\u4eba\u6c42\u52a9\u3002", "conclusion": "AI\u5de5\u5177\u5728\u7f16\u7a0b\u5b66\u4e60\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u5c24\u5176\u662f\u5728\u5199\u4ee3\u7801\u548c\u8c03\u8bd5\u4efb\u52a1\u4e2d\u3002\u5b66\u751f\u7684\u8d44\u6e90\u504f\u597d\u8868\u660e\u4ed6\u4eec\u66f4\u4f9d\u8d56\u5728\u7ebf\u548cAI\u5de5\u5177\u800c\u975e\u4f20\u7edf\u7684\u4eba\u9645\u4e92\u52a8\u3002"}}
{"id": "2508.04679", "pdf": "https://arxiv.org/pdf/2508.04679", "abs": "https://arxiv.org/abs/2508.04679", "authors": ["Amit Kumar Das", "Klaus Mueller"], "title": "MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models", "categories": ["cs.HC"], "comment": "11 pages, 6 figures. Accepted at IEEE VIS: Visualization & Visual\n  Analytics 2025 conference, November 2-7, 2025, Vienna, Austria", "summary": "Misleading visualizations pose a significant challenge to accurate data\ninterpretation. While recent research has explored the use of Large Language\nModels (LLMs) for detecting such misinformation, practical tools that also\nsupport explanation and correction remain limited. We present MisVisFix, an\ninteractive dashboard that leverages both Claude and GPT models to support the\nfull workflow of detecting, explaining, and correcting misleading\nvisualizations. MisVisFix correctly identifies 96% of visualization issues and\naddresses all 74 known visualization misinformation types, classifying them as\nmajor, minor, or potential concerns. It provides detailed explanations,\nactionable suggestions, and automatically generates corrected charts. An\ninteractive chat interface allows users to ask about specific chart elements or\nrequest modifications. The dashboard adapts to newly emerging misinformation\nstrategies through targeted user interactions. User studies with visualization\nexperts and developers of fact-checking tools show that MisVisFix accurately\nidentifies issues and offers useful suggestions for improvement. By\ntransforming LLM-based detection into an accessible, interactive platform,\nMisVisFix advances visualization literacy and supports more trustworthy data\ncommunication.", "AI": {"tldr": "MisVisFix\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u4eea\u8868\u677f\uff0c\u5229\u7528Claude\u548cGPT\u6a21\u578b\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u7ea0\u6b63\u8bef\u5bfc\u6027\u53ef\u89c6\u5316\uff0c\u51c6\u786e\u7387\u4e3a96%\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u89e3\u91ca\u548c\u81ea\u52a8\u751f\u6210\u4fee\u6b63\u56fe\u8868\u3002", "motivation": "\u5f53\u524d\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u53ef\u89c6\u5316\u8bef\u5bfc\u7684\u7814\u7a76\u867d\u591a\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u7528\u7684\u89e3\u91ca\u548c\u7ea0\u6b63\u5de5\u5177\uff0cMisVisFill\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408Claude\u548cGPT\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u4eea\u8868\u677f\u652f\u6301\u5168\u6d41\u7a0b\u7684\u68c0\u6d4b\u3001\u89e3\u91ca\u548c\u7ea0\u6b63\uff0c\u5305\u62ec\u81ea\u52a8\u751f\u6210\u4fee\u6b63\u56fe\u8868\u548c\u4ea4\u4e92\u5f0f\u804a\u5929\u529f\u80fd\u3002", "result": "MisVisFix\u51c6\u786e\u8bc6\u522b96%\u7684\u53ef\u89c6\u5316\u95ee\u9898\uff0c\u6db5\u76d674\u79cd\u5df2\u77e5\u8bef\u5bfc\u7c7b\u578b\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u89e3\u91ca\u548c\u4fee\u6b63\u5efa\u8bae\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\u5176\u6548\u679c\u663e\u8457\u3002", "conclusion": "MisVisFix\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5e73\u53f0\u63d0\u5347\u53ef\u89c6\u5316\u7d20\u517b\uff0c\u652f\u6301\u66f4\u53ef\u4fe1\u7684\u6570\u636e\u4f20\u64ad\uff0c\u4e3aLLM\u68c0\u6d4b\u5de5\u5177\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.03698", "pdf": "https://arxiv.org/pdf/2508.03698", "abs": "https://arxiv.org/abs/2508.03698", "authors": ["Se Won Oh", "Hyuntae Jeong", "Seungeun Chung", "Jeong Mook Lim", "Kyoung Ju Noh", "Sunkyung Lee", "Gyuwon Jung"], "title": "Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024", "categories": ["eess.SP", "cs.HC", "cs.LG"], "comment": "This work is intended for submission to an IEEE conference. The\n  content is also relevant to the cs.HC category", "summary": "Improving human health and well-being requires an accurate and effective\nunderstanding of an individual's physical and mental state throughout daily\nlife. To support this goal, we utilized smartphones, smartwatches, and sleep\nsensors to collect data passively and continuously for 24 hours a day, with\nminimal interference to participants' usual behavior, enabling us to gather\nquantitative data on daily behaviors and sleep activities across multiple days.\nAdditionally, we gathered subjective self-reports of participants' fatigue,\nstress, and sleep quality through surveys conducted immediately before and\nafter sleep. This comprehensive lifelog dataset is expected to provide a\nfoundational resource for exploring meaningful insights into human daily life\nand lifestyle patterns, and a portion of the data has been anonymized and made\npublicly available for further research. In this paper, we introduce the ETRI\nLifelog Dataset 2024, detailing its structure and presenting potential\napplications, such as using machine learning models to predict sleep quality\nand stress.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u667a\u80fd\u8bbe\u5907\u88ab\u52a8\u6536\u96c6\u7528\u6237\u5168\u5929\u884c\u4e3a\u4e0e\u7761\u7720\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u4e3b\u89c2\u62a5\u544a\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u751f\u6d3b\u65e5\u5fd7\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a2\u7d22\u4eba\u7c7b\u65e5\u5e38\u751f\u6d3b\u7684\u6a21\u5f0f\u3002", "motivation": "\u4e3a\u4e86\u66f4\u51c6\u786e\u5730\u7406\u89e3\u4e2a\u4f53\u7684\u751f\u7406\u548c\u5fc3\u7406\u72b6\u6001\uff0c\u4ee5\u6539\u5584\u5065\u5eb7\u548c\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u667a\u80fd\u8bbe\u5907\uff08\u5982\u667a\u80fd\u624b\u673a\u3001\u667a\u80fd\u624b\u8868\u548c\u7761\u7720\u4f20\u611f\u5668\uff0924\u5c0f\u65f6\u88ab\u52a8\u6536\u96c6\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u7761\u7720\u524d\u540e\u7684\u4e3b\u89c2\u8c03\u67e5\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u5b9a\u91cf\u884c\u4e3a\u548c\u4e3b\u89c2\u62a5\u544a\u7684\u7efc\u5408\u6027\u6570\u636e\u96c6\uff0c\u90e8\u5206\u6570\u636e\u5df2\u516c\u5f00\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "ETRI Lifelog Dataset 2024\u4e3a\u7814\u7a76\u4eba\u7c7b\u751f\u6d3b\u65b9\u5f0f\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u5e76\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u9884\u6d4b\u7761\u7720\u8d28\u91cf\u548c\u538b\u529b\u65b9\u9762\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.03715", "pdf": "https://arxiv.org/pdf/2508.03715", "abs": "https://arxiv.org/abs/2508.03715", "authors": ["Bertram Fuchs", "Mehdi Ejtehadi", "Ana Cisnal", "J\u00fcrgen Pannek", "Anke Scheel-Sailer", "Robert Riener", "Inge Eriks-Hoogland", "Diego Paez-Granados"], "title": "Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Autonomic Dysreflexia (AD) is a potentially life-threatening condition\ncharacterized by sudden, severe blood pressure (BP) spikes in individuals with\nspinal cord injury (SCI). Early, accurate detection is essential to prevent\ncardiovascular complications, yet current monitoring methods are either\ninvasive or rely on subjective symptom reporting, limiting applicability in\ndaily file. This study presents a non-invasive, explainable machine learning\nframework for detecting AD using multimodal wearable sensors. Data were\ncollected from 27 individuals with chronic SCI during urodynamic studies,\nincluding electrocardiography (ECG), photoplethysmography (PPG), bioimpedance\n(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three\ncommercial devices. Objective AD labels were derived from synchronized\ncuff-based BP measurements. Following signal preprocessing and feature\nextraction, BorutaSHAP was used for robust feature selection, and SHAP values\nfor explainability. We trained modality- and device-specific weak learners and\naggregated them using a stacked ensemble meta-model. Cross-validation was\nstratified by participants to ensure generalizability. HR- and ECG-derived\nfeatures were identified as the most informative, particularly those capturing\nrhythm morphology and variability. The Nearest Centroid ensemble yielded the\nhighest performance (Macro F1 = 0.77+/-0.03), significantly outperforming\nbaseline models. Among modalities, HR achieved the highest area under the curve\n(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature\nfeatures contributed less to overall accuracy, consistent with missing data and\nlow specificity. The model proved robust to sensor dropout and aligned well\nwith clinical AD events. These results represent an important step toward\npersonalized, real-time monitoring for individuals with SCI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7684\u975e\u4fb5\u5165\u5f0f\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u810a\u9ad3\u635f\u4f24\u60a3\u8005\u7684\u81ea\u4e3b\u795e\u7ecf\u529f\u80fd\u969c\u788d\uff0c\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u76ee\u524d\u68c0\u6d4b\u81ea\u4e3b\u795e\u7ecf\u529f\u80fd\u969c\u788d\u7684\u65b9\u6cd5\u591a\u4e3a\u4fb5\u5165\u6027\u6216\u4f9d\u8d56\u4e3b\u89c2\u62a5\u544a\uff0c\u9650\u5236\u65e5\u5e38\u5e94\u7528\uff0c\u9700\u5f00\u53d1\u975e\u4fb5\u5165\u4e14\u51c6\u786e\u7684\u76d1\u6d4b\u6280\u672f\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff0c\u7ed3\u5408BorutaSHAP\u7279\u5f81\u9009\u62e9\u548c\u5806\u53e0\u96c6\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6027\u80fd\u3002", "result": "HR\u548cECG\u7279\u5f81\u6700\u6709\u6548\uff0c\u96c6\u6210\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08Macro F1=0.77\uff09\uff0cHR\u6a21\u6001\u7684AUC\u8fbe0.93\uff0c\u6a21\u578b\u5bf9\u4f20\u611f\u5668\u4e22\u5931\u9c81\u68d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u810a\u9ad3\u635f\u4f24\u60a3\u8005\u7684\u4e2a\u6027\u5316\u5b9e\u65f6\u76d1\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.03990", "pdf": "https://arxiv.org/pdf/2508.03990", "abs": "https://arxiv.org/abs/2508.03990", "authors": ["Bohan Jiang", "Dawei Li", "Zhen Tan", "Chengshuai Zhao", "Huan Liu"], "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "9 pages, 4 figures, 3 tables", "summary": "Well-being encompasses mental, physical, and social dimensions essential to\npersonal growth and informed life decisions. As individuals increasingly\nconsult Large Language Models (LLMs) to understand well-being, a key challenge\nemerges: Can LLMs generate explanations that are not only accurate but also\ntailored to diverse audiences? High-quality explanations require both factual\ncorrectness and the ability to meet the expectations of users with varying\nexpertise. In this work, we construct a large-scale dataset comprising 43,880\nexplanations of 2,194 well-being concepts, generated by ten diverse LLMs. We\nintroduce a principle-guided LLM-as-a-judge evaluation framework, employing\ndual judges to assess explanation quality. Furthermore, we show that\nfine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO) can significantly enhance the quality of\ngenerated explanations. Our results reveal: (1) The proposed LLM judges align\nwell with human evaluations; (2) explanation quality varies significantly\nacross models, audiences, and categories; and (3) DPO- and SFT-finetuned models\noutperform their larger counterparts, demonstrating the effectiveness of\npreference-based learning for specialized explanation tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u4e3a\u4e0d\u540c\u53d7\u4f17\u751f\u6210\u5173\u4e8e\u5e78\u798f\u611f\u7684\u5b9a\u5236\u5316\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u8d8a\u6765\u8d8a\u591a\u7684\u4eba\u4f7f\u7528LLMs\u83b7\u53d6\u5e78\u798f\u611f\u76f8\u5173\u7684\u5efa\u8bae\uff0c\u5982\u4f55\u751f\u6210\u65e2\u51c6\u786e\u53c8\u80fd\u6ee1\u8db3\u4e0d\u540c\u53d7\u4f17\u9700\u6c42\u7684\u89e3\u91ca\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u7814\u7a76\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b43,880\u6761\u89e3\u91ca\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5341\u79cd\u4e0d\u540c\u7684LLMs\u751f\u6210\u89e3\u91ca\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u5219\u7684LLM-as-a-judge\u8bc4\u4f30\u6846\u67b6\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5bf9\u5f00\u6e90LLM\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1) LLM\u6cd5\u5b98\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u4e00\u81f4\uff1b2) \u89e3\u91ca\u8d28\u91cf\u56e0\u6a21\u578b\u3001\u53d7\u4f17\u548c\u7c7b\u522b\u800c\u5f02\uff1b3) \u7ecf\u8fc7DPO\u548cSFT\u4f18\u5316\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u7684\u6a21\u578b\u3002", "conclusion": "\u504f\u597d\u5b66\u4e60\uff08\u5982DPO\uff09\u5728\u4e13\u4e1a\u89e3\u91ca\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aLLMs\u751f\u6210\u9ad8\u8d28\u91cf\u5b9a\u5236\u5316\u89e3\u91ca\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.04337", "pdf": "https://arxiv.org/pdf/2508.04337", "abs": "https://arxiv.org/abs/2508.04337", "authors": ["Francisco Bola\u00f1os", "Angelo Salatino", "Francesco Osborne", "Enrico Motta"], "title": "Modelling and Classifying the Components of a Literature Review", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "comment": null, "summary": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6807\u6ce8\u6846\u67b6\u7528\u4e8e\u79d1\u5b66\u6587\u732e\u4fee\u8f9e\u89d2\u8272\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u5c55\u793a\u4e86LLMs\u5728\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u4fee\u8f9e\u89d2\u8272\u6807\u6ce8\u80fd\u63d0\u5347AI\u5bf9\u6587\u732e\u7684\u5206\u6790\u8d28\u91cf\uff0c\u5e76\u4e3a\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u732e\u7efc\u8ff0\u63d0\u4f9b\u652f\u6301\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u76f8\u5173\u6807\u6ce8\u6846\u67b6\u548c\u5927\u89c4\u6a21\u6807\u6ce8\u7b56\u7565\u3002", "method": "1)\u8bbe\u8ba1\u65b0\u6807\u6ce8\u6846\u67b6\u652f\u6301\u6587\u732e\u7efc\u8ff0\u751f\u6210\uff1b2)\u8bc4\u4f3037\u79cdLLMs\u5728\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u96f6\u6837\u672c\u5b66\u4e60\u548c\u5fae\u8c03\u65b9\u6cd5\uff1b3)\u5f15\u5165Sci-Sentence\u591a\u5b66\u79d1\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u5fae\u8c03\u540e\u7684LLMs\u8868\u73b0\u4f18\u5f02\uff08F1>96%\uff09\uff1bGPT-4o\u6700\u4f18\uff0c\u4f46\u8f7b\u91cf\u5f00\u6e90\u6a21\u578b\u4e5f\u6709\u7ade\u4e89\u529b\uff1bLLM\u751f\u6210\u7684\u534a\u5408\u6210\u6570\u636e\u80fd\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u4fee\u8f9e\u89d2\u8272\u5206\u7c7b\u4efb\u52a1\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u5f00\u6e90\u6a21\u578b\u548c\u534a\u5408\u6210\u6570\u636e\u7684\u7ed3\u5408\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04412", "pdf": "https://arxiv.org/pdf/2508.04412", "abs": "https://arxiv.org/abs/2508.04412", "authors": ["Thassilo M. Schiepanski", "Nicholas Pi\u00ebl"], "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faD2Snap\uff0c\u4e00\u79cd\u9996\u6b21\u63d0\u51fa\u7684DOM\u964d\u91c7\u6837\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8eDOM\u7684\u7f51\u9875\u4ee3\u7406\u4e2d\u72b6\u6001\u5e8f\u5217\u5316\u95ee\u9898\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0cD2Snap\u5728\u964d\u91c7\u6837DOM\u5feb\u7167\u4e0a\u7684\u6210\u529f\u7387\uff0867%\uff09\u63a5\u8fd1\u57fa\u4e8eGUI\u5feb\u7167\u7684\u57fa\u7ebf\uff0865%\uff09\uff0c\u4e14\u8f93\u5165\u4ee4\u724c\u91cf\u7ea7\u76f8\u540c\u3002\u6700\u4f73\u914d\u7f6e\u751a\u81f3\u6bd4\u57fa\u7ebf\u9ad8\u51fa8%\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eGUI\u5feb\u7167\u7684\u7f51\u9875\u4ee3\u7406\u867d\u7136\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\uff0c\u4f46\u56fe\u50cf\u5904\u7406\u5bf9LLM\u7684\u80fd\u529b\u8981\u6c42\u8f83\u9ad8\uff0c\u800cDOM\u5feb\u7167\u867d\u7ed3\u6784\u66f4\u4f18\uff0c\u4f46\u56e0\u4ee4\u724c\u91cf\u95ee\u9898\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u964d\u91c7\u6837\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faD2Snap\u7b97\u6cd5\uff0c\u5bf9DOM\u8fdb\u884c\u964d\u91c7\u6837\uff0c\u4ee5\u51cf\u5c11\u8f93\u5165\u4ee4\u724c\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952eUI\u7ed3\u6784\u4fe1\u606f\u3002\u5b9e\u9a8c\u57fa\u4e8eGPT-4o\u540e\u7aef\uff0c\u5e76\u5728Online-Mind2Web\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6548\u679c\u3002", "result": "\u964d\u91c7\u6837\u540e\u7684DOM\u5feb\u7167\u6210\u529f\u7387\uff0867%\uff09\u4e0eGUI\u5feb\u7167\u57fa\u7ebf\uff0865%\uff09\u76f8\u5f53\uff0c\u4e14\u8f93\u5165\u4ee4\u724c\u91cf\u7ea7\u76f8\u540c\u3002\u6700\u4f73\u914d\u7f6e\u6bd4\u57fa\u7ebf\u9ad88%\uff0c\u5e76\u8bc1\u5b9eDOM\u5c42\u6b21\u7ed3\u6784\u5bf9LLM\u6709\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "D2Snap\u901a\u8fc7\u964d\u91c7\u6837\u89e3\u51b3\u4e86DOM\u5feb\u7167\u7684\u4ee4\u724c\u91cf\u95ee\u9898\uff0c\u6027\u80fd\u63a5\u8fd1\u751a\u81f3\u4f18\u4e8eGUI\u5feb\u7167\uff0c\u4e3a\u7f51\u9875\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04651", "pdf": "https://arxiv.org/pdf/2508.04651", "abs": "https://arxiv.org/abs/2508.04651", "authors": ["Lyria Team", "Antoine Caillon", "Brian McWilliams", "Cassie Tarakajian", "Ian Simon", "Ilaria Manco", "Jesse Engel", "Noah Constant", "Pen Li", "Timo I. Denk", "Alberto Lalama", "Andrea Agostinelli", "Anna Huang", "Ethan Manilow", "George Brower", "Hakan Erdogan", "Heidi Lei", "Itai Rolnick", "Ivan Grishchenko", "Manu Orsini", "Matej Kastelic", "Mauricio Zuluaga", "Mauro Verzetti", "Michael Dooley", "Ondrej Skopek", "Rafael Ferrer", "Zal\u00e1n Borsos", "\u00c4aron van den Oord", "Douglas Eck", "Eli Collins", "Jason Baldridge", "Tom Hume", "Chris Donahue", "Kehang Han", "Adam Roberts"], "title": "Live Music Models", "categories": ["cs.SD", "cs.HC", "cs.LG"], "comment": null, "summary": "We introduce a new class of generative models for music called live music\nmodels that produce a continuous stream of music in real-time with synchronized\nuser control. We release Magenta RealTime, an open-weights live music model\nthat can be steered using text or audio prompts to control acoustic style. On\nautomatic metrics of music quality, Magenta RealTime outperforms other\nopen-weights music generation models, despite using fewer parameters and\noffering first-of-its-kind live generation capabilities. We also release Lyria\nRealTime, an API-based model with extended controls, offering access to our\nmost powerful model with wide prompt coverage. These models demonstrate a new\nparadigm for AI-assisted music creation that emphasizes human-in-the-loop\ninteraction for live music performance.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u5b9e\u65f6\u97f3\u4e50\u751f\u6210\u6a21\u578bMagenta RealTime\u548cLyria RealTime\uff0c\u652f\u6301\u901a\u8fc7\u6587\u672c\u6216\u97f3\u9891\u63d0\u793a\u63a7\u5236\u97f3\u4e50\u98ce\u683c\uff0c\u5e76\u5728\u97f3\u4e50\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5176\u4ed6\u516c\u5f00\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u65b0\u7684AI\u8f85\u52a9\u97f3\u4e50\u521b\u4f5c\u8303\u5f0f\uff0c\u5f3a\u8c03\u5b9e\u65f6\u97f3\u4e50\u8868\u6f14\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u5b9e\u65f6\u97f3\u4e50\u751f\u6210\u6a21\u578b\uff0cMagenta\uff08\u5f00\u6e90\uff09\u548cLyria\uff08API\uff09\uff0c\u652f\u6301\u6587\u672c\u6216\u97f3\u9891\u63d0\u793a\u63a7\u5236\u98ce\u683c\u3002", "result": "Magenta RealTime\u5728\u97f3\u4e50\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\uff1bLyria RealTime\u63d0\u4f9b\u66f4\u5e7f\u6cdb\u7684\u63d0\u793a\u63a7\u5236\u3002", "conclusion": "\u8fd9\u4e9b\u6a21\u578b\u5c55\u793a\u4e86AI\u8f85\u52a9\u97f3\u4e50\u521b\u4f5c\u7684\u65b0\u65b9\u5411\uff0c\u7a81\u51fa\u4e86\u5b9e\u65f6\u4ea4\u4e92\u7684\u91cd\u8981\u6027\u3002"}}
