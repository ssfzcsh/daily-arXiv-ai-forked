<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 17]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [RedCoder: Automated Multi-Turn Red Teaming for Code LLMs](https://arxiv.org/abs/2507.22063)
*Wenjie Jacky Mo,Qin Liu,Xiaofei Wen,Dongwon Jung,Hadi Askari,Wenxuan Zhou,Zhe Zhao,Muhao Chen*

Main category: cs.SE

TL;DR: RedCoder是一种红队代理，通过多轮对话引发漏洞代码生成，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有红队方法依赖人工且忽略多轮交互的实际情况，RedCoder旨在填补这一空白。

Method: 通过多代理模拟生成原型对话和攻击策略，并微调LLM构建RedCoder。

Result: RedCoder在多轮对话中成功诱导漏洞生成，表现优于现有方法。

Conclusion: RedCoder提供了一种可扩展的工具，用于评估代码生成系统的安全性。

Abstract: Large Language Models (LLMs) for code generation (i.e., Code LLMs) have
demonstrated impressive capabilities in AI-assisted software development and
testing. However, recent studies have shown that these models are prone to
generating vulnerable or even malicious code under adversarial settings.
Existing red-teaming approaches rely on extensive human effort, limiting their
scalability and practicality, and generally overlook the interactive nature of
real-world AI-assisted programming, which often unfolds over multiple turns. To
bridge these gaps, we present RedCoder, a red-teaming agent that engages victim
models in multi-turn conversation to elicit vulnerable code. The pipeline to
construct RedCoder begins with a multi-agent gaming process that simulates
adversarial interactions, yielding a set of prototype conversations and an
arsenal of reusable attack strategies. We then fine-tune an LLM on these
prototype conversations to serve as the backbone of RedCoder. Once deployed,
RedCoder autonomously engages Code LLMs in multi-turn conversations,
dynamically retrieving relevant strategies from the arsenal to steer the
dialogue toward vulnerability-inducing outputs. Experiments across multiple
Code LLMs show that our approach outperforms prior single-turn and multi-turn
red-team methods in inducing vulnerabilities in code generation, offering a
scalable and effective tool for evaluating the security boundaries of modern
code-generation systems.

</details>


### [2] [Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone](https://arxiv.org/abs/2507.22064)
*Michael Cohoon,Debbie Furman*

Main category: cs.SE

TL;DR: 本文概述了一个专注于软件测试的团队在机器学习（ML）中的实践旅程，介绍了他们遵循类似CRISP-DM的ML工作流程，并总结了该流程的关键步骤。


<details>
  <summary>Details</summary>
Motivation: 描述一个团队如何通过标准化工作流程将机器学习技术成功应用于软件测试项目。

Method: 采用了类似CRISP-DM的ML工作流程，包括数据收集、清理、特征工程、数据集划分、模型选择、训练、测试及性能评估。

Result: 通过遵循该工作流程，团队能够有效地将ML技术应用于其项目。

Conclusion: 标准化ML工作流程对任何项目的机器学习应用都具有普适性和实用性。

Abstract: This paper details the machine learning (ML) journey of a group of people
focused on software testing. It tells the story of how this group progressed
through a ML workflow (similar to the CRISP-DM process). This workflow consists
of the following steps and can be used by anyone applying ML techniques to a
project: gather the data; clean the data; perform feature engineering on the
data; splitting the data into two sets, one for training and one for testing;
choosing a machine learning model; training the model; testing the model and
evaluating the model performance. By following this workflow, anyone can
effectively apply ML to any project that they are doing.

</details>


### [3] [Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models](https://arxiv.org/abs/2507.22065)
*Xiaotao Feng,Xiaogang Zhu,Kun Hu,Jincheng Wang,Yingjie Cao,Guang Gong,Jianfeng Pan*

Main category: cs.SE

TL;DR: RandLuzz利用大型语言模型（LLMs）减少引导性模糊测试中的随机性，通过生成可达种子和特定于bug的变异器，显著提升了测试效率。


<details>
  <summary>Details</summary>
Motivation: 模糊测试中的随机性虽然有助于发现bug，但降低了效率。引导性模糊测试虽减少了随机性，但仍面临挑战。

Method: RandLuzz结合LLMs生成可达种子和特定bug的变异器，利用其推理和代码生成能力优化测试过程。

Result: RandLuzz将测试速度提升2.1×至4.8×，部分bug在60秒内被发现。

Conclusion: LLMs有效减少了模糊测试的随机性，显著提高了效率和bug暴露速度。

Abstract: Fuzzing is highly effective in detecting bugs due to the key contribution of
randomness. However, randomness significantly reduces the efficiency of
fuzzing, causing it to cost days or weeks to expose bugs. Even though directed
fuzzing reduces randomness by guiding fuzzing towards target buggy locations,
the dilemma of randomness still challenges directed fuzzers. Two critical
components, which are seeds and mutators, contain randomness and are closely
tied to the conditions required for triggering bugs. Therefore, to address the
challenge of randomness, we propose to use large language models (LLMs) to
remove the randomness in seeds and reduce the randomness in mutators. With
their strong reasoning and code generation capabilities, LLMs can be used to
generate reachable seeds that target pre-determined locations and to construct
bug-specific mutators tailored for specific bugs. We propose RandLuzz, which
integrates LLMs and directed fuzzing, to improve the quality of seeds and
mutators, resulting in efficient bug exposure. RandLuzz analyzes function call
chain or functionality to guide LLMs in generating reachable seeds. To
construct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,
obtaining information such as bug causes and mutation suggestions, which
further help generate code that performs bug-specific mutations. We evaluate
RandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,
Beacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers
achieve an average speedup ranging from 2.1$\times$ to 4.8$\times$ compared to
using widely-used initial seeds. Additionally, when evaluated on individual
bugs, RandLuzz achieves up to a 2.7$\times$ speedup compared to the
second-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60
seconds.

</details>


### [4] [CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation](https://arxiv.org/abs/2507.22066)
*Dylan Manuel,Paul Rad*

Main category: cs.SE

TL;DR: CodableLLM是一个Python框架，用于自动生成高质量代码理解与生成数据集，特别是针对反编译二进制与原代码的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前生成高质量代码数据集（尤其是反编译二进制与源代码对齐）存在挑战，CodableLLM旨在解决这一问题。

Method: CodableLLM通过将反编译函数映射到对应源代码函数，支持多语言，并与现有反编译器和解析器集成。

Result: 结果表明，CodableLLM在数据集生成中表现高效且鲁棒，优于现有工具。

Conclusion: CodableLLM为面向代码的大语言模型提供了一种高效的数据集生成方法。

Abstract: The generation of large, high-quality datasets for code understanding and
generation remains a significant challenge, particularly when aligning
decompiled binaries with their original source code. To address this, we
present CodableLLM, a Python framework designed to automate the creation and
curation of datasets by mapping decompiled functions to their corresponding
source functions. This process enhances the alignment between decompiled and
source code representations, facilitating the development of large language
models (LLMs) capable of understanding and generating code across multiple
abstraction levels. CodableLLM supports multiple programming languages and
integrates with existing decompilers and parsers to streamline dataset
generation. This paper presents the design and implementation of CodableLLM,
evaluates its performance in dataset creation, and compares it to existing
tools in the field. The results demonstrate that CodableLLM offers a robust and
efficient solution for generating datasets tailored for code-focused LLMS.

</details>


### [5] [Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach](https://arxiv.org/abs/2507.22070)
*Y. Du*

Main category: cs.SE

TL;DR: 本文提出了一种基于Python元类系统和生产日志统计分析的测试数据生成框架，显著提升了企业级协议缓冲区（protobuf）复杂嵌套结构的测试效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 大规模企业系统使用协议缓冲区时，传统测试数据生成方法难以应对复杂的嵌套和图结构，亟需高效解决方案。

Method: 结合自动模式内省、统计值分布分析和递归下降算法，利用Python元类系统动态增强类型，从生产日志中提取真实值域。

Result: 在三个实际系统中，测试数据准备时间减少95％，测试覆盖率提升80％，支持15层嵌套结构并每秒生成超10万测试用例。

Conclusion: 该框架有效解决了复杂协议缓冲区结构的测试数据生成问题，显著提升了测试效率和覆盖率。

Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present
significant challenges for performance testing, particularly when targeting
intermediate business interfaces with complex nested data structures.
Traditional test data generation approaches are inadequate for handling the
intricate hierarchical and graph-like structures inherent in enterprise
protobuf schemas. This paper presents a novel test data generation framework
that leverages Python's metaclass system for dynamic type enhancement and
statistical analysis of production logs for realistic value domain extraction.
Our approach combines automatic schema introspection, statistical value
distribution analysis, and recursive descent algorithms for handling deeply
nested structures. Experimental evaluation on three real-world enterprise
systems demonstrates up to 95\% reduction in test data preparation time and
80\% improvement in test coverage compared to existing approaches. The
framework successfully handles protobuf structures with up to 15 levels of
nesting and generates comprehensive test suites containing over 100,000 test
cases within seconds.

</details>


### [6] [Analyzing and Evaluating the Behavior of Git Diff and Merge](https://arxiv.org/abs/2507.22071)
*Niels Glodny*

Main category: cs.SE

TL;DR: 该论文探讨了Git中diff和merge算法的功能及其潜在问题，揭示了Git中的一些意外行为。


<details>
  <summary>Details</summary>
Motivation: Git的协作算法（尤其是diff和merge）虽然被广泛使用，但其实现细节和潜在问题并未被充分理解。

Method: 通过分析Git的主要功能，包括diff计算、merge操作及其在复杂操作中的应用，揭示了算法的不透明行为。

Result: 发现多种意外行为，例如histogram diff算法的极端情况、默认merge策略的指数时间复杂度问题、merge与rebase的非交换性等。

Conclusion: Git的diff和merge算法存在未明确文档化的行为，可能影响其可靠性和适用性。

Abstract: Despite being widely used, the algorithms that enable collaboration with Git
are not well understood. The diff and merge algorithms are particularly
interesting, as they could be applied in other contexts. In this thesis, I
document the main functionalities of Git: how diffs are computed, how they are
used to run merges, and how merges enable more complex operations. In the
process, I show multiple unexpected behaviors in Git, including the following:
The histogram diff algorithm has pathological cases where a single-line change
can cause the entire rest of the file to be marked as changed. The default
merge strategy (ort) can result in merges requiring exponential time in the
number of commits in the history. Merges and rebases are not commutative, and
even when merges do not result in a conflict, the result is not specified but
depends on the diff algorithm used. And finally, sometimes when two sides of a
merge add different lines at the same position, the result is not a conflict,
but a merge containing both changes after each other, in arbitrary order.

</details>


### [7] [CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback](https://arxiv.org/abs/2507.22080)
*Qiushi Sun,Jinyang Gong,Lei Li,Qipeng Guo,Fei Yuan*

Main category: cs.SE

TL;DR: CodeEvo提出了一种通过两个LLM代理（编码者和评审者）协同交互合成高质量代码数据的方法，显著提升了代码生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 手动标注代码数据昂贵且规模有限，现有方法缺乏严格的数据验证，导致合成数据质量不高。

Method: 采用两个LLM代理（编码者和评审者）迭代交互合成代码数据，结合编译确定性和代理生成反馈的混合机制。

Result: 实验表明，基于CodeEvo数据微调的模型在各类代码生成基准测试中显著优于基线。

Conclusion: CodeEvo为高效合成代码数据提供了新视角，验证了其在实际应用中的有效性。

Abstract: Acquiring high-quality instruction-code pairs is essential for training Large
Language Models (LLMs) for code generation. Manually curated data is expensive
and inherently limited in scale, motivating the development of code-centric
synthesis methods. Yet, current approaches either focus on augmenting existing
code or rely on predefined heuristics, both lacking rigorous data validation,
which results in synthetic data that is ungrounded, repetitive, or overly
simplistic. Inspired by collaborative programming practices, we propose
CodeEvo, a framework that synthesizes code data through iterative interactions
between two LLM agents: a Coder, which generates candidate code and test cases
based on given instructions, and a Reviewer, which guides the synthesis process
by producing new instructions and feedback. We further introduce a hybrid
feedback mechanism that combines compiler determinism with the generative
flexibility of agents, enabling automatic quality control throughout synthesis.
Extensive experiments demonstrate that models fine-tuned on CodeEvo data
significantly outperform established baselines across code generation
benchmarks with various difficulties. In-depth analyses further provide
insights from multiple perspectives into effective code-centric data synthesis.

</details>


### [8] [TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories](https://arxiv.org/abs/2507.22086)
*Honghua Dong,Jiacheng Yang,Xun Deng,Yuhe Jiang,Gennady Pekhimenko,Fan Long,Xujie Si*

Main category: cs.SE

TL;DR: TypyBench是一个新的基准测试工具，用于评估大型语言模型（LLMs）在Python代码库中的类型推断能力，提出两种新指标（TypeSim和TypeCheck），并揭示了LLMs在复杂类型推断上的不足。


<details>
  <summary>Details</summary>
Motivation: 动态语言（如Python）的类型推断是软件工程中的挑战，目前LLMs在此领域的表现尚未充分研究。

Method: TypyBench通过50个高质量Python代码库评估LLMs，引入TypeSim（类型相似性）和TypeCheck（类型一致性）指标。

Result: LLMs在TypeSim表现尚可，但在复杂嵌套类型和代码库一致性方面存在显著问题。

Conclusion: 未来研究应关注代码库级别的类型一致性，而非仅改进类型相似性；TypyBench为此提供了基础。

Abstract: Type inference for dynamic languages like Python is a persistent challenge in
software engineering. While large language models (LLMs) have shown promise in
code understanding, their type inference capabilities remain underexplored. We
introduce TypyBench, a benchmark designed to evaluate LLMs' type inference
across entire Python repositories. TypyBench features two novel metrics:
TypeSim, which captures nuanced semantic relationships between predicted and
ground truth types, and TypeCheck, which assesses type consistency across
codebases. Our evaluation of various LLMs on a curated dataset of 50
high-quality Python repositories reveals that, although LLMs achieve decent
TypeSim scores, they struggle with complex nested types and exhibit significant
type consistency errors. These findings suggest that future research should
shift focus from improving type similarity to addressing repository-level
consistency. TypyBench provides a foundation for this new direction, offering
insights into model performance across different type complexities and usage
contexts. Our code and data are available at
https://github.com/typybench/typybench.

</details>


### [9] [BOOP: Write Right Code](https://arxiv.org/abs/2507.22085)
*Vaani Goenka,Aalok D. Thakkar*

Main category: cs.SE

TL;DR: BOOP框架通过强制四个阶段（规范、算法开发、实现和证明）来改善编程教学，减少试错，提升学生算法理解和问题分解能力。


<details>
  <summary>Details</summary>
Motivation: 传统编程教学中，学生倾向于试错而非系统思考，AI工具可能提供有语法但概念错误的代码，BOOP旨在解决这一问题。

Method: BOOP框架包括四个阶段：规范、语言无关算法开发、实现和正确性证明，通过VS Code扩展强制执行。

Result: 初步评估显示学生算法推理能力提升，试错减少，教师观察到基础技能更强，但部分学生认为格式繁琐。

Conclusion: BOOP通过结构化方法成功转移编程学习的重点到理解代码正确性，但需权衡其复杂度与接受度。

Abstract: Novice programmers frequently adopt a syntax-specific and test-case-driven
approach, writing code first and adjusting until programs compile and test
cases pass, rather than developing correct solutions through systematic
reasoning. AI coding tools exacerbate this challenge by providing syntactically
correct but conceptually flawed solutions. In this paper, we introduce BOOP
(Blueprint, Operations, OCaml, Proof), a structured framework requiring four
mandatory phases: formal specification, language-agnostic algorithm
development, implementation, and correctness proof. This shifts focus from
``making code work'' to understanding why code is correct.
  BOOP was implemented at our institution using a VS Code extension and
preprocessor that enforces constraints and identifies counterproductive
patterns. Initial evaluation shows improved algorithmic reasoning and reduced
trial-and-error debugging. Students reported better edge case understanding and
problem decomposition, though some initially found the format verbose.
Instructors observed stronger foundational skills compared to traditional
approaches.

</details>


### [10] [Secure coding for web applications: Frameworks, challenges, and the role of LLMs](https://arxiv.org/abs/2507.22223)
*Kiana Kiashemshaki,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.SE

TL;DR: 本文综述了安全编码实践在主要框架和领域中的应用，比较了结构化框架，并将威胁与OWASP Top 10对齐。此外，探讨了大型语言模型（LLMs）在评估和推荐安全代码中的作用，并通过案例研究展示了其在现实开发中的实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管对安全编码的认知已广泛推广，但由于组织、教育和技术障碍，实际采用仍不一致。本文旨在为研究人员、开发者和教育者提供将安全编码整合到现实开发过程中的实用见解。

Method: 提供了安全编码实践的综合综述，比较了结构化框架，并与OWASP Top 10威胁分类对齐。通过可重复的案例研究，展示了LLMs在评估和推荐安全代码中的应用。

Result: 提出了安全编码框架的比较和威胁分类，并通过LLMs的案例研究验证了其在现实开发中的潜力。

Conclusion: 本文为研究人员、开发者和教育者提供了安全编码实践的实用指南，强调了LLMs在促进安全编码中的新兴作用。

Abstract: Secure coding is a critical yet often overlooked practice in software
development. Despite extensive awareness efforts, real-world adoption remains
inconsistent due to organizational, educational, and technical barriers. This
paper provides a comprehensive review of secure coding practices across major
frameworks and domains, including web development, DevSecOps, and cloud
security. It introduces a structured framework comparison and categorizes
threats aligned with the OWASP Top 10. Additionally, we explore the rising role
of Large Language Models (LLMs) in evaluating and recommending secure code,
presenting a reproducible case study across four major vulnerability types.
This paper offers practical insights for researchers, developers, and educators
on integrating secure coding into real-world development processes.

</details>


### [11] [From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications](https://arxiv.org/abs/2507.22324)
*Cameron S. Movassaghi,Amanda Momenzadeh,Jesse G. Meyer*

Main category: cs.SE

TL;DR: 论文提出利用科学论文中丰富的方法描述作为大型语言模型（LLM）的独立规范，实现按需代码生成，从而替代人工维护的软件库。实验表明，当前LLM能可靠地复现软件包功能，性能与传统库无异。


<details>
  <summary>Details</summary>
Motivation: 减少软件包的维护成本，包括依赖管理、错误修复和版本控制。

Method: 通过对顶级模型（GPT-o4-mini-high、Gemini Pro 2.5、Claude Sonnet 4）进行基准测试，要求其根据原始论文实现核心算法。

Result: 当前LLM能够可靠地复现软件包功能，性能与传统库无显著差异。

Conclusion: 按需代码生成可能成为未来趋势，减少人工维护工作量，科学论文可作为自动化实现分析的上下文。

Abstract: Maintaining software packages imposes significant costs due to dependency
management, bug fixes, and versioning. We show that rich method descriptions in
scientific publications can serve as standalone specifications for modern large
language models (LLMs), enabling on-demand code generation that could supplant
human-maintained libraries. We benchmark state-of-the-art models
(GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with
implementing a diverse set of core algorithms drawn from original publications.
Our results demonstrate that current LLMs can reliably reproduce package
functionality with performance indistinguishable from conventional libraries.
These findings foreshadow a paradigm shift toward flexible, on-demand code
generation and away from static, human-maintained packages, which will result
in reduced maintenance overhead by leveraging published articles as sufficient
context for the automated implementation of analytical workflows.

</details>


### [12] [AutoCodeSherpa: Symbolic Explanations in AI Coding Agents](https://arxiv.org/abs/2507.22414)
*Sungmin Kang,Haifeng Ruan,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: LLM代理利用外部工具和程序分析技术自动修复软件缺陷，并通过符号公式提供解释，以提升补丁质量和开发者认知。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理的补丁建议缺乏证据和高置信度，本研究旨在通过提供符号公式形式的解释（输入条件、感染条件和输出条件）来改进这一点。

Method: 提出一个工作流，代理用符号公式（如基于属性的测试和程序内部符号表达式）解释Bug，这些解释可执行且支持自动化修复。

Result: 解释形式不仅帮助开发者理解代理输出，还能在完全自动化环境中用于验证补丁，并提升其他LLM修复技术的效果。

Conclusion: 符号公式驱动的解释能增强LLM代理在软件工程中的可信度和自动化能力，为未来代理技术的发展提供支持。

Abstract: Large Language Model (LLM) agents autonomously use external tools on top of
one or more LLMs to accomplish specific tasks. Lately LLM agents for software
engineering tasks have become popular. These agents can benefit from the use of
program analysis tools working on program representations. This is demonstrated
by existing agentic AI solutions such as AutoCodeRover or SpecRover which
perform automated program repair. Specifically the goal of these works is to
use program analysis to improve the patch quality. These agents are currently
being used to automatically fix static analysis issues from the widely used
SonarQube static analyzer.
  Nevertheless, for the agents to be deployed in a production environment,
agents need to suggest software artifacts, such as patches, with evidence and
with high confidence. In this work, we provide a workflow where an agent
provides explanations of the bug in the form of symbolic formulae. The
explanations are in the form of input conditions, infection conditions and
output conditions, implemented as property based tests (PBT) and
program-internal symbolic expressions. These can help in human developer
cognition of the agent outputs as well as in achieving completely automated
agentic workflows for software. The human developer can benefit from the input
condition, represented as a PBT, to generate various concrete inputs showing a
given issue. Furthermore, since the PBTs are executable, our explanations are
executable as well. We can thus also use the explanations in a completely
automated issue resolution environment for accepting or rejecting the patches
that are suggested by patching agents such as AutoCodeRover. Finally, as
agentic AI approaches continue to develop, the program analysis driven
explanations can be provided to other LLM-based repair techniques such as
Agentless to improve their output.

</details>


### [13] [Ensemble Fuzzing with Dynamic Resource Scheduling and Multidimensional Seed Evaluation](https://arxiv.org/abs/2507.22442)
*Yukai Zhao,Shaohua Wang,Jue Wang,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: Legion是一种新型的集成模糊测试框架，通过动态资源调度和多维种子评估策略，显著提高了测试效率和漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的集成模糊测试技术在资源调度和性能评估方面存在局限性，导致资源浪费。

Method: Legion采用基于置信上界算法的资源调度算法，并结合多维种子评估策略，实现更高效的测试。

Result: Legion在测试中表现优异，检测到20个漏洞，其中包括5个未知漏洞和3个CVE漏洞。

Conclusion: Legion通过创新算法和策略，显著提升了集成模糊测试的效果，解决了资源浪费问题。

Abstract: Fuzzing is widely used for detecting bugs and vulnerabilities, with various
techniques proposed to enhance its effectiveness. To combine the advantages of
multiple technologies, researchers proposed ensemble fuzzing, which integrates
multiple base fuzzers. Despite promising results, state-of-the-art ensemble
fuzzing techniques face limitations in resource scheduling and performance
evaluation, leading to unnecessary resource waste. In this paper, we propose
Legion, a novel ensemble fuzzing framework that dynamically schedules resources
during the ensemble fuzzing campaign. We designed a novel resource scheduling
algorithm based on the upper confidence bound algorithm to reduce the resource
consumption of ineffective base fuzzers. Additionally, we introduce a
multidimensional seed evaluation strategy, which considers multiple metrics to
achieve more comprehensive fine-grained performance evaluation. We implemented
Legion as a prototype tool and evaluated its effectiveness on Google's
fuzzer-test-suite as well as real-world open-source projects. Results show that
Legion outperforms existing state-of-the-art base fuzzers and ensemble fuzzing
techniques, detecting 20 vulnerabilities in real-world open-source
projects-five previously unknown and three classified as CVEs.

</details>


### [14] [Inside madupite: Technical Design and Performance](https://arxiv.org/abs/2507.22538)
*Matilde Gargiani,Robin Sieber,Philip Pawlowsky,John Lygeros*

Main category: cs.SE

TL;DR: Madupite是一种高性能求解器，专为大规模无限时域折扣马尔可夫决策过程（MDP）设计，适用于有限状态和动作空间。其创新在于能够高效计算大规模MDP的精确解，支持分布式计算，并可定制算法以加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的MDP求解器难以处理大规模问题，特别是在内存限制或接近无折扣设置的情况下。Madupite旨在填补这一空白，提供一种可扩展且灵活的解决方案。

Method: Madupite基于数学优化方法，支持完全分布式计算，可以充分利用高性能计算集群的资源。用户还可根据问题结构定制算法。

Result: Madupite在流行病学和控制等领域的应用中表现出卓越的扩展性和效率，能够处理超出笔记本电脑内存容量的MDP问题，并在高折扣因子设置下加速收敛。

Conclusion: Madupite在大规模MDP求解中取得了显著进展，提供了无与伦比的可扩展性和灵活性，适用于复杂且资源密集型的决策问题。

Abstract: In this work, we introduce and benchmark madupite, a newly proposed
high-performance solver designed for large-scale discounted infinite-horizon
Markov decision processes with finite state and action spaces. After a brief
overview of the class of mathematical optimization methods on which madupite
relies, we provide details on implementation choices, technical design and
deployment. We then demonstrate its scalability and efficiency by showcasing
its performance on the solution of Markov decision processes arising from
different application areas, including epidemiology and classical control.
Madupite sets a new standard as, to the best of our knowledge, it is the only
solver capable of efficiently computing exact solutions for large-scale Markov
decision processes, even when these exceed the memory capacity of modern
laptops and operate in near-undiscounted settings. This is possible as madupite
can work in a fully distributed manner and therefore leverage the memory
storage and computation capabilities of modern high-performance computing
clusters. This key feature enables the solver to efficiently handle problems of
medium to large size in an exact manner instead of necessarily resorting to
function approximations. Moreover, madupite is unique in allowing users to
customize the solution algorithm to better exploit the specific structure of
their problem, significantly accelerating convergence especially in
large-discount factor settings. Overall, madupite represents a significant
advancement, offering unmatched scalability and flexibility in solving
large-scale Markov decision processes.

</details>


### [15] [RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment](https://arxiv.org/abs/2507.22580)
*Marcos Fuster-Pena,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.SE

TL;DR: RePaCA是一个新的静态APCA技术，利用大型语言模型(LLM)来分析和评估程序修复补丁的正确性，通过强化学习提升模型的推理能力，达到83.1%的准确率和84.8%的F1分数，表现优异且具有较好的解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的静态APCA技术在可靠性、灵活性和透明度方面存在不足，为解决这些问题，论文提出了RePaCA。

Method: RePaCA通过LLM对代码片段进行分析，生成Chain of Thought推理补丁的正确性，并采用强化学习优化模型。

Result: 在Defects4J测试集上，RePaCA表现优异，准确率达到83.1%，F1分数84.8%，泛化能力也优于现有技术。

Conclusion: 研究表明，经过优化的LLM能显著提升静态APCA的准确性、泛化能力和解释性。

Abstract: Automated Program Repair (APR) seeks to automatically correct software bugs
without requiring human intervention. However, existing tools tend to generate
patches that satisfy test cases without fixing the underlying bug, those are
known as overfitting patches. To address this issue, Automated Patch
Correctness Assessment (APCA) attempts to identify overfitting patches
generated by APR tools. It can be solved as a static approach, meaning that no
additional information is needed beyond the original and fixed code snippets.
Current static techniques often struggle with reliability, flexibility and
transparency. To address these issues, we introduce RePaCA, a novel static APCA
technique that leverages Large Language Models (LLMs) specialized in thinking
tasks. Our model is prompted with both buggy and fixed code snippets and guided
to generate a Chain of Thought that analyses code differences, reasons about
how the patch addresses the root cause, and ultimately provides a binary
classification: correct or overfitting. To enhance these reasoning capabilities
for the APCA task specifically, the LLM is finetuned using Reinforcement
Learning with the Group Relative Policy Optimization algorithm. When evaluated
on a standard Defects4J-derived test, our approach achieves state-of-the-art
performance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model
demonstrates superior generalization capabilities when trained on different
datasets, outperforming the leading technique. This reasoning capability also
provides enhanced explainability for the patch assessment. These findings
underscore the considerable promise of finetuned, reasoning LLMs to advance
static APCA by enhancing accuracy, generalization, and explainability.

</details>


### [16] [Metamorphic Testing of Deep Code Models: A Systematic Literature Review](https://arxiv.org/abs/2507.22610)
*Ali Asgari,Milan de Koning,Pouria Derakhshanfar,Annibale Panichella*

Main category: cs.SE

TL;DR: 这篇论文系统回顾了针对深度代码模型的蜕变测试方法，分析了45篇相关文献，总结了当前的研究现状、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 深度代码模型在软件工程中表现出色，但其鲁棒性仍存疑，尤其在对抗条件下表现不稳定。蜕变测试成为评估其鲁棒性的重要方法。

Method: 通过系统文献综述，研究45篇相关论文，分析用于评估鲁棒性的蜕变变换、技术和评估方法。

Result: 总结了当前研究中的常用模型、编程任务、数据集、目标语言和评估指标，并指出了关键挑战。

Conclusion: 论文为深度代码模型的鲁棒性研究提供了全面视角，指出了未来研究的重点方向。

Abstract: Large language models and deep learning models designed for code intelligence
have revolutionized the software engineering field due to their ability to
perform various code-related tasks. These models can process source code and
software artifacts with high accuracy in tasks such as code completion, defect
detection, and code summarization; therefore, they can potentially become an
integral part of modern software engineering practices. Despite these
capabilities, robustness remains a critical quality attribute for deep-code
models as they may produce different results under varied and adversarial
conditions (e.g., variable renaming). Metamorphic testing has become a widely
used approach to evaluate models' robustness by applying semantic-preserving
transformations to input programs and analyzing the stability of model outputs.
While prior research has explored testing deep learning models, this systematic
literature review focuses specifically on metamorphic testing for deep code
models. By studying 45 primary papers, we analyze the transformations,
techniques, and evaluation methods used to assess robustness. Our review
summarizes the current landscape, identifying frequently evaluated models,
programming tasks, datasets, target languages, and evaluation metrics, and
highlights key challenges and future directions for advancing the field.

</details>


### [17] [A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2507.22659)
*Sabrina Kaniewski,Fabian Schmidt,Markus Enzweiler,Michael Menth,Tobias Heer*

Main category: cs.SE

TL;DR: 论文通过系统性文献综述，分析了227项关于基于大语言模型（LLM）的软件漏洞检测研究，提出了细粒度的分类方法，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程中的广泛应用引发了对其在漏洞检测中应用的兴趣，但由于研究分散且难以比较，需要一个系统性的综述来梳理现状。

Method: 进行了系统性文献综述，分析了227篇2020年1月至2025年6月发表的研究，分类标准包括任务定义、输入表示、系统架构和适应技术，并分析了数据集特征。

Result: 提出了漏洞检测方法的细粒度分类，识别了关键局限性，并明确了未来研究方向。

Conclusion: 通过系统性综述，为研究者提供了结构化概览，增强了研究的透明度和可重复性，并公开了所有研究资料。

Abstract: The increasing adoption of Large Language Models (LLMs) in software
engineering has sparked interest in their use for software vulnerability
detection. However, the rapid development of this field has resulted in a
fragmented research landscape, with diverse studies that are difficult to
compare due to differences in, e.g., system designs and dataset usage. This
fragmentation makes it difficult to obtain a clear overview of the
state-of-the-art or compare and categorize studies meaningfully. In this work,
we present a comprehensive systematic literature review (SLR) of LLM-based
software vulnerability detection. We analyze 227 studies published between
January 2020 and June 2025, categorizing them by task formulation, input
representation, system architecture, and adaptation techniques. Further, we
analyze the datasets used, including their characteristics, vulnerability
coverage, and diversity. We present a fine-grained taxonomy of vulnerability
detection approaches, identify key limitations, and outline actionable future
research opportunities. By providing a structured overview of the field, this
review improves transparency and serves as a practical guide for researchers
and practitioners aiming to conduct more comparable and reproducible research.
We publicly release all artifacts and maintain a living repository of LLM-based
software vulnerability detection studies.

</details>


### [18] [RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots](https://arxiv.org/abs/2507.22664)
*Mashal Afzal Memon,Gianluca Filippone,Gian Luca Scoccia,Marco Autili,Paola Inverardi*

Main category: cs.SE

TL;DR: 论文提出RobEthiChor方法，使自主系统能通过基于伦理的协商纳入用户伦理偏好。


<details>
  <summary>Details</summary>
Motivation: 自主系统缺乏用户伦理偏好个性化的能力，影响信任和行为一致性。

Method: 提出RobEthiChor及其实现RobEthiChor-Ros，通过伦理协商框架设计。

Result: 系统在73%场景下达成协议，平均协商时间0.67秒，展现可行性和扩展性。

Conclusion: RobEthiChor有效实现基于伦理的协商，提升自主系统适应性。

Abstract: The presence of autonomous systems is growing at a fast pace and it is
impacting many aspects of our lives. Designed to learn and act independently,
these systems operate and perform decision-making without human intervention.
However, they lack the ability to incorporate users' ethical preferences, which
are unique for each individual in society and are required to personalize the
decision-making processes. This reduces user trust and prevents autonomous
systems from behaving according to the moral beliefs of their end-users. When
multiple systems interact with differing ethical preferences, they must
negotiate to reach an agreement that satisfies the ethical beliefs of all the
parties involved and adjust their behavior consequently. To address this
challenge, this paper proposes RobEthiChor, an approach that enables autonomous
systems to incorporate user ethical preferences and contextual factors into
their decision-making through ethics-based negotiation. RobEthiChor features a
domain-agnostic reference architecture for designing autonomous systems capable
of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an
implementation of RobEthiChor within the Robot Operating System (ROS), which
can be deployed on robots to provide them with ethics-based negotiation
capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real
robots and ran scenarios where a pair of robots negotiate upon resource
contention. Experimental results demonstrate the feasibility and effectiveness
of the system in realizing ethics-based negotiation. RobEthiChor allowed robots
to reach an agreement in more than 73\% of the scenarios with an acceptable
negotiation time (0.67s on average). Experiments also demonstrate that the
negotiation approach implemented in RobEthiChor is scalable.

</details>


### [19] [The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach](https://arxiv.org/abs/2507.22800)
*Rui Ren*

Main category: cs.SE

TL;DR: 论文提出KnowledgeMind，一种基于蒙特卡洛树搜索和知识库奖励机制的LLM多智能体系统，用于标准化服务推理，显著提升根因定位准确性并减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 微服务场景下系统可靠性挑战加剧，传统RCA方法在定位速度和准确性上不足，LLM虽提供新思路但存在幻觉和上下文窗口限制问题。

Method: 提出KnowledgeMind，结合蒙特卡洛树搜索、知识库奖励机制和标准化服务推理，有效减少上下文窗口负担并抑制幻觉。

Result: 相比SOTA方法，根因定位准确率提升49.29%到128.35%，且上下文窗口需求降至十分之一。

Conclusion: KnowledgeMind通过多智能体协作和奖励机制，显著优化了LLM在RCA中的性能。

Abstract: In real-world scenarios, due to the highly decoupled and flexible nature of
microservices, it poses greater challenges to system reliability. The more
frequent occurrence of incidents has created a demand for Root Cause
Analysis(RCA) methods that enable rapid identification and recovery of
incidents. Large language model (LLM) provides a new path for quickly locating
and recovering from incidents by leveraging their powerful generalization
ability combined with expert experience. Current LLM for RCA frameworks are
based on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM
and the propagation nature of anomalies often lead to incorrect localization
results. Moreover, the massive amount of anomalous information generated in
large, complex systems presents a huge challenge for the context window length
of LLMs. To address these challenges, we propose KnowledgeMind, an innovative
LLM multi-agent system based on Monte Carlo Tree Search and a knowledge base
reward mechanism for standardized service-by-service reasoning. Compared to
State-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration
approach significantly reduces the burden on the maximum context window length,
requiring only one-tenth of its size. Additionally, by incorporating a
rule-based real-time reward mechanism, our method effectively mitigates
hallucinations during the inference process. Compared to the SOTA LLM for RCA
framework, our method achieves a 49.29% to 128.35% improvement in root cause
localization accuracy.

</details>


### [20] [Repair-R1: Better Test Before Repair](https://arxiv.org/abs/2507.22853)
*Haichuan Hu,Xiaochen Xie,Quanjun Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种名为Repair-R1的自动化程序修复方法，通过将测试用例引入模型训练阶段并在修复前生成测试，显著提高了修复成功率、测试生成成功率和测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的APR方法通常在推理阶段才使用测试用例，忽略了训练阶段的测试用例贡献和修复前的测试生成潜力，导致修复效果受限。

Method: Repair-R1在训练阶段引入测试用例，要求模型首先生成可区分缺陷行为的测试用例，再进行修复。采用强化学习优化测试生成与修复过程。

Result: 在四个广泛使用的基准测试上，Repair-R1的修复成功率提升2.68%至48.29%，测试生成成功率提升16.38%至53.28%，测试覆盖率提升0.78%至53.96%。

Conclusion: Repair-R1通过结合测试用例和修复优化，显著提升了自动化程序修复的效果，具有实际应用价值。

Abstract: APR (Automated Program Repair) aims to automatically locate program defects,
generate patches and validate the repairs. Existing techniques for APR are
often combined with LLMs (Large Language Models), which leverages the
code-related knowledge of LLMs to improve repair effectiveness. Current
LLM-based APR methods typically utilize test cases only during the inference
stage, adopting an iterative approach that performs repair first and validates
it through test execution afterward. This conventional paradigm neglects two
important aspects: the potential contribution of test cases in the training
phase, and the possibility of leveraging testing prior to repair. To address
this, we propose Repair-R1, which introduces test cases into the model's
training phase and shifts test generation to precede repair. The model is
required to first generate discriminative test cases that can distinguish
defective behaviors, and then perform repair based on these tests. This enables
the model to better locate defects and understand the underlying causes of
defects, thereby improving repair effectiveness. We implement Repair-R1 with
three different backbone models, using RL (reinforcement learning) to
co-optimize test generation and bug repair. Experimental results on four widely
adopted benchmarks demonstrate the superiority of Repair-R1. Specially,
compared to vanilla models, Repair-R1 improves repair success rate by 2.68\% to
48.29\%, test generation success rate by 16.38\% to 53.28\%, and test coverage
by 0.78\% to 53.96\%. We publish the code and weights at
https://github.com/Tomsawyerhu/APR-RL and
https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.

</details>


### [21] [Tracking research software outputs in the UK](https://arxiv.org/abs/2507.22871)
*Domhnall Carlin,Austen Rainer*

Main category: cs.SE

TL;DR: 研究探讨了英国学术机构如何存储和注册研究软件作为独特的研究成果，发现软件分享率低，URL问题多，主要依赖GitHub等公共代码库。


<details>
  <summary>Details</summary>
Motivation: 研究软件在开放科学中的重要性日益凸显，但如何将其作为研究产物进行确认和共享仍存在问题。

Method: 通过分析UKRI的GtR元数据，调查英国公共资助的研究软件的存储和注册情况。

Result: 软件作为研究成果的数量较少，45%的URL缺失或错误，GitHub是主要托管平台。

Conclusion: 缺乏共享可能使研究软件沦为短期工具，影响科学的长期发展。

Abstract: Research software is crucial in the research process and the growth of Open
Science underscores the importance of accessing research artifacts, like data
and code, raising traceability challenges among outputs. While it is a clear
principle that research code, along with other essential outputs, should be
recognised as artifacts of the research process, the how of this principle
remains variable. This study examines where UK academic institutions store and
register software as a unique research output, searching the UKRI's Gateway to
Research (GtR) metadata for publicly funded research software in the UK. The
quantity of software reported as research outcomes remains low in proportion to
other categories. Artifact sharing appears low, with one-quarter of the
reported software having no links and 45% having either a missing or erroneous
URL. Of the valid URLs, we find the single largest category is Public
Commercial Code Repository, with GitHub being the host of 18% of all publicly
funded research software listed. These observations are contrasted with past
findings from 2023 and finally, we discuss the lack of artifact sharing in UK
research, with resulting implications for the maintenance and evolution of
research software. Without dissemination, research software risks demotion to a
transient artifact, useful only to meet short term research demands but
ultimately lost to the broader enterprise of science.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [22] [A Compute-Matched Re-Evaluation of TroVE on MATH](https://arxiv.org/abs/2507.22069)
*Tobias Sesterhenn,Ian Berlot-Attwell,Janis Zenkner,Christian Bartelt*

Main category: cs.PL

TL;DR: TroVE声称通过工具重用提升LLMs在MATH基准上的表现，但研究发现其优势主要源于更高的计算预算，而非工具箱方法本身。


<details>
  <summary>Details</summary>
Motivation: 重新评估TroVECode生成模型在MATH基准上的表现，验证其声称的工具重用优势是否成立。

Method: 分析TroVE的三个模式（直接生成代码、创建工具、重用工具），匹配计算预算进行比较。

Result: TroVE的改进主要源于更高的计算预算，而非工具箱方法本身。计算匹配后，性能提升仅为1%。

Conclusion: 工具箱方法对MATH基准的显著改进不成立。

Abstract: Reusing established theorems and formulas is central to mathematical problem
solving, serving as essential building blocks for tackling increasingly complex
challenges. Recent work, TroVE, argues that code-generating Large Language
Models (LLMs) can benefit similarly on the MATH benchmark by inducing and
reusing higher-level toolboxes. By allocating computational budget across an
ensemble of three modes -- directly generating code, creating tools, and
reusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only
performs direct generation. However, recent analysis (Berlot-Attwell et al.,
2024) casts doubt on these gains, noting that the tools created are often
trivial or rarely reused, suggesting that improvements may stem from
self-consistency or self-correction. In this work, we re-evaluate TroVE on
MATH, analyze the impact of each of its modes, and show that its benefit does
not come from these mechanisms, but simply from a higher computational budget
spent for TroVE compared to PRIMITIVE. To this end, we also perform a small
correction in the original implementation of TroVE's selection mechanism,
boosting TroVE's performance on MATH by 3\% in accuracy. After matching for
compute, the benefit of TroVE reduces to a marginal improvement of 1\%,
suggesting that this toolbox approach does not provide a significant benefit on
MATH.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [23] [Dissecting RISC-V Performance: Practical PMU Profiling and Hardware-Agnostic Roofline Analysis on Emerging Platforms](https://arxiv.org/abs/2507.22451)
*Alexander Batashev*

Main category: cs.PF

TL;DR: 论文提出了一种实用的方法，用于在RISC-V系统中提取性能洞察，包括规避硬件缺陷和基于编译器的Roofline分析工具。


<details>
  <summary>Details</summary>
Motivation: RISC-V架构在嵌入式和高性能领域应用广泛，但性能优化面临工具碎片化、硬件特性不成熟等问题。

Method: 通过规避硬件缺陷的解决方案，以及利用LLVM工具链实现不依赖硬件PMU的Roofline分析工具。

Result: 开发了一个开源工具链，整合了PMU数据校正和编译器驱动的Roofline构建。

Conclusion: 该方法为RISC-V系统性能优化提供了可行方案，尤其是硬件条件受限或不可靠时。

Abstract: As RISC-V architectures proliferate across embedded and high-performance
domains, developers face persistent challenges in performance optimization due
to fragmented tooling, immature hardware features, and platform-specific
defects. This paper delivers a pragmatic methodology for extracting actionable
performance insights on RISC-V systems, even under constrained or unreliable
hardware conditions. We present a workaround to circumvent hardware bugs in one
of the popular RISC-V implementations, enabling robust event sampling. For
memory-compute bottleneck analysis, we introduce compiler-driven Roofline
tooling that operates without hardware PMU dependencies, leveraging LLVM-based
instrumentation to derive operational intensity and throughput metrics directly
from application IR. Our open source toolchain automates these workarounds,
unifying PMU data correction and compiler-guided Roofline construction into a
single workflow.

</details>


### [24] [Ecoscape: Fault Tolerance Benchmark for Adaptive Remediation Strategies in Real-Time Edge ML](https://arxiv.org/abs/2507.22702)
*Hendrik Reiter,Ahmad Rzgar Hamid,Florian Schlösser,Mikkel Baun Kjærgaard,Wilhelm Hasselbring*

Main category: cs.PF

TL;DR: Ecoscape 是一个用于评估故障修复策略性能的综合基准测试工具，通过混沌工程模拟真实故障场景，为不同修复方法提供量化评分，特别适用于边缘机器学习推理。


<details>
  <summary>Details</summary>
Motivation: 边缘计算在实时数据处理（如目标识别）中具有低延迟和带宽优势，但边缘环境易受故障影响。目前缺乏公平比较修复策略的方法。

Method: 基于Kubernetes实现自动修复组件，引入Ecoscape基准测试工具，利用混沌工程技术模拟故障，并提供量化评分框架。

Result: Ecoscape能够在虚拟边缘测试环境中有效评估和优化故障修复策略，支持特定领域的服务级别目标。

Conclusion: Ecoscape为边缘机器学习推理系统提供了无需物理测试环境的故障容忍优化框架。

Abstract: Edge computing offers significant advantages for realtime data processing
tasks, such as object recognition, by reducing network latency and bandwidth
usage. However, edge environments are susceptible to various types of fault. A
remediator is an automated software component designed to adjust the
configuration parameters of a software service dynamically. Its primary
function is to maintain the services operational state within predefined
Service Level Objectives by applying corrective actions in response to
deviations from these objectives. Remediators can be implemented based on the
Kubernetes container orchestration tool by implementing remediation strategies
such as rescheduling or adjusting application parameters. However, currently,
there is no method to compare these remediation strategies fairly. This paper
introduces Ecoscape, a comprehensive benchmark designed to evaluate the
performance of remediation strategies in fault-prone environments. Using Chaos
Engineering techniques, Ecoscape simulates realistic fault scenarios and
provides a quantifiable score to assess the efficacy of different remediation
approaches. In addition, it is configurable to support domain-specific Service
Level Objectives. We demonstrate the capabilities of Ecoscape in edge machine
learning inference, offering a clear framework to optimize fault tolerance in
these systems without needing a physical edge testbed.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [25] [OpenRASE: Service Function Chain Emulation](https://arxiv.org/abs/2507.22131)
*Theviyanthan Krishnamohan,Paul Harvey*

Main category: cs.NI

TL;DR: 论文提出了一种基于Mininet和Docker的SFC仿真工具OpenRASE，用于动态环境下资源分配算法的研究，解决了现有工具的不准确性和不灵活性问题。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估SFC资源分配算法的工具存在不准确、低仿真度、不可扩展或不灵活等问题，需要一个更有效的工具来支持动态网络条件的算法验证。

Method: 设计了基于Mininet和Docker的仿真工具OpenRASE，用于动态环境中SFC资源分配算法的探索，支持实际CPU使用和延迟的测量。

Result: 通过实验评估，验证了OpenRASE在动态网络条件下对资源分配算法（如在线遗传算法）的有效性和实用性。

Conclusion: OpenRASE为SFC资源分配算法提供了一个高仿真、灵活且可扩展的评估平台，适用于动态网络环境的研究。

Abstract: Service Function Chains (SFCs) are one of the key enablers in providing
programmable computer networks, paving the way for network autonomy. However,
this also introduces new challenges, such as resource allocation and
optimisation related to their operation, requiring new algorithms to address
these challenges. Various tools have been used in the literature to evaluate
these algorithms. However, these tools suffer from inaccuracy, low fidelity,
unscalability, inflexibility, or additional code requirements. This paper
introduces an emulator based on Mininet and Docker for SFCs called OpenRASE.
The goal of OpenRASE is to enable the exploration of resource allocation
algorithms for SFCs in a dynamic setting, allowing real CPU usage and latency
to be measured. We describe the design and implementation of OpenRASE and
discuss its characteristics. We also experimentally evaluate two different
algorithms to address the SFC resource allocation challenge, including an
online Genetic Algorithm, using OpenRASE to show its effectiveness and
practicality for dynamic network conditions.

</details>


### [26] [AdapSCA-PSO: An Adaptive Localization Algorithm with AI-Based Hybrid SCA-PSO for IoT WSNs](https://arxiv.org/abs/2507.22317)
*Ze Zhang,Qian Dong,Wenhan Wang*

Main category: cs.NI

TL;DR: 本文提出了一种用于物联网传感器节点定位的混合元启发式算法，结合了SCA和PSO，并通过自适应切换模块优化性能。


<details>
  <summary>Details</summary>
Motivation: 物联网应用需要精确的传感器节点定位，但现有算法在多样环境中的鲁棒性不足。

Method: 提出了一种结合SCA（全局搜索）和PSO（局部搜索）的混合算法，并设计了自适应切换模块及优化参数。

Result: 仿真结果显示，相比单独PSO和未优化的SCAPSO，该方法显著减少迭代次数并降低定位误差84.97%。

Conclusion: 该混合算法在物联网节点定位中表现出更高的效率和精度。

Abstract: The accurate localization of sensor nodes is a fundamental requirement for
the practical application of the Internet of Things (IoT). To enable robust
localization across diverse environments, this paper proposes a hybrid
meta-heuristic localization algorithm. Specifically, the algorithm integrates
the Sine Cosine Algorithm (SCA), which is effective in global search, with
Particle Swarm Optimization (PSO), which excels at local search. An adaptive
switching module is introduced to dynamically select between the two
algorithms. Furthermore, the initialization, fitness evaluation, and parameter
settings of the algorithm have been specifically redesigned and optimized to
address the characteristics of the node localization problem. Simulation
results across varying numbers of sensor nodes demonstrate that, compared to
standalone PSO and the unoptimized SCAPSO algorithm, the proposed method
significantly reduces the number of required iterations and achieves an average
localization error reduction of 84.97%.

</details>


### [27] [802.11bf Multiband Passive Sensing: Reusing Wi-Fi Signaling for Sensing](https://arxiv.org/abs/2507.22591)
*Pablo Picazo-Martinez,Carlos Barroso-Fernández,Alejandro Calvillo-Fernandez,Milan Groshev,Carlos J. Bernardos,Antonio de la Oliva,Alain Mourad*

Main category: cs.NI

TL;DR: 提出了一种基于IEEE 802.11bf Wi-Fi信号的多频段被动传感系统，结合亚7 GHz和毫米波频段的CSI数据，显著提升室内环境检测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 通过Wi-Fi信号的多频段数据融合，解决被动传感中的准确性和安全性问题，减少对主动传感基础设施的依赖。

Method: 采用新的MILAGRO模型，结合多频段CSI数据，检测人类存在、移动和活动。

Result: 实验结果显示系统准确率达95-100%，在多场景中表现出色，同时提出了安全风险缓解措施。

Conclusion: 这一工作推动了Wi-Fi在被动传感中的应用，为低成本、非侵入式环境监测提供了扩展能力。

Abstract: This paper presents a novel multiband passive sensing system that leverages
IEEE 802.11bf Wi-Fi signals for environmental sensing, focusing on both sub-7
GHz and millimeter-wave (mmWave) bands. By combining Channel State Information
(CSI) from multiple bands, the system enhances accuracy and reliability in
detecting human presence, movement, and activities in indoor environments.
Utilizing a novel model, called MILAGRO, the system demonstrates robust
performance across different scenarios, including monitoring human presence in
workspaces and tracking movement in corridors. Experimental results show high
accuracy (95-100%), with improved performance by integrating multiband data.
The system also addresses key security concerns associated with passive
sensing, proposing measures to mitigate potential risks. This work advances the
use of Wi-Fi for passive sensing by reducing reliance on active sensing
infrastructure and extending the capabilities of low-cost, non-intrusive
environmental monitoring.

</details>


### [28] [Bifröst: Spatial Networking with Bigraphs](https://arxiv.org/abs/2507.22687)
*Josh Millar,Ryan Gibb,Roy Ang,Anil Madhavapeddy,Hamed Haddadi*

Main category: cs.NI

TL;DR: 提出了一种基于bigraphs的统一表示方法，用于协调物理空间，并结合分层代理架构实现分布式空间推理，支持低延迟且私密的空间网络。


<details>
  <summary>Details</summary>
Motivation: 现代网络环境缺乏对物理空间的统一表示，导致空间访问策略等任务脆弱且依赖人工。

Method: 1. 使用bigraphs统一表示空间、社交和通信关系；2. 设计分层代理架构，结合上下文感知执行模型。

Result: 实现了私密、可靠且低延迟的空间网络，支持与代理工作流的安全交互。

Conclusion: 该方法为现代网络环境中的空间推理提供了高效且安全的解决方案。

Abstract: Modern networked environments increasingly rely on spatial reasoning, but
lack a coherent representation for coordinating physical space. Consequently,
tasks such as enforcing spatial access policies remain fragile and manual. We
first propose a unifying representation based on bigraphs, capturing spatial,
social, and communication relationships within a single formalism, with
user-facing tools to generate bigraphs from physical environments. Second, we
present a hierarchical agent architecture for distributed spatial reasoning,
with runtimes for agentic processes to interact the spatial representation, and
a context-aware execution model that scopes reasoning to the smallest viable
subspace. Together, these enable private, reliable, and low-latency spatial
networking that can safely interact with agentic workflows.

</details>


### [29] [OFCnetLLM: Large Language Model for Network Monitoring and Alertness](https://arxiv.org/abs/2507.22711)
*Hong-Jun Yoon,Mariam Kiran,Danial Ebling,Joe Breen*

Main category: cs.NI

TL;DR: 利用大语言模型（LLMs）改进网络监控管理，减少查询和模式分析的局限性，通过多智能体方法开发OFCNetLLM，并在实际网络中展示初步成果。


<details>
  <summary>Details</summary>
Motivation: 网络基础设施的快速演进带来了高效管理、优化和安全的新挑战，传统大型监控数据库成本高昂，AI技术可降低管理成本。

Method: 基于开源LLM模型开发多智能体OFCNetLLM，增强异常检测、自动化根因分析和事件分析。

Result: 展示了OFCNetLLM在OFC会议网络中的实际应用，并提供了早期成果。

Conclusion: LLMs在网络监控管理中有潜力，OFCNetLLM的多智能体方法展示了初步成功，但仍在持续演进。

Abstract: The rapid evolution of network infrastructure is bringing new challenges and
opportunities for efficient network management, optimization, and security.
With very large monitoring databases becoming expensive to explore, the use of
AI and Generative AI can help reduce costs of managing these datasets. This
paper explores the use of Large Language Models (LLMs) to revolutionize network
monitoring management by addressing the limitations of query finding and
pattern analysis. We leverage LLMs to enhance anomaly detection, automate
root-cause analysis, and automate incident analysis to build a well-monitored
network management team using AI. Through a real-world example of developing
our own OFCNetLLM, based on the open-source LLM model, we demonstrate practical
applications of OFCnetLLM in the OFC conference network. Our model is developed
as a multi-agent approach and is still evolving, and we present early results
here.

</details>


### [30] [Morph: ChirpTransformer-based Encoder-decoder Co-design for Reliable LoRa Communication](https://arxiv.org/abs/2507.22851)
*Yidong Ren,Maolin Gan,Chenning Li,Shakhrul Iman Siam,Mi Zhang,Shigang Chen,Zhichao Cao*

Main category: cs.NI

TL;DR: 提出了一种名为Morph的LoRa编码器-解码器协同设计，旨在极低信噪比（SNR）下提升通信可靠性和计算效率，通过模拟超越标准SF-12的扩展因子（SF）配置，并结合深度神经网络（DNN）解码器，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决标准LoRa在极低SNR环境下的通信可靠性不足和计算效率低下的问题，提出了Morph，通过创新设计提升性能兼容现有设备。

Method: 开发了基于SF配置的编码器和DNN解码器，模拟超越SF-12的SF配置，并优化DNN结构以提高解码效率和可靠性。

Result: 在室内和校园规模测试中，Morph在-28.8dB SNR下可靠解码数据，比标准LoRa（SF-12）低6.4dB，且DNN解码器计算效率提升3倍。

Conclusion: Morph通过编码器和DNN解码器的协同设计，显著提升了极低SNR环境下的通信性能和计算效率，具有实际应用价值。

Abstract: In this paper, we propose Morph, a LoRa encoder-decoder co-design to enhance
communication reliability while improving its computation efficiency in
extremely-low signal-to-noise ratio (SNR) situations. The standard LoRa encoder
controls 6 Spreading Factors (SFs) to tradeoff SNR tolerance with data rate.
SF-12 is the maximum SF providing the lowest SNR tolerance on commercial
off-the-shelf (COTS) LoRa nodes. In Morph, we develop an SF-configuration based
encoder to mimic the larger SFs beyond SF-12 while it is compatible with COTS
LoRa nodes. Specifically, we manipulate four SF configurations of a Morph
symbol to encode 2-bit data. Accordingly, we recognize the used SF
configuration of the symbol for data decoding. We leverage a Deep Neural
Network (DNN) decoder to fully capture multi-dimensional features among diverse
SF configurations to maximize the SNR gain. Moreover, we customize the input
size, neural network structure, and training method of the DNN decoder to
improve its efficiency, reliability, and generalizability. We implement Morph
with COTS LoRa nodes and a USRP N210, then evaluate its performance on indoor
and campus-scale testbeds. Results show that we can reliably decode data at
-28.8~dB SNR, which is 6.4~dB lower than the standard LoRa with SF-12 chirps.
In addition, the computation efficiency of our DNN decoder is about 3x higher
than state-of-the-art.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [31] [GestureHYDRA: Semantic Co-speech Gesture Synthesis via Hybrid Modality Diffusion Transformer and Cascaded-Synchronized Retrieval-Augmented Generation](https://arxiv.org/abs/2507.22731)
*Quanwei Yang,Luying Huang,Kaisiyuan Wang,Jiazhi Guan,Shengyi He,Fengguo Li,Hang Zhou,Lingyun Yu,Yingying Li,Haocheng Feng,Hongtao Xie*

Main category: cs.MM

TL;DR: 论文提出了一种用于共语手势生成的新方法GestureHYDRA，强调具语义明确性的特定手部手势，并通过混合模态扩散变压器架构及新设计实现了高效手势建模与操作。


<details>
  <summary>Details</summary>
Motivation: 现有共语手势合成研究多忽视语义明确的手势，而本文旨在通过特定手部手势传递更多教学信息。

Method: 构建高质量3D人体动作数据集，开发基于混合模态扩散变压器的GestureHYDRA系统，引入级联检索增强生成策略及自适应同步机制。

Result: 实验证明，该方法在语义手势激活和生成效率上优于现有技术。

Conclusion: GestureHYDRA显著提升了语义手势生成能力，为共语手势合成提供了新思路。

Abstract: While increasing attention has been paid to co-speech gesture synthesis, most
previous works neglect to investigate hand gestures with explicit and essential
semantics. In this paper, we study co-speech gesture generation with an
emphasis on specific hand gesture activation, which can deliver more
instructional information than common body movements. To achieve this, we first
build a high-quality dataset of 3D human body movements including a set of
semantically explicit hand gestures that are commonly used by live streamers.
Then we present a hybrid-modality gesture generation system GestureHYDRA built
upon a hybrid-modality diffusion transformer architecture with novelly designed
motion-style injective transformer layers, which enables advanced gesture
modeling ability and versatile gesture operations. To guarantee these specific
hand gestures can be activated, we introduce a cascaded retrieval-augmented
generation strategy built upon a semantic gesture repository annotated for each
subject and an adaptive audio-gesture synchronization mechanism, which
substantially improves semantic gesture activation and production efficiency.
Quantitative and qualitative experiments demonstrate that our proposed approach
achieves superior performance over all the counterparts. The project page can
be found at https://mumuwei.github.io/GestureHYDRA/.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [32] [Infinite Traces by Finality: a Sheaf-Theoretic Approach](https://arxiv.org/abs/2507.22536)
*Marco Peressotti*

Main category: cs.LO

TL;DR: 论文提出了一种基于层理论的框架，用于在Kleisli范畴中系统地构建捕获无限迹语义的最终余代数。


<details>
  <summary>Details</summary>
Motivation: Kleisli范畴长期以来被用于模拟各类系统的线性行为，但其最终余代数通常无法对应固定的线性语义。尽管有限迹语义已有成熟条件，无限迹语义的通用理论仍不完善。

Method: 结合Kleisli范畴、序数上的层理论以及守卫（共）递归，通过融合有限逼近的连贯族，使无限行为得以显现。引入守卫行为函子的概念。

Result: 在温和条件下，守卫行为函子的最终余代数直接刻画了无限迹。

Conclusion: 该方法为Kleisli范畴中的无限迹语义提供了一个系统的理论框架，填补了现有研究空白。

Abstract: Kleisli categories have long been recognised as a setting for modelling the
linear behaviour of various types of systems. However, the final coalgebra in
such settings does not, in general, correspond to a fixed notion of linear
semantics. While there are well-understood conditions under which final
coalgebras capture finite trace semantics, a general account of infinite trace
semantics via finality has remained elusive. In this work, we present a
sheaf-theoretic framework for infinite trace semantics in Kleisli categories
that systematically constructs final coalgebras capturing infinite traces. Our
approach combines Kleisli categories, sheaves over ordinals, and guarded
(co)recursion, enabling infinite behaviours to emerge from coherent families of
finite approximations via amalgamation. We introduce the notion of guarded
behavioural functor and show that, under mild conditions, their final
coalgebras directly characterise infinite traces.

</details>


### [33] [Concrete Security Bounds for Simulation-Based Proofs of Multi-Party Computation Protocols](https://arxiv.org/abs/2507.22705)
*Kristina Sojakova,Mihai Codescu,Joshua Gancher*

Main category: cs.LO

TL;DR: 本文介绍了一种自动计算MPC协议具体安全边界的新方法，通过Maude实现，显著减少了证明的代码量。


<details>
  <summary>Details</summary>
Motivation: 解决传统渐进安全难以提供实际安全边界的问题，提升多方计算协议的安全性分析效率。

Method: 基于IPDL元理论，开发新框架并借助Maude工具实现，支持协议运行时及对手优势的推理。

Result: 案例研究中，代码量减少72%，首次实现了GMW协议的正式验证。


Conclusion: 新方法为MPC协议提供了高效、形式化的具体安全边界验证工具。

Abstract: The concrete security paradigm aims to give precise bounds on the probability
that an adversary can subvert a cryptographic mechanism. This is in contrast to
asymptotic security, where the probability of subversion may be eventually
small, but large enough in practice to be insecure. Fully satisfactory concrete
security bounds for Multi-Party Computation (MPC) protocols are difficult to
attain, as they require reasoning about the running time of cryptographic
adversaries and reductions. In this paper we close this gap by introducing a
new foundational approach that allows us to automatically compute concrete
security bounds for MPC protocols. We take inspiration from the meta-theory of
IPDL, a prior approach for formally verified distributed cryptography, to
support reasoning about the runtime of protocols and adversarial advantage. For
practical proof developments, we implement our approach in Maude, an extensible
logic for equational rewriting. We carry out four case studies of concrete
security for simulation-based proofs. Most notably, we deliver the first formal
verification of the GMW MPC protocol over N parties. To our knowledge, this is
the first time that formally verified concrete security bounds are computed for
a proof of an MPC protocol in the style of Universal Composability. Our tool
provides a layer of abstraction that allows the user to write proofs at a high
level, which drastically simplifies the proof size. For comparison, a case
study that in prior works required 2019 LoC only takes 567 LoC, thus reducing
proof size by 72%

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [34] [IntentFlow: Interactive Support for Communicating Intent with LLMs in Writing Tasks](https://arxiv.org/abs/2507.22134)
*Yoonsu Kim,Brandon Chin,Kihoon Son,Seoyoung Kim,Juho Kim*

Main category: cs.HC

TL;DR: IntentFlow通过可编辑的界面组件帮助用户动态表达和调整写作意图，提升LLM辅助写作效果。


<details>
  <summary>Details</summary>
Motivation: 用户在使用LLM辅助写作时难以清晰表达和动态调整模糊、流动或潜意识的写作意图。

Method: 开发了IntentFlow，从用户提示中提取目标和意图，并以可编辑的界面组件呈现，支持直接修改或后续提示调整。视觉链接帮助理解模型行为。

Result: 用户研究表明，IntentFlow比聊天基线更易于表达详细意图，用户行为更具意义，输出更符合动态意图。

Conclusion: 可编辑意图表示帮助用户精炼意图，并支持跨任务复用，提升LLM辅助写作的一致性和可迁移性。

Abstract: While large language models (LLMs) are widely used for writing, users often
struggle to express their nuanced and evolving intents through prompt-based
interfaces. Intents -- low-level strategies or preferences for achieving a
writing goal -- are often vague, fluid, or even subconscious, making it
difficult for users to articulate and adjust them. To address this, we present
IntentFlow, which supports the communication of dynamically evolving intents
throughout LLM-assisted writing. IntentFlow extracts goals and intents from
user prompts and presents them as editable interface components, which users
can revise, remove, or refine via direct manipulation or follow-up prompts.
Visual links connect each component to the output segments it influences,
helping users understand model behavior. In a within-subjects study (N=12),
participants using IntentFlow, compared to a chat-based baseline, expressed
their intents more easily and in detail, engaged in more meaningful actions to
communicate intents, such as adjusting and deleting, and produced outputs that
better aligned with their evolving intents. We found that editable intent
representations help users refine and consolidate a final set of intents, which
can be reused across similar tasks to support consistent and transferable
LLM-assisted writing.

</details>


### [35] [Towards Privacy-preserving Photorealistic Self-avatars in Mixed Reality](https://arxiv.org/abs/2507.22153)
*Ethan Wilson,Vincent Bindschaedler,Sophie Jörg,Sean Sheikholeslam,Kevin Butler,Eakta Jain*

Main category: cs.HC

TL;DR: 本文提出了一种保护用户隐私的3D虚拟化身生成方法，通过身份编码生成模型分离身份特征，提供了两种算法来实现不同的隐私保护级别。


<details>
  <summary>Details</summary>
Motivation: 随着3D虚拟化身技术的进步，用户的真实外貌更容易被滥用，作者希望通过技术手段保护用户的生物识别信息隐私。

Method: 提出两种算法：一种提供差分隐私保证（\epsmethod{}），另一种提供细粒度的身份偏移控制（\fhetamethod{}），通过身份编码生成模型实现化身的去标识化。

Result: 实验表明，该方法能成功生成去标识化的2D和3D虚拟化身，同时保留用户的身份感知属性。

Conclusion: 这项技术可在公共环境中安全使用高真实感的虚拟化身，兼顾真实感和隐私保护。

Abstract: Photorealistic 3D avatar generation has rapidly improved in recent years, and
realistic avatars that match a user's true appearance are more feasible in
Mixed Reality (MR) than ever before. Yet, there are known risks to sharing
one's likeness online, and photorealistic MR avatars could exacerbate these
risks. If user likenesses were to be shared broadly, there are risks for cyber
abuse or targeted fraud based on user appearances. We propose an alternate
avatar rendering scheme for broader social MR -- synthesizing realistic avatars
that preserve a user's demographic identity while being distinct enough from
the individual user to protect facial biometric information. We introduce a
methodology for privatizing appearance by isolating identity within the feature
space of identity-encoding generative models. We develop two algorithms that
then obfuscate identity: \epsmethod{} provides differential privacy guarantees
and \thetamethod{} provides fine-grained control for the level of identity
offset. These methods are shown to successfully generate de-identified virtual
avatars across multiple generative architectures in 2D and 3D. With these
techniques, it is possible to protect user privacy while largely preserving
attributes related to sense of self. Employing these techniques in public
settings could enable the use of photorealistic avatars broadly in MR,
maintaining high realism and immersion without privacy risk.

</details>


### [36] [IdeaBlocks: Expressing and Reusing Exploratory Intents for Design Exploration with Generative AI](https://arxiv.org/abs/2507.22163)
*DaEun Choi,Kihoon Son,Jaesang Yu,Hyunjoon Jung,Juho Kim*

Main category: cs.HC

TL;DR: IdeaBlocks通过结构化输入和模块化设计帮助设计师克服生成式AI探索的局限性，实现更高效和多样化的设计探索。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在设计探索中虽有潜力，但存在表达意图困难、探索不连贯和迭代支持不足等问题，阻碍设计师高效探索。

Method: 提出IdeaBlocks系统，将探索意图模块化为可链式重用的探索块，支持非线性连续探索。

Result: 用户研究表明，使用IdeaBlocks的设计师探索的图像数量增加112.8%，视觉多样性提升12.5%，且探索模式更迭代和连续。

Conclusion: IdeaBlocks有效提升生成式AI的设计探索效率，未来工具需更好平衡发散与收敛支持，并更有效捕捉探索意图。

Abstract: Generative AI opens new possibilities for design exploration by rapidly
generating images aligned with user goals. However, our formative study (N=7)
revealed three key limitations hindering designers' broad and efficient
exploration when interacting with these models. These include difficulty
expressing open-ended exploratory intent, lack of continuity in exploration,
and limited support for reusing or iterating on previous ideas. We propose
IdeaBlocks, where users can express their exploratory intents to generative AI
with structured input and modularize them into Exploration Blocks. These blocks
can be chained for continuous, non-linear exploration and reused across
contexts, enabling broad exploration without losing creative momentum. Our user
study with 12 designers showed that participants using IdeaBlocks explored
112.8% more images with 12.5% greater visual diversity than the baseline. They
also developed ideas in more iterative and continuous patterns, such as
branching, chaining, and revisiting ideas. We discuss design implications for
future tools to better balance divergent and convergent support during
different phases of exploration, and to capture and leverage exploratory
intents more effectively.

</details>


### [37] [DissolvPCB: Fully Recyclable 3D-Printed Electronics with Liquid Metal Conductors and PVA Substrates](https://arxiv.org/abs/2507.22193)
*Zeyu Yan,SuHwan Hong,Josiah Hester,Tingyu Cheng,Huaishu Peng*

Main category: cs.HC

TL;DR: DissolvPCB是一种利用3D打印和可溶性材料制造完全可回收电路板的技术，通过生命周期评估量化环境影响。


<details>
  <summary>Details</summary>
Motivation: 为减少电子废弃物并实现电路板的可持续制造。

Method: 使用FDM 3D打印和可溶性PVA基板，搭配液态金属EGaIn作为导电材料。

Result: 开发了自动化转换软件，制造并回收了三种功能原型，展示了技术的可行性。

Conclusion: DissolvPCB具备环保潜力，但仍存在技术限制，需进一步优化。

Abstract: We introduce DissolvPCB, an electronic prototyping technique for fabricating
fully recyclable printed circuit board assemblies (PCBAs) using affordable FDM
3D printing, with polyvinyl alcohol (PVA) as a water-soluble substrate and
eutectic gallium-indium (EGaIn) as the conductive material. When obsolete, the
PCBA can be easily recycled by immersing it in water: the PVA dissolves, the
EGaIn re-forms into a liquid metal bead, and the electronic components are
recovered. These materials can then be reused to fabricate a new PCBA.
  We present the DissolvPCB workflow, characterize its design parameters,
evaluate the performance of circuits produced with it, and quantify its
environmental impact through a lifecycle assessment (LCA) comparing it to
conventional CNC-milled FR-4 boards. We further develop a software plugin that
automatically converts PCB design files into 3D-printable circuit substrate
models. To demonstrate the capabilities of DissolvPCB, we fabricate and recycle
three functional prototypes: a Bluetooth speaker featuring a double-sided PCB,
a finger fidget toy with a 3D circuit topology, and a shape-changing gripper
enabled by Joule-heat-driven 4D printing. The paper concludes with a discussion
of current technical limitations and opportunities for future directions.

</details>


### [38] [Verisimilitude as Boon and Bane: How People Initiate Opportunistic Interactions at Professional Events in Social VR](https://arxiv.org/abs/2507.22241)
*Victoria Chang,Caro Williams-Pierce,Huaishu Peng,Ge Gao*

Main category: cs.HC

TL;DR: 该研究探讨了社交VR中如何通过非言语信号启动偶发性互动，并分析了参与者对真实性的感知及其对社交行为的影响，为VR平台设计提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决专业活动中从线下转为线上后偶发性互动难以实现的问题，尤其是通过社交VR如何保留现实世界中的非言语信号的社交意义。

Method: 研究方法包括对16名有丰富VR活动经验的参与者进行观察，并在活动后进行访谈，分析其对非言语信号真实性的感知及社交行为。

Result: 研究发现参与者通过评估真实性（verisimilitude）来调整行为，同时揭示了VR平台在技术上简化社交机制的方式。

Conclusion: 结论指出，VR平台设计需更好地支持偶发性互动，并提供了基于研究结果的具体设计建议。

Abstract: Opportunistic interactions-the unstructured exchanges that emerge as
individuals become aware of each other's presence-are essential for
relationship building and information sharing in everyday life. Yet, fostering
effective opportunistic interactions has proven challenging, especially at
professional events that have increasingly transitioned from in person to
online formats. In the current paper, we offer an in-depth qualitative account
of how people initiate opportunistic interactions in social VR. Our
participants consisted of 16 individuals with ongoing experience attending
VR-mediated events in their professional communities. We conducted extensive
observations with each participant during one or more events they attended. We
also interviewed them after every observed event, obtaining self-reflections on
their attempts to navigate opportunistic interactions with others. Our analysis
revealed that participants sought to understand the extent to which social VR
preserved the real-world meanings of various nonverbal cues, which we refer to
as verisimilitude. We detailed the unique connections between a person's
perceived verisimilitude and their social behaviors at each of the three steps
toward initiating opportunistic interactions: availability recognition,
attention capture, and ice-breaking. Across these steps, the VR platform
typically replaces complex social mechanisms with feasible technical ones in
order to function, thereby altering the preconditions necessary for a nonverbal
cue's social meanings to remain intact. We identified a rich set of strategies
that participants developed to assess verisimilitude and act upon it, while
also confirming a lack of systematic knowledge guiding their practices. Based
on these findings, we provide actionable insights for social VR platform design
that can best support the initiation of opportunistic interactions for
professional purposes.

</details>


### [39] [Multidimensional Assessment of Takeover Performance in Conditionally Automated Driving](https://arxiv.org/abs/2507.22252)
*Kexin Liang,Jan Luca Kästleb,Bani Anvarib,Simeon C. Calverta,J. W. C. van Lint*

Main category: cs.HC

TL;DR: 该研究探讨了在自动驾驶系统中，驾驶员接管车辆时的表现，重点关注情境意识（SA）和备用能力（SC）对响应效率、用户体验和驾驶安全的影响。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在复杂情况下需要驾驶员接管控制，但接管过程对驾驶员要求高，需了解影响接管表现的关键因素以确保安全过渡。

Method: 通过驾驶模拟器实验评估驾驶员的接管表现，并使用XGBoost模型比较SA和SC与基本驾驶员特征（DC）对表现的影响。

Result: 研究发现，较高的SA能更快响应接管请求，而SC对接管质量的提升更为显著。

Conclusion: SA和SC在接管表现中发挥不同但互补的作用，为优化人车交互和自动驾驶系统设计提供了重要见解。

Abstract: When automated driving systems encounter complex situations beyond their
operational capabilities, they issue takeover requests, prompting drivers to
resume vehicle control and return to the driving loop as a critical safety
backup. However, this control transition places significant demands on drivers,
requiring them to promptly respond to takeover requests while executing
high-quality interventions. To ensure safe and comfortable control transitions,
it is essential to develop a deep understanding of the key factors influencing
various takeover performance aspects. This study evaluates drivers' takeover
performance across three dimensions: response efficiency, user experience, and
driving safety - using a driving simulator experiment. EXtreme Gradient
Boosting (XGBoost) models are used to investigate the contributions of two
critical factors, i.e., Situational Awareness (SA) and Spare Capacity (SC), in
predicting various takeover performance metrics by comparing the predictive
results to the baseline models that rely solely on basic Driver Characteristics
(DC). The results reveal that (i) higher SA enables drivers to respond to
takeover requests more quickly, particularly for reflexive responses; and (ii)
SC shows a greater overall impact on takeover quality than SA, where higher SC
generally leads to enhanced subjective rating scores and objective execution
trajectories. These findings highlight the distinct yet complementary roles of
SA and SC in shaping performance components, offering valuable insights for
optimizing human-vehicle interactions and enhancing automated driving system
design.

</details>


### [40] [Towards Safe and Comfortable Vehicle Control Transitions: A Systematic Review of Takeover Time, Time Budget, and Takeover Performance](https://arxiv.org/abs/2507.22262)
*Kexin Liang,Simeon C. Calvert,J. W. C. van Lint*

Main category: cs.HC

TL;DR: 该论文探讨了自动驾驶系统中人类驾驶员接管车辆控制的时间预算问题，旨在确保安全舒适的过渡。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决接管时间预算设置中的变异性问题，以降低驾驶风险并提升用户体验。

Method: 方法包括综述接管序列中的时间预算、接管时间和性能，并提出分类法和概念模型。

Result: 成果包括提出了接管缓冲区概念和两个分类法，并建立了时间预算与性能的关系模型。

Conclusion: 结论强调了标准化测量和适应性时间预算的重要性，并提出了未来研究的六个方向。

Abstract: Conditionally automated driving systems require human drivers to disengage
from non-driving-related activities and resume vehicle control within limited
time budgets when encountering scenarios beyond system capabilities. Ensuring
safe and comfortable transitions is critical for reducing driving risks and
improving user experience. However, takeovers involve complex human-vehicle
interactions, resulting in substantial variability in drivers' responses,
especially in takeover time, defined as the duration needed to regain control.
This variability presents challenges in setting sufficient time budgets that
are neither too short (risking safety and comfort) nor too long (reducing
driver alertness and transition efficiency).
  Although previous research has examined the role of time budgets in
influencing takeover time and performance, few studies have systematically
addressed how to determine sufficient time budgets that adapt to diverse
scenarios and driver needs. This review supports such efforts by examining the
entire takeover sequence, including takeover time, time budget, and takeover
performance. Specifically, we (i) synthesize causal factors influencing
takeover time and propose a taxonomy of its determinants using the
task-capability interface model; (ii) review existing work on fixed and
adaptive time budgets, introducing the concept of the takeover buffer to
describe the gap between takeover time and allocated time budget; (iii) present
a second taxonomy to support standardized and context-sensitive measurement of
takeover performance; (iv) propose a conceptual model describing the
relationships among takeover time, time budget, and performance; and (v)
outline a research agenda with six directions.

</details>


### [41] [Promoting Online Safety by Simulating Unsafe Conversations with LLMs](https://arxiv.org/abs/2507.22267)
*Owen Hoffman,Kangze Peng,Zehua You,Sajid Kamal,Sukrit Venkatagiri*

Main category: cs.HC

TL;DR: 这篇论文探讨了如何利用生成式 AI（如大型语言模型 LLMs）模拟在线不安全对话，旨在通过用户反馈教育人们识别和应对这些风险。


<details>
  <summary>Details</summary>
Motivation: LLMs 降低了不良行为者创建不安全对话的门槛，因为它们可以生成逼真的文本。研究希望通过模拟这些对话帮助用户提升在线安全意识。

Method: 研究者使用两个 LLM 相互对话，模拟诈骗者与受害者的不安全对话，并让用户为目标 LLM 提供反馈。

Result: 通过用户对目标 LLM 的反馈，研究验证了模拟对话和反馈机制在提升用户安全意识方面的潜力。

Conclusion: 利用 LLM 模拟不安全对话并结合用户反馈，可以有效帮助用户识别和应对在线风险。

Abstract: Generative AI, including large language models (LLMs) have the potential --
and already are being used -- to increase the speed, scale, and types of unsafe
conversations online. LLMs lower the barrier for entry for bad actors to create
unsafe conversations in particular because of their ability to generate
persuasive and human-like text. In our current work, we explore ways to promote
online safety by teaching people about unsafe conversations that can occur
online with and without LLMs. We build on prior work that shows that LLMs can
successfully simulate scam conversations. We also leverage research in the
learning sciences that shows that providing feedback on one's hypothetical
actions can promote learning. In particular, we focus on simulating scam
conversations using LLMs. Our work incorporates two LLMs that converse with
each other to simulate realistic, unsafe conversations that people may
encounter online between a scammer LLM and a target LLM but users of our system
are asked provide feedback to the target LLM.

</details>


### [42] [VRISE: A Virtual Reality Platfrom for Immersive and Interactive Surveying Education](https://arxiv.org/abs/2507.22810)
*Daniel Udekwe,Dimitrios Bolkas,Eren Erman Ozguven,Ren Moses,Qianwen,Guo*

Main category: cs.HC

TL;DR: VRISE是一种虚拟现实实验室，旨在通过交互式和沉浸式学习体验改进传统测量教育的不足。


<details>
  <summary>Details</summary>
Motivation: 传统测量教育存在后勤和认知挑战，不利于学生参与和技能发展。

Method: 开发了VRISE虚拟实验室，模拟地面和空中测量任务，提供可定制、用户友好的模块和实时反馈。

Result: 评估显示，VRISE显著提高了测量准确性、任务效率和技能发展。

Conclusion: VRISE通过沉浸式环境减轻认知负担，为测量教育提供安全、灵活的解决方案。

Abstract: Surveying is a core component of civil engineering education, requiring
students to engage in hands-on spatial measurement, instrumentation handling,
and field-based decision-making. However, traditional instruction often poses
logistical and cognitive challenges that can hinder accessibility and student
engagement. While virtual laboratories have gained traction in engineering
education, few are purposefully designed to support flexible, adaptive learning
in surveying. To address this gap, we developed Virtual Reality for Immersive
and Interactive Surveying Education (VRISE), an immersive virtual reality
laboratory that replicates ground-based and aerial surveying tasks through
customizable, accessible, and user-friendly modules. VRISE features interactive
experiences such as differential leveling with a digital level equipment and
waypoint-based drone navigation, enhanced by input smoothing, adaptive
interfaces, and real-time feedback to accommodate diverse learning styles.
Evaluation across multiple user sessions demonstrated consistent gains in
measurement accuracy, task efficiency, and interaction quality, with a clear
progression in skill development across the ground-based and aerial surveying
modalities. By reducing cognitive load and physical demands, even in tasks
requiring fine motor control and spatial reasoning, VRISE demonstrates the
potential of immersive, repeatable digital environments to enhance surveying
education, broaden participation, and strengthen core competencies in a safe
and engaging setting.

</details>


### [43] [ConGaIT: A Clinician-Centered Dashboard for Contestable AI in Parkinson's Disease Care](https://arxiv.org/abs/2507.22300)
*Phuc Truong Loc Nguyen,Thanh Hung Do*

Main category: cs.HC

TL;DR: Con-GaIT是一款基于HCI原则的临床AI系统，通过可视化和可追溯的设计提升AI决策的透明度和可争议性。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助步态分析在帕金森病护理中缺乏透明性，无法让临床医生质疑或争议AI决策，因此设计了Con-GaIT系统。

Method: 结合HCI原则，设计了包括可视化解释、基于角色的反馈和可追溯的日志的界面，支持‘争议与解释’交互模式。

Result: 系统在可争议性评估分数（CAS）中达到0.970，证明其设计成功实现了监管标准的可争议性。

Conclusion: Con-GaIT通过人本设计实现了AI决策的可争议性，为临床AI系统提供了透明和可控的解决方案。

Abstract: AI-assisted gait analysis holds promise for improving Parkinson's Disease
(PD) care, but current clinical dashboards lack transparency and offer no
meaningful way for clinicians to interrogate or contest AI decisions. We
present Con-GaIT (Contestable Gait Interpretation & Tracking), a
clinician-centered system that advances Contestable AI through a tightly
integrated interface designed for interpretability, oversight, and procedural
recourse. Grounded in HCI principles, ConGaIT enables structured disagreement
via a novel Contest & Justify interaction pattern, supported by visual
explanations, role-based feedback, and traceable justification logs. Evaluated
using the Contestability Assessment Score (CAS), the framework achieves a score
of 0.970, demonstrating that contestability can be operationalized through
human-centered design in compliance with emerging regulatory standards. A
demonstration of the framework is available at
https://github.com/hungdothanh/Con-GaIT.

</details>


### [44] [A Node on the Constellation: The Role of Feminist Makerspaces in Building and Sustaining Alternative Cultures of Technology Production](https://arxiv.org/abs/2507.22329)
*Erin Gatz,Yasmine Kotturi,Andrea Afua Kwamya,Sarah Fox*

Main category: cs.HC

TL;DR: 探讨女性主义创客空间如何通过关怀、本地正义运动和共同治理等社会实践活动实现长期可持续性。


<details>
  <summary>Details</summary>
Motivation: 研究女性主义创客空间如何在结构性不稳定的环境中通过独特的社会实践实现长期可持续性。

Method: 通过访谈8个美国女性主义创客空间的18名创始人和成员，结合自民族志反思。

Result: 可持续性不依赖规模增长或制度化，而是通过关怀驱动的管理、与本地正义运动的团结和共同治理实现。

Conclusion: 女性主义创客空间作为预演性的反空间，通过日常实践体现女性主义价值观，为替代性社会技术基础设施提供了范例。

Abstract: Feminist makerspaces offer community led alternatives to dominant tech
cultures by centering care, mutual aid, and collective knowledge production.
While prior CSCW research has explored their inclusive practices, less is known
about how these spaces sustain themselves over time. Drawing on interviews with
18 founders and members across 8 U.S. feminist makerspaces as well as
autoethnographic reflection, we examine the organizational and relational
practices that support long-term endurance. We find that sustainability is not
achieved through growth or institutionalization, but through care-driven
stewardship, solidarity with local justice movements, and shared governance.
These social practices position feminist makerspaces as prefigurative
counterspaces - sites that enact, rather than defer, feminist values in
everyday practice. This paper offers empirical insight into how feminist
makerspaces persist amid structural precarity, and highlights the forms of
labor and coalition-building that underpin alternative sociotechnical
infrastructures.

</details>


### [45] [Mitigating Response Delays in Free-Form Conversations with LLM-powered Intelligent Virtual Agents](https://arxiv.org/abs/2507.22352)
*Mykola Maslych,Mohammadreza Katebi,Christopher Lee,Yahya Hmaiti,Amirpouya Ghasemaghaei,Christian Pumarada,Janneese Palmer,Esteban Segarra Martinez,Marco Emporio,Warren Snipes,Ryan P. McMahan,Joseph J. LaViola Jr*

Main category: cs.HC

TL;DR: 研究了虚拟现实（VR）中基于大语言模型（LLM）的虚拟代理在自由对话中缓解响应延迟的挑战，发现自然对话填充物能提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在VR中通过对话填充物（如手势和语言提示）缓解LLM虚拟代理的响应延迟问题。

Method: 在不同延迟水平和交互场景下，测试对话填充物对改善用户体验的有效性。

Result: 延迟超过4秒会降低体验质量，而自然对话填充物能显著提升感知响应速度。

Conclusion: 研究成果为优化延迟对话系统的用户参与度提供了实践指导，并贡献了一个开源部署工具。

Abstract: We investigated the challenges of mitigating response delays in free-form
conversations with virtual agents powered by Large Language Models (LLMs)
within Virtual Reality (VR). For this, we used conversational fillers, such as
gestures and verbal cues, to bridge delays between user input and system
responses and evaluate their effectiveness across various latency levels and
interaction scenarios. We found that latency above 4 seconds degrades quality
of experience, while natural conversational fillers improve perceived response
time, especially in high-delay conditions. Our findings provide insights for
practitioners and researchers to optimize user engagement whenever
conversational systems' responses are delayed by network limitations or slow
hardware. We also contribute an open-source pipeline that streamlines deploying
conversational agents in virtual environments.

</details>


### [46] [A Fuzzy Set-based Approach for Matching Hand-Drawing Shapes of Touch-based Gestures for Graphical Passwords](https://arxiv.org/abs/2507.22382)
*Adel Sabour,Ahmed Gadallah,Hesham Hefny*

Main category: cs.HC

TL;DR: 提出了一种基于二维模糊集的触摸手势匹配方法，旨在通过模糊线索点击点技术提高用户绘制手势与预存参考手势的匹配度。


<details>
  <summary>Details</summary>
Motivation: 当前手势技术普遍无法很好地处理因手指和手部运动自然性导致的手势不准确问题。

Method: 采用二维模糊集和模糊线索点击点技术，灵活匹配用户绘制的手势与预存参考手势。

Result: 通过提出的方法，能够更灵活地处理手势交互中的不准确性问题。

Conclusion: 该方法为手势交互提供了一种更灵活的解决方案，提高了用户手势的接受度。

Abstract: This paper presents a two-dimension fuzzy set based approach for matching
touch-based gestures using fuzzy cued click point technique. The pro posed
approach aims mainly to improve the acceptance of the most closed inac curate
hand drawn gestures generated by the user compared with a predefined referenced
gesture value that is stored in the user profile. Commonly, gestures are used
in order to facilitate the interactive capabilities between humans and
computerized systems. Unfortunately, most of current gesturing techniques don't
deal at the same level of inaccuracy of gesturing, resulted from the nature of
hu man fingers and hands movements. This paper aims, in a more flexible manner,
to tackle the inaccuracy problem existed with gesture-based interactions
between humans and a computerized system.

</details>


### [47] [Analysis of User Experience Evaluation Methods for Deaf users: A Case Study on a mobile App](https://arxiv.org/abs/2507.22455)
*A. E. Fuentes-Cortázar,A. Rivera-Hernández,J. R. Rojano-Cáceres*

Main category: cs.HC

TL;DR: 研究探讨了传统用户体验（UX）评估方法对聋人用户的不适用性，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 传统UX评估方法主要针对听力用户，未能满足聋人用户的独特需求，导致评估结果不完整或有偏见。

Method: 分析了一组推荐用于聋人用户的UX评估方法，验证其可访问性及局限性。

Result: 尽管这些方法在文献中常被推荐，但仍存在局限性，需改进以适应聋人社区的特殊沟通需求。

Conclusion: 评估方法需调整以确保对聋人用户的可访问性，从而准确反映其体验和需求。

Abstract: User Experience (UX) evaluation methods that are commonly used with hearing
users may not be functional or effective for Deaf users. This is because these
methods are primarily designed for users with hearing abilities, which can
create limitations in the interaction, perception, and understanding of the
methods for Deaf individuals. Furthermore, traditional UX evaluation approaches
often fail to address the unique accessibility needs of Deaf users, resulting
in an incomplete or biased assessment of their user experience. This research
focused on analyzing a set of UX evaluation methods recommended for use with
Deaf users, with the aim of validating the accessibility of each method through
findings and limitations. The results indicate that, although these evaluation
methods presented here are commonly recommended in the literature for use with
Deaf users, they present various limitations that must be addressed in order to
better adapt to the communication skills specific to the Deaf community. This
research concludes that evaluation methods must be adapted to ensure accessible
software evaluation for Deaf individuals, enabling the collection of data that
accurately reflects their experiences and needs.

</details>


### [48] [Exploring Student-AI Interactions in Vibe Coding](https://arxiv.org/abs/2507.22614)
*Francis Geng,Anshul Shah,Haolin Li,Nawab Mulla,Steven Swanson,Gerald Soosai Raj,Daniel Zingaro,Leo Porter*

Main category: cs.HC

TL;DR: 论文探讨了学生如何使用 Replit 平台进行“vibe coding”编程，发现学生主要通过测试和调试与平台互动，高级软件工程学生在提示中包含更多相关上下文。


<details>
  <summary>Details</summary>
Motivation: 研究学生如何与“vibe coding”平台 Replit 互动，以及编程背景差异如何影响这种互动。

Method: 通过访谈录音和主题分析，观察学生在构建 Web 应用时与 Replit 的互动。

Result: 多数学生互动集中于测试和调试，高级学生更倾向于在提示中包含应用功能和代码库上下文。

Conclusion: 编程背景显著影响学生与“vibe coding”平台的互动方式，高级学生更具上下文意识。

Abstract: Background and Context. Chat-based and inline-coding-based GenAI has already
had substantial impact on the CS Education community. The recent introduction
of ``vibe coding'' may further transform how students program, as it introduces
a new way for students to create software projects with minimal oversight.
  Objectives. The purpose of this study is to understand how students in
introductory programming and advanced software engineering classes interact
with a vibe coding platform (Replit) when creating software and how the
interactions differ by programming background.
  Methods. Interview participants were asked to think-aloud while building a
web application using Replit. Thematic analysis was then used to analyze the
video recordings with an emphasis on the interactions between the student and
Replit.
  Findings. For both groups, the majority of student interactions with Replit
were to test or debug the prototype and only rarely did students visit code.
Prompts by advanced software engineering students were much more likely to
include relevant app feature and codebase contexts than those by introductory
programming students.

</details>


### [49] [Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach](https://arxiv.org/abs/2507.22671)
*Sami Saeed Alghamdi,Christopher Bull,Ahmed Kharrufa*

Main category: cs.HC

TL;DR: 论文开发了一个支持在线自学的系统，通过AI生成的学习故事和反馈，帮助编程学习者提升自组织能力。用户反馈显示系统具有潜力。


<details>
  <summary>Details</summary>
Motivation: 许多自学编程的人面临孤立、信息过载和缺乏指导的问题。社交媒体的自组织实践启发了系统设计。

Method: 设计了一个包含网页平台和浏览器插件的系统，支持学习者通过AI生成的学习故事进行反思和结构化学习。

Result: 15名用户的测试表明，系统在支持自组织方面表现良好，尤其是实时反思和自动化反馈功能受认可。

Conclusion: 系统展示了AI工具在支持自学中的潜力，未来可进一步优化功能和解决用户反馈中的摩擦点。

Abstract: Many people learn programming independently from online resources and often
report struggles in achieving their personal learning goals. Learners
frequently describe their experiences as isolating and frustrating, challenged
by abundant uncertainties, information overload, and distraction, compounded by
limited guidance. At the same time, social media serves as a personal space
where many engage in diverse self-regulation practices, including help-seeking,
using external memory aids (e.g., self-notes), self-reflection, emotion
regulation, and self-motivation. For instance, learners often mark achievements
and set milestones through their posts. In response, we developed a system
consisting of a web platform and browser extensions to support self-regulation
online. The design aims to add learner-defined structure to otherwise
unstructured experiences and bring meaning to curation and reflection
activities by translating them into learning stories with AI-generated
feedback. We position storytelling as an integrative approach to design that
connects resource curation, reflective and sensemaking practice, and narrative
practices learners already use across social platforms. We recruited 15
informal programming learners who are regular social media users to engage with
the system in a self-paced manner; participation concluded upon submitting a
learning story and survey. We used three quantitative scales and a qualitative
survey to examine users' characteristics and perceptions of the system's
support for their self-regulation. User feedback suggests the system's
viability as a self-regulation aid. Learners particularly valued in-situ
reflection, automated story feedback, and video annotation, while other
features received mixed views. We highlight perceived benefits, friction
points, and design opportunities for future AI-augmented self-regulation tools.

</details>


### [50] [Progressive Web Application for Storytelling Therapy Support](https://arxiv.org/abs/2507.22839)
*Javier Jimenez-Honrado,Javier Gomez Garcia,Felipe Costa-Tebar,Felix A. Marco,Jose A. Gallud,Gabriel Sebastian Rivera*

Main category: cs.HC

TL;DR: 本文旨在设计一种渐进式Web应用（PWA）以支持心理健康领域中的叙事疗法，同时验证PWA在实际应用中的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管信息技术发展迅速，但某些领域（如非营利组织或未现代化的工作活动）仍未被覆盖。PWA作为一种新兴技术，有望打破传统应用开发的限制。

Method: 设计并开发一个PWA，用于支持叙事疗法工作坊，同时通过实际案例验证其优势和可行性。

Result: 提供了一个可增强叙事疗法工作坊效果的软件应用，并验证了PWA在实际应用中的优势。

Conclusion: PWA在叙事疗法中的应用展示了其在功能性和便捷性上的潜力，尤其是在心理健康领域。

Abstract: In spite of all advances promoted by information technologies, there are
still activities where this technology is not applied for reasons such as being
carried out in non-profit organizations or because they have not adapted to
this modernization. Until recently, the way to work with mobile devices was
either by connecting through a web page with the device's browser, or by
downloading an application from the corresponding platform. But lately,
technologies are being developed that aim to break with this, as in the case of
Progressive Web Applications (PWA). One of the advantages offered by PWA is to
access the web page and install it as an application on the device. The purpose
of this article is to design a progressive Web application for the support of
Storytelling Therapy, one of the novel therapies applied in the field of mental
health. In addition to providing a software application to enhance Storytelling
Therapy workshops, it is also intended to analyze and verify the advantages of
PWA in a real case.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [51] [Toward Intelligent Electronic-Photonic Design Automation for Large-Scale Photonic Integrated Circuits: from Device Inverse Design to Physical Layout Generation](https://arxiv.org/abs/2507.22301)
*Hongjian Zhou,Pingchuan Ma,Jiaqi Gu*

Main category: cs.ET

TL;DR: PoLaRIS 是一种智能电子-光子设计自动化框架，用于自动化生成高性能、无设计规则违规的光子集成电路布局，加速开发并降低设计门槛。


<details>
  <summary>Details</summary>
Motivation: 现代光子集成电路设计仍依赖手工，效率低、扩展性差且易出错，亟需自动化解决方案。

Method: PoLaRIS 结合逆向设计引擎和智能路由与布局算法，融合物理优化与机器学习。

Result: PoLaRIS 实现了自动化生成无违规且性能优化的布局，显著加速光子系统开发。

Conclusion: PoLaRIS 为光子系统设计自动化奠定了基础，提升了效率和可扩展性。

Abstract: Photonic Integrated Circuits (PICs) offer tremendous advantages in bandwidth,
parallelism, and energy efficiency, making them essential for emerging
applications in artificial intelligence (AI), high-performance computing (HPC),
sensing, and communications. However, the design of modern PICs, which now
integrate hundreds to thousands of components, remains largely manual,
resulting in inefficiency, poor scalability, and susceptibility to errors. To
address these challenges, we propose PoLaRIS, a comprehensive Intelligent
Electronic-Photonic Design Automation (EPDA) framework that spans both
device-level synthesis and system-level physical layout. PoLaRIS combines a
robust, fabrication-aware inverse design engine with a routing-informed
placement and curvy-aware detailed router, enabling the automated generation of
design rule violation (DRV)-free and performance-optimized layouts. By unifying
physics-driven optimization with machine learning and domain-specific
algorithms, PoLaRIS significantly accelerates PIC development, lowers design
barriers, and lays the groundwork for scalable photonic system design
automation.

</details>


### [52] [Green Wave as an Integral Part for the Optimization of Traffic Efficiency and Safety: A Survey](https://arxiv.org/abs/2507.22511)
*Kranthi Kumar Talluri,Christopher Stang,Galia Weidl*

Main category: cs.ET

TL;DR: 该论文综述了Green Wave交通控制策略，探讨了其如何通过新兴技术（如AI和V2X）提升交通效率和安全性，并讨论了与智能城市结合的潜力、减排优势及行人/骑行者的安全问题。


<details>
  <summary>Details</summary>
Motivation: 探索Green Wave系统的潜力，结合新兴技术和智能城市理念，提升交通管理效率和安全性，同时关注减排和弱势道路使用者的安全问题。

Method: 通过全面调研现有Green Wave交通控制策略，分析其对未来交通管理系统和城市基础设施的影响。

Result: 提出Green Wave系统在智能交通管理中的应用潜力，并识别了现有挑战和研究空白。

Conclusion: 需进一步研究以填补Green Wave系统在可持续城市交通中的技术和管理空白，实现更高效的交通协调。

Abstract: Green Wave provides practical and advanced solutions to improve traffic
efficiency and safety through network coordination. Nevertheless, the complete
potential of Green Wave systems has yet to be explored. Utilizing emerging
technologies and advanced algorithms, such as AI or V2X, would aid in achieving
more robust traffic management strategies, especially when integrated with
Green Wave. This work comprehensively surveys existing traffic control
strategies that enable Green Waves and analyzes their impact on future traffic
management systems and urban infrastructure. Understanding previous research on
traffic management and its effect on traffic efficiency and safety helps
explore the integration of Green Wave solutions with smart city initiatives for
effective traffic signal coordination. This paper also discusses the advantages
of using Green Wave strategies for emission reduction and considers road safety
issues for vulnerable road users, such as pedestrians and cyclists. Finally,
the existing challenges and research gaps in building robust and successful
Green Wave systems are discussed to articulate explicitly the future
requirement of sustainable urban transport.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [53] [Minimizing CGYRO HPC Communication Costs in Ensembles with XGYRO by Sharing the Collisional Constant Tensor Structure](https://arxiv.org/abs/2507.22245)
*Igor Sfiligoi,Emily A. Belli,Jeff Candy*

Main category: cs.DC

TL;DR: XGYRO是一种新工具，通过将一组CGYRO模拟作为一个HPC任务执行，共享碰撞常数张量结构，显著降低了内存需求和通信开销。


<details>
  <summary>Details</summary>
Motivation: CGYRO模拟由于计算和内存需求高，导致在多个HPC节点上运行时通信开销大。为解决这一问题，开发了XGYRO。

Method: XGYRO将整个模拟集合作为一个单一HPC任务处理，优化全局缓冲分配并共享碰撞常数张量结构。

Result: XGYRO显著降低了单个模拟的内存消耗和整体通信开销。

Conclusion: XGYRO通过集合模拟优化，有效解决了CGYRO在HPC环境中的内存和通信问题。

Abstract: First-principles fusion plasma simulations are both compute and memory
intensive, and CGYRO is no exception. The use of many HPC nodes to fit the
problem in the available memory thus results in significant communication
overhead, which is hard to avoid for any single simulation. That said, most
fusion studies are composed of ensembles of simulations, so we developed a new
tool, named XGYRO, that executes a whole ensemble of CGYRO simulations as a
single HPC job. By treating the ensemble as a unit, XGYRO can alter the global
buffer distribution logic and apply optimizations that are not feasible on any
single simulation, but only on the ensemble as a whole. The main saving comes
from the sharing of the collisional constant tensor structure, since its values
are typically identical between parameter-sweep simulations. This data
structure dominates the memory consumption of CGYRO simulations, so
distributing it among the whole ensemble results in drastic memory savings for
each simulation, which in turn results in overall lower communication overhead.

</details>


### [54] [Towards Experiment Execution in Support of Community Benchmark Workflows for HPC](https://arxiv.org/abs/2507.22294)
*Gregor von Laszewski,Wesley Brewer,Sean R. Wilkinson,Andrew Shao,J. P. Fleischer,Harshad Pitkar,Christine R. Kirkpatrick,Geoffrey C. Fox*

Main category: cs.DC

TL;DR: 论文提出了一种名为工作流模板的解决方案，用于解决在有限基准测试下展示计算资源能力的问题。通过MLCommons Science工作组的经验，总结了常见的使用模式，并通过两个独立工具验证了提升适应性的‘基准木工’概念。


<details>
  <summary>Details</summary>
Motivation: 解决在有限基准测试下展示计算资源能力的难题，提高计算工作流的适应性，特别是在教育领域。

Method: 提出了工作流模板的概念，总结了常见使用模式，并通过Cloudmesh的实验执行器和HPE的SmartSim两个工具进行验证。

Result: 工作流模板和‘基准木工’概念在多个科学应用中得到验证，包括云掩模、地震预测、模拟-AI/ML交互和计算流体动力学代理开发。

Conclusion: 工作流模板和‘基准木工’概念是提升计算资源适配性和教育领域应用的有效解决方案，得到工具验证的广泛支持。

Abstract: A key hurdle is demonstrating compute resource capability with limited
benchmarks. We propose workflow templates as a solution, offering adaptable
designs for specific scientific applications. Our paper identifies common usage
patterns for these templates, drawn from decades of HPC experience, including
recent work with the MLCommons Science working group.
  We found that focusing on simple experiment management tools within the
broader computational workflow improves adaptability, especially in education.
This concept, which we term benchmark carpentry, is validated by two
independent tools: Cloudmesh's Experiment Executor and Hewlett Packard
Enterprise's SmartSim. Both frameworks, with significant functional overlap,
have been tested across various scientific applications, including conduction
cloudmask, earthquake prediction, simulation-AI/ML interactions, and the
development of computational fluid dynamics surrogates.

</details>


### [55] [A Semi-Supervised Federated Learning Framework with Hierarchical Clustering Aggregation for Heterogeneous Satellite Networks](https://arxiv.org/abs/2507.22339)
*Zhuocheng Liu,Zhishu Shen,Qiushi Zheng,Tiehua Zhang,Zheng Lei,Jiong Jin*

Main category: cs.DC

TL;DR: 本文提出了一种面向低轨卫星网络的半监督联邦学习框架，通过分层聚合和通信优化技术显著降低了处理时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 低轨卫星网络资源受限且异构，传统联邦学习难以实现可靠收敛和高效通信，需要针对性的解决方案。

Method: 采用分层聚类聚合（卫星集群和地面站聚合两阶段），并结合稀疏化和自适应权重量化以减少通信开销。

Result: 实验表明，该方法在处理时间（提升3倍）和能耗（提升4倍）上优于其他方法，同时保持了模型准确性。

Conclusion: 该框架为低轨卫星网络中的分布式智能提供了高效且实用的解决方案。

Abstract: Low Earth Orbit (LEO) satellites are emerging as key components of 6G
networks, with many already deployed to support large-scale Earth observation
and sensing related tasks. Federated Learning (FL) presents a promising
paradigm for enabling distributed intelligence in these resource-constrained
and dynamic environments. However, achieving reliable convergence, while
minimizing both processing time and energy consumption, remains a substantial
challenge, particularly in heterogeneous and partially unlabeled satellite
networks. To address this challenge, we propose a novel semi-supervised
federated learning framework tailored for LEO satellite networks with
hierarchical clustering aggregation. To further reduce communication overhead,
we integrate sparsification and adaptive weight quantization techniques. In
addition, we divide the FL clustering into two stages: satellite cluster
aggregation stage and Ground Stations (GSs) aggregation stage. The supervised
learning at GSs guides selected Parameter Server (PS) satellites, which in turn
support fully unlabeled satellites during the federated training process.
Extensive experiments conducted on a satellite network testbed demonstrate that
our proposal can significantly reduce processing time (up to 3x) and energy
consumption (up to 4x) compared to other comparative methods while maintaining
model accuracy.

</details>


### [56] [Leveraging Caliper and Benchpark to Analyze MPI Communication Patterns: Insights from AMG2023, Kripke, and Laghos](https://arxiv.org/abs/2507.22372)
*Grace Nansamba,Evelyn Namugwanya,David Boehme,Dewi Yokelson,Riley Shipley,Derek Schafer,Michael McKinsey,Olga Pearce,Anthony Skjellum*

Main category: cs.DC

TL;DR: 在Caliper HPC分析工具中引入“通信区域”，以捕获通信数据和MPI进程的指标，通过三个应用验证其效用，揭示通信瓶颈和行为差异。


<details>
  <summary>Details</summary>
Motivation: 扩展Caliper功能，以更详细地分析和可视化MPI通信行为，解决传统工具无法捕获的通信数据和进程指标问题。

Method: 通过引入“通信区域”概念，并利用Caliper和Thicket工具结合，分析AMG2023、Kripke和Laghos等应用的通信模式。

Result: 揭示了通信瓶颈和详细行为，展示了不同区域内应用的扩展性和消息流量差异，验证了新方法的实用性。

Conclusion: 通信区域的引入显著提升了Caliper对MPI通信行为的分析能力，为高性能计算提供了更深入的工具支持。

Abstract: We introduce ``communication regions'' into the widely used Caliper HPC
profiling tool. A communication region is an annotation enabling capture of
metrics about the data being communicated (including statistics of these
metrics), and metrics about the MPI processes involved in the communications,
something not previously possible in Caliper. We explore the utility of
communication regions with three representative modeling and simulation
applications, AMG2023, Kripke, and Laghos, all part of the comprehensive
Benchpark suite that includes Caliper annotations. Enhanced Caliper reveals
detailed communication behaviors. Using Caliper and Thicket in tandem, we
create new visualizations of MPI communication patterns, including halo
exchanges. Our findings reveal communication bottlenecks and detailed
behaviors, indicating significant utility of the special-regions addition to
Caliper. The comparative scaling behavior of both CPU and GPU oriented systems
are shown; we are able to look at different regions within a given application,
and see how scalability and message-traffic metrics differ.

</details>


### [57] [DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic Space Partitioning with Erasure Code](https://arxiv.org/abs/2507.22801)
*Shubhradeep Roy,Suvarthi Sarkar,Vivek Verma,Aryabartta Sahu*

Main category: cs.DC

TL;DR: 提出了一种基于利润驱动的边缘存储系统框架，通过协同缓存、纠删码和弹性存储分区提升低延迟数据访问效率。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器的存储容量有限，难以应对高流量和低延迟的数据访问需求，尤其是在动态工作负载下。

Method: 整合了协同缓存、纠删码和弹性存储分区机制，动态划分存储为私有和公共区域，并设计数据放置与替换策略。

Result: 实验表明，该方法在Netflix和Spotify的真实数据上提升了系统利润5-8%。

Conclusion: 该框架显著提升了边缘存储系统的性能和盈利能力。

Abstract: Edge Storage Systems have emerged as a critical enabler of low latency data
access in modern cloud networks by bringing storage and computation closer to
end users. However, the limited storage capacity of edge servers poses
significant challenges in handling high volume and latency sensitive data
access requests, particularly under dynamic workloads. In this work, we propose
a profit driven framework that integrates three key mechanisms which are
collaborative caching, erasure coding, and elastic storage partitioning. Unlike
traditional replication, erasure coding enables space efficient redundancy,
allowing data to be reconstructed from any subset of K out of K plus M coded
blocks. We dynamically partition each edge server s storage into private and
public regions. The private region is further subdivided among access points
based on their incoming request rates, enabling adaptive control over data
locality and ownership. We design a data placement and replacement policy that
determines how and where to store or evict coded data blocks to maximize data
access within deadlines. While the private region serves requests from local
APs, the public region handles cooperative storage requests from neighboring
servers. Our proposed Dynamic Space Partitioning and Elastic caching strategy
is evaluated on both synthetic and real world traces from Netflix and Spotify.
Experimental results show that our method improves overall system profitability
by approximately 5 to 8% compared to state of the art approaches under varied
workload conditions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [58] [Compact Answers to Temporal Path Queries](https://arxiv.org/abs/2507.22143)
*Muhammad Adnan,Diego Calvanese,Julien Corman,Anton Dignös,Werner Nutt,Ognjen Savković*

Main category: cs.DB

TL;DR: 论文研究了基于路径的图查询，引入时间导航以分析网络动态，探讨了四种紧凑表示方法及其优缺点。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过时间导航分析网络动态（如交通、疾病传播），并解决紧凑表示和查询效率问题。

Method: 研究四种基于区间集的紧凑表示方法，比较其简洁性、唯一性和计算成本。

Result: 最精细的编码方法可在稠密时间下有限表示查询结果。

Conclusion: 四种方法各有优劣，最精细的编码方法适用于稠密时间场景。

Abstract: We study path-based graph queries that, in addition to navigation through
edges, also perform navigation through time. This allows asking questions about
the dynamics of networks, like traffic movement, cause-effect relationships, or
the spread of a disease. In this setting, a graph consists of triples annotated
with validity intervals, and a query produces pairs of nodes where each pair is
associated with a binary relation over time. For instance, such a pair could be
two airports, and the temporal relation could map potential departure times to
possible arrival times. An open question is how to represent such a relation in
a compact form and maintain this property during query evaluation. We
investigate four compact representations of answers to a such queries, which
are based on alternative ways to encode sets of intervals. We discuss their
respective advantages and drawbacks, in terms of conciseness, uniqueness, and
computational cost. Notably, the most refined encoding guarantees that query
answers over dense time can be finitely represented.

</details>


### [59] [Is SHACL Suitable for Data Quality Assessment?](https://arxiv.org/abs/2507.22305)
*Carolina Cortés Lasalle,Lisa Ehrlinger,Lorena Etcheverry,Felix Naumann*

Main category: cs.DB

TL;DR: 本文探讨了如何使用SHACL语言评估知识图谱的数据质量，填补了现有方法未能全面覆盖所有数据质量维度的空白。


<details>
  <summary>Details</summary>
Motivation: 知识图谱因缺乏强制模式可能导致数据质量问题，现有方法未能全面评估质量维度。

Method: 作者定义并实现了69个基于SHACL的数据质量指标原型，并提供可重复资源。

Result: 通过SHACL形状验证结果计算数据质量度量，展示了可行性。

Conclusion: SHACL可用于全面评估知识图谱数据质量，提供了实用工具与资源。

Abstract: Knowledge graphs have been widely adopted in both enterprises, such as the
Google Knowledge Graph, and open platforms like Wikidata to represent domain
knowledge and support analysis with artificial intelligence. They model
real-world information as nodes and edges. To embrace flexibility, knowledge
graphs often lack enforced schemas (i.e., ontologies), leading to potential
data quality issues, such as semantically overlapping nodes. Therefore,
ensuring their quality is essential, as issues in the data can affect
applications relying on them. To assess the quality of knowledge graphs,
existing works either propose high-level frameworks comprising various data
quality dimensions without concrete implementations, define tools that measure
data quality with ad-hoc SPARQL (SPARQL Protocol and RDF Query Language)
queries, or promote the usage of constraint languages, such as the Shapes
Constraint Language (SHACL), to assess and improve the quality of the graph.
Although the latter approaches claim to address data quality assessment, none
of them comprehensively tries to cover all data quality dimensions. In this
paper, we explore this gap by investigating the extent to which SHACL can be
used to assess data quality in knowledge graphs. Specifically, we defined SHACL
shapes for 69 data quality metrics proposed by Zaveri et al. [1] and
implemented a prototype that automatically instantiates these shapes and
computes the corresponding data quality measures from their validation results.
All resources are provided for repeatability at
https://github.com/caroocortes/SHACL-DQA-prototype/tree/main

</details>


### [60] [Scalability, Availability, Reproducibility and Extensibility in Islamic Database Systems](https://arxiv.org/abs/2507.22384)
*Umar Siddiqui,Habiba Youssef,Adel Sabour,Mohamed Ali*

Main category: cs.DB

TL;DR: 论文介绍了QuranResearch.Org系统，旨在解决伊斯兰知识领域软件系统中的真实性、准确性、可扩展性和可用性等问题。


<details>
  <summary>Details</summary>
Motivation: 随着伊斯兰知识领域软件系统的普及，系统的真实性、准确性、可扩展性和可用性等问题日益突出。

Method: 提出QuranResearch.Org系统，旨在通过其设计和愿景解决上述问题。

Result: 系统致力于提供可扩展、可用、可重现和可扩展的解决方案。

Conclusion: QuranResearch.Org系统为伊斯兰数据库系统提供了可持续的解决方案。

Abstract: With the widespread of software systems and applications that serve the
Islamic knowledge domain, several concerns arise. Authenticity and accuracy of
the databases that back up these systems are questionable. With the excitement
that some software developers and amateur researchers may have, false
statements and incorrect claims may be made around numerical signs or miracles
in the Quran. Reproducibility of these claims may not be addressed by the
people making such claims. Moreover, with the increase in the number of users,
scalability and availability of these systems become a concern. In addition to
all these concerns, extensibility is also another major issue. Properly
designed systems can be extensible, reusable and built on top of one another,
instead of each system being built from scratch every time a new framework is
developed. In this paper, we introduce the QuranResearch.Org system and its
vision for scalability, availability, reproducibility and extensibility to
serve Islamic database systems.

</details>


### [61] [Systematic Evaluation of Knowledge Graph Repair with Large Language Models](https://arxiv.org/abs/2507.22419)
*Tung-Wei Lin,Gabe Fierro,Han Li,Tianzhen Hong,Pierluigi Nuzzo,Alberto Sangiovanni-Vinentelli*

Main category: cs.DB

TL;DR: 提出了一种系统性评估知识图谱修复质量的方法，通过violation-inducing operations（VIOs）生成违反约束的案例，并评估多种修复系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖特定数据集，无法全面分析修复系统，需要更通用的评估框架。

Method: 使用VIOs系统性生成违反SHACL约束的数据，并基于大型语言模型构建修复系统，比较不同提示策略的效果。

Result: 包含SHACL约束和知识图谱关键上下文的简洁提示表现最佳。

Conclusion: 系统性生成违反约束的数据是评估修复系统的有效方法，简洁且上下文丰富的提示能提升性能。

Abstract: We present a systematic approach for evaluating the quality of knowledge
graph repairs with respect to constraint violations defined in shapes
constraint language (SHACL). Current evaluation methods rely on \emph{ad hoc}
datasets, which limits the rigorous analysis of repair systems in more general
settings. Our method addresses this gap by systematically generating violations
using a novel mechanism, termed violation-inducing operations (VIOs). We use
the proposed evaluation framework to assess a range of repair systems which we
build using large language models. We analyze the performance of these systems
across different prompting strategies. Results indicate that concise prompts
containing both the relevant violated SHACL constraints and key contextual
information from the knowledge graph yield the best performance.

</details>


### [62] [SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases](https://arxiv.org/abs/2507.22701)
*Haoran Zhang,Decheng Zuo,Yu Yan,Zhiyu Liang,Hongzhi Wang*

Main category: cs.DB

TL;DR: SAM是一个基于AURA算法的自主缓存管理器，通过结合历史和未来收益的双因素模型，在动态工作负载下实现稳定高效。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限的边缘节点上多数据库实例共存导致的缓存争用问题，传统方法在动态工作负载下效率低且不稳定。

Method: 提出AURA算法，引入H-factor（历史稳定性）和V-factor（未来潜在收益）的双因素模型，通过自适应权重实现稳定优化。

Result: 实验显示，SAM在吞吐量、抗工作负载变化和缓存污染攻击方面优于14种基线方法，且决策延迟可扩展。

Conclusion: 稳定性优先的设计对大规模系统的持续高性能至关重要。

Abstract: The co-location of multiple database instances on resource constrained edge
nodes creates significant cache contention, where traditional schemes are
inefficient and unstable under dynamic workloads. To address this, we present
SAM, an autonomic cache manager powered by our novel AURA algorithm. AURA makes
stability a first-class design principle by resolving the
exploitation-exploration dilemma: it achieves this by synthesizing two
orthogonal factors, which we introduce as: the H-factor, representing a
database's proven, historically stable efficiency (exploitation), and the
V-factor, representing its empirically estimated marginal gain for future
improvements (exploration). This dual-factor model, governed by an adaptive
weight, enables SAM to achieve sustained high performance through strategic
stability and robustness in volatile conditions.
  Extensive experiments against 14 diverse baselines demonstrate SAM's
superiority. It achieves top-tier throughput while being uniquely resilient to
complex workload shifts and cache pollution attacks. Furthermore, its decision
latency is highly scalable, remaining nearly constant as the system grows to
120 databases. Crucially, SAM achieves superior decision stability --
maintaining consistent optimization directions despite noise, avoiding
performance oscillations while ensuring predictable Quality of Service. These
results prove that a principled, stability-aware design is essential for
sustained high performance in real-world, large-scale systems.

</details>


### [63] [CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2507.19802)
*Ziyu Zhang,Yuanhao Wei,Joshua Engels,Julian Shun*

Main category: cs.DB

TL;DR: 论文提出了一种动态图索引系统CleANN，显著提升了近似最近邻搜索的性能和质量。


<details>
  <summary>Details</summary>
Motivation: 现有图索引多为静态设计，动态更新时性能或质量下降，CleANN旨在解决这一问题。

Method: 结合三种技术：工作负载感知链接、查询自适应邻域合并和半延迟内存清理。

Result: 在多种数据集上，CleANN在高并发下性能提升7-1200倍，且质量与静态索引相当。

Conclusion: CleANN是首个在完全动态场景下同时保持高效和高质的并发ANNS索引。

Abstract: Approximate nearest neighbor search (ANNS) has become a quintessential
algorithmic problem for various other foundational data tasks for AI workloads.
Graph-based ANNS indexes have superb empirical trade-offs in indexing cost,
query efficiency, and query approximation quality. Most existing graph-based
indexes are designed for the static scenario, where there are no updates to the
data after the index is constructed. However, full dynamism (insertions,
deletions, and searches) is crucial to providing up-to-date responses in
applications using vector databases. It is desirable that the index efficiently
supports updates and search queries concurrently. Existing dynamic graph-based
indexes suffer from at least one of the following problems: (1) the query
quality degrades as updates happen; and (2) the graph structure updates used to
maintain the index quality upon updates are global and thus expensive. To solve
these problems, we propose the CleANN system which consists of three main
components: (1) workload-aware linking of diverse search tree descendants to
combat distribution shift; (2)query-adaptive on-the-fly neighborhood
consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory
cleaning to clean up stale information in the data structure and reduce the
work spent by the first two components. We evaluate CleANN on 7 diverse
datasets on fully dynamic workloads and find that CleANN has query quality at
least as good as if the index had been built statically using the corresponding
data. In the in-memory setting using 56 hyper-threads, with all types of
queries running concurrently, at the same recall level, CleANN achieves 7-1200x
throughput improvement on million-scale real-world datasets. To the best of our
knowledge, CleANN is the first concurrent ANNS index to achieve such efficiency
while maintaining quality under full dynamism.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [64] [A Customized Memory-aware Architecture for Biological Sequence Alignment](https://arxiv.org/abs/2507.22221)
*Nasrin Akbari,Mehdi Modarressi,Alireza Khadem*

Main category: cs.AR

TL;DR: 本文提出了一种内存感知架构，以减少序列比对算法的带宽需求，并通过3D DRAM中的内存计算进一步提速，实现了2.4倍的速度提升和37%的功耗降低。


<details>
  <summary>Details</summary>
Motivation: 随着生物信息学数据量的指数增长，序列比对算法的时间和资源需求急剧增加，而传统的并行处理架构难以满足其高内存带宽需求。

Method: 设计了一种内存感知架构，并通过集成到3D DRAM的逻辑层中，实现内存计算，以减少带宽需求。

Result: 实验结果显示，该架构比基于GPU的设计提速2.4倍，平均功耗降低37%。

Conclusion: 通过内存感知架构和内存计算，有效解决了序列比对算法中的内存带宽瓶颈问题，显著提升了性能和能效。

Abstract: Sequence alignment is a fundamental process in computational biology which
identifies regions of similarity in biological sequences. With the exponential
growth in the volume of data in bioinformatics databases, the time, processing
power, and memory bandwidth for comparing a query sequence with the available
databases grows proportionally. The sequence alignment algorithms often involve
simple arithmetic operations and feature high degrees of inherent fine-grained
and coarse-grained parallelism. These features can be potentially exploited by
a massive parallel processor, such as a GPU, to increase throughput. In this
paper, we show that the excessive memory bandwidth demand of the sequence
alignment algorithms prevents exploiting the maximum achievable throughput on
conventional parallel machines. We then propose a memory-aware architecture to
reduce the bandwidth demand of the sequence alignment algorithms, effectively
pushing the memory wall to extract higher throughput. The design is integrated
at the logic layer of an emerging 3D DRAM as a processing-in-memory
architecture to further increase the available bandwidth. The experimental
results show that the proposed architecture results in up to 2.4x speedup over
a GPU-based design. Moreover, by moving the computation closer to the memory,
power consumption is reduced by 37%, on average.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [65] [Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors](https://arxiv.org/abs/2507.22367)
*Jia Li,Yichao He,Jiacheng Xu,Tianhao Luo,Zhenzhen Hu,Richang Hong,Meng Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为Traits Run Deep的新框架，利用心理学提示和多模态融合技术提升人格评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 人格评估在情感智能、心理健康诊断等领域至关重要，但传统方法难以建模多模态异步信号。

Method: 采用心理学提示引导LLMs提取人格语义，并设计Text-Centric Trait Fusion Network融合多模态信号。

Result: 实验显示MSE降低约45%，并在AVI Challenge 2025测试集上排名第一。

Conclusion: 该框架显著提升了人格评估的准确性和跨模态理解能力。

Abstract: Accurate and reliable personality assessment plays a vital role in many
fields, such as emotional intelligence, mental health diagnostics, and
personalized education. Unlike fleeting emotions, personality traits are
stable, often subconsciously leaked through language, facial expressions, and
body behaviors, with asynchronous patterns across modalities. It was hard to
model personality semantics with traditional superficial features and seemed
impossible to achieve effective cross-modal understanding. To address these
challenges, we propose a novel personality assessment framework called
\textit{\textbf{Traits Run Deep}}. It employs
\textit{\textbf{psychology-informed prompts}} to elicit high-level
personality-relevant semantic representations. Besides, it devises a
\textit{\textbf{Text-Centric Trait Fusion Network}} that anchors rich text
semantics to align and integrate asynchronous signals from other modalities. To
be specific, such fusion module includes a Chunk-Wise Projector to decrease
dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for
effective modality fusion and an ensemble regression head to improve
generalization in data-scarce situations. To our knowledge, we are the first to
apply personality-specific prompts to guide large language models (LLMs) in
extracting personality-aware semantics for improved representation quality.
Furthermore, extracting and fusing audio-visual apparent behavior features
further improves the accuracy. Experimental results on the AVI validation set
have demonstrated the effectiveness of the proposed components, i.e.,
approximately a 45\% reduction in mean squared error (MSE). Final evaluations
on the test set of the AVI Challenge 2025 confirm our method's superiority,
ranking first in the Personality Assessment track. The source code will be made
available at https://github.com/MSA-LMC/TraitsRunDeep.

</details>


### [66] [Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment](https://arxiv.org/abs/2507.22676)
*Jia Li,Yang Wang,Wenhao Qian,Zhenzhen Hu,Richang Hong,Meng Wang*

Main category: cs.CL

TL;DR: 提出了一种新颖的面试评估框架，整合了视频、音频和文本三种模态，通过多模态特征融合和两级集成学习实现了全面且公平的评估。


<details>
  <summary>Details</summary>
Motivation: 为了确保面试评估的全面性和公平性，探索了365个面试表现方面的多维数据。

Method: 采用多模态特征提取器和共享压缩多层感知机融合数据，结合两级集成学习策略（独立回归头和均值池化）进行评分预测。

Result: 在多维平均MSE为0.1824的成绩下，框架在AVI Challenge 2025中排名第一。

Conclusion: 该框架通过捕捉显性和隐性线索，有效提升了自动化和多模态面试评估的准确性与鲁棒性。

Abstract: Interview performance assessment is essential for determining candidates'
suitability for professional positions. To ensure holistic and fair
evaluations, we propose a novel and comprehensive framework that explores
``365'' aspects of interview performance by integrating \textit{three}
modalities (video, audio, and text), \textit{six} responses per candidate, and
\textit{five} key evaluation dimensions. The framework employs
modality-specific feature extractors to encode heterogeneous data streams and
subsequently fused via a Shared Compression Multilayer Perceptron. This module
compresses multimodal embeddings into a unified latent space, facilitating
efficient feature interaction. To enhance prediction robustness, we incorporate
a two-level ensemble learning strategy: (1) independent regression heads
predict scores for each response, and (2) predictions are aggregated across
responses using a mean-pooling mechanism to produce final scores for the five
target dimensions. By listening to the unspoken, our approach captures both
explicit and implicit cues from multimodal data, enabling comprehensive and
unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our
framework secured first place in the AVI Challenge 2025, demonstrating its
effectiveness and robustness in advancing automated and multimodal interview
performance assessment. The full implementation is available at
https://github.com/MSA-LMC/365Aspects.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [67] [Of Good Demons and Bad Angels: Guaranteeing Safe Control under Finite Precision](https://arxiv.org/abs/2507.22760)
*Samuel Teuber,Debasmita Lohar,Bernhard Beckert*

Main category: eess.SY

TL;DR: 该论文提出了一种考虑有限精度扰动的神经网络控制系统无限时间安全验证方法，填补了理论保证与现实实现之间的差距。


<details>
  <summary>Details</summary>
Motivation: 神经网络在安全关键系统中广泛应用，但其安全性需要无限时间验证。现有的方法忽略了有限精度实现中的舍入误差，无法保证实际安全性。

Method: 通过差分动态逻辑（dL）和混合博弈模型（好恶魔与坏天使），结合有限精度扰动，提出一种鲁棒性验证框架，并利用混合精度固定点调谐器生成高效实现。

Result: 在汽车和航空领域案例中，成功生成具有无限时间安全保证的高效神经网络实现。

Conclusion: 该方法为神经网络控制系统的实际部署提供了完整的端到端安全验证解决方案。

Abstract: As neural networks (NNs) become increasingly prevalent in safety-critical
neural network-controlled cyber-physical systems (NNCSs), formally guaranteeing
their safety becomes crucial. For these systems, safety must be ensured
throughout their entire operation, necessitating infinite-time horizon
verification. To verify the infinite-time horizon safety of NNCSs, recent
approaches leverage Differential Dynamic Logic (dL). However, these dL-based
guarantees rely on idealized, real-valued NN semantics and fail to account for
roundoff errors introduced by finite-precision implementations. This paper
bridges the gap between theoretical guarantees and real-world implementations
by incorporating robustness under finite-precision perturbations -- in sensing,
actuation, and computation -- into the safety verification. We model the
problem as a hybrid game between a good Demon, responsible for control actions,
and a bad Angel, introducing perturbations. This formulation enables formal
proofs of robustness w.r.t. a given (bounded) perturbation. Leveraging this
bound, we employ state-of-the-art mixed-precision fixed-point tuners to
synthesize sound and efficient implementations, thus providing a complete
end-to-end solution. We evaluate our approach on case studies from the
automotive and aeronautics domains, producing efficient NN implementations with
rigorous infinite-time horizon safety guarantees.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [SourceSplice: Source Selection for Machine Learning Tasks](https://arxiv.org/abs/2507.22186)
*Ambarish Singh,Romila Pradhan*

Main category: cs.LG

TL;DR: 论文提出SourceGrasp和SourceSplice框架，用于高效选择数据源子集以优化机器学习任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据发现方法未考虑数据源质量对下游ML任务的影响，本文旨在解决这一问题。

Method: 提出SourceGrasp（基于贪心准则和随机化）和SourceSplice（受基因拼接启发）两种算法，用于数据源选择。

Result: 实证表明SourceSplice能在较少子集探索下，高效识别提升任务效用的数据源子集。

Conclusion: SourceSplice在不同设置下表现稳健，有效解决了数据源选择问题。

Abstract: Data quality plays a pivotal role in the predictive performance of machine
learning (ML) tasks - a challenge amplified by the deluge of data sources
available in modern organizations.Prior work in data discovery largely focus on
metadata matching, semantic similarity or identifying tables that should be
joined to answer a particular query, but do not consider source quality for
high performance of the downstream ML task.This paper addresses the problem of
determining the best subset of data sources that must be combined to construct
the underlying training dataset for a given ML task.We propose SourceGrasp and
SourceSplice, frameworks designed to efficiently select a suitable subset of
sources that maximizes the utility of the downstream ML model.Both the
algorithms rely on the core idea that sources (or their combinations)
contribute differently to the task utility, and must be judiciously
chosen.While SourceGrasp utilizes a metaheuristic based on a greediness
criterion and randomization, the SourceSplice framework presents a source
selection mechanism inspired from gene splicing - a core concept used in
protein synthesis.We empirically evaluate our algorithms on three real-world
datasets and synthetic datasets and show that, with significantly fewer subset
explorations, SourceSplice effectively identifies subsets of data sources
leading to high task utility.We also conduct studies reporting the sensitivity
of SourceSplice to the decision choices under several settings.

</details>


### [69] [Thermodynamics-Inspired Computing with Oscillatory Neural Networks for Inverse Matrix Computation](https://arxiv.org/abs/2507.22544)
*George Tsormpatzoglou,Filip Sabo,Aida Todri-Sanial*

Main category: cs.LG

TL;DR: 论文提出了一种基于振荡神经网络（ONN）的热力学启发计算范式，用于解决线性代数中的逆矩阵问题，并通过理论分析和数值模拟验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 研究振荡神经网络在解决线性代数问题（特别是逆矩阵）中的潜力，结合热力学原理，探索新的计算范式。

Method: 利用耦合Kuramoto振荡器模型的线性近似推导出逆矩阵解，并进行数值模拟验证理论框架。

Result: 数值模拟验证了理论框架的有效性，并确定了计算精度最高的参数范围。

Conclusion: 该工作展示了ONN在解决线性代数问题中的潜力，为未来计算范式提供了新思路。

Abstract: We describe a thermodynamic-inspired computing paradigm based on oscillatory
neural networks (ONNs). While ONNs have been widely studied as Ising machines
for tackling complex combinatorial optimization problems, this work
investigates their feasibility in solving linear algebra problems, specifically
the inverse matrix. Grounded in thermodynamic principles, we analytically
demonstrate that the linear approximation of the coupled Kuramoto oscillator
model leads to the inverse matrix solution. Numerical simulations validate the
theoretical framework, and we examine the parameter regimes that computation
has the highest accuracy.

</details>


### [70] [Hybrid activation functions for deep neural networks: S3 and S4 -- a novel approach to gradient flow optimization](https://arxiv.org/abs/2507.22090)
*Sergii Kavun*

Main category: cs.LG

TL;DR: 论文提出两种新型混合激活函数S3和S4，通过结合sigmoid和softsign解决了传统激活函数如ReLU的死神经元问题以及sigmoid和tanh的梯度消失问题。S4在多种任务和网络结构中表现出色，提供稳定梯度流和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统激活函数（如ReLU、sigmoid、tanh）存在死神经元和梯度消失等问题，限制了深度神经网络的性能。论文旨在通过设计新型混合激活函数来改善这些问题。

Method: 提出两种混合激活函数：S3（结合sigmoid和softsign）和其改进版S4（平滑过渡机制，带可调参数k）。实验覆盖二分类、多分类和回归任务，使用三种不同网络架构。

Result: S4在MNIST、Iris分类和Boston Housing回归任务中表现优异，准确率分别为97.4%、96.0%，MSE为18.7。收敛速度比ReLU快19倍，梯度范围稳定在[0.24, 0.59]。

Conclusion: S4通过混合设计和平滑过渡机制解决了现有激活函数的局限性，其可调参数k使其适用于不同任务和网络深度，为神经网络训练提供了新方向。

Abstract: Activation functions are critical components in deep neural networks,
directly influencing gradient flow, training stability, and model performance.
Traditional functions like ReLU suffer from dead neuron problems, while sigmoid
and tanh exhibit vanishing gradient issues. We introduce two novel hybrid
activation functions: S3 (Sigmoid-Softsign) and its improved version S4
(smoothed S3). S3 combines sigmoid for negative inputs with softsign for
positive inputs, while S4 employs a smooth transition mechanism controlled by a
steepness parameter k. We conducted comprehensive experiments across binary
classification, multi-class classification, and regression tasks using three
different neural network architectures. S4 demonstrated superior performance
compared to nine baseline activation functions, achieving 97.4% accuracy on
MNIST, 96.0% on Iris classification, and 18.7 MSE on Boston Housing regression.
The function exhibited faster convergence (-19 for ReLU) and maintained stable
gradient flow across network depths. Comparative analysis revealed S4's
gradient range of [0.24, 0.59] compared to ReLU's 18% dead neurons in deep
networks. The S4 activation function addresses key limitations of existing
functions through its hybrid design and smooth transition mechanism. The
tunable parameter k allows adaptation to different tasks and network depths,
making S4 a versatile choice for deep learning applications. These findings
suggest that hybrid activation functions represent a promising direction for
improving neural network training dynamics.

</details>


### [71] [Hypernetworks for Model-Heterogeneous Personalized Federated Learning](https://arxiv.org/abs/2507.22330)
*Chen Zhang,Husheng Li,Xiang Liu,Linshan Jiang,Danxin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于超网络的个性化联邦学习框架MH-pFedHN，通过服务器端超网络为不同客户端生成个性化参数，并引入多头结构以促进知识共享。进一步提出MH-pFedHNGD，结合轻量级全局模型提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有个性化联邦学习方法依赖外部数据或模型解耦等限制，提升实用性和可扩展性。

Method: 利用超网络的强泛化能力，设计多头结构，并为客户端生成个性化参数。

Result: 在多个基准测试和模型设置中展现出竞争力，具有强泛化能力。

Conclusion: 提供了一种无需外部数据且保护隐私的模型异质个性化联邦学习框架，为未来研究奠定基础。

Abstract: Recent advances in personalized federated learning have focused on addressing
client model heterogeneity. However, most existing methods still require
external data, rely on model decoupling, or adopt partial learning strategies,
which can limit their practicality and scalability. In this paper, we revisit
hypernetwork-based methods and leverage their strong generalization
capabilities to design a simple yet effective framework for heterogeneous
personalized federated learning. Specifically, we propose MH-pFedHN, which
leverages a server-side hypernetwork that takes client-specific embedding
vectors as input and outputs personalized parameters tailored to each client's
heterogeneous model. To promote knowledge sharing and reduce computation, we
introduce a multi-head structure within the hypernetwork, allowing clients with
similar model sizes to share heads. Furthermore, we further propose
MH-pFedHNGD, which integrates an optional lightweight global model to improve
generalization. Our framework does not rely on external datasets and does not
require disclosure of client model architectures, thereby offering enhanced
privacy and flexibility. Extensive experiments on multiple benchmarks and model
settings demonstrate that our approach achieves competitive accuracy, strong
generalization, and serves as a robust baseline for future research in
model-heterogeneous personalized federated learning.

</details>


### [72] [CTG-Insight: A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification](https://arxiv.org/abs/2507.22205)
*Black Sun,Die,Hu*

Main category: cs.LG

TL;DR: CTG-Insight是一种利用多智能体LLM系统解释胎儿心率和宫缩信号的技术，提供透明且可解释的输出，并达到高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的远程胎儿监测系统缺乏解释性，难以满足父母的需求，因此需要一种能提供结构化解释的技术。

Method: 采用多智能体LLM系统分解CTG信号为五个医学特征，并通过聚合智能体提供整体健康分类和自然语言解释。

Result: 在NeuroFetalNet数据集上，CTG-Insight达到96.4%的准确率和97.8%的F1分数，优于深度学习和单智能体基线。

Conclusion: 该研究提供了一个可解释且可扩展的CTG分析框架，显著提升了胎儿健康监测的透明度和准确性。

Abstract: Remote fetal monitoring technologies are becoming increasingly common. Yet,
most current systems offer limited interpretability, leaving expectant parents
with raw cardiotocography (CTG) data that is difficult to understand. In this
work, we present CTG-Insight, a multi-agent LLM system that provides structured
interpretations of fetal heart rate (FHR) and uterine contraction (UC) signals.
Drawing from established medical guidelines, CTG-Insight decomposes each CTG
trace into five medically defined features: baseline, variability,
accelerations, decelerations, and sinusoidal pattern, each analyzed by a
dedicated agent. A final aggregation agent synthesizes the outputs to deliver a
holistic classification of fetal health, accompanied by a natural language
explanation. We evaluate CTG-Insight on the NeuroFetalNet Dataset and compare
it against deep learning models and the single-agent LLM baseline. Results show
that CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score
(97.8%) while producing transparent and interpretable outputs. This work
contributes an interpretable and extensible CTG analysis framework.

</details>


### [73] [Cluster-Based Random Forest Visualization and Interpretation](https://arxiv.org/abs/2507.22665)
*Max Sondag,Christofer Meinecke,Dennis Collaris,Tatiana von Landesberger,Stef van den Elzen*

Main category: cs.LG

TL;DR: 提出了一种可视化方法和系统，通过聚类相似树并引入新的距离度量，提升随机森林的可解释性。


<details>
  <summary>Details</summary>
Motivation: 随机森林性能高但难以解释，需改进其可解释性。

Method: 聚类相似树，引入新距离度量及两种可视化方法：特征图和规则图。

Result: 在Glass数据集和小型用户研究中验证了方法的有效性。

Conclusion: 新方法提高了随机森林的可解释性，便于用户理解模型。

Abstract: Random forests are a machine learning method used to automatically classify
datasets and consist of a multitude of decision trees. While these random
forests often have higher performance and generalize better than a single
decision tree, they are also harder to interpret. This paper presents a
visualization method and system to increase interpretability of random forests.
We cluster similar trees which enables users to interpret how the model
performs in general without needing to analyze each individual decision tree in
detail, or interpret an oversimplified summary of the full forest. To
meaningfully cluster the decision trees, we introduce a new distance metric
that takes into account both the decision rules as well as the predictions of a
pair of decision trees. We also propose two new visualization methods that
visualize both clustered and individual decision trees: (1) The Feature Plot,
which visualizes the topological position of features in the decision trees,
and (2) the Rule Plot, which visualizes the decision rules of the decision
trees. We demonstrate the efficacy of our approach through a case study on the
"Glass" dataset, which is a relatively complex standard machine learning
dataset, as well as a small user study.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [74] [Efficient handover based on Near-field and Far-field RIS for seamless connectivity](https://arxiv.org/abs/2507.22141)
*Atiquzzaman Mondal,Waheeb Tashan,Ayat Al-Olaimat,Hüseyin Arslan*

Main category: eess.SP

TL;DR: 本文提出了一种利用可重构智能表面（RIS）在Fresnel区域（近场和远场）优化切换管理的方案，通过减少信令开销和优化移动性管理，显著降低切换率和信令负载。


<details>
  <summary>Details</summary>
Motivation: 6G通信网络中，可重构智能表面（RIS）技术通过智能操控电磁波，提升了覆盖范围和连接性。然而，现有的切换管理方案存在信令开销大和移动性管理效率低的问题。因此，本文旨在利用RIS优化切换管理。

Method: 分析了RIS辅助网络中的信号强度变化，推导了近场区域RIS-UE距离的概率密度函数（PDF），提出了一种新的切换算法，包括硬切换（HHO）、软切换（SHO）、RIS辅助小区呼吸（RIS-CB）和RIS辅助乒乓避免（RIS-PP）策略，并以误码率（BER）为关键参数预测和减少不必要的切换。

Result: 数值结果显示，该方法显著降低了切换率和信令负载，提升了频谱效率（SE）和能量效率（EE），同时保证了无缝连接和服务质量（QoS）。

Conclusion: 本文提出的RIS辅助切换管理方案在6G系统中表现出高效性，特别是在密集蜂窝网络中，能够优化移动性管理并提升整体性能。

Abstract: Reconfigurable Intelligent Surfaces (RIS) is becoming a transformative
technology for the upcoming 6G communication networks, providing a way for
smartly maneuvering the electromagnetic waves to enhance coverage and
connectivity. This paper presents an efficient handover (HO) management scheme
leveraging RIS in the Fresnel region i.e., in both the near-field (NF) and
far-field (FF) regions to reduce signaling overhead and optimize mobility
management. For this, we analyzed the signal strength variations in the
considered RIS-aided networks, considering the radiative NF and FF regions, and
derive the probability density function (PDF) of the RIS-UE distance in the NF
region to quantify RIS reflection gains along the user equipment (UE)
trajectory. We propose a new HO algorithm incorporating several HO categories
like hard handover (HHO), soft handover (SHO), RIS-aided cell breathing
(RIS-CB), and RIS-aided ping-pong avoidance (RIS-PP) strategies. The proposed
algorithm uses bit error rate (BER) as a key parameter to predict the
minimization of unnecessary HOs by using RIS-aided pathways to retain
connectivity with the serving base station (BS), which minimizes the
requirement for frequent target BS searching and ultimately optimizes the HO.
By restricting measurement reports and HO requests, the suggested method
improves spectrum efficiency (SE) and energy efficiency (EE), especially in
crowded cellular networks. Numerical results highlight significant reductions
in HO rates and signaling load, ensuring seamless connectivity and improved
quality of service (QoS) in 6G systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [75] [Programmable Data Planes for Network Security](https://arxiv.org/abs/2507.22165)
*Gursimran Singh,H. B. Acharya,Minseok Kwon*

Main category: cs.CR

TL;DR: 可编程数据平面的出现，尤其是支持P4语言的交换机，通过实现自定义、线速数据包处理改变了网络安全的格局。这些交换机不仅能灵活转发数据包，还能检测和缓解DDoS和欺骗攻击，支持新一代防火墙策略与机器学习等功能。论文系统化了这些技术，探讨了其能力、挑战及架构上的变通方法。


<details>
  <summary>Details</summary>
Motivation: 研究可编程交换机在网络安全中的新兴应用，揭示其在受限硬件平台上如何实现复杂安全功能。

Method: 通过系统化分析，总结了如recirculate-and-truncate和查找表预计算等技术，突破硬件限制。

Result: 展示了可编程交换机在安全领域的多样化应用及其独特设计技巧。

Conclusion: 尽管存在硬件约束，可编程交换机仍能高效支持复杂安全功能，但仍有未解决问题和新兴研究方向。

Abstract: The emergence of programmable data planes, and particularly switches
supporting the P4 language, has transformed network security by enabling
customized, line-rate packet processing. These switches, originally intended
for flexible forwarding, now play a broader role: detecting and mitigating
attacks such as DDoS and spoofing, enforcing next-generation firewall policies,
and even supporting in-network cryptography and machine learning. These
capabilities are made possible by techniques such as recirculate-and-truncate
and lookup-table precomputation, which work around architectural constraints
like limited memory and restricted instruction sets. In this paper, we
systematize recent advances in security applications built on programmable
switches, with an emphasis on the capabilities, challenges, and architectural
workarounds. We highlight the non-obvious design techniques that make complex
in-network security functions feasible despite the constraints of the hardware
platform, and also comment on remaining issues and emerging research
directions.

</details>


### [76] [SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2507.22371)
*Lei Yu,Shiqi Cheng,Zhirong Huang,Jingyuan Zhang,Chenjie Shen,Junyi Lu,Li Yang,Fengjun Zhang,Jiajia Ma*

Main category: cs.CR

TL;DR: SAEL是一个基于LLM的智能合约漏洞检测框架,通过结合LLM的解释特征、代码特征和预测,显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法在复杂场景或泛化能力上存在不足,而通用大语言模型(LLM)虽能适应新漏洞模式,但对特定漏洞检测表现不佳。

Method: SAEL设计了针对性提示引导LLM生成解释特征,结合CodeT5和T5的提示调整,并引入自适应专家混合架构动态调整特征权重。

Result: 实验表明,SAEL在多种漏洞检测上优于现有方法。

Conclusion: SAEL通过融合LLM的多方面特征,显著提升了智能合约漏洞检测的准确性和泛化能力。

Abstract: With the increasing security issues in blockchain, smart contract
vulnerability detection has become a research focus. Existing vulnerability
detection methods have their limitations: 1) Static analysis methods struggle
with complex scenarios. 2) Methods based on specialized pre-trained models
perform well on specific datasets but have limited generalization capabilities.
In contrast, general-purpose Large Language Models (LLMs) demonstrate
impressive ability in adapting to new vulnerability patterns. However, they
often underperform on specific vulnerability types compared to methods based on
specialized pre-trained models. We also observe that explanations generated by
general-purpose LLMs can provide fine-grained code understanding information,
contributing to improved detection performance.
  Inspired by these observations, we propose SAEL, an LLM-based framework for
smart contract vulnerability detection. We first design targeted prompts to
guide LLMs in identifying vulnerabilities and generating explanations, which
serve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to
process contract code and explanations, enhancing task-specific performance. To
combine the strengths of each approach, we introduce an Adaptive
Mixture-of-Experts architecture. This dynamically adjusts feature weights via a
Gating Network, which selects relevant features using TopK filtering and
Softmax normalization, and incorporates a Multi-Head Self-Attention mechanism
to enhance cross-feature relationships. This design enables effective
integration of LLM predictions, explanation features, and code features through
gradient optimization. The loss function jointly considers both independent
feature performance and overall weighted predictions. Experiments show that
SAEL outperforms existing methods across various vulnerabilities.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [77] [Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?](https://arxiv.org/abs/2507.22099)
*Shuqing Li,Qiang Chen,Xiaoxue Ren,Michael R. Lyu*

Main category: cs.CV

TL;DR: 这篇论文通过大规模的实证研究，分析了物理引擎（PE）中物理失效的表现形式、检测方法的有效性及开发者的认知，提出了失效分类法并评估了多种检测技术。


<details>
  <summary>Details</summary>
Motivation: 物理引擎（PE）在多种应用中至关重要，但其物理失效可能影响软件可靠性或用户体验，甚至导致关键系统故障。现有的测试方法多为白盒且仅关注崩溃检测，无法有效应对复杂的物理失效问题。

Method: 通过实证研究，调查物理失效的表现形式（分类法），评估包括深度学习、提示技术和多模态模型在内的多种检测方法的有效性，并分析开发者的实践认知。

Result: 提出了物理失效的分类法，展示了不同检测技术的效果，并从开发者角度提供了改进检测方法的实践建议。

Conclusion: 研究为物理失效检测提供了新的理论和实践基础，并开源了相关资源以支持未来研究。

Abstract: Physics Engines (PEs) are fundamental software frameworks that simulate
physical interactions in applications ranging from entertainment to
safety-critical systems. Despite their importance, PEs suffer from physics
failures, deviations from expected physical behaviors that can compromise
software reliability, degrade user experience, and potentially cause critical
failures in autonomous vehicles or medical robotics. Current testing approaches
for PE-based software are inadequate, typically requiring white-box access and
focusing on crash detection rather than semantically complex physics failures.
This paper presents the first large-scale empirical study characterizing
physics failures in PE-based software. We investigate three research questions
addressing the manifestations of physics failures, the effectiveness of
detection techniques, and developer perceptions of current detection practices.
Our contributions include: (1) a taxonomy of physics failure manifestations;
(2) a comprehensive evaluation of detection methods including deep learning,
prompt-based techniques, and large multimodal models; and (3) actionable
insights from developer experiences for improving detection approaches. To
support future research, we release PhysiXFails, code, and other materials at
https://sites.google.com/view/physics-failure-detection.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [78] [Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation Model-driven Framework](https://arxiv.org/abs/2507.22481)
*Tianyi Liu,Kejun Wu,Chen Cai,Yi Wang,Kim-Hui Yap,Lap-Pui Chau*

Main category: eess.IV

TL;DR: 提出了一种基于视觉基础模型的盲比特流损坏视频恢复框架，无需人工标注损坏区域，显著提升了恢复质量。


<details>
  <summary>Details</summary>
Motivation: 视频信号在传输和存储中易受损坏，现有方法需大量人工标注且恢复效果有限，亟需高效自动解决方案。

Method: 结合视觉基础模型与恢复模型，引入DAC模型定位损坏，CFC模块自适应处理残差信息，采用MoRE结构增强特征。

Result: 全面评估表明该方法在无需人工标注的情况下实现了出色的视频恢复效果。

Conclusion: 该方法显著提升了用户体验和多媒体系统的可靠性，具有广泛应用潜力。

Abstract: Video signals are vulnerable in multimedia communication and storage systems,
as even slight bitstream-domain corruption can lead to significant pixel-domain
degradation. To recover faithful spatio-temporal content from corrupted inputs,
bitstream-corrupted video recovery has recently emerged as a challenging and
understudied task. However, existing methods require time-consuming and
labor-intensive annotation of corrupted regions for each corrupted video frame,
resulting in a large workload in practice. In addition, high-quality recovery
remains difficult as part of the local residual information in corrupted frames
may mislead feature completion and successive content recovery. In this paper,
we propose the first blind bitstream-corrupted video recovery framework that
integrates visual foundation models with a recovery model, which is adapted to
different types of corruption and bitstream-level prompts. Within the
framework, the proposed Detect Any Corruption (DAC) model leverages the rich
priors of the visual foundation model while incorporating bitstream and
corruption knowledge to enhance corruption localization and blind recovery.
Additionally, we introduce a novel Corruption-aware Feature Completion (CFC)
module, which adaptively processes residual contributions based on high-level
corruption understanding. With VFM-guided hierarchical feature augmentation and
high-level coordination in a mixture-of-residual-experts (MoRE) structure, our
method suppresses artifacts and enhances informative residuals. Comprehensive
evaluations show that the proposed method achieves outstanding performance in
bitstream-corrupted video recovery without requiring a manually labeled mask
sequence. The demonstrated effectiveness will help to realize improved user
experience, wider application scenarios, and more reliable multimedia
communication and storage systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [79] [A comprehensive benchmark of an Ising machine on the Max-Cut problem](https://arxiv.org/abs/2507.22117)
*Salwa Shaglel,Markus Kirsch,Marten Winkler,Christian Münch,Stefan Walter,Fritz Schinkel,Martin Kliesch*

Main category: quant-ph

TL;DR: 论文提出了一种利用Fujitsu数字退火器（DA）解决Max-Cut问题的基准测试方法，并通过与其他启发式算法的比较，显示了DA的竞争力。


<details>
  <summary>Details</summary>
Motivation: 测试大规模QUBO问题的启发式求解能力，展示当前技术在解决此类问题上的潜力。

Method: 使用DA对Max-Cut问题进行基准测试，并与D-Wave混合量子-经典退火器和QIS3启发式算法进行比较。

Result: DA在53,000变量的图上表现出竞争力，且结果与其他求解器一致。

Conclusion: 该研究展示了当前启发式算法在大规模QUBO问题中的应用潜力，并为未来研究提供了基准参考。

Abstract: QUBO formulations of combinatorial optimization problems allow for solving
them using various quantum heuristics. While large-scale quantum computations
are currently still out of reach, we can already numerically test such QUBO
formulations on a perhaps surprisingly large scale. In this work, we benchmark
Fujitsu's Digital Annealer (DA) on the Max-Cut problem, which captures the main
complexity of the QUBO problem. We make a comprehensive benchmark against
leading other heuristic algorithms on graphs with up to 53,000 variables by
focusing on the wall-clock time. Moreover, we compare the DA performance
against published performance results of the D-Wave hybrid quantum-classical
annealer and the recently proposed QIS3 heuristic. Based on performance
statistics for over 2,000 graphs from the MQLib, we find that the DA yields
competitive results. We hope that this benchmark demonstrates the extent to
which large QUBO instances can be heuristically solved today, yielding
consistent results across different solvers.

</details>


### [80] [Hamiltonian Expressibility for Ansatz Selection in Variational Quantum Algorithms](https://arxiv.org/abs/2507.22550)
*Filippo Brozzi,Gloria Turati,Maurizio Ferrari Dacrema,Filippo Caruso,Paolo Cremonesi*

Main category: quant-ph

TL;DR: 本文研究了变分量子算法中哈米顿可表达性对解质量的影响，发现高可表达性在理想条件下对非对角哈米顿问题更有效，而低可表达性对角哈密顿问题更优。噪声条件下，低可表达性仍适用于基态问题。


<details>
  <summary>Details</summary>
Motivation: 探索哈密顿可表达性对变分量子算法解质量的影响，尤其是在不同问题和噪声条件下的表现。

Method: 使用蒙特卡洛方法估计哈密顿可表达性，分析电路深度与可表达性的关系，并通过VQE训练各电路。

Result: 高可表达性电路在非对角哈密顿问题中表现更好，低可表达性对角哈密顿问题更有效；噪声条件下，低可表达性仍适用于基态问题。

Conclusion: 哈密顿可表达性是影响解质量的关键因素，需根据问题类型和噪声条件选择合适的电路可表达性。

Abstract: In the context of Variational Quantum Algorithms (VQAs), selecting an
appropriate ansatz is crucial for efficient problem-solving. Hamiltonian
expressibility has been introduced as a metric to quantify a circuit's ability
to uniformly explore the energy landscape associated with a Hamiltonian ground
state search problem. However, its influence on solution quality remains
largely unexplored. In this work, we estimate the Hamiltonian expressibility of
a well-defined set of circuits applied to various Hamiltonians using a Monte
Carlo-based approach. We analyze how ansatz depth influences expressibility and
identify the most and least expressive circuits across different problem types.
We then train each ansatz using the Variational Quantum Eigensolver (VQE) and
analyze the correlation between solution quality and expressibility.Our results
indicate that, under ideal or low-noise conditions and particularly for
small-scale problems, ans\"atze with high Hamiltonian expressibility yield
better performance for problems with non-diagonal Hamiltonians and
superposition-state solutions. Conversely, circuits with low expressibility are
more effective for problems whose solutions are basis states, including those
defined by diagonal Hamiltonians. Under noisy conditions, low-expressibility
circuits remain preferable for basis-state problems, while intermediate
expressibility yields better results for some problems involving
superposition-state solutions.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [81] [Scaling and Distilling Transformer Models for sEMG](https://arxiv.org/abs/2507.22094)
*Nicholas Mehlman,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Kelvin Niu,Alexander H. Miller,Shagun Sodhani*

Main category: eess.AS

TL;DR: 研究表明，vanilla transformer模型在sEMG数据上可以扩展至110M参数，超越现有研究规模（通常<10M），并通过蒸馏技术压缩至50倍小模型，性能损失极小（<1.5%）。


<details>
  <summary>Details</summary>
Motivation: sEMG信号在开发人机交互界面中潜力巨大，但训练数据量有限且计算资源受限，阻碍了大模型的研究。

Method: 扩展vanilla transformer模型规模至110M参数，并利用蒸馏技术压缩模型。

Result: 大模型在跨用户性能上表现优异，蒸馏后的小模型保持高性能（性能损失<1.5%）。

Conclusion: 该方法提供了适用于复杂实时sEMG任务的高效且表现力强的模型。

Abstract: Surface electromyography (sEMG) signals offer a promising avenue for
developing innovative human-computer interfaces by providing insights into
muscular activity. However, the limited volume of training data and
computational constraints during deployment have restricted the investigation
of scaling up the model size for solving sEMG tasks. In this paper, we
demonstrate that vanilla transformer models can be effectively scaled up on
sEMG data and yield improved cross-user performance up to 110M parameters,
surpassing the model size regime investigated in other sEMG research (usually
<10M parameters). We show that >100M-parameter models can be effectively
distilled into models 50x smaller with minimal loss of performance (<1.5%
absolute). This results in efficient and expressive models suitable for complex
real-time sEMG tasks in real-world environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [82] [Automatically discovering heuristics in a complex SAT solver with large language models](https://arxiv.org/abs/2507.22876)
*Yiwen Sun,Furong Ye,Zhihan Chen,Ke Wei,Shaowei Cai*

Main category: cs.AI

TL;DR: 论文提出了AutoModSAT，一种通过大型语言模型（LLM）优化SAT求解器的新方法，解决了LLM兼容性、自动提示优化和高效搜索策略三大挑战。


<details>
  <summary>Details</summary>
Motivation: 由于现代SAT求解器架构复杂，现有自动配置框架性能提升有限，因此需要开发新的优化方法。

Method: 提出了系统化指南开发LLM友好的求解器、无监督自动提示优化方法，以及结合预搜索和进化算法的高效搜索策略。

Result: AutoModSAT在性能上比基准求解器提升了50%，比SOTA求解器提高了30%，速度平均快20%。

Conclusion: 该研究填补了AI驱动启发式发现与关键系统优化之间的鸿沟，为下一代复杂求解器开发提供了方法论和实证结果。

Abstract: Satisfiability problem (SAT) is a cornerstone of computational complexity
with broad industrial applications, and it remains challenging to optimize
modern SAT solvers in real-world settings due to their intricate architectures.
While automatic configuration frameworks have been developed, they rely on
manually constrained search spaces and yield limited performance gains. This
work introduces a novel paradigm which effectively optimizes complex SAT
solvers via Large Language Models (LLMs), and a tool called AutoModSAT is
developed. Three fundamental challenges are addressed in order to achieve
superior performance: (1) LLM-friendly solver: Systematic guidelines are
proposed for developing a modularized solver to meet LLMs' compatibility,
emphasizing code simplification, information share and bug reduction; (2)
Automatic prompt optimization: An unsupervised automatic prompt optimization
method is introduced to advance the diversity of LLMs' output; (3) Efficient
search strategy: We design a presearch strategy and an EA evolutionary
algorithm for the final efficient and effective discovery of heuristics.
Extensive experiments across a wide range of datasets demonstrate that
AutoModSAT achieves 50% performance improvement over the baseline solver and
achieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover,
AutoModSAT attains a 20% speedup on average compared to parameter-tuned
alternatives of the SOTA solvers, showcasing the enhanced capability in
handling complex problem instances. This work bridges the gap between AI-driven
heuristics discovery and mission-critical system optimization, and provides
both methodological advancements and empirically validated results for
next-generation complex solver development.

</details>


### [83] [Magentic-UI: Towards Human-in-the-loop Agentic Systems](https://arxiv.org/abs/2507.22358)
*Hussein Mozannar,Gagan Bansal,Cheng Tan,Adam Fourney,Victor Dibia,Jingya Chen,Jack Gerrits,Tyler Payne,Matheus Kunzler Maldaner,Madeleine Grunde-McLaughlin,Eric Zhu,Griffin Bassman,Jacob Alber,Peter Chang,Ricky Loynd,Friederike Niedtner,Ece Kamar,Maya Murad,Rafah Hosn,Saleema Amershi*

Main category: cs.AI

TL;DR: 该论文探讨了大型语言模型驱动的AI代理在自主完成复杂任务时的局限性及潜在风险，并提出了一种结合人类监督的人机协同系统Magentic-UI，以提升安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在复杂任务中表现仍不及人类，且存在安全和风险问题，因此需探索人机协同的解决方案。

Method: 开发了开源Web界面Magentic-UI，支持多工具扩展，并设计了六种交互机制（如协同规划和任务监控）来优化人机协作。

Result: 通过基准测试、模拟用户实验和定性研究，验证了Magentic-UI在任务完成、交互能力和安全性方面的潜力。

Conclusion: Magentic-UI为人机协作提供了一种安全高效的框架，推动了该领域的进一步发展。

Abstract: AI agents powered by large language models are increasingly capable of
autonomously completing complex, multi-step tasks using external tools. Yet,
they still fall short of human-level performance in most domains including
computer use, software development, and research. Their growing autonomy and
ability to interact with the outside world, also introduces safety and security
risks including potentially misaligned actions and adversarial manipulation. We
argue that human-in-the-loop agentic systems offer a promising path forward,
combining human oversight and control with AI efficiency to unlock productivity
from imperfect systems. We introduce Magentic-UI, an open-source web interface
for developing and studying human-agent interaction. Built on a flexible
multi-agent architecture, Magentic-UI supports web browsing, code execution,
and file manipulation, and can be extended with diverse tools via Model Context
Protocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for
enabling effective, low-cost human involvement: co-planning, co-tasking,
multi-tasking, action guards, and long-term memory. We evaluate Magentic-UI
across four dimensions: autonomous task completion on agentic benchmarks,
simulated user testing of its interaction capabilities, qualitative studies
with real users, and targeted safety assessments. Our findings highlight
Magentic-UI's potential to advance safe and efficient human-agent
collaboration.

</details>


### [84] [Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making](https://arxiv.org/abs/2507.22365)
*ZhaoBin Li,Mark Steyvers*

Main category: cs.AI

TL;DR: 研究探讨AI辅助决策中，预测准确性和元认知敏感性对决策质量的影响，发现后者尤为重要。


<details>
  <summary>Details</summary>
Motivation: 探讨AI预测准确性和其置信度估计的可靠性如何共同影响人类决策质量。

Method: 提出理论框架分析AI预测准确性与元认知敏感性的联合影响，并通过行为实验验证。

Result: 发现AI元认知敏感性高的系统能提升人类决策准确性，即使其预测准确性较低。

Conclusion: AI辅助决策应同时优化准确性和元认知敏感性，以实现更好的决策结果。

Abstract: In settings where human decision-making relies on AI input, both the
predictive accuracy of the AI system and the reliability of its confidence
estimates influence decision quality. We highlight the role of AI metacognitive
sensitivity -- its ability to assign confidence scores that accurately
distinguish correct from incorrect predictions -- and introduce a theoretical
framework for assessing the joint impact of AI's predictive accuracy and
metacognitive sensitivity in hybrid decision-making settings. Our analysis
identifies conditions under which an AI with lower predictive accuracy but
higher metacognitive sensitivity can enhance the overall accuracy of human
decision making. Finally, a behavioral experiment confirms that greater AI
metacognitive sensitivity improves human decision performance. Together, these
findings underscore the importance of evaluating AI assistance not only by
accuracy but also by metacognitive sensitivity, and of optimizing both to
achieve superior decision outcomes.

</details>
