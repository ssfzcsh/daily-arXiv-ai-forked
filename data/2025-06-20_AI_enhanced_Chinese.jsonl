{"id": "2506.14866", "pdf": "https://arxiv.org/pdf/2506.14866", "abs": "https://arxiv.org/abs/2506.14866", "authors": ["Thomas Kuntz", "Agatha Duzan", "Hao Zhao", "Francesco Croce", "Zico Kolter", "Nicolas Flammarion", "Maksym Andriushchenko"], "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86OS-Harm\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u8ba1\u7b97\u673a\u4ee3\u7406\u5b89\u5168\u6027\u7684\u65b0\u57fa\u51c6\uff0c\u8986\u76d6\u4e86\u6ee5\u7528\u3001\u653b\u51fb\u548c\u8bef\u884c\u4e3a\u4e09\u7c7b\u5371\u5bb3\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eLLM\u7684\u8ba1\u7b97\u673a\u4ee3\u7406\uff08\u5982\u5904\u7406\u622a\u56fe\u6216\u53ef\u8bbf\u95ee\u6027\u6811\u7684\u4ee3\u7406\uff09\u65e5\u76ca\u6d41\u884c\uff0c\u4f46\u5176\u5b89\u5168\u6027\u95ee\u9898\u88ab\u5ffd\u89c6\uff0c\u4e9f\u9700\u8bc4\u4f30\u5176\u6f5c\u5728\u5371\u5bb3\u4ee5\u4fc3\u8fdb\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u5728OSWorld\u73af\u5883\u4e0a\u6784\u5efaOS-Harm\u57fa\u51c6\uff0c\u8bbe\u8ba1\u4e86150\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u591a\u79cd\u5b89\u5168\u8fdd\u89c4\u573a\u666f\uff08\u5982\u9a9a\u6270\u3001\u4fb5\u6743\u3001\u865a\u5047\u4fe1\u606f\u7b49\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\uff0c\u524d\u6cbf\u6a21\u578b\uff08\u5982o4-mini\u3001Claude 3.7 Sonnet\u7b49\uff09\u666e\u904d\u6613\u53d7\u6ee5\u7528\u548c\u653b\u51fb\uff0c\u4e14\u53ef\u80fd\u6267\u884c\u4e0d\u5b89\u5168\u64cd\u4f5c\u3002\u8bc4\u4f30\u5de5\u5177\u4e0e\u4eba\u5de5\u6807\u6ce8\u4e00\u81f4\u6027\u9ad8\uff08F1\u5206\u65700.76-0.79\uff09\u3002", "conclusion": "OS-Harm\u4e3a\u8ba1\u7b97\u673a\u4ee3\u7406\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5b89\u5168\u9690\u60a3\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u7814\u7a76\u548c\u6539\u8fdb\u3002"}}
{"id": "2506.15084", "pdf": "https://arxiv.org/pdf/2506.15084", "abs": "https://arxiv.org/abs/2506.15084", "authors": ["Weiqi Lu", "Yongqiang Tian", "Xiaohan Zhong", "Haoyang Ma", "Zhenyang Xu", "Shing-Chi Cheung", "Chengnian Sun"], "title": "An Empirical Study of Bugs in Data Visualization Libraries", "categories": ["cs.SE", "cs.CV", "cs.HC"], "comment": "Proc. ACM Softw. Eng. 2, FSE", "summary": "Data visualization (DataViz) libraries play a crucial role in presentation, data analysis, and application development, underscoring the importance of their accuracy in transforming data into visual representations. Incorrect visualizations can adversely impact user experience, distort information conveyance, and influence user perception and decision-making processes. Visual bugs in these libraries can be particularly insidious as they may not cause obvious errors like crashes, but instead mislead users of the underlying data graphically, resulting in wrong decision making. Consequently, a good understanding of the unique characteristics of bugs in DataViz libraries is essential for researchers and developers to detect and fix bugs in DataViz libraries.\n  This study presents the first comprehensive analysis of bugs in DataViz libraries, examining 564 bugs collected from five widely-used libraries. Our study systematically analyzes their symptoms and root causes, and provides a detailed taxonomy. We found that incorrect/inaccurate plots are pervasive in DataViz libraries and incorrect graphic computation is the major root cause, which necessitates further automated testing methods for DataViz libraries. Moreover, we identified eight key steps to trigger such bugs and two test oracles specific to DataViz libraries, which may inspire future research in designing effective automated testing techniques. Furthermore, with the recent advancements in Vision Language Models (VLMs), we explored the feasibility of applying these models to detect incorrect/inaccurate plots. The results show that the effectiveness of VLMs in bug detection varies from 29% to 57%, depending on the prompts, and adding more information in prompts does not necessarily increase the effectiveness. More findings can be found in our manuscript.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u9996\u6b21\u5168\u9762\u5206\u6790\u4e86\u6570\u636e\u53ef\u89c6\u5316\u5e93\u4e2d\u7684\u9519\u8bef\uff0c\u7814\u7a76\u4e86564\u4e2a\u6765\u81ea\u4e94\u4e2a\u5e38\u7528\u5e93\u7684bug\uff0c\u603b\u7ed3\u4e86\u75c7\u72b6\u548c\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u7c7b\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u9519\u8bef/\u4e0d\u51c6\u786e\u7684\u7ed8\u56fe\u666e\u904d\u5b58\u5728\uff0c\u4e14\u4e3b\u8981\u539f\u56e0\u662f\u56fe\u5f62\u8ba1\u7b97\u9519\u8bef\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u89e6\u53d1\u8fd9\u4e9b\u9519\u8bef\u7684\u516b\u4e2a\u5173\u952e\u6b65\u9aa4\u548c\u4e24\u4e2a\u6d4b\u8bd5\u9884\u8a00\uff0c\u5e76\u63a2\u7d22\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u68c0\u6d4b\u9519\u8bef\u4e2d\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u6570\u636e\u53ef\u89c6\u5316\u5e93\u7684\u51c6\u786e\u6027\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u4fe1\u606f\u4f20\u8fbe\u81f3\u5173\u91cd\u8981\uff0c\u9519\u8bef\u7684\u53ef\u89c6\u5316\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u8bef\u89e3\u548c\u51b3\u7b56\u5931\u8bef\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5e93\u4e2d\u7684\u89c6\u89c9\u9519\u8bef\u5f80\u5f80\u4e0d\u6613\u5bdf\u89c9\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u5176\u7279\u6027\u4ee5\u6539\u8fdb\u68c0\u6d4b\u548c\u4fee\u590d\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6536\u96c6\u548c\u5206\u6790564\u4e2a\u6765\u81ea\u4e94\u4e2a\u5e38\u7528\u6570\u636e\u53ef\u89c6\u5316\u5e93\u7684bug\uff0c\u7cfb\u7edf\u5730\u5206\u7c7b\u4e86\u5b83\u4eec\u7684\u75c7\u72b6\u548c\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63a2\u7d22\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u4e0d\u51c6\u786e\u7ed8\u56fe\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9519\u8bef/\u4e0d\u51c6\u786e\u7684\u7ed8\u56fe\u666e\u904d\u5b58\u5728\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u56fe\u5f62\u8ba1\u7b97\u9519\u8bef\u3002\u63d0\u51fa\u516b\u4e2a\u89e6\u53d1\u6b65\u9aa4\u548c\u4e24\u4e2a\u6d4b\u8bd5\u9884\u8a00\uff0c\u5e76\u53d1\u73b0VLMs\u7684\u68c0\u6d4b\u6548\u679c\u572829%\u81f357%\u4e4b\u95f4\uff0c\u53d6\u51b3\u4e8e\u63d0\u793a\u8bbe\u8ba1\u3002", "conclusion": "\u6570\u636e\u53ef\u89c6\u5316\u5e93\u4e2d\u7684\u9519\u8bef\u666e\u904d\u4e14\u5177\u6709\u72ec\u7279\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u5f00\u53d1\u81ea\u52a8\u5316\u6d4b\u8bd5\u65b9\u6cd5\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u9519\u8bef\u65b9\u9762\u663e\u793a\u51fa\u4e00\u5b9a\u7684\u6f5c\u529b\uff0c\u4f46\u6548\u679c\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2506.15088", "pdf": "https://arxiv.org/pdf/2506.15088", "abs": "https://arxiv.org/abs/2506.15088", "authors": ["Miao Miao"], "title": "Program Feature-based Fuzzing Benchmarking", "categories": ["cs.SE"], "comment": null, "summary": "Fuzzing is a powerful software testing technique renowned for its effectiveness in identifying software vulnerabilities. Traditional fuzzing evaluations typically focus on overall fuzzer performance across a set of target programs, yet few benchmarks consider how fine-grained program features influence fuzzing effectiveness. To bridge this gap, we introduce a novel benchmark designed to generate programs with configurable, fine-grained program features to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing studies, extracting 7 program features related to control-flow and data-flow that can impact fuzzer performance. Using these features, we generated a benchmark consisting of 153 programs controlled by 10 fine-grained configurable parameters. We evaluated 11 popular fuzzers using this benchmark. The results indicate that fuzzer performance varies significantly based on the program features and their strengths, highlighting the importance of incorporating program characteristics into fuzzing evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u53ef\u914d\u7f6e\u7684\u7ec6\u7c92\u5ea6\u7a0b\u5e8f\u7279\u5f81\u6765\u8bc4\u4f30\u6a21\u7cca\u6d4b\u8bd5\u7684\u6709\u6548\u6027\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u6a21\u7cca\u6d4b\u8bd5\u8bc4\u4f30\u901a\u5e38\u5173\u6ce8\u6574\u4f53\u6027\u80fd\uff0c\u800c\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u7a0b\u5e8f\u7279\u5f81\u5bf9\u6548\u679c\u7684\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u679025\u9879\u6700\u8fd1\u7684\u7814\u7a76\uff0c\u63d0\u53d6\u4e867\u4e2a\u5f71\u54cd\u6a21\u7cca\u6d4b\u8bd5\u6027\u80fd\u7684\u7a0b\u5e8f\u7279\u5f81\uff0c\u751f\u6210\u4e86\u5305\u542b153\u4e2a\u7a0b\u5e8f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u4e8611\u79cd\u6d41\u884c\u7684\u6a21\u7cca\u5668\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u7cca\u5668\u6027\u80fd\u53d7\u7a0b\u5e8f\u7279\u5f81\u53ca\u5176\u5f3a\u5ea6\u663e\u8457\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u5c06\u7a0b\u5e8f\u7279\u5f81\u7eb3\u5165\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u6a21\u7cca\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u7ec6\u81f4\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6d4b\u8bd5\u7684\u9488\u5bf9\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.15098", "pdf": "https://arxiv.org/pdf/2506.15098", "abs": "https://arxiv.org/abs/2506.15098", "authors": ["Haosheng Zuo", "Feifei Niu", "Chuanyi Li"], "title": "Enhancement Report Approval Prediction: A Comparative Study of Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Enhancement reports (ERs) serve as a critical communication channel between users and developers, capturing valuable suggestions for software improvement. However, manually processing these reports is resource-intensive, leading to delays and potential loss of valuable insights. To address this challenge, enhancement report approval prediction (ERAP) has emerged as a research focus, leveraging machine learning techniques to automate decision-making. While traditional approaches have employed feature-based classifiers and deep learning models, recent advancements in large language models (LLM) present new opportunities for enhancing prediction accuracy. This study systematically evaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and XLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1 8B Instruct and DeepSeek-V3 for decoder models) against traditional methods (CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1) Incorporating creator profiles increases unfine-tuned decoder-only models' overall accuracy by 10.8 percent though it may introduce bias; (2) LoRA fine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79 percent accuracy and significantly enhancing recall for approved reports (76.1 percent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5 percent under strict chronological evaluation and effectively addressing class imbalance issues. These findings establish LLM as a superior solution for ERAP, demonstrating their potential to streamline software maintenance workflows and improve decision-making in real-world development environments. We also investigated and summarized the ER cases where the large models underperformed, providing valuable directions for future research.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8618\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u589e\u5f3a\u62a5\u544a\u5ba1\u6279\u9884\u6d4b\uff08ERAP\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0LoRA\u5fae\u8c03\u7684Llama 3.1 8B Instruct\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe79%\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd55%\u3002", "motivation": "\u624b\u52a8\u5904\u7406\u589e\u5f3a\u62a5\u544a\uff08ERs\uff09\u6548\u7387\u4f4e\u4e14\u6613\u4e22\u5931\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u56e0\u6b64\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u673a\u5668\u5b66\u4e60\uff08\u5c24\u5176\u662fLLM\uff09\u63d0\u5347\u81ea\u52a8\u5316\u51b3\u7b56\u7684\u51c6\u786e\u6027\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e8618\u79cdLLM\uff08\u5982BERT\u3001GPT-3.5-turbo\u7b49\uff09\u4e0e\u4f20\u7edf\u65b9\u6cd5\uff08CNN/LSTM-BERT/GloVe\uff09\u7684\u5bf9\u6bd4\uff0c\u5e76\u5f15\u5165\u521b\u5efa\u8005\u7279\u5f81\u548cLoRA\u5fae\u8c03\u3002", "result": "\u672a\u5fae\u8c03\u7684\u4ec5\u89e3\u7801\u5668\u6a21\u578b\u52a0\u5165\u521b\u5efa\u8005\u7279\u5f81\u540e\u51c6\u786e\u7387\u63d0\u534710.8%\uff1bLoRA\u5fae\u8c03\u7684Llama 3.1 8B Instruct\u51c6\u786e\u7387\u8fbe79%\uff0c\u53ec\u56de\u7387\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "LLM\u5728ERAP\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u4f18\u5316\u8f6f\u4ef6\u7ef4\u62a4\u6d41\u7a0b\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2506.15135", "pdf": "https://arxiv.org/pdf/2506.15135", "abs": "https://arxiv.org/abs/2506.15135", "authors": ["Zhengqun Koo"], "title": "Towards Bug-Free Distributed Go Programs", "categories": ["cs.SE", "cs.LO", "cs.PL"], "comment": "Version 1. B.Comp. Dissertation", "summary": "Programmers of distributed systems need to reason about concurrency to avoid races. However, reasoning about concurrency is difficult, and unexpected races show up as bugs. Data race detection in shared memory systems is well-studied (dynamic data race detection [13], behavioral types [15], dynamic race detection [31]). Similar to how a data race consists of reads and writes not related by happens-before at a shared memory location, a communication race consists of receives and sends not related by happens-before on a shared channel. Communication races are problematic: a receiver expects a specific message from a specific sender, but with a communication race, the receiver can receive a message meant for another receiver, or not receive anything at all. In this work, we describe a verification framework that can prove the absence of communication races for distributed programs that use a subset of the Go programming language, where synchronization is mainly achieved via message passing. We statically reason about how a distributed program executes, using a happens-before order, extended to buffered and unbuffered channels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9759\u6001\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u8bc1\u660e\u4f7f\u7528Go\u8bed\u8a00\u5b50\u96c6\u7684\u5206\u5e03\u5f0f\u7a0b\u5e8f\u4e2d\u4e0d\u5b58\u5728\u901a\u4fe1\u7ade\u4e89\u3002", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5e76\u53d1\u95ee\u9898\uff08\u5982\u901a\u4fe1\u7ade\u4e89\uff09\u53ef\u80fd\u5bfc\u81f4\u63a5\u6536\u8005\u63a5\u6536\u5230\u9519\u8bef\u7684\u6d88\u606f\u6216\u65e0\u6cd5\u63a5\u6536\u6d88\u606f\uff0c\u4ece\u800c\u5f15\u53d1\u7a0b\u5e8f\u9519\u8bef\u3002\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\u7684\u6570\u636e\u7ade\u4e89\u68c0\u6d4b\u4e0a\uff0c\u800c\u901a\u4fe1\u7ade\u4e89\u7684\u7c7b\u4f3c\u95ee\u9898\u5c1a\u672a\u5145\u5206\u89e3\u51b3\u3002", "method": "\u4f5c\u8005\u6269\u5c55\u4e86happens-before\u987a\u5e8f\uff0c\u4ee5\u9002\u5e94\u7f13\u51b2\u548c\u975e\u7f13\u51b2\u901a\u9053\uff0c\u5e76\u901a\u8fc7\u9759\u6001\u5206\u6790\u5206\u5e03\u5f0f\u7a0b\u5e8f\u7684\u6267\u884c\u6d41\u7a0b\u6765\u68c0\u6d4b\u901a\u4fe1\u7ade\u4e89\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u8bc1\u660e\u4f7f\u7528Go\u5b50\u96c6\u7684\u5206\u5e03\u5f0f\u7a0b\u5e8f\u4e2d\u4e0d\u5b58\u5728\u901a\u4fe1\u7ade\u4e89\u3002", "conclusion": "\u901a\u8fc7\u9759\u6001\u9a8c\u8bc1\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u907f\u514d\u5206\u5e03\u5f0f\u7a0b\u5e8f\u4e2d\u7684\u901a\u4fe1\u7ade\u4e89\u95ee\u9898\uff0c\u63d0\u5347\u7a0b\u5e8f\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.15523", "pdf": "https://arxiv.org/pdf/2506.15523", "abs": "https://arxiv.org/abs/2506.15523", "authors": ["Jiaqi Sun", "Dingyu Yang", "Shiyou Qian", "Jian Cao", "Guangtao Xue"], "title": "Atys: An Efficient Profiling Framework for Identifying Hotspot Functions in Large-scale Cloud Microservices", "categories": ["cs.PF"], "comment": null, "summary": "To handle the high volume of requests, large-scale services are comprised of thousands of instances deployed in clouds. These services utilize diverse programming languages and are distributed across various nodes as encapsulated containers. Given their vast scale, even minor performance enhancements can lead to significant cost reductions. In this paper, we introduce Atys1, an efficient profiling framework specifically designed to identify hotspot functions within large-scale distributed services. Atys presents four key features. First, it implements a language-agnostic adaptation mechanism for multilingual microservices. Second, a two-level aggregation method is introduced to provide a comprehensive overview of flamegraphs. Third, we propose a function selective pruning (FSP) strategy to enhance the efficiency of aggregating profiling results. Finally, we develop a frequency dynamic adjustment (FDA) scheme that dynamically modifies sampling frequency based on service status, effectively minimizing profiling cost while maintaining accuracy. Cluster-scale experiments on two benchmarks show that the FSP strategy achieves a 6.8% reduction in time with a mere 0.58% mean average percentage error (MAPE) in stack traces aggregation. Additionally, the FDA scheme ensures that the mean squared error (MSE) remains on par with that at high sampling rates, while achieving an 87.6% reduction in cost.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Atys1\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u670d\u52a1\u70ed\u70b9\u51fd\u6570\u5206\u6790\u6846\u67b6\uff0c\u5177\u6709\u591a\u8bed\u8a00\u9002\u914d\u3001\u4e24\u7ea7\u805a\u5408\u3001\u9009\u62e9\u6027\u526a\u679d\u548c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u9891\u7387\u7b49\u6838\u5fc3\u7279\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5206\u6790\u6210\u672c\u5e76\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u89c4\u6a21\u670d\u52a1\u901a\u5e38\u7531\u6570\u5343\u4e2a\u4e91\u5b9e\u4f8b\u7ec4\u6210\uff0c\u6027\u80fd\u5fae\u5c0f\u63d0\u5347\u53ef\u5e26\u6765\u663e\u8457\u6210\u672c\u8282\u7ea6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u6027\u80fd\u5206\u6790\u6846\u67b6\u6765\u8bc6\u522b\u670d\u52a1\u4e2d\u7684\u70ed\u70b9\u51fd\u6570\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "method": "Atys1\u6846\u67b6\u5305\u62ec\uff1a1)\u8bed\u8a00\u65e0\u5173\u7684\u591a\u8bed\u8a00\u5fae\u670d\u52a1\u9002\u914d\u673a\u5236\uff1b2)\u4e24\u7ea7\u805a\u5408\u65b9\u6cd5\u751f\u6210\u5168\u9762\u7684\u706b\u7130\u56fe\uff1b3)\u51fd\u6570\u9009\u62e9\u6027\u526a\u679d\u7b56\u7565(FSP)\uff1b4)\u57fa\u4e8e\u670d\u52a1\u72b6\u6001\u7684\u52a8\u6001\u9891\u7387\u8c03\u6574\u65b9\u6848(FDA)\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFSP\u7b56\u7565\u51cf\u5c11\u4e866.8%\u7684\u65f6\u95f4\uff0cMAPE\u4ec5\u4e3a0.58%\u3002FDA\u65b9\u6848\u5728\u4fdd\u6301MSE\u76f8\u540c\u7684\u540c\u65f6\uff0c\u6210\u672c\u964d\u4f4e\u4e8687.6%\u3002", "conclusion": "Atys1\u901a\u8fc7\u5176\u6838\u5fc3\u7279\u6027\uff0c\u6709\u6548\u8bc6\u522b\u70ed\u70b9\u51fd\u6570\uff0c\u51cf\u5c11\u4e86\u5206\u6790\u6210\u672c\u5e76\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u670d\u52a1\u7684\u6027\u80fd\u4f18\u5316\u3002"}}
{"id": "2506.14803", "pdf": "https://arxiv.org/pdf/2506.14803", "abs": "https://arxiv.org/abs/2506.14803", "authors": ["Arbind Agrahari Baniya", "Tsz-Kwan Lee", "Peter W. Eklund", "Sunil Aryal"], "title": "Omnidirectional Video Super-Resolution using Deep Learning", "categories": ["cs.MM", "cs.CV", "cs.LG"], "comment": null, "summary": "Omnidirectional Videos (or 360\u00b0 videos) are widely used in Virtual Reality (VR) to facilitate immersive and interactive viewing experiences. However, the limited spatial resolution in 360\u00b0 videos does not allow for each degree of view to be represented with adequate pixels, limiting the visual quality offered in the immersive experience. Deep learning Video Super-Resolution (VSR) techniques used for conventional videos could provide a promising software-based solution; however, these techniques do not tackle the distortion present in equirectangular projections of 360\u00b0 video signals. An additional obstacle is the limited availability of 360\u00b0 video datasets for study. To address these issues, this paper creates a novel 360\u00b0 Video Dataset (360VDS) with a study of the extensibility of conventional VSR models to 360\u00b0 videos. This paper further proposes a novel deep learning model for 360\u00b0 Video Super-Resolution (360\u00b0 VSR), called Spherical Signal Super-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent modelling with an attention mechanism, unbound from conventional VSR techniques like alignment. With a purpose-built feature extractor and a novel loss function addressing spherical distortion, S3PO outperforms most state-of-the-art conventional VSR models and 360\u00b0~specific super-resolution models on 360\u00b0 video datasets. A step-wise ablation study is presented to understand and demonstrate the impact of the chosen architectural sub-components, targeted training and optimisation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9360\u00b0\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684\u65b0\u65b9\u6cd5S3PO\uff0c\u5e76\u521b\u5efa\u4e86\u65b0\u7684\u6570\u636e\u96c6360VDS\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfVSR\u6280\u672f\u4e2d\u7684\u5931\u771f\u95ee\u9898\u3002", "motivation": "360\u00b0\u89c6\u9891\u7684\u6709\u9650\u7a7a\u95f4\u5206\u8fa8\u7387\u5f71\u54cd\u4e86\u5176\u6c89\u6d78\u5f0f\u4f53\u9a8c\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u800c\u4f20\u7edfVSR\u6280\u672f\u672a\u80fd\u89e3\u51b3\u5176\u6295\u5f71\u5931\u771f\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86S3PO\u6a21\u578b\uff0c\u91c7\u7528\u5faa\u73af\u5efa\u6a21\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bbe\u8ba1\u4e86\u9488\u5bf9\u7403\u5f62\u5931\u771f\u7684\u7279\u5f81\u63d0\u53d6\u5668\u548c\u635f\u5931\u51fd\u6570\u3002", "result": "S3PO\u5728360\u00b0\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfVSR\u548c\u4e13\u4e3a360\u00b0\u89c6\u9891\u8bbe\u8ba1\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u3002", "conclusion": "S3PO\u4e3a360\u00b0\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u67b6\u6784\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.15174", "pdf": "https://arxiv.org/pdf/2506.15174", "abs": "https://arxiv.org/abs/2506.15174", "authors": ["Hossein Albakri", "Kazem Cheshmi"], "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs", "categories": ["cs.PL"], "comment": null, "summary": "Sparse data structures are commonly used in neural networks to reduce the memory footprint. These data structures are compact but cause irregularities such as random memory accesses, which prevent efficient use of the memory hierarchy. GPUs are a common platform for machine learning practitioners, but running compact data structures on these devices often leads to slow-downs due to inefficient use of computing and memory resources. This paper proposes a new compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse matrix-matrix multiplication (SPMM) on GPU devices. The transformation increases data reuse in registers and caches while creating more balanced workloads for GPU computing resources. The transformation is tested on sparse neural networks in convolutional and transformer models. On an A100 GPU and across a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to 128, the transformation yields a geometric mean speedup of 1.84$\\times$ to 2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u7a00\u758f\u6570\u636e\u7ed3\u6784\u5728GPU\u4e0a\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7f16\u8bd1\u5668\u8f6c\u6362\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7a00\u758f\u6570\u636e\u7ed3\u6784\u867d\u80fd\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u4f46\u56e0\u968f\u673a\u5185\u5b58\u8bbf\u95ee\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u5728GPU\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faenumerate-and-sparse-coarsen\u7f16\u8bd1\u5668\u8f6c\u6362\uff0c\u901a\u8fc7\u589e\u52a0\u6570\u636e\u91cd\u7528\u548c\u5e73\u8861\u8d1f\u8f7d\u4f18\u5316\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u3002", "result": "\u5728A100 GPU\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\uff08cuBLAS\u548ccuSPARSE\uff09\u5e26\u6765\u4e861.84\u500d\u52302.27\u500d\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86GPU\u4e0a\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5377\u79ef\u548cTransformer\u6a21\u578b\u3002"}}
{"id": "2506.14987", "pdf": "https://arxiv.org/pdf/2506.14987", "abs": "https://arxiv.org/abs/2506.14987", "authors": ["Eman Alqudah", "Ashfaq Khokhar"], "title": "CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in Industrial URLLC", "categories": ["cs.NI", "cs.LG"], "comment": "This paper has been submitted to IEEEGLOBE2025 on April 15, 2025", "summary": "Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a CNN-based dynamic priority prediction mechanism for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach uses a Convolutional Neural Network and graph coloring to adaptively assign link priorities based on real-time traffic, transmission opportunities, and network conditions. Assuming that first training phase is performed offline, our approach introduced minimal overhead, while enabling more efficient resource allocation, boosting network capacity, SINR, and schedulability. Simulation results show SINR gains of up to 113\\%, 94\\%, and 49\\% over LDP across three network configurations, highlighting its effectiveness for complex URLLC scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u52a8\u6001\u4f18\u5148\u7ea7\u9884\u6d4b\u673a\u5236\uff0c\u6539\u8fdb\u4e86LDP\u7b97\u6cd5\uff0c\u7528\u4e8e\u591a\u5c0f\u533a\u591a\u4fe1\u9053\u7f51\u7edc\u4e2d\u7684\u5e72\u6270\u534f\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\u548c\u8c03\u5ea6\u80fd\u529b\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u786e\u4fdd\u5305\u7ea7\u901a\u4fe1\u8d28\u91cf\u5bf9URLLC\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edfLDP\u7684\u9759\u6001\u4f18\u5148\u7ea7\u5206\u914d\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u52a8\u6001\u9700\u6c42\u3002", "method": "\u7ed3\u5408CNN\u548c\u56fe\u7740\u8272\u7b97\u6cd5\uff0c\u52a8\u6001\u5206\u914d\u94fe\u8def\u4f18\u5148\u7ea7\uff0c\u6839\u636e\u5b9e\u65f6\u6d41\u91cf\u3001\u4f20\u8f93\u673a\u4f1a\u548c\u7f51\u7edc\u6761\u4ef6\u81ea\u9002\u5e94\u8c03\u6574\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e09\u79cd\u7f51\u7edc\u914d\u7f6e\u4e2d\uff0cSINR\u589e\u76ca\u5206\u522b\u9ad8\u8fbe113%\u300194%\u548c49%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742URLLC\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u5bb9\u91cf\u3001SINR\u548c\u8c03\u5ea6\u6548\u7387\u3002"}}
{"id": "2506.14772", "pdf": "https://arxiv.org/pdf/2506.14772", "abs": "https://arxiv.org/abs/2506.14772", "authors": ["Jakob De Moor", "Hans Weytjens", "Johannes De Smedt", "Jochen De Weerdt"], "title": "SimBank: from Simulation to Solution in Prescriptive Process Monitoring", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Prescriptive Process Monitoring (PresPM) is an emerging area within Process Mining, focused on optimizing processes through real-time interventions for effective decision-making. PresPM holds significant promise for organizations seeking enhanced operational performance. However, the current literature faces two key limitations: a lack of extensive comparisons between techniques and insufficient evaluation approaches. To address these gaps, we introduce SimBank: a simulator designed for accurate benchmarking of PresPM methods. Modeled after a bank's loan application process, SimBank enables extensive comparisons of both online and offline PresPM methods. It incorporates a variety of intervention optimization problems with differing levels of complexity and supports experiments on key causal machine learning challenges, such as assessing a method's robustness to confounding in data. SimBank additionally offers a comprehensive evaluation capability: for each test case, it can generate the true outcome under each intervention action, which is not possible using recorded datasets. The simulator incorporates parallel activities and loops, drawing from common logs to generate cases that closely resemble real-life process instances. Our proof of concept demonstrates SimBank's benchmarking capabilities through experiments with various PresPM methods across different interventions, highlighting its value as a publicly available simulator for advancing research and practice in PresPM.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aSimBank\u7684\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u5bf9Prescriptive Process Monitoring\uff08PresPM\uff09\u65b9\u6cd5\u8fdb\u884c\u51c6\u786e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u7684\u6280\u672f\u6bd4\u8f83\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u8db3\u7684\u7a7a\u767d\u3002", "motivation": "PresPM\u9886\u57df\u7f3a\u4e4f\u6280\u672f\u95f4\u7684\u5e7f\u6cdb\u6bd4\u8f83\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e9f\u9700\u5de5\u5177\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u62df\u94f6\u884c\u8d37\u6b3e\u6d41\u7a0b\u7684\u6a21\u62df\u5668SimBank\uff0c\u652f\u6301\u5bf9\u5728\u7ebf\u548c\u79bb\u7ebfPresPM\u65b9\u6cd5\u8fdb\u884c\u5e7f\u6cdb\u6bd4\u8f83\uff0c\u5e76\u5305\u542b\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u5e72\u9884\u4f18\u5316\u95ee\u9898\u3002", "result": "SimBank\u80fd\u591f\u751f\u6210\u6bcf\u79cd\u5e72\u9884\u4e0b\u7684\u771f\u5b9e\u7ed3\u679c\uff0c\u652f\u6301\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u6570\u636e\u6df7\u6742\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u57fa\u51c6\u6d4b\u8bd5\u80fd\u529b\u3002", "conclusion": "SimBank\u662f\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6a21\u62df\u5668\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8PresPM\u9886\u57df\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u3002"}}
{"id": "2506.15006", "pdf": "https://arxiv.org/pdf/2506.15006", "abs": "https://arxiv.org/abs/2506.15006", "authors": ["Jesmin Jahan Tithi", "Hanjiang Wu", "Avishaii Abuhatzera", "Fabrizio Petrini"], "title": "Scaling Intelligence: Designing Data Centers for Next-Gen Language Models", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.ET", "cs.PF"], "comment": "14 pages, submitted to SC25 for review", "summary": "The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8 trillion parameters - demands a radical rethinking of data center architecture to ensure scalability, efficiency, and cost-effectiveness. Our work provides a comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat optical), the size of the scale-out domain, and popular parallelism/optimization strategies used in LLMs. We introduce and evaluate FullFlat network architectures, which provide uniform high-bandwidth, low-latency connectivity between all nodes, and demonstrate their transformative impact on performance and scalability. Through detailed sensitivity analyses, we quantify the benefits of overlapping compute and communication, leveraging hardware-accelerated collectives, wider scale-out domains, and larger memory capacity. Our study spans both sparse (mixture of experts) and dense transformer-based LLMs, revealing how system design choices affect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens per sec / Peak flops of the hardware) and overall throughput. For the co-design study, we extended and validated a performance modeling tool capable of predicting LLM runtime within 10% of real-world measurements. Our findings offer actionable insights and a practical roadmap for designing AI data centers that can efficiently support trillion-parameter models, reduce optimization complexity, and sustain the rapid evolution of AI capabilities.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u9762\u5411\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\u4f18\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790FLOPS\u3001HBM\u5e26\u5bbd\u4e0e\u5bb9\u91cf\u3001\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u7b49\uff0c\u8bc4\u4f30\u4e86FullFlat\u7f51\u7edc\u67b6\u6784\u5bf9\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u968f\u7740LLM\u53c2\u6570\u89c4\u6a21\u7684\u7206\u70b8\u5f0f\u589e\u957f\uff08\u5982GPT-4\u8fbe1.8\u4e07\u4ebf\u53c2\u6570\uff09\uff0c\u4f20\u7edf\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\u96be\u4ee5\u6ee1\u8db3\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u7684\u9700\u6c42\uff0c\u4e9f\u9700\u91cd\u65b0\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u8bbe\u8ba1\u6846\u67b6\uff0c\u7814\u7a76FLOPS\u3001HBM\u5e26\u5bbd\u4e0e\u5bb9\u91cf\u3001\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\u7b49\uff0c\u5e76\u5f15\u5165FullFlat\u7f51\u7edc\u67b6\u6784\uff0c\u8bc4\u4f30\u5176\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u654f\u611f\u6027\u5206\u6790\u91cf\u5316\u4e86\u8ba1\u7b97\u4e0e\u901a\u4fe1\u91cd\u53e0\u3001\u786c\u4ef6\u52a0\u901f\u96c6\u4f53\u901a\u4fe1\u7b49\u6280\u672f\u7684\u4f5c\u7528\u3002", "result": "FullFlat\u7f51\u7edc\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002\u6027\u80fd\u5efa\u6a21\u5de5\u5177\u9884\u6d4b\u8bef\u5dee\u572810%\u4ee5\u5185\uff0c\u4e3a\u652f\u6301\u4e07\u4ebf\u53c2\u6570\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u6548\u652f\u6301\u8d85\u5927\u53c2\u6570LLM\u7684\u6570\u636e\u4e2d\u5fc3\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u548c\u4f18\u5316\u65b9\u5411\uff0c\u964d\u4f4e\u4e86\u590d\u6742\u6027\u5e76\u9002\u5e94AI\u80fd\u529b\u5feb\u901f\u6f14\u8fdb\u7684\u9700\u6c42\u3002"}}
{"id": "2506.14771", "pdf": "https://arxiv.org/pdf/2506.14771", "abs": "https://arxiv.org/abs/2506.14771", "authors": ["Mengyuan Wang", "Yang Liu", "Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "title": "Empirical Studies of Large Scale Environment Scanning by Consumer Electronics", "categories": ["eess.IV", "cs.CV", "cs.ET", "cs.MM"], "comment": "Accepted by IEEE Consumer Electronics Magazine", "summary": "This paper presents an empirical evaluation of the Matterport Pro3, a consumer-grade 3D scanning device, for large-scale environment reconstruction. We conduct detailed scanning (1,099 scanning points) of a six-floor building (17,567 square meters) and assess the device's effectiveness, limitations, and performance enhancements in diverse scenarios. Challenges encountered during the scanning are addressed through proposed solutions, while we also explore advanced methods to overcome them more effectively. Comparative analysis with another consumer-grade device (iPhone) highlights the Pro3's balance between cost-effectiveness and performance. The Matterport Pro3 achieves a denser point cloud with 1,877,324 points compared to the iPhone's 506,961 points and higher alignment accuracy with an RMSE of 0.0118 meters. The cloud-to-cloud (C2C) average distance error between the two point cloud models is 0.0408 meters, with a standard deviation of 0.0715 meters. The study demonstrates the Pro3's ability to generate high-quality 3D models suitable for large-scale applications, leveraging features such as LiDAR and advanced alignment techniques.", "AI": {"tldr": "\u5bf9\u6d88\u8d39\u7ea73D\u626b\u63cf\u8bbe\u5907Matterport Pro3\u8fdb\u884c\u5927\u89c4\u6a21\u73af\u5883\u91cd\u5efa\u7684\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u9ad8\u5bc6\u5ea6\u70b9\u4e91\u548c\u5bf9\u9f50\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8eiPhone\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\u3002", "motivation": "\u8bc4\u4f30Matterport Pro3\u5728\u5927\u578b\u5efa\u7b51\u626b\u63cf\u4e2d\u7684\u6548\u679c\u3001\u9650\u5236\u53ca\u6027\u80fd\u4f18\u5316\u6f5c\u529b\u3002", "method": "\u5bf9\u516d\u5c42\u5efa\u7b51\u8fdb\u884c\u8be6\u7ec6\u626b\u63cf\uff081,099\u4e2a\u70b9\uff09\uff0c\u5e76\u4e0eiPhone\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "Pro3\u751f\u6210\u7684\u70b9\u4e91\u66f4\u5bc6\u96c6\uff081,877,324\u70b9vs. 506,961\u70b9\uff09\uff0c\u5bf9\u9f50\u7cbe\u5ea6\u66f4\u9ad8\uff08RMSE 0.0118\u7c73\uff09\u3002", "conclusion": "Pro3\u5728\u6210\u672c\u6548\u76ca\u4e0e\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u5408\u5927\u89c4\u6a213D\u5efa\u6a21\u3002"}}
{"id": "2506.14851", "pdf": "https://arxiv.org/pdf/2506.14851", "abs": "https://arxiv.org/abs/2506.14851", "authors": ["Yifei Liu", "Zuo Gan", "Zhenghao Gan", "Weiye Wang", "Chen Chen", "Yizhou Shan", "Xusheng Chen", "Zhenhua Han", "Yifei Zhu", "Shixuan Sun", "Minyi Guo"], "title": "Efficient Serving of LLM Applications with Probabilistic Demand Modeling", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Applications based on Large Language Models (LLMs) contains a series of tasks to address real-world problems with boosted capability, which have dynamic demand volumes on diverse backends. Existing serving systems treat the resource demands of LLM applications as a blackbox, compromising end-to-end efficiency due to improper queuing order and backend warm up latency. We find that the resource demands of LLM applications can be modeled in a general and accurate manner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which leverages PDGraph for efficient serving of LLM applications. Confronting probabilistic demand description, Hermes applies the Gittins policy to determine the scheduling order that can minimize the average application completion time. It also uses the PDGraph model to help prewarm cold backends at proper moments. Experiments with diverse LLM applications confirm that Hermes can effectively improve the application serving efficiency, reducing the average completion time by over 70% and the P95 completion time by over 80%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u9700\u6c42\u56fe\uff08PDGraph\uff09\u7684LLM\u5e94\u7528\u670d\u52a1\u7cfb\u7edfHermes\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u670d\u52a1\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5c06LLM\u5e94\u7528\u7684\u8d44\u6e90\u9700\u6c42\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u5bfc\u81f4\u6392\u961f\u987a\u5e8f\u4e0d\u5f53\u548c\u540e\u7aef\u9884\u70ed\u5ef6\u8fdf\uff0c\u5f71\u54cd\u6548\u7387\u3002", "method": "\u4f7f\u7528PDGraph\u5efa\u6a21LLM\u5e94\u7528\u7684\u8d44\u6e90\u9700\u6c42\uff0c\u5e76\u57fa\u4e8eGittins\u7b56\u7565\u786e\u5b9a\u8c03\u5ea6\u987a\u5e8f\u4ee5\u51cf\u5c11\u5b8c\u6210\u65f6\u95f4\uff0c\u540c\u65f6\u5229\u7528PDGraph\u9884\u70ed\u540e\u7aef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHermes\u80fd\u5c06\u5e73\u5747\u5b8c\u6210\u65f6\u95f4\u964d\u4f4e70%\u4ee5\u4e0a\uff0cP95\u5b8c\u6210\u65f6\u95f4\u964d\u4f4e80%\u4ee5\u4e0a\u3002", "conclusion": "PDGraph\u548cHermes\u6709\u6548\u63d0\u5347\u4e86LLM\u5e94\u7528\u7684\u8d44\u6e90\u8c03\u5ea6\u548c\u670d\u52a1\u6548\u7387\u3002"}}
{"id": "2506.15183", "pdf": "https://arxiv.org/pdf/2506.15183", "abs": "https://arxiv.org/abs/2506.15183", "authors": ["Xingyu Chen", "Xinmin Fang", "Shuting Zhang", "Xinyu Zhang", "Liang He", "Zhengxiong Li"], "title": "You Only Render Once: Enhancing Energy and Computation Efficiency of Mobile Virtual Reality", "categories": ["cs.GR"], "comment": null, "summary": "Mobile Virtual Reality (VR) is essential to achieving convenient and immersive human-computer interaction and realizing emerging applications such as Metaverse. However, existing VR technologies require two separate renderings of binocular images, causing a significant bottleneck for mobile devices with limited computing capability and power supply. This paper proposes an approach to rendering optimization for mobile VR called EffVR. By utilizing the per-pixel attribute, EffVR can generate binocular VR images from the monocular image through genuinely one rendering, saving half the computation over conventional approaches. Our evaluation indicates that, compared with the state-of-art, EffVRcan save 27% power consumption on average while achieving high binocular image quality (0.9679 SSIM and 34.09 PSNR) in mobile VR applications. Additionally, EffVR can increase the frame rate by 115.2%. These results corroborate EffVRsuperior computation/energy-saving performance, paving the road to a sustainable mobile VR. The source code, demo video, android app, and more are released anonymously at https://yoro-vr.github.io/", "AI": {"tldr": "EffVR\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u79fb\u52a8VR\u7684\u6e32\u67d3\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u6e32\u67d3\u751f\u6210\u53cc\u773c\u56fe\u50cf\uff0c\u663e\u8457\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u548c\u80fd\u8017\u3002", "motivation": "\u5f53\u524dVR\u6280\u672f\u9700\u8981\u5206\u522b\u6e32\u67d3\u53cc\u773c\u56fe\u50cf\uff0c\u5bf9\u8ba1\u7b97\u80fd\u529b\u548c\u7535\u6e90\u6709\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u9020\u6210\u74f6\u9888\u3002", "method": "\u5229\u7528\u50cf\u7d20\u5c5e\u6027\uff0cEffVR\u901a\u8fc7\u5355\u6b21\u6e32\u67d3\u4ece\u5355\u773c\u56fe\u50cf\u751f\u6210\u53cc\u773cVR\u56fe\u50cf\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0cEffVR\u5e73\u5747\u8282\u770127%\u80fd\u8017\uff0c\u56fe\u50cf\u8d28\u91cf\u9ad8\uff08SSIM 0.9679\uff0cPSNR 34.09\uff09\uff0c\u5e27\u7387\u63d0\u5347115.2%\u3002", "conclusion": "EffVR\u5728\u8ba1\u7b97/\u8282\u80fd\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u4e3a\u53ef\u6301\u7eed\u79fb\u52a8VR\u53d1\u5c55\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2506.14775", "pdf": "https://arxiv.org/pdf/2506.14775", "abs": "https://arxiv.org/abs/2506.14775", "authors": ["Tobias Labarta", "Nhi Hoang", "Katharina Weitz", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "title": "See What I Mean? CUE: A Cognitive Model of Understanding Explanations", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "10 pages, 5 figures (main text), 4 tables, 455-participant user study", "summary": "As machine learning systems increasingly inform critical decisions, the need for human-understandable explanations grows. Current evaluations of Explainable AI (XAI) often prioritize technical fidelity over cognitive accessibility which critically affects users, in particular those with visual impairments. We propose CUE, a model for Cognitive Understanding of Explanations, linking explanation properties to cognitive sub-processes: legibility (perception), readability (comprehension), and interpretability (interpretation). In a study (N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we found comparable task performance but lower confidence/effort for visually impaired users. Unlike expected, these gaps were not mitigated and sometimes worsened by accessibility-focused color maps like Cividis. These results challenge assumptions about perceptual optimization and support the need for adaptive XAI interfaces. They also validate CUE by demonstrating that altering explanation legibility affects understandability. We contribute: (1) a formalized cognitive model for explanation understanding, (2) an integrated definition of human-centered explanation properties, and (3) empirical evidence motivating accessible, user-tailored XAI.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86CUE\u6a21\u578b\uff0c\u901a\u8fc7\u94fe\u63a5\u89e3\u91ca\u5c5e\u6027\u548c\u8ba4\u77e5\u5b50\u8fc7\u7a0b\uff08\u6613\u8bfb\u6027\u3001\u53ef\u8bfb\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff09\u6765\u63d0\u9ad8AI\u89e3\u91ca\u7684\u8ba4\u77e5\u53ef\u8bbf\u95ee\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u89c6\u89c9\u969c\u788d\u7528\u6237\u5728\u4efb\u52a1\u8868\u73b0\u4e0a\u4e0e\u975e\u969c\u788d\u7528\u6237\u76f8\u5f53\uff0c\u4f46\u5728\u4fe1\u5fc3\u548c\u52aa\u529b\u4e0a\u8f83\u4f4e\uff0c\u4e14\u7279\u5b9a\u989c\u8272\u6620\u5c04\u5e76\u672a\u6539\u5584\u8fd9\u79cd\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u5173\u952e\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u9700\u8981\u66f4\u6613\u7406\u89e3\u7684AI\u89e3\u91ca\uff0c\u7279\u522b\u662f\u9488\u5bf9\u89c6\u89c9\u969c\u788d\u7528\u6237\u3002\u73b0\u6709\u8bc4\u4f30\u5e38\u5ffd\u89c6\u8ba4\u77e5\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u63d0\u51faCUE\u6a21\u578b\uff0c\u5c06\u89e3\u91ca\u5c5e\u6027\uff08\u6613\u8bfb\u6027\u3001\u53ef\u8bfb\u6027\u3001\u53ef\u89e3\u91ca\u6027\uff09\u4e0e\u8ba4\u77e5\u5b50\u8fc7\u7a0b\u8054\u7cfb\u8d77\u6765\u3002\u901a\u8fc7\u5b9e\u9a8c\uff08N=455\uff09\u6d4b\u8bd5\u4e0d\u540c\u989c\u8272\u6620\u5c04\u7684\u70ed\u56fe\uff0c\u5206\u6790\u7528\u6237\u8868\u73b0\u3002", "result": "\u4efb\u52a1\u8868\u73b0\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u89c6\u89c9\u969c\u788d\u7528\u6237\u4fe1\u5fc3/\u52aa\u529b\u66f4\u4f4e\u3002\u7279\u5b9a\u989c\u8272\u6620\u5c04\uff08\u5982Cividis\uff09\u672a\u6539\u5584\u751a\u81f3\u52a0\u5267\u5dee\u5f02\u3002\u7ed3\u679c\u8868\u660e\u9700\u8981\u81ea\u9002\u5e94XAI\u754c\u9762\u3002", "conclusion": "\u7ed3\u679c\u6311\u6218\u4e86\u611f\u77e5\u4f18\u5316\u7684\u5047\u8bbe\uff0c\u652f\u6301\u81ea\u9002\u5e94XAI\u754c\u9762\u7684\u5fc5\u8981\u6027\u3002CUE\u6a21\u578b\u901a\u8fc7\u6539\u53d8\u6613\u8bfb\u6027\u5f71\u54cd\u7406\u89e3\u6027\uff0c\u4e3a\u4eba\u7c7b\u4e2d\u5fc3\u89e3\u91ca\u5c5e\u6027\u63d0\u4f9b\u4e86\u5b9a\u4e49\u548c\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2506.15295", "pdf": "https://arxiv.org/pdf/2506.15295", "abs": "https://arxiv.org/abs/2506.15295", "authors": ["Massimo Bartoletti", "Enrico Lipparini"], "title": "A theory of Lending Protocols in DeFi", "categories": ["cs.GT", "cs.CR", "cs.LO"], "comment": null, "summary": "Lending protocols are one of the main applications of Decentralized Finance (DeFi), enabling crypto-assets loan markets with a total value estimated in the tens of billions of dollars. Unlike traditional lending systems, these protocols operate without relying on trusted authorities or off-chain enforcement mechanisms. To achieve key economic goals such as stability of the loan market, they devise instead trustless on-chain mechanisms, such as rewarding liquidators who repay the loans of under-collateralized borrowers by awarding them part of the borrower's collateral. The complexity of these incentive mechanisms, combined with their entanglement in low-level implementation details, makes it challenging to precisely assess the structural and economic properties of lending protocols, as well as to analyze user strategies and attacks. Crucially, since participation is open to anyone, any weaknesses in the incentive mechanism may give rise to unintended emergent behaviours, or even enable adversarial strategies aimed at making profits to the detriment of legit users, or at undermining the stability of the protocol. In this work, we propose a formal model of lending protocols that captures the essential features of mainstream platforms, enabling us to identify and prove key properties related to their economic and strategic dynamics.", "AI": {"tldr": "\u603b\u7ed3\u53bb\u4e2d\u5fc3\u5316\u91d1\u878d\uff08DeFi\uff09\u501f\u8d37\u534f\u8bae\u7684\u6fc0\u52b1\u673a\u5236\u590d\u6742\u6027\u53ca\u5176\u6f5c\u5728\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5f62\u5f0f\u5316\u6a21\u578b\u6765\u5206\u6790\u5176\u7ecf\u6d4e\u548c\u6218\u7565\u52a8\u6001\u3002", "motivation": "\u7814\u7a76\u501f\u8d37\u534f\u8bae\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u8bc4\u4f30\u5176\u7ed3\u6784\u6027\u548c\u7ecf\u6d4e\u6027\uff0c\u9632\u6b62\u56e0\u673a\u5236\u7f3a\u9677\u5bfc\u81f4\u7684\u4e0d\u826f\u884c\u4e3a\u6216\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u6355\u6349\u4e3b\u6d41\u5e73\u53f0\u7684\u6838\u5fc3\u7279\u5f81\uff0c\u5e76\u9a8c\u8bc1\u5176\u7ecf\u6d4e\u548c\u6218\u7565\u52a8\u6001\u7684\u5173\u952e\u5c5e\u6027\u3002", "result": "\u901a\u8fc7\u6a21\u578b\u8bc6\u522b\u5e76\u8bc1\u5b9e\u4e86\u501f\u8d37\u534f\u8bae\u7684\u5173\u952e\u5c5e\u6027\uff0c\u63ed\u793a\u4e86\u6f5c\u5728\u7684\u7ecf\u6d4e\u548c\u6218\u7565\u95ee\u9898\u3002", "conclusion": "\u5f62\u5f0f\u5316\u6a21\u578b\u4e3a\u501f\u8d37\u534f\u8bae\u7684\u7ecf\u6d4e\u548c\u6218\u7565\u52a8\u6001\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u5206\u6790\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u9884\u9632\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2506.14852", "pdf": "https://arxiv.org/pdf/2506.14852", "abs": "https://arxiv.org/abs/2506.14852", "authors": ["Qizheng Zhang", "Michael Wornow", "Kunle Olukotun"], "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG", "cs.PF"], "comment": "23 pages", "summary": "LLM-based agentic applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs due to extensive planning and reasoning requirements. Existing LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agentic applications where outputs depend on external data or environmental contexts. We propose agentic plan caching, a novel approach that extracts, stores, adapts, and reuses structured plan templates from planning stages of agentic applications across semantically similar tasks to reduce the cost of serving. Unlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. Evaluation across multiple real-world agentic applications shows that our system can reduce costs by 46.62% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LLM\u4ee3\u7406\u5e94\u7528\u7684\u65b0\u7f13\u5b58\u65b9\u6cd5\u2014\u2014\u4ee3\u7406\u8ba1\u5212\u7f13\u5b58\uff0c\u901a\u8fc7\u63d0\u53d6\u3001\u5b58\u50a8\u548c\u91cd\u7528\u7ed3\u6784\u5316\u7684\u8ba1\u5212\u6a21\u677f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u670d\u52a1\u6210\u672c\uff08\u5e73\u574746.62%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684LLM\u7f13\u5b58\u6280\u672f\uff08\u5982\u4e0a\u4e0b\u6587\u7f13\u5b58\u548c\u8bed\u4e49\u7f13\u5b58\uff09\u4e3b\u8981\u4e3a\u804a\u5929\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4ee3\u7406\u5e94\u7528\u4e2d\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u6216\u73af\u5883\u4e0a\u4e0b\u6587\u7684\u590d\u6742\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7f13\u5b58\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4ee3\u7406\u8ba1\u5212\u7f13\u5b58\uff0c\u4ece\u5df2\u5b8c\u6210\u7684\u4ee3\u7406\u6267\u884c\u4e2d\u63d0\u53d6\u8ba1\u5212\u6a21\u677f\uff0c\u901a\u8fc7\u5173\u952e\u8bcd\u5339\u914d\u65b0\u8bf7\u6c42\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u5c06\u6a21\u677f\u9002\u914d\u5230\u4efb\u52a1\u7279\u5b9a\u8ba1\u5212\u4e2d\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u53ef\u964d\u4f4e46.62%\u7684\u670d\u52a1\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u4ee3\u7406\u8ba1\u5212\u7f13\u5b58\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8865\u5145\u4e86\u73b0\u6709LLM\u670d\u52a1\u57fa\u7840\u8bbe\u65bd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u4ee3\u7406\u5e94\u7528\u3002"}}
{"id": "2506.15424", "pdf": "https://arxiv.org/pdf/2506.15424", "abs": "https://arxiv.org/abs/2506.15424", "authors": ["Michael Mendler", "Marc Pouzet"], "title": "PSM: Policy Synchronised Deterministic Memory", "categories": ["cs.PL"], "comment": "This report summarises work on coding the theory of policy-synchronised memory (see https://rdcu.be/erBwl) in Haskell. This was developed for a graduate level course on Functional Reactive Programming taught at Bamberg University by the first author during 2020-2023. An early version of the PSM library had been presented at the SYNCHRON Workshop (Aussois, France), November 2019", "summary": "Concurrency and determinacy do not go well with each other when resources must be shared. Haskell provides parallel programming abstractions such as IVar and LVar in the Par monad and concurrent abstractions such as MVar and TVar in the in IO and STM monads, respectively. The former are determinate but have no destructive updates and the latter have destructive updates but do not guarantee determinacy. Programming patterns that are both concurrent and determinate, such as those provided by Kahn or Berry require memory abstractions at a higher level than is currently available. In this paper we describe a new type context PSM for policy synchronised memory in Haskell. Like STM and IO, the computations in PSM can access persistent state and, as a side-effect, update the memory in imperative style. Like the Par and IO monads, PSM supports concurrent threads and shared state. However, in contrast to IO, our PSM contexts are race-free since concurrent accesses are policy coordinated which guarantees determinacy.Well-typed transactions in the PSM context can accommodate abstract data structures that are imperative, concurrently shareable and still behave deterministically, by construction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Haskell\u5185\u5b58\u7c7b\u578b\u4e0a\u4e0b\u6587PSM\uff0c\u7ed3\u5408\u4e86\u5e76\u884c\u6027\u548c\u786e\u5b9a\u6027\uff0c\u89e3\u51b3\u4e86\u8d44\u6e90\u5171\u4eab\u65f6\u7684\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684Haskell\u5e76\u884c\u7f16\u7a0b\u62bd\u8c61\uff08\u5982IVar\u548cLVar\uff09\u4e0e\u5e76\u53d1\u62bd\u8c61\uff08\u5982MVar\u548cTVar\uff09\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u786e\u5b9a\u6027\u548c\u7834\u574f\u6027\u66f4\u65b0\u7684\u9700\u6c42\uff0c\u9700\u8981\u66f4\u9ad8\u5c42\u6b21\u7684\u5185\u5b58\u62bd\u8c61\u3002", "method": "\u5f15\u5165PSM\uff08Policy Synchronised Memory\uff09\u7c7b\u578b\u4e0a\u4e0b\u6587\uff0c\u652f\u6301\u6301\u4e45\u72b6\u6001\u8bbf\u95ee\u548c\u7834\u574f\u6027\u66f4\u65b0\uff0c\u540c\u65f6\u901a\u8fc7\u7b56\u7565\u534f\u8c03\u4fdd\u8bc1\u5e76\u53d1\u8bbf\u95ee\u7684\u65e0\u7ade\u4e89\u6027\u548c\u786e\u5b9a\u6027\u3002", "result": "PSM\u4e0a\u4e0b\u6587\u4e2d\u7684\u4e8b\u52a1\u53ef\u4ee5\u6784\u5efa\u62bd\u8c61\u6570\u636e\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7ed3\u6784\u5177\u6709\u547d\u4ee4\u5f0f\u3001\u53ef\u5e76\u53d1\u5171\u4eab\u4e14\u786e\u5b9a\u6027\u7684\u7279\u70b9\u3002", "conclusion": "PSM\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u5177\u5e76\u884c\u6027\u548c\u786e\u5b9a\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5171\u4eab\u8d44\u6e90\u4e14\u4fdd\u6301\u786e\u5b9a\u6027\u7684\u7f16\u7a0b\u6a21\u5f0f\u3002"}}
{"id": "2506.15011", "pdf": "https://arxiv.org/pdf/2506.15011", "abs": "https://arxiv.org/abs/2506.15011", "authors": ["Eman Alqudah", "Ashfaq Khokhar"], "title": "GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC", "categories": ["cs.NI", "cs.LG"], "comment": "This paper has been submitted to IEEE MASS 2025 on May 7, 2025", "summary": "Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph Convolutional Network (GCN) integrated with a Deep Q-Network (DQN) reinforcement learning framework for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach dynamically learns link priorities based on real-time traffic demand, network topology, remaining transmission opportunities, and interference patterns. The GCN captures spatial dependencies, while the DQN enables adaptive scheduling decisions through reward-guided exploration. Simulation results show that our GCN-DQN model achieves mean SINR improvements of 179.6\\%, 197.4\\%, and 175.2\\% over LDP across three network configurations. Additionally, the GCN-DQN model demonstrates mean SINR improvements of 31.5\\%, 53.0\\%, and 84.7\\% over our previous CNN-based approach across the same configurations. These results underscore the effectiveness of our GCN-DQN model in addressing complex URLLC requirements with minimal overhead and superior network performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GCN\u548cDQN\u7684\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86LDP\u7b97\u6cd5\uff0c\u4ee5\u52a8\u6001\u5b66\u4e60\u94fe\u63a5\u4f18\u5148\u7ea7\uff0c\u4f18\u5316\u591a\u7ec6\u80de\u3001\u591a\u901a\u9053\u7f51\u7edc\u4e2d\u7684\u5e72\u6270\u534f\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86SINR\u6027\u80fd\u3002", "motivation": "\u4e3a\u786e\u4fdd\u5927\u89c4\u6a21\u5de5\u4e1a\u65e0\u7ebf\u7f51\u7edc\u4e2dURLLC\u7684\u5305\u7ea7\u901a\u4fe1\u8d28\u91cf\uff0c\u9700\u8981\u89e3\u51b3\u4f20\u7edf\u9759\u6001\u4f18\u5148\u7ea7\u65b9\u6cd5\uff08\u5982LDP\uff09\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u9700\u6c42\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408GCN\uff08\u6355\u6349\u7a7a\u95f4\u4f9d\u8d56\uff09\u548cDQN\uff08\u81ea\u9002\u5e94\u8c03\u5ea6\u51b3\u7b56\uff09\uff0c\u52a8\u6001\u5b66\u4e60\u94fe\u63a5\u4f18\u5148\u7ea7\u3002", "result": "\u76f8\u6bd4LDP\uff0cGCN-DQN\u5728\u4e09\u79cd\u7f51\u7edc\u914d\u7f6e\u4e2d\u5e73\u5747SINR\u5206\u522b\u63d0\u5347\u4e86179.6%\u3001197.4%\u548c175.2%\uff1b\u76f8\u6bd4\u4e4b\u524d\u7684CNN\u65b9\u6cd5\uff0c\u63d0\u5347\u4e8631.5%\u300153.0%\u548c84.7%\u3002", "conclusion": "GCN-DQN\u80fd\u9ad8\u6548\u6ee1\u8db3URLLC\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347\u7f51\u7edc\u6027\u80fd\uff0c\u4e14\u5f00\u9500\u4f4e\u3002"}}
{"id": "2506.15066", "pdf": "https://arxiv.org/pdf/2506.15066", "abs": "https://arxiv.org/abs/2506.15066", "authors": ["Jianmin Ye", "Tianyang Liu", "Qi Tian", "Shengchu Su", "Zhe Jiang", "Xi Wang"], "title": "ChatModel: Automating Reference Model Design and Verification with LLMs", "categories": ["cs.AR", "cs.MA"], "comment": null, "summary": "As the complexity of integrated circuit designs continues to escalate, the functional verification becomes increasingly challenging. Reference models, critical for accelerating the verification process, are themselves becoming more intricate and time-consuming to develop. Despite the promise shown by large language models (LLMs) in code programming, effectively generating complex reference models remains a significant hurdle. To address these challenges, we introduce ChatModel, the first LLM-aided agile reference model generation and verification platform. ChatModel streamlines the transition from design specifications to fully functional reference models by integrating design standardization and hierarchical agile modeling. Employing a building-block generation strategy, it not only enhances the design capabilities of LLMs for reference models but also significantly boosts verification efficiency. We evaluated ChatModel on 300 designs of varying complexity, demonstrating substantial improvements in both efficiency and quality of reference model generation. ChatModel achieved a peak performance improvement of 55.02% compared to alternative methods, with notable enhancements in generation stability, and delivered a 9.18x increase in its capacity to produce reference model designs. Furthermore, it accelerated the iterative process of reference model design and validation by an average of 5.90x compared to traditional approaches. These results highlight the potential of ChatModel to significantly advance the automation of reference model generation and validation.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86ChatModel\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u654f\u6377\u53c2\u8003\u6a21\u578b\u751f\u6210\u4e0e\u9a8c\u8bc1\u5e73\u53f0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53c2\u8003\u6a21\u578b\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u529f\u80fd\u9a8c\u8bc1\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\uff0c\u4f20\u7edf\u53c2\u8003\u6a21\u578b\u5f00\u53d1\u8017\u65f6\u4e14\u590d\u6742\u3002", "method": "ChatModel\u901a\u8fc7\u8bbe\u8ba1\u6807\u51c6\u5316\u548c\u5c42\u6b21\u5316\u654f\u6377\u5efa\u6a21\uff0c\u91c7\u7528\u6a21\u5757\u5316\u751f\u6210\u7b56\u7565\uff0c\u4f18\u5316LLM\u7684\u53c2\u8003\u6a21\u578b\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728300\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u8bbe\u8ba1\u4e2d\uff0cChatModel\u6548\u7387\u63d0\u534755.02%\uff0c\u751f\u6210\u7a33\u5b9a\u6027\u589e\u5f3a\uff0c\u53c2\u8003\u6a21\u578b\u8bbe\u8ba1\u80fd\u529b\u63d0\u53479.18\u500d\uff0c\u8fed\u4ee3\u901f\u5ea6\u63d0\u53475.90\u500d\u3002", "conclusion": "ChatModel\u5c55\u793a\u4e86\u5728\u53c2\u8003\u6a21\u578b\u751f\u6210\u4e0e\u9a8c\u8bc1\u81ea\u52a8\u5316\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.14928", "pdf": "https://arxiv.org/pdf/2506.14928", "abs": "https://arxiv.org/abs/2506.14928", "authors": ["Dyk Chung Nguyen", "Thomas Chetaille", "Yuan-Hang Zhang", "Yuriy V. Pershin", "Massimiliano Di Ventra"], "title": "On the solvable-unsolvable transition due to noise-induced chaos in digital memcomputing", "categories": ["nlin.CD", "cs.ET"], "comment": null, "summary": "Digital memcomputing machines (DMMs) have been designed to solve complex combinatorial optimization problems. Since DMMs are fundamentally classical dynamical systems, their ordinary differential equations (ODEs) can be efficiently simulated on modern computers. This provides a unique platform to study their performance under various conditions. An aspect that has received little attention so far is how their performance is affected by the numerical errors in the solution of their ODEs and the physical noise they would be naturally subject to if built in hardware. Here, we analyze these two aspects in detail by varying the integration time step (numerical noise) and adding stochastic perturbations (physical noise) into the equations of DMMs. We are particularly interested in understanding how noise induces a chaotic transition that marks the shift from successful problem-solving to failure in these systems. Our study includes an analysis of power spectra and Lyapunov exponents depending on the noise strength. The results reveal a correlation between the instance solvability and the sign of the ensemble averaged mean largest Lyapunov exponent. Interestingly, we find a regime in which DMMs with positive mean largest Lyapunov exponents still exhibit solvability. Furthermore, the power spectra provide additional information about our system by distinguishing between regular behavior (peaks) and chaotic behavior (broadband spectrum). Therefore, power spectra could be utilized to control whether a DMM operates in the optimal dynamical regime. Overall, we find that the qualitative effects of numerical and physical noise are mostly similar, despite their fundamentally different origin.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6570\u5b57\u5185\u5b58\u8ba1\u7b97\u673a\u5668\uff08DMMs\uff09\u5728\u6570\u503c\u566a\u58f0\u548c\u7269\u7406\u566a\u58f0\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u566a\u58f0\u5982\u4f55\u5bfc\u81f4\u7cfb\u7edf\u4ece\u6210\u529f\u89e3\u51b3\u95ee\u9898\u7684\u6df7\u6c8c\u8f6c\u53d8\u3002\u901a\u8fc7\u529f\u7387\u8c31\u548c\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u7684\u5206\u6790\uff0c\u53d1\u73b0\u566a\u58f0\u5f3a\u5ea6\u4e0e\u7cfb\u7edf\u53ef\u89e3\u6027\u76f8\u5173\u3002", "motivation": "\u7814\u7a76DMMs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u9700\u8981\u4e86\u89e3\u6570\u503c\u8bef\u5dee\u548c\u7269\u7406\u566a\u58f0\u5982\u4f55\u5f71\u54cd\u5176\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u79ef\u5206\u65f6\u95f4\u6b65\u957f\uff08\u6570\u503c\u566a\u58f0\uff09\u548c\u5f15\u5165\u968f\u673a\u6270\u52a8\uff08\u7269\u7406\u566a\u58f0\uff09\u5206\u6790DMMs\u7684\u6027\u80fd\uff0c\u91cd\u70b9\u89c2\u5bdf\u529f\u7387\u8c31\u548c\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u566a\u58f0\u5f3a\u5ea6\u4e0e\u7cfb\u7edf\u53ef\u89e3\u6027\u76f8\u5173\uff0c\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u674e\u96c5\u666e\u8bfa\u592b\u6307\u6570\u4e3a\u6b63\u503c\uff0c\u7cfb\u7edf\u4ecd\u80fd\u4fdd\u6301\u53ef\u89e3\u6027\u3002\u529f\u7387\u8c31\u80fd\u6709\u6548\u533a\u5206\u7cfb\u7edf\u7684\u89c4\u5219\u548c\u6df7\u6c8c\u884c\u4e3a\u3002", "conclusion": "\u6570\u503c\u566a\u58f0\u548c\u7269\u7406\u566a\u58f0\u5bf9DMMs\u7684\u5b9a\u6027\u5f71\u54cd\u76f8\u4f3c\uff0c\u529f\u7387\u8c31\u53ef\u7528\u4e8e\u4f18\u5316DMMs\u7684\u52a8\u6001\u5de5\u4f5c\u72b6\u6001\u3002"}}
{"id": "2506.15290", "pdf": "https://arxiv.org/pdf/2506.15290", "abs": "https://arxiv.org/abs/2506.15290", "authors": ["Andela Ilic", "Jiaxi Jiang", "Paul Streli", "Xintong Liu", "Christian Holz"], "title": "Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.HC"], "comment": "Accepted by IJCAI 2025", "summary": "Motion capture using sparse inertial sensors has shown great promise due to its portability and lack of occlusion issues compared to camera-based tracking. Existing approaches typically assume that IMU sensors are tightly attached to the human body. However, this assumption often does not hold in real-world scenarios. In this paper, we present a new task of full-body human pose estimation using sparse, loosely attached IMU sensors. To solve this task, we simulate IMU recordings from an existing garment-aware human motion dataset. We developed transformer-based diffusion models to synthesize loose IMU data and estimate human poses based on this challenging loose IMU data. In addition, we show that incorporating garment-related parameters while training the model on simulated loose data effectively maintains expressiveness and enhances the ability to capture variations introduced by looser or tighter garments. Experiments show that our proposed diffusion methods trained on simulated and synthetic data outperformed the state-of-the-art methods quantitatively and qualitatively, opening up a promising direction for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4f7f\u7528\u7a00\u758f\u3001\u677e\u6563\u56fa\u5b9a\u7684\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u4f20\u611f\u5668\u8fdb\u884c\u5168\u8eab\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u65b0\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbeIMU\u4f20\u611f\u5668\u7d27\u8d34\u4eba\u4f53\uff0c\u4f46\u73b0\u5b9e\u4e2d\u8fd9\u4e00\u5047\u8bbe\u5e38\u4e0d\u6210\u7acb\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63a2\u7d22\u4e86\u677e\u6563\u56fa\u5b9aIMU\u7684\u5b9e\u7528\u573a\u666f\u3002", "method": "\u901a\u8fc7\u4ece\u73b0\u6709\u670d\u88c5\u611f\u77e5\u8fd0\u52a8\u6570\u636e\u96c6\u4e2d\u6a21\u62dfIMU\u8bb0\u5f55\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\u6765\u5408\u6210\u677e\u6563IMU\u6570\u636e\u5e76\u4f30\u8ba1\u4eba\u4f53\u59ff\u6001\u3002\u540c\u65f6\u5f15\u5165\u670d\u88c5\u76f8\u5173\u53c2\u6570\u4ee5\u63d0\u5347\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u672c\u6587\u4e3a\u677e\u6563IMU\u4f20\u611f\u5668\u7684\u59ff\u6001\u4f30\u8ba1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5c55\u793a\u4e86\u5176\u5728\u672a\u6765\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.14777", "pdf": "https://arxiv.org/pdf/2506.14777", "abs": "https://arxiv.org/abs/2506.14777", "authors": ["Jules Leguy", "Pierre-Antoine Jean", "Felipe Torres Figueroa", "S\u00e9bastien Harispe"], "title": "WebXAII: an open-source web framework to study human-XAI interaction", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "This article introduces WebXAII, an open-source web framework designed to facilitate research on human interaction with eXplainable Artificial Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by the growing societal implications of the widespread adoption of AI (and in particular machine learning) across diverse applications. Researchers who study the interaction between humans and XAI techniques typically develop ad hoc interfaces in order to conduct their studies. These interfaces are usually not shared alongside the results of the studies, which limits their reusability and the reproducibility of experiments. In response, we design and implement WebXAII, a web-based platform that can embody full experimental protocols, meaning that it can present all aspects of the experiment to human participants and record their responses. The experimental protocols are translated into a composite architecture of generic views and modules, which offers a lot of flexibility. The architecture is defined in a structured configuration file, so that protocols can be implemented with minimal programming skills. We demonstrate that WebXAII can effectively embody relevant protocols, by reproducing the protocol of a state-of-the-art study of the literature. The framework is available at https://github.com/PAJEAN/WebXAII.", "AI": {"tldr": "WebXAII\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u65e8\u5728\u7b80\u5316\u4eba\u7c7b\u4e0e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7cfb\u7edf\u4ea4\u4e92\u7684\u7814\u7a76\uff0c\u63d0\u4f9b\u5b9e\u9a8c\u534f\u8bae\u7684\u5b8c\u6574\u5b9e\u73b0\u3002", "motivation": "\u7531\u4e8eAI\u5e7f\u6cdb\u5e94\u7528\u7684\u793e\u4f1a\u5f71\u54cd\uff0cXAI\u7814\u7a76\u9700\u6c42\u589e\u957f\uff0c\u4f46\u73b0\u6709\u5b9e\u9a8c\u63a5\u53e3\u7f3a\u4e4f\u5171\u4eab\u548c\u53ef\u590d\u7528\u6027\u3002WebXAII\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u57fa\u4e8eWeb\u7684\u5e73\u53f0\u8bbe\u8ba1\u901a\u7528\u89c6\u56fe\u548c\u6a21\u5757\u7684\u590d\u5408\u67b6\u6784\uff0c\u5b9e\u9a8c\u534f\u8bae\u53ef\u901a\u8fc7\u7ed3\u6784\u5316\u914d\u7f6e\u6587\u4ef6\u5b9e\u73b0\uff0c\u65e0\u9700\u9ad8\u7f16\u7a0b\u6280\u80fd\u3002", "result": "WebXAII\u6210\u529f\u590d\u73b0\u4e86\u4e00\u9879\u524d\u6cbf\u7814\u7a76\u7684\u534f\u8bae\uff0c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "conclusion": "WebXAII\u4e3aXAI\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\uff0c\u63d0\u5347\u5b9e\u9a8c\u53ef\u590d\u7528\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2506.15172", "pdf": "https://arxiv.org/pdf/2506.15172", "abs": "https://arxiv.org/abs/2506.15172", "authors": ["Maria Spichkova", "Kevin Iwan", "Madeleine Zwart", "Hina Lee", "Yuwon Yoon", "Xiaohan Qin"], "title": "Advanced approach for Agile/Scrum Process: RetroAI++", "categories": ["cs.SE"], "comment": "Preprint. Accepted to the 29th International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025). Final version to be published by Elsevier (In Press)", "summary": "In Agile/Scrum software development, sprint planning and retrospective analysis are the key elements of project management. The aim of our work is to support software developers in these activities. In this paper, we present our prototype tool RetroAI++, based on emerging intelligent technologies. In our RetroAI++ prototype, we aim to automate and refine the practical application of Agile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI insights, our prototype aims to automate and refine the many processes involved in the Sprint Planning, Development and Retrospective stages of Agile/Scrum development projects, offering intelligent suggestions for sprint organisation as well as meaningful insights for retrospective reflection.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u6280\u672f\u7684\u5de5\u5177RetroAI++\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u4f18\u5316Agile/Scrum\u5f00\u53d1\u4e2d\u7684\u51b2\u523a\u8ba1\u5212\u548c\u56de\u987e\u5206\u6790\u3002", "motivation": "\u652f\u6301\u8f6f\u4ef6\u5f00\u53d1\u8005\u5728Agile/Scrum\u9879\u76ee\u4e2d\u7684\u51b2\u523a\u8ba1\u5212\u548c\u56de\u987e\u6d3b\u52a8\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u5f00\u53d1\u539f\u578b\u5de5\u5177RetroAI++\uff0c\u5229\u7528AI\u6280\u672f\u81ea\u52a8\u5316\u4f18\u5316\u51b2\u523a\u8ba1\u5212\u3001\u5f00\u53d1\u548c\u56de\u987e\u9636\u6bb5\u6d41\u7a0b\u3002", "result": "\u5de5\u5177\u63d0\u4f9b\u667a\u80fd\u51b2\u523a\u7ec4\u7ec7\u5efa\u8bae\u548c\u6709\u610f\u4e49\u7684\u56de\u987e\u53cd\u601d\u89c1\u89e3\u3002", "conclusion": "RetroAI++\u80fd\u6709\u6548\u63d0\u5347Agile/Scrum\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7684\u7ba1\u7406\u6548\u7387\u3002"}}
{"id": "2506.14937", "pdf": "https://arxiv.org/pdf/2506.14937", "abs": "https://arxiv.org/abs/2506.14937", "authors": ["Luan Gon\u00e7alves Miranda", "Pedro Ivo da Cruz", "Murilo Bellezoni Loiola"], "title": "Determina\u00e7\u00e3o Autom\u00e1tica de Limiar de Detec\u00e7\u00e3o de Ataques em Redes de Computadores Utilizando Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI", "cs.PF"], "comment": "This work was accepted at SBrT 2022 (Brazilian Symposium on Telecommunications and Signal Processing), though it was not included in the official proceedings. in Portuguese language", "summary": "Currently, digital security mechanisms like Anomaly Detection Systems using Autoencoders (AE) show great potential for bypassing problems intrinsic to the data, such as data imbalance. Because AE use a non-trivial and nonstandardized separation threshold to classify the extracted reconstruction error, the definition of this threshold directly impacts the performance of the detection process. Thus, this work proposes the automatic definition of this threshold using some machine learning algorithms. For this, three algorithms were evaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u81ea\u52a8\u5b9a\u4e49\u81ea\u7f16\u7801\u5668\u7684\u5f02\u5e38\u68c0\u6d4b\u9608\u503c\uff0c\u8bc4\u4f30\u4e86KNN\u3001K-Means\u548cSVM\u4e09\u79cd\u7b97\u6cd5\u3002", "motivation": "\u81ea\u7f16\u7801\u5668\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5206\u7c7b\u9608\u503c\u7684\u975e\u6807\u51c6\u5316\u95ee\u9898\u76f4\u63a5\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u9608\u503c\u5b9a\u4e49\u65b9\u6cd5\u3002", "method": "\u8bc4\u4f30\u4e86K-Nearest Neighbors\u3001K-Means\u548cSupport Vector Machine\u4e09\u79cd\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5b9a\u4e49\u81ea\u7f16\u7801\u5668\u7684\u91cd\u6784\u8bef\u5dee\u5206\u7c7b\u9608\u503c\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u6307\u51fa\u901a\u8fc7\u7b97\u6cd5\u81ea\u52a8\u5b9a\u4e49\u9608\u503c\u53ef\u4ee5\u4f18\u5316\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u81ea\u52a8\u5b9a\u4e49\u9608\u503c\u4e3a\u81ea\u7f16\u7801\u5668\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u65b9\u6cd5\uff0c\u672a\u6765\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u7b97\u6cd5\u9009\u62e9\u3002"}}
{"id": "2506.14805", "pdf": "https://arxiv.org/pdf/2506.14805", "abs": "https://arxiv.org/abs/2506.14805", "authors": ["Yang Yao", "Lingyu Li", "Jiaxin Song", "Chiyu Chen", "Zhenqi He", "Yixu Wang", "Xin Wang", "Tianle Gu", "Jie Li", "Yan Teng", "Yingchun Wang"], "title": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "As Multimodal Large Language Models (MLLMs) continue to evolve, their cognitive and reasoning capabilities have seen remarkable progress. However, challenges in visual fine-grained perception and commonsense causal inference persist. This paper introduces Argus Inspection, a multimodal benchmark with two levels of difficulty, emphasizing detailed visual recognition while incorporating real-world commonsense understanding to evaluate causal reasoning abilities. Expanding on it, we present the Eye of Panoptes framework, which integrates a binary parametric Sigmoid metric with an indicator function, enabling a more holistic evaluation of MLLMs' responses in opinion-based reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the highest performance in visual fine-grained reasoning reaches only 0.46, highlighting considerable potential for enhancement. Our research offers valuable perspectives for the continued refinement of MLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u57fa\u51c6Argus Inspection\u548c\u6846\u67b6Eye of Panoptes\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u548c\u5e38\u8bc6\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u6a21\u578b\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u8ba4\u77e5\u548c\u63a8\u7406\u80fd\u529b\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u548c\u5e38\u8bc6\u56e0\u679c\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u5f15\u5165Argus Inspection\u591a\u6a21\u6001\u57fa\u51c6\u548cEye of Panoptes\u6846\u67b6\uff0c\u7ed3\u5408\u4e8c\u5143\u53c2\u6570\u5316Sigmoid\u6307\u6807\u548c\u6307\u793a\u51fd\u6570\uff0c\u5168\u9762\u8bc4\u4f30MLLMs\u5728\u610f\u89c1\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5bf926\u4e2a\u4e3b\u6d41MLLMs\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u89c6\u89c9\u7ec6\u7c92\u5ea6\u63a8\u7406\u7684\u6700\u9ad8\u6027\u80fd\u4ec5\u4e3a0.46\uff0c\u663e\u793a\u51fa\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u7814\u7a76\u4e3aMLLMs\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c6\u89d2\u3002"}}
{"id": "2506.15316", "pdf": "https://arxiv.org/pdf/2506.15316", "abs": "https://arxiv.org/abs/2506.15316", "authors": ["Benoit Tain", "Raphael Millet", "Romain Lemaire", "Michal Szczepanski", "Laurent Alacoque", "Emmanuel Pluchart", "Sylvain Choisnet", "Rohit Prasad", "Jerome Chossat", "Pascal Pierunek", "Pascal Vivet", "Sebastien Thuries"], "title": "J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor", "categories": ["cs.AR", "cs.AI"], "comment": "Preprint from ISLPED 2025. 979-8-3315-2710-5/25/$31.00 \\c{opyright}2025 IEEE", "summary": "This paper presents J3DAI, a tiny deep neural network-based hardware accelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial intelligence (AI) chip integrating a Deep Neural Network (DNN)-based accelerator. The DNN accelerator is designed to efficiently perform neural network tasks such as image classification and segmentation. This paper focuses on the digital system of J3DAI, highlighting its Performance-Power-Area (PPA) characteristics and showcasing advanced edge AI capabilities on a CMOS image sensor. To support hardware, we utilized the Aidge comprehensive software framework, which enables the programming of both the host processor and the DNN accelerator. Aidge supports post-training quantization, significantly reducing memory footprint and computational complexity, making it crucial for deploying models on resource-constrained hardware like J3DAI. Our experimental results demonstrate the versatility and efficiency of this innovative design in the field of edge AI, showcasing its potential to handle both simple and computationally intensive tasks. Future work will focus on further optimizing the architecture and exploring new applications to fully leverage the capabilities of J3DAI. As edge AI continues to grow in importance, innovations like J3DAI will play a crucial role in enabling real-time, low-latency, and energy-efficient AI processing at the edge.", "AI": {"tldr": "J3DAI\u662f\u4e00\u79cd\u57fa\u4e8e\u5fae\u5c0f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u4e13\u4e3a3D\u5806\u53e0CMOS\u56fe\u50cf\u4f20\u611f\u5668\u8bbe\u8ba1\uff0c\u5177\u5907\u9ad8\u6027\u80fd\u3001\u4f4e\u529f\u8017\u548c\u5c0f\u9762\u79ef\u7279\u70b9\uff0c\u7ed3\u5408Aidge\u8f6f\u4ef6\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u8fb9\u7f18AI\u5904\u7406\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u8fb9\u7f18AI\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u529f\u8017\u7684AI\u5904\u7406\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728CMOS\u56fe\u50cf\u4f20\u611f\u5668\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8bbe\u8ba1\u4e00\u4e2a\u57fa\u4e8eDNN\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5229\u7528Aidge\u8f6f\u4ef6\u6846\u67b6\u652f\u6301\u6a21\u578b\u90e8\u7f72\uff0c\u5e76\u91c7\u7528\u8bad\u7ec3\u540e\u91cf\u5316\u6280\u672f\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u5360\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aJ3DAI\u5728\u8fb9\u7f18AI\u9886\u57df\u8868\u73b0\u51fa\u9ad8\u6548\u548c\u591a\u529f\u80fd\u6027\uff0c\u80fd\u591f\u5904\u7406\u4ece\u7b80\u5355\u5230\u8ba1\u7b97\u5bc6\u96c6\u7684\u4efb\u52a1\u3002", "conclusion": "J3DAI\u5c55\u793a\u4e86\u5728\u8fb9\u7f18AI\u9886\u57df\u7684\u6f5c\u529b\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u4f18\u5316\u67b6\u6784\u5e76\u63a2\u7d22\u65b0\u5e94\u7528\uff0c\u4ee5\u9002\u5e94\u8fb9\u7f18AI\u7684\u5feb\u901f\u53d1\u5c55\u9700\u6c42\u3002"}}
{"id": "2506.14981", "pdf": "https://arxiv.org/pdf/2506.14981", "abs": "https://arxiv.org/abs/2506.14981", "authors": ["Hailiang Zhang", "Dieu My T. Nguyen", "Christine Smit", "Mahabal Hegde"], "title": "Zarr-Based Chunk-Level Cumulative Sums in Reduced Dimensions", "categories": ["cs.DC", "cs.DS"], "comment": null, "summary": "Data analysis on massive multi-dimensional data, such as high-resolution large-region time averaging or area averaging for geospatial data, often involves calculations over a significant number of data points. While performing calculations in scalable and flexible distributed or cloud environments is a viable option, a full scan of large data volumes still serves as a computationally intensive bottleneck, leading to significant cost. This paper introduces a generic and comprehensive method to address these computational challenges. This method generates a small, size-tunable supplementary dataset that stores the cumulative sums along specific subset dimensions on top of the raw data. This minor addition unlocks rapid and cheap high-resolution large-region data analysis, making calculations over large numbers of data points feasible with small instances or even microservices in the cloud. This method is general-purpose, but is particularly well-suited for data stored in chunked, cloud-optimized formats and for services running in distributed or cloud environments. We present a Zarr extension proposal to integrate the specifications of this method and facilitate its straightforward implementation in general-purpose software applications. Benchmark tests demonstrate that this method, implemented in Amazon Web services (AWS), significantly outperforms the brute-force approach used in on-premises services. With just 5% supplemental storage, this method achieves a performance that is 3-4 orders of magnitude (~10,000 times) faster than the brute-force approach, while incurring significantly reduced computational costs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u591a\u7ef4\u6570\u636e\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5c0f\u578b\u7684\u7d2f\u79ef\u548c\u8865\u5145\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u5904\u7406\u5927\u89c4\u6a21\u591a\u7ef4\u6570\u636e\u65f6\uff0c\u4f20\u7edf\u7684\u5168\u626b\u63cf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u8c03\u5927\u5c0f\u7684\u7d2f\u79ef\u548c\u8865\u5145\u6570\u636e\u96c6\uff0c\u4f18\u5316\u8ba1\u7b97\u8fc7\u7a0b\u3002", "result": "\u5728AWS\u4e0a\u5b9e\u73b0\u7684\u65b9\u6cd5\u6027\u80fd\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb3-4\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u4ec5\u97005%\u7684\u989d\u5916\u5b58\u50a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u5408\u4e91\u73af\u5883\u4e2d\u7684\u5206\u5757\u6570\u636e\u683c\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u7684\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2506.15312", "pdf": "https://arxiv.org/pdf/2506.15312", "abs": "https://arxiv.org/abs/2506.15312", "authors": ["Han Wu", "Junyao Li", "Kangbo Zhao", "Sen Zhang", "Yukai Shi", "Liang Lin"], "title": "One-shot Face Sketch Synthesis in the Wild via Generative Diffusion Prior and Instruction Tuning", "categories": ["cs.GR", "cs.CR", "cs.CV", "cs.CY"], "comment": "We propose a novel framework for face sketch synthesis, where merely a single pair of samples suffices to enable in-the-wild face sketch synthesis", "summary": "Face sketch synthesis is a technique aimed at converting face photos into sketches. Existing face sketch synthesis research mainly relies on training with numerous photo-sketch sample pairs from existing datasets. However, these large-scale discriminative learning methods will have to face problems such as data scarcity and high human labor costs. Once the training data becomes scarce, their generative performance significantly degrades. In this paper, we propose a one-shot face sketch synthesis method based on diffusion models. We optimize text instructions on a diffusion model using face photo-sketch image pairs. Then, the instructions derived through gradient-based optimization are used for inference. To simulate real-world scenarios more accurately and evaluate method effectiveness more comprehensively, we introduce a new benchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark consists of 400 pairs of face photo-sketch images, including sketches with different styles and photos with different backgrounds, ages, sexes, expressions, illumination, etc. For a solid out-of-distribution evaluation, we select only one pair of images for training at each time, with the rest used for inference. Extensive experiments demonstrate that the proposed method can convert various photos into realistic and highly consistent sketches in a one-shot context. Compared to other methods, our approach offers greater convenience and broader applicability. The dataset will be available at: https://github.com/HanWu3125/OS-Sketch", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u6b21\u4eba\u8138\u7d20\u63cf\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6587\u672c\u6307\u4ee4\u5b9e\u73b0\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6OS-Sketch\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5927\u89c4\u6a21\u5224\u522b\u5b66\u4e60\u65b9\u6cd5\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u4eba\u5de5\u6210\u672c\u9ad8\u5bfc\u81f4\u7684\u751f\u6210\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u6587\u672c\u6307\u4ee4\uff0c\u4f7f\u7528\u5355\u5bf9\u7167\u7247-\u7d20\u63cf\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u5728\u5355\u6b21\u4e0a\u4e0b\u6587\u4e2d\u5c06\u591a\u79cd\u7167\u7247\u8f6c\u6362\u4e3a\u903c\u771f\u4e14\u9ad8\u5ea6\u4e00\u81f4\u7684\u7d20\u63cf\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u4fbf\u6377\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4f9b\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2506.14799", "pdf": "https://arxiv.org/pdf/2506.14799", "abs": "https://arxiv.org/abs/2506.14799", "authors": ["Evdoxia Taka", "Debadyuti Bhattacharya", "Joanne Garde-Hansen", "Sanjay Sharma", "Tanaya Guha"], "title": "Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in AI has enabled automated analysis of complex media content at scale and generate actionable insights regarding character representation along such dimensions as gender and age. Past work focused on quantifying representation from audio/video/text using various ML models, but without having the audience in the loop. We ask, even if character distribution along demographic dimensions are available, how useful are they to the general public? Do they actually trust the numbers generated by AI models? Our work addresses these questions through a user study, while proposing a new AI-based character representation and visualization tool. Our tool based on the Contrastive Language Image Pretraining (CLIP) foundation model to analyze visual screen data to quantify character representation across dimensions of age and gender. We also designed effective visualizations suitable for presenting such analytics to lay audience. Next, we conducted a user study to seek empirical evidence on the usefulness and trustworthiness of the AI-generated results for carefully chosen movies presented in the form of our visualizations. We note that participants were able to understand the analytics from our visualization, and deemed the tool `overall useful'. Participants also indicated a need for more detailed visualizations to include more demographic categories and contextual information of the characters. Participants' trust in AI-based gender and age models is seen to be moderate to low, although they were not against the use of AI in this context. Our tool including code, benchmarking, and data from the user study can be found here: https://anonymous.4open.science/r/Character-Representation-Media-FF7B", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u6a21\u578b\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u548c\u53ef\u89c6\u5316\u5f71\u89c6\u5185\u5bb9\u4e2d\u7684\u89d2\u8272\u8868\u5f81\uff08\u5982\u5e74\u9f84\u548c\u6027\u522b\uff09\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u63a2\u8ba8\u4e86\u516c\u4f17\u5bf9AI\u751f\u6210\u7ed3\u679c\u7684\u4fe1\u4efb\u5ea6\u548c\u5de5\u5177\u7684\u6709\u7528\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u516c\u4f17\u5bf9AI\u751f\u6210\u7684\u6027\u522b\u548c\u5e74\u9f84\u8868\u5f81\u6570\u636e\u7684\u4fe1\u4efb\u5ea6\u548c\u5b9e\u7528\u6027\u7591\u95ee\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5206\u6790\u548c\u53ef\u89c6\u5316\u5de5\u5177\u3002", "method": "\u5229\u7528CLIP\u6a21\u578b\u5206\u6790\u89c6\u89c9\u6570\u636e\uff0c\u8bbe\u8ba1\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u5de5\u5177\u7684\u6709\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "result": "\u7528\u6237\u8ba4\u4e3a\u5de5\u5177\u603b\u4f53\u6709\u7528\uff0c\u4f46\u5bf9AI\u6a21\u578b\u7684\u4fe1\u4efb\u5ea6\u4e3a\u4e2d\u4f4e\u6c34\u5e73\uff0c\u540c\u65f6\u5e0c\u671b\u589e\u52a0\u66f4\u591a\u4eba\u53e3\u7edf\u8ba1\u7c7b\u522b\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "conclusion": "\u5de5\u5177\u5728\u89d2\u8272\u8868\u5f81\u5206\u6790\u4e0a\u6709\u6548\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u4fe1\u4efb\u5ea6\u548c\u529f\u80fd\u591a\u6837\u6027\u3002"}}
{"id": "2506.15227", "pdf": "https://arxiv.org/pdf/2506.15227", "abs": "https://arxiv.org/abs/2506.15227", "authors": ["Quanjun Zhang", "Chunrong Fang", "Siqi Gu", "Ye Shang", "Zhenyu Chen", "Liang Xiao"], "title": "Large Language Models for Unit Testing: A Systematic Literature Review", "categories": ["cs.SE"], "comment": null, "summary": "Unit testing is a fundamental practice in modern software engineering, with the aim of ensuring the correctness, maintainability, and reliability of individual software components. Very recently, with the advances in Large Language Models (LLMs), a rapidly growing body of research has leveraged LLMs to automate various unit testing tasks, demonstrating remarkable performance and significantly reducing manual effort. However, due to ongoing explorations in the LLM-based unit testing field, it is challenging for researchers to understand existing achievements, open challenges, and future opportunities. This paper presents the first systematic literature review on the application of LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant papers from the perspectives of both unit testing and LLMs. We first categorize existing unit testing tasks that benefit from LLMs, e.g., test generation and oracle generation. We then discuss several critical aspects of integrating LLMs into unit testing research, including model usage, adaptation strategies, and hybrid approaches. We further summarize key challenges that remain unresolved and outline promising directions to guide future research in this area. Overall, our paper provides a systematic overview of the research landscape to the unit testing community, helping researchers gain a comprehensive understanding of achievements and promote future research. Our artifacts are publicly available at the GitHub repository: https://github.com/iSEngLab/AwesomeLLM4UT.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5355\u5143\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u6210\u679c\u3001\u5f00\u653e\u6311\u6218\u53ca\u672a\u6765\u673a\u9047\u3002", "motivation": "\u968f\u7740LLMs\u5728\u5355\u5143\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7814\u7a76\u8005\u9700\u8981\u4e86\u89e3\u73b0\u6709\u6210\u5c31\u3001\u6311\u6218\u53ca\u673a\u4f1a\uff0c\u4ee5\u63a8\u52a8\u672a\u6765\u7814\u7a76\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u76f8\u5173\u8bba\u6587\uff0c\u4ece\u5355\u5143\u6d4b\u8bd5\u548cLLMs\u7684\u89d2\u5ea6\u5bf9\u4efb\u52a1\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8ba8\u8bba\u4e86LLMs\u5728\u5355\u5143\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u7b56\u7565\u3002", "result": "\u603b\u7ed3\u4e86LLMs\u5728\u5355\u5143\u6d4b\u8bd5\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u6f5c\u5728\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6982\u8ff0\u3002", "conclusion": "\u672c\u6587\u4e3a\u5355\u5143\u6d4b\u8bd5\u793e\u533a\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u7814\u7a76\u6982\u51b5\uff0c\u65e8\u5728\u63a8\u52a8\u672a\u6765\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.14824", "pdf": "https://arxiv.org/pdf/2506.14824", "abs": "https://arxiv.org/abs/2506.14824", "authors": ["Yao Zhang", "Hewei Gao", "Haokun Chen", "Weiguo Li", "Yunpu Ma", "Volker Tresp"], "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.MM"], "comment": "12 pages, 3 figures", "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal reasoning and cross-modal retrieval but face deployment challenges in real-world scenarios due to distributed multimodal data and strict privacy requirements. Federated Learning (FL) offers a solution by enabling collaborative model training without centralizing data. However, realizing FL for MLLMs presents significant challenges, including high computational demands, limited client capacity, substantial communication costs, and heterogeneous client data. Existing FL methods assume client-side deployment of full models, an assumption that breaks down for large-scale MLLMs due to their massive size and communication demands. To address these limitations, we propose FedNano, the first FL framework that centralizes the LLM on the server while introducing NanoEdge, a lightweight module for client-specific adaptation. NanoEdge employs modality-specific encoders, connectors, and trainable NanoAdapters with low-rank adaptation. This design eliminates the need to deploy LLM on clients, reducing client-side storage by 95%, and limiting communication overhead to only 0.01% of the model parameters. By transmitting only compact NanoAdapter updates, FedNano handles heterogeneous client data and resource constraints while preserving privacy. Experiments demonstrate that FedNano outperforms prior FL baselines, bridging the gap between MLLM scale and FL feasibility, and enabling scalable, decentralized multimodal AI systems.", "AI": {"tldr": "FedNano\u662f\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u5f0f\u573a\u666f\u4e0b\u7684\u90e8\u7f72\u95ee\u9898\uff0c\u901a\u8fc7\u4e2d\u592e\u670d\u52a1\u5668\u548c\u8f7b\u91cf\u7ea7\u5ba2\u6237\u7aef\u6a21\u5757NanoEdge\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b58\u50a8\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u5206\u5e03\u5f0f\u6570\u636e\u5b58\u50a8\u548c\u9690\u79c1\u4fdd\u62a4\u7b49\u6311\u6218\uff0c\u800c\u4f20\u7edf\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u3002", "method": "\u63d0\u51faFedNano\u6846\u67b6\uff0c\u4e2d\u592e\u670d\u52a1\u5668\u6258\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5ba2\u6237\u7aef\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u5757NanoEdge\uff0c\u7ed3\u5408\u4f4e\u79e9\u9002\u914d\u5668NanoAdapters\uff0c\u51cf\u5c11\u5b58\u50a8\u548c\u901a\u4fe1\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cFedNano\u5c06\u5ba2\u6237\u7aef\u5b58\u50a8\u51cf\u5c1195%\uff0c\u901a\u4fe1\u5f00\u9500\u4ec5\u4e3a\u6a21\u578b\u53c2\u6570\u76840.01%\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "FedNano\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5927\u89c4\u6a21\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6a21\u578b\u89c4\u6a21\u4e0e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2506.15440", "pdf": "https://arxiv.org/pdf/2506.15440", "abs": "https://arxiv.org/abs/2506.15440", "authors": ["Omar Numan", "Gaurav Singh", "Kazybek Adam", "Jelin Leslin", "Aleksi Korsman", "Otto Simola", "Marko Kosunen", "Jussi Ryyn\u00e4nen", "Martin Andraud"], "title": "Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration", "categories": ["cs.AR"], "comment": "This work has been submitted to the IEEE for possible publication. 12 pages, 10 figures, 2 tables", "summary": "Developing accurate and reliable Compute-In-Memory (CIM) architectures is becoming a key research focus to accelerate Artificial Intelligence (AI) tasks on hardware, particularly Deep Neural Networks (DNNs). In that regard, there has been significant interest in analog and mixed-signal CIM architectures aimed at increasing the efficiency of data storage and computation to handle the massive amount of data needed by DNNs. Specifically, resistive mixed-signal CIM cores are pushed by recent progresses in emerging Non-Volatile Memory (eNVM) solutions. Yet, mixed-signal CIM computing cores still face several integration and reliability challenges that hinder their large-scale adoption into end-to-end AI computing systems. In terms of integration, resistive and eNVM-based CIM cores need to be integrated with a control processor to realize end-to-end AI acceleration. Moreover, SRAM-based CIM architectures are still more efficient and easier to program than their eNVM counterparts. In terms of reliability, analog circuits are more susceptible to variations, leading to computation errors and degraded accuracy. This work addresses these two challenges by proposing a self-calibrated mixed-signal CIM accelerator SoC, fabricated in 22-nm FDSOI technology. The integration is facilitated by (1) the CIM architecture, combining the density and ease of SRAM-based weight storage with multi-bit computation using linear resistors, and (2) an open-source programming and testing strategy for CIM systems. The accuracy and reliability are enabled through an automated RISC-V controlled on-chip calibration, allowing us to improve the compute SNR by 25 to 45% across multiple columns to reach 18-24 dB. To showcase further integration possibilities, we show how our proof-of-concept SoC can be extended to recent high-density linear resistor technologies for enhanced computing performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6821\u51c6\u6df7\u5408\u4fe1\u53f7\u8ba1\u7b97\u5185\u5b58\uff08CIM\uff09\u52a0\u901f\u5668SoC\uff0c\u89e3\u51b3\u7535\u963b\u548c\u975e\u6613\u5931\u6027\u5b58\u50a8\u5668\uff08eNVM\uff09CIM\u6838\u5fc3\u7684\u96c6\u6210\u4e0e\u53ef\u9760\u6027\u6311\u6218\u3002", "motivation": "\u4e3a\u52a0\u901fAI\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8ba1\u7b97\u5185\u5b58\u67b6\u6784\uff0c\u4f46\u73b0\u6709\u6df7\u5408\u4fe1\u53f7CIM\u6838\u5fc3\u9762\u4e34\u96c6\u6210\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u91c7\u752822\u7eb3\u7c73FDSOI\u6280\u672f\uff0c\u7ed3\u5408SRAM\u7684\u5b58\u50a8\u4f18\u52bf\u4e0e\u7ebf\u6027\u7535\u963b\u7684\u591a\u4f4d\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7RISC-V\u63a7\u5236\u82af\u7247\u6821\u51c6\u63d0\u5347\u53ef\u9760\u6027\u3002", "result": "\u8ba1\u7b97\u4fe1\u566a\u6bd4\uff08SNR\uff09\u63d0\u9ad8\u4e8625%\u523045%\uff0c\u8fbe\u523018-24 dB\u3002", "conclusion": "\u8be5CIM\u67b6\u6784\u4e3a\u9ad8\u6027\u80fdAI\u52a0\u901f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u9ad8\u5bc6\u5ea6\u7ebf\u6027\u7535\u963b\u6280\u672f\u3002"}}
{"id": "2506.15028", "pdf": "https://arxiv.org/pdf/2506.15028", "abs": "https://arxiv.org/abs/2506.15028", "authors": ["Gargi Mitra", "Mohammadreza Hallajiyan", "Inji Kim", "Athish Pranav Dharmalingam", "Mohammed Elnawawy", "Shahrear Iqbal", "Karthik Pattabiraman", "Homa Alemzadeh"], "title": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices", "categories": ["cs.CR", "cs.ET", "cs.LG"], "comment": "32 pages, 6 figures, 6 tables", "summary": "The integration of AI/ML into medical devices is rapidly transforming healthcare by enhancing diagnostic and treatment facilities. However, this advancement also introduces serious cybersecurity risks due to the use of complex and often opaque models, extensive interconnectivity, interoperability with third-party peripheral devices, Internet connectivity, and vulnerabilities in the underlying technologies. These factors contribute to a broad attack surface and make threat prevention, detection, and mitigation challenging. Given the highly safety-critical nature of these devices, a cyberattack on these devices can cause the ML models to mispredict, thereby posing significant safety risks to patients. Therefore, ensuring the security of these devices from the time of design is essential. This paper underscores the urgency of addressing the cybersecurity challenges in ML-enabled medical devices at the pre-market phase. We begin by analyzing publicly available data on device recalls and adverse events, and known vulnerabilities, to understand the threat landscape of AI/ML-enabled medical devices and their repercussions on patient safety. Building on this analysis, we introduce a suite of tools and techniques designed by us to assist security analysts in conducting comprehensive premarket risk assessments. Our work aims to empower manufacturers to embed cybersecurity as a core design principle in AI/ML-enabled medical devices, thereby making them safe for patients.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86AI/ML\u533b\u7597\u8bbe\u5907\u7684\u7f51\u7edc\u5b89\u5168\u98ce\u9669\u53ca\u5176\u5bf9\u60a3\u8005\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u5de5\u5177\u548c\u6280\u672f\u4ee5\u652f\u6301\u4e0a\u5e02\u524d\u7684\u98ce\u9669\u8bc4\u4f30\u3002", "motivation": "\u7531\u4e8eAI/ML\u533b\u7597\u8bbe\u5907\u7684\u590d\u6742\u6027\u548c\u4e92\u8054\u6027\uff0c\u7f51\u7edc\u5b89\u5168\u95ee\u9898\u4e25\u91cd\u5a01\u80c1\u60a3\u8005\u5b89\u5168\uff0c\u9700\u4ece\u8bbe\u8ba1\u9636\u6bb5\u4fdd\u969c\u5176\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bbe\u5907\u53ec\u56de\u3001\u4e0d\u826f\u4e8b\u4ef6\u548c\u5df2\u77e5\u6f0f\u6d1e\u6570\u636e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u5957\u5de5\u5177\u5e2e\u52a9\u5b89\u5168\u5206\u6790\u5e08\u8fdb\u884c\u4e0a\u5e02\u524d\u98ce\u9669\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684\u5de5\u5177\u6709\u52a9\u4e8e\u5236\u9020\u5546\u5728\u8bbe\u8ba1\u9636\u6bb5\u5d4c\u5165\u7f51\u7edc\u5b89\u5168\u539f\u5219\uff0c\u63d0\u5347\u8bbe\u5907\u5b89\u5168\u6027\u3002", "conclusion": "\u5f3a\u8c03\u5728AI/ML\u533b\u7597\u8bbe\u5907\u4e0a\u5e02\u524d\u89e3\u51b3\u7f51\u7edc\u5b89\u5168\u95ee\u9898\u7684\u7d27\u8feb\u6027\uff0c\u4ee5\u786e\u4fdd\u60a3\u8005\u5b89\u5168\u3002"}}
{"id": "2506.15114", "pdf": "https://arxiv.org/pdf/2506.15114", "abs": "https://arxiv.org/abs/2506.15114", "authors": ["Youjia Li", "Robert Latham", "Robert Ross", "Ankit Agrawal", "Alok Choudhary", "Wei-Keng Liao"], "title": "Parallel Data Object Creation: Towards Scalable Metadata Management in High-Performance I/O Library", "categories": ["cs.DC"], "comment": null, "summary": "High-level I/O libraries, such as HDF5 and PnetCDF, are commonly used by large-scale scientific applications to perform I/O tasks in parallel. These I/O libraries store the metadata such as data types and dimensionality along with the raw data in the same files. While these libraries are well-optimized for concurrent access to the raw data, they are designed neither to handle a large number of data objects efficiently nor to create different data objects independently by multiple processes, as they require applications to call data object creation APIs collectively with consistent metadata among all processes. Applications that process data gathered from remote sensors, such as particle collision experiments in high-energy physics, may generate data of different sizes from different sensors and desire to store them as separate data objects. For such applications, the I/O library's requirement on collective data object creation can become very expensive, as the cost of metadata consistency check increases with the metadata volume as well as the number of processes. To address this limitation, using PnetCDF as an experimental platform, we investigate solutions in this paper that abide the netCDF file format, as well as propose a new file header format that enables independent data object creation. The proposed file header consists of two sections, an index table and a list of metadata blocks. The index table contains the reference to the metadata blocks and each block stores metadata of objects that can be created collectively or independently. The new design achieves a scalable performance, cutting data object creation times by up to 582x when running on 4096 MPI processes to create 5,684,800 data objects in parallel. Additionally, the new method reduces the memory footprints, with each process requiring an amount of memory space inversely proportional to the number of processes.", "AI": {"tldr": "\u9ad8\u5c42\u6b21\u7684I/O\u5e93\uff08\u5982HDF5\u548cPnetCDF\uff09\u5728\u5927\u89c4\u6a21\u79d1\u5b66\u5e94\u7528\u4e2d\u7528\u4e8e\u5e76\u884cI/O\u4efb\u52a1\uff0c\u4f46\u5bf9\u591a\u8fdb\u7a0b\u72ec\u7acb\u521b\u5efa\u5927\u91cf\u6570\u636e\u5bf9\u8c61\u7684\u6548\u7387\u4e0d\u9ad8\u3002\u672c\u6587\u57fa\u4e8ePnetCDF\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u4ef6\u5934\u90e8\u683c\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9ad8\u5c42\u6b21\u7684I/O\u5e93\u5728\u591a\u8fdb\u7a0b\u72ec\u7acb\u521b\u5efa\u5927\u91cf\u6570\u636e\u5bf9\u8c61\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u662f\u9700\u8981\u4fdd\u8bc1\u5143\u6570\u636e\u4e00\u81f4\u6027\u7684\u573a\u666f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u4ef6\u5934\u90e8\u683c\u5f0f\uff0c\u5305\u542b\u7d22\u5f15\u8868\u548c\u5143\u6570\u636e\u5757\u5217\u8868\uff0c\u652f\u6301\u72ec\u7acb\u6216\u96c6\u4f53\u521b\u5efa\u6570\u636e\u5bf9\u8c61\u3002", "result": "\u65b0\u8bbe\u8ba1\u57284096\u4e2aMPI\u8fdb\u7a0b\u4e0b\u5e76\u884c\u521b\u5efa5,684,800\u4e2a\u6570\u636e\u5bf9\u8c61\u65f6\uff0c\u6027\u80fd\u63d0\u5347\u8fbe582\u500d\uff0c\u5e76\u51cf\u5c11\u4e86\u5185\u5b58\u5360\u7528\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u8fdb\u7a0b\u72ec\u7acb\u521b\u5efa\u6570\u636e\u5bf9\u8c61\u7684\u6027\u80fd\uff0c\u4e14\u5185\u5b58\u5360\u7528\u66f4\u4f4e\u3002"}}
{"id": "2506.15684", "pdf": "https://arxiv.org/pdf/2506.15684", "abs": "https://arxiv.org/abs/2506.15684", "authors": ["Qingming Liu", "Zhen Liu", "Dinghuai Zhang", "Kui Jia"], "title": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Technical Report (21 pages, 21 figures)", "summary": "Generating high-quality and photorealistic 3D assets remains a longstanding challenge in 3D vision and computer graphics. Although state-of-the-art generative models, such as diffusion models, have made significant progress in 3D generation, they often fall short of human-designed content due to limited ability to follow instructions, align with human preferences, or produce realistic textures, geometries, and physical attributes. In this paper, we introduce Nabla-R2D3, a highly effective and sample-efficient reinforcement learning alignment framework for 3D-native diffusion models using 2D rewards. Built upon the recently proposed Nabla-GFlowNet method, which matches the score function to reward gradients in a principled manner for reward finetuning, our Nabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D reward signals. Extensive experiments show that, unlike vanilla finetuning baselines which either struggle to converge or suffer from reward hacking, Nabla-R2D3 consistently achieves higher rewards and reduced prior forgetting within a few finetuning steps.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNabla-R2D3\u7684\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb3D\u6269\u6563\u6a21\u578b\u7684\u8d28\u91cf\u548c\u6837\u672c\u6548\u7387\uff0c\u901a\u8fc72D\u5956\u52b1\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u6548\u8c03\u6574\u3002", "motivation": "\u5f53\u524d3D\u751f\u6210\u6a21\u578b\u5728\u9075\u5faa\u6307\u4ee4\u3001\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u53ca\u751f\u6210\u903c\u771f\u7eb9\u7406\u3001\u51e0\u4f55\u548c\u7269\u7406\u5c5e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u57fa\u4e8eNabla-GFlowNet\u65b9\u6cd5\uff0c\u63d0\u51faNabla-R2D3\u6846\u67b6\uff0c\u5229\u75282D\u5956\u52b1\u4fe1\u53f7\u5bf93D\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNabla-R2D3\u5728\u5c11\u91cf\u8c03\u6574\u6b65\u9aa4\u4e2d\u80fd\u7a33\u5b9a\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\u5e76\u51cf\u5c11\u5148\u9a8c\u9057\u5fd8\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Nabla-R2D3\u4e3a3D\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.14809", "pdf": "https://arxiv.org/pdf/2506.14809", "abs": "https://arxiv.org/abs/2506.14809", "authors": ["Peng Jiang", "Vinicius Cezar Monteiro de Lira", "Antonio Maiorino"], "title": "Impact of a Deployed LLM Survey Creation Tool through the IS Success Model", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Surveys are a cornerstone of Information Systems (IS) research, yet creating high-quality surveys remains labor-intensive, requiring both domain expertise and methodological rigor. With the evolution of large language models (LLMs), new opportunities emerge to automate survey generation. This paper presents the real-world deployment of an LLM-powered system designed to accelerate data collection while maintaining survey quality. Deploying such systems in production introduces real-world complexity, including diverse user needs and quality control. We evaluate the system using the DeLone and McLean IS Success Model to understand how generative AI can reshape a core IS method. This study makes three key contributions. To our knowledge, this is the first application of the IS Success Model to a generative AI system for survey creation. In addition, we propose a hybrid evaluation framework combining automated and human assessments. Finally, we implement safeguards that mitigate post-deployment risks and support responsible integration into IS workflows.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u8c03\u67e5\u95ee\u5377\u7684\u5b9e\u8df5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u90e8\u7f72\u8bc4\u4f30\u5176\u5bf9\u4fe1\u606f\u7cfb\u7edf\u7684\u6838\u5fc3\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u4eba\u5de5\u521b\u5efa\u9ad8\u8d28\u91cf\u8c03\u67e5\u95ee\u5377\u9700\u8981\u5927\u91cf\u65f6\u95f4\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0cLLMs\u7684\u53d1\u5c55\u4e3a\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u91c7\u7528DeLone\u548cMcLean\u7684\u4fe1\u606f\u7cfb\u7edf\u6210\u529f\u6a21\u578b\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u8c03\u67e5\u751f\u6210\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\u548c\u5b89\u5168\u63aa\u65bd\u3002", "result": "\u9996\u6b21\u5c06\u4fe1\u606f\u7cfb\u7edf\u6210\u529f\u6a21\u578b\u5e94\u7528\u4e8e\u751f\u6210AI\u8c03\u67e5\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u8bc4\u4f30\u7684\u6df7\u5408\u6846\u67b6\u3002", "conclusion": "\u751f\u6210AI\u7cfb\u7edf\u80fd\u591f\u52a0\u901f\u6570\u636e\u6536\u96c6\u5e76\u4fdd\u6301\u8c03\u67e5\u8d28\u91cf\uff0c\u4f46\u9700\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\u548c\u5b89\u5168\u63aa\u65bd\u4ee5\u786e\u4fdd\u6709\u6548\u96c6\u6210\u3002"}}
{"id": "2506.15453", "pdf": "https://arxiv.org/pdf/2506.15453", "abs": "https://arxiv.org/abs/2506.15453", "authors": ["Yusuf Sulistyo Nugroho", "Farah Danisha Salam", "Brittany Reid", "Raula Gaikovina Kula", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Uncovering Intention through LLM-Driven Code Snippet Description Generation", "categories": ["cs.SE", "cs.AI"], "comment": "6 pages, 3 figures, 4 tables, conference paper", "summary": "Documenting code snippets is essential to pinpoint key areas where both developers and users should pay attention. Examples include usage examples and other Application Programming Interfaces (APIs), which are especially important for third-party libraries. With the rise of Large Language Models (LLMs), the key goal is to investigate the kinds of description developers commonly use and evaluate how well an LLM, in this case Llama, can support description generation. We use NPM Code Snippets, consisting of 185,412 packages with 1,024,579 code snippets. From there, we use 400 code snippets (and their descriptions) as samples. First, our manual classification found that the majority of original descriptions (55.5%) highlight example-based usage. This finding emphasizes the importance of clear documentation, as some descriptions lacked sufficient detail to convey intent. Second, the LLM correctly identified the majority of original descriptions as \"Example\" (79.75%), which is identical to our manual finding, showing a propensity for generalization. Third, compared to the originals, the produced description had an average similarity score of 0.7173, suggesting relevance but room for improvement. Scores below 0.9 indicate some irrelevance. Our results show that depending on the task of the code snippet, the intention of the document may differ from being instructions for usage, installations, or descriptive learning examples for any user of a library.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee3\u7801\u7247\u6bb5\u7684\u6587\u6863\u63cf\u8ff0\u7c7b\u578b\u53ca\u5176\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama\uff09\u751f\u6210\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5927\u591a\u6570\u63cf\u8ff0\u57fa\u4e8e\u793a\u4f8b\u4f7f\u7528\uff0c\u4e14LLM\u5728\u8bc6\u522b\u548c\u751f\u6210\u76f8\u5173\u63cf\u8ff0\u65f6\u8868\u73b0\u826f\u597d\u4f46\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u5f00\u53d1\u8005\u5e38\u7528\u7684\u4ee3\u7801\u7247\u6bb5\u63cf\u8ff0\u7c7b\u578b\uff0c\u5e76\u8bc4\u4f30LLM\uff08\u5982Llama\uff09\u5728\u751f\u6210\u8fd9\u4e9b\u63cf\u8ff0\u65f6\u7684\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u66f4\u6e05\u6670\u7684\u6587\u6863\u5316\u9700\u6c42\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86NPM\u4ee3\u7801\u7247\u6bb5\u5e93\u4e2d\u7684185,412\u4e2a\u5305\u548c1,024,579\u4e2a\u4ee3\u7801\u7247\u6bb5\uff0c\u4ece\u4e2d\u9009\u53d6400\u4e2a\u6837\u672c\u8fdb\u884c\u624b\u52a8\u5206\u7c7b\u548cLLM\u5206\u6790\u3002", "result": "\u624b\u52a8\u5206\u7c7b\u663e\u793a55.5%\u7684\u63cf\u8ff0\u4e3a\u793a\u4f8b\u4f7f\u7528\uff1bLLM\u6b63\u786e\u8bc6\u522b\u51fa79.75%\u7684\u63cf\u8ff0\u4e3a\u201c\u793a\u4f8b\u201d\uff0c\u751f\u6210\u63cf\u8ff0\u7684\u5e73\u5747\u76f8\u4f3c\u5ea6\u4e3a0.7173\uff0c\u8868\u660e\u76f8\u5173\u4f46\u9700\u6539\u8fdb\u3002", "conclusion": "\u4ee3\u7801\u7247\u6bb5\u7684\u6587\u6863\u610f\u56fe\u56e0\u4efb\u52a1\u800c\u5f02\uff0cLLM\u5728\u63cf\u8ff0\u751f\u6210\u65b9\u9762\u8868\u73b0\u826f\u597d\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u7ec6\u8282\u8868\u8fbe\u548c\u76f8\u5173\u6027\u63d0\u5347\u3002"}}
{"id": "2506.15154", "pdf": "https://arxiv.org/pdf/2506.15154", "abs": "https://arxiv.org/abs/2506.15154", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "comment": "14 pages, 2 figures, Accepted to AIMC 2025", "summary": "Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions.", "AI": {"tldr": "SonicVerse\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u97f3\u4e50\u63cf\u8ff0\u6a21\u578b\uff0c\u901a\u8fc7\u8f85\u52a9\u97f3\u4e50\u7279\u5f81\u68c0\u6d4b\u4efb\u52a1\u63d0\u5347\u97f3\u4e50\u63cf\u8ff0\u7684\u4e30\u5bcc\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e30\u5bcc\u97f3\u4e50\u6570\u636e\u5e93\u5e76\u63a8\u52a8\u97f3\u4e50AI\u7814\u7a76\uff0c\u9700\u8981\u51c6\u786e\u53cd\u6620\u97f3\u4e50\u7279\u5f81\u7684\u8be6\u7ec6\u63cf\u8ff0\u3002", "method": "\u63d0\u51fa\u6295\u5f71\u5f0f\u67b6\u6784\uff0c\u5c06\u97f3\u9891\u8f93\u5165\u8f6c\u6362\u4e3a\u8bed\u8a00\u6807\u8bb0\uff0c\u5e76\u901a\u8fc7\u8f85\u52a9\u4efb\u52a1\u68c0\u6d4b\u97f3\u4e50\u7279\u5f81\uff0c\u589e\u5f3a\u63cf\u8ff0\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6574\u5408\u97f3\u4e50\u7279\u5f81\u63d0\u5347\u4e86\u751f\u6210\u63cf\u8ff0\u7684\u8d28\u91cf\u548c\u7ec6\u8282\u3002", "conclusion": "SonicVerse\u80fd\u751f\u6210\u4e30\u5bcc\u4e14\u65f6\u95f4\u611f\u77e5\u7684\u97f3\u4e50\u63cf\u8ff0\uff0c\u4e3a\u97f3\u4e50AI\u7814\u7a76\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.15601", "pdf": "https://arxiv.org/pdf/2506.15601", "abs": "https://arxiv.org/abs/2506.15601", "authors": ["Donghyun Gouk", "Seungkwan Kang", "Seungjun Lee", "Jiseon Kim", "Kyungkuk Nam", "Eojin Ryu", "Sangwon Lee", "Dongpyung Kim", "Junhyeok Jang", "Hanyeoreum Bae", "Myoungsoo Jung"], "title": "CXL-GPU: Pushing GPU Memory Boundaries with the Integration of CXL Technologies", "categories": ["cs.AR"], "comment": null, "summary": "This work introduces a GPU storage expansion solution utilizing CXL, featuring a novel GPU system design with multiple CXL root ports for integrating diverse storage media (DRAMs and/or SSDs). We developed and siliconized a custom CXL controller integrated at the hardware RTL level, achieving two-digit nanosecond roundtrip latency, the first in the field. This study also includes speculative read and deterministic store mechanisms to efficiently manage read and write operations to hide the endpoint's backend media latency variation. Performance evaluations reveal our approach significantly outperforms existing methods, marking a substantial advancement in GPU storage technology.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528CXL\u7684GPU\u5b58\u50a8\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u51fa\u4e86\u591aCXL\u6839\u7aef\u53e3\u7684GPU\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u9996\u6b21\u5b9e\u73b0\u4e24\u4f4d\u7eb3\u79d2\u7ea7\u5f80\u8fd4\u5ef6\u8fdf\uff0c\u5e76\u901a\u8fc7\u8bfb\u9884\u6d4b\u548c\u786e\u5b9a\u6027\u5b58\u50a8\u673a\u5236\u4f18\u5316\u8bfb\u5199\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3GPU\u5b58\u50a8\u6269\u5c55\u7684\u9700\u6c42\uff0c\u5e76\u514b\u670d\u73b0\u6709\u6280\u672f\u4e2d\u6027\u80fd\u548c\u5ef6\u8fdf\u7684\u5c40\u9650\u6027\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8eCXL\u7684\u521b\u65b0\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u591aCXL\u6839\u7aef\u53e3\u7684GPU\u7cfb\u7edf\uff0c\u5f00\u53d1\u4e86\u786c\u4ef6RTL\u7ea7\u522b\u7684CXL\u63a7\u5236\u5668\uff0c\u5e76\u91c7\u7528\u4e86\u8bfb\u9884\u6d4b\u548c\u786e\u5b9a\u6027\u5b58\u50a8\u673a\u5236\u3002", "result": "\u6027\u80fd\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4e24\u4f4d\u7eb3\u79d2\u7ea7\u7684\u5f80\u8fd4\u5ef6\u8fdf\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3aGPU\u5b58\u50a8\u6280\u672f\u5e26\u6765\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5c55\u793a\u4e86CXL\u5728GPU\u5b58\u50a8\u6269\u5c55\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.15070", "pdf": "https://arxiv.org/pdf/2506.15070", "abs": "https://arxiv.org/abs/2506.15070", "authors": ["Rasha Karakchi", "Rye Stahle-Smith", "Nishant Chinnasami", "Tiffany Yu"], "title": "Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine", "categories": ["cs.CR", "cs.ET"], "comment": "This is submitted to the ACM/IEEE Symposium on Edge Computing (SEC 2025)", "summary": "The exponential growth of Internet of Things (IoT) applications has intensified the demand for efficient, high-throughput, and energy-efficient data processing at the edge. Conventional CPU-centric encryption methods suffer from performance bottlenecks and excessive data movement, especially in latency-sensitive and resource-constrained environments. In this paper, we present SPiME, a lightweight, scalable, and FPGA-compatible Secure Processor-in-Memory Encryption architecture that integrates the Advanced Encryption Standard (AES-128) directly into a Processing-in-Memory (PiM) framework. SPiME is designed as a modular array of parallel PiM units, each combining an AES core with a minimal control unit to enable distributed in-place encryption with minimal overhead. The architecture is fully implemented in Verilog and tested on multiple AMD UltraScale and UltraScale+ FPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units while maintaining less than 5\\% utilization of key FPGA resources on high-end devices. It delivers over 25~Gbps in sustained encryption throughput with predictable, low-latency performance. The design's portability, configurability, and resource efficiency make it a compelling solution for secure edge computing, embedded cryptographic systems, and customizable hardware accelerators.", "AI": {"tldr": "SPiME\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u4e14\u4e0eFPGA\u517c\u5bb9\u7684\u5b89\u5168\u5185\u5b58\u5904\u7406\u5668\u52a0\u5bc6\u67b6\u6784\uff0c\u96c6\u6210AES-128\u81f3PiM\u6846\u67b6\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u52a0\u5bc6\uff0c\u9ad8\u6548\u4e14\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u7269\u8054\u7f51\u5e94\u7528\u9700\u6c42\u589e\u957f\uff0c\u4f20\u7edfCPU\u52a0\u5bc6\u65b9\u6cd5\u5728\u9ad8\u541e\u5410\u548c\u4f4e\u5ef6\u8fdf\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSPiME\u67b6\u6784\uff0c\u5c06AES-128\u76f4\u63a5\u96c6\u6210\u5230PiM\u6846\u67b6\u4e2d\uff0c\u4ee5\u5e76\u884cPiM\u5355\u5143\u9635\u5217\u5b9e\u73b0\u5206\u5e03\u5f0f\u52a0\u5bc6\u3002", "result": "\u5728\u9ad8\u7aefFPGA\u4e0a\u652f\u63014000+\u5e76\u884c\u5355\u5143\uff0c\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u81f35%\uff0c\u52a0\u5bc6\u541e\u5410\u91cf\u8fbe25Gbps\u3002", "conclusion": "SPiME\u5728\u5b89\u5168\u8fb9\u7f18\u8ba1\u7b97\u548c\u5d4c\u5165\u5f0f\u52a0\u5bc6\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u9ad8\u6548\u3001\u53ef\u914d\u7f6e\u548c\u53ef\u79fb\u690d\u7279\u6027\u3002"}}
{"id": "2506.15155", "pdf": "https://arxiv.org/pdf/2506.15155", "abs": "https://arxiv.org/abs/2506.15155", "authors": ["Jiale Xu", "Rui Zhang", "Yi Xiong", "Cong Guo", "Zihan Liu", "Yangjie Zhou", "Weiming Hu", "Hao Wu", "Changxu Shao", "Ziqing Wang", "Yongjie Yuan", "Junping Zhao", "Minyi Guo", "Jingwen Leng"], "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving", "categories": ["cs.DC"], "comment": null, "summary": "Large Language Models are increasingly being deployed in datacenters. Serving these models requires careful memory management, as their memory usage includes static weights, dynamic activations, and key-value caches. While static weights are constant and predictable, dynamic components such as activations and KV caches change frequently during runtime, presenting significant challenges for efficient memory management. Modern LLM serving systems typically handle runtime memory and KV caches at distinct abstraction levels: runtime memory management relies on static tensor abstractions, whereas KV caches utilize a page table-based virtualization layer built on top of the tensor abstraction. This virtualization dynamically manages KV caches to mitigate memory fragmentation. However, this dual-level approach fundamentally isolates runtime memory and KV cache management, resulting in suboptimal memory utilization under dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management framework inspired by the classical memory ballooning mechanism in operating systems. The core components of eLLM include: (1) Virtual Tensor Abstraction, which decouples the virtual address space of tensors from the physical GPU memory, creating a unified and flexible memory pool; (2) an Elastic Memory Mechanism that dynamically adjusts memory allocation through runtime memory inflation and deflation, leveraging CPU memory as an extensible buffer; and (3) a Lightweight Scheduling Strategy employing SLO-aware policies to optimize memory utilization and effectively balance performance trade-offs under stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM significantly outperforms state-of-the-art systems, 2.32x higher decoding throughput, and supporting 3x larger batch sizes for 128K-token inputs.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u4e2d\u5fc3\u90e8\u7f72\u65f6\u7684\u5185\u5b58\u7ba1\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f39\u6027\u5185\u5b58\u7ba1\u7406\u6846\u67b6eLLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5b58\u5229\u7528\u7387\u548c\u89e3\u7801\u541e\u5410\u91cf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5b58\u7ba1\u7406\u4e2d\uff0c\u52a8\u6001\u7ec4\u4ef6\uff08\u6fc0\u6d3b\u548cKV\u7f13\u5b58\uff09\u548c\u9759\u6001\u6743\u91cd\u5206\u79bb\u7ba1\u7406\u5bfc\u81f4\u5185\u5b58\u5229\u7528\u7387\u4f4e\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u63d0\u51faeLLM\u6846\u67b6\uff0c\u5305\u62ec\u865a\u62df\u5f20\u91cf\u62bd\u8c61\u3001\u5f39\u6027\u5185\u5b58\u673a\u5236\u548c\u8f7b\u91cf\u7ea7\u8c03\u5ea6\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u5185\u5b58\u5206\u914d\u5e76\u4f18\u5316\u5229\u7528\u3002", "result": "eLLM\u5728\u89e3\u7801\u541e\u5410\u91cf\u4e0a\u6bd4\u73b0\u6709\u7cfb\u7edf\u9ad82.32\u500d\uff0c\u652f\u63013\u500d\u5927\u7684\u6279\u91cf\u5904\u7406128K-token\u8f93\u5165\u3002", "conclusion": "eLLM\u901a\u8fc7\u7edf\u4e00\u5185\u5b58\u7ba1\u7406\u548c\u52a8\u6001\u8c03\u6574\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5b58\u5229\u7528\u7387\u548c\u5904\u7406\u6548\u7387\u3002"}}
{"id": "2506.15571", "pdf": "https://arxiv.org/pdf/2506.15571", "abs": "https://arxiv.org/abs/2506.15571", "authors": ["Le Vu Anh", "Nguyen Viet Anh", "Mehmet Dik", "Tu Nguyen Thi Ngoc"], "title": "MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh Smoothing", "categories": ["cs.LG", "cs.GR"], "comment": "9 pages, 8 figures, 4 tables", "summary": "Real-time mesh smoothing at scale remains a formidable challenge: classical Ricci-flow solvers demand costly global updates, while greedy heuristics suffer from slow convergence or brittle tuning. We present MicroRicci, the first truly self-tuning, local Ricci-flow solver that borrows ideas from coding theory and packs them into just 1K + 200 parameters. Its primary core is a greedy syndrome-decoding step that pinpoints and corrects the largest curvature error in O(E) time, augmented by two tiny neural modules that adaptively choose vertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes, MicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup), tightens curvature spread from 0.19 to 0.185, and achieves a remarkable UV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per iteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration over state-of-the-art methods. MicroRicci's combination of linear-time updates, automatic hyperparameter adaptation, and high-quality geometric and perceptual results makes it well suited for real-time, resource-limited applications in graphics, simulation, and related fields.", "AI": {"tldr": "MicroRicci \u662f\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u5c40\u90e8 Ricci \u6d41\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u7f16\u7801\u7406\u8bba\u548c\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u7f51\u683c\u5e73\u6ed1\u7684\u6027\u80fd\u548c\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf Ricci \u6d41\u6c42\u89e3\u5668\u9700\u8981\u8fdb\u884c\u5168\u5c40\u66f4\u65b0\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u8d2a\u5fc3\u65b9\u6cd5\u5219\u6536\u655b\u7f13\u6162\u6216\u9700\u8981\u7e41\u7410\u8c03\u53c2\u3002MicroRicci \u65e8\u5728\u5b9e\u73b0\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u5c40\u90e8\u6c42\u89e3\u3002", "method": "MicroRicci \u7ed3\u5408\u4e86\u8d2a\u5a6a\u7684\u7efc\u5408\u5f81\u89e3\u7801\u6b65\u9aa4\uff08\u5728 O(E) \u65f6\u95f4\u5185\u4fee\u6b63\u6700\u5927\u66f2\u7387\u8bef\u5dee\uff09\u548c\u4e24\u4e2a\u5c0f\u578b\u795e\u7ecf\u6a21\u5757\uff08\u52a8\u6001\u9009\u62e9\u9876\u70b9\u548c\u6b65\u957f\uff09\u3002", "result": "\u5728 110 \u4e2a SJTU-TMQA \u7f51\u683c\u4e0a\uff0cMicroRicci \u5c06\u8fed\u4ee3\u6b21\u6570\u51cf\u5c11 2.4 \u500d\uff0c\u66f2\u7387\u8303\u56f4\u6539\u5584\u4e3a 0.185\uff0cUV \u5931\u771f\u4e0e MOS \u76f8\u5173\u6027\u8fbe -0.93\uff0c\u6bcf\u6b21\u8fed\u4ee3\u4ec5\u589e\u52a0 0.25 \u6beb\u79d2\uff0c\u6574\u4f53\u52a0\u901f 1.8 \u500d\u3002", "conclusion": "MicroRicci \u4ee5\u5176\u7ebf\u6027\u65f6\u95f4\u66f4\u65b0\u3001\u81ea\u52a8\u8d85\u53c2\u6570\u8c03\u6574\u548c\u9ad8\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u56fe\u5f62\u3001\u6a21\u62df\u7b49\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2506.14820", "pdf": "https://arxiv.org/pdf/2506.14820", "abs": "https://arxiv.org/abs/2506.14820", "authors": ["Hyeon Jeon", "Hyunwook Lee", "Yun-Hsin Kuo", "Taehyun Yang", "Daniel Archambault", "Sungahn Ko", "Takanori Fujiwara", "Kwan-Liu Ma", "Jinwook Seo"], "title": "Navigating High-Dimensional Backstage: A Guide for Exploring Literature for the Reliable Use of Dimensionality Reduction", "categories": ["cs.HC", "cs.LG"], "comment": "EG/VGTC EuroVis 2025 Short paper", "summary": "Visual analytics using dimensionality reduction (DR) can easily be unreliable for various reasons, e.g., inherent distortions in representing the original data. The literature has thus proposed a wide range of methodologies to make DR-based visual analytics reliable. However, the diversity and extensiveness of the literature can leave novice analysts and researchers uncertain about where to begin and proceed. To address this problem, we propose a guide for reading papers for reliable visual analytics with DR. Relying on the previous classification of the relevant literature, our guide helps both practitioners to (1) assess their current DR expertise and (2) identify papers that will further enhance their understanding. Interview studies with three experts in DR and data visualizations validate the significance, comprehensiveness, and usefulness of our guide.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4efd\u6307\u5357\uff0c\u5e2e\u52a9\u521d\u5b66\u8005\u548c\u5b9e\u8df5\u8005\u901a\u8fc7\u5206\u7c7b\u6587\u732e\u6765\u53ef\u9760\u5730\u4f7f\u7528\u964d\u7ef4\uff08DR\uff09\u8fdb\u884c\u89c6\u89c9\u5206\u6790\u3002", "motivation": "\u7531\u4e8e\u964d\u7ef4\u6280\u672f\u5728\u89c6\u89c9\u5206\u6790\u4e2d\u53ef\u80fd\u5b58\u5728\u5931\u771f\u7b49\u95ee\u9898\uff0c\u6587\u732e\u4e2d\u65b9\u6cd5\u591a\u6837\u4e14\u5e7f\u6cdb\uff0c\u521d\u5b66\u8005\u548c\u7814\u7a76\u4eba\u5458\u5f80\u5f80\u4e0d\u77e5\u4ece\u4f55\u5165\u624b\u3002", "method": "\u57fa\u4e8e\u5148\u524d\u6587\u732e\u5206\u7c7b\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4efd\u6307\u5357\uff0c\u5e2e\u52a9\u7528\u6237\u8bc4\u4f30\u81ea\u8eabDR\u77e5\u8bc6\u6c34\u5e73\u5e76\u9009\u62e9\u76f8\u5173\u8bba\u6587\u5b66\u4e60\u3002", "result": "\u901a\u8fc7\u4e0e\u4e09\u4f4dDR\u548c\u6570\u636e\u53ef\u89c6\u5316\u4e13\u5bb6\u7684\u8bbf\u8c08\uff0c\u9a8c\u8bc1\u4e86\u6307\u5357\u7684\u91cd\u8981\u6027\u3001\u5168\u9762\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6307\u5357\u4e3a\u521d\u5b66\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u5b66\u4e60\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u63d0\u5347DR\u5728\u89c6\u89c9\u5206\u6790\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.15655", "pdf": "https://arxiv.org/pdf/2506.15655", "abs": "https://arxiv.org/abs/2506.15655", "authors": ["Yilin Zhang", "Xinran Zhao", "Zora Zhiruo Wang", "Chenyang Yang", "Jiayi Wei", "Tongshuang Wu"], "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree", "categories": ["cs.SE"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become essential for large-scale code generation, grounding predictions in external code corpora to improve actuality. However, a critical yet underexplored aspect of RAG pipelines is chunking -- the process of dividing documents into retrievable units. Existing line-based chunking heuristics often break semantic structures, splitting functions or merging unrelated code, which can degrade generation quality. We propose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method that recursively breaks large AST nodes into smaller chunks and merges sibling nodes while respecting size limits. This approach generates self-contained, semantically coherent units across programming languages and tasks, improving performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3 points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation. Our work highlights the importance of structure-aware chunking for scaling retrieval-enhanced code intelligence.", "AI": {"tldr": "RAG\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u4f9d\u8d56\u8bed\u4e49\u8fde\u8d2f\u7684\u5757\u9009\u62e9\uff0c\u73b0\u6709\u65b9\u6cd5\u7834\u574f\u8bed\u4e49\u7ed3\u6784\uff0c\u672c\u6587\u63d0\u51fa\u57fa\u4e8eAST\u7684\u5206\u5757\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u7ba1\u9053\u4e2d\u7684\u5206\u5757\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u884c\u7684\u5206\u5757\uff09\u4f1a\u7834\u574f\u4ee3\u7801\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u7684\u7ed3\u6784\u611f\u77e5\u5206\u5757\u65b9\u6cd5\uff0c\u9012\u5f52\u5206\u89e3\u5927\u8282\u70b9\u5e76\u5408\u5e76\u5144\u5f1f\u8282\u70b9\uff0c\u540c\u65f6\u9650\u5236\u5757\u5927\u5c0f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728RepoEval\u68c0\u7d22\u4efb\u52a1\u4e2dRecall@5\u63d0\u53474.3\u70b9\uff0c\u5728SWE-bench\u751f\u6210\u4efb\u52a1\u4e2dPass@1\u63d0\u53472.67\u70b9\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u5206\u5757\u5bf9\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u7684\u4ee3\u7801\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.15228", "pdf": "https://arxiv.org/pdf/2506.15228", "abs": "https://arxiv.org/abs/2506.15228", "authors": ["Yufeng Zhang", "Wenrui Dai", "Hang Yu", "Shizhan Liu", "Junhui Hou", "Jianguo Li", "Weiyao Lin"], "title": "ABC: Adaptive BayesNet Structure Learning for Computational Scalable Multi-task Image Compression", "categories": ["eess.IV", "cs.MM"], "comment": null, "summary": "Neural Image Compression (NIC) has revolutionized image compression with its superior rate-distortion performance and multi-task capabilities, supporting both human visual perception and machine vision tasks. However, its widespread adoption is hindered by substantial computational demands. While existing approaches attempt to address this challenge through module-specific optimizations or pre-defined complexity levels, they lack comprehensive control over computational complexity. We present ABC (Adaptive BayesNet structure learning for computational scalable multi-task image Compression), a novel, comprehensive framework that achieves computational scalability across all NIC components through Bayesian network (BayesNet) structure learning. ABC introduces three key innovations: (i) a heterogeneous bipartite BayesNet (inter-node structure) for managing neural backbone computations; (ii) a homogeneous multipartite BayesNet (intra-node structure) for optimizing autoregressive unit processing; and (iii) an adaptive control module that dynamically adjusts the BayesNet structure based on device capabilities, input data complexity, and downstream task requirements. Experiments demonstrate that ABC enables full computational scalability with better complexity adaptivity and broader complexity control span, while maintaining competitive compression performance. Furthermore, the framework's versatility allows integration with various NIC architectures that employ BayesNet representations, making it a robust solution for ensuring computational scalability in NIC applications. Code is available in https://github.com/worldlife123/cbench_BaSIC.", "AI": {"tldr": "ABC\u6846\u67b6\u901a\u8fc7\u8d1d\u53f6\u65af\u7f51\u7edc\u7ed3\u6784\u5b66\u4e60\u5b9e\u73b0\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u7684\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97\u590d\u6742\u6027\u63a7\u5236\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u56e0\u5176\u9ad8\u8ba1\u7b97\u9700\u6c42\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u8ba1\u7b97\u590d\u6742\u6027\u7684\u5168\u9762\u63a7\u5236\u3002", "method": "ABC\u6846\u67b6\u901a\u8fc7\u5f02\u6784\u4e8c\u5206\u8d1d\u53f6\u65af\u7f51\u7edc\u7ba1\u7406\u795e\u7ecf\u4e3b\u5e72\u8ba1\u7b97\uff0c\u540c\u6784\u591a\u5206\u8d1d\u53f6\u65af\u7f51\u7edc\u4f18\u5316\u81ea\u56de\u5f52\u5355\u5143\u5904\u7406\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u63a7\u5236\u6a21\u5757\u52a8\u6001\u8c03\u6574\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eABC\u5728\u4fdd\u6301\u538b\u7f29\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8ba1\u7b97\u9002\u5e94\u6027\u548c\u66f4\u5e7f\u7684\u590d\u6742\u6027\u63a7\u5236\u8303\u56f4\u3002", "conclusion": "ABC\u662f\u4e00\u79cd\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u67b6\u6784\u3002"}}
{"id": "2506.15613", "pdf": "https://arxiv.org/pdf/2506.15613", "abs": "https://arxiv.org/abs/2506.15613", "authors": ["Miryeong Kwon", "Donghyun Gouk", "Junhyeok Jang", "Jinwoo Baek", "Hyunwoo You", "Sangyoon Ji", "Hongjoo Jung", "Junseok Moon", "Seungkwan Kang", "Seungjun Lee", "Myoungsoo Jung"], "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and Instruction Annotation", "categories": ["cs.AR"], "comment": null, "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based block storage into a scalable, byte-addressable working memory. We address the challenges of adapting block storage to CXL's memory-centric model by emphasizing cacheability as a key enabler and advocating for Type 3 endpoint devices, referred to as CXL-SSDs. To validate our approach, we prototype a CXL-SSD on a custom FPGA platform and propose annotation mechanisms, Determinism and Bufferability, to enhance performance while preserving data persistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves 10.9x better performance than PCIe-based memory expanders and further reduces latency by 5.4x with annotation enhancements. In workloads with high locality, CXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This work highlights the feasibility of integrating block storage into CXL's ecosystem and provides a foundation for future memory-storage convergence.", "AI": {"tldr": "\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7CXL\u5c06PCIe\u5757\u5b58\u50a8\u6269\u5c55\u4e3a\u53ef\u4f38\u7f29\u7684\u5b57\u8282\u5bfb\u5740\u5de5\u4f5c\u5185\u5b58\uff0c\u901a\u8fc7CXL-SSD\u548c\u6ce8\u91ca\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5757\u5b58\u50a8\u9002\u5e94CXL\u5185\u5b58\u4e2d\u5fc3\u6a21\u578b\u7684\u6311\u6218\uff0c\u63a8\u52a8\u5185\u5b58-\u5b58\u50a8\u878d\u5408\u3002", "method": "\u91c7\u7528FPGA\u539f\u578b\u548c\u6ce8\u91ca\u673a\u5236\uff08Determinism\u548cBufferability\uff09\u4f18\u5316\u6027\u80fd\u3002", "result": "CXL-SSD\u6027\u80fd\u6bd4PCIe\u5185\u5b58\u6269\u5c55\u5668\u9ad810.9\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e5.4\u500d\uff1b\u9ad8\u6548\u7f13\u5b58\u4f7f\u5176\u63a5\u8fd1DRAM\u6027\u80fd\u3002", "conclusion": "CXL-SSD\u4e3a\u5185\u5b58-\u5b58\u50a8\u878d\u5408\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\uff0c\u5960\u5b9a\u672a\u6765\u7814\u7a76\u57fa\u7840\u3002"}}
{"id": "2506.15418", "pdf": "https://arxiv.org/pdf/2506.15418", "abs": "https://arxiv.org/abs/2506.15418", "authors": ["Nick Brown"], "title": "RISC-V for HPC: An update of where we are and main action points", "categories": ["cs.DC"], "comment": "Extended abstract accepted to the RISC-V Summit Europe 2025", "summary": "This extended abstract is submitted on behalf of the RISC-V HPC SIG who have been undertaking an analysis to explore the current state and limitations of the RISC-V ecosystem for HPC. Whilst it is right to celebrate that there has been great progress made in recent years, we also highlight limitations and where effort should be focussed.", "AI": {"tldr": "\u5206\u6790RISC-V\u5728HPC\u9886\u57df\u7684\u73b0\u72b6\u4e0e\u9650\u5236\uff0c\u5f3a\u8c03\u672a\u6765\u7684\u52aa\u529b\u65b9\u5411\u3002", "motivation": "\u8bc4\u4f30RISC-V\u751f\u6001\u7cfb\u7edf\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u4e2d\u7684\u5f53\u524d\u72b6\u6001\u4e0e\u4e0d\u8db3\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u53d1\u5c55\u3002", "method": "\u7531RISC-V HPC SIG\u7275\u5934\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5c3d\u7ba1\u8fd1\u5e74\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u9650\u5236\uff0c\u9700\u8fdb\u4e00\u6b65\u52aa\u529b\u3002", "conclusion": "\u9700\u5173\u6ce8RISC-V\u5728HPC\u4e2d\u7684\u77ed\u677f\uff0c\u96c6\u4e2d\u8d44\u6e90\u63a8\u52a8\u751f\u6001\u5b8c\u5584\u3002"}}
{"id": "2506.14829", "pdf": "https://arxiv.org/pdf/2506.14829", "abs": "https://arxiv.org/abs/2506.14829", "authors": ["Aditya Majumdar", "Wenbo Zhang", "Kashvi Prawal", "Amulya Yadav"], "title": "The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects focus on harnessing AI to address societal issues in areas such as healthcare, social justice, etc. Unfortunately, despite growing interest in AI4SI, achieving tangible, on-the-ground impact remains a significant challenge. For example, identifying and engaging motivated collaborators who are willing to co-design and deploy AI based solutions in real-world settings is often difficult. Even when such partnerships are established, many AI4SI projects \"fail\" to progress beyond the proof-of-concept stage, and hence, are unable to transition to at-scale production-level solutions. Furthermore, the unique challenges faced by AI4SI researchers are not always fully recognized within the broader AI community, where such work is sometimes viewed as primarily applied and not aligning with the traditional criteria for novelty emphasized in core AI venues. This paper attempts to shine a light on the diverse challenges faced in AI4SI research by diagnosing a multitude of factors that prevent AI4SI partnerships from achieving real-world impact on the ground. Drawing on semi-structured interviews with six leading AI4SI researchers - complemented by the authors' own lived experiences in conducting AI4SI research - this paper attempts to understand the day-to-day difficulties faced in developing and deploying socially impactful AI solutions. Through thematic analysis, we identify structural and organizational, communication, collaboration, and operational challenges as key barriers to deployment. While there are no easy fixes, we synthesize best practices and actionable strategies drawn from these interviews and our own work in this space. In doing so, we hope this paper serves as a practical reference guide for AI4SI researchers and partner organizations seeking to engage more effectively in socially impactful AI collaborations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86AI4SI\u7814\u7a76\u4e2d\u963b\u788d\u5b9e\u9645\u5f71\u54cd\u7684\u591a\u79cd\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u6700\u4f73\u5b9e\u8df5\u548c\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3AI4SI\u9879\u76ee\u96be\u4ee5\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0\u89c4\u6a21\u5316\u5e94\u7528\u7684\u6311\u6218\uff0c\u5e76\u63d0\u5347\u5176\u793e\u4f1a\u5f71\u54cd\u529b\u3002", "method": "\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u4f5c\u8005\u81ea\u8eab\u7ecf\u9a8c\uff0c\u91c7\u7528\u4e3b\u9898\u5206\u6790\u6cd5\u8bc6\u522b\u4e3b\u8981\u969c\u788d\u3002", "result": "\u8bc6\u522b\u51fa\u7ed3\u6784\u6027\u3001\u7ec4\u7ec7\u6027\u3001\u6c9f\u901a\u3001\u534f\u4f5c\u548c\u8fd0\u8425\u7b49\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u5e94\u5bf9\u7b56\u7565\u3002", "conclusion": "\u8bba\u6587\u4e3aAI4SI\u7814\u7a76\u8005\u548c\u5408\u4f5c\u4f19\u4f34\u63d0\u4f9b\u4e86\u4e00\u4efd\u5b9e\u7528\u6307\u5357\uff0c\u5e2e\u52a9\u66f4\u6709\u6548\u5730\u5f00\u5c55\u793e\u4f1a\u5f71\u54cd\u529b\u5f3a\u7684AI\u5408\u4f5c\u3002"}}
{"id": "2506.15648", "pdf": "https://arxiv.org/pdf/2506.15648", "abs": "https://arxiv.org/abs/2506.15648", "authors": ["Georgios Androutsopoulos", "Antonio Bianchi"], "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses", "categories": ["cs.CR", "cs.LG", "cs.SE"], "comment": null, "summary": "Although Rust ensures memory safety by default, it also permits the use of unsafe code, which can introduce memory safety vulnerabilities if misused. Unfortunately, existing tools for detecting memory bugs in Rust typically exhibit limited detection capabilities, inadequately handle Rust-specific types, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates static analysis with Large Language Model (LLM)-guided fuzzing harness generation to effectively identify memory safety vulnerabilities in Rust libraries, specifically targeting unsafe code. deepSURF introduces a novel approach for handling generics by substituting them with custom types and generating tailored implementations for the required traits, enabling the fuzzer to simulate user-defined behaviors within the fuzzed library. Additionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically, facilitating exploration of complex API interactions and significantly increasing the likelihood of exposing memory safety vulnerabilities. We evaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20 known memory safety bugs and uncovering 6 previously unknown vulnerabilities, demonstrating clear improvements over state-of-the-art tools.", "AI": {"tldr": "deepSURF\u662f\u4e00\u79cd\u7ed3\u5408\u9759\u6001\u5206\u6790\u548cLLM\u5f15\u5bfc\u7684\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\uff0c\u7528\u4e8e\u6709\u6548\u68c0\u6d4bRust\u5e93\u4e2d\u7684\u5185\u5b58\u5b89\u5168\u6f0f\u6d1e\uff0c\u5c24\u5176\u9488\u5bf9\u4e0d\u5b89\u5168\u4ee3\u7801\u3002", "motivation": "\u5c3d\u7ba1Rust\u9ed8\u8ba4\u63d0\u4f9b\u5185\u5b58\u5b89\u5168\u6027\uff0c\u4f46\u4e0d\u5b89\u5168\u4ee3\u7801\u7684\u4f7f\u7528\u53ef\u80fd\u5f15\u5165\u6f0f\u6d1e\uff0c\u73b0\u6709\u5de5\u5177\u68c0\u6d4b\u80fd\u529b\u6709\u9650\u3002", "method": "deepSURF\u901a\u8fc7\u81ea\u5b9a\u4e49\u7c7b\u578b\u66ff\u4ee3\u6cdb\u578b\u3001\u751f\u6210\u7279\u5b9a\u7279\u8d28\u5b9e\u73b0\uff0c\u5e76\u7ed3\u5408LLM\u52a8\u6001\u589e\u5f3a\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u94fe\u3002", "result": "\u572827\u4e2a\u771f\u5b9eRust\u5e93\u4e2d\uff0c\u6210\u529f\u91cd\u73b020\u4e2a\u5df2\u77e5\u6f0f\u6d1e\u5e76\u53d1\u73b06\u4e2a\u65b0\u6f0f\u6d1e\u3002", "conclusion": "deepSURF\u5728\u68c0\u6d4b\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.15276", "pdf": "https://arxiv.org/pdf/2506.15276", "abs": "https://arxiv.org/abs/2506.15276", "authors": ["Jun Zhu", "Xinfeng Zhang", "Lv Tang", "JunHao Jiang"], "title": "MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": null, "summary": "Implicit Neural representations (INRs) have emerged as a promising approach for video compression, and have achieved comparable performance to the state-of-the-art codecs such as H.266/VVC. However, existing INR-based methods struggle to effectively represent detail-intensive and fast-changing video content. This limitation mainly stems from the underutilization of internal network features and the absence of video-specific considerations in network design. To address these challenges, we propose a multi-scale feature fusion framework, MSNeRV, for neural video representation. In the encoding stage, we enhance temporal consistency by employing temporal windows, and divide the video into multiple Groups of Pictures (GoPs), where a GoP-level grid is used for background representation. Additionally, we design a multi-scale spatial decoder with a scale-adaptive loss function to integrate multi-resolution and multi-frequency information. To further improve feature extraction, we introduce a multi-scale feature block that fully leverages hidden features. We evaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and compression. Experimental results demonstrate that our model exhibits superior representation capability among INR-based approaches and surpasses VTM-23.7 (Random Access) in dynamic scenarios in terms of compression efficiency.", "AI": {"tldr": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6846\u67b6MSNeRV\uff0c\u589e\u5f3a\u795e\u7ecf\u89c6\u9891\u8868\u793a\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7ec6\u8282\u548c\u5feb\u901f\u53d8\u5316\u5185\u5bb9\u8868\u73b0\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709INR\u65b9\u6cd5\u5728\u7ec6\u8282\u5bc6\u96c6\u548c\u5feb\u901f\u53d8\u5316\u7684\u89c6\u9891\u5185\u5bb9\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6e90\u4e8e\u7f51\u7edc\u7279\u5f81\u5229\u7528\u4e0d\u8db3\u548c\u7f3a\u4e4f\u89c6\u9891\u7279\u5b9a\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faMSNeRV\u6846\u67b6\uff0c\u5305\u62ec\u65f6\u95f4\u7a97\u53e3\u589e\u5f3a\u4e00\u81f4\u6027\u3001GoP\u7ea7\u7f51\u683c\u80cc\u666f\u8868\u793a\u3001\u591a\u5c3a\u5ea6\u7a7a\u95f4\u89e3\u7801\u5668\u53ca\u81ea\u9002\u5e94\u635f\u5931\u51fd\u6570\uff0c\u591a\u5c3a\u5ea6\u7279\u5f81\u5757\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5728HEVC ClassB\u548cUVG\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cMSNeRV\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6INR\u65b9\u6cd5\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e2d\u538b\u7f29\u6548\u7387\u8d85\u8d8aVTM-23.7\u3002", "conclusion": "MSNeRV\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u663e\u8457\u63d0\u5347\u89c6\u9891\u8868\u793a\u548c\u538b\u7f29\u6027\u80fd\uff0c\u5c24\u5176\u5728\u52a8\u6001\u5185\u5bb9\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.15634", "pdf": "https://arxiv.org/pdf/2506.15634", "abs": "https://arxiv.org/abs/2506.15634", "authors": ["Hasnain A. Ziad", "Alexander C. Bodoh", "Ashiq A. Sakib"], "title": "SR-NCL: an Area-/Energy-Efficient Resilient NCL Architecture Based on Selective Redundancy", "categories": ["cs.AR"], "comment": "5 pages. Accepted for publication in the Proceedings of IEEE ISCAS 2025", "summary": "Duplication-based redundancy schemes have proven to be effective in designing fully-resilient Quasi-delay Insensitive (QDI) asynchronous circuits. The complete resiliency, however, is accompanied by significant energy, latency, and area overhead. This paper presents a novel error-tolerant Null Convention Logic (NCL) architecture based on selective redundancy. Results demonstrate the efficacy of the proposed method in terms of area and energy utilization as compared to existing duplication-based NCL designs, targeting an image processing application.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9009\u62e9\u6027\u5197\u4f59\u7684\u65b0\u578b\u5bb9\u9519NCL\u67b6\u6784\uff0c\u76f8\u6bd4\u73b0\u6709\u7684\u57fa\u4e8e\u590d\u5236\u7684NCL\u8bbe\u8ba1\uff0c\u5728\u9762\u79ef\u548c\u80fd\u8017\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5b8c\u5168\u5197\u4f59\u7684QDI\u5f02\u6b65\u7535\u8def\u8bbe\u8ba1\u867d\u7136\u80fd\u5b9e\u73b0\u5b8c\u5168\u5f39\u6027\uff0c\u4f46\u5e26\u6765\u663e\u8457\u7684\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u9762\u79ef\u5f00\u9500\u3002", "method": "\u91c7\u7528\u9009\u62e9\u6027\u5197\u4f59\u7684NCL\u67b6\u6784\u3002", "result": "\u5728\u56fe\u50cf\u5904\u7406\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u9762\u79ef\u548c\u80fd\u8017\u5229\u7528\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u9009\u62e9\u6027\u5197\u4f59\u7684NCL\u67b6\u6784\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5bb9\u9519\u65b9\u6848\u3002"}}
{"id": "2506.15437", "pdf": "https://arxiv.org/pdf/2506.15437", "abs": "https://arxiv.org/abs/2506.15437", "authors": ["Nick Brown", "Jake Davies", "Felix LeClair"], "title": "Exploring Fast Fourier Transforms on the Tenstorrent Wormhole", "categories": ["cs.DC"], "comment": "Author accepted version of paper submitted to RISC-V for HPC ISC workshop 2025", "summary": "Whilst numerous areas of computing have adopted the RISC-V Instruction Set Architecture (ISA) wholesale in recent years, it is yet to become widespread in HPC. RISC-V accelerators offer a compelling option where the HPC community can benefit from the specialisation offered by the open nature of the standard but without the extensive ecosystem changes required when adopting RISC-V CPUs. In this paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT) algorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon Tenstorrent's Tensix architecture, this technology decouples the movement of data from compute, potentially offering increased control to the programmer. Exploring different optimisation techniques to address the bottlenecks inherent in data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is slower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around 8 times less power and consumes around 2.8 times less energy than the CPU when computing the Fourier transform.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06Cooley-Tukey FFT\u7b97\u6cd5\u79fb\u690d\u5230RISC-V\u52a0\u901f\u5668\u4e0a\u7684\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u529f\u8017\u548c\u80fd\u6e90\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1RISC-V ISA\u5728\u591a\u4e2a\u8ba1\u7b97\u9886\u57df\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5c1a\u672a\u5728HPC\u4e2d\u666e\u53ca\u3002RISC-V\u52a0\u901f\u5668\u4e3aHPC\u63d0\u4f9b\u4e86\u65e0\u9700\u5927\u89c4\u6a21\u751f\u6001\u53d8\u5316\u7684\u4e13\u4e1a\u5316\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5c06FFT\u7b97\u6cd5\u79fb\u690d\u5230Tenstorrent Wormhole PCIe RISC-V\u52a0\u901f\u5668\u4e0a\uff0c\u4f18\u5316\u6570\u636e\u79fb\u52a8\u7684\u74f6\u9888\u95ee\u9898\u3002", "result": "\u867d\u7136Wormhole n300\u5728\u5904\u74062D FFT\u65f6\u901f\u5ea6\u4e0d\u53ca24\u6838Xeon Platinum CPU\uff0c\u4f46\u5176\u529f\u8017\u548c\u80fd\u6e90\u6d88\u8017\u5206\u522b\u964d\u4f4e8\u500d\u548c2.8\u500d\u3002", "conclusion": "RISC-V\u52a0\u901f\u5668\u5728HPC\u4e2d\u5177\u6709\u663e\u8457\u7684\u80fd\u6548\u4f18\u52bf\uff0c\u9002\u5408\u7279\u5b9a\u573a\u666f\u7684\u5e94\u7528\u3002"}}
{"id": "2506.14948", "pdf": "https://arxiv.org/pdf/2506.14948", "abs": "https://arxiv.org/abs/2506.14948", "authors": ["Mohna Chakraborty", "Lu Wang", "David Jurgens"], "title": "Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework", "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in domains requiring moral understanding, yet their reasoning often remains shallow, and misaligned with human reasoning. Unlike humans, whose moral reasoning integrates contextual trade-offs, value systems, and ethical theories, LLMs often rely on surface patterns, leading to biased decisions in morally and ethically complex scenarios. To address this gap, we present a value-grounded framework for evaluating and distilling structured moral reasoning in LLMs. We benchmark 12 open-source models across four moral datasets using a taxonomy of prompts grounded in value systems, ethical theories, and cognitive reasoning strategies. Our evaluation is guided by four questions: (1) Does reasoning improve LLM decision-making over direct prompting? (2) Which types of value/ethical frameworks most effectively guide LLM reasoning? (3) Which cognitive reasoning strategies lead to better moral performance? (4) Can small-sized LLMs acquire moral competence through distillation? We find that prompting with explicit moral structure consistently improves accuracy and coherence, with first-principles reasoning and Schwartz's + care-ethics scaffolds yielding the strongest gains. Furthermore, our supervised distillation approach transfers moral competence from large to small models without additional inference cost. Together, our results offer a scalable path toward interpretable and value-grounded models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ef7\u503c\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u4e0e\u4eba\u7c7b\u9053\u5fb7\u63a8\u7406\u7684\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65b9\u6cd5\u63d0\u5347\u5176\u9053\u5fb7\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u4ef7\u503c\u7cfb\u7edf\u3001\u4f26\u7406\u7406\u8bba\u548c\u8ba4\u77e5\u7b56\u7565\u7684\u5206\u7c7b\u63d0\u793a\u57fa\u51c6\uff0c\u8bc4\u4f3012\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u76d1\u7763\u84b8\u998f\u65b9\u6cd5\u5c06\u9053\u5fb7\u80fd\u529b\u4ece\u5927\u6a21\u578b\u8fc1\u79fb\u5230\u5c0f\u6a21\u578b\u3002", "result": "\u5177\u6709\u660e\u786e\u9053\u5fb7\u7ed3\u6784\u7684\u63d0\u793a\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u7b2c\u4e00\u539f\u7406\u63a8\u7406\u548c\u7279\u5b9a\u4f26\u7406\u6846\u67b6\u6548\u679c\u6700\u4f73\uff1b\u76d1\u7763\u84b8\u998f\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u9053\u5fb7\u80fd\u529b\u7684\u8fc1\u79fb\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4ef7\u503c\u57fa\u7840\u7684\u9053\u5fb7\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2506.15298", "pdf": "https://arxiv.org/pdf/2506.15298", "abs": "https://arxiv.org/abs/2506.15298", "authors": ["Xinqi Fan", "Jingting Li", "John See", "Moi Hoon Yap", "Wen-Huang Cheng", "Xiaobai Li", "Xiaopeng Hong", "Su-Jing Wang", "Adrian K. Davision"], "title": "MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering", "categories": ["cs.CV", "cs.MM"], "comment": "Micro-Expression Grand Challenge (MEGC) at ACM MM 2025", "summary": "Facial micro-expressions (MEs) are involuntary movements of the face that occur spontaneously when a person experiences an emotion but attempts to suppress or repress the facial expression, typically found in a high-stakes environment. In recent years, substantial advancements have been made in the areas of ME recognition, spotting, and generation. However, conventional approaches that treat spotting and recognition as separate tasks are suboptimal, particularly for analyzing long-duration videos in realistic settings. Concurrently, the emergence of multimodal large language models (MLLMs) and large vision-language models (LVLMs) offers promising new avenues for enhancing ME analysis through their powerful multimodal reasoning capabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that reflect these evolving research directions: (1) ME spot-then-recognize (ME-STR), which integrates ME spotting and subsequent recognition in a unified sequential pipeline; and (2) ME visual question answering (ME-VQA), which explores ME understanding through visual question answering, leveraging MLLMs or LVLMs to address diverse question types related to MEs. All participating algorithms are required to run on this test set and submit their results on a leaderboard. More details are available at https://megc2025.github.io.", "AI": {"tldr": "MEGC 2025\u63d0\u51fa\u4e24\u4e2a\u65b0\u4efb\u52a1\uff1aME-STR\uff08\u7edf\u4e00\u5fae\u8868\u60c5\u68c0\u6d4b\u4e0e\u8bc6\u522b\uff09\u548cME-VQA\uff08\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u95ee\u7b54\uff09\uff0c\u4ee5\u6539\u8fdb\u5fae\u8868\u60c5\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5fae\u8868\u60c5\u68c0\u6d4b\u4e0e\u8bc6\u522b\u5206\u5f00\u5904\u7406\uff0c\u4e0d\u9002\u7528\u4e8e\u957f\u89c6\u9891\u5206\u6790\uff0c\u800c\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e3a\u65b0\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u901a\u8fc7ME-STR\u4efb\u52a1\u7edf\u4e00\u68c0\u6d4b\u4e0e\u8bc6\u522b\u6d41\u7a0b\uff0c\u5229\u7528ME-VQA\u4efb\u52a1\u7ed3\u5408\u591a\u6a21\u6001\u5927\u6a21\u578b\u8fdb\u884c\u95ee\u7b54\u5f0f\u7406\u89e3\u3002", "result": "MEGC 2025\u4e3a\u5fae\u8868\u60c5\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u8bbe\u7acb\u516c\u5f00\u6d4b\u8bd5\u96c6\u4e0e\u6392\u884c\u699c\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u5f15\u5165\u6709\u671b\u63d0\u5347\u5fae\u8868\u60c5\u5206\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u63a8\u52a8\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.14830", "pdf": "https://arxiv.org/pdf/2506.14830", "abs": "https://arxiv.org/abs/2506.14830", "authors": ["Zhizhao Wen", "Ruoxin Zhang", "Chao Wang"], "title": "Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": "Source code available; Accepted by 2025 6th International Conference on Electronic Communication and Artificial Intelligence; 5 pages; 7 figures", "summary": "Aiming at the critical role of SSD health state prediction in data reliability assurance, this study proposes a hybrid BiGRU-MHA model that incorporates a multi-head attention mechanism to enhance the accuracy and stability of storage device health classification. The model innovatively integrates temporal feature extraction and key information focusing capabilities. Specifically, it leverages the bidirectional timing modeling advantages of the BiGRU network to capture both forward and backward dependencies of SSD degradation features. Simultaneously, the multi-head attention mechanism dynamically assigns feature weights, improving the model's sensitivity to critical health indicators. Experimental results show that the proposed model achieves classification accuracies of 92.70% on the training set and 92.44% on the test set, with a minimal performance gap of only 0.26%, demonstrating excellent generalization ability. Further analysis using the receiver operating characteristic (ROC) curve shows an area under the curve (AUC) of 0.94 on the test set, confirming the model's robust binary classification performance. This work not only presents a new technical approach for SSD health prediction but also addresses the generalization bottleneck of traditional models, offering a verifiable method with practical value for preventive maintenance of industrial-grade storage systems. The results show the model can significantly reduce data loss risks by providing early failure warnings and help optimize maintenance costs, supporting intelligent decision-making in building reliable storage systems for cloud computing data centers and edge storage environments.", "AI": {"tldr": "\u63d0\u51fa\u7684\u6df7\u5408BiGRU-MHA\u6a21\u578b\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u4e86SSD\u5065\u5eb7\u72b6\u6001\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5177\u6709\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "SSD\u5065\u5eb7\u72b6\u6001\u9884\u6d4b\u5bf9\u6570\u636e\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u6a21\u578b\u7ed3\u5408\u53cc\u5411GRU\u7f51\u7edc\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6355\u6349\u65f6\u95f4\u7279\u5f81\u5e76\u52a8\u6001\u5206\u914d\u6743\u91cd\u3002", "result": "\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\u5206\u522b\u4e3a92.70%\u548c92.44%\uff0cAUC\u4e3a0.94\uff0c\u6cdb\u5316\u6027\u80fd\u4f18\u79c0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aSSD\u5065\u5eb7\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u6570\u636e\u4e22\u5931\u98ce\u9669\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.15454", "pdf": "https://arxiv.org/pdf/2506.15454", "abs": "https://arxiv.org/abs/2506.15454", "authors": ["Nizar ALHafez", "Ahmad Kurdi"], "title": "Parallel Paradigms in Modern HPC: A Comparative Analysis of MPI, OpenMP, and CUDA", "categories": ["cs.DC"], "comment": "10 pages", "summary": "This paper presents a comprehensive comparison of three dominant parallel programming models in High Performance Computing (HPC): Message Passing Interface (MPI), Open Multi-Processing (OpenMP), and Compute Unified Device Architecture (CUDA). Selecting optimal programming approaches for modern heterogeneous HPC architectures has become increasingly critical. We systematically analyze these models across multiple dimensions: architectural foundations, performance characteristics, domain-specific suitability, programming complexity, and recent advancements. We examine each model's strengths, weaknesses, and optimization techniques. Our investigation demonstrates that MPI excels in distributed memory environments with near-linear scalability for communication-intensive applications, but faces communication overhead challenges. OpenMP provides strong performance and usability in shared-memory systems and loop-centric tasks, though it is limited by shared memory contention. CUDA offers substantial performance gains for data-parallel GPU workloads, but is restricted to NVIDIA GPUs and requires specialized expertise. Performance evaluations across scientific simulations, machine learning, and data analytics reveal that hybrid approaches combining two or more models often yield optimal results in heterogeneous environments. The paper also discusses implementation challenges, optimization best practices, and emerging trends such as performance portability frameworks, task-based programming, and the convergence of HPC and Big Data. This research helps developers and researchers make informed decisions when selecting programming models for modern HPC applications, emphasizing that the best choice depends on application requirements, hardware, and development constraints.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86HPC\u4e2d\u7684\u4e09\u79cd\u4e3b\u6d41\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\uff1aMPI\u3001OpenMP\u548cCUDA\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\u53ca\u9002\u7528\u573a\u666f\uff0c\u5e76\u6307\u51fa\u6df7\u5408\u65b9\u6cd5\u5728\u5f02\u6784\u73af\u5883\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u4ee3\u5f02\u6784HPC\u67b6\u6784\u4e2d\u5982\u4f55\u9009\u62e9\u6700\u4f18\u7f16\u7a0b\u65b9\u6cd5\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u7cfb\u7edf\u5bf9\u6bd4MPI\u3001OpenMP\u548cCUDA\u5728\u67b6\u6784\u57fa\u7840\u3001\u6027\u80fd\u7279\u70b9\u3001\u9886\u57df\u9002\u7528\u6027\u3001\u7f16\u7a0b\u590d\u6742\u5ea6\u548c\u6700\u65b0\u8fdb\u5c55\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "MPI\u5728\u5206\u5e03\u5f0f\u5185\u5b58\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cOpenMP\u64c5\u957f\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\uff0cCUDA\u9002\u7528\u4e8eGPU\u4efb\u52a1\uff0c\u800c\u6df7\u5408\u65b9\u6cd5\u901a\u5e38\u6700\u4f18\u3002", "conclusion": "\u6700\u4f73\u7f16\u7a0b\u6a21\u578b\u53d6\u51b3\u4e8e\u5e94\u7528\u9700\u6c42\u3001\u786c\u4ef6\u6761\u4ef6\u548c\u5f00\u53d1\u9650\u5236\uff0c\u6df7\u5408\u65b9\u6cd5\u66f4\u9002\u5408\u73b0\u4ee3HPC\u5e94\u7528\u3002"}}
{"id": "2506.15008", "pdf": "https://arxiv.org/pdf/2506.15008", "abs": "https://arxiv.org/abs/2506.15008", "authors": ["Richa Gupta", "Alexander Htet Kyaw"], "title": "Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output", "categories": ["cs.HC", "cs.AI"], "comment": "15 Pages, 6 figures, CAAD Futures 2025", "summary": "Generative AI, specifically text-to-image models, have revolutionized interior architectural design by enabling the rapid translation of conceptual ideas into visual representations from simple text prompts. While generative AI can produce visually appealing images they often lack actionable data for designers In this work, we propose a novel pipeline that integrates DALL-E 3 with a materials dataset to enrich AI-generated designs with sustainability metrics and material usage insights. After the model generates an interior design image, a post-processing module identifies the top ten materials present and pairs them with carbon dioxide equivalent (CO2e) values from a general materials dictionary. This approach allows designers to immediately evaluate environmental impacts and refine prompts accordingly. We evaluate the system through three user tests: (1) no mention of sustainability to the user prior to the prompting process with generative AI, (2) sustainability goals communicated to the user before prompting, and (3) sustainability goals communicated along with quantitative CO2e data included in the generative AI outputs. Our qualitative and quantitative analyses reveal that the introduction of sustainability metrics in the third test leads to more informed design decisions, however, it can also trigger decision fatigue and lower overall satisfaction. Nevertheless, the majority of participants reported incorporating sustainability principles into their workflows in the third test, underscoring the potential of integrated metrics to guide more ecologically responsible practices. Our findings showcase the importance of balancing design freedom with practical constraints, offering a clear path toward holistic, data-driven solutions in AI-assisted architectural design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DALL-E 3\u548c\u6750\u6599\u6570\u636e\u96c6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728AI\u751f\u6210\u7684\u8bbe\u8ba1\u4e2d\u6dfb\u52a0\u53ef\u6301\u7eed\u6027\u6307\u6807\u548c\u6750\u6599\u4f7f\u7528\u6570\u636e\uff0c\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u8bc4\u4f30\u73af\u5883\u5f71\u54cd\u3002", "motivation": "\u867d\u7136\u73b0\u6709\u7684\u751f\u6210\u5f0fAI\u53ef\u4ee5\u5feb\u901f\u751f\u6210\u89c6\u89c9\u6548\u679c\uff0c\u4f46\u7f3a\u4e4f\u8bbe\u8ba1\u5e08\u6240\u9700\u7684\u53ef\u64cd\u4f5c\u6570\u636e\uff0c\u5c24\u5176\u662f\u53ef\u6301\u7eed\u6027\u65b9\u9762\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u7a0b\uff0c\u5229\u7528\u540e\u5904\u7406\u6a21\u5757\u4eceAI\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u8bc6\u522b\u6750\u6599\uff0c\u5e76\u4e0e\u4e8c\u6c27\u5316\u78b3\u5f53\u91cf\uff08CO2e\uff09\u6570\u636e\u5339\u914d\uff0c\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u73af\u5883\u5f71\u54cd\u7684\u5373\u65f6\u8bc4\u4f30\u3002", "result": "\u7528\u6237\u6d4b\u8bd5\u663e\u793a\uff0c\u5f15\u5165\u53ef\u6301\u7eed\u6027\u6307\u6807\u53ef\u4ee5\u4fc3\u8fdb\u66f4\u660e\u667a\u7684\u8bbe\u8ba1\u51b3\u7b56\uff0c\u4f46\u4e5f\u53ef\u80fd\u5bfc\u81f4\u51b3\u7b56\u75b2\u52b3\u548c\u6ee1\u610f\u5ea6\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5728AI\u8f85\u52a9\u8bbe\u8ba1\u4e2d\u5e73\u8861\u8bbe\u8ba1\u81ea\u7531\u4e0e\u53ef\u6301\u7eed\u6027\u7ea6\u675f\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u66f4\u5168\u9762\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u8def\u5f84\u3002"}}
{"id": "2506.15677", "pdf": "https://arxiv.org/pdf/2506.15677", "abs": "https://arxiv.org/abs/2506.15677", "authors": ["Yining Hong", "Rui Sun", "Bingxuan Li", "Xingcheng Yao", "Maxine Wu", "Alexander Chien", "Da Yin", "Ying Nian Wu", "Zhecan James Wang", "Kai-Wei Chang"], "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM", "cs.RO"], "comment": null, "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bAI\u4ee3\u7406\u8303\u5f0f\u2014\u2014Embodied Web Agents\uff0c\u65e8\u5728\u878d\u5408\u7269\u7406\u4e16\u754c\u611f\u77e5\u4e0e\u5728\u7ebf\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5f80\u5f80\u5c40\u9650\u4e8e\u6570\u5b57\u4fe1\u606f\u5904\u7406\u6216\u7269\u7406\u4e16\u754c\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u4e24\u8005\u7ed3\u5408\u7684\u80fd\u529b\uff0c\u800c\u8bb8\u591a\u4efb\u52a1\u9700\u8981\u8de8\u9886\u57df\u7684\u667a\u80fd\u534f\u4f5c\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u4eff\u771f\u5e73\u53f0\uff0c\u96c6\u62103D\u73af\u5883\u4e0e\u529f\u80fd\u5316\u7f51\u7edc\u63a5\u53e3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6db5\u76d6\u591a\u79cd\u8de8\u9886\u57df\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982\u70f9\u996a\u3001\u5bfc\u822a\u7b49\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709AI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u80fd\u529b\u76f8\u6bd4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u51f8\u663e\u4e86\u8de8\u9886\u57df\u8ba4\u77e5\u4e0e\u77e5\u8bc6\u8bbf\u95ee\u7684\u6311\u6218\u4e0e\u673a\u9047\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7269\u7406\u4e0e\u6570\u5b57\u667a\u80fd\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u4e0e\u5e73\u53f0\u4ee5\u4f9b\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2506.15461", "pdf": "https://arxiv.org/pdf/2506.15461", "abs": "https://arxiv.org/abs/2506.15461", "authors": ["Nikolay Blagoev", "O\u011fuzhan Ersoy", "Lydia Yiyu Chen"], "title": "All is Not Lost: LLM Recovery without Checkpoints", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple on-spot instances, lowers the training cost and enables model democratization. The inevitable challenge here is the churn of nodes due to failures and the operator's scheduling policies, leading to losing a stage - a part of the model. The conventional approaches to recover from failures are to either use checkpointing, where periodically a copy of the entire model is sent to an additional storage, or redundant computation. These approaches yield significant communication and/or computation overhead even in non-failure cases and scale poorly in settings with large models. In this paper, we propose, CheckFree, an efficient recovery method where a failing stage is substituted by a weighted average of the closest neighboring stages. In contrast to the state of the art, CheckFree requires no additional computation or storage. However, because of the nature of averaging neighbouring stages, it can only recover failures of intermediate stages. We further extend our method to CheckFree+ with out-of-order pipeline execution to tolerate crashes of the first and last stages. Thanks to out-of-order pipelining, behaviour of those stages is mimicked by their neighboring ones, which allows CheckFree+ to recover them by simply copying the weights from the immediate neighbour. To be able to recover the (de)embedding layers, CheckFree+ copies those layers to the neighboring stages, which requires relatively small storage overhead. We extensively evaluate our method on LLaMa models of model sizes from 124M to 1.5B with varying failure frequencies. In the case of low and medium failure rates (5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant computation in terms of convergence in wall-clock time by over 12%. Both of our proposals can be run via our code available at: https://github.com/gensyn-ai/CheckFree.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCheckFree\u7684\u9ad8\u6548\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u90bb\u8fd1\u9636\u6bb5\u6765\u66ff\u4ee3\u6545\u969c\u9636\u6bb5\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6216\u5b58\u50a8\u3002", "motivation": "\u89e3\u51b3\u5728\u5206\u5e03\u5f0f\u5f31\u8ba1\u7b97\u8282\u70b9\u4e0a\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u8282\u70b9\u6545\u969c\u5bfc\u81f4\u7684\u9636\u6bb5\u4e22\u5931\u95ee\u9898\uff0c\u964d\u4f4e\u4f20\u7edf\u6062\u590d\u65b9\u6cd5\u7684\u5f00\u9500\u3002", "method": "CheckFree\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u90bb\u8fd1\u9636\u6bb5\u6765\u6062\u590d\u6545\u969c\u9636\u6bb5\uff1bCheckFree+\u8fdb\u4e00\u6b65\u7ed3\u5408\u4e71\u5e8f\u6d41\u6c34\u7ebf\u6267\u884c\u4ee5\u5bb9\u5fcd\u9996\u672b\u9636\u6bb5\u7684\u6545\u969c\u3002", "result": "\u5728\u4f4e\u81f3\u4e2d\u7b49\u6545\u969c\u7387\uff085-10%\uff09\u4e0b\uff0cCheckFree\u548cCheckFree+\u5728\u6536\u655b\u65f6\u95f4\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u5347\u8d85\u8fc712%\u3002", "conclusion": "CheckFree\u548cCheckFree+\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4f4e\u5f00\u9500\u7684\u6545\u969c\u6062\u590d\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u6a21\u578b\u573a\u666f\u3002"}}
{"id": "2506.15047", "pdf": "https://arxiv.org/pdf/2506.15047", "abs": "https://arxiv.org/abs/2506.15047", "authors": ["Jiayue Melissa Shi", "Dong Whi Yoo", "Keran Wang", "Violeta J. Rodriguez", "Ravi Karkar", "Koustuv Saha"], "title": "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Family caregivers of individuals with Alzheimer's Disease and Related Dementia (AD/ADRD) face significant emotional and logistical challenges that place them at heightened risk for stress, anxiety, and depression. Although recent advances in generative AI -- particularly large language models (LLMs) -- offer new opportunities to support mental health, little is known about how caregivers perceive and engage with such technologies. To address this gap, we developed Carey, a GPT-4o-based chatbot designed to provide informational and emotional support to AD/ADRD caregivers. Using Carey as a technology probe, we conducted semi-structured interviews with 16 family caregivers following scenario-driven interactions grounded in common caregiving stressors. Through inductive coding and reflexive thematic analysis, we surface a systemic understanding of caregiver needs and expectations across six themes -- on-demand information access, emotional support, safe space for disclosure, crisis management, personalization, and data privacy. For each of these themes, we also identified the nuanced tensions in the caregivers' desires and concerns. We present a mapping of caregiver needs, AI chatbot's strengths, gaps, and design recommendations. Our findings offer theoretical and practical insights to inform the design of proactive, trustworthy, and caregiver-centered AI systems that better support the evolving mental health needs of AD/ADRD caregivers.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u6b3e\u57fa\u4e8eGPT-4o\u7684\u804a\u5929\u673a\u5668\u4ebaCarey\uff0c\u7528\u4e8e\u652f\u6301\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u53ca\u76f8\u5173\u75f4\u5446\u75c7\uff08AD/ADRD\uff09\u60a3\u8005\u5bb6\u5c5e\u7684\u5fc3\u7406\u5065\u5eb7\uff0c\u5e76\u901a\u8fc7\u8bbf\u8c08\u5206\u6790\u4e86\u516d\u7c7b\u9700\u6c42\u53ca\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "AD/ADRD\u60a3\u8005\u5bb6\u5c5e\u9762\u4e34\u5de8\u5927\u7684\u60c5\u7eea\u548c\u540e\u52e4\u538b\u529b\uff0c\u73b0\u6709\u751f\u6210\u5f0fAI\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\uff09\u672a\u88ab\u5145\u5206\u63a2\u7d22\u5982\u4f55\u652f\u6301\u5176\u5fc3\u7406\u5065\u5eb7\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eGPT-4o\u7684\u804a\u5929\u673a\u5668\u4ebaCarey\u4f5c\u4e3a\u6280\u672f\u63a2\u9488\uff0c\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u60c5\u666f\u9a71\u52a8\u7684\u4ea4\u4e92\uff0c\u5bf916\u540d\u5bb6\u5c5e\u8fdb\u884c\u8c03\u7814\uff0c\u91c7\u7528\u5f52\u7eb3\u7f16\u7801\u548c\u53cd\u5c04\u6027\u4e3b\u9898\u5206\u6790\u3002", "result": "\u63d0\u53d6\u4e86\u516d\u7c7b\u9700\u6c42\uff1a\u5b9e\u65f6\u4fe1\u606f\u83b7\u53d6\u3001\u60c5\u611f\u652f\u6301\u3001\u5b89\u5168\u62ab\u9732\u7a7a\u95f4\u3001\u5371\u673a\u7ba1\u7406\u3001\u4e2a\u6027\u5316\u670d\u52a1\u548c\u6570\u636e\u9690\u79c1\uff0c\u5e76\u5206\u6790\u4e86\u9700\u6c42\u4e0eAI\u80fd\u529b\u95f4\u7684\u5f20\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u66f4\u4e3b\u52a8\u3001\u53ef\u4fe1\u4e14\u4ee5\u5bb6\u5c5e\u4e3a\u4e2d\u5fc3\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u4f9d\u636e\u3002"}}
{"id": "2506.15488", "pdf": "https://arxiv.org/pdf/2506.15488", "abs": "https://arxiv.org/abs/2506.15488", "authors": ["Hussam Al Daas", "Grey Ballard", "Laura Grigori", "Suraj Kumar", "Kathryn Rouse", "Mathieu V\u00e9rit\u00e9"], "title": "Minimizing Communication for Parallel Symmetric Tensor Times Same Vector Computation", "categories": ["cs.DC"], "comment": "19 pages, 1 figure", "summary": "In this article, we focus on the parallel communication cost of multiplying the same vector along two modes of a $3$-dimensional symmetric tensor. This is a key computation in the higher-order power method for determining eigenpairs of a $3$-dimensional symmetric tensor and in gradient-based methods for computing a symmetric CP decomposition. We establish communication lower bounds that determine how much data movement is required to perform the specified computation in parallel. The core idea of the proof relies on extending a key geometric inequality for $3$-dimensional symmetric computations. We demonstrate that the communication lower bounds are tight by presenting an optimal algorithm where the data distribution is a natural extension of the triangle block partition scheme for symmetric matrices to 3-dimensional symmetric tensors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e863\u7ef4\u5bf9\u79f0\u5f20\u91cf\u7684\u4e24\u79cd\u6a21\u5f0f\u4e0a\u4e58\u540c\u4e00\u5411\u91cf\u7684\u5e76\u884c\u901a\u4fe1\u6210\u672c\uff0c\u5efa\u7acb\u4e86\u901a\u4fe1\u4e0b\u754c\uff0c\u5e76\u5c55\u793a\u4e86\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u5728\u9ad8\u9636\u5e42\u65b9\u6cd5\u548c\u5bf9\u79f0CP\u5206\u89e3\u7684\u68af\u5ea6\u65b9\u6cd5\u4e2d\uff0c\u4f18\u53163\u7ef4\u5bf9\u79f0\u5f20\u91cf\u7684\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u6269\u5c553\u7ef4\u5bf9\u79f0\u8ba1\u7b97\u7684\u5173\u952e\u51e0\u4f55\u4e0d\u7b49\u5f0f\uff0c\u5efa\u7acb\u901a\u4fe1\u4e0b\u754c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e09\u89d2\u5f62\u5757\u5206\u914d\u7684\u6700\u4f18\u7b97\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u901a\u4fe1\u4e0b\u754c\u662f\u7d27\u7684\uff0c\u5e76\u901a\u8fc7\u7b97\u6cd5\u9a8c\u8bc1\u4e86\u5176\u6700\u4f18\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u6269\u5c55\u7684\u4e09\u89d2\u5f62\u5757\u5206\u914d\u65b9\u6848\u80fd\u591f\u6709\u6548\u6ee1\u8db33\u7ef4\u5bf9\u79f0\u5f20\u91cf\u8ba1\u7b97\u7684\u901a\u4fe1\u9700\u6c42\u3002"}}
{"id": "2506.15129", "pdf": "https://arxiv.org/pdf/2506.15129", "abs": "https://arxiv.org/abs/2506.15129", "authors": ["Paul Murrell"], "title": "Data Verbalisation: What is Text Doing in a Data Visualisation?", "categories": ["cs.HC", "stat.OT"], "comment": "43 pages (including appendix), 20 figures", "summary": "This article discusses the role that text elements play in a data visualisation. We argue that there is a need for a simple, coherent explanation of text elements similar to the understanding that already exists for non-text elements like bars, points, and lines. We explore examples of how text is used within a data visualisation and use existing knowledge and assessment techniques to evaluate when text is effective and when it is not. The result is a framework that aims to be easy to understand and easy to apply in order to understand the purpose and effectiveness of the text elements in any data visualisation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6587\u672c\u5143\u7d20\u5728\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6613\u4e8e\u7406\u89e3\u548c\u5e94\u7528\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u6587\u672c\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u53ef\u89c6\u5316\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u975e\u6587\u672c\u5143\u7d20\uff08\u5982\u6761\u5f62\u3001\u70b9\u548c\u7ebf\uff09\uff0c\u800c\u7f3a\u4e4f\u5bf9\u6587\u672c\u5143\u7d20\u7684\u7cfb\u7edf\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6587\u672c\u5728\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u5e94\u7528\u6848\u4f8b\uff0c\u5e76\u7ed3\u5408\u73b0\u6709\u77e5\u8bc6\u548c\u8bc4\u4f30\u6280\u672f\uff0c\u7814\u7a76\u6587\u672c\u7684\u6709\u6548\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6613\u4e8e\u7406\u89e3\u548c\u5e94\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u6587\u672c\u5143\u7d20\u7684\u76ee\u7684\u548c\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u4f18\u5316\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u6587\u672c\u5143\u7d20\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.15537", "pdf": "https://arxiv.org/pdf/2506.15537", "abs": "https://arxiv.org/abs/2506.15537", "authors": ["Polina Shpilker", "Line Pouchard"], "title": "Automatic Metadata Capture and Processing for High-Performance Workflows", "categories": ["cs.DC"], "comment": null, "summary": "Modern workflows run on increasingly heterogeneous computing architectures and with this heterogeneity comes additional complexity. We aim to apply the FAIR principles for research reproducibility by developing software to collect metadata annotations for workflows run on HPC systems. We experiment with two possible formats to uniformly store these metadata, and reorganize the collected metadata to be as easy to use as possible for researchers studying their workflow performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u8f6f\u4ef6\u6536\u96c6HPC\u7cfb\u7edf\u4e0a\u5de5\u4f5c\u6d41\u7684\u5143\u6570\u636e\u6ce8\u91ca\uff0c\u5e94\u7528FAIR\u539f\u5219\u63d0\u5347\u7814\u7a76\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u5f02\u6784\u8ba1\u7b97\u67b6\u6784\u589e\u52a0\u4e86\u5de5\u4f5c\u6d41\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u7edf\u4e00\u65b9\u6cd5\u6765\u7ba1\u7406\u548c\u5206\u6790\u5de5\u4f5c\u6d41\u6027\u80fd\u3002", "method": "\u8bd5\u9a8c\u4e86\u4e24\u79cd\u5143\u6570\u636e\u5b58\u50a8\u683c\u5f0f\uff0c\u5e76\u4f18\u5316\u5176\u7ed3\u6784\u4ee5\u4fbf\u7814\u7a76\u4eba\u5458\u66f4\u5bb9\u6613\u4f7f\u7528\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8f6f\u4ef6\u5de5\u5177\uff0c\u80fd\u591f\u6536\u96c6\u5e76\u4f18\u5316HPC\u5de5\u4f5c\u6d41\u7684\u5143\u6570\u636e\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u7684\u5143\u6570\u636e\u6536\u96c6\u548c\u4f18\u5316\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5de5\u4f5c\u6d41\u6027\u80fd\u7814\u7a76\u7684\u6548\u7387\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2506.15189", "pdf": "https://arxiv.org/pdf/2506.15189", "abs": "https://arxiv.org/abs/2506.15189", "authors": ["Yikan Wang"], "title": "Accessible Gesture-Driven Augmented Reality Interaction System", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Augmented reality (AR) offers immersive interaction but remains inaccessible for users with motor impairments or limited dexterity due to reliance on precise input methods. This study proposes a gesture-based interaction system for AR environments, leveraging deep learning to recognize hand and body gestures from wearable sensors and cameras, adapting interfaces to user capabilities. The system employs vision transformers (ViTs), temporal convolutional networks (TCNs), and graph attention networks (GATs) for gesture processing, with federated learning ensuring privacy-preserving model training across diverse users. Reinforcement learning optimizes interface elements like menu layouts and interaction modes. Experiments demonstrate a 20% improvement in task completion efficiency and a 25% increase in user satisfaction for motor-impaired users compared to baseline AR systems. This approach enhances AR accessibility and scalability. Keywords: Deep learning, Federated learning, Gesture recognition, Augmented reality, Accessibility, Human-computer interaction", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u624b\u52bf\u7684AR\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8bc6\u522b\u624b\u52bf\u5e76\u4f18\u5316\u754c\u9762\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u969c\u788d\u7528\u6237\u7684\u4f7f\u7528\u6548\u7387\u548c\u6ee1\u610f\u5ea6\u3002", "motivation": "AR\u4ea4\u4e92\u4f9d\u8d56\u7cbe\u786e\u8f93\u5165\u65b9\u5f0f\uff0c\u5bf9\u8fd0\u52a8\u969c\u788d\u7528\u6237\u4e0d\u53cb\u597d\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u63d0\u5347AR\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u7ed3\u5408ViTs\u3001TCNs\u548cGATs\u5904\u7406\u624b\u52bf\u6570\u636e\uff0c\u5229\u7528\u8054\u90a6\u5b66\u4e60\u4fdd\u62a4\u9690\u79c1\uff0c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u754c\u9762\u5e03\u5c40\u548c\u4ea4\u4e92\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u7cfb\u7edf\u6bd4\u57fa\u51c6AR\u7cfb\u7edf\u63d0\u5347\u4e8620%\u7684\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u548c25%\u7684\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86AR\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u8fd0\u52a8\u969c\u788d\u7528\u6237\u3002"}}
{"id": "2506.15595", "pdf": "https://arxiv.org/pdf/2506.15595", "abs": "https://arxiv.org/abs/2506.15595", "authors": ["Kunming Zhang", "Hanlong Liao", "Guoming Tang"], "title": "LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale Heterogeneous Clusters", "categories": ["cs.DC"], "comment": "12 pages, 19 figures,7 tables", "summary": "Parallel computing with multiple GPUs has become the dominant paradigm for machine learning tasks, especially those of large language models (LLMs). To reduce the latency incurred by inter-GPU communication, a common practice for parallel tasks has been to allocate GPUs based on their physical proximity. However, this long-standing assumption has notable limitations, particularly in large-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs is irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU dispatching system based on global perspectives. To tackle the difficulty of storing massive GPU topology information, LiteGD adopts a computation-aware design that leverages a lightweight Transformer network trained on sampled data. Our customized design for network structure ensures both transferability and scalability. LiteGD also employs a bidirectional tree search approach to find the optimal GPU dispatching in the data generated in the previous step, which can identify near-optimal solutions while reducing search overhead. We implement and evaluate LiteGD in both real and simulated GPU clusters with homogeneous and heterogeneous interconnects, respectively. Experimental results demonstrate that LiteGD consistently achieves high GPU bandwidth efficacy (approximately 90\\%) across various cluster configurations and 80\\% in real-world H100 cluster, significantly outperforming conventional default and interconnect topology-aware dispatching methods, particularly in large-scale heterogeneous environments.", "AI": {"tldr": "LiteGD\u662f\u4e00\u4e2a\u52a8\u6001GPU\u8c03\u5ea6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5168\u5c40\u89c6\u89d2\u4f18\u5316\u591aGPU\u5e76\u884c\u8ba1\u7b97\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728\u5927\u89c4\u6a21\u5f02\u6784\u96c6\u7fa4\u4e2d\u663e\u8457\u63d0\u5347\u5e26\u5bbd\u5229\u7528\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u7269\u7406\u90bb\u8fd1\u6027\u7684GPU\u8c03\u5ea6\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5f02\u6784\u96c6\u7fa4\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u901a\u4fe1\u5e26\u5bbd\u5206\u5e03\u4e0d\u5747\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002LiteGD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "LiteGD\u91c7\u7528\u8f7b\u91cf\u7ea7Transformer\u7f51\u7edc\u5b58\u50a8GPU\u62d3\u6251\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u53cc\u5411\u6811\u641c\u7d22\u7b97\u6cd5\u52a8\u6001\u4f18\u5316\u8c03\u5ea6\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLiteGD\u5728\u4e0d\u540c\u96c6\u7fa4\u914d\u7f6e\u4e0b\u5e26\u5bbd\u5229\u7528\u7387\u8fbe90%\uff08\u771f\u5b9eH100\u96c6\u7fa4\u4e2d\u4e3a80%\uff09\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "LiteGD\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5168\u5c40\u4f18\u5316\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u5f02\u6784\u73af\u5883\u4e0b\u7684GPU\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2506.15293", "pdf": "https://arxiv.org/pdf/2506.15293", "abs": "https://arxiv.org/abs/2506.15293", "authors": ["Francesco Chiossi", "Julian Rasch", "Robin Welsch", "Albrecht Schmidt", "Florian Michahelles"], "title": "Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces", "categories": ["cs.HC", "cs.RO"], "comment": "9 pages", "summary": "As robots enter collaborative workspaces, ensuring mutual understanding between human workers and robotic systems becomes a prerequisite for trust, safety, and efficiency. In this position paper, we draw on the cooperation scenario of the AIMotive project in which a human and a cobot jointly perform assembly tasks to argue for a structured approach to intent communication. Building on the Situation Awareness-based Agent Transparency (SAT) framework and the notion of task abstraction levels, we propose a multidimensional design space that maps intent content (SAT1, SAT3), planning horizon (operational to strategic), and modality (visual, auditory, haptic). We illustrate how this space can guide the design of multimodal communication strategies tailored to dynamic collaborative work contexts. With this paper, we lay the conceptual foundation for a future design toolkit aimed at supporting transparent human-robot interaction in the workplace. We highlight key open questions and design challenges, and propose a shared agenda for multimodal, adaptive, and trustworthy robotic collaboration in hybrid work environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bbe\u8ba1\u7a7a\u95f4\uff08\u610f\u56fe\u5185\u5bb9\u3001\u89c4\u5212\u8303\u56f4\u548c\u6a21\u6001\uff09\u6307\u5bfc\u52a8\u6001\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u610f\u56fe\u901a\u4fe1\u8bbe\u8ba1\uff0c\u4e3a\u900f\u660e\u4eba\u673a\u4ea4\u4e92\u5960\u5b9a\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u8fdb\u5165\u534f\u4f5c\u5de5\u4f5c\u7a7a\u95f4\uff0c\u786e\u4fdd\u4eba\u4e0e\u673a\u5668\u4eba\u7cfb\u7edf\u4e4b\u95f4\u7684\u76f8\u4e92\u7406\u89e3\u6210\u4e3a\u4fe1\u4efb\u3001\u5b89\u5168\u548c\u6548\u7387\u7684\u524d\u63d0\u3002", "method": "\u57fa\u4e8e\u60c5\u5883\u611f\u77e5\u4ee3\u7406\u900f\u660e\u5ea6\uff08SAT\uff09\u6846\u67b6\u548c\u4efb\u52a1\u62bd\u8c61\u5c42\u6b21\u7684\u6982\u5ff5\uff0c\u63d0\u51fa\u4e00\u4e2a\u591a\u7ef4\u8bbe\u8ba1\u7a7a\u95f4\uff08\u610f\u56fe\u5185\u5bb9\u3001\u89c4\u5212\u8303\u56f4\u548c\u6a21\u6001\uff09\uff0c\u5e76\u6307\u5bfc\u591a\u6a21\u6001\u901a\u4fe1\u7b56\u7565\u7684\u8bbe\u8ba1\u3002", "result": "\u8bba\u6587\u4e3a\u672a\u6765\u7684\u900f\u660e\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u5de5\u5177\u5305\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u5171\u4eab\u8bae\u7a0b\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5173\u952e\u5f00\u653e\u95ee\u9898\u548c\u8bbe\u8ba1\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u5728\u6df7\u5408\u5de5\u4f5c\u73af\u5883\u4e2d\u5b9e\u73b0\u591a\u6a21\u6001\u3001\u81ea\u9002\u5e94\u548c\u53ef\u4fe1\u8d56\u7684\u673a\u5668\u4eba\u534f\u4f5c\u7684\u5171\u4eab\u8bae\u7a0b\u3002"}}
{"id": "2506.14911", "pdf": "https://arxiv.org/pdf/2506.14911", "abs": "https://arxiv.org/abs/2506.14911", "authors": ["Ganyu Wang", "Boyu Wang", "Bin Gu", "Charles Ling"], "title": "Event-Driven Online Vertical Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": "Published as a conference paper at ICLR 2025", "summary": "Online learning is more adaptable to real-world scenarios in Vertical Federated Learning (VFL) compared to offline learning. However, integrating online learning into VFL presents challenges due to the unique nature of VFL, where clients possess non-intersecting feature sets for the same sample. In real-world scenarios, the clients may not receive data streaming for the disjoint features for the same entity synchronously. Instead, the data are typically generated by an \\emph{event} relevant to only a subset of clients. We are the first to identify these challenges in online VFL, which have been overlooked by previous research. To address these challenges, we proposed an event-driven online VFL framework. In this framework, only a subset of clients were activated during each event, while the remaining clients passively collaborated in the learning process. Furthermore, we incorporated \\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by online learning problems with non-convex models within a non-stationary environment. We conducted a comprehensive regret analysis of our proposed framework, specifically examining the DLR under non-convex conditions with event-driven online VFL. Extensive experiments demonstrated that our proposed framework was more stable than the existing online VFL framework under non-stationary data conditions while also significantly reducing communication and computation costs.", "AI": {"tldr": "\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u4e2d\u5728\u7ebf\u5b66\u4e60\u66f4\u9002\u5e94\u73b0\u5b9e\u573a\u666f\uff0c\u4f46\u5f02\u6b65\u6570\u636e\u6d41\u548c\u4e8b\u4ef6\u9a71\u52a8\u7279\u6027\u5e26\u6765\u6311\u6218\u3002\u4f5c\u8005\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u7684\u5728\u7ebfVFL\u6846\u67b6\uff0c\u52a8\u6001\u5c40\u90e8\u9057\u61be\uff08DLR\uff09\u5e94\u5bf9\u975e\u51f8\u4e0e\u975e\u7a33\u6001\u73af\u5883\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u7a33\u5b9a\u6027\u548c\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2dVFL\u5ba2\u6237\u7aef\u7684\u6570\u636e\u6d41\u5e38\u5f02\u6b65\u4e14\u4e8b\u4ef6\u9a71\u52a8\uff0c\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u8fd9\u4e00\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u7684\u5728\u7ebfVFL\u6846\u67b6\u4ee5\u89e3\u51b3\u5f02\u6b65\u6027\u548c\u975e\u7a33\u6001\u6027\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u9a71\u52a8\u7684\u5728\u7ebfVFL\u6846\u67b6\uff0c\u4ec5\u6fc0\u6d3b\u76f8\u5173\u5ba2\u6237\u7aef\uff0c\u5176\u4f59\u88ab\u52a8\u534f\u4f5c\uff1b\u5f15\u5165\u52a8\u6001\u5c40\u90e8\u9057\u61be\uff08DLR\uff09\u5904\u7406\u975e\u51f8\u4e0e\u975e\u7a33\u6001\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6846\u67b6\u5728\u975e\u7a33\u6001\u6570\u636e\u4e0b\u66f4\u7a33\u5b9a\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u4e8b\u4ef6\u9a71\u52a8\u7684\u5728\u7ebfVFL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u5f02\u6b65\u6570\u636e\u6d41\u95ee\u9898\uff0c\u7ed3\u5408DLR\u63d0\u5347\u4e86\u975e\u7a33\u6001\u73af\u5883\u4e0b\u7684\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2506.15294", "pdf": "https://arxiv.org/pdf/2506.15294", "abs": "https://arxiv.org/abs/2506.15294", "authors": ["Jonas Lau", "Annie Tran"], "title": "UXR Point of View on Product Feature Prioritization Prior To Multi-Million Engineering Commitments", "categories": ["cs.HC"], "comment": null, "summary": "This paper discusses a popular UX research activity, feature prioritization, using the User Experience Research Point of View (UXR PoV) Playbook framework. We describe an application of multinomial logistic regression, frequently marketed as MaxDiff, for prioritizing product features in consumer product development. It addresses challenges of traditional surveying techniques. We propose a solution using MaxDiff to generate a reliable preference list with a reasonable sample size. We also adapt the MaxDiff method to reduce the number of survey responses in half, making it less tedious from the survey takers' perspective. We present a case study using the adapted MaxDiff method for tablet feature prioritization research involving users with disabilities.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528UXR PoV Playbook\u6846\u67b6\u8fdb\u884c\u529f\u80fd\u4f18\u5148\u7ea7\u6392\u5e8f\u7684UX\u7814\u7a76\u65b9\u6cd5\uff0c\u63d0\u51fa\u7528MaxDiff\uff08\u591a\u9879\u903b\u8f91\u56de\u5f52\uff09\u89e3\u51b3\u4f20\u7edf\u8c03\u67e5\u6280\u672f\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u8c03\u67e5\u6280\u672f\u5728\u529f\u80fd\u4f18\u5148\u7ea7\u6392\u5e8f\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u4e14\u6837\u672c\u91cf\u5408\u7406\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u7814\u7a76\u7684\u6548\u7387\u3002", "method": "\u91c7\u7528MaxDiff\u65b9\u6cd5\u751f\u6210\u53ef\u9760\u7684\u529f\u80fd\u504f\u597d\u5217\u8868\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u65b9\u6cd5\u51cf\u5c11\u4e00\u534a\u7684\u95ee\u5377\u56de\u7b54\u91cf\uff0c\u51cf\u8f7b\u53d7\u8bbf\u8005\u8d1f\u62c5\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8c03\u6574\u540e\u7684MaxDiff\u65b9\u6cd5\u80fd\u6709\u6548\u7528\u4e8e\u7528\u6237\uff08\u5305\u62ec\u6b8b\u969c\u4eba\u58eb\uff09\u7684\u529f\u80fd\u4f18\u5148\u7ea7\u7814\u7a76\u3002", "conclusion": "MaxDiff\u65b9\u6cd5\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7528\u6237\u53cb\u597d\u7684\u529f\u80fd\u4f18\u5148\u7ea7\u6392\u5e8f\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u6d88\u8d39\u4ea7\u54c1\u5f00\u53d1\u3002"}}
{"id": "2506.15314", "pdf": "https://arxiv.org/pdf/2506.15314", "abs": "https://arxiv.org/abs/2506.15314", "authors": ["Jason Dong", "Anna Wu"], "title": "Case Study for Developing a UXR Point of View for FinOps Product Innovation", "categories": ["cs.HC"], "comment": null, "summary": "In the dynamic landscape of Cloud financial management, we are sharing a case study exploring the development of a User Experience Research (UXR) Point of View (PoV) to drive FinOps product innovation. We demonstrate how qualitative and quantitative research methods working together to navigate the challenges of understanding customer needs, aligning cross-functional teams, and prioritizing limited resources. Through a multi-phased research approach, the research team identifies opportunities, quantifies pain points, and segments diverse customer cohorts. This culminated in a UXR PoV that informed the creation of a differentiated product strategy, a 'one-stop shop' dashboard empowering FinOps practitioners with actionable insights and tools. This case study highlights the power of mixed-methods research in uncovering actionable insights that drive impactful product innovation.", "AI": {"tldr": "\u6848\u4f8b\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u6237\u4f53\u9a8c\u7814\u7a76\uff08UXR\uff09\u89c2\u70b9\u5728\u63a8\u52a8FinOps\u4ea7\u54c1\u521b\u65b0\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u5408\u5b9a\u6027\u4e0e\u5b9a\u91cf\u65b9\u6cd5\u89e3\u51b3\u5ba2\u6237\u9700\u6c42\u7406\u89e3\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e91\u8d22\u52a1\u7ba1\u7406\u4e2d\u5ba2\u6237\u9700\u6c42\u591a\u6837\u5316\u548c\u8d44\u6e90\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u7814\u7a76\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u8bc6\u522b\u673a\u4f1a\u3001\u91cf\u5316\u75db\u70b9\u5e76\u7ec6\u5206\u5ba2\u6237\u7fa4\u4f53\u3002", "result": "\u6700\u7ec8\u5f62\u6210UXR\u89c2\u70b9\uff0c\u6307\u5bfc\u5f00\u53d1\u4e86\u4e00\u7ad9\u5f0f\u4eea\u8868\u76d8\uff0c\u4e3aFinOps\u4ece\u4e1a\u8005\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u548c\u6d1e\u5bdf\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u80fd\u6709\u6548\u63ed\u793a\u9a71\u52a8\u4ea7\u54c1\u521b\u65b0\u7684\u5173\u952e\u6d1e\u5bdf\u3002"}}
{"id": "2506.15264", "pdf": "https://arxiv.org/pdf/2506.15264", "abs": "https://arxiv.org/abs/2506.15264", "authors": ["M\u00e9lanie Cambus", "Darya Melnyk", "Tijana Milentijevi\u0107", "Stefan Schmid"], "title": "Centroid Approximation for Byzantine-Tolerant Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": "19 pages, 10 figures", "summary": "Federated learning allows each client to keep its data locally when training machine learning models in a distributed setting. Significant recent research established the requirements that the input must satisfy in order to guarantee convergence of the training loop. This line of work uses averaging as the aggregation rule for the training models. In particular, we are interested in whether federated learning is robust to Byzantine behavior, and observe and investigate a tradeoff between the average/centroid and the validity conditions from distributed computing. We show that the various validity conditions alone do not guarantee a good approximation of the average. Furthermore, we show that reaching good approximation does not give good results in experimental settings due to possible Byzantine outliers. Our main contribution is the first lower bound of $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ on the centroid approximation under box validity that is often considered in the literature, where $n$ is the number of clients, $t$ the upper bound on the number of Byzantine faults, and $d$ is the dimension of the machine learning model. We complement this lower bound by an upper bound of $2\\min\\{n,\\sqrt{d}\\}$, by providing a new analysis for the case $n<d$. In addition, we present a new algorithm that achieves a $\\sqrt{2d}$-approximation under convex validity, which also proves that the existing lower bound in the literature is tight. We show that all presented bounds can also be achieved in the distributed peer-to-peer setting. We complement our analytical results with empirical evaluations in federated stochastic gradient descent and federated averaging settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u5b58\u5728\u62dc\u5360\u5ead\u884c\u4e3a\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u5173\u4e8e\u4e2d\u5fc3\u70b9\u8fd1\u4f3c\u7684\u65b0\u4e0b\u754c\u548c\u4e0a\u754c\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u7b97\u6cd5\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u5982\u4f55\u5e94\u5bf9\u62dc\u5360\u5ead\u884c\u4e3a\uff0c\u786e\u4fdd\u6a21\u578b\u7684\u6536\u655b\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63d0\u51fa\u4e86\u4e2d\u5fc3\u70b9\u8fd1\u4f3c\u7684\u4e0b\u754c\u548c\u4e0a\u754c\uff0c\u5e76\u8bbe\u8ba1\u65b0\u7b97\u6cd5\u5728\u51f8\u6709\u6548\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u8fd1\u4f3c\u3002", "result": "\u9996\u6b21\u8bc1\u660e\u4e86\u5728\u6587\u732e\u5e38\u89c1\u7684\u76d2\u6709\u6548\u6027\u6761\u4ef6\u4e0b\u4e2d\u5fc3\u70b9\u8fd1\u4f3c\u7684\u4e0b\u754c\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u65b0\u7684\u4e0a\u754c\u548c\u7b97\u6cd5\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u5173\u4e8e\u8054\u90a6\u5b66\u4e60\u62dc\u5360\u9c81\u68d2\u6027\u7684\u65b0\u7406\u8bba\u7ed3\u679c\u548c\u7b97\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9a8c\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.15325", "pdf": "https://arxiv.org/pdf/2506.15325", "abs": "https://arxiv.org/abs/2506.15325", "authors": ["Festus Adedoyin", "Huseyin Dogan"], "title": "Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook", "categories": ["cs.HC"], "comment": null, "summary": "Advancements in Artificial Intelligence (AI) have significantly transformed the financial industry, enabling the development of more personalised and adaptable financial products and services. This research paper explores various instances where Human-Centred AI (HCAI) has facilitated these advancements, drawing from contemporary studies and industry progress. The paper examines how the application of HCAI-powered data analytics, machine learning, and natural language processing enables financial institutions to gain a deeper understanding of their customers' unique needs, preferences, and behavioural patterns. This, in turn, allows for the creation of tailored financial solutions that address individual consumer requirements, ultimately enhancing overall user experience and satisfaction. Additionally, the study highlights the integration of AI-powered robo-advisory services, which offer customised investment recommendations and portfolio management tailored to diverse risk profiles and investment goals. Moreover, the paper underscores the role of AI in strengthening fraud detection, risk assessment, and regulatory compliance, leading to a more secure and adaptable financial landscape. The findings of this research demonstrate the substantial impact of Human-Centred AI on the financial industry, offering a strategic framework for financial institutions to leverage these technologies. By incorporating a User Experience Research (UXR) Point of View (PoV), financial institutions can ensure that AI-driven solutions align with user needs and business objectives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\uff08HCAI\uff09\u5728\u91d1\u878d\u884c\u4e1a\u4e2d\u7684\u4e2a\u6027\u5316\u670d\u52a1\u548c\u4ea7\u54c1\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u53ca\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5c55\u793aHCAI\u5982\u4f55\u901a\u8fc7\u6570\u636e\u5206\u6790\u3001\u673a\u5668\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u6280\u672f\uff0c\u5e2e\u52a9\u91d1\u878d\u673a\u6784\u66f4\u597d\u5730\u7406\u89e3\u5ba2\u6237\u9700\u6c42\u5e76\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5206\u6790HCAI\u5728\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u5408\u884c\u4e1a\u5b9e\u4f8b\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u4e2a\u6027\u5316\u91d1\u878d\u89e3\u51b3\u65b9\u6848\u3001\u6295\u8d44\u987e\u95ee\u670d\u52a1\u548c\u98ce\u9669\u7ba1\u7406\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0HCAI\u663e\u8457\u63d0\u5347\u4e86\u91d1\u878d\u4ea7\u54c1\u7684\u4e2a\u6027\u5316\u7a0b\u5ea6\uff0c\u6539\u5584\u4e86\u7528\u6237\u4f53\u9a8c\uff0c\u5e76\u589e\u5f3a\u4e86\u6b3a\u8bc8\u68c0\u6d4b\u548c\u98ce\u9669\u7ba1\u7406\u80fd\u529b\u3002", "conclusion": "HCAI\u4e3a\u91d1\u878d\u673a\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6218\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u7814\u7a76\u89c6\u89d2\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5c06AI\u6280\u672f\u4e0e\u7528\u6237\u9700\u6c42\u548c\u4e1a\u52a1\u76ee\u6807\u5bf9\u9f50\u3002"}}
{"id": "2506.15626", "pdf": "https://arxiv.org/pdf/2506.15626", "abs": "https://arxiv.org/abs/2506.15626", "authors": ["Vincent Roca", "Marc Tommasi", "Paul Andrey", "Aur\u00e9lien Bellet", "Markus D. Schirmer", "Hilde Henon", "Laurent Puy", "Julien Ramon", "Gr\u00e9gory Kuchcinski", "Martin Bretzner", "Renaud Lopes"], "title": "Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "$\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a neuroimaging biomarker reflecting brain health. However, training robust BrainAGE models requires large datasets, often restricted by privacy concerns. This study evaluates the performance of federated learning (FL) for BrainAGE estimation in ischemic stroke patients treated with mechanical thrombectomy, and investigates its association with clinical phenotypes and functional outcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients across 16 hospital centers. We implemented standard machine learning and deep learning models for BrainAGE estimates under three data management strategies: centralized learning (pooled data), FL (local training at each site), and single-site learning. We reported prediction errors and examined associations between BrainAGE and vascular risk factors (e.g., diabetes mellitus, hypertension, smoking), as well as functional outcomes at three months post-stroke. Logistic regression evaluated BrainAGE's predictive value for these outcomes, adjusting for age, sex, vascular risk factors, stroke severity, time between MRI and arterial puncture, prior intravenous thrombolysis, and recanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate predictions, FL consistently outperformed single-site models. BrainAGE was significantly higher in patients with diabetes mellitus across all models. Comparisons between patients with good and poor functional outcomes, and multivariate predictions of these outcomes showed the significance of the association between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data centralization. The strong association between BrainAGE, vascular risk factors, and post-stroke recovery highlights its potential for prognostic modeling in stroke care.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u7f3a\u8840\u6027\u4e2d\u98ce\u60a3\u8005\u4e2d\u7528\u4e8eBrainAGE\u4f30\u8ba1\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u4e0e\u4e34\u5e8a\u8868\u578b\u53ca\u529f\u80fd\u7ed3\u679c\u7684\u5173\u8054\uff0c\u7ed3\u679c\u8868\u660eFL\u5728\u4e0d\u96c6\u4e2d\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u63d0\u4f9b\u51c6\u786e\u9884\u6d4b\u3002", "motivation": "BrainAGE\u662f\u53cd\u6620\u5927\u8111\u5065\u5eb7\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u5176\u8bad\u7ec3\u901a\u5e38\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u4e14\u9690\u79c1\u95ee\u9898\u9650\u5236\u4e86\u6570\u636e\u5171\u4eab\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22FL\u7684\u53ef\u884c\u6027\u53ca\u5176\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u4f7f\u75281674\u540d\u4e2d\u98ce\u60a3\u8005\u7684FLAIR\u5927\u8111\u56fe\u50cf\uff0c\u91c7\u7528\u96c6\u4e2d\u5b66\u4e60\u3001FL\u548c\u5355\u70b9\u5b66\u4e60\u4e09\u79cd\u7b56\u7565\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5206\u6790BrainAGE\u4e0e\u8840\u7ba1\u98ce\u9669\u56e0\u7d20\u53ca\u529f\u80fd\u7ed3\u679c\u7684\u5173\u7cfb\u3002", "result": "FL\u8868\u73b0\u4f18\u4e8e\u5355\u70b9\u5b66\u4e60\uff0c\u4e14BrainAGE\u4e0e\u7cd6\u5c3f\u75c5\u7b49\u8840\u7ba1\u98ce\u9669\u56e0\u7d20\u53ca\u4e2d\u98ce\u540e\u529f\u80fd\u6062\u590d\u663e\u8457\u76f8\u5173\u3002", "conclusion": "FL\u65e0\u9700\u6570\u636e\u96c6\u4e2d\u5373\u53ef\u63d0\u4f9b\u51c6\u786e\u9884\u6d4b\uff0cBrainAGE\u5728\u8111\u5352\u4e2d\u9884\u540e\u6a21\u578b\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.15332", "pdf": "https://arxiv.org/pdf/2506.15332", "abs": "https://arxiv.org/abs/2506.15332", "authors": ["Patricia Diaz"], "title": "Building Blocks of a User Experience Research Point of View", "categories": ["cs.HC"], "comment": null, "summary": "This paper presents three User Experience Research (UXR) perspectives based on data, evidence and insights - known as Point of View (POV) - showcasing how the strategies and methods of building a POV work in an enterprise setting. The POV are: 1. Smart Visuals: Use AI to extract and translate text from visuals in videos (2019). 2. Assessable Code Editor: Focus on direct AI-feedback to the learner as it is the loop that requires the least effort for the highest impact(2023). 3. Opportunity Landscape: Identify high-impact opportunities at the intersection of emergent technical capabilities that unlock novel approaches to critical user needs while addressing business strategic priorities (2019). They all seemed far-fetched and went against common practice. All were adopted and had long-lasting impact.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u57fa\u4e8e\u6570\u636e\u3001\u8bc1\u636e\u548c\u6d1e\u5bdf\u7684\u7528\u6237\u4f53\u9a8c\u7814\u7a76\uff08UXR\uff09\u89c6\u89d2\uff08POV\uff09\uff0c\u5c55\u793a\u4e86\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u6784\u5efaPOV\u7684\u7b56\u7565\u548c\u65b9\u6cd5\u5982\u4f55\u53d1\u6325\u4f5c\u7528\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5229\u7528\u6570\u636e\u548cAI\u6280\u672f\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u8fd9\u4e9b\u770b\u4f3c\u8d85\u524d\u7684POV\u5982\u4f55\u88ab\u91c7\u7eb3\u5e76\u4ea7\u751f\u6301\u4e45\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u5177\u4f53\u6848\u4f8b\uff08\u667a\u80fd\u89c6\u89c9\u3001\u53ef\u8bbf\u95ee\u4ee3\u7801\u7f16\u8f91\u5668\u548c\u673a\u4f1a\u666f\u89c2\uff09\u5c55\u793aPOV\u7684\u6784\u5efa\u548c\u5e94\u7528\uff0c\u7ed3\u5408AI\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u7387\u7684\u7528\u6237\u53cd\u9988\u548c\u9700\u6c42\u6316\u6398\u3002", "result": "\u6240\u6709\u63d0\u51fa\u7684POV\u5747\u88ab\u4f01\u4e1a\u91c7\u7eb3\uff0c\u5e76\u5b9e\u73b0\u4e86\u957f\u671f\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u770b\u4f3c\u8d85\u524d\u7684POV\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u548cAI\u6280\u672f\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u7528\u6237\u4f53\u9a8c\u6539\u8fdb\u548c\u4e1a\u52a1\u4ef7\u503c\u63d0\u5347\u3002"}}
{"id": "2506.15468", "pdf": "https://arxiv.org/pdf/2506.15468", "abs": "https://arxiv.org/abs/2506.15468", "authors": ["Ryota Okumura", "Tadahiro Taniguchi", "Akira Taniguchi", "Yoshinobu Hagiwara"], "title": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose co-creative learning as a novel paradigm where humans and AI, i.e., biological and artificial agents, mutually integrate their partial perceptual information and knowledge to construct shared external representations, a process we interpret as symbol emergence. Unlike traditional AI teaching based on unilateral knowledge transfer, this addresses the challenge of integrating information from inherently different modalities. We empirically test this framework using a human-AI interaction model based on the Metropolis-Hastings naming game (MHNG), a decentralized Bayesian inference mechanism. In an online experiment, 69 participants played a joint attention naming game (JA-NG) with one of three computer agent types (MH-based, always-accept, or always-reject) under partial observability. Results show that human-AI pairs with an MH-based agent significantly improved categorization accuracy through interaction and achieved stronger convergence toward a shared sign system. Furthermore, human acceptance behavior aligned closely with the MH-derived acceptance probability. These findings provide the first empirical evidence for co-creative learning emerging in human-AI dyads via MHNG-based interaction. This suggests a promising path toward symbiotic AI systems that learn with humans, rather than from them, by dynamically aligning perceptual experiences, opening a new venue for symbiotic AI alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5171\u540c\u521b\u9020\u6027\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u4eba\u7c7b\u4e0eAI\u7684\u4e92\u52a8\u6784\u5efa\u5171\u4eab\u5916\u90e8\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u6574\u5408\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edfAI\u6559\u5b66\u57fa\u4e8e\u5355\u5411\u77e5\u8bc6\u4f20\u9012\uff0c\u96be\u4ee5\u6574\u5408\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u5171\u540c\u521b\u9020\u6027\u5b66\u4e60\uff0c\u4ee5\u5f25\u5408\u4eba\u7c7b\u4e0eAI\u4e4b\u95f4\u7684\u611f\u77e5\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eMetropolis-Hastings\u547d\u540d\u6e38\u620f\uff08MHNG\uff09\u7684\u5206\u6563\u5f0f\u8d1d\u53f6\u65af\u63a8\u7406\u673a\u5236\uff0c\u8fdb\u884c\u4eba\u7c7b-AI\u4e92\u52a8\u5b9e\u9a8c\u300269\u540d\u53c2\u4e0e\u8005\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u4e0e\u4e09\u79cd\u8ba1\u7b97\u673a\u4ee3\u7406\u7c7b\u578b\u4e4b\u4e00\u4e92\u52a8\u3002", "result": "\u57fa\u4e8eMHNG\u7684\u4ee3\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u5171\u4eab\u7b26\u53f7\u7cfb\u7edf\u6536\u655b\u3002\u4eba\u7c7b\u63a5\u53d7\u884c\u4e3a\u4e0eMHNG\u63a8\u5bfc\u7684\u63a5\u53d7\u6982\u7387\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4eba\u7c7b-AI\u5171\u540c\u521b\u9020\u6027\u5b66\u4e60\u63d0\u4f9b\u4e86\u9996\u4e2a\u5b9e\u8bc1\u8bc1\u636e\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u52a8\u6001\u5bf9\u9f50\u611f\u77e5\u4f53\u9a8c\u5b9e\u73b0\u5171\u751fAI\u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.15497", "pdf": "https://arxiv.org/pdf/2506.15497", "abs": "https://arxiv.org/abs/2506.15497", "authors": ["Changzeng Fu"], "title": "Foundation of Affective Computing and Interaction", "categories": ["cs.HC"], "comment": null, "summary": "This book provides a comprehensive exploration of affective computing and human-computer interaction technologies. It begins with the historical development and basic concepts of human-computer interaction, delving into the technical frameworks and practical applications of emotional computing, visual interaction, voice interaction, brain-computer interfaces, physiological electrical signal analysis, and social robotics. The book covers a wide range of topics, including the psychological and neuroscience foundations of emotion, multimodal emotion recognition, emotional expression mechanisms, and the principles of brain-computer interfaces.\n  Key technologies such as affective computing based on discrete emotion theory and dimensional models, visual perception principles, speech recognition and synthesis, EEG signal acquisition and processing, and multimodal emotion recognition are explained in detail. This book also addresses the technical challenges in the field, including multimodal data fusion, privacy and security, and ethical considerations in human-machine relationships. It discusses the applications of these technologies across various domains such as education, healthcare, entertainment, and intelligent assistance.\n  Looking to the future, the book anticipates trends such as the deep integration of artificial intelligence with emotion recognition, the advancement of multimodal interaction technologies, and the development of more personalized and adaptive emotion recognition systems. It emphasizes the importance of balancing technological innovation with ethical considerations to ensure the responsible development and application of affective computing technologies.", "AI": {"tldr": "\u672c\u4e66\u5168\u9762\u63a2\u8ba8\u4e86\u60c5\u611f\u8ba1\u7b97\u4e0e\u4eba\u673a\u4ea4\u4e92\u6280\u672f\uff0c\u6db5\u76d6\u5386\u53f2\u53d1\u5c55\u3001\u7406\u8bba\u57fa\u7840\u3001\u5173\u952e\u6280\u672f\u3001\u5e94\u7528\u9886\u57df\u53ca\u672a\u6765\u8d8b\u52bf\u3002", "motivation": "\u65e8\u5728\u63d0\u4f9b\u5173\u4e8e\u60c5\u611f\u8ba1\u7b97\u4e0e\u4eba\u673a\u4ea4\u4e92\u7684\u5168\u9762\u77e5\u8bc6\uff0c\u4fc3\u8fdb\u6280\u672f\u53d1\u5c55\u5e76\u89e3\u51b3\u4f26\u7406\u6311\u6218\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u6001\u6280\u672f\uff08\u5982\u89c6\u89c9\u3001\u8bed\u97f3\u3001\u8111\u673a\u63a5\u53e3\uff09\u548c\u5fc3\u7406\u5b66\u3001\u795e\u7ecf\u79d1\u5b66\u7406\u8bba\uff0c\u5206\u6790\u60c5\u611f\u8bc6\u522b\u4e0e\u4ea4\u4e92\u3002", "result": "\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u60c5\u611f\u8ba1\u7b97\u7684\u5173\u952e\u6280\u672f\u3001\u5e94\u7528\u573a\u666f\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u5f3a\u8c03\u6280\u672f\u521b\u65b0\u4e0e\u4f26\u7406\u5e73\u8861\u7684\u91cd\u8981\u6027\uff0c\u63a8\u52a8\u60c5\u611f\u8ba1\u7b97\u6280\u672f\u7684\u8d1f\u8d23\u4efb\u53d1\u5c55\u4e0e\u5e94\u7528\u3002"}}
{"id": "2506.15512", "pdf": "https://arxiv.org/pdf/2506.15512", "abs": "https://arxiv.org/abs/2506.15512", "authors": ["Wenqi Guan", "Yang Fang"], "title": "Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Large Language Models have brought a radical change in the process of remote learning students, among other aspects of educative activities. Current retrieval of remote learning resources lacks depth in contextual meaning that provides comprehensive information on complex student queries. This work proposes a novel approach to enhancing remote learning retrieval by integrating GPT-based models within the LangChain framework. We achieve this system in a more intuitive and productive manner using CoT reasoning and prompt engineering. The framework we propose puts much emphasis on increasing the precision and relevance of the retrieval results to return comprehensive and contextually enriched explanations and resources that best suit each student's needs. We also assess the effectiveness of our approach against paradigmatic LLMs and report improvements in user satisfaction and learning outcomes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPT\u548cLangChain\u6846\u67b6\u7684\u8fdc\u7a0b\u5b66\u4e60\u8d44\u6e90\u68c0\u7d22\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7CoT\u63a8\u7406\u548c\u63d0\u793a\u5de5\u7a0b\u63d0\u5347\u68c0\u7d22\u7ed3\u679c\u7684\u7cbe\u51c6\u5ea6\u548c\u76f8\u5173\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u8fdc\u7a0b\u5b66\u4e60\u8d44\u6e90\u7684\u68c0\u7d22\u7f3a\u4e4f\u6df1\u5ea6\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b66\u751f\u5bf9\u590d\u6742\u67e5\u8be2\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528GPT\u6a21\u578b\u548cLangChain\u6846\u67b6\uff0c\u7ed3\u5408CoT\u63a8\u7406\u4e0e\u63d0\u793a\u5de5\u7a0b\uff0c\u4f18\u5316\u68c0\u7d22\u7cfb\u7edf\u3002", "result": "\u4e0e\u73b0\u6709LLM\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u5b66\u4e60\u6548\u679c\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u66f4\u7cbe\u51c6\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u8fdc\u7a0b\u5b66\u4e60\u8d44\u6e90\uff0c\u6ee1\u8db3\u5b66\u751f\u4e2a\u6027\u5316\u9700\u6c42\u3002"}}
{"id": "2506.15525", "pdf": "https://arxiv.org/pdf/2506.15525", "abs": "https://arxiv.org/abs/2506.15525", "authors": ["Isabella Pu", "Prerna Ravi", "Linh Dieu Dinh", "Chelsea Joe", "Caitlin Ogoe", "Zixuan Li", "Cynthia Breazeal", "Anastasia K. Ostrowski"], "title": "\"How can we learn and use AI at the same time?:: Participatory Design of GenAI with High School Students", "categories": ["cs.HC"], "comment": "Copyright protected by ACM, 17 pages, 5 figures, 2 tables, in proceedings of 24th annual ACM Interaction Design and Children Conference (IDC 2025)", "summary": "As generative AI (GenAI) emerges as a transformative force, clear understanding of high school students' perspectives is essential for GenAI's meaningful integration in high school environments. In this work, we draw insights from a participatory design workshop where we engaged 17 high school students -- a group rarely involved in prior research in this area -- through the design of novel GenAI tools and school policies addressing their key concerns. Students identified challenges and developed solutions outlining their ideal features in GenAI tools, appropriate school use, and regulations. These centered around the problem spaces of combating bias & misinformation, tackling crime & plagiarism, preventing over-reliance on AI, and handling false accusations of academic dishonesty. Building on our participants' underrepresented perspectives, we propose new guidelines targeted at educational technology designers for development of GenAI technologies in high schools. We also argue for further incorporation of student voices in development of AI policies in their schools.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u9ad8\u4e2d\u751f\u5bf9\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u7684\u770b\u6cd5\uff0c\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u5de5\u4f5c\u574a\u6536\u96c6\u5b66\u751f\u610f\u89c1\uff0c\u63d0\u51fa\u9488\u5bf9\u6559\u80b2\u6280\u672f\u8bbe\u8ba1\u7684\u6307\u5357\uff0c\u5e76\u547c\u5401\u5728\u5b66\u6821AI\u653f\u7b56\u5236\u5b9a\u4e2d\u66f4\u591a\u878d\u5165\u5b66\u751f\u58f0\u97f3\u3002", "motivation": "\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5bf9\u9ad8\u4e2d\u73af\u5883\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u5173\u6ce8\u5b66\u751f\u7684\u89c2\u70b9\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e86\u89e3\u5b66\u751f\u7684\u9700\u6c42\u548c\u62c5\u5fe7\u3002", "method": "\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c17\u540d\u9ad8\u4e2d\u751f\u53c2\u4e0e\u4e86\u8bbe\u8ba1GenAI\u5de5\u5177\u548c\u5b66\u6821\u653f\u7b56\u7684\u8ba8\u8bba\uff0c\u63d0\u51fa\u95ee\u9898\u548c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b66\u751f\u5173\u6ce8\u7684\u56db\u5927\u95ee\u9898\u7a7a\u95f4\u5305\u62ec\uff1a\u504f\u89c1\u4e0e\u9519\u8bef\u4fe1\u606f\u3001\u72af\u7f6a\u4e0e\u6284\u88ad\u3001\u8fc7\u5ea6\u4f9d\u8d56AI\u3001\u5b66\u672f\u4e0d\u8bda\u4fe1\u7684\u8bef\u5224\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u9488\u5bf9\u6559\u80b2\u6280\u672f\u8bbe\u8ba1\u7684\u6307\u5357\u3002", "conclusion": "\u5e94\u66f4\u591a\u5730\u7eb3\u5165\u5b66\u751f\u58f0\u97f3\u4ee5\u4f18\u5316GenAI\u5de5\u5177\u548c\u653f\u7b56\uff0c\u4e3a\u9ad8\u4e2d\u73af\u5883\u63d0\u4f9b\u66f4\u6709\u9488\u5bf9\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.13776", "pdf": "https://arxiv.org/pdf/2506.13776", "abs": "https://arxiv.org/abs/2506.13776", "authors": ["Kevin L. Wei", "Patricia Paskov", "Sunishchal Dev", "Michael J. Byun", "Anka Reuel", "Xavier Roberts-Gaal", "Rachel Calcott", "Evie Coxon", "Chinmay Deshpande"], "title": "Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations", "categories": ["cs.AI", "cs.CY", "cs.HC"], "comment": "A version of this paper has been accepted to ICML 2025 as a position paper (spotlight), with the title: \"Position: Human Baselines in Model Evaluations Need Rigor and Transparency (With Recommendations & Reporting Checklist).\"", "summary": "In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve \"super-human\" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u4e3b\u5f20\u5728\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u4e2d\u9700\u8981\u66f4\u4e25\u683c\u548c\u900f\u660e\u7684\u4eba\u7c7b\u57fa\u7ebf\uff0c\u4ee5\u66f4\u597d\u5730\u6bd4\u8f83\u4eba\u7c7b\u4e0eAI\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u76f8\u5173\u5efa\u8bae\u548c\u62a5\u544a\u6e05\u5355\u3002", "motivation": "\u4eba\u7c7b\u6027\u80fd\u57fa\u7ebf\u5bf9\u673a\u5668\u5b66\u4e60\u793e\u533a\u3001\u4e0b\u6e38\u7528\u6237\u548c\u653f\u7b56\u5236\u5b9a\u8005\u7406\u89e3AI\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4e25\u8c28\u6027\u548c\u6587\u6863\u5316\u65b9\u9762\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5143\u7efc\u8ff0\u6d4b\u91cf\u7406\u8bba\u548cAI\u8bc4\u4f30\u6587\u732e\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u3001\u6267\u884c\u548c\u62a5\u544a\u4eba\u7c7b\u57fa\u7ebf\u7684\u6846\u67b6\u548c\u5efa\u8bae\uff0c\u5e76\u8f6c\u5316\u4e3a\u6e05\u5355\u5f62\u5f0f\u3002", "result": "\u4f5c\u8005\u7528\u6e05\u5355\u7cfb\u7edf\u5ba1\u67e5\u4e86115\u9879\u4eba\u7c7b\u57fa\u7ebf\u7814\u7a76\uff0c\u53d1\u73b0\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u6e05\u5355\u4e5f\u80fd\u5e2e\u52a9\u7814\u7a76\u8005\u8fdb\u884c\u66f4\u89c4\u8303\u7684\u57fa\u7ebf\u7814\u7a76\u548c\u7ed3\u679c\u62a5\u544a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u65e8\u5728\u63a8\u52a8\u66f4\u4e25\u683c\u7684AI\u8bc4\u4f30\u5b9e\u8df5\uff0c\u4ee5\u66f4\u597d\u5730\u670d\u52a1\u4e8e\u7814\u7a76\u793e\u533a\u548c\u653f\u7b56\u5236\u5b9a\u8005\u3002"}}
{"id": "2506.14774", "pdf": "https://arxiv.org/pdf/2506.14774", "abs": "https://arxiv.org/abs/2506.14774", "authors": ["Burcu Sayin", "Ipek Baris Schlicht", "Ngoc Vo Hong", "Sara Allievi", "Jacopo Staiano", "Pasquale Minervini", "Andrea Passerini"], "title": "MedSyn: Enhancing Diagnostics with Human-AI Collaboration", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": "Accepted to the Trustworthy and Collaborative Artificial Intelligence Workshop 2025 (TCAI 2025) in the 4th International Conference Series on Hybrid Human-Artificial Intelligence (HHAI 2025)", "summary": "Clinical decision-making is inherently complex, often influenced by cognitive biases, incomplete information, and case ambiguity. Large Language Models (LLMs) have shown promise as tools for supporting clinical decision-making, yet their typical one-shot or limited-interaction usage may overlook the complexities of real-world medical practice. In this work, we propose a hybrid human-AI framework, MedSyn, where physicians and LLMs engage in multi-step, interactive dialogues to refine diagnoses and treatment decisions. Unlike static decision-support tools, MedSyn enables dynamic exchanges, allowing physicians to challenge LLM suggestions while the LLM highlights alternative perspectives. Through simulated physician-LLM interactions, we assess the potential of open-source LLMs as physician assistants. Results show open-source LLMs are promising as physician assistants in the real world. Future work will involve real physician interactions to further validate MedSyn's usefulness in diagnostic accuracy and patient outcomes.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u4ecb\u7ecd\u4e86MedSyn\uff0c\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u533b\u751f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u591a\u6b65\u4ea4\u4e92\u6846\u67b6\uff0c\u7528\u4e8e\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u590d\u6742\u4e14\u6613\u53d7\u8ba4\u77e5\u504f\u5dee\u3001\u4fe1\u606f\u4e0d\u5b8c\u6574\u548c\u75c5\u4f8b\u6a21\u7cca\u6027\u7684\u5f71\u54cd\uff0c\u73b0\u6709\u7684LLMs\u4f7f\u7528\u65b9\u5f0f\u901a\u5e38\u4e3a\u4e00\u6b21\u6027\u6216\u6709\u9650\u4ea4\u4e92\uff0c\u672a\u80fd\u5145\u5206\u4f53\u73b0\u73b0\u5b9e\u533b\u7597\u5b9e\u8df5\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faMedSyn\u6846\u67b6\uff0c\u901a\u8fc7\u533b\u751f\u4e0eLLMs\u7684\u591a\u6b65\u4ea4\u4e92\u5bf9\u8bdd\u6765\u4f18\u5316\u8bca\u65ad\u548c\u6cbb\u7597\u51b3\u7b56\uff0c\u652f\u6301\u52a8\u6001\u4ea4\u6d41\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f00\u6e90LLMs\u5728\u6a21\u62df\u533b\u751f-LLM\u4ea4\u4e92\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u6709\u6f5c\u529b\u6210\u4e3a\u5b9e\u9645\u533b\u7597\u4e2d\u7684\u52a9\u624b\u3002", "conclusion": "\u672a\u6765\u8ba1\u5212\u901a\u8fc7\u771f\u5b9e\u533b\u751f\u4e92\u52a8\u8fdb\u4e00\u6b65\u9a8c\u8bc1MedSyn\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u60a3\u8005\u7ed3\u679c\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.14783", "pdf": "https://arxiv.org/pdf/2506.14783", "abs": "https://arxiv.org/abs/2506.14783", "authors": ["Mohamed Masry", "Mohamed Amen", "Mohamed Elzyat", "Mohamed Hamed", "Norhan Magdy", "Maram Khaled"], "title": "ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification", "categories": ["cs.LG", "cs.CL", "cs.HC"], "comment": "Graduation project report submitted at Faculty of Computer Science and Artificial Intelligence, Helwan University", "summary": "Decoding natural language from brain activity using non-invasive electroencephalography (EEG) remains a significant challenge in neuroscience and machine learning, particularly for open-vocabulary scenarios where traditional methods struggle with noise and variability. Previous studies have achieved high accuracy on small-closed vocabularies, but it still struggles on open vocabularies. In this study, we propose ETS, a framework that integrates EEG with synchronized eye-tracking data to address two critical tasks: (1) open-vocabulary text generation and (2) sentiment classification of perceived language. Our model achieves a superior performance on BLEU and Rouge score for EEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment classification, which significantly outperforms supervised baselines. Furthermore, we show that our proposed model can handle data from various subjects and sources, showing great potential for high performance open vocabulary eeg-to-text system.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faETS\u6846\u67b6\uff0c\u7ed3\u5408EEG\u548c\u773c\u52a8\u6570\u636e\uff0c\u63d0\u5347\u5f00\u653e\u8bcd\u6c47\u6587\u672c\u751f\u6210\u548c\u60c5\u611f\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u975e\u4fb5\u5165\u5f0fEEG\u5728\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u4e0b\u56e0\u566a\u58f0\u548c\u53d8\u5f02\u6027\u5bfc\u81f4\u7684\u6027\u80fd\u4e0d\u4f73\u95ee\u9898\u3002", "method": "\u96c6\u6210EEG\u4e0e\u540c\u6b65\u773c\u52a8\u6570\u636e\uff0c\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u6587\u672c\u751f\u6210\u548c\u60c5\u611f\u5206\u7c7b\u3002", "result": "\u5728BLEU\u548cRouge\u5206\u6570\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u60c5\u611f\u5206\u7c7bF1\u5206\u6570\u63d0\u534710%\uff0c\u652f\u6301\u591a\u6e90\u6570\u636e\u3002", "conclusion": "ETS\u6846\u67b6\u5728\u5f00\u653e\u8bcd\u6c47EEG\u5230\u6587\u672c\u7cfb\u7edf\u4e2d\u5177\u6709\u9ad8\u6027\u80fd\u6f5c\u529b\u3002"}}
{"id": "2506.14854", "pdf": "https://arxiv.org/pdf/2506.14854", "abs": "https://arxiv.org/abs/2506.14854", "authors": ["Varun Mannam", "Zhenyu Shi"], "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "Submitting to ICCV 2025 workshop: https://retailvisionworkshop.github.io/", "summary": "Accurate video annotation plays a vital role in modern retail applications, including customer behavior analysis, product interaction detection, and in-store activity recognition. However, conventional annotation methods heavily rely on time-consuming manual labeling by human annotators, introducing non-robust frame selection and increasing operational costs. To address these challenges in the retail domain, we propose a deep learning-based approach that automates key-frame identification in retail videos and provides automatic annotations of products and customers. Our method leverages deep neural networks to learn discriminative features by embedding video frames and incorporating object detection-based techniques tailored for retail environments. Experimental results showcase the superiority of our approach over traditional methods, achieving accuracy comparable to human annotator labeling while enhancing the overall efficiency of retail video annotation. Remarkably, our approach leads to an average of 2 times cost savings in video annotation. By allowing human annotators to verify/adjust less than 5% of detected frames in the video dataset, while automating the annotation process for the remaining frames without reducing annotation quality, retailers can significantly reduce operational costs. The automation of key-frame detection enables substantial time and effort savings in retail video labeling tasks, proving highly valuable for diverse retail applications such as shopper journey analysis, product interaction detection, and in-store security monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u96f6\u552e\u89c6\u9891\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u96f6\u552e\u89c6\u9891\u6807\u6ce8\u5bf9\u987e\u5ba2\u884c\u4e3a\u5206\u6790\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u624b\u5de5\u6807\u6ce8\u8017\u65f6\u4e14\u4f4e\u6548\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u89c6\u9891\u5e27\u7279\u5f81\uff0c\u7ed3\u5408\u9488\u5bf9\u96f6\u552e\u73af\u5883\u7684\u76ee\u6807\u68c0\u6d4b\u6280\u672f\uff0c\u81ea\u52a8\u5316\u6807\u6ce8\u3002", "result": "\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0a\u5ab2\u7f8e\u4eba\u5de5\u6807\u6ce8\uff0c\u8282\u770150%\u6210\u672c\uff0c\u540c\u65f6\u4ec5\u9700\u4eba\u5de5\u9a8c\u8bc15%\u7684\u5e27\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u7ecf\u6d4e\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u96f6\u552e\u5e94\u7528\uff0c\u5982\u987e\u5ba2\u65c5\u7a0b\u5206\u6790\u548c\u5b89\u5168\u76d1\u63a7\u3002"}}
{"id": "2506.15085", "pdf": "https://arxiv.org/pdf/2506.15085", "abs": "https://arxiv.org/abs/2506.15085", "authors": ["Paige Tutt\u00f6s\u00ed", "Shivam Mehta", "Zachary Syvenky", "Bermet Burkanova", "Gustav Eje Henter", "Angelica Lim"], "title": "EmojiVoice: Towards long-term controllable expressivity in robot speech", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to RO-MAN 2025, Demo at HRI 2025 : https://dl.acm.org/doi/10.5555/3721488.3721774", "summary": "Humans vary their expressivity when speaking for extended periods to maintain engagement with their listener. Although social robots tend to be deployed with ``expressive'' joyful voices, they lack this long-term variation found in human speech. Foundation model text-to-speech systems are beginning to mimic the expressivity in human speech, but they are difficult to deploy offline on robots. We present EmojiVoice, a free, customizable text-to-speech (TTS) toolkit that allows social roboticists to build temporally variable, expressive speech on social robots. We introduce emoji-prompting to allow fine-grained control of expressivity on a phase level and use the lightweight Matcha-TTS backbone to generate speech in real-time. We explore three case studies: (1) a scripted conversation with a robot assistant, (2) a storytelling robot, and (3) an autonomous speech-to-speech interactive agent. We found that using varied emoji prompting improved the perception and expressivity of speech over a long period in a storytelling task, but expressive voice was not preferred in the assistant use case.", "AI": {"tldr": "EmojiVoice\u662f\u4e00\u4e2a\u514d\u8d39\u7684\u3001\u53ef\u5b9a\u5236\u7684\u6587\u672c\u8f6c\u8bed\u97f3\uff08TTS\uff09\u5de5\u5177\u5305\uff0c\u65e8\u5728\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u957f\u671f\u53ef\u53d8\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u3002", "motivation": "\u793e\u4ea4\u673a\u5668\u4eba\u901a\u5e38\u4f7f\u7528\u5355\u8c03\u7684\u2018\u9ad8\u5174\u2019\u8bed\u97f3\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u8bed\u97f3\u4e2d\u7684\u957f\u671f\u53d8\u5316\u3002\u73b0\u6709\u7684\u57fa\u7840\u6a21\u578bTTS\u7cfb\u7edf\u96be\u4ee5\u5728\u673a\u5668\u4eba\u4e0a\u79bb\u7ebf\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7Matcha-TTS\u6846\u67b6\u548c\u8868\u60c5\u7b26\u53f7\u63d0\u793a\uff08emoji-prompting\uff09\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5e76\u5728\u4e09\u4e2a\u6848\u4f8b\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728\u8bb2\u6545\u4e8b\u4efb\u52a1\u4e2d\uff0c\u591a\u6837\u5316\u7684\u8868\u60c5\u7b26\u53f7\u63d0\u793a\u63d0\u9ad8\u4e86\u8bed\u97f3\u7684\u611f\u77e5\u548c\u8868\u8fbe\u6027\uff0c\u4f46\u5728\u52a9\u624b\u4efb\u52a1\u4e2d\u672a\u88ab\u4f18\u5148\u9009\u62e9\u3002", "conclusion": "EmojiVoice\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u53ef\u63d0\u5347\u673a\u5668\u4eba\u8bed\u97f3\u7684\u8868\u8fbe\u6027\uff0c\u4f46\u9700\u6839\u636e\u7528\u9014\u8c03\u6574\u3002"}}
{"id": "2506.15107", "pdf": "https://arxiv.org/pdf/2506.15107", "abs": "https://arxiv.org/abs/2506.15107", "authors": ["Paige Tutt\u00f6s\u00ed"], "title": "I Know You're Listening: Adaptive Voice for HRI", "categories": ["cs.RO", "cs.HC", "cs.SD", "eess.AS"], "comment": "PhD Thesis Simon Fraser University https://summit.sfu.ca/item/39353 Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts IROS 23 Mmm whatcha say? Uncovering distal and proximal context effects in first and second-language word perception using psychophysical reverse correlation INTERSPEECH 24 Emojivoice: Towards long-term controllable expressivity in robot speech RO-MAN 25", "summary": "While the use of social robots for language teaching has been explored, there remains limited work on a task-specific synthesized voices for language teaching robots. Given that language is a verbal task, this gap may have severe consequences for the effectiveness of robots for language teaching tasks. We address this lack of L2 teaching robot voices through three contributions: 1. We address the need for a lightweight and expressive robot voice. Using a fine-tuned version of Matcha-TTS, we use emoji prompting to create an expressive voice that shows a range of expressivity over time. The voice can run in real time with limited compute resources. Through case studies, we found this voice more expressive, socially appropriate, and suitable for long periods of expressive speech, such as storytelling. 2. We explore how to adapt a robot's voice to physical and social ambient environments to deploy our voices in various locations. We found that increasing pitch and pitch rate in noisy and high-energy environments makes the robot's voice appear more appropriate and makes it seem more aware of its current environment. 3. We create an English TTS system with improved clarity for L2 listeners using known linguistic properties of vowels that are difficult for these listeners. We used a data-driven, perception-based approach to understand how L2 speakers use duration cues to interpret challenging words with minimal tense (long) and lax (short) vowels in English. We found that the duration of vowels strongly influences the perception for L2 listeners and created an \"L2 clarity mode\" for Matcha-TTS that applies a lengthening to tense vowels while leaving lax vowels unchanged. Our clarity mode was found to be more respectful, intelligible, and encouraging than base Matcha-TTS while reducing transcription errors in these challenging tense/lax minimal pairs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u5728\u8bed\u8a00\u6559\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u4efb\u52a1\u7279\u5b9a\u5408\u6210\u8bed\u97f3\u7684\u7f3a\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8868\u8fbe\u6027\u8bed\u97f3\u3001\u73af\u5883\u9002\u5e94\u6027\u8bed\u97f3\u8c03\u6574\u548c\u9488\u5bf9L2\u5b66\u4e60\u8005\u7684\u6e05\u6670\u8bed\u97f3\u7cfb\u7edf\u4e09\u4e2a\u65b9\u9762\u8fdb\u884c\u4e86\u6539\u8fdb\u3002", "motivation": "\u8bed\u8a00\u662f\u53e3\u5934\u4efb\u52a1\uff0c\u793e\u4ea4\u673a\u5668\u4eba\u7f3a\u4e4f\u9488\u5bf9\u8bed\u8a00\u6559\u5b66\u7684\u7279\u5b9a\u8bed\u97f3\u7cfb\u7edf\u53ef\u80fd\u5f71\u54cd\u6559\u5b66\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9002\u5408\u7684\u8bed\u97f3\u6280\u672f\u3002", "method": "1. \u4f7f\u7528Matcha-TTS\u7684\u5fae\u8c03\u7248\u672c\uff0c\u901a\u8fc7\u8868\u60c5\u7b26\u53f7\u63d0\u793a\u521b\u5efa\u8f7b\u91cf\u7ea7\u4e14\u8868\u8fbe\u4e30\u5bcc\u7684\u8bed\u97f3\uff1b2. \u6839\u636e\u7269\u7406\u548c\u793e\u4f1a\u73af\u5883\u8c03\u6574\u8bed\u97f3\u7684\u97f3\u9ad8\u548c\u97f3\u901f\uff1b3. \u57fa\u4e8eL2\u5b66\u4e60\u8005\u5bf9\u5143\u97f3\u65f6\u957f\u7684\u611f\u77e5\uff0c\u521b\u5efa\u66f4\u6e05\u6670\u7684\u82f1\u8bedTTS\u7cfb\u7edf\u3002", "result": "1. \u8868\u8fbe\u6027\u8bed\u97f3\u66f4\u751f\u52a8\u4e14\u9002\u5408\u957f\u65f6\u95f4\u8bb2\u8ff0\uff1b2. \u73af\u5883\u9002\u5e94\u6027\u8c03\u6574\u4f7f\u8bed\u97f3\u66f4\u8d34\u5408\u73af\u5883\uff1b3. \"L2\u6e05\u6670\u6a21\u5f0f\"\u663e\u8457\u63d0\u9ad8\u4e86L2\u5b66\u4e60\u8005\u7684\u7406\u89e3\u80fd\u529b\u548c\u51cf\u5c11\u9519\u8bef\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u8bed\u97f3\u7cfb\u7edf\u7684\u8868\u8fbe\u6027\u3001\u73af\u5883\u9002\u5e94\u6027\u548c\u6e05\u6670\u5ea6\uff0c\u793e\u4ea4\u673a\u5668\u4eba\u5728\u8bed\u8a00\u6559\u5b66\u4e2d\u7684\u6548\u679c\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2506.15278", "pdf": "https://arxiv.org/pdf/2506.15278", "abs": "https://arxiv.org/abs/2506.15278", "authors": ["Reuben Binns", "Jake Stein", "Siddhartha Datta", "Max Van Kleek", "Nigel Shadbolt"], "title": "Not Even Nice Work If You Can Get It; A Longitudinal Study of Uber's Algorithmic Pay and Pricing", "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Ride-sharing platforms like Uber market themselves as enabling `flexibility' for their workforce, meaning that drivers are expected to anticipate when and where the algorithm will allocate them jobs, and how well remunerated those jobs will be. In this work we describe our process of participatory action research with drivers and trade union organisers, culminating in a participatory audit of Uber's algorithmic pay and work allocation, before and after the introduction of dynamic pricing. Through longitudinal analysis of 1.5 million trips from 258 drivers in the UK, we find that after dynamic pricing, pay has decreased, Uber's cut has increased, job allocation and pay is less predictable, inequality between drivers is increased, and drivers spend more time waiting for jobs. In addition to these findings, we provide methodological and theoretical contributions to algorithm auditing, gig work, and the emerging practice of worker data science.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86Uber\u7684\u52a8\u6001\u5b9a\u4ef7\u5bf9\u53f8\u673a\u6536\u5165\u548c\u5206\u914d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5bfc\u81f4\u6536\u5165\u4e0b\u964d\u3001\u5e73\u53f0\u62bd\u6210\u589e\u52a0\u3001\u5206\u914d\u4e0d\u5747\u7b49\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8Uber\u5e73\u53f0\u5ba3\u79f0\u7684\u2018\u7075\u6d3b\u6027\u2019\u662f\u5426\u5b9e\u9645\u6539\u5584\u4e86\u53f8\u673a\u7684\u5de5\u4f5c\u6761\u4ef6\uff0c\u7279\u522b\u662f\u52a8\u6001\u5b9a\u4ef7\u5bf9\u53f8\u673a\u6536\u5165\u548c\u5de5\u4f5c\u5206\u914d\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u53c2\u4e0e\u5f0f\u884c\u52a8\u7814\u7a76\u4e0e\u53f8\u673a\u548c\u5de5\u4f1a\u5408\u4f5c\uff0c\u5bf9Uber\u7684\u7b97\u6cd5\u85aa\u916c\u548c\u5de5\u4f5c\u5206\u914d\u8fdb\u884c\u5ba1\u6838\uff0c\u5206\u6790\u4e86\u82f1\u56fd258\u540d\u53f8\u673a\u7684150\u4e07\u6b21\u884c\u7a0b\u6570\u636e\u3002", "result": "\u52a8\u6001\u5b9a\u4ef7\u540e\uff0c\u53f8\u673a\u6536\u5165\u4e0b\u964d\uff0c\u5e73\u53f0\u62bd\u6210\u589e\u52a0\uff0c\u5de5\u4f5c\u5206\u914d\u548c\u85aa\u916c\u66f4\u4e0d\u53ef\u9884\u6d4b\uff0c\u53f8\u673a\u4e4b\u95f4\u7684\u4e0d\u5e73\u7b49\u52a0\u5267\uff0c\u7b49\u5f85\u65f6\u95f4\u66f4\u957f\u3002", "conclusion": "\u52a8\u6001\u5b9a\u4ef7\u5bf9\u53f8\u673a\u4e0d\u5229\uff0c\u7814\u7a76\u4e3a\u7b97\u6cd5\u5ba1\u6838\u3001\u96f6\u5de5\u7ecf\u6d4e\u4ee5\u53ca\u5de5\u4eba\u6570\u636e\u79d1\u5b66\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u548c\u7406\u8bba\u8d21\u732e\u3002"}}
