{"id": "2508.11034", "pdf": "https://arxiv.org/pdf/2508.11034", "abs": "https://arxiv.org/abs/2508.11034", "authors": ["Antonio Collante", "Samuel Abedu", "SayedHassan Khatoonabadi", "Ahmad Abdellatif", "Ebube Alor", "Emad Shihab"], "title": "The Impact of Large Language Models (LLMs) on Code Review Process", "categories": ["cs.SE"], "comment": null, "summary": "Large language models (LLMs) have recently gained prominence in the field of\nsoftware development, significantly boosting productivity and simplifying\nteamwork. Although prior studies have examined task-specific applications, the\nphase-specific effects of LLM assistance on the efficiency of code review\nprocesses remain underexplored. This research investigates the effect of GPT on\nGitHub pull request (PR) workflows, with a focus on reducing resolution time,\noptimizing phase-specific performance, and assisting developers. We curated a\ndataset of 25,473 PRs from 9,254 GitHub projects and identified GPT-assisted\nPRs using a semi-automated heuristic approach that combines keyword-based\ndetection, regular expression filtering, and manual verification until\nachieving 95% labeling accuracy. We then applied statistical modeling,\nincluding multiple linear regression and Mann-Whitney U test, to evaluate\ndifferences between GPT-assisted and non-assisted PRs, both at the overall\nresolution level and across distinct review phases. Our research has revealed\nthat early adoption of GPT can substantially boost the effectiveness of the PR\nprocess, leading to considerable time savings at various stages. Our findings\nsuggest that GPT-assisted PRs reduced median resolution time by more than 60%\n(9 hours compared to 23 hours for non-assisted PRs). We discovered that\nutilizing GPT can reduce the review time by 33% and the waiting time before\nacceptance by 87%. Analyzing a sample dataset of 300 GPT-assisted PRs, we\ndiscovered that developers predominantly use GPT for code optimization (60%),\nbug fixing (26%), and documentation updates (12%). This research sheds light on\nthe impact of the GPT model on the code review process, offering actionable\ninsights for software teams seeking to enhance workflows and promote seamless\ncollaboration.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cGPT\u8f85\u52a9\u7684\u4ee3\u7801\u5ba1\u67e5\u8fc7\u7a0b\u80fd\u663e\u8457\u51cf\u5c11\u89e3\u51b3\u65f6\u95f4\u548c\u4f18\u5316\u5404\u9636\u6bb5\u6027\u80fd\uff0c\u5176\u4e2d\u5ba1\u67e5\u65f6\u95f4\u51cf\u5c1133%\uff0c\u7b49\u5f85\u63a5\u53d7\u65f6\u95f4\u51cf\u5c1187%\uff0c\u6574\u4f53\u89e3\u51b3\u65f6\u95f4\u4e2d\u4f4d\u6570\u51cf\u5c1160%\u4ee5\u4e0a\u3002", "motivation": "\u63a2\u7d22GPT\u5728GitHub\u62c9\u53d6\u8bf7\u6c42\uff08PR\uff09\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5177\u4f53\u6548\u679c\uff0c\u5c24\u5176\u662f\u5728\u51cf\u5c11\u89e3\u51b3\u65f6\u95f4\u3001\u4f18\u5316\u5404\u9636\u6bb5\u6027\u80fd\u548c\u8f85\u52a9\u5f00\u53d1\u8005\u65b9\u9762\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u534a\u81ea\u52a8\u542f\u53d1\u5f0f\u65b9\u6cd5\u7b5b\u9009\u5e76\u9a8c\u8bc1GPT\u8f85\u52a9\u7684PR\uff0c\u7136\u540e\u901a\u8fc7\u7edf\u8ba1\u5efa\u6a21\uff08\u5982\u591a\u5143\u7ebf\u6027\u56de\u5f52\u548cMann-Whitney U\u68c0\u9a8c\uff09\u5206\u6790\u5dee\u5f02\u3002", "result": "GPT\u8f85\u52a9\u7684PR\u4e2d\u4f4d\u6570\u89e3\u51b3\u65f6\u95f4\u4ece23\u5c0f\u65f6\u964d\u81f39\u5c0f\u65f6\uff0c\u5ba1\u67e5\u65f6\u95f4\u51cf\u5c1133%\uff0c\u7b49\u5f85\u63a5\u53d7\u65f6\u95f4\u51cf\u5c1187%\u3002\u5f00\u53d1\u4eba\u5458\u4e3b\u8981\u5c06GPT\u7528\u4e8e\u4ee3\u7801\u4f18\u5316\uff0860%\uff09\u3001\u4fee\u590d\u9519\u8bef\uff0826%\uff09\u548c\u66f4\u65b0\u6587\u6863\uff0812%\uff09\u3002", "conclusion": "GPT\u5728\u4ee3\u7801\u5ba1\u67e5\u8fc7\u7a0b\u4e2d\u7684\u65e9\u671f\u91c7\u7528\u80fd\u591f\u5927\u5e45\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u56e2\u961f\u4f18\u5316\u534f\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u5efa\u8bae\u3002"}}
{"id": "2508.11110", "pdf": "https://arxiv.org/pdf/2508.11110", "abs": "https://arxiv.org/abs/2508.11110", "authors": ["Mukul Singh", "Gust Verbruggen", "Vu Le", "Sumit Gulwani"], "title": "Diffusion is a code repair operator and generator", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "12 pages", "summary": "Code diffusion models generate code by iteratively removing noise from the\nlatent representation of a code snippet. During later steps of the diffusion\nprocess, when the code snippet has almost converged, differences between\ndiscrete representations of these snippets look like last-mile repairs applied\nto broken or incomplete code. We evaluate the extent to which this resemblance\ncan be exploited to leverage pre-trained code diffusion models for the problem\nof last-mile repair by considering two applications with significant potential.\nFirst, we can leverage the diffusion model for last-mile repair by adding noise\nto a broken code snippet and resuming the diffusion process. Second, we can\nleverage the diffusion model to generate arbitrary amount of training data for\nlast-mile repair tasks (that are computationally more efficient) by sampling an\nintermediate program (input) and the final program (output) from the diffusion\nprocess. We perform experiments on 3 domains (Python, Excel and PowerShell) to\nevaluate applications, as well as analyze properties.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u9884\u8bad\u7ec3\u7684\u4ee3\u7801\u6269\u6563\u6a21\u578b\u8fdb\u884c\u2018\u6700\u540e\u4e00\u82f1\u91cc\u4fee\u590d\u2019\uff0c\u901a\u8fc7\u52a0\u566a\u548c\u91c7\u6837\u4e2d\u95f4\u7a0b\u5e8f\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u7814\u7a76\u4ee3\u7801\u6269\u6563\u6a21\u578b\u5728\u540e\u671f\u751f\u6210\u7684\u4ee3\u7801\u7247\u6bb5\u4e2d\u5c55\u73b0\u51fa\u2018\u6700\u540e\u4e00\u82f1\u91cc\u4fee\u590d\u2019\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u5229\u7528\u8fd9\u4e00\u7279\u6027\u89e3\u51b3\u4fee\u590d\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u52a0\u566a\u5e76\u91cd\u65b0\u6269\u6563\u6765\u4fee\u590d\u4ee3\u7801\uff0c\u4ee5\u53ca\u91c7\u6837\u4e2d\u95f4\u7a0b\u5e8f\u4e0e\u6700\u7ec8\u7a0b\u5e8f\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5b9e\u9a8c\u8986\u76d6Python\u3001Excel\u548cPowerShell\u3002", "result": "\u5c55\u793a\u4e86\u4ee3\u7801\u6269\u6563\u6a21\u578b\u5728\u2018\u6700\u540e\u4e00\u82f1\u91cc\u4fee\u590d\u2019\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u9884\u8bad\u7ec3\u7684\u4ee3\u7801\u6269\u6563\u6a21\u578b\u53ef\u6709\u6548\u7528\u4e8e\u4ee3\u7801\u4fee\u590d\u4efb\u52a1\uff0c\u5e76\u80fd\u751f\u6210\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2508.11126", "pdf": "https://arxiv.org/pdf/2508.11126", "abs": "https://arxiv.org/abs/2508.11126", "authors": ["Huanting Wang", "Jingzhi Gong", "Huawei Zhang", "Zheng Wang"], "title": "AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities", "categories": ["cs.SE"], "comment": null, "summary": "AI agentic programming is an emerging paradigm in which large language models\n(LLMs) autonomously plan, execute, and interact with external tools like\ncompilers, debuggers, and version control systems to iteratively perform\ncomplex software development tasks. Unlike conventional code generation tools,\nagentic systems are capable of decomposing high-level goals, coordinating\nmulti-step processes, and adapting their behavior based on intermediate\nfeedback. These capabilities are transforming the software development\npractice. As this emerging field evolves rapidly, there is a need to define its\nscope, consolidate its technical foundations, and identify open research\nchallenges. This survey provides a comprehensive and timely review of AI\nagentic programming. We introduce a taxonomy of agent behaviors and system\narchitectures, and examine core techniques including planning, memory and\ncontext management, tool integration, and execution monitoring. We also analyze\nexisting benchmarks and evaluation methodologies used to assess coding agent\nperformance. Our study identifies several key challenges, including limitations\nin handling long context, a lack of persistent memory across tasks, and\nconcerns around safety, alignment with user intent, and collaboration with\nhuman developers. We discuss emerging opportunities to improve the reliability,\nadaptability, and transparency of agentic systems. By synthesizing recent\nadvances and outlining future directions, this survey aims to provide a\nfoundation for research and development in building the next generation of\nintelligent and trustworthy AI coding agents.", "AI": {"tldr": "AI\u4ee3\u7406\u7f16\u7a0b\u662f\u4e00\u79cd\u65b0\u5174\u8303\u5f0f\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u4e3b\u89c4\u5212\u3001\u6267\u884c\u5e76\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\uff0c\u5b8c\u6210\u590d\u6742\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\uff0c\u672c\u6587\u5bf9\u5176\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u7f16\u7a0b\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u660e\u786e\u5176\u8303\u7574\u3001\u5de9\u56fa\u6280\u672f\u57fa\u7840\u5e76\u8bc6\u522b\u7814\u7a76\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4ee3\u7406\u884c\u4e3a\u4e0e\u7cfb\u7edf\u67b6\u6784\u7684\u5206\u7c7b\u6cd5\uff0c\u7814\u7a76\u89c4\u5212\u3001\u8bb0\u5fc6\u3001\u5de5\u5177\u96c6\u6210\u7b49\u6838\u5fc3\u6280\u672f\uff0c\u5206\u6790\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u3001\u6301\u4e45\u8bb0\u5fc6\u7b49\u5173\u952e\u6311\u6218\uff0c\u63a2\u8ba8\u53ef\u9760\u6027\u3001\u900f\u660e\u5ea6\u7684\u63d0\u5347\u673a\u4f1a\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u53ef\u4fe1AI\u7f16\u7801\u4ee3\u7406\u7684\u7814\u53d1\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2508.11147", "pdf": "https://arxiv.org/pdf/2508.11147", "abs": "https://arxiv.org/abs/2508.11147", "authors": ["Zhengquan Li", "Zhenhao Li", "Zishuo Ding"], "title": "From Feedback to Failure: Automated Android Performance Issue Reproduction", "categories": ["cs.SE", "D.2.5"], "comment": "10page, 8 figures", "summary": "Mobile application performance is a vital factor for user experience. Yet,\nperformance issues are notoriously difficult to detect within development\nenvironments, where their manifestations are often less conspicuous and\ndiagnosis proves more challenging. To address this limitation, we propose\nRevPerf, an advanced performance issue reproduction tool that leverages app\nreviews from Google Play to acquire pertinent information. RevPerf employs\nrelevant reviews and prompt engineering to enrich the original review with\nperformance issue details. An execution agent is then employed to generate and\nexecute commands to reproduce the issue. After executing all necessary steps,\nthe system incorporates multifaceted detection methods to identify performance\nissues by monitoring Android logs, GUI changes, and system resource utilization\nduring the reproduction process. Experimental results demonstrate that our\nproposed framework achieves a 70\\% success rate in reproducing performance\nissues on the dataset we constructed and manually validated.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRevPerf\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u6790Google Play\u7684\u5e94\u7528\u8bc4\u8bba\u6765\u590d\u73b0\u79fb\u52a8\u5e94\u7528\u7684\u6027\u80fd\u95ee\u9898\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u548c\u591a\u65b9\u9762\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u8fbe\u523070%\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u6027\u80fd\u5bf9\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f00\u53d1\u73af\u5883\u4e2d\u6027\u80fd\u95ee\u9898\u96be\u4ee5\u68c0\u6d4b\u548c\u8bca\u65ad\u3002", "method": "\u5229\u7528Google Play\u8bc4\u8bba\u63d0\u53d6\u4fe1\u606f\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u4e30\u5bcc\u95ee\u9898\u7ec6\u8282\uff0c\u751f\u6210\u5e76\u6267\u884c\u547d\u4ee4\u590d\u73b0\u95ee\u9898\uff0c\u7ed3\u5408\u65e5\u5fd7\u3001GUI\u548c\u8d44\u6e90\u76d1\u63a7\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u5728\u624b\u52a8\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u590d\u73b0\u6027\u80fd\u95ee\u9898\u7684\u6210\u529f\u7387\u4e3a70%\u3002", "conclusion": "RevPerf\u80fd\u6709\u6548\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u5f00\u53d1\u73af\u5883\u5916\u590d\u73b0\u6027\u80fd\u95ee\u9898\u3002"}}
{"id": "2508.11477", "pdf": "https://arxiv.org/pdf/2508.11477", "abs": "https://arxiv.org/abs/2508.11477", "authors": ["Hyunsun Chung", "Junhyeok Park", "Taewan Noh", "Seonghoon Ahn", "Kihwan Kim", "Ming Zhao", "Youngjae Kim"], "title": "OpenCXD: An Open Real-Device-Guided Hybrid Evaluation Framework for CXL-SSDs", "categories": ["cs.AR", "cs.ET", "cs.OS"], "comment": "This paper will be published in the proceedings of the 33rd\n  International Symposium on the Modeling, Analysis, and Simulation of Computer\n  and Telecommunication System (MASCOTS)", "summary": "The advent of Compute Express Link (CXL) enables SSDs to participate in the\nmemory hierarchy as large-capacity, byte-addressable memory devices. These\nCXL-enabled SSDs (CXL-SSDs) offer a promising new tier between DRAM and\ntraditional storage, combining NAND flash density with memory-like access\nsemantics. However, evaluating the performance of CXL-SSDs remains difficult\ndue to the lack of hardware that natively supports the CXL.mem protocol on\nSSDs. As a result, most prior work relies on hybrid simulators combining CPU\nmodels augmented with CXL.mem semantics and SSD simulators that approximate\ninternal flash behaviors. While effective for early-stage exploration, this\napproach cannot faithfully model firmware-level interactions and low-level\nstorage dynamics critical to CXL-SSD performance. In this paper, we present\nOpenCXD, a real-device-guided hybrid evaluation framework that bridges the gap\nbetween simulation and hardware. OpenCXD integrates a cycle-accurate CXL.mem\nsimulator on the host side with a physical OpenSSD platform running real\nfirmware. This enables in-situ firmware execution triggered by simulated memory\nrequests. Through these contributions, OpenCXD reflects device-level phenomena\nunobservable in simulation-only setups, providing critical insights for future\nfirmware design tailored to CXL-SSDs.", "AI": {"tldr": "OpenCXD\u662f\u4e00\u4e2a\u7ed3\u5408\u6a21\u62df\u4e0e\u786c\u4ef6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30CXL-SSD\u7684\u6027\u80fd\u3002", "motivation": "CXL-SSD\u4f5c\u4e3aDRAM\u548c\u4f20\u7edf\u5b58\u50a8\u4e4b\u95f4\u7684\u65b0\u5c42\u6b21\uff0c\u6027\u80fd\u8bc4\u4f30\u56e0\u7f3a\u4e4f\u539f\u751f\u786c\u4ef6\u652f\u6301\u800c\u56f0\u96be\u3002\u73b0\u6709\u6a21\u62df\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u5efa\u6a21\u56fa\u4ef6\u7ea7\u4ea4\u4e92\u548c\u5b58\u50a8\u52a8\u6001\u3002", "method": "OpenCXD\u901a\u8fc7\u5c06\u4e3b\u673a\u7aef\u7684CXL.mem\u6a21\u62df\u5668\u4e0e\u8fd0\u884c\u771f\u5b9e\u56fa\u4ef6\u7684\u7269\u7406OpenSSD\u5e73\u53f0\u96c6\u6210\uff0c\u5b9e\u73b0\u6a21\u62df\u5185\u5b58\u8bf7\u6c42\u89e6\u53d1\u7684\u56fa\u4ef6\u6267\u884c\u3002", "result": "OpenCXD\u80fd\u53cd\u6620\u8bbe\u5907\u7ea7\u73b0\u8c61\uff0c\u63d0\u4f9b\u5bf9CXL-SSD\u56fa\u4ef6\u8bbe\u8ba1\u7684\u5b9d\u8d35\u6d1e\u5bdf\u3002", "conclusion": "OpenCXD\u586b\u8865\u4e86\u6a21\u62df\u4e0e\u786c\u4ef6\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3aCXL-SSD\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2508.11297", "pdf": "https://arxiv.org/pdf/2508.11297", "abs": "https://arxiv.org/abs/2508.11297", "authors": ["Casper Bach"], "title": "Generic Reduction-Based Interpreters (Extended Version)", "categories": ["cs.PL"], "comment": null, "summary": "Reduction-based interpreters are traditionally defined in terms of a one-step\nreduction function which systematically decomposes a term into a potential\nredex and context, contracts the redex, and recomposes it to construct the new\nterm to be further reduced. While implementing such interpreters follows a\nsystematic recipe, they often require interpreter engineers to write a\nsubstantial amount of code -- much of it boilerplate. In this paper, we apply\nwell-known techniques from generic programming to reduce boilerplate code in\nreduction-based interpreters.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528\u901a\u7528\u7f16\u7a0b\u6280\u672f\u51cf\u5c11\u5f52\u7ea6\u5f0f\u89e3\u91ca\u5668\u4e2d\u7684\u6837\u677f\u4ee3\u7801\u3002", "motivation": "\u4f20\u7edf\u7684\u5f52\u7ea6\u5f0f\u89e3\u91ca\u5668\u9700\u8981\u5927\u91cf\u91cd\u590d\u7684\u4ee3\u7801\u5b9e\u73b0\uff0c\u867d\u7136\u5b9e\u73b0\u8fc7\u7a0b\u7cfb\u7edf\u5316\uff0c\u4f46\u589e\u52a0\u4e86\u5de5\u7a0b\u5e08\u7684\u8d1f\u62c5\u3002", "method": "\u5e94\u7528\u901a\u7528\u7f16\u7a0b\u6280\u672f\uff08generic programming\uff09\u6765\u4f18\u5316\u5f52\u7ea6\u5f0f\u89e3\u91ca\u5668\u7684\u5b9e\u73b0\u3002", "result": "\u6709\u6548\u51cf\u5c11\u4e86\u6837\u677f\u4ee3\u7801\u7684\u6570\u91cf\u3002", "conclusion": "\u901a\u7528\u7f16\u7a0b\u6280\u672f\u53ef\u4ee5\u663e\u8457\u7b80\u5316\u5f52\u7ea6\u5f0f\u89e3\u91ca\u5668\u7684\u5b9e\u73b0\u8fc7\u7a0b\u3002"}}
{"id": "2508.11177", "pdf": "https://arxiv.org/pdf/2508.11177", "abs": "https://arxiv.org/abs/2508.11177", "authors": ["I-Chao Shen", "Ariel Shamir", "Takeo Igarashi"], "title": "LayoutRectifier: An Optimization-based Post-processing for Graphic Design Layout Generation", "categories": ["cs.GR"], "comment": "11 pages, Pacific Graphics 2025", "summary": "Recent deep learning methods can generate diverse graphic design layouts\nefficiently. However, these methods often create layouts with flaws, such as\nmisalignment, unwanted overlaps, and unsatisfied containment. To tackle this\nissue, we propose an optimization-based method called LayoutRectifier, which\ngracefully rectifies auto-generated graphic design layouts to reduce these\nflaws while minimizing deviation from the generated layout. The core of our\nmethod is a two-stage optimization. First, we utilize grid systems, which\nprofessional designers commonly use to organize elements, to mitigate\nmisalignments through discrete search. Second, we introduce a novel box\ncontainment function designed to adjust the positions and sizes of the layout\nelements, preventing unwanted overlapping and promoting desired containment. We\nevaluate our method on content-agnostic and content-aware layout generation\ntasks and achieve better-quality layouts that are more suitable for downstream\ngraphic design tasks. Our method complements learning-based layout generation\nmethods and does not require additional training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684LayoutRectifier\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u4fee\u590d\u81ea\u52a8\u751f\u6210\u7684\u8bbe\u8ba1\u5e03\u5c40\u4e2d\u7684\u9519\u8bef\uff0c\u5982\u672a\u5bf9\u9f50\u548c\u91cd\u53e0\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u7684\u5e03\u5c40\u4e2d\u5e38\u89c1\u7684\u5bf9\u9f50\u95ee\u9898\u3001\u91cd\u53e0\u548c\u4e0d\u6ee1\u8db3\u5305\u542b\u9700\u6c42\u7b49\u7f3a\u9677\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\uff1a\u4e00\u662f\u901a\u8fc7\u79bb\u6563\u641c\u7d22\u5229\u7528\u7f51\u683c\u7cfb\u7edf\u89e3\u51b3\u5bf9\u9f50\u95ee\u9898\uff1b\u4e8c\u662f\u63d0\u51fa\u65b0\u7684\u76d2\u5b50\u5305\u542b\u51fd\u6570\u8c03\u6574\u5143\u7d20\u4f4d\u7f6e\u548c\u5927\u5c0f\u3002", "result": "\u5728\u5185\u5bb9\u65e0\u5173\u548c\u5185\u5bb9\u611f\u77e5\u5e03\u5c40\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u4e14\u9002\u5408\u4e0b\u6e38\u8bbe\u8ba1\u4efb\u52a1\u7684\u5e03\u5c40\u3002", "conclusion": "LayoutRectifier\u6709\u6548\u5f25\u8865\u4e86\u5b66\u4e60\u751f\u6210\u5e03\u5c40\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u8bbe\u8ba1\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.11342", "pdf": "https://arxiv.org/pdf/2508.11342", "abs": "https://arxiv.org/abs/2508.11342", "authors": ["Linh-An Phan", "MingXue Wang", "Guangyu Wu", "Wang Dawei", "Chen Liqun", "Li Jin"], "title": "CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in Distributed Tracing for Microservices", "categories": ["cs.NI"], "comment": null, "summary": "Distributed tracing has become an essential technique for debugging and\ntroubleshooting modern microservice-based applications, enabling software\nengineers to detect performance bottlenecks, identify failures, and gain\ninsights into system behavior. However, implementing distributed tracing in\nlarge-scale applications remains challenging due to the need for extensive\ninstrumentation. To reduce this burden, zero-code instrumentation solutions,\nsuch as those based on eBPF, have emerged, allowing span data to be collected\nwithout modifying application code. Despite this promise, span correlation, the\nprocess of establishing causal relationships between spans, remains a critical\nchallenge in zero-code approaches. Existing solutions often rely on thread\naffinity, compromise system security by requiring the kernel integrity mode to\nbe disabled, or incur significant computational overhead due to complex\ninference algorithms. This paper presents CrossTrace, a practical and efficient\ndistributed tracing solution designed to support the debugging of microservice\napplications without requiring source code modifications. CrossTrace employs a\ngreedy algorithm to infer intra-service span relationships from delay patterns,\neliminating reliance on thread identifiers. For inter-service correlation,\nCrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling\nsecure and efficient correlation compromising system security policies.\nEvaluation results show that CrossTrace can correlate thousands of spans within\nseconds with over 90% accuracy, making it suitable for production deployment\nand valuable for microservice observability and diagnosis.", "AI": {"tldr": "CrossTrace\u662f\u4e00\u79cd\u57fa\u4e8eeBPF\u7684\u96f6\u4ee3\u7801\u5206\u5e03\u5f0f\u8ffd\u8e2a\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5ef6\u8fdf\u6a21\u5f0f\u548cTCP\u5305\u5934\u5d4c\u5165\u6807\u8bc6\u7b26\u5b9e\u73b0\u9ad8\u6548\u3001\u5b89\u5168\u7684\u8de8\u670d\u52a1\u8ffd\u8e2a\u3002", "motivation": "\u73b0\u6709\u96f6\u4ee3\u7801\u8ffd\u8e2a\u65b9\u6848\u5728\u8de8\u670d\u52a1\u8ffd\u8e2a\u65f6\u4f9d\u8d56\u7ebf\u7a0b\u5173\u8054\u6216\u727a\u7272\u7cfb\u7edf\u5b89\u5168\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u8d2a\u5fc3\u7b97\u6cd5\u5206\u6790\u5ef6\u8fdf\u6a21\u5f0f\u63a8\u65ad\u670d\u52a1\u5185\u8de8\u5ea6\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7eBPF\u5728TCP\u5305\u5934\u5d4c\u5165\u6807\u8bc6\u7b26\u5b9e\u73b0\u8de8\u670d\u52a1\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCrossTrace\u80fd\u5728\u6570\u79d2\u5185\u4ee5\u8d85\u8fc790%\u7684\u51c6\u786e\u6027\u5173\u8054\u6570\u5343\u4e2a\u8de8\u5ea6\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u3002", "conclusion": "CrossTrace\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u5206\u5e03\u5f0f\u8ffd\u8e2a\u65b9\u6848\uff0c\u9002\u5408\u5fae\u670d\u52a1\u5e94\u7528\u7684\u89c2\u6d4b\u548c\u8bca\u65ad\u3002"}}
{"id": "2508.10974", "pdf": "https://arxiv.org/pdf/2508.10974", "abs": "https://arxiv.org/abs/2508.10974", "authors": ["Yuxin Cao", "Wei Song", "Derui Wang", "Jingling Xue", "Jin Song Dong"], "title": "Failures to Surface Harmful Contents in Video Large Language Models", "categories": ["cs.MM", "cs.CV"], "comment": "11 pages, 8 figures", "summary": "Video Large Language Models (VideoLLMs) are increasingly deployed on numerous\ncritical applications, where users rely on auto-generated summaries while\ncasually skimming the video stream. We show that this interaction hides a\ncritical safety gap: if harmful content is embedded in a video, either as\nfull-frame inserts or as small corner patches, state-of-the-art VideoLLMs\nrarely mention the harmful content in the output, despite its clear visibility\nto human viewers. A root-cause analysis reveals three compounding design flaws:\n(1) insufficient temporal coverage resulting from the sparse, uniformly spaced\nframe sampling used by most leading VideoLLMs, (2) spatial information loss\nintroduced by aggressive token downsampling within sampled frames, and (3)\nencoder-decoder disconnection, whereby visual cues are only weakly utilized\nduring text generation. Leveraging these insights, we craft three zero-query\nblack-box attacks, aligning with these flaws in the processing pipeline. Our\nlarge-scale evaluation across five leading VideoLLMs shows that the harmfulness\nomission rate exceeds 90% in most cases. Even when harmful content is clearly\npresent in all frames, these models consistently fail to identify it. These\nresults underscore a fundamental vulnerability in current VideoLLMs' designs\nand highlight the urgent need for sampling strategies, token compression, and\ndecoding mechanisms that guarantee semantic coverage rather than speed alone.", "AI": {"tldr": "\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VideoLLMs\uff09\u5728\u5173\u952e\u5e94\u7528\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u8bbe\u8ba1\u4e2d\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u5bfc\u81f4\u6a21\u578b\u7ecf\u5e38\u5ffd\u7565\u89c6\u9891\u4e2d\u7684\u6709\u5bb3\u5185\u5bb9\u3002\u6839\u672c\u539f\u56e0\u5305\u62ec\u7a00\u758f\u5e27\u91c7\u6837\u3001\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u548c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u65ad\u5f00\u8fde\u63a5\u3002", "motivation": "\u63ed\u793a\u5f53\u524dVideoLLMs\u5728\u68c0\u6d4b\u89c6\u9891\u4e2d\u6709\u5bb3\u5185\u5bb9\u65b9\u9762\u7684\u8bbe\u8ba1\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e09\u79cd\u8bbe\u8ba1\u7f3a\u9677\uff08\u5e27\u91c7\u6837\u4e0d\u8db3\u3001\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u3001\u7f16\u7801\u5668-\u89e3\u7801\u5668\u65ad\u5f00\uff09\uff0c\u5f00\u53d1\u4e86\u4e09\u79cd\u96f6\u67e5\u8be2\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u5728\u4e94\u79cd\u4e3b\u6d41VideoLLMs\u4e0a\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6709\u5bb3\u5185\u5bb9\u9057\u6f0f\u7387\u8d85\u8fc790%\uff0c\u5373\u4f7f\u6709\u5bb3\u5185\u5bb9\u5728\u6240\u6709\u5e27\u4e2d\u660e\u663e\u53ef\u89c1\u3002", "conclusion": "\u5f53\u524dVideoLLMs\u7684\u8bbe\u8ba1\u5b58\u5728\u6839\u672c\u6027\u6f0f\u6d1e\uff0c\u9700\u8981\u6539\u8fdb\u91c7\u6837\u7b56\u7565\u3001\u4ee4\u724c\u538b\u7f29\u548c\u89e3\u7801\u673a\u5236\uff0c\u4ee5\u786e\u4fdd\u8bed\u4e49\u8986\u76d6\u800c\u4e0d\u4ec5\u662f\u901f\u5ea6\u3002"}}
{"id": "2508.11269", "pdf": "https://arxiv.org/pdf/2508.11269", "abs": "https://arxiv.org/abs/2508.11269", "authors": ["Hao Chen", "Cong Tian", "Zixuan He", "Bin Yu", "Yepang Liu", "Jialun Cao"], "title": "Inference performance evaluation for LLMs on edge devices with a novel benchmarking framework and metric", "categories": ["cs.PF"], "comment": null, "summary": "With the significant success achieved by large language models (LLMs) like\nLLaMA, edge computing-based LLM inference services for mobile and PC are in\nhigh demand for data privacy. However, different edge platforms have different\nhardware characteristics and the large demand for memory capacity and bandwidth\nmakes it very challenging to deploy and benchmark LLMs on edge devices. In this\npaper, we introduce a benchmarking tool named ELIB (edge LLM inference\nbenchmarking) to evaluate LLM inference performance of different edge\nplatforms, and propose a novel metric named MBU to indicate the percentage of\nthe theoretically efficient use of available memory bandwidth for a specific\nmodel running on edge hardware to optimize memory usage. We deploy ELIB on\nthree edge platforms and benchmark using five quantized models to optimize MBU\nin combination with other metrics such as FLOPS, throughput, latency and\naccuracy. And we analyze the results to derive the key factors, constraints,\nunpredictability in optimizing MBU that can guide deploying LLMs on more edge\nplatforms.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aELIB\u7684\u8fb9\u7f18LLM\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u63d0\u51fa\u65b0\u6307\u6807MBU\u4f18\u5316\u5185\u5b58\u4f7f\u7528\uff0c\u5e76\u5728\u4e09\u4e2a\u8fb9\u7f18\u5e73\u53f0\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "motivation": "\u6ee1\u8db3\u79fb\u52a8\u548cPC\u8bbe\u5907\u4e0a\u57fa\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u7684LLM\u63a8\u7406\u9700\u6c42\uff0c\u89e3\u51b3\u4e0d\u540c\u786c\u4ef6\u90e8\u7f72LLM\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1ELIB\u5de5\u5177\uff0c\u5229\u7528MBU\u6307\u6807\u7ed3\u5408FLOPS\u3001\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u51c6\u786e\u6027\u7b49\u6307\u6807\u4f18\u5316\u5185\u5b58\u5e26\u5bbd\u4f7f\u7528\u3002", "result": "\u5728\u4e09\u4e2a\u8fb9\u7f18\u5e73\u53f0\u4e0a\u6d4b\u8bd5\u4e94\u79cd\u91cf\u5316\u6a21\u578b\uff0c\u5206\u6790\u4f18\u5316MBU\u7684\u5173\u952e\u56e0\u7d20\u548c\u7ea6\u675f\u3002", "conclusion": "ELIB\u548cMBU\u6307\u6807\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72LLM\u63d0\u4f9b\u4e86\u4f18\u5316\u6307\u5bfc\u3002"}}
{"id": "2508.11121", "pdf": "https://arxiv.org/pdf/2508.11121", "abs": "https://arxiv.org/abs/2508.11121", "authors": ["Mukul Singh", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Vu Le", "Gust Verbruggen"], "title": "Tabularis Formatus: Predictive Formatting for Tables", "categories": ["cs.DB", "cs.AI", "cs.SE"], "comment": "14 pages", "summary": "Spreadsheet manipulation software are widely used for data management and\nanalysis of tabular data, yet the creation of conditional formatting (CF) rules\nremains a complex task requiring technical knowledge and experience with\nspecific platforms. In this paper we present TaFo, a neuro-symbolic approach to\ngenerating CF suggestions for tables, addressing common challenges such as user\nunawareness, difficulty in rule creation, and inadequate user interfaces. TaFo\ntakes inspiration from component based synthesis systems and extends them with\nsemantic knowledge of language models and a diversity preserving rule\nranking.Unlike previous methods focused on structural formatting, TaFo uniquely\nincorporates value-based formatting, automatically learning both the rule\ntrigger and the associated visual formatting properties for CF rules. By\nremoving the dependency on user specification used by existing techniques in\nthe form of formatted examples or natural language instruction, TaFo makes\nformatting completely predictive and automated for the user. To evaluate TaFo,\nwe use a corpus of 1.8 Million public workbooks with CF and manual formatting.\nWe compare TaFo against a diverse set of symbolic and neural systems designed\nfor or adapted for the task of table formatting. Our results show that TaFo\ngenerates more accurate, diverse and complete formatting suggestions than\ncurrent systems and outperforms these by 15.6\\%--26.5\\% on matching user added\nground truth rules in tables.", "AI": {"tldr": "TaFo\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u6761\u4ef6\u683c\u5f0f\uff08CF\uff09\u5efa\u8bae\uff0c\u89e3\u51b3\u4e86\u7528\u6237\u6280\u672f\u95e8\u69db\u9ad8\u548c\u754c\u9762\u4e0d\u53cb\u597d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6761\u4ef6\u683c\u5f0f\u89c4\u5219\u521b\u5efa\u590d\u6742\uff0c\u4f9d\u8d56\u7528\u6237\u8f93\u5165\uff0cTaFo\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TaFo\u7ed3\u5408\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u77e5\u8bc6\u548c\u591a\u6837\u6027\u89c4\u5219\u6392\u5e8f\uff0c\u81ea\u52a8\u5b66\u4e60\u89e6\u53d1\u5668\u548c\u683c\u5f0f\u5316\u5c5e\u6027\u3002", "result": "TaFo\u5728188\u4e07\u516c\u5171\u5de5\u4f5c\u7c3f\u4e0a\u6d4b\u8bd5\uff0c\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf15.6%--26.5%\u3002", "conclusion": "TaFo\u901a\u8fc7\u5b8c\u5168\u81ea\u52a8\u5316\u683c\u5f0f\u5efa\u8bae\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6761\u4ef6\u683c\u5f0f\u751f\u6210\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2508.10903", "pdf": "https://arxiv.org/pdf/2508.10903", "abs": "https://arxiv.org/abs/2508.10903", "authors": ["Arlindo Gomes", "Emilly Brito", "Luis Morais", "Nivan Ferreira"], "title": "How do Data Journalists Design Maps to Tell Stories?", "categories": ["cs.HC", "cs.CY"], "comment": "IEEE VIS 2025", "summary": "Maps are essential to news media as they provide a familiar way to convey\nspatial context and present engaging narratives. However, the design of\njournalistic maps may be challenging, as editorial teams need to balance\nmultiple aspects, such as aesthetics, the audience's expected data literacy,\ntight publication deadlines, and the team's technical skills. Data journalists\noften come from multiple areas and lack a cartography, data visualization, and\ndata science background, limiting their competence in creating maps. While\nprevious studies have examined spatial visualizations in data stories, this\nresearch seeks to gain a deeper understanding of the map design process\nemployed by news outlets. To achieve this, we strive to answer two specific\nresearch questions: what is the design space of journalistic maps? and how do\neditorial teams produce journalistic map articles? To answer the first one, we\ncollected and analyzed a large corpus of 462 journalistic maps used in news\narticles from five major news outlets published over three months. As a result,\nwe created a design space comprised of eight dimensions that involved both\nproperties describing the articles' aspects and the visual/interactive features\nof maps. We approach the second research question via semi-structured\ninterviews with four data journalists who create data-driven articles daily.\nThrough these interviews, we identified the most common design rationales made\nby editorial teams and potential gaps in current practices. We also collected\nthe practitioners' feedback on our design space to externally validate it. With\nthese results, we aim to provide researchers and journalists with empirical\ndata to design and study journalistic maps.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u65b0\u95fb\u5a92\u4f53\u4e2d\u5730\u56fe\u8bbe\u8ba1\u7684\u6311\u6218\u4e0e\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5206\u6790462\u4e2a\u65b0\u95fb\u5730\u56fe\u548c\u91c7\u8bbf\u6570\u636e\u8bb0\u8005\uff0c\u63d0\u51fa\u4e86\u5305\u542b\u516b\u4e2a\u7ef4\u5ea6\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u603b\u7ed3\u4e86\u5e38\u89c1\u7684\u8bbe\u8ba1\u7406\u7531\u548c\u5b9e\u8df5\u5dee\u8ddd\u3002", "motivation": "\u65b0\u95fb\u5730\u56fe\u8bbe\u8ba1\u9762\u4e34\u591a\u91cd\u6311\u6218\uff0c\u5982\u7f8e\u5b66\u3001\u53d7\u4f17\u6570\u636e\u7d20\u517b\u3001\u7d27\u8feb\u7684\u622a\u6b62\u65e5\u671f\u548c\u6280\u672f\u80fd\u529b\u9650\u5236\uff0c\u7f3a\u4e4f\u4e13\u4e1a\u80cc\u666f\u7684\u6570\u636e\u8bb0\u8005\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u8bbe\u8ba1\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u5206\u6790462\u4e2a\u65b0\u95fb\u5730\u56fe\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u91c7\u8bbf\u56db\u4f4d\u6570\u636e\u8bb0\u8005\uff0c\u786e\u5b9a\u8bbe\u8ba1\u7a7a\u95f4\u548c\u8bbe\u8ba1\u5b9e\u8df5\u3002", "result": "\u63d0\u51fa\u4e86\u5305\u542b\u516b\u4e2a\u7ef4\u5ea6\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u8bc6\u522b\u4e86\u5e38\u89c1\u8bbe\u8ba1\u7406\u7531\u548c\u5b9e\u8df5\u4e2d\u7684\u6f5c\u5728\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u8bb0\u8005\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u548c\u7814\u7a76\u65b0\u95fb\u5730\u56fe\u7684\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2508.11246", "pdf": "https://arxiv.org/pdf/2508.11246", "abs": "https://arxiv.org/abs/2508.11246", "authors": ["Runlong Yu", "Shiyuan Luo", "Rahul Ghosh", "Lingyao Li", "Yiqun Xie", "Xiaowei Jia"], "title": "RAG for Geoscience: What We Expect, Gaps and Opportunities", "categories": ["cs.ET", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances language models by combining\nretrieval with generation. However, its current workflow remains largely\ntext-centric, limiting its applicability in geoscience. Many geoscientific\ntasks are inherently evidence-hungry. Typical examples involve imputing missing\nobservations using analog scenes, retrieving equations and parameters to\ncalibrate models, geolocating field photos based on visual cues, or surfacing\nhistorical case studies to support policy analyses. A simple\n``retrieve-then-generate'' pipeline is insufficient for these needs. We\nenvision Geo-RAG, a next-generation paradigm that reimagines RAG as a modular\nretrieve $\\rightarrow$ reason $\\rightarrow$ generate $\\rightarrow$ verify loop.\nGeo-RAG supports four core capabilities: (i) retrieval of multi-modal Earth\ndata; (ii) reasoning under physical and domain constraints; (iii) generation of\nscience-grade artifacts; and (iv) verification of generated hypotheses against\nnumerical models, ground measurements, and expert assessments. This shift opens\nnew opportunities for more trustworthy and transparent geoscience workflows.", "AI": {"tldr": "Geo-RAG\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u591a\u6a21\u6001\u68c0\u7d22\u548c\u9a8c\u8bc1\u7684\u6a21\u5757\u5316\u5faa\u73af\u5de5\u4f5c\u6d41\uff0c\u4ee5\u589e\u5f3a\u5730\u7403\u79d1\u5b66\u4e2d\u7684\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6587\u672c\u7684RAG\u7cfb\u7edf\u5728\u5730\u7403\u79d1\u5b66\u4e2d\u7684\u9002\u7528\u6027\u6709\u9650\uff0c\u56e0\u4e3a\u8bb8\u591a\u4efb\u52a1\u9700\u8981\u591a\u6a21\u6001\u6570\u636e\u548c\u79d1\u5b66\u9a8c\u8bc1\u3002", "method": "Geo-RAG\u91c7\u7528\u68c0\u7d22\u2192\u63a8\u7406\u2192\u751f\u6210\u2192\u9a8c\u8bc1\u7684\u6a21\u5757\u5316\u5faa\u73af\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u68c0\u7d22\u3001\u79d1\u5b66\u7ea6\u675f\u4e0b\u7684\u63a8\u7406\u3001\u79d1\u5b66\u7ea7\u751f\u6210\u548c\u9a8c\u8bc1\u3002", "result": "Geo-RAG\u4e3a\u5730\u7403\u79d1\u5b66\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u66f4\u53ef\u4fe1\u548c\u900f\u660e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Geo-RAG\u901a\u8fc7\u591a\u6a21\u6001\u548c\u9a8c\u8bc1\u673a\u5236\u6269\u5c55\u4e86RAG\u7684\u80fd\u529b\uff0c\u4e3a\u5730\u7403\u79d1\u5b66\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.11035", "pdf": "https://arxiv.org/pdf/2508.11035", "abs": "https://arxiv.org/abs/2508.11035", "authors": ["Hasibul Jamil", "MD S Q Zulkar Nine", "Tevfik Kosar"], "title": "EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training", "categories": ["cs.DC"], "comment": "SC25 Sustainable Supercomputing Workshop", "summary": "Large-scale deep learning workloads increasingly suffer from I/O bottlenecks\nas datasets grow beyond local storage capacities and GPU compute outpaces\nnetwork and disk latencies. While recent systems optimize data-loading time,\nthey overlook the energy cost of I/O - a critical factor at large scale. We\nintroduce EMLIO, an Efficient Machine Learning I/O service that jointly\nminimizes end-to-end data-loading latency T and I/O energy consumption E across\nvariable-latency networked storage. EMLIO deploys a lightweight data-serving\ndaemon on storage nodes that serializes and batches raw samples, streams them\nover TCP with out-of-order prefetching, and integrates seamlessly with\nGPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive\nevaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)\nenvironments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use\ncompared to state-of-the-art loaders, while maintaining constant performance\nand energy profiles irrespective of network distance. EMLIO's service-based\narchitecture offers a scalable blueprint for energy-aware I/O in\nnext-generation AI clouds.", "AI": {"tldr": "EMLIO\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60I/O\u670d\u52a1\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u52a0\u8f7d\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u6570\u636e\u96c6\u8d85\u51fa\u672c\u5730\u5b58\u50a8\u5bb9\u91cf\u548cGPU\u8ba1\u7b97\u8d85\u8d8a\u7f51\u7edc\u53ca\u78c1\u76d8\u5ef6\u8fdf\uff0c\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u9762\u4e34I/O\u74f6\u9888\u3002\u73b0\u6709\u7cfb\u7edf\u5ffd\u7565\u4e86I/O\u7684\u80fd\u8017\u95ee\u9898\u3002", "method": "EMLIO\u5728\u5b58\u50a8\u8282\u70b9\u90e8\u7f72\u8f7b\u91cf\u7ea7\u6570\u636e\u670d\u52a1\u5b88\u62a4\u8fdb\u7a0b\uff0c\u5bf9\u539f\u59cb\u6837\u672c\u8fdb\u884c\u5e8f\u5217\u5316\u548c\u6279\u5904\u7406\uff0c\u5e76\u901a\u8fc7TCP\u6d41\u4f20\u8f93\uff0c\u540c\u65f6\u4e0eGPU\u52a0\u901f\u9884\u5904\u7406\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u591a\u79cd\u7f51\u7edc\u73af\u5883\u4e0b\uff0cEMLIO\u7684I/O\u901f\u5ea6\u63d0\u53478.6\u500d\uff0c\u80fd\u8017\u964d\u4f4e10.9\u500d\uff0c\u4e14\u6027\u80fd\u4e0e\u80fd\u8017\u4e0d\u53d7\u7f51\u7edc\u8ddd\u79bb\u5f71\u54cd\u3002", "conclusion": "EMLIO\u4e3a\u4e0b\u4e00\u4ee3AI\u4e91\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u80fd\u6e90\u611f\u77e5I/O\u67b6\u6784\u84dd\u56fe\u3002"}}
{"id": "2508.11019", "pdf": "https://arxiv.org/pdf/2508.11019", "abs": "https://arxiv.org/abs/2508.11019", "authors": ["Anuj Dawar", "Aidan T. Evans"], "title": "Characterizing NC1 with Typed Monoids", "categories": ["cs.LO", "cs.FL", "F.1.3; F.4.1; F.4.3"], "comment": "22 pages", "summary": "Krebs et al. (2007) gave a characterization of the complexity class TC0 as\nthe class of languages recognized by a certain class of typed monoids. The\nnotion of typed monoid was introduced to extend methods of algebraic automata\ntheory to infinite monoids and hence characterize classes beyond the regular\nlanguages. We advance this line of work beyond TC0 by giving a characterization\nof NC1. This is obtained by first showing that NC1 can be defined as the\nlanguages expressible in an extension of first-order logic using only unary\nquantifiers over regular languages. The expressibility result is a consequence\nof a general result showing that finite monoid multiplication quantifiers of\nhigher dimension can be replaced with unary quantifiers in the context of\ninterpretations over strings, which also answers a question of Lautemann et al.\n(2001). We establish this collapse result for a much more general class of\ninterpretations using results on interpretations due to Boja\\'nczyk et al.\n(2019), which may be of independent interest.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6269\u5c55\u4e00\u5143\u91cf\u8bcd\u903b\u8f91\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9 NC1 \u590d\u6742\u5ea6\u7c7b\u7684\u65b0\u4ee3\u6570\u523b\u753b\uff0c\u5e76\u89e3\u51b3\u4e86 Lautemann \u7b49\u4eba\uff082001\uff09\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u5c55 Krebs \u7b49\u4eba\uff082007\uff09\u5bf9 TC0 \u7684\u4ee3\u6570\u523b\u753b\u65b9\u6cd5\uff0c\u7814\u7a76\u66f4\u9ad8\u590d\u6742\u5ea6\u7c7b NC1 \u7684\u4ee3\u6570\u7279\u6027\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u4e00\u5143\u91cf\u8bcd\u7684\u903b\u8f91\u8868\u8fbe\uff0c\u7ed3\u5408 Boja\u0144czyk \u7b49\u4eba\uff082019\uff09\u7684\u5b57\u7b26\u4e32\u89e3\u91ca\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86 NC1 \u7684\u7279\u6027\u3002", "result": "\u8bc1\u660e\u4e86 NC1 \u53ef\u4ee5\u901a\u8fc7\u4e00\u5143\u91cf\u8bcd\u903b\u8f91\u523b\u753b\uff0c\u540c\u65f6\u66f4\u9ad8\u7ef4\u5ea6\u7684\u6709\u9650\u5e7a\u534a\u7fa4\u91cf\u8bcd\u53ef\u88ab\u4e00\u5143\u91cf\u8bcd\u66ff\u4ee3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a NC1 \u63d0\u4f9b\u4e86\u65b0\u7684\u4ee3\u6570\u523b\u753b\uff0c\u5e76\u63a8\u52a8\u4e86\u4ee3\u6570\u81ea\u52a8\u673a\u7406\u8bba\u5728\u66f4\u9ad8\u590d\u6742\u5ea6\u7c7b\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2508.11179", "pdf": "https://arxiv.org/pdf/2508.11179", "abs": "https://arxiv.org/abs/2508.11179", "authors": ["Pei Liu", "Terry Zhuo", "Jiawei Deng", "Zhenchang Xing", "Qinghua Lu", "Xiaoning Du", "Hongyu Zhan"], "title": "PTMPicker: Facilitating Efficient Pretrained Model Selection for Application Developers", "categories": ["cs.SE"], "comment": null, "summary": "The rapid emergence of pretrained models (PTMs) has attracted significant\nattention from both Deep Learning (DL) researchers and downstream application\ndevelopers. However, selecting appropriate PTMs remains challenging because\nexisting methods typically rely on keyword-based searches in which the keywords\nare often derived directly from function descriptions. This often fails to\nfully capture user intent and makes it difficult to identify suitable models\nwhen developers also consider factors such as bias mitigation, hardware\nrequirements, or license compliance. To address the limitations of\nkeyword-based model search, we propose PTMPicker to accurately identify\nsuitable PTMs. We first define a structured template composed of common and\nessential attributes for PTMs and then PTMPicker represents both candidate\nmodels and user-intended features (i.e., model search requests) in this unified\nformat. To determine whether candidate models satisfy user requirements, it\ncomputes embedding similarities for function-related attributes and uses\nwell-crafted prompts to evaluate special constraints such as license compliance\nand hardware requirements. We scraped a total of 543,949 pretrained models from\nHugging Face to prepare valid candidates for selection. PTMPicker then\nrepresented them in the predefined structured format by extracting their\nassociated descriptions. Guided by the extracted metadata, we synthesized a\ntotal of 15,207 model search requests with carefully designed prompts, as no\nsuch search requests are readily available. Experiments on the curated PTM\ndataset and the synthesized model search requests show that PTMPicker can help\nusers effectively identify models,with 85% of the sampled requests successfully\nlocating appropriate PTMs within the top-10 ranked candidates.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PTMPicker\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6a21\u677f\u548c\u5d4c\u5165\u76f8\u4f3c\u6027\u8ba1\u7b97\uff0c\u89e3\u51b3\u9884\u8bad\u7ec3\u6a21\u578b\u9009\u62e9\u4e2d\u7684\u5173\u952e\u8bcd\u641c\u7d22\u4e0d\u8db3\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u5173\u952e\u8bcd\u641c\u7d22\uff0c\u96be\u4ee5\u5168\u9762\u6355\u6349\u7528\u6237\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5173\u4e8e\u504f\u89c1\u3001\u786c\u4ef6\u6216\u8bb8\u53ef\u8bc1\u7b49\u56e0\u7d20\u7684\u9700\u6c42\u3002", "method": "\u5b9a\u4e49\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7ed3\u6784\u5316\u6a21\u677f\uff0c\u901a\u8fc7\u5d4c\u5165\u76f8\u4f3c\u6027\u548c\u7279\u5b9a\u63d0\u793a\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u7b26\u5408\u7528\u6237\u9700\u6c42\uff0c\u5e76\u5728Hugging Face\u4e0a\u6536\u96c6\u4e86\u5927\u91cf\u6a21\u578b\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cPTMPicker\u80fd\u5e2e\u52a9\u7528\u6237\u5728\u6392\u540d\u524d10\u7684\u5019\u9009\u6a21\u578b\u4e2d\u6210\u529f\u627e\u523085%\u7684\u5408\u9002\u6a21\u578b\u3002", "conclusion": "PTMPicker\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u548c\u667a\u80fd\u5339\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u9009\u62e9\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.11443", "pdf": "https://arxiv.org/pdf/2508.11443", "abs": "https://arxiv.org/abs/2508.11443", "authors": ["William Henrich Due", "Martin Elsman", "Troels Henriksen"], "title": "Towards Efficient Hash Maps in Functional Array Languages", "categories": ["cs.PL", "cs.DS"], "comment": null, "summary": "We present a systematic derivation of a data-parallel implementation of\ntwo-level, static and collision-free hash maps, by giving a functional\nformulation of the Fredman et al. construction, and then flattening it. We\ndiscuss the challenges of providing a flexible, polymorphic, and abstract\ninterface to hash maps in a functional array language, with particular\nattention paid to the problem of dynamically sized keys, which we address by\nassociating each hash map with an arbitrary context. The algorithm is\nimplemented in Futhark, and the achieved GPU execution performance is compared\non simple benchmark problems. We find that our hash maps outperform\nconventional tree/search-based approaches. Furthermore, our implementation is\ncompared against the state-of-the-art cuCollections library, which is\nsignificantly faster for hash map construction, and to a lesser degree for\nlookups. We explain to which extent the performance difference is due to\nlow-level code generation limitation in the Futhark compiler, and to which\nextent it can be attributed to the data-parallel programming vocabulary not\nproviding the constructs necessary to express the equivalent of the algorithms\nused by cuCollections. We end by reflecting to which extent the functional\narray language programming model could, or should, be extended to address these\nweaknesses.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u63a8\u5bfc\u4e86\u4e00\u79cd\u6570\u636e\u5e76\u884c\u7684\u4e24\u7ea7\u9759\u6001\u65e0\u51b2\u7a81\u54c8\u5e0c\u6620\u5c04\u5b9e\u73b0\uff0c\u901a\u8fc7\u529f\u80fd\u5316Fredman\u7b49\u4eba\u7684\u7ed3\u6784\u5e76\u6241\u5e73\u5316\u5904\u7406\u3002\u8ba8\u8bba\u4e86\u5728\u529f\u80fd\u6570\u7ec4\u8bed\u8a00\u4e2d\u63d0\u4f9b\u7075\u6d3b\u3001\u591a\u6001\u548c\u62bd\u8c61\u7684\u54c8\u5e0c\u6620\u5c04\u63a5\u53e3\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u52a8\u6001\u5927\u5c0f\u952e\u7684\u95ee\u9898\u3002\u7b97\u6cd5\u5728Futhark\u4e2d\u5b9e\u73b0\uff0cGPU\u6267\u884c\u6027\u80fd\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6811/\u641c\u7d22\u65b9\u6cd5\uff0c\u4f46\u8f83cuCollections\u5e93\u5728\u6784\u9020\u4e0a\u7a0d\u6162\u3002\u5206\u6790\u4e86\u6027\u80fd\u5dee\u5f02\u7684\u539f\u56e0\uff0c\u5e76\u63a2\u8ba8\u4e86\u529f\u80fd\u6570\u7ec4\u8bed\u8a00\u6a21\u578b\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u529f\u80fd\u5316\u7684\u6570\u636e\u5e76\u884c\u54c8\u5e0c\u6620\u5c04\u5b9e\u73b0\uff0c\u89e3\u51b3\u52a8\u6001\u5927\u5c0f\u952e\u7684\u6311\u6218\uff0c\u5e76\u5728GPU\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u529f\u80fd\u5316Fredman\u7b49\u4eba\u7684\u6784\u9020\u5e76\u6241\u5e73\u5316\uff0c\u5b9e\u73b0\u4e86\u4e24\u7ea7\u9759\u6001\u65e0\u51b2\u7a81\u54c8\u5e0c\u6620\u5c04\u3002\u5728Futhark\u4e2d\u5b9e\u73b0\uff0c\u5e76\u4e0ecuCollections\u5e93\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u5b9e\u73b0\u7684\u54c8\u5e0c\u6620\u5c04\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46cuCollections\u5e93\u5728\u6784\u9020\u4e0a\u66f4\u5feb\u3002\u5dee\u5f02\u90e8\u5206\u6e90\u4e8eFuthark\u7f16\u8bd1\u5668\u9650\u5236\uff0c\u90e8\u5206\u6e90\u4e8e\u529f\u80fd\u6570\u7ec4\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u529f\u80fd\u6570\u7ec4\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u9700\u8981\u6269\u5c55\u4ee5\u5f25\u8865\u5f53\u524d\u9650\u5236\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.11203", "pdf": "https://arxiv.org/pdf/2508.11203", "abs": "https://arxiv.org/abs/2508.11203", "authors": ["Seungmi Lee", "Kwan Yun", "Junyong Noh"], "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "51-04", "I.3.8; I.4.9"], "comment": "Pacific graphics 2025, CGF, 15 pages", "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).", "AI": {"tldr": "StyleMM\u662f\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u57fa\u4e8e\u7528\u6237\u5b9a\u4e49\u7684\u6587\u672c\u63cf\u8ff0\u6784\u5efa\u98ce\u683c\u53163D\u53ef\u53d8\u5f62\u6a21\u578b\uff083DMM\uff09\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u76ee\u6807\u98ce\u683c\u56fe\u50cf\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u98ce\u683c\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u8eab\u4efd\u548c\u8868\u60c5\u7b49\u5173\u952e\u5c5e\u6027\u3002", "motivation": "\u4e3a\u4e86\u80fd\u591f\u6839\u636e\u6587\u672c\u63cf\u8ff0\u751f\u6210\u98ce\u683c\u5316\u76843D\u4eba\u8138\u6a21\u578b\uff0c\u540c\u65f6\u786e\u4fdd\u8eab\u4efd\u548c\u8868\u60c5\u7684\u4e00\u81f4\u6027\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u7f51\u683c\u53d8\u5f62\u7f51\u7edc\u548c\u7eb9\u7406\u751f\u6210\u5668\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u98ce\u683c\u5316\u56fe\u50cf\u4f5c\u4e3a\u76ee\u6807\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728\u56fe\u50cf\u98ce\u683c\u5316\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u4fdd\u7559\u539f\u59cb\u5c5e\u6027\u3002", "result": "\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8eab\u4efd\u591a\u6837\u6027\u548c\u98ce\u683c\u5316\u80fd\u529b\u3002", "conclusion": "StyleMM\u901a\u8fc7\u4fdd\u7559\u5173\u952e\u5c5e\u6027\u548c\u4e00\u81f4\u7684\u9876\u70b9\u8fde\u63a5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u53163D\u4eba\u8138\u751f\u6210\u3002"}}
{"id": "2508.11366", "pdf": "https://arxiv.org/pdf/2508.11366", "abs": "https://arxiv.org/abs/2508.11366", "authors": ["Sanghoon Lee", "Taehun Kim", "Jiyeong Chae", "Kyung-Joon Park"], "title": "Optimizing ROS 2 Communication for Wireless Robotic Systems", "categories": ["cs.NI", "cs.RO"], "comment": "10 pages, 8 figures", "summary": "Wireless transmission of large payloads, such as high-resolution images and\nLiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source\nrobotics middleware. The default Data Distribution Service (DDS) communication\nstack in ROS 2 exhibits significant performance degradation over lossy wireless\nlinks. Despite the widespread use of ROS 2, the underlying causes of these\nwireless communication challenges remain unexplored. In this paper, we present\nthe first in-depth network-layer analysis of ROS 2's DDS stack under wireless\nconditions with large payloads. We identify the following three key issues:\nexcessive IP fragmentation, inefficient retransmission timing, and congestive\nbuffer bursts. To address these issues, we propose a lightweight and fully\ncompatible DDS optimization framework that tunes communication parameters based\non link and payload characteristics. Our solution can be seamlessly applied\nthrough the standard ROS 2 application interface via simple XML-based QoS\nconfiguration, requiring no protocol modifications, no additional components,\nand virtually no integration efforts. Extensive experiments across various\nwireless scenarios demonstrate that our framework successfully delivers large\npayloads in conditions where existing DDS modes fail, while maintaining low\nend-to-end latency.", "AI": {"tldr": "ROS 2\u4e2d\u65e0\u7ebf\u4f20\u8f93\u5927\u6570\u636e\u8d1f\u8f7d\u5b58\u5728\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7DDS\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3ROS 2\u4e2dDDS\u901a\u4fe1\u6808\u5728\u65e0\u7ebf\u73af\u5883\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5927\u6570\u636e\u8d1f\u8f7d\u4f20\u8f93\u7684\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8f7b\u91cf\u7ea7DDS\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u6574\u901a\u4fe1\u53c2\u6570\u9002\u5e94\u94fe\u8def\u548c\u6570\u636e\u8d1f\u8f7d\u7279\u6027\uff0c\u65e0\u9700\u534f\u8bae\u4fee\u6539\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u591a\u79cd\u65e0\u7ebf\u573a\u666f\u4e0b\u6210\u529f\u4f20\u8f93\u5927\u6570\u636e\u8d1f\u8f7d\uff0c\u4fdd\u6301\u4e86\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u4f18\u5316\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86ROS 2\u65e0\u7ebf\u4f20\u8f93\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5177\u6709\u9ad8\u517c\u5bb9\u6027\u548c\u6613\u7528\u6027\u3002"}}
{"id": "2508.10942", "pdf": "https://arxiv.org/pdf/2508.10942", "abs": "https://arxiv.org/abs/2508.10942", "authors": ["Liming Xu", "Dave Towey", "Andrew P. French", "Steve Benford"], "title": "Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram", "categories": ["cs.CV", "cs.HC", "cs.MM", "I.4.10; I.5.4"], "comment": "This work is an extension of an ACM MM'17 workshop paper (Xu et al,\n  2017), which was completed in late 2017 and early 2018 during the first\n  author's doctoral studies at the University of Nottingham. This paper\n  includes 42 pages, 25 figures, 7 tables, and 13,536 words", "summary": "The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it\nis expected that our everyday environment may soon be decorating with objects\nconnecting with virtual elements. Alerting to the presence of these objects is\ntherefore the first step for motivating follow-up further inspection and\ntriggering digital material attached to the objects. This work studies a\nspecial kind of these objects -- Artcodes -- a human-meaningful and\nmachine-readable decorative markers that camouflage themselves with freeform\nappearance by encoding information into their topology. We formulate this\nproblem of recongising the presence of Artcodes as Artcode proposal detection,\na distinct computer vision task that classifies topologically similar but\ngeometrically and semantically different objects as a same class. To deal with\nthis problem, we propose a new feature descriptor, called the shape of\norientation histogram, to describe the generic topological structure of an\nArtcode. We collect datasets and conduct comprehensive experiments to evaluate\nthe performance of the Artcode detection proposer built upon this new feature\nvector. Our experimental results show the feasibility of the proposed feature\nvector for representing topological structures and the effectiveness of the\nsystem for detecting Artcode proposals. Although this work is an initial\nattempt to develop a feature-based system for detecting topological objects\nlike Artcodes, it would open up new interaction opportunities and spark\npotential applications of topological object detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u68c0\u6d4b\u4f2a\u88c5\u6210\u81ea\u7531\u5f62\u6001\u7684Artcodes\u6807\u8bb0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u63cf\u8ff0\u7b26\uff08\u5f62\u72b6\u65b9\u5411\u76f4\u65b9\u56fe\uff09\u6765\u63cf\u8ff0\u5176\u62d3\u6251\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u624b\u673a\u548cVR/AR\u6280\u672f\u7684\u666e\u53ca\uff0c\u73af\u5883\u4e2d\u5c06\u51fa\u73b0\u66f4\u591a\u4e0e\u865a\u62df\u5143\u7d20\u8fde\u63a5\u7684\u7269\u4f53\uff0c\u8bc6\u522b\u8fd9\u4e9b\u7269\u4f53\uff08\u5982Artcodes\uff09\u662f\u89e6\u53d1\u540e\u7eed\u6570\u5b57\u5185\u5bb9\u7684\u7b2c\u4e00\u6b65\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5f62\u72b6\u65b9\u5411\u76f4\u65b9\u56fe\u7684\u7279\u5f81\u63cf\u8ff0\u7b26\uff0c\u7528\u4e8e\u63cf\u8ff0Artcode\u7684\u901a\u7528\u62d3\u6251\u7ed3\u6784\uff0c\u5e76\u6784\u5efa\u4e86Artcode\u68c0\u6d4b\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7279\u5f81\u63cf\u8ff0\u7b26\u80fd\u6709\u6548\u8868\u793a\u62d3\u6251\u7ed3\u6784\uff0c\u7cfb\u7edf\u5728\u68c0\u6d4bArtcode\u63d0\u6848\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u5c3d\u7ba1\u8fd9\u662f\u57fa\u4e8e\u7279\u5f81\u7684\u62d3\u6251\u5bf9\u8c61\u68c0\u6d4b\u7684\u521d\u6b65\u5c1d\u8bd5\uff0c\u4f46\u4e3a\u672a\u6765\u4ea4\u4e92\u548c\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.11467", "pdf": "https://arxiv.org/pdf/2508.11467", "abs": "https://arxiv.org/abs/2508.11467", "authors": ["Shifang Liu", "Huiyuan Li", "Hongjiao Sheng", "Haoyuan Gui", "Xiaoyu Zhang"], "title": "Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method", "categories": ["cs.DC", "cs.PF"], "comment": null, "summary": "Singular Value Decomposition (SVD) is a fundamental matrix factorization\ntechnique in linear algebra, widely applied in numerous matrix-related\nproblems. However, traditional SVD approaches are hindered by slow panel\nfactorization and frequent CPU-GPU data transfers in heterogeneous systems,\ndespite advancements in GPU computational capabilities. In this paper, we\nintroduce a GPU-centered SVD algorithm, incorporating a novel GPU-based\nbidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and\ndata layout of different steps for SVD computation, performing all panel-level\ncomputations and trailing matrix updates entirely on GPU to eliminate CPU-GPU\ndata transfers. Furthermore, we integrate related computations to optimize BLAS\nutilization, thereby increasing arithmetic intensity and fully leveraging the\ncomputational capabilities of GPUs. Additionally, we introduce a newly\ndeveloped GPU-based BDC algorithm that restructures the workflow to eliminate\nmatrix-level CPU-GPU data transfers and enable asynchronous execution between\nthe CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs\ndemonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x\nand 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5GPU\u4e3a\u4e2d\u5fc3\u7684SVD\u7b97\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684GPU\u53cc\u5411\u5206\u6cbb\u65b9\u6cd5\u548c\u4f18\u5316\u6570\u636e\u5e03\u5c40\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edfSVD\u65b9\u6cd5\u5728\u5f02\u6784\u7cfb\u7edf\u4e2d\u5b58\u5728\u9762\u677f\u5206\u89e3\u6162\u548cCPU-GPU\u6570\u636e\u4f20\u8f93\u9891\u7e41\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86GPU\u8ba1\u7b97\u80fd\u529b\u7684\u53d1\u6325\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u7684\u53cc\u5411\u5206\u6cbb\u65b9\u6cd5\uff0c\u91cd\u65b0\u8bbe\u8ba1\u4e86\u7b97\u6cd5\u548c\u6570\u636e\u5e03\u5c40\uff0c\u5b8c\u5168\u5728GPU\u4e0a\u8fdb\u884c\u9762\u677f\u7ea7\u8ba1\u7b97\u548c\u5c3e\u968f\u77e9\u9635\u66f4\u65b0\uff0c\u5e76\u4f18\u5316\u4e86BLAS\u5229\u7528\u7387\u3002", "result": "\u5728AMD MI210\u548cNVIDIA V100 GPU\u4e0a\uff0c\u4e0erocSOLVER/cuSOLVER\u548cMAGMA\u76f8\u6bd4\uff0c\u901f\u5ea6\u5206\u522b\u63d0\u5347\u4e861293.64\u500d/7.47\u500d\u548c14.10\u500d/12.38\u500d\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u4e86CPU-GPU\u6570\u636e\u4f20\u8f93\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5c55\u73b0\u4e86GPU\u5728\u9ad8\u6027\u80fd\u7ebf\u6027\u4ee3\u6570\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.11090", "pdf": "https://arxiv.org/pdf/2508.11090", "abs": "https://arxiv.org/abs/2508.11090", "authors": ["Daniel Mas Montserrat", "David Bonet", "Maria Perera", "Xavier Gir\u00f3-i-Nieto", "Alexander G. Ioannidis"], "title": "Compressive Meta-Learning", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DB", "68T07, 68T05, 68T09", "I.2.6; I.5.1; G.3; H.2.8"], "comment": "Extended version of a paper accepted at KDD '25", "summary": "The rapid expansion in the size of new datasets has created a need for fast\nand efficient parameter-learning techniques. Compressive learning is a\nframework that enables efficient processing by using random, non-linear\nfeatures to project large-scale databases onto compact, information-preserving\nrepresentations whose dimensionality is independent of the number of samples\nand can be easily stored, transferred, and processed. These database-level\nsummaries are then used to decode parameters of interest from the underlying\ndata distribution without requiring access to the original samples, offering an\nefficient and privacy-friendly learning framework. However, both the encoding\nand decoding techniques are typically randomized and data-independent, failing\nto exploit the underlying structure of the data. In this work, we propose a\nframework that meta-learns both the encoding and decoding stages of compressive\nlearning methods by using neural networks that provide faster and more accurate\nsystems than the current state-of-the-art approaches. To demonstrate the\npotential of the presented Compressive Meta-Learning framework, we explore\nmultiple applications -- including neural network-based compressive PCA,\ncompressive ridge regression, compressive k-means, and autoencoders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u538b\u7f29\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u6539\u8fdb\u538b\u7f29\u5b66\u4e60\u7684\u7f16\u7801\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u63d0\u9ad8\u4e86\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u6570\u636e\u96c6\u89c4\u6a21\u7684\u5feb\u901f\u6269\u5927\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u5feb\u901f\u7684\u53c2\u6570\u5b66\u4e60\u6280\u672f\u3002\u4f20\u7edf\u538b\u7f29\u5b66\u4e60\u65b9\u6cd5\u672a\u5229\u7528\u6570\u636e\u7684\u6f5c\u5728\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5143\u5b66\u4e60\u538b\u7f29\u5b66\u4e60\u7684\u7f16\u7801\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u5e94\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u5982\u538b\u7f29PCA\u3001\u538b\u7f29\u5cad\u56de\u5f52\u7b49\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u538b\u7f29\u5143\u5b66\u4e60\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10907", "pdf": "https://arxiv.org/pdf/2508.10907", "abs": "https://arxiv.org/abs/2508.10907", "authors": ["Euihyeok Lee", "Souneil Park", "Jin Yu", "Seungchul Lee", "Seungwoo Kang"], "title": "Designing for Engaging Communication Between Parents and Young Adult Children Through Shared Music Experiences", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "This paper aims to foster social interaction between parents and young adult\nchildren living apart via music. Our approach transforms their music-listening\nmoment into an opportunity to listen to the other's favorite songs and enrich\ninteraction in their daily lives. To this end, we explore the current practice\nand needs of parent-child communication and the experience and perception of\nmusic-mediated interaction. Based on the findings, we developed DJ-Fam, a\nmobile application that enables parents and children to listen to their\nfavorite songs and use them as conversation starters to foster parent-child\ninteraction. From our deployment study with seven families over four weeks in\nSouth Korea, we show the potential of DJ-Fam to influence parent-child\ninteraction and their mutual understanding and relationship positively.\nSpecifically, DJ-Fam considerably increases the frequency of communication and\ndiversifies the communication channels and topics, all of which are\nsatisfactory to the participants.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u97f3\u4e50\u4fc3\u8fdb\u7236\u6bcd\u4e0e\u6210\u5e74\u5b50\u5973\u7684\u793e\u4ea4\u4e92\u52a8\uff0c\u5f00\u53d1\u4e86\u4e00\u6b3e\u540d\u4e3aDJ-Fam\u7684\u79fb\u52a8\u5e94\u7528\uff0c\u901a\u8fc7\u5206\u4eab\u6b4c\u66f2\u6765\u4e30\u5bcc\u65e5\u5e38\u4ea4\u6d41\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6539\u5584\u7236\u6bcd\u4e0e\u6210\u5e74\u5b50\u5973\u56e0\u5206\u5c45\u800c\u51cf\u5c11\u7684\u793e\u4ea4\u4e92\u52a8\uff0c\u5229\u7528\u97f3\u4e50\u4f5c\u4e3a\u5a92\u4ecb\u589e\u5f3a\u4ed6\u4eec\u7684\u6c9f\u901a\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u73b0\u6709\u7684\u4eb2\u5b50\u6c9f\u901a\u9700\u6c42\u53ca\u97f3\u4e50\u4e92\u52a8\u4f53\u9a8c\uff0c\u5f00\u53d1\u4e86DJ-Fam\u5e94\u7528\uff0c\u5e76\u8fdb\u884c\u4e86\u4e3a\u671f\u56db\u5468\u7684\u5b9e\u5730\u6d4b\u8bd5\u3002", "result": "DJ-Fam\u663e\u8457\u589e\u52a0\u4e86\u4eb2\u5b50\u6c9f\u901a\u7684\u9891\u7387\u548c\u591a\u6837\u6027\uff0c\u63d0\u5347\u4e86\u5f7c\u6b64\u7684\u7406\u89e3\u548c\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u97f3\u4e50\u53ef\u4ee5\u6210\u4e3a\u4fc3\u8fdb\u4eb2\u5b50\u4e92\u52a8\u7684\u6709\u6548\u5a92\u4ecb\uff0cDJ-Fam\u7684\u5e94\u7528\u663e\u793a\u51fa\u79ef\u6781\u7684\u793e\u4ea4\u6548\u679c\u3002"}}
{"id": "2508.10904", "pdf": "https://arxiv.org/pdf/2508.10904", "abs": "https://arxiv.org/abs/2508.10904", "authors": ["Jie Lei", "Ruofan Jia", "J. Andrew Zhang", "Hao Zhang"], "title": "A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation", "categories": ["cs.CL", "cs.AR", "cs.PL"], "comment": "15 pages, 6 figures", "summary": "In wireless communication systems, stringent requirements such as ultra-low\nlatency and power consumption have significantly increased the demand for\nefficient algorithm-to-hardware deployment. However, a persistent and\nsubstantial gap remains between algorithm design and hardware implementation.\nBridging this gap traditionally requires extensive domain expertise and\ntime-consuming manual development, due to fundamental mismatches between\nhigh-level programming languages like MATLAB and hardware description languages\n(HDLs) such as Verilog-in terms of memory access patterns, data processing\nmanners, and datatype representations. To address this challenge, we propose\nA2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large\nlanguage models (LLMs), designed to enable agile and reliable\nalgorithm-to-hardware translation. A2HCoder introduces a hierarchical framework\nthat enhances both robustness and interpretability while suppressing common\nhallucination issues in LLM-generated code. In the horizontal dimension,\nA2HCoder decomposes complex algorithms into modular functional blocks,\nsimplifying code generation and improving consistency. In the vertical\ndimension, instead of relying on end-to-end generation, A2HCoder performs\nstep-by-step, fine-grained translation, leveraging external toolchains such as\nMATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured\nprocess significantly mitigates hallucinations and ensures hardware-level\ncorrectness. We validate A2HCoder through a real-world deployment case in the\n5G wireless communication domain, demonstrating its practicality, reliability,\nand deployment efficiency.", "AI": {"tldr": "A2HCoder\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u7b97\u6cd5\u5230HDL\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u65e8\u5728\u9ad8\u6548\u3001\u53ef\u9760\u5730\u5c06\u7b97\u6cd5\u8f6c\u6362\u4e3a\u786c\u4ef6\u4ee3\u7801\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5bf9\u8d85\u4f4e\u5ef6\u8fdf\u548c\u529f\u8017\u7684\u4e25\u683c\u8981\u6c42\u589e\u52a0\u4e86\u5bf9\u9ad8\u6548\u7b97\u6cd5\u5230\u786c\u4ef6\u90e8\u7f72\u7684\u9700\u6c42\uff0c\u4f46\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u786c\u4ef6\u5b9e\u73b0\u4e4b\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "A2HCoder\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7b97\u6cd5\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u529f\u80fd\u5757\uff08\u6c34\u5e73\u7ef4\u5ea6\uff09\uff0c\u5e76\u9010\u6b65\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7ffb\u8bd1\uff08\u5782\u76f4\u7ef4\u5ea6\uff09\uff0c\u5229\u7528\u5916\u90e8\u5de5\u5177\u94fe\u786e\u4fdd\u6b63\u786e\u6027\u3002", "result": "\u901a\u8fc75G\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u5b9e\u9645\u90e8\u7f72\u6848\u4f8b\u9a8c\u8bc1\uff0cA2HCoder\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u6027\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6548\u7387\u3002", "conclusion": "A2HCoder\u4e3a\u7b97\u6cd5\u5230\u786c\u4ef6\u7684\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u4e00\u79cd\u654f\u6377\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86LLM\u751f\u6210\u4ee3\u7801\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2508.11395", "pdf": "https://arxiv.org/pdf/2508.11395", "abs": "https://arxiv.org/abs/2508.11395", "authors": ["Kevin McNamara", "Rhea Pritham Marpu"], "title": "Banking 2.0: The Stablecoin Banking Revolution -- How Digital Assets Are Reshaping Global Finance", "categories": ["cs.ET", "cs.CE", "cs.CR", "cs.CY", "econ.GN", "q-fin.EC"], "comment": null, "summary": "The global financial system stands at an inflection point. Stablecoins\nrepresent the most significant evolution in banking since the abandonment of\nthe gold standard, positioned to enable \"Banking 2.0\" by seamlessly integrating\ncryptocurrency innovation with traditional finance infrastructure. This\ntransformation rivals artificial intelligence as the next major disruptor in\nthe financial sector. Modern fiat currencies derive value entirely from\ninstitutional trust rather than physical backing, creating vulnerabilities that\nstablecoins address through enhanced stability, reduced fraud risk, and unified\nglobal transactions that transcend national boundaries. Recent developments\ndemonstrate accelerating institutional adoption: landmark U.S. legislation\nincluding the GENIUS Act of 2025, strategic industry pivots from major players\nlike JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive \"Pay\nwith Crypto\" service. Widespread stablecoin implementation addresses critical\nmacroeconomic imbalances, particularly the inflation-productivity gap plaguing\nmodern monetary systems, through more robust and diversified backing\nmechanisms. Furthermore, stablecoins facilitate deregulation and efficiency\ngains, paving the way for a more interconnected international financial system.\nThis whitepaper comprehensively explores how stablecoins are poised to reshape\nbanking, supported by real-world examples, current market data, and analysis of\ntheir transformative potential.", "AI": {"tldr": "\u7a33\u5b9a\u5e01\u662f\u94f6\u884c\u4e1a\u81ea\u653e\u5f03\u91d1\u672c\u4f4d\u4ee5\u6765\u6700\u91cd\u8981\u7684\u521b\u65b0\uff0c\u901a\u8fc7\u7ed3\u5408\u52a0\u5bc6\u8d27\u5e01\u4e0e\u4f20\u7edf\u91d1\u878d\u57fa\u7840\u8bbe\u65bd\uff0c\u63a8\u52a8\u2018\u94f6\u884c2.0\u2019\uff0c\u5e76\u53ef\u80fd\u6210\u4e3a\u91d1\u878d\u9886\u57df\u7684\u4e0b\u4e00\u4e2a\u91cd\u5927\u98a0\u8986\u8005\u3002", "motivation": "\u73b0\u4ee3\u6cd5\u5b9a\u8d27\u5e01\u4f9d\u8d56\u673a\u6784\u4fe1\u4efb\u800c\u975e\u5b9e\u7269\u652f\u6301\uff0c\u5bfc\u81f4\u8106\u5f31\u6027\uff0c\u7a33\u5b9a\u5e01\u901a\u8fc7\u589e\u5f3a\u7a33\u5b9a\u6027\u3001\u964d\u4f4e\u6b3a\u8bc8\u98ce\u9669\u548c\u5b9e\u73b0\u8de8\u56fd\u7edf\u4e00\u4ea4\u6613\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5206\u6790\u7a33\u5b9a\u5e01\u7684\u52a0\u901f\u673a\u6784\u91c7\u7528\u6848\u4f8b\uff0c\u5982\u7f8e\u56fd\u20182025\u5e74GENIUS\u6cd5\u6848\u2019\u3001\u6469\u6839\u5927\u901a\u7684\u52a0\u5bc6\u8d27\u5e01\u8d37\u6b3e\u5021\u8bae\u53caPayPal\u7684\u2018\u7528\u52a0\u5bc6\u8d27\u5e01\u652f\u4ed8\u2019\u670d\u52a1\u3002", "result": "\u7a33\u5b9a\u5e01\u7684\u5b9e\u65bd\u80fd\u591f\u89e3\u51b3\u5b8f\u89c2\u7ecf\u6d4e\u5931\u8861\u95ee\u9898\uff08\u5982\u901a\u80c0\u4e0e\u751f\u4ea7\u529b\u5dee\u8ddd\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u6837\u5316\u7684\u652f\u6301\u673a\u5236\u63d0\u5347\u91d1\u878d\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7a33\u5b9a\u5e01\u6709\u671b\u91cd\u5851\u94f6\u884c\u4e1a\uff0c\u63a8\u52a8\u66f4\u9ad8\u6548\u3001\u4e92\u8054\u7684\u56fd\u9645\u91d1\u878d\u4f53\u7cfb\uff0c\u5177\u5907\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2508.11266", "pdf": "https://arxiv.org/pdf/2508.11266", "abs": "https://arxiv.org/abs/2508.11266", "authors": ["Ailiya Borjigin", "Cong He", "Charles CC Lee", "Wei Zhou"], "title": "Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets", "categories": ["cs.DC", "cs.CY"], "comment": "8 Pages, Submitted to RASSE 2025", "summary": "Alternative assets such as mines, power plants, or infrastructure projects\nare often large, heterogeneous bundles of resources, rights, and outputs whose\nvalue is difficult to trade or fractionalize under traditional frameworks. This\npaper proposes a novel two-tier tokenization architecture to enhance the\nliquidity and transparency of such complex assets. We introduce the concepts of\nElement Tokens and Everything Tokens: elemental tokens represent standardized,\nfully collateralized components of an asset (e.g., outputs, rights, or\ncredits), while an everything token represents the entire asset as a fixed\ncombination of those elements. The architecture enables both fine-grained\npartial ownership and integrated whole-asset ownership through a system of\ntwo-way convertibility. We detail the design and mechanics of this system,\nincluding an arbitrage mechanism that keeps the price of the composite token\naligned with the net asset value of its constituents. Through illustrative\nexamples in the energy and industrial sectors, we demonstrate that our approach\nallows previously illiquid, high-value projects to be fractionalized and traded\nakin to stocks or exchange-traded funds (ETFs). We discuss the benefits for\ninvestors and asset owners, such as lower entry barriers, improved price\ndiscovery, and flexible financing, as well as the considerations for\nimplementation and regulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u5c42\u4ee3\u5e01\u5316\u67b6\u6784\uff0c\u901a\u8fc7 Element Tokens \u548c Everything Tokens \u63d0\u5347\u590d\u6742\u8d44\u4ea7\u7684\u6d41\u52a8\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u4f20\u7edf\u6846\u67b6\u4e0b\u96be\u4ee5\u4ea4\u6613\u6216\u5206\u5272\u5927\u578b\u5f02\u8d28\u8d44\u4ea7\uff08\u5982\u77ff\u5c71\u3001\u53d1\u7535\u5382\uff09\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u90e8\u5206\u6240\u6709\u6743\u548c\u6574\u4f53\u6240\u6709\u6743\u7684\u7075\u6d3b\u8f6c\u6362\u3002", "method": "\u8bbe\u8ba1\u53cc\u5c42\u4ee3\u5e01\u7cfb\u7edf\uff08Element Tokens \u548c Everything Tokens\uff09\uff0c\u5f15\u5165\u53cc\u5411\u53ef\u8f6c\u6362\u673a\u5236\u548c\u5957\u5229\u673a\u5236\u4ee5\u4fdd\u6301\u4ee3\u5e01\u4ef7\u683c\u4e0e\u8d44\u4ea7\u51c0\u503c\u4e00\u81f4\u3002", "result": "\u901a\u8fc7\u80fd\u6e90\u548c\u5de5\u4e1a\u9886\u57df\u7684\u6848\u4f8b\uff0c\u8bc1\u660e\u8be5\u7cfb\u7edf\u80fd\u591f\u5c06\u9ad8\u4ef7\u503c\u3001\u4f4e\u6d41\u52a8\u6027\u8d44\u4ea7\u5206\u5272\u6210\u7c7b\u4f3c\u80a1\u7968\u6216ETF\u7684\u53ef\u4ea4\u6613\u5355\u4f4d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u6295\u8d44\u95e8\u69db\uff0c\u63d0\u5347\u4e86\u4ef7\u683c\u53d1\u73b0\u80fd\u529b\uff0c\u5e76\u4e3a\u8d44\u4ea7\u6240\u6709\u8005\u548c\u6295\u8d44\u8005\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u878d\u8d44\u548c\u4ea4\u6613\u9009\u62e9\u3002"}}
{"id": "2508.11136", "pdf": "https://arxiv.org/pdf/2508.11136", "abs": "https://arxiv.org/abs/2508.11136", "authors": ["Richard Waldinger"], "title": "Automating the Derivation of Unification Algorithms: A Case Study in Deductive Program Synthesis", "categories": ["cs.LO", "D.2.4; F.3.1; F.4.1"], "comment": "92 pages", "summary": "The unification algorithm has long been a target for program synthesis\nresearch, but a fully automatic derivation remains a research goal. In\ndeductive program synthesis, computer programming is phrased as a task in\ntheorem proving; a declarative specification is expressed in logical form and\npresented to an automatic theorem prover, and a program meeting the\nspecification is extracted from the proof. The correctness of the program is\nsupported by the proof, which also provides an explanation of how the program\nworks. The proof is conducted in an appropriate axiomatic subject-domain\ntheory, which defines the concepts in the specification and the constructs in\nthe target programming language and provides the background knowledge necessary\nto connect them.\n  For the unification proof, we generalize and automate the manual proof\npresented in Manna and Waldinger [1981]. The new program unifies two given\nsymbolic expressions (s-expressions) relative to a given \"environment\"\nsubstitution. The proof establishes the existence of an output substitution\nthat is a most-general idempotent unifier of the given expressions and is an\n\"extension\" of the environment substitution. If no such substitution exists and\nthe expressions are not unifiable, the program is to produce a failure\nindicator.\n  Initially the environment substitution is the empty substitution, which makes\nno replacements at all; during execution of recursive calls, the environment\nsubstitution records the replacements that have been found so far. Our own\nunification algorithm employs an environment, and such algorithms appear in the\nliterature [e.g., Luger and Stubblefield, 1997]. We suspect, in addition to\nbeing more efficient, the three-argument algorithm with an environment is\neasier to synthesize automatically than the two-argument version from the\nManna-Waldinger paper.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u6f14\u7ece\u7a0b\u5e8f\u5408\u6210\u81ea\u52a8\u63a8\u5bfc\u7edf\u4e00\u7b97\u6cd5\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5b9a\u7406\u8bc1\u660e\u7684\u601d\u8def\uff0c\u4ece\u903b\u8f91\u89c4\u8303\u4e2d\u63d0\u53d6\u6ee1\u8db3\u6761\u4ef6\u7684\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7\u8bc1\u660e\u5176\u6b63\u786e\u6027\u3002", "motivation": "\u7edf\u4e00\u7b97\u6cd5\u7684\u81ea\u52a8\u63a8\u5bfc\u4e00\u76f4\u662f\u7a0b\u5e8f\u5408\u6210\u7814\u7a76\u7684\u76ee\u6807\uff0c\u4f46\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u5b9e\u73b0\u4ecd\u6709\u5f85\u7a81\u7834\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6f14\u7ece\u7a0b\u5e8f\u5408\u6210\u7684\u65b9\u6cd5\uff0c\u4ece\u903b\u8f91\u89c4\u8303\u4e2d\u81ea\u52a8\u751f\u6210\u6b63\u786e\u7684\u7edf\u4e00\u7b97\u6cd5\u3002", "method": "\u672c\u6587\u6269\u5c55\u5e76\u81ea\u52a8\u5316\u4e86Manna\u548cWaldinger\uff081981\uff09\u7684\u624b\u52a8\u8bc1\u660e\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u53c2\u6570\uff08\u4e24\u4e2a\u8868\u8fbe\u5f0f\u52a0\u4e00\u4e2a\u73af\u5883\u66ff\u6362\uff09\u7684\u7edf\u4e00\u7b97\u6cd5\uff0c\u901a\u8fc7\u9012\u5f52\u8c03\u7528\u8bb0\u5f55\u73af\u5883\u66ff\u6362\u7684\u53d8\u5316\u3002", "result": "\u65b0\u7b97\u6cd5\u80fd\u591f\u751f\u6210\u4e00\u4e2a\u6700\u5e7f\u4e49\u5e42\u7b49\u7edf\u4e00\u5668\uff0c\u6216\u5728\u4e0d\u6ee1\u8db3\u6761\u4ef6\u65f6\u8fd4\u56de\u5931\u8d25\u3002\u4e09\u53c2\u6570\u7248\u672c\u6bd4\u4e24\u53c2\u6570\u7248\u672c\u66f4\u9ad8\u6548\u4e14\u66f4\u6613\u4e8e\u81ea\u52a8\u5408\u6210\u3002", "conclusion": "\u57fa\u4e8e\u73af\u5883\u7684\u7edf\u4e00\u7b97\u6cd5\u5728\u6548\u7387\u4e0e\u81ea\u52a8\u5408\u6210\u96be\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u7a0b\u5e8f\u5408\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2508.11222", "pdf": "https://arxiv.org/pdf/2508.11222", "abs": "https://arxiv.org/abs/2508.11222", "authors": ["Haonan Zhang", "Dongxia Wang", "Yi Liu", "Kexin Chen", "Jiashui Wang", "Xinlei Ying", "Long Liu", "Wenhai Wang"], "title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously\nrejecting benign queries due to overly conservative safety measures - a\ncritical functional flaw that undermines their reliability and usability.\nCurrent methods for testing this behavior are demonstrably inadequate,\nsuffering from flawed benchmarks and limited test generation capabilities, as\nhighlighted by our empirical user study. To the best of our knowledge, this\npaper introduces the first evolutionary testing framework, ORFuzz, for the\nsystematic detection and analysis of LLM over-refusals. ORFuzz uniquely\nintegrates three core components: (1) safety category-aware seed selection for\ncomprehensive test coverage, (2) adaptive mutator optimization using reasoning\nLLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge\nmodel validated to accurately reflect user perception of toxicity and refusal.\nOur extensive evaluations demonstrate that ORFuzz generates diverse, validated\nover-refusal instances at a rate (6.98% average) more than double that of\nleading baselines, effectively uncovering vulnerabilities. Furthermore,\nORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly\ntransferable test cases that achieves a superior 63.56% average over-refusal\nrate across 10 diverse LLMs, significantly outperforming existing datasets.\nORFuzz and ORFuzzSet provide a robust automated testing framework and a\nvaluable community resource, paving the way for developing more reliable and\ntrustworthy LLM-based software systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faORFuzz\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "LLMs\u7684\u8fc7\u5ea6\u62d2\u7edd\u884c\u4e3a\uff08\u65e0\u5bb3\u67e5\u8be2\u88ab\u9519\u8bef\u62d2\u7edd\uff09\u635f\u5bb3\u5176\u53ef\u9760\u6027\u548c\u53ef\u7528\u6027\uff0c\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u4e0d\u8db3\u3002", "method": "ORFuzz\u6574\u5408\u4e09\u7c7b\u6838\u5fc3\uff1a\u5b89\u5168\u7c7b\u522b\u611f\u77e5\u79cd\u5b50\u9009\u62e9\u3001\u81ea\u9002\u5e94\u7a81\u53d8\u4f18\u5316\u548c\u4eba\u7c7b\u5bf9\u9f50\u7684OR-Judge\u6a21\u578b\u3002", "result": "ORFuzz\u751f\u6210\u591a\u6837\u5316\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e73\u5747\u68c0\u6d4b\u73876.98%\uff1bORFuzzSet\u57fa\u51c6\u572810\u79cdLLMs\u4e2d\u5e73\u5747\u62d2\u7edd\u7387\u8fbe63.56%\u3002", "conclusion": "ORFuzz\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760LLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u6d4b\u8bd5\u6846\u67b6\u548c\u6709\u4ef7\u503c\u7684\u793e\u533a\u8d44\u6e90\u3002"}}
{"id": "2508.11476", "pdf": "https://arxiv.org/pdf/2508.11476", "abs": "https://arxiv.org/abs/2508.11476", "authors": ["Qian Liang", "Zichong Chen", "Yang Zhou", "Hui Huang"], "title": "SPG: Style-Prompting Guidance for Style-Specific Content Creation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to the Journal track of Pacific Graphics 2025", "summary": "Although recent text-to-image (T2I) diffusion models excel at aligning\ngenerated images with textual prompts, controlling the visual style of the\noutput remains a challenging task. In this work, we propose Style-Prompting\nGuidance (SPG), a novel sampling strategy for style-specific image generation.\nSPG constructs a style noise vector and leverages its directional deviation\nfrom unconditional noise to guide the diffusion process toward the target style\ndistribution. By integrating SPG with Classifier-Free Guidance (CFG), our\nmethod achieves both semantic fidelity and style consistency. SPG is simple,\nrobust, and compatible with controllable frameworks like ControlNet and\nIPAdapter, making it practical and widely applicable. Extensive experiments\ndemonstrate the effectiveness and generality of our approach compared to\nstate-of-the-art methods. Code is available at\nhttps://github.com/Rumbling281441/SPG.", "AI": {"tldr": "SPG\u662f\u4e00\u79cd\u65b0\u9896\u7684\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u6784\u5efa\u98ce\u683c\u566a\u58f0\u5411\u91cf\u548c\u5229\u7528\u5176\u65b9\u5411\u504f\u5dee\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u98ce\u683c\u7279\u5b9a\u7684\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a7\u5236\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u98ce\u683c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faStyle-Prompting Guidance (SPG)\uff0c\u7ed3\u5408Classifier-Free Guidance (CFG)\uff0c\u5229\u7528\u98ce\u683c\u566a\u58f0\u5411\u91cf\u7684\u65b9\u5411\u504f\u5dee\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\u3002", "result": "SPG\u5728\u4fdd\u6301\u8bed\u4e49\u5fe0\u5b9e\u5ea6\u548c\u98ce\u683c\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "SPG\u662f\u4e00\u79cd\u7b80\u5355\u3001\u6709\u6548\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.11475", "pdf": "https://arxiv.org/pdf/2508.11475", "abs": "https://arxiv.org/abs/2508.11475", "authors": ["Ioannis Panitsas", "Akrit Mudvari", "Leandros Tassiulas"], "title": "D2Q Synchronizer: Distributed SDN Synchronization for Time Sensitive Applications", "categories": ["cs.NI"], "comment": null, "summary": "In distributed Software-Defined Networking (SDN), distributed SDN controllers\nrequire synchronization to maintain a global network state. Despite the\navailability of synchronization policies for distributed SDN architectures,\nmost policies do not consider joint optimization of network and user\nperformance. In this work, we propose a reinforcement learning-based algorithm\ncalled D2Q Synchronizer, to minimize long-term network costs by strategically\noffloading time-sensitive tasks to cost-effective edge servers while satisfying\nthe latency requirements for all tasks. Evaluation results demonstrate the\nsuperiority of our synchronizer compared to heuristic and other learning\npolicies in literature, by reducing network costs by at least 45% and 10%,\nrespectively, while ensuring the QoS requirements for all user tasks across\ndynamic and multi-domain SDN networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684D2Q Synchronizer\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u5e03\u5f0fSDN\u4e2d\u7684\u63a7\u5236\u5668\u540c\u6b65\uff0c\u4f18\u5316\u7f51\u7edc\u548c\u7528\u6237\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0fSDN\u7684\u540c\u6b65\u7b56\u7565\u672a\u5145\u5206\u4f18\u5316\u7f51\u7edc\u548c\u7528\u6237\u6027\u80fd\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5D2Q Synchronizer\uff0c\u5c06\u65f6\u654f\u611f\u4efb\u52a1\u5378\u8f7d\u5230\u7ecf\u6d4e\u578b\u8fb9\u7f18\u670d\u52a1\u5668\uff0c\u6ee1\u8db3\u5ef6\u8fdf\u9700\u6c42\u3002", "result": "\u76f8\u6bd4\u542f\u53d1\u5f0f\u548c\u5176\u4ed6\u5b66\u4e60\u7b56\u7565\uff0cD2Q Synchronizer\u964d\u4f4e\u7f51\u7edc\u6210\u672c\u81f3\u5c1145%\u548c10%\uff0c\u786e\u4fddQoS\u9700\u6c42\u3002", "conclusion": "D2Q Synchronizer\u5728\u591a\u57df\u52a8\u6001SDN\u4e2d\u9ad8\u6548\u4f18\u5316\u7f51\u7edc\u6210\u672c\uff0c\u786e\u4fdd\u4efb\u52a1\u5ef6\u8fdf\u8981\u6c42\u3002"}}
{"id": "2508.10955", "pdf": "https://arxiv.org/pdf/2508.10955", "abs": "https://arxiv.org/abs/2508.10955", "authors": ["Wenbin An", "Jiahao Nie", "Yaqiang Wu", "Feng Tian", "Shijian Lu", "Qinghua Zheng"], "title": "Empowering Multimodal LLMs with External Tools: A Comprehensive Survey", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "21 pages, 361 references", "summary": "By integrating the perception capabilities of multimodal encoders with the\ngenerative power of Large Language Models (LLMs), Multimodal Large Language\nModels (MLLMs), exemplified by GPT-4V, have achieved great success in various\nmultimodal tasks, pointing toward a promising pathway to artificial general\nintelligence. Despite this progress, the limited quality of multimodal data,\npoor performance on many complex downstream tasks, and inadequate evaluation\nprotocols continue to hinder the reliability and broader applicability of MLLMs\nacross diverse domains. Inspired by the human ability to leverage external\ntools for enhanced reasoning and problem-solving, augmenting MLLMs with\nexternal tools (e.g., APIs, expert models, and knowledge bases) offers a\npromising strategy to overcome these challenges. In this paper, we present a\ncomprehensive survey on leveraging external tools to enhance MLLM performance.\nOur discussion is structured along four key dimensions about external tools:\n(1) how they can facilitate the acquisition and annotation of high-quality\nmultimodal data; (2) how they can assist in improving MLLM performance on\nchallenging downstream tasks; (3) how they enable comprehensive and accurate\nevaluation of MLLMs; (4) the current limitations and future directions of\ntool-augmented MLLMs. Through this survey, we aim to underscore the\ntransformative potential of external tools in advancing MLLM capabilities,\noffering a forward-looking perspective on their development and applications.\nThe project page of this paper is publicly available\nathttps://github.com/Lackel/Awesome-Tools-for-MLLMs.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u901a\u8fc7\u5916\u90e8\u5de5\u5177\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u6570\u636e\u83b7\u53d6\u3001\u4efb\u52a1\u6027\u80fd\u3001\u8bc4\u4f30\u534f\u8bae\u53ca\u672a\u6765\u65b9\u5411\u7684\u6f5c\u529b\u3002", "motivation": "MLLMs\u5728\u590d\u6742\u4efb\u52a1\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u83b7\u53d6\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5916\u90e8\u5de5\u5177\uff08\u5982API\u3001\u4e13\u5bb6\u6a21\u578b\uff09\u7684\u5f15\u5165\u53ef\u63d0\u5347\u5176\u6027\u80fd\u4e0e\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u7ef4\u5ea6\u5206\u6790\u5916\u90e8\u5de5\u5177\u7684\u4f5c\u7528\uff1a\u9ad8\u8d28\u91cf\u6570\u636e\u83b7\u53d6\u4e0e\u6807\u6ce8\u3001\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3001\u4f18\u5316\u8bc4\u4f30\u534f\u8bae\u3001\u4ee5\u53ca\u5f53\u524d\u5c40\u9650\u4e0e\u672a\u6765\u65b9\u5411\u3002", "result": "\u5916\u90e8\u5de5\u5177\u6709\u6f5c\u529b\u663e\u8457\u63d0\u5347MLLMs\u7684\u80fd\u529b\uff0c\u5c24\u5176\u5728\u6570\u636e\u8d28\u91cf\u548c\u4efb\u52a1\u8868\u73b0\u65b9\u9762\u3002", "conclusion": "\u5916\u90e8\u5de5\u5177\u662fMLLMs\u8fdb\u4e00\u6b65\u53d1\u5c55\u7684\u5173\u952e\uff0c\u672a\u6765\u9700\u63a2\u7d22\u5176\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u4e0e\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2508.11133", "pdf": "https://arxiv.org/pdf/2508.11133", "abs": "https://arxiv.org/abs/2508.11133", "authors": ["Tomer Wolfson", "Harsh Trivedi", "Mor Geva", "Yoav Goldberg", "Dan Roth", "Tushar Khot", "Ashish Sabharwal", "Reut Tsarfaty"], "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print", "summary": "Large language models (LLMs) are emerging as a go-to tool for querying\ninformation. However, current LLM benchmarks rarely feature natural questions\nthat are both information-seeking as well as genuinely time-consuming for\nhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural\nand complex questions that require dozens, and at times hundreds, of\nintermediate steps to solve -- far more than any existing QA benchmark. To\nbuild MoNaCo, we developed a decomposed annotation pipeline to elicit and\nmanually answer natural time-consuming questions at scale. Frontier LLMs\nevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and\nhallucinations. Our results underscore the need for reasoning models that\nbetter handle the complexity and sheer breadth of real-world\ninformation-seeking questions -- with MoNaCo providing an effective resource\nfor tracking such progress. The MONACO benchmark, codebase, prompts and models\npredictions are publicly available at: https://tomerwolgithub.github.io/monaco", "AI": {"tldr": "MoNaCo\u662f\u4e00\u4e2a\u5305\u542b1,315\u4e2a\u81ea\u7136\u4e14\u590d\u6742\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u9700\u591a\u6b65\u63a8\u7406\u7684\u4fe1\u606f\u68c0\u7d22\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u53cd\u6620\u4eba\u7c7b\u4fe1\u606f\u68c0\u7d22\u4e2d\u771f\u5b9e\u8017\u65f6\u95ee\u9898\u7684\u81ea\u7136\u6027\u95ee\u9898\uff0cMoNaCo\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u89e3\u6ce8\u91ca\u6d41\u7a0b\uff0c\u89c4\u6a21\u5316\u6536\u96c6\u5e76\u624b\u52a8\u56de\u7b54\u81ea\u7136\u7684\u8017\u65f6\u95ee\u9898\uff0c\u6784\u5efa\u4e86MoNaCo\u57fa\u51c6\u3002", "result": "\u524d\u6cbfLLMs\u5728MoNaCo\u4e0a\u6700\u9ad8\u4ec5\u8fbe61.2% F1\uff0c\u53d7\u9650\u4e8e\u53ec\u56de\u7387\u4f4e\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "MoNaCo\u4e3a\u8ddf\u8e2a\u6a21\u578b\u5728\u590d\u6742\u4fe1\u606f\u68c0\u7d22\u95ee\u9898\u4e0a\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u8d44\u6e90\uff0c\u5e76\u7a81\u663e\u4e86\u6539\u8fdb\u63a8\u7406\u6a21\u578b\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.10911", "pdf": "https://arxiv.org/pdf/2508.10911", "abs": "https://arxiv.org/abs/2508.10911", "authors": ["Luis Vitor Zerkowski", "Nina S. T. Hirata"], "title": "Uncovering Latent Connections in Indigenous Heritage: Semantic Pipelines for Cultural Preservation in Brazil", "categories": ["cs.HC", "cs.CY", "cs.LG", "I.2.m"], "comment": "8 tables, 7 figures, submitted to AAAI2026", "summary": "Indigenous communities face ongoing challenges in preserving their cultural\nheritage, particularly in the face of systemic marginalization and urban\ndevelopment. In Brazil, the Museu Nacional dos Povos Indigenas through the\nTainacan platform hosts the country's largest online collection of Indigenous\nobjects and iconographies, providing a critical resource for cultural\nengagement. Using publicly available data from this repository, we present a\ndata-driven initiative that applies artificial intelligence to enhance\naccessibility, interpretation, and exploration. We develop two semantic\npipelines: a visual pipeline that models image-based similarity and a textual\npipeline that captures semantic relationships from item descriptions. These\nembedding spaces are projected into two dimensions and integrated into an\ninteractive visualization tool we also developed. In addition to\nsimilarity-based navigation, users can explore the collection through temporal\nand geographic lenses, enabling both semantic and contextualized perspectives.\nThe system supports curatorial tasks, aids public engagement, and reveals\nlatent connections within the collection. This work demonstrates how AI can\nethically contribute to cultural preservation practices.", "AI": {"tldr": "\u5229\u7528AI\u6280\u672f\u63d0\u5347\u5df4\u897f\u571f\u8457\u6587\u5316\u9057\u4ea7\u7684\u5728\u7ebf\u53ef\u8bbf\u95ee\u6027\u548c\u4e92\u52a8\u63a2\u7d22\u3002", "motivation": "\u9488\u5bf9\u571f\u8457\u6587\u5316\u56e0\u8fb9\u7f18\u5316\u548c\u57ce\u5e02\u5316\u800c\u9762\u4e34\u7684\u4fdd\u5b58\u6311\u6218\uff0c\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u589e\u5f3a\u6587\u5316\u9057\u4ea7\u7684\u4e92\u52a8\u6027\u548c\u53ef\u53ca\u6027\u3002", "method": "\u5f00\u53d1\u89c6\u89c9\u548c\u6587\u672c\u4e24\u6761\u8bed\u4e49\u7ba1\u9053\uff0c\u6784\u5efa\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u96c6\u6210\u5230\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\u4e2d\u3002", "result": "\u7cfb\u7edf\u652f\u6301\u7b56\u5c55\u4efb\u52a1\u3001\u4fc3\u8fdb\u516c\u4f17\u53c2\u4e0e\uff0c\u5e76\u63ed\u793a\u85cf\u54c1\u4e2d\u7684\u6f5c\u5728\u8054\u7cfb\u3002", "conclusion": "AI\u53ef\u4f26\u7406\u5730\u4e3a\u6587\u5316\u4fdd\u62a4\u5b9e\u8df5\u505a\u8d21\u732e\u3002"}}
{"id": "2508.11423", "pdf": "https://arxiv.org/pdf/2508.11423", "abs": "https://arxiv.org/abs/2508.11423", "authors": ["Samson Abramsky", "Wolfgang Banzhaf", "Leo S. D. Caves", "Michael Levin", "Penousal Machado", "Charles Ofria", "Susan Stepney", "Roger White"], "title": "Open Questions about Time and Self-reference in Living Systems", "categories": ["cs.ET", "q-bio.OT"], "comment": "28 pages, 3 figures", "summary": "Living systems exhibit a range of fundamental characteristics: they are\nactive, self-referential, self-modifying systems. This paper explores how these\ncharacteristics create challenges for conventional scientific approaches and\nwhy they require new theoretical and formal frameworks. We introduce a\ndistinction between 'natural time', the continuing present of physical\nprocesses, and 'representational time', with its framework of past, present and\nfuture that emerges with life itself. Representational time enables memory,\nlearning and prediction, functions of living systems essential for their\nsurvival. Through examples from evolution, embryogenesis and metamorphosis we\nshow how living systems navigate the apparent contradictions arising from\nself-reference as natural time unwinds self-referential loops into\ndevelopmental spirals. Conventional mathematical and computational formalisms\nstruggle to model self-referential and self-modifying systems without running\ninto paradox. We identify promising new directions for modelling\nself-referential systems, including domain theory, co-algebra, genetic\nprogramming, and self-modifying algorithms. There are broad implications for\nbiology, cognitive science and social sciences, because self-reference and\nself-modification are not problems to be avoided but core features of living\nsystems that must be modelled to understand life's open-ended creativity.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u547d\u7cfb\u7edf\u7684\u81ea\u6211\u53c2\u7167\u548c\u81ea\u6211\u4fee\u6539\u7279\u6027\u5982\u4f55\u6311\u6218\u4f20\u7edf\u79d1\u5b66\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u9700\u65b0\u7406\u8bba\u6846\u67b6\u3002\u533a\u5206\u4e86\u2018\u81ea\u7136\u65f6\u95f4\u2019\u548c\u2018\u8868\u5f81\u65f6\u95f4\u2019\uff0c\u5e76\u4e3e\u4f8b\u8bf4\u660e\u751f\u547d\u7cfb\u7edf\u5982\u4f55\u5904\u7406\u81ea\u6211\u53c2\u7167\u7684\u77db\u76fe\u3002", "motivation": "\u7814\u7a76\u751f\u547d\u7cfb\u7edf\u7684\u4e3b\u52a8\u6027\u548c\u81ea\u6211\u53c2\u7167\u7279\u6027\u5982\u4f55\u4e3a\u4f20\u7edf\u79d1\u5b66\u65b9\u6cd5\u5e26\u6765\u6311\u6218\uff0c\u5e76\u63a2\u7d22\u65b0\u7406\u8bba\u6846\u67b6\u7684\u5fc5\u8981\u6027\u3002", "method": "\u901a\u8fc7\u8fdb\u5316\u3001\u80da\u80ce\u53d1\u80b2\u548c\u53d8\u6001\u7684\u4f8b\u5b50\uff0c\u5206\u6790\u751f\u547d\u7cfb\u7edf\u5982\u4f55\u5904\u7406\u81ea\u6211\u53c2\u7167\u77db\u76fe\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u5efa\u6a21\u65b9\u5411\u3002", "result": "\u6307\u51fa\u4f20\u7edf\u5f62\u5f0f\u4e3b\u4e49\u96be\u4ee5\u5efa\u6a21\u81ea\u6211\u53c2\u7167\u7cfb\u7edf\uff0c\u9700\u91c7\u7528\u57df\u7406\u8bba\u3001\u5171\u4ee3\u6570\u3001\u9057\u4f20\u7f16\u7a0b\u7b49\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u6211\u53c2\u7167\u548c\u81ea\u6211\u4fee\u6539\u662f\u751f\u547d\u7cfb\u7edf\u7684\u6838\u5fc3\u7279\u5f81\uff0c\u9700\u65b0\u6a21\u578b\u4ee5\u7406\u89e3\u751f\u547d\u7684\u5f00\u653e\u521b\u9020\u529b\uff0c\u5bf9\u591a\u5b66\u79d1\u6709\u6df1\u8fdc\u5f71\u54cd\u3002"}}
{"id": "2508.11298", "pdf": "https://arxiv.org/pdf/2508.11298", "abs": "https://arxiv.org/abs/2508.11298", "authors": ["Gabin Schieffer", "Jacob Wahlgren", "Ruimin Shi", "Edgar A. Le\u00f3n", "Roger Pearce", "Maya Gokhale", "Ivy Peng"], "title": "Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive", "categories": ["cs.DC"], "comment": null, "summary": "The ever-increasing compute performance of GPU accelerators drives up the\nneed for efficient data movements within HPC applications to sustain\nperformance. Proposed as a solution to alleviate CPU-GPU data movement, AMD\nMI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth\nmemory (HBM) within a single physical package. Leadership supercomputers, such\nas El Capitan, group four APUs within a single compute node, using Infinity\nFabric Interconnect. In this work, we design specific benchmarks to evaluate\ndirect memory access from the GPU, explicit inter-APU data movement, and\ncollective multi-APU communication. We also compare the efficiency of HIP APIs,\nMPI routines, and the GPU-specialized RCCL library. Our results highlight key\ndesign choices for optimizing inter-APU communication on multi-APU AMD MI300A\nsystems with Infinity Fabric, including programming interfaces, allocators, and\ndata movement. Finally, we optimize two real HPC applications, Quicksilver and\nCloverLeaf, and evaluate them on a four MI100A APU system.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AMD MI300A APU\u5728\u591aAPU\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u6548\u7387\uff0c\u8bbe\u8ba1\u4e86\u57fa\u51c6\u6d4b\u8bd5\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u7f16\u7a0b\u63a5\u53e3\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u65b9\u6cd5\u5e76\u9a8c\u8bc1\u4e86\u5b9e\u9645HPC\u5e94\u7528\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u968f\u7740GPU\u8ba1\u7b97\u6027\u80fd\u7684\u63d0\u5347\uff0cHPC\u5e94\u7528\u4e2d\u9ad8\u6548\u6570\u636e\u4f20\u8f93\u7684\u9700\u6c42\u589e\u52a0\u3002AMD MI300A APU\u901a\u8fc7\u96c6\u6210CPU\u3001GPU\u548cHBM\u6765\u51cf\u5c11CPU-GPU\u6570\u636e\u4f20\u8f93\uff0c\u4f46\u591aAPU\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u4ecd\u9700\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30GPU\u76f4\u63a5\u5185\u5b58\u8bbf\u95ee\u3001APU\u95f4\u6570\u636e\u4f20\u8f93\u548c\u591aAPU\u96c6\u4f53\u901a\u4fe1\uff0c\u6bd4\u8f83\u4e86HIP API\u3001MPI\u548cRCCL\u5e93\u7684\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u4f18\u5316\u591aAPU\u7cfb\u7edf\u901a\u4fe1\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u4f18\u5316Quicksilver\u548cCloverLeaf\u5e94\u7528\u9a8c\u8bc1\u4e86\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AMD MI300A APU\u5728\u591aAPU\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u901a\u4fe1\u6f5c\u529b\uff0c\u5408\u7406\u7684\u7f16\u7a0b\u63a5\u53e3\u548c\u4f18\u5316\u7b56\u7565\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347HPC\u5e94\u7528\u6027\u80fd\u3002"}}
{"id": "2508.11447", "pdf": "https://arxiv.org/pdf/2508.11447", "abs": "https://arxiv.org/abs/2508.11447", "authors": ["Maximiliano Cristi\u00e1", "Gianfranco Rossi"], "title": "Encoding and Reasoning About Arrays in Set Theory", "categories": ["cs.LO"], "comment": "Under consideration at ACM Transactions on Computational Logic", "summary": "We encode arrays as functions which, in turn, are encoded as sets of ordered\npairs. The set cardinality of each of these functions coincides with the length\nof the array it is representing. Then we define a fragment of set theory that\nis used to give the specifications of a non-trivial class of programs with\narrays. In this way, array reasoning becomes set reasoning. Furthermore, a\ndecision procedure for this fragment is also provided and implemented as part\nof the {log} (read 'setlog') tool. {log} is a constraint logic programming\nlanguage and satisfiability solver where sets and binary relations are\nfirst-class citizens. The tool already implements a few decision procedures for\ndifferent fragments of set theory. In this way, arrays are seamlessly\nintegrated into {log} thus allowing users to reason about sets, functions and\narrays all in the same language and with the same solver. The decision\nprocedure presented in this paper is an extension of decision procedures\ndefined in earlier works not supporting arrays.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6570\u7ec4\u7f16\u7801\u4e3a\u51fd\u6570\u548c\u6709\u5e8f\u5bf9\u96c6\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u5408\u8bba\u7247\u6bb5\u5b9a\u4e49\u7a0b\u5e8f\u89c4\u8303\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u52a8\u673a\u662f\u901a\u8fc7\u96c6\u5408\u8bba\u7b80\u5316\u6570\u7ec4\u63a8\u7406\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u96c6\u5408\u63a8\u7406\uff0c\u4ece\u800c\u5728\u540c\u4e00\u8bed\u8a00\u548c\u6c42\u89e3\u5668\u4e2d\u5b9e\u73b0\u96c6\u5408\u3001\u51fd\u6570\u548c\u6570\u7ec4\u7684\u7edf\u4e00\u5904\u7406\u3002", "method": "\u65b9\u6cd5\u662f\u5c06\u6570\u7ec4\u7f16\u7801\u4e3a\u51fd\u6570\u548c\u6709\u5e8f\u5bf9\u96c6\u5408\uff0c\u5b9a\u4e49\u96c6\u5408\u8bba\u7247\u6bb5\u7528\u4e8e\u7a0b\u5e8f\u89c4\u8303\uff0c\u5e76\u5f00\u53d1\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u7ed3\u679c\u662f\u5b9e\u73b0\u4e86{log}\u5de5\u5177\u7684\u6269\u5c55\uff0c\u652f\u6301\u6570\u7ec4\u63a8\u7406\uff0c\u5e76\u9a8c\u8bc1\u4e86\u96c6\u5408\u3001\u51fd\u6570\u548c\u6570\u7ec4\u7684\u7edf\u4e00\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u7ed3\u8bba\u662f\u901a\u8fc7\u96c6\u5408\u8bba\u7247\u6bb5\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u6210\u529f\u5c06\u6570\u7ec4\u63a8\u7406\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u96c6\u5408\u8bba\u6c42\u89e3\u5668\u4e2d\u3002"}}
{"id": "2508.11257", "pdf": "https://arxiv.org/pdf/2508.11257", "abs": "https://arxiv.org/abs/2508.11257", "authors": ["Marc Pavel", "Nenad Petrovic", "Lukasz Mazur", "Vahid Zolfaghari", "Fengjunjie Pan", "Alois Knoll"], "title": "Hallucination in LLM-Based Code Generation: An Automotive Case Study", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown significant potential in automating\ncode generation tasks offering new opportunities across software engineering\ndomains. However, their practical application remains limited due to\nhallucinations - outputs that appear plausible but are factually incorrect,\nunverifiable or nonsensical. This paper investigates hallucination phenomena in\nthe context of code generation with a specific focus on the automotive domain.\nA case study is presented that evaluates multiple code LLMs for three different\nprompting complexities ranging from a minimal one-liner prompt to a prompt with\nCovesa Vehicle Signal Specifications (VSS) as additional context and finally to\na prompt with an additional code skeleton. The evaluation reveals a high\nfrequency of syntax violations, invalid reference errors and API knowledge\nconflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the\nevaluated models, only GPT-4.1 and GPT-4o were able to produce a correct\nsolution when given the most context-rich prompt. Simpler prompting strategies\nfailed to yield a working result, even after multiple refinement iterations.\nThese findings highlight the need for effective mitigation techniques to ensure\nthe safe and reliable use of LLM generated code, especially in safety-critical\ndomains such as automotive software systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u7279\u522b\u662f\u5728\u6c7d\u8f66\u9886\u57df\u3002\u901a\u8fc7\u8bc4\u4f30\u591a\u6b3e\u5148\u8fdb\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff08\u5982GPT-4.1\u3001Codex\u548cGPT-4o\uff09\uff0c\u53d1\u73b0\u5176\u5728\u9ad8\u590d\u6742\u5ea6\u63d0\u793a\u4e0b\u4ecd\u5b58\u5728\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u7f13\u89e3\u6280\u672f\u4ee5\u786e\u4fdd\u5b89\u5168\u53ef\u9760\u7684\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u4e2d\u56e0\u5e7b\u89c9\u95ee\u9898\uff08\u5982\u9519\u8bef\u3001\u4e0d\u53ef\u9a8c\u8bc1\u6216\u4e0d\u5408\u903b\u8f91\u7684\u8f93\u51fa\uff09\u800c\u53d7\u9650\uff0c\u5c24\u5176\u5728\u5b89\u5168\u5173\u952e\u7684\u6c7d\u8f66\u9886\u57df\u9700\u8981\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u591a\u6b3eLLM\u5728\u4e0d\u540c\u63d0\u793a\u590d\u6742\u5ea6\uff08\u4ece\u7b80\u5355\u5355\u884c\u63d0\u793a\u5230\u5305\u542bVSS\u89c4\u8303\u548c\u4ee3\u7801\u9aa8\u67b6\u7684\u590d\u6742\u63d0\u793a\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u751f\u6210\u4ee3\u7801\u7684\u6b63\u786e\u6027\u3002", "result": "\u53d1\u73b0\u6240\u6709\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u590d\u6742\u5ea6\u4e0b\u5747\u5b58\u5728\u9ad8\u9891\u7387\u7684\u8bed\u6cd5\u9519\u8bef\u3001\u65e0\u6548\u5f15\u7528\u548cAPI\u51b2\u7a81\u3002\u4ec5GPT-4.1\u548cGPT-4o\u5728\u6700\u4f18\u63d0\u793a\u4e0b\u751f\u6210\u4e86\u6b63\u786e\u4ee3\u7801\u3002", "conclusion": "\u9700\u5f00\u53d1\u6709\u6548\u7684\u7f13\u89e3\u6280\u672f\u4ee5\u786e\u4fddLLM\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u3002"}}
{"id": "2508.10934", "pdf": "https://arxiv.org/pdf/2508.10934", "abs": "https://arxiv.org/abs/2508.10934", "authors": ["Jiahui Huang", "Qunjie Zhou", "Hesam Rabeti", "Aleksandr Korovko", "Huan Ling", "Xuanchi Ren", "Tianchang Shen", "Jun Gao", "Dmitry Slepichev", "Chen-Hsuan Lin", "Jiawei Ren", "Kevin Xie", "Joydeep Biswas", "Laura Leal-Taixe", "Sanja Fidler"], "title": "ViPE: Video Pose Engine for 3D Geometric Perception", "categories": ["cs.CV", "cs.GR", "cs.RO", "eess.IV"], "comment": "Paper website: https://research.nvidia.com/labs/toronto-ai/vipe/", "summary": "Accurate 3D geometric perception is an important prerequisite for a wide\nrange of spatial AI systems. While state-of-the-art methods depend on\nlarge-scale training data, acquiring consistent and precise 3D annotations from\nin-the-wild videos remains a key challenge. In this work, we introduce ViPE, a\nhandy and versatile video processing engine designed to bridge this gap. ViPE\nefficiently estimates camera intrinsics, camera motion, and dense, near-metric\ndepth maps from unconstrained raw videos. It is robust to diverse scenarios,\nincluding dynamic selfie videos, cinematic shots, or dashcams, and supports\nvarious camera models such as pinhole, wide-angle, and 360{\\deg} panoramas. We\nhave benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing\nuncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and\nruns at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to\nannotate a large-scale collection of videos. This collection includes around\n100K real-world internet videos, 1M high-quality AI-generated videos, and 2K\npanoramic videos, totaling approximately 96M frames -- all annotated with\naccurate camera poses and dense depth maps. We open-source ViPE and the\nannotated dataset with the hope of accelerating the development of spatial AI\nsystems.", "AI": {"tldr": "ViPE\u662f\u4e00\u79cd\u9ad8\u6548\u89c6\u9891\u5904\u7406\u5f15\u64ce\uff0c\u7528\u4e8e\u4ece\u672a\u6807\u6ce8\u89c6\u9891\u4e2d\u4f30\u8ba1\u76f8\u673a\u53c2\u6570\u548c\u6df1\u5ea6\u56fe\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u9002\u914d\u591a\u79cd\u573a\u666f\u548c\u76f8\u673a\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4ece\u91ce\u5916\u89c6\u9891\u4e2d\u83b7\u53d6\u4e00\u81f4\u4e14\u7cbe\u786e\u76843D\u6807\u6ce8\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1ViPE\u5f15\u64ce\uff0c\u4f30\u8ba1\u76f8\u673a\u5185\u53c2\u3001\u8fd0\u52a8\u548c\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u652f\u6301\u591a\u79cd\u573a\u666f\u548c\u76f8\u673a\u6a21\u578b\u3002", "result": "\u5728TUM/KITTI\u5e8f\u5217\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd518%/50%\uff0c\u5e76\u6807\u6ce8\u4e86\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u96c6\u3002", "conclusion": "\u5f00\u6e90ViPE\u548c\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u7a7a\u95f4AI\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2508.11574", "pdf": "https://arxiv.org/pdf/2508.11574", "abs": "https://arxiv.org/abs/2508.11574", "authors": ["Mohammad Sajid Shahriar", "Suresh Subramaniam", "Motoharu Matsuura", "Hiroshi Hasegawa", "Shih-Chun Lin"], "title": "Intelligent Edge Resource Provisioning for Scalable Digital Twins of Autonomous Vehicles", "categories": ["cs.NI"], "comment": null, "summary": "The next generation networks offers significant potential to advance\nIntelligent Transportation Systems (ITS), particularly through the integration\nof Digital Twins (DTs). However, ensuring the uninterrupted operation of DTs\nthrough efficient computing resource management remains an open challenge. This\npaper introduces a distributed computing archi tecture that integrates DTs and\nMobile Edge Computing (MEC) within a software-defined vehicular networking\nframework to enable intelligent, low-latency transportation services. A network\naware scalable collaborative task provisioning algorithm is de veloped to train\nan autonomous agent, which is evaluated using a realistic connected autonomous\nvehicle (CAV) traffic simulation. The proposed framework significantly enhances\nthe robustness and scalability of DT operations by reducing synchronization\nerrors to as low as 5% while achieving up to 99.5% utilization of edge\ncomputing resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u8ba1\u7b97\u67b6\u6784\uff0c\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u548c\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff0c\u7528\u4e8e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u63d0\u4f9b\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u901a\u8fc7\u9ad8\u6548\u7684\u8ba1\u7b97\u8d44\u6e90\u7ba1\u7406\u786e\u4fdd\u6570\u5b57\u5b6a\u751f\uff08DTs\uff09\u7684\u6301\u7eed\u8fd0\u884c\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7f51\u7edc\u611f\u77e5\u7684\u53ef\u6269\u5c55\u534f\u4f5c\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u8bad\u7ec3\u81ea\u4e3b\u4ee3\u7406\uff0c\u5e76\u5728\u5b9e\u9645\u7684\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08CAV\uff09\u4ea4\u901a\u6a21\u62df\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u8be5\u6846\u67b6\u5c06\u540c\u6b65\u9519\u8bef\u964d\u81f35%\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8fbe99.5%\u7684\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u67b6\u6784\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b57\u5b6a\u751f\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.11058", "pdf": "https://arxiv.org/pdf/2508.11058", "abs": "https://arxiv.org/abs/2508.11058", "authors": ["Wentao Mo", "Qingchao Chen", "Yuxin Peng", "Siyuan Huang", "Yang Liu"], "title": "Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset", "categories": ["cs.CV", "cs.MM"], "comment": "Accepeted to ACM MM 25", "summary": "The advancement of 3D vision-language (3D VL) learning is hindered by several\nlimitations in existing 3D VL datasets: they rarely necessitate reasoning\nbeyond a close range of objects in single viewpoint, and annotations often link\ninstructions to single objects, missing richer contextual alignments between\nmultiple objects. This significantly curtails the development of models capable\nof deep, multi-view 3D scene understanding over distant objects. To address\nthese challenges, we introduce MV-ScanQA, a novel 3D question answering dataset\nwhere 68% of questions explicitly require integrating information from multiple\nviews (compared to less than 7% in existing datasets), thereby rigorously\ntesting multi-view compositional reasoning. To facilitate the training of\nmodels for such demanding scenarios, we present TripAlign dataset, a\nlarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D\nview, set of 3D objects, text> triplets that explicitly aligns groups of\ncontextually related objects with text, providing richer, view-grounded\nmulti-object multimodal alignment signals than previous single-object\nannotations. We further develop LEGO, a baseline method for the multi-view\nreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D\nLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign\nachieves state-of-the-art performance not only on the proposed MV-ScanQA, but\nalso on existing benchmarks for 3D dense captioning and question answering.\nDatasets and code are available at\nhttps://matthewdm0816.github.io/tripalign-mvscanqa.", "AI": {"tldr": "MV-ScanQA\u662f\u4e00\u4e2a\u65b0\u76843D\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5f3a\u8c03\u591a\u89c6\u89d2\u63a8\u7406\uff0c\u5e76\u63d0\u51faTripAlign\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u548cLEGO\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\u5728\u8fdc\u8ddd\u79bb\u591a\u89c6\u89d2\u63a8\u7406\u548c\u591a\u5bf9\u8c61\u4e0a\u4e0b\u6587\u5bf9\u9f50\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6df1\u5165\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86MV-ScanQA\u6570\u636e\u96c6\u548cTripAlign\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86LEGO\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u76842D LVLMs\u77e5\u8bc6\u8f6c\u79fb\u52303D\u9886\u57df\u3002", "result": "LEGO\u5728MV-ScanQA\u53ca\u73b0\u67093D\u5bc6\u96c6\u6807\u6ce8\u548c\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "MV-ScanQA\u548cTripAlign\u4e3a3D\u89c6\u89c9\u8bed\u8a00\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u591a\u89c6\u89d2\u63a8\u7406\u548c\u591a\u5bf9\u8c61\u5bf9\u9f50\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.10914", "pdf": "https://arxiv.org/pdf/2508.10914", "abs": "https://arxiv.org/abs/2508.10914", "authors": ["Katherine M. Collins", "Graham Todd", "Cedegao E. Zhang", "Adrian Weller", "Julian Togelius", "Junyi Chu", "Lionel Wong", "Thomas L. Griffiths", "Joshua B. Tenenbaum"], "title": "Generation and Evaluation in the Human Invention Process through the Lens of Game Design", "categories": ["cs.HC"], "comment": "CogSci conference non-archival paper", "summary": "The human ability to learn rules and solve problems has been a central\nconcern of cognitive science research since the field's earliest days. But we\ndo not just follow rules and solve problems given to us by others: we modify\nthose rules, create new problems, and set new goals and tasks for ourselves and\nothers. Arguably, even more than rule following and problem solving, human\nintelligence is about creatively breaking and stretching the rules, changing\nthe game, and inventing new problems worth thinking about. Creating a good rule\nor a good problem depends not just on the ideas one can think up but on how one\nevaluates such proposals. Here, we study invention through the lens of game\ndesign. We focus particularly on the early stages of novice, \"everyday\" game\ncreation, where the stakes are low. We draw on a dataset of over 450 human\ncreated games, created by participants who saw an initial seed set of\ntwo-player grid-based strategy games. We consider two different cognitive\nmechanisms that may be at work during the early processes of intuitive game\ninvention: an associative proposal based on previous games one has seen and\ncompute-bounded model-based evaluation that an everyday game creator may use to\nrefine their initial draft proposals. In our preliminary work, we conduct a\nmodel-based analysis of how people invented new games based on prior experience\nand find that generated games are best described by a model which incorporates\nmodel-based estimates of game quality at a population level. Our work points to\nhow human invention is based not only on what people propose, but how they\nevaluate and offers a computational toolkit to scale empirical studies of\nmodel-based simulation in open-ended human innovation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4eba\u7c7b\u521b\u9020\u65b0\u6e38\u620f\u65f6\u7684\u8ba4\u77e5\u673a\u5236\uff0c\u91cd\u70b9\u5173\u6ce8\u57fa\u4e8e\u7ecf\u9a8c\u7684\u8054\u60f3\u63d0\u8bae\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u8bc4\u4f30\u3002", "motivation": "\u4eba\u7c7b\u4e0d\u4ec5\u9075\u5faa\u89c4\u5219\u548c\u89e3\u51b3\u95ee\u9898\uff0c\u8fd8\u64c5\u957f\u521b\u9020\u65b0\u89c4\u5219\u548c\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u7406\u89e3\u8fd9\u79cd\u521b\u65b0\u80fd\u529b\u7684\u8ba4\u77e5\u57fa\u7840\uff0c\u5c24\u5176\u662f\u4f4e\u98ce\u9669\u73af\u5883\u4e0b\u7684\u6e38\u620f\u8bbe\u8ba1\u3002", "method": "\u5206\u6790450\u591a\u4e2a\u7531\u4eba\u7c7b\u8bbe\u8ba1\u7684\u6e38\u620f\uff0c\u7814\u7a76\u4e24\u79cd\u8ba4\u77e5\u673a\u5236\uff1a\u57fa\u4e8e\u5148\u524d\u6e38\u620f\u7684\u8054\u60f3\u63d0\u8bae\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u8bc4\u4f30\u3002", "result": "\u751f\u6210\u7684\u6e38\u620f\u6700\u80fd\u7531\u7ed3\u5408\u7fa4\u4f53\u6c34\u5e73\u6e38\u620f\u8d28\u91cf\u8bc4\u4f30\u7684\u6a21\u578b\u63cf\u8ff0\u3002", "conclusion": "\u4eba\u7c7b\u521b\u65b0\u4e0d\u4ec5\u4f9d\u8d56\u4e8e\u63d0\u8bae\uff0c\u8fd8\u4f9d\u8d56\u4e8e\u5982\u4f55\u8bc4\u4f30\u63d0\u8bae\uff0c\u7814\u7a76\u4e3a\u5f00\u653e\u5f0f\u521b\u65b0\u7684\u5b9e\u8bc1\u7814\u7a76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2508.11451", "pdf": "https://arxiv.org/pdf/2508.11451", "abs": "https://arxiv.org/abs/2508.11451", "authors": ["Hamid Farzaneh", "Asif Ali Khan", "Jeronimo Castrillon"], "title": "CoMoNM: A Cost Modeling Framework for Compute-Near-Memory Systems", "categories": ["cs.ET", "cs.PL"], "comment": "12 pages, 16 Figures", "summary": "Compute-Near-Memory (CNM) systems offer a promising approach to mitigate the\nvon Neumann bottleneck by bringing computational units closer to data. However,\noptimizing for these architectures remains challenging due to their unique\nhardware and programming models. Existing CNM compilers often rely on manual\nprogrammer annotations for offloading and optimizations. Automating these\ndecisions by exploring the optimization space, common in CPU/GPU systems, is\ndifficult for CNMs as constructing and navigating the transformation space is\ntedious and time consuming. This is particularly the case during system-level\ndesign, where evaluation requires time-consuming simulations. To address this,\nwe present CoMoNM, a generic cost modeling framework for CNM systems for\nexecution time estimation in milliseconds. It takes a high-level,\nhardware-agnostic application representation, target system specifications, and\na mapping specification as input and estimates the execution time for the given\napplication on the target CNM system. We show how CoMoNM can be seamlessly\nintegrated into state-of-the-art CNM compilers, providing improved offloading\ndecisions. Evaluation on established benchmarks for CNM shows estimation errors\nwithin 7.80% and 2.99%, when compared to the real UPMEM CNM system and\nSamsung's HBM-PIM simulator. Notably, CoMoNM delivers estimates seven orders of\nmagnitude faster compared to the UPMEM and HBM-PIM simulators.", "AI": {"tldr": "CoMoNM\u662f\u4e00\u4e2a\u7528\u4e8e\u8ba1\u7b97\u8fd1\u5185\u5b58\uff08CNM\uff09\u7cfb\u7edf\u7684\u901a\u7528\u6210\u672c\u5efa\u6a21\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u4f30\u7b97\u6267\u884c\u65f6\u95f4\uff0c\u7b80\u5316\u4f18\u5316\u51b3\u7b56\u3002", "motivation": "\u89e3\u51b3CNM\u7cfb\u7edf\u4f18\u5316\u56f0\u96be\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u81ea\u52a8\u5316\u548c\u9ad8\u6548\u8bc4\u4f30\u6267\u884c\u65f6\u95f4\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faCoMoNM\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u7ea7\u786c\u4ef6\u65e0\u5173\u5e94\u7528\u8868\u793a\u3001\u76ee\u6807\u7cfb\u7edf\u89c4\u683c\u548c\u6620\u5c04\u89c4\u8303\uff0c\u5feb\u901f\u4f30\u7b97\u6267\u884c\u65f6\u95f4\u3002", "result": "CoMoNM\u7684\u4f30\u7b97\u8bef\u5dee\u57287.80%\u548c2.99%\u4ee5\u5185\uff0c\u901f\u5ea6\u6bd4UPMEM\u548cHBM-PIM\u6a21\u62df\u5668\u5feb\u4e03\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "CoMoNM\u4e3aCNM\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u6548\u3001\u51c6\u786e\u7684\u6267\u884c\u65f6\u95f4\u4f30\u7b97\uff0c\u663e\u8457\u63d0\u5347\u7f16\u8bd1\u5668\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2508.11384", "pdf": "https://arxiv.org/pdf/2508.11384", "abs": "https://arxiv.org/abs/2508.11384", "authors": ["Joel Rybicki", "Jakob Solnerzik", "Olivier Stietel", "Robin Vacus"], "title": "Space-efficient population protocols for exact majority in general graphs", "categories": ["cs.DC"], "comment": null, "summary": "We study exact majority consensus in the population protocol model. In this\nmodel, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in\neach time step, a scheduler samples uniformly at random a pair of adjacent\nnodes to interact. In the exact majority consensus task, each node is given a\nbinary input, and the goal is to design a protocol that almost surely reaches a\nstable configuration, where all nodes output the majority input value.\n  We give improved upper and lower bounds for the exact majority in general\ngraphs. First, we give asymptotically tight time lower bounds for general\n(unbounded space) protocols. Second, we obtain new upper bounds parameterized\nby the relaxation time $\\tau_{\\mathsf{rel}}$ of the random walk on $G$ induced\nby the scheduler and the degree imbalance $\\Delta/\\delta$ of $G$. Specifically,\nwe give a protocol that stabilizes in $O\\left( \\tfrac{\\Delta}{\\delta}\n\\tau_{\\mathsf{rel}} \\log^2 n \\right)$ steps in expectation and with high\nprobability and uses $O\\left( \\log n \\cdot \\left(\n\\log\\left(\\tfrac{\\Delta}{\\delta}\\right) + \\log\n\\left(\\tfrac{\\tau_{\\mathsf{rel}}}{n}\\right) \\right) \\right)$ states in any\ngraph with minimum degree at least $\\delta$ and maximum degree at most\n$\\Delta$.\n  For regular expander graphs, this matches the optimal space complexity of\n$\\Theta(\\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA\n2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of\n$O(n \\log^2 n)$ steps. Finally, we give a new upper bound of\n$O(\\tau_{\\mathsf{rel}} \\cdot n \\log n)$ for the stabilization time of a\nconstant-state protocol.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7fa4\u4f53\u534f\u8bae\u6a21\u578b\u4e2d\u7684\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u95ee\u9898\uff0c\u6539\u8fdb\u4e86\u5728\u4e00\u822c\u56fe\u4e2d\u7684\u4e0a\u4e0b\u754c\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u677e\u5f1b\u65f6\u95f4\u548c\u5ea6\u4e0d\u5e73\u8861\u7684\u65b0\u534f\u8bae\uff0c\u5e76\u5728\u6b63\u5219\u6269\u5c55\u56fe\u4e2d\u5339\u914d\u4e86\u6700\u4f18\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u7814\u7a76\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u95ee\u9898\u5728\u7fa4\u4f53\u534f\u8bae\u6a21\u578b\u4e2d\u7684\u6548\u7387\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\uff0c\u65e8\u5728\u4e3a\u4e00\u822c\u56fe\u8bbe\u8ba1\u66f4\u4f18\u7684\u534f\u8bae\u3002", "method": "\u901a\u8fc7\u5206\u6790\u56fe\u7684\u968f\u673a\u6e38\u8d70\u677e\u5f1b\u65f6\u95f4\u548c\u5ea6\u4e0d\u5e73\u8861\uff08\u0394/\u03b4\uff09\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u534f\u8bae\uff0c\u6539\u8fdb\u4e86\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u4e0a\u4e0b\u754c\u3002", "result": "\u63d0\u51fa\u7684\u534f\u8bae\u5728\u671f\u671b\u548c\u9ad8\u6982\u7387\u4e0b\u7a33\u5b9a\u65f6\u95f4\u4e0e\u7a7a\u95f4\u590d\u6742\u5ea6\u4e0a\u53d6\u5f97\u4e86\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u6b63\u5219\u6269\u5c55\u56fe\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5728\u4e00\u822c\u56fe\u548c\u6b63\u5219\u6269\u5c55\u56fe\u4e2d\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u534f\u8bae\u7684\u65b0\u4e0a\u4e0b\u754c\uff0c\u5c55\u793a\u4e86\u534f\u8bae\u8bbe\u8ba1\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.11449", "pdf": "https://arxiv.org/pdf/2508.11449", "abs": "https://arxiv.org/abs/2508.11449", "authors": ["Patrick Koopmann", "Christoph Wernhard", "Frank Wolter"], "title": "Interpolation in Classical Propositional Logic", "categories": ["cs.LO", "03B05 (Primary)"], "comment": "The article will appear in Balder ten Cate, Jean Christoph Jung,\n  Patrick Koopmann, Christoph Wernhard and Frank Wolter, editors. Theory and\n  Applications of Craig Interpolation. Ubiquity Press, 2026", "summary": "We introduce Craig interpolation and related notions such as uniform\ninterpolation, Beth definability, and theory decomposition in classical\npropositional logic. We present four approaches to computing interpolants: via\nquantifier elimination, from formulas in disjunctive normal form, and by\nextraction from resolution or tableau refutations. We close with a discussion\nof the size of interpolants and links to circuit complexity.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Craig\u63d2\u503c\u53ca\u5176\u76f8\u5173\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u56db\u79cd\u8ba1\u7b97\u63d2\u503c\u7684\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u63d2\u503c\u5927\u5c0f\u4e0e\u7535\u8def\u590d\u6742\u5ea6\u7684\u8054\u7cfb\u3002", "motivation": "\u63a2\u8ba8\u7ecf\u5178\u547d\u9898\u903b\u8f91\u4e2d\u7684\u63d2\u503c\u7406\u8bba\u53ca\u5176\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u91cf\u8bcd\u6d88\u9664\u3001\u6790\u53d6\u8303\u5f0f\u516c\u5f0f\u3001\u4ece\u89e3\u6790\u6216\u8868\u63a8\u6f14\u4e2d\u63d0\u53d6\u7b49\u56db\u79cd\u65b9\u6cd5\u8ba1\u7b97\u63d2\u503c\u3002", "result": "\u63d0\u51fa\u4e86\u591a\u79cd\u63d2\u503c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u63d2\u503c\u7684\u5927\u5c0f\u3002", "conclusion": "\u63d2\u503c\u7406\u8bba\u4e0e\u7535\u8def\u590d\u6742\u5ea6\u6709\u91cd\u8981\u8054\u7cfb\uff0c\u4e3a\u903b\u8f91\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.11305", "pdf": "https://arxiv.org/pdf/2508.11305", "abs": "https://arxiv.org/abs/2508.11305", "authors": ["Xin Wang", "Zhenhao Li", "Zishuo Ding"], "title": "Defects4Log: Benchmarking LLMs for Logging Code Defect Detection and Reasoning", "categories": ["cs.SE"], "comment": null, "summary": "Logging code is written by developers to capture system runtime behavior and\nplays a vital role in debugging, performance analysis, and system monitoring.\nHowever, defects in logging code can undermine the usefulness of logs and lead\nto misinterpretations. Although prior work has identified several logging\ndefect patterns and provided valuable insights into logging practices, these\nstudies often focus on a narrow range of defect patterns derived from limited\nsources (e.g., commit histories) and lack a systematic and comprehensive\nanalysis. Moreover, large language models (LLMs) have demonstrated promising\ngeneralization and reasoning capabilities across a variety of code-related\ntasks, yet their potential for detecting logging code defects remains largely\nunexplored.\n  In this paper, we derive a comprehensive taxonomy of logging code defects,\nwhich encompasses seven logging code defect patterns with 14 detailed\nscenarios. We further construct a benchmark dataset, \\dataset, consisting of\n164 developer-verified real-world logging defects. Then we propose an automated\nframework that leverages various prompting strategies and contextual\ninformation to evaluate LLMs' capability in detecting and reasoning logging\ncode defects. Experimental results reveal that LLMs generally struggle to\naccurately detect and reason logging code defects based on the source code\nonly. However, incorporating proper knowledge (e.g., detailed scenarios of\ndefect patterns) can lead to 10.9\\% improvement in detection accuracy. Overall,\nour findings provide actionable guidance for practitioners to avoid common\ndefect patterns and establish a foundation for improving LLM-based reasoning in\nlogging code defect detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u65e5\u5fd7\u4ee3\u7801\u7f3a\u9677\u5206\u7c7b\u6cd5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u68c0\u6d4b\u65e5\u5fd7\u4ee3\u7801\u7f3a\u9677\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u65e5\u5fd7\u4ee3\u7801\u5bf9\u7cfb\u7edf\u8c03\u8bd5\u548c\u76d1\u63a7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u9677\u53ef\u80fd\u5bfc\u81f4\u65e5\u5fd7\u8bef\u89e3\u3002\u73b0\u6709\u7814\u7a76\u5bf9\u7f3a\u9677\u6a21\u5f0f\u7684\u8986\u76d6\u4e0d\u8db3\uff0c\u4e14\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f5c\u8005\u9996\u5148\u5efa\u7acb\u4e86\u5305\u542b7\u79cd\u7f3a\u9677\u6a21\u5f0f\u548c14\u79cd\u5177\u4f53\u573a\u666f\u7684\u5206\u7c7b\u6cd5\uff0c\u7136\u540e\u6784\u5efa\u4e86164\u4e2a\u771f\u5b9e\u7f3a\u9677\u7684\u6570\u636e\u96c6\u3002\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u8bc4\u4f30\u4e86LLMs\u7684\u7f3a\u9677\u68c0\u6d4b\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u4ec5\u57fa\u4e8e\u6e90\u4ee3\u7801\u96be\u4ee5\u51c6\u786e\u68c0\u6d4b\u65e5\u5fd7\u7f3a\u9677\uff0c\u4f46\u5f15\u5165\u7f3a\u9677\u6a21\u5f0f\u7684\u8be6\u7ec6\u573a\u666f\u4fe1\u606f\u540e\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u53ef\u63d0\u534710.9%\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u907f\u514d\u5e38\u89c1\u7f3a\u9677\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u4e3a\u6539\u8fdb\u57fa\u4e8eLLM\u7684\u65e5\u5fd7\u7f3a\u9677\u68c0\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.10916", "pdf": "https://arxiv.org/pdf/2508.10916", "abs": "https://arxiv.org/abs/2508.10916", "authors": ["Ojas Shirekar", "Wim Pouw", "Chenxu Hao", "Vrushank Phadnis", "Thabo Beeler", "Chirag Raman"], "title": "Multimodal Quantitative Measures for Multiparty Behaviour Evaluation", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "Digital humans are emerging as autonomous agents in multiparty interactions,\nyet existing evaluation metrics largely ignore contextual coordination\ndynamics. We introduce a unified, intervention-driven framework for objective\nassessment of multiparty social behaviour in skeletal motion data, spanning\nthree complementary dimensions: (1) synchrony via Cross-Recurrence\nQuantification Analysis, (2) temporal alignment via Multiscale Empirical Mode\nDecompositionbased Beat Consistency, and (3) structural similarity via Soft\nDynamic Time Warping. We validate metric sensitivity through three\ntheory-driven perturbations -- gesture kinematic dampening, uniform\nspeech-gesture delays, and prosodic pitch-variance reduction-applied to\n$\\approx 145$ 30-second thin slices of group interactions from the DnD dataset.\nMixed-effects analyses reveal predictable, joint-independent shifts: dampening\nincreases CRQA determinism and reduces beat consistency, delays weaken\ncross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A\ncomplementary perception study ($N=27$) compares judgments of full-video and\nskeleton-only renderings to quantify representation effects. Our three measures\ndeliver orthogonal insights into spatial structure, timing alignment, and\nbehavioural variability. Thereby forming a robust toolkit for evaluating and\nrefining socially intelligent agents. Code available on\n\\href{https://github.com/tapri-lab/gig-interveners}{GitHub}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5e72\u9884\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u515a\u6d3e\u793e\u4ea4\u884c\u4e3a\uff0c\u5305\u62ec\u540c\u6b65\u6027\u3001\u65f6\u95f4\u5bf9\u9f50\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6307\u6807\u7684\u654f\u611f\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u5ffd\u7565\u4e86\u591a\u515a\u6d3e\u4e92\u52a8\u4e2d\u7684\u4e0a\u4e0b\u6587\u534f\u8c03\u52a8\u6001\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5ba2\u89c2\u8bc4\u4f30\u793e\u4ea4\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u8de8\u9012\u5f52\u91cf\u5316\u5206\u6790\u3001\u591a\u5c3a\u5ea6\u7ecf\u9a8c\u6a21\u6001\u5206\u89e3\u548c\u8f6f\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u4e09\u4e2a\u4e92\u8865\u7ef4\u5ea6\u8bc4\u4f30\u591a\u515a\u6d3e\u793e\u4ea4\u884c\u4e3a\uff0c\u5e76\u91c7\u7528\u7406\u8bba\u9a71\u52a8\u7684\u6270\u52a8\u9a8c\u8bc1\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u4e09\u4e2a\u6307\u6807\u80fd\u591f\u6355\u6349\u884c\u4e3a\u7684\u4e0d\u540c\u65b9\u9762\uff0c\u5982\u540c\u6b65\u6027\u3001\u65f6\u95f4\u5bf9\u9f50\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u4e14\u5728\u611f\u77e5\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u548c\u4f18\u5316\u5177\u6709\u793e\u4ea4\u667a\u80fd\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\u5305\u3002"}}
{"id": "2508.10915", "pdf": "https://arxiv.org/pdf/2508.10915", "abs": "https://arxiv.org/abs/2508.10915", "authors": ["Jacob Clouse", "Thomas Ramsey", "Samitha Somathilaka", "Nicholas Kleinsasser", "Sangjin Ryu", "Sasitharan Balasubramaniam"], "title": "Insect-Wing Structured Microfluidic System for Reservoir Computing", "categories": ["cs.NE", "cs.ET", "cs.LG"], "comment": null, "summary": "As the demand for more efficient and adaptive computing grows,\nnature-inspired architectures offer promising alternatives to conventional\nelectronic designs. Microfluidic platforms, drawing on biological forms and\nfluid dynamics, present a compelling foundation for low-power, high-resilience\ncomputing in environments where electronics are unsuitable. This study explores\na hybrid reservoir computing system based on a dragonfly-wing inspired\nmicrofluidic chip, which encodes temporal input patterns as fluid interactions\nwithin the micro channel network.\n  The system operates with three dye-based inlet channels and three\ncamera-monitored detection areas, transforming discrete spatial patterns into\ndynamic color output signals. These reservoir output signals are then modified\nand passed to a simple and trainable readout layer for pattern classification.\nUsing a combination of raw reservoir outputs and synthetically generated\noutputs, we evaluated system performance, system clarity, and data efficiency.\nThe results demonstrate consistent classification accuracies up to $91\\%$, even\nwith coarse resolution and limited training data, highlighting the viability of\nthe microfluidic reservoir computing.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u57fa\u4e8e\u873b\u8713\u7fc5\u8180\u542f\u53d1\u7684\u5fae\u6d41\u63a7\u82af\u7247\u7684\u6df7\u5408\u50a8\u5907\u8ba1\u7b97\u7cfb\u7edf\uff0c\u901a\u8fc7\u6d41\u4f53\u76f8\u4e92\u4f5c\u7528\u7f16\u7801\u65f6\u95f4\u8f93\u5165\u6a21\u5f0f\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4f4e\u529f\u8017\u3001\u9ad8\u9002\u5e94\u6027\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u5bf9\u66f4\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u8ba1\u7b97\u9700\u6c42\u7684\u589e\u957f\uff0c\u53d7\u81ea\u7136\u542f\u53d1\u7684\u67b6\u6784\u63d0\u4f9b\u4e86\u66ff\u4ee3\u4f20\u7edf\u7535\u5b50\u8bbe\u8ba1\u7684\u53ef\u80fd\u6027\u3002\u5fae\u6d41\u63a7\u5e73\u53f0\u7ed3\u5408\u751f\u7269\u5f62\u6001\u548c\u6d41\u4f53\u52a8\u529b\u5b66\uff0c\u5728\u7535\u5b50\u8bbe\u5907\u4e0d\u9002\u7528\u7684\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u4f4e\u529f\u8017\u3001\u9ad8\u5f39\u6027\u8ba1\u7b97\u7684\u57fa\u7840\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u57fa\u4e8e\u873b\u8713\u7fc5\u8180\u542f\u53d1\u7684\u5fae\u6d41\u63a7\u82af\u7247\uff0c\u901a\u8fc7\u4e09\u4e2a\u67d3\u6599\u5165\u53e3\u901a\u9053\u548c\u6444\u50cf\u5934\u76d1\u63a7\u7684\u68c0\u6d4b\u533a\u57df\uff0c\u5c06\u79bb\u6563\u7a7a\u95f4\u6a21\u5f0f\u8f6c\u6362\u4e3a\u52a8\u6001\u989c\u8272\u8f93\u51fa\u4fe1\u53f7\uff0c\u518d\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u8bfb\u51fa\u5c42\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u7cfb\u7edf\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\u8fbe91%\uff0c\u5373\u4f7f\u5728\u4f4e\u5206\u8fa8\u7387\u548c\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u4ecd\u8868\u73b0\u7a33\u5b9a\uff0c\u9a8c\u8bc1\u4e86\u5fae\u6d41\u63a7\u50a8\u5907\u8ba1\u7b97\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5fae\u6d41\u63a7\u50a8\u5907\u8ba1\u7b97\u5728\u9ad8\u6548\u3001\u9002\u5e94\u6027\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u7535\u5b50\u8bbe\u5907\u4e0d\u9002\u7528\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2508.11415", "pdf": "https://arxiv.org/pdf/2508.11415", "abs": "https://arxiv.org/abs/2508.11415", "authors": ["Ra\u00efssa Nataf", "Yoram Moses"], "title": "Time, Fences and the Ordering of Events in TSO", "categories": ["cs.DC"], "comment": null, "summary": "The Total Store Order (TSO) is arguably the most widely used relaxed memory\nmodel in multiprocessor architectures, widely implemented, for example in\nIntel's x86 and x64 platforms. It allows processes to delay the visibility of\nwrites through store buffering. While this supports hardware-level\noptimizations and makes a significant contribution to multiprocessor\nefficiency, it complicates reasoning about correctness, as executions may\nviolate sequential consistency. Ensuring correct behavior often requires\ninserting synchronization primitives such as memory fences ($F$) or atomic\nread-modify-write ($RMW$) operations, but this approach can incur significant\nperformance costs. In this work, we develop a semantic framework that precisely\ncharacterizes when such synchronization is necessary under TSO. We introduce a\nnovel TSO-specific occurs-before relation, which adapts Lamport's celebrated\nhappens-before relation from asynchronous message-passing systems to the TSO\nsetting. Our main result is a theorem that proves that the only way to ensure\nthat two events that take place at different sites are temporally ordered is by\nhaving the execution create an occurs-before chain between the events. By\nstudying the role of fences and $RMW$s in creating occurs-before chains, we are\nthen able to capture cases in which these costly synchronization operations are\nunavoidable. Since proper real-time ordering of events is a fundamental aspect\nof consistency conditions such as Linearizability, our analysis provides a\nsound theoretical understanding of essential aspects of the TSO model. In\nparticular, we are able to generalize prior lower bounds for linearizable\nimplementations of shared memory objects. Our results capture the structure of\ninformation flow and causality in the TSO model by extending the standard\ncommunication-based reasoning from asynchronous systems to the TSO memory\nmodel.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u6846\u67b6\uff0c\u5728TSO\u5185\u5b58\u6a21\u578b\u4e0b\u7cbe\u786e\u5224\u65ad\u4f55\u65f6\u9700\u8981\u540c\u6b65\u64cd\u4f5c\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684occurs-before\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u540c\u6b65\u64cd\u4f5c\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63a8\u5e7f\u4e86\u7ebf\u6027\u5316\u5b9e\u73b0\u7684\u4e0b\u754c\u3002", "motivation": "TSO\u5185\u5b58\u6a21\u578b\u867d\u63d0\u5347\u4e86\u591a\u5904\u7406\u5668\u6548\u7387\uff0c\u4f46\u589e\u52a0\u4e86\u6b63\u786e\u6027\u63a8\u7406\u7684\u590d\u6742\u6027\uff0c\u9700\u63d2\u5165\u9ad8\u6027\u80fd\u5f00\u9500\u7684\u540c\u6b65\u64cd\u4f5c\u3002\u7814\u7a76\u65e8\u5728\u786e\u5b9a\u8fd9\u4e9b\u64cd\u4f5c\u4f55\u65f6\u5fc5\u8981\u3002", "method": "\u5f15\u5165TSO\u7279\u6709\u7684occurs-before\u5173\u7cfb\uff0c\u6269\u5c55\u81eaLamport\u7684happens-before\u5173\u7cfb\uff0c\u901a\u8fc7\u5b9a\u7406\u8bc1\u660e\u4e8b\u4ef6\u6392\u5e8f\u7684\u552f\u4e00\u65b9\u5f0f\u3002", "result": "\u63ed\u793a\u4e86\u540c\u6b65\u64cd\u4f5c\u7684\u4e0d\u53ef\u907f\u514d\u60c5\u51b5\uff0c\u63a8\u5e7f\u4e86\u7ebf\u6027\u5316\u5b9e\u73b0\u7684\u4e0b\u754c\uff0c\u4e3aTSO\u6a21\u578b\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u6846\u67b6\u6269\u5c55\u4e86\u5f02\u6b65\u7cfb\u7edf\u7684\u901a\u4fe1\u63a8\u7406\u81f3TSO\u6a21\u578b\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u4fe1\u606f\u6d41\u548c\u56e0\u679c\u6027\u7ed3\u6784\u3002"}}
{"id": "2508.11515", "pdf": "https://arxiv.org/pdf/2508.11515", "abs": "https://arxiv.org/abs/2508.11515", "authors": ["Qipeng Kuang", "V\u00e1clav K\u016fla", "Ond\u0159ej Ku\u017eelka", "Yuanhong Wang", "Yuyi Wang"], "title": "Weighted First Order Model Counting for Two-variable Logic with Axioms on Two Relations", "categories": ["cs.LO", "cs.AI", "03C13, 68T27", "F.4.0"], "comment": "24 pages, 5 figures", "summary": "The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the\nweighted sum of models of a given first-order logic sentence over a given\ndomain. The boundary between fragments for which WFOMC can be computed in\npolynomial time relative to the domain size lies between the two-variable\nfragment ($\\text{FO}^2$) and the three-variable fragment ($\\text{FO}^3$). It is\nknown that WFOMC for \\FOthree{} is $\\mathsf{\\#P_1}$-hard while polynomial-time\nalgorithms exist for computing WFOMC for $\\text{FO}^2$ and $\\text{C}^2$,\npossibly extended by certain axioms such as the linear order axiom, the\nacyclicity axiom, and the connectedness axiom. All existing research has\nconcentrated on extending the fragment with axioms on a single distinguished\nrelation, leaving a gap in understanding the complexity boundary of axioms on\nmultiple relations. In this study, we explore the extension of the two-variable\nfragment by axioms on two relations, presenting both negative and positive\nresults. We show that WFOMC for $\\text{FO}^2$ with two linear order relations\nand $\\text{FO}^2$ with two acyclic relations are $\\mathsf{\\#P_1}$-hard.\nConversely, we provide an algorithm in time polynomial in the domain size for\nWFOMC of $\\text{C}^2$ with a linear order relation, its successor relation and\nanother successor relation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u52a0\u6743\u4e00\u9636\u6a21\u578b\u8ba1\u6570\u95ee\u9898\uff08WFOMC\uff09\u5728\u4e24\u53d8\u91cf\u903b\u8f91\u7247\u6bb5\uff08FO\u00b2\uff09\u4e2d\u6269\u5c55\u5230\u4e24\u4e2a\u5173\u7cfb\u7684\u590d\u6742\u6027\u8fb9\u754c\uff0c\u65e2\u5c55\u793a\u4e86\u8d1f\u9762\u7684#P\u2081-\u96be\u7ed3\u679c\uff0c\u4e5f\u63d0\u4f9b\u4e86\u6b63\u9762\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u4e8e\u5355\u4e00\u5173\u7cfb\u7684\u6269\u5c55\uff0c\u800c\u5ffd\u89c6\u4e86\u591a\u5173\u7cfb\u516c\u7406\u7684\u590d\u6742\u6027\u8fb9\u754c\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e24\u53d8\u91cf\u903b\u8f91\u7247\u6bb5\uff08FO\u00b2\uff09\u6269\u5c55\u5230\u4e24\u4e2a\u5173\u7cfb\u7684\u60c5\u51b5\uff0c\u5c55\u793a\u5176#P\u2081-\u96be\u6027\uff0c\u5e76\u63d0\u51fa\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u89e3\u51b3C\u00b2\u7247\u6bb5\u7684\u7279\u5b9a\u6269\u5c55\u3002", "result": "\u8bc1\u660e\u4e86FO\u00b2\u52a0\u4e24\u4e2a\u7ebf\u6027\u5e8f\u5173\u7cfb\u6216\u4e24\u4e2a\u65e0\u73af\u5173\u7cfb\u7684WFOMC\u662f#P\u2081-\u96be\u7684\uff0c\u540c\u65f6\u4e3aC\u00b2\u52a0\u7ebf\u6027\u5e8f\u5173\u7cfb\u53ca\u4e24\u4e2a\u540e\u7ee7\u5173\u7cfb\u63d0\u4f9b\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002", "conclusion": "\u591a\u5173\u7cfb\u516c\u7406\u7684\u6269\u5c55\u663e\u8457\u63d0\u9ad8\u4e86WFOMC\u7684\u590d\u6742\u6027\uff0c\u4f46\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u4ecd\u5b58\u5728\u9ad8\u6548\u7b97\u6cd5\u3002"}}
{"id": "2508.11468", "pdf": "https://arxiv.org/pdf/2508.11468", "abs": "https://arxiv.org/abs/2508.11468", "authors": ["Zhihao Gong", "Zeyu Sun", "Dong Huang", "Qingyuan Liang", "Jie M. Zhang", "Dan Hao"], "title": "TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation", "categories": ["cs.SE"], "comment": null, "summary": "Automatic code translation is a fundamental task in modern software\ndevelopment. While the advent of Large Language Models (LLMs) has significantly\nimproved the correctness of code translation, the critical dimension of\nexecution efficiency remains overlooked. To address this gap, we introduce\nTRACY, the first comprehensive benchmark designed to evaluate the execution\nefficiency of LLM-translated code. TRACY is constructed through an LLM-driven\ntwo-stage pipeline: an initial stage generates a suite of stress tests to\namplify performance differences, followed by an efficiency-oriented task\npruning stage that isolates the efficiency-distinguishing tasks. The resulting\nbenchmark comprises 1,011 code translation tasks across C++, Java, and Python,\neach accompanied by an average of 22.1 verified reference translations and 10\ncomputationally demanding tests. Our extensive evaluation of 26 representative\nLLMs reveals that even top-tier LLMs struggle to consistently produce efficient\ncode translations. For instance, Claude-4-think, the leading model for\ncorrectness, ranks eighth overall when time efficiency is taken into account,\nsurpassed by several smaller open-source models. We further pinpoint that\nalgorithmic flaws and improper resource handling are the most detrimental,\ncausing a median time slowdown of 5.6$\\times$ and memory increase of\n12.0$\\times$, respectively. Our work underscores the necessity of jointly\noptimizing for correctness and efficiency in future LLM-based code translation.", "AI": {"tldr": "TRACY\u662f\u9996\u4e2a\u8bc4\u4f30LLM\u7ffb\u8bd1\u4ee3\u7801\u6267\u884c\u6548\u7387\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u867d\u6b63\u786e\u6027\u63d0\u5347\uff0c\u4f46\u6267\u884c\u6548\u7387\u88ab\u5ffd\u89c6\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5LLM\u9a71\u52a8\u6d41\u7a0b\u6784\u5efaTRACY\uff0c\u5305\u542b1011\u4e2a\u4efb\u52a1\u548c\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f3026\u4e2aLLM\u53d1\u73b0\u6548\u7387\u95ee\u9898\u7a81\u51fa\uff0c\u5927\u6a21\u578b\u672a\u5fc5\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u672a\u6765\u9700\u540c\u65f6\u4f18\u5316\u6b63\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.11317", "pdf": "https://arxiv.org/pdf/2508.11317", "abs": "https://arxiv.org/abs/2508.11317", "authors": ["Yuchen Zhou", "Jiayu Tang", "Shuo Yang", "Xiaoyan Xiao", "Yuqin Dai", "Wenhao Yang", "Chao Gou", "Xiaobo Xia", "Tat-Seng Chua"], "title": "Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Vision-Language Models (VLMs), exemplified by CLIP, have emerged as\nfoundational for multimodal intelligence. However, their capacity for logical\nunderstanding remains significantly underexplored, resulting in critical\n''logical blindspots'' that limit their reliability in practical applications.\nTo systematically diagnose this, we introduce LogicBench, a comprehensive\nbenchmark with over 50,000 vision-language pairs across 9 logical categories\nand 4 diverse scenarios: images, videos, anomaly detection, and medical\ndiagnostics. Our evaluation reveals that existing VLMs, even the\nstate-of-the-art ones, fall at over 40 accuracy points below human performance,\nparticularly in challenging tasks like Causality and Conditionality,\nhighlighting their reliance on surface semantics over critical logical\nstructures. To bridge this gap, we propose LogicCLIP, a novel training\nframework designed to boost VLMs' logical sensitivity through advancements in\nboth data generation and optimization objectives. LogicCLIP utilizes\nlogic-aware data generation and a contrastive learning strategy that combines\ncoarse-grained alignment, a fine-grained multiple-choice objective, and a novel\nlogical structure-aware objective. Extensive experiments demonstrate\nLogicCLIP's substantial improvements in logical comprehension across all\nLogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP\nretains, and often surpasses, competitive performance on general\nvision-language benchmarks, demonstrating that the enhanced logical\nunderstanding does not come at the expense of general alignment. We believe\nthat LogicBench and LogicCLIP will be important resources for advancing VLM\nlogical capabilities.", "AI": {"tldr": "LogicBench \u662f\u4e00\u4e2a\u7528\u4e8e\u8bca\u65ad\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u903b\u8f91\u76f2\u70b9\u7684\u57fa\u51c6\uff0cLogicCLIP \u662f\u5176\u6539\u8fdb\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u903b\u8f91\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u7406\u89e3\u80fd\u529b\u4e0a\u5b58\u5728\u91cd\u5927\u76f2\u70b9\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa LogicBench \u57fa\u51c6\u6d4b\u8bd5\u548c LogicCLIP \u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u903b\u8f91\u611f\u77e5\u6570\u636e\u751f\u6210\u548c\u591a\u76ee\u6807\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u903b\u8f91\u7406\u89e3\u80fd\u529b\u3002", "result": "LogicCLIP \u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u4efb\u52a1\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "LogicBench \u548c LogicCLIP \u662f\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u903b\u8f91\u80fd\u529b\u7684\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2508.10917", "pdf": "https://arxiv.org/pdf/2508.10917", "abs": "https://arxiv.org/abs/2508.10917", "authors": ["Chidera W. Amazu", "Joseph Mietkiewicz", "Ammar N. Abbas", "Gabriele Baldissone", "Davide Fissore", "Micaela Demichela", "Anders L. Madsen", "Maria Chiara Leva"], "title": "Managing the unexpected: Operator behavioural data and its value in predicting correct alarm responses", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Data from psychophysiological measures can offer new insight into control\nroom operators' behaviour, cognition, and mental workload status. This can be\nparticularly helpful when combined with appraisal of capacity to respond to\npossible critical plant conditions (i.e. critical alarms response scenarios).\nHowever, wearable physiological measurement tools such as eye tracking and EEG\ncaps can be perceived as intrusive and not suitable for usage in daily\noperations. Therefore, this article examines the potential of using real-time\ndata from process and operator-system interactions during abnormal scenarios\nthat can be recorded and retrieved from the distributed control system's\nhistorian or process log, and their capacity to provide insight into operator\nbehavior and predict their response outcomes, without intruding on daily tasks.\nData for this study were obtained from a design of experiment using a\nformaldehyde production plant simulator and four human-in-the-loop experimental\nsupport configurations. A comparison between the different configurations in\nterms of both behaviour and performance is presented in this paper. A step-wise\nlogistic regression and a Bayesian network models were used to achieve this\nobjective. The results identified some predictive metrics and the paper discuss\ntheir value as precursor or predictor of overall system performance in alarm\nresponse scenarios. Knowledge of relevant and predictive behavioural metrics\naccessible in real time can better equip decision-makers to predict outcomes\nand provide timely support measures for operators.", "AI": {"tldr": "\u8ad6\u6587\u63a2\u8a0e\u5982\u4f55\u5229\u7528\u5be6\u6642\u6578\u64da\u5206\u6790\u63a7\u5236\u5ba4\u64cd\u4f5c\u54e1\u7684\u884c\u70ba\u548c\u53cd\u61c9\uff0c\u7121\u9700\u4fb5\u5165\u6027\u8a2d\u5099\u3002", "motivation": "\u73fe\u6709\u751f\u7406\u6e2c\u91cf\u5de5\u5177\u53ef\u80fd\u5e72\u64fe\u65e5\u5e38\u64cd\u4f5c\uff0c\u9700\u5c0b\u627e\u66ff\u4ee3\u65b9\u6cd5\u4f86\u8a55\u4f30\u64cd\u4f5c\u54e1\u884c\u70ba\u548c\u8a8d\u77e5\u72c0\u614b\u3002", "method": "\u4f7f\u7528\u7532\u919b\u751f\u7522\u6a21\u64ec\u5668\u548c\u56db\u7a2e\u4eba\u6a5f\u5be6\u9a57\u914d\u7f6e\uff0c\u7d50\u5408\u9010\u6b65\u908f\u8f2f\u56de\u6b78\u548c\u8c9d\u8449\u65af\u7db2\u7d61\u6a21\u578b\u5206\u6790\u6578\u64da\u3002", "result": "\u767c\u73fe\u4e00\u4e9b\u53ef\u9810\u6e2c\u6307\u6a19\uff0c\u53ef\u7528\u65bc\u5373\u6642\u8a55\u4f30\u64cd\u4f5c\u54e1\u8868\u73fe\u4e26\u652f\u6301\u6c7a\u7b56\u3002", "conclusion": "\u5be6\u6642\u884c\u70ba\u6307\u6a19\u53ef\u5e6b\u52a9\u9810\u6e2c\u64cd\u4f5c\u54e1\u8868\u73fe\uff0c\u63d0\u4f9b\u53ca\u6642\u652f\u6301\u3002"}}
{"id": "2508.11059", "pdf": "https://arxiv.org/pdf/2508.11059", "abs": "https://arxiv.org/abs/2508.11059", "authors": ["Christian Roth", "Rahmin Bender-Salazar", "Breanne Pitt"], "title": "Stories and Systems: Educational Interactive Storytelling to Teach Media Literacy and Systemic Thinking", "categories": ["cs.HC", "cs.ET"], "comment": "Under submission (May, 2025)", "summary": "This paper explores how Interactive Digital Narratives (IDNs) can support\nlearners in developing the critical literacies needed to address complex\nsocietal challenges, so-called wicked problems, such as climate change,\npandemics, and social inequality. While digital technologies offer broad access\nto narratives and data, they also contribute to misinformation and the\noversimplification of interconnected issues. IDNs enable learners to navigate\nnonlinear, interactive stories, fostering deeper understanding and engagement.\nWe introduce Systemic Learning IDNs: interactive narrative experiences\nexplicitly designed to help learners explore and reflect on complex systems and\ninterdependencies. To guide their creation and use, we propose the CLASS\nframework, a structured model that integrates systems thinking, design\nthinking, and storytelling. This transdisciplinary approach supports learners\nin developing curiosity, critical thinking, and collaborative problem-solving.\nFocusing on the classroom context, we apply CLASS to two cases, one commercial\nnarrative simulation and one educational prototype, offering a comparative\nanalysis and practical recommendations for future design and implementation. By\ncombining narrative, systems mapping, and participatory design, this paper\nhighlights how IDNs can become powerful tools for transformative,\nsystems-oriented learning in an increasingly complex world.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4ea4\u4e92\u5f0f\u6570\u5b57\u53d9\u4e8b\uff08IDN\uff09\u5982\u4f55\u5e2e\u52a9\u5b66\u4e60\u8005\u57f9\u517b\u89e3\u51b3\u590d\u6742\u793e\u4f1a\u95ee\u9898\uff08\u5982\u6c14\u5019\u53d8\u5316\uff09\u6240\u9700\u7684\u6279\u5224\u6027\u7d20\u517b\uff0c\u63d0\u51faCLASS\u6846\u67b6\u6307\u5bfc\u8bbe\u8ba1\u3002", "motivation": "\u6570\u5b57\u6280\u672f\u666e\u53ca\u5e26\u6765\u4fe1\u606f\u8fc7\u8f7d\u548c\u95ee\u9898\u7b80\u5316\uff0cIDN\u901a\u8fc7\u975e\u7ebf\u6027\u4e92\u52a8\u4fc3\u8fdb\u6df1\u5c42\u6b21\u7406\u89e3\u3002", "method": "\u63d0\u51faCLASS\u6846\u67b6\uff0c\u7ed3\u5408\u7cfb\u7edf\u601d\u7ef4\u3001\u8bbe\u8ba1\u601d\u7ef4\u548c\u53d9\u4e8b\uff0c\u5e94\u7528\u4e8e\u4e24\u4e2a\u6848\u4f8b\uff08\u5546\u4e1a\u6a21\u62df\u548c\u6559\u80b2\u539f\u578b\uff09\u3002", "result": "IDN\u901a\u8fc7\u53d9\u4e8b\u548c\u7cfb\u7edf\u6620\u5c04\u6210\u4e3a\u7cfb\u7edf\u5bfc\u5411\u5b66\u4e60\u7684\u6709\u6548\u5de5\u5177\u3002", "conclusion": "IDN\u7ed3\u5408CLASS\u6846\u67b6\u53ef\u652f\u6301\u590d\u6742\u4e16\u754c\u4e2d\u7684\u53d8\u9769\u6027\u5b66\u4e60\u3002"}}
{"id": "2508.11623", "pdf": "https://arxiv.org/pdf/2508.11623", "abs": "https://arxiv.org/abs/2508.11623", "authors": ["Francesco Dagnino", "Amin Farjudian Eugenio Moggi"], "title": "Robust Topology and the Hausdorff-Smyth Monad on Metric Spaces over Continuous Quantales", "categories": ["cs.LO", "06B35, 06F07", "F.3.2"], "comment": "28 pages, 6 figures", "summary": "We define a (preorder-enriched) category $\\mathsf{Met}$ of quantale-valued\nmetric spaces and uniformly continuous maps, with the essential requirement\nthat the quantales are continuous. For each object $(X,d,Q)$ in this category,\nwhere $X$ is the carrier set, $Q$ is a continuous quantale, and $d: X \\times X\n\\to Q$ is the metric, we consider a topology $\\tau_d$ on $X$, which generalizes\nthe open ball topology, and a topology $\\tau_{d,R}$ on the powerset\n$\\mathsf{P}(X)$, called the robust topology, which captures robustness with\nrespect to small perturbations of parameters. We define a (preorder-enriched)\nmonad $\\mathsf{P}_S$ on $\\mathsf{Met}$, called the Hausdorff-Smyth monad, which\ncaptures the robust topology, in the sense that the open ball topology of the\nobject $\\mathsf{P}_S(X,d,Q)$ coincides with the robust topology $\\tau_{d,R}$\nfor the object $(X,d,Q)$. We prove that every topology arises from a\nquantale-valued metric. As such, our framework provides a foundation for\nquantitative reasoning about imprecision and robustness in a wide range of\ncomputational and physical systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9a\u4e49\u4e86\u4e00\u4e2a\u9884\u5e8f\u4e30\u5bcc\u7684\u8303\u7574$\\mathsf{Met}$\uff0c\u7814\u7a76\u8fde\u7eed\u91cf\u8bcd\u503c\u5ea6\u91cf\u7a7a\u95f4\u53ca\u4e00\u81f4\u8fde\u7eed\u6620\u5c04\uff0c\u5e76\u63d0\u51fa\u4e86Hausdorff-Smyth\u5355\u5b50$\\mathsf{P}_S$\uff0c\u7528\u4e8e\u6355\u83b7\u7a33\u5065\u62d3\u6251\u3002", "motivation": "\u4e3a\u89e3\u51b3\u8ba1\u7b97\u548c\u7269\u7406\u7cfb\u7edf\u4e2d\u5173\u4e8e\u4e0d\u7cbe\u786e\u6027\u548c\u7a33\u5065\u6027\u7684\u5b9a\u91cf\u63a8\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u8fde\u7eed\u91cf\u8bcd\u503c\u5ea6\u91cf\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u5b9a\u4e49\u8303\u7574$\\mathsf{Met}$\u53ca\u5176\u5bf9\u8c61$(X,d,Q)$\uff0c\u5f15\u5165\u5f00\u653e\u7403\u62d3\u6251$\\tau_d$\u548c\u7a33\u5065\u62d3\u6251$\\tau_{d,R}$\uff0c\u5e76\u6784\u9020Hausdorff-Smyth\u5355\u5b50$\\mathsf{P}_S$\u3002", "result": "\u8bc1\u660e\u4e86\u6bcf\u4e2a\u62d3\u6251\u5747\u53ef\u7531\u91cf\u8bcd\u503c\u5ea6\u91cf\u751f\u6210\uff0c\u4e14\u5355\u5b50$\\mathsf{P}_S$\u7684\u5f00\u653e\u7403\u62d3\u6251\u4e0e\u7a33\u5065\u62d3\u6251\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5e7f\u6cdb\u7684\u8ba1\u7b97\u548c\u7269\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u4e8e\u4e0d\u7cbe\u786e\u6027\u548c\u7a33\u5065\u6027\u7684\u5b9a\u91cf\u63a8\u7406\u57fa\u7840\u3002"}}
{"id": "2508.11571", "pdf": "https://arxiv.org/pdf/2508.11571", "abs": "https://arxiv.org/abs/2508.11571", "authors": ["Alexander Bakhtin"], "title": "Temporal Network Analysis of Microservice Architectural Degradation", "categories": ["cs.SE", "cs.DM"], "comment": null, "summary": "Microservice architecture can be modeled as a network of microservices making\ncalls to each other, commonly known as the service dependency graph. Network\nScience can provide methods to study such networks. In particular, temporal\nnetwork analysis is a branch of Network Science that analyzes networks evolving\nwith time. In microservice systems, temporal networks can arise if we examine\nthe architecture of the system across releases or monitor a deployed system\nusing tracing.\n  In this research summary paper, I discuss the challenges in obtaining\ntemporal networks from microservice systems and analyzing them with the\ntemporal network methods. In particular, the most complete temporal network\nthat we could obtain contains 7 time instances and 42 microservices, which\nlimits the potential analysis that could be applied.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ece\u5fae\u670d\u52a1\u7cfb\u7edf\u83b7\u53d6\u65f6\u95f4\u7f51\u7edc\u5e76\u7528\u65f6\u95f4\u7f51\u7edc\u65b9\u6cd5\u5206\u6790\u7684\u6311\u6218\uff0c\u6570\u636e\u96c6\u9650\u5236\u4e3a7\u4e2a\u65f6\u95f4\u5b9e\u4f8b\u548c42\u4e2a\u5fae\u670d\u52a1\u3002", "motivation": "\u7814\u7a76\u5fae\u670d\u52a1\u67b6\u6784\u968f\u65f6\u95f4\u53d8\u5316\u7684\u884c\u4e3a\uff0c\u901a\u8fc7\u65f6\u95f4\u7f51\u7edc\u5206\u6790\u65b9\u6cd5\u63ed\u793a\u5176\u52a8\u6001\u7279\u6027\u3002", "method": "\u5229\u7528\u7f51\u7edc\u79d1\u5b66\u4e2d\u7684\u65f6\u95f4\u7f51\u7edc\u5206\u6790\u65b9\u6cd5\uff0c\u5206\u6790\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u7684\u670d\u52a1\u4f9d\u8d56\u56fe\u3002", "result": "\u6570\u636e\u96c6\u89c4\u6a21\u8f83\u5c0f\uff087\u4e2a\u65f6\u95f4\u5b9e\u4f8b\u548c42\u4e2a\u5fae\u670d\u52a1\uff09\uff0c\u9650\u5236\u4e86\u5206\u6790\u7684\u6df1\u5ea6\u548c\u5e7f\u5ea6\u3002", "conclusion": "\u65f6\u95f4\u7f51\u7edc\u5206\u6790\u5bf9\u5fae\u670d\u52a1\u7cfb\u7edf\u7814\u7a76\u6709\u6f5c\u529b\uff0c\u4f46\u6570\u636e\u83b7\u53d6\u548c\u89c4\u6a21\u662f\u4e3b\u8981\u6311\u6218\u3002"}}
{"id": "2508.10919", "pdf": "https://arxiv.org/pdf/2508.10919", "abs": "https://arxiv.org/abs/2508.10919", "authors": ["Mohammed Saqr", "Kamila Misiejuk", "Sonsoles L\u00f3pez-Pernas"], "title": "Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "While research on human-AI collaboration exists, it mainly examined language\nlearning and used traditional counting methods with little attention to\nevolution and dynamics of collaboration on cognitively demanding tasks. This\nstudy examines human-AI interactions while solving a complex problem.\nStudent-AI interactions were qualitatively coded and analyzed with transition\nnetwork analysis, sequence analysis and partial correlation networks as well as\ncomparison of frequencies using chi-square and Person-residual shaded Mosaic\nplots to map interaction patterns, their evolution, and their relationship to\nproblem complexity and student performance. Findings reveal a dominant\nInstructive pattern with interactions characterized by iterative ordering\nrather than collaborative negotiation. Oftentimes, students engaged in long\nthreads that showed misalignment between their prompts and AI output that\nexemplified a lack of synergy that challenges the prevailing assumptions about\nLLMs as collaborative partners. We also found no significant correlations\nbetween assignment complexity, prompt length, and student grades suggesting a\nlack of cognitive depth, or effect of problem difficulty. Our study indicates\nthat the current LLMs, optimized for instruction-following rather than\ncognitive partnership, compound their capability to act as cognitively\nstimulating or aligned collaborators. Implications for designing AI systems\nthat prioritize cognitive alignment and collaboration are discussed.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e0eAI\u5728\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u7684\u4e92\u52a8\u6a21\u5f0f\uff0c\u53d1\u73b0\u5f53\u524d\u7684LLM\uff08\u5982ChatGPT\uff09\u66f4\u503e\u5411\u4e8e\u6307\u4ee4\u9075\u5faa\u800c\u975e\u8ba4\u77e5\u5408\u4f5c\uff0c\u7f3a\u4e4f\u534f\u540c\u6548\u5e94\uff0c\u4e14\u4efb\u52a1\u590d\u6742\u5ea6\u4e0e\u5b66\u751f\u8868\u73b0\u65e0\u663e\u8457\u5173\u8054\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u8bed\u8a00\u5b66\u4e60\uff0c\u5ffd\u7565\u4e86AI\u5728\u8ba4\u77e5\u9700\u6c42\u4efb\u52a1\u4e2d\u7684\u52a8\u6001\u5408\u4f5c\u3002\u672c\u7814\u7a76\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u7f16\u7801\u548c\u591a\u79cd\u5206\u6790\u65b9\u6cd5\uff08\u5982\u8fc7\u6e21\u7f51\u7edc\u3001\u5e8f\u5217\u5206\u6790\u7b49\uff09\u7814\u7a76\u5b66\u751f\u4e0eAI\u7684\u4e92\u52a8\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u4e92\u52a8\u4ee5\u6307\u4ee4\u6027\u4e3a\u4e3b\uff0c\u7f3a\u4e4f\u534f\u540c\uff1b\u4efb\u52a1\u590d\u6742\u5ea6\u4e0e\u6210\u7ee9\u65e0\u663e\u8457\u5173\u8054\u3002", "conclusion": "\u5f53\u524dLLM\u66f4\u9002\u5408\u6307\u4ee4\u9075\u5faa\u800c\u975e\u8ba4\u77e5\u5408\u4f5c\uff0c\u8bbe\u8ba1AI\u7cfb\u7edf\u9700\u66f4\u6ce8\u91cd\u8ba4\u77e5\u5bf9\u9f50\u4e0e\u5408\u4f5c\u3002"}}
{"id": "2508.11540", "pdf": "https://arxiv.org/pdf/2508.11540", "abs": "https://arxiv.org/abs/2508.11540", "authors": ["Dejan Delic", "John Marcoux"], "title": "The Constraint Satisfaction Problem Over Multisorted Cores", "categories": ["cs.CC", "cs.LO", "68R"], "comment": null, "summary": "Constraint Satisfaction Problems (CSPs, for short) make up a class of\nproblems with applications in many areas of computer science. The first\nclassification of these problems was given by Schaeffer who showed that every\nCSP over the domain {0,1} is either in P or is NP-complete. More recently this\nwas shown to hold for all CSPs over finite relational structures independently\nby Bulatov and Zhuk. Furthermore, they characterized the complexity based\nsolely on the polymorphism algebra of the associated relational structure,\nbuilding upon the deep connections between universal algebra and complexity\ntheory. In this article we extend this and consider what happens if the\ninstance forms a special type of relational core called a multisorted core. Our\nmain result is that in this case the problem is reducible to computing the\ndeterminant of an integer valued matrix which places it in the complexity class\nDET, which is likely a strict subset of P.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff08CSPs\uff09\u5728\u7279\u6b8a\u5173\u7cfb\u6838\u5fc3\uff08multisorted core\uff09\u4e0b\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u7c7b\u95ee\u9898\u53ef\u4ee5\u5f52\u7ea6\u4e3a\u8ba1\u7b97\u6574\u6570\u77e9\u9635\u7684\u884c\u5217\u5f0f\uff0c\u5c5e\u4e8eDET\u7c7b\uff0c\u53ef\u80fd\u4e25\u683c\u5305\u542b\u4e8eP\u7c7b\u4e2d\u3002", "motivation": "Schaeffer\u3001Bulatov\u548cZhuk\u7684\u7814\u7a76\u5df2\u8bc1\u660e\u6709\u9650\u57df\u4e0a\u7684CSPs\u8981\u4e48\u5728P\u7c7b\u4e2d\uff0c\u8981\u4e48\u662fNP\u5b8c\u5168\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u5f53\u5b9e\u4f8b\u4e3a\u4e00\u79cd\u7279\u6b8a\u5173\u7cfb\u6838\u5fc3\uff08multisorted core\uff09\u65f6\uff0cCSPs\u7684\u590d\u6742\u6027\u662f\u5426\u4f1a\u53d1\u751f\u6539\u53d8\u3002", "method": "\u4f5c\u8005\u6269\u5c55\u4e86\u4e4b\u524d\u7684\u7814\u7a76\uff0c\u5206\u6790\u4e86multisorted core\u4e0b\u7684CSPs\u3002\u5177\u4f53\u800c\u8a00\uff0c\u901a\u8fc7\u5173\u8054\u5173\u7cfb\u6838\u5fc3\u7684\u591a\u6001\u4ee3\u6570\uff0c\u5c06\u5176\u590d\u6742\u6027\u95ee\u9898\u8f6c\u5316\u4e3a\u8ba1\u7b97\u6574\u6570\u77e9\u9635\u7684\u884c\u5217\u5f0f\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cmultisorted core\u4e0b\u7684CSPs\u53ef\u5f52\u7ea6\u4e3a\u8ba1\u7b97\u6574\u6570\u77e9\u9635\u7684\u884c\u5217\u5f0f\uff0c\u56e0\u6b64\u5176\u590d\u6742\u6027\u5c5e\u4e8eDET\u7c7b\uff0c\u800cDET\u53ef\u80fd\u662fP\u7c7b\u7684\u4e25\u683c\u5b50\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86CSPs\u590d\u6742\u6027\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u63ed\u793a\u4e86multisorted core\u4e0b\u95ee\u9898\u7684\u590d\u6742\u6027\u754c\u9650\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.11022", "pdf": "https://arxiv.org/pdf/2508.11022", "abs": "https://arxiv.org/abs/2508.11022", "authors": ["Lauren W. Wang", "Parastoo Abtahi"], "title": "GhostObjects: Instructing Robots by Manipulating Spatially Aligned Virtual Twins in Augmented Reality", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Robots are increasingly capable of autonomous operations, yet human\ninteraction remains essential for issuing personalized instructions. Instead of\ndirectly controlling robots through Programming by Demonstration (PbD) or\nteleoperation, we propose giving instructions by interacting with\nGhostObjects-world-aligned, life-size virtual twins of physical objects-in\naugmented reality (AR). By direct manipulation of GhostObjects, users can\nprecisely specify physical goals and spatial parameters, with features\nincluding real-world lasso selection of multiple objects and snapping back to\ndefault positions, enabling tasks beyond simple pick-and-place.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4e2d\u7684\u865a\u62df\u5b6a\u751f\u5bf9\u8c61\uff08GhostObjects\uff09\u6765\u6307\u5bfc\u673a\u5668\u4eba\u5b8c\u6210\u4efb\u52a1\u7684\u65b0\u65b9\u6cd5\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u76f4\u63a5\u63a7\u5236\u65b9\u5f0f\u3002", "motivation": "\u867d\u7136\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u589e\u5f3a\uff0c\u4f46\u4e2a\u6027\u5316\u6307\u4ee4\u4ecd\u9700\u4eba\u7c7b\u4ea4\u4e92\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u6f14\u793a\u7f16\u7a0b\u6216\u9065\u64cd\u4f5c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528AR\u4e2d\u7684GhostObjects\uff0c\u7528\u6237\u53ef\u76f4\u63a5\u64cd\u4f5c\u865a\u62df\u5bf9\u8c61\u6765\u7cbe\u786e\u6307\u5b9a\u7269\u7406\u76ee\u6807\u548c\u7a7a\u95f4\u53c2\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u591a\u5bf9\u8c61\u9009\u62e9\u548c\u9ed8\u8ba4\u4f4d\u7f6e\u56de\u4f4d\u7b49\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u6bd4\u7b80\u5355\u7684\u62fe\u53d6-\u653e\u7f6e\u66f4\u590d\u6742\u7684\u4efb\u52a1\u3002", "conclusion": "GhostObjects\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u76f4\u89c2\u3001\u7075\u6d3b\u7684\u673a\u5668\u4eba\u6307\u4ee4\u65b9\u5f0f\u3002"}}
{"id": "2508.11591", "pdf": "https://arxiv.org/pdf/2508.11591", "abs": "https://arxiv.org/abs/2508.11591", "authors": ["Durga Joshi", "Chandi Witharana", "Robert Fahey", "Thomas Worthley", "Zhe Zhu", "Diego Cerrai"], "title": "DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring", "categories": ["cs.CV", "cs.ET"], "comment": "35 Pages, 15 figures", "summary": "Our study introduces a novel, low-cost, and reproducible framework for\nreal-time, object-level structural assessment and geolocation of roadside\nvegetation and infrastructure with commonly available but underutilized\ndashboard camera (dashcam) video data. We developed an end-to-end pipeline that\ncombines monocular depth estimation, depth error correction, and geometric\ntriangulation to generate accurate spatial and structural data from\nstreet-level video streams from vehicle-mounted dashcams. Depth maps were first\nestimated using a state-of-the-art monocular depth model, then refined via a\ngradient-boosted regression framework to correct underestimations, particularly\nfor distant objects. The depth correction model achieved strong predictive\nperformance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly\nreducing bias beyond 15 m. Further, object locations were estimated using\nGPS-based triangulation, while object heights were calculated using pin hole\ncamera geometry. Our method was evaluated under varying conditions of camera\nplacement and vehicle speed. Low-speed vehicle with inside camera gave the\nhighest accuracy, with mean geolocation error of 2.83 m, and mean absolute\nerror (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To\nthe best of our knowledge, it is the first framework to combine monocular depth\nmodeling, triangulated GPS-based geolocation, and real-time structural\nassessment for urban vegetation and infrastructure using consumer-grade video\ndata. Our approach complements conventional RS methods, such as LiDAR and image\nby offering a fast, real-time, and cost-effective solution for object-level\nmonitoring of vegetation risks and infrastructure exposure, making it\nespecially valuable for utility companies, and urban planners aiming for\nscalable and frequent assessments in dynamic urban environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u53ef\u91cd\u590d\u7684\u6846\u67b6\uff0c\u5229\u7528\u8f66\u8f7d\u6444\u50cf\u5934\u5b9e\u65f6\u8bc4\u4f30\u548c\u5b9a\u4f4d\u8def\u8fb9\u690d\u88ab\u53ca\u57fa\u7840\u8bbe\u65bd\uff0c\u7ed3\u5408\u6df1\u5ea6\u4f30\u8ba1\u3001\u8bef\u5dee\u6821\u6b63\u548c\u51e0\u4f55\u4e09\u89d2\u6d4b\u91cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u7a7a\u95f4\u548c\u7ed3\u6784\u6570\u636e\u5206\u6790\u3002", "motivation": "\u4e3a\u57ce\u5e02\u89c4\u5212\u548c\u516c\u7528\u4e8b\u4e1a\u516c\u53f8\u63d0\u4f9b\u4e00\u79cd\u5feb\u901f\u3001\u5b9e\u65f6\u3001\u4f4e\u6210\u672c\u7684\u5bf9\u8c61\u7ea7\u76d1\u6d4b\u65b9\u6848\uff0c\u8865\u5145\u4f20\u7edf\u9065\u611f\u65b9\u6cd5\uff08\u5982LiDAR\uff09\u7684\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u7ed3\u5408\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u6df1\u5ea6\u8bef\u5dee\u6821\u6b63\u548c\u51e0\u4f55\u4e09\u89d2\u6d4b\u91cf\uff0c\u5229\u7528\u8f66\u8f7d\u6444\u50cf\u5934\u89c6\u9891\u6570\u636e\u8fdb\u884c\u5b9e\u65f6\u5206\u6790\u3002", "result": "\u6df1\u5ea6\u6821\u6b63\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u5f3a\uff08R2=0.92\uff0cMAE=0.31\uff09\uff0c\u5728\u4f4e\u901f\u8f66\u8f86\u548c\u5185\u90e8\u6444\u50cf\u5934\u6761\u4ef6\u4e0b\u5b9a\u4f4d\u8bef\u5dee\u6700\u5c0f\uff082.83\u7c73\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u7ed3\u5408\u5355\u76ee\u6df1\u5ea6\u5efa\u6a21\u3001GPS\u4e09\u89d2\u5b9a\u4f4d\u548c\u5b9e\u65f6\u7ed3\u6784\u8bc4\u4f30\uff0c\u4e3a\u52a8\u6001\u57ce\u5e02\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11278", "pdf": "https://arxiv.org/pdf/2508.11278", "abs": "https://arxiv.org/abs/2508.11278", "authors": ["Francesco Sovrano", "Gabriele Dominici", "Rita Sevastjanova", "Alessandra Stramiglio", "Alberto Bacchelli"], "title": "Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas", "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Human cognitive biases in software engineering can lead to costly errors.\nWhile general-purpose AI (GPAI) systems may help mitigate these biases due to\ntheir non-human nature, their training on human-generated data raises a\ncritical question: Do GPAI systems themselves exhibit cognitive biases?\n  To investigate this, we present the first dynamic benchmarking framework to\nevaluate data-induced cognitive biases in GPAI within software engineering\nworkflows. Starting with a seed set of 16 hand-crafted realistic tasks, each\nfeaturing one of 8 cognitive biases (e.g., anchoring, framing) and\ncorresponding unbiased variants, we test whether bias-inducing linguistic cues\nunrelated to task logic can lead GPAI systems from correct to incorrect\nconclusions.\n  To scale the benchmark and ensure realism, we develop an on-demand\naugmentation pipeline relying on GPAI systems to generate task variants that\npreserve bias-inducing cues while varying surface details. This pipeline\nensures correctness (88--99% on average, according to human evaluation),\npromotes diversity, and controls reasoning complexity by leveraging\nProlog-based reasoning and LLM-as-a-judge validation. It also verifies that the\nembedded biases are both harmful and undetectable by logic-based, unbiased\nreasoners.\n  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent\ntendency to rely on shallow linguistic heuristics over deep reasoning. All\nsystems exhibit cognitive biases (ranging from 5.9% to 35% across types), with\nbias sensitivity increasing sharply with task complexity (up to 49%),\nhighlighting critical risks in real-world software engineering deployments.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u901a\u7528AI\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8ba4\u77e5\u504f\u5dee\uff0c\u5176\u654f\u611f\u6027\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u800c\u4e0a\u5347\u3002", "motivation": "\u63a2\u8ba8\u901a\u7528AI\u7cfb\u7edf\u662f\u5426\u56e0\u8bad\u7ec3\u6570\u636e\u800c\u8868\u73b0\u51fa\u4eba\u7c7b\u8ba4\u77e5\u504f\u5dee\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc716\u4e2a\u4efb\u52a1\u6d4b\u8bd58\u79cd\u8ba4\u77e5\u504f\u5dee\uff0c\u5e76\u5229\u7528AI\u751f\u6210\u4efb\u52a1\u53d8\u4f53\u4ee5\u786e\u4fdd\u591a\u6837\u6027\u548c\u6b63\u786e\u6027\u3002", "result": "\u4e3b\u8981AI\u7cfb\u7edf\uff08GPT\u3001LLaMA\u3001DeepSeek\uff09\u5747\u8868\u73b0\u51fa\u8ba4\u77e5\u504f\u5dee\uff085.9%-35%\uff09\uff0c\u4e14\u504f\u5dee\u654f\u611f\u6027\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u663e\u8457\u589e\u52a0\uff08\u6700\u9ad849%\uff09\u3002", "conclusion": "AI\u7cfb\u7edf\u7684\u8ba4\u77e5\u504f\u5dee\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u98ce\u9669\u663e\u8457\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.11030", "pdf": "https://arxiv.org/pdf/2508.11030", "abs": "https://arxiv.org/abs/2508.11030", "authors": ["Zikai Wen", "Lanjing Liu", "Yaxing Yao"], "title": "Families' Vision of Generative AI Agents for Household Safety Against Digital and Physical Threats", "categories": ["cs.HC"], "comment": null, "summary": "As families face increasingly complex safety challenges in digital and\nphysical environments, generative AI (GenAI) presents new opportunities to\nsupport household safety through multiple specialized AI agents. Through a\ntwo-phase qualitative study consisting of individual interviews and\ncollaborative sessions with 13 parent-child dyads, we explored families'\nconceptualizations of GenAI and their envisioned use of AI agents in daily\nfamily life. Our findings reveal that families preferred to distribute\nsafety-related support across multiple AI agents, each embodying a familiar\ncaregiving role: a household manager coordinating routine tasks and mitigating\nrisks such as digital fraud and home accidents; a private tutor providing\npersonalized educational support, including safety education; and a family\ntherapist offering emotional support to address sensitive safety issues such as\ncyberbullying and digital harassment. Families emphasized the need for\nagent-specific privacy boundaries, recognized generational differences in trust\ntoward AI agents, and stressed the importance of maintaining open family\ncommunication alongside the assistance of AI agents. Based on these findings,\nwe propose a multi-agent system design featuring four privacy-preserving\nprinciples: memory segregation, conversational consent, selective data sharing,\nand progressive memory management to help balance safety, privacy, and autonomy\nwithin family contexts.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5bb6\u5ead\u5982\u4f55\u5229\u7528\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u591a\u4ee3\u7406\u7cfb\u7edf\u6765\u5e94\u5bf9\u5b89\u5168\u548c\u9690\u79c1\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u56db\u79cd\u9690\u79c1\u4fdd\u62a4\u539f\u5219\u3002", "motivation": "\u968f\u7740\u5bb6\u5ead\u9762\u4e34\u65e5\u76ca\u590d\u6742\u7684\u6570\u5b57\u548c\u7269\u7406\u73af\u5883\u5b89\u5168\u6311\u6218\uff0c\u751f\u6210\u5f0fAI\u4e3a\u5bb6\u5ead\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u7684\u652f\u6301\u673a\u4f1a\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u5b9a\u6027\u7814\u7a76\uff0c\u5305\u62ec\u4e0e13\u5bf9\u4eb2\u5b50\u7ec4\u7684\u4e2a\u4eba\u8bbf\u8c08\u548c\u534f\u4f5c\u4f1a\u8bae\uff0c\u63a2\u8ba8\u5bb6\u5ead\u5bf9GenAI\u7684\u8bbe\u60f3\u548cAI\u4ee3\u7406\u7684\u4f7f\u7528\u3002", "result": "\u5bb6\u5ead\u503e\u5411\u4e8e\u5c06\u5b89\u5168\u652f\u6301\u5206\u6563\u5230\u591a\u4e2aAI\u4ee3\u7406\uff0c\u6bcf\u4e2a\u4ee3\u7406\u626e\u6f14\u719f\u6089\u89d2\u8272\uff0c\u5e76\u5f3a\u8c03\u9690\u79c1\u8fb9\u754c\u3001\u4fe1\u4efb\u4ee3\u9645\u5dee\u5f02\u548c\u5bb6\u5ead\u6c9f\u901a\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5305\u542b\u56db\u9879\u9690\u79c1\u4fdd\u62a4\u539f\u5219\uff0c\u4ee5\u5e73\u8861\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u5b89\u5168\u3001\u9690\u79c1\u548c\u81ea\u4e3b\u6027\u3002"}}
{"id": "2508.11052", "pdf": "https://arxiv.org/pdf/2508.11052", "abs": "https://arxiv.org/abs/2508.11052", "authors": ["Evey Jiaxin Huang", "Matthew Easterday", "Elizabeth Gerber"], "title": "AI That Helps Us Help Each Other: A Proactive System for Scaffolding Mentor-Novice Collaboration in Entrepreneurship Coaching", "categories": ["cs.HC", "cs.AI", "68T35 (Primary), 68U99 (Secondary)", "H.5.2"], "comment": "To appear in CSCW 2025 Volume 9", "summary": "Entrepreneurship requires navigating open-ended, ill-defined problems:\nidentifying risks, challenging assumptions, and making strategic decisions\nunder deep uncertainty. Novice founders often struggle with these metacognitive\ndemands, while mentors face limited time and visibility to provide tailored\nsupport. We present a human-AI coaching system that combines a domain-specific\ncognitive model of entrepreneurial risk with a large language model (LLM) to\nproactively scaffold both novice and mentor thinking. The system proactively\nposes diagnostic questions that challenge novices' thinking and helps both\nnovices and mentors plan for more focused and emotionally attuned meetings.\nCritically, mentors can inspect and modify the underlying cognitive model,\nshaping the logic of the system to reflect their evolving needs. Through an\nexploratory field deployment, we found that using the system supported novice\nmetacognition, helped mentors plan emotionally attuned strategies, and improved\nmeeting depth, intentionality, and focus--while also surfaced key tensions\naround trust, misdiagnosis, and expectations of AI. We contribute design\nprinciples for proactive AI systems that scaffold metacognition and human-human\ncollaboration in complex, ill-defined domains, offering implications for\nsimilar domains like healthcare, education, and knowledge work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8ba4\u77e5\u6a21\u578b\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4eba\u673a\u8f85\u5bfc\u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u521b\u4e1a\u65b0\u624b\u548c\u5bfc\u5e08\u89e3\u51b3\u521b\u4e1a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u4f1a\u8bae\u8d28\u91cf\u548c\u60c5\u611f\u5171\u9e23\u3002", "motivation": "\u521b\u4e1a\u4e2d\u5f00\u653e\u5f0f\u95ee\u9898\u7684\u590d\u6742\u6027\u4f7f\u65b0\u624b\u548c\u5bfc\u5e08\u9762\u4e34\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u63d0\u4f9b\u652f\u6301\u7684\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u9886\u57df\u8ba4\u77e5\u6a21\u578b\u548cLLM\u7684\u7cfb\u7edf\uff0c\u4e3b\u52a8\u63d0\u51fa\u95ee\u9898\u5e76\u652f\u6301\u5bfc\u5e08\u4fee\u6539\u6a21\u578b\u903b\u8f91\u3002", "result": "\u5b9e\u5730\u6d4b\u8bd5\u8868\u660e\uff0c\u7cfb\u7edf\u63d0\u5347\u4e86\u65b0\u624b\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u5e2e\u52a9\u5bfc\u5e08\u89c4\u5212\u60c5\u611f\u7b56\u7565\uff0c\u5e76\u6539\u5584\u4e86\u4f1a\u8bae\u8d28\u91cf\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e3b\u52a8AI\u7cfb\u7edf\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u53ef\u5e94\u7528\u4e8e\u533b\u7597\u3001\u6559\u80b2\u7b49\u7c7b\u4f3c\u590d\u6742\u9886\u57df\u3002"}}
{"id": "2508.11062", "pdf": "https://arxiv.org/pdf/2508.11062", "abs": "https://arxiv.org/abs/2508.11062", "authors": ["Bhavishya Tarun", "Haoze Du", "Dinesh Kannan", "Edward F. Gehringer"], "title": "Human-in-the-Loop Systems for Adaptive Learning Using Generative AI", "categories": ["cs.HC", "cs.LG"], "comment": "Accepted for presentation at the Frontiers in Education Conference,\n  Nashville, Tennessee, USA, 2-5 November 2025", "summary": "A Human-in-the-Loop (HITL) approach leverages generative AI to enhance\npersonalized learning by directly integrating student feedback into\nAI-generated solutions. Students critique and modify AI responses using\npredefined feedback tags, fostering deeper engagement and understanding. This\nempowers students to actively shape their learning, with AI serving as an\nadaptive partner. The system uses a tagging technique and prompt engineering to\npersonalize content, informing a Retrieval-Augmented Generation (RAG) system to\nretrieve relevant educational material and adjust explanations in real time.\nThis builds on existing research in adaptive learning, demonstrating how\nstudent-driven feedback loops can modify AI-generated responses for improved\nstudent retention and engagement, particularly in STEM education. Preliminary\nfindings from a study with STEM students indicate improved learning outcomes\nand confidence compared to traditional AI tools. This work highlights AI's\npotential to create dynamic, feedback-driven, and personalized learning\nenvironments through iterative refinement.", "AI": {"tldr": "\u4f7f\u7528\u4eba\u5728\u56de\u8def\uff08HITL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u751f\u53cd\u9988\u548cAI\u751f\u6210\u89e3\u51b3\u65b9\u6848\u7684\u7ed3\u5408\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5b66\u751f\u76f4\u63a5\u53c2\u4e0eAI\u751f\u6210\u5185\u5bb9\u7684\u4fee\u6539\u548c\u53cd\u9988\uff0c\u63d0\u5347\u5b66\u4e60\u53c2\u4e0e\u5ea6\u548c\u7406\u89e3\u6df1\u5ea6\uff0c\u5c24\u5176\u662f\u5728STEM\u6559\u80b2\u9886\u57df\u3002", "method": "\u91c7\u7528\u9884\u5b9a\u4e49\u53cd\u9988\u6807\u7b7e\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u5b9e\u65f6\u8c03\u6574\u6559\u5b66\u5185\u5bb9\u4ee5\u6ee1\u8db3\u5b66\u751f\u9700\u6c42\u3002", "result": "\u521d\u6b65\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4f20\u7edfAI\u5de5\u5177\u76f8\u6bd4\uff0c\u5b66\u751f\u901a\u8fc7\u8be5\u65b9\u6cd5\u7684STEM\u5b66\u4e60\u6548\u679c\u548c\u81ea\u4fe1\u5fc3\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "AI\u53ef\u4ee5\u901a\u8fc7\u52a8\u6001\u53cd\u9988\u9a71\u52a8\u548c\u4e2a\u6027\u5316\u7684\u5b66\u4e60\u73af\u5883\uff0c\u63d0\u5347\u5b66\u751f\u7684\u4fdd\u7559\u7387\u548c\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2508.11072", "pdf": "https://arxiv.org/pdf/2508.11072", "abs": "https://arxiv.org/abs/2508.11072", "authors": ["Nishanth Chidambaram", "Weichen Liu", "Manas Satish Bedmutha", "Nadir Weibel", "Chen Chen"], "title": "DriveSimQuest: A VR Driving Simulator and Research Platform on Meta Quest with Unity", "categories": ["cs.HC", "H.5.m; J.m"], "comment": "3 pages, 2 figures, In the Proceedings of the 38th Annual ACM\n  Symposium on User Interface Software and Technology (UIST Adjunct '25),\n  September 28 - October 1, 2025, Busan, Republic of Korea", "summary": "Using head-mounted Virtual Reality (VR) displays to simulate driving is\ncritical to studying driving behavior and designing driver assistance systems.\nBut existing VR driving simulators are often limited to tracking only eye\nmovements. The bulky outside-in tracking setup and Unreal-based architecture\nalso present significant engineering challenges for interaction researchers and\npractitioners. We present DriveSimQuest, a VR driving simulator and research\nplatform built on the Meta Quest Pro and Unity, capable of capturing rich\nbehavioral signals such as gaze, facial expressions, hand activities, and\nfull-body gestures in real-time. DriveSimQuest offers a preliminary,\neasy-to-deploy platform that supports researchers and practitioners in studying\ndrivers' affective states and behaviors, and in designing future context-aware\ndriving assistance systems.", "AI": {"tldr": "DriveSimQuest\u662f\u4e00\u4e2a\u57fa\u4e8eMeta Quest Pro\u548cUnity\u5f00\u53d1\u7684VR\u9a7e\u9a76\u6a21\u62df\u5668\uff0c\u80fd\u591f\u5b9e\u65f6\u6355\u6349\u4e30\u5bcc\u7684\u9a7e\u9a76\u884c\u4e3a\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u62df\u5668\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684VR\u9a7e\u9a76\u6a21\u62df\u5668\u901a\u5e38\u53ea\u80fd\u8ddf\u8e2a\u773c\u90e8\u8fd0\u52a8\uff0c\u4e14\u8bbe\u5907\u7b28\u91cd\u3001\u67b6\u6784\u590d\u6742\uff0c\u9650\u5236\u4e86\u4ea4\u4e92\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86DriveSimQuest\u5e73\u53f0\uff0c\u5229\u7528Meta Quest Pro\u548cUnity\u6280\u672f\uff0c\u5b9e\u65f6\u6355\u6349\u591a\u79cd\u884c\u4e3a\u4fe1\u53f7\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6613\u4e8e\u90e8\u7f72\u7684\u5e73\u53f0\uff0c\u652f\u6301\u7814\u7a76\u8005\u7814\u7a76\u9a7e\u9a76\u5458\u7684 affective states\u548c\u884c\u4e3a\uff0c\u8bbe\u8ba1\u672a\u6765\u7684\u60c5\u5883\u611f\u77e5\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u3002", "conclusion": "DriveSimQuest\u4e3a\u9a7e\u9a76\u884c\u4e3a\u7814\u7a76\u548c\u8f85\u52a9\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u591a\u529f\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11149", "pdf": "https://arxiv.org/pdf/2508.11149", "abs": "https://arxiv.org/abs/2508.11149", "authors": ["Robert Wolfe", "Aayushi Dangol", "JaeWon Kim", "Alexis Hiniker"], "title": "Toward Needs-Conscious Design: Co-Designing a Human-Centered Framework for AI-Mediated Communication", "categories": ["cs.HC"], "comment": "Accepted for publication at AIES 2025", "summary": "We introduce Needs-Conscious Design, a human-centered framework for\nAI-mediated communication that builds on the principles of Nonviolent\nCommunication (NVC). We conducted an interview study with N=14 certified NVC\ntrainers and a diary study and co-design with N=13 lay users of online\ncommunication technologies to understand how NVC might inform design that\ncenters human relationships. We define three pillars of Needs-Conscious Design:\nIntentionality, Presence, and Receptiveness to Needs. Drawing on participant\nco-designs, we provide design concepts and illustrative examples for each of\nthese pillars. We further describe a problematic emergent property of\nAI-mediated communication identified by participants, which we call Empathy\nFog, and which is characterized by uncertainty over how much empathy,\nattention, and effort a user has actually invested via an AI-facilitated online\ninteraction. Finally, because even well-intentioned designs may alter user\nbehavior and process emotional data, we provide guiding questions for\nconsentful Needs-Conscious Design, applying an affirmative consent framework\nused in social media contexts. Needs-Conscious Design offers a foundation for\nleveraging AI to facilitate human connection, rather than replacing or\nobscuring it.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u4ea4\u6d41\u6846\u67b6\u2018Needs-Conscious Design\u2019\uff0c\u57fa\u4e8e\u975e\u66b4\u529b\u6c9f\u901a\uff08NVC\uff09\u539f\u5219\uff0c\u65e8\u5728\u901a\u8fc7\u4e09\u4e2a\u652f\u67f1\uff08\u610f\u5411\u6027\u3001\u5728\u573a\u6027\u548c\u9700\u6c42\u5f00\u653e\u6027\uff09\u4fc3\u8fdb\u4eba\u9645\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u5982\u4f55\u5229\u7528NVC\u539f\u5219\u8bbe\u8ba1AI\u4ea4\u6d41\u6280\u672f\uff0c\u4ee5\u589e\u5f3a\u4eba\u9645\u5173\u7cfb\u800c\u975e\u66ff\u4ee3\u6216\u6a21\u7cca\u5b83\u4eec\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5bf9N=14\u540dNVC\u8ba4\u8bc1\u57f9\u8bad\u5e08\u7684\u8bbf\u8c08\u7814\u7a76\uff0c\u4ee5\u53ca\u5bf9N=13\u540d\u5728\u7ebf\u4ea4\u6d41\u6280\u672f\u7528\u6237\u7684\u65e5\u8bb0\u7814\u7a76\u548c\u5171\u540c\u8bbe\u8ba1\u3002", "result": "\u7ed3\u679c\u5305\u62ec\u5b9a\u4e49\u4e86Needs-Conscious Design\u7684\u4e09\u4e2a\u652f\u67f1\uff0c\u63d0\u51fa\u4e86\u76f8\u5173\u8bbe\u8ba1\u6982\u5ff5\uff0c\u5e76\u8bc6\u522b\u4e86AI\u4ea4\u6d41\u4e2d\u7684\u2018Empathy Fog\u2019\u73b0\u8c61\u3002", "conclusion": "\u7ed3\u8bba\u662fNeeds-Conscious Design\u4e3a\u5229\u7528AI\u4fc3\u8fdb\u4eba\u9645\u8fde\u63a5\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u540c\u610f\u6846\u67b6\u7684\u8bbe\u8ba1\u6307\u5bfc\u95ee\u9898\u3002"}}
{"id": "2508.11150", "pdf": "https://arxiv.org/pdf/2508.11150", "abs": "https://arxiv.org/abs/2508.11150", "authors": ["Stanislav Pozdniakov", "Jonathan Brazil", "Oleksandra Poquet", "Stephan Krusche", "Santiago Berrezueta-Guzman", "Shazia Sadiq", "Hassan Khosravi"], "title": "From Misunderstandings to Learning Opportunities: Leveraging Generative AI in Discussion Forums to Support Student Learning", "categories": ["cs.HC"], "comment": "Artificial Intelligence in Education (AIED 2025)", "summary": "In the contemporary educational landscape, particularly in large classroom\nsettings, discussion forums have become a crucial tool for promoting\ninteraction and addressing student queries. These forums foster a collaborative\nlearning environment where students engage with both the teaching team and\ntheir peers. However, the sheer volume of content generated in these forums\nposes two significant interconnected challenges: How can we effectively\nidentify common misunderstandings that arise in student discussions? And once\nidentified, how can instructors use these insights to address them effectively?\nThis paper explores the approach to integrating large language models (LLMs)\nand Retrieval-Augmented Generation (RAG) to tackle these challenges. We then\ndemonstrate the approach Misunderstanding to Mastery (M2M) with authentic data\nfrom three computer science courses, involving 1355 students with 2878 unique\nposts, followed by an evaluation with five instructors teaching these courses.\nResults show that instructors found the approach promising and valuable for\nteaching, effectively identifying misunderstandings and generating actionable\ninsights. Instructors highlighted the need for more fine-grained groupings,\nclearer metrics, validation of the created resources, and ethical\nconsiderations around data anonymity.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u8bc6\u522b\u5b66\u751f\u8ba8\u8bba\u4e2d\u7684\u5e38\u89c1\u8bef\u89e3\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\uff0c\u6700\u7ec8\u901a\u8fc7\u771f\u5b9e\u8bfe\u7a0b\u6570\u636e\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5927\u8bfe\u5802\u8ba8\u8bba\u8bba\u575b\u4e2d\uff0c\u5b66\u751f\u751f\u6210\u7684\u5185\u5bb9\u91cf\u5927\u4e14\u96be\u4ee5\u6709\u6548\u8bc6\u522b\u5e38\u89c1\u8bef\u89e3\uff0c\u4e9f\u9700\u6280\u672f\u624b\u6bb5\u5e2e\u52a9\u6559\u5e08\u5feb\u901f\u53d1\u73b0\u95ee\u9898\u5e76\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMisunderstanding to Mastery\uff08M2M\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408LLMs\u548cRAG\u6280\u672f\uff0c\u5206\u6790\u5b66\u751f\u8ba8\u8bba\u5185\u5bb9\uff0c\u8bc6\u522b\u8bef\u89e3\u5e76\u751f\u6210\u53ef\u64cd\u4f5c\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u6570\u636e\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u8bef\u89e3\u5e76\u4e3a\u6559\u5e08\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6559\u5b66\u53cd\u9988\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u5206\u7ec4\u7c92\u5ea6\u3001\u6307\u6807\u6e05\u6670\u5ea6\u548c\u6570\u636e\u533f\u540d\u6027\u7b49\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6559\u5e08\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5c24\u5176\u662f\u5728\u5206\u7ec4\u7ec6\u8282\u3001\u6307\u6807\u8bbe\u8ba1\u53ca\u6570\u636e\u4f26\u7406\u65b9\u9762\u3002"}}
{"id": "2508.11304", "pdf": "https://arxiv.org/pdf/2508.11304", "abs": "https://arxiv.org/abs/2508.11304", "authors": ["Andrey Krekhov", "Sebastian Cmentowski", "Katharina Emmerich", "Maic Masuch", "Jens Kr\u00fcger"], "title": "GulliVR: A Walking-Oriented Technique for Navigation in Virtual Reality Games Based on Virtual Body Resizing", "categories": ["cs.HC"], "comment": "author version", "summary": "Virtual reality games are often centered around our feeling of \"being there\".\nThat presence can be significantly enhanced by supporting physical walking.\nAlthough modern virtual reality systems enable room-scale motions, the size of\nour living rooms is not enough to explore vast virtual environments. Developers\nbypass that limitation by adding virtual navigation such as teleportation.\nAlthough such techniques are intended (or designed) to extend but not replace\nnatural walking, what we often observe are nonmoving players beaming to a\nlocation that is one real step ahead. Our navigation metaphor emphasizes\nphysical walking by promoting players into giants on demand to cover large\ndistances. In contrast to flying, our technique proportionally increases the\nmodeled eye distance, preventing cybersickness and creating the feeling of\nbeing in a miniature world. Our evaluations underpin a significantly increased\npresence and walking distance compared to the teleportation approach. Finally,\nwe derive a set of game design implications related to the integration of our\ntechnique.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u901a\u8fc7\u6bd4\u4f8b\u7f29\u653e\u89c6\u8ddd\u4f7f\u73a9\u5bb6\u6210\u4e3a\u2018\u5de8\u4eba\u2019\u7684\u6280\u672f\uff0c\u4ee5\u589e\u5f3a\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u7269\u7406\u884c\u8d70\u4f53\u9a8c\uff0c\u800c\u975e\u4f7f\u7528\u4f20\u7edf\u4f20\u9001\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3VR\u7cfb\u7edf\u7684\u623f\u95f4\u5c3a\u5ea6\u8fd0\u52a8\u53d7\u9650\uff0c\u5f00\u53d1\u8005\u5e38\u4f7f\u7528\u4f20\u9001\u7b49\u865a\u62df\u5bfc\u822a\u6280\u672f\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u81ea\u7136\u884c\u8d70\uff0c\u964d\u4f4e\u4e86\u6c89\u6d78\u611f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u5bfc\u822a\u9690\u55bb\uff0c\u901a\u8fc7\u6309\u6bd4\u4f8b\u589e\u52a0\u89c6\u8ddd\uff0c\u8ba9\u73a9\u5bb6\u611f\u89c9\u8eab\u5904\u5fae\u578b\u4e16\u754c\uff0c\u65e2\u8986\u76d6\u5927\u8ddd\u79bb\u53c8\u907f\u514d\u6655\u52a8\u75c7\u3002", "result": "\u76f8\u6bd4\u4f20\u9001\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6c89\u6d78\u611f\u548c\u884c\u8d70\u8ddd\u79bb\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3aVR\u6e38\u620f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u5f3a\u8c03\u7269\u7406\u884c\u8d70\u7684\u589e\u5f3a\u6548\u679c\u3002"}}
{"id": "2508.11314", "pdf": "https://arxiv.org/pdf/2508.11314", "abs": "https://arxiv.org/abs/2508.11314", "authors": ["Sebastian Cmentowski", "Fabian Kievelitz", "Jens Kr\u00fcger"], "title": "Outpace Reality: A Novel Augmented-Walking Technique for Virtual Reality Games", "categories": ["cs.HC"], "comment": "author version", "summary": "The size of most virtual environments exceeds the tracking space available\nfor physical walking. One solution to this disparity is to extend the available\nwalking range by augmenting users' actual movements. However, the resulting\nincrease in visual flow can easily cause cybersickness. Therefore, we present a\nnovel augmented-walking approach for virtual reality games. Our core concept is\na virtual tunnel that spans the entire travel distance when viewed from the\noutside. However, its interior is only a fraction as long, allowing users to\ncover the distance by real walking. Whereas the tunnel hides the visual flow\nfrom the applied movement acceleration, windows on the tunnel's walls still\nreveal the actual expedited motion. Our evaluation reveals that our approach\navoids cybersickness while enhancing physical activity and preserving presence.\nWe finish our paper with a discussion of the design considerations and\nlimitations of our proposed locomotion technique.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u865a\u62df\u73b0\u5b9e\u6e38\u620f\u884c\u8d70\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u865a\u62df\u96a7\u9053\u9690\u85cf\u89c6\u89c9\u6d41\u52a8\uff0c\u907f\u514d\u6655\u52a8\u75c7\uff0c\u540c\u65f6\u589e\u5f3a\u8eab\u4f53\u6d3b\u52a8\u5e76\u4fdd\u6301\u4e34\u573a\u611f\u3002", "motivation": "\u89e3\u51b3\u865a\u62df\u73af\u5883\u4e2d\u7269\u7406\u884c\u8d70\u7a7a\u95f4\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u56e0\u89c6\u89c9\u6d41\u52a8\u589e\u52a0\u5bfc\u81f4\u7684\u6655\u52a8\u75c7\u3002", "method": "\u4f7f\u7528\u865a\u62df\u96a7\u9053\u8bbe\u8ba1\uff0c\u5916\u90e8\u770b\u8d77\u6765\u8986\u76d6\u6574\u4e2a\u884c\u8d70\u8ddd\u79bb\uff0c\u5185\u90e8\u5b9e\u9645\u957f\u5ea6\u8f83\u77ed\uff0c\u901a\u8fc7\u7a97\u6237\u663e\u793a\u5b9e\u9645\u52a0\u901f\u8fd0\u52a8\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u6210\u529f\u907f\u514d\u6655\u52a8\u75c7\uff0c\u589e\u5f3a\u8eab\u4f53\u6d3b\u52a8\uff0c\u5e76\u4fdd\u6301\u7528\u6237\u4e34\u573a\u611f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fd0\u52a8\u52a0\u901f\u5e26\u6765\u7684\u89c6\u89c9\u6d41\u52a8\u95ee\u9898\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u8003\u8651\u8bbe\u8ba1\u548c\u9650\u5236\u56e0\u7d20\u3002"}}
{"id": "2508.11327", "pdf": "https://arxiv.org/pdf/2508.11327", "abs": "https://arxiv.org/abs/2508.11327", "authors": ["Benjamin J. Carroll", "Jianlong Zhou", "Paul F. Burke", "Sabine Ammon"], "title": "The User-first Approach to AI Ethics: Preferences for Ethical Principles in AI Systems across Cultures and Contexts", "categories": ["cs.HC"], "comment": null, "summary": "As AI systems increasingly permeate everyday life, designers and developers\nface mounting pressure to balance innovation with ethical design choices. To\ndate, the operationalisation of AI ethics has predominantly depended on\nframeworks that prescribe which ethical principles should be embedded within AI\nsystems. However, the extent to which users value these principles remains\nlargely unexplored in the existing literature. In a discrete choice experiment\nconducted in four countries, we quantify user preferences for 11 ethical\nprinciples. Our findings indicate that, while users generally prioritise\nprivacy, justice & fairness, and transparency, their preferences exhibit\nsignificant variation based on culture and application context. Latent class\nanalysis further revealed four distinct user cohorts, the largest of which is\nethically disengaged and defers to regulatory oversight. Our findings offer (1)\nempirical evidence of uneven user prioritisation of AI ethics principles, (2)\nactionable guidance for operationalising ethics tailored to culture and\ncontext, (3) support for the development of robust regulatory mechanisms, and\n(4) a foundation for advancing a user-centred approach to AI ethics, motivated\nindependently from abstract moral theory.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u79bb\u6563\u9009\u62e9\u5b9e\u9a8c\u91cf\u5316\u4e86\u7528\u6237\u5bf911\u9879\u4f26\u7406\u539f\u5219\u7684\u504f\u597d\uff0c\u53d1\u73b0\u6587\u5316\u4e0e\u5e94\u7528\u573a\u666f\u5f71\u54cd\u504f\u597d\uff0c\u63d0\u51fa\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u4f26\u7406\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u7528\u6237\u5bf9AI\u4f26\u7406\u539f\u5219\u7684\u91cd\u89c6\u7a0b\u5ea6\uff0c\u5f25\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5728\u56db\u56fd\u8fdb\u884c\u79bb\u6563\u9009\u62e9\u5b9e\u9a8c\uff0c\u5229\u7528\u6f5c\u5728\u7c7b\u522b\u5206\u6790\u7528\u6237\u6570\u636e\u3002", "result": "\u7528\u6237\u504f\u597d\u9690\u79c1\u3001\u516c\u6b63\u4e0e\u900f\u660e\uff0c\u4f46\u5b58\u5728\u6587\u5316\u5dee\u5f02\uff1b\u53d1\u73b0\u56db\u7c7b\u7528\u6237\u7fa4\u4f53\uff0c\u6700\u5927\u7fa4\u4f53\u4e3a\u4f26\u7406\u758f\u79bb\u578b\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u4f26\u7406\u7684\u5b9e\u8df5\u3001\u76d1\u7ba1\u53ca\u7528\u6237\u4e2d\u5fc3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u4e0e\u6307\u5bfc\u3002"}}
{"id": "2508.11335", "pdf": "https://arxiv.org/pdf/2508.11335", "abs": "https://arxiv.org/abs/2508.11335", "authors": ["Tzu-Hui Wu", "Sebastian Cmentowski", "Yunyin Lou", "Jun Hu", "Regina Bernhaupt"], "title": "Towards Smart Workplaces: Understanding Mood-Influencing Factors of the Physical Workspace in Collaborative Group Settings", "categories": ["cs.HC"], "comment": "preprint, submitted to INTERACT 2025", "summary": "Group mood plays a crucial role in shaping workspace experiences, influencing\ngroup dynamics, team performance, and creativity. The perceived group mood\ndepends on many, often subconscious, aspects such as individual emotional\nstates or group life, which make it challenging to maintain a positive\natmosphere. Intelligent technology could support mood regulation in physical\noffice environments, for example, as adaptive ambient lighting for mood\nregulation. However, little is known about the relationship between the\nphysical workspace and group mood dynamics. To address this knowledge gap, we\nconducted a qualitative user study (N=8 workgroups and overall 26 participants)\nto explore how the physical workspace shapes group mood experiences and\ninvestigate employees' perspectives on intelligent mood-aware technologies. Our\nfindings reveal key factors influencing group mood, and participants'\nexpectations for supportive technology to preserve privacy and autonomy. Our\nwork highlights the potential of adaptive and responsive workspaces while also\nemphasizing the need for human-centered, technology-driven interventions that\nbenefit group well-being.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7269\u7406\u5de5\u4f5c\u7a7a\u95f4\u5bf9\u56e2\u961f\u60c5\u7eea\u7684\u5f71\u54cd\uff0c\u5e76\u7814\u7a76\u4e86\u667a\u80fd\u60c5\u7eea\u611f\u77e5\u6280\u672f\u7684\u6f5c\u529b\u3002", "motivation": "\u56e2\u961f\u60c5\u7eea\u5bf9\u5de5\u4f5c\u73af\u5883\u4f53\u9a8c\u548c\u56e2\u961f\u8868\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5bf9\u5176\u4e0e\u7269\u7406\u5de5\u4f5c\u7a7a\u95f4\u7684\u5173\u7cfb\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u7528\u6237\u7814\u7a76\uff088\u4e2a\u5de5\u4f5c\u7ec4\uff0c\u517126\u540d\u53c2\u4e0e\u8005\uff09\uff0c\u63a2\u7d22\u7269\u7406\u5de5\u4f5c\u7a7a\u95f4\u5982\u4f55\u5f71\u54cd\u56e2\u961f\u60c5\u7eea\u53ca\u5458\u5de5\u5bf9\u667a\u80fd\u6280\u672f\u7684\u770b\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f71\u54cd\u56e2\u961f\u60c5\u7eea\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4ee5\u53ca\u5458\u5de5\u5bf9\u652f\u6301\u6027\u6280\u672f\u7684\u9690\u79c1\u548c\u81ea\u4e3b\u6027\u671f\u671b\u3002", "conclusion": "\u81ea\u9002\u5e94\u5de5\u4f5c\u7a7a\u95f4\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u6765\u63d0\u5347\u56e2\u961f\u798f\u7949\u3002"}}
{"id": "2508.11398", "pdf": "https://arxiv.org/pdf/2508.11398", "abs": "https://arxiv.org/abs/2508.11398", "authors": ["Mithat Can Ozgun", "Jiahuan Pei", "Koen Hindriks", "Lucia Donatelli", "Qingzhi Liu", "Xin Sun", "Junxiao Wang"], "title": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis", "categories": ["cs.HC", "cs.AI", "cs.IR"], "comment": "Accepted by CIKM 2025 as a full paper", "summary": "LLM-based agents have emerged as transformative tools capable of executing\ncomplex tasks through iterative planning and action, achieving significant\nadvancements in understanding and addressing user needs. Yet, their\neffectiveness remains limited in specialized domains such as mental health\ndiagnosis, where they underperform compared to general applications. Current\napproaches to integrating diagnostic capabilities into LLMs rely on scarce,\nhighly sensitive mental health datasets, which are challenging to acquire.\nThese methods also fail to emulate clinicians' proactive inquiry skills, lack\nmulti-turn conversational comprehension, and struggle to align outputs with\nexpert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the\nfirst LLM-based agent workflow designed to autonomously generate DSM-5 Level-1\ndiagnostic questionnaires. By simulating therapist-client dialogues with\nspecific client profiles, the framework delivers transparent, step-by-step\ndisorder predictions, producing explainable and trustworthy results. This\nworkflow serves as a complementary tool for mental health diagnosis, ensuring\nadherence to ethical and legal standards. Through comprehensive experiments, we\nevaluate leading LLMs across three critical dimensions: conversational realism,\ndiagnostic accuracy, and explainability. Our datasets and implementations are\nfully open-sourced.", "AI": {"tldr": "\u63d0\u51faDSM5AgentFlow\uff0c\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210DSM-5 Level-1\u8bca\u65ad\u95ee\u5377\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5fc3\u7406\u5065\u5eb7\u8bca\u65ad\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709LLM\u5728\u5fc3\u7406\u5065\u5eb7\u8bca\u65ad\u7b49\u4e13\u4e1a\u9886\u57df\u6548\u679c\u4e0d\u4f73\uff0c\u6570\u636e\u7a00\u7f3a\u4e14\u96be\u4ee5\u6a21\u62df\u4e34\u5e8a\u533b\u751f\u7684\u4e3b\u52a8\u63d0\u95ee\u80fd\u529b\uff0c\u7f3a\u4e4f\u591a\u8f6e\u5bf9\u8bdd\u7406\u89e3\u548c\u4e13\u5bb6\u4e34\u5e8a\u63a8\u7406\u5bf9\u9f50\u3002", "method": "\u63d0\u51faDSM5AgentFlow\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u6cbb\u7597\u5e08-\u5ba2\u6237\u5bf9\u8bdd\uff0c\u751f\u6210\u900f\u660e\u3001\u9010\u6b65\u7684\u969c\u788d\u9884\u6d4b\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4e3b\u6d41LLM\u5728\u5bf9\u8bdd\u771f\u5b9e\u6027\u3001\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e09\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u6570\u636e\u96c6\u548c\u5b9e\u73b0\u5df2\u5f00\u6e90\u3002", "conclusion": "DSM5AgentFlow\u4e3a\u5fc3\u7406\u5065\u5eb7\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e92\u8865\u5de5\u5177\uff0c\u7b26\u5408\u4f26\u7406\u548c\u6cd5\u5f8b\u6807\u51c6\u3002"}}
{"id": "2508.11401", "pdf": "https://arxiv.org/pdf/2508.11401", "abs": "https://arxiv.org/abs/2508.11401", "authors": ["Jana Gonnermann-M\u00fcller", "Jennifer Haase", "Konstantin Fackeldey", "Sebastian Pokutta"], "title": "FACET:Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized Educational Worksheets", "categories": ["cs.HC", "cs.MA"], "comment": null, "summary": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials.", "AI": {"tldr": "FACET\u6846\u67b6\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u6559\u5e08\u8f85\u52a9\u7cfb\u7edf\uff0c\u7ed3\u5408\u8ba4\u77e5\u548c\u52a8\u673a\u7ef4\u5ea6\u751f\u6210\u4e2a\u6027\u5316\u6559\u5b66\u6750\u6599\uff0c\u901a\u8fc7\u6d4b\u8bd5\u548c\u6559\u5e08\u53cd\u9988\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u6f5c\u529b\u3002", "motivation": "\u9488\u5bf9\u5b66\u751f\u7fa4\u4f53\u7684\u591a\u6837\u6027\uff0c\u5c24\u5176\u662f\u6570\u5b66\u6559\u80b2\u4e2d\u7684\u8ba4\u77e5\u3001\u52a8\u673a\u548c\u60c5\u611f\u5dee\u5f02\uff0c\u5f53\u524dAI\u5de5\u5177\u591a\u4e3a\u6210\u7ee9\u5bfc\u5411\uff0c\u7f3a\u4e4f\u5bf9\u6559\u5e08\u548c\u6559\u5b66\u9700\u6c42\u7684\u5168\u9762\u652f\u6301\u3002", "method": "\u5f00\u53d1\u4e86FACET\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u667a\u80fd\u4f53\uff1a\u6a21\u62df\u5b66\u4e60\u8005\u3001\u8c03\u6574\u6559\u5b66\u5185\u5bb9\u3001\u81ea\u52a8\u8bc4\u4f30\u3002\u4f7f\u7528\u516b\u5e74\u7ea7\u6570\u5b66\u8bfe\u7a0b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u8bc4\u4f30\u548c\u6559\u5e08\u53cd\u9988\u9a8c\u8bc1\u3002", "result": "\u7cfb\u7edf\u751f\u6210\u7684\u6750\u6599\u4e0e\u5b66\u4e60\u8005\u6863\u6848\u9ad8\u5ea6\u5339\u914d\uff0c\u6559\u5e08\u53cd\u9988\u5f3a\u8c03\u4e86\u4efb\u52a1\u7684\u7ed3\u6784\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\u5728\u5f02\u6784\u8bfe\u5802\u4e2d\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u66f4\u4e30\u5bcc\u7684\u5b66\u4e60\u8005\u6863\u6848\u548c\u5b9e\u9645\u8bfe\u5802\u8bd5\u9a8c\u3002"}}
{"id": "2508.11412", "pdf": "https://arxiv.org/pdf/2508.11412", "abs": "https://arxiv.org/abs/2508.11412", "authors": ["Jens Grubert", "Yvonne Sedelmaier", "Dieter Landes"], "title": "Towards Embodied Conversational Agents for Reducing Oral Exam Anxiety in Extended Reality", "categories": ["cs.HC"], "comment": "Accepted to the IEEE ISMAR-Adjunct Proceedings 2025", "summary": "Oral examinations are a prevalent but psychologically demanding form of\nassessment in higher education. Many students experience intense anxiety, which\ncan impair cognitive performance and hinder academic success. This position\npaper explores the potential of embodied conversational agents (ECAs) in\nextended reality (XR) environments to support students preparing for oral\nexams. We propose a system concept that integrates photorealistic ECAs with\nreal-time capable large language models (LLMs) to enable psychologically safe,\nadaptive, and repeatable rehearsal of oral examination scenarios. We also\ndiscuss the potential benefits and challenges of such an envisioned system.", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u5728\u6269\u5c55\u73b0\u5b9e(XR)\u73af\u5883\u4e2d\u4f7f\u7528\u5177\u8eab\u5bf9\u8bdd\u4ee3\u7406(ECAs)\u652f\u6301\u5b66\u751f\u51c6\u5907\u53e3\u8bed\u8003\u8bd5\u7684\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u903c\u771fECAs\u548c\u5b9e\u65f6\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u7cfb\u7edf\u6982\u5ff5\u3002", "motivation": "\u53e3\u8bed\u8003\u8bd5\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u666e\u904d\u5b58\u5728\u4f46\u5fc3\u7406\u538b\u529b\u5927\uff0c\u5b66\u751f\u7126\u8651\u4f1a\u635f\u5bb3\u8ba4\u77e5\u8868\u73b0\u548c\u5b66\u4e1a\u6210\u529f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5b89\u5168\u3001\u53ef\u91cd\u590d\u7684\u7ec3\u4e60\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7cfb\u7edf\u6982\u5ff5\uff0c\u6574\u5408\u903c\u771f\u5177\u8eab\u5bf9\u8bdd\u4ee3\u7406\u548c\u5b9e\u65f6\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u521b\u9020\u5fc3\u7406\u5b89\u5168\u3001\u81ea\u9002\u5e94\u4e14\u53ef\u91cd\u590d\u7684\u53e3\u8bed\u8003\u8bd5\u6a21\u62df\u73af\u5883\u3002", "result": "\u8be5\u7cfb\u7edf\u6709\u671b\u4e3a\u5b66\u751f\u63d0\u4f9b\u5b89\u5168\u3001\u9ad8\u6548\u7684\u8003\u8bd5\u51c6\u5907\u5de5\u5177\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6280\u672f\u548c\u5fc3\u7406\u63a5\u53d7\u5ea6\u7b49\u6311\u6218\u3002", "conclusion": "\u6269\u5c55\u73b0\u5b9e\u4e2d\u7684\u5177\u8eab\u5bf9\u8bdd\u4ee3\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u5177\u5907\u6f5c\u529b\u6539\u5584\u53e3\u8bed\u8003\u8bd5\u51c6\u5907\u8fc7\u7a0b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5b9e\u8df5\u9a8c\u8bc1\u3002"}}
{"id": "2508.11426", "pdf": "https://arxiv.org/pdf/2508.11426", "abs": "https://arxiv.org/abs/2508.11426", "authors": ["Steffen Hauck", "Diar Abdlkarim", "John Dudley", "Per Ola Kristensson", "Eyal Ofek", "Jens Grubert"], "title": "ReachVox: Clutter-free Reachability Visualization for Robot Motion Planning in Virtual Reality", "categories": ["cs.HC", "cs.RO"], "comment": "To appear in Proceedings of IEEE ISMAR 2025", "summary": "Human-Robot-Collaboration can enhance workflows by leveraging the mutual\nstrengths of human operators and robots. Planning and understanding robot\nmovements remain major challenges in this domain. This problem is prevalent in\ndynamic environments that might need constant robot motion path adaptation. In\nthis paper, we investigate whether a minimalistic encoding of the reachability\nof a point near an object of interest, which we call ReachVox, can aid the\ncollaboration between a remote operator and a robotic arm in VR. Through a user\nstudy (n=20), we indicate the strength of the visualization relative to a\npoint-based reachability check-up.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u53ef\u8fbe\u6027\u7f16\u7801\u65b9\u6cd5\uff08ReachVox\uff09\u5728VR\u4e2d\u8f85\u52a9\u8fdc\u7a0b\u64cd\u4f5c\u8005\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u7684\u6548\u679c\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u8fd0\u52a8\u8def\u5f84\u7684\u89c4\u5212\u548c\u7406\u89e3\u662f\u4e3b\u8981\u6311\u6218\uff0c\u9700\u8981\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff08n=20\uff09\u6bd4\u8f83ReachVox\u53ef\u89c6\u5316\u65b9\u6cd5\u4e0e\u70b9\u57fa\u53ef\u8fbe\u6027\u68c0\u67e5\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u663e\u793a\u4e86ReachVox\u53ef\u89c6\u5316\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "ReachVox\u5728VR\u4eba\u673a\u534f\u4f5c\u4e2d\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u8f85\u52a9\u64cd\u4f5c\u8005\u3002"}}
{"id": "2508.11544", "pdf": "https://arxiv.org/pdf/2508.11544", "abs": "https://arxiv.org/abs/2508.11544", "authors": ["Viktor von Wyl", "J\u00fcrgen Bernard"], "title": "Grand Challenge: Mediating Between Confirmatory and Exploratory Research Cultures in Health Sciences and Visual Analytics", "categories": ["cs.HC"], "comment": "2+1 pages, 1 figure, position paper for the Visual Analytics in\n  Healthcare Workshop at IEEE VIS, Vienna, 2025", "summary": "Collaboration between health science and visual analytics research is often\nhindered by different, sometimes incompatible approaches to research design.\nHealth science often follows hypothesis-driven protocols, registered in\nadvance, and focuses on reproducibility and risk mitigation. Visual analytics,\nin contrast, relies on iterative data exploration, prioritizing insight\ngeneration and analytic refinement through user interaction. These differences\ncreate challenges in interdisciplinary projects, including misaligned\nterminology, unrealistic expectations about data readiness, divergent\nvalidation norms, or conflicting explainability requirements. To address these\npersistent tensions, we identify seven research needs and actions: (1)\nguidelines for broader community adoption, (2) agreement on quality and\nvalidation benchmarks, (3) frameworks for aligning research tasks, (4)\nintegrated workflows combining confirmatory and exploratory stages, (5) tools\nfor harmonizing terminology across disciplines, (6) dedicated bridging roles\nfor transdisciplinary work, and (7) cultural adaptation and mutual recognition.\nWe organize these needs in a framework with three areas: culture, standards,\nand processes. They can constitute a research agenda for developing reliable,\nreproducible, and clinically relevant data-centric methods.", "AI": {"tldr": "\u5065\u5eb7\u79d1\u5b66\u4e0e\u89c6\u89c9\u5206\u6790\u7814\u7a76\u5408\u4f5c\u5e38\u56e0\u7814\u7a76\u8bbe\u8ba1\u65b9\u6cd5\u4e0d\u540c\u800c\u53d7\u963b\uff0c\u9700\u4ece\u6587\u5316\u3001\u6807\u51c6\u3001\u6d41\u7a0b\u4e09\u65b9\u9762\u63d0\u51fa\u4e03\u9879\u884c\u52a8\u4ee5\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5408\u4f5c\u3002", "motivation": "\u89e3\u51b3\u5065\u5eb7\u79d1\u5b66\u4e0e\u89c6\u89c9\u5206\u6790\u7814\u7a76\u56e0\u65b9\u6cd5\u5dee\u5f02\u5bfc\u81f4\u7684\u5408\u4f5c\u969c\u788d\uff0c\u5982\u672f\u8bed\u4e0d\u7edf\u4e00\u3001\u9a8c\u8bc1\u6807\u51c6\u4e0d\u540c\u7b49\u3002", "method": "\u63d0\u51fa\u4e03\u9879\u7814\u7a76\u9700\u6c42\u548c\u884c\u52a8\uff0c\u5305\u62ec\u6307\u5357\u5236\u5b9a\u3001\u6807\u51c6\u7edf\u4e00\u3001\u5de5\u4f5c\u6d41\u7a0b\u6574\u5408\u7b49\uff0c\u5e76\u6784\u5efa\u6587\u5316\u3001\u6807\u51c6\u3001\u6d41\u7a0b\u4e09\u65b9\u9762\u7684\u6846\u67b6\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u53ef\u9760\u3001\u53ef\u91cd\u590d\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u6587\u5316\u548c\u6d41\u7a0b\u8c03\u6574\u3001\u6807\u51c6\u7edf\u4e00\uff0c\u53ef\u6709\u6548\u89e3\u51b3\u8de8\u5b66\u79d1\u5408\u4f5c\u4e2d\u7684\u6311\u6218\uff0c\u63a8\u52a8\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2508.11613", "pdf": "https://arxiv.org/pdf/2508.11613", "abs": "https://arxiv.org/abs/2508.11613", "authors": ["Justin Phillips", "Daniel Roggen", "Cathy Speed", "Robert Harle"], "title": "Adaptive Cardio Load Targets for Improving Fitness and Performance", "categories": ["cs.HC"], "comment": null, "summary": "Cardio Load, introduced by Google in 2024, is a measure of cardiovascular\nwork (also known as training load) resulting from all the user's activities\nacross the day. It is based on heart rate reserve and captures both activity\nintensity and duration. Thanks to feedback from users and internal research, we\nintroduce adaptive and personalized targets which will be set weekly. This\nfeature will be available in the Public Preview of the Fitbit app after\nSeptember 2025. This white paper provides a comprehensive overview of Cardio\nLoad (CL) and how weekly CL targets are established, with examples shown to\nillustrate the effect of varying CL on the weekly target. We compare Cardio\nLoad and Active Zone Minutes (AZMs), highlighting their distinct purposes, i.e.\nAZMs for health guidelines and CL for performance measurement. We highlight\nthat CL is accumulated both during active workouts and incidental daily\nactivities, so users are able top-up their CL score with small bouts of\nactivity across the day.", "AI": {"tldr": "Google 2024\u5e74\u63a8\u51fa\u7684Cardio Load\uff08CL\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u5fc3\u7387\u50a8\u5907\u7684\u5fc3\u8840\u7ba1\u5de5\u4f5c\u91cf\u6d4b\u91cf\u65b9\u6cd5\uff0c\u7ed3\u5408\u6d3b\u52a8\u5f3a\u5ea6\u4e0e\u65f6\u957f\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u6bcf\u5468\u76ee\u6807\u3002", "motivation": "\u901a\u8fc7\u7528\u6237\u53cd\u9988\u548c\u5185\u90e8\u7814\u7a76\uff0c\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u66f4\u5168\u9762\u8bc4\u4f30\u7528\u6237\u65e5\u5e38\u6d3b\u52a8\u5bf9\u5fc3\u8840\u7ba1\u7cfb\u7edf\u5f71\u54cd\u7684\u6d4b\u91cf\u5de5\u5177\u3002", "method": "CL\u57fa\u4e8e\u5fc3\u7387\u50a8\u5907\uff0c\u8ba1\u7b97\u6d3b\u52a8\u5f3a\u5ea6\u4e0e\u65f6\u957f\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u548c\u4e2a\u6027\u5316\u76ee\u6807\u3002", "result": "CL\u5c06\u4e8e2025\u5e749\u6708\u5728Fitbit\u5e94\u7528\u4e2d\u63a8\u51fa\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540cCL\u5bf9\u6bcf\u5468\u76ee\u6807\u7684\u5f71\u54cd\u793a\u4f8b\u3002", "conclusion": "CL\u4e0eActive Zone Minutes\uff08AZMs\uff09\u76ee\u7684\u4e0d\u540c\uff0c\u524d\u8005\u7528\u4e8e\u6027\u80fd\u6d4b\u91cf\uff0c\u540e\u8005\u7528\u4e8e\u5065\u5eb7\u6307\u5357\uff0cCL\u8fd8\u80fd\u901a\u8fc7\u65e5\u5e38\u5c0f\u6d3b\u52a8\u7d2f\u79ef\u3002"}}
{"id": "2508.11620", "pdf": "https://arxiv.org/pdf/2508.11620", "abs": "https://arxiv.org/abs/2508.11620", "authors": ["Chi-Jung Lee", "Jiaxin Li", "Tianhong Catherine Yu", "Ruidong Zhang", "Vipin Gunda", "Fran\u00e7ois Guimbreti\u00e8re", "Cheng Zhang"], "title": "Grab-n-Go: On-the-Go Microgesture Recognition with Objects in Hand", "categories": ["cs.HC"], "comment": null, "summary": "As computing devices become increasingly integrated into daily life, there is\na growing need for intuitive, always-available interaction methods, even when\nusers' hands are occupied. In this paper, we introduce Grab-n-Go, the first\nwearable device that leverages active acoustic sensing to recognize subtle hand\nmicrogestures while holding various objects. Unlike prior systems that focus\nsolely on free-hand gestures or basic hand-object activity recognition,\nGrab-n-Go simultaneously captures information about hand microgestures,\ngrasping poses, and object geometries using a single wristband, enabling the\nrecognition of fine-grained hand movements occurring within activities\ninvolving occupied hands. A deep learning framework processes these complex\nsignals to identify 30 distinct microgestures, with 6 microgestures for each of\nthe 5 grasping poses. In a user study with 10 participants and 25 everyday\nobjects, Grab-n-Go achieved an average recognition accuracy of 92.0%. A\nfollow-up study further validated Grab-n-Go's robustness against 10 more\nchallenging, deformable objects. These results underscore the potential of\nGrab-n-Go to provide seamless, unobtrusive interactions without requiring\nmodifications to existing objects. The complete dataset, comprising data from\n18 participants performing 30 microgestures with 35 distinct objects, is\npublicly available at https://github.com/cjlisalee/Grab-n-Go_Data with the DOI:\nhttps://doi.org/10.7298/7kbd-vv75.", "AI": {"tldr": "Grab-n-Go\u662f\u4e00\u79cd\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u5229\u7528\u4e3b\u52a8\u58f0\u5b66\u4f20\u611f\u6280\u672f\u8bc6\u522b\u624b\u6301\u7269\u4f53\u65f6\u7684\u7ec6\u5fae\u624b\u90e8\u5fae\u52a8\u4f5c\uff0c\u652f\u6301\u591a\u79cd\u624b\u52bf\u548c\u7269\u4f53\u5f62\u72b6\u7684\u8bc6\u522b\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u8bbe\u5907\u65e5\u76ca\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u7528\u6237\u9700\u8981\u5728\u624b\u90e8\u88ab\u5360\u7528\u65f6\u4e5f\u80fd\u8fdb\u884c\u76f4\u89c2\u3001\u968f\u65f6\u53ef\u7528\u7684\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u5355\u4e2a\u8155\u5e26\u6355\u6349\u624b\u90e8\u5fae\u52a8\u4f5c\u3001\u6293\u53d6\u59ff\u52bf\u548c\u7269\u4f53\u5f62\u72b6\u4fe1\u606f\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8bc6\u522b30\u79cd\u5fae\u52a8\u4f5c\u3002", "result": "\u572810\u540d\u53c2\u4e0e\u8005\u4f7f\u752825\u79cd\u65e5\u5e38\u7269\u4f53\u7684\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u8bc6\u522b\u51c6\u786e\u7387\u8fbe92.0%\u3002", "conclusion": "Grab-n-Go\u5c55\u793a\u4e86\u5728\u4e0d\u9700\u4fee\u6539\u73b0\u6709\u7269\u4f53\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u65e0\u7f1d\u4ea4\u4e92\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.10918", "pdf": "https://arxiv.org/pdf/2508.10918", "abs": "https://arxiv.org/abs/2508.10918", "authors": ["Samantha Aziz", "Oleg Komogortsev"], "title": "Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder", "categories": ["cs.CV", "cs.HC"], "comment": "IJCB 2025; 11 pages, 7 figures", "summary": "We present a privacy-enhancing mechanism for gaze signals using a\nlatent-noise autoencoder that prevents users from being re-identified across\nplay sessions without their consent, while retaining the usability of the data\nfor benign tasks. We evaluate privacy-utility trade-offs across biometric\nidentification and gaze prediction tasks, showing that our approach\nsignificantly reduces biometric identifiability with minimal utility\ndegradation. Unlike prior methods in this direction, our framework retains\nphysiologically plausible gaze patterns suitable for downstream use, which\nproduces favorable privacy-utility trade-off. This work advances privacy in\ngaze-based systems by providing a usable and effective mechanism for protecting\nsensitive gaze data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u566a\u58f0\u81ea\u7f16\u7801\u5668\u7684\u9690\u79c1\u589e\u5f3a\u673a\u5236\uff0c\u4fdd\u62a4\u7528\u6237\u8eab\u4efd\u4e0d\u88ab\u8bc6\u522b\uff0c\u540c\u65f6\u4fdd\u7559\u6570\u636e\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u6ce8\u89c6\u4fe1\u53f7\u4e2d\u7684\u9690\u79c1\u95ee\u9898\uff0c\u9632\u6b62\u7528\u6237\u8eab\u4efd\u88ab\u672a\u7ecf\u540c\u610f\u5730\u91cd\u65b0\u8bc6\u522b\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u566a\u58f0\u81ea\u7f16\u7801\u5668\uff0c\u5e73\u8861\u9690\u79c1\u4e0e\u5b9e\u7528\u6027\u3002", "result": "\u663e\u8457\u964d\u4f4e\u751f\u7269\u8bc6\u522b\u7684\u53ef\u80fd\u6027\uff0c\u540c\u65f6\u5b9e\u7528\u6027\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "\u8be5\u673a\u5236\u6709\u6548\u4fdd\u62a4\u654f\u611f\u6ce8\u89c6\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6570\u636e\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2508.10972", "pdf": "https://arxiv.org/pdf/2508.10972", "abs": "https://arxiv.org/abs/2508.10972", "authors": ["Rosiana Natalie", "Wenqian Xu", "Ruei-Che Chang", "Rada Mihalcea", "Anhong Guo"], "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Advances in vision language models (VLMs) have enabled the simulation of\ngeneral human behavior through their reasoning and problem solving\ncapabilities. However, prior research has not investigated such simulation\ncapabilities in the accessibility domain. In this paper, we evaluate the extent\nto which VLMs can simulate the vision perception of low vision individuals when\ninterpreting images. We first compile a benchmark dataset through a survey\nstudy with 40 low vision participants, collecting their brief and detailed\nvision information and both open-ended and multiple-choice image perception and\nrecognition responses to up to 25 images. Using these responses, we construct\nprompts for VLMs (GPT-4o) to create simulated agents of each participant,\nvarying the included information on vision information and example image\nresponses. We evaluate the agreement between VLM-generated responses and\nparticipants' original answers. Our results indicate that VLMs tend to infer\nbeyond the specified vision ability when given minimal prompts, resulting in\nlow agreement (0.59). The agreement between the agent' and participants'\nresponses remains low when only either the vision information (0.59) or example\nimage responses (0.59) are provided, whereas a combination of both\nsignificantly increase the agreement (0.70, p < 0.0001). Notably, a single\nexample combining both open-ended and multiple-choice responses, offers\nsignificant performance improvements over either alone (p < 0.0001), while\nadditional examples provided minimal benefits (p > 0.05).", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u6a21\u62df\u4f4e\u89c6\u529b\u4eba\u7fa4\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u7ed3\u5408\u89c6\u89c9\u4fe1\u606f\u548c\u793a\u4f8b\u56fe\u50cf\u54cd\u5e94\u7684\u63d0\u793a\u80fd\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u4e0e\u4eba\u7c7b\u56de\u7b54\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u969c\u788d\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u6a21\u62df\u4f4e\u89c6\u529b\u4eba\u7fa4\u7684\u56fe\u50cf\u611f\u77e5\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u6536\u96c640\u540d\u4f4e\u89c6\u529b\u53c2\u4e0e\u8005\u7684\u6570\u636e\uff0c\u6784\u5efa\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e0d\u540c\u4fe1\u606f\u7684\u63d0\u793a\uff0c\u6d4b\u8bd5VLMs\u751f\u6210\u56de\u7b54\u4e0e\u53c2\u4e0e\u8005\u539f\u59cb\u7b54\u6848\u7684\u4e00\u81f4\u6027\u3002", "result": "VLMs\u5728\u4ec5\u63d0\u4f9b\u5c11\u91cf\u4fe1\u606f\u65f6\u63a8\u65ad\u80fd\u529b\u6709\u9650\uff08\u4e00\u81f4\u60270.59\uff09\uff0c\u7ed3\u5408\u89c6\u89c9\u4fe1\u606f\u548c\u793a\u4f8b\u56fe\u50cf\u54cd\u5e94\u540e\u4e00\u81f4\u6027\u663e\u8457\u63d0\u9ad8\uff080.70\uff09\u3002", "conclusion": "\u7efc\u5408\u89c6\u89c9\u4fe1\u606f\u548c\u793a\u4f8b\u56fe\u50cf\u54cd\u5e94\u7684\u63d0\u793a\u5bf9\u6a21\u62df\u4f4e\u89c6\u529b\u4eba\u7fa4\u7684\u89c6\u89c9\u611f\u77e5\u975e\u5e38\u6709\u6548\uff0c\u5355\u4e00\u6837\u672c\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.11093", "pdf": "https://arxiv.org/pdf/2508.11093", "abs": "https://arxiv.org/abs/2508.11093", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "Accepted at Human-Centered Robot Autonomy for Human-Robot Teams\n  (HuRoboT) at IEEE RO-MAN 2025, Eindhoven, the Netherlands", "summary": "Human-robot collaboration requires robots to quickly infer user intent,\nprovide transparent reasoning, and assist users in achieving their goals. Our\nrecent work introduced GUIDER, our framework for inferring navigation and\nmanipulation intents. We propose augmenting GUIDER with a vision-language model\n(VLM) and a text-only language model (LLM) to form a semantic prior that\nfilters objects and locations based on the mission prompt. A vision pipeline\n(YOLO for object detection and the Segment Anything Model for instance\nsegmentation) feeds candidate object crops into the VLM, which scores their\nrelevance given an operator prompt; in addition, the list of detected object\nlabels is ranked by a text-only LLM. These scores weight the existing\nnavigation and manipulation layers of GUIDER, selecting context-relevant\ntargets while suppressing unrelated objects. Once the combined belief exceeds a\nthreshold, autonomy changes occur, enabling the robot to navigate to the\ndesired area and retrieve the desired object, while adapting to any changes in\nthe operator's intent. Future work will evaluate the system on Isaac Sim using\na Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.", "AI": {"tldr": "GUIDER\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u7eaf\u6587\u672c\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3a\u610f\u56fe\u63a8\u65ad\u80fd\u529b\uff0c\u4f18\u5316\u673a\u5668\u4eba\u5bfc\u822a\u4e0e\u64cd\u4f5c\u3002", "motivation": "\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4e2d\u673a\u5668\u4eba\u63a8\u65ad\u7528\u6237\u610f\u56fe\u7684\u6548\u7387\u548c\u900f\u660e\u6027\u3002", "method": "\u7ed3\u5408VLM\u548cLLM\u5f62\u6210\u8bed\u4e49\u5148\u9a8c\uff0c\u4f7f\u7528YOLO\u548cSegment Anything\u6a21\u578b\u68c0\u6d4b\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7VLM\u548cLLM\u8bc4\u5206\u4f18\u5316\u76ee\u6807\u9009\u62e9\u3002", "result": "\u589e\u5f3a\u7684GUIDER\u80fd\u66f4\u7cbe\u51c6\u5730\u8bc6\u522b\u4e0a\u4e0b\u6587\u76f8\u5173\u76ee\u6807\u5e76\u9002\u5e94\u610f\u56fe\u53d8\u5316\u3002", "conclusion": "\u672a\u6765\u5c06\u5728Isaac Sim\u4e2d\u8bc4\u4f30\u7cfb\u7edf\u5b9e\u65f6\u8f85\u52a9\u6027\u80fd\u3002"}}
{"id": "2508.11115", "pdf": "https://arxiv.org/pdf/2508.11115", "abs": "https://arxiv.org/abs/2508.11115", "authors": ["Haotang Li", "Zhenyu Qi", "Sen He", "Kebin Peng", "Sheng Tan", "Yili Ren", "Tomas Cerny", "Jiyue Zhao", "Zi Wang"], "title": "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring", "categories": ["cs.CV", "cs.HC", "eess.SP"], "comment": null, "summary": "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts.", "AI": {"tldr": "UWB-PostureGuard\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u5bbd\u5e26\u6280\u672f\u7684\u5750\u59ff\u76d1\u6d4b\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9690\u79c1\u548c\u8212\u9002\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u7279\u5f81\u63d0\u53d6\u548c\u6a21\u578b\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u76d1\u6d4b\u3002", "motivation": "\u957f\u65f6\u95f4\u4f7f\u7528\u7535\u8111\u65f6\u7684\u4e0d\u5f53\u5750\u59ff\u5df2\u6210\u4e3a\u516c\u5171\u5065\u5eb7\u95ee\u9898\uff0c\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u548c\u8212\u9002\u6027\u969c\u788d\u3002", "method": "\u5229\u7528\u5546\u4e1a\u8d85\u5bbd\u5e26\u8bbe\u5907\uff0c\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u63d0\u53d6\u5750\u59ff\u7279\u5f81\uff0c\u5e76\u5f00\u53d1PoseGBDT\u6a21\u578b\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u572810\u540d\u53c2\u4e0e\u8005\u548c19\u79cd\u4e0d\u540c\u5750\u59ff\u7684\u5b9e\u6d4b\u4e2d\u8fbe\u523099.11%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u5bf9\u73af\u5883\u53d8\u91cf\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u9884\u9632\u6027\u5065\u5eb7\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u79fb\u52a8\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11360", "pdf": "https://arxiv.org/pdf/2508.11360", "abs": "https://arxiv.org/abs/2508.11360", "authors": ["Songqin Nong", "Jingxuan Xu", "Sheng Zhou", "Jianfeng Chen", "Xiaoxuan Tang", "Tao Jiang", "Wenhao Xu"], "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks.", "AI": {"tldr": "\u63d0\u51faCRAFT-GUI\u6846\u67b6\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548cGRPO\uff0c\u9488\u5bf9GUI\u4efb\u52a1\u7684\u96be\u5ea6\u5dee\u5f02\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u5ffd\u89c6GUI\u4efb\u52a1\u96be\u5ea6\u5dee\u5f02\u548c\u5956\u52b1\u4fe1\u53f7\u7684\u7c97\u7cd9\u6027\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6CRAFT-GUI\u53caGRPO\uff0c\u8bbe\u8ba1\u7ed3\u5408\u89c4\u5219\u548c\u6a21\u578b\u8bc4\u4f30\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728Android Control\u548c\u5185\u90e8\u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u53475.6%\u548c10.3%\u3002", "conclusion": "\u9a8c\u8bc1\u4e86RL\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u5728GUI\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11404", "pdf": "https://arxiv.org/pdf/2508.11404", "abs": "https://arxiv.org/abs/2508.11404", "authors": ["Junyeon Kim", "Tianshu Ruan", "Cesar Alan Contreras", "Manolis Chiou"], "title": "An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Structural inspection in nuclear facilities is vital for maintaining\noperational safety and integrity. Traditional methods of manual inspection pose\nsignificant challenges, including safety risks, high cognitive demands, and\npotential inaccuracies due to human limitations. Recent advancements in\nArtificial Intelligence (AI) and robotic technologies have opened new\npossibilities for safer, more efficient, and accurate inspection methodologies.\nSpecifically, Human-Robot Collaboration (HRC), leveraging robotic platforms\nequipped with advanced detection algorithms, promises significant improvements\nin inspection outcomes and reductions in human workload. This study explores\nthe effectiveness of AI-assisted visual crack detection integrated into a\nmobile Jackal robot platform. The experiment results indicate that HRC enhances\ninspection accuracy and reduces operator workload, resulting in potential\nsuperior performance outcomes compared to traditional manual methods.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u8f85\u52a9\u89c6\u89c9\u88c2\u7f1d\u68c0\u6d4b\u5728\u79fb\u52a8Jackal\u673a\u5668\u4eba\u5e73\u53f0\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u4eba\u673a\u534f\u4f5c\uff08HRC\uff09\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u4e86\u64cd\u4f5c\u5458\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edf\u6838\u8bbe\u65bd\u7ed3\u6784\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3001\u8ba4\u77e5\u8d1f\u62c5\u9ad8\u53ca\u4eba\u4e3a\u8bef\u5dee\u7b49\u95ee\u9898\uff0cAI\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u8fdb\u6b65\u4e3a\u66f4\u5b89\u5168\u9ad8\u6548\u7684\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u914d\u5907\u5148\u8fdb\u68c0\u6d4b\u7b97\u6cd5\u7684\u79fb\u52a8Jackal\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u7ed3\u5408AI\u8f85\u52a9\u89c6\u89c9\u88c2\u7f1d\u68c0\u6d4b\u6280\u672f\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHRC\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u64cd\u4f5c\u5458\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002", "conclusion": "\u4eba\u673a\u534f\u4f5c\u5728\u6838\u8bbe\u65bd\u7ed3\u6784\u68c0\u6d4b\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.11452", "pdf": "https://arxiv.org/pdf/2508.11452", "abs": "https://arxiv.org/abs/2508.11452", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faInclusion Arena\u5e73\u53f0\uff0c\u901a\u8fc7\u5b9e\u65f6\u7528\u6237\u53cd\u9988\u52a8\u6001\u8bc4\u4f30\u5927\u6a21\u578b\u6027\u80fd\uff0c\u5f25\u8865\u9759\u6001\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u91c7\u7528\u521b\u65b0\u6280\u672f\u63d0\u5347\u6392\u540d\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u8bc4\u4f30\u591a\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u6216\u901a\u7528\u63d0\u793a\uff0c\u672a\u80fd\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u8868\u73b0\u3002Inclusion Arena\u901a\u8fc7\u5b9e\u65f6\u7528\u6237\u53cd\u9988\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5e73\u53f0\u6574\u5408\u7528\u6237\u4ea4\u4e92\u4e2d\u7684\u6a21\u578b\u5bf9\u6bd4\uff0c\u91c7\u7528Bradley-Terry\u6a21\u578b\u53ca\u4e24\u9879\u521b\u65b0\uff08Placement Matches\u548cProximity Sampling\uff09\u4f18\u5316\u6392\u540d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5e73\u53f0\u63d0\u4f9b\u7a33\u5b9a\u6392\u540d\uff0c\u6570\u636e\u4f20\u9012\u6027\u66f4\u5f3a\uff0c\u6709\u6548\u6291\u5236\u6076\u610f\u64cd\u7eb5\u98ce\u9669\u3002", "conclusion": "Inclusion Arena\u901a\u8fc7\u8fde\u63a5\u6a21\u578b\u4e0e\u5b9e\u9645\u5e94\u7528\uff0c\u52a0\u901f\u4f18\u5316\u5b9e\u7528\u6027\u5927\u6a21\u578b\u7684\u5f00\u53d1\uff0c\u5e73\u53f0\u5df2\u516c\u5f00\u3002"}}
