{"id": "2506.21634", "pdf": "https://arxiv.org/pdf/2506.21634", "abs": "https://arxiv.org/abs/2506.21634", "authors": ["Lutz Prechelt", "Lloyd Montgomery", "Julian Frattini", "Franz Zieris"], "title": "How (Not) To Write a Software Engineering Abstract", "categories": ["cs.SE", "D.2.0; A.m; K.m"], "comment": "16 pages, 11 figures, 2 tables", "summary": "Background: Abstracts are a particularly valuable element in a software\nengineering research article. However, not all abstracts are as informative as\nthey could be. Objective: Characterize the structure of abstracts in\nhigh-quality software engineering venues. Observe and quantify deficiencies.\nSuggest guidelines for writing informative abstracts. Methods: Use qualitative\nopen coding to derive concepts that explain relevant properties of abstracts.\nIdentify the archetypical structure of abstracts. Use quantitative content\nanalysis to objectively characterize abstract structure of a sample of 362\nabstracts from five presumably high-quality venues. Use exploratory data\nanalysis to find recurring issues in abstracts. Compare the archetypical\nstructure to actual structures. Infer guidelines for producing informative\nabstracts. Results: Only 29% of the sampled abstracts are complete, i.e.,\nprovide background, objective, method, result, and conclusion information. For\nstructured abstracts, the ratio is twice as big. Only 4% of the abstracts are\nproper, i.e., they also have good readability (Flesch-Kincaid score) and have\nno informativeness gaps, understandability gaps, nor highly ambiguous\nsentences. Conclusions: (1) Even in top venues, a large majority of abstracts\nare far from ideal. (2) Structured abstracts tend to be better than\nunstructured ones. (3) Artifact-centric works need a different structured\nformat. (4) The community should start requiring conclusions that generalize,\nwhich currently are often missing in abstracts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u9ad8\u8d28\u91cf\u8f6f\u4ef6\u5de5\u7a0b\u4f1a\u8bae\u6458\u8981\u7684\u7ed3\u6784\uff0c\u53d1\u73b0\u5927\u591a\u6570\u6458\u8981\u4e0d\u5b8c\u6574\u6216\u5b58\u5728\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u6307\u5357\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u91cf\u5316\u9ad8\u8d28\u91cf\u8f6f\u4ef6\u5de5\u7a0b\u4f1a\u8bae\u6458\u8981\u7684\u7ed3\u6784\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u5f00\u653e\u7f16\u7801\u548c\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\uff0c\u5206\u6790362\u7bc7\u9ad8\u5f71\u54cd\u529b\u4f1a\u8bae\u6458\u8981\u7684\u7ed3\u6784\u548c\u4fe1\u606f\u5b8c\u6574\u6027\u3002", "result": "\u4ec529%\u7684\u6458\u8981\u5b8c\u6574\uff0c4%\u7b26\u5408\u7406\u60f3\u6807\u51c6\uff1b\u7ed3\u6784\u5316\u6458\u8981\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u591a\u6570\u6458\u8981\u672a\u8fbe\u7406\u60f3\u6807\u51c6\uff0c\u7ed3\u6784\u5316\u683c\u5f0f\u66f4\u6709\u6548\uff0c\u5efa\u8bae\u793e\u533a\u5f3a\u5236\u8981\u6c42\u7ed3\u8bba\u90e8\u5206\u3002"}}
{"id": "2506.21654", "pdf": "https://arxiv.org/pdf/2506.21654", "abs": "https://arxiv.org/abs/2506.21654", "authors": ["Wolfgang Bangerth"], "title": "Experience converting a large mathematical software package written in C++ to C++20 modules", "categories": ["cs.SE", "cs.MS"], "comment": null, "summary": "Mathematical software has traditionally been built in the form of \"packages\"\nthat build on each other. A substantial fraction of these packages is written\nin C++ and, as a consequence, the interface of a package is described in the\nform of header files that downstream packages and applications can then\n#include. C++ has inherited this approach towards exporting interfaces from C,\nbut the approach is clunky, unreliable, and slow. As a consequence, C++20 has\nintroduced a \"module\" system in which packages explicitly export declarations\nand code that compilers then store in machine-readable form and that downstream\nusers can \"import\" -- a system in line with what many other programming\nlanguages have used for decades.\n  Herein, I explore how one can convert large mathematical software packages\nwritten in C++ to this system, using the deal.II finite element library with\nits around 800,000 lines of code as an example. I describe an approach that\nallows providing both header-based and module-based interfaces from the same\ncode base, discuss the challenges one encounters, and how modules actually work\nin practice in a variety of technical and human metrics. The results show that\nwith a non-trivial, but also not prohibitive effort, the conversion to modules\nis possible, resulting in a reduction in compile time for the converted library\nitself; on the other hand, for downstream projects, compile times show no clear\ntrend. I end with thoughts about long-term strategies for converting the entire\necosystem of mathematical software over the coming years or decades.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u57fa\u4e8eC++\u7684\u5927\u578b\u6570\u5b66\u8f6f\u4ef6\u5305\uff08\u5982deal.II\u6709\u9650\u5143\u5e93\uff09\u8f6c\u6362\u4e3aC++20\u7684\u6a21\u5757\u7cfb\u7edf\uff0c\u63a2\u8ba8\u4e86\u517c\u5bb9\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002\u8f6c\u6362\u540e\u5e93\u81ea\u8eab\u7684\u7f16\u8bd1\u65f6\u95f4\u51cf\u5c11\uff0c\u4f46\u5bf9\u4e0b\u6e38\u9879\u76ee\u7684\u7f16\u8bd1\u65f6\u95f4\u5f71\u54cd\u4e0d\u4e00\u3002", "motivation": "\u4f20\u7edf\u7684C++\u5934\u6587\u4ef6\u63a5\u53e3\u65b9\u5f0f\u5b58\u5728\u7b28\u62d9\u3001\u4e0d\u53ef\u9760\u548c\u6162\u7684\u95ee\u9898\uff0cC++20\u5f15\u5165\u6a21\u5757\u7cfb\u7edf\u4ee5\u6539\u5584\u6b64\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u5c06\u5927\u578b\u6570\u5b66\u8f6f\u4ef6\u5305\u8fc1\u79fb\u5230\u8fd9\u4e00\u65b0\u7cfb\u7edf\u3002", "method": "\u91c7\u7528deal.II\u5e93\uff08\u7ea680\u4e07\u884c\u4ee3\u7801\uff09\u4f5c\u4e3a\u6848\u4f8b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540c\u65f6\u652f\u6301\u5934\u6587\u4ef6\u548c\u6a21\u5757\u63a5\u53e3\u7684\u8f6c\u6362\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u6280\u672f\u4e0e\u4eba\u56e0\u65b9\u9762\u7684\u6311\u6218\u3002", "result": "\u8f6c\u6362\u540e\u5e93\u81ea\u8eab\u7684\u7f16\u8bd1\u65f6\u95f4\u51cf\u5c11\uff0c\u4f46\u4e0b\u6e38\u9879\u76ee\u7684\u7f16\u8bd1\u65f6\u95f4\u672a\u663e\u793a\u660e\u786e\u8d8b\u52bf\u3002\u8f6c\u6362\u5de5\u4f5c\u91cf\u867d\u4e0d\u5c0f\uff0c\u4f46\u5e76\u975e\u4e0d\u53ef\u884c\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u957f\u671f\u7b56\u7565\uff0c\u9010\u6b65\u5c06\u6574\u4e2a\u6570\u5b66\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u8fc1\u79fb\u5230\u6a21\u5757\u7cfb\u7edf\uff0c\u9884\u8ba1\u9700\u8981\u6570\u5e74\u6216\u6570\u5341\u5e74\u65f6\u95f4\u3002"}}
{"id": "2506.21693", "pdf": "https://arxiv.org/pdf/2506.21693", "abs": "https://arxiv.org/abs/2506.21693", "authors": ["Ali Nouri", "Beatriz Cabrero-Daniel", "Fredrik T\u00f6rner", "Christian Berger"], "title": "The DevSafeOps Dilemma: A Systematic Literature Review on Rapidity in Safe Autonomous Driving Development and Operation", "categories": ["cs.SE", "cs.RO"], "comment": "Accepted for publication in the Journal of Systems and Software (JSS)", "summary": "Developing autonomous driving (AD) systems is challenging due to the\ncomplexity of the systems and the need to assure their safe and reliable\noperation. The widely adopted approach of DevOps seems promising to support the\ncontinuous technological progress in AI and the demand for fast reaction to\nincidents, which necessitate continuous development, deployment, and\nmonitoring. We present a systematic literature review meant to identify,\nanalyse, and synthesise a broad range of existing literature related to usage\nof DevOps in autonomous driving development. Our results provide a structured\noverview of challenges and solutions, arising from applying DevOps to\nsafety-related AI-enabled functions. Our results indicate that there are still\nseveral open topics to be addressed to enable safe DevOps for the development\nof safe AD.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86DevOps\u5728\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e86\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6307\u51fa\u4ecd\u9700\u89e3\u51b3\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5f00\u53d1\u590d\u6742\u4e14\u9700\u786e\u4fdd\u5b89\u5168\u53ef\u9760\uff0cDevOps\u7684\u6301\u7eed\u5f00\u53d1\u548c\u76d1\u63a7\u7279\u6027\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u8bc6\u522b\u3001\u5206\u6790\u548c\u7efc\u5408DevOps\u5728\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u4e2d\u7684\u76f8\u5173\u6587\u732e\u3002", "result": "\u603b\u7ed3\u4e86\u5e94\u7528DevOps\u4e8e\u5b89\u5168\u76f8\u5173AI\u529f\u80fd\u65f6\u7684\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5b9e\u73b0\u5b89\u5168\u7684DevOps\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u4ecd\u9700\u89e3\u51b3\u591a\u4e2a\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2506.21703", "pdf": "https://arxiv.org/pdf/2506.21703", "abs": "https://arxiv.org/abs/2506.21703", "authors": ["Victoria Jackson", "Susannah Liu", "Andre van der Hoek"], "title": "Using Generative AI in Software Design Education: An Experience Report", "categories": ["cs.SE"], "comment": "12 pages, 1 figure", "summary": "With the rapid adoption of Generative AI (GenAI) tools, software engineering\neducators have grappled with how best to incorporate them into the classroom.\nWhile some research discusses the use of GenAI in the context of learning to\ncode, there is little research that explores the use of GenAI in the classroom\nfor other areas of software development. This paper provides an experience\nreport on introducing GenAI into an undergraduate software design class.\nStudents were required to use GenAI (in the form of ChatGPT) to help complete a\nteam-based assignment. The data collected consisted of the ChatGPT conversation\nlogs and students' reflections on using ChatGPT for the assignment.\nSubsequently, qualitative analysis was undertaken on the data. Students\nidentified numerous ways ChatGPT helped them in their design process while\nrecognizing the need to critique the response before incorporating it into\ntheir design. At the same time, we identified several key lessons for educators\nin how to deploy GenAI in a software design class effectively. Based on our\nexperience, we believe students can benefit from using GenAI in software design\neducation as it helps them design and learn about the strengths and weaknesses\nof GenAI.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u672c\u79d1\u8f6f\u4ef6\u8bbe\u8ba1\u8bfe\u7a0b\u4e2d\u5f15\u5165\u751f\u6210\u5f0fAI\uff08\u5982ChatGPT\uff09\u7684\u5b9e\u8df5\uff0c\u5206\u6790\u4e86\u5b66\u751f\u4f7f\u7528AI\u7684\u4f53\u9a8c\uff0c\u5e76\u603b\u7ed3\u4e86\u5bf9\u6559\u80b2\u8005\u7684\u542f\u793a\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5173\u4e8e\u5176\u5728\u8f6f\u4ef6\u8bbe\u8ba1\u8bfe\u7a0b\u4e2d\u7684\u4f7f\u7528\u7814\u7a76\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5b66\u751f\u5728\u56e2\u961f\u4f5c\u4e1a\u4e2d\u4f7f\u7528ChatGPT\uff0c\u6536\u96c6\u5bf9\u8bdd\u8bb0\u5f55\u548c\u53cd\u601d\uff0c\u8fdb\u884c\u5b9a\u6027\u5206\u6790\u3002", "result": "\u5b66\u751f\u53d1\u73b0ChatGPT\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u6709\u5e2e\u52a9\uff0c\u4f46\u4e5f\u610f\u8bc6\u5230\u9700\u8981\u5bf9\u5176\u56de\u7b54\u8fdb\u884c\u6279\u5224\u6027\u8bc4\u4f30\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u8bbe\u8ba1\u6559\u80b2\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u5e2e\u52a9\u5b66\u751f\u8bbe\u8ba1\u5e76\u4e86\u89e3\u5176\u4f18\u7f3a\u70b9\u3002"}}
{"id": "2506.21678", "pdf": "https://arxiv.org/pdf/2506.21678", "abs": "https://arxiv.org/abs/2506.21678", "authors": ["Raffaele Di Donna", "Lorenzo Tortora de Falco"], "title": "On the role of connectivity in Linear Logic proofs", "categories": ["cs.LO"], "comment": "29 pages, 8 figures", "summary": "We investigate a property that extends the Danos-Regnier correctness\ncriterion for linear logic proof-structures. The property applies to the\ncorrectness graphs of a proof-structure: it states that any such graph is\nacyclic and that the number of its connected components is exactly one more\nthan the number of nodes bottom or weakening. This is known to be necessary but\nnot sufficient in multiplicative exponential linear logic (MELL) to recover a\nsequent calculus proof from a proof-structure. We present a geometric condition\non untyped proof-structures allowing us to turn this necessary property into a\nsufficient one: we can thus isolate several fragments of MELL for which this\nproperty is indeed a correctness criterion. We also recover as by-product some\nknown results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u6269\u5c55\u7ebf\u6027\u903b\u8f91\u8bc1\u660e\u7ed3\u6784\u7684Danos-Regnier\u6b63\u786e\u6027\u51c6\u5219\u7684\u6027\u8d28\u3002\u8be5\u6027\u8d28\u5e94\u7528\u4e8e\u8bc1\u660e\u7ed3\u6784\u7684\u6b63\u786e\u6027\u56fe\uff0c\u8981\u6c42\u56fe\u65e0\u73af\u4e14\u8fde\u901a\u5206\u91cf\u6570\u6bd4\u5e95\u90e8\u6216\u5f31\u5316\u8282\u70b9\u6570\u591a\u4e00\u3002\u5728MELL\u4e2d\uff0c\u8fd9\u4e00\u6027\u8d28\u662f\u5fc5\u8981\u6761\u4ef6\u4f46\u975e\u5145\u5206\u6761\u4ef6\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u51e0\u4f55\u6761\u4ef6\uff0c\u4f7f\u5176\u6210\u4e3a\u5145\u5206\u6761\u4ef6\uff0c\u4ece\u800c\u9694\u79bb\u4e86MELL\u7684\u51e0\u4e2a\u7247\u6bb5\u3002", "motivation": "\u7814\u7a76\u6269\u5c55Danos-Regnier\u51c6\u5219\u7684\u6027\u8d28\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u9a8c\u8bc1\u7ebf\u6027\u903b\u8f91\u8bc1\u660e\u7ed3\u6784\u7684\u6b63\u786e\u6027\u3002", "method": "\u5f15\u5165\u51e0\u4f55\u6761\u4ef6\uff0c\u5c06\u539f\u672c\u4ec5\u4e3a\u5fc5\u8981\u6761\u4ef6\u7684\u6027\u8d28\u8f6c\u5316\u4e3a\u5145\u5206\u6761\u4ef6\uff0c\u7528\u4e8eMELL\u7247\u6bb5\u7684\u6b63\u786e\u6027\u5224\u5b9a\u3002", "result": "\u6210\u529f\u9694\u79bb\u4e86MELL\u7684\u51e0\u4e2a\u7247\u6bb5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u6027\u8d28\u7684\u5145\u5206\u6027\u3002", "conclusion": "\u51e0\u4f55\u6761\u4ef6\u7684\u5f15\u5165\u6269\u5c55\u4e86\u6b63\u786e\u6027\u51c6\u5219\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5e76\u91cd\u65b0\u8bc1\u5b9e\u4e86\u4e00\u4e9b\u5df2\u77e5\u7ed3\u679c\u3002"}}
{"id": "2506.21933", "pdf": "https://arxiv.org/pdf/2506.21933", "abs": "https://arxiv.org/abs/2506.21933", "authors": ["Yifan Xue", "Ruihuai Liang", "Bo Yang", "Xuelin Cao", "Zhiwen Yu", "M\u00e9rouane Debbah", "Chau Yuen"], "title": "Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "With the rapid development of the low-altitude economy, air-ground integrated\nmulti-access edge computing (MEC) systems are facing increasing demands for\nreal-time and intelligent task scheduling. In such systems, task offloading and\nresource allocation encounter multiple challenges, including node\nheterogeneity, unstable communication links, and dynamic task variations. To\naddress these issues, this paper constructs a three-layer heterogeneous MEC\nsystem architecture for low-altitude economic networks, encompassing aerial and\nground users as well as edge servers. The system is systematically modeled from\nthe perspectives of communication channels, computational costs, and constraint\nconditions, and the joint optimization problem of offloading decisions and\nresource allocation is uniformly abstracted into a graph-structured modeling\ntask. On this basis, we propose a graph attention diffusion-based solution\ngenerator (GADSG). This method integrates the contextual awareness of graph\nattention networks with the solution distribution learning capability of\ndiffusion models, enabling joint modeling and optimization of discrete\noffloading variables and continuous resource allocation variables within a\nhigh-dimensional latent space. We construct multiple simulation datasets with\nvarying scales and topologies. Extensive experiments demonstrate that the\nproposed GADSG model significantly outperforms existing baseline methods in\nterms of optimization performance, robustness, and generalization across task\nstructures, showing strong potential for efficient task scheduling in dynamic\nand complex low-altitude economic network environments.", "AI": {"tldr": "\u9488\u5bf9\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u4e2d\u7684\u4efb\u52a1\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u6269\u6563\u7684\u89e3\u51b3\u65b9\u6848\u751f\u6210\u5668\uff08GADSG\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784MEC\u7cfb\u7edf\u4e2d\u7684\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u6311\u6218\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u63d0\u51fa\u4e86\u5b9e\u65f6\u548c\u667a\u80fd\u4efb\u52a1\u8c03\u5ea6\u7684\u9700\u6c42\uff0c\u4f46\u8282\u70b9\u5f02\u6784\u6027\u3001\u901a\u4fe1\u94fe\u8def\u4e0d\u7a33\u5b9a\u548c\u4efb\u52a1\u52a8\u6001\u6027\u7b49\u95ee\u9898\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e09\u5c42\u7684\u5f02\u6784MEC\u7cfb\u7edf\u67b6\u6784\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u6269\u6563\u6a21\u578b\uff0c\u63d0\u51fa\u4e86GADSG\u65b9\u6cd5\uff0c\u5bf9\u5378\u8f7d\u51b3\u7b56\u548c\u8d44\u6e90\u5206\u914d\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u591a\u7ec4\u4eff\u771f\u6570\u636e\u5b9e\u9a8c\uff0cGADSG\u5728\u4f18\u5316\u6027\u80fd\u3001\u7a33\u5065\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GADSG\u5c55\u73b0\u51fa\u4e86\u5728\u52a8\u6001\u590d\u6742\u7684\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u73af\u5883\u4e2d\u9ad8\u6548\u4efb\u52a1\u8c03\u5ea6\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.21960", "pdf": "https://arxiv.org/pdf/2506.21960", "abs": "https://arxiv.org/abs/2506.21960", "authors": ["Zixuan Wang", "Liang Yuan", "Xianmeng Jiang", "Kun Li", "Junmin Xiao", "Yunquan Zhang"], "title": "Redundant Array Computation Elimination", "categories": ["cs.PF"], "comment": null, "summary": "Redundancy elimination is a key optimization direction, and loop nests are\nthe main optimization target in modern compilers. Previous work on redundancy\nelimination of array computations in loop nests lacks universality. These\napproaches either focus on specific computation patterns or fail to recognize\nredundancies with complex structures. This paper proposes RACE (Redundant Array\nComputation Elimination), a more general redundancy elimination technique. RACE\nutilizes a novel two-level scheme to identify the data reuse between array\nreferences and the computation redundancies between expressions. It traverses\nthe expression trees in loop nests to detect redundancies hierarchically in\nlinear time and generates efficient code with optimized auxiliary arrays that\nstore redundant computation results. Furthermore, RACE supports the expression\nreassociation with various aggressive strategies to improve the redundancy\nopportunities. Experimental results demonstrate the effectiveness of RACE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5197\u4f59\u6570\u7ec4\u8ba1\u7b97\u6d88\u9664\u6280\u672fRACE\uff0c\u901a\u8fc7\u4e24\u5c42\u65b9\u6848\u9ad8\u6548\u8bc6\u522b\u6570\u7ec4\u5f15\u7528\u95f4\u6570\u636e\u91cd\u7528\u548c\u8868\u8fbe\u5f0f\u95f4\u8ba1\u7b97\u5197\u4f59\uff0c\u5e76\u751f\u6210\u4f18\u5316\u4ee3\u7801\u3002", "motivation": "\u73b0\u6709\u5197\u4f59\u6d88\u9664\u65b9\u6cd5\u7f3a\u4e4f\u666e\u9002\u6027\uff0c\u8981\u4e48\u9488\u5bf9\u7279\u5b9a\u6a21\u5f0f\uff0c\u8981\u4e48\u65e0\u6cd5\u8bc6\u522b\u590d\u6742\u7ed3\u6784\u5197\u4f59\u3002", "method": "RACE\u91c7\u7528\u4e24\u5c42\u65b9\u6848\u904d\u5386\u5faa\u73af\u5d4c\u5957\u4e2d\u7684\u8868\u8fbe\u5f0f\u6811\uff0c\u7ebf\u6027\u65f6\u95f4\u5206\u5c42\u68c0\u6d4b\u5197\u4f59\uff0c\u5e76\u652f\u6301\u8868\u8fbe\u5f0f\u91cd\u5173\u8054\u589e\u5f3a\u5197\u4f59\u673a\u4f1a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRACE\u7684\u6709\u6548\u6027\u3002", "conclusion": "RACE\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u5197\u4f59\u6570\u7ec4\u8ba1\u7b97\u6d88\u9664\u6280\u672f\u3002"}}
{"id": "2506.21865", "pdf": "https://arxiv.org/pdf/2506.21865", "abs": "https://arxiv.org/abs/2506.21865", "authors": ["Haofeng Wang", "Yilin Guo", "Zehao Li", "Tong Yue", "Yizong Wang", "Enci Zhang", "Rongqun Lin", "Feng Gao", "Shiqi Wang", "Siwei Ma"], "title": "RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture", "categories": ["cs.MM", "cs.CL"], "comment": "IEEE International Conference on Multimedia and Expo Workshop,\n  2025.(Accepted)", "summary": "The Yellow River is China's mother river and a cradle of human civilization.\nThe ancient Yellow River culture is, moreover, an indispensable part of human\nart history. To conserve and inherit the ancient Yellow River culture, we\ndesigned RiverEcho, a real-time interactive system that responds to voice\nqueries using a large language model and a cultural knowledge dataset,\ndelivering explanations through a talking-head digital human. Specifically, we\nbuilt a knowledge database focused on the ancient Yellow River culture,\nincluding the collection of historical texts and the processing pipeline.\nExperimental results demonstrate that leveraging Retrieval-Augmented Generation\n(RAG) on the proposed dataset enhances the response quality of the Large\nLanguage Model(LLM), enabling the system to generate more professional and\ninformative responses. Our work not only diversifies the means of promoting\nYellow River culture but also provides users with deeper cultural insights.", "AI": {"tldr": "RiverEcho\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6587\u5316\u77e5\u8bc6\u6570\u636e\u96c6\u7684\u5b9e\u65f6\u4ea4\u4e92\u7cfb\u7edf\uff0c\u7528\u4e8e\u4fdd\u62a4\u548c\u4f20\u627f\u53e4\u8001\u7684\u9ec4\u6cb3\u6587\u5316\uff0c\u901a\u8fc7\u8bed\u97f3\u67e5\u8be2\u548c\u6570\u5b57\u4eba\u63d0\u4f9b\u4e13\u4e1a\u56de\u7b54\u3002", "motivation": "\u4fdd\u62a4\u548c\u4f20\u627f\u9ec4\u6cb3\u6587\u5316\uff0c\u4e30\u5bcc\u5176\u63a8\u5e7f\u624b\u6bb5\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u6587\u5316\u89c1\u89e3\u3002", "method": "\u6784\u5efa\u9ec4\u6cb3\u6587\u5316\u77e5\u8bc6\u6570\u636e\u5e93\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff08RAG\uff09\uff0c\u901a\u8fc7\u8bed\u97f3\u4ea4\u4e92\u548c\u6570\u5b57\u4eba\u5c55\u793a\u7ed3\u679c\u3002", "result": "RAG\u6280\u672f\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u56de\u7b54\u8d28\u91cf\uff0c\u751f\u6210\u66f4\u4e13\u4e1a\u3001\u4fe1\u606f\u4e30\u5bcc\u7684\u56de\u5e94\u3002", "conclusion": "RiverEcho\u4e0d\u4ec5\u4e30\u5bcc\u4e86\u9ec4\u6cb3\u6587\u5316\u7684\u63a8\u5e7f\u65b9\u5f0f\uff0c\u8fd8\u589e\u5f3a\u4e86\u7528\u6237\u7684\u6587\u5316\u4f53\u9a8c\u548c\u7406\u89e3\u3002"}}
{"id": "2506.21629", "pdf": "https://arxiv.org/pdf/2506.21629", "abs": "https://arxiv.org/abs/2506.21629", "authors": ["Chenhao Zhang", "Yezhi Shen", "Fengqing Zhu"], "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes", "categories": ["cs.GR"], "comment": "6 pages, Source code is available at\n  https://github.com/Chenhao-Z/ICP-3DGS. To appear at ICIP 2025", "summary": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian\nSplatting (3DGS) have made significant progress in scene reconstruction and\nnovel view synthesis. However, they heavily rely on preprocessed camera poses\nand 3D structural priors from structure-from-motion (SfM), which are\nchallenging to obtain in outdoor scenarios. To address this challenge, we\npropose to incorporate Iterative Closest Point (ICP) with optimization-based\nrefinement to achieve accurate camera pose estimation under large camera\nmovements. Additionally, we introduce a voxel-based scene densification\napproach to guide the reconstruction in large-scale scenes. Experiments\ndemonstrate that our approach ICP-3DGS outperforms existing methods in both\ncamera pose estimation and novel view synthesis across indoor and outdoor\nscenes of various scales. Source code is available at\nhttps://github.com/Chenhao-Z/ICP-3DGS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fed\u4ee3\u6700\u8fd1\u70b9\uff08ICP\uff09\u548c\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u8303\u56f4\u8fd0\u52a8\u4e0b\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u4f53\u7d20\u5316\u573a\u666f\u589e\u5f3a\u6765\u6307\u5bfc\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff08\u5982NeRF\u548c3DGS\uff09\u4e25\u91cd\u4f9d\u8d56\u9884\u5904\u7406\u76f8\u673a\u59ff\u6001\u548c3D\u7ed3\u6784\u5148\u9a8c\uff0c\u8fd9\u5728\u6237\u5916\u573a\u666f\u4e2d\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408ICP\u548c\u4f18\u5316\u65b9\u6cd5\u8fdb\u884c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u4f7f\u7528\u4f53\u7d20\u5316\u573a\u666f\u589e\u5f3a\u6765\u6307\u5bfc\u91cd\u5efa\u3002", "result": "ICP-3DGS\u5728\u5ba4\u5185\u5916\u573a\u666f\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u795e\u7ecf\u6e32\u67d3\u4e2d\u4f9d\u8d56\u9884\u5904\u7406\u76f8\u673a\u59ff\u6001\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22052", "pdf": "https://arxiv.org/pdf/2506.22052", "abs": "https://arxiv.org/abs/2506.22052", "authors": ["Nico Ostendorf", "Keno Garlichs", "Lars Wolf"], "title": "Evaluating Redundancy Mitigation in Vulnerable Road User Awareness Messages for Bicycles", "categories": ["cs.ET", "cs.NI", "eess.SP"], "comment": null, "summary": "V2X communication has become crucial for enhancing road safety, especially\nfor Vulnerable Road Users (VRU) such as pedestrians and cyclists. However, the\nincreasing number of devices communicating on the same channels will lead to\nsignificant channel load. To address this issue this study evaluates the\neffectiveness of Redundancy Mitigation (RM) for VRU Awareness Messages (VAM),\nfocusing specifically on cyclists. The objective of RM is to minimize the\ntransmission of redundant information. We conducted a simulation study using a\nurban scenario with a high bicycle density based on traffic data from Hannover,\nGermany. This study assessed the impact of RM on channel load, measured by\nChannel Busy Ratio (CBR), and safety, measured by VRU Perception Rate (VPR) in\nsimulation. To evaluate the accuracy and reliability of the RM mechanisms, we\nanalyzed the actual differences in position, speed, and heading between the ego\nVRU and the VRU, which was assumed to be redundant. Our findings indicate that\nwhile RM can reduce channel congestion, it also leads to a decrease in VPR. The\nanalysis of actual differences revealed that the RM mechanism standardized by\nETSI often uses outdated information, leading to significant discrepancies in\nposition, speed, and heading, which could result in dangerous situations. To\naddress these limitations, we propose an adapted RM mechanism that improves the\nbalance between reducing channel load and maintaining VRU awareness. The\nadapted approach shows a significant reduction in maximum CBR and a less\nsignificant decrease in VPR compared to the standardized RM. Moreover, it\ndemonstrates better performance in the actual differences in position, speed,\nand heading, thereby enhancing overall safety. Our results highlight the need\nfor further research to optimize RM techniques and ensure they effectively\nenhance V2X communication without compromising the safety of VRUs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5197\u4f59\u7f13\u89e3\uff08RM\uff09\u5728V2X\u901a\u4fe1\u4e2d\u5bf9\u9a91\u884c\u8005\u7684\u5b89\u5168\u6027\u5f71\u54cd\uff0c\u53d1\u73b0\u6807\u51c6\u5316RM\u673a\u5236\u4f7f\u7528\u8fc7\u65f6\u4fe1\u606f\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u4ee5\u51cf\u5c11\u4fe1\u9053\u8d1f\u8f7d\u5e76\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740V2X\u901a\u4fe1\u8bbe\u5907\u589e\u591a\uff0c\u4fe1\u9053\u8d1f\u8f7d\u589e\u52a0\uff0c\u5197\u4f59\u4fe1\u606f\u4f20\u8f93\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u95ee\u9898\uff0c\u9700\u4f18\u5316RM\u673a\u5236\u4ee5\u5e73\u8861\u4fe1\u9053\u8d1f\u8f7d\u548cVRU\uff08\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff09\u7684\u611f\u77e5\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u9ad8\u81ea\u884c\u8f66\u5bc6\u5ea6\u7684\u57ce\u5e02\u573a\u666f\uff0c\u8bc4\u4f30RM\u5bf9\u4fe1\u9053\u7e41\u5fd9\u6bd4\uff08CBR\uff09\u548cVRU\u611f\u77e5\u7387\uff08VPR\uff09\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u65b9\u5411\u7684\u5b9e\u9645\u5dee\u5f02\u3002", "result": "\u6807\u51c6\u5316RM\u867d\u51cf\u5c11\u4fe1\u9053\u62e5\u585e\uff0c\u4f46\u964d\u4f4e\u4e86VPR\uff1b\u6539\u8fdb\u7684RM\u663e\u8457\u964d\u4f4e\u6700\u5927CBR\uff0c\u4e14\u51cf\u5c11\u5bf9\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u65b9\u5411\u5dee\u5f02\u66f4\u5c0f\u3002", "conclusion": "\u6539\u8fdb\u7684RM\u673a\u5236\u5728\u51cf\u5c11\u4fe1\u9053\u8d1f\u8f7d\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u66f4\u9ad8\u7684\u5b89\u5168\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4f18\u5316RM\u6280\u672f\u4ee5\u786e\u4fddVRU\u5b89\u5168\u3002"}}
{"id": "2506.22033", "pdf": "https://arxiv.org/pdf/2506.22033", "abs": "https://arxiv.org/abs/2506.22033", "authors": ["Yongchao He", "Bohan Zhao", "Zheng Cao"], "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference", "categories": ["cs.DC"], "comment": null, "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.", "AI": {"tldr": "SiPipe\u901a\u8fc7\u5229\u7528\u672a\u5145\u5206\u5229\u7528\u7684CPU\u8d44\u6e90\u6765\u5378\u8f7d\u8f85\u52a9\u8ba1\u7b97\u548c\u901a\u4fe1\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740LLM\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u589e\u957f\uff0c\u7ba1\u9053\u5e76\u884c(PP)\u5728\u591aGPU\u90e8\u7f72\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0cSiPipe\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528CPU\u91c7\u6837\u3001\u4ee4\u724c\u5b89\u5168\u6267\u884c\u6a21\u578b\u548c\u7ed3\u6784\u611f\u77e5\u4f20\u8f93\u4e09\u79cd\u5173\u952e\u6280\u672f\uff0c\u4f18\u5316\u7ba1\u9053\u6c14\u6ce1\u95ee\u9898\u3002", "result": "SiPipe\u5728\u76f8\u540cPP\u914d\u7f6e\u4e0b\uff0c\u541e\u5410\u91cf\u63d0\u53472.1\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e43%\uff0cGPU\u5229\u7528\u7387\u63d0\u9ad823%\u3002", "conclusion": "SiPipe\u901a\u8fc7\u667a\u80fd\u8d44\u6e90\u8c03\u5ea6\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.21811", "pdf": "https://arxiv.org/pdf/2506.21811", "abs": "https://arxiv.org/abs/2506.21811", "authors": ["Lingkai Meng", "Yu Shao", "Long Yuan", "Longbin Lai", "Peng Cheng", "Xue Li", "Wenyuan Yu", "Wenjie Zhang", "Xuemin Lin", "Jingren Zhou"], "title": "Revisiting Graph Analytics Benchmark", "categories": ["cs.DB", "cs.GR"], "comment": null, "summary": "The rise of graph analytics platforms has led to the development of various\nbenchmarks for evaluating and comparing platform performance. However, existing\nbenchmarks often fall short of fully assessing performance due to limitations\nin core algorithm selection, data generation processes (and the corresponding\nsynthetic datasets), as well as the neglect of API usability evaluation. To\naddress these shortcomings, we propose a novel graph analytics benchmark.\nFirst, we select eight core algorithms by extensively reviewing both academic\nand industrial settings. Second, we design an efficient and flexible data\ngenerator and produce eight new synthetic datasets as the default datasets for\nour benchmark. Lastly, we introduce a multi-level large language model\n(LLM)-based framework for API usability evaluation-the first of its kind in\ngraph analytics benchmarks. We conduct comprehensive experimental evaluations\non existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and\nG-thinker). The experimental results demonstrate the superiority of our\nproposed benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u56fe\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7cbe\u9009\u6838\u5fc3\u7b97\u6cd5\u3001\u8bbe\u8ba1\u7075\u6d3b\u7684\u6570\u636e\u751f\u6210\u5668\u5e76\u5f15\u5165\u57fa\u4e8eLLM\u7684API\u53ef\u7528\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u5728\u6838\u5fc3\u7b97\u6cd5\u9009\u62e9\u3001\u6570\u636e\u751f\u6210\u548cAPI\u53ef\u7528\u6027\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "1. \u7cbe\u90098\u79cd\u6838\u5fc3\u7b97\u6cd5\uff1b2. \u8bbe\u8ba1\u9ad8\u6548\u7075\u6d3b\u7684\u6570\u636e\u751f\u6210\u5668\u5e76\u751f\u62108\u4e2a\u65b0\u6570\u636e\u96c6\uff1b3. \u5f15\u5165\u57fa\u4e8eLLM\u7684\u591a\u5c42\u6b21API\u53ef\u7528\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u73b0\u6709\u56fe\u5206\u6790\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65b0\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u65b0\u57fa\u51c6\u6d4b\u8bd5\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u4e0d\u8db3\uff0c\u4e3a\u56fe\u5206\u6790\u5e73\u53f0\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u65b9\u6848\u3002"}}
{"id": "2506.21762", "pdf": "https://arxiv.org/pdf/2506.21762", "abs": "https://arxiv.org/abs/2506.21762", "authors": ["Oliver Huang", "Carolina Nobre"], "title": "ViStruct: Simulating Expert-Like Reasoning Through Task Decomposition and Visual Attention Cues", "categories": ["cs.HC"], "comment": "VIS 2025", "summary": "Data visualization tasks often require multi-step reasoning, and the\ninterpretive strategies experts use, such as decomposing complex goals into\nsmaller subtasks and selectively attending to key chart regions are rarely made\nexplicit. ViStruct is an automated pipeline that simulates these expert\nbehaviours by breaking high-level visual questions into structured analytic\nsteps and highlighting semantically relevant chart areas. Leveraging large\nlanguage and vision-language models, ViStruct identifies chart components, maps\nsubtasks to spatial regions, and presents visual attention cues to externalize\nexpert-like reasoning flows. While not designed for direct novice instruction,\nViStruct provides a replicable model of expert interpretation that can inform\nthe development of future visual literacy tools. We evaluate the system on 45\ntasks across 12 chart types and validate its outputs with trained visualization\nusers, confirming its ability to produce interpretable and expert-aligned\nreasoning sequences.", "AI": {"tldr": "ViStruct\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u89e3\u9ad8\u9636\u89c6\u89c9\u95ee\u9898\u4e3a\u7ed3\u6784\u5316\u5206\u6790\u6b65\u9aa4\uff0c\u5e76\u7a81\u51fa\u663e\u793a\u5173\u952e\u56fe\u8868\u533a\u57df\uff0c\u6a21\u62df\u4e13\u5bb6\u7684\u6570\u636e\u53ef\u89c6\u5316\u89e3\u91ca\u884c\u4e3a\u3002", "motivation": "\u4e13\u5bb6\u5728\u591a\u6b65\u6570\u636e\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u4f7f\u7528\u7684\u89e3\u91ca\u7b56\u7565\uff08\u5982\u5206\u89e3\u590d\u6742\u76ee\u6807\u6216\u9009\u62e9\u6027\u5173\u6ce8\u5173\u952e\u533a\u57df\uff09\u901a\u5e38\u4e0d\u660e\u786e\uff0cViStruct\u65e8\u5728\u901a\u8fc7\u6a21\u578b\u6a21\u62df\u8fd9\u4e9b\u7b56\u7565\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0cViStruct\u8bc6\u522b\u56fe\u8868\u7ec4\u4ef6\uff0c\u5c06\u5b50\u4efb\u52a1\u6620\u5c04\u5230\u7a7a\u95f4\u533a\u57df\uff0c\u5e76\u5c55\u793a\u89c6\u89c9\u6ce8\u610f\u529b\u7ebf\u7d22\u3002", "result": "\u572812\u79cd\u56fe\u8868\u7c7b\u578b\u768445\u4e2a\u4efb\u52a1\u4e2d\uff0cViStruct\u751f\u6210\u7684\u89e3\u91ca\u5e8f\u5217\u4e0e\u4e13\u5bb6\u4e00\u81f4\uff0c\u4e14\u9a8c\u8bc1\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "ViStruct\u63d0\u4f9b\u4e86\u4e13\u5bb6\u89e3\u91ca\u7684\u53ef\u590d\u5236\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u89c6\u89c9\u7d20\u517b\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2506.22169", "pdf": "https://arxiv.org/pdf/2506.22169", "abs": "https://arxiv.org/abs/2506.22169", "authors": ["Zheng Zhang", "Donglin Yang", "Xiaobo Zhou", "Dazhao Cheng"], "title": "MCFuser: High-Performance and Rapid Fusion of Memory-Bound Compute-Intensive Operators", "categories": ["cs.DC", "cs.PL"], "comment": "12 pages, accepted at SC 2024", "summary": "Operator fusion, a key technique to improve data locality and alleviate GPU\nmemory bandwidth pressure, often fails to extend to the fusion of multiple\ncompute-intensive operators due to saturated computation throughput. However,\nthe dynamicity of tensor dimension sizes could potentially lead to these\noperators becoming memory-bound, necessitating the generation of fused kernels,\na task hindered by limited search spaces for fusion strategies, redundant\nmemory access, and prolonged tuning time, leading to sub-optimal performance\nand inefficient deployment.\n  We introduce MCFuser, a pioneering framework designed to overcome these\nobstacles by generating high-performance fused kernels for what we define as\nmemory-bound compute-intensive (MBCI) operator chains. Leveraging high-level\ntiling expressions to delineate a comprehensive search space, coupled with\nDirected Acyclic Graph (DAG) analysis to eliminate redundant memory accesses,\nMCFuser streamlines kernel optimization. By implementing guidelines to prune\nthe search space and incorporating an analytical performance model with a\nheuristic search, MCFuser not only significantly accelerates the tuning process\nbut also demonstrates superior performance. Benchmarked against leading\ncompilers like Ansor on NVIDIA A100 and RTX3080 GPUs, MCFuser achieves up to a\n5.9x speedup in kernel performance and outpaces other baselines while reducing\ntuning time by over 70-fold, showcasing its agility.", "AI": {"tldr": "MCFuser\u6846\u67b6\u901a\u8fc7\u9ad8\u7ea7\u5206\u5757\u8868\u8fbe\u5f0f\u548cDAG\u5206\u6790\u4f18\u5316\u8ba1\u7b97\u5bc6\u96c6\u578b\u64cd\u4f5c\u94fe\u7684\u878d\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u5e76\u51cf\u5c11\u8c03\u4f18\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u5bc6\u96c6\u578b\u64cd\u4f5c\u94fe\u5728\u52a8\u6001\u5f20\u91cf\u7ef4\u5ea6\u4e0b\u7531\u4e8e\u5185\u5b58\u74f6\u9888\u548c\u5197\u4f59\u8bbf\u95ee\u5bfc\u81f4\u7684\u6027\u80fd\u4e0d\u4f73\u95ee\u9898\u3002", "method": "\u5229\u7528\u9ad8\u7ea7\u5206\u5757\u8868\u8fbe\u5f0f\u5b9a\u4e49\u641c\u7d22\u7a7a\u95f4\uff0c\u7ed3\u5408DAG\u5206\u6790\u6d88\u9664\u5197\u4f59\u5185\u5b58\u8bbf\u95ee\uff0c\u5e76\u901a\u8fc7\u542f\u53d1\u5f0f\u641c\u7d22\u5feb\u901f\u4f18\u5316\u3002", "result": "\u5728NVIDIA GPU\u4e0a\u6027\u80fd\u6700\u9ad8\u63d0\u53475.9\u500d\uff0c\u8c03\u4f18\u65f6\u95f4\u51cf\u5c1170\u500d\u4ee5\u4e0a\u3002", "conclusion": "MCFuser\u9ad8\u6548\u89e3\u51b3\u4e86\u8ba1\u7b97\u5bc6\u96c6\u578b\u64cd\u4f5c\u94fe\u7684\u878d\u5408\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e0e\u90e8\u7f72\u6548\u7387\u3002"}}
{"id": "2506.22107", "pdf": "https://arxiv.org/pdf/2506.22107", "abs": "https://arxiv.org/abs/2506.22107", "authors": ["Amir Hossein Jalilvand", "M. Hassan Najafi"], "title": "Power- and Area-Efficient Unary Sorting Architecture Using FSM-Based Unary Number Generator", "categories": ["cs.AR"], "comment": "6 pages", "summary": "Sorting is a fundamental operation in computer systems and is widely used in\napplications such as databases, data analytics, and hardware accelerators.\nUnary computing has recently emerged as a low-cost and power-efficient paradigm\nfor implementing hardware sorters by eliminating the need for complex\narithmetic operations. However, existing comparison-free unary computing-based\ndesigns suffer from significant area and power overhead due to costly unary\nnumber generators.\n  In this paper, we present a novel ascending-order unary sorting module\nfeaturing a finite-state-machine-based unary number generator that\nsignificantly reduces implementation costs. By generating right-aligned unary\nstreams using a two-state finite-state machine, our architecture iteratively\nidentifies the minimum input value in each cycle without conventional\ncomparators. Synthesis results in a 45nm technology node demonstrate up to 82%\nreduction in area and 70% reduction in power consumption compared to\nstate-of-the-art unary designs. The proposed sorter offers a promising solution\nfor energy-constrained and resource-limited hardware systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u673a\u7684\u5347\u5e8f\u4e00\u5143\u6392\u5e8f\u6a21\u5757\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b9e\u73b0\u6210\u672c\uff0c\u76f8\u6bd4\u73b0\u6709\u8bbe\u8ba1\u9762\u79ef\u51cf\u5c1182%\uff0c\u529f\u8017\u964d\u4f4e70%\u3002", "motivation": "\u4e00\u5143\u8ba1\u7b97\u4f5c\u4e3a\u4e00\u79cd\u4f4e\u6210\u672c\u548c\u9ad8\u6548\u7684\u786c\u4ef6\u6392\u5e8f\u8303\u5f0f\uff0c\u73b0\u6709\u8bbe\u8ba1\u56e0\u590d\u6742\u7684\u4e00\u5143\u6570\u751f\u6210\u5668\u5bfc\u81f4\u9762\u79ef\u548c\u529f\u8017\u5f00\u9500\u5927\u3002", "method": "\u901a\u8fc7\u4e24\u72b6\u6001\u6709\u9650\u72b6\u6001\u673a\u751f\u6210\u53f3\u5bf9\u9f50\u4e00\u5143\u6d41\uff0c\u8fed\u4ee3\u8bc6\u522b\u6700\u5c0f\u503c\uff0c\u65e0\u9700\u4f20\u7edf\u6bd4\u8f83\u5668\u3002", "result": "\u572845nm\u6280\u672f\u8282\u70b9\u4e0a\uff0c\u9762\u79ef\u548c\u529f\u8017\u5206\u522b\u51cf\u5c1182%\u548c70%\u3002", "conclusion": "\u8be5\u6392\u5e8f\u5668\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22037", "pdf": "https://arxiv.org/pdf/2506.22037", "abs": "https://arxiv.org/abs/2506.22037", "authors": ["Jiawei Li", "Zan Liang", "Guoxin Wang", "Jinzhi Lu", "Yan Yan", "Shouxuan Wu", "Hao Wang"], "title": "KARMA Approach supporting Development Process Reconstruction in Model-based Systems Engineering", "categories": ["cs.SE"], "comment": "12 pages, 9 figures, submitted to the 15th international Complex\n  Systems Design & Management (CSD&M) conference", "summary": "Model reconstruction is a method used to drive the development of complex\nsystem development processes in model-based systems engineering. Currently,\nduring the iterative design process of a system, there is a lack of an\neffective method to manage changes in development requirements, such as\ndevelopment cycle requirements and cost requirements, and to realize the\nreconstruction of the system development process model. To address these\nissues, this paper proposes a model reconstruction method to support the\ndevelopment process model. Firstly, the KARMA language, based on the GOPPRR-E\nmetamodeling method, is utilized to uniformly formalize the process models\nconstructed based on different modeling languages. Secondly, a model\nreconstruction framework is introduced. This framework takes a structured\ndevelopment requirements based natural language as input, employs natural\nlanguage processing techniques to analyze the development requirements text,\nand extracts structural and optimization constraint information. Then, after\nstructural reorganization and algorithm optimization, a development process\nmodel that meets the development requirements is obtained. Finally, as a case\nstudy, the development process of the aircraft onboard maintenance system is\nreconstructed. The results demonstrate that this method can significantly\nenhance the design efficiency of the development process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKARMA\u8bed\u8a00\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u6a21\u578b\u91cd\u6784\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6d41\u7a0b\u8bbe\u8ba1\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u7cfb\u7edf\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u6709\u6548\u65b9\u6cd5\u7ba1\u7406\u9700\u6c42\u53d8\u66f4\u548c\u5b9e\u73b0\u6a21\u578b\u91cd\u6784\uff0c\u4e3a\u6b64\u63d0\u51fa\u4e00\u79cd\u652f\u6301\u5f00\u53d1\u6d41\u7a0b\u6a21\u578b\u7684\u6a21\u578b\u91cd\u6784\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u57fa\u4e8eGOPPRR-E\u7684KARMA\u8bed\u8a00\u7edf\u4e00\u5f62\u5f0f\u5316\u6d41\u7a0b\u6a21\u578b\uff0c\u5f15\u5165\u6a21\u578b\u91cd\u6784\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5206\u6790\u9700\u6c42\u6587\u672c\u5e76\u63d0\u53d6\u4fe1\u606f\uff0c\u7ecf\u8fc7\u7ed3\u6784\u91cd\u7ec4\u548c\u7b97\u6cd5\u4f18\u5316\u751f\u6210\u6ee1\u8db3\u9700\u6c42\u7684\u6d41\u7a0b\u6a21\u578b\u3002", "result": "\u4ee5\u98de\u673a\u673a\u8f7d\u7ef4\u62a4\u7cfb\u7edf\u5f00\u53d1\u4e3a\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u8bbe\u8ba1\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u53d1\u6d41\u7a0b\u6a21\u578b\u91cd\u6784\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bbe\u8ba1\u6548\u7387\u3002"}}
{"id": "2506.22061", "pdf": "https://arxiv.org/pdf/2506.22061", "abs": "https://arxiv.org/abs/2506.22061", "authors": ["Vojt\u011bch Havlena", "Michal He\u010dko", "Luk\u00e1\u0161 Hol\u00edk", "Ond\u0159ej Leng\u00e1l"], "title": "Negated String Containment is Decidable (Technical Report)", "categories": ["cs.LO", "cs.FL"], "comment": null, "summary": "We provide a positive answer to a long-standing open question of the\ndecidability of the not-contains string predicate. Not-contains is practically\nrelevant, for instance in symbolic execution of string manipulating programs.\nParticularly, we show that the predicate notContains(x1 ... xn, y1 ... ym),\nwhere x1 ... xn and y1 ... ym are sequences of string variables constrained by\nregular languages, is decidable. Decidability of a not-contains predicate\ncombined with chain-free word equations and regular membership constraints\nfollows.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86\u5b57\u7b26\u4e32\u4e0d\u5305\u542b\u8c13\u8bcd\u7684\u4e0d\u53ef\u5224\u5b9a\u6027\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u5b57\u7b26\u4e32\u4e0d\u5305\u542b\u8c13\u8bcd\u7684\u4e0d\u53ef\u5224\u5b9a\u6027\uff0c\u89e3\u51b3\u7b26\u53f7\u6267\u884c\u7b49\u9886\u57df\u4e2d\u7684\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u5206\u6790\u6b63\u5219\u8bed\u8a00\u7ea6\u675f\u4e0b\u7684\u5b57\u7b26\u4e32\u53d8\u91cf\u5e8f\u5217\uff0c\u5e76\u7ed3\u5408\u94fe\u81ea\u7531\u5b57\u65b9\u7a0b\u548c\u6b63\u5219\u6210\u5458\u7ea6\u675f\u3002", "result": "\u8bc1\u660e\u4e86\u5b57\u7b26\u4e32\u4e0d\u5305\u542b\u8c13\u8bcd\u7684\u53ef\u5224\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7b26\u53f7\u6267\u884c\u7b49\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2506.22148", "pdf": "https://arxiv.org/pdf/2506.22148", "abs": "https://arxiv.org/abs/2506.22148", "authors": ["Joshua Goulton", "Milena Radenkovic"], "title": "Resilient Communication For Avalanche Response in Infrastructure-Limited Environments", "categories": ["cs.NI"], "comment": null, "summary": "Delay Tolerant Networks (DTNs) offer a promising paradigm for maintaining\ncommunication in infrastructure limited environments, such as those encountered\nduring natural disasters. This paper investigates the viability of leveraging\nan existing national transport system - the Swiss rail network - as a data mule\nbackbone for disseminating critical avalanche alerts. Using The Opportunistic\nNetwork Environment (ONE) simulator, we model the entire Swiss rail network and\nconduct a rigorous comparative analysis of two seminal DTN routing protocols:\nEpidemic and PROPHET. Experiments are performed in two distinct scenarios:\nalerts originating from dense urban centres and from sparse, remote mountainous\nregions. Our results demonstrate that the rail network provides robust\nconnectivity for opportunistic communication in both environments thus\nvalidating the integration of DTN principles in remote scenarios.", "AI": {"tldr": "DTN\u6846\u67b6\u901a\u8fc7\u745e\u58eb\u94c1\u8def\u7f51\u7edc\u4f20\u8f93\u96ea\u5d29\u8b66\u62a5\u7684\u6709\u6548\u6027\u5206\u6790\uff0c\u5bf9\u6bd4Epidemic\u548cPROPHET\u534f\u8bae\uff0c\u9a8c\u8bc1\u5176\u5728\u57ce\u5e02\u548c\u504f\u8fdc\u5730\u533a\u7684\u9002\u7528\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u7684\u707e\u5bb3\u73af\u5883\u4e2d\uff08\u5982\u96ea\u5d29\u8b66\u62a5\uff09\u5229\u7528\u73b0\u6709\u4ea4\u901a\u7f51\u7edc\u5b9e\u73b0\u53ef\u9760\u7684\u901a\u4fe1\u3002", "method": "\u4f7f\u7528ONE\u6a21\u62df\u5668\u5efa\u6a21\u745e\u58eb\u94c1\u8def\u7f51\u7edc\uff0c\u5bf9\u6bd4Epidemic\u548cPROPHET\u4e24\u79cdDTN\u8def\u7531\u534f\u8bae\uff0c\u5206\u522b\u5728\u57ce\u5e02\u548c\u504f\u8fdc\u5c71\u533a\u573a\u666f\u6d4b\u8bd5\u3002", "result": "\u94c1\u8def\u7f51\u7edc\u5728\u4e24\u79cd\u73af\u5883\u4e0b\u5747\u80fd\u63d0\u4f9b\u7a33\u5b9a\u7684\u673a\u4f1a\u6027\u901a\u4fe1\uff0c\u652f\u6301DTN\u6280\u672f\u5728\u504f\u8fdc\u5730\u533a\u7684\u5e94\u7528\u3002", "conclusion": "\u745e\u58eb\u94c1\u8def\u7f51\u7edc\u4f5c\u4e3aDTN\u6570\u636e\u4f20\u8f93\u9aa8\u5e72\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u707e\u5bb3\u901a\u4fe1\u4e2d\u3002"}}
{"id": "2506.21851", "pdf": "https://arxiv.org/pdf/2506.21851", "abs": "https://arxiv.org/abs/2506.21851", "authors": ["Haofeng Wang", "Fangtao Zhou", "Qi Zhang", "Zeyuan Chen", "Enci Zhang", "Zhao Wang", "Xiaofeng Huang", "Siwei Ma"], "title": "End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "IEEE International Conference on Systems, Man, and Cybernetics 2025.\n  (SMC), under review", "summary": "RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in\nvarious applications like intelligent surveillance. However, as the number of\nmodalities increases, the required data storage and transmission costs also\ndouble. Therefore, efficient RGB-IR data compression is essential. This work\nproposes a joint compression framework for RGB-IR image pair. Specifically, to\nfully utilize cross-modality prior information for accurate context probability\nmodeling within and between modalities, we propose a Channel-wise\nCross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context\nExtraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are\ndesigned for extracting and aggregating the global low-frequency information\nfrom both modalities, which assist the model in predicting entropy parameters\nmore accurately. Experimental results demonstrate that our approach outperforms\nexisting RGB-IR image pair and single-modality compression methods on LLVIP and\nKAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate\nsaving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec\npresented at CVPR 2022.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eRGB-IR\u56fe\u50cf\u5bf9\u8054\u5408\u538b\u7f29\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u71b5\u6a21\u578b\u548c\u4f4e\u9891\u4fe1\u606f\u63d0\u53d6\u5757\u4f18\u5316\u538b\u7f29\u6548\u7387\u3002", "motivation": "\u968f\u7740RGB\u548c\u7ea2\u5916\u56fe\u50cf\u5728\u591a\u6a21\u6001\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u589e\u52a0\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u901a\u9053\u5f0f\u8de8\u6a21\u6001\u71b5\u6a21\u578b\uff08CCEM\uff09\uff0c\u5305\u542b\u4f4e\u9891\u4e0a\u4e0b\u6587\u63d0\u53d6\u5757\uff08LCEB\uff09\u548c\u4f4e\u9891\u4e0a\u4e0b\u6587\u878d\u5408\u5757\uff08LCFB\uff09\uff0c\u7528\u4e8e\u8de8\u6a21\u6001\u4fe1\u606f\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728LLVIP\u548cKAIST\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728LLVIP\u6570\u636e\u96c6\u4e0a\u6bd4\u7279\u7387\u8282\u7701\u4e8623.1%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u4fe1\u606f\u5229\u7528\u663e\u8457\u63d0\u5347\u4e86RGB-IR\u56fe\u50cf\u5bf9\u7684\u538b\u7f29\u6548\u7387\u3002"}}
{"id": "2506.21632", "pdf": "https://arxiv.org/pdf/2506.21632", "abs": "https://arxiv.org/abs/2506.21632", "authors": ["Da Li", "Donggang Jia", "Markus Hadwiger", "Ivan Viola"], "title": "SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model", "categories": ["cs.GR"], "comment": null, "summary": "Reconstructing an interactive human avatar and the background from a\nmonocular video of a dynamic human scene is highly challenging. In this work we\nadopt a strategy of point cloud decoupling and joint optimization to achieve\nthe decoupled reconstruction of backgrounds and human bodies while preserving\nthe interactivity of human motion. We introduce a position texture to subdivide\nthe Skinned Multi-Person Linear (SMPL) body model's surface and grow the human\npoint cloud. To capture fine details of human dynamics and deformations, we\nincorporate a convolutional neural network structure to predict human body\npoint cloud features based on texture. This strategy makes our approach free of\nhyperparameter tuning for densification and efficiently represents human points\nwith half the point cloud of HUGS. This approach ensures high-quality human\nreconstruction and reduces GPU resource consumption during training. As a\nresult, our method surpasses the previous state-of-the-art HUGS in\nreconstruction metrics while maintaining the ability to generalize to novel\nposes and views. Furthermore, our technique achieves real-time rendering at\nover 100 FPS, $\\sim$6$\\times$ the HUGS speed using only Linear Blend Skinning\n(LBS) weights for human transformation. Additionally, this work demonstrates\nthat this framework can be extended to animal scene reconstruction when an\naccurately-posed model of an animal is available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u70b9\u4e91\u89e3\u8026\u548c\u8054\u5408\u4f18\u5316\u7b56\u7565\uff0c\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u4ea4\u4e92\u5f0f\u4eba\u4f53\u548c\u80cc\u666f\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u91cd\u5efa\u548c\u5b9e\u65f6\u6e32\u67d3\u3002", "motivation": "\u4ece\u5355\u76ee\u52a8\u6001\u573a\u666f\u89c6\u9891\u4e2d\u91cd\u5efa\u4ea4\u4e92\u5f0f\u4eba\u4f53\u548c\u80cc\u666f\u662f\u4e00\u4e2a\u9ad8\u5ea6\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u70b9\u4e91\u89e3\u8026\u548c\u8054\u5408\u4f18\u5316\u7b56\u7565\uff0c\u5f15\u5165\u4f4d\u7f6e\u7eb9\u7406\u7ec6\u5206SMPL\u6a21\u578b\u8868\u9762\u751f\u6210\u4eba\u4f53\u70b9\u4e91\uff0c\u5e76\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u4eba\u4f53\u70b9\u4e91\u7279\u5f81\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u6307\u6807\u4e0a\u4f18\u4e8eHUGS\uff0c\u5b9e\u73b0\u4e86100 FPS\u7684\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\uff0c\u5e76\u5728GPU\u8d44\u6e90\u6d88\u8017\u4e0a\u66f4\u9ad8\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\uff0c\u8fd8\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u7269\u573a\u666f\u91cd\u5efa\u3002"}}
{"id": "2506.22227", "pdf": "https://arxiv.org/pdf/2506.22227", "abs": "https://arxiv.org/abs/2506.22227", "authors": ["Simone D'Agostino", "Marco Massarotto", "Tristan Torchet", "Filippo Moro", "Niccol\u00f2 Castellani", "Laurent Grenouillet", "Yann Beilliard", "David Esseni", "Melika Payvand", "Elisa Vianello"], "title": "Unified Memcapacitor-Memristor Memory for Synaptic Weights and Neuron Temporal Dynamics", "categories": ["cs.ET", "cs.NE", "eess.SP"], "comment": "2 pages, accepted and discussed at Silicon Nanoelectronics Workshop\n  2025", "summary": "We present a fabricated and experimentally characterized memory stack that\nunifies memristive and memcapacitive behavior. Exploiting this dual\nfunctionality, we design a circuit enabling simultaneous control of spatial and\ntemporal dynamics in recurrent spiking neural networks (RSNNs). Hardware-aware\nsimulations highlight its promise for efficient neuromorphic processing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5fc6\u963b\u548c\u5fc6\u5bb9\u884c\u4e3a\u7684\u8bb0\u5fc6\u5806\u6808\uff0c\u8bbe\u8ba1\u4e86\u80fd\u591f\u540c\u65f6\u63a7\u5236RSNN\u65f6\u7a7a\u52a8\u6001\u7684\u7535\u8def\uff0c\u786c\u4ef6\u6a21\u62df\u663e\u793a\u4e86\u5176\u5728\u795e\u7ecf\u5f62\u6001\u5904\u7406\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u5fc6\u963b\u548c\u5fc6\u5bb9\u884c\u4e3a\u7684\u7edf\u4e00\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u9a8c\u8868\u5f81\u4e86\u7ed3\u5408\u5fc6\u963b\u548c\u5fc6\u5bb9\u884c\u4e3a\u7684\u8bb0\u5fc6\u5806\u6808\uff0c\u5e76\u5f00\u53d1\u4e86\u63a7\u5236RSNN\u65f6\u7a7a\u52a8\u6001\u7684\u7535\u8def\u3002", "result": "\u786c\u4ef6\u6a21\u62df\u8868\u660e\u8be5\u7535\u8def\u5728\u795e\u7ecf\u5f62\u6001\u5904\u7406\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5fc6\u963b\u548c\u5fc6\u5bb9\u884c\u4e3a\u7684\u7ed3\u5408\uff0c\u4e3a\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u65b9\u6848\u3002"}}
{"id": "2506.22035", "pdf": "https://arxiv.org/pdf/2506.22035", "abs": "https://arxiv.org/abs/2506.22035", "authors": ["Qiqi GU", "Chenpeng Wu", "Heng Shi", "Jianguo Yao"], "title": "SPTCStencil: Unleashing Sparse Tensor Cores for Stencil Computation via Strided Swap", "categories": ["cs.DC"], "comment": null, "summary": "Stencil computation, a pivotal numerical method in science and engineering,\niteratively updates grid points using weighted neighbor contributions and\nexhibits strong parallelism for multi-core processors. Current optimization\ntechniques targeting conducting stencil computation on tensor core accelerators\nincur substantial overheads due to redundant zero-padding during the\ntransformation to matrix multiplication. To address this, we introduce a sparse\ncomputation paradigm that eliminates inefficiencies by exploiting specialized\nhardware units.\n  This paper exploits the sparsity in these matrices as a feature and presents\nSPTCStencil, a high-performance stencil computation system accelerated by\nSparse Tensor Core (SpTCs). SPTCStencil is the first to harness SpTCs for\nacceleration beyond deep learning domains. First, Our approach generalizes an\nefficient transformation of stencil computation into matrix multiplications and\nspecializes this conversion for SpTC compatibility through a novel\nsparsification strategy. Furthermore, SPTCStencil incorporates a\nhigh-performance GPU kernel with systematic optimizations designed to maximize\nefficiency on SpTCs. Experimental evaluations demonstrate that SPTCStencil\n5.46$\\times$ and Tensor Core-based approaches by 2.00$\\times$ on average.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPTCStencil\u7684\u9ad8\u6027\u80fd\u6a21\u677f\u8ba1\u7b97\u7cfb\u7edf\uff0c\u5229\u7528\u7a00\u758f\u5f20\u91cf\u6838\u5fc3\uff08SpTCs\uff09\u52a0\u901f\u4f20\u7edf\u6a21\u677f\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u6a21\u677f\u8ba1\u7b97\u8f6c\u6362\u4e3a\u77e9\u9635\u4e58\u6cd5\u65f6\u56e0\u5197\u4f59\u96f6\u586b\u5145\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u7a00\u758f\u8ba1\u7b97\u8303\u5f0f\u53ef\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7a00\u758f\u5316\u7b56\u7565\u5c06\u6a21\u677f\u8ba1\u7b97\u9ad8\u6548\u8f6c\u6362\u4e3a\u77e9\u9635\u4e58\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9488\u5bf9SpTC\u4f18\u5316\u7684GPU\u5185\u6838\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPTCStencil\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e865.46\u500d\uff0c\u4f18\u4e8e\u57fa\u4e8e\u5f20\u91cf\u6838\u5fc3\u7684\u65b9\u6cd5\u3002", "conclusion": "SPTCStencil\u9996\u6b21\u5c06SpTC\u6280\u672f\u5e94\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4e4b\u5916\u7684\u9886\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u677f\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.21901", "pdf": "https://arxiv.org/pdf/2506.21901", "abs": "https://arxiv.org/abs/2506.21901", "authors": ["James Pan", "Guoliang Li"], "title": "A Survey of LLM Inference Systems", "categories": ["cs.DB"], "comment": "25 pages", "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u63a8\u7406\u7cfb\u7edf\u7684\u6280\u672f\uff0c\u5305\u62ec\u8bf7\u6c42\u5904\u7406\u3001\u6a21\u578b\u4f18\u5316\u3001\u5185\u5b58\u7ba1\u7406\u7b49\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u6280\u672f\u5982\u4f55\u901a\u8fc7\u8d1f\u8f7d\u9884\u6d4b\u548c\u81ea\u9002\u5e94\u673a\u5236\u514b\u670d\u81ea\u56de\u5f52\u751f\u6210\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740LLM\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bbe\u8ba1\u9ad8\u6027\u80fd\u63a8\u7406\u7cfb\u7edf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u6280\u672f\u7684\u7cfb\u7edf\u5316\u5206\u6790\u548c\u6bd4\u8f83\u3002", "method": "\u8bba\u6587\u4ece\u8bf7\u6c42\u5904\u7406\u7684\u7b97\u6cd5\u548c\u64cd\u4f5c\u7b26\u5f00\u59cb\uff0c\u9010\u6b65\u63a2\u8ba8\u6a21\u578b\u4f18\u5316\u3001\u6267\u884c\u6280\u672f\uff08\u5982\u5185\u6838\u8bbe\u8ba1\u3001\u6279\u5904\u7406\u548c\u8c03\u5ea6\uff09\u4ee5\u53ca\u5185\u5b58\u7ba1\u7406\u6280\u672f\uff08\u5982\u5206\u9875\u5185\u5b58\u3001\u91cf\u5316\u7b49\uff09\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u6280\u672f\u4f9d\u8d56\u4e8e\u8d1f\u8f7d\u9884\u6d4b\u3001\u81ea\u9002\u5e94\u673a\u5236\u548c\u6210\u672c\u964d\u4f4e\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u63a8\u7406\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u5355\u526f\u672c\u548c\u591a\u526f\u672c\u63a8\u7406\u7cfb\u7edf\u7684\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.21780", "pdf": "https://arxiv.org/pdf/2506.21780", "abs": "https://arxiv.org/abs/2506.21780", "authors": ["Anya Osborne", "Sabrina Fielder", "Lee Taber", "Tara Lamb", "Joshua McVeigh-Schultz", "Katherine Isbister"], "title": "Avatars and Environments for Meetings in Social VR: What Styles and Choices Matter to People in Group Creativity Tasks?", "categories": ["cs.HC"], "comment": null, "summary": "Due to the COVID-19 pandemic, many professional entities shifted toward\nremote collaboration and video conferencing (VC) tools. Social virtual reality\n(VR) platforms present an alternative to VC for meetings and collaborative\nactivities. Well-crafted social VR environments could enhance feelings of\nco-presence and togetherness at meetings, helping reduce the need for\ncarbon-intensive travel to face-to-face meetings. This research contributes to\ncreating meeting tools in VR by exploring the effects of avatar styles and\nvirtual environments on groups creative performance using the Mozilla Hubs\nplatform. We present the results of two sequential studies. Study One surveys\navatar and environment preferences in various VR meeting contexts (N=87). Study\nTwo applies these findings to the design of a between-subjects and\nwithin-subjects research where participants (N=40) perform creativity tasks in\npairs as embodied avatars in different virtual settings using VR headsets. We\ndiscuss the design implications of avatar appearances and meeting settings on\nteamwork.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u793e\u4ea4\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u5e73\u53f0\u5728\u8fdc\u7a0b\u4f1a\u8bae\u4e2d\u7684\u6f5c\u529b\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u865a\u62df\u73af\u5883\u548c\u5934\u50cf\u98ce\u683c\u5bf9\u56e2\u961f\u521b\u9020\u529b\u7684\u5f71\u54cd\u3002", "motivation": "COVID-19\u75ab\u60c5\u4fc3\u4f7f\u8fdc\u7a0b\u534f\u4f5c\u5de5\u5177\u9700\u6c42\u6fc0\u589e\uff0c\u793e\u4ea4VR\u5e73\u53f0\u53ef\u80fd\u63d0\u4f9b\u6bd4\u89c6\u9891\u4f1a\u8bae\u66f4\u5177\u4e34\u573a\u611f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u51cf\u5c11\u9762\u5bf9\u9762\u4f1a\u8bae\u7684\u78b3\u8db3\u8ff9\u3002", "method": "\u901a\u8fc7\u4e24\u9879\u8fde\u7eed\u7814\u7a76\uff1a\u7814\u7a76\u4e00\u8c03\u67e5\u4e8687\u540d\u7528\u6237\u5bf9VR\u4f1a\u8bae\u4e2d\u5934\u50cf\u548c\u73af\u5883\u7684\u504f\u597d\uff1b\u7814\u7a76\u4e8c\u572840\u540d\u53c2\u4e0e\u8005\u4e2d\u6d4b\u8bd5\u4e86\u4e0d\u540c\u865a\u62df\u73af\u5883\u548c\u5934\u50cf\u5bf9\u521b\u9020\u529b\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3aVR\u4f1a\u8bae\u4e2d\u5934\u50cf\u548c\u865a\u62df\u73af\u5883\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u4ee5\u4f18\u5316\u56e2\u961f\u534f\u4f5c\u4f53\u9a8c\u3002", "conclusion": "\u793e\u4ea4VR\u5e73\u53f0\u5728\u589e\u5f3a\u56e2\u961f\u534f\u4f5c\u548c\u51cf\u5c11\u78b3\u6392\u653e\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5934\u50cf\u548c\u73af\u5883\u8bbe\u8ba1\u662f\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2506.22323", "pdf": "https://arxiv.org/pdf/2506.22323", "abs": "https://arxiv.org/abs/2506.22323", "authors": ["Alessio Di Santo"], "title": "Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin America", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.OS", "cs.PL"], "comment": null, "summary": "A sophisticated malspam campaign was recently uncovered targeting Latin\nAmerican countries, with a particular focus on Brazil. This operation utilizes\na highly deceptive phishing email to trick users into executing a malicious MSI\nfile, initiating a multi-stage infection. The core of the attack leverages DLL\nside-loading, where a legitimate executable from Valve Corporation is used to\nload a trojanized DLL, thereby bypassing standard security defenses.\n  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is\ncapable of a wide range of malicious activities. It is designed to steal\nsensitive browser-stored credentials and banking information, the latter\nthrough fake login windows mimicking well-known Brazilian banks. The threat\nestablishes persistence by modifying the Windows registry , captures user\nkeystrokes through keylogging , and exfiltrates stolen data to a\nCommand-and-Control (C2) server using encrypted payloads. Despite its advanced\ncapabilities, the malware code exhibits signs of rushed development, with\ninefficiencies and poor error handling that suggest the threat actors\nprioritized rapid deployment over meticulous design. Nonetheless, the campaign\nextensive reach and sophisticated mechanisms pose a serious and immediate\nthreat to the targeted regions, underscoring the need for robust cybersecurity\ndefenses.", "AI": {"tldr": "\u4e00\u7bc7\u8bba\u6587\u5206\u6790\u4e86\u9488\u5bf9\u62c9\u4e01\u7f8e\u6d32\uff08\u5c24\u5176\u662f\u5df4\u897f\uff09\u7684\u6076\u610f\u8f6f\u4ef6\u6d3b\u52a8\uff0c\u4f7f\u7528\u9493\u9c7c\u90ae\u4ef6\u548cDLL\u4fa7\u52a0\u8f7d\u6280\u672f\u4f20\u64ad\u4e00\u79cd\u540d\u4e3aBlotchyQuasar\u7684\u6076\u610f\u8f6f\u4ef6\u53d8\u79cd\uff0c\u5177\u5907\u591a\u79cd\u6076\u610f\u529f\u80fd\uff0c\u5305\u62ec\u7a83\u53d6\u94f6\u884c\u4fe1\u606f\u3002", "motivation": "\u63ed\u793a\u5e76\u5206\u6790\u8fd1\u671f\u9488\u5bf9\u62c9\u4e01\u7f8e\u6d32\u7684\u590d\u6742\u6076\u610f\u8f6f\u4ef6\u6d3b\u52a8\uff0c\u4ee5\u589e\u5f3a\u5bf9\u6b64\u7c7b\u5a01\u80c1\u7684\u8ba4\u77e5\u548c\u9632\u5fa1\u80fd\u529b\u3002", "method": "\u7814\u7a76\u6076\u610f\u8f6f\u4ef6\u7684\u5206\u53d1\u65b9\u5f0f\uff08\u9493\u9c7c\u90ae\u4ef6\u548c\u591a\u9636\u6bb5\u611f\u67d3\uff09\u3001\u52a0\u8f7d\u6280\u672f\uff08DLL\u4fa7\u52a0\u8f7d\uff09\u4ee5\u53ca\u6076\u610f\u529f\u80fd\uff08\u5982\u51ed\u8bc1\u7a83\u53d6\u548c\u952e\u76d8\u8bb0\u5f55\uff09\u3002", "result": "\u53d1\u73b0BlotchyQuasar\u867d\u7136\u5f00\u53d1\u4ed3\u4fc3\uff0c\u4f46\u529f\u80fd\u5f3a\u5927\uff0c\u5a01\u80c1\u4e25\u91cd\u3002", "conclusion": "\u5f3a\u8c03\u9700\u52a0\u5f3a\u7f51\u7edc\u5b89\u5168\u9632\u5fa1\u4ee5\u5e94\u5bf9\u6b64\u7c7b\u9ad8\u7ea7\u5a01\u80c1\u3002"}}
{"id": "2506.22156", "pdf": "https://arxiv.org/pdf/2506.22156", "abs": "https://arxiv.org/abs/2506.22156", "authors": ["Mattia Ricchi", "Fabrizio Alfonsi", "Camilla Marella", "Marco Barbieri", "Alessandra Retico", "Leonardo Brizi", "Alessandro Gabrielli", "Claudia Testa"], "title": "Hardware acceleration for ultra-fast Neural Network training on FPGA for MRF map reconstruction", "categories": ["cs.AR", "cs.CV", "physics.ins-det"], "comment": "8 pages, 2 figures, to be published in conference proceedings of SDPS\n  2024: 2024 International Conference of the Society for Design and Process\n  Science on Advances and Challenges of Applying AI/GenAI in Design and Process\n  Science", "summary": "Magnetic Resonance Fingerprinting (MRF) is a fast quantitative MR Imaging\ntechnique that provides multi-parametric maps with a single acquisition. Neural\nNetworks (NNs) accelerate reconstruction but require significant resources for\ntraining. We propose an FPGA-based NN for real-time brain parameter\nreconstruction from MRF data. Training the NN takes an estimated 200 seconds,\nsignificantly faster than standard CPU-based training, which can be up to 250\ntimes slower. This method could enable real-time brain analysis on mobile\ndevices, revolutionizing clinical decision-making and telemedicine.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u78c1\u5171\u632f\u6307\u7eb9\u6570\u636e\u4e2d\u5b9e\u65f6\u91cd\u5efa\u5927\u8111\u53c2\u6570\uff0c\u8bad\u7ec3\u65f6\u95f4\u663e\u8457\u7f29\u77ed\u3002", "motivation": "\u4f20\u7edfMRF\u6280\u672f\u9700\u8981\u5927\u91cf\u8d44\u6e90\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u5e94\u7528\u3002", "method": "\u91c7\u7528FPGA\u52a0\u901f\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u8bad\u7ec3\u65f6\u95f4\u964d\u81f3200\u79d2\uff0c\u6bd4\u4f20\u7edfCPU\u65b9\u6cd5\u5feb250\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u5b9e\u73b0\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u5927\u8111\u5206\u6790\uff0c\u63a8\u52a8\u4e34\u5e8a\u51b3\u7b56\u548c\u8fdc\u7a0b\u533b\u7597\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.22185", "pdf": "https://arxiv.org/pdf/2506.22185", "abs": "https://arxiv.org/abs/2506.22185", "authors": ["Matteo Esposito", "Alexander Bakhtin", "Noman Ahmad", "Mikel Robredo", "Ruoyu Su", "Valentina Lenarduzzi", "Davide Taibi"], "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMAPE-K\u548c\u667a\u80fd\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5fae\u670d\u52a1\u7684\u81ea\u4e3b\u5f02\u5e38\u68c0\u6d4b\u4e0e\u4fee\u590d\uff0c\u63d0\u5347\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "motivation": "\u5fae\u670d\u52a1\u7684\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u5e26\u6765\u5b89\u5168\u548c\u7ba1\u7406\u6311\u6218\uff0c\u5a01\u80c1\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528MAPE-K\u6846\u67b6\uff0c\u7ed3\u5408\u667a\u80fd\u4ee3\u7406\uff0c\u5b9e\u73b0\u81ea\u4e3b\u5f02\u5e38\u68c0\u6d4b\u4e0e\u4fee\u590d\u3002", "result": "\u4e3a\u884c\u4e1a\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u5b9a\u5236\u5316\uff0c\u9002\u7528\u4e8e\u63d0\u5347\u5fae\u670d\u52a1\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2506.22144", "pdf": "https://arxiv.org/pdf/2506.22144", "abs": "https://arxiv.org/abs/2506.22144", "authors": ["Lucie Guillou", "Arnaud Sangnier", "Nathalie Sznajder"], "title": "Wait-Only Broadcast Protocols are Easier to Verify", "categories": ["cs.LO"], "comment": "Long version of a paper accepted to appear at MFCS 2025", "summary": "We study networks of processes that all execute the same finite-state\nprotocol and communicate via broadcasts. We are interested in two problems with\na parameterized number of processes: the synchronization problem which asks\nwhether there is an execution which puts all processes on a given state; and\nthe repeated coverability problem which asks if there is an infinite execution\nwhere a given transition is taken infinitely often. Since both problems are\nundecidable in the general case, we investigate those problems when the\nprotocol is Wait-Only, i.e., it has no state from which a process can both\nbroadcast and receive messages. We establish that the synchronization problem\nbecomes Ackermann-complete, and the repeated coverability problem is in\nEXPSPACE, and PSPACE-hard.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6267\u884c\u76f8\u540c\u6709\u9650\u72b6\u6001\u534f\u8bae\u5e76\u901a\u8fc7\u5e7f\u64ad\u901a\u4fe1\u7684\u8fdb\u7a0b\u7f51\u7edc\uff0c\u91cd\u70b9\u5206\u6790\u540c\u6b65\u95ee\u9898\u548c\u91cd\u590d\u8986\u76d6\u95ee\u9898\u7684\u53ef\u89e3\u6027\u3002\u5728Wait-Only\u534f\u8bae\u4e0b\uff0c\u540c\u6b65\u95ee\u9898\u4e3aAckermann-Complete\uff0c\u91cd\u590d\u8986\u76d6\u95ee\u9898\u5728EXPSPACE\u4e2d\u4e14PSPACE-hard\u3002", "motivation": "\u63a2\u7d22\u53c2\u6570\u5316\u8fdb\u7a0b\u6570\u91cf\u4e0b\u7684\u540c\u6b65\u548c\u91cd\u590d\u8986\u76d6\u95ee\u9898\u7684\u53ef\u89e3\u6027\uff0c\u5c24\u5176\u5728\u534f\u8bae\u9650\u5236\u4e3aWait-Only\u65f6\u7684\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76Wait-Only\u534f\u8bae\u4e0b\u540c\u6b65\u548c\u91cd\u590d\u8986\u76d6\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "result": "\u540c\u6b65\u95ee\u9898\u4e3aAckermann-Complete\uff0c\u91cd\u590d\u8986\u76d6\u95ee\u9898\u5728EXPSPACE\u4e2d\u4e14PSPACE-hard\u3002", "conclusion": "Wait-Only\u534f\u8bae\u9650\u5236\u4e86\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u4f7f\u5f97\u540c\u6b65\u548c\u91cd\u590d\u8986\u76d6\u95ee\u9898\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u89e3\u6216\u90e8\u5206\u53ef\u89e3\u3002"}}
{"id": "2506.22223", "pdf": "https://arxiv.org/pdf/2506.22223", "abs": "https://arxiv.org/abs/2506.22223", "authors": ["Felipe Valle Quiroz", "Johan Elfing", "Joel P\u00e5lsson", "Elena Haller", "Oscar Amador Molina"], "title": "V2X Intention Sharing for Cooperative Electrically Power-Assisted Cycles", "categories": ["cs.NI"], "comment": "Accepted into FAST-zero'25: 8th International Symposium on Future\n  Active Safety Technology toward zero traffic accidents", "summary": "This paper introduces a novel intention-sharing mechanism for Electrically\nPower-Assisted Cycles (EPACs) within V2X communication frameworks, enhancing\nthe ETSI VRU Awareness Message (VAM) protocol. The method replaces discrete\npredicted trajectory points with a compact elliptical geographical area\nrepresentation derived via quadratic polynomial fitting and Least Squares\nMethod (LSM). This approach encodes trajectory predictions with fixed-size data\npayloads, independent of the number of forecasted points, enabling\nhigher-frequency transmissions and improved network reliability. Simulation\nresults demonstrate superior inter-packet gap (IPG) performance compared to\nstandard ETSI VAMs, particularly under constrained communication conditions. A\nphysical experiment validates the feasibility of real-time deployment on\nembedded systems. The method supports scalable, low-latency intention sharing,\ncontributing to cooperative perception and enhanced safety for vulnerable road\nusers in connected and automated mobility ecosystems. Finally, we discuss the\nviability of LSM and open the door to other methods for prediction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u610f\u56fe\u5171\u4eab\u673a\u5236\uff0c\u7528\u4e8eV2X\u901a\u4fe1\u6846\u67b6\u4e2d\u7684\u7535\u52a8\u52a9\u529b\u81ea\u884c\u8f66\uff08EPAC\uff09\uff0c\u901a\u8fc7\u692d\u5706\u533a\u57df\u8868\u793a\u548c\u6700\u5c0f\u4e8c\u4e58\u6cd5\u4f18\u5316\u8f68\u8ff9\u9884\u6d4b\uff0c\u63d0\u5347\u7f51\u7edc\u53ef\u9760\u6027\u548c\u4f20\u8f93\u9891\u7387\u3002", "motivation": "\u65e8\u5728\u4e3aEPACs\u5728V2X\u901a\u4fe1\u4e2d\u63d0\u4f9b\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u610f\u56fe\u5171\u4eab\uff0c\u4ee5\u63d0\u5347\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u5b89\u5168\u548c\u534f\u4f5c\u611f\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5\u62df\u5408\u4e8c\u6b21\u591a\u9879\u5f0f\uff0c\u5c06\u79bb\u6563\u9884\u6d4b\u8f68\u8ff9\u70b9\u66ff\u6362\u4e3a\u7d27\u51d1\u7684\u692d\u5706\u5730\u7406\u533a\u57df\u8868\u793a\uff0c\u56fa\u5b9a\u6570\u636e\u8d1f\u8f7d\u4ee5\u5b9e\u73b0\u9ad8\u9891\u4f20\u8f93\u3002", "result": "\u4eff\u771f\u663e\u793a\u5728\u53d7\u9650\u901a\u4fe1\u6761\u4ef6\u4e0b\u4f18\u4e8e\u6807\u51c6ETSI VAMs\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5d4c\u5165\u5f0f\u7cfb\u7edf\u5b9e\u65f6\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u53ef\u6269\u5c55\u3001\u4f4e\u5ef6\u8fdf\u7684\u610f\u56fe\u5171\u4eab\uff0c\u4e3a\u5f00\u653e\u5f0f\u9884\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7814\u7a76\u57fa\u7840\u3002"}}
{"id": "2506.21862", "pdf": "https://arxiv.org/pdf/2506.21862", "abs": "https://arxiv.org/abs/2506.21862", "authors": ["Boyuan Sun", "Jiaxing Zhao", "Xihan Wei", "Qibin Hou"], "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "comment": "21 pages, 4 figures, 7 tables", "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.", "AI": {"tldr": "LLaVA-Scissor\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684token\u538b\u7f29\u7b56\u7565\uff0c\u7528\u4e8e\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bed\u4e49\u8fde\u63a5\u7ec4\u4ef6\uff08SCC\uff09\u7684\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709token\u538b\u7f29\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6570\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u83b7\u6240\u6709\u8bed\u4e49\u533a\u57df\u4e14\u6613\u5bfc\u81f4\u5197\u4f59\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u6b65\u9aa4\u65f6\u7a7atoken\u538b\u7f29\u7b56\u7565\uff0c\u5229\u7528SCC\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u57df\u5206\u914dtoken\u5230\u4e0d\u540c\u8bed\u4e49\u533a\u57df\uff0c\u786e\u4fdd\u5168\u9762\u8bed\u4e49\u8986\u76d6\u3002", "result": "\u5728\u89c6\u9891\u95ee\u7b54\u3001\u957f\u89c6\u9891\u7406\u89e3\u7b49\u591a\u9879\u8bc4\u6d4b\u4e2d\uff0cLLaVA-Scissor\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u4f4etoken\u4fdd\u7559\u7387\u4e0b\u3002", "conclusion": "LLaVA-Scissor\u901a\u8fc7SCC\u5b9e\u73b0\u9ad8\u6548token\u538b\u7f29\uff0c\u63d0\u5347\u89c6\u9891\u7406\u89e3\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.21633", "pdf": "https://arxiv.org/pdf/2506.21633", "abs": "https://arxiv.org/abs/2506.21633", "authors": ["Aobo Li", "Zhengxin Lei", "Jiangtao Wei", "Feng Xu"], "title": "SAR-GS: 3D Gaussian Splatting for Synthetic Aperture Radar Target Reconstruction", "categories": ["cs.GR"], "comment": null, "summary": "Three-dimensional target reconstruction from synthetic aperture radar (SAR)\nimagery is crucial for interpreting complex scattering information in SAR data.\nHowever, the intricate electromagnetic scattering mechanisms inherent to SAR\nimaging pose significant reconstruction challenges. Inspired by the remarkable\nsuccess of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this\npaper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR)\nspecifically designed for SAR target reconstruction. Our approach combines\nGaussian splatting with the Mapping and Projection Algorithm to compute\nscattering intensities of Gaussian primitives and generate simulated SAR images\nthrough SDGR. Subsequently, the loss function between the rendered image and\nthe ground truth image is computed to optimize the Gaussian primitive\nparameters representing the scene, while a custom CUDA gradient flow is\nemployed to replace automatic differentiation for accelerated gradient\ncomputation. Through experiments involving the rendering of simplified\narchitectural targets and SAR images of multiple vehicle targets, we validate\nthe imaging rationality of SDGR on simulated SAR imagery. Furthermore, the\neffectiveness of our method for target reconstruction is demonstrated on both\nsimulated and real-world datasets containing multiple vehicle targets, with\nquantitative evaluations conducted to assess its reconstruction performance.\nExperimental results indicate that our approach can effectively reconstruct the\ngeometric structures and scattering properties of targets, thereby providing a\nnovel solution for 3D reconstruction in the field of SAR imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bSAR\u76ee\u6807\u91cd\u5efa\u65b9\u6cd5SDGR\uff0c\u7ed3\u5408\u9ad8\u65af\u6e85\u5c04\u4e0e\u6620\u5c04\u6295\u5f71\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u9ad8\u65af\u57fa\u5143\u53c2\u6570\u5b9e\u73b0\u9ad8\u65483D\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3SAR\u56fe\u50cf\u4e2d\u590d\u6742\u7535\u78c1\u6563\u5c04\u673a\u5236\u5e26\u6765\u7684\u91cd\u5efa\u6311\u6218\uff0c\u53d7\u5149\u5b66\u57df3D\u9ad8\u65af\u6e85\u5c04\u6210\u529f\u7684\u542f\u53d1\u3002", "method": "\u7ed3\u5408\u9ad8\u65af\u6e85\u5c04\u4e0e\u6620\u5c04\u6295\u5f71\u7b97\u6cd5\uff0c\u8ba1\u7b97\u9ad8\u65af\u57fa\u5143\u7684\u6563\u5c04\u5f3a\u5ea6\uff0c\u901a\u8fc7SDGR\u751f\u6210\u6a21\u62dfSAR\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u81ea\u5b9a\u4e49CUDA\u68af\u5ea6\u6d41\u4f18\u5316\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SDGR\u5728\u6a21\u62df\u548c\u771f\u5b9eSAR\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\uff0c\u80fd\u51c6\u786e\u91cd\u5efa\u76ee\u6807\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u6563\u5c04\u7279\u6027\u3002", "conclusion": "SDGR\u4e3aSAR\u56fe\u50cf3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6848\uff0c\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2506.22122", "pdf": "https://arxiv.org/pdf/2506.22122", "abs": "https://arxiv.org/abs/2506.22122", "authors": ["Gianluca Kosmella", "Ripalta Stabile", "Jaron Sanders"], "title": "In situ fine-tuning of in silico trained Optical Neural Networks", "categories": ["cs.NE", "cs.ET", "eess.SP"], "comment": null, "summary": "Optical Neural Networks (ONNs) promise significant advantages over\ntraditional electronic neural networks, including ultrafast computation, high\nbandwidth, and low energy consumption, by leveraging the intrinsic capabilities\nof photonics. However, training ONNs poses unique challenges, notably the\nreliance on simplified in silico models whose trained parameters must\nsubsequently be mapped to physical hardware. This process often introduces\ninaccuracies due to discrepancies between the idealized digital model and the\nphysical ONN implementation, particularly stemming from noise and fabrication\nimperfections.\n  In this paper, we analyze how noise misspecification during in silico\ntraining impacts ONN performance and we introduce Gradient-Informed Fine-Tuning\n(GIFT), a lightweight algorithm designed to mitigate this performance\ndegradation. GIFT uses gradient information derived from the noise structure of\nthe ONN to adapt pretrained parameters directly in situ, without requiring\nexpensive retraining or complex experimental setups. GIFT comes with formal\nconditions under which it improves ONN performance.\n  We also demonstrate the effectiveness of GIFT via simulation on a five-layer\nfeed forward ONN trained on the MNIST digit classification task. GIFT achieves\nup to $28\\%$ relative accuracy improvement compared to the baseline performance\nunder noise misspecification, without resorting to costly retraining. Overall,\nGIFT provides a practical solution for bridging the gap between simplified\ndigital models and real-world ONN implementations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGIFT\u7684\u8f7b\u91cf\u7ea7\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5149\u5b66\u795e\u7ecf\u7f51\u7edc(ONN)\u8bad\u7ec3\u4e2d\u56e0\u566a\u58f0\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002GIFT\u5229\u7528\u68af\u5ea6\u4fe1\u606f\u4f18\u5316\u9884\u8bad\u7ec3\u53c2\u6570\uff0c\u65e0\u9700\u590d\u6742\u91cd\u65b0\u8bad\u7ec3\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5728MNIST\u4efb\u52a1\u4e2d\uff0cGIFT\u76f8\u5bf9\u57fa\u7ebf\u63d0\u5347\u4e8628%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u7531\u4e8e\u7269\u7406\u5b9e\u73b0\u4e2d\u7684\u566a\u58f0\u548c\u5236\u9020\u7f3a\u9677\uff0c\u5149\u5b66\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u5b57\u6a21\u578b\u4e0e\u5b9e\u9645\u786c\u4ef6\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faGradient-Informed Fine-Tuning (GIFT)\u7b97\u6cd5\uff0c\u5229\u7528ONN\u7684\u566a\u58f0\u7ed3\u6784\u68af\u5ea6\u4fe1\u606f\uff0c\u76f4\u63a5\u5728\u73b0\u573a\u8c03\u6574\u9884\u8bad\u7ec3\u53c2\u6570\uff0c\u907f\u514d\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u5c42\u524d\u9988ONN\u548cMNIST\u4efb\u52a1\u4e0a\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0cGIFT\u5728\u566a\u58f0\u4e0d\u5339\u914d\u60c5\u51b5\u4e0b\u5c06\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u5347\u4e8628%\u3002", "conclusion": "GIFT\u4e3a\u5f25\u8865\u6570\u5b57\u6a21\u578b\u4e0e\u5b9e\u9645ONN\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21593", "pdf": "https://arxiv.org/pdf/2506.21593", "abs": "https://arxiv.org/abs/2506.21593", "authors": ["Abu Hanif Muhammad Syarubany", "Chang Dong Yoo"], "title": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications", "categories": ["cs.IR", "cs.DB"], "comment": "Annual Conference of The Institute of Electronics and Information\n  Engineers", "summary": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems.", "AI": {"tldr": "PentaRAG\u901a\u8fc7\u4e94\u5c42\u6a21\u5757\u4f18\u5316LLM\u5728\u4f01\u4e1a\u6587\u6863\u90e8\u7f72\u4e2d\u7684\u54cd\u5e94\u901f\u5ea6\u548cGPU\u6210\u672c\uff0c\u663e\u8457\u63d0\u5347\u7f13\u5b58\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f01\u4e1a\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5feb\u901f\u54cd\u5e94\u548c\u6210\u672c\u53ef\u63a7\uff0c\u73b0\u6709RAG\u6280\u672f\u672a\u80fd\u5b8c\u5168\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e94\u5c42\u6a21\u5757\uff08\u7f13\u5b58\u3001\u8bed\u4e49\u7f13\u5b58\u3001\u8bb0\u5fc6\u53ec\u56de\u3001\u4f1a\u8bdd\u8bb0\u5fc6\u548c\u4f20\u7edf\u68c0\u7d22\u5c42\uff09\uff0c\u7ed3\u5408Mistral-8B\u7b49\u6280\u672f\u5b9e\u73b0\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\uff0cPentaRAG\u63d0\u5347\u7b54\u6848\u76f8\u4f3c\u5ea68%\uff0c\u51c6\u786e\u602716%\uff0c\u67e5\u8be2\u5ef6\u8fdf\u964d\u81f30.248\u79d2\uff0c\u541e\u5410\u91cf\u8fbe10\u4e07QPS\u3002", "conclusion": "\u5206\u5c42\u8def\u7531\u7b56\u7565\u80fd\u540c\u65f6\u5b9e\u73b0\u65b0\u9c9c\u5ea6\u3001\u901f\u5ea6\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u7ea7RAG\u7cfb\u7edf\u3002"}}
{"id": "2506.21814", "pdf": "https://arxiv.org/pdf/2506.21814", "abs": "https://arxiv.org/abs/2506.21814", "authors": ["Yuanfang Ren", "Esra Adiyeke", "Ziyuan Guan", "Zhenhong Hu", "Mackenzie J Meni", "Benjamin Shickel", "Parisa Rashidi", "Tezcan Ozrazgat-Baslanti", "Azra Bihorac"], "title": "Validation of the MySurgeryRisk Algorithm for Predicting Complications and Death after Major Surgery: A Retrospective Multicenter Study Using OneFlorida Data Trust", "categories": ["cs.HC"], "comment": "28 pages, 4 figures, 6 tables, 1 supplemental table", "summary": "Despite advances in surgical techniques and care, postoperative complications\nare prevalent and effects up to 15% of the patients who underwent a major\nsurgery. The objective of this study is to develop and validate models for\npredicting postoperative complications and death after major surgery on a large\nand multicenter dataset, following the previously validated MySurgeryRisk\nalgorithm. This retrospective, longitudinal and multicenter cohort analysis\nincluded 508,097 encounters from 366,875 adult inpatients who underwent major\nsurgeries and were admitted to healthcare institutions within the OneFlorida+\nnetwork between 01/01/2012 and 04/29/2023. We applied the validated feature\nselection and transformation approach in MySurgeryRisk models and redeveloped\neXtreme Gradient Boosting (XGBoost) models for predicting risk of postoperative\nacute kidney injury (AKI), need for intensive care unit (ICU) admission, need\nfor mechanical ventilation (MV) therapy and in-hospital mortality on a\ndevelopment set and evaluated the model performance on a validation set. Area\nunder the receiver operating characteristics curve values were obtained for\nneed for ICU admission, 0.93 (95% Confidence Interval [CI], 0.93-0.93); need\nfor MV, 0.94 (95% CI, 0.94-0.94); AKI, 0.92 (95% CI, 0.92-0.92); and\nin-hospital mortality, 0.95 (95% CI, 0.94-0.95). Area under the\nprecision-recall curve values were computed for need for ICU admission, 0.62\n(95% CI, 0.62-0.63); need for MV, 0.51 (95% CI, 0.49-0.52); AKI, 0.53 (95% CI,\n0.53-0.54); and in-hospital mortality, 0.26 (95% CI, 0.24-0.29). The\nperformance of these models is comparable to that of the previously validated\nMySurgeryRisk models, suggesting the enhanced generalizability of the models.\nPrimary procedure code and provider specialty consistently appeared as the top\ninfluential variables, providing valuable insights into the factors influencing\nsurgical outcomes.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u548c\u9a8c\u8bc1\u9884\u6d4b\u91cd\u5927\u624b\u672f\u540e\u5e76\u53d1\u75c7\u548c\u6b7b\u4ea1\u7684\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u4e2d\u5fc3\u6570\u636e\u8fdb\u884c\u91cd\u65b0\u5f00\u53d1\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u6027\u80fd\u4f18\u5f02\u4e14\u5177\u6709\u666e\u9002\u6027\u3002", "motivation": "\u672f\u540e\u5e76\u53d1\u75c7\u666e\u904d\u4e14\u5f71\u54cd\u663e\u8457\uff0c\u9700\u8981\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u4ee5\u63d0\u9ad8\u60a3\u8005\u7ba1\u7406\u3002", "method": "\u91c7\u7528MySurgeryRisk\u7b97\u6cd5\uff0c\u7ed3\u5408XGBoost\u6a21\u578b\uff0c\u5bf9\u591a\u4e2d\u5fc3\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6a21\u578b\u5728\u9884\u6d4bICU\u9700\u6c42\u3001\u673a\u68b0\u901a\u6c14\u3001\u6025\u6027\u80be\u635f\u4f24\u548c\u9662\u5185\u6b7b\u4ea1\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff08AUC 0.92-0.95\uff09\u3002", "conclusion": "\u6a21\u578b\u6027\u80fd\u4e0e\u5148\u524d\u9a8c\u8bc1\u4e00\u81f4\uff0c\u666e\u9002\u6027\u5f3a\uff0c\u624b\u672f\u4ee3\u7801\u548c\u533b\u751f\u4e13\u4e1a\u662f\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002"}}
{"id": "2506.22370", "pdf": "https://arxiv.org/pdf/2506.22370", "abs": "https://arxiv.org/abs/2506.22370", "authors": ["Carolina Carreira", "\u00c1lvaro Silva", "Alexandre Abreu", "Alexandra Mendes"], "title": "Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Students in computing education increasingly use large language models (LLMs)\nsuch as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding\ntasks, like deductive program verification, remains poorly understood. This\npaper investigates how students interact with an LLM when solving formal\nverification exercises in Dafny, a language that supports functional\ncorrectness, by allowing programmers to write formal specifications and\nautomatically verifying that the implementation satisfies the specification. We\nconducted a mixed-methods study with master's students enrolled in a formal\nmethods course. Each participant completed two verification problems, one with\naccess to a custom ChatGPT interface, that logged all interactions, and the\nother without. We identified strategies used by successful students and\nassessed the level of trust students place in LLMs. %\\todo{Our findings show\nthat something here} Our findings show that students perform significantly\nbetter when using ChatGPT; however, performance gains are tied to prompt\nquality. We conclude with practical recommendations for integrating LLMs into\nformal methods courses more effectively, including designing LLM-aware\nchallenges that promote learning rather than substitution.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5b66\u751f\u5728\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u89e3\u51b3\u5f62\u5f0f\u9a8c\u8bc1\u95ee\u9898\u65f6\u7684\u8868\u73b0\u53ca\u5176\u7b56\u7565\uff0c\u53d1\u73b0\u5b66\u751f\u5728\u4f7f\u7528ChatGPT\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u6548\u679c\u4e0e\u63d0\u793a\u8d28\u91cf\u76f8\u5173\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u5728\u652f\u6301\u5b66\u751f\u5b8c\u6210\u9ad8\u8ba4\u77e5\u9700\u6c42\u4efb\u52a1\uff08\u5982\u7a0b\u5e8f\u5f62\u5f0f\u9a8c\u8bc1\uff09\u4e2d\u7684\u6f5c\u529b\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u8ba9\u7855\u58eb\u751f\u5728\u5f62\u5f0f\u65b9\u6cd5\u8bfe\u7a0b\u4e2d\u5b8c\u6210\u4e24\u4e2a\u9a8c\u8bc1\u95ee\u9898\uff0c\u4e00\u4e2a\u4f7f\u7528\u5b9a\u5236\u7684ChatGPT\u754c\u9762\uff0c\u53e6\u4e00\u4e2a\u4e0d\u4f7f\u7528\uff0c\u5e76\u8bb0\u5f55\u6240\u6709\u4ea4\u4e92\u3002", "result": "\u4f7f\u7528ChatGPT\u7684\u5b66\u751f\u8868\u73b0\u663e\u8457\u66f4\u597d\uff0c\u4f46\u8868\u73b0\u63d0\u5347\u4e0e\u63d0\u793a\u8d28\u91cf\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u6709\u6548\u5730\u878d\u5165\u5f62\u5f0f\u65b9\u6cd5\u8bfe\u7a0b\u7684\u5b9e\u9645\u5efa\u8bae\uff0c\u5305\u62ec\u8bbe\u8ba1\u4fc3\u8fdb\u5b66\u4e60\u7684LLM\u611f\u77e5\u6311\u6218\u3002"}}
{"id": "2506.21562", "pdf": "https://arxiv.org/pdf/2506.21562", "abs": "https://arxiv.org/abs/2506.21562", "authors": ["Jun Yin", "Pengyu Zeng", "Jing Zhong", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction", "categories": ["cs.CL", "cs.AI", "cs.AR"], "comment": null, "summary": "In the architectural design process, floor plan generation is inherently\nprogressive and iterative. However, existing generative models for floor plans\nare predominantly end-to-end generation that produce an entire pixel-based\nlayout in a single pass. This paradigm is often incompatible with the\nincremental workflows observed in real-world architectural practice. To address\nthis issue, we draw inspiration from the autoregressive 'next token prediction'\nmechanism commonly used in large language models, and propose a novel 'next\nroom prediction' paradigm tailored to architectural floor plan modeling.\nExperimental evaluation indicates that FPDS demonstrates competitive\nperformance in comparison to diffusion models and Tell2Design in the\ntext-to-floorplan task, indicating its potential applicability in supporting\nfuture intelligent architectural design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u673a\u5236\u7684\u6e10\u8fdb\u5f0f\u5e73\u9762\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u7aef\u5230\u7aef\u751f\u6210\u6a21\u578b\uff0c\u66f4\u7b26\u5408\u5b9e\u9645\u5efa\u7b51\u8bbe\u8ba1\u7684\u8fed\u4ee3\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u5e73\u9762\u56fe\u751f\u6210\u6a21\u578b\u591a\u4e3a\u7aef\u5230\u7aef\u751f\u6210\uff0c\u4e0e\u5b9e\u9645\u5efa\u7b51\u8bbe\u8ba1\u7684\u6e10\u8fdb\u5f0f\u5de5\u4f5c\u6d41\u7a0b\u4e0d\u5339\u914d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7b26\u5408\u5b9e\u9645\u9700\u6c42\u7684\u65b9\u6cd5\u3002", "method": "\u501f\u9274\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u81ea\u56de\u5f52'\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b'\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd'\u4e0b\u4e00\u4e2a\u623f\u95f4\u9884\u6d4b'\u8303\u5f0f\uff0c\u7528\u4e8e\u5efa\u7b51\u5e73\u9762\u56fe\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cFPDS\u5728\u6587\u672c\u5230\u5e73\u9762\u56fe\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6269\u6563\u6a21\u578b\u548cTell2Design\u3002", "conclusion": "FPDS\u5c55\u793a\u4e86\u5728\u652f\u6301\u672a\u6765\u667a\u80fd\u5efa\u7b51\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u5728\u9002\u7528\u6027\u3002"}}
{"id": "2506.22196", "pdf": "https://arxiv.org/pdf/2506.22196", "abs": "https://arxiv.org/abs/2506.22196", "authors": ["Arnoud van der Leer", "Kobe Wullaert", "Benedikt Ahrens"], "title": "Scott's Representation Theorem and the Univalent Karoubi Envelope", "categories": ["cs.LO", "math.CT", "F.3.2; D.2.4"], "comment": "20 pages, LaTeX; submitted to the 16th International Conference on\n  Interactive Theorem Proving (ITP 2025)", "summary": "Lambek and Scott constructed a correspondence between simply-typed lambda\ncalculi and Cartesian closed categories. Scott's Representation Theorem is a\ncousin to this result for untyped lambda calculi. It states that every untyped\nlambda calculus arises from a reflexive object in some category. We present a\nformalization of Scott's Representation Theorem in univalent foundations, in\nthe (Rocq-)UniMath library. Specifically, we implement two proofs of that\ntheorem, one by Scott and one by Hyland. We also explain the role of the\nKaroubi envelope -- a categorical construction -- in the proofs and the impact\nthe chosen foundation has on this construction. Finally, we report on some\nautomation we have implemented for the reduction of $\\lambda$-terms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5728\u7edf\u4e00\u6570\u5b66\u57fa\u7840\u4e2d\u5f62\u5f0f\u5316\u4e86Scott\u7684\u8868\u793a\u5b9a\u7406\uff0c\u5b9e\u73b0\u4e86Scott\u548cHyland\u7684\u4e24\u79cd\u8bc1\u660e\uff0c\u5e76\u63a2\u8ba8\u4e86Karoubi\u5305\u7edc\u7684\u4f5c\u7528\u53ca\u57fa\u7840\u9009\u62e9\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5728\u4e00\u81f4\u578b\u6570\u5b66\u57fa\u7840\u4e0a\u5f62\u5f0f\u5316Scott\u7684\u8868\u793a\u5b9a\u7406\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u8303\u7574\u6784\u9020\u4e2d\u7684\u4f5c\u7528\u53ca\u57fa\u7840\u9009\u62e9\u7684\u5f71\u54cd\u3002", "method": "\u5728UniMath\u5e93\u4e2d\u5b9e\u73b0\u4e86Scott\u548cHyland\u7684\u4e24\u79cd\u8bc1\u660e\u65b9\u6cd5\uff0c\u8ba8\u8bba\u4e86Karoubi\u5305\u7edc\u7684\u4f5c\u7528\uff0c\u5e76\u81ea\u52a8\u5316\u4e86\u03bb-\u9879\u7684\u7b80\u5316\u3002", "result": "\u6210\u529f\u5f62\u5f0f\u5316\u4e86Scott\u5b9a\u7406\uff0c\u5c55\u793a\u4e86\u4e24\u79cd\u8bc1\u660e\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u03bb-\u9879\u7684\u81ea\u52a8\u5316\u7b80\u5316\u3002", "conclusion": "\u8bba\u6587\u5b9e\u73b0\u4e86\u5b9a\u7406\u7684\u5f62\u5f0f\u5316\uff0c\u4e3a\u7814\u7a76\u65e0\u7c7b\u578b\u03bb-\u6f14\u7b97\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5e76\u63a2\u8ba8\u4e86\u6570\u5b66\u57fa\u7840\u5bf9\u6784\u9020\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.22260", "pdf": "https://arxiv.org/pdf/2506.22260", "abs": "https://arxiv.org/abs/2506.22260", "authors": ["Douglas Dziedzorm Agbeve", "Andrey Belogaev", "Jeroen Famaey"], "title": "Design and Evaluation of IEEE 802.11ax Uplink Orthogonal Frequency Division Multiple Random Access in ns-3", "categories": ["cs.NI"], "comment": null, "summary": "Wi-Fi networks have long relied on the Enhanced Distributed Channel Access\n(EDCA) mechanism, allowing stations to compete for transmission opportunities.\nHowever, as networks become denser and emerging applications demand lower\nlatency and higher reliability, the limitations of EDCA such as overhead due to\ncontention and collisions have become more pronounced. To address these\nchallenges, Orthogonal Frequency Division Multiple Access (OFDMA) has been\nintroduced in Wi-Fi, enabling more efficient channel utilization through\nscheduled resource allocation. Furthermore, Wi-Fi 6 defines Uplink Orthogonal\nFrequency Division Multiple Random Access (UORA), a hybrid mechanism that\ncombines both scheduled and random access, balancing efficiency and\nresponsiveness in resource allocation. Despite significant research on UORA,\nmost studies rely on custom simulators that are not publicly available,\nlimiting reproducibility and preventing validation of the presented results.\nThe only known open-source UORA implementation in the ns-3 simulator exhibits\nkey limitations, such as usage of the same trigger frame (TF) to schedule\nresources for buffer status reports and data transmissions, and lack of\nsignaling for UORA configuration. In this paper, we present a fully\nstandard-compliant and open source UORA implementation that is compatible with\nns-3 version 3.38, addressing these limitations to improve resource allocation\nefficiency and adaptability. This implementation enables more accurate and\nflexible evaluation of UORA, fostering future research on Wi-Fi resource\nallocation strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u7b26\u5408\u6807\u51c6\u4e14\u5f00\u6e90\u7684UORA\u5b9e\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u62df\u5668\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u8d44\u6e90\u5206\u914d\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u968f\u7740Wi-Fi\u7f51\u7edc\u5bc6\u5ea6\u589e\u52a0\u548c\u65b0\u5174\u5e94\u7528\u5bf9\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u53ef\u9760\u6027\u7684\u9700\u6c42\uff0cEDCA\u673a\u5236\u7684\u5c40\u9650\u6027\u65e5\u76ca\u663e\u73b0\uff0c\u73b0\u6709\u7684UORA\u7814\u7a76\u591a\u4f9d\u8d56\u4e0d\u516c\u5f00\u7684\u6a21\u62df\u5668\uff0c\u9650\u5236\u4e86\u7ed3\u679c\u7684\u53ef\u590d\u73b0\u6027\u548c\u9a8c\u8bc1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e0ens-3\u7248\u672c3.38\u517c\u5bb9\u7684\u5b8c\u5168\u6807\u51c6\u5316\u7684\u5f00\u6e90UORA\u5b9e\u73b0\uff0c\u6539\u8fdb\u4e86\u8d44\u6e90\u5206\u914d\u7684\u8c03\u5ea6\u903b\u8f91\u548c\u914d\u7f6e\u4fe1\u53f7\u3002", "result": "\u8be5\u5b9e\u73b0\u663e\u8457\u63d0\u9ad8\u4e86\u8d44\u6e90\u5206\u914d\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\uff0c\u652f\u6301\u66f4\u51c6\u786e\u7684UORA\u8bc4\u4f30\u3002", "conclusion": "\u8fd9\u4e00\u5f00\u6e90\u5de5\u5177\u6709\u52a9\u4e8e\u63a8\u52a8\u672a\u6765Wi-Fi\u8d44\u6e90\u5206\u914d\u7b56\u7565\u7684\u7814\u7a76\u3002"}}
{"id": "2506.21885", "pdf": "https://arxiv.org/pdf/2506.21885", "abs": "https://arxiv.org/abs/2506.21885", "authors": ["Chuheng Wei", "Ziye Qin", "Ziyan Zhang", "Guoyuan Wu", "Matthew J. Barth"], "title": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles", "categories": ["cs.CV", "cs.MM", "cs.RO"], "comment": "Accepted by IEEE IV 2025", "summary": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u7efc\u8ff0\u4e86\u591a\u4f20\u611f\u5668\u878d\u5408\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u6570\u636e\u7ea7\u3001\u7279\u5f81\u7ea7\u548c\u51b3\u7b56\u7ea7\u7b56\u7565\uff0c\u5e76\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3001\u591a\u6a21\u6001\u6570\u636e\u96c6\u53ca\u65b0\u5174\u8d8b\u52bf\u3002", "motivation": "\u591a\u4f20\u611f\u5668\u878d\u5408\u5bf9\u4e8e\u514b\u670d\u5355\u4e00\u4f20\u611f\u5668\u7684\u5c40\u9650\u5e76\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u6570\u636e\u7ea7\u3001\u7279\u5f81\u7ea7\u548c\u51b3\u7b56\u7ea7\u878d\u5408\u7b56\u7565\uff0c\u5e76\u56de\u987e\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u5e94\u7528\u5206\u6790\uff0c\u5e76\u63a2\u8ba8\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u878d\u5408\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u5f53\u524d\u65b9\u6cd5\uff0c\u5e76\u5c55\u671b\u4e86\u591a\u4f20\u611f\u5668\u878d\u5408\u5728\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2506.22250", "pdf": "https://arxiv.org/pdf/2506.22250", "abs": "https://arxiv.org/abs/2506.22250", "authors": ["Yucheng Lu", "Tobias Rau", "Benjamin Lee", "Andreas K\u00f6hn", "Michael Sedlmair", "Christian Sandor", "Tobias Isenberg"], "title": "A Design Space for Visualization Transitions of 3D Spatial Data in Hybrid AR-Desktop Environments", "categories": ["cs.GR"], "comment": "14 pages, 6 figures", "summary": "We present a design space for animated transitions of the appearance of 3D\nspatial datasets in a hybrid Augmented Reality (AR)-desktop context. Such\nhybrid interfaces combine both traditional and immersive displays to facilitate\nthe exploration of 2D and 3D data representations in the environment in which\nthey are best displayed. One key aspect is to introduce transitional animations\nthat change between the different dimensionalities to illustrate the connection\nbetween the different representations and to reduce the potential cognitive\nload on the user. The specific transitions to be used depend on the type of\ndata, the needs of the application domain, and other factors. We summarize\nthese as a transition design space to simplify the decision-making process and\nprovide inspiration for future designs. First, we discuss 3D visualizations\nfrom a spatial perspective: a spatial encoding pipeline, where 3D data sampled\nfrom the physical world goes through various transformations, being mapped to\nvisual representations, and then being integrated into a hybrid AR-desktop\nenvironment. The transition design then focuses on interpolating between two\nspatial encoding pipelines to provide a smooth experience. To illustrate the\nuse of our design space, we apply it to three case studies that focus on\napplications in astronomy, radiology, and chemistry; we then discuss lessons\nlearned from these applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6df7\u5408AR-\u684c\u9762\u73af\u5883\u4e2d\u4e3a3D\u7a7a\u95f4\u6570\u636e\u96c6\u7684\u5916\u89c2\u8bbe\u8ba1\u8fc7\u6e21\u52a8\u753b\u7684\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u5e76\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u6df7\u5408\u754c\u9762\u7ed3\u5408\u4f20\u7edf\u548c\u6c89\u6d78\u5f0f\u663e\u793a\uff0c\u9700\u8981\u8fc7\u6e21\u52a8\u753b\u6765\u8fde\u63a5\u4e0d\u540c\u7ef4\u5ea6\u7684\u6570\u636e\u8868\u793a\uff0c\u51cf\u5c11\u7528\u6237\u8ba4\u77e5\u8d1f\u8377\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fc7\u6e21\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u57fa\u4e8e\u6570\u636e\u548c\u5e94\u7528\u9700\u6c42\u9009\u62e9\u8fc7\u6e21\u52a8\u753b\u3002\u8ba8\u8bba\u4e863D\u53ef\u89c6\u5316\u7684\u7a7a\u95f4\u7f16\u7801\u6d41\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u4e24\u4e2a\u7a7a\u95f4\u7f16\u7801\u6d41\u7a0b\u4e4b\u95f4\u63d2\u503c\u7684\u8fc7\u6e21\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5929\u6587\u3001\u653e\u5c04\u5b66\u548c\u5316\u5b66\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u603b\u7ed3\u4e86\u7ecf\u9a8c\u6559\u8bad\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u7a7a\u95f4\u4e3a\u6df7\u5408AR-\u684c\u9762\u73af\u5883\u4e2d3D\u6570\u636e\u7684\u8fc7\u6e21\u52a8\u753b\u63d0\u4f9b\u4e86\u51b3\u7b56\u652f\u6301\u548c\u8bbe\u8ba1\u7075\u611f\uff0c\u6539\u5584\u4e86\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2506.22137", "pdf": "https://arxiv.org/pdf/2506.22137", "abs": "https://arxiv.org/abs/2506.22137", "authors": ["Milica Leki\u0107", "Mohammad Zoofaghari", "Ilangko Balasingham", "Mladen Veleti\u0107"], "title": "On Drug Delivery System Parameter Optimisation via Semantic Information Theory", "categories": ["cs.IT", "cs.ET", "math.IT"], "comment": "This work has been submitted for possible publication in the IEEE\n  TRANSACTIONS ON MOLECULAR, BIOLOGICAL, AND MULTI-SCALE COMMUNICATIONS journal", "summary": "We investigate the application of semantic information theory to drug\ndelivery systems (DDS) within the molecular communication (MC) framework. To\noperationalise this, we observe a DDS as a molecular concentration-based\nchannel. Semantic information is defined as the amount of information required\nfor a DDS to achieve its therapeutic goal in a dynamic environment. We derive\nit by introducing interventions, defined as modifications to DDS parameters, a\nviability function, and system-environment correlations quantified via the\nchannel capacity. Here, the viability function represents DDS performance based\non a drug dose-response relationship. Our model considers a system capable of\ninducing functional changes in a receiver cancer cell, where exceeding critical\nDDS parameter values can significantly reduce performance or\ncost-effectiveness. By analysing the MC-based DDS model through a semantic\ninformation perspective, we examine how correlations between the internalised\nparticle concentration $(Y)$ and the particle concentration in the\nextracellular environment $(X)$ evolve under interventions. The final catalogue\nof results provides a quantitative basis for DDS design and optimisation,\noffering a method to determine optimal DDS parameter values under constraints\nsuch as chemical budget, desired effect and accuracy. Thus, the proposed\nframework can serve as a novel tool for guiding DDS design and optimisation.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u8bed\u4e49\u4fe1\u606f\u7406\u8bba\u7684\u836f\u7269\u9012\u9001\u7cfb\u7edf\uff08DDS\uff09\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u5b50\u901a\u4fe1\u6846\u67b6\u91cf\u5316\u4fe1\u606f\u9700\u6c42\uff0c\u4f18\u5316DDS\u53c2\u6570\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u7406\u8bba\u6307\u5bfcDDS\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u836f\u7269\u9012\u9001\u3002", "method": "\u5c06DDS\u89c6\u4e3a\u5206\u5b50\u6d53\u5ea6\u901a\u9053\uff0c\u5f15\u5165\u5e72\u9884\u3001\u751f\u5b58\u51fd\u6570\u53ca\u7cfb\u7edf-\u73af\u5883\u76f8\u5173\u6027\uff0c\u91cf\u5316\u8bed\u4e49\u4fe1\u606f\u9700\u6c42\u3002", "result": "\u63d0\u4f9b\u4e86\u5b9a\u91cf\u5206\u6790\u5de5\u5177\uff0c\u53ef\u4f18\u5316DDS\u53c2\u6570\uff0c\u5e73\u8861\u6210\u672c\u3001\u6548\u679c\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aDDS\u8bbe\u8ba1\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u671b\u63d0\u5347\u836f\u7269\u9012\u9001\u6548\u7387\u3002"}}
{"id": "2506.22171", "pdf": "https://arxiv.org/pdf/2506.22171", "abs": "https://arxiv.org/abs/2506.22171", "authors": ["Ailiya Borjigin", "Wei Zhou", "Cong He"], "title": "Proof-of-Behavior: Behavior-Driven Consensus for Trustworthy Decentralized Finance", "categories": ["cs.DC"], "comment": "8 pages, submitted to WI IAT 2025", "summary": "Current blockchain protocols (e.g., Proof-of-Work and Proof-of-Stake) secure\nthe ledger yet cannot measure validator trustworthiness, allowing subtle\nmisconduct that is especially damaging in decentralized-finance (DeFi)\nsettings. We introduce Proof-of-Behavior (PoB), a consensus model that (i)\ngives each action a layered utility score -- covering motivation and outcome,\n(ii) adapts validator weights using recent scores, and (iii) applies\ndecentralized verification with proportional slashing. The reward design is\nincentive-compatible, yielding a Nash equilibrium in which honest behavior\nmaximizes long-run pay-offs. Simulated DeFi experiments (loan-fraud detection,\nreputation-weighted validation) show that PoB cuts fraud acceptance by more\nthan 90%, demotes malicious validators within two rounds, and improves proposer\nfairness versus standard PoS, all with no more than a 5% throughput overhead.\nBy linking consensus influence to verifiably trustworthy conduct, PoB offers a\nscalable, regulation-friendly foundation for secure and fair blockchain\ngovernance in financial applications.", "AI": {"tldr": "PoB\uff08\u884c\u4e3a\u8bc1\u660e\uff09\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u9a8c\u8bc1\u8005\u6743\u91cd\u548c\u53bb\u4e2d\u5fc3\u5316\u9a8c\u8bc1\uff0c\u663e\u8457\u51cf\u5c11DeFi\u4e2d\u7684\u6b3a\u8bc8\u884c\u4e3a\uff0c\u5e76\u63d0\u9ad8\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u533a\u5757\u94fe\u534f\u8bae\uff08\u5982PoW\u548cPoS\uff09\u65e0\u6cd5\u8861\u91cf\u9a8c\u8bc1\u8005\u7684\u53ef\u4fe1\u5ea6\uff0c\u5bfc\u81f4DeFi\u73af\u5883\u4e2d\u7684\u9690\u853d\u4e0d\u5f53\u884c\u4e3a\u3002", "method": "\u63d0\u51faPoB\u6a21\u578b\uff0c\u5305\u62ec\u884c\u4e3a\u5206\u5c42\u8bc4\u5206\u3001\u52a8\u6001\u6743\u91cd\u8c03\u6574\u548c\u53bb\u4e2d\u5fc3\u5316\u9a8c\u8bc1\uff0c\u5e76\u8bbe\u8ba1\u6fc0\u52b1\u517c\u5bb9\u7684\u5956\u52b1\u673a\u5236\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793aPoB\u5c06\u6b3a\u8bc8\u63a5\u53d7\u7387\u964d\u4f4e90%\u4ee5\u4e0a\uff0c\u5feb\u901f\u8bc6\u522b\u6076\u610f\u9a8c\u8bc1\u8005\uff0c\u4e14\u541e\u5410\u91cf\u5f00\u9500\u4e0d\u8d85\u8fc75%\u3002", "conclusion": "PoB\u4e3a\u533a\u5757\u94fe\u91d1\u878d\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7b26\u5408\u76d1\u7ba1\u7684\u5b89\u5168\u6cbb\u7406\u57fa\u7840\u3002"}}
{"id": "2506.22199", "pdf": "https://arxiv.org/pdf/2506.22199", "abs": "https://arxiv.org/abs/2506.22199", "authors": ["Jakub Pele\u0161ka", "Gustav \u0160\u00edr"], "title": "REDELEX: A Framework for Relational Deep Learning Exploration", "categories": ["cs.LG", "cs.DB"], "comment": "Accepted to ECMLPKDD 2025 at Porto, Portugal", "summary": "Relational databases (RDBs) are widely regarded as the gold standard for\nstoring structured information. Consequently, predictive tasks leveraging this\ndata format hold significant application promise. Recently, Relational Deep\nLearning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized\nas graph structures, enabling the application of various graph neural\narchitectures to effectively address these tasks. However, given its novelty,\nthere is a lack of analysis into the relationships between the performance of\nvarious RDL models and the characteristics of the underlying RDBs.\n  In this study, we present REDELEX$-$a comprehensive exploration framework for\nevaluating RDL models of varying complexity on the most diverse collection of\nover 70 RDBs, which we make available to the community. Benchmarked alongside\nkey representatives of classic methods, we confirm the generally superior\nperformance of RDL while providing insights into the main factors shaping\nperformance, including model complexity, database sizes and their structural\nproperties.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86REDELEX\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u572870\u591a\u4e2aRDB\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\u3002", "motivation": "\u5173\u7cfb\u6570\u636e\u5e93\uff08RDB\uff09\u662f\u5b58\u50a8\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u9ec4\u91d1\u6807\u51c6\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u4e0eRDB\u7279\u6027\u4e4b\u95f4\u5173\u7cfb\u7684\u5206\u6790\u3002", "method": "\u63d0\u51faREDELEX\u6846\u67b6\uff0c\u8bc4\u4f30\u591a\u79cd\u590d\u6742\u5ea6RDL\u6a21\u578b\u572870\u591a\u4e2aRDB\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u7ecf\u5178\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "\u9a8c\u8bc1\u4e86RDL\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3001\u6570\u636e\u5e93\u5927\u5c0f\u548c\u7ed3\u6784\u7279\u6027\u662f\u5f71\u54cd\u6027\u80fd\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "conclusion": "REDELEX\u4e3aRDL\u6a21\u578b\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u5168\u9762\u5de5\u5177\uff0c\u5e76\u5f00\u6e90\u4e86RDB\u96c6\u5408\uff0c\u63a8\u52a8\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2506.21845", "pdf": "https://arxiv.org/pdf/2506.21845", "abs": "https://arxiv.org/abs/2506.21845", "authors": ["Zhuodi Cai"], "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.GR", "I.2; I.2.1; I.2.7; I.3; H.5; J.5"], "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)", "summary": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity.", "AI": {"tldr": "3Description\u662f\u4e00\u4e2a\u5b9e\u9a8c\u6027\u7684\u4eba\u673a\u534f\u4f5c3D\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u548c\u624b\u52bf\u63cf\u8ff0\u8ba9\u975e\u4e13\u4e1a\u4eba\u58eb\u4e5f\u80fd\u53c2\u4e0e\u5efa\u6a21\uff0c\u7ed3\u5408AI\u6280\u672f\u63d0\u9ad8\u53ef\u8bbf\u95ee\u6027\u548c\u6613\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf3D\u5efa\u6a21\u5bf9\u975e\u4e13\u4e1a\u4eba\u58eb\u7684\u590d\u6742\u6027\u548c\u53ef\u7528\u6027\u95ee\u9898\uff0c\u63a8\u52a8\u4eba\u673a\u534f\u4f5c\u7684\u666e\u53ca\uff0c\u907f\u514d\u6280\u672f\u4e3b\u5bfc\u7684\u540c\u65f6\u4fdd\u7559\u4eba\u7c7b\u521b\u9020\u529b\u3002", "method": "\u7ed3\u5408\u5b9a\u6027\u7814\u7a76\u3001\u4ea7\u54c1\u5206\u6790\u548c\u7528\u6237\u6d4b\u8bd5\uff0c\u6574\u5408NLP\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\uff08OpenAI\u548cMediaPipe\uff09\u6280\u672f\uff0c\u901a\u8fc7\u7f51\u9875\u5b9e\u73b0\u8bed\u97f3\u548c\u624b\u52bf\u8f93\u5165\u76843D\u5efa\u6a21\u8c03\u6574\u3002", "result": "\u5f00\u53d1\u51fa\u57fa\u4e8e\u7f51\u9875\u76843Description\u5de5\u5177\uff0c\u652f\u6301\u975e\u4e13\u4e1a\u4eba\u58eb\u901a\u8fc7\u8bed\u8a00\u548c\u624b\u52bf\u534f\u4f5c\u521b\u5efa3D\u6a21\u578b\u3002", "conclusion": "3Description\u4e0d\u4ec5\u63d0\u5347\u4e863D\u5efa\u6a21\u7684\u5305\u5bb9\u6027\u548c\u6613\u7528\u6027\uff0c\u8fd8\u5f3a\u8c03\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u4eba\u7c7b\u521b\u9020\u529b\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u67653D\u4e16\u754c\u7684\u6784\u5efa\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.22390", "pdf": "https://arxiv.org/pdf/2506.22390", "abs": "https://arxiv.org/abs/2506.22390", "authors": ["Ramtin Ehsani", "Sakshi Pathak", "Esteban Parra", "Sonia Haiduc", "Preetha Chatterjee"], "title": "What Makes ChatGPT Effective for Software Issue Resolution? An Empirical Study of Developer-ChatGPT Conversations in GitHub", "categories": ["cs.SE"], "comment": null, "summary": "Conversational large-language models are extensively used for issue\nresolution tasks. However, not all developer-LLM conversations are useful for\neffective issue resolution. In this paper, we analyze 686 developer-ChatGPT\nconversations shared within GitHub issue threads to identify characteristics\nthat make these conversations effective for issue resolution. First, we analyze\nthe conversations and their corresponding issues to distinguish helpful from\nunhelpful conversations. We begin by categorizing the types of tasks developers\nseek help with to better understand the scenarios in which ChatGPT is most\neffective. Next, we examine a wide range of conversational, project, and\nissue-related metrics to uncover factors associated with helpful conversations.\nFinally, we identify common deficiencies in unhelpful ChatGPT responses to\nhighlight areas that could inform the design of more effective developer-facing\ntools. We found that only 62% of the ChatGPT conversations were helpful for\nsuccessful issue resolution. ChatGPT is most effective for code generation and\ntools/libraries/APIs recommendations, but struggles with code explanations.\nHelpful conversations tend to be shorter, more readable, and exhibit stronger\nsemantic and linguistic alignment. Larger, more popular projects and more\nexperienced developers benefit more from ChatGPT. At the issue level, ChatGPT\nperforms best on simpler problems with limited developer activity and faster\nresolution, typically well-scoped tasks like compilation errors. The most\ncommon deficiencies in unhelpful ChatGPT responses include incorrect\ninformation and lack of comprehensiveness. Our findings have wide implications\nincluding guiding developers on effective interaction strategies for issue\nresolution, informing the development of tools or frameworks to support optimal\nprompt design, and providing insights on fine-tuning LLMs for issue resolution\ntasks.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86686\u4e2a\u5f00\u53d1\u8005\u4e0eChatGPT\u7684\u5bf9\u8bdd\uff0c\u53d1\u73b062%\u7684\u5bf9\u8bdd\u5bf9\u95ee\u9898\u89e3\u51b3\u6709\u5e2e\u52a9\uff0cChatGPT\u5728\u4ee3\u7801\u751f\u6210\u548c\u5de5\u5177\u63a8\u8350\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u4ee3\u7801\u89e3\u91ca\u4e0a\u8f83\u5f31\u3002", "motivation": "\u4e86\u89e3ChatGPT\u5728\u5f00\u53d1\u8005\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4ee5\u6307\u5bfc\u5f00\u53d1\u8005\u548c\u5de5\u5177\u8bbe\u8ba1\u3002", "method": "\u5206\u6790GitHub\u4e0a\u7684\u5f00\u53d1\u8005-ChatGPT\u5bf9\u8bdd\uff0c\u5206\u7c7b\u4efb\u52a1\u7c7b\u578b\u548c\u8bc4\u4f30\u5bf9\u8bdd\u3001\u9879\u76ee\u53ca\u95ee\u9898\u76f8\u5173\u6307\u6807\u3002", "result": "ChatGPT\u5bf962%\u7684\u5bf9\u8bdd\u6709\u5e2e\u52a9\uff0c\u64c5\u957f\u4ee3\u7801\u751f\u6210\u548c\u5de5\u5177\u63a8\u8350\uff0c\u77ed\u4e14\u6613\u8bfb\u7684\u5bf9\u8bdd\u66f4\u6709\u6548\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4ea4\u4e92\u7b56\u7565\u5efa\u8bae\uff0c\u5e76\u4e3a\u4f18\u5316LLM\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.22206", "pdf": "https://arxiv.org/pdf/2506.22206", "abs": "https://arxiv.org/abs/2506.22206", "authors": ["Sebastian Enqvist"], "title": "Computation by infinite descent made explicit", "categories": ["cs.LO"], "comment": null, "summary": "We introduce a non-wellfounded proof system for intuitionistic logic extended\nwith ordinary inductive and co-inductive definitions, based on a syntax in\nwhich fixpoint formulas are annotated with explicit variables for ordinals. We\nexplore the computational content of this system, in particular we introduce a\nnotion of computability and show that every valid proof is computable. As a\nconsequence, we obtain a normalization result for proofs of what we call\nfinitary formulas. A special case of this result is that every proof of a\nsequent of the appropriate form represents a unique function on natural\nnumbers. Finally, we derive a categorical model from the proof system and show\nthat least and greatest fixpoint formulas correspond to initial algebras and\nfinal coalgebras respectively.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u5e26\u5e8f\u6570\u53d8\u91cf\u7684\u4e0d\u52a8\u70b9\u516c\u5f0f\u7684\u76f4\u89c9\u903b\u8f91\u975e\u826f\u57fa\u8bc1\u660e\u7cfb\u7edf,\u63a2\u7d22\u4e86\u5176\u8ba1\u7b97\u5185\u5bb9,\u5e76\u5c55\u793a\u4e86\u8bc1\u660e\u7684\u8ba1\u7b97\u6027\u3002", "motivation": "\u6269\u5c55\u76f4\u89c9\u903b\u8f91\u7684\u8bc1\u660e\u7cfb\u7edf,\u4ee5\u652f\u6301\u5f52\u7eb3\u548c\u5171\u5f52\u7eb3\u5b9a\u4e49,\u5e76\u63a2\u7d22\u5176\u8ba1\u7b97\u542b\u4e49\u3002", "method": "\u4f7f\u7528\u5e26\u5e8f\u6570\u53d8\u91cf\u6807\u6ce8\u7684\u4e0d\u52a8\u70b9\u516c\u5f0f,\u5f15\u5165\u8ba1\u7b97\u6027\u6982\u5ff5,\u5e76\u9a8c\u8bc1\u8bc1\u660e\u7684\u53ef\u8ba1\u7b97\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u6bcf\u4e2a\u6709\u6548\u8bc1\u660e\u90fd\u662f\u53ef\u8ba1\u7b97\u7684,\u5e76\u5c55\u793a\u4e86\u89c4\u8303\u5316\u548c\u552f\u4e00\u51fd\u6570\u8868\u793a\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u8303\u7574\u6a21\u578b\u9a8c\u8bc1\u4e86\u4e0d\u52a8\u70b9\u516c\u5f0f\u4e0e\u4ee3\u6570\u548c\u4f59\u4ee3\u6570\u7684\u5bf9\u5e94\u5173\u7cfb\u3002"}}
{"id": "2506.22359", "pdf": "https://arxiv.org/pdf/2506.22359", "abs": "https://arxiv.org/abs/2506.22359", "authors": ["Viswanath Kumarskandpriya", "Abdulhalim Dandoush", "Abbas Bradai", "Ali Belgacem"], "title": "Concept-Level AI for Telecom: Moving Beyond Large Language Models", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LCMs\u5728\u7535\u4fe1\u9886\u57df\u7684\u4f18\u52bf\uff0c\u8ba4\u4e3a\u5176\u80fd\u514b\u670dLLMs\u5728\u5904\u7406\u8de8\u5c42\u4f9d\u8d56\u548c\u5b9e\u65f6\u534f\u8c03\u7b49\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\uff0c\u662f\u5b9e\u73b0\u9ad8\u6548AI\u9a71\u52a8\u7684\u7535\u4fe1\u7ba1\u7406\u7684\u5173\u952e\u3002", "motivation": "\u7535\u4fe1\u7f51\u7edc\u7ba1\u7406\u9762\u4e34\u590d\u6742\u591a\u5c42\u6b21\u548c\u591a\u8fd0\u8425\u5546\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6280\u672f\u652f\u6301\u3002", "method": "\u901a\u8fc7LCMs\u5229\u7528\u53cc\u66f2\u9690\u7a7a\u95f4\u8fdb\u884c\u5c42\u6b21\u8868\u793a\uff0c\u66ff\u4ee3LLMs\u7684\u9010\u8bcd\u5904\u7406\uff0c\u4ee5\u6982\u5ff5\u5d4c\u5165\u89e3\u51b3\u590d\u6742\u7f51\u7edc\u4ea4\u4e92\u95ee\u9898\u3002", "result": "LCMs\u5728\u5185\u5b58\u6548\u7387\u3001\u8de8\u5c42\u76f8\u5173\u6027\u548c\u591a\u6a21\u6001\u96c6\u6210\u65b9\u9762\u8868\u73b0\u4f18\u4e8eLLMs\u3002", "conclusion": "\u91c7\u7528LCMs\u662f\u7535\u4fe1\u9886\u57dfAI\u7ba1\u7406\u7684\u5fc5\u8981\u8fdb\u5316\uff0c\u800c\u975e\u7b80\u5355\u6539\u8fdb\u3002"}}
{"id": "2506.21912", "pdf": "https://arxiv.org/pdf/2506.21912", "abs": "https://arxiv.org/abs/2506.21912", "authors": ["Xinghan Wang", "Kun Xu", "Fei Li", "Cao Sheng", "Jiazhong Yu", "Yadong Mu"], "title": "Generating Attribute-Aware Human Motions from Textual Prompt", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Text-driven human motion generation has recently attracted considerable\nattention, allowing models to generate human motions based on textual\ndescriptions. However, current methods neglect the influence of human\nattributes (such as age, gender, weight, and height) which are key factors\nshaping human motion patterns. This work represents a pilot exploration for\nbridging this gap. We conceptualize each motion as comprising both attribute\ninformation and action semantics, where textual descriptions align exclusively\nwith action semantics. To achieve this, a new framework inspired by Structural\nCausal Models is proposed to decouple action semantics from human attributes,\nenabling text-to-semantics prediction and attribute-controlled generation. The\nresulting model is capable of generating realistic, attribute-aware motion\naligned with the user's text and attribute inputs. For evaluation, we introduce\nHumanAttr, a comprehensive dataset containing attribute annotations for\ntext-motion pairs, setting the first benchmark for attribute-aware\ntext-to-motion generation. Extensive experiments on the new dataset validate\nour model's effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u672c\u9a71\u52a8\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u6846\u67b6\uff0c\u8003\u8651\u4e86\u4eba\u4f53\u5c5e\u6027\uff08\u5982\u5e74\u9f84\u3001\u6027\u522b\u3001\u4f53\u91cd\u548c\u8eab\u9ad8\uff09\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u89e3\u8026\u52a8\u4f5c\u8bed\u4e49\u548c\u5c5e\u6027\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u5c5e\u6027\u548c\u6587\u672c\u53cc\u91cd\u63a7\u5236\u4e0b\u7684\u52a8\u4f5c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u9a71\u52a8\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5ffd\u7565\u4e86\u4eba\u4f53\u5c5e\u6027\u5bf9\u52a8\u4f5c\u6a21\u5f0f\u7684\u5f71\u54cd\uff0c\u800c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u5173\u952e\u56e0\u7d20\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u7ed3\u6784\u6027\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u7684\u6846\u67b6\uff0c\u89e3\u8026\u52a8\u4f5c\u8bed\u4e49\u4e0e\u4eba\u4f53\u5c5e\u6027\uff0c\u5b9e\u73b0\u6587\u672c\u5230\u8bed\u4e49\u7684\u9884\u6d4b\u548c\u5c5e\u6027\u63a7\u5236\u7684\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u751f\u6210\u7b26\u5408\u7528\u6237\u6587\u672c\u548c\u5c5e\u6027\u8f93\u5165\u7684\u903c\u771f\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u65b0\u63d0\u51fa\u7684\u6570\u636e\u96c6HumanAttr\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5c06\u4eba\u4f53\u5c5e\u6027\u7eb3\u5165\u6587\u672c\u9a71\u52a8\u52a8\u4f5c\u751f\u6210\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2109.05721", "pdf": "https://arxiv.org/pdf/2109.05721", "abs": "https://arxiv.org/abs/2109.05721", "authors": ["Yangyu Huang", "Hao Yang", "Chong Li", "Jongyoo Kim", "Fangyun Wei"], "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision. 2021 (ICCV 2021)", "summary": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4eba\u8138\u5bf9\u9f50\u4e2d\u8bef\u5dee\u5206\u5e03\u504f\u5dee\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5404\u5411\u5f02\u6027\u65b9\u5411\u635f\u5931\uff08ADL\uff09\u548c\u5404\u5411\u5f02\u6027\u6ce8\u610f\u529b\u6a21\u5757\uff08AAM\uff09\u63d0\u5347CNN\u6a21\u578b\u7684\u6536\u655b\u6027\u3002", "motivation": "\u4eba\u8138\u5bf9\u9f50\u4e2d\u7684\u8bef\u5dee\u5206\u5e03\u5b58\u5728\u504f\u5dee\uff0c\u8868\u73b0\u4e3a\u6cbf\u5730\u6807\u66f2\u7ebf\u7684\u5207\u7ebf\u65b9\u5411\u6269\u6563\u3002\u8fd9\u4e00\u504f\u5dee\u4e0e\u6a21\u7cca\u7684\u5730\u6807\u6807\u6ce8\u4efb\u52a1\u76f8\u5173\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u4f18\u5316\u8bef\u5dee\u5206\u5e03\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u6a21\u578b\u6536\u655b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86ADL\u548cAAM\uff0c\u5206\u522b\u7528\u4e8e\u5750\u6807\u56de\u5f52\u548c\u70ed\u56fe\u56de\u5f52\u3002ADL\u5728\u6cd5\u5411\u65bd\u52a0\u5f3a\u7ea6\u675f\u529b\uff0cAAM\u5219\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5757\u5728\u5207\u7ebf\u65b9\u5411\u653e\u677e\u7ea6\u675f\uff0c\u4e24\u8005\u4e92\u8865\u5b66\u4e60\u3002\u8fd9\u4e9b\u65b9\u6cd5\u6574\u5408\u4e3aADNet\u8bad\u7ec3\u6846\u67b6\u3002", "result": "ADNet\u5728300W\u3001WFLW\u548cCOFW\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "ADL\u548cAAM\u7684\u4e92\u8865\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u8138\u5bf9\u9f50\u4e2d\u7684\u8bef\u5dee\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2506.22175", "pdf": "https://arxiv.org/pdf/2506.22175", "abs": "https://arxiv.org/abs/2506.22175", "authors": ["Zheng Zhang", "Donglin Yang", "Yaqi Xia", "Liang Ding", "Dacheng Tao", "Xiaobo Zhou", "Dazhao Cheng"], "title": "MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism", "categories": ["cs.DC"], "comment": "11 pages, accepted at IPDPS 2023", "summary": "Recently, Mixture-of-Experts (MoE) has become one of the most popular\ntechniques to scale pre-trained models to extraordinarily large sizes. Dynamic\nactivation of experts allows for conditional computation, increasing the number\nof parameters of neural networks, which is critical for absorbing the vast\namounts of knowledge available in many deep learning areas. However, despite\nthe existing system and algorithm optimizations, there are significant\nchallenges to be tackled when it comes to the inefficiencies of communication\nand memory consumption.\n  In this paper, we present the design and implementation of MPipeMoE, a\nhigh-performance library that accelerates MoE training with adaptive and\nmemory-efficient pipeline parallelism. Inspired by that the MoE training\nprocedure can be divided into multiple independent sub-stages, we design\nadaptive pipeline parallelism with an online algorithm to configure the\ngranularity of the pipelining. Further, we analyze the memory footprint\nbreakdown of MoE training and identify that activations and temporary buffers\nare the primary contributors to the overall memory footprint. Toward memory\nefficiency, we propose memory reusing strategies to reduce memory requirements\nby eliminating memory redundancies, and develop an adaptive selection component\nto determine the optimal strategy that considers both hardware capacities and\nmodel characteristics at runtime. We implement MPipeMoE upon PyTorch and\nevaluate it with common MoE models in a physical cluster consisting of 8 NVIDIA\nDGX A100 servers. Compared with the state-of-art approach, MPipeMoE achieves up\nto 2.8x speedup and reduces memory footprint by up to 47% in training large\nmodels.", "AI": {"tldr": "MPipeMoE\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u5e93\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u548c\u9ad8\u6548\u5185\u5b58\u7684\u7ba1\u9053\u5e76\u884c\u6280\u672f\u52a0\u901fMoE\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e862.8\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c47%\u7684\u5185\u5b58\u6d88\u8017\u51cf\u5c11\u3002", "motivation": "\u5c3d\u7ba1MoE\u6280\u672f\u5728\u6269\u5c55\u9884\u8bad\u7ec3\u6a21\u578b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u901a\u4fe1\u548c\u5185\u5b58\u6d88\u8017\u65b9\u9762\u7684\u4f4e\u6548\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\u3002", "method": "\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u7ba1\u9053\u5e76\u884c\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u5185\u5b58\u91cd\u7528\u7b56\u7565\u4ee5\u51cf\u5c11\u5197\u4f59\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u81ea\u9002\u5e94\u9009\u62e9\u7ec4\u4ef6\u6765\u4f18\u5316\u8fd0\u884c\u65f6\u7b56\u7565\u3002", "result": "\u57288\u53f0NVIDIA DGX A100\u670d\u52a1\u5668\u96c6\u7fa4\u4e2d\u6d4b\u8bd5\uff0cMPipeMoE\u6bd4\u73b0\u6709\u6280\u672f\u5feb2.8\u500d\uff0c\u5185\u5b58\u6d88\u8017\u51cf\u5c1147%\u3002", "conclusion": "MPipeMoE\u663e\u8457\u63d0\u5347\u4e86MoE\u8bad\u7ec3\u7684\u6548\u7387\u548c\u5185\u5b58\u5229\u7528\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21896", "pdf": "https://arxiv.org/pdf/2506.21896", "abs": "https://arxiv.org/abs/2506.21896", "authors": ["Jumanh Atoum", "Jinkyung Park", "Mamtaj Akter", "Nicholas Kavoussi", "Pamela Wisniewski", "Jie Ying Wu"], "title": "Focus on the Experts: Co-designing an Augmented Reality Eye-Gaze Tracking System with Surgical Trainees to Improve Endoscopic Instruction", "categories": ["cs.HC"], "comment": null, "summary": "The current apprenticeship model for surgical training requires a high level\nof supervision, which does not scale well to meet the growing need for more\nsurgeons. Many endoscopic procedures are directly taught in the operating room\n(OR) while the attending surgeon and trainee operate on patients. The need to\nprioritize patient care limits the trainees' opportunities to experiment and\nreceive feedback on their performance. Augmented reality (AR) has the potential\nto increase efficiency in endoscopic surgical training, but additional research\nis critical to understanding the needs of surgical trainees to inform the\ndesign of AR training systems. Therefore, we worked with 18 surgical trainees\nto understand the strengths, limitations, and unmet needs of their current\ntraining environment and to co-design an AR eye-gaze tracking system based on\ntheir preferences. Trainees emphasized the need to practice the 2D to 3D\nmapping needed to properly familiarize oneself with the anatomy of patients to\nprepare for real surgery. The trainees felt that an AR-based eye gaze tracking\nsystem would be a useful supplemental training method that would improve their\nlearning in OR cases without detracting from patient care. To tailor the AR\nsystem to their needs, they co-designed features to improve their ability to\ntrack the attending surgeon's eye gaze and to provide a real-time, interactive\nsystem. Our results are valuable in shaping the endoscopic training modules by\ngenerating user-informed guidelines to design future collaborative AR-based\neye-gaze tracking systems.", "AI": {"tldr": "\u901a\u8fc7\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u6280\u672f\u6539\u8fdb\u5185\u7aa5\u955c\u624b\u672f\u57f9\u8bad\uff0c\u7814\u7a76\u4eba\u5458\u4e0e18\u540d\u5916\u79d1\u5b66\u5458\u5408\u4f5c\uff0c\u5171\u540c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eAR\u7684\u773c\u7403\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u4ee5\u6ee1\u8db3\u5b66\u5458\u5bf92D\u52303D\u6620\u5c04\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u60a3\u8005\u62a4\u7406\u3002", "motivation": "\u4f20\u7edf\u7684\u5185\u7aa5\u955c\u624b\u672f\u57f9\u8bad\u6a21\u578b\u4f9d\u8d56\u9ad8\u5f3a\u5ea6\u7684\u76d1\u7763\uff0c\u96be\u4ee5\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u5916\u79d1\u533b\u751f\u9700\u6c42\u3002AR\u6280\u672f\u6709\u671b\u63d0\u9ad8\u57f9\u8bad\u6548\u7387\uff0c\u4f46\u9700\u6df1\u5165\u4e86\u89e3\u5b66\u5458\u9700\u6c42\u4ee5\u8bbe\u8ba1\u5408\u9002\u7684AR\u57f9\u8bad\u7cfb\u7edf\u3002", "method": "\u7814\u7a76\u4eba\u5458\u4e0e18\u540d\u5916\u79d1\u5b66\u5458\u5408\u4f5c\uff0c\u5206\u6790\u5f53\u524d\u57f9\u8bad\u73af\u5883\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u5171\u540c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eAR\u7684\u773c\u7403\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u91cd\u70b9\u5173\u6ce82D\u52303D\u6620\u5c04\u8bad\u7ec3\u9700\u6c42\u3002", "result": "\u5b66\u5458\u8ba4\u4e3aAR\u773c\u7403\u8ffd\u8e2a\u7cfb\u7edf\u662f\u4e00\u79cd\u6709\u7528\u7684\u8865\u5145\u57f9\u8bad\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u60a3\u8005\u62a4\u7406\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1AR\u57f9\u8bad\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7528\u6237\u6307\u5bfc\u3002", "conclusion": "AR\u773c\u7403\u8ffd\u8e2a\u7cfb\u7edf\u6709\u671b\u6539\u8fdb\u5185\u7aa5\u955c\u624b\u672f\u57f9\u8bad\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u7cfb\u7edf\u529f\u80fd\u4ee5\u6ee1\u8db3\u5b66\u5458\u9700\u6c42\u3002"}}
{"id": "2506.21718", "pdf": "https://arxiv.org/pdf/2506.21718", "abs": "https://arxiv.org/abs/2506.21718", "authors": ["Yash Akhauri", "Bryan Lewandowski", "Cheng-Hsi Lin", "Adrian N. Reyes", "Grant C. Forbes", "Arissa Wongpanich", "Bangding Yang", "Mohamed S. Abdelfattah", "Sagi Perel", "Xingyou Song"], "title": "Performance Prediction for Large Systems via Text-to-Text Regression", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE", "cs.SY", "eess.SY"], "comment": "Code can be found at https://github.com/google-deepmind/regress-lm", "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5230\u6587\u672c\u56de\u5f52\u7684\u901a\u7528\u3001\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u590d\u6742\u7cfb\u7edf\u6570\u636e\u7684\u6307\u6807\u7ed3\u679c\uff0c\u4f18\u4e8e\u4f20\u7edf\u8868\u683c\u56de\u5f52\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u56de\u5f52\u65b9\u6cd5\u5728\u590d\u6742\u7cfb\u7edf\u6570\u636e\uff08\u5982\u914d\u7f6e\u6587\u4ef6\u6216\u7cfb\u7edf\u65e5\u5fd7\uff09\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u752860M\u53c2\u6570\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u4ece\u968f\u673a\u521d\u59cb\u5316\u5f00\u59cb\u8bad\u7ec3\uff0c\u8fdb\u884c\u6587\u672c\u5230\u6587\u672c\u56de\u5f52\u3002", "result": "\u5728Google\u7684Borg\u7cfb\u7edf\u4e0a\uff0c\u6a21\u578b\u5b9e\u73b0\u4e860.99\u7684\u6392\u540d\u76f8\u5173\u6027\uff08\u5e73\u57470.9\uff09\uff0c\u5747\u65b9\u8bef\u5dee\u6bd4\u4f20\u7edf\u65b9\u6cd5\u4f4e100\u500d\uff0c\u5e76\u80fd\u8f7b\u677e\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u7cfb\u7edf\u7ed3\u679c\u7684\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5c55\u793a\u4e86\u901a\u7528\u6a21\u62df\u5668\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.22344", "pdf": "https://arxiv.org/pdf/2506.22344", "abs": "https://arxiv.org/abs/2506.22344", "authors": ["Francesco Di Cosmo", "Soumodev Mal", "Tephilla Prince"], "title": "Nets-within-Nets through the Lens of Data Nets", "categories": ["cs.CC", "cs.FL", "cs.LO"], "comment": "34 pages, 19 figures", "summary": "Elementary Object Systems (EOSs) are a model in the nets-within-nets (NWNs)\nparadigm, where tokens in turn can host standard Petri nets. We study the\ncomplexity of the reachability problem of EOSs when subjected to\nnon-deterministic token losses. It is known that this problem is equivalent to\nthe coverability problem with no lossiness of conservative EOSs (cEOSs). We\nprecisely characterize cEOS coverability into the framework of data nets, whose\ntokens carry data from an infinite domain. Specifically, we show that cEOS\ncoverability is equivalent to the coverability of an interesting fragment of\ndata nets that extends beyond $\\nu$PNs (featuring globally fresh name\ncreation), yet remains less expressive than Unordered Data Nets (featuring\nlossy name creation as well as powerful forms of whole-place operations and\nbroadcasts). This insight bridges two apparently orthogonal approaches to PN\nextensions, namely data nets and NWNs. At the same time, it enables us to\nanalyze cEOS coverability taking advantage of known results on data nets. As a\nbyproduct, we immediately get that the complexity of cEOS coverability lies\nbetween $\\mathbf{F}_{\\omega 2}$ and $\\mathbf{F}_{\\omega^\\omega}$, two classes\nbeyond Primitive Recursive.", "AI": {"tldr": "\u7814\u7a76\u5728\u975e\u786e\u5b9a\u6027\u4ee4\u724c\u4e22\u5931\u60c5\u51b5\u4e0b\u57fa\u672c\u5bf9\u8c61\u7cfb\u7edf\uff08EOSs\uff09\u7684\u53ef\u8fbe\u6027\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u5e76\u5c06\u5176\u4e0e\u6570\u636e\u7f51\u7edc\u7684\u8986\u76d6\u6027\u95ee\u9898\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u63a2\u7d22\u57fa\u672c\u5bf9\u8c61\u7cfb\u7edf\uff08EOSs\uff09\u5728\u4ee4\u724c\u4e22\u5931\u60c5\u51b5\u4e0b\u7684\u590d\u6742\u6027\uff0c\u5e76\u5c06\u5176\u4e0e\u6570\u636e\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u548c\u590d\u6742\u6027\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u5efa\u7acb\u4e24\u79cd\u6269\u5c55Petri\u7f51\u65b9\u6cd5\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "method": "\u901a\u8fc7\u5c06\u4fdd\u5b88EOSs\uff08cEOSs\uff09\u7684\u8986\u76d6\u6027\u95ee\u9898\u8f6c\u5316\u4e3a\u6570\u636e\u7f51\u7edc\u7684\u4e00\u4e2a\u7247\u6bb5\uff0c\u5229\u7528\u5df2\u77e5\u7684\u6570\u636e\u7f51\u7edc\u7ed3\u679c\u5206\u6790cEOSs\u7684\u590d\u6742\u6027\u3002", "result": "\u8bc1\u660e\u4e86cEOSs\u7684\u8986\u76d6\u6027\u95ee\u9898\u4e0e\u6570\u636e\u7f51\u7edc\u4e2d\u4e00\u4e2a\u6709\u8da3\u7247\u6bb5\u7684\u8986\u76d6\u6027\u95ee\u9898\u7b49\u4ef7\uff0c\u5176\u590d\u6742\u6027\u4ecb\u4e8eF\u03c92\u548cF\u03c9\u03c9\u4e4b\u95f4\u3002", "conclusion": "\u901a\u8fc7\u5efa\u7acbcEOSs\u4e0e\u6570\u636e\u7f51\u7edc\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4e0d\u4ec5\u63ed\u793a\u4e86\u4e24\u79cdPetri\u7f51\u6269\u5c55\u65b9\u6cd5\u7684\u5171\u6027\uff0c\u8fd8\u5229\u7528\u6570\u636e\u7f51\u7edc\u7684\u7ed3\u679c\u5206\u6790\u4e86cEOSs\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2506.22036", "pdf": "https://arxiv.org/pdf/2506.22036", "abs": "https://arxiv.org/abs/2506.22036", "authors": ["Ying Zhang", "Yu Zhao", "Xuhui Sui", "Baohang Zhou", "Xiangrui Cai", "Li Shen", "Xiaojie Yuan", "Dacheng Tao"], "title": "Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion", "categories": ["cs.LG", "cs.MM"], "comment": "Submitted to the IEEE for possible publication", "summary": "With the increasing multimodal knowledge privatization requirements,\nmultimodal knowledge graphs in different institutes are usually decentralized,\nlacking of effective collaboration system with both stronger reasoning ability\nand transmission safety guarantees. In this paper, we propose the Federated\nMultimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over\nfederated MKGs for better predicting the missing links in clients without\nsharing sensitive knowledge. We propose a framework named MMFeD3-HidE for\naddressing multimodal uncertain unavailability and multimodal client\nheterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed\nHyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete\nmultimodal distributions from incomplete entity embeddings constrained by\navailable modalities. (2) Among clients, our proposed Multimodal FeDerated Dual\nDistillation (MMFeD3) transfers knowledge mutually between clients and the\nserver with logit and feature distillation to improve both global convergence\nand semantic consistency. We propose a FedMKGC benchmark for a comprehensive\nevaluation, consisting of a general FedMKGC backbone named MMFedE, datasets\nwith heterogeneous multimodal information, and three groups of constructed\nbaselines. Experiments conducted on our benchmark validate the effectiveness,\nsemantic consistency, and convergence robustness of MMFeD3-HidE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFedMKGC\u7684\u4efb\u52a1\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff08MKGs\uff09\u6765\u9884\u6d4b\u7f3a\u5931\u94fe\u63a5\uff0c\u540c\u65f6\u4fdd\u62a4\u654f\u611f\u77e5\u8bc6\u4e0d\u5916\u6cc4\u3002\u63d0\u51fa\u7684MMFeD3-HidE\u6846\u67b6\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u5f02\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u5206\u6563\u4e14\u7f3a\u4e4f\u534f\u4f5c\u7684\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u63a8\u7406\u80fd\u529b\u548c\u4f20\u8f93\u5b89\u5168\u3002", "method": "\u63d0\u51faHidE\u6a21\u578b\u6062\u590d\u591a\u6a21\u6001\u5206\u5e03\uff0c\u5e76\u8bbe\u8ba1MMFeD3\u6846\u67b6\u5b9e\u73b0\u5ba2\u6237\u7aef\u4e0e\u670d\u52a1\u5668\u95f4\u7684\u4e92\u84b8\u998f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMMFeD3-HidE\u5728\u6709\u6548\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6536\u655b\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MMFeD3-HidE\u6210\u529f\u89e3\u51b3\u4e86FedMKGC\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u652f\u6301\u3002"}}
{"id": "2212.09525", "pdf": "https://arxiv.org/pdf/2212.09525", "abs": "https://arxiv.org/abs/2212.09525", "authors": ["Yangyu Huang", "Xi Chen", "Jongyoo Kim", "Hao Yang", "Chong Li", "Jiaolong Yang", "Dong Chen"], "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "comment": "AAAI 2023", "summary": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u73b0\u6709\u7a00\u758f\u5730\u6807\u6570\u636e\u96c6\u589e\u5f3a\u5730\u6807\u5bc6\u5ea6\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9762\u90e8\u5bf9\u9f50\u4efb\u52a1\u3002", "motivation": "\u9488\u5bf9\u7a00\u758f\u9762\u90e8\u5730\u6807\u5728\u7f8e\u5bb9\u533b\u5b66\u7b49\u573a\u666f\u4e2d\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63d0\u9ad8\u5730\u6807\u7684\u5bc6\u5ea6\u3002", "method": "\u5229\u7528\u7a00\u758f\u5730\u6807\u6570\u636e\u7684\u5c40\u90e8\u76f8\u4f3c\u6027\uff0c\u63d0\u51fa\u5f31\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u6765\u4f18\u5316\u5730\u6807\u5bc6\u5ea6\uff0c\u8bbe\u8ba1\u4e86\u591a\u4e2a\u64cd\u4f5c\u7b26\u5b9e\u73b0\u8be5\u76ee\u6807\u3002", "result": "\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u5730\u6807\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9762\u90e8\u5bf9\u9f50\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.22267", "pdf": "https://arxiv.org/pdf/2506.22267", "abs": "https://arxiv.org/abs/2506.22267", "authors": ["Junaid Ahmed Khan", "Hiari Pizzini Cavagna", "Andrea Proia", "Andrea Bartolini"], "title": "Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph is All You Need", "categories": ["cs.DC"], "comment": "11 pages", "summary": "With generative artificial intelligence challenging computational scientific\ncomputing, data centers are experiencing unprecedented growth in both scale and\nvolume. As a result, computing efficiency has become more critical than ever.\nOperational Data Analytics (ODA) relies on the collection of data center\ntelemetry to improve efficiency, but so far has been focusing on real-time\ntelemetry data visualization and post-mortem analysis. However, with NoSQL\ndatabases now serving as the default storage backend to support scalability,\nquerying this data is challenging due to its schema-less nature, which requires\ndomain knowledge to traverse relationships between data sources. Ontologies and\nKnowledge Graphs (KGs) can capture these relationships, but traditional KGs are\ncostly to scale and have not been widely applied to multivariate timeseries.\nVirtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating\nquery-specific graphs at runtime. In this work, we present a full end-to-end\nODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL\nqueries, utilizing VKG for data retrieval. This approach achieves 92.5%\naccuracy compared to 25% with direct NoSQL queries. The proposed methodology\noptimizes VKG construction and LLM inference, cutting previous work average\nquery latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179\nMiB. This performance makes the tool suitable for deployment and real-time\ninteraction with ODA end-users.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u548c\u865a\u62df\u77e5\u8bc6\u56fe\u8c31(VKG)\u7684\u7aef\u5230\u7aefODA\u804a\u5929\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u67e5\u8be2\u51c6\u786e\u7387\u548c\u6548\u7387\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5bf9\u79d1\u5b66\u8ba1\u7b97\u7684\u6311\u6218\uff0c\u6570\u636e\u4e2d\u5fc3\u7684\u89c4\u6a21\u548c\u5bb9\u91cf\u6fc0\u589e\uff0c\u8ba1\u7b97\u6548\u7387\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfODA\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u65e0\u6a21\u5f0fNoSQL\u6570\u636e\u5e93\u7684\u67e5\u8be2\u95ee\u9898\u3002", "method": "\u91c7\u7528LLM\u751f\u6210SPARQL\u67e5\u8be2\uff0c\u7ed3\u5408VKG\u8fdb\u884c\u5b9e\u65f6\u6570\u636e\u68c0\u7d22\uff0c\u4f18\u5316VKG\u6784\u5efa\u548cLLM\u63a8\u7406\u6548\u7387\u3002", "result": "\u7cfb\u7edf\u67e5\u8be2\u51c6\u786e\u7387\u63d0\u5347\u81f392.5%\uff0c\u8f83\u76f4\u63a5NoSQL\u67e5\u8be2(25%)\u663e\u8457\u63d0\u9ad8\uff1b\u67e5\u8be2\u5ef6\u8fdf\u4ece20.36\u79d2\u964d\u81f33.03\u79d2\uff0cVKG\u5927\u5c0f\u63a7\u5236\u5728179 MiB\u5185\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u90e8\u7f72\u5e76\u4e0eODA\u7ec8\u7aef\u7528\u6237\u5b9e\u65f6\u4ea4\u4e92\uff0c\u4e3a\u6570\u636e\u4e2d\u5fc3\u6548\u7387\u63d0\u5347\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.21898", "pdf": "https://arxiv.org/pdf/2506.21898", "abs": "https://arxiv.org/abs/2506.21898", "authors": ["Aimen Gaba", "Emily Wall", "Tejas Ramkumar Babu", "Yuriy Brun", "Kyle Hall", "Cindy Xiong Bearfield"], "title": "Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language Models", "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are becoming increasingly ubiquitous in our\ndaily lives, but numerous concerns about bias in LLMs exist. This study\nexamines how gender-diverse populations perceive bias, accuracy, and\ntrustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews\nwith non-binary/transgender, male, and female participants, we investigate how\ngendered and neutral prompts influence model responses and how users evaluate\nthese responses. Our findings reveal that gendered prompts elicit more\nidentity-specific responses, with non-binary participants particularly\nsusceptible to condescending and stereotypical portrayals. Perceived accuracy\nwas consistent across gender groups, with errors most noted in technical topics\nand creative tasks. Trustworthiness varied by gender, with men showing higher\ntrust, especially in performance, and non-binary participants demonstrating\nhigher performance-based trust. Additionally, participants suggested improving\nthe LLMs by diversifying training data, ensuring equal depth in gendered\nresponses, and incorporating clarifying questions. This research contributes to\nthe CSCW/HCI field by highlighting the need for gender-diverse perspectives in\nLLM development in particular and AI in general, to foster more inclusive and\ntrustworthy systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u6027\u522b\u7fa4\u4f53\u5bf9ChatGPT\u7684\u504f\u89c1\u3001\u51c6\u786e\u6027\u548c\u4fe1\u4efb\u5ea6\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u975e\u4e8c\u5143\u6027\u522b\u53c2\u4e0e\u8005\u66f4\u5bb9\u6613\u53d7\u5230\u523b\u677f\u5370\u8c61\u7684\u5f71\u54cd\uff0c\u800c\u7537\u6027\u5bf9\u6a21\u578b\u7684\u4fe1\u4efb\u5ea6\u66f4\u9ad8\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u666e\u53ca\uff0c\u5bf9\u5176\u504f\u89c1\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u52a0\uff0c\u672c\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u6027\u522b\u591a\u6837\u5316\u7fa4\u4f53\u5bf9LLMs\u7684\u770b\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf925\u540d\u975e\u4e8c\u5143/\u8de8\u6027\u522b\u8005\u3001\u7537\u6027\u548c\u5973\u6027\u7684\u6df1\u5165\u8bbf\u8c08\uff0c\u89c2\u5bdf\u6027\u522b\u5316\u4e0e\u4e2d\u6027\u63d0\u793a\u5bf9\u6a21\u578b\u54cd\u5e94\u7684\u5f71\u54cd\u53ca\u5176\u8bc4\u4ef7\u3002", "result": "\u6027\u522b\u5316\u63d0\u793a\u5f15\u53d1\u66f4\u591a\u7279\u5b9a\u8eab\u4efd\u54cd\u5e94\uff0c\u975e\u4e8c\u5143\u53c2\u4e0e\u8005\u66f4\u6613\u53d7\u5230\u523b\u677f\u5370\u8c61\u5f71\u54cd\uff1b\u51c6\u786e\u5ea6\u611f\u77e5\u5728\u6027\u522b\u95f4\u4e00\u81f4\uff0c\u6280\u672f\u6027\u548c\u521b\u610f\u4efb\u52a1\u9519\u8bef\u8f83\u591a\uff1b\u4fe1\u4efb\u5ea6\u56e0\u6027\u522b\u800c\u5f02\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u5728LLM\u5f00\u53d1\u4e2d\u7eb3\u5165\u6027\u522b\u591a\u6837\u5316\u89c6\u89d2\uff0c\u4ee5\u63d0\u5347\u5305\u5bb9\u6027\u548c\u4fe1\u4efb\u5ea6\uff0c\u5efa\u8bae\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u3001\u5e73\u8861\u6027\u522b\u54cd\u5e94\u6df1\u5ea6\u5e76\u52a0\u5165\u6f84\u6e05\u95ee\u9898\u3002"}}
{"id": "2506.22237", "pdf": "https://arxiv.org/pdf/2506.22237", "abs": "https://arxiv.org/abs/2506.22237", "authors": ["Sebastian Murgul", "Moritz Reiser", "Michael Heizmann", "Christoph Seibert"], "title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "9 pages, 3 figures, 6 tables", "summary": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08CRNN\uff09\u7684\u97f3\u9891\u4e0eMIDI\u540c\u6b65\u65b9\u6cd5\uff0c\u6bd4\u4f20\u7edfDTW\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u534720%\uff0c\u7ed3\u5408DTW\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u94a2\u7434\u6f14\u594f\u5f55\u97f3\u4e0eMIDI\u6587\u4ef6\u677e\u6563\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u63d0\u5347\u540c\u6b65\u7cbe\u5ea6\u3002", "method": "\u4f7f\u7528CRNN\u5904\u7406\u672a\u5bf9\u9f50\u7684\u94a2\u7434\u5377\u548c\u9891\u8c31\u56fe\uff0c\u8bad\u7ec3\u65f6\u901a\u8fc7\u589e\u5f3a\u7684MIDI\u6a21\u62df\u4eba\u4e3a\u65f6\u95f4\u8bef\u5dee\u3002", "result": "CRNN\u6bd4DTW\u51c6\u786e\u7387\u9ad820%\uff0c\u7ed3\u5408\u4e24\u8005\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u53ef\u663e\u8457\u63d0\u5347MIDI\u4e0e\u97f3\u9891\u5bf9\u9f50\u6280\u672f\uff0c\u5c55\u73b0\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2506.22180", "pdf": "https://arxiv.org/pdf/2506.22180", "abs": "https://arxiv.org/abs/2506.22180", "authors": ["\u00d6nder G\u00fcrcan"], "title": "Reliability Analysis of Smart Contract Execution Architectures: A Comparative Simulation Study", "categories": ["cs.CR", "cs.DC"], "comment": "23 pages, 5 figures, 2 tables", "summary": "The industrial market continuously needs reliable solutions to secure\nautonomous systems. Especially as these systems become more complex and\ninterconnected, reliable security solutions are becoming increasingly\nimportant. One promising solution to tackle this challenge is using smart\ncontracts designed to meet contractual conditions, avoid malicious errors,\nsecure exchanges, and minimize the need for reliable intermediaries. However,\nsmart contracts are immutable. Moreover, there are different smart contract\nexecution architectures (namely Order-Execute and Execute-Order-Validate) that\nhave different throughputs. In this study, we developed an evaluation model for\nassessing the security of reliable smart contract execution. We then developed\na realistic smart contract enabled IoT energy case study. Finally, we simulate\nthe developed case study to evaluate several smart contract security\nvulnerabilities reported in the literature. Our results show that the\nExecute-Order-Validate architecture is more promising regarding reliability and\nsecurity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u667a\u80fd\u5408\u7ea6\u6267\u884c\u67b6\u6784\u7684\u5b89\u5168\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86Execute-Order-Validate\u67b6\u6784\u5728\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\u548c\u4e92\u8054\uff0c\u9700\u8981\u53ef\u9760\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u667a\u80fd\u5408\u7ea6\u56e0\u5176\u81ea\u52a8\u5316\u548c\u53bb\u4e2d\u4ecb\u5316\u7279\u6027\u6210\u4e3a\u6f5c\u5728\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u667a\u80fd\u5408\u7ea6\u6267\u884c\u5b89\u5168\u6027\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7269\u8054\u7f51\u80fd\u6e90\u6848\u4f8b\u8fdb\u884c\u4eff\u771f\uff0c\u5206\u6790\u6587\u732e\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "result": "Execute-Order-Validate\u67b6\u6784\u5728\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660eExecute-Order-Validate\u67b6\u6784\u662f\u66f4\u53ef\u9760\u7684\u667a\u80fd\u5408\u7ea6\u6267\u884c\u65b9\u6848\u3002"}}
{"id": "2506.21962", "pdf": "https://arxiv.org/pdf/2506.21962", "abs": "https://arxiv.org/abs/2506.21962", "authors": ["Tianrun Qiu", "Yuxin Ma"], "title": "AnyAni: An Interactive System with Generative AI for Animation Effect Creation and Code Understanding in Web Development", "categories": ["cs.HC", "J.6"], "comment": null, "summary": "Generative AI assistants have been widely used in front-end programming.\nHowever, besides code writing, developers often encounter the need to generate\nanimation effects. As novices in creative design without the assistance of\nprofessional designers, developers typically face difficulties in describing,\ndesigning, and implementing desired animations. To address this issue, we\nconducted a formative study (N=6) to identify the challenges that code\ndevelopers face when dealing with animation design issues. Then, we introduce\nAnyAni, a human-AI collaborative system that supports front-end developers in\nthe ideation, manipulation, and implementation of animation effects. The system\ncombines the assistance of generative AI in creative design by adopting a\nnonlinear workflow for iterative animation development. In addition, developers\ncan understand and learn the code generated for implementing animations through\nvarious interactive methods. A user study (N=9) demonstrated the usability of\nAnyAni in animation effect creation support for developers.", "AI": {"tldr": "AnyAni \u662f\u4e00\u4e2a\u5e2e\u52a9\u524d\u7aef\u5f00\u53d1\u8005\u8bbe\u8ba1\u52a8\u753b\u7684\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u751f\u6210\u5f0f AI \u548c\u975e\u7ebf\u6027\u5de5\u4f5c\u6d41\u652f\u6301\u52a8\u753b\u521b\u4f5c\u3002", "motivation": "\u5f00\u53d1\u8005\u7f3a\u4e4f\u4e13\u4e1a\u8bbe\u8ba1\u80cc\u666f\uff0c\u96be\u4ee5\u63cf\u8ff0\u548c\u5b9e\u73b0\u52a8\u753b\u6548\u679c\uff0c\u9700\u8981\u5de5\u5177\u8f85\u52a9\u3002", "method": "\u901a\u8fc7\u5f62\u6210\u6027\u7814\u7a76\uff08N=6\uff09\u8bc6\u522b\u95ee\u9898\uff0c\u5f00\u53d1 AnyAni \u7cfb\u7edf\uff0c\u7ed3\u5408\u751f\u6210\u5f0f AI \u548c\u975e\u7ebf\u6027\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u52a8\u753b\u8bbe\u8ba1\u3002", "result": "\u7528\u6237\u7814\u7a76\uff08N=9\uff09\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u3002", "conclusion": "AnyAni \u662f\u6709\u6548\u7684\u52a8\u753b\u8bbe\u8ba1\u52a9\u624b\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u514b\u670d\u521b\u610f\u548c\u5b9e\u73b0\u96be\u9898\u3002"}}
{"id": "2506.22066", "pdf": "https://arxiv.org/pdf/2506.22066", "abs": "https://arxiv.org/abs/2506.22066", "authors": ["Maciej Grzeszczuk", "Grzegorz Pochwatko", "Barbara Karpowicz", "Stanis\u0142aw Knapi\u0144ski", "Wies\u0142aw Kope\u0107"], "title": "Building Trustworthy Cognitive Monitoring for Safety-Critical Human Tasks: A Phased Methodological Approach", "categories": ["cs.HC"], "comment": "11 pages, 5 figures, 1 table", "summary": "Operators performing high-stakes, safety-critical tasks - such as air traffic\ncontrollers, surgeons, or mission control personnel - must maintain exceptional\ncognitive performance under variable and often stressful conditions. This paper\npresents a phased methodological approach to building cognitive monitoring\nsystems for such environments. By integrating insights from human factors\nresearch, simulation-based training, sensor technologies, and fundamental\npsychological principles, the proposed framework supports real-time performance\nassessment with minimum intrusion. The approach begins with simplified\nsimulations and evolves towards operational contexts. Key challenges addressed\ninclude variability in workload, the effects of fatigue and stress, thus the\nneed for adaptive monitoring for early warning support mechanisms. The\nmethodology aims to improve situational awareness, reduce human error, and\nsupport decision-making without undermining operator autonomy. Ultimately, the\nwork contributes to the development of resilient and transparent systems in\ndomains where human performance is critical to safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u6784\u5efa\u8ba4\u77e5\u76d1\u63a7\u7cfb\u7edf\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\u624b\u6bb5\uff0c\u5b9e\u65f6\u8bc4\u4f30\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u64cd\u4f5c\u5458\u7684\u8ba4\u77e5\u8868\u73b0\uff0c\u4ee5\u51cf\u5c11\u4eba\u4e3a\u9519\u8bef\u5e76\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u9ad8\u98ce\u9669\u4efb\u52a1\uff08\u5982\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\u3001\u624b\u672f\u7b49\uff09\u4e2d\u7684\u64cd\u4f5c\u5458\u9700\u8981\u5728\u591a\u53d8\u4e14\u538b\u529b\u5927\u7684\u6761\u4ef6\u4e0b\u4fdd\u6301\u51fa\u8272\u8ba4\u77e5\u8868\u73b0\uff0c\u4e9f\u9700\u4e00\u79cd\u5b9e\u65f6\u76d1\u63a7\u65b9\u6cd5\u4ee5\u652f\u6301\u5176\u5de5\u4f5c\u3002", "method": "\u91c7\u7528\u5206\u9636\u6bb5\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u56e0\u7814\u7a76\u3001\u6a21\u62df\u8bad\u7ec3\u3001\u4f20\u611f\u5668\u6280\u672f\u548c\u5fc3\u7406\u5b66\u539f\u7406\uff0c\u6784\u5efa\u4ece\u7b80\u5316\u6a21\u62df\u5230\u5b9e\u9645\u64cd\u4f5c\u7684\u8ba4\u77e5\u76d1\u63a7\u7cfb\u7edf\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u5b9e\u65f6\u8bc4\u4f30\u64cd\u4f5c\u5458\u8868\u73b0\uff0c\u9002\u5e94\u5de5\u4f5c\u8d1f\u8377\u53d8\u5316\u548c\u538b\u529b\u75b2\u52b3\u5f71\u54cd\uff0c\u540c\u65f6\u652f\u6301\u51b3\u7b56\u800c\u4e0d\u635f\u5bb3\u64cd\u4f5c\u5458\u81ea\u4e3b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u900f\u660e\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5728\u5173\u952e\u5b89\u5168\u9886\u57df\u4e2d\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u6548\u80fd\u3002"}}
{"id": "2506.22319", "pdf": "https://arxiv.org/pdf/2506.22319", "abs": "https://arxiv.org/abs/2506.22319", "authors": ["Di Zhang", "Ligang Liu"], "title": "Asymptotic analysis and design of shell-based thermal lattice metamaterials", "categories": ["math.AP", "cs.GR", "math-ph", "math.MP", "physics.comp-ph", "74Q15 (Primary) 35Q74, 74Q20, 74K25 (Secondary)", "I.3.5; J.2"], "comment": null, "summary": "We present a rigorous asymptotic analysis framework for investigating the\nthermal conductivity of shell lattice metamaterials, extending prior work from\nmechanical stiffness to heat transfer. Central to our analysis is a new metric,\nthe asymptotic directional conductivity (ADC), which captures the leading-order\ninfluence of the middle surface geometry on the effective thermal conductivity\nin the vanishing-thickness limit. A convergence theorem is established for\nevaluating ADC, along with a sharp upper bound and the necessary and sufficient\ncondition for achieving this bound. These results provide the first theoretical\njustification for the optimal thermal conductivity of triply periodic minimal\nsurfaces. Furthermore, we show that ADC yields a third-order approximation to\nthe effective conductivity of shell lattices at low volume fractions. To\nsupport practical design applications, we develop a discrete algorithm for\ncomputing and optimizing ADC over arbitrary periodic surfaces. Numerical\nresults confirm the theoretical predictions and demonstrate the robustness and\neffectiveness of the proposed optimization algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e25\u8c28\u7684\u6e10\u8fd1\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u58f3\u683c\u8d85\u6750\u6599\u7684\u5bfc\u70ed\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u6307\u6807\u2014\u2014\u6e10\u8fd1\u65b9\u5411\u5bfc\u70ed\u6027\uff08ADC\uff09\uff0c\u7528\u4e8e\u6355\u6349\u51e0\u4f55\u5f62\u72b6\u5bf9\u5bfc\u70ed\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u6269\u5c55\u5148\u524d\u5173\u4e8e\u673a\u68b0\u521a\u5ea6\u7684\u7814\u7a76\uff0c\u63a2\u8ba8\u58f3\u683c\u8d85\u6750\u6599\u5728\u70ed\u4f20\u5bfc\u65b9\u9762\u7684\u6027\u80fd\uff0c\u4e3a\u5176\u4f18\u5316\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u901a\u8fc7\u6e10\u8fd1\u5206\u6790\u548c\u65b0\u6307\u6807ADC\u7684\u5f15\u5165\uff0c\u5efa\u7acb\u4e86\u8bc4\u4f30ADC\u7684\u6536\u655b\u5b9a\u7406\u3001\u4e0a\u754c\u53ca\u5176\u5b9e\u73b0\u6761\u4ef6\u3002", "result": "\u9996\u6b21\u4e3a\u4e09\u91cd\u5468\u671f\u6781\u5c0f\u66f2\u9762\u7684\u6700\u4f18\u5bfc\u70ed\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u8bc1\u660e\uff0c\u5e76\u8bc1\u660e\u4e86ADC\u5728\u4f4e\u4f53\u79ef\u5206\u6570\u4e0b\u5bf9\u6709\u6548\u5bfc\u70ed\u6027\u7684\u4e09\u9636\u8fd1\u4f3c\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u4e3a\u58f3\u683c\u8d85\u6750\u6599\u7684\u70ed\u4f20\u5bfc\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.22125", "pdf": "https://arxiv.org/pdf/2506.22125", "abs": "https://arxiv.org/abs/2506.22125", "authors": ["Marie Altmann", "Kimberly Hegemann", "Ali Askari", "Vineetha Rallabandi", "Max Pascher", "Jens Gerken"], "title": "NoticeLight: Embracing Socio-Technical Asymmetry through Tangible Peripheral Robotic Embodiment in Hybrid Collaboration", "categories": ["cs.HC"], "comment": "Workshop on The Future of Human-Robot Synergy in Interactive\n  Environments: The Role of Robots at the Workplace at CHIWORK 2025, Amsterdam,\n  Netherlands", "summary": "Hybrid collaboration has become a fixture in modern workplaces, yet it\nintroduces persistent socio-technical asymmetries-especially disadvantaging\nremote participants, who struggle with presence disparity, reduced visibility,\nand limited non-verbal communication. Traditional solutions often seek to erase\nthese asymmetries, but recent research suggests embracing them as productive\ndesign constraints. In this context, we introduce NoticeLight: a tangible,\nperipheral robotic embodiment designed to augment hybrid meetings. NoticeLight\ntransforms remote participants' digital presence into ambient, physical signals\n-- such as mood dynamics, verbal contribution mosaics, and attention cues --\nwithin the co-located space. By abstracting group states into subtle light\npatterns, NoticeLight fosters peripheral awareness and balanced participation\nwithout disrupting meeting flow or demanding cognitive overload. This approach\naligns with emerging perspectives in human-robot synergy, positioning robots as\nmediators that reshape, rather than replicate, human presence. Our work thereby\nadvances the discourse on how robotic embodiments can empower equitable,\ndynamic collaboration in the workplace.", "AI": {"tldr": "NoticeLight\u662f\u4e00\u4e2a\u6709\u5f62\u3001\u5916\u56f4\u7684\u673a\u5668\u4eba\u88c5\u7f6e\uff0c\u65e8\u5728\u901a\u8fc7\u5c06\u8fdc\u7a0b\u53c2\u4e0e\u8005\u7684\u6570\u5b57\u5b58\u5728\u8f6c\u5316\u4e3a\u73af\u5883\u7269\u7406\u4fe1\u53f7\uff08\u5982\u60c5\u7eea\u52a8\u6001\u3001\u8bed\u8a00\u8d21\u732e\u9a6c\u8d5b\u514b\u548c\u6ce8\u610f\u529b\u63d0\u793a\uff09\u6765\u589e\u5f3a\u6df7\u5408\u4f1a\u8bae\uff0c\u4ece\u800c\u4fc3\u8fdb\u5916\u56f4\u610f\u8bc6\u548c\u5e73\u8861\u53c2\u4e0e\u3002", "motivation": "\u73b0\u4ee3\u5de5\u4f5c\u573a\u6240\u4e2d\u7684\u6df7\u5408\u534f\u4f5c\u6a21\u5f0f\u5e26\u6765\u4e86\u6301\u7eed\u7684\u793e\u4f1a\u6280\u672f\u4e0d\u5bf9\u79f0\uff0c\u5c24\u5176\u662f\u8fdc\u7a0b\u53c2\u4e0e\u8005\u5728\u5b58\u5728\u5dee\u5f02\u3001\u53ef\u89c1\u6027\u964d\u4f4e\u548c\u975e\u8bed\u8a00\u6c9f\u901a\u53d7\u9650\u65b9\u9762\u5904\u4e8e\u52a3\u52bf\u3002\u4f20\u7edf\u65b9\u6cd5\u8bd5\u56fe\u6d88\u9664\u8fd9\u4e9b\u4e0d\u5bf9\u79f0\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u5efa\u8bae\u5c06\u5176\u4f5c\u4e3a\u8bbe\u8ba1\u7ea6\u675f\u3002", "method": "\u5f15\u5165NoticeLight\uff0c\u5b83\u901a\u8fc7\u62bd\u8c61\u7fa4\u4f53\u72b6\u6001\u4e3a\u5fae\u5f31\u7684\u5149\u6a21\u5f0f\uff0c\u5728\u5171\u5904\u7a7a\u95f4\u4e2d\u5c55\u73b0\u8fdc\u7a0b\u53c2\u4e0e\u8005\u7684\u6570\u5b57\u5316\u5b58\u5728\uff0c\u4ece\u800c\u5728\u4e0d\u4e2d\u65ad\u4f1a\u8bae\u6d41\u7a0b\u6216\u589e\u52a0\u8ba4\u77e5\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\u4fc3\u8fdb\u5e73\u8861\u53c2\u4e0e\u3002", "result": "NoticeLight\u901a\u8fc7\u5176\u7269\u7406\u4fe1\u53f7\u63d0\u5347\u4e86\u5916\u56f4\u610f\u8bc6\u548c\u53c2\u4e0e\u5e73\u8861\uff0c\u652f\u6301\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u534f\u540c\u7684\u65b0\u89c6\u89d2\uff0c\u5373\u673a\u5668\u4eba\u4f5c\u4e3a\u91cd\u5851\u800c\u975e\u590d\u5236\u4eba\u7c7b\u5b58\u5728\u7684\u5a92\u4ecb\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u52a8\u4e86\u5173\u4e8e\u673a\u5668\u4eba\u88c5\u7f6e\u5982\u4f55\u5728\u5de5\u4f5c\u573a\u6240\u4e2d\u8d4b\u80fd\u516c\u5e73\u3001\u52a8\u6001\u534f\u4f5c\u7684\u8ba8\u8bba\u3002"}}
{"id": "2506.22426", "pdf": "https://arxiv.org/pdf/2506.22426", "abs": "https://arxiv.org/abs/2506.22426", "authors": ["Xiang Dai", "Kyrollos Yanny", "Kristina Monakhova", "Nicholas Antipa"], "title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "comment": null, "summary": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the\ndynamic range limits of image sensors. The classic method relies on multiple\nexposures, which slows capture time, resulting in motion artifacts when imaging\ndynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR\ndata into a single exposure, then computationally recovering it. Many\nestablished methods use strong image priors to recover improperly exposed image\ndetail. These approaches struggle with extended highlight regions. We utilize\nthe global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR\nshutter mode applies a longer exposure time to rows closer to the bottom of the\nsensor. We use optics that relay a randomly permuted (shuffled) image onto the\nsensor, effectively creating spatially randomized exposures across the scene.\nThe exposure diversity allows us to recover HDR data by solving an optimization\nproblem with a simple total variation image prior. In simulation, we\ndemonstrate that our method outperforms other single-shot methods when many\nsensor pixels are saturated (10% or more), and is competitive at a modest\nsaturation (1%). Finally, we demonstrate a physical lab prototype that uses an\noff-the-shelf random fiber bundle for the optical shuffling. The fiber bundle\nis coupled to a low-cost commercial sensor operating in GRR shutter mode. Our\nprototype achieves a dynamic range of up to 73dB using an 8-bit sensor with\n48dB dynamic range.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u5c40\u590d\u4f4d\u91ca\u653e\uff08GRR\uff09\u5feb\u95e8\u6a21\u5f0f\u548c\u5149\u5b66\u968f\u673a\u7f6e\u6362\u7684\u5355\u6b21\u66dd\u5149\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u6210\u50cf\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u5149\u533a\u57df\u548c\u8fd0\u52a8\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u514b\u670d\u4f20\u7edf\u591a\u66dd\u5149HDR\u6210\u50cf\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u5355\u6b21\u66dd\u5149\u65b9\u6cd5\u5728\u9ad8\u5149\u533a\u57df\u7684\u6062\u590d\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u5229\u7528GRR\u5feb\u95e8\u6a21\u5f0f\u4e3a\u4e0d\u540c\u884c\u5206\u914d\u4e0d\u540c\u66dd\u5149\u65f6\u95f4\uff0c\u7ed3\u5408\u5149\u5b66\u968f\u673a\u7f6e\u6362\u6280\u672f\uff0c\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u548c\u603b\u53d8\u5dee\u56fe\u50cf\u5148\u9a8c\u6062\u590dHDR\u6570\u636e\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u50cf\u7d20\u9971\u548c\u7387\u9ad8\u65f6\uff0810%\u4ee5\u4e0a\uff09\u4f18\u4e8e\u5176\u4ed6\u5355\u6b21\u66dd\u5149\u65b9\u6cd5\uff1b\u5b9e\u9a8c\u539f\u578b\u4f7f\u7528\u4f4e\u6210\u672c\u5546\u7528\u4f20\u611f\u5668\u548c\u968f\u673a\u5149\u7ea4\u675f\uff0c\u52a8\u6001\u8303\u56f4\u8fbe\u523073dB\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u6027\u7684\u786c\u4ef6\u8bbe\u8ba1\u548c\u7b80\u5355\u7684\u8ba1\u7b97\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u6b21\u66dd\u5149HDR\u6210\u50cf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u50cf\u7d20\u9971\u548c\u7387\u9ad8\u7684\u573a\u666f\u4e0b\u3002"}}
{"id": "2506.22231", "pdf": "https://arxiv.org/pdf/2506.22231", "abs": "https://arxiv.org/abs/2506.22231", "authors": ["Russell Beale"], "title": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education", "categories": ["cs.HC", "cs.AI", "cs.CY", "K.3.1; K.3.2; K.6.0"], "comment": null, "summary": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff08\u5982ChatGPT\uff09\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u5feb\u901f\u666e\u53ca\u5e26\u6765\u673a\u9047\u4e0e\u6311\u6218\uff0c\u9700\u5236\u5b9a\u653f\u7b56\u4ee5\u5e73\u8861\u6280\u672f\u6f5c\u529b\u4e0e\u5b66\u672f\u8bda\u4fe1\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u5bf9\u5b66\u672f\u8bda\u4fe1\u3001\u4f26\u7406\u548c\u516c\u5e73\u7684\u5f71\u54cd\u3002", "method": "\u7efc\u5408\u8fd1\u671f\u7814\u7a76\u548c\u6848\u4f8b\uff0c\u5206\u6790AI\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u653f\u7b56\u5efa\u8bae\u3002", "result": "\u7814\u7a76\u8868\u660e47%\u7684\u5b66\u751f\u4f7f\u7528LLM\u5b8c\u6210\u4f5c\u4e1a\uff0c\u68c0\u6d4b\u5de5\u5177\u51c6\u786e\u738788%\uff0c\u9700\u6539\u8fdb\u8bc4\u4f30\u548c\u57f9\u8bad\u3002", "conclusion": "\u9700\u4e3b\u52a8\u8c03\u6574\u653f\u7b56\uff0c\u4ee5\u5229\u7528AI\u6f5c\u529b\uff0c\u540c\u65f6\u4fdd\u969c\u5b66\u672f\u8bda\u4fe1\u548c\u516c\u5e73\u3002"}}
{"id": "2506.22379", "pdf": "https://arxiv.org/pdf/2506.22379", "abs": "https://arxiv.org/abs/2506.22379", "authors": ["Marvin Kopka", "Markus A. Feufel"], "title": "How to Evaluate the Accuracy of Online and AI-Based Symptom Checkers: A Standardized Methodological Framework", "categories": ["cs.HC"], "comment": null, "summary": "Online and AI-based symptom checkers are applications that assist medical\nlaypeople in diagnosing their symptoms and determining which course of action\nto take. When evaluating these tools, previous studies primarily used an\napproach introduced a decade ago that lacked any type of quality control.\nNumerous studies have criticized this approach, and several empirical studies\nhave sought to improve specific aspects of evaluations. However, even after a\ndecade, a high-quality methodological framework for standardizing the\nevaluation of symptom checkers remains missing. This article synthesizes\nempirical studies to outline a framework for standardized evaluations based on\nrepresentative case selection, an externally and internally valid evaluation\ndesign, and metrics that increase cross-study comparability. This approach is\nbacked up by several open-access resources to facilitate implementation.\nUltimately, this approach should enhance the quality and comparability of\nfuture evaluations of online and AI-based symptom checkers to enable\nmeta-analyses and help stakeholders make more informed decisions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u51c6\u5316\u8bc4\u4f30\u5728\u7ebf\u548cAI\u75c7\u72b6\u68c0\u67e5\u5de5\u5177\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u672a\u6765\u7814\u7a76\u7684\u8d28\u91cf\u548c\u53ef\u6bd4\u6027\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u8d28\u91cf\u63a7\u5236\u4e14\u5df2\u8fc7\u65f6\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u8d28\u91cf\u7684\u65b9\u6cd5\u6846\u67b6\u6765\u6807\u51c6\u5316\u75c7\u72b6\u68c0\u67e5\u5de5\u5177\u7684\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51fa\u57fa\u4e8e\u4ee3\u8868\u6027\u6848\u4f8b\u9009\u62e9\u3001\u5185\u5916\u6709\u6548\u8bc4\u4f30\u8bbe\u8ba1\u4ee5\u53ca\u63d0\u9ad8\u8de8\u7814\u7a76\u53ef\u6bd4\u6027\u6307\u6807\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u8d44\u6e90\u8f85\u52a9\u5b9e\u65bd\u3002", "result": "\u8be5\u6846\u67b6\u65e8\u5728\u63d0\u5347\u672a\u6765\u5bf9\u5728\u7ebf\u548cAI\u75c7\u72b6\u68c0\u67e5\u5de5\u5177\u8bc4\u4f30\u7684\u8d28\u91cf\u548c\u53ef\u6bd4\u6027\uff0c\u652f\u6301\u5143\u5206\u6790\u5e76\u5e2e\u52a9\u5229\u76ca\u76f8\u5173\u8005\u505a\u51fa\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u586b\u8865\u4e86\u957f\u671f\u5b58\u5728\u7684\u7a7a\u767d\uff0c\u4e3a\u75c7\u72b6\u68c0\u67e5\u5de5\u5177\u7684\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u79d1\u5b66\u7684\u65b9\u6cd5\u3002"}}
{"id": "2501.06184", "pdf": "https://arxiv.org/pdf/2501.06184", "abs": "https://arxiv.org/abs/2501.06184", "authors": ["Yangyu Huang", "Tianyi Gao", "Haoran Xu", "Qihao Zhao", "Yang Song", "Zhipeng Gui", "Tengchao Lv", "Hao Chen", "Lei Cui", "Scarlett Li", "Furu Wei"], "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.HC", "cs.MA"], "comment": null, "summary": "Geologic map, as a fundamental diagram in geology science, provides critical\ninsights into the structure and composition of Earth's subsurface and surface.\nThese maps are indispensable in various fields, including disaster detection,\nresource exploration, and civil engineering. Despite their significance,\ncurrent Multimodal Large Language Models (MLLMs) often fall short in geologic\nmap understanding. This gap is primarily due to the challenging nature of\ncartographic generalization, which involves handling high-resolution map,\nmanaging multiple associated components, and requiring domain-specific\nknowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever\nbenchmark for evaluating MLLMs in geologic map understanding, which assesses\nthe full-scale abilities in extracting, referring, grounding, reasoning, and\nanalyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent\ndesigned for geologic map understanding, which features three modules:\nHierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),\nand Prompt-enhanced Question Answering (PEQA). Inspired by the\ninterdisciplinary collaboration among human scientists, an AI expert group acts\nas consultants, utilizing a diverse tool pool to comprehensively analyze\nquestions. Through comprehensive experiments, GeoMap-Agent achieves an overall\nscore of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.\nOur work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,\npaves the way for advanced AI applications in geology, enhancing the efficiency\nand accuracy of geological investigations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86GeoMap-Bench\uff08\u9996\u4e2a\u5730\u8d28\u56fe\u7406\u89e3\u8bc4\u4f30\u57fa\u51c6\uff09\u548cGeoMap-Agent\uff08\u9996\u4e2a\u5730\u8d28\u56fe\u7406\u89e3\u667a\u80fd\u4f53\uff09\uff0c\u901a\u8fc7\u591a\u6a21\u5757\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728\u5730\u8d28\u56fe\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u5730\u8d28\u56fe\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u5730\u56fe\u901a\u7528\u7684\u9ad8\u5206\u8fa8\u7387\u3001\u591a\u7ec4\u4ef6\u548c\u9886\u57df\u77e5\u8bc6\u9700\u6c42\u3002", "method": "\u6784\u5efaGeoMap-Bench\u57fa\u51c6\uff0c\u8bbe\u8ba1GeoMap-Agent\uff08\u542bHIE\u3001DKI\u3001PEQA\u6a21\u5757\uff09\u5e76\u5f15\u5165AI\u4e13\u5bb6\u7ec4\u534f\u4f5c\u5de5\u5177\u3002", "result": "GeoMap-Agent\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u52060.811\uff0c\u8fdc\u8d85GPT-4o\u76840.369\u3002", "conclusion": "\u5de5\u4f5c\u4e3a\u5730\u8d28\u5b66AI\u5e94\u7528\u94fa\u8def\uff0c\u63d0\u5347\u4e86\u5730\u8d28\u7814\u7a76\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.21582", "pdf": "https://arxiv.org/pdf/2506.21582", "abs": "https://arxiv.org/abs/2506.21582", "authors": ["Sam Yu-Te Lee", "Chengyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyi Liu", "Kwan-Liu Ma"], "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems.", "AI": {"tldr": "VIDEE\u7cfb\u7edf\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u5e2e\u52a9\u975e\u4e13\u5bb6\u7528\u6237\u8fdb\u884c\u9ad8\u7ea7\u6587\u672c\u5206\u6790\uff0c\u5305\u62ec\u5206\u89e3\u3001\u6267\u884c\u548c\u8bc4\u4f30\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u5206\u6790\u9700\u8981NLP\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5165\u95e8\u7ea7\u5206\u6790\u5e08\u7684\u4f7f\u7528\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u4e3a\u81ea\u52a8\u5316\u6587\u672c\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "VIDEE\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\uff1a\u5206\u89e3\uff08\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7ed3\u5408\u4eba\u5de5\u53cd\u9988\uff09\u3001\u6267\u884c\uff08\u751f\u6210\u53ef\u6267\u884c\u5206\u6790\u7ba1\u9053\uff09\u3001\u8bc4\u4f30\uff08\u57fa\u4e8eLLM\u7684\u9a8c\u8bc1\u4e0e\u53ef\u89c6\u5316\uff09\u3002", "result": "\u5b9a\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86VIDEE\u7684\u6709\u6548\u6027\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\u7cfb\u7edf\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u53cb\u597d\uff0c\u5e76\u63ed\u793a\u4e86\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "VIDEE\u4e3a\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u667a\u80fd\u6587\u672c\u5206\u6790\u7cfb\u7edf\u7684\u4eba\u673a\u534f\u4f5c\u8bbe\u8ba1\u3002"}}
{"id": "2506.21604", "pdf": "https://arxiv.org/pdf/2506.21604", "abs": "https://arxiv.org/abs/2506.21604", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u3001\u5b9a\u91cf\u5316\u7684\u591a\u6a21\u6001\u751f\u6210AI\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6a21\u6001\u6743\u91cd\u63d0\u5347\u4f01\u4e1a\u6587\u6863\u667a\u80fd\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u751f\u6210AI\u7684\u8bc4\u4f30\u6846\u67b6\u96be\u4ee5\u5efa\u7acb\u53ef\u4fe1\u5ea6\uff0c\u5f71\u54cd\u4e86\u53ef\u9760\u6027\u548c\u4f01\u4e1a\u91c7\u7528\u7387\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u5b9a\u91cf\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u3001\u56fe\u50cf\u3001\u6807\u9898\u548cOCR\u7b49\u8de8\u6a21\u6001\u8f93\u5165\uff0c\u5efa\u7acb\u6280\u672f\u6307\u6807\u4e0e\u7528\u6237\u4fe1\u4efb\u5ea6\u7684\u5b9a\u91cf\u5173\u7cfb\u3002", "result": "\u4f18\u5316\u6a21\u6001\u6743\u91cd\uff0830%\u6587\u672c\u300115%\u56fe\u50cf\u300125%\u6807\u9898\u300130%OCR\uff09\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u6027\u80fd\u63d0\u534757.3%\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u4e3a\u591a\u6a21\u6001RAG\u5728\u5173\u952e\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u4e25\u683c\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u8d1f\u8d23\u4efbAI\u7684\u90e8\u7f72\u3002"}}
{"id": "2506.21819", "pdf": "https://arxiv.org/pdf/2506.21819", "abs": "https://arxiv.org/abs/2506.21819", "authors": ["Lena John", "Kheir Eddine Farfar", "S\u00f6ren Auer", "Oliver Karras"], "title": "SciMantify -- A Hybrid Approach for the Evolving Semantification of Scientific Knowledge", "categories": ["cs.DL", "cs.AI", "cs.HC"], "comment": "Accepted at the 25th International Conference on Web Engineering 2025", "summary": "Scientific publications, primarily digitized as PDFs, remain static and\nunstructured, limiting the accessibility and reusability of the contained\nknowledge. At best, scientific knowledge from publications is provided in\ntabular formats, which lack semantic context. A more flexible, structured, and\nsemantic representation is needed to make scientific knowledge understandable\nand processable by both humans and machines. We propose an evolution model of\nknowledge representation, inspired by the 5-star Linked Open Data (LOD) model,\nwith five stages and defined criteria to guide the stepwise transition from a\ndigital artifact, such as a PDF, to a semantic representation integrated in a\nknowledge graph (KG). Based on an exemplary workflow implementing the entire\nmodel, we developed a hybrid approach, called SciMantify, leveraging tabular\nformats of scientific knowledge, e.g., results from secondary studies, to\nsupport its evolving semantification. In the approach, humans and machines\ncollaborate closely by performing semantic annotation tasks (SATs) and refining\nthe results to progressively improve the semantic representation of scientific\nknowledge. We implemented the approach in the Open Research Knowledge Graph\n(ORKG), an established platform for improving the findability, accessibility,\ninteroperability, and reusability of scientific knowledge. A preliminary user\nexperiment showed that the approach simplifies the preprocessing of scientific\nknowledge, reduces the effort for the evolving semantification, and enhances\nthe knowledge representation through better alignment with the KG structures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e5\u661fLOD\u6a21\u578b\u7684\u77e5\u8bc6\u8868\u793a\u8fdb\u5316\u6a21\u578bSciMantify\uff0c\u9010\u6b65\u5c06PDF\u8f6c\u6362\u4e3a\u8bed\u4e49\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u4f18\u5316\u8bed\u4e49\u6807\u6ce8\uff0c\u5e76\u5728ORKG\u5e73\u53f0\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u79d1\u5b66\u51fa\u7248\u7269\u591a\u4e3aPDF\u683c\u5f0f\uff0c\u7f3a\u4e4f\u7ed3\u6784\u5316\u4e0e\u8bed\u4e49\u8868\u793a\uff0c\u9650\u5236\u4e86\u77e5\u8bc6\u7684\u53ef\u8bbf\u95ee\u6027\u4e0e\u91cd\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e94\u9636\u6bb5\u8fdb\u5316\u6a21\u578bSciMantify\uff0c\u7ed3\u5408\u4eba\u673a\u534f\u4f5c\u8fdb\u884c\u8bed\u4e49\u6807\u6ce8\uff0c\u9010\u6b65\u4f18\u5316\u79d1\u5b66\u77e5\u8bc6\u7684\u8bed\u4e49\u8868\u793a\u3002", "result": "\u521d\u6b65\u7528\u6237\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u79d1\u5b66\u77e5\u8bc6\u9884\u5904\u7406\uff0c\u964d\u4f4e\u4e86\u8bed\u4e49\u5316\u5de5\u4f5c\u91cf\uff0c\u5e76\u63d0\u5347\u4e86\u4e0e\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u7684\u5bf9\u9f50\u3002", "conclusion": "SciMantify\u4e3a\u79d1\u5b66\u77e5\u8bc6\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u8bed\u4e49\u8868\u793a\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u5176\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\u4e0e\u91cd\u7528\u6027\u3002"}}
{"id": "2506.22111", "pdf": "https://arxiv.org/pdf/2506.22111", "abs": "https://arxiv.org/abs/2506.22111", "authors": ["Ruthvik Bokkasam", "Shankar Gangisetty", "A. H. Abdul Hafez", "C. V. Jawahar"], "title": "Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "With the rapid advancements in autonomous driving, accurately predicting\npedestrian behavior has become essential for ensuring safety in complex and\nunpredictable traffic conditions. The growing interest in this challenge\nhighlights the need for comprehensive datasets that capture unstructured\nenvironments, enabling the development of more robust prediction models to\nenhance pedestrian safety and vehicle navigation. In this paper, we introduce\nan Indian driving pedestrian dataset designed to address the complexities of\nmodeling pedestrian behavior in unstructured environments, such as illumination\nchanges, occlusion of pedestrians, unsignalized scene types and\nvehicle-pedestrian interactions. The dataset provides high-level and detailed\nlow-level comprehensive annotations focused on pedestrians requiring the\nego-vehicle's attention. Evaluation of the state-of-the-art intention\nprediction methods on our dataset shows a significant performance drop of up to\n$\\mathbf{15\\%}$, while trajectory prediction methods underperform with an\nincrease of up to $\\mathbf{1208}$ MSE, defeating standard pedestrian datasets.\nAdditionally, we present exhaustive quantitative and qualitative analysis of\nintention and trajectory baselines. We believe that our dataset will open new\nchallenges for the pedestrian behavior research community to build robust\nmodels. Project Page:\nhttps://cvit.iiit.ac.in/research/projects/cvit-projects/iddped", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5370\u5ea6\u9a7e\u9a76\u884c\u4eba\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5efa\u6a21\u884c\u4eba\u884c\u4e3a\u7684\u590d\u6742\u6027\uff0c\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u51c6\u786e\u9884\u6d4b\u884c\u4eba\u884c\u4e3a\u5bf9\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u6570\u636e\u96c6\u96be\u4ee5\u8986\u76d6\u975e\u7ed3\u6784\u5316\u73af\u5883\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u9ad8\u6c34\u5e73\u548c\u4f4e\u6c34\u5e73\u6ce8\u91ca\u7684\u5370\u5ea6\u9a7e\u9a76\u884c\u4eba\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5149\u7167\u53d8\u5316\u3001\u906e\u6321\u7b49\u6311\u6218\u3002", "result": "\u73b0\u6709\u610f\u56fe\u9884\u6d4b\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d15%\uff0c\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u7684MSE\u589e\u52a01208\uff0c\u8868\u73b0\u663e\u8457\u52a3\u4e8e\u6807\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "\u65b0\u6570\u636e\u96c6\u4e3a\u884c\u4eba\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u6311\u6218\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u3002"}}
