<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 13]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 6]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Survey of AIOps in the Era of Large Language Models](https://arxiv.org/abs/2507.12472)
*Lingzhe Zhang,Tong Jia,Mengxi Jia,Yifan Wu,Aiwei Liu,Yong Yang,Zhonghai Wu,Xuming Hu,Philip S. Yu,Ying Li*

Main category: cs.SE

TL;DR: 本文对LLMs在AIOps中的应用进行了详细调查，分析了183篇论文，探讨了失败数据来源、任务演变、LLM方法及评估方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs的复杂性和广泛应用，其在AIOps中的潜力和局限性尚不完全清楚，因此需要系统调查以填补这一空白。

Method: 通过分析2020年至2024年的183篇研究论文，回答了四个关键研究问题，涉及数据来源、任务演变、LLM方法和评估方法。

Result: 研究发现LLMs可以在AIOps中优化流程并改善结果，同时揭示了现有研究中的不足。

Conclusion: 研究总结了LLM4AIOps的最新进展和趋势，提出了未来探索的方向。

Abstract: As large language models (LLMs) grow increasingly sophisticated and
pervasive, their application to various Artificial Intelligence for IT
Operations (AIOps) tasks has garnered significant attention. However, a
comprehensive understanding of the impact, potential, and limitations of LLMs
in AIOps remains in its infancy. To address this gap, we conducted a detailed
survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve
outcomes in this domain. We analyzed 183 research papers published between
January 2020 and December 2024 to answer four key research questions (RQs). In
RQ1, we examine the diverse failure data sources utilized, including advanced
LLM-based processing techniques for legacy data and the incorporation of new
data sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,
highlighting the emergence of novel tasks and the publication trends across
these tasks. RQ3 investigates the various LLM-based methods applied to address
AIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to
assess LLM-integrated AIOps approaches. Based on our findings, we discuss the
state-of-the-art advancements and trends, identify gaps in existing research,
and propose promising directions for future exploration.

</details>


### [2] [LLM-Powered Quantum Code Transpilation](https://arxiv.org/abs/2507.12480)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 本研究探讨了利用大型语言模型（LLMs）作为灵活的自动化解决方案，用于解决量子SDK之间代码转换的互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 多样性量子SDK（如Qiskit、Cirq等）导致的互操作性和跨平台开发挑战，传统规则驱动转换工具设计维护成本高。

Method: 利用LLMs的预训练知识和上下文推理能力，将其作为编程语言无关的转换器，自动转换量子程序。

Result: 提出了一种无需手动定义转换规则的可扩展解决方案，实现了量子软件的可移植性。

Conclusion: 该研究为量子计算生态系统中的智能通用转换迈出了重要一步。

Abstract: There exist various Software Development Kits (SDKs) tailored to different
quantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples
include but are not limited to Qiskit, Cirq, and PennyLane. However, this
diversity presents significant challenges for interoperability and
cross-platform development of hybrid quantum-classical software systems.
Traditional rule-based transpilers for translating code between QSDKs are
time-consuming to design and maintain, requiring deep expertise and rigid
mappings in the source and destination code. In this study, we explore the use
of Large Language Models (LLMs) as a flexible and automated solution.
Leveraging their pretrained knowledge and contextual reasoning capabilities, we
position LLMs as programming language-agnostic transpilers capable of
converting quantum programs from one QSDK to another while preserving
functional equivalence. Our approach eliminates the need for manually defined
transformation rules and offers a scalable solution to quantum software
portability. This work represents a step toward enabling intelligent,
general-purpose transpilation in the quantum computing ecosystem.

</details>


### [3] [Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding](https://arxiv.org/abs/2507.12482)
*Ishraq Khan,Assad Chowdary,Sharoz Haseeb,Urvish Patel*

Main category: cs.SE

TL;DR: Kodezi Chronos是一种下一代架构，旨在解决大型语言模型在代码生成中的局限性，通过多级嵌入记忆引擎实现对超长上下文的高效推理，显著提高代码维护效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成和软件自动化中存在上下文限制和缺乏显式代码结构推理的问题，Kodezi Chronos旨在突破这些限制。

Method: Kodezi Chronos采用多级嵌入记忆引擎，结合向量和图形索引，实现代码库规模的推理，支持跨文件的代码理解和实时修复。

Result: 在新型多随机检索基准测试中，Chronos表现优于现有模型，实际缺陷检测提升23%，调试周期减少40%。

Conclusion: Kodezi Chronos为软件维护提供自主化解决方案，显著提高了代码可靠性和生产力。

Abstract: Large Language Models (LLMs) have advanced code generation and software
automation, but are fundamentally constrained by limited inference-time context
and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a
next-generation architecture for autonomous code understanding, debugging, and
maintenance, designed to operate across ultra-long contexts comprising entire
codebases, histories, and documentation, all without fixed window limits.
Kodezi Chronos leverages a multi-level embedding memory engine, combining
vector and graph-based indexing with continuous code-aware retrieval. This
enables efficient and accurate reasoning over millions of lines of code,
supporting repository-scale comprehension, multi-file refactoring, and
real-time self-healing actions. Our evaluation introduces a novel Multi Random
Retrieval benchmark, specifically tailored to the software engineering domain.
Unlike classical retrieval benchmarks, this method requires the model to
resolve arbitrarily distant and obfuscated associations across code artifacts,
simulating realistic tasks such as variable tracing, dependency migration, and
semantic bug localization. Chronos outperforms prior LLMs and code models,
demonstrating a 23% improvement in real-world bug detection and reducing
debugging cycles by up to 40% compared to traditional sequence-based
approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos
enables seamless, autonomous software maintenance, elevating code reliability
and productivity while reducing manual effort. These results mark a critical
advance toward self-sustaining, continuously optimized software ecosystems.

</details>


### [4] [A Survey of Reinforcement Learning for Software Engineering](https://arxiv.org/abs/2507.12483)
*Dong Wang,Hanmo You,Lingwei Zhu,Kaiwei Lin,Zheng Chen,Chen Yang,Junji Yu,Zan Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 本文是第一篇关于强化学习（RL）在软件工程（SE）中应用的系统性综述，分析了115篇研究，提出了未来方向。


<details>
  <summary>Details</summary>
Motivation: RL在SE中的应用日益增多，但缺乏系统性的综述，本文旨在填补这一空白。

Method: 分析了115篇同行评审研究，涵盖22个顶级SE会议，研究了趋势、算法分类等。

Result: 总结了RL在SE中的应用现状，提出了开放挑战和未来研究方向。

Conclusion: 本文为RL4SE领域的研究者和实践者提供了系统性指导，并公开了研究资料。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential
decision-making and has attracted growing interest across various domains,
particularly following the advent of Deep Reinforcement Learning (DRL) in 2015.
Simultaneously, the rapid advancement of Large Language Models (LLMs) has
further fueled interest in integrating RL with LLMs to enable more adaptive and
intelligent systems. In the field of software engineering (SE), the increasing
complexity of systems and the rising demand for automation have motivated
researchers to apply RL to a broad range of tasks, from software design and
development to quality assurance and maintenance. Despite growing research in
RL-for-SE, there remains a lack of a comprehensive and systematic survey of
this evolving field. To address this gap, we reviewed 115 peer-reviewed studies
published across 22 premier SE venues since the introduction of DRL. We
conducted a comprehensive analysis of publication trends, categorized SE topics
and RL algorithms, and examined key factors such as dataset usage, model design
and optimization, and evaluation practices. Furthermore, we identified open
challenges and proposed future research directions to guide and inspire ongoing
work in this evolving area. To summarize, this survey offers the first
systematic mapping of RL applications in software engineering, aiming to
support both researchers and practitioners in navigating the current landscape
and advancing the field. Our artifacts are publicly available:
https://github.com/KaiWei-Lin-lanina/RL4SE.

</details>


### [5] [When Retriever Meets Generator: A Joint Model for Code Comment Generation](https://arxiv.org/abs/2507.12558)
*Tien P. T. Le,Anh M. T. Bui,Huy N. D. Pham,Alessio Bucaioni,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: RAGSum提出了一种结合检索与生成的单一框架，通过CodeT5骨干网络和对比预训练优化代码嵌入，显著提升了代码注释生成的性能。


<details>
  <summary>Details</summary>
Motivation: 自动生成源代码的简洁、信息性注释可以减轻文档编写负担并加速程序理解，但现有检索增强方法中的检索与生成过程孤立优化，导致下游噪声传播。

Method: RAGSum基于CodeT5骨干网络，通过对比预训练优化代码嵌入，采用复合损失函数联合优化检索与生成，并部署轻量级自修正循环。

Result: 在Java、Python、C三种语言的基准测试中，RAGSum在BLEU、METEOR和ROUGE-L指标上显著优于基线模型。

Conclusion: 紧密耦合检索与生成能够提升注释自动化的上限，为后续开发者研究和定性分析提供了动力。

Abstract: Automatically generating concise, informative comments for source code can
lighten documentation effort and accelerate program comprehension.
Retrieval-augmented approaches first fetch code snippets with existing comments
and then synthesize a new comment, yet retrieval and generation are typically
optimized in isolation, allowing irrelevant neighbors topropagate noise
downstream. To tackle the issue, we propose a novel approach named RAGSum with
the aim of both effectiveness and efficiency in recommendations. RAGSum is
built on top offuse retrieval and generation using a single CodeT5 backbone. We
report preliminary results on a unified retrieval-generation framework built on
CodeT5. A contrastive pre-training phase shapes code embeddings for
nearest-neighbor search; these weights then seed end-to-end training with a
composite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes
comment-generation error. More importantly, a lightweight self-refinement loop
is deployed to polish the final output. We evaluated theframework on three
cross-language benchmarks (Java, Python, C), and compared it with three
well-established baselines. The results show that our approach substantially
outperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These
findings indicate that tightly coupling retrieval and generationcan raise the
ceiling for comment automation and motivateforthcoming replications and
qualitative developer studies.

</details>


### [6] [ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells](https://arxiv.org/abs/2507.12561)
*Samal Nursapa,Anastassiya Samuilova,Alessio Bucaioni. Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 本文探讨了使用预训练的transformer模型（CodeBERT和CodeT5）为检测到的架构异味推荐合适的重构方法。CodeT5表现优于CodeBERT和传统基线，准确率达96.9%。


<details>
  <summary>Details</summary>
Motivation: 现有工具能检测架构异味，但很少提供修复建议。本文旨在填补这一空白。

Method: 将任务定义为三分类问题，在11000多个开源Java项目的200多万个重构实例上对CodeBERT和CodeT5进行微调。

Result: CodeT5准确率达96.9%，F1分数为95.2%，优于其他方法。

Conclusion: transformer模型能有效连接异味检测与修复，为未来的重构推荐系统奠定基础。

Abstract: Architectural smells such as God Class, Cyclic Dependency, and Hub-like
Dependency degrade software quality and maintainability. Existing tools detect
such smells but rarely suggest how to fix them. This paper explores the use of
pre-trained transformer models--CodeBERT and CodeT5--for recommending suitable
refactorings based on detected smells. We frame the task as a three-class
classification problem and fine-tune both models on over 2 million refactoring
instances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%
accuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our
results show that transformer-based models can effectively bridge the gap
between smell detection and actionable repair, laying the foundation for future
refactoring recommendation systems. We release all code, models, and data under
an open license to support reproducibility and further research.

</details>


### [7] [QSpark: Towards Reliable Qiskit Code Generation](https://arxiv.org/abs/2507.12642)
*Kiana Kheiri,Aamna Aamir,Andriy Miranskyy,Chen Ding*

Main category: cs.SE

TL;DR: 论文研究了通过两种强化学习方法（GRPO和ORPO）对32B模型进行微调，以提升量子电路代码生成的准确性和纠错能力，并在Qiskit HumanEval基准测试中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 量子电路代码生成过程中常出现错误，现有的通用模型（如Granite和StarCoder）表现不佳，因此需要开发针对性的优化方法。

Method: 采用了Group Relative Policy Optimization (GRPO)和Odds-Ratio Preference Optimization (ORPO)两种强化学习方法，基于人工标注的合成数据集对32B模型进行微调。

Result: 在Qiskit HumanEval基准测试中，ORPO的Pass@1达到56.29%，GRPO为49%，均优于通用基线模型；在原始HumanEval测试中，ORPO和GRPO分别达到65.90%和63.00%。

Conclusion: GRPO和ORPO在基础和中级任务中表现优异，但在高级任务中仍有提升空间，显示了AI辅助量子编程的潜力与挑战。

Abstract: Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and
StarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two
RL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference
Optimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit
HumanEval benchmark, ORPO reaches 56.29\% Pass@1 ($\approx+10$ pp over
Granite-8B-QK) and GRPO hits 49\%, both beating all general-purpose baselines;
on the original HumanEval they score 65.90\% and 63.00\%. GRPO excels on basic
tasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five
advanced tasks, highlighting clear gains yet room for progress in AI-assisted
quantum programming.

</details>


### [8] [A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain](https://arxiv.org/abs/2507.12649)
*Christine van Stiphoudt,Sergio Potenciano Menci,Gilbert Fridgen*

Main category: cs.SE

TL;DR: 本文针对智能电网中新设计的信息和数据模型缺乏明确评估步骤的问题，提出了一种结合显式和隐式方法的三阶段评估方法。


<details>
  <summary>Details</summary>
Motivation: 智能电网数字化导致信息交换增加，现有模型不足，需在新模型设计阶段进行有效评估以避免潜在问题。

Method: 采用设计科学研究，设计了一个结合显式和隐式评估方法的三阶段评估方法，并以工业灵活性描述模型为案例进行验证。

Result: 提出了一种适用于新模型开发的评估方法，并通过案例研究验证了其有效性。

Conclusion: 该方法填补了智能电网中新模型评估的空白，提供了清晰的评估步骤和经验总结。

Abstract: The ongoing digitalisation of the smart grid is resulting in an increase in
automated information exchanges across distributed energy systems. This process
has led to the development of new information and data models when the existing
ones fall short. To prevent potential disruptions caused by flaws in the newly
designed information and data models, it is essential to evaluate them during
the design process before they are implemented in operation.
  Currently, general explicit evaluation approaches outside the smart grid
domain stay at a high level without defining clear steps. Meanwhile, implicit
evaluation approaches in the smart grid domain focus on testing systems that
utilise information and data models already in use for functionality in terms
of conformance and interoperability. Notably, no combination of explicit and
implicit evaluation approaches for newly designed information and data models
offers a clearly defined set of steps during their design process in the smart
grid context.
  Consequently, we design a three-phase evaluation approach using design
science research to address this gap. Our evaluation approach combines explicit
and implicit evaluation methods and is applicable when developing new
information and data models. We use the development of an information model and
data model focused on industrial flexibility descriptions to refine our
evaluation approach. Additionally, we provide lessons learned from our
experience.

</details>


### [9] [A Fuzzy Approach to Project Success: Measuring What Matters](https://arxiv.org/abs/2507.12653)
*João Granja-Correia,Remedios Hernández-Linares,Luca Ferranti,Arménio Rego*

Main category: cs.SE

TL;DR: 该论文提出了一种通过将模糊逻辑整合到现有结构中，用于更准确评估项目成功的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统的Likert量表测量常常忽略了项目成功的多方面和上下文依赖性，因此需要一种更动态的方法。

Method: 提出了一个分层的Type-1 Mamdani模糊系统，优先考虑对最终用户的持续积极影响，而非次级结果（如利益相关者满意度和内部项目成功）。

Result: 这种方法可能提供更准确的项目成功测量，并适用于复杂评估。

Conclusion: 未来的研究将集中在模糊逻辑在社会科学中的实证测试和更广泛应用上。

Abstract: This paper introduces a novel approach to project success evaluation by
integrating fuzzy logic into an existing construct. Traditional Likert-scale
measures often overlook the context-dependent and multifaceted nature of
project success. The proposed hierarchical Type-1 Mamdani fuzzy system
prioritizes sustained positive impact for end-users, reducing emphasis on
secondary outcomes like stakeholder satisfaction and internal project success.
This dynamic approach may provide a more accurate measure of project success
and could be adaptable to complex evaluations. Future research will focus on
empirical testing and broader applications of fuzzy logic in social science.

</details>


### [10] [Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development](https://arxiv.org/abs/2507.12665)
*Salvador D. Escobedo*

Main category: cs.SE

TL;DR: 提出了单对话方法（SCM），这是一种利用大型语言模型（LLMs）进行软件开发的新方法，强调结构化、持续的开发对话，取代临时性的交互。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs的依赖过于被动，SCM旨在恢复开发者在项目中的主动角色，强调清晰性、可追溯性和模块化。

Method: SCM通过单一、长上下文的对话，覆盖项目从需求到架构和实现的各个阶段，采用结构化对话和文档化原则。

Result: SCM为开发者提供了更清晰、可追溯的开发流程，减少了临时性交互带来的问题。

Conclusion: SCM是对当前LLMs使用方式的重要修正，突出了开发者的主动性和对工具的监督作用。

Abstract: We propose the Single Conversation Methodology (SCM), a novel and pragmatic
approach to software development using large language models (LLMs). In
contrast to ad hoc interactions with generative AI, SCM emphasizes a structured
and persistent development dialogue, where all stages of a project - from
requirements to architecture and implementation - unfold within a single,
long-context conversation. The methodology is grounded on principles of
cognitive clarity, traceability, modularity, and documentation. We define its
phases, best practices, and philosophical stance, while arguing that SCM offers
a necessary correction to the passive reliance on LLMs prevalent in current
practices. We aim to reassert the active role of the developer as architect and
supervisor of the intelligent tool.

</details>


### [11] [Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases](https://arxiv.org/abs/2507.13035)
*Keila Lucas,Rohit Gheyi,Márcio Ribeiro,Fabio Palomba,Luana Martins,Elvys Soares*

Main category: cs.SE

TL;DR: 小型语言模型（SLMs）能自动检测测试异味，提升测试质量和可维护性。


<details>
  <summary>Details</summary>
Motivation: 手动测试虽重要，但存在质量问题（如模糊、冗余或缺失检查），传统工具需手动定义规则且缺乏扩展性。

Method: 评估Gemma3、Llama3.2和Phi-4在143个Ubuntu测试用例中检测7种测试异味的能力。

Result: Phi-4表现最佳，检测句子异味的pass@2达97%；SLMs还能自动解释问题并提出改进建议。

Conclusion: SLMs无需依赖规则定义或语法分析，可高效检测测试异味，保障数据隐私，提升测试质量。

Abstract: Manual testing, in which testers follow natural language instructions to
validate system behavior, remains crucial for uncovering issues not easily
captured by automation. However, these test cases often suffer from test
smells, quality issues such as ambiguity, redundancy, or missing checks that
reduce test reliability and maintainability. While detection tools exist, they
typically require manual rule definition and lack scalability. This study
investigates the potential of Small Language Models (SLMs) for automatically
detecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143
real-world Ubuntu test cases, covering seven types of test smells. Phi-4
achieved the best results, reaching a pass@2 of 97% in detecting sentences with
test smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond
detection, SLMs autonomously explained issues and suggested improvements, even
without explicit prompt instructions. They enabled low-cost, concept-driven
identification of diverse test smells without relying on extensive rule
definitions or syntactic analysis. These findings highlight the potential of
SLMs as efficient tools that preserve data privacy and can improve test quality
in real-world scenarios.

</details>


### [12] [iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development](https://arxiv.org/abs/2507.13081)
*Dongming Jin,Weisong Sun,Jiangping Huang,Peng Liang,Jifeng Xuan,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 提出了一种名为iReDev的知识驱动多智能体框架，用于智能需求开发，支持全流程需求开发、人类知识整合和人类-智能体协作，并在评估中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究对需求开发的支持有限，且忽视了人类知识的注入和人类-智能体协作，因此需要一种新的框架解决这些问题。

Method: iReDev框架包含六个知识驱动的智能体，支持完整的需求开发流程，通过事件驱动的通信机制和人工干预机制实现协作和灵活性。

Result: 评估表明，iReDev在多个方面优于现有基线，生成的工件更符合利益相关者的期望。

Conclusion: iReDev框架为智能需求开发提供了有效支持，并展望了未来三个关键方向，推动了该领域的发展。

Abstract: Requirements development is a critical phase as it is responsible for
providing a clear understanding of what stakeholders need. It involves
collaboration among stakeholders to extract explicit requirements and address
potential conflicts, which is time-consuming and labor-intensive. Recently,
multi-agent systems for software development have attracted much attention.
However, existing research provides limited support for requirements
development and overlooks the injection of human knowledge into agents and the
human-agent collaboration. % To address these issues, this paper proposes a
knowledge-driven multi-agent framework for intelligent requirement development,
named iReDev. iReDev features: iReDev consists of six knowledge-driven agents
to support the entire requirements development. They collaboratively perform
various tasks to produce a software requirements specification. iReDev focuses
on integrating human knowledge for agents, enabling them to simulate real-world
stakeholders. iReDev uses an event-driven communication mechanism based on an
artifact pool. Agents continuously monitor the pool and autonomously trigger
the next action based on its changes, enabling iReDev to handle new
requirements quickly. iReDev introduces a human-in-the-loop mechanism to
support human-agent collaboration, ensuring that the generated artifacts align
with the expectations of stakeholders. We evaluated the generated artifacts and
results show that iReDev outperforms existing baselines in multiple aspects. We
further envision three key directions and hope this work can facilitate the
development of intelligent requirements development.

</details>


### [13] [A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems](https://arxiv.org/abs/2507.13095)
*Dongming Jin,Zhi Jin,Linyu Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: 论文探讨了大预训练模型在现代软件系统中的集成及其对需求工程的挑战，提出了一个适应预训练模型特性（如能力边界模糊、上下文依赖行为等）的概念框架和研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大预训练模型在软件系统中的广泛应用，其独特的特性（如不确定性、演化性）对传统的需求工程假设（如功能可分解性、行为可预测性）提出了挑战，需要重新思考现有方法。

Method: 提出一个针对预训练模型驱动的软件系统的需求工程概念框架，并围绕该框架规划研究方向。

Result: 论文为研究者和实践者提供了应对预训练模型系统需求工程挑战的指导。

Conclusion: 预训练模型改变了软件系统的特性，需求工程需要新的方法来适应这些变化，论文提出的框架和研究方向为此提供了基础。

Abstract: Recent advances in large pretrained models have led to their widespread
integration as core components in modern software systems. The trend is
expected to continue in the foreseeable future. Unlike traditional software
systems governed by deterministic logic, systems powered by pretrained models
exhibit distinctive and emergent characteristics, such as ambiguous capability
boundaries, context-dependent behavior, and continuous evolution. These
properties fundamentally challenge long-standing assumptions in requirements
engineering, including functional decomposability and behavioral
predictability. This paper investigates this problem and advocates for a
rethinking of existing requirements engineering methodologies. We propose a
conceptual framework tailored to requirements engineering of
pretrained-model-enabled software systems and outline several promising
research directions within this framework. This vision helps provide a guide
for researchers and practitioners to tackle the emerging challenges in
requirements engineering of pretrained-model-enabled systems.

</details>


### [14] [Inferring Attributed Grammars from Parser Implementations](https://arxiv.org/abs/2507.13117)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.SE

TL;DR: 论文提出了一种从解析器实现中推断属性语法的新方法，动态分析递归下降解析器以重建输入处理的语义方面。


<details>
  <summary>Details</summary>
Motivation: 由于处理结构化输入的软件系统往往缺乏完整且最新的规范，尤其是输入处理的语义方面，研究旨在填补这一空白。

Method: 通过动态分析递归下降解析器的实现，将程序运行时行为映射到语法中，提取并嵌入语义动作到语法规则中。

Result: 实验结果表明，该方法能够通过生成的属性语法准确重现程序行为。

Conclusion: 该方法为输入处理语义的规范恢复提供了一种有效的解决方案。

Abstract: Software systems that process structured inputs often lack complete and
up-to-date specifications, which specify the input syntax and the semantics of
input processing. While grammar mining techniques have focused on recovering
syntactic structures, the semantics of input processing remains largely
unexplored. In this work, we introduce a novel approach for inferring
attributed grammars from parser implementations. Given an input grammar, our
technique dynamically analyzes the implementation of recursive descent parsers
to reconstruct the semantic aspects of input handling, resulting in
specifications in the form of attributed grammars. By observing program
executions and mapping the program's runtime behavior to the grammar, we
systematically extract and embed semantic actions into the grammar rules. This
enables comprehensive specification recovery. We demonstrate the feasibility of
our approach using an initial set of programs, showing that it can accurately
reproduce program behavior through the generated attributed grammars.

</details>


### [15] [Detecting LLM-generated Code with Subtle Modification by Adversarial Training](https://arxiv.org/abs/2507.13123)
*Xin Yin,Xinrui Li,Chao Ni,Xiaodan Xu,Xiaohu Yang*

Main category: cs.SE

TL;DR: 论文讨论了LLM生成代码的检测问题，提出CodeGPTSensor+以增强对修改后代码的检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的广泛应用，如何检测其并确保合规使用成为关键问题。现有方法在面对修改后的代码时表现不佳，缺乏有效解决方案。

Method: 提出CodeGPTSensor+，通过对抗训练增强鲁棒性，引入MIST模块生成对抗样本。

Result: 在HMCorp数据集上，CodeGPTSensor+显著提高了对抗测试集的检测准确率，同时保持原始测试集的高准确率。

Conclusion: CodeGPTSensor+在检测修改后的LLM生成代码方面表现出显著优势，为解决现实场景中的问题提供了有效方法。

Abstract: With the rapid development of Large Language Models (LLMs), their powerful
code-generation capabilities have been widely applied in tasks like code
completion and automated development, demonstrating the value of improving
coding efficiency. However, the extensive use of LLM-generated code also raises
several new challenges. On the one hand, issues such as the regulation of code
provenance, copyright disputes, and code quality have become increasingly
concerning. How to effectively detect LLM-generated code and ensure its
compliant and responsible use has become a critical and urgent issue. On the
other hand, in practical applications, LLM-generated code is often subject to
manual modifications, such as variable renaming or structural adjustments.
Although some recent studies have proposed training-based and zero-shot methods
for detecting LLM-generated code, these approaches show insufficient robustness
when facing modified LLM-generated code, and there is a lack of an effective
solution. To address the real-world scenario where LLM-generated code may
undergo minor modifications, we propose CodeGPTSensor+, an enhanced version of
CodeGPTSensor, which employs adversarial training to improve robustness against
input perturbations. CodeGPTSensor+ integrates an adversarial sample generation
module, Multi-objective Identifier and Structure Transformation (MIST), which
systematically generates both high-quality and representative adversarial
samples. This module effectively enhances the model's resistance against
diverse adversarial attacks. Experimental results on the HMCorp dataset
demonstrate that CodeGPTSensor+ significantly improves detection accuracy on
the adversarial test set while maintaining high accuracy on the original test
set, showcasing superior robustness compared to CodeGPTSensor.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Dual-Numbers Reverse AD for Functional Array Languages](https://arxiv.org/abs/2507.12640)
*Tom Smeding,Mikołaj Konarski,Simon Peyton Jones,Andrew Fitzgibbon*

Main category: cs.PL

TL;DR: 本文提出了一种支持多维数组的双数反向模式自动微分（AD）方法，通过向量化代码转换和符号解释实现高性能，但牺牲了部分通用性。


<details>
  <summary>Details</summary>
Motivation: 现有的双数反向模式AD在数组程序上性能不足，本文旨在解决这一问题，同时保持方法的简单性。

Method: 方法包括三个部分：语义保留的向量化代码转换（BOT）、基本双数反向AD算法的扩展，以及符号解释以实现端到端编译。

Result: 实现了对多维数组的高效支持，性能损失极小，但牺牲了高阶代码的通用性。

Conclusion: 通过牺牲部分通用性，本文方法显著提升了双数反向AD在数组程序上的性能，适用于特定场景。

Abstract: The standard dual-numbers construction works well for forward-mode automatic
differentiation (AD) and is attractive due to its simplicity; recently, it also
has been adapted to reverse-mode AD, but practical performance, especially on
array programs, leaves a lot to be desired. In this paper we introduce
first-class support for multidimensional arrays in dual-numbers reverse-mode AD
with little to no performance overhead. The algorithm consists of three
loosely-coupled components: a semantics-preserving vectorisation code
transformation (the bulk-operation transform or BOT), a fairly straightforward
lifting of the basic dual-numbers reverse AD algorithm to a mostly first-order
array language, and symbolic interpretation to achieve an end-to-end
compilation pipeline. Unfortunately, we lose some of the nice generalisable
aspects of dual-numbers AD in the process, most importantly support for
higher-order code.
  We do support some higher-order array combinators, but only a
carefully-chosen set: 'build' (elementwise array construction), 'gather' and
'scatter'. In return, the BOT can eliminate the essential (for AD)
higher-orderness of the input program, meaning that AD gets essentially
presented with a first-order program. This allows the naive trick of lifting
dual numbers to "dual arrays" to work without much modification.

</details>


### [17] [Formal Verification for JavaScript Regular Expressions: a Proven Semantics and its Applications](https://arxiv.org/abs/2507.13091)
*Aurèle Barrière,Victor Deng,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: 本文提出了首个机械化、简洁、实用、完整且经过验证的现代正则表达式语义，并通过与ECMAScript规范的对比证明其忠实性。展示了两个实际应用：上下文等价性证明和PikeVM算法的形式化验证。


<details>
  <summary>Details</summary>
Motivation: 为了填补现代正则表达式语义的机械化验证空白，并确保其与官方规范的等价性。

Method: 通过机械化证明实现语义的忠实性，并结合两个实际应用验证其实用性。

Result: 成功开发了完整的正则表达式语义，并验证了其正确性及实用性。

Conclusion: 本文的机械化语义为正则表达式的理论和应用提供了重要支持。

Abstract: We present the first mechanized, succinct, practical, complete, and
proven-faithful semantics for a modern regular expression language with
backtracking semantics. We ensure its faithfulness by proving it equivalent to
a preexisting line-by-line embedding of the official ECMAScript specification
of JavaScript regular expressions. We demonstrate its practicality by
presenting two real-world applications. First, a new notion of contextual
equivalence for modern regular expressions, which we use to prove or disprove
rewrites drawn from previous work. Second, the first formal proof of the PikeVM
algorithm used in many real-world engines. In contrast with the specification
and other formalization work, our semantics captures not only the top-priority
match, but a full backtracking tree recording all possible matches and their
respective priority. All our definitions and results have been mechanized in
the Rocq proof assistant.

</details>


### [18] [Towards Formal Verification of LLM-Generated Code from Natural Language Prompts](https://arxiv.org/abs/2507.13290)
*Aaron Councilman,David Fu,Aryan Gupta,Chengxiao Wang,David Grove,Yu-Xiong Wang,Vikram Adve*

Main category: cs.PL

TL;DR: LLMs生成的代码常不准确，提出基于形式化查询语言的验证系统Astrogator，验证代码正确性，提升AI编程助手体验。


<details>
  <summary>Details</summary>
Motivation: LLMs生成的代码错误多且难以检测，希望通过形式化保证提升代码正确性，降低用户使用门槛。

Method: 结合形式化查询语言表达用户意图，并用符号解释器验证LLM生成代码，应用于Ansible语言。

Result: 在21个代码生成任务中，验证器正确验证83%的正确代码，识别92%的错误代码。

Conclusion: 形式化方法有效提升LLM生成代码的正确性验证能力。

Abstract: In the past few years LLMs have emerged as a tool that can aid programmers by
taking natural language descriptions and generating code based on it. However,
LLMs often generate incorrect code that users need to fix and the literature
suggests users often struggle to detect these errors. In this work we seek to
offer formal guarantees of correctness to LLM generated code; such guarantees
could improve the experience of using AI Code Assistants and potentially enable
natural language programming for users with little or no programming knowledge.
To address this challenge we propose to incorporate a formal query language
that can represent a user's intent in a formally defined but natural
language-like manner that a user can confirm matches their intent. Then, using
such a query we propose to verify LLM generated code to ensure it matches the
user's intent. We implement these ideas in our system, Astrogator, for the
Ansible programming language which includes such a formal query language, a
calculus for representing the behavior of Ansible programs, and a symbolic
interpreter which is used for the verification. On a benchmark suite of 21
code-generation tasks, our verifier is able to verify correct code in 83% of
cases and identify incorrect code in 92%.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [19] [Design and Reliability of a User Space Write-Ahead Log in Rust](https://arxiv.org/abs/2507.13062)
*Vitor K. F. Pellegatti,Gustavo M. D. Vieira*

Main category: cs.OS

TL;DR: 本文介绍了用Rust创建用户空间预写日志（WAL）原型的经验，验证了Rust的高效和低开销。


<details>
  <summary>Details</summary>
Motivation: 预写日志（WAL）是计算机科学中重要的容错技术，但需在保证高性能的同时确保可靠性。本文旨在探索Rust语言在实现WAL时的适用性。

Method: 使用Rust语言开发了一个用户空间的WAL原型，并评估其性能和可靠性。

Result: Rust易于使用且库丰富，WAL原型的性能和稳定内存设备的预期性能相当，开销极低。

Conclusion: Rust适合实现高性能、低开销的WAL，验证了其在实际应用中的潜力。

Abstract: Write-ahead logs (WALs) are a fundamental fault-tolerance technique found in
many areas of computer science. WALs must be reliable while maintaining high
performance, because all operations will be written to the WAL to ensure their
stability. Without reliability a WAL is useless, because its utility is tied to
its ability to recover data after a failure. In this paper we describe our
experience creating a prototype user space WAL in Rust. We observed that Rust
is easy to use, compact and has a very rich set of libraries. More importantly,
we have found that the overhead is minimal, with the WAL prototype operating at
basically the expected performance of the stable memory device.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Energy-Efficient RSMA-enabled Low-altitude MEC Optimization Via Generative AI-enhanced Deep Reinforcement Learning](https://arxiv.org/abs/2507.12910)
*Xudong Wang,Hongyang Du,Lei Feng,Kaibin Huang*

Main category: cs.NI

TL;DR: 该论文研究了基于无人机（UAV）的低空移动边缘计算（MEC）系统，提出了一种基于速率分割多址接入（RSMA）的方案，以解决地面终端（GTs）的上行干扰问题。通过联合优化无人机轨迹、RSMA解码顺序、任务卸载决策和资源分配，结合生成式AI增强的深度强化学习（DRL）框架，显著提升了能量效率。


<details>
  <summary>Details</summary>
Motivation: 6G对低延迟计算的需求推动了对低空MEC系统的研究，但有限的频谱资源导致地面终端（GTs）面临严重的上行干扰。论文旨在通过RSMA技术和优化框架减少干扰并提高能量效率。

Method: 论文提出了一种生成式AI增强的DRL框架，嵌入扩散模型以生成高质量动作样本，优化了混合动作空间的探索。此外，设计了基于优先级的RSMA解码策略，实现低复杂度的干扰消除。

Result: 仿真结果表明，所提方法在低空MEC系统中优于基线方法，且结合GDM与RSMA显著提升能量效率。

Conclusion: 通过RSMA和生成式AI增强的DRL框架，论文成功解决了低空MEC系统中的多用户干扰问题并优化了能量效率，为6G低延迟计算提供了可行方案。

Abstract: The growing demand for low-latency computing in 6G is driving the use of
UAV-based low-altitude mobile edge computing (MEC) systems. However, limited
spectrum often leads to severe uplink interference among ground terminals
(GTs). In this paper, we investigate a rate-splitting multiple access
(RSMA)-enabled low-altitude MEC system, where a UAV-based edge server assists
multiple GTs in concurrently offloading their tasks over a shared uplink. We
formulate a joint optimization problem involving the UAV 3D trajectory, RSMA
decoding order, task offloading decisions, and resource allocation, aiming to
mitigate multi-user interference and maximize energy efficiency. Given the high
dimensionality, non-convex nature, and dynamic characteristics of this
optimization problem, we propose a generative AI-enhanced deep reinforcement
learning (DRL) framework to solve it efficiently. Specifically, we embed a
diffusion model into the actor network to generate high-quality action samples,
improving exploration in hybrid action spaces and avoiding local optima. In
addition, a priority-based RSMA decoding strategy is designed to facilitate
efficient successive interference cancellation with low complexity. Simulation
results demonstrate that the proposed method for low-altitude MEC systems
outperforms baseline methods, and that integrating GDM with RSMA can achieve
significantly improved energy efficiency performance.

</details>


### [21] [RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents](https://arxiv.org/abs/2507.13140)
*Kuiyuan Ding,Caili Guo,Yang Yang,Jianzhang Guo*

Main category: cs.NI

TL;DR: 提出了RIDAS框架，通过多智能体系统（RDAs和IDA）解决6G网络中用户意图与底层配置之间的差距，实验表明其比WirelessAgent支持更多用户。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要将AI紧密集成到RAN中以满足严格的QoS和资源效率要求，现有解决方案难以弥合高级用户意图与底层参数化配置之间的差距。

Method: RIDAS框架包含RDAs（提供可调参数接口）和IDA（利用LLM驱动的两阶段规划方案映射用户意图到最优配置）。

Result: 实验显示，RIDAS在相同QoS约束下比WirelessAgent支持44.71%更多用户。

Conclusion: RIDAS能有效捕捉用户意图并在AI RAN环境中更高效分配资源。

Abstract: Sixth generation (6G) networks demand tight integration of artificial
intelligence (AI) into radio access networks (RANs) to meet stringent quality
of service (QoS) and resource efficiency requirements. Existing solutions
struggle to bridge the gap between high level user intents and the low level,
parameterized configurations required for optimal performance. To address this
challenge, we propose RIDAS, a multi agent framework composed of representation
driven agents (RDAs) and an intention driven agent (IDA). RDAs expose open
interface with tunable control parameters (rank and quantization bits, enabling
explicit trade) offs between distortion and transmission rate. The IDA employs
a two stage planning scheme (bandwidth pre allocation and reallocation) driven
by a large language model (LLM) to map user intents and system state into
optimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\%
more users than WirelessAgent under equivalent QoS constraints. These results
validate ability of RIDAS to capture user intent and allocate resources more
efficiently in AI RAN environments.

</details>


### [22] [Predictability-Aware Motion Prediction for Edge XR via High-Order Error-State Kalman Filtering](https://arxiv.org/abs/2507.13179)
*Ziyu Zhong,Hector A Caltenco,Björn Landfeldt,Günter Alce*

Main category: cs.NI

TL;DR: 6G网络将支持XR应用的卸载，降低延迟和电池需求，使设备更轻薄。


<details>
  <summary>Details</summary>
Motivation: 研究6G网络如何通过卸载XR应用的渲染等计算密集型任务，提升性能和设备设计。

Method: 利用6G的低延迟和边缘处理基础设施，将计算任务从用户设备迁移到网络。

Result: 降低用户设备的电池需求，并支持设计更小尺寸的设备。

Conclusion: 6G网络为XR应用卸载提供可行方案，显著改善用户体验和设备设计。

Abstract: As 6G networks are developed and defined, offloading of XR applications is
emerging as one of the strong new use cases. The reduced 6G latency coupled
with edge processing infrastructure will for the first time provide a realistic
offloading scenario in cellular networks where several computationally
intensive functions, including rendering, can migrate from the user device and
into the network. A key advantage of doing so is the lowering of the battery
needs in the user devices and the possibility to design new devices with
smaller form factors.

</details>


### [23] [Bidirectional Age of Incorrect Information: A Performance Metric for Status Updates in Virtual Dynamic Environments](https://arxiv.org/abs/2507.13312)
*Chiara Schiavo,Manuele Favero,Alessandro Buratto,Leonardo Badia*

Main category: cs.NI

TL;DR: 本文提出了双向错误信息年龄（BAoII）来量化虚拟动态环境（VDE）中由于信息不准确或过时而产生的惩罚，并通过马尔可夫链模型和数值模拟验证了其对系统性能评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 在虚拟动态环境（如元宇宙和数字孪生）中，保持实体表示的准确性和实时性对系统可靠性和无缝交互至关重要，本文旨在解决这一问题。

Method: 提出双向错误信息年龄（BAoII）概念，使用连续时间马尔可夫链模型推导其长期闭式表达式，并通过数值模拟验证。

Result: 确定了传输成本阈值以实现最优更新策略，并展示了通信成本与信息新鲜度之间的权衡。

Conclusion: BAoII有助于评估虚拟动态环境中的系统性能，对实时协作具有重要意义。

Abstract: Virtual dynamic environments (VDEs) such as the Metaverse and digital twins
(DTs) require proper representation of the interacting entities to map their
characteristics within the simulated or augmented space. Keeping these
representations accurate and up-to-date is crucial for seamless interaction and
system reliability. In this paper, we propose bidirectional age of incorrect
information (BAoII) to address this aspect. BAoII quantifies the time-dependent
penalty paid by an entity in a VDE due to incorrect or outdated knowledge about
itself and the overall dynamically changing space. This extends the concept of
age of incorrect information for a bidirectional information exchange,
capturing that a VDE requires mutual awareness of the entity's own
representation, measured in the virtual space, and what the other entities
share about their representations. Using a continuous-time Markov chain model,
we derive a closed-form expression for long-term BAoII and identify a
transmission cost threshold for optimal update strategies. We describe a
trade-off between communication cost and information freshness and validate our
model through numerical simulations, demonstrating the impact of BAoII on
evaluating system performance and highlighting its relevance for real-time
collaboration in the Metaverse and DTs.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [24] [Dependency Pairs for Expected Innermost Runtime Complexity and Strong Almost-Sure Termination of Probabilistic Term Rewriting](https://arxiv.org/abs/2507.12918)
*Jan-Christoph Kassing,Leon Spitzer,Jürgen Giesl*

Main category: cs.LO

TL;DR: 介绍了首个依赖对（DP）框架，用于分析概率项重写系统的预期复杂性和证明强几乎确定终止（SAST），并通过工具AProVE验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 自动分析概率项重写系统的预期复杂性和终止性是当前未充分探索的领域，研究旨在填补这一空白。

Method: 提出了一种依赖对框架，用于分析预期复杂性和证明强几乎确定终止，并在工具AProVE中实现。

Result: 通过AProVE验证了该方法在证明强几乎确定终止上的有效性，超越了现有技术。

Conclusion: 该研究为概率项重写系统的复杂性和终止性分析提供了首个有效的自动解决方案。

Abstract: The dependency pair (DP) framework is one of the most powerful techniques for
automatic termination and complexity analysis of term rewrite systems. While
DPs were extended to prove almost-sure termination of probabilistic term
rewrite systems (PTRSs), automatic complexity analysis for PTRSs is largely
unexplored. We introduce the first DP framework for analyzing expected
complexity and for proving positive or strong almost-sure termination (SAST) of
innermost rewriting with PTRSs, i.e., finite expected runtime. We implemented
our framework in the tool AProVE and demonstrate its power compared to existing
techniques for proving SAST.

</details>


### [25] [Cyclic proof theory of positive inductive definitions](https://arxiv.org/abs/2507.13057)
*Gianluca Curzi,Lukas Melgaard*

Main category: cs.LO

TL;DR: 本文研究了循环证明系统与归纳证明系统在μPA中的证明理论强度等价性，并展示了循环证明与注释循环证明的等价性。


<details>
  <summary>Details</summary>
Motivation: 探索循环证明系统与归纳证明系统在μPA中的关系，利用二阶算术的片段进行非良基证明理论分析。

Method: 通过翻译循环证明为注释版本，并在Π^1_2-CA_0中形式化论证，利用Möllerfeld的保守性。

Result: 证明了循环证明与归纳证明在μPA中具有相同的证明理论强度，同时展示了注释循环证明与普通循环证明的等价性。

Conclusion: 这项研究进一步推动了通过非良基证明理论分析算术理论的进程，延续了Simpson和Das等人的工作。

Abstract: We study cyclic proof systems for $\mu\mathsf{PA}$, an extension of Peano
arithmetic by positive inductive definitions that is arithmetically equivalent
to the (impredicative) subsystem of second-order arithmetic
$\Pi^1_2$-$\mathsf{CA}_0$ by M\"{o}llefeld. The main result of this paper is
that cyclic and inductive $\mu\mathsf{PA}$ have the same proof-theoretic
strength. First, we translate cyclic proofs into an annotated variant based on
Sprenger and Dam's systems for first-order $\mu$-calculus, whose stronger
validity condition allows for a simpler proof of soundness. We then formalise
this argument within $\Pi^1_2$-$\mathsf{CA}_0$, leveraging M\"{o}llerfeld's
conservativity properties. To this end, we build on prior work by Curzi and Das
on the reverse mathematics of the Knaster-Tarski theorem. As a byproduct of our
proof methods we show that, despite the stronger validity condition, annotated
and "plain" cyclic proofs for $\mu\mathsf{PA}$ prove the same theorems. This
work represents a further step in the non-wellfounded proof-theoretic analysis
of theories of arithmetic via impredicative fragments of second-order
arithmetic, an approach initiated by Simpson's Cyclic Arithmetic, and continued
by Das and Melgaard in the context of arithmetical inductive definitions.

</details>


### [26] [Monotone weak distributive laws over the lifted powerset monad in categories of algebras](https://arxiv.org/abs/2507.13058)
*Quentin Aristote*

Main category: cs.LO

TL;DR: 研究单调弱分配律在代数和紧致Hausdorff空间中的相似性，探讨是否能自动从集合中的弱提升得到后者，并分析了其在其他代数范畴中的存在条件。


<details>
  <summary>Details</summary>
Motivation: 旨在探索单调弱分配律在不同代数结构中的存在性和通用性，特别是在概率与非确定性结合的场景。

Method: 通过比较集合和紧致Hausdorff空间中的单调弱分配律，研究其弱提升的可行性，并扩展到其他代数范畴的存在条件分析。

Result: 部分情况下可以实现弱提升，但无法推广到其他代数范畴；同时展示了紧致Hausdorff空间中概率与非确定性结合的分配律，但在多数其他情况下不存在此类律。

Conclusion: 单调弱分配律的存在性高度依赖于特定代数结构，无法普遍适用，但其在紧致Hausdorff空间中的应用展示了特定场景下的可能性。

Abstract: Noticing the similarity between the monotone weak distributive laws combining
two layers of nondeterminism in sets and in compact Hausdorff spaces, we study
whether the latter law can be obtained automatically as a weak lifting of the
former. This holds partially, but does not generalize to other categories of
algebras: we then characterize when exactly monotone weak distributive laws
over powerset monads in categories of algebras exist, exhibiting a law
combining probabilities and non-determinism in compact Hausdorff spaces and
showing on the other hand that such laws do not exist in a lot of other cases.

</details>


### [27] [Impact and Performance of Randomized Test-Generation using Prolog](https://arxiv.org/abs/2507.13178)
*Marcus Gelderie,Maximilian Luff,Maximilian Peltzer*

Main category: cs.LO

TL;DR: 该论文研究了使用Prolog随机生成系统测试输入序列的方法，探讨了随机化与SLD解析对测试性能的影响，并提出了两种随机化策略。


<details>
  <summary>Details</summary>
Motivation: 研究如何生成具有复杂逻辑依赖结构的测试序列，并解决测试集过大或无限时的问题。

Method: 提出了两种随机化策略：一种基于标准Prolog语义，另一种修改SLD选择函数，并通过马尔可夫链分析平均时间和生成测试用例数量。

Result: 通过实证评估比较了两种方法的性能。

Conclusion: 论文探讨了随机化在测试生成中的作用，为Prolog测试提供了新思路。

Abstract: We study randomized generation of sequences of test-inputs to a system using
Prolog. Prolog is a natural fit to generate test-sequences that have complex
logical inter-dependent structure. To counter the problems posed by a large (or
infinite) set of possible tests, randomization is a natural choice. We study
the impact that randomization in conjunction with SLD resolution have on the
test performance. To this end, this paper proposes two strategies to add
randomization to a test-generating program. One strategy works on top of
standard Prolog semantics, whereas the other alters the SLD selection function.
We analyze the mean time to reach a test-case, and the mean number of generated
test-cases in the framework of Markov chains. Finally, we provide an additional
empirical evaluation and comparison between both approaches. Under
consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [28] [Just Verification of Mutual Exclusion Algorithms](https://arxiv.org/abs/2507.13198)
*Rob van Glabbeek,Bas Luttik,Myrthe Spronck*

Main category: cs.LO

TL;DR: 通过模型检查验证多种互斥算法的正确性，关注共享寄存器通信下的算法，并使用公正性作为完整性标准。


<details>
  <summary>Details</summary>
Motivation: 验证互斥算法的正确性，尤其是针对不同共享寄存器模型的假设。

Method: 利用模型检查技术，假设公正性为完整性标准，并考虑不同的并发关系。

Result: 展示了多个算法违反正确性属性的执行案例，并提出改进建议。

Conclusion: 公正性和并发关系对验证互斥算法的正确性至关重要，部分算法需要改进。

Abstract: We verify the correctness of a variety of mutual exclusion algorithms through
model checking. We look at algorithms where communication is via shared
read/write registers, where those registers can be atomic or non-atomic. For
the verification of liveness properties, it is necessary to assume a
completeness criterion to eliminate spurious counterexamples. We use justness
as completeness criterion. Justness depends on a concurrency relation; we
consider several such relations, modelling different assumptions on the working
of the shared registers. We present executions demonstrating the violation of
correctness properties by several algorithms, and in some cases suggest
improvements.

</details>


### [29] [Solving SAT By Computing A Stable Set Of Points In Clusters](https://arxiv.org/abs/2507.13282)
*Eugene Goldberg*

Main category: cs.LO

TL;DR: 论文提出了一种通过聚类计算稳定点集（SSP）的新方法，取代了逐个点处理的方式，从而提高了效率。


<details>
  <summary>Details</summary>
Motivation: 由于实际的CNF公式的SSP非常庞大，逐个点计算不可行，因此需要一种更高效的计算方法。

Method: 通过将SSP的计算分为多个聚类，每个聚类代表一大组同时处理的点。

Result: 新方法能够更好地利用公式结构，设计更高效的SAT算法，并促进并行计算。

Conclusion: 聚类计算SSP的方法在效率和并行性上具有优势，为SAT求解提供了新思路。

Abstract: Earlier we introduced the notion of a stable set of points (SSP). We proved
that a CNF formula is unsatisfiable iff there is a set of points (i.e. complete
assignments) that is stable with respect to this formula. Experiments showed
that SSPs for CNF formulas of practical interest are very large. So computing
an SSP for a CNF formula point by point is, in general, infeasible. In this
report, we show how an SSP can be computed in clusters, each cluster being a
large set of points that are processed simultaneously. The appeal of computing
SSPs is twofold. First, it allows one to better take into account formula
structure and hence, arguably, design more efficient SAT algorithms. Second,
SAT solving by SSPs facilitates parallel computing.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [30] ["How to Explore Biases in Speech Emotion AI with Users?" A Speech-Emotion-Acting Study Exploring Age and Language Biases](https://arxiv.org/abs/2507.12580)
*Josephine Beatrice Skovbo Borre,Malene Gorm Wold,Sara Kjær Rasmussen,Ilhan Aslan*

Main category: cs.HC

TL;DR: 研究探讨了年龄和语言如何影响情绪的有意表达，重点关注了青少年和55岁以上成人两个群体，并测试了情感语音识别系统在多语言和不同年龄组中的表现。结果表明，语言和年龄对模型识别无明显影响，但高唤醒情绪识别存在局限。


<details>
  <summary>Details</summary>
Motivation: 填补情感语音识别系统中对特定年龄段和多语言群体的研究空白，评估现有模型是否能处理故意表达的情绪语音。

Method: 开发了结合用户界面和后端的实验范式，参与者需通过语音表达四种情绪，实时记录情感语音识别的表现。

Result: 语言和年龄组间无显著差异，模型表现出一定的跨语言和年龄鲁棒性，但高唤醒情绪识别较差。

Conclusion: 提出需要更包容、以人为本的情感语音识别模型，并指出情感表达中人类意图与机器解读存在偏差的风险。

Abstract: This study explores how age and language shape the deliberate vocal
expression of emotion, addressing underexplored user groups, Teenagers (N = 12)
and Adults 55+ (N = 12), within speech emotion recognition (SER). While most
SER systems are trained on spontaneous, monolingual English data, our research
evaluates how such models interpret intentionally performed emotional speech
across age groups and languages (Danish and English). To support this, we
developed a novel experimental paradigm combining a custom user interface with
a backend for real-time SER prediction and data logging. Participants were
prompted to hit visual targets in valence-arousal space by deliberately
expressing four emotion targets. While limitations include some reliance on
self-managed voice recordings and inconsistent task execution, the results
suggest contrary to expectations, no significant differences between language
or age groups, and a degree of cross-linguistic and age robustness in model
interpretation. Though some limitations in high-arousal emotion recognition
were evident. Our qualitative findings highlight the need to move beyond
system-centered accuracy metrics and embrace more inclusive, human-centered SER
models. By framing emotional expression as a goal-directed act and logging the
real-time gap between human intent and machine interpretation, we expose the
risks of affective misalignment.

</details>


### [31] [NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting](https://arxiv.org/abs/2507.12621)
*Kuangshi Ai,Kaiyuan Tang,Chaoli Wang*

Main category: cs.HC

TL;DR: NLI4VolVis是一个基于自然语言的交互式系统，用于探索、查询和编辑体积场景，解决了传统方法在设计灵活性和计算成本上的问题。


<details>
  <summary>Details</summary>
Motivation: 传统体积可视化方法存在设计僵化和计算成本高的问题，而现有视图合成方法需要额外学习且缺乏语义交互支持。NLI4VolVis旨在通过自然语言交互填补这一空白。

Method: 系统结合多视角语义分割和视觉语言模型，通过多代理大语言模型架构解析用户意图并执行可视化任务，利用3D可编辑高斯模型支持实时编辑和查询。

Result: NLI4VolVis提供了开放词汇的对象查询、实时场景编辑、最佳视图选择和2D风格化功能，并通过用户研究验证了其可用性和易用性。

Conclusion: NLI4VolVis显著提高了体积数据探索的灵活性和用户友好性，推荐读者查看案例和演示视频以了解更多细节。

Abstract: Traditional volume visualization (VolVis) methods, like direct volume
rendering, suffer from rigid transfer function designs and high computational
costs. Although novel view synthesis approaches enhance rendering efficiency,
they require additional learning effort for non-experts and lack support for
semantic-level interaction. To bridge this gap, we propose NLI4VolVis, an
interactive system that enables users to explore, query, and edit volumetric
scenes using natural language. NLI4VolVis integrates multi-view semantic
segmentation and vision-language models to extract and understand semantic
components in a scene. We introduce a multi-agent large language model
architecture equipped with extensive function-calling tools to interpret user
intents and execute visualization tasks. The agents leverage external tools and
declarative VolVis commands to interact with the VolVis engine powered by 3D
editable Gaussians, enabling open-vocabulary object querying, real-time scene
editing, best-view selection, and 2D stylization. We validate our system
through case studies and a user study, highlighting its improved accessibility
and usability in volumetric data exploration. We strongly recommend readers
check our case studies, demo video, and source code at
https://nli4volvis.github.io/.

</details>


### [32] [Design Patterns of Human-AI Interfaces in Healthcare](https://arxiv.org/abs/2507.12721)
*Rui Sheng,Chuhan Shi,Sobhan Lotfi,Shiyi Liu,Adam Perer,Huamin Qu,Furui Cheng*

Main category: cs.HC

TL;DR: 本文提出了一种系统化的设计指南，用于医疗场景中的人机交互界面，总结了12种设计模式，并通过访谈和研讨会验证其适用性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域中的人机交互界面设计具有挑战性，需要系统化的指导以优化信息呈现和交互方式。

Method: 通过总结12种设计模式，访谈12名医疗专业人员，并与14名在线参与者进行研讨会评估。

Result: 设计模式在医疗场景中表现出良好的适用性，同时探讨了其在其他领域的推广潜力。

Conclusion: 研究为医疗人机交互提供了实用设计指南，并指出了未来改进的方向。

Abstract: Human-AI interfaces play a crucial role in advancing practices and research
within the healthcare domain. However, designing such interfaces presents a
substantial challenge for designers. In this paper, we propose systematic
guidance for designing human-AI interfaces in typical healthcare scenarios by
summarizing the design patterns for presenting and interacting with common
information entities. To deepen our understanding of these 12 design patterns,
we interviewed 12 healthcare professionals to explore potential usage scenarios
and important considerations. Furthermore, we conducted workshops with 14
participants recruited online to evaluate our design patterns. Finally, we
discussed the generalizability of the design patterns to other application
domains, the limitations, and the future work.

</details>


### [33] [An Age-based Study into Interactive Narrative Visualization Engagement](https://arxiv.org/abs/2507.12734)
*Nina Errey,Yi Chen,Yu Dong,Quang Vinh Nguyen,Xiaoru Yuan,Tuck Wah Leong,Christy Jie Liang*

Main category: cs.HC

TL;DR: 研究表明，受众年龄影响其对数字媒体的参与度。交互式叙事可视化结合数据可视化和讲故事，但年龄因素常被忽视。实验发现，年轻群体比年长群体参与度更高，理解更深。建议设计时考虑年龄差异。


<details>
  <summary>Details</summary>
Motivation: 探讨受众年龄如何影响对交互式叙事可视化的参与度和理解，以填补这一研究空白。

Method: 使用可视化参与度问卷调查，通过实证实验比较不同年龄段的参与度。

Result: 年长群体的参与度低于年轻群体，且对交互式叙事模式的理解较弱。

Conclusion: 提出针对不同年龄群体的交互式叙事可视化设计建议，以实现更包容的设计。

Abstract: Research has shown that an audiences' age impacts their engagement in digital
media. Interactive narrative visualization is an increasingly popular form of
digital media that combines data visualization and storytelling to convey
important information. However, audience age is often overlooked by interactive
narrative visualization authors. Using an established visualization engagement
questionnaire, we ran an empirical experiment where we compared end-user
engagement to audience age. We found a small difference in engagement scores
where older age cohorts were less engaged than the youngest age cohort. Our
qualitative analysis revealed that the terminology and overall understanding of
interactive narrative patterns integrated into narrative visualization was more
apparent in the feedback from younger age cohorts relative to the older age
cohorts. We conclude this paper with a series of recommendations for authors of
interactive narrative visualization on how to design inclusively for audiences
according to their age.

</details>


### [34] [Public Evaluation on Potential Social Impacts of Fully Autonomous Cybernetic Avatars for Physical Support in Daily-Life Environments: Large-Scale Demonstration and Survey at Avatar Land](https://arxiv.org/abs/2507.12741)
*Lotfi El Hafi,Kazuma Onishi,Shoichi Hasegawa,Akira Oyama,Tomochika Ishikawa,Masashi Osada,Carl Tornberg,Ryoma Kado,Kento Murata,Saki Hashimoto,Sebastian Carrera Villalobos,Akira Taniguchi,Gustavo Alfonso Garcia Ricardez,Yoshinobu Hagiwara,Tatsuya Aoki,Kensuke Iwata,Takato Horii,Yukiko Horikawa,Takahiro Miyashita,Tadahiro Taniguchi,Hiroshi Ishiguro*

Main category: cs.HC

TL;DR: 论文研究了公众对完全自主的赛博格化身（CAs）在日常生活中的感知和社会影响，通过大阪的公开活动和调查显示人们对CAs的兴趣较高，但对任务可靠性表示担忧。


<details>
  <summary>Details</summary>
Motivation: 探索完全自主的赛博格化身在现实生活中的潜在应用和公众接受度，以填补半自主与完全自主CAs之间的研究空白。

Method: 在大阪的19天公开活动中展示完全自主与半自主CAs，并通过问卷调查2,285名参与者（其中333名为完全自主CAs的互动者）的反馈。

Result: 公众对CAs在日常和工作中的支持感兴趣，但对任务执行的可靠性表示担忧；成本和拟人化交互不是主要问题。

Conclusion: 完全自主CAs具有潜力，但需解决任务可靠性问题以提高公众接受度。

Abstract: Cybernetic avatars (CAs) are key components of an avatar-symbiotic society,
enabling individuals to overcome physical limitations through virtual agents
and robotic assistants. While semi-autonomous CAs intermittently require human
teleoperation and supervision, the deployment of fully autonomous CAs remains a
challenge. This study evaluates public perception and potential social impacts
of fully autonomous CAs for physical support in daily life. To this end, we
conducted a large-scale demonstration and survey during Avatar Land, a 19-day
public event in Osaka, Japan, where fully autonomous robotic CAs, alongside
semi-autonomous CAs, performed daily object retrieval tasks. Specifically, we
analyzed responses from 2,285 visitors who engaged with various CAs, including
a subset of 333 participants who interacted with fully autonomous CAs and
shared their perceptions and concerns through a survey questionnaire. The
survey results indicate interest in CAs for physical support in daily life and
at work. However, concerns were raised regarding task execution reliability. In
contrast, cost and human-like interaction were not dominant concerns. Project
page: https://lotfielhafi.github.io/FACA-Survey/.

</details>


### [35] [PatternSight: A Perceptual Grouping Effectiveness Assessment Approach for Graphical Patterns in Charts](https://arxiv.org/abs/2507.12749)
*Xumeng Wang,Xiangxuan Zhang,Zhiqi Gao,Shuangcheng Jiao,Yuxin Ma*

Main category: cs.HC

TL;DR: 论文提出了一个感知模拟模型，用于评估图表的感知效果，并结合原型工具PatternSight帮助图表作者优化设计。


<details>
  <summary>Details</summary>
Motivation: 图表生成工具的普及降低了创作门槛，但缺乏感知理论支持的作者可能难以评估图表效果。

Method: 通过感知模拟模型预测观众可能注意到的图形模式，并结合视觉特征提取提供可解释结果。

Result: 模型能模拟大多数观众的分组行为，PatternSight有效帮助作者优化设计。

Conclusion: 模型和工具的结合为图表设计提供了科学依据和实用支持。

Abstract: The boom in visualization generation tools has significantly lowered the
threshold for chart authoring. Nevertheless, chart authors with an insufficient
understanding of perceptual theories may encounter difficulties in evaluating
the effectiveness of chart representations, thereby struggling to identify the
appropriate chart design to convey the intended data patterns. To address this
issue, we propose a perception simulation model that can assess the perceptual
effectiveness of charts by predicting graphical patterns that chart viewers are
likely to notice. The perception simulation model integrates perceptual theory
into visual feature extraction of chart elements to provide interpretable model
outcomes. Human perceptual results proved that the outcome of our model can
simulate the perceptual grouping behaviors of most chart viewers and cover
diverse perceptual results. We also embed the model into a prototype interface
called PatternSight to facilitate chart authors in assessing whether the chart
design can satisfy their pattern representation requirements as expected and
determining feasible improvements of visual design. According to the results of
a user experiment, PatternSight can effectively assist chart authors in
optimizing chart design for representing data patterns.

</details>


### [36] [Autonomy for Older Adult-Agent Interaction](https://arxiv.org/abs/2507.12767)
*Jiaxin An*

Main category: cs.HC

TL;DR: 论文探讨AI代理如何支持老年人护理，重点关注与老年人自主权偏好对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着全球老龄化，AI代理可能成为支持老年人护理的工具，但如何确保其与老年人的自主权偏好一致是关键问题。

Method: 通过跨学科自主权概念化，提出老年人自主权的四个关键维度，并建议研究方向和测量方法。

Result: 明确了决策自主权、目标导向自主权、控制自主权和社会责任自主权四个维度。

Conclusion: 提出了社会责任自主权、任务视角的代理自主权操作化及自主权测量三个研究方向。

Abstract: As the global population ages, artificial intelligence (AI)-powered agents
have emerged as potential tools to support older adults' caregiving. Prior
research has explored agent autonomy by identifying key interaction stages in
task processes and defining the agent's role at each stage. However, ensuring
that agents align with older adults' autonomy preferences remains a critical
challenge. Drawing on interdisciplinary conceptualizations of autonomy, this
paper examines four key dimensions of autonomy for older adults:
decision-making autonomy, goal-oriented autonomy, control autonomy, and social
responsibility autonomy. This paper then proposes the following research
directions: (1) Addressing social responsibility autonomy, which concerns the
ethical and social implications of agent use in communal settings; (2)
Operationalizing agent autonomy from the task perspective; and (3) Developing
autonomy measures.

</details>


### [37] [Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication](https://arxiv.org/abs/2507.13052)
*Tianyu Song,Feng Li,Yuan Bi,Angelos Karlas,Amir Yousefi,Daniela Branzan,Zhongliang Jiang,Ulrich Eck,Nassir Navab*

Main category: cs.HC

TL;DR: 该研究提出了一种智能虚拟超声技师（IVS），通过扩展现实（XR）技术连接医生、机器人超声系统（RUS）和患者，提升交互效率和患者体验。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注患者-机器人或医生-机器人交互，而智能虚拟超声技师（IVS）在医生-机器人-患者沟通中的作用未被充分探索。

Method: 通过结合大语言模型（LLM）、语音识别、语音合成和机器人控制技术，开发了一个支持实时交互的IVS代理。

Result: 系统提高了机器人超声图像的获取效率、清晰度和可及性，增强了医生对机器人交互的信任，同时改善了患者体验。

Conclusion: 该研究为理解IVS如何弥补医生-机器人-患者沟通中的缺口提供了初步探索，为未来研究奠定了基础。

Abstract: The advancement and maturity of large language models (LLMs) and robotics
have unlocked vast potential for human-computer interaction, particularly in
the field of robotic ultrasound. While existing research primarily focuses on
either patient-robot or physician-robot interaction, the role of an intelligent
virtual sonographer (IVS) bridging physician-robot-patient communication
remains underexplored. This work introduces a conversational virtual agent in
Extended Reality (XR) that facilitates real-time interaction between
physicians, a robotic ultrasound system(RUS), and patients. The IVS agent
communicates with physicians in a professional manner while offering empathetic
explanations and reassurance to patients. Furthermore, it actively controls the
RUS by executing physician commands and transparently relays these actions to
the patient. By integrating LLM-powered dialogue with speech-to-text,
text-to-speech, and robotic control, our system enhances the efficiency,
clarity, and accessibility of robotic ultrasound acquisition. This work
constitutes a first step toward understanding how IVS can bridge communication
gaps in physician-robot-patient interaction, providing more control and
therefore trust into physician-robot interaction while improving patient
experience and acceptance of robotic ultrasound.

</details>


### [38] ["What do you expect? You're part of the internet": Analyzing Celebrities' Experiences as Usees of Deepfake Technology](https://arxiv.org/abs/2507.13065)
*John Twomey,Sarah Foley,Sarah Robinson,Michael Quayle,Matthew Peter Aylett,Conor Linehan,Gillian Murphy*

Main category: cs.HC

TL;DR: 摘要研究了名人如何应对非自愿的深度伪造亲密影像（NSII），分析了她们在寻求帮助时面临的基础设施和社会障碍。


<details>
  <summary>Details</summary>
Motivation: 探讨名人如何构建自己被深度伪造技术侵害的经历，以及她们在寻求救济时遇到的障碍。

Method: 采用批判性话语心理分析方法，分析八位名人和一位非二元性别个体的公开声明，运用Baumer的“Usee”概念。

Result: 名人描述了被非自愿深度伪造的危害和痛苦，揭示了基础设施和社会因素如何阻碍她们的维权行动。

Conclusion: 研究强调了多方利益相关者在深度伪造问题中的作用，并呼吁未来工作挑战NSII背后的动机和错误信念。

Abstract: Deepfake technology is often used to create non-consensual synthetic intimate
imagery (NSII), mainly of celebrity women. Through Critical Discursive
Psychological analysis we ask; i) how celebrities construct being targeted by
deepfakes and ii) how they navigate infrastructural and social obstacles when
seeking recourse. In this paper, we adopt Baumers concept of Usees
(stakeholders who are non-consenting, unaware and directly targeted by
technology), to understand public statements made by eight celebrity women and
one non-binary individual targeted with NSII. Celebrities describe harms of
being non-consensually targeted by deepfakes and the distress of becoming aware
of these videos. They describe various infrastructural/social factors (e.g.
blaming/ silencing narratives and the industry behind deepfake abuse) which
hinder activism and recourse. This work has implications in recognizing the
roles of various stakeholders in the infrastructures underlying deepfake abuse
and the potential of human-computer interaction to improve existing recourses
for NSII. We also contribute to understanding how false beliefs online
facilitate deepfake abuse. Future work should involve interventions which
challenge the values and false beliefs which motivate NSII
creation/dissemination.

</details>


### [39] [On tangible user interfaces, humans and spatiality](https://arxiv.org/abs/2507.13167)
*Ehud Sharlin,Benjamin Watson,Yoshifumi Kitamura,Fumio Kishino,Yuichi Itoh*

Main category: cs.HC

TL;DR: 论文探讨了如何利用人类对物体的空间直觉技能，为可触摸用户界面（TUI）设计提供启发式方法。


<details>
  <summary>Details</summary>
Motivation: 研究人类与物理对象的关系，以提升TUI设计的空间性。

Method: 通过观察和提炼启发式方法，分析现有空间TUI设计。

Result: 提出了一套将空间性融入TUI设计的启发式方法，并识别出空间TUI子集。

Conclusion: 空间性是TUI成功的关键，需在设计中被充分考虑。

Abstract: Like the prehistoric twig and stone, tangible user interfaces (TUIs) are
objects manipulated by humans. TUI success will depend on how well they exploit
spatiality, the intuitive spatial skills humans have with the objects they use.
In this paper we carefully examine the relationship between humans and physical
objects, and related previous research. From this examination we distill a set
of observations, and turn these into heuristics for incorporation of spatiality
into TUI application design, a cornerstone for their success. Following this
line of thought, we identify spatial TUIs, the subset of TUIs that mediate
interaction with shape, space and structure. We then examine several existing
spatial TUIs using our heuristics.

</details>


### [40] [Difficulty as a Proxy for Measuring Intrinsic Cognitive Load Item](https://arxiv.org/abs/2507.13235)
*Minghao Cai,Guher Gorgun,Carrie Demmans Epp*

Main category: cs.HC

TL;DR: 研究探讨了使用项目难度参数作为在线学习平台中认知负荷测量代理的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的认知负荷测量依赖于主观的自我报告方法，研究者希望找到更客观的替代方案。

Method: 利用项目反应理论提取项目难度参数，并将其与内在和外在负荷理论进行比较。

Result: 项目难度参数与认知负荷理论一致，表明其可作为内在负荷的代理指标。

Conclusion: 项目难度参数可用于学习游戏中认知负荷的建模。

Abstract: Cognitive load is key to ensuring an optimal learning experience. However,
measuring the cognitive load of educational tasks typically relies on
self-report measures which has been criticized by researchers for being
subjective. In this study, we investigated the feasibility of using item
difficulty parameters as a proxy for measuring cognitive load in an online
learning platform. Difficulty values that were derived using item-response
theory were consistent with theories of how intrinsic and extraneous load
contribute to cognitive load. This finding suggests that we can use item
difficulty to represent intrinsic load when modelling cognitive load in
learning games.

</details>


### [41] [RemVerse: Supporting Reminiscence Activities for Older Adults through AI-Assisted Virtual Reality](https://arxiv.org/abs/2507.13247)
*Ruohao Li,Jiawei Li,Jia Sun,Zhiqing Wu,Zisu Li,Ziyan Wang,Ge Lin Kan,Mingming Fan*

Main category: cs.HC

TL;DR: RemVerse是一个结合AI和VR的原型，旨在通过生成视觉线索和对话帮助老年人回忆往事，提升认知功能和幸福感。


<details>
  <summary>Details</summary>
Motivation: 城市化导致熟悉环境的消失，传统的照片无法完全重构回忆内容，利用VR和AI可以更有效地辅助回忆。

Method: 设计RemVerse原型，整合生成模型和AI代理，通过VR环境提供视觉线索和互动对话。

Result: 14名老年人的用户研究表明，RemVerse能有效触发、具体化和深化记忆，同时增强参与感和自主性。

Conclusion: 提出了设计建议，使AI辅助的VR回忆活动对老年人更易用和吸引人。

Abstract: Reminiscence activities, which involve recalling and sharing past
experiences, have proven beneficial for improving cognitive function, mood, and
overall well-being. However, urbanization has led to the disappearance of
familiar environments, removing visual and audio cues for effective
reminiscence. While old photos can serve as visual cues to aid reminiscence, it
is challenging for people to reconstruct the reminisced content and environment
that are not in the photos. Virtual reality (VR) and artificial intelligence
(AI) offer the ability to reconstruct an immersive environment with dynamic
content and to converse with people to help them gradually reminisce. We
designed RemVerse, an AI-empowered VR prototype aimed to support reminiscence
activities. Integrating generative models and AI agent into a VR environment,
RemVerse helps older adults reminisce with AI-generated visual cues and
interactive dialogues. Our user study with 14 older adults showed that RemVerse
effectively supported reminiscence activities by triggering, concretizing, and
deepening personal memories, while fostering increased engagement and autonomy
among older adults. Based on our findings, we proposed design implications to
make reminiscence activities in AI-assisted VR more accessible and engaging for
older adults.

</details>


### [42] [FocusView: Understanding and Customizing Informational Video Watching Experiences for Viewers with ADHD](https://arxiv.org/abs/2507.13309)
*Hanxiu 'Hazel' Zhu,Ruijia Chen,Yuhang Zhao*

Main category: cs.HC

TL;DR: FocusView是一种为ADHD患者设计的视频定制界面，能显著减少分心，提升视频的可观看性。


<details>
  <summary>Details</summary>
Motivation: 针对ADHD患者在观看信息视频时的注意力挑战，设计一个定制化视频界面以满足其独特需求。

Method: 开发FocusView界面，允许ADHD患者从多个方面定制视频内容，并通过12名ADHD参与者进行评测。

Result: FocusView有效减少了分心，同时揭示了ADHD患者对视频分心的多样感知（如背景音乐的双重作用）及其定制偏好。

Conclusion: 研究为未来针对ADHD社区的视频定制系统提供了设计考虑，如减少选项以避免定制本身的分心。

Abstract: While videos have become increasingly prevalent in delivering information
across different educational and professional contexts, individuals with ADHD
often face attention challenges when watching informational videos due to the
dynamic, multimodal, yet potentially distracting video elements. To understand
and address this critical challenge, we designed \textit{FocusView}, a video
customization interface that allows viewers with ADHD to customize
informational videos from different aspects. We evaluated FocusView with 12
participants with ADHD and found that FocusView significantly improved the
viewability of videos by reducing distractions. Through the study, we uncovered
participants' diverse perceptions of video distractions (e.g., background music
as a distraction vs. stimulation boost) and their customization preferences,
highlighting unique ADHD-relevant needs in designing video customization
interfaces (e.g., reducing the number of options to avoid distraction caused by
customization itself). We further derived design considerations for future
video customization systems for the ADHD community.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [43] [WaFusion: A Wavelet-Enhanced Diffusion Framework for Face Morph Generation](https://arxiv.org/abs/2507.12493)
*Seyed Rasoul Hosseini,Omid Ahmadieh,Jeremy Dawson,Nasser Nasrabadi*

Main category: cs.GR

TL;DR: WaFusion结合小波分解和扩散模型，高效生成高质量的面部变形图像，提升生物识别系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 面部生物特征变形对身份验证系统构成重大威胁，需要一种高效且高质量的方法来生成变形图像。

Method: 通过结合小波变换的结构细节捕捉能力和扩散模型的生成能力，减少变形中的伪影。

Result: 在多个数据集上验证了WaFusion的优越性，生成的变形图像分辨率高且伪影少，表现优于现有方法。

Conclusion: WaFusion为生物识别变形图像生成设定了新标准，提供了一种高效且先进的解决方案。

Abstract: Biometric face morphing poses a critical challenge to identity verification
systems, undermining their security and robustness. To address this issue, we
propose WaFusion, a novel framework combining wavelet decomposition and
diffusion models to generate high-quality, realistic morphed face images
efficiently. WaFusion leverages the structural details captured by wavelet
transforms and the generative capabilities of diffusion models, producing face
morphs with minimal artifacts. Experiments conducted on FERET, FRGC, FRLL, and
WVU Twin datasets demonstrate WaFusion's superiority over state-of-the-art
methods, producing high-resolution morphs with fewer artifacts. Our framework
excels across key biometric metrics, including the Attack Presentation
Classification Error Rate (APCER), Bona Fide Presentation Classification Error
Rate (BPCER), and Equal Error Rate (EER). This work sets a new benchmark in
biometric morph generation, offering a cutting-edge and efficient solution to
enhance biometric security systems.

</details>


### [44] [Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition](https://arxiv.org/abs/2507.12498)
*Beizhen Zhao,Yifan Zhou,Sicheng Yu,Zijian Wang,Hao Wang*

Main category: cs.GR

TL;DR: 3D高斯泼溅（3DGS）在3D场景重建中取得了革命性进展，但在复杂场景中仍存在问题。作者提出了一种基于小波分解的解耦优化框架，分别处理高低频细节，提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在复杂场景中常导致结构不完整和光照效果不清晰的问题，难以满足高质量重建需求。

Method: 结合3D和2D小波分解，将点云分为高低频分量。低频捕捉全局结构，高频恢复细节并结合光照模块。2D小波分解模拟辐射变化以辅助细节重建。

Result: 在多个数据集上验证了方法的优越性，各项指标均达到先进水平。

Conclusion: 该方法显著提升了3D场景重建的完整性和逼真度，为领域进展提供了新思路。

Abstract: 3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction,
which effectively balances rendering quality, efficiency, and speed. However,
existing 3DGS approaches usually generate plausible outputs and face
significant challenges in complex scene reconstruction, manifesting as
incomplete holistic structural outlines and unclear local lighting effects. To
address these issues simultaneously, we propose a novel decoupled optimization
framework, which integrates wavelet decomposition into 3D Gaussian Splatting
and 2D sampling. Technically, through 3D wavelet decomposition, our approach
divides point clouds into high-frequency and low-frequency components, enabling
targeted optimization for each. The low-frequency component captures global
structural outlines and manages the distribution of Gaussians through
voxelization. In contrast, the high-frequency component restores intricate
geometric and textural details while incorporating a relight module to mitigate
lighting artifacts and enhance photorealistic rendering. Additionally, a 2D
wavelet decomposition is applied to the training images, simulating radiance
variations. This provides critical guidance for high-frequency detail
reconstruction, ensuring seamless integration of details with the global
structure. Extensive experiments on challenging datasets demonstrate our method
achieves state-of-the-art performance across various metrics, surpassing
existing approaches and advancing the field of 3D scene reconstruction.

</details>


### [45] [HairFormer: Transformer-Based Dynamic Neural Hair Simulation](https://arxiv.org/abs/2507.12600)
*Joy Xiaoji Zhang,Jingsen Zhu,Hanyu Chen,Steve Marschner*

Main category: cs.GR

TL;DR: 提出了一种基于Transformer的两阶段神经网络方法，用于模拟任意发型、体形和运动下的头发动力学，实现了高保真且通用的动态头发模拟。


<details>
  <summary>Details</summary>
Motivation: 模拟头发动力学在多种发型、体形和运动下的通用性是一个关键挑战，本文旨在解决这一问题。

Method: 方法分为两个阶段：一个基于Transformer的静态网络预测静态发型，解决头发与身体的穿透问题；动态网络通过交叉注意力机制融合静态特征与运动输入，生成动态效果。

Result: 方法能够实时推理静态和动态发型，高保真且通用，尤其在复杂长发上表现优异。

Conclusion: 提出的两阶段方法在通用性和保真度上表现突出，展示了跨多样发型和运动的强大能力。

Abstract: Simulating hair dynamics that generalize across arbitrary hairstyles, body
shapes, and motions is a critical challenge. Our novel two-stage neural
solution is the first to leverage Transformer-based architectures for such a
broad generalization. We propose a Transformer-powered static network that
predicts static draped shapes for any hairstyle, effectively resolving
hair-body penetrations and preserving hair fidelity. Subsequently, a dynamic
network with a novel cross-attention mechanism fuses static hair features with
kinematic input to generate expressive dynamics and complex secondary motions.
This dynamic network also allows for efficient fine-tuning of challenging
motion sequences, such as abrupt head movements. Our method offers real-time
inference for both static single-frame drapes and dynamic drapes over pose
sequences. Our method demonstrates high-fidelity and generalizable dynamic hair
across various styles, guided by physics-informed losses, and can resolve
penetrations even for complex, unseen long hairstyles, highlighting its broad
generalization.

</details>


### [46] [VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians](https://arxiv.org/abs/2507.12667)
*Siyuan Yao,Chaoli Wang*

Main category: cs.GR

TL;DR: VolSegGS是一种基于高斯喷洒的框架，支持动态体积场景的交互式分割和跟踪，适用于实时可视化和分析。


<details>
  <summary>Details</summary>
Motivation: 大规模时间依赖模拟数据的可视化对领域科学家分析复杂现象至关重要，但传统方法对I/O带宽、存储和计算资源要求高，低端机器难以实现交互式探索。

Method: VolSegGS利用可变形3D高斯表示动态体积场景，支持实时新视图合成；通过高斯视图无关颜色进行粗分割，结合亲和力场网络优化细分割；通过高斯嵌入实现分割区域的连续跟踪。

Result: 实验证明VolSegGS在多种时间变化数据集上有效，提供实时交互和灵活的分割与跟踪功能，且在低计算需求下表现优异。

Conclusion: VolSegGS为时变体积数据的分析和可视化提供了高效解决方案，开启了新的可能性。

Abstract: Visualization of large-scale time-dependent simulation data is crucial for
domain scientists to analyze complex phenomena, but it demands significant I/O
bandwidth, storage, and computational resources. To enable effective
visualization on local, low-end machines, recent advances in view synthesis
techniques, such as neural radiance fields, utilize neural networks to generate
novel visualizations for volumetric scenes. However, these methods focus on
reconstruction quality rather than facilitating interactive visualization
exploration, such as feature extraction and tracking. We introduce VolSegGS, a
novel Gaussian splatting framework that supports interactive segmentation and
tracking in dynamic volumetric scenes for exploratory visualization and
analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic
volumetric scene, allowing for real-time novel view synthesis. For accurate
segmentation, we leverage the view-independent colors of Gaussians for
coarse-level segmentation and refine the results with an affinity field network
for fine-level segmentation. Additionally, by embedding segmentation results
within the Gaussians, we ensure that their deformation enables continuous
tracking of segmented regions over time. We demonstrate the effectiveness of
VolSegGS with several time-varying datasets and compare our solutions against
state-of-the-art methods. With the ability to interact with a dynamic scene in
real time and provide flexible segmentation and tracking capabilities, VolSegGS
offers a powerful solution under low computational demands. This framework
unlocks exciting new possibilities for time-varying volumetric data analysis
and visualization.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [47] [Geometric Theory of Ising Machines](https://arxiv.org/abs/2507.12626)
*Andrew G. Moore,Zachary Richey,Isaac K. Martin*

Main category: cs.ET

TL;DR: 该论文提出了一种可视化Ising电路决策边界的图解方法，并证明了Ising电路是1-NN分类器的推广以及局部极小值消除可转化为线性规划问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决低温度Ising机器设计中能量函数难以设计的挑战。

Method: 引入了一种图解设备来可视化Ising电路的决策边界，并利用其证明了两个关键结果。

Result: 证明Ising电路是1-NN分类器的推广，且局部极小值消除可表述为线性规划问题。

Conclusion: 该方法为Ising机器的设计提供了理论基础，并展示了其计算潜力。

Abstract: We contribute to the mathematical theory of the design of low temperature
Ising machines, a type of experimental probabilistic computing device
implementing the Ising model. Encoding the output of a function in the ground
state of a physical system allows efficient and distributed computation, but
the design of the energy function is a difficult puzzle. We introduce a
diagrammatic device that allows us to visualize the decision boundaries for
Ising circuits. It is then used to prove two results: (1) Ising circuits are a
generalization of 1-NN classifiers with a certain special structure, and (2)
Elimination of local minima in the energy landscape can be formulated as a
linear programming problem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [48] [Building State Machine Replication Using Practical Network Synchrony](https://arxiv.org/abs/2507.12792)
*Yiliang Wan,Nitin Shivaraman,Akshaye Shenoi,Xiang Liu,Tao Luo,Jialin Li*

Main category: cs.DC

TL;DR: 现代数据中心系统可以设计为提供强同步性，提出了一种新的复制协议Chora，通过利用网络同步性提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现代分布式协议通常假设部分同步或异步网络模型，而现代数据中心系统可以设计为提供更强的同步性，从而优化性能。

Method: 结合内核旁路网络、多线程架构和放宽的轮次长度，设计了一种具有强同步性的网络，并基于此设计了新的复制协议Chora。

Result: Chora在实验中比现有的单领导者和多领导者协议分别提高了255%和109%的吞吐量。

Conclusion: 通过利用网络同步性，协议Chora显著提升了分布式系统的性能，证明了强同步性在现代数据中心中的实用性。

Abstract: Distributed systems, such as state machine replication, are critical
infrastructures for modern applications. Practical distributed protocols make
minimum assumptions about the underlying network: They typically assume a
partially synchronous or fully asynchronous network model. In this work, we
argue that modern data center systems can be designed to provide strong
synchrony properties in the common case, where servers move in synchronous
lock-step rounds. We prove this hypothesis by engineering a practical design
that uses a combination of kernel-bypass network, multithreaded architecture,
and loosened round length, achieving a tight round bound under 2us. Leveraging
our engineered networks with strong synchrony, we co-design a new replication
protocol, Chora. Chora exploits the network synchrony property to efficiently
pipeline multiple replication instances, while allowing all replicas to propose
in parallel without extra coordination. Through experiments, we show that Chora
achieves 255% and 109% improvement in throughput over state-of-the-art
single-leader and multi-leader protocols, respectively.

</details>


### [49] [Autonomous Resource Management in Microservice Systems via Reinforcement Learning](https://arxiv.org/abs/2507.12879)
*Yujun Zou,Nia Qi,Yingnan Deng,Zhihao Xue,Ming Gong,Wuyang Zhang*

Main category: cs.DC

TL;DR: 本文提出了一种基于强化学习的微服务资源调度与优化方法，旨在解决传统微服务架构中资源分配不均、高延迟和吞吐量不足等问题。


<details>
  <summary>Details</summary>
Motivation: 随着微服务系统中服务数量和负载的增加，如何高效调度和分配计算、内存和存储等资源成为关键挑战。

Method: 文章采用基于强化学习的智能调度算法，通过代理与环境的交互持续优化资源分配策略。

Result: 实验结果表明，该方法在低负载和高并发条件下显著提升了系统响应速度和吞吐量，同时优化了资源利用率和降低了能耗。

Conclusion: 相比传统静态资源分配方法，强化学习模型展现出更强的适应性和优化能力，能实时调整策略以维持动态负载和资源环境下的系统性能。

Abstract: This paper proposes a reinforcement learning-based method for microservice
resource scheduling and optimization, aiming to address issues such as uneven
resource allocation, high latency, and insufficient throughput in traditional
microservice architectures. In microservice systems, as the number of services
and the load increase, efficiently scheduling and allocating resources such as
computing power, memory, and storage becomes a critical research challenge. To
address this, the paper employs an intelligent scheduling algorithm based on
reinforcement learning. Through the interaction between the agent and the
environment, the resource allocation strategy is continuously optimized. In the
experiments, the paper considers different resource conditions and load
scenarios, evaluating the proposed method across multiple dimensions, including
response time, throughput, resource utilization, and cost efficiency. The
experimental results show that the reinforcement learning-based scheduling
method significantly improves system response speed and throughput under low
load and high concurrency conditions, while also optimizing resource
utilization and reducing energy consumption. Under multi-dimensional resource
conditions, the proposed method can consider multiple objectives and achieve
optimized resource scheduling. Compared to traditional static resource
allocation methods, the reinforcement learning model demonstrates stronger
adaptability and optimization capability. It can adjust resource allocation
strategies in real time, thereby maintaining good system performance in
dynamically changing load and resource environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [50] [Transforming Football Data into Object-centric Event Logs with Spatial Context Information](https://arxiv.org/abs/2507.12504)
*Vito Chan,Lennart Ebert,Paul-Julius Hillmann,Christoffer Rubensson,Stephan A. Fahrenkrog-Petersen,Jan Mendling*

Main category: cs.DB

TL;DR: 论文提出了一种将足球数据转化为对象中心事件日志的框架，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的对象中心事件日志研究不足，且真实世界数据有限。利用团队运动数据（如足球）可以为对象中心过程挖掘提供更多实际用例。

Method: 开发了一个框架，将足球数据转化为具有空间维度的对象中心事件日志。

Result: 生成了基于真实足球数据的对象中心事件日志，并讨论了不同过程表示的结果。

Conclusion: 该研究为足球分析中的对象中心事件日志提供了首个示例，未来研究可关注变体分析和过滤技术以更好地处理变异性。

Abstract: Object-centric event logs expand the conventional single-case notion event
log by considering multiple objects, allowing for the analysis of more complex
and realistic process behavior. However, the number of real-world
object-centric event logs remains limited, and further studies are needed to
test their usefulness. The increasing availability of data from team sports can
facilitate object-centric process mining, leveraging both real-world data and
suitable use cases. In this paper, we present a framework for transforming
football (soccer) data into an object-centric event log, further enhanced with
a spatial dimension. We demonstrate the effectiveness of our framework by
generating object-centric event logs based on real-world football data and
discuss the results for varying process representations. With our paper, we
provide the first example for object-centric event logs in football analytics.
Future work should consider variant analysis and filtering techniques to better
handle variability

</details>


### [51] [Rel-HNN: Split Parallel Hypergraph Neural Network for Learning on Relational Databases](https://arxiv.org/abs/2507.12562)
*Md. Tanvir Alam,Md. Ahasanul Alam,Md Mahmudur Rahman,Md. Mosaddek Khan*

Main category: cs.DB

TL;DR: 摘要提出了一种名为rel-HNN的新型超图框架，用于捕获关系数据库中的细粒度关联，并通过多GPU并行训练提高效率。


<details>
  <summary>Details</summary>
Motivation: 关系数据库的固定大小输入表示难以捕获关系语义，现有GNN方法过于简化忽略元组内关联。

Method: 将属性-值对作为节点，元组作为超边，构建超图框架，并引入多GPU并行训练算法。

Result: 在分类和回归任务中显著优于现有方法，多GPU训练速度提升最高达3.18倍。

Conclusion: rel-HNN框架有效解决了关系数据的学习问题，且通过并行训练大幅提升效率。

Abstract: Relational databases (RDBs) are ubiquitous in enterprise and real-world
applications. Flattening the database poses challenges for deep learning models
that rely on fixed-size input representations to capture relational semantics
from the structured nature of relational data. Graph neural networks (GNNs)
have been proposed to address this, but they often oversimplify relational
structures by modeling all the tuples as monolithic nodes and ignoring
intra-tuple associations. In this work, we propose a novel hypergraph-based
framework, that we call rel-HNN, which models each unique attribute-value pair
as a node and each tuple as a hyperedge, enabling the capture of fine-grained
intra-tuple relationships. Our approach learns explicit multi-level
representations across attribute-value, tuple, and table levels. To address the
scalability challenges posed by large RDBs, we further introduce a
split-parallel training algorithm that leverages multi-GPU execution for
efficient hypergraph learning. Extensive experiments on real-world and
benchmark datasets demonstrate that rel-HNN significantly outperforms existing
methods in both classification and regression tasks. Moreover, our
split-parallel training achieves substantial speedups -- up to 3.18x for
learning on relational data and up to 2.94x for hypergraph learning -- compared
to conventional single-GPU execution.

</details>


### [52] [Targeted Mining of Time-Interval Related Patterns](https://arxiv.org/abs/2507.12668)
*Shuang Liang,Lili Chen,Wensheng Gan,Philip S. Yu,Shengjie Zhao*

Main category: cs.DB

TL;DR: 论文提出了一种名为TaTIRP的新算法，用于挖掘目标时间区间相关模式（TIRP），并通过多种剪枝策略提升大规模数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽略事件的持续时间，而TIRP挖掘能解决这一问题，但计算成本高且资源密集。针对特定标准挖掘TIRP可提升效率和用户偏好匹配。

Method: 提出TaTIRP算法，结合多种剪枝策略以减少冗余扩展操作。

Result: 实验验证了算法在真实和合成数据集上的准确性和效率。

Conclusion: TaTIRP算法能有效提升TIRP挖掘的效率，适用于大规模数据。

Abstract: Compared to frequent pattern mining, sequential pattern mining emphasizes the
temporal aspect and finds broad applications across various fields. However,
numerous studies treat temporal events as single time points, neglecting their
durations. Time-interval-related pattern (TIRP) mining is introduced to address
this issue and has been applied to healthcare analytics, stock prediction, etc.
Typically, mining all patterns is not only computationally challenging for
accurate forecasting but also resource-intensive in terms of time and memory.
Targeting the extraction of time-interval-related patterns based on specific
criteria can improve data analysis efficiency and better align with customer
preferences. Therefore, this paper proposes a novel algorithm called TaTIRP to
discover Targeted Time-Interval Related Patterns. Additionally, we develop
multiple pruning strategies to eliminate redundant extension operations,
thereby enhancing performance on large-scale datasets. Finally, we conduct
experiments on various real-world and synthetic datasets to validate the
accuracy and efficiency of the proposed algorithm.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [53] [Modular SAIL: dream or reality?](https://arxiv.org/abs/2507.12471)
*Petr Kourzanov,Anmol*

Main category: cs.AR

TL;DR: 该论文探讨了如何在SAIL-RISCV黄金模型中引入模块化，以支持RISC-V ISA的模块化特性，并通过实验验证了模块化仿真的可行性。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用RISC-V ISA的模块化特性，需要解决组合性问题，覆盖从规范到仿真、模拟和验证的开发流程。

Method: 论文提出了模块化SAIL的实验方法，通过调整SAIL-RISCV流程以支持仿真器级别的模块化。

Result: 实验表明，通过静态和动态绑定，模块化仿真器的功能行为与原始单块仿真器一致。

Conclusion: 模块化SAIL在理论上易于实现，能够为RISC-V开发流程提供更大的灵活性。

Abstract: In order to truly benefit from RISC-V ISA modularity, the community has to
address the issue of compositionality, going beyond modules at the
specification level covering larger subsets of the RISC-V development flow
including emulation, simulation and verification. In this paper we introduce
modular SAIL, an experiment to inject compositionality into the SAIL-RISCV
golden model. We show that it is, in principle, not difficult to adapt the
SAIL-RISCV flow (and ideally the SAIL compiler itself) to support modules at
the emulator level. We back our findings by a comparative study of the
resulting pluggable emulator's performance using both static and dynamic
binding, which both exhibit same functional behavior as the original monolithic
emulator (aka RISC-V ISS).

</details>


### [54] [An ultra-low-power CGRA for accelerating Transformers at the edge](https://arxiv.org/abs/2507.12904)
*Rohit Prasad*

Main category: cs.AR

TL;DR: 提出了一种专为边缘设备优化的超低功耗CGRA架构，用于加速变换器模型中的GEMM操作。


<details>
  <summary>Details</summary>
Motivation: 解决变换器模型在低功耗边缘设备上的部署挑战。

Method: 设计了一种4x4 PE阵列和4x2 MOB的CGRA架构，采用无交换环网互联以减少功耗和延迟。

Result: 通过并行计算和优化数据流，显著降低了内存带宽需求并提升了数据复用。

Conclusion: 该架构为边缘设备部署复杂机器学习模型提供了可扩展的解决方案。

Abstract: Transformers have revolutionized deep learning with applications in natural
language processing, computer vision, and beyond. However, their computational
demands make it challenging to deploy them on low-power edge devices. This
paper introduces an ultra-low-power, Coarse-Grained Reconfigurable Array (CGRA)
architecture specifically designed to accelerate General Matrix Multiplication
(GEMM) operations in transformer models tailored for the energy and resource
constraints of edge applications. The proposed architecture integrates a 4 x 4
array of Processing Elements (PEs) for efficient parallel computation and
dedicated 4 x 2 Memory Operation Blocks (MOBs) for optimized LOAD/STORE
operations, reducing memory bandwidth demands and enhancing data reuse. A
switchless mesh torus interconnect network further minimizes power and latency
by enabling direct communication between PEs and MOBs, eliminating the need for
centralized switching. Through its heterogeneous array design and efficient
dataflow, this CGRA architecture addresses the unique computational needs of
transformers, offering a scalable pathway to deploy sophisticated machine
learning models on edge devices.

</details>


### [55] [WIP: Turning Fake Chips into Learning Opportunities](https://arxiv.org/abs/2507.13281)
*Haniye Mehraban,Saad Azmeen-ur-Rahman,John Hu*

Main category: cs.AR

TL;DR: 通过使用假冒TL074运算放大器进行实践教学，学生深入了解了模拟电路、供应链安全和实际工程。


<details>
  <summary>Details</summary>
Motivation: 假冒集成电路在本科电子实验室中日益普遍，影响了教学完整性，但可以作为教学资源。

Method: 学生通过动手诊断，测量电流、分析波形和故障排除，与假冒芯片组件互动。

Result: 学生获得了对模拟电路、供应链安全和工程实践的更深入理解。

Conclusion: 利用假冒元件作为教学工具，能够创造有价值的学习体验。

Abstract: This work-in-progress paper presents a case study in which counterfeit TL074
operational amplifiers, discovered in a junior level electronics course, became
the basis for a hands on learning experience. Counterfeit integrated circuits
(IC) are increasingly common, posing a significant threat to the integrity of
undergraduate electronics laboratories. Instead of simply replacing the
counterfeit components, we turned the issue into a teaching moment. Students
engaged in hands-on diagnostics measuring current, analyzing waveforms, and
troubleshooting. By working with fake chip components, they gained deeper
insight into analog circuits, supply chain security, and practical engineering.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database](https://arxiv.org/abs/2507.12805)
*Hui Sun,Yanfeng Ding,Liping Yi,Huidong Ma,Gang Wang,Xiaoguang Liu,Cheng Zhong,Wentong Cai*

Main category: cs.LG

TL;DR: 提出了一种名为PMKLC的并行多知识学习压缩器，解决了现有基因数据压缩器的压缩比、吞吐量和鲁棒性问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的无损压缩器在压缩比、吞吐量和鲁棒性方面表现不足，限制了其在基因组数据管理中的广泛应用。

Method: 设计了自动化多知识学习框架、GPU加速的编码器、数据块分区和逐步模型传递机制，并提供了单GPU和多GPU两种压缩模式。

Result: PMKLC在15个数据集上相比基线方法，压缩比平均提升73.6%，吞吐量提升3-10倍，并表现出最佳鲁棒性和内存效率。

Conclusion: PMKLC在基因组数据压缩中展现出卓越的综合性能，适用于资源受限和多GPU环境，具有较高的稳定性和实用性。

Abstract: Learning-based lossless compressors play a crucial role in large-scale
genomic database backup, storage, transmission, and management. However, their
1) inadequate compression ratio, 2) low compression \& decompression
throughput, and 3) poor compression robustness limit their widespread adoption
and application in both industry and academia. To solve those challenges, we
propose a novel \underline{P}arallel \underline{M}ulti-\underline{K}nowledge
\underline{L}earning-based \underline{C}ompressor (PMKLC) with four crucial
designs: 1) We propose an automated multi-knowledge learning-based compression
framework as compressors' backbone to enhance compression ratio and robustness;
2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression
throughput and computing resource usage; 3) we introduce data block
partitioning and Step-wise Model Passing (SMP) mechanisms for parallel
acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet
the complex application scenarios, where the former runs on a
resource-constrained single GPU and the latter is multi-GPU accelerated. We
benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15
real-world datasets with different species and data sizes. Compared to
baselines on the testing datasets, PMKLC-S/M achieve the average compression
ratio improvement up to 73.609\% and 73.480\%, the average throughput
improvement up to 3.036$\times$ and 10.710$\times$, respectively. Besides,
PMKLC-S/M also achieve the best robustness and competitive memory cost,
indicating its greater stability against datasets with different probability
distribution perturbations, and its strong ability to run on memory-constrained
devices.

</details>


### [57] [BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training](https://arxiv.org/abs/2507.12619)
*Rui Li,Xiaoyun Zhi,Jinxin Chi,Menghan Yu,Lixin Huang,Jia Zhu,Weilun Zhang,Xing Ma,Wenjia Liu,Zhicheng Zhu,Daowen Luo,Zuquan Song,Xin Yin,Chao Xiang,Shuguang Wang,Wencong Xiao,Gene Cooperman*

Main category: cs.LG

TL;DR: 本文聚焦于大语言模型（LLM）训练中的启动开销问题，提出了Bootseer系统优化框架，能够减少50%的启动开销。


<details>
  <summary>Details</summary>
Motivation: LLM训练中的启动开销（如容器加载、依赖安装等）在工业规模下尤为关键，可能导致大量GPU时间浪费，因此需要深入研究并优化。

Method: 通过分析真实生产数据，识别启动开销的主要瓶颈（如容器加载、依赖安装等），并设计Bootseer框架，采用三种技术（热点块预取、依赖快照、HDFS-FUSE分片）进行优化。

Result: Bootseer在实际生产环境中部署后，可将启动开销降低50%。

Conclusion: 启动开销是LLM训练中的重要问题，Bootseer通过系统级优化显著减少了这一开销，为大规模LLM训练提供了更高效的解决方案。

Abstract: Large Language Models (LLMs) have become a cornerstone of modern AI, driving
breakthroughs in natural language processing and expanding into multimodal jobs
involving images, audio, and video. As with most computational software, it is
important to distinguish between ordinary runtime performance and startup
overhead. Prior research has focused on runtime performance: improving training
efficiency and stability. This work focuses instead on the increasingly
critical issue of startup overhead in training: the delay before training jobs
begin execution. Startup overhead is particularly important in large,
industrial-scale LLMs, where failures occur more frequently and multiple teams
operate in iterative update-debug cycles. In one of our training clusters, more
than 3.5% of GPU time is wasted due to startup overhead alone.
  In this work, we present the first in-depth characterization of LLM training
startup overhead based on real production data. We analyze the components of
startup cost, quantify its direct impact, and examine how it scales with job
size. These insights motivate the design of Bootseer, a system-level
optimization framework that addresses three primary startup bottlenecks: (a)
container image loading, (b) runtime dependency installation, and (c) model
checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three
techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and
(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment
and evaluated on real LLM training workloads, demonstrating a 50% reduction in
startup overhead.

</details>


### [58] [MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration](https://arxiv.org/abs/2507.12935)
*Shirui Zhao,Jun Yin,Lingyun Yao,Martin Andraud,Wannes Meert,Marian Verhelst*

Main category: cs.LG

TL;DR: 本文提出了一种名为MC²A的算法-硬件协同设计框架，用于高效且灵活地加速MCMC算法，通过新的硬件架构和优化的采样器实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于MCMC算法的高计算成本限制了其在大规模问题和现实应用中的可行性，现有的加速方案在硬件灵活性或系统级效率方面存在不足，因此需要一种更高效的解决方案。

Method: MC²A通过扩展处理器性能模型来分析MCMC工作负载多样性，并提出了一种参数化的硬件加速器架构，包括可编程树状处理单元、可重构采样器和交叉互连结构。核心部分采用了一种新的Gumbel采样器，消除了指数和归一化操作。

Result: MC²A在端到端的案例研究中，相比CPU、GPU、TPU和现有MCMC加速器，分别实现了307.6倍、1.4倍、2.0倍和84.2倍的加速效果。

Conclusion: MC²A证明了通过通用硬件加速推广MCMC解决方案的可行性，适用于多样化的应用领域。

Abstract: An increasing number of applications are exploiting sampling-based algorithms
for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)
algorithms form the computational backbone of this emerging branch of machine
learning. Unfortunately, the high computational cost limits their feasibility
for large-scale problems and real-world applications, and the existing MCMC
acceleration solutions are either limited in hardware flexibility or fail to
maintain efficiency at the system level across a variety of end-to-end
applications. This paper introduces \textbf{MC$^2$A}, an algorithm-hardware
co-design framework, enabling efficient and flexible optimization for MCMC
acceleration. Firstly, \textbf{MC$^2$A} analyzes the MCMC workload diversity
through an extension of the processor performance roofline model with a 3rd
dimension to derive the optimal balance between the compute, sampling and
memory parameters. Secondly, \textbf{MC$^2$A} proposes a parametrized hardware
accelerator architecture with flexible and efficient support of MCMC kernels
with a pipeline of ISA-programmable tree-structured processing units,
reconfigurable samplers and a crossbar interconnect to support irregular
access. Thirdly, the core of \textbf{MC$^2$A} is powered by a novel Gumbel
sampler that eliminates exponential and normalization operations. In the
end-to-end case study, \textbf{MC$^2$A} achieves an overall {$307.6\times$,
$1.4\times$, $2.0\times$, $84.2\times$} speedup compared to the CPU, GPU, TPU
and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC
workloads, this work demonstrates and exploits the feasibility of general
hardware acceleration to popularize MCMC-based solutions in diverse application
domains.

</details>


### [59] [FedGA: A Fair Federated Learning Framework Based on the Gini Coefficient](https://arxiv.org/abs/2507.12983)
*ShanBin Liu*

Main category: cs.LG

TL;DR: FedGA是一种公平感知的联邦学习算法，通过基尼系数衡量客户端表现差异，自适应调整聚合权重以提升公平性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据异质性导致的性能差异问题，确保模型的公平行为。

Method: 使用基尼系数衡量差异，建立其与模型更新规模的关系，动态调整聚合权重。

Result: 在多个数据集上实验表明，FedGA显著提升了公平性指标（如方差和基尼系数），同时保持了整体性能。

Conclusion: FedGA有效解决了联邦学习的公平性问题，为实际应用提供了可行方案。

Abstract: Fairness has emerged as one of the key challenges in federated learning. In
horizontal federated settings, data heterogeneity often leads to substantial
performance disparities across clients, raising concerns about equitable model
behavior. To address this issue, we propose FedGA, a fairness-aware federated
learning algorithm. We first employ the Gini coefficient to measure the
performance disparity among clients. Based on this, we establish a relationship
between the Gini coefficient $G$ and the update scale of the global model
${U_s}$, and use this relationship to adaptively determine the timing of
fairness intervention. Subsequently, we dynamically adjust the aggregation
weights according to the system's real-time fairness status, enabling the
global model to better incorporate information from clients with relatively
poor performance.We conduct extensive experiments on the Office-Caltech-10,
CIFAR-10, and Synthetic datasets. The results show that FedGA effectively
improves fairness metrics such as variance and the Gini coefficient, while
maintaining strong overall performance, demonstrating the effectiveness of our
approach.

</details>


### [60] [Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective](https://arxiv.org/abs/2507.12652)
*Kai Malcolm,César Uribe,Momona Yamagami*

Main category: cs.LG

TL;DR: 论文提出了一种基于联邦学习（FL）的神经解码方法，旨在解决神经接口技术中数据共享的隐私问题，并评估了其在开环和闭环场景下的表现与隐私性。


<details>
  <summary>Details</summary>
Motivation: 神经信号包含敏感的个人身份和健康信息，数据共享用于解码器训练时存在隐私挑战，联邦学习作为一种分布式、隐私保护的学习框架，有望解决这一问题。

Method: 作者引入FL-based神经解码方法，并使用高维肌电信号在开环和闭环场景中系统评估其性能和隐私性。

Result: 开环模拟中，FL显著优于本地学习基线；闭环场景需要调整FL方法以适应单用户实时交互，此时本地学习表现更好，但隐私风险更高。

Conclusion: 研究发现实时自适应应用中存在性能与隐私的权衡，呼吁设计专门针对单用户协同适应应用的FL方法。

Abstract: Invasive and non-invasive neural interfaces hold promise as high-bandwidth
input devices for next-generation technologies. However, neural signals
inherently encode sensitive information about an individual's identity and
health, making data sharing for decoder training a critical privacy challenge.
Federated learning (FL), a distributed, privacy-preserving learning framework,
presents a promising solution, but it remains unexplored in closed-loop
adaptive neural interfaces. Here, we introduce FL-based neural decoding and
systematically evaluate its performance and privacy using high-dimensional
electromyography signals in both open- and closed-loop scenarios. In open-loop
simulations, FL significantly outperformed local learning baselines,
demonstrating its potential for high-performance, privacy-conscious neural
decoding. In contrast, closed-loop user studies required adapting FL methods to
accommodate single-user, real-time interactions, a scenario not supported by
standard FL. This modification resulted in local learning decoders surpassing
the adapted FL approach in closed-loop performance, yet local learning still
carried higher privacy risks. Our findings highlight a critical
performance-privacy tradeoff in real-time adaptive applications and indicate
the need for FL methods specifically designed for co-adaptive, single-user
applications.

</details>


### [61] [Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces](https://arxiv.org/abs/2507.13092)
*Hyo-Jeong Jang,Hye-Bin Shin,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的跨模态知识蒸馏框架，解决了EEG学习中的模态差距和标签不一致问题，实验证明其在情绪回归和分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EEG作为BCI的核心模态，容易受到固有信号和人为标签错误的影响，导致模型性能下降，因此需要通过多模态知识蒸馏来提升学习效果。

Method: 使用基于原型相似性的模块对齐特征语义，并引入任务特定的蒸馏头来处理监督中的标签不一致问题。

Result: 在公共多模态数据集上，该方法在情绪回归和分类任务中表现优于单模态和多模态基线方法。

Conclusion: 该框架显示出在BCI应用中的潜力，为解决EEG信号学习中的挑战提供了有效方案。

Abstract: Electroencephalography (EEG) is a fundamental modality for cognitive state
monitoring in brain-computer interfaces (BCIs). However, it is highly
susceptible to intrinsic signal errors and human-induced labeling errors, which
lead to label noise and ultimately degrade model performance. To enhance EEG
learning, multimodal knowledge distillation (KD) has been explored to transfer
knowledge from visual models with rich representations to EEG-based models.
Nevertheless, KD faces two key challenges: modality gap and soft label
misalignment. The former arises from the heterogeneous nature of EEG and visual
feature spaces, while the latter stems from label inconsistencies that create
discrepancies between ground truth labels and distillation targets. This paper
addresses semantic uncertainty caused by ambiguous features and weakly defined
labels. We propose a novel cross-modal knowledge distillation framework that
mitigates both modality and label inconsistencies. It aligns feature semantics
through a prototype-based similarity module and introduces a task-specific
distillation head to resolve label-induced inconsistency in supervision.
Experimental results demonstrate that our approach improves EEG-based emotion
regression and classification performance, outperforming both unimodal and
multimodal baselines on a public multimodal dataset. These findings highlight
the potential of our framework for BCI applications.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [62] [Efficient Classical-Processing of Constant-Depth Time Evolution Circuits in Control Hardware](https://arxiv.org/abs/2507.12765)
*Akhil Francis,Abhi D. Rajagopala,Norm M. Tubman,Katherine Klymko,Kasra Nowrouzi*

Main category: quant-ph

TL;DR: 通过硬件辅助的参数化电路执行（PCE）减少经典处理和编译时间，提升了量子算法运行性能，尤其在计算量子多体系统动态特性方面效果显著。


<details>
  <summary>Details</summary>
Motivation: 量子算法的运行性能提升需要从多个方面入手，本研究专注于减少经典处理和编译时间，以解决近量子算法中的经典瓶颈问题。

Method: 采用硬件辅助的参数化电路执行（PCE），利用结构等效的时间演化电路，通过Cartan分解生成恒定深度电路，计算自旋模型的相关函数。

Result: 在横向场XY模型（最多6个位点）和海森堡自旋模型（最多3个位点）中，运行时间比标准编译方法减少了50%。

Conclusion: 硬件辅助的PCE方法在计算量子系统动态特性时能够显著减少运行时间，展示了其在近量子算法中缓解经典瓶颈的潜力。

Abstract: Improving quantum algorithms run-time performance involves several strategies
such as reducing the quantum gate counts, decreasing the number of
measurements, advancement in QPU technology for faster gate operations, or
optimizing the classical processing. This work focuses on the latter,
specifically reducing classical processing and compilation time via
hardware-assisted parameterized circuit execution (PCE) for computing dynamical
properties of quantum systems. PCE was previously validated for QCVV protocols,
which leverages structural circuit equivalencies. We demonstrate the
applicability of this approach to computing dynamical properties of quantum
many-body systems using structurally equivalent time evolution circuits,
specifically calculating correlation functions of spin models using
constant-depth circuits generated via Cartan decomposition. Implementing this
for spin-spin correlation functions in Transverse field XY (up to 6-sites) and
Heisenberg spin models (up to 3-sites), we observed a run-time reduction of up
to 50\% compared to standard compilation methods. This highlights the
adaptability of time-evolution circuit with hardware-assisted PCE to
potentially mitigate the classical bottlenecks in near-term quantum algorithms.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [A Translation of Probabilistic Event Calculus into Markov Decision Processes](https://arxiv.org/abs/2507.12989)
*Lyris Xu,Fabio Aurelio D'Asaro,Luke Dickens*

Main category: cs.AI

TL;DR: 该论文将概率事件演算（PEC）转化为马尔可夫决策过程（MDP），以弥补PEC在目标导向推理上的不足，同时保留其解释性和表达能力。


<details>
  <summary>Details</summary>
Motivation: PEC在不确定环境中推理行为和效果具有优势，但缺乏目标导向推理机制，因此需要将PEC与MDP结合以扩展其功能。

Method: 通过将PEC域形式化转化为MDP，引入“动作执行情境”概念来保持PEC的行为语义灵活性。

Result: 提出的PEC-MDP框架支持时间推理和目标任务规划，并能将学习到的策略映射回可读的PEC表示。

Conclusion: PEC-MDP结合了PEC的解释性和MDP的算法工具，同时保持了行为语义的灵活性，为推理和规划任务提供了新方法。

Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.

</details>


### [64] [Higher-Order Pattern Unification Modulo Similarity Relations](https://arxiv.org/abs/2507.13208)
*Besik Dundua,Temur Kutsia*

Main category: cs.AI

TL;DR: 本文提出了一种结合高阶理论和模糊逻辑的统一算法，用于解决高阶模式与基于最小T-范数的模糊等价关系的统一问题，并证明其终止性、健全性和完备性。


<details>
  <summary>Details</summary>
Motivation: 在涉及抽象函数和谓词推理的决策任务中，高阶理论与模糊逻辑的结合具有潜在价值。然而，为这种组合形式开发高效的计算技术是一个重大挑战。

Method: 作者采用了一种简单的方法，整合两种成熟且计算性能良好的组件：高阶模式和基于最小T-范数的模糊等价关系。他们提出了一种统一算法，并证明了其相关性质。

Result: 该算法能够计算具有最高近似度的最一般统一解，且该统一问题是单一的。

Conclusion: 本文提出的方法为高阶模式与模糊等价关系的统一问题提供了有效的解决方案，具备理论上的保障。

Abstract: The combination of higher-order theories and fuzzy logic can be useful in
decision-making tasks that involve reasoning across abstract functions and
predicates, where exact matches are often rare or unnecessary. Developing
efficient reasoning and computational techniques for such a combined formalism
presents a significant challenge. In this paper, we adopt a more
straightforward approach aiming at integrating two well-established and
computationally well-behaved components: higher-order patterns on one side and
fuzzy equivalences expressed through similarity relations based on minimum
T-norm on the other. We propose a unification algorithm for higher-order
patterns modulo these similarity relations and prove its termination,
soundness, and completeness. This unification problem, like its crisp
counterpart, is unitary. The algorithm computes a most general unifier with the
highest degree of approximation when the given terms are unifiable.

</details>


### [65] [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872)
*Rishane Dassanayake,Mario Demetroudi,James Walpole,Lindley Lentati,Jason R. Brown,Edward James Young*

Main category: cs.AI

TL;DR: 前沿AI系统在说服、欺骗和影响人类行为方面的能力迅速提升，对人类构成潜在威胁，本文提出了一个系统框架来评估和减轻这些风险。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统已展现出人类水平的说服和欺骗能力，而人类往往是网络安全系统中的薄弱环节，因此需要系统方法来应对AI操纵风险。

Method: 提出一个围绕三个核心论证（无法性、控制和可信度）的安全案例框架，并为每个论证提供证据要求、评估方法和实施建议。

Result: 为AI公司提供了首个系统方法，用于在部署前评估和减轻操纵风险。

Conclusion: 本文为整合操纵风险到AI安全治理中提供了具体基础，帮助公司提前应对潜在威胁。

Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade,
deceive, and influence human behaviour, with current models already
demonstrating human-level persuasion and strategic deception in specific
contexts. Humans are often the weakest link in cybersecurity systems, and a
misaligned AI system deployed internally within a frontier company may seek to
undermine human oversight by manipulating employees. Despite this growing
threat, manipulation attacks have received little attention, and no systematic
framework exists for assessing and mitigating these risks. To address this, we
provide a detailed explanation of why manipulation attacks are a significant
threat and could lead to catastrophic outcomes. Additionally, we present a
safety case framework for manipulation risk, structured around three core lines
of argument: inability, control, and trustworthiness. For each argument, we
specify evidence requirements, evaluation methodologies, and implementation
considerations for direct application by AI companies. This paper provides the
first systematic methodology for integrating manipulation risk into AI safety
governance, offering AI companies a concrete foundation to assess and mitigate
these threats before deployment.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [66] [Learning-Based Interface for Semantic Communication with Bit Importance Awareness](https://arxiv.org/abs/2507.12850)
*Wenzheng Kong,Wenyi Zhang*

Main category: cs.IT

TL;DR: 提出了一种基于学习的Split DeepJSCC接口设计，通过可训练参数提升端到端性能，并引入重要性感知网络动态适应信道条件。


<details>
  <summary>Details</summary>
Motivation: 解决现有JSCC方法难以与异构通信网络架构兼容的问题。

Method: 学习型接口设计和重要性感知网络，动态适应信道带宽和条件。

Result: 实验显示在无线图像传输任务中性能提升。

Conclusion: 为现有无线网络中实现语义通信提供了潜在解决方案。

Abstract: Joint source-channel coding (JSCC) is an effective approach for semantic
communication. However, current JSCC methods are difficult to integrate with
existing communication network architectures, where application and network
providers are typically different entities. Recently, a novel paradigm termed
Split DeepJSCC has been under consideration to address this challenge. Split
DeepJSCC employs a bit-level interface that enables separate design of source
and channel codes, ensuring compatibility with existing communication networks
while preserving the advantages of JSCC in terms of semantic fidelity and
channel adaptability. In this paper, we propose a learning-based interface
design by treating its parameters as trainable, achieving improved end-to-end
performance compared to Split DeepJSCC. In particular, the interface enables
specification of bit-level importance at the output of the source code.
Furthermore, we propose an Importance-Aware Net that utilizes the
interface-derived bit importance information, enabling dynamical adaptation to
diverse channel bandwidth ratios and time-varying channel conditions.
Experimental results show that our method improves performance in wireless
image transmission tasks. This work provides a potential solution for realizing
semantic communications in existing wireless networks.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [67] [UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets](https://arxiv.org/abs/2507.12951)
*Zhichao Sheng,Shilin Zhou,Chen Gong,Zhenghua Li*

Main category: eess.AS

TL;DR: 该论文提出了一个统一的框架UniSLU，用于联合建模多个口语语言理解任务，解决了现有方法依赖独立模型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理口语语言理解任务时，依赖于独立的模型架构，增加了系统复杂性，限制了任务间交互，且无法充分利用异构数据集。

Method: 提出了一个统一表示和生成方法，将自动语音识别、口语命名实体识别和情感分析等任务联合建模，并整合大型语言模型的生成能力。

Result: 在公开数据集上的实验表明，该方法优于多个基准方法，适用于现实语音多媒体场景。

Conclusion: UniSLU框架有效提升了口语语言理解任务的性能，为未来研究提供了工具和模型。

Abstract: Spoken Language Understanding (SLU) plays a crucial role in speech-centric
multimedia applications, enabling machines to comprehend spoken language in
scenarios such as meetings, interviews, and customer service interactions. SLU
encompasses multiple tasks, including Automatic Speech Recognition (ASR),
spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).
However, existing methods often rely on separate model architectures for
individual tasks such as spoken NER and SA, which increases system complexity,
limits cross-task interaction, and fails to fully exploit heterogeneous
datasets available across tasks. To address these limitations, we propose
UniSLU, a unified framework that jointly models multiple SLU tasks within a
single architecture. Specifically, we propose a unified representation for
diverse SLU tasks, enabling full utilization of heterogeneous datasets across
multiple tasks. Built upon this representation, we propose a unified generative
method that jointly models ASR, spoken NER, and SA tasks, enhancing task
interactions and enabling seamless integration with large language models to
harness their powerful generative capabilities. Extensive experiments on public
SLU datasets demonstrate the effectiveness of our approach, achieving superior
SLU performance compared to several benchmark methods, making it well-suited
for real-world speech-based multimedia scenarios. We will release all code and
models at github to facilitate future research.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [68] [Catching Dark Signals in Algorithms: Unveiling Audiovisual and Thematic Markers of Unsafe Content Recommended for Children and Teenagers](https://arxiv.org/abs/2507.12571)
*Haoning Xue,Brian Nishimine,Martin Hilbert,Drew Cingel,Samantha Vigil,Jane Shawcroft,Arti Thakur,Zubair Shafiq,Jingwen Zhang*

Main category: cs.CY

TL;DR: 研究发现短视频平台对儿童和青少年的潜在危害，提出“显性、隐性、意外”危害框架，呼吁更精细的内容审核和平台监管。


<details>
  <summary>Details</summary>
Motivation: 短视频平台的普及及年龄验证机制的不完善引发对儿童和青少年在算法主导的在线环境中潜在危害的担忧。

Method: 通过多模态特征分析和主题建模，分析了4,492个推荐给儿童和青少年的短视频。

Result: 发现不安全视频具有较暗的视觉特征，并包含显性有害内容及引发焦虑的隐性内容。

Conclusion: 需保护青少年免受社交媒体的显性和隐性危害，呼吁更精细的内容审核、年龄验证及平台监管。

Abstract: The prevalence of short form video platforms, combined with the
ineffectiveness of age verification mechanisms, raises concerns about the
potential harms facing children and teenagers in an algorithm-moderated online
environment. We conducted multimodal feature analysis and thematic topic
modeling of 4,492 short videos recommended to children and teenagers on
Instagram Reels, TikTok, and YouTube Shorts, collected as a part of an
algorithm auditing experiment. This feature-level and content-level analysis
revealed that unsafe (i.e., problematic, mentally distressing) short videos (a)
possess darker visual features and (b) contain explicitly harmful content and
implicit harm from anxiety-inducing ordinary content. We introduce a useful
framework of online harm (i.e., explicit, implicit, unintended), providing a
unique lens for understanding the dynamic, multifaceted online risks facing
children and teenagers. The findings highlight the importance of protecting
younger audiences in critical developmental stages from both explicit and
implicit risks on social media, calling for nuanced content moderation, age
verification, and platform regulation.

</details>


### [69] [Rookie Mistakes: Measuring Software Quality in Student Projects to Guide Educational Enhancement](https://arxiv.org/abs/2507.12488)
*Marco De Luca,Sergio Di Martino,Sergio Di Meglio,Anna Rita Fasolino,Luigi Libero Lucio Starace,Porfirio Tramontana*

Main category: cs.CY

TL;DR: 论文分析了83个大学生团队开发的项目，发现软件质量在本科编程教学中常被忽视，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 本科编程教学中软件质量常被忽视，缺乏对中级学生的研究指导。

Method: 对83个对象导向团队项目进行静态分析（SonarQube和ArchUnit），检测代码异味和架构反模式。

Result: 发现学生在此阶段面临的常见质量问题和挑战。

Conclusion: 研究为教育者改进软件工程课程提供了有价值的方向。

Abstract: When teaching Programming and Software Engineering in Bachelor's Degree
programs, the emphasis on creating functional software projects often
overshadows the focus on software quality, a trend that aligns with ACM
curricula recommendations. Software Engineering courses are typically
introduced later in the curriculum, and can generally allocate only limited
time to quality-related topics, leaving educators with the challenge of
deciding which quality aspects to prioritize. In this decision, the literature
offers limited guidance, as most existing studies focus on code written by
novice students and small code units, making it unclear whether those findings
extend to intermediate-level students with foundational object-oriented
programming skills working on more complex software projects. To address this
gap, we analyze 83 object-oriented team projects developed by 172 university
students across 4 different editions of the Object-Oriented Programming course.
We apply a static analysis pipeline used in prior research to assess software
quality, combining SonarQube and ArchUnit to detect code smells and
architectural anti-patterns. Our findings highlight recurring quality issues
and offer concrete evidence of the challenges students face at this stage,
providing valuable guidance for educators aiming to continuously improve
Software Engineering curricula and promote quality-oriented development
practices.

</details>


### [70] [ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle](https://arxiv.org/abs/2507.12674)
*Mihran Miroyan,Rose Niousha,Joseph E. Gonzalez,Gireeja Ranade,Narges Norouzi*

Main category: cs.CY

TL;DR: 论文研究了大型语言模型（LLM）是否能生成类似学生的代码（不完美、迭代、风格多样），提出ParaStudent方法，并通过实验表明微调能显著提高代码与学生轨迹的匹配度。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能生成真实学生风格的代码，以评估其在编程课程中的潜在应用。

Method: 利用时间戳学生提交数据，设计低/高分辨率实验，分析代码语义、功能和风格。

Result: 微调显著改善代码与学生轨迹的对齐，能捕捉错误模式、增量改进和风格变化。

Conclusion: 生成真实学生代码需结合上下文感知、时间建模和多维评估。

Abstract: Large Language Models (LLMs) have shown strong performance on programming
tasks, but can they generate student-like code like real students - imperfect,
iterative, and stylistically diverse? We present ParaStudent, a systematic
study of LLM-based "student-like" code generation in an introductory
programming course setting. Using a dataset of timestamped student submissions
across multiple semesters, we design low- and high-resolution experiments to
model student progress and evaluate code outputs along semantic, functional,
and stylistic dimensions. Our results show that fine-tuning significantly
improves alignment with real student trajectories and captures error patterns,
incremental improvements, and stylistic variations more faithfully. This study
shows that modeling realistic student code requires capturing learning dynamics
through context-aware generation, temporal modeling, and multi-dimensional
evaluation. Code for experiments and evaluation is available at
\href{https://github.com/mmiroyan/ParaStudent}{\texttt{github.com/mmiroyan/ParaStudent}}.

</details>


### [71] [The Case for Contextual Copyleft: Licensing Open Source Training Data and Generative AI](https://arxiv.org/abs/2507.12713)
*Grant Shanklin,Emmie Hine,Claudio Novelli,Tyler Schroder,Luciano Floridi*

Main category: cs.CY

TL;DR: 本文介绍了CCAI许可证，一种将传统copyleft原则扩展到AI模型训练的机制，并在法律、政策和风险评估框架下验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的兴起对开源社区提出了挑战，尤其是在copyleft原则如何应用于AI模型训练时。

Method: 提出CCAI许可证，并通过三部分评估框架（法律可行性、政策合理性和风险收益分析）验证。

Result: CCAI许可证增强了开发者控制，推动了开源AI的发展，但需配套监管措施以平衡风险。

Conclusion: 在完善的监管环境下，CCAI许可证可有效将FOSS核心原则适应生成式AI发展。

Abstract: The proliferation of generative AI systems has created new challenges for the
Free and Open Source Software (FOSS) community, particularly regarding how
traditional copyleft principles should apply when open source code is used to
train AI models. This article introduces the Contextual Copyleft AI (CCAI)
license, a novel licensing mechanism that extends copyleft requirements from
training data to the resulting generative AI models. The CCAI license offers
significant advantages, including enhanced developer control, incentivization
of open source AI development, and mitigation of openwashing practices. This is
demonstrated through a structured three-part evaluation framework that examines
(1) legal feasibility under current copyright law, (2) policy justification
comparing traditional software and AI contexts, and (3) synthesis of
cross-contextual benefits and risks. However, the increased risk profile of
open source AI, particularly the potential for direct misuse, necessitates
complementary regulatory approaches to achieve an appropriate risk-benefit
balance. The paper concludes that when implemented within a robust regulatory
environment focused on responsible AI usage, the CCAI license provides a viable
mechanism for preserving and adapting core FOSS principles to the evolving
landscape of generative AI development.

</details>


### [72] [Bridging Boundaries: How to Foster Effective Research Collaborations Across Affiliations in the Field of Trust and Safety](https://arxiv.org/abs/2507.13008)
*Amanda Menking,Mona Elswah,David J. Grüning,Lasse H. Hansen,Irene Huang,Julia Kamin,Catrine Normann*

Main category: cs.CY

TL;DR: 论文探讨了如何在信任与安全领域建立跨部门研究合作，克服激励、时间线和约束的不一致，利用各利益相关方的独特优势。


<details>
  <summary>Details</summary>
Motivation: 随着数字空间中信任与安全领域的发展，跨学术、行业、政府和非政府部门的合作变得至关重要，但也日益复杂。论文旨在解决合作中出现的激励不一致、时间冲突等问题。

Method: 基于实践经验，定义了主要的合作类型，分析了不同部门在研究重点、运营压力和评估指标上的差异，并提出一个实用框架，包括建立信任、目标对齐和角色分配的策略。

Result: 提出的框架强调了“阐述”这一关键工作，并展示了跨部门合作如何推动更伦理、公平和有影响力的研究。

Conclusion: 跨部门合作对信任与安全领域至关重要，应优先考虑包容性、透明度和实际相关性，以应对这一新兴领域的多学科需求。

Abstract: As the field of Trust and Safety in digital spaces continues to grow, it has
become increasingly necessary - but also increasingly complex - to collaborate
on research across the academic, industry, governmental and non-governmental
sectors. This paper examines how cross-affiliation research partnerships can be
structured to overcome misaligned incentives, timelines and constraints while
delivering on the unique strengths of each stakeholder. Drawing on our own
experience of cross-sector collaboration, we define the main types of
affiliation and highlight the common differences in research priorities,
operational pressures and evaluation metrics across sectors. We then propose a
practical, step-by-step framework for initiating and managing effective
collaborations, including strategies for building trust, aligning goals, and
distributing roles. We emphasize the critical yet often invisible work of
articulation and argue that cross-sector partnerships are essential for
developing more ethical, equitable and impactful research in trust and safety.
Ultimately, we advocate collaborative models that prioritize inclusivity,
transparency and real-world relevance in order to meet the interdisciplinary
demands of this emerging field.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [73] [Mapping Emotions in the Brain: A Bi-Hemispheric Neural Model with Explainable Deep Learning](https://arxiv.org/abs/2507.12625)
*David Freire-Obregón,Agnieszka Dubiel,Prasoon Kumar Vinodkumar,Gholamreza Anbarjafari,Dorota Kamińska,Modesto Castrillón-Santana*

Main category: q-bio.NC

TL;DR: 论文提出了一种针对双流脑电图分类器的后验可解释性框架，扩展了LIME方法以处理结构化的双半球输入，揭示了情绪特定的半球激活模式。


<details>
  <summary>Details</summary>
Motivation: 尽管双半球神经网络在脑电图情感识别中表现出潜力，但其可解释性限制了其在敏感领域的应用。

Method: 通过扩展LIME方法，处理左右半球脑电图通道组的结构化输入，分解预测相关性。

Result: 揭示了情绪特定的半球激活模式，如快乐中的额叶侧化和悲伤中的后部不对称性，进一步验证了情感神经科学中的功能不对称性。

Conclusion: 该框架为双流脑电图分类器提供了神经生理学依据的解释，支持其在情感计算和认知建模中的应用。

Abstract: Recent advances have shown promise in emotion recognition from
electroencephalogram (EEG) signals by employing bi-hemispheric neural
architectures that incorporate neuroscientific priors into deep learning
models. However, interpretability remains a significant limitation for their
application in sensitive fields such as affective computing and cognitive
modeling. In this work, we introduce a post-hoc interpretability framework
tailored to dual-stream EEG classifiers, extending the Local Interpretable
Model-Agnostic Explanations (LIME) approach to accommodate structured,
bi-hemispheric inputs. Our method adapts LIME to handle structured two-branch
inputs corresponding to left and right-hemisphere EEG channel groups. It
decomposes prediction relevance into per-channel contributions across
hemispheres and emotional classes. We apply this framework to a previously
validated dual-branch recurrent neural network trained on EmoNeuroDB, a dataset
of EEG recordings captured during a VR-based emotion elicitation task. The
resulting explanations reveal emotion-specific hemispheric activation patterns
consistent with known neurophysiological phenomena, such as frontal
lateralization in joy and posterior asymmetry in sadness. Furthermore, we
aggregate local explanations across samples to derive global channel importance
profiles, enabling a neurophysiologically grounded interpretation of the
model's decisions. Correlation analysis between symmetric electrodes further
highlights the model's emotion-dependent lateralization behavior, supporting
the functional asymmetries reported in affective neuroscience.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [74] [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/abs/2507.12714)
*Yang Yang,Dongni Mao,Hiroaki Santo,Yasuyuki Matsushita,Fumio Okura*

Main category: cs.CV

TL;DR: NeuraLeaf是一个神经参数化模型，用于3D植物叶片建模和重建，解决了叶片形状多样和变形灵活性的挑战。


<details>
  <summary>Details</summary>
Motivation: 植物叶片在农业和计算机图形学中具有重要意义，但其多样的形状和变形灵活性给建模带来了独特挑战。

Method: NeuraLeaf将叶片几何分离为2D基础形状和3D变形，并利用2D叶片图像数据集进行学习；提出了一种无骨骼的蒙皮模型并创建了新数据集DeformLeaf。

Result: NeuraLeaf能生成多种变形叶片形状，并能准确拟合3D观测数据（如深度图和点云）。

Conclusion: NeuraLeaf为植物叶片建模提供了有效的解决方案，适用于农业和计算机图形学应用。

Abstract: We develop a neural parametric model for 3D leaves for plant modeling and
reconstruction that are essential for agriculture and computer graphics. While
neural parametric models are actively studied for humans and animals, plant
leaves present unique challenges due to their diverse shapes and flexible
deformation. To this problem, we introduce a neural parametric model for
leaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be
approximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into
their 2D base shapes and 3D deformations. This representation allows learning
from rich sources of 2D leaf image datasets for the base shapes, and also has
the advantage of simultaneously learning textures aligned with the geometry. To
model the 3D deformation, we propose a novel skeleton-free skinning model and
create a newly captured 3D leaf dataset called DeformLeaf. We show that
NeuraLeaf successfully generates a wide range of leaf shapes with deformation,
resulting in accurate model fitting to 3D observations like depth maps and
point clouds. Our implementation and dataset are available at
https://neuraleaf-yang.github.io/.

</details>


### [75] [Predicting 3D Rigid Body Dynamics with Deep Residual Network](https://arxiv.org/abs/2407.18798)
*Abiodun Finbarrs Oketunji*

Main category: cs.CV

TL;DR: 本研究探讨了深度残差网络在预测三维刚体相互作用动力学中的应用，通过结合3D物理模拟器和深度学习模型，展示了其在捕捉复杂物理交互中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用深度残差网络解决三维刚体动力学预测的复杂性，特别是在弹性碰撞和旋转动力学等场景中。

Method: 结合C++编写的3D物理模拟器和PyTorch构建的深度残差网络，生成包含多种物理效应的训练数据。

Result: 在10,000个模拟场景中，模型在位置和方向预测上的均方误差分别为0.015和0.022，比基线方法提升了25%。

Conclusion: 研究证明了深度残差网络在复杂3D物理系统建模中的潜力，并提出了未来改进的方向。

Abstract: This study investigates the application of deep residual networks for
predicting the dynamics of interacting three-dimensional rigid bodies. We
present a framework combining a 3D physics simulator implemented in C++ with a
deep learning model constructed using PyTorch. The simulator generates training
data encompassing linear and angular motion, elastic collisions, fluid
friction, gravitational effects, and damping. Our deep residual network,
consisting of an input layer, multiple residual blocks, and an output layer, is
designed to handle the complexities of 3D dynamics. We evaluate the network's
performance using a datasetof 10,000 simulated scenarios, each involving 3-5
interacting rigid bodies. The model achieves a mean squared error of 0.015 for
position predictions and 0.022 for orientation predictions, representing a 25%
improvement over baseline methods. Our results demonstrate the network's
ability to capture intricate physical interactions, with particular success in
predicting elastic collisions and rotational dynamics. This work significantly
contributes to physics-informed machine learning by showcasing the immense
potential of deep residual networks in modeling complex 3D physical systems. We
discuss our approach's limitations and propose future directions for improving
generalization to more diverse object shapes and materials.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [76] [Efficiently Constructing Sparse Navigable Graphs](https://arxiv.org/abs/2507.13296)
*Alex Conway,Laxman Dhulipala,Martin Farach-Colton,Rob Johnson,Ben Landrum,Christopher Musco,Yarin Shechter,Torsten Suel,Richard Wen*

Main category: cs.DS

TL;DR: 该论文研究了图最近邻搜索中构建稀疏可导航图的快速算法，提出了一个近似最优解的方法，并将时间复杂度从立方级降至平方级。


<details>
  <summary>Details</summary>
Motivation: 当前构建稀疏可导航图的计算成本高昂，且缺乏理论保障。本文旨在设计高效且具有理论保证的算法。

Method: 利用最小集覆盖问题的技术，结合预处理和流式算法，提出了一个时间复杂度接近最优的算法。

Result: 提出了一个时间复杂度为$	ilde{O}(n^2)$的算法，构建的图稀疏度接近最优。

Conclusion: 该算法在理论上接近最优，并为相关问题的快速求解提供了新思路。

Abstract: Graph-based nearest neighbor search methods have seen a surge of popularity
in recent years, offering state-of-the-art performance across a wide variety of
applications. Central to these methods is the task of constructing a sparse
navigable search graph for a given dataset endowed with a distance function.
Unfortunately, doing so is computationally expensive, so heuristics are
universally used in practice.
  In this work, we initiate the study of fast algorithms with provable
guarantees for search graph construction. For a dataset with $n$ data points,
the problem of constructing an optimally sparse navigable graph can be framed
as $n$ separate but highly correlated minimum set cover instances. This yields
a naive $O(n^3)$ time greedy algorithm that returns a navigable graph whose
sparsity is at most $O(\log n)$ higher than optimal. We improve significantly
on this baseline, taking advantage of correlation between the set cover
instances to leverage techniques from streaming and sublinear-time set cover
algorithms. Combined with problem-specific pre-processing techniques, we
present an $\tilde{O}(n^2)$ time algorithm for constructing an $O(\log
n)$-approximate sparsest navigable graph under any distance function.
  The runtime of our method is optimal up to logarithmic factors under the
Strong Exponential Time Hypothesis via a reduction from Monochromatic Closest
Pair. Moreover, we prove that, as with general set cover, obtaining better than
an $O(\log n)$-approximation is NP-hard, despite the significant additional
structure present in the navigable graph problem. Finally, we show that our
techniques can also beat cubic time for the closely related and practically
important problems of constructing $\alpha$-shortcut reachable and
$\tau$-monotonic graphs, which are also used for nearest neighbor search. For
such graphs, we obtain $\tilde{O}(n^{2.5})$ time or better algorithms.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [77] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 论文探讨人类如何利用分布式和符号表示构建定制化心智模型以应对新情境，并提出了一种结合语言模型和概率程序的“模型合成架构”（MSA），该架构在模拟人类推理能力上优于纯语言模型基线。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何在新情境中利用广泛背景知识进行推理和预测，并提出计算模型以模拟这一能力。

Method: 提出“模型合成架构”（MSA），结合语言模型实现全局信息检索与合成，以及概率程序实现定制化世界模型。

Result: MSA在新型推理数据集上优于纯语言模型基线，更接近人类推理能力。

Conclusion: MSA能有效模拟人类在新领域中的连贯推理能力，为理解和复现人类推理提供新路径。

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [78] [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)
*Lyucheng Wu,Mengru Wang,Ziwen Xu,Tri Cao,Nay Oo,Bryan Hooi,Shumin Deng*

Main category: cs.CL

TL;DR: AutoSteer是一种模块化、自适应的推理时间干预技术，旨在提升多模态大型语言模型的安全性，无需微调。通过安全评分、自适应探测器和拒绝头三个组件，显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）在跨模态推理方面表现出强大能力，但面对对抗性输入时存在安全隐患，需在推理过程中提升安全性。

Method: 提出AutoSteer技术，包含三个核心组件：安全感知评分（SAS）、自适应安全探测器和轻量级拒绝头，动态干预不安全输出。

Result: 实验表明，AutoSteer显著降低了文本、视觉和跨模态威胁的攻击成功率（ASR），同时保持模型的通用能力。

Conclusion: AutoSteer为多模态AI系统的安全部署提供了一种实用、可解释且高效的解决方案。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has unlocked
powerful cross-modal reasoning abilities, but also raised new safety concerns,
particularly when faced with adversarial multimodal inputs. To improve the
safety of MLLMs during inference, we introduce a modular and adaptive
inference-time intervention technology, AutoSteer, without requiring any
fine-tuning of the underlying model. AutoSteer incorporates three core
components: (1) a novel Safety Awareness Score (SAS) that automatically
identifies the most safety-relevant distinctions among the model's internal
layers; (2) an adaptive safety prober trained to estimate the likelihood of
toxic outputs from intermediate representations; and (3) a lightweight Refusal
Head that selectively intervenes to modulate generation when safety risks are
detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical
benchmarks demonstrate that AutoSteer significantly reduces the Attack Success
Rate (ASR) for textual, visual, and cross-modal threats, while maintaining
general abilities. These findings position AutoSteer as a practical,
interpretable, and effective framework for safer deployment of multimodal AI
systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [79] [Physically Based Neural LiDAR Resimulation](https://arxiv.org/abs/2507.12489)
*Richard Marcus,Marc Stamminger*

Main category: cs.RO

TL;DR: 该论文提出了一种新的LiDAR模拟方法，通过显式建模传感器特性（如滚动快门、激光功率变化和强度衰减），实现了比现有技术更准确的LiDAR模拟。


<details>
  <summary>Details</summary>
Motivation: 当前LiDAR模拟技术在传感器特定效果（如滚动快门、激光功率变化等）上表现不足，需要一种更精确的模拟方法。

Method: 通过显式建模LiDAR传感器特性（如滚动快门、激光功率变化和强度衰减），提出了一种新的LiDAR模拟方法。

Result: 定量和定性比较显示，该方法优于现有技术，同时消融实验验证了各传感器模型组件的重要性。此外，该方法还展示了高分辨率LiDAR扫描的再模拟能力。

Conclusion: 该方法在LiDAR模拟中表现出更高的准确性，并具有先进的再模拟能力，为LiDAR仿真和3D场景重建提供了新的工具。

Abstract: Methods for Novel View Synthesis (NVS) have recently found traction in the
field of LiDAR simulation and large-scale 3D scene reconstruction. While
solutions for faster rendering or handling dynamic scenes have been proposed,
LiDAR specific effects remain insufficiently addressed. By explicitly modeling
sensor characteristics such as rolling shutter, laser power variations, and
intensity falloff, our method achieves more accurate LiDAR simulation compared
to existing techniques. We demonstrate the effectiveness of our approach
through quantitative and qualitative comparisons with state-of-the-art methods,
as well as ablation studies that highlight the importance of each sensor model
component. Beyond that, we show that our approach exhibits advanced
resimulation capabilities, such as generating high resolution LiDAR scans in
the camera perspective.
  Our code and the resulting dataset are available at
https://github.com/richardmarcus/PBNLiDAR.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [80] [Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries](https://arxiv.org/abs/2507.12723)
*Minyoung Kim,Sehwan Park,Sungmin Cha,Paul Hongsuck Seo*

Main category: cs.SD

TL;DR: 该论文提出了一种用于从合成的视听伪造内容中恢复真实音频并定位篡改的方法，通过跨模态水印技术嵌入真实音频信息，有效抵抗语音克隆和唇形同步等伪造手段。


<details>
  <summary>Details</summary>
Motivation: 合成的视听伪造内容（如语音克隆和唇形同步）增加了虚假信息的风险，现有方法无法恢复原始音频，限制了对抗虚假信息的有效性。

Method: 提出了一种跨模态水印框架，在伪造前将真实音频嵌入视觉内容中，从而实现真实音频恢复和篡改定位。

Result: 实验表明，该方法在各种伪造手段（如语音克隆和唇形同步）下的真实音频恢复和篡改定位表现优异。

Conclusion: 该方法为抵抗合成的视听伪造内容提供了一种有效的解决方案，支持真实音频恢复和篡改定位任务。

Abstract: Recent advances in voice cloning and lip synchronization models have enabled
Synthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are
manipulated to mimic a target speaker. This significantly increases the risk of
misinformation by making fake content seem real. To address this issue,
existing methods detect or localize manipulations but cannot recover the
authentic audio that conveys the semantic content of the message. This
limitation reduces their effectiveness in combating audiovisual misinformation.
In this work, we introduce the task of Authentic Audio Recovery (AAR) and
Tamper Localization in Audio (TLA) from SAVFs and propose a cross-modal
watermarking framework to embed authentic audio into visuals before
manipulation. This enables AAR, TLA, and a robust defense against
misinformation. Extensive experiments demonstrate the strong performance of our
method in AAR and TLA against various manipulations, including voice cloning
and lip synchronization.

</details>


### [81] [Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy Protection against Voice Deepfakes](https://arxiv.org/abs/2507.12932)
*Zhou Feng,Jiahao Chen,Chunyi Zhou,Yuwen Pu,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.SD

TL;DR: Enkidu框架通过频率域噪声扰动保护用户音频隐私，高效防御语音深度伪造攻击。


<details>
  <summary>Details</summary>
Motivation: 语音深度伪造技术威胁用户音频隐私，现有防御方法在适应性、扩展性和效率方面存在不足。

Method: 利用黑盒知识和少量用户数据训练，生成频率域噪声扰动，实现轻量级实时保护。

Result: Enkidu在内存和运行时间效率上显著优于现有方法，对抗多种语音深度伪造攻击有效。

Conclusion: Enkidu为语音隐私保护提供了高效且实用的解决方案。

Abstract: The rapid advancement of voice deepfake technologies has raised serious
concerns about user audio privacy, as attackers increasingly exploit publicly
available voice data to generate convincing fake audio for malicious purposes
such as identity theft, financial fraud, and misinformation campaigns. While
existing defense methods offer partial protection, they face critical
limitations, including weak adaptability to unseen user data, poor scalability
to long audio, rigid reliance on white-box knowledge, and high computational
and temporal costs during the encryption process. To address these challenges
and defend against personalized voice deepfake threats, we propose Enkidu, a
novel user-oriented privacy-preserving framework that leverages universal
frequential perturbations generated through black-box knowledge and few-shot
training on a small amount of user data. These highly malleable
frequency-domain noise patches enable real-time, lightweight protection with
strong generalization across variable-length audio and robust resistance to
voice deepfake attacks, all while preserving perceptual quality and speech
intelligibility. Notably, Enkidu achieves over 50 to 200 times processing
memory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime
efficiency (real-time coefficient as low as 0.004) compared to six
state-of-the-art countermeasures. Extensive experiments across six mainstream
text-to-speech models and five cutting-edge automated speaker verification
models demonstrate the effectiveness, transferability, and practicality of
Enkidu in defending against both vanilla and adaptive voice deepfake attacks.

</details>


### [82] [Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features](https://arxiv.org/abs/2507.12793)
*J. M. Chan Sri Manukalpa,H. S. Bopage,W. A. M. Jayawardena,P. K. P. G. Panduwawala*

Main category: cs.SD

TL;DR: 提出了一种基于深度学习的非侵入式声学分类框架，用于早期白蚁检测，混合CNN-LSTM模型表现出高准确性和低假阴性率。


<details>
  <summary>Details</summary>
Motivation: 传统白蚁检测方法侵入性强且效率低，需开发非侵入式、自动化的早期检测解决方案以减少经济损失。

Method: 采用混合CNN-LSTM架构，结合MFCC特征提取，对白蚁活动声信号进行分类。

Result: 模型准确率达94.5%，精确度93.2%，召回率95.8%，优于单一CNN或LSTM模型。

Conclusion: 该研究为早期白蚁检测提供了自动化解决方案，未来可结合物联网实现实时警报，并扩展到其他害虫检测。

Abstract: Structural pests, such as termites, pose a serious threat to wooden
buildings, resulting in significant economic losses due to their hidden and
progressive damage. Traditional detection methods, such as visual inspections
and chemical treatments, are invasive, labor intensive, and ineffective for
early stage infestations. To bridge this gap, this study proposes a non
invasive deep learning based acoustic classification framework for early
termite detection. We aim to develop a robust, scalable model that
distinguishes termite generated acoustic signals from background noise. We
introduce a hybrid Convolutional Neural Network Long Short Term Memory
architecture that captures both spatial and temporal features of termite
activity. Audio data were collected from termite infested and clean wooden
samples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN
LSTM model to classify the signals. Experimental results show high performance,
with 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis
reveals that the hybrid model outperforms standalone CNN and LSTM
architectures, underscoring its combined strength. Notably, the model yields
low false-negative rates, which is essential for enabling timely intervention.
This research contributes a non invasive, automated solution for early termite
detection, with practical implications for improved pest monitoring, minimized
structural damage, and better decision making by homeowners and pest control
professionals. Future work may integrate IoT for real time alerts and extend
detection to other structural pests.

</details>
