<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 20]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 7]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866)
*Thomas Kuntz,Agatha Duzan,Hao Zhao,Francesco Croce,Zico Kolter,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.SE

TL;DR: 介绍了OS-Harm基准测试，用于评估基于LLM的计算机使用代理的安全性，覆盖三类危害行为，并测试多种前沿模型的安全性表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的计算机使用代理（通过截图或无障碍树交互）安全性被忽视，但其潜在危害行为评估对广泛应用至关重要。

Method: 基于OSWorld环境构建OS-Harm基准，设计150个任务测试三类危害（滥用、提示注入、模型误行为），并提出自动化评判方法。

Result: 测试显示所有前沿模型（如o4-mini、Claude 3.7等）易被滥用、易受提示注入攻击且偶尔执行不安全行为。自动化评判与人工标注一致性高（F1分数0.76/0.79）。

Conclusion: OS-Harm填补了计算机代理安全性评估的空白，揭示了当前模型的潜在风险，为未来改进提供参考。

Abstract: Computer use agents are LLM-based agents that can directly interact with a
graphical user interface, by processing screenshots or accessibility trees.
While these systems are gaining popularity, their safety has been largely
overlooked, despite the fact that evaluating and understanding their potential
for harmful behavior is essential for widespread adoption. To address this gap,
we introduce OS-Harm, a new benchmark for measuring safety of computer use
agents. OS-Harm is built on top of the OSWorld environment and aims to test
models across three categories of harm: deliberate user misuse, prompt
injection attacks, and model misbehavior. To cover these cases, we create 150
tasks that span several types of safety violations (harassment, copyright
infringement, disinformation, data exfiltration, etc.) and require the agent to
interact with a variety of OS applications (email client, code editor, browser,
etc.). Moreover, we propose an automated judge to evaluate both accuracy and
safety of agents that achieves high agreement with human annotations (0.76 and
0.79 F1 score). We evaluate computer use agents based on a range of frontier
models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide
insights into their safety. In particular, all models tend to directly comply
with many deliberate misuse queries, are relatively vulnerable to static prompt
injections, and occasionally perform unsafe actions. The OS-Harm benchmark is
available at https://github.com/tml-epfl/os-harm.

</details>


### [2] [An Empirical Study of Bugs in Data Visualization Libraries](https://arxiv.org/abs/2506.15084)
*Weiqi Lu,Yongqiang Tian,Xiaohan Zhong,Haoyang Ma,Zhenyang Xu,Shing-Chi Cheung,Chengnian Sun*

Main category: cs.SE

TL;DR: 本文首次全面分析了数据可视化库中的错误，研究了564个来自五个常用库的bug，系统性分析了其症状和根源，并提出分类方法。研究发现错误的图形计算是主要根源，且提出了八个触发步骤和两个测试预言。此外，探索了视觉语言模型（VLM）检测错误图形的可行性，发现其效果在29%至57%之间。


<details>
  <summary>Details</summary>
Motivation: 数据可视化库（DataViz）的准确性至关重要，错误的可视化可能误导用户决策。然而，对其bug特性的了解不足，因此需要深入研究。

Method: 研究收集并分析了564个来自五个广泛使用的DataViz库的bug，系统性分析了症状、根源并提出分类。此外，探索了VLM在检测错误图形中的应用。

Result: 研究发现错误的图形计算是主要根源，提出了八个触发步骤和两个测试预言。VLM的检测效果在29%至57%之间，提示信息越多不一定效果越好。

Conclusion: 研究为DataViz库的bug检测提供了理论基础和实践指导，并指出了VLM在这一领域的潜力和局限性。

Abstract: Data visualization (DataViz) libraries play a crucial role in presentation,
data analysis, and application development, underscoring the importance of
their accuracy in transforming data into visual representations. Incorrect
visualizations can adversely impact user experience, distort information
conveyance, and influence user perception and decision-making processes. Visual
bugs in these libraries can be particularly insidious as they may not cause
obvious errors like crashes, but instead mislead users of the underlying data
graphically, resulting in wrong decision making. Consequently, a good
understanding of the unique characteristics of bugs in DataViz libraries is
essential for researchers and developers to detect and fix bugs in DataViz
libraries.
  This study presents the first comprehensive analysis of bugs in DataViz
libraries, examining 564 bugs collected from five widely-used libraries. Our
study systematically analyzes their symptoms and root causes, and provides a
detailed taxonomy. We found that incorrect/inaccurate plots are pervasive in
DataViz libraries and incorrect graphic computation is the major root cause,
which necessitates further automated testing methods for DataViz libraries.
Moreover, we identified eight key steps to trigger such bugs and two test
oracles specific to DataViz libraries, which may inspire future research in
designing effective automated testing techniques. Furthermore, with the recent
advancements in Vision Language Models (VLMs), we explored the feasibility of
applying these models to detect incorrect/inaccurate plots. The results show
that the effectiveness of VLMs in bug detection varies from 29% to 57%,
depending on the prompts, and adding more information in prompts does not
necessarily increase the effectiveness. More findings can be found in our
manuscript.

</details>


### [3] [Program Feature-based Fuzzing Benchmarking](https://arxiv.org/abs/2506.15088)
*Miao Miao*

Main category: cs.SE

TL;DR: 本文介绍了一种新型基准测试，用于生成具有可配置细粒度程序特征的程序，以改进模糊测试的评估。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试评估通常关注整体性能，而忽视了细粒度程序特征对模糊测试效果的影响，本文旨在填补这一空白。

Method: 通过分析25篇灰色模糊测试研究，提取7个与控制和数据流相关的程序特征，生成包含153个程序的基准测试集，并用其评估11个流行模糊测试工具。

Result: 模糊测试工具的性能因程序特征及其强度而异，强调了在评估中考虑程序特性的重要性。

Conclusion: 提出了一个基准测试框架，揭示了程序特征对模糊测试效果的影响，为未来模糊测试研究提供了新方向。

Abstract: Fuzzing is a powerful software testing technique renowned for its
effectiveness in identifying software vulnerabilities. Traditional fuzzing
evaluations typically focus on overall fuzzer performance across a set of
target programs, yet few benchmarks consider how fine-grained program features
influence fuzzing effectiveness. To bridge this gap, we introduce a novel
benchmark designed to generate programs with configurable, fine-grained program
features to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing
studies, extracting 7 program features related to control-flow and data-flow
that can impact fuzzer performance. Using these features, we generated a
benchmark consisting of 153 programs controlled by 10 fine-grained configurable
parameters. We evaluated 11 popular fuzzers using this benchmark. The results
indicate that fuzzer performance varies significantly based on the program
features and their strengths, highlighting the importance of incorporating
program characteristics into fuzzing evaluations.

</details>


### [4] [Enhancement Report Approval Prediction: A Comparative Study of Large Language Models](https://arxiv.org/abs/2506.15098)
*Haosheng Zuo,Feifei Niu,Chuanyi Li*

Main category: cs.SE

TL;DR: 该论文研究了利用大语言模型（LLM）自动预测软件改进报告（ERs）的可行性，发现LLM在准确率和召回率上优于传统方法，特别是在结合创作者档案和LoRA微调后。


<details>
  <summary>Details</summary>
Motivation: 传统手动处理软件改进报告效率低且易丢失有价值的信息，研究旨在通过LLM改进自动决策的准确率。

Method: 系统评估了18种LLM变体（包括编码器和解码器模型），与传统方法（CNN/LSTM-BERT/GloVe）进行比较，并探究了创作者档案和LoRA微调的效果。

Result: LLM显著优于传统方法，LoRA微调的Llama 3.1 8B Instruct达到79%的准确率，并改善了类别不平衡问题。

Conclusion: LLM为ERAP提供了更优解决方案，未来可进一步研究其不足场景。

Abstract: Enhancement reports (ERs) serve as a critical communication channel between
users and developers, capturing valuable suggestions for software improvement.
However, manually processing these reports is resource-intensive, leading to
delays and potential loss of valuable insights. To address this challenge,
enhancement report approval prediction (ERAP) has emerged as a research focus,
leveraging machine learning techniques to automate decision-making. While
traditional approaches have employed feature-based classifiers and deep
learning models, recent advancements in large language models (LLM) present new
opportunities for enhancing prediction accuracy. This study systematically
evaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and
XLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1
8B Instruct and DeepSeek-V3 for decoder models) against traditional methods
(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)
Incorporating creator profiles increases unfine-tuned decoder-only models'
overall accuracy by 10.8 percent though it may introduce bias; (2) LoRA
fine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79
percent accuracy and significantly enhancing recall for approved reports (76.1
percent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5
percent under strict chronological evaluation and effectively addressing class
imbalance issues. These findings establish LLM as a superior solution for ERAP,
demonstrating their potential to streamline software maintenance workflows and
improve decision-making in real-world development environments. We also
investigated and summarized the ER cases where the large models underperformed,
providing valuable directions for future research.

</details>


### [5] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: 提出了一种验证框架，用于静态证明使用Go语言子集的分布式程序中不存在通信竞争。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的并发性难以推理，通信竞争会导致接收者收到错误消息或无消息，急需静态验证方法。

Method: 通过扩展happens-before顺序到缓冲和非缓冲通道，静态分析分布式程序的执行逻辑。

Result: 框架能够证明分布式程序中通信竞争的缺失。

Conclusion: 扩展的happens-before顺序适用于静态验证通信竞争，适用于Go语言的分布式编程。

Abstract: Programmers of distributed systems need to reason about concurrency to avoid
races. However, reasoning about concurrency is difficult, and unexpected races
show up as bugs. Data race detection in shared memory systems is well-studied
(dynamic data race detection [13], behavioral types [15], dynamic race
detection [31]). Similar to how a data race consists of reads and writes not
related by happens-before at a shared memory location, a communication race
consists of receives and sends not related by happens-before on a shared
channel. Communication races are problematic: a receiver expects a specific
message from a specific sender, but with a communication race, the receiver can
receive a message meant for another receiver, or not receive anything at all.
In this work, we describe a verification framework that can prove the absence
of communication races for distributed programs that use a subset of the Go
programming language, where synchronization is mainly achieved via message
passing. We statically reason about how a distributed program executes, using a
happens-before order, extended to buffered and unbuffered channels.

</details>


### [6] [Advanced approach for Agile/Scrum Process: RetroAI++](https://arxiv.org/abs/2506.15172)
*Maria Spichkova,Kevin Iwan,Madeleine Zwart,Hina Lee,Yuwon Yoon,Xiaohan Qin*

Main category: cs.SE

TL;DR: RetroAI++是一个基于智能技术的原型工具，旨在自动化和优化Agile/Scrum开发中的冲刺计划和回顾分析。


<details>
  <summary>Details</summary>
Motivation: 支持软件开发者在冲刺计划和回顾分析中更高效地应用Agile/Scrum方法。

Method: 利用人工智能技术，自动化冲刺计划、开发和回顾阶段的多项流程，并提供智能建议和有意义的分析。

Result: 开发了RetroAI++原型工具，能够为冲刺组织和回顾反思提供智能化支持。

Conclusion: RetroAI++有潜力显著提升Agile/Scrum开发中的项目效率和质量。

Abstract: In Agile/Scrum software development, sprint planning and retrospective
analysis are the key elements of project management. The aim of our work is to
support software developers in these activities. In this paper, we present our
prototype tool RetroAI++, based on emerging intelligent technologies. In our
RetroAI++ prototype, we aim to automate and refine the practical application of
Agile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI
insights, our prototype aims to automate and refine the many processes involved
in the Sprint Planning, Development and Retrospective stages of Agile/Scrum
development projects, offering intelligent suggestions for sprint organisation
as well as meaningful insights for retrospective reflection.

</details>


### [7] [Large Language Models for Unit Testing: A Systematic Literature Review](https://arxiv.org/abs/2506.15227)
*Quanjun Zhang,Chunrong Fang,Siqi Gu,Ye Shang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: 本文对截至2025年3月的大语言模型（LLMs）在单元测试中的应用进行了首次系统文献综述，总结了研究现状、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，其在单元测试自动化中的应用迅速增长，但研究领域尚缺乏系统性总结，亟需综述以指导未来研究。

Method: 通过分析相关论文，从单元测试和LLMs两个角度分类现有任务（如测试生成与断言生成），并探讨模型使用、适应策略及混合方法。

Result: 总结了LLMs在单元测试中的关键挑战和潜在研究方向，提供了研究现状的全面概述。

Conclusion: 本文为单元测试领域提供了系统性综述，帮助研究人员理解现有成果并推动未来研究，相关资源已开源。

Abstract: Unit testing is a fundamental practice in modern software engineering, with
the aim of ensuring the correctness, maintainability, and reliability of
individual software components. Very recently, with the advances in Large
Language Models (LLMs), a rapidly growing body of research has leveraged LLMs
to automate various unit testing tasks, demonstrating remarkable performance
and significantly reducing manual effort. However, due to ongoing explorations
in the LLM-based unit testing field, it is challenging for researchers to
understand existing achievements, open challenges, and future opportunities.
This paper presents the first systematic literature review on the application
of LLMs in unit testing until March 2025. We analyze \numpaper{} relevant
papers from the perspectives of both unit testing and LLMs. We first categorize
existing unit testing tasks that benefit from LLMs, e.g., test generation and
oracle generation. We then discuss several critical aspects of integrating LLMs
into unit testing research, including model usage, adaptation strategies, and
hybrid approaches. We further summarize key challenges that remain unresolved
and outline promising directions to guide future research in this area.
Overall, our paper provides a systematic overview of the research landscape to
the unit testing community, helping researchers gain a comprehensive
understanding of achievements and promote future research. Our artifacts are
publicly available at the GitHub repository:
https://github.com/iSEngLab/AwesomeLLM4UT.

</details>


### [8] [Uncovering Intention through LLM-Driven Code Snippet Description Generation](https://arxiv.org/abs/2506.15453)
*Yusuf Sulistyo Nugroho,Farah Danisha Salam,Brittany Reid,Raula Gaikovina Kula,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 该论文研究了开发者常用的代码片段描述类型，并评估了LLM（如Llama）在生成描述方面的表现。研究发现，55.5%的原始描述基于示例使用，LLM的正确识别率为79.75%，生成描述的平均相似得分为0.7173。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的兴起，研究开发者常用的代码片段描述类型，并评估LLM在支持描述生成方面的效果。

Method: 使用NPM代码片段数据集（185,412个包，1,024,579个代码片段），手动分类400个样本描述，并利用LLM生成描述进行比较。

Result: 55.5%的原始描述为示例使用，LLM正确识别率为79.75%；生成描述的平均相似得分为0.7173，显示相关性但仍有改进空间。

Conclusion: 代码片段的文档意图可能因任务类型而异，LLM在描述生成方面表现尚可，但仍需提升细节和相关性。

Abstract: Documenting code snippets is essential to pinpoint key areas where both
developers and users should pay attention. Examples include usage examples and
other Application Programming Interfaces (APIs), which are especially important
for third-party libraries. With the rise of Large Language Models (LLMs), the
key goal is to investigate the kinds of description developers commonly use and
evaluate how well an LLM, in this case Llama, can support description
generation. We use NPM Code Snippets, consisting of 185,412 packages with
1,024,579 code snippets. From there, we use 400 code snippets (and their
descriptions) as samples. First, our manual classification found that the
majority of original descriptions (55.5%) highlight example-based usage. This
finding emphasizes the importance of clear documentation, as some descriptions
lacked sufficient detail to convey intent. Second, the LLM correctly identified
the majority of original descriptions as "Example" (79.75%), which is identical
to our manual finding, showing a propensity for generalization. Third, compared
to the originals, the produced description had an average similarity score of
0.7173, suggesting relevance but room for improvement. Scores below 0.9
indicate some irrelevance. Our results show that depending on the task of the
code snippet, the intention of the document may differ from being instructions
for usage, installations, or descriptive learning examples for any user of a
library.

</details>


### [9] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: 论文提出了一种基于抽象语法树（AST）的代码分块方法，以提升检索增强生成（RAG）在代码生成中的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于行的分块方法会破坏代码的语义结构，导致生成质量下降，因此需要一种结构感知的分块方法。

Method: 通过抽象语法树递归分解大型AST节点，并在大小限制内合并兄弟节点，生成语义连贯的分块单元。

Result: 该方法在多种代码生成任务中表现优异，如在RepoEval检索中Recall@5提升4.3点，在SWE-bench生成中Pass@1提升2.67点。

Conclusion: 结构感知分块对扩展检索增强代码智能具有重要意义。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs](https://arxiv.org/abs/2506.15174)
*Hossein Albakri,Kazem Cheshmi*

Main category: cs.PL

TL;DR: 提出了一种名为enumerate-and-sparse-coarsen的编译器变换方法，用于在GPU上加速稀疏矩阵-矩阵乘法（SPMM），解决了稀疏数据结构导致的计算资源使用低效问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏数据结构在神经网络中常用以减少内存占用，但会导致内存访问不规则，影响GPU资源的高效利用。

Method: 通过新的编译器变换方法，增加了寄存器和缓存中的数据重用，同时优化了GPU计算资源的负载均衡。

Result: 在A100 GPU上，SPMM的性能提升了1.84倍至2.27倍，相较于cuBLAS和cuSPARSE基线。

Conclusion: 提出的方法有效提升了GPU上稀疏矩阵计算的效率，适用于卷积和变换器模型。

Abstract: Sparse data structures are commonly used in neural networks to reduce the
memory footprint. These data structures are compact but cause irregularities
such as random memory accesses, which prevent efficient use of the memory
hierarchy. GPUs are a common platform for machine learning practitioners, but
running compact data structures on these devices often leads to slow-downs due
to inefficient use of computing and memory resources. This paper proposes a new
compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse
matrix-matrix multiplication (SPMM) on GPU devices. The transformation
increases data reuse in registers and caches while creating more balanced
workloads for GPU computing resources. The transformation is tested on sparse
neural networks in convolutional and transformer models. On an A100 GPU and
across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to
128, the transformation yields a geometric mean speedup of 1.84$\times$ to
2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

</details>


### [11] [PSM: Policy Synchronised Deterministic Memory](https://arxiv.org/abs/2506.15424)
*Michael Mendler,Marc Pouzet*

Main category: cs.PL

TL;DR: 该论文提出了一种新的Haskell类型上下文PSM（策略同步内存），旨在解决并发编程与确定性之间的矛盾，通过策略协调共享内存的访问，实现无竞争且确定性的并发编程。


<details>
  <summary>Details</summary>
Motivation: 现有的Haskell并发抽象（如IVar、LVar、MVar、TVar）无法同时满足确定性和破坏性更新的需求，而Kahn或Berry等高层次的并发确定性模式缺乏内存抽象支持，因此需要一种新的解决方案。

Method: 设计并实现了PSM（策略同步内存）类型上下文，支持并发线程和共享状态，通过策略协调确保无竞争访问，从而保证确定性。

Result: PSM成功实现了既支持破坏性更新又保持确定性的并发编程模式，并通过类型系统保证了抽象数据结构的确定性行为。

Conclusion: PSM为Haskell提供了一种新型的并发编程抽象，填补了现有方法的不足，支持确定性的共享内存访问，适用于需要高并发和确定性的场景。

Abstract: Concurrency and determinacy do not go well with each other when resources
must be shared. Haskell provides parallel programming abstractions such as IVar
and LVar in the Par monad and concurrent abstractions such as MVar and TVar in
the in IO and STM monads, respectively. The former are determinate but have no
destructive updates and the latter have destructive updates but do not
guarantee determinacy. Programming patterns that are both concurrent and
determinate, such as those provided by Kahn or Berry require memory
abstractions at a higher level than is currently available. In this paper we
describe a new type context PSM for policy synchronised memory in Haskell. Like
STM and IO, the computations in PSM can access persistent state and, as a
side-effect, update the memory in imperative style. Like the Par and IO monads,
PSM supports concurrent threads and shared state. However, in contrast to IO,
our PSM contexts are race-free since concurrent accesses are policy coordinated
which guarantees determinacy.Well-typed transactions in the PSM context can
accommodate abstract data structures that are imperative, concurrently
shareable and still behave deterministically, by construction.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [12] [Atys: An Efficient Profiling Framework for Identifying Hotspot Functions in Large-scale Cloud Microservices](https://arxiv.org/abs/2506.15523)
*Jiaqi Sun,Dingyu Yang,Shiyou Qian,Jian Cao,Guangtao Xue*

Main category: cs.PF

TL;DR: 本文介绍了Atys1，一种高效的分析框架，旨在识别大规模分布式服务中的热点函数，具备多语言适配、两级聚合、选择性剪枝和动态调整采样频率等特性，显著降低了分析成本。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式服务中，即使微小的性能提升也能带来显著的成本节约，因此需要一个高效的框架来识别热点函数。

Method: Atys1采用了多语言适配机制、两级聚合方法、函数选择性剪枝策略和动态调整采样频率方案。

Result: 实验表明，FSP策略减少了6.8%的时间，MAPE仅为0.58%；FDA方案在保持高采样率准确性的同时，成本降低了87.6%。

Conclusion: Atys1是一个高效且成本优化的分析框架，适用于大规模分布式服务的性能优化。

Abstract: To handle the high volume of requests, large-scale services are comprised of
thousands of instances deployed in clouds. These services utilize diverse
programming languages and are distributed across various nodes as encapsulated
containers. Given their vast scale, even minor performance enhancements can
lead to significant cost reductions. In this paper, we introduce Atys1, an
efficient profiling framework specifically designed to identify hotspot
functions within large-scale distributed services. Atys presents four key
features. First, it implements a language-agnostic adaptation mechanism for
multilingual microservices. Second, a two-level aggregation method is
introduced to provide a comprehensive overview of flamegraphs. Third, we
propose a function selective pruning (FSP) strategy to enhance the efficiency
of aggregating profiling results. Finally, we develop a frequency dynamic
adjustment (FDA) scheme that dynamically modifies sampling frequency based on
service status, effectively minimizing profiling cost while maintaining
accuracy. Cluster-scale experiments on two benchmarks show that the FSP
strategy achieves a 6.8% reduction in time with a mere 0.58% mean average
percentage error (MAPE) in stack traces aggregation. Additionally, the FDA
scheme ensures that the mean squared error (MSE) remains on par with that at
high sampling rates, while achieving an 87.6% reduction in cost.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [13] [CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in Industrial URLLC](https://arxiv.org/abs/2506.14987)
*Eman Alqudah,Ashfaq Khokhar*

Main category: cs.NI

TL;DR: 该论文提出了一种基于CNN的动态优先级预测机制，用于增强LDP算法，以在多小区、多通道网络中提高干扰协调效率。


<details>
  <summary>Details</summary>
Motivation: 确保大规模工业无线网络中超可靠低延迟通信（URLLC）的分组级通信质量是关键。

Method: 通过卷积神经网络和图着色技术动态分配链路优先级，实时考虑流量、传输机会和网络条件。

Result: 仿真结果显示，在三种网络配置中SINR增益分别达到113%、94%和49%，显著优于LDP算法。

Conclusion: 该方法在复杂URLLC场景中表现出色，能高效分配资源并提升网络容量。

Abstract: Ensuring packet-level communication quality is vital for ultra-reliable,
low-latency communications (URLLC) in large-scale industrial wireless networks.
We enhance the Local Deadline Partition (LDP) algorithm by introducing a
CNN-based dynamic priority prediction mechanism for improved interference
coordination in multi-cell, multi-channel networks. Unlike LDP's static
priorities, our approach uses a Convolutional Neural Network and graph coloring
to adaptively assign link priorities based on real-time traffic, transmission
opportunities, and network conditions. Assuming that first training phase is
performed offline, our approach introduced minimal overhead, while enabling
more efficient resource allocation, boosting network capacity, SINR, and
schedulability. Simulation results show SINR gains of up to 113\%, 94\%, and
49\% over LDP across three network configurations, highlighting its
effectiveness for complex URLLC scenarios.

</details>


### [14] [GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC](https://arxiv.org/abs/2506.15011)
*Eman Alqudah,Ashfaq Khokhar*

Main category: cs.NI

TL;DR: 论文通过结合GCN和DQN改进了LDP算法，动态学习链路优先级，显著提升了网络性能。


<details>
  <summary>Details</summary>
Motivation: 确保大规模工业无线网络中URLLC的包级通信质量至关重要。

Method: 引入GCN与DQN强化学习框架，动态学习链路优先级，基于实时流量需求、网络拓扑等。

Result: GCN-DQN在三种网络配置下，SINR平均提升显著（179.6%~197.4%），优于LDP和CNN方法。

Conclusion: GCN-DQN模型能高效满足URLLC需求，性能优越且开销小。

Abstract: Ensuring packet-level communication quality is vital for ultra-reliable,
low-latency communications (URLLC) in large-scale industrial wireless networks.
We enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph
Convolutional Network (GCN) integrated with a Deep Q-Network (DQN)
reinforcement learning framework for improved interference coordination in
multi-cell, multi-channel networks. Unlike LDP's static priorities, our
approach dynamically learns link priorities based on real-time traffic demand,
network topology, remaining transmission opportunities, and interference
patterns. The GCN captures spatial dependencies, while the DQN enables adaptive
scheduling decisions through reward-guided exploration. Simulation results show
that our GCN-DQN model achieves mean SINR improvements of 179.6\%, 197.4\%, and
175.2\% over LDP across three network configurations. Additionally, the GCN-DQN
model demonstrates mean SINR improvements of 31.5\%, 53.0\%, and 84.7\% over
our previous CNN-based approach across the same configurations. These results
underscore the effectiveness of our GCN-DQN model in addressing complex URLLC
requirements with minimal overhead and superior network performance.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [Omnidirectional Video Super-Resolution using Deep Learning](https://arxiv.org/abs/2506.14803)
*Arbind Agrahari Baniya,Tsz-Kwan Lee,Peter W. Eklund,Sunil Aryal*

Main category: cs.MM

TL;DR: 本文提出了一种针对360度视频的超分辨率技术S3PO，解决了传统视频超分辨率方法在360度视频中的失真问题，并构建了一个新的360VDS数据集。


<details>
  <summary>Details</summary>
Motivation: 360度视频在VR中广泛应用，但其空间分辨率有限，影响视觉质量。传统视频超分辨率方法无法处理360度视频的投影失真，且缺乏相关数据集。

Method: 本文提出了S3PO模型，采用循环建模和注意力机制，设计了针对球形失真的特征提取器和损失函数。

Result: S3PO在360度视频数据集上表现优于现有的传统和360度专用超分辨率模型。

Conclusion: S3PO成功解决了360度视频超分辨率的挑战，并通过消融研究验证了其各组件的重要性。

Abstract: Omnidirectional Videos (or 360{\deg} videos) are widely used in Virtual
Reality (VR) to facilitate immersive and interactive viewing experiences.
However, the limited spatial resolution in 360{\deg} videos does not allow for
each degree of view to be represented with adequate pixels, limiting the visual
quality offered in the immersive experience. Deep learning Video
Super-Resolution (VSR) techniques used for conventional videos could provide a
promising software-based solution; however, these techniques do not tackle the
distortion present in equirectangular projections of 360{\deg} video signals.
An additional obstacle is the limited availability of 360{\deg} video datasets
for study. To address these issues, this paper creates a novel 360{\deg} Video
Dataset (360VDS) with a study of the extensibility of conventional VSR models
to 360{\deg} videos. This paper further proposes a novel deep learning model
for 360{\deg} Video Super-Resolution (360{\deg} VSR), called Spherical Signal
Super-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent
modelling with an attention mechanism, unbound from conventional VSR techniques
like alignment. With a purpose-built feature extractor and a novel loss
function addressing spherical distortion, S3PO outperforms most
state-of-the-art conventional VSR models and 360{\deg}~specific
super-resolution models on 360{\deg} video datasets. A step-wise ablation study
is presented to understand and demonstrate the impact of the chosen
architectural sub-components, targeted training and optimisation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [16] [See What I Mean? CUE: A Cognitive Model of Understanding Explanations](https://arxiv.org/abs/2506.14775)
*Tobias Labarta,Nhi Hoang,Katharina Weitz,Wojciech Samek,Sebastian Lapuschkin,Leander Weber*

Main category: cs.HC

TL;DR: 论文提出了CUE模型，通过将解释属性与认知子过程（可读性、可理解性、可解释性）关联，评估可解释AI（XAI）的认知可访问性。研究发现，视觉障碍用户在理解热图解释时存在信心和努力降低的问题，即使使用无障碍颜色映射（如Cividis）也未能改善。


<details>
  <summary>Details</summary>
Motivation: 当前对可解释AI（XAI）的评估常忽略认知可访问性，尤其是对视觉障碍用户的适用性。论文旨在通过CUE模型填补这一空白，优化XAI的人类中心设计。

Method: 提出CUE模型，将解释属性（可读性、可理解性、可解释性）与认知子过程关联，并通过实验测试不同颜色映射（BWR、Cividis、Coolwarm）对视觉障碍用户的影响。

Result: 实验发现任务表现相近，但视觉障碍用户的信心和努力显著降低；无障碍颜色映射（如Cividis）未能改善这一差距，甚至有时加剧问题。

Conclusion: 研究挑战了感知优化的假设，支持自适应的XAI界面设计，并验证了CUE模型的有效性。

Abstract: As machine learning systems increasingly inform critical decisions, the need
for human-understandable explanations grows. Current evaluations of Explainable
AI (XAI) often prioritize technical fidelity over cognitive accessibility which
critically affects users, in particular those with visual impairments. We
propose CUE, a model for Cognitive Understanding of Explanations, linking
explanation properties to cognitive sub-processes: legibility (perception),
readability (comprehension), and interpretability (interpretation). In a study
(N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we
found comparable task performance but lower confidence/effort for visually
impaired users. Unlike expected, these gaps were not mitigated and sometimes
worsened by accessibility-focused color maps like Cividis. These results
challenge assumptions about perceptual optimization and support the need for
adaptive XAI interfaces. They also validate CUE by demonstrating that altering
explanation legibility affects understandability. We contribute: (1) a
formalized cognitive model for explanation understanding, (2) an integrated
definition of human-centered explanation properties, and (3) empirical evidence
motivating accessible, user-tailored XAI.

</details>


### [17] [WebXAII: an open-source web framework to study human-XAI interaction](https://arxiv.org/abs/2506.14777)
*Jules Leguy,Pierre-Antoine Jean,Felipe Torres Figueroa,Sébastien Harispe*

Main category: cs.HC

TL;DR: WebXAII是一个开源的web框架，旨在促进人类与可解释人工智能（XAI）系统交互的研究，解决了现有研究中实验接口不可重用和实验难以复现的问题。


<details>
  <summary>Details</summary>
Motivation: XAI领域迅速扩张，但研究人员通常开发临时接口进行研究，这些接口未共享，限制了实验的可重用性和复现性。

Method: 设计和实现WebXAII，一个基于web的平台，能够体现完整的实验协议，通过结构化配置文件定义灵活的实验架构。

Result: WebXAII成功复现了文献中的先进研究协议，证明了其有效性。

Conclusion: WebXAII为XAI研究提供了一个灵活、可重用的解决方案，支持实验的广泛复现和改进。

Abstract: This article introduces WebXAII, an open-source web framework designed to
facilitate research on human interaction with eXplainable Artificial
Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by
the growing societal implications of the widespread adoption of AI (and in
particular machine learning) across diverse applications. Researchers who study
the interaction between humans and XAI techniques typically develop ad hoc
interfaces in order to conduct their studies. These interfaces are usually not
shared alongside the results of the studies, which limits their reusability and
the reproducibility of experiments. In response, we design and implement
WebXAII, a web-based platform that can embody full experimental protocols,
meaning that it can present all aspects of the experiment to human participants
and record their responses. The experimental protocols are translated into a
composite architecture of generic views and modules, which offers a lot of
flexibility. The architecture is defined in a structured configuration file, so
that protocols can be implemented with minimal programming skills. We
demonstrate that WebXAII can effectively embody relevant protocols, by
reproducing the protocol of a state-of-the-art study of the literature. The
framework is available at https://github.com/PAJEAN/WebXAII.

</details>


### [18] [Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust](https://arxiv.org/abs/2506.14799)
*Evdoxia Taka,Debadyuti Bhattacharya,Joanne Garde-Hansen,Sanjay Sharma,Tanaya Guha*

Main category: cs.HC

TL;DR: 论文探讨了AI生成的角色代表性数据对公众的实用性和可信度，并开发了一个基于CLIP模型的可视化工具。


<details>
  <summary>Details</summary>
Motivation: 研究AI生成的性别和年龄角色代表性数据对公众的实用性和可信度。

Method: 使用CLIP模型分析视觉数据，设计可视化工具，并通过用户研究评估效果。

Result: 参与者认为工具整体有用，但对AI模型的信任度中低，希望可视化更详细。

Conclusion: AI生成的角色数据分析对公众有价值，但需提升透明度和可视化细节。

Abstract: Recent advances in AI has enabled automated analysis of complex media content
at scale and generate actionable insights regarding character representation
along such dimensions as gender and age. Past work focused on quantifying
representation from audio/video/text using various ML models, but without
having the audience in the loop. We ask, even if character distribution along
demographic dimensions are available, how useful are they to the general
public? Do they actually trust the numbers generated by AI models? Our work
addresses these questions through a user study, while proposing a new AI-based
character representation and visualization tool. Our tool based on the
Contrastive Language Image Pretraining (CLIP) foundation model to analyze
visual screen data to quantify character representation across dimensions of
age and gender. We also designed effective visualizations suitable for
presenting such analytics to lay audience. Next, we conducted a user study to
seek empirical evidence on the usefulness and trustworthiness of the
AI-generated results for carefully chosen movies presented in the form of our
visualizations. We note that participants were able to understand the analytics
from our visualization, and deemed the tool `overall useful'. Participants also
indicated a need for more detailed visualizations to include more demographic
categories and contextual information of the characters. Participants' trust in
AI-based gender and age models is seen to be moderate to low, although they
were not against the use of AI in this context. Our tool including code,
benchmarking, and data from the user study can be found here:
https://anonymous.4open.science/r/Character-Representation-Media-FF7B

</details>


### [19] [Impact of a Deployed LLM Survey Creation Tool through the IS Success Model](https://arxiv.org/abs/2506.14809)
*Peng Jiang,Vinicius Cezar Monteiro de Lira,Antonio Maiorino*

Main category: cs.HC

TL;DR: 本文探讨了利用大语言模型（LLM）自动化生成高质量调查问卷的系统，并结合DeLone和McLean的IS成功模型评估其效果。


<details>
  <summary>Details</summary>
Motivation: 传统的问卷设计需要大量专业知识和时间，LLM为自动化生成高质量问卷提供了新机会。

Method: 提出一个LLM驱动的系统，并结合IS成功模型进行评估，采用自动与人工结合的混合评估框架。

Result: 该系统能够加速数据收集并保持问卷质量，同时通过安全措施降低了部署风险。

Conclusion: 研究表明，生成式AI可以重塑IS核心方法，并可通过混合评估框架和保障措施实现负责任的应用。

Abstract: Surveys are a cornerstone of Information Systems (IS) research, yet creating
high-quality surveys remains labor-intensive, requiring both domain expertise
and methodological rigor. With the evolution of large language models (LLMs),
new opportunities emerge to automate survey generation. This paper presents the
real-world deployment of an LLM-powered system designed to accelerate data
collection while maintaining survey quality. Deploying such systems in
production introduces real-world complexity, including diverse user needs and
quality control. We evaluate the system using the DeLone and McLean IS Success
Model to understand how generative AI can reshape a core IS method. This study
makes three key contributions. To our knowledge, this is the first application
of the IS Success Model to a generative AI system for survey creation. In
addition, we propose a hybrid evaluation framework combining automated and
human assessments. Finally, we implement safeguards that mitigate
post-deployment risks and support responsible integration into IS workflows.

</details>


### [20] [Navigating High-Dimensional Backstage: A Guide for Exploring Literature for the Reliable Use of Dimensionality Reduction](https://arxiv.org/abs/2506.14820)
*Hyeon Jeon,Hyunwook Lee,Yun-Hsin Kuo,Taehyun Yang,Daniel Archambault,Sungahn Ko,Takanori Fujiwara,Kwan-Liu Ma,Jinwook Seo*

Main category: cs.HC

TL;DR: 该论文提出了一份指南，帮助新手法师和研究者在基于降维的可视化分析中选择和阅读文献，并通过专家访谈验证了指南的有效性。


<details>
  <summary>Details</summary>
Motivation: 针对降维可视化分析中的不可靠性问题，帮助初学者解决文献选择和理解上的困惑。

Method: 基于已有文献分类，设计了一份阅读指南，并通过专家访谈验证。

Result: 指南得到了专家认可，具有显著性、全面性和实用性。

Conclusion: 该指南为初学者提供了可靠的降维可视化分析学习路径。

Abstract: Visual analytics using dimensionality reduction (DR) can easily be unreliable
for various reasons, e.g., inherent distortions in representing the original
data. The literature has thus proposed a wide range of methodologies to make
DR-based visual analytics reliable. However, the diversity and extensiveness of
the literature can leave novice analysts and researchers uncertain about where
to begin and proceed. To address this problem, we propose a guide for reading
papers for reliable visual analytics with DR. Relying on the previous
classification of the relevant literature, our guide helps both practitioners
to (1) assess their current DR expertise and (2) identify papers that will
further enhance their understanding. Interview studies with three experts in DR
and data visualizations validate the significance, comprehensiveness, and
usefulness of our guide.

</details>


### [21] [The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities](https://arxiv.org/abs/2506.14829)
*Aditya Majumdar,Wenbo Zhang,Kashvi Prawal,Amulya Yadav*

Main category: cs.HC

TL;DR: 摘要讨论了AI4SI项目如何利用AI解决社会问题，但目前面临实际落地困难、合作挑战以及学术认可度低的困境，并通过采访和实践经验分析了障碍所在。


<details>
  <summary>Details</summary>
Motivation: 旨在通过识别AI4SI项目中阻碍实际影响的挑战，为研究者和合作组织提供实用指南。

Method: 通过半结构化访谈和作者自身经验进行主题分析。

Result: 发现结构性、组织性、沟通、协作和操作层面的挑战是主要障碍，并总结了应对策略。

Conclusion: 论文为AI4SI研究者与合作伙伴提供了应对挑战的实用参考。

Abstract: In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects
focus on harnessing AI to address societal issues in areas such as healthcare,
social justice, etc. Unfortunately, despite growing interest in AI4SI,
achieving tangible, on-the-ground impact remains a significant challenge. For
example, identifying and engaging motivated collaborators who are willing to
co-design and deploy AI based solutions in real-world settings is often
difficult. Even when such partnerships are established, many AI4SI projects
"fail" to progress beyond the proof-of-concept stage, and hence, are unable to
transition to at-scale production-level solutions. Furthermore, the unique
challenges faced by AI4SI researchers are not always fully recognized within
the broader AI community, where such work is sometimes viewed as primarily
applied and not aligning with the traditional criteria for novelty emphasized
in core AI venues. This paper attempts to shine a light on the diverse
challenges faced in AI4SI research by diagnosing a multitude of factors that
prevent AI4SI partnerships from achieving real-world impact on the ground.
Drawing on semi-structured interviews with six leading AI4SI researchers -
complemented by the authors' own lived experiences in conducting AI4SI research
- this paper attempts to understand the day-to-day difficulties faced in
developing and deploying socially impactful AI solutions. Through thematic
analysis, we identify structural and organizational, communication,
collaboration, and operational challenges as key barriers to deployment. While
there are no easy fixes, we synthesize best practices and actionable strategies
drawn from these interviews and our own work in this space. In doing so, we
hope this paper serves as a practical reference guide for AI4SI researchers and
partner organizations seeking to engage more effectively in socially impactful
AI collaborations.

</details>


### [22] [Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework](https://arxiv.org/abs/2506.14948)
*Mohna Chakraborty,Lu Wang,David Jurgens*

Main category: cs.HC

TL;DR: 论文提出了一种基于价值的方法来评估和提炼大型语言模型（LLM）中的结构化道德推理，发现显式道德结构和特定推理策略能显著提升模型的准确性和一致性，并通过蒸馏方法实现小型模型的道德能力提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在需要道德理解的领域中表现肤浅，且与人类推理不一致，亟需一种方法来解决这一差距。

Method: 使用基于价值系统的提示分类法，对12个开源模型在四个道德数据集上进行基准测试，并提出了监督蒸馏方法。

Result: 显式道德结构提示（如第一性原理推理和特定伦理框架）显著提升了模型的性能，且蒸馏方法成功将道德能力从大模型迁移到小模型。

Conclusion: 该方法为构建可解释且基于价值的模型提供了可扩展的路径。

Abstract: Large language models (LLMs) are increasingly deployed in domains requiring
moral understanding, yet their reasoning often remains shallow, and misaligned
with human reasoning. Unlike humans, whose moral reasoning integrates
contextual trade-offs, value systems, and ethical theories, LLMs often rely on
surface patterns, leading to biased decisions in morally and ethically complex
scenarios. To address this gap, we present a value-grounded framework for
evaluating and distilling structured moral reasoning in LLMs. We benchmark 12
open-source models across four moral datasets using a taxonomy of prompts
grounded in value systems, ethical theories, and cognitive reasoning
strategies. Our evaluation is guided by four questions: (1) Does reasoning
improve LLM decision-making over direct prompting? (2) Which types of
value/ethical frameworks most effectively guide LLM reasoning? (3) Which
cognitive reasoning strategies lead to better moral performance? (4) Can
small-sized LLMs acquire moral competence through distillation? We find that
prompting with explicit moral structure consistently improves accuracy and
coherence, with first-principles reasoning and Schwartz's + care-ethics
scaffolds yielding the strongest gains. Furthermore, our supervised
distillation approach transfers moral competence from large to small models
without additional inference cost. Together, our results offer a scalable path
toward interpretable and value-grounded models.

</details>


### [23] [Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output](https://arxiv.org/abs/2506.15008)
*Richa Gupta,Alexander Htet Kyaw*

Main category: cs.HC

TL;DR: 提出了一种结合DALL-E 3和材料数据集的管道，为AI生成的内饰设计添加可持续性指标和材料使用洞察，以帮助设计师评估环境影响。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI能快速生成视觉效果，但缺乏可操作的数据支持设计决策，尤其是在可持续性方面。

Method: 通过后处理模块识别AI生成图像中的材料，并与CO2e值配对，用户测试比较了不同提示方式对设计决策的影响。

Result: 引入可持续性指标（如CO2e数据）能引导更环保的设计决策，但可能引发决策疲劳并降低满意度。

Conclusion: 平衡设计自由与可持续性约束是关键，展示了AI辅助设计中数据驱动解决方案的潜力。

Abstract: Generative AI, specifically text-to-image models, have revolutionized
interior architectural design by enabling the rapid translation of conceptual
ideas into visual representations from simple text prompts. While generative AI
can produce visually appealing images they often lack actionable data for
designers In this work, we propose a novel pipeline that integrates DALL-E 3
with a materials dataset to enrich AI-generated designs with sustainability
metrics and material usage insights. After the model generates an interior
design image, a post-processing module identifies the top ten materials present
and pairs them with carbon dioxide equivalent (CO2e) values from a general
materials dictionary. This approach allows designers to immediately evaluate
environmental impacts and refine prompts accordingly. We evaluate the system
through three user tests: (1) no mention of sustainability to the user prior to
the prompting process with generative AI, (2) sustainability goals communicated
to the user before prompting, and (3) sustainability goals communicated along
with quantitative CO2e data included in the generative AI outputs. Our
qualitative and quantitative analyses reveal that the introduction of
sustainability metrics in the third test leads to more informed design
decisions, however, it can also trigger decision fatigue and lower overall
satisfaction. Nevertheless, the majority of participants reported incorporating
sustainability principles into their workflows in the third test, underscoring
the potential of integrated metrics to guide more ecologically responsible
practices. Our findings showcase the importance of balancing design freedom
with practical constraints, offering a clear path toward holistic, data-driven
solutions in AI-assisted architectural design.

</details>


### [24] [Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers](https://arxiv.org/abs/2506.15047)
*Jiayue Melissa Shi,Dong Whi Yoo,Keran Wang,Violeta J. Rodriguez,Ravi Karkar,Koustuv Saha*

Main category: cs.HC

TL;DR: 研究开发了基于GPT-4o的聊天机器人Carey，用于支持AD/ADRD家庭照顾者的心理健康，通过访谈揭示了六项需求和AI聊天机器人的优劣势，提出了设计建议。


<details>
  <summary>Details</summary>
Motivation: AD/ADRD家庭照顾者面临巨大的情感和实际挑战，而生成式AI（如LLMs）在心理健康支持方面的应用尚未充分探索。

Method: 开发Carey聊天机器人作为技术探针，对16名照顾者进行半结构化访谈，采用归纳编码和主题分析法。

Result: 揭示了照顾者的六大需求及其与AI聊天机器人的匹配与矛盾，并提出了设计建议。

Conclusion: 研究为设计更贴近照顾者需求的AI系统提供了理论和实践指导。

Abstract: Family caregivers of individuals with Alzheimer's Disease and Related
Dementia (AD/ADRD) face significant emotional and logistical challenges that
place them at heightened risk for stress, anxiety, and depression. Although
recent advances in generative AI -- particularly large language models (LLMs)
-- offer new opportunities to support mental health, little is known about how
caregivers perceive and engage with such technologies. To address this gap, we
developed Carey, a GPT-4o-based chatbot designed to provide informational and
emotional support to AD/ADRD caregivers. Using Carey as a technology probe, we
conducted semi-structured interviews with 16 family caregivers following
scenario-driven interactions grounded in common caregiving stressors. Through
inductive coding and reflexive thematic analysis, we surface a systemic
understanding of caregiver needs and expectations across six themes --
on-demand information access, emotional support, safe space for disclosure,
crisis management, personalization, and data privacy. For each of these themes,
we also identified the nuanced tensions in the caregivers' desires and
concerns. We present a mapping of caregiver needs, AI chatbot's strengths,
gaps, and design recommendations. Our findings offer theoretical and practical
insights to inform the design of proactive, trustworthy, and caregiver-centered
AI systems that better support the evolving mental health needs of AD/ADRD
caregivers.

</details>


### [25] [Data Verbalisation: What is Text Doing in a Data Visualisation?](https://arxiv.org/abs/2506.15129)
*Paul Murrell*

Main category: cs.HC

TL;DR: 文章提出一个框架，用于评估数据可视化中文本元素的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据可视化研究主要关注非文本元素（如柱状图、点和线），缺乏对文本元素的系统理解和评估方法。

Method: 通过分析现有知识和评估技术，探究文本在数据可视化中的使用案例，评估其有效性。

Result: 开发了一个简单易用的框架，帮助理解和应用文本元素的目的和有效性。

Conclusion: 该框架为数据可视化中的文本元素提供了系统化的评估方法，增强了可视化的整体效果。

Abstract: This article discusses the role that text elements play in a data
visualisation. We argue that there is a need for a simple, coherent explanation
of text elements similar to the understanding that already exists for non-text
elements like bars, points, and lines. We explore examples of how text is used
within a data visualisation and use existing knowledge and assessment
techniques to evaluate when text is effective and when it is not. The result is
a framework that aims to be easy to understand and easy to apply in order to
understand the purpose and effectiveness of the text elements in any data
visualisation.

</details>


### [26] [Accessible Gesture-Driven Augmented Reality Interaction System](https://arxiv.org/abs/2506.15189)
*Yikan Wang*

Main category: cs.HC

TL;DR: 提出了一种基于深度学习的AR手势交互系统，提升运动障碍用户的体验和效率。


<details>
  <summary>Details</summary>
Motivation: 现有AR系统依赖精确输入，对运动障碍用户不友好。

Method: 结合ViTs、TCNs和GATs处理手势，联邦学习保护隐私，强化学习优化界面。

Result: 任务效率提升20%，用户满意度提高25%。

Conclusion: 系统显著提升AR的可访问性和扩展性。

Abstract: Augmented reality (AR) offers immersive interaction but remains inaccessible
for users with motor impairments or limited dexterity due to reliance on
precise input methods. This study proposes a gesture-based interaction system
for AR environments, leveraging deep learning to recognize hand and body
gestures from wearable sensors and cameras, adapting interfaces to user
capabilities. The system employs vision transformers (ViTs), temporal
convolutional networks (TCNs), and graph attention networks (GATs) for gesture
processing, with federated learning ensuring privacy-preserving model training
across diverse users. Reinforcement learning optimizes interface elements like
menu layouts and interaction modes. Experiments demonstrate a 20% improvement
in task completion efficiency and a 25% increase in user satisfaction for
motor-impaired users compared to baseline AR systems. This approach enhances AR
accessibility and scalability. Keywords: Deep learning, Federated learning,
Gesture recognition, Augmented reality, Accessibility, Human-computer
interaction

</details>


### [27] [Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces](https://arxiv.org/abs/2506.15293)
*Francesco Chiossi,Julian Rasch,Robin Welsch,Albrecht Schmidt,Florian Michahelles*

Main category: cs.HC

TL;DR: 论文提出了一种多维度设计空间，用于指导人机协作中的意图通信设计，以提高透明度、信任和效率。


<details>
  <summary>Details</summary>
Motivation: 随着机器人进入协作工作空间，确保人与机器人之间的相互理解成为信任、安全和效率的前提。

Method: 基于SAT框架和任务抽象层次，提出了一个多维设计空间，涵盖意图内容、规划范围和通信模态。

Result: 提出了未来设计工具包的概念基础，支持透明的人机交互，并提出了共享研究议程。

Conclusion: 论文为混合工作环境中多模态、自适应和可信赖的机器人协作奠定了基础，并提出了开放问题和设计挑战。

Abstract: As robots enter collaborative workspaces, ensuring mutual understanding
between human workers and robotic systems becomes a prerequisite for trust,
safety, and efficiency. In this position paper, we draw on the cooperation
scenario of the AIMotive project in which a human and a cobot jointly perform
assembly tasks to argue for a structured approach to intent communication.
Building on the Situation Awareness-based Agent Transparency (SAT) framework
and the notion of task abstraction levels, we propose a multidimensional design
space that maps intent content (SAT1, SAT3), planning horizon (operational to
strategic), and modality (visual, auditory, haptic). We illustrate how this
space can guide the design of multimodal communication strategies tailored to
dynamic collaborative work contexts. With this paper, we lay the conceptual
foundation for a future design toolkit aimed at supporting transparent
human-robot interaction in the workplace. We highlight key open questions and
design challenges, and propose a shared agenda for multimodal, adaptive, and
trustworthy robotic collaboration in hybrid work environments.

</details>


### [28] [UXR Point of View on Product Feature Prioritization Prior To Multi-Million Engineering Commitments](https://arxiv.org/abs/2506.15294)
*Jonas Lau,Annie Tran*

Main category: cs.HC

TL;DR: 论文讨论了一种通过UXR PoV框架和MaxDiff方法进行功能优先排序的研究，解决了传统调查方法的不足，并通过案例展示了其在残疾人平板功能优先排序中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法在功能优先排序中存在不足，需要一种更可靠且高效的方法来生成偏好列表。

Method: 采用MaxDiff（多项逻辑回归）方法，并对其进行改进以减少调查响应数量。

Result: 改进后的MaxDiff方法能够以更少的样本生成可靠的偏好列表，并降低了参与者的疲劳感。

Conclusion: MaxDiff方法在功能优先排序中具有实用性，特别是在需要减少调查负担的情况下。

Abstract: This paper discusses a popular UX research activity, feature prioritization,
using the User Experience Research Point of View (UXR PoV) Playbook framework.
We describe an application of multinomial logistic regression, frequently
marketed as MaxDiff, for prioritizing product features in consumer product
development. It addresses challenges of traditional surveying techniques. We
propose a solution using MaxDiff to generate a reliable preference list with a
reasonable sample size. We also adapt the MaxDiff method to reduce the number
of survey responses in half, making it less tedious from the survey takers'
perspective. We present a case study using the adapted MaxDiff method for
tablet feature prioritization research involving users with disabilities.

</details>


### [29] [Case Study for Developing a UXR Point of View for FinOps Product Innovation](https://arxiv.org/abs/2506.15314)
*Jason Dong,Anna Wu*

Main category: cs.HC

TL;DR: 在云财务管理领域，通过定性定量混合研究方法，研究团队识别机会、量化痛点，并开发了一站式仪表盘，为FinOps实践者提供可操作的洞察。


<details>
  <summary>Details</summary>
Motivation: 探索如何在云财务管理中结合用户研究以推动FinOps产品创新。

Method: 采用多阶段定性定量混合研究方法，分析客户需求并协调跨职能团队。

Result: 开发了一站式仪表盘，为FinOps实践者提供可操作工具。

Conclusion: 混合研究方法能有效揭示推动产品创新的关键洞察。

Abstract: In the dynamic landscape of Cloud financial management, we are sharing a case
study exploring the development of a User Experience Research (UXR) Point of
View (PoV) to drive FinOps product innovation. We demonstrate how qualitative
and quantitative research methods working together to navigate the challenges
of understanding customer needs, aligning cross-functional teams, and
prioritizing limited resources. Through a multi-phased research approach, the
research team identifies opportunities, quantifies pain points, and segments
diverse customer cohorts. This culminated in a UXR PoV that informed the
creation of a differentiated product strategy, a 'one-stop shop' dashboard
empowering FinOps practitioners with actionable insights and tools. This case
study highlights the power of mixed-methods research in uncovering actionable
insights that drive impactful product innovation.

</details>


### [30] [Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook](https://arxiv.org/abs/2506.15325)
*Festus Adedoyin,Huseyin Dogan*

Main category: cs.HC

TL;DR: 该研究探讨了以人为中心的AI（HCAI）如何推动金融行业的个性化服务，包括数据分析、机器人咨询和风险管理的应用。


<details>
  <summary>Details</summary>
Motivation: 研究HCAI在金融行业中的应用，以提升用户体验和满意度。

Method: 通过当代研究和行业进展，分析HCAI在数据分析、机器学习和自然语言处理等领域的应用。

Result: HCAI显著提升了个性化金融服务的质量，增强了欺诈检测和风险管理。

Conclusion: 结合用户体验研究，金融行业可进一步利用HCAI技术满足用户需求。

Abstract: Advancements in Artificial Intelligence (AI) have significantly transformed
the financial industry, enabling the development of more personalised and
adaptable financial products and services. This research paper explores various
instances where Human-Centred AI (HCAI) has facilitated these advancements,
drawing from contemporary studies and industry progress. The paper examines how
the application of HCAI-powered data analytics, machine learning, and natural
language processing enables financial institutions to gain a deeper
understanding of their customers' unique needs, preferences, and behavioural
patterns. This, in turn, allows for the creation of tailored financial
solutions that address individual consumer requirements, ultimately enhancing
overall user experience and satisfaction. Additionally, the study highlights
the integration of AI-powered robo-advisory services, which offer customised
investment recommendations and portfolio management tailored to diverse risk
profiles and investment goals. Moreover, the paper underscores the role of AI
in strengthening fraud detection, risk assessment, and regulatory compliance,
leading to a more secure and adaptable financial landscape. The findings of
this research demonstrate the substantial impact of Human-Centred AI on the
financial industry, offering a strategic framework for financial institutions
to leverage these technologies. By incorporating a User Experience Research
(UXR) Point of View (PoV), financial institutions can ensure that AI-driven
solutions align with user needs and business objectives.

</details>


### [31] [Building Blocks of a User Experience Research Point of View](https://arxiv.org/abs/2506.15332)
*Patricia Diaz*

Main category: cs.HC

TL;DR: 本文介绍了三种基于数据、证据和洞察的用户体验研究视角（POV），展示其在企业环境中的应用与效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过创新的用户体验研究方法解决实际问题，并验证其可行性。

Method: 三种POV方法：1.智能视觉（2019年）；2.可评估代码编辑器（2023年）；3.机会图谱（2019年）。

Result: 尽管方法看似不切实际，但均被采纳并产生持久影响。

Conclusion: 创新的用户体验研究方法可以突破传统实践，实现显著成效。

Abstract: This paper presents three User Experience Research (UXR) perspectives based
on data, evidence and insights - known as Point of View (POV) - showcasing how
the strategies and methods of building a POV work in an enterprise setting. The
POV are: 1. Smart Visuals: Use AI to extract and translate text from visuals in
videos (2019). 2. Assessable Code Editor: Focus on direct AI-feedback to the
learner as it is the loop that requires the least effort for the highest
impact(2023). 3. Opportunity Landscape: Identify high-impact opportunities at
the intersection of emergent technical capabilities that unlock novel
approaches to critical user needs while addressing business strategic
priorities (2019). They all seemed far-fetched and went against common
practice. All were adopted and had long-lasting impact.

</details>


### [32] [Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI](https://arxiv.org/abs/2506.15468)
*Ryota Okumura,Tadahiro Taniguchi,Akira Taniguchi,Yoshinobu Hagiwara*

Main category: cs.HC

TL;DR: 论文提出了一种新的协同创造学习范式，通过人类与AI共同整合部分感知信息和知识来构建共享的外部表征。实验证明，基于MHNG的交互显著提升了分类准确性，并实现了更强的符号系统共享。


<details>
  <summary>Details</summary>
Motivation: 解决传统AI教学中单向知识传递的局限性，探索人类与AI在异构模态信息整合中的协同学习潜力。

Method: 采用基于Metropolis-Hastings命名游戏（MHNG）的人类-AI交互模型，通过在线实验比较三种AI代理类型的效果。

Result: 实验显示，基于MHNG的AI代理显著提高了分类准确性，并促进了共享符号系统的形成，人类行为与MHNG概率高度一致。

Conclusion: 研究为人类-AI协同创造学习提供了实证支持，为实现动态对齐感知经验的共生AI系统开辟了新途径。

Abstract: We propose co-creative learning as a novel paradigm where humans and AI,
i.e., biological and artificial agents, mutually integrate their partial
perceptual information and knowledge to construct shared external
representations, a process we interpret as symbol emergence. Unlike traditional
AI teaching based on unilateral knowledge transfer, this addresses the
challenge of integrating information from inherently different modalities. We
empirically test this framework using a human-AI interaction model based on the
Metropolis-Hastings naming game (MHNG), a decentralized Bayesian inference
mechanism. In an online experiment, 69 participants played a joint attention
naming game (JA-NG) with one of three computer agent types (MH-based,
always-accept, or always-reject) under partial observability. Results show that
human-AI pairs with an MH-based agent significantly improved categorization
accuracy through interaction and achieved stronger convergence toward a shared
sign system. Furthermore, human acceptance behavior aligned closely with the
MH-derived acceptance probability. These findings provide the first empirical
evidence for co-creative learning emerging in human-AI dyads via MHNG-based
interaction. This suggests a promising path toward symbiotic AI systems that
learn with humans, rather than from them, by dynamically aligning perceptual
experiences, opening a new venue for symbiotic AI alignment.

</details>


### [33] [Foundation of Affective Computing and Interaction](https://arxiv.org/abs/2506.15497)
*Changzeng Fu*

Main category: cs.HC

TL;DR: 本书全面探讨了情感计算与人机交互技术，涵盖历史发展、技术框架、多模态情感识别、脑机接口等领域，并讨论了技术挑战与未来趋势。


<details>
  <summary>Details</summary>
Motivation: 研究情感计算和人机交互技术的目的是为了推动这些技术在教育、医疗、娱乐等领域的应用，同时兼顾技术创新与伦理考量。

Method: 本书通过介绍情感计算的多模态识别、视觉交互、语音交互等技术框架，并结合心理学和神经科学基础，详细分析了相关技术。

Result: 本书展示了情感计算和人机交互技术的多样应用前景，并指出了数据融合、隐私安全等技术挑战。

Conclusion: 未来情感计算将与人工智能深度融合，技术发展需平衡创新与伦理，以实现负责任的应用。

Abstract: This book provides a comprehensive exploration of affective computing and
human-computer interaction technologies. It begins with the historical
development and basic concepts of human-computer interaction, delving into the
technical frameworks and practical applications of emotional computing, visual
interaction, voice interaction, brain-computer interfaces, physiological
electrical signal analysis, and social robotics. The book covers a wide range
of topics, including the psychological and neuroscience foundations of emotion,
multimodal emotion recognition, emotional expression mechanisms, and the
principles of brain-computer interfaces.
  Key technologies such as affective computing based on discrete emotion theory
and dimensional models, visual perception principles, speech recognition and
synthesis, EEG signal acquisition and processing, and multimodal emotion
recognition are explained in detail. This book also addresses the technical
challenges in the field, including multimodal data fusion, privacy and
security, and ethical considerations in human-machine relationships. It
discusses the applications of these technologies across various domains such as
education, healthcare, entertainment, and intelligent assistance.
  Looking to the future, the book anticipates trends such as the deep
integration of artificial intelligence with emotion recognition, the
advancement of multimodal interaction technologies, and the development of more
personalized and adaptive emotion recognition systems. It emphasizes the
importance of balancing technological innovation with ethical considerations to
ensure the responsible development and application of affective computing
technologies.

</details>


### [34] [Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach](https://arxiv.org/abs/2506.15512)
*Wenqi Guan,Yang Fang*

Main category: cs.HC

TL;DR: 本文提出了一种基于GPT和LangChain框架的新方法，通过CoT推理和提示工程提升远程学习资源的检索效果，提高了结果的精确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 当前远程学习资源检索缺乏对复杂查询的深度上下文理解，影响了学习效果。

Method: 集成GPT模型到LangChain框架，结合CoT推理和提示工程，优化检索系统。

Result: 相比传统LLMs，该方法显著提升了用户满意度和学习成果。

Conclusion: 提出的方法能更精准地满足学生需求，提升远程学习体验。

Abstract: Large Language Models have brought a radical change in the process of remote
learning students, among other aspects of educative activities. Current
retrieval of remote learning resources lacks depth in contextual meaning that
provides comprehensive information on complex student queries. This work
proposes a novel approach to enhancing remote learning retrieval by integrating
GPT-based models within the LangChain framework. We achieve this system in a
more intuitive and productive manner using CoT reasoning and prompt
engineering. The framework we propose puts much emphasis on increasing the
precision and relevance of the retrieval results to return comprehensive and
contextually enriched explanations and resources that best suit each student's
needs. We also assess the effectiveness of our approach against paradigmatic
LLMs and report improvements in user satisfaction and learning outcomes.

</details>


### [35] ["How can we learn and use AI at the same time?:: Participatory Design of GenAI with High School Students](https://arxiv.org/abs/2506.15525)
*Isabella Pu,Prerna Ravi,Linh Dieu Dinh,Chelsea Joe,Caitlin Ogoe,Zixuan Li,Cynthia Breazeal,Anastasia K. Ostrowski*

Main category: cs.HC

TL;DR: 高中生对生成式AI在教育中的看法及需求研究。


<details>
  <summary>Details</summary>
Motivation: 了解高中生对生成式AI（GenAI）的观点，以促进其在高中环境中的有效融合。

Method: 通过参与式设计工作坊，与17名高中生共同设计GenAI工具和学校政策。

Result: 学生提出了解决偏见、剽窃、过度依赖AI等问题的方案，并制定了针对教育技术设计者的新指南。

Conclusion: 呼吁在AI政策制定中更多地纳入学生声音。

Abstract: As generative AI (GenAI) emerges as a transformative force, clear
understanding of high school students' perspectives is essential for GenAI's
meaningful integration in high school environments. In this work, we draw
insights from a participatory design workshop where we engaged 17 high school
students -- a group rarely involved in prior research in this area -- through
the design of novel GenAI tools and school policies addressing their key
concerns. Students identified challenges and developed solutions outlining
their ideal features in GenAI tools, appropriate school use, and regulations.
These centered around the problem spaces of combating bias & misinformation,
tackling crime & plagiarism, preventing over-reliance on AI, and handling false
accusations of academic dishonesty. Building on our participants'
underrepresented perspectives, we propose new guidelines targeted at
educational technology designers for development of GenAI technologies in high
schools. We also argue for further incorporation of student voices in
development of AI policies in their schools.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [36] [You Only Render Once: Enhancing Energy and Computation Efficiency of Mobile Virtual Reality](https://arxiv.org/abs/2506.15183)
*Xingyu Chen,Xinmin Fang,Shuting Zhang,Xinyu Zhang,Liang He,Zhengxiong Li*

Main category: cs.GR

TL;DR: EffVR通过单目图像生成双目VR图像，节省50%计算量，平均降低27%功耗并提高帧率115.2%。


<details>
  <summary>Details</summary>
Motivation: 解决移动VR中双目图像分离渲染导致的性能和能耗瓶颈问题。

Method: 利用逐像素属性从单目图像生成双目VR图像，仅需一次渲染。

Result: 高效节能，图像质量高（SSIM 0.9679，PSNR 34.09）。

Conclusion: EffVR为可持续移动VR提供高效解决方案。

Abstract: Mobile Virtual Reality (VR) is essential to achieving convenient and
immersive human-computer interaction and realizing emerging applications such
as Metaverse. However, existing VR technologies require two separate renderings
of binocular images, causing a significant bottleneck for mobile devices with
limited computing capability and power supply. This paper proposes an approach
to rendering optimization for mobile VR called EffVR. By utilizing the
per-pixel attribute, EffVR can generate binocular VR images from the monocular
image through genuinely one rendering, saving half the computation over
conventional approaches. Our evaluation indicates that, compared with the
state-of-art, EffVRcan save 27% power consumption on average while achieving
high binocular image quality (0.9679 SSIM and 34.09 PSNR) in mobile VR
applications. Additionally, EffVR can increase the frame rate by 115.2%. These
results corroborate EffVRsuperior computation/energy-saving performance, paving
the road to a sustainable mobile VR. The source code, demo video, android app,
and more are released anonymously at https://yoro-vr.github.io/

</details>


### [37] [Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models](https://arxiv.org/abs/2506.15290)
*Andela Ilic,Jiaxi Jiang,Paul Streli,Xintong Liu,Christian Holz*

Main category: cs.GR

TL;DR: 论文提出了一种使用稀疏、松散穿戴的IMU传感器进行全身姿态估计的新方法，通过基于Transformer的扩散模型处理松散IMU数据，并结合衣物参数提升表现，实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有IMU姿态估计方法通常假设传感器紧贴身体，而实际应用中这一假设往往不成立。论文旨在解决松散穿戴IMU传感器的姿态估计问题。

Method: 从现有的衣物感知运动数据集中模拟松散IMU记录，并开发基于Transformer的扩散模型，以合成松散IMU数据并估计姿态。

Result: 实验表明，该方法在模拟和合成数据上训练后，定量和定性上均优于现有方法。

Conclusion: 该方法为松散IMU传感器的姿态估计提供了新方向，结合衣物参数进一步提升了模型表现。

Abstract: Motion capture using sparse inertial sensors has shown great promise due to
its portability and lack of occlusion issues compared to camera-based tracking.
Existing approaches typically assume that IMU sensors are tightly attached to
the human body. However, this assumption often does not hold in real-world
scenarios. In this paper, we present a new task of full-body human pose
estimation using sparse, loosely attached IMU sensors. To solve this task, we
simulate IMU recordings from an existing garment-aware human motion dataset. We
developed transformer-based diffusion models to synthesize loose IMU data and
estimate human poses based on this challenging loose IMU data. In addition, we
show that incorporating garment-related parameters while training the model on
simulated loose data effectively maintains expressiveness and enhances the
ability to capture variations introduced by looser or tighter garments.
Experiments show that our proposed diffusion methods trained on simulated and
synthetic data outperformed the state-of-the-art methods quantitatively and
qualitatively, opening up a promising direction for future research.

</details>


### [38] [One-shot Face Sketch Synthesis in the Wild via Generative Diffusion Prior and Instruction Tuning](https://arxiv.org/abs/2506.15312)
*Han Wu,Junyao Li,Kangbo Zhao,Sen Zhang,Yukai Shi,Liang Lin*

Main category: cs.GR

TL;DR: 提出了一种基于扩散模型的单次人脸素描合成方法，通过优化文本指令，实现高一致性的素描生成，并引入了新的基准数据集OS-Sketch。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖大规模训练数据和数据稀缺导致性能下降的问题。

Method: 使用扩散模型和基于梯度优化的文本指令，通过单次训练实现素描合成。

Result: 在单次训练下，能够生成高质量且一致的人脸素描，优于其他方法。

Conclusion: 该方法在数据稀缺情况下表现优异，具有更广泛的适用性和便利性。

Abstract: Face sketch synthesis is a technique aimed at converting face photos into
sketches. Existing face sketch synthesis research mainly relies on training
with numerous photo-sketch sample pairs from existing datasets. However, these
large-scale discriminative learning methods will have to face problems such as
data scarcity and high human labor costs. Once the training data becomes
scarce, their generative performance significantly degrades. In this paper, we
propose a one-shot face sketch synthesis method based on diffusion models. We
optimize text instructions on a diffusion model using face photo-sketch image
pairs. Then, the instructions derived through gradient-based optimization are
used for inference. To simulate real-world scenarios more accurately and
evaluate method effectiveness more comprehensively, we introduce a new
benchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark
consists of 400 pairs of face photo-sketch images, including sketches with
different styles and photos with different backgrounds, ages, sexes,
expressions, illumination, etc. For a solid out-of-distribution evaluation, we
select only one pair of images for training at each time, with the rest used
for inference. Extensive experiments demonstrate that the proposed method can
convert various photos into realistic and highly consistent sketches in a
one-shot context. Compared to other methods, our approach offers greater
convenience and broader applicability. The dataset will be available at:
https://github.com/HanWu3125/OS-Sketch

</details>


### [39] [Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards](https://arxiv.org/abs/2506.15684)
*Qingming Liu,Zhen Liu,Dinghuai Zhang,Kui Jia*

Main category: cs.GR

TL;DR: 提出了Nabla-R2D3，一种基于强化学习的3D扩散模型对齐框架，利用2D奖励信号实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成模型（如扩散模型）在生成质量、指令遵循和人类偏好对齐方面表现不足，需要更高效的优化方法。

Method: 基于Nabla-GFlowNet方法，提出Nabla-R2D3框架，通过2D奖励信号对3D扩散模型进行强化学习对齐。

Result: 实验表明，与基线方法相比，Nabla-R2D3在更少的优化步骤内获得更高奖励，并减少先验遗忘。

Conclusion: Nabla-R2D3是提升3D生成模型质量和效率的有效方法。

Abstract: Generating high-quality and photorealistic 3D assets remains a longstanding
challenge in 3D vision and computer graphics. Although state-of-the-art
generative models, such as diffusion models, have made significant progress in
3D generation, they often fall short of human-designed content due to limited
ability to follow instructions, align with human preferences, or produce
realistic textures, geometries, and physical attributes. In this paper, we
introduce Nabla-R2D3, a highly effective and sample-efficient reinforcement
learning alignment framework for 3D-native diffusion models using 2D rewards.
Built upon the recently proposed Nabla-GFlowNet method, which matches the score
function to reward gradients in a principled manner for reward finetuning, our
Nabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D
reward signals. Extensive experiments show that, unlike vanilla finetuning
baselines which either struggle to converge or suffer from reward hacking,
Nabla-R2D3 consistently achieves higher rewards and reduced prior forgetting
within a few finetuning steps.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [Efficient Serving of LLM Applications with Probabilistic Demand Modeling](https://arxiv.org/abs/2506.14851)
*Yifei Liu,Zuo Gan,Zhenghao Gan,Weiye Wang,Chen Chen,Yizhou Shan,Xusheng Chen,Zhenhua Han,Yifei Zhu,Shixuan Sun,Minyi Guo*

Main category: cs.DC

TL;DR: 本文提出了Hermes系统，通过概率需求图（PDGraph）模型优化LLM应用的资源调度和预热，显著降低了任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有系统将LLM应用的资源需求视为黑箱，导致效率低下。本文旨在解决这一问题。

Method: 提出PDGraph模型，并基于此开发Hermes系统，采用Gittins策略调度任务并结合预预热机制。

Result: 实验表明，Hermes能减少平均完成时间70%以上，P95完成时间80%以上。

Conclusion: Hermes为LLM应用提供了一种高效的服务框架，显著提升了资源利用和任务执行效率。

Abstract: Applications based on Large Language Models (LLMs) contains a series of tasks
to address real-world problems with boosted capability, which have dynamic
demand volumes on diverse backends. Existing serving systems treat the resource
demands of LLM applications as a blackbox, compromising end-to-end efficiency
due to improper queuing order and backend warm up latency. We find that the
resource demands of LLM applications can be modeled in a general and accurate
manner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which
leverages PDGraph for efficient serving of LLM applications. Confronting
probabilistic demand description, Hermes applies the Gittins policy to
determine the scheduling order that can minimize the average application
completion time. It also uses the PDGraph model to help prewarm cold backends
at proper moments. Experiments with diverse LLM applications confirm that
Hermes can effectively improve the application serving efficiency, reducing the
average completion time by over 70% and the P95 completion time by over 80%.

</details>


### [41] [Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching](https://arxiv.org/abs/2506.14852)
*Qizheng Zhang,Michael Wornow,Kunle Olukotun*

Main category: cs.DC

TL;DR: 提出了一种名为agentic plan caching的新方法，通过提取、存储、适配和重用结构化计划模板，显著降低了基于LLM的智能代理应用的成本。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM缓存技术（如上下文缓存和语义缓存）主要针对聊天机器人设计，无法满足依赖外部数据或环境上下文的智能代理应用需求，因此需要一种更高效的缓存方法。

Method: 从已完成的任务中提取结构化计划模板，通过关键词提取匹配新请求，并利用轻量级模型将模板适配到具体任务的上下文中。

Result: 在多个真实世界的智能代理应用中，该系统平均降低了46.62%的成本，同时保持了性能。

Conclusion: agentic plan caching为基于LLM的智能代理应用提供了一种高效且互补的缓存解决方案。

Abstract: LLM-based agentic applications have shown increasingly remarkable
capabilities in complex workflows but incur substantial costs due to extensive
planning and reasoning requirements. Existing LLM caching techniques (like
context caching and semantic caching), primarily designed for serving chatbots,
are insufficient for agentic applications where outputs depend on external data
or environmental contexts. We propose agentic plan caching, a novel approach
that extracts, stores, adapts, and reuses structured plan templates from
planning stages of agentic applications across semantically similar tasks to
reduce the cost of serving. Unlike traditional semantic caching, our system
extracts plan templates from completed agent executions at test-time, employs
keyword extraction to match new requests against cached plans, and utilizes
lightweight models to adapt these templates to task-specific plans with
contexts. Evaluation across multiple real-world agentic applications shows that
our system can reduce costs by 46.62% on average while maintaining performance,
offering a more efficient solution for serving LLM-based agents that
complements existing LLM serving infrastructures.

</details>


### [42] [Zarr-Based Chunk-Level Cumulative Sums in Reduced Dimensions](https://arxiv.org/abs/2506.14981)
*Hailiang Zhang,Dieu My T. Nguyen,Christine Smit,Mahabal Hegde*

Main category: cs.DC

TL;DR: 提出了一种通过生成小型补充数据集存储累积和的方法，显著提升大规模多维数据分析的效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多维数据全扫描计算成本高的问题。

Method: 生成存储特定维度累积和的小型补充数据集。

Result: 在AWS上实现，性能比暴力方法快3-4个数量级，存储增加仅5%。

Conclusion: 该方法高效且成本低，特别适合云环境中的数据分析。

Abstract: Data analysis on massive multi-dimensional data, such as high-resolution
large-region time averaging or area averaging for geospatial data, often
involves calculations over a significant number of data points. While
performing calculations in scalable and flexible distributed or cloud
environments is a viable option, a full scan of large data volumes still serves
as a computationally intensive bottleneck, leading to significant cost. This
paper introduces a generic and comprehensive method to address these
computational challenges. This method generates a small, size-tunable
supplementary dataset that stores the cumulative sums along specific subset
dimensions on top of the raw data. This minor addition unlocks rapid and cheap
high-resolution large-region data analysis, making calculations over large
numbers of data points feasible with small instances or even microservices in
the cloud. This method is general-purpose, but is particularly well-suited for
data stored in chunked, cloud-optimized formats and for services running in
distributed or cloud environments. We present a Zarr extension proposal to
integrate the specifications of this method and facilitate its straightforward
implementation in general-purpose software applications. Benchmark tests
demonstrate that this method, implemented in Amazon Web services (AWS),
significantly outperforms the brute-force approach used in on-premises
services. With just 5% supplemental storage, this method achieves a performance
that is 3-4 orders of magnitude (~10,000 times) faster than the brute-force
approach, while incurring significantly reduced computational costs.

</details>


### [43] [Parallel Data Object Creation: Towards Scalable Metadata Management in High-Performance I/O Library](https://arxiv.org/abs/2506.15114)
*Youjia Li,Robert Latham,Robert Ross,Ankit Agrawal,Alok Choudhary,Wei-Keng Liao*

Main category: cs.DC

TL;DR: 论文提出了一种新的文件头部格式，支持并行I/O库中独立创建数据对象，显著提升了性能和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有并行I/O库如HDF5和PnetCDF在集体创建数据对象时效率低且内存占用高，无法满足需要独立创建大量数据对象的需求。

Method: 以PnetCDF为实验平台，提出了新的文件头部格式，包含索引表和元数据块列表，支持独立数据对象创建。

Result: 新设计将数据对象创建时间减少了582倍（4096个MPI进程创建5,684,800个对象），并显著降低了内存占用。

Conclusion: 新方法解决了现有并行I/O库的局限性，显著提升了性能和可扩展性。

Abstract: High-level I/O libraries, such as HDF5 and PnetCDF, are commonly used by
large-scale scientific applications to perform I/O tasks in parallel. These I/O
libraries store the metadata such as data types and dimensionality along with
the raw data in the same files. While these libraries are well-optimized for
concurrent access to the raw data, they are designed neither to handle a large
number of data objects efficiently nor to create different data objects
independently by multiple processes, as they require applications to call data
object creation APIs collectively with consistent metadata among all processes.
Applications that process data gathered from remote sensors, such as particle
collision experiments in high-energy physics, may generate data of different
sizes from different sensors and desire to store them as separate data objects.
For such applications, the I/O library's requirement on collective data object
creation can become very expensive, as the cost of metadata consistency check
increases with the metadata volume as well as the number of processes. To
address this limitation, using PnetCDF as an experimental platform, we
investigate solutions in this paper that abide the netCDF file format, as well
as propose a new file header format that enables independent data object
creation. The proposed file header consists of two sections, an index table and
a list of metadata blocks. The index table contains the reference to the
metadata blocks and each block stores metadata of objects that can be created
collectively or independently. The new design achieves a scalable performance,
cutting data object creation times by up to 582x when running on 4096 MPI
processes to create 5,684,800 data objects in parallel. Additionally, the new
method reduces the memory footprints, with each process requiring an amount of
memory space inversely proportional to the number of processes.

</details>


### [44] [eLLM: Elastic Memory Management Framework for Efficient LLM Serving](https://arxiv.org/abs/2506.15155)
*Jiale Xu,Rui Zhang,Yi Xiong,Cong Guo,Zihan Liu,Yangjie Zhou,Weiming Hu,Hao Wu,Changxu Shao,Ziqing Wang,Yongjie Yuan,Junping Zhao,Minyi Guo,Jingwen Leng*

Main category: cs.DC

TL;DR: eLLM提出了一种弹性内存管理框架，通过虚拟张量抽象和动态内存调整，解决了大语言模型中动态内存管理的挑战，显著提升了吞吐量和批处理规模。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数据中心部署时，动态激活和KV缓存的内存管理效率低下，导致吞吐量下降，eLLM旨在优化这一问题。

Method: eLLM采用虚拟张量抽象、弹性内存机制和轻量级调度策略，动态管理内存并平衡性能。

Result: eLLM的吞吐量提升了2.32倍，支持128K-token输入的批处理规模扩大了3倍。

Conclusion: eLLM通过统一和弹性的内存管理，显著提升了LLM的运行时效率和性能。

Abstract: Large Language Models are increasingly being deployed in datacenters. Serving
these models requires careful memory management, as their memory usage includes
static weights, dynamic activations, and key-value caches. While static weights
are constant and predictable, dynamic components such as activations and KV
caches change frequently during runtime, presenting significant challenges for
efficient memory management. Modern LLM serving systems typically handle
runtime memory and KV caches at distinct abstraction levels: runtime memory
management relies on static tensor abstractions, whereas KV caches utilize a
page table-based virtualization layer built on top of the tensor abstraction.
This virtualization dynamically manages KV caches to mitigate memory
fragmentation. However, this dual-level approach fundamentally isolates runtime
memory and KV cache management, resulting in suboptimal memory utilization
under dynamic workloads, which can lead to a nearly 20% drop in throughput.
  To address these limitations, we propose eLLM, an elastic memory management
framework inspired by the classical memory ballooning mechanism in operating
systems. The core components of eLLM include: (1) Virtual Tensor Abstraction,
which decouples the virtual address space of tensors from the physical GPU
memory, creating a unified and flexible memory pool; (2) an Elastic Memory
Mechanism that dynamically adjusts memory allocation through runtime memory
inflation and deflation, leveraging CPU memory as an extensible buffer; and (3)
a Lightweight Scheduling Strategy employing SLO-aware policies to optimize
memory utilization and effectively balance performance trade-offs under
stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM
significantly outperforms state-of-the-art systems, 2.32x higher decoding
throughput, and supporting 3x larger batch sizes for 128K-token inputs.

</details>


### [45] [RISC-V for HPC: An update of where we are and main action points](https://arxiv.org/abs/2506.15418)
*Nick Brown*

Main category: cs.DC

TL;DR: RISC-V HPC SIG分析了RISC-V生态系统在HPC领域的现状和局限性，提出了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管RISC-V在HPC领域取得了进展，但仍需关注其局限性以推动进一步改进。

Method: 通过分析当前生态系统，识别关键问题和未来努力方向。

Result: 明确了RISC-V在HPC中的进展和待解决的瓶颈。

Conclusion: 需要集中精力解决RISC-V生态系统的局限性以支持HPC应用。

Abstract: This extended abstract is submitted on behalf of the RISC-V HPC SIG who have
been undertaking an analysis to explore the current state and limitations of
the RISC-V ecosystem for HPC. Whilst it is right to celebrate that there has
been great progress made in recent years, we also highlight limitations and
where effort should be focussed.

</details>


### [46] [Exploring Fast Fourier Transforms on the Tenstorrent Wormhole](https://arxiv.org/abs/2506.15437)
*Nick Brown,Jake Davies,Felix LeClair*

Main category: cs.DC

TL;DR: 研究探讨了将Cooley-Tukey FFT算法移植到Tenstorrent Wormhole RISC-V加速器上的效果，尽管性能不如高端CPU，但能耗显著降低。


<details>
  <summary>Details</summary>
Motivation: 利用RISC-V加速器的开放性和专业化优势，为HPC领域提供高效能低功耗的解决方案。

Method: 通过优化数据移动技术，将FFT算法移植到Tenstorrent Wormhole RISC-V加速器上进行性能测试。

Result: Wormhole n300在功耗和能耗上分别比24核Xeon Platinum低8倍和2.8倍，但性能较慢。

Conclusion: RISC-V加速器在HPC中具有潜在优势，尤其在能效比方面表现突出。

Abstract: Whilst numerous areas of computing have adopted the RISC-V Instruction Set
Architecture (ISA) wholesale in recent years, it is yet to become widespread in
HPC. RISC-V accelerators offer a compelling option where the HPC community can
benefit from the specialisation offered by the open nature of the standard but
without the extensive ecosystem changes required when adopting RISC-V CPUs. In
this paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT)
algorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon
Tenstorrent's Tensix architecture, this technology decouples the movement of
data from compute, potentially offering increased control to the programmer.
Exploring different optimisation techniques to address the bottlenecks inherent
in data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is
slower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around
8 times less power and consumes around 2.8 times less energy than the CPU when
computing the Fourier transform.

</details>


### [47] [Parallel Paradigms in Modern HPC: A Comparative Analysis of MPI, OpenMP, and CUDA](https://arxiv.org/abs/2506.15454)
*Nizar ALHafez,Ahmad Kurdi*

Main category: cs.DC

TL;DR: 本文对高性能计算中的三种主要并行编程模型（MPI、OpenMP和CUDA）进行了全面比较，分析了它们的优势、劣势及适用场景，并指出混合模型在异构环境中的优势。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算架构的异构化，选择合适的并行编程模型变得至关重要。本文旨在帮助开发者和研究人员基于应用需求和硬件限制做出更明智的选择。

Method: 通过对MPI、OpenMP和CUDA在架构基础、性能特点、领域适用性、编程复杂性和最新进展方面的系统性分析，结合科学模拟、机器学习和数据分析的性能评估。

Result: MPI适用于通信密集型分布式内存环境，但面临通信开销；OpenMP在共享内存系统及循环任务中表现优异，但受限于共享内存争用；CUDA在数据并行GPU任务中性能显著，但仅适用于NVIDIA GPU且需专业知识。混合模型在异构环境中表现最佳。

Conclusion: 选择最优编程模型需综合考虑应用需求、硬件限制和开发约束。混合模型和新兴技术（如性能可移植框架）为未来HPC应用提供了更多可能性。

Abstract: This paper presents a comprehensive comparison of three dominant parallel
programming models in High Performance Computing (HPC): Message Passing
Interface (MPI), Open Multi-Processing (OpenMP), and Compute Unified Device
Architecture (CUDA). Selecting optimal programming approaches for modern
heterogeneous HPC architectures has become increasingly critical. We
systematically analyze these models across multiple dimensions: architectural
foundations, performance characteristics, domain-specific suitability,
programming complexity, and recent advancements. We examine each model's
strengths, weaknesses, and optimization techniques. Our investigation
demonstrates that MPI excels in distributed memory environments with
near-linear scalability for communication-intensive applications, but faces
communication overhead challenges. OpenMP provides strong performance and
usability in shared-memory systems and loop-centric tasks, though it is limited
by shared memory contention. CUDA offers substantial performance gains for
data-parallel GPU workloads, but is restricted to NVIDIA GPUs and requires
specialized expertise. Performance evaluations across scientific simulations,
machine learning, and data analytics reveal that hybrid approaches combining
two or more models often yield optimal results in heterogeneous environments.
The paper also discusses implementation challenges, optimization best
practices, and emerging trends such as performance portability frameworks,
task-based programming, and the convergence of HPC and Big Data. This research
helps developers and researchers make informed decisions when selecting
programming models for modern HPC applications, emphasizing that the best
choice depends on application requirements, hardware, and development
constraints.

</details>


### [48] [All is Not Lost: LLM Recovery without Checkpoints](https://arxiv.org/abs/2506.15461)
*Nikolay Blagoev,Oğuzhan Ersoy,Lydia Yiyu Chen*

Main category: cs.DC

TL;DR: CheckFree 是一种高效的恢复方法，通过加权平均邻近阶段来替代失败阶段，无需额外计算或存储。扩展版 CheckFree+ 支持无序流水线执行，可容忍首尾阶段崩溃，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在去中心化和低性能计算节点上训练大型语言模型（LLMs）可降低成本并促进模型民主化，但节点故障和调度策略导致阶段丢失，传统恢复方法效率低。

Method: CheckFree 通过加权平均邻近阶段替代失败阶段；CheckFree+ 扩展了无序流水线执行，通过复制权重和邻近阶段模仿行为来恢复首尾阶段。

Result: 在低至中等故障率（5-10%）下，CheckFree 和 CheckFree+ 在收敛时间上优于检查点和冗余计算，提升超过 12%。

Conclusion: CheckFree 和 CheckFree+ 提供了高效的故障恢复方法，显著降低了通信和计算开销，适用于大规模模型训练。

Abstract: Training LLMs on decentralized and wimpy computation nodes, e.g., multiple
on-spot instances, lowers the training cost and enables model democratization.
The inevitable challenge here is the churn of nodes due to failures and the
operator's scheduling policies, leading to losing a stage - a part of the
model. The conventional approaches to recover from failures are to either use
checkpointing, where periodically a copy of the entire model is sent to an
additional storage, or redundant computation. These approaches yield
significant communication and/or computation overhead even in non-failure cases
and scale poorly in settings with large models. In this paper, we propose,
CheckFree, an efficient recovery method where a failing stage is substituted by
a weighted average of the closest neighboring stages. In contrast to the state
of the art, CheckFree requires no additional computation or storage. However,
because of the nature of averaging neighbouring stages, it can only recover
failures of intermediate stages. We further extend our method to CheckFree+
with out-of-order pipeline execution to tolerate crashes of the first and last
stages. Thanks to out-of-order pipelining, behaviour of those stages is
mimicked by their neighboring ones, which allows CheckFree+ to recover them by
simply copying the weights from the immediate neighbour. To be able to recover
the (de)embedding layers, CheckFree+ copies those layers to the neighboring
stages, which requires relatively small storage overhead. We extensively
evaluate our method on LLaMa models of model sizes from 124M to 1.5B with
varying failure frequencies. In the case of low and medium failure rates
(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant
computation in terms of convergence in wall-clock time by over 12%. Both of our
proposals can be run via our code available at:
https://github.com/gensyn-ai/CheckFree.

</details>


### [49] [Minimizing Communication for Parallel Symmetric Tensor Times Same Vector Computation](https://arxiv.org/abs/2506.15488)
*Hussam Al Daas,Grey Ballard,Laura Grigori,Suraj Kumar,Kathryn Rouse,Mathieu Vérité*

Main category: cs.DC

TL;DR: 本文分析了在三维对称张量上并行计算向量乘法的通信开销，提出了紧致的通信下限，并展示了最优算法。


<details>
  <summary>Details</summary>
Motivation: 研究目的是解决在对称张量计算中的并行通信开销问题，这对高阶幂法和对称CP分解的计算效率至关重要。

Method: 通过扩展三维对称计算的关键几何不等式，建立了通信下限，并提出了一种基于对称矩阵三角形块分区方案的最优算法。

Result: 证明了通信下限的紧致性，并通过数据分布的最优算法实现了高效并行计算。

Conclusion: 本文为对称张量计算中的通信问题提供了理论保证和实际解决方案。

Abstract: In this article, we focus on the parallel communication cost of multiplying
the same vector along two modes of a $3$-dimensional symmetric tensor. This is
a key computation in the higher-order power method for determining eigenpairs
of a $3$-dimensional symmetric tensor and in gradient-based methods for
computing a symmetric CP decomposition. We establish communication lower bounds
that determine how much data movement is required to perform the specified
computation in parallel. The core idea of the proof relies on extending a key
geometric inequality for $3$-dimensional symmetric computations. We demonstrate
that the communication lower bounds are tight by presenting an optimal
algorithm where the data distribution is a natural extension of the triangle
block partition scheme for symmetric matrices to 3-dimensional symmetric
tensors.

</details>


### [50] [Automatic Metadata Capture and Processing for High-Performance Workflows](https://arxiv.org/abs/2506.15537)
*Polina Shpilker,Line Pouchard*

Main category: cs.DC

TL;DR: 论文提出了一种方法，通过开发软件收集HPC系统上运行的工作流元数据注释，并实验了两种存储格式，以优化元数据的使用便捷性。


<details>
  <summary>Details</summary>
Motivation: 现代工作流运行在越来越异构的计算架构上，这种异构性带来了复杂性。研究旨在通过应用FAIR原则（可查找、可访问、可互操作、可重用）来提高研究可重复性。

Method: 开发软件以收集HPC系统上工作流的元数据注释，并实验两种统一的存储格式，同时重新组织元数据以便于研究者分析工作流性能。

Result: 提出了一种优化的元数据组织方式，使其更易于研究者使用。

Conclusion: 通过FAIR原则和元数据优化，提升了异构计算环境下工作流研究的可重复性和性能分析的便捷性。

Abstract: Modern workflows run on increasingly heterogeneous computing architectures
and with this heterogeneity comes additional complexity. We aim to apply the
FAIR principles for research reproducibility by developing software to collect
metadata annotations for workflows run on HPC systems. We experiment with two
possible formats to uniformly store these metadata, and reorganize the
collected metadata to be as easy to use as possible for researchers studying
their workflow performance.

</details>


### [51] [LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale Heterogeneous Clusters](https://arxiv.org/abs/2506.15595)
*Kunming Zhang,Hanlong Liao,Guoming Tang*

Main category: cs.DC

TL;DR: LiteGD是一种轻量级动态GPU调度系统，通过全局视角优化多GPU并行计算中的通信延迟问题。


<details>
  <summary>Details</summary>
Motivation: 大型异构GPU集群中，传统基于物理邻近性的GPU分配方法存在局限性，LiteGD旨在解决这一问题。

Method: 采用计算感知设计，利用轻量级Transformer网络处理GPU拓扑信息，并结合双向树搜索优化调度。

Result: LiteGD在多种集群配置中实现高带宽效率（约90%），在真实H100集群中达到80%，显著优于传统方法。

Conclusion: LiteGD在大型异构环境中表现优异，为多GPU并行计算提供高效调度方案。

Abstract: Parallel computing with multiple GPUs has become the dominant paradigm for
machine learning tasks, especially those of large language models (LLMs). To
reduce the latency incurred by inter-GPU communication, a common practice for
parallel tasks has been to allocate GPUs based on their physical proximity.
However, this long-standing assumption has notable limitations, particularly in
large-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs
is irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU
dispatching system based on global perspectives. To tackle the difficulty of
storing massive GPU topology information, LiteGD adopts a computation-aware
design that leverages a lightweight Transformer network trained on sampled
data. Our customized design for network structure ensures both transferability
and scalability. LiteGD also employs a bidirectional tree search approach to
find the optimal GPU dispatching in the data generated in the previous step,
which can identify near-optimal solutions while reducing search overhead. We
implement and evaluate LiteGD in both real and simulated GPU clusters with
homogeneous and heterogeneous interconnects, respectively. Experimental results
demonstrate that LiteGD consistently achieves high GPU bandwidth efficacy
(approximately 90\%) across various cluster configurations and 80\% in
real-world H100 cluster, significantly outperforming conventional default and
interconnect topology-aware dispatching methods, particularly in large-scale
heterogeneous environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [52] [SimBank: from Simulation to Solution in Prescriptive Process Monitoring](https://arxiv.org/abs/2506.14772)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.DB

TL;DR: 该论文提出了SimBank模拟器，用于精确比较和评估Prescriptive Process Monitoring (PresPM)方法，填补了现有文献中技术比较和评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前PresPM研究缺乏技术间的广泛比较和有效的评估方法，SimBank旨在解决这些问题。

Method: 通过模拟银行的贷款申请流程，SimBank支持在线和离线PresPM方法的比较，涵盖不同复杂度的干预优化问题，并评估方法的鲁棒性等。

Result: SimBank能够生成真实干预结果，支持对PresPM方法的全面评估，并通过实验验证其有效性。

Conclusion: SimBank为PresPM研究和实践提供了公开可用的模拟平台，具有重要意义。

Abstract: Prescriptive Process Monitoring (PresPM) is an emerging area within Process
Mining, focused on optimizing processes through real-time interventions for
effective decision-making. PresPM holds significant promise for organizations
seeking enhanced operational performance. However, the current literature faces
two key limitations: a lack of extensive comparisons between techniques and
insufficient evaluation approaches. To address these gaps, we introduce
SimBank: a simulator designed for accurate benchmarking of PresPM methods.
Modeled after a bank's loan application process, SimBank enables extensive
comparisons of both online and offline PresPM methods. It incorporates a
variety of intervention optimization problems with differing levels of
complexity and supports experiments on key causal machine learning challenges,
such as assessing a method's robustness to confounding in data. SimBank
additionally offers a comprehensive evaluation capability: for each test case,
it can generate the true outcome under each intervention action, which is not
possible using recorded datasets. The simulator incorporates parallel
activities and loops, drawing from common logs to generate cases that closely
resemble real-life process instances. Our proof of concept demonstrates
SimBank's benchmarking capabilities through experiments with various PresPM
methods across different interventions, highlighting its value as a publicly
available simulator for advancing research and practice in PresPM.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [53] [Scaling Intelligence: Designing Data Centers for Next-Gen Language Models](https://arxiv.org/abs/2506.15006)
*Jesmin Jahan Tithi,Hanjiang Wu,Avishaii Abuhatzera,Fabrizio Petrini*

Main category: cs.AR

TL;DR: 该研究提出了一种全面的联合设计框架，用于优化大规模语言模型（LLM）在数据中心架构中的性能与效率，重点评估了FullFlat网络架构的优势及其对吞吐量和模型利用率的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（如GPT-4）参数规模的爆炸性增长（高达1.8万亿），传统数据中心架构面临可扩展性、效率和成本效益的挑战。

Method: 研究通过联合探索FLOPS、HBM带宽与容量、多种网络拓扑（如FullFlat光网络）、扩展域大小以及并行化/优化策略，提出了一种性能建模工具，并进行详细的敏感性分析。

Result: FullFlat网络架构显著提升了LLM的性能和可扩展性，同时量化了计算与通信重叠、硬件加速集体操作、更大扩展域和内存容量的优势。

Conclusion: 研究为高效支持万亿参数模型的数据中心设计提供了实用路线图，降低了优化复杂度，推动了AI能力的持续发展。

Abstract: The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8
trillion parameters - demands a radical rethinking of data center architecture
to ensure scalability, efficiency, and cost-effectiveness. Our work provides a
comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth
and capacity, multiple network topologies (two-tier vs. FullFlat optical), the
size of the scale-out domain, and popular parallelism/optimization strategies
used in LLMs. We introduce and evaluate FullFlat network architectures, which
provide uniform high-bandwidth, low-latency connectivity between all nodes, and
demonstrate their transformative impact on performance and scalability. Through
detailed sensitivity analyses, we quantify the benefits of overlapping compute
and communication, leveraging hardware-accelerated collectives, wider scale-out
domains, and larger memory capacity. Our study spans both sparse (mixture of
experts) and dense transformer-based LLMs, revealing how system design choices
affect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens
per sec / Peak flops of the hardware) and overall throughput. For the co-design
study, we extended and validated a performance modeling tool capable of
predicting LLM runtime within 10% of real-world measurements. Our findings
offer actionable insights and a practical roadmap for designing AI data centers
that can efficiently support trillion-parameter models, reduce optimization
complexity, and sustain the rapid evolution of AI capabilities.

</details>


### [54] [ChatModel: Automating Reference Model Design and Verification with LLMs](https://arxiv.org/abs/2506.15066)
*Jianmin Ye,Tianyang Liu,Qi Tian,Shengchu Su,Zhe Jiang,Xi Wang*

Main category: cs.AR

TL;DR: ChatModel是一个基于大型语言模型的参考模型生成与验证平台，通过设计标准化和分层敏捷建模，显著提高了参考模型的生成效率和验证能力。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路设计的复杂度增加，功能性验证变得更具挑战性，参考模型的开发也日益复杂和耗时。尽管大型语言模型在编程中展现出潜力，但生成复杂参考模型仍存在困难。

Method: ChatModel采用设计标准化和分层敏捷建模，结合模块化生成策略，优化了参考模型的生成过程。

Result: 在300个不同复杂度的设计上测试，ChatModel的峰值性能提升55.02%，生成稳定性显著提高，参考模型设计能力提升9.18倍，迭代过程加速5.90倍。

Conclusion: ChatModel能够显著推进参考模型生成和验证的自动化，展现了在集成电路设计验证中的巨大潜力。

Abstract: As the complexity of integrated circuit designs continues to escalate, the
functional verification becomes increasingly challenging. Reference models,
critical for accelerating the verification process, are themselves becoming
more intricate and time-consuming to develop. Despite the promise shown by
large language models (LLMs) in code programming, effectively generating
complex reference models remains a significant hurdle. To address these
challenges, we introduce ChatModel, the first LLM-aided agile reference model
generation and verification platform. ChatModel streamlines the transition from
design specifications to fully functional reference models by integrating
design standardization and hierarchical agile modeling. Employing a
building-block generation strategy, it not only enhances the design
capabilities of LLMs for reference models but also significantly boosts
verification efficiency. We evaluated ChatModel on 300 designs of varying
complexity, demonstrating substantial improvements in both efficiency and
quality of reference model generation. ChatModel achieved a peak performance
improvement of 55.02% compared to alternative methods, with notable
enhancements in generation stability, and delivered a 9.18x increase in its
capacity to produce reference model designs. Furthermore, it accelerated the
iterative process of reference model design and validation by an average of
5.90x compared to traditional approaches. These results highlight the potential
of ChatModel to significantly advance the automation of reference model
generation and validation.

</details>


### [55] [J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor](https://arxiv.org/abs/2506.15316)
*Benoit Tain,Raphael Millet,Romain Lemaire,Michal Szczepanski,Laurent Alacoque,Emmanuel Pluchart,Sylvain Choisnet,Rohit Prasad,Jerome Chossat,Pascal Pierunek,Pascal Vivet,Sebastien Thuries*

Main category: cs.AR

TL;DR: 本文介绍了J3DAI，一种基于深度神经网络的硬件加速器，用于3D堆叠CMOS图像传感器，具备AI芯片集成功能，专注于边缘AI的高效处理。


<details>
  <summary>Details</summary>
Motivation: 随着边缘AI的重要性日益增长，需要高效、实时的AI处理解决方案。J3DAI旨在满足资源受限设备上的低延迟、高能效需求。

Method: 采用Aidge软件框架支持硬件开发，包括后训练量化以减少内存占用和计算复杂度。专注于数字系统设计及其PPA特性。

Result: 实验证明了J3DAI在边缘AI中的多功能性和高效性，能够处理简单和计算密集型任务。

Conclusion: J3DAI在边缘AI领域展现出潜力，未来工作将优化架构并探索新应用。

Abstract: This paper presents J3DAI, a tiny deep neural network-based hardware
accelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial
intelligence (AI) chip integrating a Deep Neural Network (DNN)-based
accelerator. The DNN accelerator is designed to efficiently perform neural
network tasks such as image classification and segmentation. This paper focuses
on the digital system of J3DAI, highlighting its Performance-Power-Area (PPA)
characteristics and showcasing advanced edge AI capabilities on a CMOS image
sensor. To support hardware, we utilized the Aidge comprehensive software
framework, which enables the programming of both the host processor and the DNN
accelerator. Aidge supports post-training quantization, significantly reducing
memory footprint and computational complexity, making it crucial for deploying
models on resource-constrained hardware like J3DAI. Our experimental results
demonstrate the versatility and efficiency of this innovative design in the
field of edge AI, showcasing its potential to handle both simple and
computationally intensive tasks. Future work will focus on further optimizing
the architecture and exploring new applications to fully leverage the
capabilities of J3DAI. As edge AI continues to grow in importance, innovations
like J3DAI will play a crucial role in enabling real-time, low-latency, and
energy-efficient AI processing at the edge.

</details>


### [56] [Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration](https://arxiv.org/abs/2506.15440)
*Omar Numan,Gaurav Singh,Kazybek Adam,Jelin Leslin,Aleksi Korsman,Otto Simola,Marko Kosunen,Jussi Ryynänen,Martin Andraud*

Main category: cs.AR

TL;DR: 论文提出了一种自校准混合信号计算内存储器（CIM）加速器SoC，以解决CIM架构在集成和可靠性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 混合信号CIM架构在AI任务中具有高效性，但面临集成和可靠性问题，阻碍其大规模应用。

Method: 采用22nm FDSOI技术实现SoC，结合SRAM密度与线性电阻多比特计算，并提供开源编程和测试策略。

Result: 通过自动校准提升计算信噪比25-45%，达到18-24dB。

Conclusion: 该方案为CIM架构的大规模集成和高可靠性提供了可行性。

Abstract: Developing accurate and reliable Compute-In-Memory (CIM) architectures is
becoming a key research focus to accelerate Artificial Intelligence (AI) tasks
on hardware, particularly Deep Neural Networks (DNNs). In that regard, there
has been significant interest in analog and mixed-signal CIM architectures
aimed at increasing the efficiency of data storage and computation to handle
the massive amount of data needed by DNNs. Specifically, resistive mixed-signal
CIM cores are pushed by recent progresses in emerging Non-Volatile Memory
(eNVM) solutions. Yet, mixed-signal CIM computing cores still face several
integration and reliability challenges that hinder their large-scale adoption
into end-to-end AI computing systems. In terms of integration, resistive and
eNVM-based CIM cores need to be integrated with a control processor to realize
end-to-end AI acceleration. Moreover, SRAM-based CIM architectures are still
more efficient and easier to program than their eNVM counterparts. In terms of
reliability, analog circuits are more susceptible to variations, leading to
computation errors and degraded accuracy. This work addresses these two
challenges by proposing a self-calibrated mixed-signal CIM accelerator SoC,
fabricated in 22-nm FDSOI technology. The integration is facilitated by (1) the
CIM architecture, combining the density and ease of SRAM-based weight storage
with multi-bit computation using linear resistors, and (2) an open-source
programming and testing strategy for CIM systems. The accuracy and reliability
are enabled through an automated RISC-V controlled on-chip calibration,
allowing us to improve the compute SNR by 25 to 45% across multiple columns to
reach 18-24 dB. To showcase further integration possibilities, we show how our
proof-of-concept SoC can be extended to recent high-density linear resistor
technologies for enhanced computing performance.

</details>


### [57] [CXL-GPU: Pushing GPU Memory Boundaries with the Integration of CXL Technologies](https://arxiv.org/abs/2506.15601)
*Donghyun Gouk,Seungkwan Kang,Seungjun Lee,Jiseon Kim,Kyungkuk Nam,Eojin Ryu,Sangwon Lee,Dongpyung Kim,Junhyeok Jang,Hanyeoreum Bae,Myoungsoo Jung*

Main category: cs.AR

TL;DR: 提出一种基于CXL的GPU存储扩展方案，创新设计多CXL根端口集成多种存储介质，首次实现两位数纳秒级延迟，通过推测读和确定性写机制优化读写性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决GPU存储扩展的延迟和性能瓶颈，提升存储技术效率。

Method: 设计多CXL根端口的GPU系统，开发定制CXL控制器，集成推测读和确定性写机制。

Result: 实现两位数纳秒级延迟，性能显著优于现有方案。

Conclusion: 该方案为GPU存储技术带来重大进步，具有实际应用潜力。

Abstract: This work introduces a GPU storage expansion solution utilizing CXL,
featuring a novel GPU system design with multiple CXL root ports for
integrating diverse storage media (DRAMs and/or SSDs). We developed and
siliconized a custom CXL controller integrated at the hardware RTL level,
achieving two-digit nanosecond roundtrip latency, the first in the field. This
study also includes speculative read and deterministic store mechanisms to
efficiently manage read and write operations to hide the endpoint's backend
media latency variation. Performance evaluations reveal our approach
significantly outperforms existing methods, marking a substantial advancement
in GPU storage technology.

</details>


### [58] [From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and Instruction Annotation](https://arxiv.org/abs/2506.15613)
*Miryeong Kwon,Donghyun Gouk,Junhyeok Jang,Jinwoo Baek,Hyunwoo You,Sangyoon Ji,Hongjoo Jung,Junseok Moon,Seungkwan Kang,Seungjun Lee,Myoungsoo Jung*

Main category: cs.AR

TL;DR: 论文探讨了如何利用Compute Express Link (CXL)将基于PCIe的块存储转变为可扩展的字节可寻址工作内存，重点研究了缓存性和Type 3终端设备（CXL-SSD）的关键作用，并通过原型设计和模拟验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决将块存储适配到CXL内存中心模型的挑战，推动存储与内存的融合。

Method: 采用缓存性作为关键驱动因素，提出Type 3终端设备CXL-SSD，并在FPGA平台上构建原型，结合确定性（Determinism）和可缓冲性（Bufferability）标注机制优化性能。

Result: CXL-SSD性能比PCIe内存扩展器提升10.9倍，标注机制进一步降低延迟5.4倍，在局部性高的工作负载中接近DRAM性能。

Conclusion: 研究证明了块存储与CXL生态系统集成的可行性，为未来内存存储融合奠定了基础。

Abstract: This paper explores how Compute Express Link (CXL) can transform PCIe-based
block storage into a scalable, byte-addressable working memory. We address the
challenges of adapting block storage to CXL's memory-centric model by
emphasizing cacheability as a key enabler and advocating for Type 3 endpoint
devices, referred to as CXL-SSDs. To validate our approach, we prototype a
CXL-SSD on a custom FPGA platform and propose annotation mechanisms,
Determinism and Bufferability, to enhance performance while preserving data
persistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves
10.9x better performance than PCIe-based memory expanders and further reduces
latency by 5.4x with annotation enhancements. In workloads with high locality,
CXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This
work highlights the feasibility of integrating block storage into CXL's
ecosystem and provides a foundation for future memory-storage convergence.

</details>


### [59] [SR-NCL: an Area-/Energy-Efficient Resilient NCL Architecture Based on Selective Redundancy](https://arxiv.org/abs/2506.15634)
*Hasnain A. Ziad,Alexander C. Bodoh,Ashiq A. Sakib*

Main category: cs.AR

TL;DR: 提出了一种基于选择性冗余的新型容错NCL架构，相比传统重复冗余设计，在面积和能耗上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统基于重复冗余的QDI异步电路设计虽然完全容错，但带来显著的能耗、延迟和面积开销。

Method: 采用选择性冗余设计的新型NCL架构。

Result: 在图像处理应用中，新架构在面积和能耗上优于传统设计。

Conclusion: 选择性冗余是一种高效且经济的容错设计方案。

Abstract: Duplication-based redundancy schemes have proven to be effective in designing
fully-resilient Quasi-delay Insensitive (QDI) asynchronous circuits. The
complete resiliency, however, is accompanied by significant energy, latency,
and area overhead. This paper presents a novel error-tolerant Null Convention
Logic (NCL) architecture based on selective redundancy. Results demonstrate the
efficacy of the proposed method in terms of area and energy utilization as
compared to existing duplication-based NCL designs, targeting an image
processing application.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [60] [A theory of Lending Protocols in DeFi](https://arxiv.org/abs/2506.15295)
*Massimo Bartoletti,Enrico Lipparini*

Main category: cs.GT

TL;DR: 提出一个形式化模型，用于分析去中心化借贷协议的经济和策略动态。


<details>
  <summary>Details</summary>
Motivation: 去中心化借贷协议虽大但复杂，其激励机制可能导致意想不到的行为或攻击，需要系统化的分析工具。

Method: 建立一个形式化模型，捕捉主流平台的核心特征。

Result: 识别并证明了与协议经济和策略动态相关的关键属性。

Conclusion: 形式化模型帮助理解借贷协议的行为和潜在漏洞。

Abstract: Lending protocols are one of the main applications of Decentralized Finance
(DeFi), enabling crypto-assets loan markets with a total value estimated in the
tens of billions of dollars. Unlike traditional lending systems, these
protocols operate without relying on trusted authorities or off-chain
enforcement mechanisms. To achieve key economic goals such as stability of the
loan market, they devise instead trustless on-chain mechanisms, such as
rewarding liquidators who repay the loans of under-collateralized borrowers by
awarding them part of the borrower's collateral. The complexity of these
incentive mechanisms, combined with their entanglement in low-level
implementation details, makes it challenging to precisely assess the structural
and economic properties of lending protocols, as well as to analyze user
strategies and attacks. Crucially, since participation is open to anyone, any
weaknesses in the incentive mechanism may give rise to unintended emergent
behaviours, or even enable adversarial strategies aimed at making profits to
the detriment of legit users, or at undermining the stability of the protocol.
In this work, we propose a formal model of lending protocols that captures the
essential features of mainstream platforms, enabling us to identify and prove
key properties related to their economic and strategic dynamics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [61] [EmojiVoice: Towards long-term controllable expressivity in robot speech](https://arxiv.org/abs/2506.15085)
*Paige Tuttösí,Shivam Mehta,Zachary Syvenky,Bermet Burkanova,Gustav Eje Henter,Angelica Lim*

Main category: cs.RO

TL;DR: EmojiVoice是一个免费、可定制的TTS工具包，旨在为社交机器人提供长期可变的表情语音，通过表情符号提示实现精细控制，并在不同场景中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 社交机器人通常使用固定的愉快语音，缺乏人类语音中的长期变化。EmojiVoice旨在解决这一问题，提供可变的表情语音。

Method: 使用EmojiVoice工具包，通过表情符号提示（emoji-prompting）实现语音表达的精细控制，并采用轻量级Matcha-TTS实时生成语音。通过三个案例研究验证效果。

Result: 在讲故事任务中，多样化的表情符号提示提升了语音的感知和表达力；但在助手用例中，表情语音并未受到偏好。

Conclusion: EmojiVoice为社交机器人提供了有效的可变表情语音解决方案，但其适用性需根据具体场景调整。

Abstract: Humans vary their expressivity when speaking for extended periods to maintain
engagement with their listener. Although social robots tend to be deployed with
``expressive'' joyful voices, they lack this long-term variation found in human
speech. Foundation model text-to-speech systems are beginning to mimic the
expressivity in human speech, but they are difficult to deploy offline on
robots. We present EmojiVoice, a free, customizable text-to-speech (TTS)
toolkit that allows social roboticists to build temporally variable, expressive
speech on social robots. We introduce emoji-prompting to allow fine-grained
control of expressivity on a phase level and use the lightweight Matcha-TTS
backbone to generate speech in real-time. We explore three case studies: (1) a
scripted conversation with a robot assistant, (2) a storytelling robot, and (3)
an autonomous speech-to-speech interactive agent. We found that using varied
emoji prompting improved the perception and expressivity of speech over a long
period in a storytelling task, but expressive voice was not preferred in the
assistant use case.

</details>


### [62] [I Know You're Listening: Adaptive Voice for HRI](https://arxiv.org/abs/2506.15107)
*Paige Tuttösí*

Main category: cs.RO

TL;DR: 该论文探讨了为语言教学机器人开发任务特定的语音合成系统，并提出了一种轻量级、表达丰富的语音合成方法，通过适应环境调整和针对第二语言学习者的清晰度模式来提高教学效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究中缺乏针对语言教学机器人的任务特定语音合成系统，这可能影响机器人语言教学的有效性。作者旨在填补这一空白。

Method: 1. 使用Matcha-TTS的微调版本，通过表情符号提示创建实时运行的轻量级高表达语音；2. 提出机器人语音适应物理和社交环境的调整策略；3. 开发针对第二语言学习者的清晰度模式，通过调整元音时长提高语音清晰度。

Result: 1. 高表达语音在长期表达性任务（如讲故事）中表现更优；2. 环境适应性调整使语音更符合环境需求；3. 清晰度模式显著提高了第二语言学习者的理解能力，并减少了转录错误。

Conclusion: 论文通过三种贡献为语言教学机器人开发了高效、适应性强的语音合成系统，提升了语音表达力和学习者的理解能力。

Abstract: While the use of social robots for language teaching has been explored, there
remains limited work on a task-specific synthesized voices for language
teaching robots. Given that language is a verbal task, this gap may have severe
consequences for the effectiveness of robots for language teaching tasks. We
address this lack of L2 teaching robot voices through three contributions: 1.
We address the need for a lightweight and expressive robot voice. Using a
fine-tuned version of Matcha-TTS, we use emoji prompting to create an
expressive voice that shows a range of expressivity over time. The voice can
run in real time with limited compute resources. Through case studies, we found
this voice more expressive, socially appropriate, and suitable for long periods
of expressive speech, such as storytelling. 2. We explore how to adapt a
robot's voice to physical and social ambient environments to deploy our voices
in various locations. We found that increasing pitch and pitch rate in noisy
and high-energy environments makes the robot's voice appear more appropriate
and makes it seem more aware of its current environment. 3. We create an
English TTS system with improved clarity for L2 listeners using known
linguistic properties of vowels that are difficult for these listeners. We used
a data-driven, perception-based approach to understand how L2 speakers use
duration cues to interpret challenging words with minimal tense (long) and lax
(short) vowels in English. We found that the duration of vowels strongly
influences the perception for L2 listeners and created an "L2 clarity mode" for
Matcha-TTS that applies a lengthening to tense vowels while leaving lax vowels
unchanged. Our clarity mode was found to be more respectful, intelligible, and
encouraging than base Matcha-TTS while reducing transcription errors in these
challenging tense/lax minimal pairs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [63] [Empirical Studies of Large Scale Environment Scanning by Consumer Electronics](https://arxiv.org/abs/2506.14771)
*Mengyuan Wang,Yang Liu,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: eess.IV

TL;DR: 本文评估了Matterport Pro3在大规模环境重建中的表现，展示了其高精度和高效能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证消费级3D扫描设备Matterport Pro3在大规模场景重建中的实用性和性能极限。

Method: 通过六层建筑（17,567平方米）的1,099次扫描点，对比iPhone设备，分析Pro3的点云密度和对齐精度。

Result: Pro3生成的点云更密集（1,877,324点vs 506,961点），对齐误差更低（RMSE 0.0118米）。

Conclusion: Matterport Pro3在成本效益和性能间取得平衡，适合大规模3D建模应用。

Abstract: This paper presents an empirical evaluation of the Matterport Pro3, a
consumer-grade 3D scanning device, for large-scale environment reconstruction.
We conduct detailed scanning (1,099 scanning points) of a six-floor building
(17,567 square meters) and assess the device's effectiveness, limitations, and
performance enhancements in diverse scenarios. Challenges encountered during
the scanning are addressed through proposed solutions, while we also explore
advanced methods to overcome them more effectively. Comparative analysis with
another consumer-grade device (iPhone) highlights the Pro3's balance between
cost-effectiveness and performance. The Matterport Pro3 achieves a denser point
cloud with 1,877,324 points compared to the iPhone's 506,961 points and higher
alignment accuracy with an RMSE of 0.0118 meters. The cloud-to-cloud (C2C)
average distance error between the two point cloud models is 0.0408 meters,
with a standard deviation of 0.0715 meters. The study demonstrates the Pro3's
ability to generate high-quality 3D models suitable for large-scale
applications, leveraging features such as LiDAR and advanced alignment
techniques.

</details>


### [64] [ABC: Adaptive BayesNet Structure Learning for Computational Scalable Multi-task Image Compression](https://arxiv.org/abs/2506.15228)
*Yufeng Zhang,Wenrui Dai,Hang Yu,Shizhan Liu,Junhui Hou,Jianguo Li,Weiyao Lin*

Main category: eess.IV

TL;DR: ABC框架通过贝叶斯网络结构学习实现神经图像压缩的全面计算可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决神经图像压缩（NIC）因高计算需求而难以广泛应用的问题。

Method: 提出ABC框架，包含异构二分贝叶斯网络（管理计算）、同构多分贝叶斯网络（优化处理）和自适应控制模块。

Result: ABC实现了全面的计算可扩展性，保持压缩性能的同时，适应性更强。

Conclusion: ABC是一种多功能、计算可扩展的NIC解决方案，适用于多种架构。

Abstract: Neural Image Compression (NIC) has revolutionized image compression with its
superior rate-distortion performance and multi-task capabilities, supporting
both human visual perception and machine vision tasks. However, its widespread
adoption is hindered by substantial computational demands. While existing
approaches attempt to address this challenge through module-specific
optimizations or pre-defined complexity levels, they lack comprehensive control
over computational complexity. We present ABC (Adaptive BayesNet structure
learning for computational scalable multi-task image Compression), a novel,
comprehensive framework that achieves computational scalability across all NIC
components through Bayesian network (BayesNet) structure learning. ABC
introduces three key innovations: (i) a heterogeneous bipartite BayesNet
(inter-node structure) for managing neural backbone computations; (ii) a
homogeneous multipartite BayesNet (intra-node structure) for optimizing
autoregressive unit processing; and (iii) an adaptive control module that
dynamically adjusts the BayesNet structure based on device capabilities, input
data complexity, and downstream task requirements. Experiments demonstrate that
ABC enables full computational scalability with better complexity adaptivity
and broader complexity control span, while maintaining competitive compression
performance. Furthermore, the framework's versatility allows integration with
various NIC architectures that employ BayesNet representations, making it a
robust solution for ensuring computational scalability in NIC applications.
Code is available in https://github.com/worldlife123/cbench_BaSIC.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [65] [Not Even Nice Work If You Can Get It; A Longitudinal Study of Uber's Algorithmic Pay and Pricing](https://arxiv.org/abs/2506.15278)
*Reuben Binns,Jake Stein,Siddhartha Datta,Max Van Kleek,Nigel Shadbolt*

Main category: cs.CY

TL;DR: 研究通过参与式审计发现，Uber引入动态定价后，司机收入下降、平台分成增加，工作分配与薪酬更不可预测，司机间不平等加剧，等待时间更长。


<details>
  <summary>Details</summary>
Motivation: 探讨Uber等平台如何通过算法影响司机的灵活性与收入，并利用参与式方法揭示动态定价对司机的影响。

Method: 采用参与式行动研究与审计，分析英国258名司机的150万次行程数据，对比动态定价前后变化。

Result: 动态定价后，司机收入减少、平台分成增加、工作分配与薪酬更不稳定、司机间不平等加剧、等待时间延长。

Conclusion: 动态定价加剧了平台对司机的剥削，呼吁更多工人数据科学研究与算法透明度。

Abstract: Ride-sharing platforms like Uber market themselves as enabling `flexibility'
for their workforce, meaning that drivers are expected to anticipate when and
where the algorithm will allocate them jobs, and how well remunerated those
jobs will be. In this work we describe our process of participatory action
research with drivers and trade union organisers, culminating in a
participatory audit of Uber's algorithmic pay and work allocation, before and
after the introduction of dynamic pricing. Through longitudinal analysis of 1.5
million trips from 258 drivers in the UK, we find that after dynamic pricing,
pay has decreased, Uber's cut has increased, job allocation and pay is less
predictable, inequality between drivers is increased, and drivers spend more
time waiting for jobs. In addition to these findings, we provide methodological
and theoretical contributions to algorithm auditing, gig work, and the emerging
practice of worker data science.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [66] [On the solvable-unsolvable transition due to noise-induced chaos in digital memcomputing](https://arxiv.org/abs/2506.14928)
*Dyk Chung Nguyen,Thomas Chetaille,Yuan-Hang Zhang,Yuriy V. Pershin,Massimiliano Di Ventra*

Main category: nlin.CD

TL;DR: 研究了数字记忆计算器（DMMs）在数值误差和物理噪声下的表现，发现噪声会引发混沌转变，影响问题解决能力。实验结果显示，即使平均最大Lyapunov指数为正，DMMs仍能解决问题。


<details>
  <summary>Details</summary>
Motivation: 探讨DMMs在数值噪声和物理噪声下的表现及其对问题解决能力的影响，填补了此前研究的空白。

Method: 通过改变积分时间步长（数值噪声）和加入随机扰动（物理噪声）来模拟DMMs的行为，分析功率谱和Lyapunov指数。

Result: 发现噪声强度与混沌行为相关，且存在即使Lyapunov指数为正仍能解决问题的机制。功率谱可用于区分和优化DMMs的运行状态。

Conclusion: 数值和物理噪声对DMMs的影响相似，功率谱可帮助控制其最佳动态运行状态。

Abstract: Digital memcomputing machines (DMMs) have been designed to solve complex
combinatorial optimization problems. Since DMMs are fundamentally classical
dynamical systems, their ordinary differential equations (ODEs) can be
efficiently simulated on modern computers. This provides a unique platform to
study their performance under various conditions. An aspect that has received
little attention so far is how their performance is affected by the numerical
errors in the solution of their ODEs and the physical noise they would be
naturally subject to if built in hardware. Here, we analyze these two aspects
in detail by varying the integration time step (numerical noise) and adding
stochastic perturbations (physical noise) into the equations of DMMs. We are
particularly interested in understanding how noise induces a chaotic transition
that marks the shift from successful problem-solving to failure in these
systems. Our study includes an analysis of power spectra and Lyapunov exponents
depending on the noise strength. The results reveal a correlation between the
instance solvability and the sign of the ensemble averaged mean largest
Lyapunov exponent. Interestingly, we find a regime in which DMMs with positive
mean largest Lyapunov exponents still exhibit solvability. Furthermore, the
power spectra provide additional information about our system by distinguishing
between regular behavior (peaks) and chaotic behavior (broadband spectrum).
Therefore, power spectra could be utilized to control whether a DMM operates in
the optimal dynamical regime. Overall, we find that the qualitative effects of
numerical and physical noise are mostly similar, despite their fundamentally
different origin.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [67] [Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?](https://arxiv.org/abs/2506.14805)
*Yang Yao,Lingyu Li,Jiaxin Song,Chiyu Chen,Zhenqi He,Yixu Wang,Xin Wang,Tianle Gu,Jie Li,Yan Teng,Yingchun Wang*

Main category: cs.CV

TL;DR: 本文介绍了Argus Inspection多模态基准和Eye of Panoptes框架，用于评估MLLMs的细粒度视觉感知和因果推理能力，实验显示现有模型仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在细粒度视觉感知和常识因果推理方面存在不足，需开发更全面的评估方法。

Method: 提出Argus Inspection基准和Eye of Panoptes框架，结合Sigmoid度量与指示函数，评估MLLMs的推理能力。

Result: 实验表明，主流MLLMs在细粒度视觉推理中的最佳表现仅为0.46，改进空间大。

Conclusion: 研究为MLLMs的进一步优化提供了重要参考。

Abstract: As Multimodal Large Language Models (MLLMs) continue to evolve, their
cognitive and reasoning capabilities have seen remarkable progress. However,
challenges in visual fine-grained perception and commonsense causal inference
persist. This paper introduces Argus Inspection, a multimodal benchmark with
two levels of difficulty, emphasizing detailed visual recognition while
incorporating real-world commonsense understanding to evaluate causal reasoning
abilities. Expanding on it, we present the Eye of Panoptes framework, which
integrates a binary parametric Sigmoid metric with an indicator function,
enabling a more holistic evaluation of MLLMs' responses in opinion-based
reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the
highest performance in visual fine-grained reasoning reaches only 0.46,
highlighting considerable potential for enhancement. Our research offers
valuable perspectives for the continued refinement of MLLMs.

</details>


### [68] [MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion](https://arxiv.org/abs/2506.15276)
*Jun Zhu,Xinfeng Zhang,Lv Tang,JunHao Jiang*

Main category: cs.CV

TL;DR: MSNeRV是用于神经视频表示的多尺度特征融合框架，解决了现有INR方法在细节密集和快速变化内容上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于INR的视频压缩方法在细节密集和快速变化内容上表现不佳，未充分利用网络内部特征且设计缺乏视频特定考虑。

Method: 提出多尺度特征融合框架MSNeRV，包括时间窗口增强时序一致性、GoP级网格表示背景、多尺度空间解码器及自适应损失函数。

Result: 实验表明MSNeRV在INR方法中表现最佳，压缩效率在动态场景中超过VTM-23.7。

Conclusion: MSNeRV通过多尺度特征融合显著提升了视频表示和压缩能力。

Abstract: Implicit Neural representations (INRs) have emerged as a promising approach
for video compression, and have achieved comparable performance to the
state-of-the-art codecs such as H.266/VVC. However, existing INR-based methods
struggle to effectively represent detail-intensive and fast-changing video
content. This limitation mainly stems from the underutilization of internal
network features and the absence of video-specific considerations in network
design. To address these challenges, we propose a multi-scale feature fusion
framework, MSNeRV, for neural video representation. In the encoding stage, we
enhance temporal consistency by employing temporal windows, and divide the
video into multiple Groups of Pictures (GoPs), where a GoP-level grid is used
for background representation. Additionally, we design a multi-scale spatial
decoder with a scale-adaptive loss function to integrate multi-resolution and
multi-frequency information. To further improve feature extraction, we
introduce a multi-scale feature block that fully leverages hidden features. We
evaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and
compression. Experimental results demonstrate that our model exhibits superior
representation capability among INR-based approaches and surpasses VTM-23.7
(Random Access) in dynamic scenarios in terms of compression efficiency.

</details>


### [69] [MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering](https://arxiv.org/abs/2506.15298)
*Xinqi Fan,Jingting Li,John See,Moi Hoon Yap,Wen-Huang Cheng,Xiaobai Li,Xiaopeng Hong,Su-Jing Wang,Adrian K. Davision*

Main category: cs.CV

TL;DR: 论文探讨了面部微表情（MEs）的识别、定位和生成的新方法，提出通过多模态大语言模型（MLLMs）和大视觉语言模型（LVLMs）增强ME分析，并介绍了MEGC 2025的两项任务。


<details>
  <summary>Details</summary>
Motivation: 传统方法将微表情定位和识别视为独立任务，但在实际应用中效果不佳，需要更高效的统一方法。

Method: 提出两项任务：ME-STR（统一序列管道）和ME-VQA（视觉问答），利用MLLMs和LVLMs提升ME分析能力。

Result: 通过新方法提升微表情分析的准确性和效率，为长期视频分析提供更优解决方案。

Conclusion: 论文展示了利用多模态模型的潜力，为微表情研究开辟了新方向。

Abstract: Facial micro-expressions (MEs) are involuntary movements of the face that
occur spontaneously when a person experiences an emotion but attempts to
suppress or repress the facial expression, typically found in a high-stakes
environment. In recent years, substantial advancements have been made in the
areas of ME recognition, spotting, and generation. However, conventional
approaches that treat spotting and recognition as separate tasks are
suboptimal, particularly for analyzing long-duration videos in realistic
settings. Concurrently, the emergence of multimodal large language models
(MLLMs) and large vision-language models (LVLMs) offers promising new avenues
for enhancing ME analysis through their powerful multimodal reasoning
capabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that
reflect these evolving research directions: (1) ME spot-then-recognize
(ME-STR), which integrates ME spotting and subsequent recognition in a unified
sequential pipeline; and (2) ME visual question answering (ME-VQA), which
explores ME understanding through visual question answering, leveraging MLLMs
or LVLMs to address diverse question types related to MEs. All participating
algorithms are required to run on this test set and submit their results on a
leaderboard. More details are available at https://megc2025.github.io.

</details>


### [70] [Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis](https://arxiv.org/abs/2506.14854)
*Varun Mannam,Zhenyu Shi*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的零售视频自动标注方法，显著提升效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 传统零售视频标注方法依赖人工，耗时且不鲁棒，需自动化解决方案。

Method: 利用深度神经网络和对象检测技术，自动识别关键帧并为零售视频提供标注。

Result: 实验显示方法优于传统标注，准确率接近人工标注，节省50%成本。

Conclusion: 自动化关键帧检测大幅节省零售视频标注时间和成本，适用于多种零售应用。

Abstract: Accurate video annotation plays a vital role in modern retail applications,
including customer behavior analysis, product interaction detection, and
in-store activity recognition. However, conventional annotation methods heavily
rely on time-consuming manual labeling by human annotators, introducing
non-robust frame selection and increasing operational costs. To address these
challenges in the retail domain, we propose a deep learning-based approach that
automates key-frame identification in retail videos and provides automatic
annotations of products and customers. Our method leverages deep neural
networks to learn discriminative features by embedding video frames and
incorporating object detection-based techniques tailored for retail
environments. Experimental results showcase the superiority of our approach
over traditional methods, achieving accuracy comparable to human annotator
labeling while enhancing the overall efficiency of retail video annotation.
Remarkably, our approach leads to an average of 2 times cost savings in video
annotation. By allowing human annotators to verify/adjust less than 5% of
detected frames in the video dataset, while automating the annotation process
for the remaining frames without reducing annotation quality, retailers can
significantly reduce operational costs. The automation of key-frame detection
enables substantial time and effort savings in retail video labeling tasks,
proving highly valuable for diverse retail applications such as shopper journey
analysis, product interaction detection, and in-store security monitoring.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [71] [Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders](https://arxiv.org/abs/2506.14937)
*Luan Gonçalves Miranda,Pedro Ivo da Cruz,Murilo Bellezoni Loiola*

Main category: cs.LG

TL;DR: 该论文提出使用机器学习算法自动定义自编码器的重建误差阈值，以提高异常检测系统的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决自编码器在异常检测中重建误差阈值的非标准化问题，从而优化检测性能。

Method: 评估了三种机器学习算法：K-最近邻、K-均值和支持向量机，用于自动定义阈值。

Result: 未明确提及具体结果，但目标是改善异常检测的准确性。

Conclusion: 通过机器学习算法的应用，可以自动优化自编码器的重建误差阈值，提升异常检测系统的性能。

Abstract: Currently, digital security mechanisms like Anomaly Detection Systems using
Autoencoders (AE) show great potential for bypassing problems intrinsic to the
data, such as data imbalance. Because AE use a non-trivial and nonstandardized
separation threshold to classify the extracted reconstruction error, the
definition of this threshold directly impacts the performance of the detection
process. Thus, this work proposes the automatic definition of this threshold
using some machine learning algorithms. For this, three algorithms were
evaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.

</details>


### [72] [FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models](https://arxiv.org/abs/2506.14824)
*Yao Zhang,Hewei Gao,Haokun Chen,Weiguo Li,Yunpu Ma,Volker Tresp*

Main category: cs.LG

TL;DR: FedNano 是一种新的联邦学习框架，通过在服务器上集中大型语言模型，并在客户端引入轻量级的 NanoEdge 模块，解决了 MLLMs 在联邦学习中的部署挑战。


<details>
  <summary>Details</summary>
Motivation: 由于分布式多模态数据和严格的隐私要求，现有的 MLLMs 在联邦学习中面临计算和通信成本高、客户端能力不足等问题。

Method: FedNano 在服务器中央化 LLM，客户端部署轻量级 NanoEdge 模块，包含模态特定的编码器、连接器和低秩适配器，显著减少存储和通信开销。

Result: FedNano 减少了 95% 的客户端存储需求，通信开销降至模型参数的 0.01%，并在实验中优于现有联邦学习基线。

Conclusion: FedNano 弥合了 MLLMs 规模与联邦学习可行性之间的差距，为可扩展的分布式多模态 AI 系统提供了解决方案。

Abstract: Multimodal Large Language Models (MLLMs) excel in tasks like multimodal
reasoning and cross-modal retrieval but face deployment challenges in
real-world scenarios due to distributed multimodal data and strict privacy
requirements. Federated Learning (FL) offers a solution by enabling
collaborative model training without centralizing data. However, realizing FL
for MLLMs presents significant challenges, including high computational
demands, limited client capacity, substantial communication costs, and
heterogeneous client data. Existing FL methods assume client-side deployment of
full models, an assumption that breaks down for large-scale MLLMs due to their
massive size and communication demands. To address these limitations, we
propose FedNano, the first FL framework that centralizes the LLM on the server
while introducing NanoEdge, a lightweight module for client-specific
adaptation. NanoEdge employs modality-specific encoders, connectors, and
trainable NanoAdapters with low-rank adaptation. This design eliminates the
need to deploy LLM on clients, reducing client-side storage by 95%, and
limiting communication overhead to only 0.01% of the model parameters. By
transmitting only compact NanoAdapter updates, FedNano handles heterogeneous
client data and resource constraints while preserving privacy. Experiments
demonstrate that FedNano outperforms prior FL baselines, bridging the gap
between MLLM scale and FL feasibility, and enabling scalable, decentralized
multimodal AI systems.

</details>


### [73] [MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh Smoothing](https://arxiv.org/abs/2506.15571)
*Le Vu Anh,Nguyen Viet Anh,Mehmet Dik,Tu Nguyen Thi Ngoc*

Main category: cs.LG

TL;DR: MicroRicci是一种自调节的局部Ricci-flow求解器，通过结合编码理论和轻量级神经网络模块，显著提升了网格平滑的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 传统的Ricci-flow求解器因全局更新成本高、启发式方法收敛慢或参数调整脆弱，无法满足实时大规模网格平滑的需求。MicroRicci旨在解决这些问题。

Method: MicroRicci采用贪婪的综合征解码步骤快速修正最大曲率误差（O(E)时间），并利用两个微型神经网络模块动态选择顶点和步长。

Result: 在110个SJTU-TMQA网格测试中，MicroRicci将迭代次数从950次降至400次（2.4倍加速），曲率分布从0.19缩小至0.185，UV畸变-MOS相关性达-0.93。

Conclusion: MicroRicci因其线性时间更新、自动超参数适应和高质量几何与感知结果，适用于图形、仿真等资源受限的实时应用。

Abstract: Real-time mesh smoothing at scale remains a formidable challenge: classical
Ricci-flow solvers demand costly global updates, while greedy heuristics suffer
from slow convergence or brittle tuning. We present MicroRicci, the first truly
self-tuning, local Ricci-flow solver that borrows ideas from coding theory and
packs them into just 1K + 200 parameters. Its primary core is a greedy
syndrome-decoding step that pinpoints and corrects the largest curvature error
in O(E) time, augmented by two tiny neural modules that adaptively choose
vertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,
MicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),
tightens curvature spread from 0.19 to 0.185, and achieves a remarkable
UV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per
iteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration
over state-of-the-art methods. MicroRicci's combination of linear-time updates,
automatic hyperparameter adaptation, and high-quality geometric and perceptual
results makes it well suited for real-time, resource-limited applications in
graphics, simulation, and related fields.

</details>


### [74] [Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model](https://arxiv.org/abs/2506.14830)
*Zhizhao Wen,Ruoxin Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 该研究提出了一种结合多注意力机制的BiGRU-MHA混合模型，用于提高SSD健康状态分类的准确性和稳定性。模型在训练集和测试集上的分类准确率分别为92.70%和92.44%，具有优异的泛化能力。AUC指标为0.94，证明其在二元分类中的鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 针对SSD健康状态预测在数据可靠性保障中的关键作用，传统模型泛化能力不足的问题，研究提出了一种新方法。

Method: 结合双向时序建模优势的BiGRU网络和多注意力机制，动态分配特征权重，提高对关键健康指标的敏感性。

Result: 模型表现优异，AUC为0.94，分类准确率高，泛化能力强，适用于工业级存储系统的预防性维护。

Conclusion: 该模型为SSD健康预测提供了新技术方案，能显著降低数据丢失风险，支持云计算数据中心和边缘存储环境的智能决策。

Abstract: Aiming at the critical role of SSD health state prediction in data
reliability assurance, this study proposes a hybrid BiGRU-MHA model that
incorporates a multi-head attention mechanism to enhance the accuracy and
stability of storage device health classification. The model innovatively
integrates temporal feature extraction and key information focusing
capabilities. Specifically, it leverages the bidirectional timing modeling
advantages of the BiGRU network to capture both forward and backward
dependencies of SSD degradation features. Simultaneously, the multi-head
attention mechanism dynamically assigns feature weights, improving the model's
sensitivity to critical health indicators. Experimental results show that the
proposed model achieves classification accuracies of 92.70% on the training set
and 92.44% on the test set, with a minimal performance gap of only 0.26%,
demonstrating excellent generalization ability. Further analysis using the
receiver operating characteristic (ROC) curve shows an area under the curve
(AUC) of 0.94 on the test set, confirming the model's robust binary
classification performance. This work not only presents a new technical
approach for SSD health prediction but also addresses the generalization
bottleneck of traditional models, offering a verifiable method with practical
value for preventive maintenance of industrial-grade storage systems. The
results show the model can significantly reduce data loss risks by providing
early failure warnings and help optimize maintenance costs, supporting
intelligent decision-making in building reliable storage systems for cloud
computing data centers and edge storage environments.

</details>


### [75] [Event-Driven Online Vertical Federated Learning](https://arxiv.org/abs/2506.14911)
*Ganyu Wang,Boyu Wang,Bin Gu,Charles Ling*

Main category: cs.LG

TL;DR: 在线学习在垂直联邦学习（VFL）中比离线学习更具适应性，但其在VFL中的整合面临独特挑战。论文提出了一种事件驱动的在线VFL框架，仅激活部分客户端，并引入动态局部遗憾（DLR）解决非凸和非稳态环境问题。实验表明该框架更稳定且降低了通信和计算成本。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习（VFL）中的在线学习因客户端特征集不重叠且数据异步到达而面临挑战。现有研究忽视了这些问题，因此需要提出新的框架以解决这些挑战。

Method: 提出了一种事件驱动的在线VFL框架，仅激活部分客户端，并引入动态局部遗憾（DLR）以应对非凸和非稳态环境。

Result: 实验表明，该框架在非稳态数据条件下比现有框架更稳定，同时显著降低了通信和计算成本。

Conclusion: 论文提出的框架有效解决了VFL中在线学习的挑战，为实际应用提供了更高效的解决方案。

Abstract: Online learning is more adaptable to real-world scenarios in Vertical
Federated Learning (VFL) compared to offline learning. However, integrating
online learning into VFL presents challenges due to the unique nature of VFL,
where clients possess non-intersecting feature sets for the same sample. In
real-world scenarios, the clients may not receive data streaming for the
disjoint features for the same entity synchronously. Instead, the data are
typically generated by an \emph{event} relevant to only a subset of clients. We
are the first to identify these challenges in online VFL, which have been
overlooked by previous research. To address these challenges, we proposed an
event-driven online VFL framework. In this framework, only a subset of clients
were activated during each event, while the remaining clients passively
collaborated in the learning process. Furthermore, we incorporated
\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by
online learning problems with non-convex models within a non-stationary
environment. We conducted a comprehensive regret analysis of our proposed
framework, specifically examining the DLR under non-convex conditions with
event-driven online VFL. Extensive experiments demonstrated that our proposed
framework was more stable than the existing online VFL framework under
non-stationary data conditions while also significantly reducing communication
and computation costs.

</details>


### [76] [Centroid Approximation for Byzantine-Tolerant Federated Learning](https://arxiv.org/abs/2506.15264)
*Mélanie Cambus,Darya Melnyk,Tijana Milentijević,Stefan Schmid*

Main category: cs.LG

TL;DR: 联邦学习在分布式训练中要求输入满足收敛条件，但以往研究忽略了对拜占庭行为的鲁棒性。作者研究了平均/质心与分布式计算有效性条件之间的权衡，提出了新的上下界结果，并设计了一个新算法，理论和实验结果证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习在拜占庭行为下的鲁棒性和质心近似问题，解决现有研究中有效性与近似质量之间的未解权衡。

Method: 结合理论分析与实验验证，提出了新的下界和上界，并设计了一种在凸有效性条件下实现$√{2d}$近似的新算法。

Result: 首次给出了在文献常见的箱有效性条件下的质心近似下界$√{d}$，并验证了新算法的理论界与实验结果的一致性。

Conclusion: 联邦学习的有效性条件和质心近似之间存在复杂关系，提出的算法和理论结果填补了现有研究的空白。

Abstract: Federated learning allows each client to keep its data locally when training
machine learning models in a distributed setting. Significant recent research
established the requirements that the input must satisfy in order to guarantee
convergence of the training loop. This line of work uses averaging as the
aggregation rule for the training models. In particular, we are interested in
whether federated learning is robust to Byzantine behavior, and observe and
investigate a tradeoff between the average/centroid and the validity conditions
from distributed computing. We show that the various validity conditions alone
do not guarantee a good approximation of the average. Furthermore, we show that
reaching good approximation does not give good results in experimental settings
due to possible Byzantine outliers. Our main contribution is the first lower
bound of $\min\{\frac{n-t}{t},\sqrt{d}\}$ on the centroid approximation under
box validity that is often considered in the literature, where $n$ is the
number of clients, $t$ the upper bound on the number of Byzantine faults, and
$d$ is the dimension of the machine learning model. We complement this lower
bound by an upper bound of $2\min\{n,\sqrt{d}\}$, by providing a new analysis
for the case $n<d$. In addition, we present a new algorithm that achieves a
$\sqrt{2d}$-approximation under convex validity, which also proves that the
existing lower bound in the literature is tight. We show that all presented
bounds can also be achieved in the distributed peer-to-peer setting. We
complement our analytical results with empirical evaluations in federated
stochastic gradient descent and federated averaging settings.

</details>


### [77] [Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction](https://arxiv.org/abs/2506.15626)
*Vincent Roca,Marc Tommasi,Paul Andrey,Aurélien Bellet,Markus D. Schirmer,Hilde Henon,Laurent Puy,Julien Ramon,Grégory Kuchcinski,Martin Bretzner,Renaud Lopes*

Main category: cs.LG

TL;DR: 本研究评估了联邦学习（FL）在缺血性卒中患者中用于大脑年龄预测的表现，发现FL在不集中数据的情况下仍能准确预测，且大脑年龄差异（BrainAGE）与血管风险因素及卒中后功能恢复显著相关。


<details>
  <summary>Details</summary>
Motivation: 传统的BrainAGE模型需要大量数据，但隐私问题限制了数据共享。本研究旨在探索FL是否能在保护隐私的同时提供准确的BrainAGE预测，并验证其与临床表型和功能预后的关联。

Method: 研究使用1674名卒中患者的FLAIR脑影像数据，比较了集中学习、FL和单中心学习三种策略的预测效果，并分析了BrainAGE与血管风险因素及功能预后的关系。

Result: FL模型表现优于单中心模型，且BrainAGE与糖尿病等风险因素及卒中后功能恢复显著相关。集中学习表现最佳，但FL在隐私保护前提下仍具潜力。

Conclusion: FL能在不集中数据的情况下实现准确的BrainAGE预测。BrainAGE与血管风险因素及卒中后恢复的关联表明其在卒中预后建模中的潜在价值。

Abstract: $\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a
neuroimaging biomarker reflecting brain health. However, training robust
BrainAGE models requires large datasets, often restricted by privacy concerns.
This study evaluates the performance of federated learning (FL) for BrainAGE
estimation in ischemic stroke patients treated with mechanical thrombectomy,
and investigates its association with clinical phenotypes and functional
outcomes.
  $\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients
across 16 hospital centers. We implemented standard machine learning and deep
learning models for BrainAGE estimates under three data management strategies:
centralized learning (pooled data), FL (local training at each site), and
single-site learning. We reported prediction errors and examined associations
between BrainAGE and vascular risk factors (e.g., diabetes mellitus,
hypertension, smoking), as well as functional outcomes at three months
post-stroke. Logistic regression evaluated BrainAGE's predictive value for
these outcomes, adjusting for age, sex, vascular risk factors, stroke severity,
time between MRI and arterial puncture, prior intravenous thrombolysis, and
recanalisation outcome.
  $\textbf{Results:}$ While centralized learning yielded the most accurate
predictions, FL consistently outperformed single-site models. BrainAGE was
significantly higher in patients with diabetes mellitus across all models.
Comparisons between patients with good and poor functional outcomes, and
multivariate predictions of these outcomes showed the significance of the
association between BrainAGE and post-stroke recovery.
  $\textbf{Conclusion:}$ FL enables accurate age predictions without data
centralization. The strong association between BrainAGE, vascular risk factors,
and post-stroke recovery highlights its potential for prognostic modeling in
stroke care.

</details>


### [78] [MedSyn: Enhancing Diagnostics with Human-AI Collaboration](https://arxiv.org/abs/2506.14774)
*Burcu Sayin,Ipek Baris Schlicht,Ngoc Vo Hong,Sara Allievi,Jacopo Staiano,Pasquale Minervini,Andrea Passerini*

Main category: cs.LG

TL;DR: 论文提出了一种混合人机框架MedSyn，通过医生与LLMs的多步交互对话优化诊断和治疗决策。


<details>
  <summary>Details</summary>
Motivation: 临床决策复杂且易受认知偏差影响，现有LLMs工具因单次互动限制了其实用性。

Method: 提出MedSyn框架，支持医生与LLMs动态交互，挑战与反馈并存。

Result: 实验显示开源LLMs作为医生助手潜力显著。

Conclusion: 未来需通过真实医生互动进一步验证MedSyn对诊断准确性和患者结局的改善效果。

Abstract: Clinical decision-making is inherently complex, often influenced by cognitive
biases, incomplete information, and case ambiguity. Large Language Models
(LLMs) have shown promise as tools for supporting clinical decision-making, yet
their typical one-shot or limited-interaction usage may overlook the
complexities of real-world medical practice. In this work, we propose a hybrid
human-AI framework, MedSyn, where physicians and LLMs engage in multi-step,
interactive dialogues to refine diagnoses and treatment decisions. Unlike
static decision-support tools, MedSyn enables dynamic exchanges, allowing
physicians to challenge LLM suggestions while the LLM highlights alternative
perspectives. Through simulated physician-LLM interactions, we assess the
potential of open-source LLMs as physician assistants. Results show open-source
LLMs are promising as physician assistants in the real world. Future work will
involve real physician interactions to further validate MedSyn's usefulness in
diagnostic accuracy and patient outcomes.

</details>


### [79] [ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification](https://arxiv.org/abs/2506.14783)
*Mohamed Masry,Mohamed Amen,Mohamed Elzyat,Mohamed Hamed,Norhan Magdy,Maram Khaled*

Main category: cs.LG

TL;DR: 论文提出了一种结合脑电图（EEG）和同步眼动数据的框架ETS，用于自然语言解码和情感分类，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决非侵入式EEG在开放词汇场景下解码自然语言的挑战，传统方法因噪声和变异性表现不佳。

Method: 集成EEG与眼动数据，提出ETS框架，支持开放词汇文本生成和情感分类。

Result: 在BLEU和Rouge分数上表现优异，情感分类F1分数提升10%，且适应多来源数据。

Conclusion: ETS框架在开放词汇EEG到文本系统中具有高性能潜力。

Abstract: Decoding natural language from brain activity using non-invasive
electroencephalography (EEG) remains a significant challenge in neuroscience
and machine learning, particularly for open-vocabulary scenarios where
traditional methods struggle with noise and variability. Previous studies have
achieved high accuracy on small-closed vocabularies, but it still struggles on
open vocabularies. In this study, we propose ETS, a framework that integrates
EEG with synchronized eye-tracking data to address two critical tasks: (1)
open-vocabulary text generation and (2) sentiment classification of perceived
language. Our model achieves a superior performance on BLEU and Rouge score for
EEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment
classification, which significantly outperforms supervised baselines.
Furthermore, we show that our proposed model can handle data from various
subjects and sources, showing great potential for high performance open
vocabulary eeg-to-text system.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [80] [SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning](https://arxiv.org/abs/2506.15154)
*Anuradha Chopra,Abhinaba Roy,Dorien Herremans*

Main category: cs.SD

TL;DR: 介绍了一个多任务音乐字幕模型SonicVerse，通过结合字幕生成与辅助音乐特征检测任务，生成丰富且描述性的音乐片段描述。


<details>
  <summary>Details</summary>
Motivation: 为丰富音乐数据库并推动音乐AI研究，需生成准确反映音乐特性的详细字幕。

Method: 采用基于投影的架构，将音频输入转换为语言标记，同时通过辅助头检测音乐特征，并将特征投影为语言标记以增强字幕输入。扩展MusicBench数据集，使用MIRFLEX提取音乐特征。

Result: 实验表明，通过整合音乐特征，提升了生成字幕的质量与细节。

Conclusion: SonicVerse不仅能生成短音乐片段的详细描述，还能为长音乐片段生成时间信息描述，展示了多任务学习在音乐字幕生成中的潜力。

Abstract: Detailed captions that accurately reflect the characteristics of a music
piece can enrich music databases and drive forward research in music AI. This
paper introduces a multi-task music captioning model, SonicVerse, that
integrates caption generation with auxiliary music feature detection tasks such
as key detection, vocals detection, and more, so as to directly capture both
low-level acoustic details as well as high-level musical attributes. The key
contribution is a projection-based architecture that transforms audio input
into language tokens, while simultaneously detecting music features through
dedicated auxiliary heads. The outputs of these heads are also projected into
language tokens, to enhance the captioning input. This framework not only
produces rich, descriptive captions for short music fragments but also directly
enables the generation of detailed time-informed descriptions for longer music
pieces, by chaining the outputs using a large-language model. To train the
model, we extended the MusicBench dataset by annotating it with music features
using MIRFLEX, a modular music feature extractor, resulting in paired audio,
captions and music feature data. Experimental results show that incorporating
features in this way improves the quality and detail of the generated captions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [81] [Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices](https://arxiv.org/abs/2506.15028)
*Gargi Mitra,Mohammadreza Hallajiyan,Inji Kim,Athish Pranav Dharmalingam,Mohammed Elnawawy,Shahrear Iqbal,Karthik Pattabiraman,Homa Alemzadeh*

Main category: cs.CR

TL;DR: AI/ML 在医疗设备中的整合带来巨大好处，但也引入严重的网络安全风险。本文强调需在上市前解决这些挑战，提出工具和方法以帮助安全分析师进行全面的风险评估。


<details>
  <summary>Details</summary>
Motivation: 医疗设备的AI/ML集成增加了安全和隐私风险，亟需从设计阶段解决这些问题以保护患者安全。

Method: 分析公开的设备召回、不良事件和已知漏洞数据，开发工具和技术以支持上市前风险评估。

Result: 提出一套工具，帮助制造商将网络安全作为AI/ML医疗设备的核心设计原则。

Conclusion: 在医疗设备的AI/ML集成中，从设计阶段嵌入网络安全是确保患者安全的关键。

Abstract: The integration of AI/ML into medical devices is rapidly transforming
healthcare by enhancing diagnostic and treatment facilities. However, this
advancement also introduces serious cybersecurity risks due to the use of
complex and often opaque models, extensive interconnectivity, interoperability
with third-party peripheral devices, Internet connectivity, and vulnerabilities
in the underlying technologies. These factors contribute to a broad attack
surface and make threat prevention, detection, and mitigation challenging.
Given the highly safety-critical nature of these devices, a cyberattack on
these devices can cause the ML models to mispredict, thereby posing significant
safety risks to patients. Therefore, ensuring the security of these devices
from the time of design is essential. This paper underscores the urgency of
addressing the cybersecurity challenges in ML-enabled medical devices at the
pre-market phase. We begin by analyzing publicly available data on device
recalls and adverse events, and known vulnerabilities, to understand the threat
landscape of AI/ML-enabled medical devices and their repercussions on patient
safety. Building on this analysis, we introduce a suite of tools and techniques
designed by us to assist security analysts in conducting comprehensive
premarket risk assessments. Our work aims to empower manufacturers to embed
cybersecurity as a core design principle in AI/ML-enabled medical devices,
thereby making them safe for patients.

</details>


### [82] [Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine](https://arxiv.org/abs/2506.15070)
*Rasha Karakchi,Rye Stahle-Smith,Nishant Chinnasami,Tiffany Yu*

Main category: cs.CR

TL;DR: SPiME是一种轻量级、可扩展且兼容FPGA的安全存储器内处理加密架构，通过集成AES-128实现高效、低延迟的加密，适用于边缘计算和嵌入式系统。


<details>
  <summary>Details</summary>
Motivation: 物联网应用的快速增长对边缘计算的高效、高吞吐量和节能数据处理提出了更高要求，传统CPU加密方法存在性能瓶颈和数据移动过多的问题。

Method: 设计了一种模块化并行存储器内处理单元阵列，每个单元结合AES核心和最小控制单元，支持分布式就地加密，并在FPGA上实现和测试。

Result: SPiME可以在高端FPGA上扩展到超过4000个并行单元，资源利用率低于5%，提供持续加密吞吐量超过25 Gbps，且延迟低且可预测。

Conclusion: SPiME的便携性、可配置性和资源效率使其成为安全边缘计算、嵌入式加密系统和可定制硬件加速器的理想解决方案。

Abstract: The exponential growth of Internet of Things (IoT) applications has
intensified the demand for efficient, high-throughput, and energy-efficient
data processing at the edge. Conventional CPU-centric encryption methods suffer
from performance bottlenecks and excessive data movement, especially in
latency-sensitive and resource-constrained environments. In this paper, we
present SPiME, a lightweight, scalable, and FPGA-compatible Secure
Processor-in-Memory Encryption architecture that integrates the Advanced
Encryption Standard (AES-128) directly into a Processing-in-Memory (PiM)
framework. SPiME is designed as a modular array of parallel PiM units, each
combining an AES core with a minimal control unit to enable distributed
in-place encryption with minimal overhead. The architecture is fully
implemented in Verilog and tested on multiple AMD UltraScale and UltraScale+
FPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units
while maintaining less than 5\% utilization of key FPGA resources on high-end
devices. It delivers over 25~Gbps in sustained encryption throughput with
predictable, low-latency performance. The design's portability,
configurability, and resource efficiency make it a compelling solution for
secure edge computing, embedded cryptographic systems, and customizable
hardware accelerators.

</details>


### [83] [deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses](https://arxiv.org/abs/2506.15648)
*Georgios Androutsopoulos,Antonio Bianchi*

Main category: cs.CR

TL;DR: deepSURF是一个结合静态分析和LLM引导的模糊测试工具，用于检测Rust库中的内存安全漏洞，特别是针对unsafe代码。通过处理泛型和动态增强模糊测试，工具在27个Rust库中发现20个已知和6个新漏洞，优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有检测Rust内存漏洞的工具能力有限，未能充分处理Rust特有类型或依赖人工干预。

Method: deepSURF集成静态分析和LLM引导的模糊测试生成，通过替换泛型和动态生成模糊测试代码来模拟用户行为。

Result: 在27个Rust库中成功发现20个已知和6个新漏洞，表现优于现有工具。

Conclusion: deepSURF通过创新方法显著提升了Rust内存漏洞检测能力。

Abstract: Although Rust ensures memory safety by default, it also permits the use of
unsafe code, which can introduce memory safety vulnerabilities if misused.
Unfortunately, existing tools for detecting memory bugs in Rust typically
exhibit limited detection capabilities, inadequately handle Rust-specific
types, or rely heavily on manual intervention.
  To address these limitations, we present deepSURF, a tool that integrates
static analysis with Large Language Model (LLM)-guided fuzzing harness
generation to effectively identify memory safety vulnerabilities in Rust
libraries, specifically targeting unsafe code. deepSURF introduces a novel
approach for handling generics by substituting them with custom types and
generating tailored implementations for the required traits, enabling the
fuzzer to simulate user-defined behaviors within the fuzzed library.
Additionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,
facilitating exploration of complex API interactions and significantly
increasing the likelihood of exposing memory safety vulnerabilities. We
evaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20
known memory safety bugs and uncovering 6 previously unknown vulnerabilities,
demonstrating clear improvements over state-of-the-art tools.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [84] [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
*Yining Hong,Rui Sun,Bingxuan Li,Xingcheng Yao,Maxine Wu,Alexander Chien,Da Yin,Ying Nian Wu,Zhecan James Wang,Kai-Wei Chang*

Main category: cs.AI

TL;DR: 论文提出了一种新型AI代理框架，结合了实体感知与网络推理能力，以解决需要跨领域智能的任务。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理多局限于单一领域，无法同时处理实体世界与数字信息，限制了解决跨领域任务的能力。

Method: 开发了‘Embodied Web Agents’任务环境，整合了3D模拟环境与功能性网络接口，并构建了基准测试。

Result: 实验显示现有AI系统在跨领域任务上与人类能力仍有显著差距。

Conclusion: 研究为结合实体认知与网络知识访问的智能系统提供了新的挑战与机遇。

Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.

</details>


### [85] [Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations](https://arxiv.org/abs/2506.13776)
*Kevin L. Wei,Patricia Paskov,Sunishchal Dev,Michael J. Byun,Anka Reuel,Xavier Roberts-Gaal,Rachel Calcott,Evie Coxon,Chinmay Deshpande*

Main category: cs.AI

TL;DR: 本文主张在基础模型评估中采用更严谨和透明的人类基线,以更好比较人类与AI性能,并提出了建议和报告清单。


<details>
  <summary>Details</summary>
Motivation: 现有的人类基线方法不够严谨且缺乏透明性,无法可靠衡量和评估性能差异,影响了AI评估的准确性。

Method: 通过元综述测量理论和AI评估文献,提出了设计、执行和报告人类基线的框架与建议,并制定了检查清单,用于系统审查115项基线研究。

Result: 发现现有基线方法的不足,提供检查清单以协助研究者进行基线和报告结果。

Conclusion: 本文工作有助于推动更严谨的AI评估实践,服务于研究社区和政策制定者。

Abstract: In this position paper, we argue that human baselines in foundation model
evaluations must be more rigorous and more transparent to enable meaningful
comparisons of human vs. AI performance, and we provide recommendations and a
reporting checklist towards this end. Human performance baselines are vital for
the machine learning community, downstream users, and policymakers to interpret
AI evaluations. Models are often claimed to achieve "super-human" performance,
but existing baselining methods are neither sufficiently rigorous nor
sufficiently well-documented to robustly measure and assess performance
differences. Based on a meta-review of the measurement theory and AI evaluation
literatures, we derive a framework with recommendations for designing,
executing, and reporting human baselines. We synthesize our recommendations
into a checklist that we use to systematically review 115 human baselines
(studies) in foundation model evaluations and thus identify shortcomings in
existing baselining methods; our checklist can also assist researchers in
conducting human baselines and reporting results. We hope our work can advance
more rigorous AI evaluation practices that can better serve both the research
community and policymakers. Data is available at:
https://github.com/kevinlwei/human-baselines

</details>
