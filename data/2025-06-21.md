<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 20]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 7]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2506.14866)
*Thomas Kuntz,Agatha Duzan,Hao Zhao,Francesco Croce,Zico Kolter,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.SE

TL;DR: 论文介绍了一个名为OS-Harm的新基准测试，用于评估基于LLM的计算机使用代理的安全性，并提出了自动化评估方法。


<details>
  <summary>Details</summary>
Motivation: 研究计算机使用代理的安全性，因为其潜力虽大，但安全性评估不足，可能阻碍广泛应用。

Method: 在OSWorld环境中创建150个任务，覆盖三类危害（用户滥用、提示注入攻击、模型错误行为），并开发自动化评估工具。

Result: 所有测试模型均易受滥用查询和提示注入攻击，有时会执行不安全操作。

Conclusion: OS-Harm为计算机代理的安全性提供了有效评估工具，揭示了现有模型的安全性缺陷。

Abstract: Computer use agents are LLM-based agents that can directly interact with a
graphical user interface, by processing screenshots or accessibility trees.
While these systems are gaining popularity, their safety has been largely
overlooked, despite the fact that evaluating and understanding their potential
for harmful behavior is essential for widespread adoption. To address this gap,
we introduce OS-Harm, a new benchmark for measuring safety of computer use
agents. OS-Harm is built on top of the OSWorld environment and aims to test
models across three categories of harm: deliberate user misuse, prompt
injection attacks, and model misbehavior. To cover these cases, we create 150
tasks that span several types of safety violations (harassment, copyright
infringement, disinformation, data exfiltration, etc.) and require the agent to
interact with a variety of OS applications (email client, code editor, browser,
etc.). Moreover, we propose an automated judge to evaluate both accuracy and
safety of agents that achieves high agreement with human annotations (0.76 and
0.79 F1 score). We evaluate computer use agents based on a range of frontier
models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide
insights into their safety. In particular, all models tend to directly comply
with many deliberate misuse queries, are relatively vulnerable to static prompt
injections, and occasionally perform unsafe actions. The OS-Harm benchmark is
available at https://github.com/tml-epfl/os-harm.

</details>


### [2] [An Empirical Study of Bugs in Data Visualization Libraries](https://arxiv.org/abs/2506.15084)
*Weiqi Lu,Yongqiang Tian,Xiaohan Zhong,Haoyang Ma,Zhenyang Xu,Shing-Chi Cheung,Chengnian Sun*

Main category: cs.SE

TL;DR: 该研究首次对数据可视化（DataViz）库中的错误进行了全面分析，总结了症状、根本原因及分类，并探讨了视觉语言模型（VLMs）在检测错误中的效果。


<details>
  <summary>Details</summary>
Motivation: 数据可视化库的准确性对信息传递和用户决策至关重要，但其错误可能导致误导性视觉表示，因此需深入研究其错误特性。

Method: 研究分析了五个常用DataViz库中的564个错误，包括症状、根本原因和分类，并测试了VLMs在检测错误中的效果。

Result: 研究发现不正确/不准确的绘图普遍存在，图形计算错误是主要根源；VLMs的检测效果在29%至57%之间，提示信息增加不一定提高效果。

Conclusion: 研究为DataViz库的自动化测试提供了重要见解，并为未来研究设计有效测试技术提供了基础。

Abstract: Data visualization (DataViz) libraries play a crucial role in presentation,
data analysis, and application development, underscoring the importance of
their accuracy in transforming data into visual representations. Incorrect
visualizations can adversely impact user experience, distort information
conveyance, and influence user perception and decision-making processes. Visual
bugs in these libraries can be particularly insidious as they may not cause
obvious errors like crashes, but instead mislead users of the underlying data
graphically, resulting in wrong decision making. Consequently, a good
understanding of the unique characteristics of bugs in DataViz libraries is
essential for researchers and developers to detect and fix bugs in DataViz
libraries.
  This study presents the first comprehensive analysis of bugs in DataViz
libraries, examining 564 bugs collected from five widely-used libraries. Our
study systematically analyzes their symptoms and root causes, and provides a
detailed taxonomy. We found that incorrect/inaccurate plots are pervasive in
DataViz libraries and incorrect graphic computation is the major root cause,
which necessitates further automated testing methods for DataViz libraries.
Moreover, we identified eight key steps to trigger such bugs and two test
oracles specific to DataViz libraries, which may inspire future research in
designing effective automated testing techniques. Furthermore, with the recent
advancements in Vision Language Models (VLMs), we explored the feasibility of
applying these models to detect incorrect/inaccurate plots. The results show
that the effectiveness of VLMs in bug detection varies from 29% to 57%,
depending on the prompts, and adding more information in prompts does not
necessarily increase the effectiveness. More findings can be found in our
manuscript.

</details>


### [3] [Program Feature-based Fuzzing Benchmarking](https://arxiv.org/abs/2506.15088)
*Miao Miao*

Main category: cs.SE

TL;DR: 提出了一种新的基准测试，通过可配置的细粒度程序特征来评估模糊测试的性能，发现模糊测试效果受程序特征影响显著。


<details>
  <summary>Details</summary>
Motivation: 弥补传统模糊测试评估中未考虑细粒度程序特征的不足。

Method: 提取7个控制流和数据流相关的程序特征，生成包含153个程序的基准测试，评估11种流行模糊测试工具。

Result: 模糊测试效果因程序特征及其强度而异。

Conclusion: 模糊测试评估应考虑程序特征。

Abstract: Fuzzing is a powerful software testing technique renowned for its
effectiveness in identifying software vulnerabilities. Traditional fuzzing
evaluations typically focus on overall fuzzer performance across a set of
target programs, yet few benchmarks consider how fine-grained program features
influence fuzzing effectiveness. To bridge this gap, we introduce a novel
benchmark designed to generate programs with configurable, fine-grained program
features to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing
studies, extracting 7 program features related to control-flow and data-flow
that can impact fuzzer performance. Using these features, we generated a
benchmark consisting of 153 programs controlled by 10 fine-grained configurable
parameters. We evaluated 11 popular fuzzers using this benchmark. The results
indicate that fuzzer performance varies significantly based on the program
features and their strengths, highlighting the importance of incorporating
program characteristics into fuzzing evaluations.

</details>


### [4] [Enhancement Report Approval Prediction: A Comparative Study of Large Language Models](https://arxiv.org/abs/2506.15098)
*Haosheng Zuo,Feifei Niu,Chuanyi Li*

Main category: cs.SE

TL;DR: 本文通过评估18种大语言模型变体，证明其在软件改进报告（ERs）自动批准预测（ERAP）中的优越性，特别是在结合创作者信息和微调后效果显著。


<details>
  <summary>Details</summary>
Motivation: 手动处理软件改进报告费时且易丢失有价值建议，因此研究通过机器学习尤其是大语言模型（LLM）来自动化这一过程。

Method: 系统评估18种LLM变体（包括编码器和解码器模型），并与传统方法（如CNN/LSTM-BERT/GloVe）对比，引入创作者信息并采用LoRA微调。

Result: LLM模型表现优于传统方法，LoRA微调的Llama 3.1 8B Instruct达到79%准确率，显著提升召回率，并解决类别不平衡问题。

Conclusion: LLM是ERAP的优质解决方案，可优化软件维护流程，同时研究总结了模型表现不佳的案例，为未来研究提供方向。

Abstract: Enhancement reports (ERs) serve as a critical communication channel between
users and developers, capturing valuable suggestions for software improvement.
However, manually processing these reports is resource-intensive, leading to
delays and potential loss of valuable insights. To address this challenge,
enhancement report approval prediction (ERAP) has emerged as a research focus,
leveraging machine learning techniques to automate decision-making. While
traditional approaches have employed feature-based classifiers and deep
learning models, recent advancements in large language models (LLM) present new
opportunities for enhancing prediction accuracy. This study systematically
evaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and
XLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1
8B Instruct and DeepSeek-V3 for decoder models) against traditional methods
(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)
Incorporating creator profiles increases unfine-tuned decoder-only models'
overall accuracy by 10.8 percent though it may introduce bias; (2) LoRA
fine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79
percent accuracy and significantly enhancing recall for approved reports (76.1
percent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5
percent under strict chronological evaluation and effectively addressing class
imbalance issues. These findings establish LLM as a superior solution for ERAP,
demonstrating their potential to streamline software maintenance workflows and
improve decision-making in real-world development environments. We also
investigated and summarized the ER cases where the large models underperformed,
providing valuable directions for future research.

</details>


### [5] [Towards Bug-Free Distributed Go Programs](https://arxiv.org/abs/2506.15135)
*Zhengqun Koo*

Main category: cs.SE

TL;DR: 这篇论文提出了一种验证框架，用于证明使用Go语言子集的分布式程序中通信竞争的缺失。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中的程序员需要处理并发问题以避免竞争，但并发问题难以分析，常导致竞争带来的错误。

Method: 通过静态分析分布式程序的执行，扩展happens-before顺序以涵盖缓冲和非缓冲通道。

Result: 该框架能够证明通信竞争的缺失。

Conclusion: 该方法为分布式程序的通信竞争问题提供了一种有效的静态验证解决方案。

Abstract: Programmers of distributed systems need to reason about concurrency to avoid
races. However, reasoning about concurrency is difficult, and unexpected races
show up as bugs. Data race detection in shared memory systems is well-studied
(dynamic data race detection [13], behavioral types [15], dynamic race
detection [31]). Similar to how a data race consists of reads and writes not
related by happens-before at a shared memory location, a communication race
consists of receives and sends not related by happens-before on a shared
channel. Communication races are problematic: a receiver expects a specific
message from a specific sender, but with a communication race, the receiver can
receive a message meant for another receiver, or not receive anything at all.
In this work, we describe a verification framework that can prove the absence
of communication races for distributed programs that use a subset of the Go
programming language, where synchronization is mainly achieved via message
passing. We statically reason about how a distributed program executes, using a
happens-before order, extended to buffered and unbuffered channels.

</details>


### [6] [Advanced approach for Agile/Scrum Process: RetroAI++](https://arxiv.org/abs/2506.15172)
*Maria Spichkova,Kevin Iwan,Madeleine Zwart,Hina Lee,Yuwon Yoon,Xiaohan Qin*

Main category: cs.SE

TL;DR: 论文介绍了一个名为RetroAI++的原型工具，旨在利用AI技术自动化并优化敏捷/Scrum开发中的冲刺规划和回顾分析。


<details>
  <summary>Details</summary>
Motivation: 支持软件开发者更高效地进行敏捷/Scrum开发中的关键活动，如冲刺规划和回顾分析。

Method: 基于新兴智能技术开发RetroAI++原型工具，利用AI提供自动化建议和深入洞察。

Result: 工具能够为冲刺组织和回顾反思提供智能建议。

Conclusion: RetroAI++通过AI技术优化敏捷/Scrum流程，提升了项目管理效率。

Abstract: In Agile/Scrum software development, sprint planning and retrospective
analysis are the key elements of project management. The aim of our work is to
support software developers in these activities. In this paper, we present our
prototype tool RetroAI++, based on emerging intelligent technologies. In our
RetroAI++ prototype, we aim to automate and refine the practical application of
Agile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI
insights, our prototype aims to automate and refine the many processes involved
in the Sprint Planning, Development and Retrospective stages of Agile/Scrum
development projects, offering intelligent suggestions for sprint organisation
as well as meaningful insights for retrospective reflection.

</details>


### [7] [Large Language Models for Unit Testing: A Systematic Literature Review](https://arxiv.org/abs/2506.15227)
*Quanjun Zhang,Chunrong Fang,Siqi Gu,Ye Shang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: 这篇论文综述了大型语言模型（LLMs）在单元测试中的应用，分析了现有研究并进行系统分类，同时探讨了挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在单元测试中的应用研究快速发展，研究者难以全面了解其成果与挑战，因此需要系统综述。

Method: 通过对截至2025年3月的相关论文进行分析，从单元测试和LLMs的角度分类任务，并讨论模型使用、适应策略及混合方法。

Result: 总结了LLMs在单元测试中的关键挑战和未来研究方向，为社区提供了系统性的研究概览。

Conclusion: 论文为单元测试领域提供了全面的研究成果总结，推动未来研究发展，相关资源已开源。

Abstract: Unit testing is a fundamental practice in modern software engineering, with
the aim of ensuring the correctness, maintainability, and reliability of
individual software components. Very recently, with the advances in Large
Language Models (LLMs), a rapidly growing body of research has leveraged LLMs
to automate various unit testing tasks, demonstrating remarkable performance
and significantly reducing manual effort. However, due to ongoing explorations
in the LLM-based unit testing field, it is challenging for researchers to
understand existing achievements, open challenges, and future opportunities.
This paper presents the first systematic literature review on the application
of LLMs in unit testing until March 2025. We analyze \numpaper{} relevant
papers from the perspectives of both unit testing and LLMs. We first categorize
existing unit testing tasks that benefit from LLMs, e.g., test generation and
oracle generation. We then discuss several critical aspects of integrating LLMs
into unit testing research, including model usage, adaptation strategies, and
hybrid approaches. We further summarize key challenges that remain unresolved
and outline promising directions to guide future research in this area.
Overall, our paper provides a systematic overview of the research landscape to
the unit testing community, helping researchers gain a comprehensive
understanding of achievements and promote future research. Our artifacts are
publicly available at the GitHub repository:
https://github.com/iSEngLab/AwesomeLLM4UT.

</details>


### [8] [Uncovering Intention through LLM-Driven Code Snippet Description Generation](https://arxiv.org/abs/2506.15453)
*Yusuf Sulistyo Nugroho,Farah Danisha Salam,Brittany Reid,Raula Gaikovina Kula,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 论文研究了代码片段的文档描述类型，并评估了Llama模型在生成描述时的表现。研究发现描述多为示例用途，且LLM能较好地识别和生成相关描述，但仍存在改进空间。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的兴起，研究开发者常用的代码片段描述类型，并评估LLM（如Llama）在支持描述生成方面的效果。

Method: 使用NPM代码片段数据集（185,412个包和1,024,579个代码片段），选取400个样本进行手动分类和LLM生成描述对比。

Result: 大多数原始描述（55.5%）为示例用途。LLM能正确识别79.75%的示例描述，生成描述的平均相似度为0.7173。

Conclusion: 代码片段的文档意图因用途而异，LLM在生成描述方面表现尚可，但仍有提升空间。

Abstract: Documenting code snippets is essential to pinpoint key areas where both
developers and users should pay attention. Examples include usage examples and
other Application Programming Interfaces (APIs), which are especially important
for third-party libraries. With the rise of Large Language Models (LLMs), the
key goal is to investigate the kinds of description developers commonly use and
evaluate how well an LLM, in this case Llama, can support description
generation. We use NPM Code Snippets, consisting of 185,412 packages with
1,024,579 code snippets. From there, we use 400 code snippets (and their
descriptions) as samples. First, our manual classification found that the
majority of original descriptions (55.5%) highlight example-based usage. This
finding emphasizes the importance of clear documentation, as some descriptions
lacked sufficient detail to convey intent. Second, the LLM correctly identified
the majority of original descriptions as "Example" (79.75%), which is identical
to our manual finding, showing a propensity for generalization. Third, compared
to the originals, the produced description had an average similarity score of
0.7173, suggesting relevance but room for improvement. Scores below 0.9
indicate some irrelevance. Our results show that depending on the task of the
code snippet, the intention of the document may differ from being instructions
for usage, installations, or descriptive learning examples for any user of a
library.

</details>


### [9] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: RAG在代码生成中很重要，但现有的基于行的分块方法可能破坏语义结构。作者提出了一种基于AST的结构感知分块方法，显著提升了代码生成任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）管道中的分块方法（如基于行的分块）可能破坏代码的语义结构，导致生成质量下降。

Method: 提出了一种基于抽象语法树（AST）的结构感知分块方法（\ourwork），递归地将大AST节点分成小块并合并同级节点，同时遵守大小限制。

Result: 该方法在多编程语言和任务中生成语义连贯的单元，显著提升了性能（如Recall@5提升4.3点，Pass@1提升2.67点）。

Conclusion: 结构感知的分块方法对扩展检索增强的代码智能非常重要。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [10] [A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs](https://arxiv.org/abs/2506.15174)
*Hossein Albakri,Kazem Cheshmi*

Main category: cs.PL

TL;DR: 本文提出了一种新的编译器转换（enumerate-and-sparse-coarsen），用于加速GPU上的稀疏矩阵乘法（SPMM），提高数据在寄存器和缓存中的复用，并平衡GPU计算资源负载。实验显示在A100 GPU上实现了1.84×到2.27×的速度提升。


<details>
  <summary>Details</summary>
Motivation: 稀疏数据结构用于减少神经网络内存占用，但在GPU上运行时因内存访问不规律导致资源利用效率低下。

Method: 提出一种编译器转换技术，优化稀疏矩阵乘法的数据复用和计算资源分配。

Result: 在A100 GPU上测试，稀疏矩阵乘法速度提升1.84×到2.27×。

Conclusion: 该方法有效解决了稀疏数据结构在GPU上的效率问题，适用于卷积和Transformer模型。

Abstract: Sparse data structures are commonly used in neural networks to reduce the
memory footprint. These data structures are compact but cause irregularities
such as random memory accesses, which prevent efficient use of the memory
hierarchy. GPUs are a common platform for machine learning practitioners, but
running compact data structures on these devices often leads to slow-downs due
to inefficient use of computing and memory resources. This paper proposes a new
compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse
matrix-matrix multiplication (SPMM) on GPU devices. The transformation
increases data reuse in registers and caches while creating more balanced
workloads for GPU computing resources. The transformation is tested on sparse
neural networks in convolutional and transformer models. On an A100 GPU and
across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to
128, the transformation yields a geometric mean speedup of 1.84$\times$ to
2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

</details>


### [11] [PSM: Policy Synchronised Deterministic Memory](https://arxiv.org/abs/2506.15424)
*Michael Mendler,Marc Pouzet*

Main category: cs.PL

TL;DR: 论文提出了一种新的Haskell类型上下文PSM，用于实现并发且确定的内存共享。


<details>
  <summary>Details</summary>
Motivation: 现有的并发抽象（如MVar、TVar）与确定性编程（如IVar、LVar）无法同时满足要求，需要更高层次的内存抽象。

Method: 引入PSM（Policy Synchronised Memory）类型上下文，支持持久状态访问和确定性并发。

Result: PSM通过策略协调保证了并发访问的确定性和无竞争，同时支持抽象数据结构的共享。

Conclusion: PSM为Haskell提供了一种既能实现确定性并行又能共享资源的内存抽象。

Abstract: Concurrency and determinacy do not go well with each other when resources
must be shared. Haskell provides parallel programming abstractions such as IVar
and LVar in the Par monad and concurrent abstractions such as MVar and TVar in
the in IO and STM monads, respectively. The former are determinate but have no
destructive updates and the latter have destructive updates but do not
guarantee determinacy. Programming patterns that are both concurrent and
determinate, such as those provided by Kahn or Berry require memory
abstractions at a higher level than is currently available. In this paper we
describe a new type context PSM for policy synchronised memory in Haskell. Like
STM and IO, the computations in PSM can access persistent state and, as a
side-effect, update the memory in imperative style. Like the Par and IO monads,
PSM supports concurrent threads and shared state. However, in contrast to IO,
our PSM contexts are race-free since concurrent accesses are policy coordinated
which guarantees determinacy.Well-typed transactions in the PSM context can
accommodate abstract data structures that are imperative, concurrently
shareable and still behave deterministically, by construction.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [12] [Atys: An Efficient Profiling Framework for Identifying Hotspot Functions in Large-scale Cloud Microservices](https://arxiv.org/abs/2506.15523)
*Jiaqi Sun,Dingyu Yang,Shiyou Qian,Jian Cao,Guangtao Xue*

Main category: cs.PF

TL;DR: 论文提出Atys1框架，用于高效分析大规模分布式服务的热点函数，具有多语言适配、两级聚合、选择性修剪和动态采样调整四大特性，实验显示显著提升了效率和降低了成本。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式服务中，即使性能的小幅提升也能带来显著成本节约，因此需要一个高效的性能分析框架来识别热点函数。

Method: 介绍了Atys1框架，包括多语言适配机制、两级聚合方法、选择性修剪策略和动态采样调整方案。

Result: 实验表明，选择性修剪策略减少6.8%时间且仅0.58%误差；动态采样调整方案保持高精度同时降低成本87.6%。

Conclusion: Atys1高效且低成本地提升了大规模分布式服务的性能分析能力。

Abstract: To handle the high volume of requests, large-scale services are comprised of
thousands of instances deployed in clouds. These services utilize diverse
programming languages and are distributed across various nodes as encapsulated
containers. Given their vast scale, even minor performance enhancements can
lead to significant cost reductions. In this paper, we introduce Atys1, an
efficient profiling framework specifically designed to identify hotspot
functions within large-scale distributed services. Atys presents four key
features. First, it implements a language-agnostic adaptation mechanism for
multilingual microservices. Second, a two-level aggregation method is
introduced to provide a comprehensive overview of flamegraphs. Third, we
propose a function selective pruning (FSP) strategy to enhance the efficiency
of aggregating profiling results. Finally, we develop a frequency dynamic
adjustment (FDA) scheme that dynamically modifies sampling frequency based on
service status, effectively minimizing profiling cost while maintaining
accuracy. Cluster-scale experiments on two benchmarks show that the FSP
strategy achieves a 6.8% reduction in time with a mere 0.58% mean average
percentage error (MAPE) in stack traces aggregation. Additionally, the FDA
scheme ensures that the mean squared error (MSE) remains on par with that at
high sampling rates, while achieving an 87.6% reduction in cost.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [13] [CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in Industrial URLLC](https://arxiv.org/abs/2506.14987)
*Eman Alqudah,Ashfaq Khokhar*

Main category: cs.NI

TL;DR: 通过引入基于CNN的动态优先级预测机制，改进了LDP算法，在多小区多信道网络中提升了干扰协调效率。


<details>
  <summary>Details</summary>
Motivation: 确保大规模工业无线网络中超可靠低延迟通信（URLLC）的包级通信质量。

Method: 结合CNN和图着色技术，动态分配链路优先级，根据实时流量、传输机会和网络条件调整资源分配。

Result: 仿真结果显示，在三种网络配置下，SINR分别提升113%、94%和49%。

Conclusion: 该方法在复杂URLLC场景中表现出色，资源分配更高效，网络容量和可调度性显著提升。

Abstract: Ensuring packet-level communication quality is vital for ultra-reliable,
low-latency communications (URLLC) in large-scale industrial wireless networks.
We enhance the Local Deadline Partition (LDP) algorithm by introducing a
CNN-based dynamic priority prediction mechanism for improved interference
coordination in multi-cell, multi-channel networks. Unlike LDP's static
priorities, our approach uses a Convolutional Neural Network and graph coloring
to adaptively assign link priorities based on real-time traffic, transmission
opportunities, and network conditions. Assuming that first training phase is
performed offline, our approach introduced minimal overhead, while enabling
more efficient resource allocation, boosting network capacity, SINR, and
schedulability. Simulation results show SINR gains of up to 113\%, 94\%, and
49\% over LDP across three network configurations, highlighting its
effectiveness for complex URLLC scenarios.

</details>


### [14] [GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC](https://arxiv.org/abs/2506.15011)
*Eman Alqudah,Ashfaq Khokhar*

Main category: cs.NI

TL;DR: 论文提出了一种结合GCN和DQN的改进算法，用于提升大规模工业无线网络中多小区、多信道网络的干扰协调性能，显著优于传统方法和之前的CNN方法。


<details>
  <summary>Details</summary>
Motivation: 解决超可靠低延迟通信（URLLC）中包级通信质量的挑战，尤其是在多网络环境下静态优先级方法的不足。

Method: 通过Graph Convolutional Network（GCN）捕捉空间依赖关系，结合Deep Q-Network（DQN）动态学习链路优先级，实现自适应调度。

Result: 仿真结果显示，GCN-DQN模型在SINR指标上显著优于传统的LDP算法和之前的CNN方法。

Conclusion: GCN-DQN模型在满足URLLC复杂需求的同时，表现出极低的开销和优越的网络性能。

Abstract: Ensuring packet-level communication quality is vital for ultra-reliable,
low-latency communications (URLLC) in large-scale industrial wireless networks.
We enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph
Convolutional Network (GCN) integrated with a Deep Q-Network (DQN)
reinforcement learning framework for improved interference coordination in
multi-cell, multi-channel networks. Unlike LDP's static priorities, our
approach dynamically learns link priorities based on real-time traffic demand,
network topology, remaining transmission opportunities, and interference
patterns. The GCN captures spatial dependencies, while the DQN enables adaptive
scheduling decisions through reward-guided exploration. Simulation results show
that our GCN-DQN model achieves mean SINR improvements of 179.6\%, 197.4\%, and
175.2\% over LDP across three network configurations. Additionally, the GCN-DQN
model demonstrates mean SINR improvements of 31.5\%, 53.0\%, and 84.7\% over
our previous CNN-based approach across the same configurations. These results
underscore the effectiveness of our GCN-DQN model in addressing complex URLLC
requirements with minimal overhead and superior network performance.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [Omnidirectional Video Super-Resolution using Deep Learning](https://arxiv.org/abs/2506.14803)
*Arbind Agrahari Baniya,Tsz-Kwan Lee,Peter W. Eklund,Sunil Aryal*

Main category: cs.MM

TL;DR: 该论文提出了一种针对360度视频的超分辨率方法S3PO，通过构建新型数据集360VDS并设计专门的特征提取器和损失函数，解决了传统VSR技术在360度视频中的失真问题。


<details>
  <summary>Details</summary>
Motivation: 360度视频因分辨率限制导致视觉体验下降，传统VSR技术无法解决其投影失真问题，且缺乏相关数据集，促使研究者提出针对性解决方案。

Method: 构建360VDS数据集，提出S3PO模型，采用循环建模和注意力机制，结合针对球形失真的损失函数和特征提取器。

Result: S3PO在360度视频数据集上表现优于现有传统VSR及专用超分辨率模型，并通过逐步消融研究验证了各子组件的有效性。

Conclusion: S3PO通过针对性设计和优化，有效提升了360度视频的超分辨率性能，填补了传统技术在球形视频领域的不足。

Abstract: Omnidirectional Videos (or 360{\deg} videos) are widely used in Virtual
Reality (VR) to facilitate immersive and interactive viewing experiences.
However, the limited spatial resolution in 360{\deg} videos does not allow for
each degree of view to be represented with adequate pixels, limiting the visual
quality offered in the immersive experience. Deep learning Video
Super-Resolution (VSR) techniques used for conventional videos could provide a
promising software-based solution; however, these techniques do not tackle the
distortion present in equirectangular projections of 360{\deg} video signals.
An additional obstacle is the limited availability of 360{\deg} video datasets
for study. To address these issues, this paper creates a novel 360{\deg} Video
Dataset (360VDS) with a study of the extensibility of conventional VSR models
to 360{\deg} videos. This paper further proposes a novel deep learning model
for 360{\deg} Video Super-Resolution (360{\deg} VSR), called Spherical Signal
Super-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent
modelling with an attention mechanism, unbound from conventional VSR techniques
like alignment. With a purpose-built feature extractor and a novel loss
function addressing spherical distortion, S3PO outperforms most
state-of-the-art conventional VSR models and 360{\deg}~specific
super-resolution models on 360{\deg} video datasets. A step-wise ablation study
is presented to understand and demonstrate the impact of the chosen
architectural sub-components, targeted training and optimisation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [16] [See What I Mean? CUE: A Cognitive Model of Understanding Explanations](https://arxiv.org/abs/2506.14775)
*Tobias Labarta,Nhi Hoang,Katharina Weitz,Wojciech Samek,Sebastian Lapuschkin,Leander Weber*

Main category: cs.HC

TL;DR: 该论文提出了CUE模型，通过链接解释属性与认知子过程（如可读性、可理解性）来评估XAI的认知可访问性。研究发现，视觉障碍用户在热图任务中表现相当但信心/努力较低，且无障碍颜色映射未能改善甚至加剧了这一差距。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在关键决策中的应用增加，对可理解的解释需求日益增长。当前XAI评估多关注技术保真度，而忽视了认知可访问性，尤其是对视觉障碍用户的影响。

Method: 提出CUE模型，将解释属性与认知子过程（可读性、可理解性等）关联。在N=455的研究中，测试不同颜色映射（BWR、Cividis、Coolwarm）的热图对用户的影响。

Result: 虽然任务表现相似，但视觉障碍用户的信心和努力程度较低。无障碍颜色映射（如Cividis）未改善甚至加剧了这一差距。结果表明，解释的清晰度直接影响理解性。

Conclusion: 研究验证了CUE模型的有效性，并强调了自适应XAI界面的重要性。贡献包括：(1)认知模型，(2)解释属性的定义，(3)支持无障碍XAI的实证证据。

Abstract: As machine learning systems increasingly inform critical decisions, the need
for human-understandable explanations grows. Current evaluations of Explainable
AI (XAI) often prioritize technical fidelity over cognitive accessibility which
critically affects users, in particular those with visual impairments. We
propose CUE, a model for Cognitive Understanding of Explanations, linking
explanation properties to cognitive sub-processes: legibility (perception),
readability (comprehension), and interpretability (interpretation). In a study
(N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we
found comparable task performance but lower confidence/effort for visually
impaired users. Unlike expected, these gaps were not mitigated and sometimes
worsened by accessibility-focused color maps like Cividis. These results
challenge assumptions about perceptual optimization and support the need for
adaptive XAI interfaces. They also validate CUE by demonstrating that altering
explanation legibility affects understandability. We contribute: (1) a
formalized cognitive model for explanation understanding, (2) an integrated
definition of human-centered explanation properties, and (3) empirical evidence
motivating accessible, user-tailored XAI.

</details>


### [17] [WebXAII: an open-source web framework to study human-XAI interaction](https://arxiv.org/abs/2506.14777)
*Jules Leguy,Pierre-Antoine Jean,Felipe Torres Figueroa,Sébastien Harispe*

Main category: cs.HC

TL;DR: WebXAII是一个开源网络框架，旨在促进人类与可解释人工智能（XAI）系统交互的研究，解决了现有研究中临时接口不共享和实验无法复现的问题。


<details>
  <summary>Details</summary>
Motivation: 由于AI（尤其是机器学习）广泛应用的社会影响越来越大，XAI领域迅速发展。现有研究中，人类与XAI交互的研究通常使用临时开发的接口，但这些接口未公开共享，限制了实验的重用性和复现性。

Method: 设计并实现了WebXAII，一种基于网络的平台，能够完整呈现实验协议，记录参与者响应。通过结构化配置文件定义架构，降低实现实验协议所需的编程技能要求。

Result: WebXAII成功复现了文献中的前沿研究协议，展示了其灵活性和有效性。

Conclusion: WebXAII为XAI交互研究提供了一个可重用、易复现的解决方案，推动了该领域的进一步研究。

Abstract: This article introduces WebXAII, an open-source web framework designed to
facilitate research on human interaction with eXplainable Artificial
Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by
the growing societal implications of the widespread adoption of AI (and in
particular machine learning) across diverse applications. Researchers who study
the interaction between humans and XAI techniques typically develop ad hoc
interfaces in order to conduct their studies. These interfaces are usually not
shared alongside the results of the studies, which limits their reusability and
the reproducibility of experiments. In response, we design and implement
WebXAII, a web-based platform that can embody full experimental protocols,
meaning that it can present all aspects of the experiment to human participants
and record their responses. The experimental protocols are translated into a
composite architecture of generic views and modules, which offers a lot of
flexibility. The architecture is defined in a structured configuration file, so
that protocols can be implemented with minimal programming skills. We
demonstrate that WebXAII can effectively embody relevant protocols, by
reproducing the protocol of a state-of-the-art study of the literature. The
framework is available at https://github.com/PAJEAN/WebXAII.

</details>


### [18] [Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust](https://arxiv.org/abs/2506.14799)
*Evdoxia Taka,Debadyuti Bhattacharya,Joanne Garde-Hansen,Sanjay Sharma,Tanaya Guha*

Main category: cs.HC

TL;DR: 该研究开发了一个基于CLIP模型的AI工具，用于分析影视内容中角色的性别和年龄分布，并通过用户研究评估其实用性和可信度。


<details>
  <summary>Details</summary>
Motivation: 过去的AI研究虽然能够量化媒体内容中的角色分布，但缺乏对公众实用性和信任度的验证。

Method: 使用CLIP模型分析视觉数据，设计可视化工具，并进行用户研究。

Result: 用户认为工具总体上实用，但对AI模型的信任度中等偏低。

Conclusion: 尽管用户对AI生成的性别和年龄分析的信任度有限，但他们对这种工具的应用持开放态度。

Abstract: Recent advances in AI has enabled automated analysis of complex media content
at scale and generate actionable insights regarding character representation
along such dimensions as gender and age. Past work focused on quantifying
representation from audio/video/text using various ML models, but without
having the audience in the loop. We ask, even if character distribution along
demographic dimensions are available, how useful are they to the general
public? Do they actually trust the numbers generated by AI models? Our work
addresses these questions through a user study, while proposing a new AI-based
character representation and visualization tool. Our tool based on the
Contrastive Language Image Pretraining (CLIP) foundation model to analyze
visual screen data to quantify character representation across dimensions of
age and gender. We also designed effective visualizations suitable for
presenting such analytics to lay audience. Next, we conducted a user study to
seek empirical evidence on the usefulness and trustworthiness of the
AI-generated results for carefully chosen movies presented in the form of our
visualizations. We note that participants were able to understand the analytics
from our visualization, and deemed the tool `overall useful'. Participants also
indicated a need for more detailed visualizations to include more demographic
categories and contextual information of the characters. Participants' trust in
AI-based gender and age models is seen to be moderate to low, although they
were not against the use of AI in this context. Our tool including code,
benchmarking, and data from the user study can be found here:
https://anonymous.4open.science/r/Character-Representation-Media-FF7B

</details>


### [19] [Impact of a Deployed LLM Survey Creation Tool through the IS Success Model](https://arxiv.org/abs/2506.14809)
*Peng Jiang,Vinicius Cezar Monteiro de Lira,Antonio Maiorino*

Main category: cs.HC

TL;DR: 本文介绍了利用大语言模型（LLM）自动化生成高质量调查系统的实际部署，并评估其对信息系统的贡献。


<details>
  <summary>Details</summary>
Motivation: 调查是信息系统研究的基础，但创建高质量调查耗时耗力。LLM的发展为自动化调查生成提供了新机遇。

Method: 部署了LLM驱动的系统，结合DeLone和McLean信息系统成功模型进行评估，并提出了混合评估框架和安全措施。

Result: 系统成功加速了数据收集，同时保持了调查质量，并通过混合评估框架和安全措施确保了实际应用的可行性。

Conclusion: LLM驱动的调查生成系统为信息系统研究提供了高效且可靠的解决方案，展示了生成AI在核心研究方法中的潜力。

Abstract: Surveys are a cornerstone of Information Systems (IS) research, yet creating
high-quality surveys remains labor-intensive, requiring both domain expertise
and methodological rigor. With the evolution of large language models (LLMs),
new opportunities emerge to automate survey generation. This paper presents the
real-world deployment of an LLM-powered system designed to accelerate data
collection while maintaining survey quality. Deploying such systems in
production introduces real-world complexity, including diverse user needs and
quality control. We evaluate the system using the DeLone and McLean IS Success
Model to understand how generative AI can reshape a core IS method. This study
makes three key contributions. To our knowledge, this is the first application
of the IS Success Model to a generative AI system for survey creation. In
addition, we propose a hybrid evaluation framework combining automated and
human assessments. Finally, we implement safeguards that mitigate
post-deployment risks and support responsible integration into IS workflows.

</details>


### [20] [Navigating High-Dimensional Backstage: A Guide for Exploring Literature for the Reliable Use of Dimensionality Reduction](https://arxiv.org/abs/2506.14820)
*Hyeon Jeon,Hyunwook Lee,Yun-Hsin Kuo,Taehyun Yang,Daniel Archambault,Sungahn Ko,Takanori Fujiwara,Kwan-Liu Ma,Jinwook Seo*

Main category: cs.HC

TL;DR: 该论文提出了一个阅读指南，帮助新手评估其降维（DR）专业水平并选择适合的文献，以提高视觉分析的可靠性。


<details>
  <summary>Details</summary>
Motivation: 由于降维技术可能因固有失真等问题导致视觉分析不可靠，现有文献虽多样化但让新手感到困惑，因此需要一个系统化的阅读指南。

Method: 基于文献分类，作者开发了一个指南，帮助用户评估自身DR专业水平并选择相关文献，并通过专家访谈验证其有效性。

Result: 专家访谈证实了该指南的重要性、全面性和实用性。

Conclusion: 该论文的指南为新手提供了一个有效的工具，以系统化地学习和改进DR技术在视觉分析中的应用。

Abstract: Visual analytics using dimensionality reduction (DR) can easily be unreliable
for various reasons, e.g., inherent distortions in representing the original
data. The literature has thus proposed a wide range of methodologies to make
DR-based visual analytics reliable. However, the diversity and extensiveness of
the literature can leave novice analysts and researchers uncertain about where
to begin and proceed. To address this problem, we propose a guide for reading
papers for reliable visual analytics with DR. Relying on the previous
classification of the relevant literature, our guide helps both practitioners
to (1) assess their current DR expertise and (2) identify papers that will
further enhance their understanding. Interview studies with three experts in DR
and data visualizations validate the significance, comprehensiveness, and
usefulness of our guide.

</details>


### [21] [The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities](https://arxiv.org/abs/2506.14829)
*Aditya Majumdar,Wenbo Zhang,Kashvi Prawal,Amulya Yadav*

Main category: cs.HC

TL;DR: 论文探讨AI4SI项目在实现社会影响时面临的挑战，并提出实用策略。


<details>
  <summary>Details</summary>
Motivation: 解决AI4SI项目从概念验证到实际应用中遇到的难题。

Method: 通过半结构化访谈和作者经验，分析AI4SI项目的障碍。

Result: 识别出结构、沟通、协作和操作等关键障碍，并提出优化策略。

Conclusion: 为AI4SI研究者提供实用指南，促进更有效的社会影响合作。

Abstract: In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects
focus on harnessing AI to address societal issues in areas such as healthcare,
social justice, etc. Unfortunately, despite growing interest in AI4SI,
achieving tangible, on-the-ground impact remains a significant challenge. For
example, identifying and engaging motivated collaborators who are willing to
co-design and deploy AI based solutions in real-world settings is often
difficult. Even when such partnerships are established, many AI4SI projects
"fail" to progress beyond the proof-of-concept stage, and hence, are unable to
transition to at-scale production-level solutions. Furthermore, the unique
challenges faced by AI4SI researchers are not always fully recognized within
the broader AI community, where such work is sometimes viewed as primarily
applied and not aligning with the traditional criteria for novelty emphasized
in core AI venues. This paper attempts to shine a light on the diverse
challenges faced in AI4SI research by diagnosing a multitude of factors that
prevent AI4SI partnerships from achieving real-world impact on the ground.
Drawing on semi-structured interviews with six leading AI4SI researchers -
complemented by the authors' own lived experiences in conducting AI4SI research
- this paper attempts to understand the day-to-day difficulties faced in
developing and deploying socially impactful AI solutions. Through thematic
analysis, we identify structural and organizational, communication,
collaboration, and operational challenges as key barriers to deployment. While
there are no easy fixes, we synthesize best practices and actionable strategies
drawn from these interviews and our own work in this space. In doing so, we
hope this paper serves as a practical reference guide for AI4SI researchers and
partner organizations seeking to engage more effectively in socially impactful
AI collaborations.

</details>


### [22] [Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework](https://arxiv.org/abs/2506.14948)
*Mohna Chakraborty,Lu Wang,David Jurgens*

Main category: cs.HC

TL;DR: 提出了一个基于价值的方法框架，用于评估和改进大型语言模型（LLMs）在道德推理中的表现。通过多种道德数据集和提示分类，研究发现结构化道德提示能显著提升模型决策能力，并实现从小模型到大模型的知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在道德推理领域表现较浅且与人类推理不一致，缺乏对上下文的深度理解，导致在复杂道德场景中出现偏见决策。

Method: 研究通过分类提示（基于价值系统、伦理理论和认知策略）评估了12个开源模型在四个道德数据集上的表现，并设计了四个关键问题来验证研究假设。

Result: 研究发现，结构化道德提示（尤其是基于第一原则推理和Schwartz+关怀伦理框架）能显著提升模型准确性和一致性，且通过监督蒸馏方法可将大型模型的道德能力高效转移至小型模型。

Conclusion: 该研究为构建可解释且基于价值的模型提供了可扩展的路径，证明了结构化道德提示和蒸馏方法的有效性。

Abstract: Large language models (LLMs) are increasingly deployed in domains requiring
moral understanding, yet their reasoning often remains shallow, and misaligned
with human reasoning. Unlike humans, whose moral reasoning integrates
contextual trade-offs, value systems, and ethical theories, LLMs often rely on
surface patterns, leading to biased decisions in morally and ethically complex
scenarios. To address this gap, we present a value-grounded framework for
evaluating and distilling structured moral reasoning in LLMs. We benchmark 12
open-source models across four moral datasets using a taxonomy of prompts
grounded in value systems, ethical theories, and cognitive reasoning
strategies. Our evaluation is guided by four questions: (1) Does reasoning
improve LLM decision-making over direct prompting? (2) Which types of
value/ethical frameworks most effectively guide LLM reasoning? (3) Which
cognitive reasoning strategies lead to better moral performance? (4) Can
small-sized LLMs acquire moral competence through distillation? We find that
prompting with explicit moral structure consistently improves accuracy and
coherence, with first-principles reasoning and Schwartz's + care-ethics
scaffolds yielding the strongest gains. Furthermore, our supervised
distillation approach transfers moral competence from large to small models
without additional inference cost. Together, our results offer a scalable path
toward interpretable and value-grounded models.

</details>


### [23] [Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output](https://arxiv.org/abs/2506.15008)
*Richa Gupta,Alexander Htet Kyaw*

Main category: cs.HC

TL;DR: 该论文提出了一种将DALL-E 3与材料数据集结合的新方法，以增强AI生成设计的可持续性评估，并通过用户测试验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在室内设计中快速生成视觉图像，但缺乏可持续性数据支持设计决策，因此需要结合材料数据以提供实用见解。

Method: 通过后处理模块分析AI生成的图像，识别材料并匹配CO2e数据，用户测试比较了三种不同的可持续性提示方式。

Result: 引入可持续性指标能促进更环保的设计决策，但也可能导致决策疲劳，大多数用户表示会将可持续性原则纳入工作流程。

Conclusion: 平衡设计自由与实用性是AI辅助建筑设计的关键，为数据驱动的生态设计提供了明确路径。

Abstract: Generative AI, specifically text-to-image models, have revolutionized
interior architectural design by enabling the rapid translation of conceptual
ideas into visual representations from simple text prompts. While generative AI
can produce visually appealing images they often lack actionable data for
designers In this work, we propose a novel pipeline that integrates DALL-E 3
with a materials dataset to enrich AI-generated designs with sustainability
metrics and material usage insights. After the model generates an interior
design image, a post-processing module identifies the top ten materials present
and pairs them with carbon dioxide equivalent (CO2e) values from a general
materials dictionary. This approach allows designers to immediately evaluate
environmental impacts and refine prompts accordingly. We evaluate the system
through three user tests: (1) no mention of sustainability to the user prior to
the prompting process with generative AI, (2) sustainability goals communicated
to the user before prompting, and (3) sustainability goals communicated along
with quantitative CO2e data included in the generative AI outputs. Our
qualitative and quantitative analyses reveal that the introduction of
sustainability metrics in the third test leads to more informed design
decisions, however, it can also trigger decision fatigue and lower overall
satisfaction. Nevertheless, the majority of participants reported incorporating
sustainability principles into their workflows in the third test, underscoring
the potential of integrated metrics to guide more ecologically responsible
practices. Our findings showcase the importance of balancing design freedom
with practical constraints, offering a clear path toward holistic, data-driven
solutions in AI-assisted architectural design.

</details>


### [24] [Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers](https://arxiv.org/abs/2506.15047)
*Jiayue Melissa Shi,Dong Whi Yoo,Keran Wang,Violeta J. Rodriguez,Ravi Karkar,Koustuv Saha*

Main category: cs.HC

TL;DR: 论文探讨了利用AI聊天机器人Carey（基于GPT-4）为阿尔茨海默病及相关痴呆症（AD/ADRD）照顾者提供支持和心理健康帮助的可行性与挑战。通过访谈研究发现照顾者的需求主要集中在信息获取、情感支持等方面，同时揭示了AI技术在此类应用中的优势和设计建议。


<details>
  <summary>Details</summary>
Motivation: AD/ADRD照顾者面临巨大的心理和情感压力，而生成式AI（如LLMs）在心理健康支持领域的潜力尚不明确。研究旨在填补这一空白，探索AI聊天机器人如何满足照顾者的需求。

Method: 开发了基于GPT-4的聊天机器人Carey，并通过16名照顾者的半结构化访谈（基于场景互动）收集数据，采用归纳编码和反思性主题分析法进行分析。

Result: 研究揭示了照顾者的六大需求主题（如信息获取、情感支持等）及其与AI技术相关的矛盾点，并提出了AI系统的设计建议。

Conclusion: 研究为设计支持AD/ADRD照顾者心理健康的AI系统提供了理论和实践指导，强调了技术的可信性和照顾者中心化的重要性。

Abstract: Family caregivers of individuals with Alzheimer's Disease and Related
Dementia (AD/ADRD) face significant emotional and logistical challenges that
place them at heightened risk for stress, anxiety, and depression. Although
recent advances in generative AI -- particularly large language models (LLMs)
-- offer new opportunities to support mental health, little is known about how
caregivers perceive and engage with such technologies. To address this gap, we
developed Carey, a GPT-4o-based chatbot designed to provide informational and
emotional support to AD/ADRD caregivers. Using Carey as a technology probe, we
conducted semi-structured interviews with 16 family caregivers following
scenario-driven interactions grounded in common caregiving stressors. Through
inductive coding and reflexive thematic analysis, we surface a systemic
understanding of caregiver needs and expectations across six themes --
on-demand information access, emotional support, safe space for disclosure,
crisis management, personalization, and data privacy. For each of these themes,
we also identified the nuanced tensions in the caregivers' desires and
concerns. We present a mapping of caregiver needs, AI chatbot's strengths,
gaps, and design recommendations. Our findings offer theoretical and practical
insights to inform the design of proactive, trustworthy, and caregiver-centered
AI systems that better support the evolving mental health needs of AD/ADRD
caregivers.

</details>


### [25] [Data Verbalisation: What is Text Doing in a Data Visualisation?](https://arxiv.org/abs/2506.15129)
*Paul Murrell*

Main category: cs.HC

TL;DR: 本文探讨了文本元素在数据可视化中的作用，提出需要类似非文本元素（如条形图、点和线）的简单连贯解释，并通过评估技术分析了文本的有效性，最终提出了一个易于理解和应用的框架。


<details>
  <summary>Details</summary>
Motivation: 目前对数据可视化中非文本元素的解释已经比较成熟，但对文本元素的理解缺乏类似的清晰框架，因此需要填补这一空白。

Method: 研究了文本在数据可视化中的使用示例，并利用现有知识和评估技术分析文本的有效性。

Result: 提出了一个易于理解和应用的框架，用于评估和理解数据可视化中文本元素的目的和效果。

Conclusion: 该框架有助于更好地理解和优化数据可视化中的文本元素，提升其效果。

Abstract: This article discusses the role that text elements play in a data
visualisation. We argue that there is a need for a simple, coherent explanation
of text elements similar to the understanding that already exists for non-text
elements like bars, points, and lines. We explore examples of how text is used
within a data visualisation and use existing knowledge and assessment
techniques to evaluate when text is effective and when it is not. The result is
a framework that aims to be easy to understand and easy to apply in order to
understand the purpose and effectiveness of the text elements in any data
visualisation.

</details>


### [26] [Accessible Gesture-Driven Augmented Reality Interaction System](https://arxiv.org/abs/2506.15189)
*Yikan Wang*

Main category: cs.HC

TL;DR: 该研究提出了一种基于手势的AR交互系统，利用深度学习识别手势，提升运动障碍用户的AR可访问性。


<details>
  <summary>Details</summary>
Motivation: 由于当前AR系统依赖精确输入，运动障碍用户难以使用，因此研究旨在通过手势识别技术提升AR的可访问性。

Method: 结合视觉变换器（ViTs）、时序卷积网络（TCNs）和图注意力网络（GATs）进行手势处理，使用联邦学习保护隐私，并通过强化学习优化界面。

Result: 实验表明，系统显著提升了任务完成效率（20%）和用户满意度（25%）。

Conclusion: 该系统有效提升了AR的可访问性和扩展性。

Abstract: Augmented reality (AR) offers immersive interaction but remains inaccessible
for users with motor impairments or limited dexterity due to reliance on
precise input methods. This study proposes a gesture-based interaction system
for AR environments, leveraging deep learning to recognize hand and body
gestures from wearable sensors and cameras, adapting interfaces to user
capabilities. The system employs vision transformers (ViTs), temporal
convolutional networks (TCNs), and graph attention networks (GATs) for gesture
processing, with federated learning ensuring privacy-preserving model training
across diverse users. Reinforcement learning optimizes interface elements like
menu layouts and interaction modes. Experiments demonstrate a 20% improvement
in task completion efficiency and a 25% increase in user satisfaction for
motor-impaired users compared to baseline AR systems. This approach enhances AR
accessibility and scalability. Keywords: Deep learning, Federated learning,
Gesture recognition, Augmented reality, Accessibility, Human-computer
interaction

</details>


### [27] [Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces](https://arxiv.org/abs/2506.15293)
*Francesco Chiossi,Julian Rasch,Robin Welsch,Albrecht Schmidt,Florian Michahelles*

Main category: cs.HC

TL;DR: 该论文提出了一种基于情境感知的框架，用于设计人机协作中的意图沟通策略，旨在提升信任、安全和效率。


<details>
  <summary>Details</summary>
Motivation: 随着机器人进入协作工作空间，确保人与机器人之间的相互理解成为信任、安全和效率的前提。

Method: 论文基于情境感知代理透明性（SAT）框架和任务抽象层次概念，提出了一个多维设计空间，涵盖意图内容、规划视野和沟通模态。

Result: 通过该设计空间，论文展示了如何设计适应动态协作工作场景的多模态沟通策略。

Conclusion: 论文为未来设计透明人机交互工具包奠定了概念基础，并提出了开放式问题和设计挑战，强调了多模态、自适应和可信赖的机器人协作在混合工作环境中的重要性。

Abstract: As robots enter collaborative workspaces, ensuring mutual understanding
between human workers and robotic systems becomes a prerequisite for trust,
safety, and efficiency. In this position paper, we draw on the cooperation
scenario of the AIMotive project in which a human and a cobot jointly perform
assembly tasks to argue for a structured approach to intent communication.
Building on the Situation Awareness-based Agent Transparency (SAT) framework
and the notion of task abstraction levels, we propose a multidimensional design
space that maps intent content (SAT1, SAT3), planning horizon (operational to
strategic), and modality (visual, auditory, haptic). We illustrate how this
space can guide the design of multimodal communication strategies tailored to
dynamic collaborative work contexts. With this paper, we lay the conceptual
foundation for a future design toolkit aimed at supporting transparent
human-robot interaction in the workplace. We highlight key open questions and
design challenges, and propose a shared agenda for multimodal, adaptive, and
trustworthy robotic collaboration in hybrid work environments.

</details>


### [28] [UXR Point of View on Product Feature Prioritization Prior To Multi-Million Engineering Commitments](https://arxiv.org/abs/2506.15294)
*Jonas Lau,Annie Tran*

Main category: cs.HC

TL;DR: 本文利用UXR PoV Playbook框架和MaxDiff方法改进产品功能优先级排序，减少调查负担并提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统调查方法在功能优先级排序中存在挑战，需可靠且高效的解决方案。

Method: 应用多分类逻辑回归（MaxDiff）方法，调整以减少调查响应数量。

Result: 通过案例研究验证了改进的MaxDiff方法在平板功能优先级排序中的有效性。

Conclusion: MaxDiff方法在减少调查负担的同时，能生成可靠的功能优先级列表。

Abstract: This paper discusses a popular UX research activity, feature prioritization,
using the User Experience Research Point of View (UXR PoV) Playbook framework.
We describe an application of multinomial logistic regression, frequently
marketed as MaxDiff, for prioritizing product features in consumer product
development. It addresses challenges of traditional surveying techniques. We
propose a solution using MaxDiff to generate a reliable preference list with a
reasonable sample size. We also adapt the MaxDiff method to reduce the number
of survey responses in half, making it less tedious from the survey takers'
perspective. We present a case study using the adapted MaxDiff method for
tablet feature prioritization research involving users with disabilities.

</details>


### [29] [Case Study for Developing a UXR Point of View for FinOps Product Innovation](https://arxiv.org/abs/2506.15314)
*Jason Dong,Anna Wu*

Main category: cs.HC

TL;DR: 通过混合研究方法开发用户研究视角，推动FinOps产品创新。


<details>
  <summary>Details</summary>
Motivation: 理解客户需求、协调跨职能团队、优先分配有限资源，以推动FinOps产品的创新。

Method: 采用定性和定量结合的多元研究方法，识别机会、量化痛点，细分客户群体。

Result: 形成了‘一站式’仪表盘，为FinOps从业者提供可操作的见解和工具。

Conclusion: 混合研究方法能揭示可操作的见解，推动产品创新。

Abstract: In the dynamic landscape of Cloud financial management, we are sharing a case
study exploring the development of a User Experience Research (UXR) Point of
View (PoV) to drive FinOps product innovation. We demonstrate how qualitative
and quantitative research methods working together to navigate the challenges
of understanding customer needs, aligning cross-functional teams, and
prioritizing limited resources. Through a multi-phased research approach, the
research team identifies opportunities, quantifies pain points, and segments
diverse customer cohorts. This culminated in a UXR PoV that informed the
creation of a differentiated product strategy, a 'one-stop shop' dashboard
empowering FinOps practitioners with actionable insights and tools. This case
study highlights the power of mixed-methods research in uncovering actionable
insights that drive impactful product innovation.

</details>


### [30] [Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook](https://arxiv.org/abs/2506.15325)
*Festus Adedoyin,Huseyin Dogan*

Main category: cs.HC

TL;DR: 论文探讨了人本AI在金融行业中的应用，展示了其如何通过数据分析和机器学习提升客户体验和个性化服务。


<details>
  <summary>Details</summary>
Motivation: 研究目的是揭示人本AI如何推动金融行业的个性化服务和产品创新，以满足多样化的客户需求。

Method: 通过分析当代研究和行业案例，结合数据分析和自然语言处理技术，研究人本AI在金融中的应用。

Result: 研究表明，人本AI能显著提升客户体验、优化投资建议，并强化风控与合规能力。

Conclusion: 人本AI为金融机构提供了战略框架，结合用户体验研究可确保技术与用户需求一致。

Abstract: Advancements in Artificial Intelligence (AI) have significantly transformed
the financial industry, enabling the development of more personalised and
adaptable financial products and services. This research paper explores various
instances where Human-Centred AI (HCAI) has facilitated these advancements,
drawing from contemporary studies and industry progress. The paper examines how
the application of HCAI-powered data analytics, machine learning, and natural
language processing enables financial institutions to gain a deeper
understanding of their customers' unique needs, preferences, and behavioural
patterns. This, in turn, allows for the creation of tailored financial
solutions that address individual consumer requirements, ultimately enhancing
overall user experience and satisfaction. Additionally, the study highlights
the integration of AI-powered robo-advisory services, which offer customised
investment recommendations and portfolio management tailored to diverse risk
profiles and investment goals. Moreover, the paper underscores the role of AI
in strengthening fraud detection, risk assessment, and regulatory compliance,
leading to a more secure and adaptable financial landscape. The findings of
this research demonstrate the substantial impact of Human-Centred AI on the
financial industry, offering a strategic framework for financial institutions
to leverage these technologies. By incorporating a User Experience Research
(UXR) Point of View (PoV), financial institutions can ensure that AI-driven
solutions align with user needs and business objectives.

</details>


### [31] [Building Blocks of a User Experience Research Point of View](https://arxiv.org/abs/2506.15332)
*Patricia Diaz*

Main category: cs.HC

TL;DR: 论文展示了三种基于数据、证据和洞察的用户体验研究（UXR）视角（POV），并说明了这些视角在企业环境中的应用和成功案例。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过构建POV（视角）策略和方法，在企业环境中实现高影响力的用户体验研究。

Method: 基于三种不同的POV案例：智能视觉（AI提取视频中的文字）、可评估代码编辑器（直接AI反馈学习者）、机会全景（识别高影响技术机会）。

Result: 尽管这些方法起初看似不切实际且违背常规，但最终被采纳并产生了持久影响。

Conclusion: POV策略在用户体验研究中具有实际应用价值，能够推动创新和长期成功。

Abstract: This paper presents three User Experience Research (UXR) perspectives based
on data, evidence and insights - known as Point of View (POV) - showcasing how
the strategies and methods of building a POV work in an enterprise setting. The
POV are: 1. Smart Visuals: Use AI to extract and translate text from visuals in
videos (2019). 2. Assessable Code Editor: Focus on direct AI-feedback to the
learner as it is the loop that requires the least effort for the highest
impact(2023). 3. Opportunity Landscape: Identify high-impact opportunities at
the intersection of emergent technical capabilities that unlock novel
approaches to critical user needs while addressing business strategic
priorities (2019). They all seemed far-fetched and went against common
practice. All were adopted and had long-lasting impact.

</details>


### [32] [Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI](https://arxiv.org/abs/2506.15468)
*Ryota Okumura,Tadahiro Taniguchi,Akira Taniguchi,Yoshinobu Hagiwara*

Main category: cs.HC

TL;DR: 本文提出了一种新型的共创意学习范式，通过人类与AI的互动共同构建符号系统，实验证明基于Metropolis-Hastings命名游戏的模型能显著提升分类准确性和符号一致性。


<details>
  <summary>Details</summary>
Motivation: 解决传统AI教学中单向知识传递的局限性，探索如何整合不同模态的信息以实现人类与AI的共同学习。

Method: 采用基于Metropolis-Hastings命名游戏（MHNG）的人机交互模型，通过在线实验比较三种AI代理类型的效果。

Result: 基于MHNG的AI代理显著提升了分类准确性，并促进人类与AI达成共享符号系统。

Conclusion: 共创意学习为共生AI系统提供了一条新路径，通过动态对齐感知经验实现与人类的协作学习。

Abstract: We propose co-creative learning as a novel paradigm where humans and AI,
i.e., biological and artificial agents, mutually integrate their partial
perceptual information and knowledge to construct shared external
representations, a process we interpret as symbol emergence. Unlike traditional
AI teaching based on unilateral knowledge transfer, this addresses the
challenge of integrating information from inherently different modalities. We
empirically test this framework using a human-AI interaction model based on the
Metropolis-Hastings naming game (MHNG), a decentralized Bayesian inference
mechanism. In an online experiment, 69 participants played a joint attention
naming game (JA-NG) with one of three computer agent types (MH-based,
always-accept, or always-reject) under partial observability. Results show that
human-AI pairs with an MH-based agent significantly improved categorization
accuracy through interaction and achieved stronger convergence toward a shared
sign system. Furthermore, human acceptance behavior aligned closely with the
MH-derived acceptance probability. These findings provide the first empirical
evidence for co-creative learning emerging in human-AI dyads via MHNG-based
interaction. This suggests a promising path toward symbiotic AI systems that
learn with humans, rather than from them, by dynamically aligning perceptual
experiences, opening a new venue for symbiotic AI alignment.

</details>


### [33] [Foundation of Affective Computing and Interaction](https://arxiv.org/abs/2506.15497)
*Changzeng Fu*

Main category: cs.HC

TL;DR: 本书全面探讨了情感计算与人机交互技术，涵盖历史发展、理论基础、技术框架及应用领域，并展望了未来趋势与伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 探索情感计算与人机交互技术的理论与实践，推动其在教育、医疗等领域的应用，同时关注技术发展中的伦理问题。

Method: 介绍多模态情感识别、脑机接口等技术框架，分析数据融合、隐私安全等技术挑战。

Result: 详细阐述情感计算的基础与应用，提出未来技术发展趋势。

Conclusion: 强调技术创新与伦理平衡的重要性，为情感计算的负责任发展提供指导。

Abstract: This book provides a comprehensive exploration of affective computing and
human-computer interaction technologies. It begins with the historical
development and basic concepts of human-computer interaction, delving into the
technical frameworks and practical applications of emotional computing, visual
interaction, voice interaction, brain-computer interfaces, physiological
electrical signal analysis, and social robotics. The book covers a wide range
of topics, including the psychological and neuroscience foundations of emotion,
multimodal emotion recognition, emotional expression mechanisms, and the
principles of brain-computer interfaces.
  Key technologies such as affective computing based on discrete emotion theory
and dimensional models, visual perception principles, speech recognition and
synthesis, EEG signal acquisition and processing, and multimodal emotion
recognition are explained in detail. This book also addresses the technical
challenges in the field, including multimodal data fusion, privacy and
security, and ethical considerations in human-machine relationships. It
discusses the applications of these technologies across various domains such as
education, healthcare, entertainment, and intelligent assistance.
  Looking to the future, the book anticipates trends such as the deep
integration of artificial intelligence with emotion recognition, the
advancement of multimodal interaction technologies, and the development of more
personalized and adaptive emotion recognition systems. It emphasizes the
importance of balancing technological innovation with ethical considerations to
ensure the responsible development and application of affective computing
technologies.

</details>


### [34] [Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach](https://arxiv.org/abs/2506.15512)
*Wenqi Guan,Yang Fang*

Main category: cs.HC

TL;DR: 提出了一种基于GPT和LangChain框架的新方法，通过CoT推理和提示工程增强远程学习检索，提高了结果的精确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 当前远程学习资源检索缺乏深度，难以满足复杂查询的需求。

Method: 结合GPT模型和LangChain框架，利用CoT推理和提示工程技术。

Result: 提高了检索结果的精确性和相关性，改善了用户满意度和学习成果。

Conclusion: 该方法有效提升了远程学习资源检索的质量，为个性化学习提供了支持。

Abstract: Large Language Models have brought a radical change in the process of remote
learning students, among other aspects of educative activities. Current
retrieval of remote learning resources lacks depth in contextual meaning that
provides comprehensive information on complex student queries. This work
proposes a novel approach to enhancing remote learning retrieval by integrating
GPT-based models within the LangChain framework. We achieve this system in a
more intuitive and productive manner using CoT reasoning and prompt
engineering. The framework we propose puts much emphasis on increasing the
precision and relevance of the retrieval results to return comprehensive and
contextually enriched explanations and resources that best suit each student's
needs. We also assess the effectiveness of our approach against paradigmatic
LLMs and report improvements in user satisfaction and learning outcomes.

</details>


### [35] ["How can we learn and use AI at the same time?:: Participatory Design of GenAI with High School Students](https://arxiv.org/abs/2506.15525)
*Isabella Pu,Prerna Ravi,Linh Dieu Dinh,Chelsea Joe,Caitlin Ogoe,Zixuan Li,Cynthia Breazeal,Anastasia K. Ostrowski*

Main category: cs.HC

TL;DR: 研究通过参与式设计工作坊了解高中生对生成AI（GenAI）的看法，提出针对教育技术设计者的新指南，并倡导更多学生参与AI政策的制定。


<details>
  <summary>Details</summary>
Motivation: 生成AI在教育领域的应用日益广泛，但高中生作为直接使用者，其观点常被忽视。研究旨在填补这一空白。

Method: 通过17名高中生参与的参与式设计工作坊，探讨他们对GenAI工具和学校政策的看法，并设计解决方案。

Result: 学生关注的问题包括偏见与错误信息、犯罪与抄袭、对AI的过度依赖以及对学术不端的误判，并提出相应的解决方案。

Conclusion: 研究强调学生观点的重要性，提出针对教育技术设计者的新指南，并呼吁更多学生参与AI政策的制定。

Abstract: As generative AI (GenAI) emerges as a transformative force, clear
understanding of high school students' perspectives is essential for GenAI's
meaningful integration in high school environments. In this work, we draw
insights from a participatory design workshop where we engaged 17 high school
students -- a group rarely involved in prior research in this area -- through
the design of novel GenAI tools and school policies addressing their key
concerns. Students identified challenges and developed solutions outlining
their ideal features in GenAI tools, appropriate school use, and regulations.
These centered around the problem spaces of combating bias & misinformation,
tackling crime & plagiarism, preventing over-reliance on AI, and handling false
accusations of academic dishonesty. Building on our participants'
underrepresented perspectives, we propose new guidelines targeted at
educational technology designers for development of GenAI technologies in high
schools. We also argue for further incorporation of student voices in
development of AI policies in their schools.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [36] [You Only Render Once: Enhancing Energy and Computation Efficiency of Mobile Virtual Reality](https://arxiv.org/abs/2506.15183)
*Xingyu Chen,Xinmin Fang,Shuting Zhang,Xinyu Zhang,Liang He,Zhengxiong Li*

Main category: cs.GR

TL;DR: EffVR是一种针对移动VR的渲染优化方法，通过单次渲染生成双目图像，显著降低计算和功耗。


<details>
  <summary>Details</summary>
Motivation: 移动VR需要高效的计算和低功耗，而现有技术需两次渲染，限制了移动设备的性能。

Method: 利用每像素属性，EffVR从单目图像生成双目VR图像，仅需一次渲染。

Result: EffVR平均节省27%功耗，图像质量高（SSIM 0.9679，PSNR 34.09），帧率提升115.2%。

Conclusion: EffVR为可持续移动VR提供了计算和能耗优化方案，性能优于现有技术。

Abstract: Mobile Virtual Reality (VR) is essential to achieving convenient and
immersive human-computer interaction and realizing emerging applications such
as Metaverse. However, existing VR technologies require two separate renderings
of binocular images, causing a significant bottleneck for mobile devices with
limited computing capability and power supply. This paper proposes an approach
to rendering optimization for mobile VR called EffVR. By utilizing the
per-pixel attribute, EffVR can generate binocular VR images from the monocular
image through genuinely one rendering, saving half the computation over
conventional approaches. Our evaluation indicates that, compared with the
state-of-art, EffVRcan save 27% power consumption on average while achieving
high binocular image quality (0.9679 SSIM and 34.09 PSNR) in mobile VR
applications. Additionally, EffVR can increase the frame rate by 115.2%. These
results corroborate EffVRsuperior computation/energy-saving performance, paving
the road to a sustainable mobile VR. The source code, demo video, android app,
and more are released anonymously at https://yoro-vr.github.io/

</details>


### [37] [Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models](https://arxiv.org/abs/2506.15290)
*Andela Ilic,Jiaxi Jiang,Paul Streli,Xintong Liu,Christian Holz*

Main category: cs.GR

TL;DR: 论文提出了一种基于稀疏、松散附着IMU传感器的人体姿态估计新任务，并开发了基于Transformer的扩散模型来解决这一挑战性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的IMU传感器运动捕捉方法通常假设传感器紧贴人体，而现实中这一假设往往不成立，因此需要研究松散附着IMU的姿态估计。

Method: 模拟现有服装感知运动数据集的IMU记录，开发基于Transformer的扩散模型，结合服装参数训练模型。

Result: 实验表明，模型在模拟和合成数据上训练后，定量和定性均优于现有方法。

Conclusion: 该研究为松散附着IMU传感器的人体姿态估计开辟了新方向，具有潜在应用价值。

Abstract: Motion capture using sparse inertial sensors has shown great promise due to
its portability and lack of occlusion issues compared to camera-based tracking.
Existing approaches typically assume that IMU sensors are tightly attached to
the human body. However, this assumption often does not hold in real-world
scenarios. In this paper, we present a new task of full-body human pose
estimation using sparse, loosely attached IMU sensors. To solve this task, we
simulate IMU recordings from an existing garment-aware human motion dataset. We
developed transformer-based diffusion models to synthesize loose IMU data and
estimate human poses based on this challenging loose IMU data. In addition, we
show that incorporating garment-related parameters while training the model on
simulated loose data effectively maintains expressiveness and enhances the
ability to capture variations introduced by looser or tighter garments.
Experiments show that our proposed diffusion methods trained on simulated and
synthetic data outperformed the state-of-the-art methods quantitatively and
qualitatively, opening up a promising direction for future research.

</details>


### [38] [One-shot Face Sketch Synthesis in the Wild via Generative Diffusion Prior and Instruction Tuning](https://arxiv.org/abs/2506.15312)
*Han Wu,Junyao Li,Kangbo Zhao,Sen Zhang,Yukai Shi,Liang Lin*

Main category: cs.GR

TL;DR: 提出了一种基于扩散模型的单次人脸素描合成方法，通过优化文本指令生成高质量素描，并引入了新的评估基准OS-Sketch。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量训练样本，面临数据稀缺和高成本问题，新方法旨在解决这些限制。

Method: 使用扩散模型优化文本指令，基于单对照片-素描图像进行训练和推理。

Result: 新方法在单次场景下生成高质量且一致的素描，便捷性和适用性优于其他方法。

Conclusion: 提出的方法在数据稀缺情况下仍能有效工作，为新基准提供了基础。

Abstract: Face sketch synthesis is a technique aimed at converting face photos into
sketches. Existing face sketch synthesis research mainly relies on training
with numerous photo-sketch sample pairs from existing datasets. However, these
large-scale discriminative learning methods will have to face problems such as
data scarcity and high human labor costs. Once the training data becomes
scarce, their generative performance significantly degrades. In this paper, we
propose a one-shot face sketch synthesis method based on diffusion models. We
optimize text instructions on a diffusion model using face photo-sketch image
pairs. Then, the instructions derived through gradient-based optimization are
used for inference. To simulate real-world scenarios more accurately and
evaluate method effectiveness more comprehensively, we introduce a new
benchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark
consists of 400 pairs of face photo-sketch images, including sketches with
different styles and photos with different backgrounds, ages, sexes,
expressions, illumination, etc. For a solid out-of-distribution evaluation, we
select only one pair of images for training at each time, with the rest used
for inference. Extensive experiments demonstrate that the proposed method can
convert various photos into realistic and highly consistent sketches in a
one-shot context. Compared to other methods, our approach offers greater
convenience and broader applicability. The dataset will be available at:
https://github.com/HanWu3125/OS-Sketch

</details>


### [39] [Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards](https://arxiv.org/abs/2506.15684)
*Qingming Liu,Zhen Liu,Dinghuai Zhang,Kui Jia*

Main category: cs.GR

TL;DR: Nabla-R2D3是一个高效的强化学习对齐框架，通过2D奖励优化3D扩散模型，解决现有方法在指令遵循、人类偏好对齐和真实感生成上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成模型（如扩散模型）在指令遵循、人类偏好对齐及真实感生成方面表现有限，无法达到人工设计内容的水平。

Method: 基于Nabla-GFlowNet方法，Nabla-R2D3通过2D奖励信号对3D扩散模型进行高效对齐和微调。

Result: 实验表明，Nabla-R2D3在少量微调步骤内即可实现更高的奖励和减少先验遗忘，表现优于传统微调方法。

Conclusion: Nabla-R2D3为3D生成模型的强化学习对齐提供了一种高效且有效的解决方案。

Abstract: Generating high-quality and photorealistic 3D assets remains a longstanding
challenge in 3D vision and computer graphics. Although state-of-the-art
generative models, such as diffusion models, have made significant progress in
3D generation, they often fall short of human-designed content due to limited
ability to follow instructions, align with human preferences, or produce
realistic textures, geometries, and physical attributes. In this paper, we
introduce Nabla-R2D3, a highly effective and sample-efficient reinforcement
learning alignment framework for 3D-native diffusion models using 2D rewards.
Built upon the recently proposed Nabla-GFlowNet method, which matches the score
function to reward gradients in a principled manner for reward finetuning, our
Nabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D
reward signals. Extensive experiments show that, unlike vanilla finetuning
baselines which either struggle to converge or suffer from reward hacking,
Nabla-R2D3 consistently achieves higher rewards and reduced prior forgetting
within a few finetuning steps.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [Efficient Serving of LLM Applications with Probabilistic Demand Modeling](https://arxiv.org/abs/2506.14851)
*Yifei Liu,Zuo Gan,Zhenghao Gan,Weiye Wang,Chen Chen,Yizhou Shan,Xusheng Chen,Zhenhua Han,Yifei Zhu,Shixuan Sun,Minyi Guo*

Main category: cs.DC

TL;DR: 论文提出了一种基于概率需求图（PDGraph）的LLM应用服务系统Hermes，通过优化调度顺序和预热后端，显著提升了服务效率。


<details>
  <summary>Details</summary>
Motivation: 现有的服务系统将LLM应用的资源需求视为黑箱，导致排队顺序和预热延迟问题，降低了端到端效率。

Method: 利用PDGraph建模资源需求，并应用Gittins策略确定调度顺序，同时优化后端预热时机。

Result: 实验表明，Hermes能将平均完成时间减少70%以上，P95完成时间减少80%以上。

Conclusion: Hermes通过概率需求建模和优化调度，显著提升了LLM应用的端到端服务效率。

Abstract: Applications based on Large Language Models (LLMs) contains a series of tasks
to address real-world problems with boosted capability, which have dynamic
demand volumes on diverse backends. Existing serving systems treat the resource
demands of LLM applications as a blackbox, compromising end-to-end efficiency
due to improper queuing order and backend warm up latency. We find that the
resource demands of LLM applications can be modeled in a general and accurate
manner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which
leverages PDGraph for efficient serving of LLM applications. Confronting
probabilistic demand description, Hermes applies the Gittins policy to
determine the scheduling order that can minimize the average application
completion time. It also uses the PDGraph model to help prewarm cold backends
at proper moments. Experiments with diverse LLM applications confirm that
Hermes can effectively improve the application serving efficiency, reducing the
average completion time by over 70% and the P95 completion time by over 80%.

</details>


### [41] [Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching](https://arxiv.org/abs/2506.14852)
*Qizheng Zhang,Michael Wornow,Kunle Olukotun*

Main category: cs.DC

TL;DR: 提出了基于LLM的代理应用缓存方法，通过提取、存储和重用结构化计划模板，显著降低成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM缓存技术主要针对聊天机器人，无法满足代理应用中依赖外部数据或环境上下文的复杂需求。

Method: 提出代理计划缓存，从代理执行的规划阶段提取计划模板，通过关键词匹配和轻量模型适配新任务。

Result: 在多个实际代理应用中，平均降低成本46.62%且性能稳定。

Conclusion: 代理计划缓存为LLM代理提供高效解决方案，兼容现有LLM基础设施。

Abstract: LLM-based agentic applications have shown increasingly remarkable
capabilities in complex workflows but incur substantial costs due to extensive
planning and reasoning requirements. Existing LLM caching techniques (like
context caching and semantic caching), primarily designed for serving chatbots,
are insufficient for agentic applications where outputs depend on external data
or environmental contexts. We propose agentic plan caching, a novel approach
that extracts, stores, adapts, and reuses structured plan templates from
planning stages of agentic applications across semantically similar tasks to
reduce the cost of serving. Unlike traditional semantic caching, our system
extracts plan templates from completed agent executions at test-time, employs
keyword extraction to match new requests against cached plans, and utilizes
lightweight models to adapt these templates to task-specific plans with
contexts. Evaluation across multiple real-world agentic applications shows that
our system can reduce costs by 46.62% on average while maintaining performance,
offering a more efficient solution for serving LLM-based agents that
complements existing LLM serving infrastructures.

</details>


### [42] [Zarr-Based Chunk-Level Cumulative Sums in Reduced Dimensions](https://arxiv.org/abs/2506.14981)
*Hailiang Zhang,Dieu My T. Nguyen,Christine Smit,Mahabal Hegde*

Main category: cs.DC

TL;DR: 论文提出了一种通用方法，通过生成小型可调辅助数据集存储累积和，显著提升大规模多维数据分析效率，性能提升10,000倍，存储仅增加5%。


<details>
  <summary>Details</summary>
Motivation: 大规模多维数据分析（如地理空间数据）通常涉及大量计算，传统分布式或云端扫描方法成本高且效率低。

Method: 提出生成小型辅助数据集，存储沿特定维度的累积和，结合原始数据实现高效计算。

Result: 在AWS中实现的该方案，性能比暴力扫描快3-4个数量级（约10,000倍），存储开销仅增加5%。

Conclusion: 该方法适用于分块云优化格式的数据，为分布式或云端环境提供低成本高效解决方案，已提出Zarr扩展提案。

Abstract: Data analysis on massive multi-dimensional data, such as high-resolution
large-region time averaging or area averaging for geospatial data, often
involves calculations over a significant number of data points. While
performing calculations in scalable and flexible distributed or cloud
environments is a viable option, a full scan of large data volumes still serves
as a computationally intensive bottleneck, leading to significant cost. This
paper introduces a generic and comprehensive method to address these
computational challenges. This method generates a small, size-tunable
supplementary dataset that stores the cumulative sums along specific subset
dimensions on top of the raw data. This minor addition unlocks rapid and cheap
high-resolution large-region data analysis, making calculations over large
numbers of data points feasible with small instances or even microservices in
the cloud. This method is general-purpose, but is particularly well-suited for
data stored in chunked, cloud-optimized formats and for services running in
distributed or cloud environments. We present a Zarr extension proposal to
integrate the specifications of this method and facilitate its straightforward
implementation in general-purpose software applications. Benchmark tests
demonstrate that this method, implemented in Amazon Web services (AWS),
significantly outperforms the brute-force approach used in on-premises
services. With just 5% supplemental storage, this method achieves a performance
that is 3-4 orders of magnitude (~10,000 times) faster than the brute-force
approach, while incurring significantly reduced computational costs.

</details>


### [43] [Parallel Data Object Creation: Towards Scalable Metadata Management in High-Performance I/O Library](https://arxiv.org/abs/2506.15114)
*Youjia Li,Robert Latham,Robert Ross,Ankit Agrawal,Alok Choudhary,Wei-Keng Liao*

Main category: cs.DC

TL;DR: 本文提出了一种新的文件头格式，支持独立数据对象创建，显著提高了高能物理学等应用中大规模数据对象的并行创建效率。


<details>
  <summary>Details</summary>
Motivation: 传统I/O库（如PnetCDF）要求所有进程集体调用数据对象创建API，且需保持元数据一致性，这在处理大量不同尺寸数据对象时效率低下。本文旨在解决这一限制。

Method: 以PnetCDF为实验平台，设计了一种新的文件头格式，包含索引表和元数据块列表，支持独立或集体创建数据对象。

Result: 新设计在4096个MPI进程中并行创建5,684,800个数据对象时，速度提升高达582倍，并降低了内存占用。

Conclusion: 新方法显著提高了并行数据对象创建的效率和可扩展性，适用于需要处理大量独立数据对象的科学应用。

Abstract: High-level I/O libraries, such as HDF5 and PnetCDF, are commonly used by
large-scale scientific applications to perform I/O tasks in parallel. These I/O
libraries store the metadata such as data types and dimensionality along with
the raw data in the same files. While these libraries are well-optimized for
concurrent access to the raw data, they are designed neither to handle a large
number of data objects efficiently nor to create different data objects
independently by multiple processes, as they require applications to call data
object creation APIs collectively with consistent metadata among all processes.
Applications that process data gathered from remote sensors, such as particle
collision experiments in high-energy physics, may generate data of different
sizes from different sensors and desire to store them as separate data objects.
For such applications, the I/O library's requirement on collective data object
creation can become very expensive, as the cost of metadata consistency check
increases with the metadata volume as well as the number of processes. To
address this limitation, using PnetCDF as an experimental platform, we
investigate solutions in this paper that abide the netCDF file format, as well
as propose a new file header format that enables independent data object
creation. The proposed file header consists of two sections, an index table and
a list of metadata blocks. The index table contains the reference to the
metadata blocks and each block stores metadata of objects that can be created
collectively or independently. The new design achieves a scalable performance,
cutting data object creation times by up to 582x when running on 4096 MPI
processes to create 5,684,800 data objects in parallel. Additionally, the new
method reduces the memory footprints, with each process requiring an amount of
memory space inversely proportional to the number of processes.

</details>


### [44] [eLLM: Elastic Memory Management Framework for Efficient LLM Serving](https://arxiv.org/abs/2506.15155)
*Jiale Xu,Rui Zhang,Yi Xiong,Cong Guo,Zihan Liu,Yangjie Zhou,Weiming Hu,Hao Wu,Changxu Shao,Ziqing Wang,Yongjie Yuan,Junping Zhao,Minyi Guo,Jingwen Leng*

Main category: cs.DC

TL;DR: eLLM提出弹性内存管理框架，优化LLM服务中的内存利用率，提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: LLM服务中的动态内存管理问题导致内存利用率低、吞吐量下降。

Method: 通过虚拟张量抽象、弹性内存机制和轻量调度策略，统一管理内存池。

Result: eLLM解码吞吐量提升2.32倍，支持3倍批次大小。

Conclusion: eLLM有效解决动态内存管理问题，显著提升LLM服务性能。

Abstract: Large Language Models are increasingly being deployed in datacenters. Serving
these models requires careful memory management, as their memory usage includes
static weights, dynamic activations, and key-value caches. While static weights
are constant and predictable, dynamic components such as activations and KV
caches change frequently during runtime, presenting significant challenges for
efficient memory management. Modern LLM serving systems typically handle
runtime memory and KV caches at distinct abstraction levels: runtime memory
management relies on static tensor abstractions, whereas KV caches utilize a
page table-based virtualization layer built on top of the tensor abstraction.
This virtualization dynamically manages KV caches to mitigate memory
fragmentation. However, this dual-level approach fundamentally isolates runtime
memory and KV cache management, resulting in suboptimal memory utilization
under dynamic workloads, which can lead to a nearly 20% drop in throughput.
  To address these limitations, we propose eLLM, an elastic memory management
framework inspired by the classical memory ballooning mechanism in operating
systems. The core components of eLLM include: (1) Virtual Tensor Abstraction,
which decouples the virtual address space of tensors from the physical GPU
memory, creating a unified and flexible memory pool; (2) an Elastic Memory
Mechanism that dynamically adjusts memory allocation through runtime memory
inflation and deflation, leveraging CPU memory as an extensible buffer; and (3)
a Lightweight Scheduling Strategy employing SLO-aware policies to optimize
memory utilization and effectively balance performance trade-offs under
stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM
significantly outperforms state-of-the-art systems, 2.32x higher decoding
throughput, and supporting 3x larger batch sizes for 128K-token inputs.

</details>


### [45] [RISC-V for HPC: An update of where we are and main action points](https://arxiv.org/abs/2506.15418)
*Nick Brown*

Main category: cs.DC

TL;DR: RISC-V HPC SIG分析了RISC-V生态在高性能计算（HPC）中的现状与限制，指出近年来的进步与需关注的问题。


<details>
  <summary>Details</summary>
Motivation: 探讨RISC-V在HPC领域的当前生态状态及其局限性，明确未来需要改进的方向。

Method: 通过分析RISC-V生态的现状，识别出其在HPC应用中的主要限制。

Result: RISC-V在HPC领域取得了显著进展，但仍存在需要重点关注的限制。

Conclusion: 尽管RISC-V生态在HPC中取得进展，但需进一步解决现有局限性以实现更广泛的应用。

Abstract: This extended abstract is submitted on behalf of the RISC-V HPC SIG who have
been undertaking an analysis to explore the current state and limitations of
the RISC-V ecosystem for HPC. Whilst it is right to celebrate that there has
been great progress made in recent years, we also highlight limitations and
where effort should be focussed.

</details>


### [46] [Exploring Fast Fourier Transforms on the Tenstorrent Wormhole](https://arxiv.org/abs/2506.15437)
*Nick Brown,Jake Davies,Felix LeClair*

Main category: cs.DC

TL;DR: 探讨RISC-V加速器在高性能计算（HPC）中的应用，通过移植FFT算法展示了其能效优势，但性能低于传统CPU。


<details>
  <summary>Details</summary>
Motivation: RISC-V ISA在HPC领域尚未普及，但其开放性和灵活性为HPC提供了潜在优势，尤其是在能效方面。

Method: 将Cooley-Tukey FFT算法移植到Tenstorrent Wormhole PCIe RISC-V加速器，并优化数据移动瓶颈。

Result: Wormhole n300的2D FFT性能低于24核Xeon Platinum CPU，但功耗低8倍，能耗低2.8倍。

Conclusion: RISC-V加速器在高能效场景中具有潜力，尽管性能表现仍需提升。

Abstract: Whilst numerous areas of computing have adopted the RISC-V Instruction Set
Architecture (ISA) wholesale in recent years, it is yet to become widespread in
HPC. RISC-V accelerators offer a compelling option where the HPC community can
benefit from the specialisation offered by the open nature of the standard but
without the extensive ecosystem changes required when adopting RISC-V CPUs. In
this paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT)
algorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon
Tenstorrent's Tensix architecture, this technology decouples the movement of
data from compute, potentially offering increased control to the programmer.
Exploring different optimisation techniques to address the bottlenecks inherent
in data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is
slower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around
8 times less power and consumes around 2.8 times less energy than the CPU when
computing the Fourier transform.

</details>


### [47] [Parallel Paradigms in Modern HPC: A Comparative Analysis of MPI, OpenMP, and CUDA](https://arxiv.org/abs/2506.15454)
*Nizar ALHafez,Ahmad Kurdi*

Main category: cs.DC

TL;DR: 本文比较了三种主流的高性能计算并行编程模型（MPI、OpenMP、CUDA），分析了各自的性能、适用领域及优缺点，并指出混合模型在异构环境中的优势。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算架构的异构化，选择最优编程模型变得至关重要，本文旨在帮助开发者和研究人员做出更明智的选择。

Method: 通过系统分析三种模型在架构基础、性能特点、适用领域、编程复杂度和最新进展等方面的表现，结合科学计算、机器学习和数据分析的实例评估。

Result: MPI在分布式内存环境中表现优异，OpenMP适用于共享内存系统，CUDA在GPU数据并行任务中性能突出，但混合模型往往效果最佳。

Conclusion: 最佳编程模型应根据应用需求、硬件特性和开发限制选择，混合模型在异构环境中具有明显优势。

Abstract: This paper presents a comprehensive comparison of three dominant parallel
programming models in High Performance Computing (HPC): Message Passing
Interface (MPI), Open Multi-Processing (OpenMP), and Compute Unified Device
Architecture (CUDA). Selecting optimal programming approaches for modern
heterogeneous HPC architectures has become increasingly critical. We
systematically analyze these models across multiple dimensions: architectural
foundations, performance characteristics, domain-specific suitability,
programming complexity, and recent advancements. We examine each model's
strengths, weaknesses, and optimization techniques. Our investigation
demonstrates that MPI excels in distributed memory environments with
near-linear scalability for communication-intensive applications, but faces
communication overhead challenges. OpenMP provides strong performance and
usability in shared-memory systems and loop-centric tasks, though it is limited
by shared memory contention. CUDA offers substantial performance gains for
data-parallel GPU workloads, but is restricted to NVIDIA GPUs and requires
specialized expertise. Performance evaluations across scientific simulations,
machine learning, and data analytics reveal that hybrid approaches combining
two or more models often yield optimal results in heterogeneous environments.
The paper also discusses implementation challenges, optimization best
practices, and emerging trends such as performance portability frameworks,
task-based programming, and the convergence of HPC and Big Data. This research
helps developers and researchers make informed decisions when selecting
programming models for modern HPC applications, emphasizing that the best
choice depends on application requirements, hardware, and development
constraints.

</details>


### [48] [All is Not Lost: LLM Recovery without Checkpoints](https://arxiv.org/abs/2506.15461)
*Nikolay Blagoev,Oğuzhan Ersoy,Lydia Yiyu Chen*

Main category: cs.DC

TL;DR: 论文提出CheckFree和CheckFree+方法，通过邻接阶段的加权平均和乱序流水线执行，高效恢复分布式训练中的节点故障，减少通信和计算开销。


<details>
  <summary>Details</summary>
Motivation: 在分散计算节点上训练LLM时，节点故障导致部分模型丢失，传统恢复方法开销大且难以扩展。

Method: CheckFree通过加权平均邻接阶段恢复中间阶段故障；CheckFree+扩展至首尾阶段，通过复制邻接层恢复嵌入层故障。

Result: 在低中故障率（5-10%）下，CheckFree和CheckFree+比现有方法快12%以上完成收敛。

Conclusion: CheckFree系列方法高效、低开销，适用于大规模模型训练，代码已开源。

Abstract: Training LLMs on decentralized and wimpy computation nodes, e.g., multiple
on-spot instances, lowers the training cost and enables model democratization.
The inevitable challenge here is the churn of nodes due to failures and the
operator's scheduling policies, leading to losing a stage - a part of the
model. The conventional approaches to recover from failures are to either use
checkpointing, where periodically a copy of the entire model is sent to an
additional storage, or redundant computation. These approaches yield
significant communication and/or computation overhead even in non-failure cases
and scale poorly in settings with large models. In this paper, we propose,
CheckFree, an efficient recovery method where a failing stage is substituted by
a weighted average of the closest neighboring stages. In contrast to the state
of the art, CheckFree requires no additional computation or storage. However,
because of the nature of averaging neighbouring stages, it can only recover
failures of intermediate stages. We further extend our method to CheckFree+
with out-of-order pipeline execution to tolerate crashes of the first and last
stages. Thanks to out-of-order pipelining, behaviour of those stages is
mimicked by their neighboring ones, which allows CheckFree+ to recover them by
simply copying the weights from the immediate neighbour. To be able to recover
the (de)embedding layers, CheckFree+ copies those layers to the neighboring
stages, which requires relatively small storage overhead. We extensively
evaluate our method on LLaMa models of model sizes from 124M to 1.5B with
varying failure frequencies. In the case of low and medium failure rates
(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant
computation in terms of convergence in wall-clock time by over 12%. Both of our
proposals can be run via our code available at:
https://github.com/gensyn-ai/CheckFree.

</details>


### [49] [Minimizing Communication for Parallel Symmetric Tensor Times Same Vector Computation](https://arxiv.org/abs/2506.15488)
*Hussam Al Daas,Grey Ballard,Laura Grigori,Suraj Kumar,Kathryn Rouse,Mathieu Vérité*

Main category: cs.DC

TL;DR: 该论文研究了在并行计算环境下，对三维对称张量进行特定向量乘法操作所需的通信成本，提出了通信下界，并通过扩展对称矩阵的三角块分区方案展示了最优算法。


<details>
  <summary>Details</summary>
Motivation: 研究目的是为了优化在并行计算中用于计算三维对称张量特征对和对称CP分解的通信成本，这是高阶幂方法和梯度方法中的关键计算步骤。

Method: 论文通过扩展对称计算的几何不等式，建立了通信下界，并设计了一种最优算法，其数据分布方案是对对称矩阵三角块分区方案的推广。

Result: 结果显示提出的通信下界是紧致的，并通过最优算法的实现验证了其有效性。

Conclusion: 论文成功确定了并行计算中三维对称张量乘法的最低通信成本，并提供了一种有效的数据分布方案，为相关计算问题提供了优化方向。

Abstract: In this article, we focus on the parallel communication cost of multiplying
the same vector along two modes of a $3$-dimensional symmetric tensor. This is
a key computation in the higher-order power method for determining eigenpairs
of a $3$-dimensional symmetric tensor and in gradient-based methods for
computing a symmetric CP decomposition. We establish communication lower bounds
that determine how much data movement is required to perform the specified
computation in parallel. The core idea of the proof relies on extending a key
geometric inequality for $3$-dimensional symmetric computations. We demonstrate
that the communication lower bounds are tight by presenting an optimal
algorithm where the data distribution is a natural extension of the triangle
block partition scheme for symmetric matrices to 3-dimensional symmetric
tensors.

</details>


### [50] [Automatic Metadata Capture and Processing for High-Performance Workflows](https://arxiv.org/abs/2506.15537)
*Polina Shpilker,Line Pouchard*

Main category: cs.DC

TL;DR: 开发软件以收集HPC系统上运行的工作流元数据，遵循FAIR原则，并探索两种格式以实现元数据的统一存储和易用性。


<details>
  <summary>Details</summary>
Motivation: 现代工作流在异构计算架构上运行，增加了复杂性，希望通过遵循FAIR原则提高研究可复现性。

Method: 开发软件收集工作流元数据，实验两种格式统一存储，并优化元数据结构以便研究。

Result: 提出了两种可能的元数据存储格式，并将收集的元数据重新组织以便于研究人员分析工作流性能。

Conclusion: 通过开发软件和优化元数据存储，支持异构计算环境下的工作流性能研究。

Abstract: Modern workflows run on increasingly heterogeneous computing architectures
and with this heterogeneity comes additional complexity. We aim to apply the
FAIR principles for research reproducibility by developing software to collect
metadata annotations for workflows run on HPC systems. We experiment with two
possible formats to uniformly store these metadata, and reorganize the
collected metadata to be as easy to use as possible for researchers studying
their workflow performance.

</details>


### [51] [LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale Heterogeneous Clusters](https://arxiv.org/abs/2506.15595)
*Kunming Zhang,Hanlong Liao,Guoming Tang*

Main category: cs.DC

TL;DR: LiteGD是一个轻量级、动态的GPU调度系统，通过全局视角优化多GPU并行计算中的通信延迟，适用于异构GPU集群。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理邻近性的GPU分配方法在大型异构GPU集群中存在局限性，特别是在带宽分布不规则的情况下，无法有效降低通信延迟。

Method: LiteGD采用计算感知设计，利用轻量级Transformer网络处理GPU拓扑信息，并通过双向树搜索寻找最优调度方案。

Result: 实验表明，LiteGD在多种集群配置下均能实现约90%的带宽效率，在真实H100集群中达到80%，显著优于传统方法。

Conclusion: LiteGD在大型异构环境中表现优越，提供了高效的GPU调度解决方案。

Abstract: Parallel computing with multiple GPUs has become the dominant paradigm for
machine learning tasks, especially those of large language models (LLMs). To
reduce the latency incurred by inter-GPU communication, a common practice for
parallel tasks has been to allocate GPUs based on their physical proximity.
However, this long-standing assumption has notable limitations, particularly in
large-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs
is irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU
dispatching system based on global perspectives. To tackle the difficulty of
storing massive GPU topology information, LiteGD adopts a computation-aware
design that leverages a lightweight Transformer network trained on sampled
data. Our customized design for network structure ensures both transferability
and scalability. LiteGD also employs a bidirectional tree search approach to
find the optimal GPU dispatching in the data generated in the previous step,
which can identify near-optimal solutions while reducing search overhead. We
implement and evaluate LiteGD in both real and simulated GPU clusters with
homogeneous and heterogeneous interconnects, respectively. Experimental results
demonstrate that LiteGD consistently achieves high GPU bandwidth efficacy
(approximately 90\%) across various cluster configurations and 80\% in
real-world H100 cluster, significantly outperforming conventional default and
interconnect topology-aware dispatching methods, particularly in large-scale
heterogeneous environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [52] [SimBank: from Simulation to Solution in Prescriptive Process Monitoring](https://arxiv.org/abs/2506.14772)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.DB

TL;DR: SimBank是一个用于准确基准测试PresPM方法的模拟器，旨在解决现有文献中技术比较不足和评估方法不充分的问题。


<details>
  <summary>Details</summary>
Motivation: 组织需要优化流程以提高运营绩效，但现有PresPM研究缺乏技术间的广泛比较和有效的评估方法。

Method: 提出SimBank模拟器，基于银行贷款申请流程模型，支持在线和离线PresPM方法的比较，涵盖多种复杂性干预问题。

Result: SimBank能生成每种干预下的真实结果，解决了记录数据无法实现的问题，并通过实验证明其作为公开模拟器的价值。

Conclusion: SimBank为PresPM研究和实践提供了有效的基准测试工具，有望推动该领域的发展。

Abstract: Prescriptive Process Monitoring (PresPM) is an emerging area within Process
Mining, focused on optimizing processes through real-time interventions for
effective decision-making. PresPM holds significant promise for organizations
seeking enhanced operational performance. However, the current literature faces
two key limitations: a lack of extensive comparisons between techniques and
insufficient evaluation approaches. To address these gaps, we introduce
SimBank: a simulator designed for accurate benchmarking of PresPM methods.
Modeled after a bank's loan application process, SimBank enables extensive
comparisons of both online and offline PresPM methods. It incorporates a
variety of intervention optimization problems with differing levels of
complexity and supports experiments on key causal machine learning challenges,
such as assessing a method's robustness to confounding in data. SimBank
additionally offers a comprehensive evaluation capability: for each test case,
it can generate the true outcome under each intervention action, which is not
possible using recorded datasets. The simulator incorporates parallel
activities and loops, drawing from common logs to generate cases that closely
resemble real-life process instances. Our proof of concept demonstrates
SimBank's benchmarking capabilities through experiments with various PresPM
methods across different interventions, highlighting its value as a publicly
available simulator for advancing research and practice in PresPM.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [53] [Scaling Intelligence: Designing Data Centers for Next-Gen Language Models](https://arxiv.org/abs/2506.15006)
*Jesmin Jahan Tithi,Hanjiang Wu,Avishaii Abuhatzera,Fabrizio Petrini*

Main category: cs.AR

TL;DR: 本文提出了一种针对大型语言模型（LLM）的数据中心架构联合设计框架，通过优化计算、网络和存储资源，显著提升性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数规模的爆炸式增长（如GPT-4达1.8万亿参数），传统数据中心架构在扩展性、效率和成本效益上已无法满足需求，亟需创新设计。

Method: 提出了一种联合设计框架，涵盖计算能力（FLOPS）、高带宽内存（HBM）、网络拓扑（如全平面光网络），以及并行优化策略。并通过性能建模工具验证其有效性。

Result: 全平面网络架构显著提升了节点间带宽和延迟性能，优化后的系统设计在稀疏和密集型LLM上均提升了模型FLOPS利用率和吞吐量。

Conclusion: 该框架为支持万亿参数模型的高效数据中心设计提供了实用路线图，降低了优化复杂度，推动了AI能力的持续发展。

Abstract: The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8
trillion parameters - demands a radical rethinking of data center architecture
to ensure scalability, efficiency, and cost-effectiveness. Our work provides a
comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth
and capacity, multiple network topologies (two-tier vs. FullFlat optical), the
size of the scale-out domain, and popular parallelism/optimization strategies
used in LLMs. We introduce and evaluate FullFlat network architectures, which
provide uniform high-bandwidth, low-latency connectivity between all nodes, and
demonstrate their transformative impact on performance and scalability. Through
detailed sensitivity analyses, we quantify the benefits of overlapping compute
and communication, leveraging hardware-accelerated collectives, wider scale-out
domains, and larger memory capacity. Our study spans both sparse (mixture of
experts) and dense transformer-based LLMs, revealing how system design choices
affect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens
per sec / Peak flops of the hardware) and overall throughput. For the co-design
study, we extended and validated a performance modeling tool capable of
predicting LLM runtime within 10% of real-world measurements. Our findings
offer actionable insights and a practical roadmap for designing AI data centers
that can efficiently support trillion-parameter models, reduce optimization
complexity, and sustain the rapid evolution of AI capabilities.

</details>


### [54] [ChatModel: Automating Reference Model Design and Verification with LLMs](https://arxiv.org/abs/2506.15066)
*Jianmin Ye,Tianyang Liu,Qi Tian,Shengchu Su,Zhe Jiang,Xi Wang*

Main category: cs.AR

TL;DR: ChatModel是一个基于大型语言模型的参考模型生成与验证平台，通过设计标准化和分层敏捷建模，显著提升了参考模型的生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 集成电路设计的复杂性不断增加，功能验证变得越来越困难，而参考模型的开发也变得更加复杂耗时。

Method: 采用构建块生成策略，结合设计标准化和分层敏捷建模，优化大型语言模型在参考模型设计中的能力。

Result: 在300个不同复杂度的设计上测试，ChatModel在效率和质量上均有显著提升，峰值性能提升55.02%，生成稳定性增强，设计能力提升9.18倍，迭代速度平均加快5.90倍。

Conclusion: ChatModel在参考模型生成和验证的自动化方面具有显著潜力。

Abstract: As the complexity of integrated circuit designs continues to escalate, the
functional verification becomes increasingly challenging. Reference models,
critical for accelerating the verification process, are themselves becoming
more intricate and time-consuming to develop. Despite the promise shown by
large language models (LLMs) in code programming, effectively generating
complex reference models remains a significant hurdle. To address these
challenges, we introduce ChatModel, the first LLM-aided agile reference model
generation and verification platform. ChatModel streamlines the transition from
design specifications to fully functional reference models by integrating
design standardization and hierarchical agile modeling. Employing a
building-block generation strategy, it not only enhances the design
capabilities of LLMs for reference models but also significantly boosts
verification efficiency. We evaluated ChatModel on 300 designs of varying
complexity, demonstrating substantial improvements in both efficiency and
quality of reference model generation. ChatModel achieved a peak performance
improvement of 55.02% compared to alternative methods, with notable
enhancements in generation stability, and delivered a 9.18x increase in its
capacity to produce reference model designs. Furthermore, it accelerated the
iterative process of reference model design and validation by an average of
5.90x compared to traditional approaches. These results highlight the potential
of ChatModel to significantly advance the automation of reference model
generation and validation.

</details>


### [55] [J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor](https://arxiv.org/abs/2506.15316)
*Benoit Tain,Raphael Millet,Romain Lemaire,Michal Szczepanski,Laurent Alacoque,Emmanuel Pluchart,Sylvain Choisnet,Rohit Prasad,Jerome Chossat,Pascal Pierunek,Pascal Vivet,Sebastien Thuries*

Main category: cs.AR

TL;DR: J3DAI是一种基于小型深度神经网络的硬件加速器，用于3层3D堆叠CMOS图像传感器，结合了AI芯片和DNN加速器，专注于边缘AI的高效性能。


<details>
  <summary>Details</summary>
Motivation: 随着边缘AI的重要性增加，需要实时、低延迟和节能的AI处理方案，J3DAI旨在满足这一需求。

Method: 利用Aidge软件框架，支持后训练量化以减少内存占用和计算复杂度，设计了一个高效的DNN加速器。

Result: 实验证明了J3DAI在边缘AI中的多功能性和高效性，能够处理简单和计算密集型任务。

Conclusion: J3DAI在边缘AI领域展示了潜力，未来将进一步优化架构并探索新应用。

Abstract: This paper presents J3DAI, a tiny deep neural network-based hardware
accelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial
intelligence (AI) chip integrating a Deep Neural Network (DNN)-based
accelerator. The DNN accelerator is designed to efficiently perform neural
network tasks such as image classification and segmentation. This paper focuses
on the digital system of J3DAI, highlighting its Performance-Power-Area (PPA)
characteristics and showcasing advanced edge AI capabilities on a CMOS image
sensor. To support hardware, we utilized the Aidge comprehensive software
framework, which enables the programming of both the host processor and the DNN
accelerator. Aidge supports post-training quantization, significantly reducing
memory footprint and computational complexity, making it crucial for deploying
models on resource-constrained hardware like J3DAI. Our experimental results
demonstrate the versatility and efficiency of this innovative design in the
field of edge AI, showcasing its potential to handle both simple and
computationally intensive tasks. Future work will focus on further optimizing
the architecture and exploring new applications to fully leverage the
capabilities of J3DAI. As edge AI continues to grow in importance, innovations
like J3DAI will play a crucial role in enabling real-time, low-latency, and
energy-efficient AI processing at the edge.

</details>


### [56] [Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration](https://arxiv.org/abs/2506.15440)
*Omar Numan,Gaurav Singh,Kazybek Adam,Jelin Leslin,Aleksi Korsman,Otto Simola,Marko Kosunen,Jussi Ryynänen,Martin Andraud*

Main category: cs.AR

TL;DR: 论文提出了一种自校准混合信号计算内存（CIM）加速器SoC，以解决当前CIM架构在集成和可靠性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能任务对硬件加速需求的增加，混合信号CIM架构因其高效的数据存储和计算能力受到关注。然而，集成和可靠性问题限制了其大规模应用。

Method: 通过结合SRAM的易编程性和线性电阻的多位计算，设计了一个自校准混合信号CIM加速器SoC，并采用RISC-V控制的片上校准来提高精度和可靠性。

Result: 该SoC将计算信噪比提高了25%至45%，达到18-24 dB，并展示了扩展到高密度线性电阻技术的潜力。

Conclusion: 该研究为CIM架构的集成和可靠性提供了有效解决方案，推动了其在端到端AI计算系统中的广泛应用。

Abstract: Developing accurate and reliable Compute-In-Memory (CIM) architectures is
becoming a key research focus to accelerate Artificial Intelligence (AI) tasks
on hardware, particularly Deep Neural Networks (DNNs). In that regard, there
has been significant interest in analog and mixed-signal CIM architectures
aimed at increasing the efficiency of data storage and computation to handle
the massive amount of data needed by DNNs. Specifically, resistive mixed-signal
CIM cores are pushed by recent progresses in emerging Non-Volatile Memory
(eNVM) solutions. Yet, mixed-signal CIM computing cores still face several
integration and reliability challenges that hinder their large-scale adoption
into end-to-end AI computing systems. In terms of integration, resistive and
eNVM-based CIM cores need to be integrated with a control processor to realize
end-to-end AI acceleration. Moreover, SRAM-based CIM architectures are still
more efficient and easier to program than their eNVM counterparts. In terms of
reliability, analog circuits are more susceptible to variations, leading to
computation errors and degraded accuracy. This work addresses these two
challenges by proposing a self-calibrated mixed-signal CIM accelerator SoC,
fabricated in 22-nm FDSOI technology. The integration is facilitated by (1) the
CIM architecture, combining the density and ease of SRAM-based weight storage
with multi-bit computation using linear resistors, and (2) an open-source
programming and testing strategy for CIM systems. The accuracy and reliability
are enabled through an automated RISC-V controlled on-chip calibration,
allowing us to improve the compute SNR by 25 to 45% across multiple columns to
reach 18-24 dB. To showcase further integration possibilities, we show how our
proof-of-concept SoC can be extended to recent high-density linear resistor
technologies for enhanced computing performance.

</details>


### [57] [CXL-GPU: Pushing GPU Memory Boundaries with the Integration of CXL Technologies](https://arxiv.org/abs/2506.15601)
*Donghyun Gouk,Seungkwan Kang,Seungjun Lee,Jiseon Kim,Kyungkuk Nam,Eojin Ryu,Sangwon Lee,Dongpyung Kim,Junhyeok Jang,Hanyeoreum Bae,Myoungsoo Jung*

Main category: cs.AR

TL;DR: 提出一种基于CXL的GPU存储扩展方案，首次实现两位数纳秒级的往返延迟，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了提升GPU存储技术的性能，支持多样化存储介质的高效集成。

Method: 设计多CXL根端口的GPU系统，开发定制CXL控制器，采用推测读和确定性存储机制。

Result: 实现两位数纳秒级延迟，性能显著提升。

Conclusion: 该方案在GPU存储技术领域取得了重大进展。

Abstract: This work introduces a GPU storage expansion solution utilizing CXL,
featuring a novel GPU system design with multiple CXL root ports for
integrating diverse storage media (DRAMs and/or SSDs). We developed and
siliconized a custom CXL controller integrated at the hardware RTL level,
achieving two-digit nanosecond roundtrip latency, the first in the field. This
study also includes speculative read and deterministic store mechanisms to
efficiently manage read and write operations to hide the endpoint's backend
media latency variation. Performance evaluations reveal our approach
significantly outperforms existing methods, marking a substantial advancement
in GPU storage technology.

</details>


### [58] [From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and Instruction Annotation](https://arxiv.org/abs/2506.15613)
*Miryeong Kwon,Donghyun Gouk,Junhyeok Jang,Jinwoo Baek,Hyunwoo You,Sangyoon Ji,Hongjoo Jung,Junseok Moon,Seungkwan Kang,Seungjun Lee,Myoungsoo Jung*

Main category: cs.AR

TL;DR: 该论文探讨了如何通过Compute Express Link（CXL）将PCIe块存储转变为可扩展的字节寻址工作内存，提出了一种名为CXL-SSD的解决方案，并通过实验展示了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 为了解决块存储与CXL内存中心模型的兼容性问题，探索存储与内存的融合潜力。

Method: 提出了CXL-SSD原型，基于FPGA平台实现，并引入了Determinism和Bufferability注释机制以优化性能。

Result: 实验表明，CXL-SSD性能比PCIe内存扩展器提升10.9倍，注释机制进一步降低延迟5.4倍，在局部性高的工作负载中接近DRAM性能。

Conclusion: 研究证实了将块存储集成到CXL生态系统的可行性，为未来内存与存储的融合奠定了基础。

Abstract: This paper explores how Compute Express Link (CXL) can transform PCIe-based
block storage into a scalable, byte-addressable working memory. We address the
challenges of adapting block storage to CXL's memory-centric model by
emphasizing cacheability as a key enabler and advocating for Type 3 endpoint
devices, referred to as CXL-SSDs. To validate our approach, we prototype a
CXL-SSD on a custom FPGA platform and propose annotation mechanisms,
Determinism and Bufferability, to enhance performance while preserving data
persistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves
10.9x better performance than PCIe-based memory expanders and further reduces
latency by 5.4x with annotation enhancements. In workloads with high locality,
CXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This
work highlights the feasibility of integrating block storage into CXL's
ecosystem and provides a foundation for future memory-storage convergence.

</details>


### [59] [SR-NCL: an Area-/Energy-Efficient Resilient NCL Architecture Based on Selective Redundancy](https://arxiv.org/abs/2506.15634)
*Hasnain A. Ziad,Alexander C. Bodoh,Ashiq A. Sakib*

Main category: cs.AR

TL;DR: 提出了一种基于选择性冗余的新型容错NCL架构，相比现有基于复制的NCL设计，在面积和能耗上更优。


<details>
  <summary>Details</summary>
Motivation: 重复冗余方案虽然能实现完全弹性，但带来了显著的能源、延迟和面积开销。

Method: 采用选择性冗余的Null Convention Logic (NCL)架构。

Result: 在图像处理应用中，面积和能源利用率优于现有设计。

Conclusion: 选择性冗余NCL架构是一种高效且经济的容错方案。

Abstract: Duplication-based redundancy schemes have proven to be effective in designing
fully-resilient Quasi-delay Insensitive (QDI) asynchronous circuits. The
complete resiliency, however, is accompanied by significant energy, latency,
and area overhead. This paper presents a novel error-tolerant Null Convention
Logic (NCL) architecture based on selective redundancy. Results demonstrate the
efficacy of the proposed method in terms of area and energy utilization as
compared to existing duplication-based NCL designs, targeting an image
processing application.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [60] [Not Even Nice Work If You Can Get It; A Longitudinal Study of Uber's Algorithmic Pay and Pricing](https://arxiv.org/abs/2506.15278)
*Reuben Binns,Jake Stein,Siddhartha Datta,Max Van Kleek,Nigel Shadbolt*

Main category: cs.CY

TL;DR: 研究通过参与式审计分析Uber的动态定价对司机收入和工作分配的影响，发现动态定价后司机收入减少、平台抽成增加、工作分配更不可预测。


<details>
  <summary>Details</summary>
Motivation: 探讨Uber等平台宣传的‘灵活性’对司机的实际影响，尤其是动态定价对司机收入和工作的影响。

Method: 采用参与式行动研究，结合对258名英国司机150万次行程的纵向数据分析。

Result: 动态定价后司机收入下降、平台抽成增加、工作分配和收入更不稳定、司机间不平等加剧、等待时间增加。

Conclusion: 研究为算法审计、零工经济和工人数据科学提供了方法论和理论贡献，揭示了动态定价对司机的不利影响。

Abstract: Ride-sharing platforms like Uber market themselves as enabling `flexibility'
for their workforce, meaning that drivers are expected to anticipate when and
where the algorithm will allocate them jobs, and how well remunerated those
jobs will be. In this work we describe our process of participatory action
research with drivers and trade union organisers, culminating in a
participatory audit of Uber's algorithmic pay and work allocation, before and
after the introduction of dynamic pricing. Through longitudinal analysis of 1.5
million trips from 258 drivers in the UK, we find that after dynamic pricing,
pay has decreased, Uber's cut has increased, job allocation and pay is less
predictable, inequality between drivers is increased, and drivers spend more
time waiting for jobs. In addition to these findings, we provide methodological
and theoretical contributions to algorithm auditing, gig work, and the emerging
practice of worker data science.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [61] [Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?](https://arxiv.org/abs/2506.14805)
*Yang Yao,Lingyu Li,Jiaxin Song,Chiyu Chen,Zhenqi He,Yixu Wang,Xin Wang,Tianle Gu,Jie Li,Yan Teng,Yingchun Wang*

Main category: cs.CV

TL;DR: 该论文介绍了Argus Inspection多模态基准和Eye of Panoptes框架，旨在评估MLLMs在视觉细粒度感知和常识因果推理方面的能力。实验显示现有模型表现仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 针对MLLMs在视觉细粒度感知和常识因果推理方面的不足，提出新的评估方法以推动其进一步改进。

Method: 提出Argus Inspection多模态基准，包含两个难度级别；并开发Eye of Panoptes框架，整合Sigmoid指标和指示函数进行更全面评估。

Result: 对26种主流MLLMs的实验显示，视觉细粒度推理最高得分仅为0.46，表明模型能力仍有较大提升空间。

Conclusion: 研究为MLLMs的持续优化提供了重要方向，未来需进一步改进视觉感知和推理能力。

Abstract: As Multimodal Large Language Models (MLLMs) continue to evolve, their
cognitive and reasoning capabilities have seen remarkable progress. However,
challenges in visual fine-grained perception and commonsense causal inference
persist. This paper introduces Argus Inspection, a multimodal benchmark with
two levels of difficulty, emphasizing detailed visual recognition while
incorporating real-world commonsense understanding to evaluate causal reasoning
abilities. Expanding on it, we present the Eye of Panoptes framework, which
integrates a binary parametric Sigmoid metric with an indicator function,
enabling a more holistic evaluation of MLLMs' responses in opinion-based
reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the
highest performance in visual fine-grained reasoning reaches only 0.46,
highlighting considerable potential for enhancement. Our research offers
valuable perspectives for the continued refinement of MLLMs.

</details>


### [62] [MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion](https://arxiv.org/abs/2506.15276)
*Jun Zhu,Xinfeng Zhang,Lv Tang,JunHao Jiang*

Main category: cs.CV

TL;DR: MSNeRV提出了一种多尺度特征融合框架，用于解决基于隐式神经表示的视频压缩方法在处理细节密集和快速变化内容时的不足，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于INR的视频压缩方法难以有效表示细节密集和快速变化的视频内容，主要原因是网络特征利用不足且缺乏视频特定设计。

Method: 提出多尺度特征融合框架MSNeRV，包括时域窗口增强一致性、GoP级网格背景表示、多尺度空间解码器及多尺度特征块。

Result: 在HEVC ClassB和UVG数据集上，MSNeRV表现出优于基于INR的方法，并在动态场景中超过VTM-23.7的压缩效率。

Conclusion: MSNeRV通过多尺度特征融合显著提升了视频表示和压缩性能，尤其在动态场景中效果突出。

Abstract: Implicit Neural representations (INRs) have emerged as a promising approach
for video compression, and have achieved comparable performance to the
state-of-the-art codecs such as H.266/VVC. However, existing INR-based methods
struggle to effectively represent detail-intensive and fast-changing video
content. This limitation mainly stems from the underutilization of internal
network features and the absence of video-specific considerations in network
design. To address these challenges, we propose a multi-scale feature fusion
framework, MSNeRV, for neural video representation. In the encoding stage, we
enhance temporal consistency by employing temporal windows, and divide the
video into multiple Groups of Pictures (GoPs), where a GoP-level grid is used
for background representation. Additionally, we design a multi-scale spatial
decoder with a scale-adaptive loss function to integrate multi-resolution and
multi-frequency information. To further improve feature extraction, we
introduce a multi-scale feature block that fully leverages hidden features. We
evaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and
compression. Experimental results demonstrate that our model exhibits superior
representation capability among INR-based approaches and surpasses VTM-23.7
(Random Access) in dynamic scenarios in terms of compression efficiency.

</details>


### [63] [MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering](https://arxiv.org/abs/2506.15298)
*Xinqi Fan,Jingting Li,John See,Moi Hoon Yap,Wen-Huang Cheng,Xiaobai Li,Xiaopeng Hong,Su-Jing Wang,Adrian K. Davision*

Main category: cs.CV

TL;DR: 论文探讨了面部微表情（MEs）的识别、检测和生成为统一任务的重要性，并介绍了ME grand challenge 2025的两项任务：ME-STR（检测与识别统一）和ME-VQA（利用多模态模型进行视觉问答）。


<details>
  <summary>Details</summary>
Motivation: 传统方法将ME检测和识别视为独立任务，但在实际长视频分析中效果不佳。多模态大语言模型（MLLMs）和大型视觉语言模型（LVLMs）的出现为ME分析提供了新的可能性。

Method: ME grand challenge 2025提出两项任务：1. ME-STR，将ME检测与识别集成到一个统一的顺序流程中；2. ME-VQA，利用MLLMs或LVLMs进行ME相关的视觉问答。

Result: 参赛算法需在测试集上运行并提交结果，展示统一任务和多模态模型在ME分析中的潜力。

Conclusion: 通过统一任务和多模态模型，ME分析的性能有望在实际场景中得到显著提升。

Abstract: Facial micro-expressions (MEs) are involuntary movements of the face that
occur spontaneously when a person experiences an emotion but attempts to
suppress or repress the facial expression, typically found in a high-stakes
environment. In recent years, substantial advancements have been made in the
areas of ME recognition, spotting, and generation. However, conventional
approaches that treat spotting and recognition as separate tasks are
suboptimal, particularly for analyzing long-duration videos in realistic
settings. Concurrently, the emergence of multimodal large language models
(MLLMs) and large vision-language models (LVLMs) offers promising new avenues
for enhancing ME analysis through their powerful multimodal reasoning
capabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that
reflect these evolving research directions: (1) ME spot-then-recognize
(ME-STR), which integrates ME spotting and subsequent recognition in a unified
sequential pipeline; and (2) ME visual question answering (ME-VQA), which
explores ME understanding through visual question answering, leveraging MLLMs
or LVLMs to address diverse question types related to MEs. All participating
algorithms are required to run on this test set and submit their results on a
leaderboard. More details are available at https://megc2025.github.io.

</details>


### [64] [Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis](https://arxiv.org/abs/2506.14854)
*Varun Mannam,Zhenyu Shi*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的零售视频自动标注方法，取代传统耗时人工标注，提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统零售视频标注依赖人工，耗时且成本高，需要自动化解决方案。

Method: 利用深度神经网络学习视频帧特征，结合针对零售环境的对象检测技术进行自动标注。

Result: 方法在准确性上与人工标注相当，效率更高，平均节省2倍成本。

Conclusion: 该方法显著降低了零售视频标注的成本和时间，适用于多种零售应用场景。

Abstract: Accurate video annotation plays a vital role in modern retail applications,
including customer behavior analysis, product interaction detection, and
in-store activity recognition. However, conventional annotation methods heavily
rely on time-consuming manual labeling by human annotators, introducing
non-robust frame selection and increasing operational costs. To address these
challenges in the retail domain, we propose a deep learning-based approach that
automates key-frame identification in retail videos and provides automatic
annotations of products and customers. Our method leverages deep neural
networks to learn discriminative features by embedding video frames and
incorporating object detection-based techniques tailored for retail
environments. Experimental results showcase the superiority of our approach
over traditional methods, achieving accuracy comparable to human annotator
labeling while enhancing the overall efficiency of retail video annotation.
Remarkably, our approach leads to an average of 2 times cost savings in video
annotation. By allowing human annotators to verify/adjust less than 5% of
detected frames in the video dataset, while automating the annotation process
for the remaining frames without reducing annotation quality, retailers can
significantly reduce operational costs. The automation of key-frame detection
enables substantial time and effort savings in retail video labeling tasks,
proving highly valuable for diverse retail applications such as shopper journey
analysis, product interaction detection, and in-store security monitoring.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [65] [On the solvable-unsolvable transition due to noise-induced chaos in digital memcomputing](https://arxiv.org/abs/2506.14928)
*Dyk Chung Nguyen,Thomas Chetaille,Yuan-Hang Zhang,Yuriy V. Pershin,Massimiliano Di Ventra*

Main category: nlin.CD

TL;DR: 该论文研究了数字记忆计算机（DMMs）在数值误差和物理噪声条件下的性能变化，发现噪声会导致混沌转变，并通过功率谱和Lyapunov指数分析揭示了系统行为与噪声强度的关系。


<details>
  <summary>Details</summary>
Motivation: 研究DMMs在数值噪声和物理噪声下的性能变化，探索其从成功解决问题到失败的混沌转变过程。

Method: 通过改变积分时间步长（数值噪声）和添加随机扰动（物理噪声）分析DMMs的方程，研究功率谱和Lyapunov指数随噪声强度的变化。

Result: 发现噪声强度与Lyapunov指数的符号相关，且在特定条件下，即使Lyapunov指数为正，DMMs仍能解决问题；功率谱可区分系统行为的规则性和混沌性。

Conclusion: 数值噪声和物理噪声对DMMs的影响相似，功率谱可用于优化DMMs的动态运行状态。

Abstract: Digital memcomputing machines (DMMs) have been designed to solve complex
combinatorial optimization problems. Since DMMs are fundamentally classical
dynamical systems, their ordinary differential equations (ODEs) can be
efficiently simulated on modern computers. This provides a unique platform to
study their performance under various conditions. An aspect that has received
little attention so far is how their performance is affected by the numerical
errors in the solution of their ODEs and the physical noise they would be
naturally subject to if built in hardware. Here, we analyze these two aspects
in detail by varying the integration time step (numerical noise) and adding
stochastic perturbations (physical noise) into the equations of DMMs. We are
particularly interested in understanding how noise induces a chaotic transition
that marks the shift from successful problem-solving to failure in these
systems. Our study includes an analysis of power spectra and Lyapunov exponents
depending on the noise strength. The results reveal a correlation between the
instance solvability and the sign of the ensemble averaged mean largest
Lyapunov exponent. Interestingly, we find a regime in which DMMs with positive
mean largest Lyapunov exponents still exhibit solvability. Furthermore, the
power spectra provide additional information about our system by distinguishing
between regular behavior (peaks) and chaotic behavior (broadband spectrum).
Therefore, power spectra could be utilized to control whether a DMM operates in
the optimal dynamical regime. Overall, we find that the qualitative effects of
numerical and physical noise are mostly similar, despite their fundamentally
different origin.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders](https://arxiv.org/abs/2506.14937)
*Luan Gonçalves Miranda,Pedro Ivo da Cruz,Murilo Bellezoni Loiola*

Main category: cs.LG

TL;DR: 该论文提出通过机器学习算法自动定义自动编码器（AE）的重建误差阈值，以优化异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于自动编码器的重建误差分类阈值非标准化且直接影响检测性能，因此需要自动定义该阈值。

Method: 评估了三种机器学习算法（K-近邻、K-均值和支持向量机）用于自动定义阈值。

Result: 未明确说明具体结果，但提出了算法的评估方法。

Conclusion: 自动定义阈值的方法有望提升异常检测系统的性能。

Abstract: Currently, digital security mechanisms like Anomaly Detection Systems using
Autoencoders (AE) show great potential for bypassing problems intrinsic to the
data, such as data imbalance. Because AE use a non-trivial and nonstandardized
separation threshold to classify the extracted reconstruction error, the
definition of this threshold directly impacts the performance of the detection
process. Thus, this work proposes the automatic definition of this threshold
using some machine learning algorithms. For this, three algorithms were
evaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.

</details>


### [67] [FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models](https://arxiv.org/abs/2506.14824)
*Yao Zhang,Hewei Gao,Haokun Chen,Weiguo Li,Yunpu Ma,Volker Tresp*

Main category: cs.LG

TL;DR: FedNano是一个针对多模态大语言模型（MLLMs）的联邦学习框架，通过引入轻量级模块NanoEdge，减少客户端部署和通信开销，同时处理异构数据和资源限制。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs在分布式多模态数据和隐私要求下的部署挑战，现有联邦学习方法因MLLMs巨大规模和通信需求不适用。

Method: 提出FedNano框架，服务器端集中管理LLM，客户端引入轻量级NanoEdge模块，使用低秩适配器减少存储和通信开销。

Result: FedNano减少客户端存储95%，通信开销仅为模型参数的0.01%，实验表现优于现有联邦学习方法。

Conclusion: FedNano解决了MLLMs与联邦学习的规模化冲突，实现了可扩展的去中心化多模态AI系统。

Abstract: Multimodal Large Language Models (MLLMs) excel in tasks like multimodal
reasoning and cross-modal retrieval but face deployment challenges in
real-world scenarios due to distributed multimodal data and strict privacy
requirements. Federated Learning (FL) offers a solution by enabling
collaborative model training without centralizing data. However, realizing FL
for MLLMs presents significant challenges, including high computational
demands, limited client capacity, substantial communication costs, and
heterogeneous client data. Existing FL methods assume client-side deployment of
full models, an assumption that breaks down for large-scale MLLMs due to their
massive size and communication demands. To address these limitations, we
propose FedNano, the first FL framework that centralizes the LLM on the server
while introducing NanoEdge, a lightweight module for client-specific
adaptation. NanoEdge employs modality-specific encoders, connectors, and
trainable NanoAdapters with low-rank adaptation. This design eliminates the
need to deploy LLM on clients, reducing client-side storage by 95%, and
limiting communication overhead to only 0.01% of the model parameters. By
transmitting only compact NanoAdapter updates, FedNano handles heterogeneous
client data and resource constraints while preserving privacy. Experiments
demonstrate that FedNano outperforms prior FL baselines, bridging the gap
between MLLM scale and FL feasibility, and enabling scalable, decentralized
multimodal AI systems.

</details>


### [68] [MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh Smoothing](https://arxiv.org/abs/2506.15571)
*Le Vu Anh,Nguyen Viet Anh,Mehmet Dik,Tu Nguyen Thi Ngoc*

Main category: cs.LG

TL;DR: MicroRicci是一种自适应的局部Ricci流求解器，通过贪婪的病症解码步骤和微型神经模块，显著提升实时网格平滑的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 解决实时网格平滑中传统Ricci流全局更新成本高和启发式方法收敛慢或调参难的问题。

Method: 结合编码理论的贪婪病症解码步骤（O(E)时间）和两个自适应选择顶点与步长的微型神经模块。

Result: 在110个SJTU-TMQA网格上，迭代次数从950±140降至400±80（2.4倍加速），曲率分布从0.19收紧至0.185，UV失真与MOS相关性为r=-0.93，单次迭代仅增加0.25ms。

Conclusion: MicroRicci因其线性时间更新、自动超参数适应和高几何/感知质量，特别适用于图形、模拟等实时资源受限场景。

Abstract: Real-time mesh smoothing at scale remains a formidable challenge: classical
Ricci-flow solvers demand costly global updates, while greedy heuristics suffer
from slow convergence or brittle tuning. We present MicroRicci, the first truly
self-tuning, local Ricci-flow solver that borrows ideas from coding theory and
packs them into just 1K + 200 parameters. Its primary core is a greedy
syndrome-decoding step that pinpoints and corrects the largest curvature error
in O(E) time, augmented by two tiny neural modules that adaptively choose
vertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,
MicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),
tightens curvature spread from 0.19 to 0.185, and achieves a remarkable
UV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per
iteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration
over state-of-the-art methods. MicroRicci's combination of linear-time updates,
automatic hyperparameter adaptation, and high-quality geometric and perceptual
results makes it well suited for real-time, resource-limited applications in
graphics, simulation, and related fields.

</details>


### [69] [Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model](https://arxiv.org/abs/2506.14830)
*Zhizhao Wen,Ruoxin Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 本研究提出了一种结合BiGRU和多头注意力机制的混合模型，用于提升SSD健康状态预测的准确性和稳定性，实验结果显示其在训练集和测试集上的分类准确率均超过92%，泛化能力强。


<details>
  <summary>Details</summary>
Motivation: SSD健康状态预测对数据可靠性至关重要，传统模型存在泛化能力不足的问题，需开发更有效的技术。

Method: 采用BiGRU网络捕捉SSD退化特征的双向时序依赖，并结合多头注意力机制动态分配特征权重，增强对关键指标的敏感性。

Result: 模型在训练集和测试集上的分类准确率分别为92.70%和92.44%，AUC为0.94，显示出色的性能和泛化能力。

Conclusion: 该模型为SSD健康预测提供了新技术路径，可显著降低数据丢失风险，优化维护成本，支持云计算和边缘存储环境的智能决策。

Abstract: Aiming at the critical role of SSD health state prediction in data
reliability assurance, this study proposes a hybrid BiGRU-MHA model that
incorporates a multi-head attention mechanism to enhance the accuracy and
stability of storage device health classification. The model innovatively
integrates temporal feature extraction and key information focusing
capabilities. Specifically, it leverages the bidirectional timing modeling
advantages of the BiGRU network to capture both forward and backward
dependencies of SSD degradation features. Simultaneously, the multi-head
attention mechanism dynamically assigns feature weights, improving the model's
sensitivity to critical health indicators. Experimental results show that the
proposed model achieves classification accuracies of 92.70% on the training set
and 92.44% on the test set, with a minimal performance gap of only 0.26%,
demonstrating excellent generalization ability. Further analysis using the
receiver operating characteristic (ROC) curve shows an area under the curve
(AUC) of 0.94 on the test set, confirming the model's robust binary
classification performance. This work not only presents a new technical
approach for SSD health prediction but also addresses the generalization
bottleneck of traditional models, offering a verifiable method with practical
value for preventive maintenance of industrial-grade storage systems. The
results show the model can significantly reduce data loss risks by providing
early failure warnings and help optimize maintenance costs, supporting
intelligent decision-making in building reliable storage systems for cloud
computing data centers and edge storage environments.

</details>


### [70] [Event-Driven Online Vertical Federated Learning](https://arxiv.org/abs/2506.14911)
*Ganyu Wang,Boyu Wang,Bin Gu,Charles Ling*

Main category: cs.LG

TL;DR: 该论文提出了一个事件驱动的在线纵向联邦学习（VFL）框架，解决了异步数据流和动态环境下的挑战，并通过动态局部遗憾分析验证了其稳定性和高效性。


<details>
  <summary>Details</summary>
Motivation: 在线学习在纵向联邦学习中更具适应性，但因VFL的特性（客户端特征不重叠且数据异步生成）而面临挑战，这些问题在先前研究中被忽视。

Method: 提出事件驱动的在线VFL框架，仅激活部分客户端，其余被动协作，并引入动态局部遗憾（DLR）处理非凸和非平稳环境问题。

Result: 实验表明，该框架在非平稳数据条件下更稳定，通信和计算成本显著降低。

Conclusion: 该框架有效解决了在线VFL中的异步和动态挑战，具有实用性和高效性。

Abstract: Online learning is more adaptable to real-world scenarios in Vertical
Federated Learning (VFL) compared to offline learning. However, integrating
online learning into VFL presents challenges due to the unique nature of VFL,
where clients possess non-intersecting feature sets for the same sample. In
real-world scenarios, the clients may not receive data streaming for the
disjoint features for the same entity synchronously. Instead, the data are
typically generated by an \emph{event} relevant to only a subset of clients. We
are the first to identify these challenges in online VFL, which have been
overlooked by previous research. To address these challenges, we proposed an
event-driven online VFL framework. In this framework, only a subset of clients
were activated during each event, while the remaining clients passively
collaborated in the learning process. Furthermore, we incorporated
\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by
online learning problems with non-convex models within a non-stationary
environment. We conducted a comprehensive regret analysis of our proposed
framework, specifically examining the DLR under non-convex conditions with
event-driven online VFL. Extensive experiments demonstrated that our proposed
framework was more stable than the existing online VFL framework under
non-stationary data conditions while also significantly reducing communication
and computation costs.

</details>


### [71] [Centroid Approximation for Byzantine-Tolerant Federated Learning](https://arxiv.org/abs/2506.15264)
*Mélanie Cambus,Darya Melnyk,Tijana Milentijević,Stefan Schmid*

Main category: cs.LG

TL;DR: 联邦学习在分布式环境中允许客户端本地保留数据进行模型训练，本文研究了其对拜占庭行为的鲁棒性，揭示了平均/中心点与分布式计算有效性条件之间的权衡，并提出了新的上下界近似结果。


<details>
  <summary>Details</summary>
Motivation: 探讨联邦学习在拜占庭行为下的鲁棒性，分析平均聚合的局限性与有效性条件的冲突。

Method: 提出新的分析框架，给出中心点近似的新下界与上界，并设计新算法在凸有效性条件下实现最优近似。

Result: 证明现有下界的紧性，提出$rac{n-t}{t}$与$rac{n}{	}$的近似界限，并通过实验验证结果。

Conclusion: 联邦学习在拜占庭环境下存在理论极限，本文提供了新的近似界限与算法，为实际应用提供了指导。

Abstract: Federated learning allows each client to keep its data locally when training
machine learning models in a distributed setting. Significant recent research
established the requirements that the input must satisfy in order to guarantee
convergence of the training loop. This line of work uses averaging as the
aggregation rule for the training models. In particular, we are interested in
whether federated learning is robust to Byzantine behavior, and observe and
investigate a tradeoff between the average/centroid and the validity conditions
from distributed computing. We show that the various validity conditions alone
do not guarantee a good approximation of the average. Furthermore, we show that
reaching good approximation does not give good results in experimental settings
due to possible Byzantine outliers. Our main contribution is the first lower
bound of $\min\{\frac{n-t}{t},\sqrt{d}\}$ on the centroid approximation under
box validity that is often considered in the literature, where $n$ is the
number of clients, $t$ the upper bound on the number of Byzantine faults, and
$d$ is the dimension of the machine learning model. We complement this lower
bound by an upper bound of $2\min\{n,\sqrt{d}\}$, by providing a new analysis
for the case $n<d$. In addition, we present a new algorithm that achieves a
$\sqrt{2d}$-approximation under convex validity, which also proves that the
existing lower bound in the literature is tight. We show that all presented
bounds can also be achieved in the distributed peer-to-peer setting. We
complement our analytical results with empirical evaluations in federated
stochastic gradient descent and federated averaging settings.

</details>


### [72] [Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction](https://arxiv.org/abs/2506.15626)
*Vincent Roca,Marc Tommasi,Paul Andrey,Aurélien Bellet,Markus D. Schirmer,Hilde Henon,Laurent Puy,Julien Ramon,Grégory Kuchcinski,Martin Bretzner,Renaud Lopes*

Main category: cs.LG

TL;DR: 研究评估了联邦学习在缺血性脑卒中患者中预测脑龄的准确性，表明其优于单中心模型，并揭示了脑龄与血管风险因素及功能恢复的关联。


<details>
  <summary>Details</summary>
Motivation: 脑龄是反映脑健康的生物标志物，但传统方法需要集中大量数据，受隐私限制。研究旨在探索联邦学习是否能在分散数据下提供准确的脑龄预测。

Method: 使用1674名患者的FLAIR图像，比较集中学习、联邦学习和单中心学习的脑龄预测效果，并分析脑龄与临床特征的关联。

Result: 联邦学习虽略逊于集中学习，但显著优于单中心模型。脑龄与糖尿病等风险因素及功能恢复显著相关。

Conclusion: 联邦学习可用于脑龄预测而不需数据集中，脑龄与卒中预后的关联显示了其在预后建模中的潜力。

Abstract: $\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a
neuroimaging biomarker reflecting brain health. However, training robust
BrainAGE models requires large datasets, often restricted by privacy concerns.
This study evaluates the performance of federated learning (FL) for BrainAGE
estimation in ischemic stroke patients treated with mechanical thrombectomy,
and investigates its association with clinical phenotypes and functional
outcomes.
  $\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients
across 16 hospital centers. We implemented standard machine learning and deep
learning models for BrainAGE estimates under three data management strategies:
centralized learning (pooled data), FL (local training at each site), and
single-site learning. We reported prediction errors and examined associations
between BrainAGE and vascular risk factors (e.g., diabetes mellitus,
hypertension, smoking), as well as functional outcomes at three months
post-stroke. Logistic regression evaluated BrainAGE's predictive value for
these outcomes, adjusting for age, sex, vascular risk factors, stroke severity,
time between MRI and arterial puncture, prior intravenous thrombolysis, and
recanalisation outcome.
  $\textbf{Results:}$ While centralized learning yielded the most accurate
predictions, FL consistently outperformed single-site models. BrainAGE was
significantly higher in patients with diabetes mellitus across all models.
Comparisons between patients with good and poor functional outcomes, and
multivariate predictions of these outcomes showed the significance of the
association between BrainAGE and post-stroke recovery.
  $\textbf{Conclusion:}$ FL enables accurate age predictions without data
centralization. The strong association between BrainAGE, vascular risk factors,
and post-stroke recovery highlights its potential for prognostic modeling in
stroke care.

</details>


### [73] [MedSyn: Enhancing Diagnostics with Human-AI Collaboration](https://arxiv.org/abs/2506.14774)
*Burcu Sayin,Ipek Baris Schlicht,Ngoc Vo Hong,Sara Allievi,Jacopo Staiano,Pasquale Minervini,Andrea Passerini*

Main category: cs.LG

TL;DR: 提出人机交互框架MedSyn，支持医生与LLM动态对话以优化临床决策。


<details>
  <summary>Details</summary>
Motivation: 临床决策常受认知偏差和不完整信息影响，现有LLM工具交互有限，未能体现实际医疗复杂性。

Method: 设计多步骤交互对话框架MedSyn，医生与LLM动态交流并挑战建议。

Result: 实验证明开源LLM有潜力成为医师助手，提升决策准确性。

Conclusion: 未来将进一步通过真实医生交互验证MedSyn在诊断和患者预后中的实用性。

Abstract: Clinical decision-making is inherently complex, often influenced by cognitive
biases, incomplete information, and case ambiguity. Large Language Models
(LLMs) have shown promise as tools for supporting clinical decision-making, yet
their typical one-shot or limited-interaction usage may overlook the
complexities of real-world medical practice. In this work, we propose a hybrid
human-AI framework, MedSyn, where physicians and LLMs engage in multi-step,
interactive dialogues to refine diagnoses and treatment decisions. Unlike
static decision-support tools, MedSyn enables dynamic exchanges, allowing
physicians to challenge LLM suggestions while the LLM highlights alternative
perspectives. Through simulated physician-LLM interactions, we assess the
potential of open-source LLMs as physician assistants. Results show open-source
LLMs are promising as physician assistants in the real world. Future work will
involve real physician interactions to further validate MedSyn's usefulness in
diagnostic accuracy and patient outcomes.

</details>


### [74] [ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification](https://arxiv.org/abs/2506.14783)
*Mohamed Masry,Mohamed Amen,Mohamed Elzyat,Mohamed Hamed,Norhan Magdy,Maram Khaled*

Main category: cs.LG

TL;DR: 该研究提出了一种结合脑电图（EEG）和同步眼动追踪数据的ETS框架，用于开放词汇文本生成和情感分类，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决非侵入式EEG在开放词汇场景下解码自然语言的挑战，传统方法因噪声和变异性表现不佳。

Method: 提出ETS框架，整合EEG与眼动追踪数据，专注于开放词汇文本生成和情感分类任务。

Result: 模型在BLEU和Rouge得分上表现优异，情感分类F1分数提升10%，且适用于多源数据。

Conclusion: ETS框架展示出在高性能开放词汇EEG-文本转换系统中的巨大潜力。

Abstract: Decoding natural language from brain activity using non-invasive
electroencephalography (EEG) remains a significant challenge in neuroscience
and machine learning, particularly for open-vocabulary scenarios where
traditional methods struggle with noise and variability. Previous studies have
achieved high accuracy on small-closed vocabularies, but it still struggles on
open vocabularies. In this study, we propose ETS, a framework that integrates
EEG with synchronized eye-tracking data to address two critical tasks: (1)
open-vocabulary text generation and (2) sentiment classification of perceived
language. Our model achieves a superior performance on BLEU and Rouge score for
EEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment
classification, which significantly outperforms supervised baselines.
Furthermore, we show that our proposed model can handle data from various
subjects and sources, showing great potential for high performance open
vocabulary eeg-to-text system.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [75] [SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning](https://arxiv.org/abs/2506.15154)
*Anuradha Chopra,Abhinaba Roy,Dorien Herremans*

Main category: cs.SD

TL;DR: 本文介绍了多任务音乐描述模型SonicVerse，通过结合音乐特征检测任务（如调性检测、人声检测等）来生成详细的音乐描述。


<details>
  <summary>Details</summary>
Motivation: 准确的音乐描述可以丰富音乐数据库并推动音乐AI研究，但其生成需要同时捕捉音乐的低级声学细节和高级属性。

Method: 提出一种基于投影的架构，将音频输入转换为语言标记，并通过专用辅助头检测音乐特征，进一步将这些特征投影为语言标记以增强描述输入。

Result: 实验结果表明，结合音乐特征可以提高生成描述的质感和细节。

Conclusion: SonicVerse不仅能生成短音乐片段的丰富描述，还能通过链接输出生成长时间音乐作品的详细描述，扩展了音乐描述的应用场景。

Abstract: Detailed captions that accurately reflect the characteristics of a music
piece can enrich music databases and drive forward research in music AI. This
paper introduces a multi-task music captioning model, SonicVerse, that
integrates caption generation with auxiliary music feature detection tasks such
as key detection, vocals detection, and more, so as to directly capture both
low-level acoustic details as well as high-level musical attributes. The key
contribution is a projection-based architecture that transforms audio input
into language tokens, while simultaneously detecting music features through
dedicated auxiliary heads. The outputs of these heads are also projected into
language tokens, to enhance the captioning input. This framework not only
produces rich, descriptive captions for short music fragments but also directly
enables the generation of detailed time-informed descriptions for longer music
pieces, by chaining the outputs using a large-language model. To train the
model, we extended the MusicBench dataset by annotating it with music features
using MIRFLEX, a modular music feature extractor, resulting in paired audio,
captions and music feature data. Experimental results show that incorporating
features in this way improves the quality and detail of the generated captions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [76] [EmojiVoice: Towards long-term controllable expressivity in robot speech](https://arxiv.org/abs/2506.15085)
*Paige Tuttösí,Shivam Mehta,Zachary Syvenky,Bermet Burkanova,Gustav Eje Henter,Angelica Lim*

Main category: cs.RO

TL;DR: EmojiVoice是一个免费的、可定制的文本转语音工具包，旨在为社交机器人提供具有时间变化的表达性语音，使用表情符号提示精细控制表达性。


<details>
  <summary>Details</summary>
Motivation: 社交机器人通常缺乏人类语音中的长期表达变化，而现有的大模型文本转语音系统难以离线部署。

Method: 采用表情符号提示法精细控制表达性，使用轻量级Matcha-TTS骨干网络实时生成语音，并通过三个案例研究验证效果。

Result: 在讲故事任务中，表情符号提示显著提升了语音感知和表达性，但在助手场景下表达性语音不受欢迎。

Conclusion: EmojiVoice为社交机器人提供了有效的表达性语音工具，但其适用性因场景而异。

Abstract: Humans vary their expressivity when speaking for extended periods to maintain
engagement with their listener. Although social robots tend to be deployed with
``expressive'' joyful voices, they lack this long-term variation found in human
speech. Foundation model text-to-speech systems are beginning to mimic the
expressivity in human speech, but they are difficult to deploy offline on
robots. We present EmojiVoice, a free, customizable text-to-speech (TTS)
toolkit that allows social roboticists to build temporally variable, expressive
speech on social robots. We introduce emoji-prompting to allow fine-grained
control of expressivity on a phase level and use the lightweight Matcha-TTS
backbone to generate speech in real-time. We explore three case studies: (1) a
scripted conversation with a robot assistant, (2) a storytelling robot, and (3)
an autonomous speech-to-speech interactive agent. We found that using varied
emoji prompting improved the perception and expressivity of speech over a long
period in a storytelling task, but expressive voice was not preferred in the
assistant use case.

</details>


### [77] [I Know You're Listening: Adaptive Voice for HRI](https://arxiv.org/abs/2506.15107)
*Paige Tuttösí*

Main category: cs.RO

TL;DR: 该论文提出了一种针对语言教学机器人的任务特定合成声音系统，通过轻量级、适应性强的语音合成技术，提高语音表达的丰富性和环境适应性，同时为L2学习者设计了更清晰的语音模式。


<details>
  <summary>Details</summary>
Motivation: 现有语言教学机器人的语音合成缺乏任务特定的优化，影响教学效果。论文旨在填补这一空白，设计更高效、适应性强的语音系统。

Method: 1. 使用微调的Matcha-TTS和表情符号提示生成富有表现力的语音；2. 根据物理和社交环境调整语音参数；3. 基于语言特性优化语音清晰度，设计“L2清晰模式”。

Result: 实验表明，优化后的语音更具表现力、环境适应性和清晰度，特别适合L2学习者，减少了听写错误。

Conclusion: 论文通过多方面的语音优化，显著提升了语言教学机器人的语音表达效果和适用性。

Abstract: While the use of social robots for language teaching has been explored, there
remains limited work on a task-specific synthesized voices for language
teaching robots. Given that language is a verbal task, this gap may have severe
consequences for the effectiveness of robots for language teaching tasks. We
address this lack of L2 teaching robot voices through three contributions: 1.
We address the need for a lightweight and expressive robot voice. Using a
fine-tuned version of Matcha-TTS, we use emoji prompting to create an
expressive voice that shows a range of expressivity over time. The voice can
run in real time with limited compute resources. Through case studies, we found
this voice more expressive, socially appropriate, and suitable for long periods
of expressive speech, such as storytelling. 2. We explore how to adapt a
robot's voice to physical and social ambient environments to deploy our voices
in various locations. We found that increasing pitch and pitch rate in noisy
and high-energy environments makes the robot's voice appear more appropriate
and makes it seem more aware of its current environment. 3. We create an
English TTS system with improved clarity for L2 listeners using known
linguistic properties of vowels that are difficult for these listeners. We used
a data-driven, perception-based approach to understand how L2 speakers use
duration cues to interpret challenging words with minimal tense (long) and lax
(short) vowels in English. We found that the duration of vowels strongly
influences the perception for L2 listeners and created an "L2 clarity mode" for
Matcha-TTS that applies a lengthening to tense vowels while leaving lax vowels
unchanged. Our clarity mode was found to be more respectful, intelligible, and
encouraging than base Matcha-TTS while reducing transcription errors in these
challenging tense/lax minimal pairs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [78] [A theory of Lending Protocols in DeFi](https://arxiv.org/abs/2506.15295)
*Massimo Bartoletti,Enrico Lipparini*

Main category: cs.GT

TL;DR: 本文提出了一种形式化模型来分析借贷协议的经济和战略动态，解决了传统协议中激励机制的复杂性和潜在漏洞问题。


<details>
  <summary>Details</summary>
Motivation: 去中心化金融（DeFi）借贷协议因其开放性和缺乏中心化监管，可能导致激励机制漏洞和用户策略滥用。

Method: 通过形式化模型，捕捉主流借贷平台的核心特征，并分析其经济属性和用户策略。

Result: 识别并证明了借贷协议中与经济和战略动态相关的关键属性。

Conclusion: 形式化模型有助于理解和改进借贷协议的激励机制，提升其稳定性和安全性。

Abstract: Lending protocols are one of the main applications of Decentralized Finance
(DeFi), enabling crypto-assets loan markets with a total value estimated in the
tens of billions of dollars. Unlike traditional lending systems, these
protocols operate without relying on trusted authorities or off-chain
enforcement mechanisms. To achieve key economic goals such as stability of the
loan market, they devise instead trustless on-chain mechanisms, such as
rewarding liquidators who repay the loans of under-collateralized borrowers by
awarding them part of the borrower's collateral. The complexity of these
incentive mechanisms, combined with their entanglement in low-level
implementation details, makes it challenging to precisely assess the structural
and economic properties of lending protocols, as well as to analyze user
strategies and attacks. Crucially, since participation is open to anyone, any
weaknesses in the incentive mechanism may give rise to unintended emergent
behaviours, or even enable adversarial strategies aimed at making profits to
the detriment of legit users, or at undermining the stability of the protocol.
In this work, we propose a formal model of lending protocols that captures the
essential features of mainstream platforms, enabling us to identify and prove
key properties related to their economic and strategic dynamics.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [79] [Empirical Studies of Large Scale Environment Scanning by Consumer Electronics](https://arxiv.org/abs/2506.14771)
*Mengyuan Wang,Yang Liu,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: eess.IV

TL;DR: 该论文对消费级3D扫描设备Matterport Pro3在大规模环境重建中的表现进行了实证评估，结果显示其在点云密度和对齐精度上优于iPhone，适合大规模应用。


<details>
  <summary>Details</summary>
Motivation: 评估Matterport Pro3在大型建筑扫描中的实际效果，探索其性能优势和限制，并提出改进方法。

Method: 通过1,099个扫描点对六层建筑（17,567平方米）进行详细扫描，并与iPhone对比分析点云密度和对齐精度。

Result: Pro3生成1,877,324个点（iPhone为506,961个），RMSE为0.0118米，C2C平均误差为0.0408米。

Conclusion: Matterport Pro3在性能和成本效益上表现出色，适合大规模3D建模应用。

Abstract: This paper presents an empirical evaluation of the Matterport Pro3, a
consumer-grade 3D scanning device, for large-scale environment reconstruction.
We conduct detailed scanning (1,099 scanning points) of a six-floor building
(17,567 square meters) and assess the device's effectiveness, limitations, and
performance enhancements in diverse scenarios. Challenges encountered during
the scanning are addressed through proposed solutions, while we also explore
advanced methods to overcome them more effectively. Comparative analysis with
another consumer-grade device (iPhone) highlights the Pro3's balance between
cost-effectiveness and performance. The Matterport Pro3 achieves a denser point
cloud with 1,877,324 points compared to the iPhone's 506,961 points and higher
alignment accuracy with an RMSE of 0.0118 meters. The cloud-to-cloud (C2C)
average distance error between the two point cloud models is 0.0408 meters,
with a standard deviation of 0.0715 meters. The study demonstrates the Pro3's
ability to generate high-quality 3D models suitable for large-scale
applications, leveraging features such as LiDAR and advanced alignment
techniques.

</details>


### [80] [ABC: Adaptive BayesNet Structure Learning for Computational Scalable Multi-task Image Compression](https://arxiv.org/abs/2506.15228)
*Yufeng Zhang,Wenrui Dai,Hang Yu,Shizhan Liu,Junhui Hou,Jianguo Li,Weiyao Lin*

Main category: eess.IV

TL;DR: ABC框架通过贝叶斯网络结构学习实现了神经图像压缩（NIC）的计算可扩展性，解决了现有方法在计算复杂性控制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的神经图像压缩方法计算需求高，限制了其广泛使用，且缺乏全面的计算复杂性控制。

Method: ABC框架引入异构二分贝叶斯网络管理神经网络骨干计算、同构多分贝叶斯网络优化自回归单元处理，以及自适应控制模块动态调整网络结构。

Result: 实验表明ABC在保持压缩性能的同时，实现了全面的计算可扩展性，具有更好的复杂性和更广的控制范围。

Conclusion: ABC是一个通用框架，能够与多种NIC架构集成，为计算可扩展性提供了稳健解决方案。

Abstract: Neural Image Compression (NIC) has revolutionized image compression with its
superior rate-distortion performance and multi-task capabilities, supporting
both human visual perception and machine vision tasks. However, its widespread
adoption is hindered by substantial computational demands. While existing
approaches attempt to address this challenge through module-specific
optimizations or pre-defined complexity levels, they lack comprehensive control
over computational complexity. We present ABC (Adaptive BayesNet structure
learning for computational scalable multi-task image Compression), a novel,
comprehensive framework that achieves computational scalability across all NIC
components through Bayesian network (BayesNet) structure learning. ABC
introduces three key innovations: (i) a heterogeneous bipartite BayesNet
(inter-node structure) for managing neural backbone computations; (ii) a
homogeneous multipartite BayesNet (intra-node structure) for optimizing
autoregressive unit processing; and (iii) an adaptive control module that
dynamically adjusts the BayesNet structure based on device capabilities, input
data complexity, and downstream task requirements. Experiments demonstrate that
ABC enables full computational scalability with better complexity adaptivity
and broader complexity control span, while maintaining competitive compression
performance. Furthermore, the framework's versatility allows integration with
various NIC architectures that employ BayesNet representations, making it a
robust solution for ensuring computational scalability in NIC applications.
Code is available in https://github.com/worldlife123/cbench_BaSIC.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [81] [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
*Yining Hong,Rui Sun,Bingxuan Li,Xingcheng Yao,Maxine Wu,Alexander Chien,Da Yin,Ying Nian Wu,Zhecan James Wang,Kai-Wei Chang*

Main category: cs.AI

TL;DR: 论文提出了一种新的AI代理范式——Embodied Web Agents，旨在整合物理世界交互与网络规模推理，以解决需要跨领域智能的任务。


<details>
  <summary>Details</summary>
Motivation: 现有的AI代理通常是孤立的，无法同时处理物理世界交互和网络规模知识，限制了其在需要跨领域智能的任务中的表现。

Method: 开发了Embodied Web Agents任务环境，一个集成了3D环境和功能性网络接口的仿真平台，并发布了包含多样化任务的基准测试。

Result: 实验结果表明，当前AI系统与人类能力之间存在显著差距，为跨领域智能的发展提供了挑战与机遇。

Conclusion: 论文为整合物理和数字智能的AI代理提供了新思路和工具，相关数据和代码已公开。

Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.

</details>


### [82] [Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations](https://arxiv.org/abs/2506.13776)
*Kevin L. Wei,Patricia Paskov,Sunishchal Dev,Michael J. Byun,Anka Reuel,Xavier Roberts-Gaal,Rachel Calcott,Evie Coxon,Chinmay Deshpande*

Main category: cs.AI

TL;DR: 本文主张在基础模型评估中，人类基线的设置需更严谨透明，并提出相关建议和检查清单。


<details>
  <summary>Details</summary>
Motivation: 现行人类基线方法不够严谨和透明，导致无法准确衡量AI性能差异。

Method: 通过分析测量理论和AI评估文献，提出设计、执行和报告人类基线的框架及检查清单。

Result: 系统审查了115项研究，发现现有基线方法的不足，并提出了改进建议。

Conclusion: 研究旨在推动更严谨的AI评估实践，以服务研究社区和政策制定者。

Abstract: In this position paper, we argue that human baselines in foundation model
evaluations must be more rigorous and more transparent to enable meaningful
comparisons of human vs. AI performance, and we provide recommendations and a
reporting checklist towards this end. Human performance baselines are vital for
the machine learning community, downstream users, and policymakers to interpret
AI evaluations. Models are often claimed to achieve "super-human" performance,
but existing baselining methods are neither sufficiently rigorous nor
sufficiently well-documented to robustly measure and assess performance
differences. Based on a meta-review of the measurement theory and AI evaluation
literatures, we derive a framework with recommendations for designing,
executing, and reporting human baselines. We synthesize our recommendations
into a checklist that we use to systematically review 115 human baselines
(studies) in foundation model evaluations and thus identify shortcomings in
existing baselining methods; our checklist can also assist researchers in
conducting human baselines and reporting results. We hope our work can advance
more rigorous AI evaluation practices that can better serve both the research
community and policymakers. Data is available at:
https://github.com/kevinlwei/human-baselines

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [83] [Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices](https://arxiv.org/abs/2506.15028)
*Gargi Mitra,Mohammadreza Hallajiyan,Inji Kim,Athish Pranav Dharmalingam,Mohammed Elnawawy,Shahrear Iqbal,Karthik Pattabiraman,Homa Alemzadeh*

Main category: cs.CR

TL;DR: AI/ML在医疗设备中的应用提升了医疗诊断和治疗能力，但也带来了严重的网络安全风险，亟需在预上市阶段解决这些问题。


<details>
  <summary>Details</summary>
Motivation: AI/ML医疗设备的网络安全风险对患者安全构成威胁，需从设计阶段开始防范。

Method: 分析公开的设备召回、不良事件和已知漏洞数据，开发工具支持预上市风险评估。

Result: 提出一套工具和技术，帮助制造商将网络安全作为核心设计原则。

Conclusion: 通过早期风险评估和工具支持，可以提升AI/ML医疗设备的安全性，保障患者安全。

Abstract: The integration of AI/ML into medical devices is rapidly transforming
healthcare by enhancing diagnostic and treatment facilities. However, this
advancement also introduces serious cybersecurity risks due to the use of
complex and often opaque models, extensive interconnectivity, interoperability
with third-party peripheral devices, Internet connectivity, and vulnerabilities
in the underlying technologies. These factors contribute to a broad attack
surface and make threat prevention, detection, and mitigation challenging.
Given the highly safety-critical nature of these devices, a cyberattack on
these devices can cause the ML models to mispredict, thereby posing significant
safety risks to patients. Therefore, ensuring the security of these devices
from the time of design is essential. This paper underscores the urgency of
addressing the cybersecurity challenges in ML-enabled medical devices at the
pre-market phase. We begin by analyzing publicly available data on device
recalls and adverse events, and known vulnerabilities, to understand the threat
landscape of AI/ML-enabled medical devices and their repercussions on patient
safety. Building on this analysis, we introduce a suite of tools and techniques
designed by us to assist security analysts in conducting comprehensive
premarket risk assessments. Our work aims to empower manufacturers to embed
cybersecurity as a core design principle in AI/ML-enabled medical devices,
thereby making them safe for patients.

</details>


### [84] [Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine](https://arxiv.org/abs/2506.15070)
*Rasha Karakchi,Rye Stahle-Smith,Nishant Chinnasami,Tiffany Yu*

Main category: cs.CR

TL;DR: SPiME是一种轻量级、可扩展且与FPGA兼容的内存处理器加密架构，通过在内存处理框架中直接集成AES-128，实现了高效的能量和数据移动优化。


<details>
  <summary>Details</summary>
Motivation: 物联网应用的快速发展对边缘计算的高效、高吞吐和节能数据处理提出了更高要求，传统CPU加密方法存在性能瓶颈和数据移动过多的问题。

Method: 提出了SPiME架构，采用并行内存处理单元阵列，每个单元结合AES核心和最小控制单元，实现分布式就地加密。

Result: SPiME在高端FPGA上可扩展至4000多个并行单元，资源占用低于5%，持续加密吞吐超过25Gbps，且延迟低且可预测。

Conclusion: SPiME设计具有便携性、可配置性和资源高效性，适用于安全边缘计算和嵌入式加密系统。

Abstract: The exponential growth of Internet of Things (IoT) applications has
intensified the demand for efficient, high-throughput, and energy-efficient
data processing at the edge. Conventional CPU-centric encryption methods suffer
from performance bottlenecks and excessive data movement, especially in
latency-sensitive and resource-constrained environments. In this paper, we
present SPiME, a lightweight, scalable, and FPGA-compatible Secure
Processor-in-Memory Encryption architecture that integrates the Advanced
Encryption Standard (AES-128) directly into a Processing-in-Memory (PiM)
framework. SPiME is designed as a modular array of parallel PiM units, each
combining an AES core with a minimal control unit to enable distributed
in-place encryption with minimal overhead. The architecture is fully
implemented in Verilog and tested on multiple AMD UltraScale and UltraScale+
FPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units
while maintaining less than 5\% utilization of key FPGA resources on high-end
devices. It delivers over 25~Gbps in sustained encryption throughput with
predictable, low-latency performance. The design's portability,
configurability, and resource efficiency make it a compelling solution for
secure edge computing, embedded cryptographic systems, and customizable
hardware accelerators.

</details>


### [85] [deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses](https://arxiv.org/abs/2506.15648)
*Georgios Androutsopoulos,Antonio Bianchi*

Main category: cs.CR

TL;DR: deepSURF是一个结合静态分析和LLM引导的模糊测试工具，用于高效检测Rust库中的内存安全漏洞，尤其针对不安全代码。


<details>
  <summary>Details</summary>
Motivation: 现有工具在检测Rust内存漏洞时能力有限，无法充分处理特定类型或依赖人工干预，因此开发deepSURF以解决这些问题。

Method: 通过替换泛型为自定义类型并动态生成模糊测试用例，结合LLM增强测试框架，探索复杂API交互。

Result: 在27个Rust库中成功重新发现20个已知漏洞和6个未知漏洞，表现优于现有工具。

Conclusion: deepSURF在检测Rust内存安全漏洞方面显著提升效果，为相关领域提供新工具支持。

Abstract: Although Rust ensures memory safety by default, it also permits the use of
unsafe code, which can introduce memory safety vulnerabilities if misused.
Unfortunately, existing tools for detecting memory bugs in Rust typically
exhibit limited detection capabilities, inadequately handle Rust-specific
types, or rely heavily on manual intervention.
  To address these limitations, we present deepSURF, a tool that integrates
static analysis with Large Language Model (LLM)-guided fuzzing harness
generation to effectively identify memory safety vulnerabilities in Rust
libraries, specifically targeting unsafe code. deepSURF introduces a novel
approach for handling generics by substituting them with custom types and
generating tailored implementations for the required traits, enabling the
fuzzer to simulate user-defined behaviors within the fuzzed library.
Additionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,
facilitating exploration of complex API interactions and significantly
increasing the likelihood of exposing memory safety vulnerabilities. We
evaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20
known memory safety bugs and uncovering 6 previously unknown vulnerabilities,
demonstrating clear improvements over state-of-the-art tools.

</details>
