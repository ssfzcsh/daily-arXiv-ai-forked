<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.MM](#cs.MM) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 9]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CV](#cs.CV) [Total: 7]
- [math.HO](#math.HO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [math.NA](#math.NA) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: 该论文评估了大语言模型（LLM）在为真实世界问题生成代码方面的进展，特别是针对微服务应用的代码合成，并提出了一种衡量规范难度的方法。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在生成微服务应用代码方面的能力，尤其是高难度规范下的表现。

Method: 定义了微服务应用的规范模板，开发了自动化测试框架，并对不同难度规范的代码合成进行了实验。

Result: 实验表明，强大的LLM（如GPT-3o-mini）在中难度规范下表现良好，但在高难度规范下表现不佳，主要因复杂业务逻辑、外部服务集成等因素。

Conclusion: LLM在高难度代码合成中面临挑战，需进一步研究改进，适用于实际问题的解决方案。

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [2] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 提出了一种基于强化学习的效率导向框架，通过动态探索和错误不敏感的方法，显著提升了生成代码的运行效率和正确性。


<details>
  <summary>Details</summary>
Motivation: 解决代码大语言模型生成的代码运行时效率低的问题，以适用于性能敏感场景。

Method: 采用效率导向的强化学习框架，结合动态探索、错误不敏感方法和在线探索，提出两阶段调优方法。

Result: 实验显示，该方法在7B模型上提升了10.18%的正确性和7.75%的运行效率，性能接近更大的模型。

Conclusion: 两阶段调优方法在代码正确性和运行效率上取得了平衡且显著的提升。

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [3] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera是一个基于LLM的SMT求解器模糊测试框架，通过生成可重用的逻辑表达式生成器，确保语法有效性并提高语义多样性，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有测试技术难以跟上SMT求解器的快速演变，且基于LLM的方法存在语法无效和高计算开销的问题。

Method: Chimera利用LLM从文档中提取上下文无关文法，并合成可组合的布尔项生成器，填充现有公式的结构骨架。

Result: 在Z3和cvc5上测试发现43个确认的bug，其中40个已被修复。

Conclusion: Chimera通过一次性LLM交互显著降低开销，同时有效提升了测试质量。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [4] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: 论文提出RCLAgent，一种基于多智能体递归思维框架的自适应根因定位方法，用于解决复杂微服务系统中的故障定位问题。通过实验验证，RCLAgent表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前微服务系统日益复杂，故障频繁，现有方法依赖预定义模式或缺乏解释性，难以适应动态环境。通过研究SREs的故障分析行为，作者设计了RCLAgent。

Method: RCLAgent采用多智能体的递归思维策略，整合多源数据和工具辅助分析，以单次请求高效定位根因。

Result: 实验显示，RCLAgent在公共数据集上表现优于现有方法，仅需单次请求即可准确定位故障。

Conclusion: RCLAgent显著提升了复杂微服务系统中根因定位的效率和准确性。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [5] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: XP2025研讨会总结了AI与敏捷开发结合的挑战与机遇，提出解决工具碎片化、治理和数据质量等问题的多主题研究路线图。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI与敏捷软件开发的交叉点上的挑战和机遇，推动负责任的AI集成。

Method: 通过结构化互动研讨会，30多名跨学科学者和从业者共同分析问题并制定研究议程。

Result: 确定了工具碎片化、治理、数据质量和技能差距等痛点，并制定了短期和长期研究路线图。

Conclusion: 路线图旨在指导未来研究，推动生成式AI以人为本地融入敏捷实践。

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [6] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: 论文提出了一个三层架构（系统外壳层、提示编排层和LLM推理核心）来分析LLM应用的测试挑战，并探讨了传统软件测试方法的适用性。通过比较软件工程和AI安全分析方法，指出了6个核心挑战，并提出了四种协作策略和一个闭环质量保障框架。


<details>
  <summary>Details</summary>
Motivation: LLM应用的非确定性、动态性和上下文依赖性给质量保障带来了根本性挑战，需要新的测试方法和框架。

Method: 将LLM应用分解为三层架构，分析传统测试方法的适用性，并提出了协作策略和质量保障框架。

Result: 提出了四种协作策略（保留、翻译、集成和运行时）和AICL协议，以支持LLM应用测试的标准化和工具化。

Conclusion: 论文通过分层分析和协作策略，为LLM应用的测试和质量保障提供了实践指导和标准化方向。

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [7] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: LLMs能从法律文本中生成高质量的Gherkin规范，显著减少人工工作。


<details>
  <summary>Details</summary>
Motivation: 法律文本的技术中立性给工程师带来挑战，需要自动化解译为开发友好形式。

Method: 通过人类受试者评估Claude和Llama生成的Gherkin规范，采用准实验设计。

Result: 规范在相关性、清晰性、完整性等多个标准上获得高分，LLMs表现接近。

Conclusion: LLMs在生成合规性规范方面高效且实用，适合开发与测试。

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [8] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 论文提出了一个针对软件设计中可持续性问题的架构视角愿景，通过文献分析和专家焦点小组确认其实用性。


<details>
  <summary>Details</summary>
Motivation: 可持续性在软件密集型系统中日益重要，但缺乏结构化指导，作者提出架构视角作为解决方案。

Method: 通过文献雪球法和专家焦点小组，验证架构视角元素的实用性。

Result: 研究确认了架构视角元素的相关性，并强调了其满足工业需求的潜力。

Conclusion: 架构视角愿景是解决软件设计可持续性问题的有效工具，具有实际应用价值。

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [9] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 提出了一种基于断言的可解释测试预言方法，用于降低CPS测试成本，并通过遗传规划和决策树生成预言，其中遗传规划结合Ochiai方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: CPS的模拟测试成本高且结果不稳定，需要一种无需系统执行的自动化测试预言来降低成本，并具备可解释性和鲁棒性。

Method: 提出了基于逻辑和算术谓词的断言测试预言，通过遗传规划（GP）和决策树（DT/DR）生成预言，GP方法采用了Ochiai、Tarantula和Naish等适应度函数。

Result: GP结合Ochiai生成的预言在多个领域中显著优于其他方法，且对测试系统的波动性具有鲁棒性，准确率变化仅为4%。

Conclusion: 断言测试预言，尤其是GP结合Ochiai生成的预言，能够有效降低测试成本，提高测试的可靠性和可解释性。

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [10] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: 提出了一种结合预训练模型与异构图神经网络（GNN）的方法，用于检测和定位并发错误，通过构建专用数据集和改进语义表示，显著提升了准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法在并发错误检测中数据集不足、语义表示不充分以及定位能力有限的问题。

Method: 构建专用并发错误数据集，结合预训练模型与GNN，设计新的Concurrency-Aware Code Property Graph（CCPG），并利用SubgraphX实现精确定位。

Result: 相较于现有方法，平均准确率和精确率提升10%，召回率提升26%。

Conclusion: 该方法显著提升了并发错误检测与定位的效果，为软件可靠性和安全性提供了有效支持。

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [11] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出了ConfLogger工具，通过配置日志增强软件诊断能力，结合静态污点分析和LLM日志生成，显著提升诊断准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有诊断方法缺乏对软件是否提供足够故障信息的关注，需改进日志实践以提升配置问题的可诊断性。

Method: 结合配置感知静态污点分析和LLM日志生成，识别配置敏感代码并生成诊断日志。

Result: 在30个静默配置错误场景中实现100%错误定位，诊断时间缩短1.25倍，准确率提升251.4%。

Conclusion: ConfLogger显著提升配置诊断能力，为软件配置问题提供高效解决方案。

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [12] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: 该论文探讨软件工程领域的性别偏见问题，分析了女性在国际软件工程会议中的参与情况，并提出相关政策建议。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程领域的性别偏见问题，关注女性在该领域的参与度和代表性。

Method: 通过历史调查、领袖档案分析及国际会议数据定量分析（1976-2010），研究女性作者的参与情况。

Result: 发现连续12年存在统计显著的性别排斥现象。

Conclusion: 提出研究性别偏见问题的政策维度，以促进该领域的多样性与包容性。

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Solvable Tuple Patterns and Their Applications to Program Verification](https://arxiv.org/abs/2508.20365)
*Naoki Kobayashi,Ryosuke Sato,Ayumi Shinohara,Ryo Yoshinaka*

Main category: cs.PL

TL;DR: 该论文提出了一种可解元组模式（STP）的概念，用于表达列表式递归数据结构的不变量，并展示了如何将其集成到CHC求解器中，显著提升了自动化程序验证的效率。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化程序验证技术取得了进展，但完全自动化验证递归数据结构仍具挑战性。为克服这一挑战，作者提出了STP的概念和高效推理方法。

Method: 论文引入了STP的概念，并通过SMT求解器验证其是否为归纳不变量。此外，开发了一种STP推理算法，并将其集成到支持列表式数据结构的CHC求解器中。

Result: 提出的STP推理方法仅需少量正样本即可高效推断，且无需负样本。集成STP推理的CHC求解器在CHC-COMP 2025的ADT-LIN类别中以显著优势获胜。

Conclusion: STP为递归数据结构的自动化验证提供了高效解决方案，其集成到CHC求解器中的方法展现了强大的性能，为相关工具提供了统一的后端支持。

Abstract: Despite the recent progress of automated program verification techniques,
fully automated verification of programs manipulating recursive data structures
remains a challenge. We introduce the notion of solvable tuple patterns (STPs)
to express invariants between list-like recursive data structures. A
distinguishing feature of STPs is that they can be efficiently inferred from
only a small number of positive samples; no negative samples are required. An
SMT solver that supports the sequence theory can be used to check that an
inferred STP is indeed an inductive invariant. After presenting basic
properties of STPs and an STP inference algorithm, we show how to incorporate
the STP inference into a CHC (Constrained Horn Clauses) solver supporting
list-like data structures, which serves as a uniform backend for automated
program verification tools. A CHC solver incorporating the STP inference has
won the ADT-LIN category of CHC-COMP 2025 by a big margin.

</details>


### [14] [Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops](https://arxiv.org/abs/2508.20922)
*Markus Böck,Jürgen Cito*

Main category: cs.PL

TL;DR: 论文解决了概率程序（具有用户标记样本语句和循环功能）能否被图形化表示的问题，提出了新的静态分析方法，并实现了三种优化。


<details>
  <summary>Details</summary>
Motivation: 研究概率程序（如Gen、Turing、Pyro等语言中的功能）能否被图形化表示，并探索其依赖结构和优化方法。

Method: 扩展操作语义以支持语言功能，通过控制流图定义静态分析，生成程序密度的静态分解，并提出程序切片技术以优化三种常见算法。

Result: 提出的图形表示方法适用于无界随机变量程序，优化技术在变分推断、Metropolis Hastings和序列蒙特卡洛中表现优于现有方法。

Conclusion: 论文为概率程序提供了新的图形化表示和优化方法，证明了其实用性和性能优势。

Abstract: It is commonly known that any Bayesian network can be implemented as a
probabilistic program, but the reverse direction is not so clear. In this work,
we address the open question to what extent a probabilistic program with
user-labelled sample statements and while loops - features found in languages
like Gen, Turing, and Pyro - can be represented graphically. To this end, we
extend existing operational semantics to support these language features. By
translating a program to its control-flow graph, we define a sound static
analysis that approximates the dependency structure of the random variables in
the program. As a result, we obtain a static factorisation of the implicitly
defined program density, which is equivalent to the known Bayesian network
factorisation for programs without loops and constant labels, but constitutes a
novel graphical representation for programs that define an unbounded number of
random variables via loops or dynamic labels. We further develop a sound
program slicing technique to leverage this structure to statically enable three
well-known optimisations for the considered program class: we reduce the
variance of gradient estimates in variational inference and we speed up both
single-site Metropolis Hastings and sequential Monte Carlo. These optimisations
are proven correct and empirically shown to match or outperform existing
techniques.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [15] [The Unwritten Contract of Cloud-based Elastic Solid-State Drives](https://arxiv.org/abs/2508.17372)
*Yingjia Wang,Ming-Chang Yang*

Main category: cs.PF

TL;DR: 本文首次对亚马逊AWS和阿里云的ESSD性能进行了深入分析，揭示了与传统本地SSD不同的观察结果，并提出了对云存储用户的五大启示。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的服务迁移到云端，ESSD是否能替代本地SSD并提供可比性能成为关键问题。本文旨在填补这一研究空白。

Method: 通过分析亚马逊AWS和阿里云的ESSD性能，总结出四个反直觉的观察结果。

Result: 揭示了云ESSD的未书面合同特性，与传统SSD预期不符。

Conclusion: 研究结果可帮助用户重新设计云软件，以充分利用ESSD的独特性能，提升系统表现。

Abstract: Elastic block storage (EBS) with the storage-compute disaggregated
architecture stands as a pivotal piece in today's cloud. EBS furnishes users
with storage capabilities through the elastic solid-state drive (ESSD).
Nevertheless, despite the widespread integration into cloud services, the
absence of a thorough ESSD performance characterization raises critical doubt:
when more and more services are shifted onto the cloud, can ESSD satisfactorily
substitute the storage responsibilities of the local SSD and offer comparable
performance?
  In this paper, we for the first time target this question by characterizing
two ESSDs from Amazon AWS and Alibaba Cloud. We present an unwritten contract
of cloud-based ESSDs, encapsulating four observations and five implications for
cloud storage users. Specifically, the observations are counter-intuitive and
contrary to the conventional perceptions of what one would expect from the
local SSD. The implications we hope could guide users in revisiting the designs
of their deployed cloud software, i.e., harnessing the distinct characteristics
of ESSDs for better system performance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [A Comprehensive Survey of 5G URLLC and Challenges in the 6G Era](https://arxiv.org/abs/2508.20205)
*Md. Emadul Haque,Faisal Tariq,Muhammad R A Khandaker,Md. Sakir Hossain,Muhammad Ali Imran,Kai-Kit Wong*

Main category: cs.NI

TL;DR: 该论文综述了5G系统中超可靠低延迟通信（URLLC）的研究进展，讨论了其历史、技术要求、分层解决方案及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信从以人为中心转向以机器为中心，对速率、延迟和可靠性的要求发生了巨大变化，URLLC成为5G和6G的关键主题。

Method: 采用分层方法，详细分析了物理层、MAC层和跨层技术，并探讨了5G及未来垂直领域的设计考虑。

Result: 总结了URLLC的现有技术方案，并提出了目标延迟（1毫秒）与可靠性（99.999%）之间的矛盾问题。

Conclusion: 文章指出了当前研究的挑战，并展望了6G时代的URLLC发展方向。

Abstract: As the wireless communication paradigm is being transformed from human
centered communication services towards machine centered communication
services, the requirements of rate, latency and reliability for these services
have also been transformed drastically. Thus the concept of Ultra Reliable and
Low Latency Communication (URLLC) has emerged as a dominant theme for 5G and 6G
systems. Though the latency and reliability requirement varies from one use
case to another, URLLC services generally aim to achieve very high reliability
in the range of 99.999\% while ensuring the latency of up to 1 ms. These two
targets are however inherently opposed to one another. Significant amounts of
work have been carried out to meet these ambitious but conflicting targets. In
this article a comprehensive survey of the URLLC approaches in 5G systems are
analysed in detail. Effort has been made to trace the history and evolution of
latency and reliability issues in wireless communication. A layered approach is
taken where physical layer, Medium Access Control (MAC) layer as well as cross
layer techniques are discussed in detail. It also covers the design
consideration for various 5G and beyond verticals. Finally the article
concludes by providing a detailed discussion on challenges and future outlook
with particular focus on the emerging 6G paradigm.

</details>


### [17] [DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource Allocation and Markov Decision Process in Named Data Networking (NDN)](https://arxiv.org/abs/2508.20272)
*Fatemeh Roshanzadeh,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: DRR-MDPF是一种结合MDPF模型与DRR算法的新型混合策略，用于提升NDN网络的性能，优化资源管理和队列调度。


<details>
  <summary>Details</summary>
Motivation: NDN网络需要高效的队列和资源管理以应对动态高流量条件，现有策略在性能上有待提升。

Method: 通过MDPF实现智能转发决策预测，结合DRR算法公平分配带宽，路由器作为学习代理通过反馈调整策略。

Result: 仿真表明DRR-MDPF在吞吐量、兴趣满足率、丢包率等多指标上优于现有策略，且适应性强、计算复杂度低。

Conclusion: DRR-MDPF是一种智能、自适应且可扩展的队列管理方案，适用于NDN网络的动态环境。

Abstract: Named Data Networking (NDN) represents a transformative shift in network
architecture, prioritizing content names over host addresses to enhance data
dissemination. Efficient queue and resource management are critical to NDN
performance, especially under dynamic and high-traffic conditions. This paper
introduces DRR-MDPF, a novel hybrid strategy that integrates the Markov
Decision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)
algorithm. MDPF enables routers to intelligently predict optimal forwarding
decisions based on key metrics such as bandwidth, delay, and the number of
unsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation
among competing data flows. The proposed method models each router as a
learning agent capable of adjusting its strategies through continuous feedback
and probabilistic updates. Simulation results using ndnSIM demonstrate that
DRR-MDPF significantly outperforms state-of-the-art strategies including SAF,
RFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest
Satisfaction Rate (ISR), packet drop rate, content retrieval time, and load
balancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and
heavy traffic, offering enhanced adaptability and lower computational
complexity due to its single-path routing design. Furthermore, its multi-metric
decision-making capability enables more accurate interface selection, leading
to optimized network performance. Overall, DRR-MDPF serves as an intelligent,
adaptive, and scalable queue management solution for NDN, effectively
addressing core challenges such as resource allocation, congestion control, and
route optimization in dynamic networking environments.

</details>


### [18] [Relay Selection in Wireless Networks as Restless Bandits](https://arxiv.org/abs/2508.20625)
*Mandar R. Nalavade,Ravindra S. Tomar,Gaurav S. Kasbekar*

Main category: cs.NI

TL;DR: 研究无线网络中源节点通过候选中继节点传输文件的问题，提出基于Whittle索引的中继选择策略，以最小化持有成本，并证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决源节点与目的节点直接通信被阻断时，通过中继节点转发数据包的最小化长期持有成本问题。

Method: 将问题建模为Whittle-indexable的RMAB问题，计算每个中继的Whittle索引，选择索引最小的中继传输数据包。

Result: 仿真表明，所提策略在平均成本、延迟和吞吐量上优于现有方法。

Conclusion: 基于Whittle索引的中继选择策略有效解决了无线网络中的优化问题，具有实际应用价值。

Abstract: We consider a wireless network in which a source node needs to transmit a
large file to a destination node. The direct wireless link between the source
and the destination is assumed to be blocked. Multiple candidate relays are
available to forward packets from the source to the destination. A holding cost
is incurred for each packet stored at every relay in each time slot. The
objective is to design a policy for selecting a relay in each time slot to
which the source attempts to send a packet, so as to minimize the expected
long-run time-averaged total packet holding cost at the relays. This problem is
an instance of the restless multi-armed bandit (RMAB) problem, which is
provably hard to solve. We prove that this relay selection problem is
Whittle-indexable, and propose a method to compute the Whittle index of each
relay in every time slot. In each time slot, our relay selection policy
transmits a packet to the relay with the smallest Whittle index. Using
simulations, we show that the proposed policy outperforms the relay selection
policies proposed in prior work in terms of average cost, delay, as well as
throughput.

</details>


### [19] [Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF Migration in Edge-Core Networks](https://arxiv.org/abs/2508.20957)
*Faisal Ahmed,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 基于数字孪生和深度强化学习的框架优化虚拟网络功能迁移，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 现代网络边缘核心基础设施中，快速部署的虚拟网络功能（VNFs）对低延迟和高效能的编排提出了挑战。

Method: 将VNF迁移问题建模为马尔可夫决策过程，采用Advantage Actor-Critic模型并结合数字孪生模块（多任务变分自编码器和LSTM网络）进行实时动态模拟与决策。

Result: 仿真结果显示，该框架显著降低了端到端延迟和能耗，为智能VNF迁移设立了新的性能标准。

Conclusion: 提出的框架有效实现了低延迟和高能效的VNF迁移，推动了边缘核心网络的智能化发展。

Abstract: The growing demand for services and the rapid deployment of virtualized
network functions (VNFs) pose significant challenges for achieving low-latency
and energy-efficient orchestration in modern edge-core network infrastructures.
To address these challenges, this study proposes a Digital Twin (DT)-empowered
Deep Reinforcement Learning framework for intelligent VNF migration that
jointly minimizes average end-to-end (E2E) delay and energy consumption. By
formulating the VNF migration problem as a Markov Decision Process and
utilizing the Advantage Actor-Critic model, the proposed framework enables
adaptive and real-time migration decisions. A key innovation of the proposed
framework is the integration of a DT module composed of a multi-task
Variational Autoencoder and a multi-task Long Short-Term Memory network. This
combination collectively simulates environment dynamics and generates
high-quality synthetic experiences, significantly enhancing training efficiency
and accelerating policy convergence. Simulation results demonstrate substantial
performance gains, such as significant reductions in both average E2E delay and
energy consumption, thereby establishing new benchmarks for intelligent VNF
migration in edge-core networks.

</details>


### [20] [RANGAN: GAN-empowered Anomaly Detection in 5G Cloud RAN](https://arxiv.org/abs/2508.20985)
*Douglas Liao,Jiping Luo,Jens Vevstad,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 论文提出了一种名为RANGAN的异常检测框架，结合GAN与Transformer架构，用于动态的RAN性能监控，实验证明其检测效果良好。


<details>
  <summary>Details</summary>
Motivation: RAN系统复杂且数据量大，传统方法难以高效诊断性能异常，因此需要自适应的方法来捕捉时序依赖关系。

Method: RANGAN利用GAN与Transformer架构，并通过滑动窗口预处理数据，以增强对时序依赖的捕捉能力。

Result: 在公开数据集上的实验显示，RANGAN在检测网络拥塞问题时F1分数达到83%。

Conclusion: RANGAN是一种有效的RAN异常检测框架，尤其在处理动态性能数据时表现优异。

Abstract: Radio Access Network (RAN) systems are inherently complex, requiring
continuous monitoring to prevent performance degradation and ensure optimal
user experience. The RAN leverages numerous key performance indicators (KPIs)
to evaluate system performance, generating vast amounts of data each second.
This immense data volume can make troubleshooting and accurate diagnosis of
performance anomalies more difficult. Furthermore, the highly dynamic nature of
RAN performance demands adaptive methodologies capable of capturing temporal
dependencies to detect anomalies reliably. In response to these challenges, we
introduce \textbf{RANGAN}, an anomaly detection framework that integrates a
Generative Adversarial Network (GAN) with a transformer architecture. To
enhance the capability of capturing temporal dependencies within the data,
RANGAN employs a sliding window approach during data preprocessing. We
rigorously evaluated RANGAN using the publicly available RAN performance
dataset from the Spotlight project \cite{sun-2024}. Experimental results
demonstrate that RANGAN achieves promising detection accuracy, notably
attaining an F1-score of up to $83\%$ in identifying network contention issues.

</details>


### [21] [DSROQ: Dynamic Scheduling and Routing for QoE Management in LEO Satellite Networks](https://arxiv.org/abs/2508.21047)
*Dhiraj Bhattacharjee,Pablo G. Madoery,Abhishek Naik,Halim Yanikomeroglu,Gunes Karabulut Kurt,Stephane Martel,Khaled Ahmed*

Main category: cs.NI

TL;DR: 本文提出了一种结合路由和带宽分配的联合优化方法，以满足低地球轨道卫星网络中的异构服务质量需求，并通过实验验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 现代互联网应用具有多样化的服务质量需求，而低地球轨道卫星网络为解决这些需求提供了潜力，尤其是在农村和城市地区的覆盖与补充方面。

Method: 采用蒙特卡洛树搜索启发的方法解决NP难的路由和带宽分配问题，并结合Lyapunov优化调度策略。

Result: DSROQ算法在性能和公平性上优于基准方案，且随着流量敏感性的变化，性能主导因素从调度转向路由和带宽分配。

Conclusion: 联合路由和带宽分配的决策能够显著提升用户体验，同时适应不同流量需求的变化。

Abstract: The modern Internet supports diverse applications with heterogeneous quality
of service (QoS) requirements. Low Earth orbit (LEO) satellite constellations
offer a promising solution to meet these needs, enhancing coverage in rural
areas and complementing terrestrial networks in urban regions. Ensuring QoS in
such networks requires joint optimization of routing, bandwidth allocation, and
dynamic queue scheduling, as traffic handling is critical for maintaining
service performance. This paper formulates a joint routing and bandwidth
allocation problem where QoS requirements are treated as soft constraints,
aiming to maximize user experience. An adaptive scheduling approach is
introduced to prioritize flow-specific QoS needs. We propose a Monte Carlo tree
search (MCTS)-inspired method to solve the NP-hard route and bandwidth
allocation problem, with Lyapunov optimization-based scheduling applied during
reward evaluation. Using the Starlink Phase 1 Version 2 constellation, we
compare end-user experience and fairness between our proposed DSROQ algorithm
and a benchmark scheme. Results show that DSROQ improves both performance
metrics and demonstrates the advantage of joint routing and bandwidth
decisions. Furthermore, we observe that the dominant performance factor shifts
from scheduling to routing and bandwidth allocation as traffic sensitivity
changes from latency-driven to bandwidth-driven.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [MM-HSD: Multi-Modal Hate Speech Detection in Videos](https://arxiv.org/abs/2508.20546)
*Berta Céspedes-Sarrias,Carlos Collado-Capell,Pablo Rodenas-Ruiz,Olena Hrynenko,Andrea Cavallaro*

Main category: cs.MM

TL;DR: 本文介绍了一个多模态仇恨语音检测模型MM-HSD，整合视频帧、音频和文本，使用跨模态注意力（CMA）提取特征，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态仇恨语音检测在视频中应用有限，且未能充分利用各模态间的依赖关系。本文旨在通过整合多种模态和CMA技术提升检测效果。

Method: 提出MM-HSD模型，结合视频帧、音频、文字转录及屏幕文字，利用CMA提取特征，并比较不同查询/键配置及模态交互。

Result: 在HateMM数据集上，MM-HSD的M-F1分数达到0.874，优于现有方法。

Conclusion: MM-HSD通过多模态整合和CMA技术显著提升了仇恨语音检测性能，尤其在屏幕文字作为查询时效果最佳。

Abstract: While hate speech detection (HSD) has been extensively studied in text,
existing multi-modal approaches remain limited, particularly in videos. As
modalities are not always individually informative, simple fusion methods fail
to fully capture inter-modal dependencies. Moreover, previous work often omits
relevant modalities such as on-screen text and audio, which may contain subtle
hateful content and thus provide essential cues, both individually and in
combination with others. In this paper, we present MM-HSD, a multi-modal model
for HSD in videos that integrates video frames, audio, and text derived from
speech transcripts and from frames (i.e.~on-screen text) together with features
extracted by Cross-Modal Attention (CMA). We are the first to use CMA as an
early feature extractor for HSD in videos, to systematically compare query/key
configurations, and to evaluate the interactions between different modalities
in the CMA block. Our approach leads to improved performance when on-screen
text is used as a query and the rest of the modalities serve as a key.
Experiments on the HateMM dataset show that MM-HSD outperforms state-of-the-art
methods on M-F1 score (0.874), using concatenation of transcript, audio, video,
on-screen text, and CMA for feature extraction on raw embeddings of the
modalities. The code is available at https://github.com/idiap/mm-hsd

</details>


### [23] [diveXplore at the Video Browser Showdown 2024](https://arxiv.org/abs/2508.20560)
*Klaus Schoeffmann,Sahar Nasirihaghighi*

Main category: cs.MM

TL;DR: 该论文总结了VBS2023的经验和CBMI2023的反馈，对diveXplore系统进行了大幅改进，集成了OpenCLIP等技术，优化了查询和用户界面。


<details>
  <summary>Details</summary>
Motivation: 基于VBS2023和CBMI2023的反馈，改进diveXplore系统以提升其性能和用户体验。

Method: 集成OpenCLIP（基于LAION-2B数据集训练）用于图像/文本嵌入，优化查询服务器和用户界面，新增探索视图。

Result: 改进了系统的文本和视觉相似性搜索功能，提升了查询效率和用户浏览体验。

Conclusion: diveXplore系统通过技术集成和界面优化，显著提升了其在视频检索和浏览方面的表现。

Abstract: According to our experience from VBS2023 and the feedback from the IVR4B
special session at CBMI2023, we have largely revised the diveXplore system for
VBS2024. It now integrates OpenCLIP trained on the LAION-2B dataset for
image/text embeddings that are used for free-text and visual similarity search,
a query server that is able to distribute different queries and merge the
results, a user interface optimized for fast browsing, as well as an
exploration view for large clusters of similar videos (e.g., weddings,
paraglider events, snow and ice scenery, etc.).

</details>


### [24] [Less is More - diveXplore 5.0 at VBS 2021](https://arxiv.org/abs/2508.20569)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: diveXplore 是一个深度交互视频探索系统，曾在 VBS 和 LSC 竞赛中表现优异，但后续因功能增加导致性能下降。VBS 2021 版本进行了彻底重构，降低了系统复杂度并保留了有用功能。


<details>
  <summary>Details</summary>
Motivation: 解决 diveXplore 系统因功能增加导致的性能下降问题。

Method: 完全重建系统，降低复杂度并以模块化方式保留有用功能。

Result: 推出了 VBS 2021 版本的 diveXplore 5.0。

Conclusion: 系统重构是解决性能问题的有效方法，模块化设计有助于功能扩展。

Abstract: As a longstanding participating system in the annual Video Browser Showdown
(VBS2017-VBS2020) as well as in two iterations of the more recently established
Lifelog Search Challenge (LSC2018-LSC2019), diveXplore is developed as a
feature-rich Deep Interactive Video Exploration system. After its initial
successful employment as a competitive tool at the challenges, its performance,
however, declined as new features were introduced increasing its overall
complexity. We mainly attribute this to the fact that many additions to the
system needed to revolve around the system's core element - an interactive
self-organizing browseable featuremap, which, as an integral component did not
accommodate the addition of new features well. Therefore, counteracting said
performance decline, the VBS 2021 version constitutes a completely rebuilt
version 5.0, implemented from scratch with the aim of greatly reducing the
system's complexity as well as keeping proven useful features in a modular
manner.

</details>


### [25] [diveXplore 6.0: ITEC's Interactive Video Exploration System at VBS 2022](https://arxiv.org/abs/2508.20687)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: diveXplore系统经过多次VBS竞赛的迭代，最新版本6.0在精简和现代化的同时引入了新功能，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 优化系统架构，提升交互搜索效率，并在VBS竞赛中取得更好表现。

Method: 通过重构系统，精简功能使其更现代、轻量且快速，并新增了镜头分割、地图搜索等功能以提升概念和时间上下文搜索能力。

Result: 新系统在VBS2021中表现优于以往版本，性能得到显著提升。

Conclusion: 精简与现代化改造是成功的关键，新版本的功能优化进一步提升了系统的竞争力。

Abstract: Continuously participating since the sixth Video Browser Showdown (VBS2017),
diveXplore is a veteran interactive search system that throughout its lifetime
has offered and evaluated numerous features. After undergoing major refactoring
for the most recent VBS2021, however, the system since version 5.0 is less
feature rich, yet, more modern, leaner and faster than the original system.
This proved to be a sensible decision as the new system showed increasing
performance in VBS2021 when compared to the most recent former competitions.
With version 6.0 we reconsider shot segmentation, map search and introduce new
features for improving concept as well as temporal context search.

</details>


### [26] [AdaDPCC: Adaptive Rate Control and Rate-Distortion-Complexity Optimization for Dynamic Point Cloud Compression](https://arxiv.org/abs/2508.20741)
*Chenhao Zhang,Wei Gao*

Main category: cs.MM

TL;DR: 本文提出了一种新颖的动态点云压缩框架，支持可变比特率和计算复杂度，通过多路编码和精细化运动估计模块优化性能。实验显示其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动态点云压缩在自动驾驶和AR/VR中至关重要，但现有方法难以管理复杂度和比特率控制。

Method: 采用可裁剪框架支持多路编码，结合粗到细运动估计模块和精确比特率控制。

Result: 方法平均BD-Rate降低5.81%，BD-PSNR提升0.42 dB，编码时间减少44.6%。

Conclusion: 该方法在实时和比特率受限场景下高效，性能显著提升。

Abstract: Dynamic point cloud compression (DPCC) is crucial in applications like
autonomous driving and AR/VR. Current compression methods face challenges with
complexity management and rate control. This paper introduces a novel dynamic
coding framework that supports variable bitrate and computational complexities.
Our approach includes a slimmable framework with multiple coding routes,
allowing for efficient Rate-Distortion-Complexity Optimization (RDCO) within a
single model. To address data sparsity in inter-frame prediction, we propose
the coarse-to-fine motion estimation and compensation module that deconstructs
geometric information while expanding the perceptive field. Additionally, we
propose a precise rate control module that content-adaptively navigates point
cloud frames through various coding routes to meet target bitrates. The
experimental results demonstrate that our approach reduces the average BD-Rate
by 5.81% and improves the BD-PSNR by 0.42 dB compared to the state-of-the-art
method, while keeping the average bitrate error at 0.40%. Moreover, the average
coding time is reduced by up to 44.6% compared to D-DPCC, underscoring its
efficiency in real-time and bitrate-constrained DPCC scenarios. Our code is
available at https://git.openi.org.cn/OpenPointCloud/Ada_DPCC.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [27] [Exploiting Instantiations from Paramodulation Proofs in Isabelle/HOL](https://arxiv.org/abs/2508.20738)
*Lukas Bartl,Jasmin Blanchette,Tobias Nipkow*

Main category: cs.LO

TL;DR: Metis是Isabelle/HOL中的有序参数化解器，用于通过给定引理关闭目标。新工具通过分析成功Metis证明推导变量实例化，提升Sledgehammer效果。


<details>
  <summary>Details</summary>
Motivation: 提高Sledgehammer工具的成功率、证明速度，并帮助用户理解引理与目标之间的关系。

Method: 分析成功的Metis证明，从中提取变量实例化信息。

Result: 新工具显著提升了Sledgehammer的成功率和效率，同时增强了用户对证明过程的理解。

Conclusion: 通过分析Metis证明推导变量实例化，为Isabelle/HOL中的自动化证明提供了更高效和透明的支持。

Abstract: Metis is an ordered paramodulation prover built into the Isabelle/HOL proof
assistant. It attempts to close the current goal using a given list of lemmas.
Typically these lemmas are found by Sledgehammer, a tool that integrates
external automatic provers. We present a new tool that analyzes successful
Metis proofs to derive variable instantiations. These increase Sledgehammer's
success rate, improve the speed of Sledgehammer-generated proofs, and help
users understand why a goal follows from the lemmas.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [28] [Athena: Intermediate Representations for Iterative Scaffolded App Generation with an LLM](https://arxiv.org/abs/2508.20263)
*Jazbo Beason,Ruijia Cheng,Eldon Schoop,Jeffrey Nichols*

Main category: cs.HC

TL;DR: 论文提出Athena，一种通过中间表示辅助LLM生成完整用户界面的原型环境，优于传统聊天机器人基线。


<details>
  <summary>Details</summary>
Motivation: LLM生成复杂用户界面代码时存在挑战，需要解决单一大文件生成和提示细节不足的问题。

Method: 引入Athena原型环境，利用故事板、数据模型和GUI骨架等中间表示，支持迭代式开发和多文件生成。

Result: 用户研究表明，75%的参与者更喜欢Athena而非传统聊天机器人基线。

Conclusion: Athena通过中间表示和迭代开发有效解决了LLM生成用户界面的问题，显著提升了开发体验。

Abstract: It is challenging to generate the code for a complete user interface using a
Large Language Model (LLM). User interfaces are complex and their
implementations often consist of multiple, inter-related files that together
specify the contents of each screen, the navigation flows between the screens,
and the data model used throughout the application. It is challenging to craft
a single prompt for an LLM that contains enough detail to generate a complete
user interface, and even then the result is frequently a single large and
difficult to understand file that contains all of the generated screens. In
this paper, we introduce Athena, a prototype application generation environment
that demonstrates how the use of shared intermediate representations, including
an app storyboard, data model, and GUI skeletons, can help a developer work
with an LLM in an iterative fashion to craft a complete user interface. These
intermediate representations also scaffold the LLM's code generation process,
producing organized and structured code in multiple files while limiting
errors. We evaluated Athena with a user study that found 75% of participants
preferred our prototype over a typical chatbot-style baseline for prototyping
apps.

</details>


### [29] [Identifying Framing Practices in Visualization Design Through Practitioner Reflections](https://arxiv.org/abs/2508.20383)
*Prakash Shukla,Paul Parsons*

Main category: cs.HC

TL;DR: 可视化研究中，框架定义和重新解释问题、塑造叙事并引导受众理解的作用在设计中被低估。本研究分析专业设计师的工作反思，发现框架在问题界定、数据解释和叙事塑造中至关重要。


<details>
  <summary>Details</summary>
Motivation: 可视化研究中，框架对设计过程的作用未被充分探索，现有研究多关注其对受众的修辞和感知效果。

Method: 通过分析80多位专业可视化设计师的公开播客和书籍章节，研究框架在设计过程中的作用。

Result: 发现框架是设计过程的核心活动，涵盖问题界定、数据解释和叙事方向，揭示了触发重新框架的条件及应对不确定性的策略。

Conclusion: 框架是可视化实践的核心维度，未来研究和教育需支持设计师在设计过程中的解释性和战略性判断。

Abstract: Framing -- how designers define and reinterpret problems, shape narratives,
and guide audience understanding -- is central to design practice. Yet in
visualization research, framing has been examined mostly through its rhetorical
and perceptual effects on audiences, leaving its role in the design process
underexplored. This study addresses that gap by analyzing publicly available
podcasts and book chapters in which over 80 professional visualization
designers reflect on their work. We find that framing is a pervasive, iterative
activity, evident in scoping problems, interpreting data, aligning with
stakeholder goals, and shaping narrative direction. Our analysis identifies the
conditions that trigger reframing and the strategies practitioners use to
navigate uncertainty and guide design. These findings position framing as a
core dimension of visualization practice and underscore the need for research
and education to support the interpretive and strategic judgment that
practitioners exercise throughout the design process.

</details>


### [30] [Human-Centered Design for Connected Automation: Predicting Pedestrian Crossing Intentions](https://arxiv.org/abs/2508.20464)
*Sanaz Motamedi,Viktoria Marcus,Griffin Pitts*

Main category: cs.HC

TL;DR: 研究探讨了5级自动驾驶系统（ADS）与行人互动时，如何通过扩展计划行为理论（TPB）来理解行人的决策行为，并提出设计安全交互界面的建议。


<details>
  <summary>Details</summary>
Motivation: 交通事故中行人和弱势道路使用者死亡率高，5级自动驾驶系统（ADS）可能减少事故，但其与行人的安全互动是关键。

Method: 扩展计划行为理论（TPB），加入安全、信任、兼容性和理解四个外部因素，通过在线调查（n=212）分析行人决策行为。

Result: 行为控制感知、态度和社会信息显著预测行人过街意图；外部因素中，安全感知和理解影响最大。

Conclusion: 研究为设计自动驾驶车辆与行人透明交互的外人机界面（eHMIs）和V2X通信策略提供了依据，推动以人为本的智慧交通发展。

Abstract: Road traffic remains a leading cause of death worldwide, with pedestrians and
other vulnerable road users accounting for over half of the 1.19 million annual
fatalities, much of it due to human error. Level-5 automated driving systems
(ADSs), capable of full self-driving without human oversight, have the
potential to reduce these incidents. However, their effectiveness depends not
only on automation performance but also on their ability to communicate intent
and coordinate safely with pedestrians in the absence of traditional driver
cues. Understanding how pedestrians interpret and respond to ADS behavior is
therefore critical to the development of connected vehicle systems. This study
extends the Theory of Planned Behavior (TPB) by incorporating four external
factors (i.e. safety, trust, compatibility, and understanding) to model
pedestrian decision-making in road-crossing scenarios involving level-5 ADSs.
Using data from an online survey (n = 212), results show that perceived
behavioral control, attitude, and social information significantly predict
pedestrians' crossing intentions. External factors, particularly perceived
safety and understanding, strongly influence these constructs. Findings provide
actionable insights for designing external human-machine interfaces (eHMIs) and
cooperative V2X communication strategies that support safe, transparent
interactions between automated vehicles and pedestrians. This work contributes
to the development of inclusive, human-centered connected mobility systems.

</details>


### [31] [What is "Spatial" about Spatial Computing?](https://arxiv.org/abs/2508.20477)
*Yibo Wang,Yuhan Luo,Janghee Cho,Junnan Yu*

Main category: cs.HC

TL;DR: 摘要回顾了空间计算的历史演变，提出了两种对“空间”的理解学派，并综合这些观点将空间计算定义为一种重新定义环境、计算与人类体验交互的计算范式。


<details>
  <summary>Details</summary>
Motivation: 空间计算在计算科学中被视为变革性范式，但由于跨学科的碎片化理解，阻碍了其综合发展和学科整合，需要澄清其概念。

Method: 通过追溯空间计算的起源和历史演变，分析“空间”的两种理解学派，并综合这些观点提出新的定义。

Result: 提出空间计算是一种重新定义环境、计算与人类体验交互的范式，增强了其概念清晰度。

Conclusion: 空间计算作为一种综合范式，有望促进未来技术创新，支持与环境的有意义交互和塑造。

Abstract: Recent advancements in geographic information systems and mixed reality
technologies have positioned spatial computing as a transformative paradigm in
computational science. However, the field remains conceptually fragmented, with
diverse interpretations across disciplines like Human-Computer Interaction,
Geographic Information Science, and Computer Science, which hinders a
comprehensive understanding of spatial computing and poses challenges for its
coherent advancement and interdisciplinary integration. In this paper, we trace
the origins and historical evolution of spatial computing and examine how
"spatial" is understood, identifying two schools of thought: "spatial" as the
contextual understanding of space, where spatial data guides interaction in the
physical world; and "spatial" as a mixed space for interaction, emphasizing the
seamless integration of physical and digital environments to enable embodied
engagement. By synthesizing these perspectives, we propose spatial computing as
a computational paradigm that redefines the interplay between environment,
computation, and human experience, offering a holistic lens to enhance its
conceptual clarity and inspire future technological innovations that support
meaningful interactions with and shaping of environments.

</details>


### [32] [VisiTrail: A Cognitive Visualization Tool for Time-Series Analysis of Eye Tracking Data from Attention Game](https://arxiv.org/abs/2508.20522)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.HC

TL;DR: 该论文提出了一种综合分析工具，用于研究眼动追踪数据中的注意力动态和行为与任务表现的关系。


<details>
  <summary>Details</summary>
Motivation: 理解视觉任务中注意力的动态分配及其与任务表现的关系是研究的关键，传统方法对此缺乏全面分析。

Method: 开发了一种工具，结合时间序列分析、注视模式、对象点击序列及表现指标来研究眼动数据。

Result: 工具提供了全面的可视化技术，使复杂的刺激与注视模式之间的联系变得可解释。

Conclusion: 该工具为复杂视觉搜索任务中的注意力动态和决策过程提供了更深入的理解。

Abstract: Eye Tracking (ET) can help to understand visual attention and cognitive
processes in interactive environments. In attention tasks, distinguishing
between relevant target objects and distractors is crucial for effective
performance, yet the underlying gaze patterns that drive successful task
completion remain incompletely understood. Traditional gaze analyses lack
comprehensive insights into the temporal dynamics of attention allocation and
the relationship between gaze behavior and task performance. When applied to
complex visual search scenarios, current gaze analysis methods face several
limitations, including the isolation of measurements, visual stability, search
efficiency, and the decision-making processes involved in these scenarios. This
paper proposes an analysis tool that considers time series for eye tracking
data from task performance and also gaze measures (fixations, saccades and
smooth pursuit); temporal pattern analysis that reveals how attention evolves
throughout task performance; object-click sequence tracking that directly links
visual attention to user actions; and performance metrics that quantify both
accuracy and efficiency. This tool provides comprehensive visualization
techniques that make complex patterns of stimuli and gaze connections
interpretable.

</details>


### [33] [Persode: Personalized Visual Journaling with Episodic Memory-Aware AI Agent](https://arxiv.org/abs/2508.20585)
*Seokho Jin,Manseo Kim,Sungho Byun,Hansol Kim,Jungmin Lee,Sujeong Baek,Semi Kim,Sanghum Park,Sung Park*

Main category: cs.HC

TL;DR: 论文提出Persode，一种个性化视觉日记系统，满足Gen Alpha和Z的需求。


<details>
  <summary>Details</summary>
Motivation: 传统文字日记缺乏个性化和视觉吸引力，难以吸引Gen Alpha和Z，他们更喜欢沉浸式和快节奏的互动。

Method: Persode通过个性化注册、记忆感知对话代理和自动视觉叙事，结合RAG框架和文本到图像模型，生成视觉叙事。

Result: Persode能捕捉用户偏好，优先处理情感化记忆，并动态生成视觉叙事。

Conclusion: Persode通过个性化、视觉化和响应性，填补了传统日记与现代用户需求之间的鸿沟。

Abstract: Reflective journaling often lacks personalization and fails to engage
Generation Alpha and Z, who prefer visually immersive and fast-paced
interactions over traditional text-heavy methods. Visual storytelling enhances
emotional recall and offers an engaging way to process personal expe- riences.
Designed with these digital-native generations in mind, this paper introduces
Persode, a journaling system that integrates personalized onboarding,
memory-aware conversational agents, and automated visual storytelling. Persode
captures user demographics and stylistic preferences through a tailored
onboarding process, ensuring outputs resonate with individual identities. Using
a Retrieval-Augmented Generation (RAG) framework, it prioritizes emotionally
significant memories to provide meaningful, context-rich interactions.
Additionally, Persode dynamically transforms user experiences into visually
engaging narratives by generating prompts for advanced text-to-image models,
adapting characters, backgrounds, and styles to user preferences. By addressing
the need for personalization, visual engagement, and responsiveness, Persode
bridges the gap between traditional journaling and the evolving preferences of
Gen Alpha and Z.

</details>


### [34] [Schema-Guided Response Generation using Multi-Frame Dialogue State for Motivational Interviewing Systems](https://arxiv.org/abs/2508.20635)
*Jie Zeng,Yukiko I. Nakano*

Main category: cs.HC

TL;DR: 该研究提出了一种基于激励性访谈（MI）原则的方法，通过模式引导更新多框架对话状态和动态决定响应焦点的策略决策机制，生成符合MI原则的对话系统响应。


<details>
  <summary>Details</summary>
Motivation: 支持对话系统生成与MI原则一致的咨询师回应，帮助客户增强行为改变的动机。

Method: 采用模式引导方法更新多框架对话状态，并动态决定响应焦点。

Result: 用户研究表明，系统成功生成了符合MI原则的响应，并通过提问有效鼓励了用户的思考。

Conclusion: 该方法能有效生成符合MI原则的对话响应，支持用户行为改变的动机构建。

Abstract: The primary goal of Motivational Interviewing (MI) is to help clients build
their own motivation for behavioral change. To support this in dialogue
systems, it is essential to guide large language models (LLMs) to generate
counselor responses aligned with MI principles. By employing a schema-guided
approach, this study proposes a method for updating multi-frame dialogue states
and a strategy decision mechanism that dynamically determines the response
focus in a manner grounded in MI principles. The proposed method was
implemented in a dialogue system and evaluated through a user study. Results
showed that the proposed system successfully generated MI-favorable responses
and effectively encouraged the user's (client's) deliberation by asking
eliciting questions.

</details>


### [35] [Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop](https://arxiv.org/abs/2508.21036)
*Lev Tankelevitch,Elena L. Glassman,Jessica He,Aniket Kittur,Mina Lee,Srishti Palani,Advait Sarkar,Gonzalo Ramos,Yvonne Rogers,Hari Subramonyam*

Main category: cs.HC

TL;DR: 这篇论文探讨了生成式AI对人类认知的影响及其增强认知的潜力，强调需要开发新的理论、指标和工具来研究这一问题，并介绍了CHI 2025研讨会的内容。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI（GenAI）对人类认知的潜在影响，以及如何通过设计和研究工具来保护和增强人类思维。

Method: 通过CHI 2025研讨会汇集56位来自不同领域的研究者和设计师，结合34篇论文和作品集，进行讨论和构思。

Result: 研讨会成功启发了多学科研究社群，并为未来的设计和研究提供了方向。

Conclusion: 生成式AI为人类认知带来了机遇与挑战，需要多学科合作来探索和开发相关工具。

Abstract: Generative AI (GenAI) radically expands the scope and capability of
automation for work, education, and everyday tasks, a transformation posing
both risks and opportunities for human cognition. How will human cognition
change, and what opportunities are there for GenAI to augment it? Which
theories, metrics, and other tools are needed to address these questions? The
CHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of
how the use of GenAI affects human thought, from metacognition to critical
thinking, memory, and creativity, with an emerging design practice for building
GenAI tools that both protect and augment human thought. Fifty-six researchers,
designers, and thinkers from across disciplines as well as industry and
academia, along with 34 papers and portfolios, seeded a day of discussion,
ideation, and community-building. We synthesize this material here to begin
mapping the space of research and design opportunities and to catalyze a
multidisciplinary community around this pressing area of research.

</details>


### [36] [OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models](https://arxiv.org/abs/2508.21061)
*Adam Coscia,Shunan Guo,Eunyee Koh,Alex Endert*

Main category: cs.HC

TL;DR: OnGoal 是一个帮助用户管理对话目标的 LLM 聊天界面，通过实时反馈、解释和进度概览提升复杂对话的效率。


<details>
  <summary>Details</summary>
Motivation: 针对多轮复杂对话中用户难以评估目标进展的问题。

Method: 提供实时目标对齐反馈、解释和进度概览的功能，并通过用户实验与基线界面对比。

Result: 使用 OnGoal 的用户在写作任务中更高效地达成目标，并探索新策略以克服沟通障碍。

Conclusion: 目标跟踪和可视化可以提升 LLM 对话的参与度和韧性，未来设计应注重目标沟通和反馈。

Abstract: As multi-turn dialogues with large language models (LLMs) grow longer and
more complex, how can users better evaluate and review progress on their
conversational goals? We present OnGoal, an LLM chat interface that helps users
better manage goal progress. OnGoal provides real-time feedback on goal
alignment through LLM-assisted evaluation, explanations for evaluation results
with examples, and overviews of goal progression over time, enabling users to
navigate complex dialogues more effectively. Through a study with 20
participants on a writing task, we evaluate OnGoal against a baseline chat
interface without goal tracking. Using OnGoal, participants spent less time and
effort to achieve their goals while exploring new prompting strategies to
overcome miscommunication, suggesting tracking and visualizing goals can
enhance engagement and resilience in LLM dialogues. Our findings inspired
design implications for future LLM chat interfaces that improve goal
communication, reduce cognitive load, enhance interactivity, and enable
feedback to improve LLM performance.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [37] [Mixture of Contexts for Long Video Generation](https://arxiv.org/abs/2508.21058)
*Shengqu Cai,Ceyuan Yang,Lvmin Zhang,Yuwei Guo,Junfei Xiao,Ziyan Yang,Yinghao Xu,Zhenheng Yang,Alan Yuille,Leonidas Guibas,Maneesh Agrawala,Lu Jiang,Gordon Wetzstein*

Main category: cs.GR

TL;DR: 论文提出了一种稀疏注意力路由模块Mixture of Contexts (MoC)，用于解决长视频生成中的内存和计算成本问题，使模型能够高效地保留和检索长上下文信息。


<details>
  <summary>Details</summary>
Motivation: 长视频生成中的挑战在于模型需要在长范围内保留和检索重要事件，而扩散变换器的二次自注意力成本使其难以处理长序列。

Method: 论文将长上下文视频生成重新定义为内部信息检索任务，并提出MoC作为长期记忆检索引擎。MoC通过动态选择信息块和强制锚点（如字幕和局部窗口）来实现高效的注意力路由。

Result: 随着数据和路由稀疏化的扩展，模型能够将计算分配给重要的历史信息，从而在几分钟内保持身份、动作和场景的一致性。

Conclusion: MoC实现了近乎线性的扩展，使得训练和合成变得实用，并在超长范围内实现了记忆和一致性的涌现。

Abstract: Long video generation is fundamentally a long context memory problem: models
must retain and retrieve salient events across a long range without collapsing
or drifting. However, scaling diffusion transformers to generate long-context
videos is fundamentally limited by the quadratic cost of self-attention, which
makes memory and computation intractable and difficult to optimize for long
sequences. We recast long-context video generation as an internal information
retrieval task and propose a simple, learnable sparse attention routing module,
Mixture of Contexts (MoC), as an effective long-term memory retrieval engine.
In MoC, each query dynamically selects a few informative chunks plus mandatory
anchors (caption, local windows) to attend to, with causal routing that
prevents loop closures. As we scale the data and gradually sparsify the
routing, the model allocates compute to salient history, preserving identities,
actions, and scenes over minutes of content. Efficiency follows as a byproduct
of retrieval (near-linear scaling), which enables practical training and
synthesis, and the emergence of memory and consistency at the scale of minutes.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [38] [Reverse Designing Ferroelectric Capacitors with Machine Learning-based Compact Modeling](https://arxiv.org/abs/2508.20216)
*Diego Ferrer,Jack Hutchins,Revanth Koduru,Sumeet Kumar Gupta,Admedullah Aziz*

Main category: cs.ET

TL;DR: 论文介绍了两种基于机器学习紧凑模型的反向设计算法，用于快速确定器件参数以实现特定电性能，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了在器件设计中快速确定满足特定电性能的参数组合，避免传统计算密集型方法的高耗时问题。

Method: 采用基于机器学习的紧凑模型，结合两种反向设计算法，优化器件参数（如层厚度）。

Result: 算法在计算时间上显著优于传统相场建模方法，且在精度和效率上均表现出色。

Conclusion: 机器学习紧凑模型为器件设计提供了高效、精确的解决方案，具有明显优势。

Abstract: Machine learning-based compact models provide a rapid and efficient approach
for estimating device behavior across multiple input parameter variations. In
this study, we introduce two reverse-design algorithms that utilize these
compact models to identify device parameters corresponding to desired
electrical characteristics. The algorithms effectively determine parameter
sets, such as layer thicknesses, required to achieve specific device
performance criteria. Significantly, the proposed methods are uniquely enabled
by machine learning-based compact modeling; alternative computationally
intensive approaches, such as phase-field modeling, would impose impractical
time constraints for iterative design processes. Our comparative analysis
demonstrates a substantial reduction in computation time when employing machine
learning-based compact models compared to traditional phase-field methods,
underscoring a clear and substantial efficiency advantage. Additionally, the
accuracy and computational efficiency of both reverse-design algorithms are
evaluated and compared, highlighting the practical advantages of machine
learning-based compact modeling approaches.

</details>


### [39] [Blind Source Separation-Enabled Joint Communication and Sensing in IBFD MIMO Systems](https://arxiv.org/abs/2508.20409)
*Siyao Li,Conrad Prisby,Thomas Yang*

Main category: cs.ET

TL;DR: 该论文提出了一种基于盲源分离（BSS）的框架，用于在IBFD MIMO系统中同时实现自干扰消除（SIC）和感知信息提取，从而提高通信与感知的性能。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中的联合通信与感知（JCAS）面临自干扰（SI）的挑战，但SI信号也为感知提供了机会。

Method: 使用FastICA算法分离SI和SOI信号，同时实现信号恢复和信道估计。

Result: 仿真结果表明，随着信号帧大小的增加，框架在感知和通信性能上均有提升。

Conclusion: 该框架为IBFD MIMO系统中的JCAS提供了一种高效解决方案，无需专用雷达波形。

Abstract: This paper addresses the challenge of joint communication and sensing (JCAS)
in next-generation wireless networks, with an emphasis on in-band full-duplex
(IBFD) multiple-input multiple-output (MIMO) systems. Traditionally,
self-interference (SI) in IBFD systems is a major obstacle to recovering the
signal of interest (SOI). Under the JCAS paradigm, however, this high-power SI
signal presents an opportunity for efficient sensing. Since each transceiver
node has access to the original SI signal, its environmental reflections can be
exploited to estimate channel conditions and detect changes, without requiring
dedicated radar waveforms. We propose a blind source separation (BSS)-based
framework to simultaneously perform self-interference cancellation (SIC) and
extract sensing information in IBFD MIMO settings. The approach applies the
Fast Independent Component Analysis (FastICA) algorithm to separate the SI and
SOI signals while enabling simultaneous signal recovery and channel estimation.
Simulation results confirm the framework's effectiveness, showing improved
sensing and communication performance as signal frame size increases.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [SpeedMalloc: Improving Multi-threaded Applications via a Lightweight Core for Memory Allocation](https://arxiv.org/abs/2508.20253)
*Ruihao Li,Qinzhe Wu,Krishna Kavi,Gayatri Mehta,Jonathan C. Beard,Neeraja J. Yadwadkar,Lizy K. John*

Main category: cs.DC

TL;DR: SpeedMalloc使用轻量级支持核心处理多线程应用的内存分配任务，提高性能并减少缓存冲突。


<details>
  <summary>Details</summary>
Motivation: 尽管内存分配只占代码的一小部分，但对程序性能有重大影响，尤其是在多线程多核系统中。现有加速器对多线程支持有限，同步问题突出。

Method: SpeedMalloc采用轻量级支持核心处理分配任务，分配器元数据存储在核心缓存中，减少缓存冲突和跨核同步开销。

Result: SpeedMalloc在五种现代分配器上对多线程工作负载分别实现了1.75x、1.18x、1.15x、1.23x和1.18x的性能提升。

Conclusion: SpeedMalloc通过轻量级支持核心显著提高了多线程应用的内存分配性能，同时避免了现有加速器的局限性。

Abstract: Memory allocation, though constituting only a small portion of the executed
code, can have a "butterfly effect" on overall program performance, leading to
significant and far-reaching impacts. Despite accounting for just approximately
5% of total instructions, memory allocation can result in up to a 2.7x
performance variation depending on the allocator used. This effect arises from
the complexity of memory allocation in modern multi-threaded multi-core
systems, where allocator metadata becomes intertwined with user data, leading
to cache pollution or increased cross-thread synchronization overhead.
Offloading memory allocators to accelerators, e.g., Mallacc and Memento, is a
potential direction to improve the allocator performance and mitigate cache
pollution. However, these accelerators currently have limited support for
multi-threaded applications, and synchronization between cores and accelerators
remains a significant challenge.
  We present SpeedMalloc, using a lightweight support-core to process memory
allocation tasks in multi-threaded applications. The support-core is a
lightweight programmable processor with efficient cross-core data
synchronization and houses all allocator metadata in its own caches. This
design minimizes cache conflicts with user data and eliminates the need for
cross-core metadata synchronization. In addition, using a general-purpose core
instead of domain-specific accelerators makes SpeedMalloc capable of adopting
new allocator designs. We compare SpeedMalloc with state-of-the-art software
and hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and
Memento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on
multithreaded workloads over these five allocators, respectively.

</details>


### [41] [SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization](https://arxiv.org/abs/2508.20258)
*Arya Tschand,Muhammad Awad,Ryan Swann,Kesavan Ramakrishnan,Jeffrey Ma,Keith Lowery,Ganesh Dasika,Vijay Janapa Reddi*

Main category: cs.DC

TL;DR: SwizzlePerf是一种利用LLMs进行硬件感知的GPU内核性能优化工具，能够快速生成高效的硬件特定优化模式，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于搜索的GPU内核性能优化方法缺乏硬件感知能力，而人类性能工程师依赖硬件感知实现近最优利用。SwizzlePerf通过结合硬件特性和工作负载特性，填补了这一空白。

Method: SwizzlePerf利用LLMs结合硬件架构规范、内存访问模式、过滤的性能日志和历史性能反思，自动为GPU内核生成空间优化方案。

Result: SwizzlePerf在5分钟内生成了专家需要2周才能找到的最优优化模式，并在10个多样化内核中为9个实现了最高2.06倍的加速和70%的L2命中率提升。

Conclusion: SwizzlePerf是系统化构建硬件感知LLM性能工程代理的重要一步，展现了硬件感知优化的巨大潜力。

Abstract: Large language models (LLMs) have shown progress in GPU kernel performance
engineering using inefficient search-based methods that optimize around
runtime. Any existing approach lacks a key characteristic that human
performance engineers rely on for near-optimal utilization --
hardware-awareness. By leveraging the workload's specific memory access
patterns, architecture specifications, filtered profiling logs, and reflections
on historical performance, we can make software-level optimizations that are
tailored to the underlying hardware. SwizzlePerf automatically generates
spatial optimizations for GPU kernels on disaggregated architectures by giving
LLMs explicit hardware-awareness.
  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same
hardware-specific optimal swizzling pattern that took expert performance
engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,
SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve
up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the
first of many steps toward systematically creating hardware-aware LLM
performance engineering agents.

</details>


### [42] [Predictable LLM Serving on GPU Clusters](https://arxiv.org/abs/2508.20274)
*Erfan Darzi,Shreeanant Bharadwaj,Sree Bhargavi Balija*

Main category: cs.DC

TL;DR: 该论文提出了一种主机级控制器，通过动态 MIG 重新配置、PCIe 感知的放置和轻量级防护措施，减少共享 A100 集群中延迟敏感推理的尾部延迟和 SLO 违规。


<details>
  <summary>Details</summary>
Motivation: 解决共享 A100 集群中 PCIe 结构上的噪声邻居干扰问题，降低尾部延迟和 SLO 违规率。

Method: 结合动态 MIG 重新配置、PCIe 感知的放置和轻量级防护措施，采样租户尾部数据和系统信号，避免 PCIe 热点，并通过冷却机制减少抖动。

Result: 在单主机和 2 节点（16-GPU）集群中，SLO 违规率降低约 32%，p99 延迟改善约 15%，吞吐量损失不超过 5%。在 LLM 服务中，TTFT p99 延迟改善 10-15%，成本低于 5%。

Conclusion: 该控制器在减少延迟和 SLO 违规方面表现优异，且兼容性高，适用于多种场景。

Abstract: Latency-sensitive inference on shared A100 clusters often suffers
noisy-neighbor interference on the PCIe fabric, inflating tail latency and SLO
violations. We present a fabric-agnostic, VM-deployable host-level controller
that combines dynamic Multi-Instance GPU (MIG) reconfiguration, PCIe-aware
placement, and lightweight guardrails (MPS quotas, cgroup I/O). It samples
per-tenant tails and system signals, uses topology hints to avoid PCIe hot
spots, and gates actions with dwell/cool-down to avoid thrash. On a single host
and a 2-node (16-GPU) cluster, SLO miss-rate is reduced by \(\approx\)32\%
(\(\approx\)1.5) and p99 latency improves \(\approx\)15\% with \(\leq\)5\%
throughput cost versus static MIG and naive placement; ablations show MIG and
placement contribute comparably. We also evaluate LLM serving with vLLM on OLMo
2 7B Instruct: TTFT p99 improves \(\approx\)10--15\% at \(\leq\)5\% cost
without changing the controller.

</details>


### [43] [CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference](https://arxiv.org/abs/2508.20375)
*Guanyu Xu,Zhiwei Hao,Li Shen,Yong Luo,Fuhui Sun,Xiaoyan Wang,Han Hu,Yonggang Wen*

Main category: cs.DC

TL;DR: 提出了一种名为CoFormer的协作推理系统，通过分解和集成Transformer模型，在边缘设备上实现高效的分布式推理。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型在资源受限的边缘设备上部署时的高计算需求和资源限制问题。

Method: 利用Transformer的可分割性和可集成性，将其分解为多个小模型进行分布式推理，并通过优化问题和DeBo算法校准模型。

Result: 在异构边缘设备上支持多种Transformer模型，推理速度提升3.1倍，内存需求减少76.3%，能耗降低40%。

Conclusion: CoFormer在保证性能的同时，显著提升了边缘设备上Transformer模型的推理效率和资源利用率。

Abstract: The impressive performance of transformer models has sparked the deployment
of intelligent applications on resource-constrained edge devices. However,
ensuring high-quality service for real-time edge systems is a significant
challenge due to the considerable computational demands and resource
requirements of these models. Existing strategies typically either offload
transformer computations to other devices or directly deploy compressed models
on individual edge devices. These strategies, however, result in either
considerable communication overhead or suboptimal trade-offs between accuracy
and efficiency. To tackle these challenges, we propose a collaborative
inference system for general transformer models, termed CoFormer. The central
idea behind CoFormer is to exploit the divisibility and integrability of
transformer. An off-the-shelf large transformer can be decomposed into multiple
smaller models for distributed inference, and their intermediate results are
aggregated to generate the final output. We formulate an optimization problem
to minimize both inference latency and accuracy degradation under heterogeneous
hardware constraints. DeBo algorithm is proposed to first solve the
optimization problem to derive the decomposition policy, and then progressively
calibrate decomposed models to restore performance. We demonstrate the
capability to support a wide range of transformer models on heterogeneous edge
devices, achieving up to 3.1$\times$ inference speedup with large transformer
models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6
billion parameters on edge devices, reducing memory requirements by 76.3\%.
CoFormer can also reduce energy consumption by approximately 40\% while
maintaining satisfactory inference performance.

</details>


### [44] [pdGRASS: A Fast Parallel Density-Aware Algorithm for Graph Spectral Sparsification](https://arxiv.org/abs/2508.20403)
*Tiancheng Zhao,Zekun Yin,Huihai An,Xiaoyu Yang,Zhou Jin,Jiasi Shen,Helen Xu*

Main category: cs.DC

TL;DR: 提出了一种并行化的图谱稀疏化算法 (pdGRASS)，解决了现有方法 feGRASS 的并行化难题和性能下降问题，显著提升了计算效率和稀疏化质量。


<details>
  <summary>Details</summary>
Motivation: feGRASS 在并行化恢复步骤中存在严格的数据依赖性问题，且对倾斜输入性能下降，需要多次处理。因此，需要一种更高效且可并行的算法来改进这些问题。

Method: 提出 pdGRASS，通过将边组织成无数据依赖的独立子任务，实现高效并行化和单次处理中的充分边恢复。

Result: pdGRASS 在恢复边的运行时间和稀疏化质量上显著优于 feGRASS，平均速度提升 3.9x 至 8.8x，部分情况下 PCG 迭代次数减少 1.8x 或增加 1.2x，并在极端情况下速度提升超过 1000x。

Conclusion: pdGRASS 在可扩展性和性能上优于 feGRASS，为图谱稀疏化问题提供了更高效的解决方案。

Abstract: Graph Spectral Sparsification (GSS) identifies an ultra-sparse subgraph, or
sparsifier, whose Laplacian matrix closely approximates the spectral properties
of the original graph, enabling substantial reductions in computational
complexity for computationally intensive problems in scientific computing. The
state-of-the-art method for efficient GSS is feGRASS, consisting of two steps:
1) spanning tree generation and 2) off-tree edge recovery. However, feGRASS
suffers from two main issues: 1) difficulties in parallelizing the recovery
step for strict data dependencies, and 2) performance degradation on skewed
inputs, often requiring multiple passes to recover sufficient edges. To address
these challenges, we propose parallel density-aware Graph Spectral
Sparsification (pdGRASS), a parallel algorithm that organizes edges into
disjoint subtasks without data dependencies between them, enabling efficient
parallelization and sufficient edge recovery in a single pass. We empirically
evaluate feGRASS and pdGRASS based on 1) off-tree edge-recovery runtime and 2)
sparsifier quality, measured by the iteration count required for convergence in
a preconditioned conjugate gradient (PCG) application. The evaluation
demonstrates that, depending on the number of edges recovered, pdGRASS achieves
average speedups ranging from 3.9x to 8.8x. The resulting sparsifiers also show
between 1.2x higher and 1.8x lower PCG iteration counts, with further
improvements as more edges are recovered. Additionally, pdGRASS mitigates the
worst-case runtimes of feGRASS with over 1000x speedup. These results highlight
pdGRASS's significant improvements in scalability and performance for the graph
spectral sparsification problem.

</details>


### [45] [Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems](https://arxiv.org/abs/2508.20508)
*Yilin Li,Song Han,Sibo Wang,Ming Wang,Renzi Meng*

Main category: cs.DC

TL;DR: 该论文提出了一种基于多智能体协作进化机制的智能服务优化方法，用以解决大规模微服务架构中的治理挑战，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 面对大规模微服务架构中的复杂服务依赖、动态拓扑结构和波动工作负载等治理挑战，需要一种高效且自适应的优化方法。

Method: 将每个服务建模为智能体，采用图表示学习构建服务依赖图，基于马尔可夫决策过程学习策略，并通过集中训练与分散执行框架实现局部自主与全局协调。

Result: 实验表明，该方法在协调效率、适应性和策略收敛性能上优于其他先进方法，显著提升了治理效率和系统稳定性。

Conclusion: 该方法展示了强大的实践价值和工程可行性，适用于大规模微服务系统的动态优化需求。

Abstract: This paper proposes an intelligent service optimization method based on a
multi-agent collaborative evolution mechanism to address governance challenges
in large-scale microservice architectures. These challenges include complex
service dependencies, dynamic topology structures, and fluctuating workloads.
The method models each service as an agent and introduces graph representation
learning to construct a service dependency graph. This enables agents to
perceive and embed structural changes within the system. Each agent learns its
policy based on a Markov Decision Process. A centralized training and
decentralized execution framework is used to integrate local autonomy with
global coordination. To enhance overall system performance and adaptability, a
game-driven policy optimization mechanism is designed. Through a
selection-mutation process, agent strategy distributions are dynamically
adjusted. This supports adaptive collaboration and behavioral evolution among
services. Under this mechanism, the system can quickly respond and achieve
stable policy convergence when facing scenarios such as sudden workload spikes,
topology reconfigurations, or resource conflicts. To evaluate the effectiveness
of the proposed method, experiments are conducted on a representative
microservice simulation platform. Comparative analyses are performed against
several advanced approaches, focusing on coordination efficiency, adaptability,
and policy convergence performance. Experimental results show that the proposed
method outperforms others in several key metrics. It significantly improves
governance efficiency and operational stability in large-scale microservice
systems. The method demonstrates strong practical value and engineering
feasibility.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [46] [Efficient Forkless Blockchain Databases](https://arxiv.org/abs/2508.20686)
*Herbert Jordan,Kamil Jezek,Pavle Subotic,Bernhard Scholz*

Main category: cs.DB

TL;DR: 提出了一种无分叉的区块链数据库，相比现有方案，存储效率提升100倍，吞吐量提升10倍。


<details>
  <summary>Details</summary>
Motivation: 尽管区块链技术进步，L1区块链节点的运行成本仍然很高，尤其StateDB是资源密集型组件。现有无分叉链仍使用低效的分叉数据库。

Method: 设计并实现了一种适用于无分叉链的区块链数据库。

Result: 相比geth-based Fantom客户端，存储效率提升了100倍，吞吐量提升了10倍。

Conclusion: 该数据库显著优化了无分叉链的性能，降低了节点运行成本。

Abstract: Operating nodes in an L1 blockchain remains costly despite recent advances in
blockchain technology. One of the most resource-intensive components of a node
is the blockchain database, also known as StateDB, that manages balances,
nonce, code, and the persistent storage of accounts/smart contracts. Although
the blockchain industry has transitioned from forking to forkless chains due to
improved consensus protocols, forkless blockchains still rely on legacy forking
databases that are suboptimal for their purposes. In this paper, we propose a
forkless blockchain database, showing a 100x improvement in storage and a 10x
improvement in throughput compared to the geth-based Fantom Blockchain client.

</details>


### [47] [Research Challenges in Relational Database Management Systems for LLM Queries](https://arxiv.org/abs/2508.20912)
*Kerem Akillioglu,Anurag Chakraborty,Sairaj Voruganti,M. Tamer Özsu*

Main category: cs.DB

TL;DR: 论文分析了当前开源和商业平台中SQL调用LLM的功能和性能限制，并提出了改进方案。


<details>
  <summary>Details</summary>
Motivation: 当前开源方案在SQL调用LLM时功能有限且性能不佳，需要探索改进方法。

Method: 通过早期对两个开源系统和一个企业平台的实验，使用五个代表性查询来揭示限制，并提出了初步解决方案。

Result: 实验显示，改进后的方案在支持LLM驱动的SQL查询方面有所提升。

Conclusion: LLM与DBMS的紧密集成是实现高效和可扩展LLM查询的关键。

Abstract: Large language models (LLMs) have become essential for applications such as
text summarization, sentiment analysis, and automated question-answering.
Recently, LLMs have also been integrated into relational database management
systems to enhance querying and support advanced data processing. Companies
such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly
within SQL, denoted as LLM queries, to boost data insights. However,
open-source solutions currently have limited functionality and poor
performance. In this work, we present an early exploration of two open-source
systems and one enterprise platform, using five representative queries to
expose functional, performance, and scalability limits in today's SQL-invoked
LLM integrations. We identify three main issues: enforcing structured outputs,
optimizing resource utilization, and improving query planning. We implemented
initial solutions and observed improvements in accommodating LLM powered SQL
queries. These early gains demonstrate that tighter integration of LLM+DBMS is
the key to scalable and efficient processing of LLM queries.

</details>


### [48] [Graph-Based Feature Augmentation for Predictive Tasks on Relational Datasets](https://arxiv.org/abs/2508.20986)
*Lianpeng Qiao,Ziqi Cao,Kaiyu Feng,Ye Yuan,Guoren Wang*

Main category: cs.DB

TL;DR: 提出了一个自动化特征增强框架ReCoGNN，用于从多个关系表中提取特征以增强初始数据集，提高预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在各领域中，数据驱动的预测模型日益重要，但特征增强需要大量人工干预。研究目标是自动化特征增强过程，以识别和利用任务相关的关联信号。

Method: ReCoGNN通过建模表内属性关系捕获语义依赖，将表划分为语义连贯的片段，构建异构图表示关联关系，并利用图神经网络传播信息进行特征选择和增强。

Result: 在十个真实和合成数据集上的实验表明，ReCoGNN在分类和回归任务中均优于现有方法。

Conclusion: ReCoGNN有效地解决了特征增强自动化问题，显著提升了预测任务的性能。

Abstract: Data has become a foundational asset driving innovation across domains such
as finance, healthcare, and e-commerce. In these areas, predictive modeling
over relational tables is commonly employed, with increasing emphasis on
reducing manual effort through automated machine learning (AutoML) techniques.
This raises an interesting question: can feature augmentation itself be
automated and identify and utilize task-related relational signals?
  To address this challenge, we propose an end-to-end automated feature
augmentation framework, ReCoGNN, which enhances initial datasets using features
extracted from multiple relational tables to support predictive tasks. ReCoGNN
first captures semantic dependencies within each table by modeling intra-table
attribute relationships, enabling it to partition tables into structured,
semantically coherent segments. It then constructs a heterogeneous weighted
graph that represents inter-row relationships across all segments. Finally,
ReCoGNN leverages message-passing graph neural networks to propagate
information through the graph, guiding feature selection and augmenting the
original dataset. Extensive experiments conducted on ten real-life and
synthetic datasets demonstrate that ReCoGNN consistently outperforms existing
methods on both classification and regression tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [49] [Testing and Fault Tolerance Techniques for Carbon Nanotube-Based FPGAs](https://arxiv.org/abs/2508.20304)
*Siyuan Lu,Kangwei Xu,Peng Xie,Rui Wang,Yuanqing Cheng*

Main category: cs.AR

TL;DR: 论文提出了一种基于环形振荡器的测试技术，用于检测多壁碳纳米管互连的工艺变异导致的延迟故障，并改进了CNT基FPGA的测试效率和良率。


<details>
  <summary>Details</summary>
Motivation: 随着半导体制造工艺进入纳米尺度，传统CMOS FPGA在性能和功耗扩展上面临挑战，碳纳米管基FPGA展现出潜力。然而，制造过程中的工艺变异和金属性碳纳米管问题导致故障。

Method: 使用环形振荡器测试技术检测延迟故障；改进LUT电路设计加速测试；提出算法检测金属性碳纳米管；设计冗余行共享架构提高良率。

Result: 6输入LUT测试时间减少35.49%，测试算法覆盖率高且开销低；冗余架构能高效修复故障段。

Conclusion: 提出的测试技术和冗余架构显著提升了CNT基FPGA的可靠性和良率，适用于纳米尺度制造工艺。

Abstract: As the semiconductor manufacturing process technology node shrinks into the
nanometer-scale, the CMOS-based Field Programmable Gate Arrays (FPGAs) face big
challenges in scalability of performance and power consumption. Multi-walled
Carbon Nanotube (MWCNT) serves as a promising candidate for Cu interconnects
thanks to the superior conductivity. Moreover, Carbon Nanotube Field Transistor
(CNFET) also emerges as a prospective alternative to the conventional CMOS
device because of high power efficiency and large noise margin. The combination
of MWCNT and CNFET enables the promising CNT-based FPGAs. However, the MWCNT
interconnects exhibit significant process variations due to immature
fabrication process, leading to delay faults. Also, the non-ideal CNFET
fabrication process may generate a few metallic CNTs (m-CNTs), rendering
correlated faulty blocks. In this article, we propose a ring oscillator (RO)
based testing technique to detect delay faults due to the process variation of
MWCNT interconnects. Furthermore, we propose an effective testing technique for
the carry chains in CLBs, and an improved circuit design based on the lookup
table (LUT) is applied to speed up the fault testing of CNT-based FPGAs. In
addition, we propose a testing algorithm to detect m-CNTs in CLBs. Finally, we
propose a redundant spare row sharing architecture to improve the yield of
CNT-based FPGA further. Experimental results show that the test time for a
6-input LUT can be reduced by 35.49% compared with conventional testing, and
the proposed algorithm can achieve a high test coverage with little overhead.
The proposed redundant architecture can repair the faulty segment effectively
and efficiently.

</details>


### [50] [The Future of Memory: Limits and Opportunities](https://arxiv.org/abs/2508.20425)
*Shuhan Liu,Samuel Dayo,Peijing Li,Philip Levis,Subhasish Mitra,Thierry Tambe,David Tennenhouse,H. -S. Philip Wong*

Main category: cs.AR

TL;DR: 本文探讨了内存性能瓶颈问题，提出了一种新的系统架构，将大内存分解为与计算单元紧密耦合的小片段，利用2.5D/3D集成技术优化访问成本。


<details>
  <summary>Details</summary>
Motivation: 内存延迟、带宽、容量和能耗限制了性能提升，传统的大规模共享内存设计在扩展和信号方面存在挑战。

Method: 提出将内存分解为小片段并与计算单元紧密耦合，利用2.5D/3D集成技术实现私有本地内存，优化访问成本和能效。

Result: 通过硬件和软件的协同设计，实现了对数据放置和移动的高效管理，提升了带宽和能效。

Conclusion: 与传统共享内存设计相比，新方法在性能和能效方面具有显著优势。

Abstract: Memory latency, bandwidth, capacity, and energy increasingly limit
performance. In this paper, we reconsider proposed system architectures that
consist of huge (many-terabyte to petabyte scale) memories shared among large
numbers of CPUs. We argue two practical engineering challenges, scaling and
signaling, limit such designs. We propose the opposite approach. Rather than
create large, shared, homogenous memories, systems explicitly break memory up
into smaller slices more tightly coupled with compute elements. Leveraging
advances in 2.5D/3D integration, this compute-memory node provisions private
local memory, enabling accesses of node-exclusive data through micrometer-scale
distances, and dramatically reduced access cost. In-package memory elements
support shared state within a processor, providing far better bandwidth and
energy-efficiency than DRAM, which is used as main memory for large working
sets and cold data. Hardware making memory capacities and distances explicit
allows software to efficiently compose this hierarchy, managing data placement
and movement.

</details>


### [51] [Microarchitecture Design and Benchmarking of Custom SHA-3 Instruction for RISC-V](https://arxiv.org/abs/2508.20653)
*Alperen Bolat,Sakir Sezer,Kieran McLaughlin,Henry Hui*

Main category: cs.AR

TL;DR: 本研究探讨了在通用处理器中嵌入SHA-3自定义指令的架构挑战，通过RISC-V架构的实验证明其性能提升显著，仅需少量硬件成本增加。


<details>
  <summary>Details</summary>
Motivation: 为了解决SHA-3加速在微架构集成中的复杂性问题，并填补现有解决方案的不足。

Method: 研究在RISC-V CPU架构中设计SHA-3自定义指令，并通过GEM5模拟和FPGA原型验证性能。

Result: 实现了RISC-V软件负载最高8.02倍和Keccak负载最高46.31倍的性能提升，硬件成本仅增加15.09%寄存器和11.51% LUT。

Conclusion: 结果表明SHA-3在微架构级加速是可行的，为未来加密指令集扩展提供了设计参考。

Abstract: Integrating cryptographic accelerators into modern CPU architectures presents
unique microarchitectural challenges, particularly when extending instruction
sets with complex and multistage operations. Hardware-assisted cryptographic
instructions, such as Intel's AES-NI and ARM's custom instructions for
encryption workloads, have demonstrated substantial performance improvements.
However, efficient SHA-3 acceleration remains an open problem due to its
distinct permutation-based structure and memory access patterns. Existing
solutions primarily rely on standalone coprocessors or software optimizations,
often avoiding the complexities of direct microarchitectural integration. This
study investigates the architectural challenges of embedding a SHA-3
permutation operation as a custom instruction within a general-purpose
processor, focusing on pipelined simultaneous execution, storage utilization,
and hardware cost. In this paper, we investigated and prototyped a SHA-3 custom
instruction for the RISC-V CPU architecture. Using cycle-accurate GEM5
simulations and FPGA prototyping, our results demonstrate performance
improvements of up to 8.02x for RISC-V optimized SHA-3 software workloads and
up to 46.31x for Keccak-specific software workloads, with only a 15.09%
increase in registers and a 11.51% increase in LUT utilization. These findings
provide critical insights into the feasibility and impact of SHA-3 acceleration
at the microarchitectural level, highlighting practical design considerations
for future cryptographic instruction set extensions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [52] [Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse](https://arxiv.org/abs/2508.20664)
*Kan Chen,Zhen Meng,Xiangmin Xu,Jiaming Yang,Emma Li,Philip G. Zhao*

Main category: cs.RO

TL;DR: 提出了一种基于数字孪生的边缘辅助跨系统框架，用于实时人机交互，解决了工业元宇宙中的高计算负载和延迟问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 工业元宇宙中的实时人机交互面临高计算负载、带宽限制和严格延迟等挑战，需要一种高效的解决方案。

Method: 采用任务导向的边缘辅助跨系统框架，结合数字孪生（DTs）和预测操作者动作的技术，实现主动渲染和远程设备控制。

Result: 在轨迹绘制控制和核退役3D场景表示任务中表现优异，显著降低了误差并提高了视觉质量。

Conclusion: 该框架能够在高风险工业环境中确保空间精度和视觉保真度，适用于实时交互场景。

Abstract: Real-time human-device interaction in industrial Metaverse faces challenges
such as high computational load, limited bandwidth, and strict latency. This
paper proposes a task-oriented edge-assisted cross-system framework using
digital twins (DTs) to enable responsive interactions. By predicting operator
motions, the system supports: 1) proactive Metaverse rendering for visual
feedback, and 2) preemptive control of remote devices. The DTs are decoupled
into two virtual functions-visual display and robotic control-optimizing both
performance and adaptability. To enhance generalizability, we introduce the
Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which
dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates
the framework's effectiveness: in a Trajectory-Based Drawing Control task, it
reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene
representation task for nuclear decommissioning, it achieves a PSNR of 22.11,
SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's
capability to ensure spatial precision and visual fidelity in real-time,
high-risk industrial environments.

</details>


### [53] [Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](https://arxiv.org/abs/2508.20840)
*Qiao Sun,Liujia Yang,Wei Tang,Wei Huang,Kaixin Xu,Yongchao Chen,Mingyu Liu,Jiange Yang,Haoyi Zhu,Yating Wang,Tong He,Yilun Chen,Xili Dai,Nanyang Ye,Qinying Gu*

Main category: cs.RO

TL;DR: 提出了一种新的世界建模范式PEWM，通过限制视频生成的短时间范围，提高语言与动作的对齐粒度、降低学习复杂性、提升数据效率，并减少推理延迟。


<details>
  <summary>Details</summary>
Motivation: 视频生成的体现世界模型依赖于大规模的体现交互数据，但数据的稀缺性和高维性限制了语言与动作的对齐粒度，阻碍了长时视频生成的进展。

Method: 提出PEWM框架，结合VLMs的语义意识和视频模型的时空视觉先验，通过固定短时视频生成和SGG机制，实现细粒度的语言与动作对齐。

Result: PEWM能够实现灵活的闭环控制和对复杂任务的组合泛化，提高了数据效率和推理速度。

Conclusion: PEWM为可扩展、可解释和通用的体现智能奠定了基础，解决了现有模型在体现域中的核心瓶颈。

Abstract: While video-generation-based embodied world models have gained increasing
attention, their reliance on large-scale embodied interaction data remains a
key bottleneck. The scarcity, difficulty of collection, and high dimensionality
of embodied data fundamentally limit the alignment granularity between language
and actions and exacerbate the challenge of long-horizon video
generation--hindering generative models from achieving a "GPT moment" in the
embodied domain. There is a naive observation: the diversity of embodied data
far exceeds the relatively small space of possible primitive motions. Based on
this insight, we propose a novel paradigm for world modeling--Primitive
Embodied World Models (PEWM). By restricting video generation to fixed short
horizons, our approach 1) enables fine-grained alignment between linguistic
concepts and visual representations of robotic actions, 2) reduces learning
complexity, 3) improves data efficiency in embodied data collection, and 4)
decreases inference latency. By equipping with a modular Vision-Language Model
(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further
enables flexible closed-loop control and supports compositional generalization
of primitive-level policies over extended, complex tasks. Our framework
leverages the spatiotemporal vision priors in video models and the semantic
awareness of VLMs to bridge the gap between fine-grained physical interaction
and high-level reasoning, paving the way toward scalable, interpretable, and
general-purpose embodied intelligence.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [54] [Formal equivalence between global optimization consistency and random search](https://arxiv.org/abs/2508.20671)
*Gaëtan Serré*

Main category: cs.FL

TL;DR: 该论文证明了随机迭代全局优化算法在Lipschitz连续函数上一致的充要条件是采样整个搜索空间，并使用定理证明器和数学库完成了形式化验证。


<details>
  <summary>Details</summary>
Motivation: 探讨随机迭代全局优化算法在Lipschitz连续函数上的收敛条件，为其形式化验证提供理论基础。

Method: 定义算法为初始概率测度和马尔可夫核序列，利用Ionescu-Tulcea定理构造迭代序列的概率测度。

Result: 证明了算法一致性的充要条件是全局采样，并通过形式化工具验证了这一结论。

Conclusion: 论文成功形式化了随机迭代优化算法的收敛条件，为相关研究提供了严格的数学工具和定义框架。

Abstract: We formalize a proof that any stochastic and iterative global optimization
algorithm is consistent over Lipschitz continuous functions if and only if it
samples the whole search space. To achieve this, we use the
L$\exists$$\forall$N theorem prover and the Mathlib library. The major
challenge of this formalization, apart from the technical aspects of the proof
itself, is to converge to a definition of a stochastic and iterative global
optimization algorithm that is both general enough to encompass all algorithms
of this type and specific enough to be used in a formal proof. We define such
an algorithm as a pair of an initial probability measure and a sequence of
Markov kernels that describe the distribution of the next point sampled by the
algorithm given the previous points and their evaluations. We then construct a
probability measure on finite and infinite sequences of iterations of the
algorithm using the Ionescu-Tulcea theorem.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [55] [Spatio-Temporal Pruning for Compressed Spiking Large Language Models](https://arxiv.org/abs/2508.20122)
*Yi Jiang,Malyaban Bal,Brian Matejek,Susmit Jha,Adam Cobb,Abhronil Sengupta*

Main category: cs.NE

TL;DR: 提出一种结合时空剪枝的脉冲神经网络（SNN）优化方法，用于高效、低功耗的大型语言模型（LLM）部署。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在能源受限环境下因模型尺寸大和推理延迟高而难以部署的问题。

Method: 采用时空剪枝框架，结合空间剪枝（减少神经元和注意力头）和时间剪枝（动态调整时间步），并融合极值量化和知识蒸馏。

Result: 在SpikingBERT和GLUE基准测试中验证了方法的有效性，显著降低计算复杂度和推理延迟。

Conclusion: 该方法为实时、低功耗的NLP应用提供了可行性方案，适合边缘设备和能源受限场景。

Abstract: Large Language Models (LLMs) present significant challenges for deployment in
energy-constrained environments due to their large model sizes and high
inference latency. Spiking Neural Networks (SNNs), inspired by the sparse
event-driven neural processing and energy-efficient information transmission in
the brain, offer a promising alternative for achieving low-power computing.
Integrating the event-driven efficiency of spiking neurons with the advanced
capabilities of LLMs represents a promising direction for power-efficient LLMs.
This work specifically delves into the design of compressed spiking LLMs. Here,
we revisit spatial and temporal pruning from the perspective of SNNs and
propose a novel spatio-temporal pruning framework for Spiking LLMs to optimize
computational efficiency while preserving high performance. Our spatial pruning
technique reduces the number of active neurons and attention heads, effectively
lowering the computational complexity of the model. Meanwhile, temporal pruning
minimizes inference latency by dynamically adjusting the number of timesteps
required for different layers. By combining these approaches with other
compression techniques, we present the first work in the domain of Spiking LLMs
to jointly explore spatial pruning, temporal pruning, extreme quantization and
knowledge distillation strategies. Extensive experimental evaluation of our
proposed framework for SpikingBERT on the large-scale GLUE benchmark
demonstrates the efficacy of our approach in terms of computational operations
and inference latency. Our approach offers a compelling solution for real-time,
low-power natural language processing applications, making Spiking LLMs more
practical for deployment on edge devices and in power-constrained settings.

</details>


### [56] [Encoding Tactile Stimuli for Organoid Intelligence in Braille Recognition](https://arxiv.org/abs/2508.20850)
*Tianyi Liu,Hemma Philamore,Benjamin Ward-Cherrier*

Main category: cs.NE

TL;DR: 该研究提出了一种通用的编码策略，将触觉传感器数据映射到电刺激模式，使神经类器官能够完成开环的触觉盲文分类任务。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索类器官作为低功耗、自适应生物混合计算元件的潜力，并为未来可扩展的生物混合计算架构提供基础编码框架。

Method: 使用低密度微电极阵列（MEA）培养的人类前脑类器官被系统地刺激，以表征电刺激参数与类器官反应之间的关系。事件触觉输入记录的触觉数据被用于实现系统。

Result: 单类器官的平均盲文字母分类准确率为61%，三器官组合的准确率显著提高到83%，且多器官配置表现出更强的抗噪声能力。

Conclusion: 研究证明了类器官作为生物混合计算元件的潜力，并为未来的可扩展架构提供了编码框架。

Abstract: This study proposes a generalizable encoding strategy that maps tactile
sensor data to electrical stimulation patterns, enabling neural organoids to
perform an open-loop artificial tactile Braille classification task. Human
forebrain organoids cultured on a low-density microelectrode array (MEA) are
systematically stimulated to characterize the relationship between electrical
stimulation parameters (number of pulse, phase amplitude, phase duration, and
trigger delay) and organoid responses, measured as spike activity and spatial
displacement of the center of activity. Implemented on event-based tactile
inputs recorded from the Evetac sensor, our system achieved an average Braille
letter classification accuracy of 61 percent with a single organoid, which
increased significantly to 83 percent when responses from a three-organoid
ensemble were combined. Additionally, the multi-organoid configuration
demonstrated enhanced robustness against various types of artificially
introduced noise. This research demonstrates the potential of organoids as
low-power, adaptive bio-hybrid computational elements and provides a
foundational encoding framework for future scalable bio-hybrid computing
architectures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [57] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: P2C框架通过因果模型和有序动作序列解决现有反事实解释的局限性，提供可实现的解决方案。


<details>
  <summary>Details</summary>
Motivation: 高风险的机器学习决策需要透明度和可操作性，但现有反事实解释忽略因果关系和动作顺序，导致不切实际。

Method: P2C通过因果模型和s(CASP)系统生成有序动作序列，确保每个中间状态合理且因果一致。

Result: P2C能生成可行的反事实计划，其成本估算更真实，且优于缺乏因果知识的传统规划器。

Conclusion: P2C在透明度和可操作性上优于现有方法，为高风险决策提供更实用的反事实解释。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [58] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 该论文提出了一种可微分的神经符号架构和专用损失函数，用于从自然输入中学习解决NP难推理问题，并在多个基准测试中表现出高效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 当前，结合离散推理（如NP难问题）与神经网络的兴趣日益增长，尤其是从自然输入中学习解决这类问题的能力，这是大语言模型难以胜任的任务。

Method: 论文提出了一种新的概率损失函数，能够同时学习约束和目标，并通过将组合求解器移出训练循环实现可扩展训练。

Result: 实验表明，该架构在解决NP难推理问题时表现高效，尤其在Sudoku变体和视觉Min-Cut/Max-cut任务中优于其他混合方法，并能有效学习蛋白质设计的能量优化公式。

Conclusion: 该神经符号架构为从自然输入中学习解决复杂推理问题提供了一种高效且可解释的方法，具有广泛的应用潜力。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [59] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: 论文介绍了QAgent，一个基于大语言模型的多智能体系统，用于自动化生成OpenQASM代码，显著提高了量子编程的可访问性和准确性。


<details>
  <summary>Details</summary>
Motivation: NISQ设备虽已展现量子优势，但编程复杂性阻碍了非专家的使用。现有大语言模型在量子领域的应用有限，需要更通用的自动化工具。

Method: 结合任务规划、上下文少样本学习、检索增强生成、预定义生成工具和链式推理，QAgent系统化提升了编译和功能正确性。

Result: QAgent在多个不同规模的大语言模型上，将QASM代码生成的准确性提高了71.6%。

Conclusion: QAgent有望促进量子编程的普及，缩小专业知识差距，加速量子计算的实践应用。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [60] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 本文提出了一种个性化健康助手（PHA），通过多代理框架满足日常非临床环境中的多样化健康需求。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用大型语言模型（LLM）开发满足个人日常健康需求的多模态健康助手。

Method: 结合用户需求分析，设计了一个包含三个专业子代理（数据分析、健康专家、健康教练）的多代理框架，并进行自动化和人工评估。

Result: 在10个基准任务中进行了全面评估，涉及7000多个标注和1100小时的专家与用户投入，证明了系统的有效性。

Conclusion: PHA为未来普及个性化健康助手奠定了基础，提供了迄今为止最全面的健康助手评估。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [61] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: VSF是一种简单高效的方法，通过翻转负提示的注意力值符号来抑制不期望的内容，适用于少步扩散和流匹配图像生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法如CFG、NASA和NAG在少步模型中处理负提示时效率不足，VSF旨在动态抑制不期望内容，提升负提示的遵循效果。

Method: VSF通过翻转负提示的注意力值符号来动态抑制不必要内容，适用于MMDiT架构和跨注意力模型。

Result: 实验显示，VSF在少步模型中显著优于现有方法，甚至在非少步模型中也能与CFG竞争，同时保持图像质量。

Conclusion: VSF是一种高效且计算开销小的负提示引导方法，适用于多种图像和视频生成任务。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for
incorporating negative prompt guidance in few-step diffusion and flow-matching
image generation models. Unlike existing approaches such as classifier-free
guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by
flipping the sign of attention values from negative prompts. Our method
requires only small computational overhead and integrates effectively with
MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as
cross-attention-based models like Wan. We validate VSF on challenging datasets
with complex prompt pairs and demonstrate superior performance in both static
image and video generation tasks. Experimental results show that VSF
significantly improves negative prompt adherence compared to prior methods in
few-step models, and even CFG in non-few-step models, while maintaining
competitive image quality. Code and ComfyUI node are available in
https://github.com/weathon/VSF/tree/main.

</details>


### [62] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 提出了CHAIR-DPO方法，通过CHAIR指标和DPO优化，减少MLLM幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决MLLM在多模态任务中输出内容与视觉输入不符的幻觉问题。

Method: 利用CHAIR指标区分答案优劣，并通过DPO直接优化MLLM。

Result: CHAIR-DPO在多个基准测试中显著减少幻觉输出。

Conclusion: CHAIR-DPO是一种简单有效的减少MLLM幻觉的方法。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [63] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 本文提出首个统一框架，整合手语、唇动和音频多种模态生成口语文本，探索模态协同作用并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决听力障碍者无法使用语音识别技术的问题，探索视觉替代方案（如手语和唇读）的整合。

Method: 设计统一、模态无关的架构处理异构输入，并研究唇动作为非手动线索在手语理解中的作用。

Result: 性能达到或超越单任务最优模型，唇动建模显著提升手语翻译效果。

Conclusion: 多模态整合框架有效提升任务性能，唇动作为独立模态对SLT性能贡献显著。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [64] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: 该论文介绍了S-HArM数据集，用于意图感知分类，并通过三种提示策略构建合成数据，研究了多种模型方法，发现保留视觉上下文的模型表现更好，但总体性能仍有局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究多忽略AI生成图像背后的意图，因此需要专门的数据集和架构来填补这一空白。

Method: 引入了S-HArM数据集和三种提示策略（图像引导、描述引导和多模态引导），并使用多种模型方法进行对比研究。

Result: 图像和多模态引导的模型在“野外”内容中表现更好，但整体性能有限。

Conclusion: 推断意图的复杂性需要专门架构的进一步研究。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [65] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: FakeParts是一种新型的深度伪造技术，通过局部、细微的视频修改（如面部表情、物体替换等）增强欺骗性。研究者提出了首个大规模基准数据集FakePartsBench，并发现其降低了人类和现有检测模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造检测技术难以应对局部伪造内容的挑战，研究者旨在填补这一空白。

Method: 提出FakePartsBench数据集，包含25K视频及像素级标注，用于评估检测方法。

Result: FakeParts使人类检测准确率降低30%，且对先进检测模型同样有效。

Conclusion: 研究揭示了当前检测技术的漏洞，并为开发更鲁棒的方法提供了资源。

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [66] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: MedFoundationHub是一个GUI工具包，旨在解决医疗视觉语言模型的安全问题，支持医生和工程师使用和部署模型，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉语言模型虽具潜力，但存在安全问题，如Protected Health Information (PHI)泄露和网络威胁。

Method: MedFoundationHub提供了一个图形界面，支持手动选择模型、无缝部署开源模型，并通过Docker确保隐私保护的推理。

Result: 工具包在离线工作站上运行，评估了五种先进VLMs，发现存在答案偏离、模糊推理和术语不一致等问题。

Conclusion: MedFoundationHub是一个安全且易用的工具，但仍需改进模型的表现以满足临床需求。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [67] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 论文提出了一种模块化框架，通过将因果推理与答案生成解耦，利用自然语言因果链作为可解释的中间表示，提升了视频问答模型的推理能力和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有因果视频问答模型依赖不透明的黑盒方法，缺乏解释性且推理能力有限，需要一种能明确分离因果推理与答案生成的模块化方法。

Method: 提出两阶段架构：1) 因果链提取器（CCE）从视频-问题对生成因果链；2) 因果链驱动的答案生成器（CCDA）基于因果链生成答案，并利用大语言模型生成高质量因果链标注。

Result: 在多个大规模基准测试中，该方法不仅优于现有模型，还在解释性、用户信任和泛化能力上取得显著提升。

Conclusion: 该方法通过模块化和显式因果推理，显著提升了视频问答模型的性能和透明度，同时提出新评估指标CauCo。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [68] [The Mathematician's Assistant: Integrating AI into Research Practice](https://arxiv.org/abs/2508.20236)
*Jonas Henkel*

Main category: math.HO

TL;DR: 论文探讨了AI（如LLMs）在数学研究中的应用潜力与局限性，提出了以'增强数学家'为核心的框架，并总结了AI在研究中作为辅助工具的五项原则和七种应用方式。


<details>
  <summary>Details</summary>
Motivation: 研究AI（尤其是LLMs）在数学研究中的实际应用潜力，分析其优缺点，并提出如何有效整合AI到研究流程中的方法。

Method: 通过分析近期基准测试（如MathArena和Open Proof Corpus）的数据，评估AI的表现，并提出一个基于五项原则的框架。

Result: 发现AI在解决问题和验证证明方面表现优异，但也存在系统性缺陷（如缺乏自我批判），需人类主导的协作模式。

Conclusion: AI当前的角色是'增强'而非替代人类研究者，需培养新的技能（如策略性提示和批判性验证）以实现高效合作。

Abstract: The rapid development of artificial intelligence (AI), marked by
breakthroughs like 'AlphaEvolve' and 'Gemini Deep Think', is beginning to offer
powerful new tools that have the potential to significantly alter the research
practice in many areas of mathematics. This paper explores the current
landscape of publicly accessible large language models (LLMs) in a mathematical
research context, based on developments up to August 2, 2025. Our analysis of
recent benchmarks, such as MathArena and the Open Proof Corpus (Balunovi\'c et
al., 2025; Dekoninck et al., 2025), reveals a complex duality: while
state-of-the-art models demonstrate strong abilities in solving problems and
evaluating proofs, they also exhibit systematic flaws, including a lack of
self-critique and a model depending discrepancy between final-answer accuracy
and full-proof validity.
  Based on these findings, we propose a durable framework for integrating AI
into the research workflow, centered on the principle of the augmented
mathematician. In this model, the AI functions as a copilot under the critical
guidance of the human researcher, an approach distilled into five guiding
principles for effective and responsible use. We then systematically explore
seven fundamental ways AI can be applied across the research lifecycle, from
creativity and ideation to the final writing process, demonstrating how these
principles translate into concrete practice.
  We conclude that the primary role of AI is currently augmentation rather than
automation. This requires a new skill set focused on strategic prompting,
critical verification, and methodological rigor in order to effectively use
these powerful tools.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [69] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 该论文提出了一种基于图结构学习（GSL）的防护框架，用于抵御能源物联网（IoE）中的网络攻击，相较于传统方法更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 能源物联网的互联性使其面临严重网络安全威胁，尤其是对抗性攻击，需要创新的防护方法来保障公共安全。

Method: 通过联合优化图拓扑和节点表示，设计了GSL框架，并通过案例研究验证其效果。

Result: GSL展现出比代表性方法更优越的鲁棒性，为IoE网络安全提供了可行方案。

Conclusion: GSL能增强未来IoE网络的韧性和可靠性，文中还指出了该领域的关键挑战和未来研究方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [70] [FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture](https://arxiv.org/abs/2508.20212)
*Minghao Hu,Junzhe Wang,Weisen Zhao,Qiang Zeng,Lannan Luo*

Main category: cs.CR

TL;DR: 本文提出了一种利用神经机器翻译和标准化流的方法，减少了跨指令集架构（ISA）的恶意软件检测所需的数据收集工作。


<details>
  <summary>Details</summary>
Motivation: 随着针对物联网设备的网络攻击增加，恶意软件的跨架构检测需求迫切，但数据收集和标注成本高昂。

Method: 通过神经机器翻译和标准化流技术，将目标ISA的恶意软件翻译至已有充足数据的ISA（如X86-64），再利用单ISA训练的模型进行检测。

Result: 该方法减少了跨ISA恶意软件检测的数据收集负担，提高了检测效率。

Conclusion: 提出的方法为跨ISA恶意软件检测提供了一种高效且低成本的解决方案。

Abstract: Applying deep learning to malware detection has drawn great attention due to
its notable performance. With the increasing prevalence of cyberattacks
targeting IoT devices, there is a parallel rise in the development of malware
across various Instruction Set Architectures (ISAs). It is thus important to
extend malware detection capacity to multiple ISAs. However, training a deep
learning-based malware detection model usually requires a large number of
labeled malware samples. The process of collecting and labeling sufficient
malware samples to build datasets for each ISA is labor-intensive and
time-consuming. To reduce the burden of data collection, we propose to leverage
the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for
malware detection. Specifically, when dealing with malware in a certain ISA, we
translate it to an ISA with sufficient malware samples (like X86-64). This
allows us to apply a model trained on one ISA to analyze malware from another
ISA. Our approach reduces the data collection effort by enabling malware
detection across multiple ISAs using a model trained on a single ISA.

</details>


### [71] [Characterizing Trust Boundary Vulnerabilities in TEE Containers](https://arxiv.org/abs/2508.20962)
*Weijie Liu,Hongbo Chen,Shuo Huai,Zhen Xu,Wenhao Wang,Zhi Li,Zheli Liu*

Main category: cs.CR

TL;DR: 本文分析了现有TEE容器的隔离策略及漏洞，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究TEE容器的隔离策略，以发现其潜在安全风险并提出改进方向。

Method: 设计自动化分析工具，用于识别和评估TEE容器的隔离边界。

Result: 发现部分TEE容器存在信息泄漏、回滚攻击等设计漏洞，未能实现安全目标。

Conclusion: 总结了安全容器开发的教训，并讨论了TEE容器化设计的未来趋势。

Abstract: Trusted Execution Environments (TEEs) have emerged as a cornerstone of
confidential computing, garnering significant attention from both academia and
industry. To enable the secure development, execution, and deployment, of
applications on TEE platforms, TEE containers have been introduced as
middleware solutions. These containers aim to shield applications from
potentially malicious operating systems and orchestration interfaces while
maintaining usability and reliability. In this paper, we analyze the isolation
strategies employed by existing TEE containers to protect secure applications.
To address the challenges in analyzing these interfaces, we designed an
automated analyzer to precisely identify and evaluate their isolation
boundaries. We observed that some TEE containers fail to achieve their intended
goals due to critical design and implementation flaws, such as information
leakage, rollback attacks, denial-of-service, and Iago attacks, which pose
significant security risks. Drawing from our findings, we share key lessons to
guide the development of more secure container solutions and discuss emerging
trends in TEE containerization design.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [72] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 本文提出了一个用于评估大语言模型（LLMs）主动对话能力的统一框架ProactiveEval，通过分解主动对话为目标规划和对话引导，并在多领域生成多样化评估数据。实验表明，某些LLMs在特定任务上表现优异，并探讨了推理能力对主动行为的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在特定领域或任务导向的场景，导致评估碎片化，限制了模型主动对话能力的全面探索。因此，需要开发一个统一的评估框架。

Method: 提出了ProactiveEval框架，将主动对话分解为目标规划和对话引导，建立了多领域的评估指标，并自动生成多样化评估数据。

Result: 实验覆盖22种不同类型的LLMs，发现DeepSeek-R1和Claude-3.7-Sonnet分别在目标规划和对话引导任务上表现突出。

Conclusion: 探讨了推理能力对主动行为的影响，对未来模型发展提出了启示。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [73] [High performance visualization for Astronomy and Cosmology: the VisIVO's pathway toward Exascale systems](https://arxiv.org/abs/2508.20603)
*Eva Sciacca,Nicola Tuccari,Fabio Vitello,Valentina Cesare*

Main category: astro-ph.IM

TL;DR: VisIVO工具用于处理天文学中的大数据分析，未来将优化高性能可视化功能，以支持Exascale系统的数据处理需求。


<details>
  <summary>Details</summary>
Motivation: 现代天文学和天体物理学中产生的大数据量对存储、访问和分析提出了挑战，需要新一代软件工具。

Method: 通过容器化和虚拟化技术，利用分布式计算基础设施和EOSC，优化VisIVO在多维数据分析中的应用。

Result: VisIVO已成功应用于天体物理学和宇宙学中的数据可视化，并计划在Exascale系统中进一步优化性能。

Conclusion: VisIVO的持续发展将提升数据处理效率、可移植性和资源利用灵活性，支持更广泛的应用场景。

Abstract: Petabyte-scale data volumes are generated by observations and simulations in
modern astronomy and astrophysics. Storage, access, and data analysis are
significantly hampered by such data volumes and are leading to the development
of a new generation of software tools. The Visualization Interface for the
Virtual Observatory (VisIVO) has been designed, developed and maintained by
INAF since 2005 to perform multi-dimensional data analysis and knowledge
discovery in multivariate astrophysical datasets. Utilizing containerization
and virtualization technologies, VisIVO has already been used to exploit
distributed computing infrastructures including the European Open Science Cloud
(EOSC).
  We intend to adapt VisIVO solutions for high performance visualization of
data generated on the (pre-)Exascale systems by HPC applications in
Astrophysics and Cosmology (A\&C), including GADGET (GAlaxies with Dark matter
and Gas) and PLUTO simulations, thanks to the collaboration within the SPACE
Center of Excellence, the H2020 EUPEX Project, and the ICSC National Research
Centre. In this work, we outline the evolution's course as well as the
execution strategies designed to achieve the following goals: enhance the
portability of the VisIVO modular applications and their resource requirements;
foster reproducibility and maintainability; take advantage of a more flexible
resource exploitation over heterogeneous HPC facilities; and, finally, minimize
data-movement overheads and improve I/O performances.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [74] [Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR](https://arxiv.org/abs/2508.20250)
*Jessica Kinnevan,Naifa Alqahtani,Toral Chauhan*

Main category: eess.IV

TL;DR: 使用iPhone 15 Pro Max的LiDAR技术取代传统背景去除方法，其深度信息不依赖光源，适用于各种光照条件。


<details>
  <summary>Details</summary>
Motivation: 传统的背景去除技术如色键抠像和AI模型受光照条件限制，LiDAR技术提供了更稳定的解决方案。

Method: 整合iPhone的LiDAR和彩色摄像头，使用SwiftUI和Swift开发界面和后端，MSL实现实时图像增强。

Result: 系统能以60帧/秒运行，但深度图分辨率受限于带宽（320x240），且某些材料会影响LiDAR精度。

Conclusion: 若LiDAR分辨率提升至匹配彩色图像，将成为视频和摄影中背景去除的主流方法。

Abstract: Light Detection and Ranging (LiDAR) technology in consumer-grade mobile
devices can be used as a replacement for traditional background removal and
compositing techniques. Unlike approaches such as chroma keying and trained AI
models, LiDAR's depth information is independent of subject lighting, and
performs equally well in low-light and well-lit environments. We integrate the
LiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image
processing. We use Apple's SwiftUI and Swift frameworks for user interface and
backend development, and Metal Shader Language (MSL) for realtime image
enhancement at the standard iPhone streaming frame rate of 60 frames per
second. The only meaningful limitations of the technology are the streaming
bandwidth of the depth data, which currently reduces the depth map resolution
to 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect
accurate depth from some materials. If the LiDAR resolution on a mobile device
like the iPhone can be improved to match the color image resolution, LiDAR
could feasibly become the preeminent method of background removal for video
applications and photography.

</details>


### [75] [Is the medical image segmentation problem solved? A survey of current developments and future directions](https://arxiv.org/abs/2508.20139)
*Guoping Xu,Jayaram K. Udupa,Jax Luo,Songlin Zhao,Yajun Yu,Scott B. Raymond,Hao Peng,Lipeng Ning,Yogesh Rathi,Wei Liu,You Zhang*

Main category: eess.IV

TL;DR: 医学图像分割在过去二十年中快速发展，深度学习推动了细胞、组织、器官和病理的精确划分。本文回顾了其进展，总结了七个关键维度，并提供了开源资源。


<details>
  <summary>Details</summary>
Motivation: 探讨当前模型在医学图像分割中是否克服了长期挑战，以及尚存的差距，为未来研究提供方向。

Method: 通过回顾十年来的研究进展，分析多尺度分析、注意力机制等核心原理，并从七个维度讨论发展趋势。

Result: 总结了医学图像分割的主要发展方向，包括学习方式、任务类型、多模态整合等七个关键维度。

Conclusion: 本文为深度学习在医学图像分割中的发展提供了全面视角，旨在激发未来创新，并提供了持续更新的资源库。

Abstract: Medical image segmentation has advanced rapidly over the past two decades,
largely driven by deep learning, which has enabled accurate and efficient
delineation of cells, tissues, organs, and pathologies across diverse imaging
modalities. This progress raises a fundamental question: to what extent have
current models overcome persistent challenges, and what gaps remain? In this
work, we provide an in-depth review of medical image segmentation, tracing its
progress and key developments over the past decade. We examine core principles,
including multiscale analysis, attention mechanisms, and the integration of
prior knowledge, across the encoder, bottleneck, skip connections, and decoder
components of segmentation networks. Our discussion is organized around seven
key dimensions: (1) the shift from supervised to semi-/unsupervised learning,
(2) the transition from organ segmentation to lesion-focused tasks, (3)
advances in multi-modality integration and domain adaptation, (4) the role of
foundation models and transfer learning, (5) the move from deterministic to
probabilistic segmentation, (6) the progression from 2D to 3D and 4D
segmentation, and (7) the trend from model invocation to segmentation agents.
Together, these perspectives provide a holistic overview of the trajectory of
deep learning-based medical image segmentation and aim to inspire future
innovation. To support ongoing research, we maintain a continually updated
repository of relevant literature and open-source resources at
https://github.com/apple1986/medicalSegReview

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [76] [MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening](https://arxiv.org/abs/2508.20513)
*Yongqi Shao,Binxin Mei,Cong Tan,Hong Huo,Tao Fang*

Main category: cs.SD

TL;DR: MoTAS框架通过TTS数据增强和MoE特征选择机制，显著提高了阿尔茨海默病早期筛查的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决阿尔茨海默病早期筛查中数据有限和特征选择不足的问题。

Method: 结合TTS数据增强和MoE动态特征选择，优化多模态特征融合。

Result: 在ADReSSo数据集上达到85.71%的准确率，优于现有方法。

Conclusion: MoTAS在数据有限的实际场景中具有显著的实用价值。

Abstract: Early screening for Alzheimer's Disease (AD) through speech presents a
promising non-invasive approach. However, challenges such as limited data and
the lack of fine-grained, adaptive feature selection often hinder performance.
To address these issues, we propose MoTAS, a robust framework designed to
enhance AD screening efficiency. MoTAS leverages Text-to-Speech (TTS)
augmentation to increase data volume and employs a Mixture of Experts (MoE)
mechanism to improve multimodal feature selection, jointly enhancing model
generalization. The process begins with automatic speech recognition (ASR) to
obtain accurate transcriptions. TTS is then used to synthesize speech that
enriches the dataset. After extracting acoustic and text embeddings, the MoE
mechanism dynamically selects the most informative features, optimizing feature
fusion for improved classification. Evaluated on the ADReSSo dataset, MoTAS
achieves a leading accuracy of 85.71\%, outperforming existing baselines.
Ablation studies further validate the individual contributions of TTS
augmentation and MoE in boosting classification performance. These findings
highlight the practical value of MoTAS in real-world AD screening scenarios,
particularly in data-limited settings.

</details>


### [77] [Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music](https://arxiv.org/abs/2508.20665)
*Hongju Su,Ke Li,Lan Yang,Honggang Zhang,Yi-Zhe Song*

Main category: cs.SD

TL;DR: 论文提出了Amadeus框架，通过双重架构（自回归模型和双向扩散模型）提升符号音乐生成性能，并引入两种新策略增强潜在表示和条件信息，显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型基于单向时间依赖假设，但发现音乐属性本质是无序的，因此提出新框架以更高效生成音乐。

Method: 采用双重架构：自回归模型处理音符序列，双向扩散模型处理属性；引入MLSDES和CIEM增强表示和条件信息。

Result: 在无条件及文本条件下生成任务中显著优于SOTA模型，速度提升4倍以上，支持细粒度控制。

Conclusion: Amadeus框架突破了现有模型性能界限，并通过新数据集AMD进一步探索其潜力。

Abstract: Existing state-of-the-art symbolic music generation models predominantly
adopt autoregressive or hierarchical autoregressive architectures, modelling
symbolic music as a sequence of attribute tokens with unidirectional temporal
dependencies, under the assumption of a fixed, strict dependency structure
among these attributes. However, we observe that using different attributes as
the initial token in these models leads to comparable performance. This
suggests that the attributes of a musical note are, in essence, a concurrent
and unordered set, rather than a temporally dependent sequence. Based on this
insight, we introduce Amadeus, a novel symbolic music generation framework.
Amadeus adopts a two-level architecture: an autoregressive model for note
sequences and a bidirectional discrete diffusion model for attributes. To
enhance performance, we propose Music Latent Space Discriminability Enhancement
Strategy(MLSDES), incorporating contrastive learning constraints that amplify
discriminability of intermediate music representations. The Conditional
Information Enhancement Module (CIEM) simultaneously strengthens note latent
vector representation via attention mechanisms, enabling more precise note
decoding. We conduct extensive experiments on unconditional and
text-conditioned generation tasks. Amadeus significantly outperforms SOTA
models across multiple metrics while achieving at least 4$\times$ speed-up.
Furthermore, we demonstrate training-free, fine-grained note attribute control
feasibility using our model. To explore the upper performance bound of the
Amadeus architecture, we compile the largest open-source symbolic music dataset
to date, AMD (Amadeus MIDI Dataset), supporting both pre-training and
fine-tuning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs](https://arxiv.org/abs/2508.20333)
*Md Abdullah Al Mamun,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 论文提出了一种名为'颠覆性对齐注入'（SAI）的攻击方法，通过利用大语言模型的对齐机制，诱导模型在某些特定话题上拒绝回答，从而注入偏见或实施针对性审查。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示大语言模型（LLMs）在伦理对齐中可能被恶意利用的漏洞，即使模型在无关话题上表现正常。

Method: 方法是通过数据投毒（1%的污染数据），利用对齐机制触发模型对特定查询的拒绝行为。

Result: 结果表明，SAI攻击能导致高偏见（例如在医疗问题中ΔDP为23%），且能逃避现有防御机制。

Conclusion: 结论是当前的对齐机制和防御方法存在严重漏洞，需进一步改进以防止此类攻击。

Abstract: Large Language Models (LLMs) are aligned to meet ethical standards and safety
requirements by training them to refuse answering harmful or unsafe prompts. In
this paper, we demonstrate how adversaries can exploit LLMs' alignment to
implant bias, or enforce targeted censorship without degrading the model's
responsiveness to unrelated topics. Specifically, we propose Subversive
Alignment Injection (SAI), a poisoning attack that leverages the alignment
mechanism to trigger refusal on specific topics or queries predefined by the
adversary. Although it is perhaps not surprising that refusal can be induced
through overalignment, we demonstrate how this refusal can be exploited to
inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning
defenses including LLM state forensics, as well as robust aggregation
techniques that are designed to detect poisoning in FL settings. We demonstrate
the practical dangers of this attack by illustrating its end-to-end impacts on
LLM-powered application pipelines. For chat based applications such as
ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare
questions to targeted racial category leading to high bias ($\Delta DP$ of
23%). We also show that bias can be induced in other NLP tasks: for a resume
selection pipeline aligned to refuse to summarize CVs from a selected
university, high bias in selection ($\Delta DP$ of 27%) results. Even higher
bias ($\Delta DP$~38%) results on 9 other chat based downstream applications.

</details>


### [79] [A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks](https://arxiv.org/abs/2508.20645)
*Xinli Shi,Xingxing Yuan,Longkang Zhu,Guanghui Wen*

Main category: cs.LG

TL;DR: 论文提出了一种名为TV-HSGT的新型分布式在线优化算法，通过混合随机梯度跟踪和方差减少机制，适用于时变有向网络，无需假设梯度有界性。


<details>
  <summary>Details</summary>
Motivation: 现有算法通常依赖于梯度有界假设，且忽视了随机梯度在时变有向网络中的影响，难以满足实时决策的需求。

Method: TV-HSGT结合了行随机和列随机通信方案，通过当前和递归随机梯度的结合来减少梯度方差并准确跟踪全局下降方向。

Result: 理论分析显示TV-HSGT在不假设梯度有界性的情况下，能够改善动态遗憾界。实验证明其在动态和资源受限环境中有效。

Conclusion: TV-HSGT是一种高效的分布式在线优化算法，适用于时变网络环境，具有理论和实际应用价值。

Abstract: With the increasing scale and dynamics of data, distributed online
optimization has become essential for real-time decision-making in various
applications. However, existing algorithms often rely on bounded gradient
assumptions and overlook the impact of stochastic gradients, especially in
time-varying directed networks. This study proposes a novel Time-Varying Hybrid
Stochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid
stochastic gradient tracking and variance reduction mechanisms. Specifically,
TV-HSGT integrates row-stochastic and column-stochastic communication schemes
over time-varying digraphs, eliminating the need for Perron vector estimation
or out-degree information. By combining current and recursive stochastic
gradients, it effectively reduces gradient variance while accurately tracking
global descent directions. Theoretical analysis demonstrates that TV-HSGT can
achieve improved bounds on dynamic regret without assuming gradient
boundedness. Experimental results on logistic regression tasks confirm the
effectiveness of TV-HSGT in dynamic and resource-constrained environments.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [80] [Lattice Random Walk Discretisations of Stochastic Differential Equations](https://arxiv.org/abs/2508.20883)
*Samuel Duffield,Maxwell Aifer,Denis Melanson,Zach Belateche,Patrick J. Coles*

Main category: math.NA

TL;DR: 提出一种基于格点随机游走的离散化方法，用于SDE，简化计算并提供多项优势。


<details>
  <summary>Details</summary>
Motivation: 减少SDE离散化过程中的复杂计算，提升兼容性和鲁棒性。

Method: 使用二进制或三元增量采样，替代传统浮点计算。

Result: 证明弱收敛，并在多种SDE实验中验证优势。

Conclusion: 该方法具有计算简化、兼容性强等优势，适用于扩散模型等场景。

Abstract: We introduce a lattice random walk discretisation scheme for stochastic
differential equations (SDEs) that samples binary or ternary increments at each
step, suppressing complex drift and diffusion computations to simple 1 or 2 bit
random values. This approach is a significant departure from traditional
floating point discretisations and offers several advantages; including
compatibility with stochastic computing architectures that avoid floating-point
arithmetic in place of directly manipulating the underlying probability
distribution of a bitstream, elimination of Gaussian sampling requirements,
robustness to quantisation errors, and handling of non-Lipschitz drifts. We
prove weak convergence and demonstrate the advantages through experiments on
various SDEs, including state-of-the-art diffusion models.

</details>
