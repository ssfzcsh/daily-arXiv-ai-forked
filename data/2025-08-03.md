<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 26]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DB](#cs.DB) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.AI](#cs.AI) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: 探讨大型语言模型（LLM）从业务流程描述生成智能合约代码的能力，并提出自动化评估框架。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的代码生成方法存在局限性，LLM被探索用于代码生成，但现有工作在小样本上评估，缺乏自动化和执行正确性验证。

Method: 引入自动化评估框架，测试不同规模和类型的LLM在实现流程执行关键属性（如流程控制、资源分配和数据条件）上的能力。

Result: LLM在生成智能合约代码时，性能未达到所需的完美可靠性。

Conclusion: 建议未来研究探索如何将LLM负责任地集成到现有代码生成工具中，以提高输出可靠性。提出的评估框架可作为开发和评估此类集成的基础。

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [2] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: FlowETL是一种基于示例的自主ETL流水线架构，旨在自动标准化和准备输入数据集，减少人工干预需求。


<details>
  <summary>Details</summary>
Motivation: 现代ETL解决方案需要大量人工设计特定上下文的、不可通用的转换，缺乏自动化设计及应用的能力。

Method: FlowETL通过规划引擎和ETL工作器构建并应用转换计划，同时提供监控和日志功能。

Result: 在14个不同领域、文件结构和文件大小的数据集上表现出良好的泛化能力。

Conclusion: FlowETL展示了ETL任务自动化的潜力，减少了人工干预的需求。

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [3] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: 论文提出了一种结合AI与模型驱动工程（MDE）的"vibe modeling"方法，以解决复杂软件系统开发中的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统需求的增长和复杂性的增加，传统的开发方法（如MDE）和新兴的AI编码方法（如vibe coding）各自存在局限性，需要一种更高效且可靠的方法。

Method: 提出"vibe modeling"概念，整合AI和MDE的优势，通过自然语言描述快速生成可靠且可维护的代码模型。

Result: 论文概述了vibe modeling的关键概念，并展示了其在加速复杂系统开发中的潜力。

Conclusion: vibe modeling为未来的建模技术提供了新方向，但仍需解决一些开放性问题。

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [4] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: GitHub Marketplace中约65%的新CI工具是已有功能的复制品，通常在六个月内出现，少数先行工具占据大部分分支和扩展。


<details>
  <summary>Details</summary>
Motivation: 研究GitHub Marketplace中CI工具的功能冗余现象，以帮助开发者选择最佳发布时机和识别未覆盖功能。

Method: 通过链接6,983个CI Actions到3,869个提供者，挖掘版本历史并构建图模型，追踪功能发布时间和冗余工具聚类。

Result: 发现65%的新CI工具为重复功能，少数先行工具主导后续分支和扩展。

Conclusion: 研究结果帮助开发者优化工具发布策略，维护者清理冗余工具，并公开数据集以支持软件生态系统的创新研究。

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [5] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge自动化生成多模态物联网设备的集成代码，显著减少人工编程需求，并通过多阶段调试实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统物联网设备集成需要大量人工编程，效率低下且复杂。

Method: 采用分治策略：首先生成设备控制逻辑，然后合成平台兼容的集成代码，并结合多阶段调试。

Result: 在34个设备上实现平均93.87%的成功率和94.87%的功能覆盖率，用户反馈后可达到100%。

Conclusion: AutoBridge在代码准确率和效率上显著优于人工编程和商用LLMs，证明了其实际应用价值。

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [6] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: 论文提出可解释的自主业务流程（XABPs）以解决AI/ML驱动业务流程中的信任、调试、责任等问题，并探讨了其实现方法和研究挑战。


<details>
  <summary>Details</summary>
Motivation: 自主业务流程（ABPs）虽能提升效率，但可能引发信任、责任等问题，因此需要可解释性以增强透明度和合规性。

Method: 论文提出系统化的XABPs实现方法，包括形式分类、解释性结构设计，并识别了BPM领域的关键研究挑战。

Result: XABPs能够通过说明其决策逻辑来缓解ABPs的潜在问题，增强系统透明度和用户信任。

Conclusion: XABPs是解决自主业务流程问题的可行方案，未来需进一步研究其实现方法和BPM领域的挑战。

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [7] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate是一种竞争性多代理辩论框架，通过多轮辩论实现更全面的问题定位和修复方案生成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的问题解决方法常陷入局部解，难以捕获跨代码库的问题模式。

Method: 提出SWE-Debate框架：生成多故障传播路径提案，组织三轮辩论，整合修复方案并通过MCTS代理生成补丁。

Result: 在SWE-bench基准测试中取得最新最优结果，远超基线方法。

Conclusion: SWE-Debate通过多代理辩论显著提升问题解决的全面性和准确性。

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [8] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: 提出了一个用于评估COBOL到Java代码翻译的自动化系统，结合了分析检查器和LLM作为裁判的技术。


<details>
  <summary>Details</summary>
Motivation: 解决基于LLM的翻译工具在模型不透明和翻译质量评估复杂性方面的挑战。

Method: 使用分析检查器和LLM-as-a-judge技术，支持持续集成和大规模基准测试。

Result: 系统提供了可扩展的多维度评估，减少了对人工审查的依赖。

Conclusion: 该系统为开发者和项目经理提供了可操作的见解，有助于高质量代码库的现代化演进。

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [9] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Exp提出了一种经验增强方法，通过从先前的代理轨迹中提取可操作性经验，实现问题的持续学习，显著提高了软件问题解决率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在解决软件问题时缺乏记忆性，无法复用先前经验，导致冗余探索和效率低下。

Method: 引入多维度经验库，捕获成功和失败的修复尝试，提取不同层次的可复用知识。

Result: 在SWE-bench-Verified上达到41.6%的Pass@1解决率，表现优于现有方法。

Conclusion: SWE-Exp通过系统性积累和利用修复经验，实现了从试错探索到经验驱动问题解决的转变。

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [10] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: 提出了一种基于代理的集成推理方法 Trae Agent，用于解决软件问题中的大规模集成空间和仓库级理解问题，实验证明其性能优于现有方法，达到 SWE-bench 基准的最高分。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的集成推理方法在处理大规模集成空间和缺乏仓库级理解时表现不佳，Trae Agent 旨在克服这些限制。

Method: Trae Agent 将目标建模为最优解搜索问题，通过生成、剪枝和选择模块化代理来解决两大关键挑战。

Result: 在 SWE-bench 基准测试中，Trae Agent 平均提升 10.22%，最高 Pass@1 得分为 75.20%，位居榜首。

Conclusion: Trae Agent 是一种高效的仓库级问题解决方法，已开源以支持研究社区。

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [11] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: Kieker框架最初为Java设计，现在扩展支持Python，结合静态和动态分析提供全面的系统视图。


<details>
  <summary>Details</summary>
Motivation: Python的流行使得对其应用的结构化洞察非常有价值，因此扩展Kieker框架以支持Python是值得的。

Method: 结合静态和动态分析，构建Python应用的完整分析管道。

Result: 实现了对Python应用的全面分析能力。

Conclusion: Kieker框架的Python支持为开发者提供了强大的工具，以获取其应用的结构化洞察。

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [12] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 论文研究了GitLab合并请求（MRs）中的代码审查（CR）工作量，发现71%的MRs需要调整，28%涉及200多行代码修改。工作量和审查时间或参与者数量无关。机器学习模型能有效预测CR工作量，AUC达0.84-0.88。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注基于代码修改量的CR工作量，尤其是在GitLab MRs背景下。本文旨在填补这一空白，通过量化CR工作量并提供预测模型。

Method: 使用超过23,600个MRs的数据集，定义CR工作量为提交后的代码修改量。训练可解释的机器学习模型，结合文本特征、代码复杂度、开发者经验等多维度指标。

Result: 71%的MRs需要调整，28%涉及200多行代码修改。工作量与审查时间或参与者数量无关。模型AUC达0.84-0.88，复杂度、经验和文本特征是关键预测因素。

Conclusion: 研究发现CR工作量普遍存在且可预测，机器学习模型能有效解释和预测代码审查中的修改需求。

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Abstractions of Sequences, Functions and Operators](https://arxiv.org/abs/2507.23151)
*Louis Rustenholz,Pedro Lopez-Garcia,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 论文提出了关于函数格序理论的成果，重点研究高阶抽象解释中的Galois连接，用于推断递归定义函数的封闭形式边界。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过解决递归定义函数（如操作符的固定点或功能方程的解）的封闭形式边界推断问题，以应用于程序分析和混合系统等领域。

Method: 方法包括引入B-bound域这一新的约束型抽象域家族，用于抽象数值函数；并提出域抽象函子，将值空间映射提升为函数空间的Galois连接。

Result: 研究发现约束空间的凸性简化了转移函数设计，并支持高度非线性的数值不变式推断。

Conclusion: 通过基于简单操作语言的构造，论文成功地扩展了函数抽象的范围，包括多元、分段和非离散域。

Abstract: We present theoretical and practical results on the order theory of lattices
of functions, focusing on Galois connections that abstract (sets of) functions
- a topic known as higher-order abstract interpretation.
  We are motivated by the challenge of inferring closed-form bounds on
functions which are defined recursively, i.e. as the fixed point of an operator
or, equivalently, as the solution to a functional equation. This has multiple
applications in program analysis (e.g. cost analysis, loop acceleration,
declarative language analysis) and in hybrid systems governed by differential
equations.
  Our main contribution is a new family of constraint-based abstract domains
for abstracting numerical functions, B-bound domains, which abstract a function
f by a conjunction of bounds from a preselected set of boundary functions. They
allow inferring highly non-linear numerical invariants, which classical
numerical abstract domains struggle with. We uncover a convexity property in
the constraint space that simplifies, and, in some cases, fully automates,
transfer function design.
  We also introduce domain abstraction, a functor that lifts arbitrary mappings
in value space to Galois connections in function space. This supports
abstraction from symbolic to numerical functions (i.e. size abstraction), and
enables dimensionality reduction of equations.
  We base our constructions of transfer functions on a simple operator
language, starting with sequences, and extending to more general functions,
including multivariate, piecewise, and non-discrete domains.

</details>


### [14] [Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks](https://arxiv.org/abs/2507.23205)
*Hebi Li,Forrest Sheng Bao,Qi Xiao,Jin Tian*

Main category: cs.PL

TL;DR: Kernel-FFI是一个透明的、语言无关的框架，用于在交互式笔记本中实现无缝跨语言函数调用和对象操作。


<details>
  <summary>Details</summary>
Motivation: 现有FFI解决方案不适用于现代笔记本环境的动态交互工作流，需手动配置且不支持递归调用和OOP结构。

Method: 采用源码级转换自动重写跨语言调用，支持OOP和资源管理，并引入侧信道通信机制解决阻塞问题。

Result: 实现了在Jupyter等笔记本环境中的无缝跨语言交互，支持递归和异步调用。

Conclusion: Kernel-FFI为多语言开发提供了高效、透明的解决方案，解决了现有FFI的局限性。

Abstract: Foreign Function Interfaces (FFIs) are essential for enabling
interoperability between programming languages, yet existing FFI solutions are
ill-suited for the dynamic, interactive workflows prevalent in modern notebook
environments such as Jupyter. Current approaches require extensive manual
configuration, introduce significant boilerplate, and often lack support for
recursive calls and object-oriented programming (OOP) constructs-features
critical for productive, multi-language development.
  We present Kernel-FFI, a transparent, language-agnostic framework that
enables seamless cross-language function calls and object manipulation within
interactive notebooks. Kernel-FFI employs source-level transformation to
automatically rewrite cross-language invocations, eliminating the need for
manual bindings or boilerplate. Kernel-FFI provides robust support for OOP by
enabling foreign object referencing and automatic resource management across
language boundaries. Furthermore, to address the blocking nature of Jupyter
kernels and support recursive and asynchronous foreign calls, we introduce a
novel side-channel communication mechanism. Our tool will be open-sourced and
available at https://codepod.io/docs/kernel-ffi

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [15] [PRIME: Pseudo-Random Integrated Multi-Part Entropy for Adaptive Packet Spraying in AI/ML Data centers](https://arxiv.org/abs/2507.23012)
*Ashkan Sobhani,Sogand Sadrhaghighi,Xingjun Chu*

Main category: cs.NI

TL;DR: PRIME是一种伪随机轮询包喷洒方法，通过考虑网络拓扑和拥塞信号优化负载分配和性能，相比现有方法在特定场景下性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式训练对网络基础设施提出了高要求，现有包喷洒解决方案在不对称网络条件下可能导致尾部延迟增加和性能下降。

Method: PRIME采用伪随机轮询包喷洒方法，利用拥塞信号重新平衡负载，避免网络热点。

Result: 实验结果显示，PRIME在特定场景下比现有解决方案性能提升15%（排列流量）至27%（网络退化场景）。

Conclusion: PRIME通过优化负载分布和性能，解决了现有包喷洒方案的不足，适用于大规模分布式训练网络。

Abstract: Large-scale distributed training in production data centers place significant
demands on network infrastructure. In particular, significant load balancing
challenges arise when processing AI/ML workloads, consisting of low-entropy,
bursty and long-lived flows. Existing solutions designed for Ethernet, such as
Equal-Cost Multi-Path (ECMP) struggle to maintain high network utilization.
While major industry players (e.g., Ultra Ethernet Consortium) and parts of
academia have proposed packet spraying to enhance AI/ML workload performance,
we argue that existing packet spraying solutions lead to buffer inflation over
time, negatively affecting network performance. Specifically, when ACK
coalescing is used, these solutions lead to stale information, degrading
network performance. Additionally, in asymmetric network conditions- such as
mix of ordered an unordered traffic, or link degradation and failures- existing
packet spraying solutions often lead to increased tail latency. In this paper,
we present the design and evaluation of PRIME, a pseudo-randomized round-robin
approach to packet spraying that considers the network topology to optimize
load distribution and performance. PRIME uses congestion as an indicator to
re-balance the load. To this extent, PRIME takes into account various
congestion signals, accounting for congestion severity, and their decay times
to avoid network hotspots. We extensively evaluated PRIME using large-scale
production-level simulator. Our results indicate that, compared to existing
solutions, PRIME leads to up to 15% improvement for permutation traffic and up
to 27% improvement in network degradation scenarios

</details>


### [16] [InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps](https://arxiv.org/abs/2507.23177)
*Neagin Neasamoni Santhi,Davide Villa,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文提出InterfO-RAN，一种基于CNN的实时解决方案，用于检测5G网络中带内干扰，准确率超91%，提升密集部署性能。


<details>
  <summary>Details</summary>
Motivation: 5G网络中带内干扰导致信号质量下降，影响协议操作和网络性能，尤其是在密集部署和毫米波系统中。

Method: 采用卷积神经网络（CNN）处理I/Q样本，在gNB物理层实时检测干扰，并结合GPU加速。

Result: 在真实环境中测试超过700万个NR UL时隙，干扰检测准确率超过91%，耗时低于650us。

Conclusion: InterfO-RAN是首个基于O-RAN的GPU加速dApp，能有效解决5G密集部署中的干扰问题。

Abstract: Ultra-dense fifth generation (5G) and beyond networks leverage spectrum
sharing and frequency reuse to enhance throughput, but face unpredictable
in-band uplink (UL) interference challenges that significantly degrade Signal
to Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases
(gNBs). This is particularly problematic at cell edges, where overlapping
regions force User Equipments (UEs) to increase transmit power, and in
directional millimeter wave systems, where beamforming sidelobes can create
unexpected interference. The resulting signal degradation disrupts protocol
operations, including scheduling and resource allocation, by distorting quality
indicators like Reference Signal Received Power (RSRP) and Received Signal
Strength Indicator (RSSI), and can compromise critical functions such as
channel state reporting and Hybrid Automatic Repeat Request (HARQ)
acknowledgments. To address this problem, this article introduces InterfO-RAN,
a real-time programmable solution that leverages a Convolutional Neural Network
(CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical
layer, detecting in-band interference with accuracy exceeding 91% in under 650
us. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics
Processing Unit (GPU), coexisting with the 5G NR physical layer processing of
NVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial
Radio Units (RUs) and smartphones, our solution was trained and tested on more
than 7 million NR UL slots collected from real-world environments,
demonstrating robust interference detection capabilities essential for
maintaining network performance in dense deployments.

</details>


### [17] [Optimal Packetization Towards Low Latency in Random Access Networks (extended version)](https://arxiv.org/abs/2507.23286)
*Zihong Li,Anshan Yuan,Xinghua Sun*

Main category: cs.NI

TL;DR: 研究探讨了Aloha网络中分组化对平均排队延迟（以秒为单位）的影响，并提出了优化的分组策略。


<details>
  <summary>Details</summary>
Motivation: 随着低延迟服务的需求增长，随机接入（RA）网络的延迟性能成为一个重要问题。现有研究普遍忽视分组化对排队延迟的影响，尤其是以秒为单位的更精确指标。

Method: 通过数学建模分析分组化与平均排队延迟的关系，探索最优分组化策略，并通过数值方法和仿真验证。

Result: 确定了最优的平均排队延迟及其对应的分组大小，分析了网络参数的影响，并扩展至RA-SDT在NTN场景的应用。

Conclusion: 分组化是优化Aloha网络排队延迟的关键因素，研究为连接和无连接方案提供了新的权衡视角。

Abstract: As the demand for low-latency services grows, ensuring the delay performance
of random access (RA) networks has become a priority. Existing studies on the
queueing delay performance of the Aloha model universally treat packets as
atomic transmission units, focusing primarily on delay measured in time slots.
However, the impact of packetization on queueing delay has been consistently
overlooked, particularly for the mean queueing delay measured in seconds, which
serves as a more precise and practically relevant performance metric than its
slot-based counterpart. Here, packetization refers to the process of
determining the number of bits assembled into a packet. To optimize queueing
delay from the perspective of packetization, this paper establishes the
mathematical relationship between packetization and mean queueing delay in
seconds for both connection-free and connection-based Aloha schemes, and
explores the optimal packetization strategy to minimize this delay. We identify
the optimal mean queueing delay and its corresponding packet size via numerical
methods, and further analyze the influence of various network parameters. We
further use simulations to investigate the similar impact of packetization on
jitter of queueing delay. We then apply our analysis to re-evaluate the complex
trade-off between the connection-free and connection-based schemes through the
new perspective of packetization. Furthermore, recognizing that an analysis of
the queueing delay performance for RA-SDT in NTN scenarios, especially from a
packetization perspective, also remains an unexplored area, we apply the
analysis to this scenario as a case study.

</details>


### [18] [FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies](https://arxiv.org/abs/2507.23342)
*Laura Acosta García,Juan Aznar Poveda,Fabian Margreiter,Antonio-Javier García Sánchez,Joan García Haro,Thomas Fahringer,José Lorente López,José-Víctor Rodríguez*

Main category: cs.NI

TL;DR: FAST-LoRa是一个新型仿真框架，通过简化计算和高效矩阵操作，显著减少了LoRaWAN网络评估的计算时间和开销，同时保持了高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有仿真框架在复现LoRaWAN网络时计算量大、耗时长，需要一种轻量级且高效的替代方案。

Method: FAST-LoRa采用解析模型替代复杂的分组级仿真，并通过高效矩阵操作实现网关接收模拟。

Result: 在多网络配置情况下，FAST-LoRa在关键指标（如PDR和EE）上准确性接近传统模拟器，计算时间减少三个数量级。

Conclusion: FAST-LoRa为稳定流量和上行场景提供了一种快速且准确的参数评估工具，显著提升了仿真效率。

Abstract: The Internet of Things (IoT) has transformed many industries, and LoRaWAN
(Long Range Wide Area Network), built on LoRa (Long Range) technology, has
become a crucial solution for enabling scalable, low-cost, and energy-efficient
communication in wide-area networks. Simulation tools are essential for
optimizing the transmission parameters and, therefore, the energy efficiency
and performance of LoRaWAN networks. While existing simulation frameworks
accurately replicate real-world scenarios by including multiple layers of
communication protocols, they often imply significant computational overhead
and simulation times. To address this issue, this paper introduces FAST-LoRa, a
novel simulation framework designed to enable fast and efficient evaluation of
LoRaWAN networks and selection of transmission parameters. FAST-LoRa
streamlines computation by relying on analytical models without complex
packet-level simulations and implementing gateway reception using efficient
matrix operations. Rather than aiming to replace discrete-event simulators,
FAST-LoRa is intended as a lightweight and accurate approximation tool for
evaluating transmission parameter strategies in scenarios with stable traffic
patterns and uplink-focused communications. In our evaluation, we compare
FAST-LoRa with a well-established simulator using multiple network
configurations with varying numbers of end devices and gateways. The results
show that FAST-LoRa achieves similar accuracy in estimating key network
metrics, even in complex scenarios with interference and multi-gateway
reception, with a Mean Absolute Error (MAE) of 0.940 $\times 10^{-2}$ for the
Packet Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while
significantly reducing computational time by up to three orders of magnitude.

</details>


### [19] [Dual-Mode Wireless Devices for Adaptive Pull and Push-Based Communication](https://arxiv.org/abs/2507.23421)
*Sara Cavallero,Fabio Saggese,Junya Shiraishi,Israel Leyva-Mayorga,Shashi Raj Pandey,Chiara Buratti,Petar Popovski*

Main category: cs.NI

TL;DR: 该论文提出了一种双模通信框架，结合了查询驱动（pull）和事件驱动（push）的传输方式，通过统一的时帧结构实现高效、自适应的数据传输。


<details>
  <summary>Details</summary>
Motivation: 动机是解决无线设备在不同网络条件下数据传递的及时性和能效问题，通过统一框架支持两种通信模式，提升系统性能。

Method: 方法包括设计双模通信框架、引入唤醒无线电机制和定制MAC协议，并进行了系统级分析，评估了三种关键性能指标。

Result: 数值结果表明，该方法在能效上优于传统方法（节省30%能量），同时可靠支持两种通信模式。

Conclusion: 结论是该双模框架在能效和通信可靠性上表现优异，适用于动态网络环境。

Abstract: This paper introduces a dual-mode communication framework for wireless
devices that integrates query-driven (pull) and event-driven (push)
transmissions within a unified time-frame structure. Devices typically respond
to information requests in pull mode, but if an anomaly is detected, they
preempt the regular response to report the critical condition. Additionally,
push-based communication is used to proactively send critical data without
waiting for a request. This adaptive approach ensures timely, context-aware,
and efficient data delivery across different network conditions. To achieve
high energy efficiency, we incorporate a wake-up radio mechanism and we design
a tailored medium access control (MAC) protocol that supports data traffic
belonging to the different communication classes. A comprehensive system-level
analysis is conducted, accounting for the wake-up control operation and
evaluating three key performance metrics: the success probability of anomaly
reports (push traffic), the success probability of query responses (pull
traffic) and the total energy consumption. Numerical results characterize the
system's behavior and highlight the inherent trade-off in success probabilities
between push- and pull-based traffic as a function of allocated communication
resources. Our analysis demonstrates that the proposed approach reduces energy
consumption by up to 30% compared to a traditional approach, while maintaining
reliable support for both communication paradigms.

</details>


### [20] [From Timestamps to Versions: Version AoI in Single- and Multi-Hop Networks](https://arxiv.org/abs/2507.23433)
*Erfan Delfani,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 论文研究了信息版本年龄（VAoI）的稳态分布，结合时序性和信息内容价值，分析了单跳和多跳网络中不同调度策略的表现，并优化了阈值调度策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注信息新鲜度（如AoI），但忽略了内容的信息价值，且多集中于平均值分析，缺乏对多跳网络中完整分布的研究。

Method: 使用VAoI作为指标，分析多种调度策略（随机静态、均匀、阈值调度）在传输限制下的表现，推导稳态分布和平均VAoI的闭式表达式，并优化阈值调度策略。

Result: 推导了VAoI的稳态分布和平均值的闭式表达式，确定了最小化VAoI的最优阈值及其对应的最优VAoI，数值验证了分析结果。

Conclusion: 研究为高效通信网络设计提供了基于VAoI的调度策略和优化方法，尤其在多跳网络中有重要价值。

Abstract: Timely and informative data dissemination in communication networks is
essential for enhancing system performance and energy efficiency, as it reduces
the transmission of outdated or redundant data. Timeliness metrics, such as Age
of Information (AoI), effectively quantify data freshness; however, these
metrics fail to account for the intrinsic informativeness of the content
itself. To address this limitation, content-based metrics have been proposed
that combine both timeliness and informativeness. Nevertheless, existing
studies have predominantly focused on evaluating average metric values, leaving
the complete distribution-particularly in multi-hop network scenarios-largely
unexplored. In this paper, we provide a comprehensive analysis of the
stationary distribution of the Version Age of Information (VAoI), a
content-based metric, under various scheduling policies, including randomized
stationary, uniform, and threshold-based policies, with transmission
constraints in single-hop and multi-hop networks. We derive closed-form
expressions for the stationary distribution and average VAoI under these
scheduling approaches. Furthermore, for threshold-based scheduling, we
analytically determine the optimal threshold value that minimizes VAoI and
derive the corresponding optimal VAoI in closed form. Numerical evaluations
verify our analytical findings, providing valuable insights into leveraging
VAoI in the design of efficient communication networks.

</details>


### [21] [Networked Physical Computing: A New Paradigm for Effective Task Completion via Hypergraph Aided Trusted Task-Resource Matching](https://arxiv.org/abs/2507.23556)
*Botao Zhu,Xianbin Wang*

Main category: cs.NI

TL;DR: 该论文提出了一种基于超图的信任任务资源匹配框架（TTR-matching），用于在复杂联网系统中实现价值驱动的任务完成。


<details>
  <summary>Details</summary>
Motivation: 由于计算资源和任务的多样化物理属性，在复杂系统中实现高效的任务与资源匹配变得极具挑战性。

Method: 通过定义任务特定的信任物理资源超图和任务超图，设计超图匹配算法，实现信任协作和设备资源匹配。

Result: 实验表明，TTR-matching框架在识别可信协作者和最大化任务完成价值方面优于其他算法。

Conclusion: 该框架为联网物理计算系统中的任务资源匹配提供了一种高效解决方案。

Abstract: Due to the diverse physical attributes of computing resources and tasks,
developing effective mechanisms to facilitate task and resource matching in
complex connected systems for value-oriented task completion has become
increasingly challenging. To address the challenge, this paper proposes a
networked physical computing system that integrates the physical attributes of
computing resources and tasks as well as task-specific trust relationships
among devices to enable value-driven task completion. Specifically, we propose
a state-of-the-art hypergraph-aided trusted task-resource matching
(TTR-matching) framework to achieve the envisioned physical computing. First, a
task-specific trusted physical resource hypergraph is defined, which integrates
task-specific trust, the physical attributes of resources, and task types. This
enables accurate modeling of device collaboration dependencies under specific
task types. Next, a task hypergraph is generated to associate the task
initiator with the physical attributes of the corresponding tasks. Based on
these two hypergraphs, a hypergraph matching algorithm is designed to
facilitate task-specific trusted collaborator selection and accurate
task-resource matching for value-maximizing task completion. Extensive
experimental results demonstrate that the proposed TTR-matching framework
outperforms comparison algorithms in identifying task-specific trustworthy
collaborators and maximizing the average value of task completion.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [Hybrid CNN-Mamba Enhancement Network for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2507.23444)
*Xiang Li,Xianfu Cheng,Xiaoming Zhang,Zhoujun Li*

Main category: cs.MM

TL;DR: HCMEN框架通过结合CNN和Mamba架构，有效处理多模态情感分析中的缺失模态问题，提出跨模态增强和对齐机制，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多模态情感分析中的缺失模态问题时，难以有效对齐和融合多模态信息，因此需要更鲁棒的解决方案。

Method: 提出HCMEN框架，包含层次化单模态建模、跨模态增强与对齐、多模态混合融合三个关键组件，结合CNN和Mamba架构的优势。

Result: 在两个标准MSA数据集上的实验表明，HCMEN在各种缺失模态场景中表现优于现有方法。

Conclusion: HCMEN通过新颖的跨模态增强和对齐机制，显著提升了多模态情感分析的鲁棒性和性能。

Abstract: Multimodal Sentiment Analysis (MSA) with missing modalities has recently
attracted increasing attention. Although existing research mainly focuses on
designing complex model architectures to handle incomplete data, it still faces
significant challenges in effectively aligning and fusing multimodal
information. In this paper, we propose a novel framework called the Hybrid
CNN-Mamba Enhancement Network (HCMEN) for robust multimodal sentiment analysis
under missing modality conditions. HCMEN is designed around three key
components: (1) hierarchical unimodal modeling, (2) cross-modal enhancement and
alignment, and (3) multimodal mix-up fusion. First, HCMEN integrates the
strengths of Convolutional Neural Network (CNN) for capturing local details and
the Mamba architecture for modeling global contextual dependencies across
different modalities. Furthermore, grounded in the principle of Mutual
Information Maximization, we introduce a cross-modal enhancement mechanism that
generates proxy modalities from mixed token-level representations and learns
fine-grained token-level correspondences between modalities. The enhanced
unimodal features are then fused and passed through the CNN-Mamba backbone,
enabling local-to-global cross-modal interaction and comprehensive multimodal
integration. Extensive experiments on two benchmark MSA datasets demonstrate
that HCMEN consistently outperforms existing state-of-the-art methods,
achieving superior performance across various missing modality scenarios. The
code will be released publicly in the near future.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Explanations for Unrealizability of Infinite-State Safety Shields](https://arxiv.org/abs/2507.23603)
*Andoni Rodriguez,Irfansha Shaik,Davide Corsi,Roy Fox,Cesar Sanchez*

Main category: cs.LO

TL;DR: 本文提出了一种通过时序公式展开的方法，生成无条件或有条件的解释，以解决安全强化学习中因规范不一致导致的屏蔽不可实现问题。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习旨在开发最优策略的同时确保安全性，但屏蔽合成可能因规范不一致而无法实现。为解决这一问题，作者提出了一种新的方法。

Method: 采用时序公式展开技术，生成简单无条件或有条件的解释，以验证屏蔽的不可实现性。

Result: 展示了该方法的不同变体及其适用性。

Conclusion: 该方法为解决安全强化学习中屏蔽不可实现问题提供了一种有效途径。

Abstract: Safe Reinforcement Learning focuses on developing optimal policies while
ensuring safety. A popular method to address such task is shielding, in which a
correct-by-construction safety component is synthesized from logical
specifications. Recently, shield synthesis has been extended to infinite-state
domains, such as continuous environments. This makes shielding more applicable
to realistic scenarios. However, often shields might be unrealizable because
the specification is inconsistent (e.g., contradictory). In order to address
this gap, we present a method to obtain simple unconditional and conditional
explanations that witness unrealizability, which goes by temporal formula
unrolling. In this paper, we show different variants of the technique and its
applicability.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [24] [Knowledge Is More Than Performance: How Knowledge Diversity Drives Human-Human and Human-AI Interaction Synergy and Reveals Pure-AI Interaction Shortfalls](https://arxiv.org/abs/2507.22889)
*Tom Sheffer,Alon Miron,Yaniv Dover,Ariel Goldstein*

Main category: cs.HC

TL;DR: 论文比较了人类与AI（LLM）在不同对话配置中的协作效果，发现人类参与的对话能提升准确性，而纯LLM对话则效果不佳，原因是知识多样性不足。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理之间是否能像人类对话一样产生协作增效，以期为AI开发提供新思路。

Method: 通过四种对话配置（LLM-LLM、LLM三组、人类三组、人类-LLM混合组）比较对话前后的答案准确性。

Result: 人类参与的对话显著提升准确性，而纯LLM对话反而降低准确性，关键在于知识多样性。

Conclusion: AI开发应注重模型间的多样性而非单独性能，以提升协作效果。

Abstract: Conversations transform individual knowledge into collective insight,
allowing groups of humans and increasingly groups of artificial intelligence
(AI) agents to collaboratively solve complex problems. Whether interactions
between AI agents can replicate the synergy observed in human discussions
remains an open question. To investigate this, we systematically compared four
conversational configurations: pairs of large language models (LLM-LLM), trios
of LLMs, trios of humans, and mixed human-LLM pairs. After agents answered
questions individually, they engaged in open-ended discussions and then
reconsidered their initial answers. Interactions involving humans consistently
led to accuracy improvements after the conversations, benefiting both stronger
and weaker participants. By contrast, purely LLM-based pairs and trios
exhibited declines in accuracy, demonstrating limited conversational synergy.
Analysis of participants' confidence and answer-switching behavior revealed
that knowledge diversity is a critical factor enabling collaborative
improvement. Crucially, the lack of gains in LLM-LLM interactions did not stem
from a fundamental limitation of the models' ability to collaborate, but from
highly similar knowledge states that left little room for productive exchange.
Our findings argue for a paradigm shift in AI development: rather than
optimizing individual models solely for standalone performance, explicitly
cultivating diversity across agents, even at the cost of slightly lower
individual accuracy, may yield AI collaborators that are more effective in
group settings with humans or other AI systems.

</details>


### [25] [Evaluating LLMs for Visualization Generation and Understanding](https://arxiv.org/abs/2507.22890)
*Saadiq Rauf Khan,Vinit Chandak,Sougata Mukherjea*

Main category: cs.HC

TL;DR: LLMs能生成简单可视化代码（如条形图、饼图），并回答相关问题，但在复杂可视化（如小提琴图）和某些问题的准确回答上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在信息可视化领域的潜力，评估其生成代码和回答问题的能力，为改进LLM和可视化系统提供参考。

Method: 通过简单提示让不同LLM生成可视化代码，并测试其回答问题（如常见可视化理解）的表现。

Result: LLM能处理简单可视化任务（如生成条形图代码和回答基础问题），但在复杂场景（如小提琴图生成和边界关系判断）中存在不足。

Conclusion: LLM在可视化领域有潜力但仍有局限，改进后可提升其与可视化系统的协作能力。

Abstract: Information Visualization has been utilized to gain insights from complex
data. In recent times, Large Language models (LLMs) have performed very well in
many tasks. In this paper, we showcase the capabilities of different popular
LLMs to generate code for visualization based on simple prompts. We also
analyze the power of LLMs to understand some common visualizations by answering
questions. Our study shows that LLMs could generate code for some simpler
visualizations such as bar and pie charts. Moreover, they could answer simple
questions about visualizations. However, LLMs also have several limitations.
For example, some of them had difficulty generating complex visualizations,
such as violin plot. LLMs also made errors in answering some questions about
visualizations, for example, identifying relationships between close boundaries
and determining lengths of shapes. We believe that our insights can be used to
improve both LLMs and Information Visualization systems.

</details>


### [26] [Breaking the mould of Social Mixed Reality -- State-of-the-Art and Glossary](https://arxiv.org/abs/2507.23454)
*Marta Bieńkiewicz,Julia Ayache,Panayiotis Charalambous,Cristina Becchio,Marco Corragio,Bertram Taetz,Francesco De Lellis,Antonio Grotta,Anna Server,Daniel Rammer,Richard Kulpa,Franck Multon,Azucena Garcia-Palacios,Jessica Sutherland,Kathleen Bryson,Stéphane Donikian,Didier Stricker,Benoît Bardy*

Main category: cs.HC

TL;DR: 文章探讨了混合现实（MR）技术在人类具身和社会运动互动方面的不足，提出通过多模态数据流和多智能体互动提升MR的社会体验。


<details>
  <summary>Details</summary>
Motivation: MR技术尚无法真实模拟人类具身和社会运动互动，限制了其社交体验的真实性和丰富性。

Method: 提出了一个综合术语表，涵盖虚拟角色、自主学习、负责任AI、设计伦理等主题，旨在从神经科学、具身和技术的角度推动MR技术发展。

Result: 提出了一种以人为中心的MR技术改进方向，强调社会互动、伦理设计和心理安全。

Conclusion: 呼吁发展更具包容性和伦理性的MR系统，以促进人类与虚拟自主代理之间的社交互动与合作。

Abstract: This article explores a critical gap in Mixed Reality (MR) technology: while
advances have been made, MR still struggles to authentically replicate human
embodiment and socio-motor interaction. For MR to enable truly meaningful
social experiences, it needs to incorporate multi-modal data streams and
multi-agent interaction capabilities. To address this challenge, we present a
comprehensive glossary covering key topics such as Virtual Characters and
Autonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges
of Social MR within Neuroscience, Embodiment, and Technology. Our aim is to
drive the transformative evolution of MR technologies that prioritize
human-centric innovation, fostering richer digital connections. We advocate for
MR systems that enhance social interaction and collaboration between humans and
virtual autonomous agents, ensuring inclusivity, ethical design and
psychological safety in the process.

</details>


### [27] [Real-time energy monitoring infrastructure for residential collective self-consumption operations using Linky meter](https://arxiv.org/abs/2507.22891)
*Jérôme Ferrari,Benoit Delinchant,Frédéric Wurtz,Olga Rouchouze*

Main category: cs.HC

TL;DR: 简介集体自消费能源监测的现状问题，提出基于Linky电表的开源实时监控基础设施xKy，并通过九户参与的案例展示其应用效果。


<details>
  <summary>Details</summary>
Motivation: 能源转型与价格上涨推动法国集体自消费行为增加，但现有Linky电表的“day+1”数据无法提供实时反馈，阻碍参与者调整消费行为。

Method: 设计开源基础设施xKy，结合Linky电表数据，搭建网关与实时监控网站，应用于九户集体自消费项目。

Result: 实现实时能源流监控，帮助参与者提高自消费率。

Conclusion: xKy基础设施有效解决实时监控需求，支持集体自消费的可持续实践。

Abstract: As part of the energy transition and the rise in energy prices, the number of
collective self-consumption operations in France is steadily increasing.
However, energy flow monitoring currently relies on historical ''day+1'' data
provided by Linky meters, which does not offer real time feedback to help
participants adapt their energy consumption behaviors. This article introduces
a new open-source infrastructure for real-time monitoring based on Linky meter
data, enabling participants to make informed decisions and take timely actions.
It includes a description of the xKy device, applied to a collective
self-consumption operation involving nine participants, supported by the Energy
Transition Observatory (OTE). The project encompasses the implementation of
gateways in participants' homes and the development and operation of real-time
monitoring website, aimed at increasing participants' self-consumption rate.

</details>


### [28] [Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation](https://arxiv.org/abs/2507.22892)
*Ismail Hossain,Mridul Banik*

Main category: cs.HC

TL;DR: 一种结合EEG-BCI和LLM的混合框架，用于实时适应语言康复训练，适用于中风后失语症等神经疾病患者。


<details>
  <summary>Details</summary>
Motivation: 传统辅助沟通系统无法实时适应用户认知和语言需求，尤其在神经疾病如中风后失语症中表现不足。

Method: 提出一种混合框架，利用实时EEG信号驱动基于LLM的语言康复助手，实现心理指令导航、动态个性化练习和实时调整任务难度。

Result: 该系统能够帮助严重语言或运动障碍患者进行语言学习，并根据认知努力动态调整训练内容。

Conclusion: EEG-BCI与LLM的结合为神经疾病患者的语言康复提供了创新解决方案。

Abstract: Conventional augmentative and alternative communication (AAC) systems and
language-learning platforms often fail to adapt in real time to the user's
cognitive and linguistic needs, especially in neurological conditions such as
post-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in
noninvasive electroencephalography (EEG)--based brain-computer interfaces
(BCIs) and transformer--based large language models (LLMs) offer complementary
strengths: BCIs capture users' neural intent with low fatigue, while LLMs
generate contextually tailored language content. We propose and evaluate a
novel hybrid framework that leverages real-time EEG signals to drive an
LLM-powered language rehabilitation assistant. This system aims to: (1) enable
users with severe speech or motor impairments to navigate language-learning
modules via mental commands; (2) dynamically personalize vocabulary,
sentence-construction exercises, and corrective feedback; and (3) monitor
neural markers of cognitive effort to adjust task difficulty on the fly.

</details>


### [29] [Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure](https://arxiv.org/abs/2507.22893)
*Giuseppe Riva*

Main category: cs.HC

TL;DR: 本文提出‘认知基础设施研究’（CIS）作为新领域，探讨AI作为‘认知基础设施’如何无形地重塑人类认知，并影响公共推理和社会认知。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互研究忽视了AI如何在潜意识层面重塑人类认知，CIS旨在填补这一空白，重新定义AI为‘认知基础设施’。

Method: 提出‘基础设施崩溃方法论’，通过实验方法揭示AI预处理对人类认知的依赖。

Result: CIS揭示了AI如何通过语义基础设施和自适应不可见性影响个体、集体和社会认知。

Conclusion: CIS为跨学科研究提供了新框架，填补了认知科学、数字社会学和计算方法的缺口。

Abstract: Contemporary human-AI interaction research overlooks how AI systems
fundamentally reshape human cognition pre-consciously, a critical blind spot
for understanding distributed cognition. This paper introduces "Cognitive
Infrastructure Studies" (CIS) as a new interdisciplinary domain to
reconceptualize AI as "cognitive infrastructures": foundational, often
invisible systems conditioning what is knowable and actionable in digital
societies. These semantic infrastructures transport meaning, operate through
anticipatory personalization, and exhibit adaptive invisibility, making their
influence difficult to detect. Critically, they automate "relevance judgment,"
shifting the "locus of epistemic agency" to non-human systems. Through
narrative scenarios spanning individual (cognitive dependency), collective
(democratic deliberation), and societal (governance) scales, we describe how
cognitive infrastructures reshape human cognition, public reasoning, and social
epistemologies. CIS aims to address how AI preprocessing reshapes distributed
cognition across individual, collective, and cultural scales, requiring
unprecedented integration of diverse disciplinary methods. The framework also
addresses critical gaps across disciplines: cognitive science lacks
population-scale preprocessing analysis capabilities, digital sociology cannot
access individual cognitive mechanisms, and computational approaches miss
cultural transmission dynamics. To achieve this goal CIS also provides
methodological innovations for studying invisible algorithmic influence:
"infrastructure breakdown methodologies", experimental approaches that reveal
cognitive dependencies by systematically withdrawing AI preprocessing after
periods of habituation.

</details>


### [30] [When no one shows up (at first): Navigating the uncertainties of participatory workshops in interdisciplinary research](https://arxiv.org/abs/2507.22894)
*Monique Munarini*

Main category: cs.HC

TL;DR: 本文反思了设计和主持协作设计和参与式研讨会的挑战，为早期职业研究人员提供了实用策略。


<details>
  <summary>Details</summary>
Motivation: 探讨协作设计和参与式研讨会中未言明的挑战，帮助早期职业研究人员应对这些方法。

Method: 基于个人经验，描述如何从概念化到招募参与者并主持研讨会的完整过程，特别是在缺乏支持的情况下。

Result: 尽管初期遇到低参与率等问题，研讨会促成了多样化群体的深入讨论，并成功将一位参与者转变为共同主持人。

Conclusion: 将失败视为学习机会，为跨学科研究人员提供了实用策略，并强调了生活经验在研究和实践中的重要性。

Abstract: This reflective paper explores often-unspoken challenges of designing and
facilitating co-design and participatory workshops, offering practical
strategies for early career researchers (ECRs) navigating these methods.
Drawing from personal experience conducting a series of workshops titled: How
to Think About Equity in the AI Ecosystem. It follows the full arc of the
workshop experience, from conceptualization and activity planning to
participant recruitment and facilitation, offering a grounded account of what
happens when participation does not go as expected. The paper examines the
methodological challenges of engaging non-expert participants, particularly
when operating without institutional support, financial incentives, or
integration into larger events. Despite initial difficulties such as low
attendance, the workshop fostered rich discussions among a demographically
diverse group and ultimately led to one participant volunteering to
co-facilitate a subsequent session. This transition from participant to
co-facilitator exemplifies the redistribution of epistemic authority,
positioning lived experience as central to research and engagement practices.
By reframing perceived failure as a productive site of learning, the paper
offers practical strategies for ECRs working across disciplines who often
navigate unfamiliar methodological terrains, contributing to broader
conversations on the realities of doing interdisciplinary, participatory work
in practice.

</details>


### [31] [Brain motor intention Extraction Amplifier: Non-invasive brain-muscle interface](https://arxiv.org/abs/2507.22895)
*Ye Sun,Bowei Zhao,Dezhong Yao,Rui Zhang,Bohan Zhang,Xiaoyuan Li,Jing Wang,Mingxuan Qu,Gang Liu*

Main category: cs.HC

TL;DR: 提出了一种基于脑肌肉接口（BMuI）的新型运动意图提取框架，通过模拟神经通路增强信号，结合EMG提高识别准确率，实验验证了其可行性与高效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有运动想象BCI因信号与行为意图不匹配导致的伪标签问题，提升解码精度和系统鲁棒性。

Method: 提出BMuI框架模拟神经通路，利用EMG作为高保真中继介质，通过离线和在线实验验证。

Result: 离线实验预测精度达到0.8314，在线实验中所有参与者成功控制Unity虚拟手臂。

Conclusion: BMuI方法有效解决了意图识别不精确的问题，为BCI系统提供了更可靠的技术支持。

Abstract: Brain-computer interfaces (BCIs) enable real-time interaction between the
brain and external devices by decoding neural signals. However, existing
motor-based BCI paradigms, like motor imagery BCI, face challenges with
imprecise labeling in real-world use. This mismatch between EEG signals and
true behavioral intentions leads to pseudo-labels, undermining decoding
accuracy and system robustness. To overcome this bottleneck, this paper first
proposes a novel motor intention extraction framework based on a non-invasive
brain-muscle interface (BMuI)($\text{BCI} =
\frac{\text{Brain}}{\text{Computer}} \text{ Interface} =
\frac{\text{Brain}}{\not\text{Muscle}}\! \text{ (BMuI)} \times
\!\frac{\not\text{Muscle}}{\text{Computer}}\! \text{ Interface}$). This method
simulates the neural pathway from the brain to the muscles in order to capture
and enhance the weak motor intention signals originating in the brain. It then
uses EMG as a high-fidelity relay medium to achieve more accurate intention
recognition and transmission. To systematically validate the feasibility and
effectiveness of this approach, we conducted both offline experiments (to
repeatedly verify feasibility) and online experiments (to construct a real-time
interactive system and evaluate its performance). The results show that BMuI is
feasible, achieving a prediction accuracy of 0.8314; in the online experiment,
all participants are able to successfully control the Unity virtual arm.

</details>


### [32] [iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement](https://arxiv.org/abs/2507.22896)
*Kohou Wang,ZhaoXiang Liu,Lin Bai,Kun Fan,Xiang Liu,Huan Hu,Kai Wang,Shiguo Lian*

Main category: cs.HC

TL;DR: 论文提出了一种基于多模态大型语言模型（MLLM）的交互式学习机器人系统，能够通过与普通用户的自然对话学习，并通过问题链和双模态检索模块避免重复错误。


<details>
  <summary>Details</summary>
Motivation: 机器人部署后可能遇到从未见过的新场景，因此需要提升其适应性。现有基于MLLM的机器人系统缺乏交互学习能力。

Method: 利用MLLM驱动交互式学习系统，结合问题链澄清用户意图，通过双模态检索模块利用交互事件避免重复错误。

Result: 实验证明该方法在定量和定性上均有效，提升了机器人的适应性和性能。

Conclusion: 该系统为机器人领域引入了交互式学习的新方法，有望在多样环境中实现更优表现。

Abstract: It is crucial that robots' performance can be improved after deployment, as
they are inherently likely to encounter novel scenarios never seen before. This
paper presents an innovative solution: an interactive learning-based robot
system powered by a Multi-modal Large Language Model(MLLM). A key feature of
our system is its ability to learn from natural dialogues with non-expert
users. We also propose chain of question to clarify the exact intent of the
question before providing an answer and dual-modality retrieval modules to
leverage these interaction events to avoid repeating same mistakes, ensuring a
seamless user experience before model updates, which is in contrast to current
mainstream MLLM-based robotic systems. Our system marks a novel approach in
robotics by integrating interactive learning, paving the way for superior
adaptability and performance in diverse environments. We demonstrate the
effectiveness and improvement of our method through experiments, both
quantitively and qualitatively.

</details>


### [33] [RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems](https://arxiv.org/abs/2507.22897)
*Luyu Chen,Quanyu Dai,Zeyu Zhang,Xueyang Feng,Mingyu Zhang,Pengcheng Tang,Xu Chen,Yue Zhu,Zhenhua Dong*

Main category: cs.HC

TL;DR: 提出了RecUserSim，一种基于大型语言模型的用户模拟器，旨在通过增强模拟真实性和多样性，并提供明确的评分机制，改善对话推荐系统的评估。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的用户模拟器难以真实模拟多样化用户场景，且缺乏明确的评分机制。

Method: RecUserSim包含多个关键模块：用户画像模块、记忆模块、核心行动模块（基于有限理性理论）和优化模块。这些模块共同作用，生成多样化且可控的模拟输出。

Result: 实验表明，RecUserSim能生成高质量对话，且评分在不同大型语言模型间具有高一致性，适合对话推荐系统评估。

Conclusion: RecUserSim通过增强真实性和多样性，提供了更有效的评估工具，适用于对话推荐系统研究。

Abstract: Conversational recommender systems (CRS) enhance user experience through
multi-turn interactions, yet evaluating CRS remains challenging. User
simulators can provide comprehensive evaluations through interactions with CRS,
but building realistic and diverse simulators is difficult. While recent work
leverages large language models (LLMs) to simulate user interactions, they
still fall short in emulating individual real users across diverse scenarios
and lack explicit rating mechanisms for quantitative evaluation. To address
these gaps, we propose RecUserSim, an LLM agent-based user simulator with
enhanced simulation realism and diversity while providing explicit scores.
RecUserSim features several key modules: a profile module for defining
realistic and diverse user personas, a memory module for tracking interaction
history and discovering unknown preferences, and a core action module inspired
by Bounded Rationality theory that enables nuanced decision-making while
generating more fine-grained actions and personalized responses. To further
enhance output control, a refinement module is designed to fine-tune final
responses. Experiments demonstrate that RecUserSim generates diverse,
controllable outputs and produces realistic, high-quality dialogues, even with
smaller base LLMs. The ratings generated by RecUserSim show high consistency
across different base LLMs, highlighting its effectiveness for CRS evaluation.

</details>


### [34] [Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment](https://arxiv.org/abs/2507.22898)
*Julian Acosta,Scott Adams,Julius Kernbach,Romain Hardy,Sung Eun Kim,Luyang Luo,Xiaoman Zhang,Shreya Johri,Mohammed Baharoon,Pranav Rajpurkar*

Main category: cs.HC

TL;DR: 开发了一种语音驱动的AI系统，辅助任何人进行专业级的中风评估，结果显示其识别中风标志及大血管闭塞中风的准确性较高，但需医生监督。


<details>
  <summary>Details</summary>
Motivation: 解决急救中中风识别不一致和准确性低的问题（敏感性仅58%），以减少治疗延误。

Method: 三名非医疗志愿者使用AI系统评估10例模拟中风患者，测量诊断准确性、完成时间、用户信心及专家评审AI报告。

Result: AI系统识别84%的中风标志及75%的大血管闭塞中风，评估仅需6分钟，用户信心高（4.5/5），但误报2例非中风病例。专家评审后正确诊断100%病例，但仅40%可初步治疗。

Conclusion: 现有系统需人工监督，但技术进步有望实现高准确性评估，未来可普及专业急救能力。

Abstract: We developed a voice-driven artificial intelligence (AI) system that guides
anyone - from paramedics to family members - through expert-level stroke
evaluations using natural conversation, while also enabling smartphone video
capture of key examination components for documentation and potential expert
review. This addresses a critical gap in emergency care: current stroke
recognition by first responders is inconsistent and often inaccurate, with
sensitivity for stroke detection as low as 58%, causing life-threatening delays
in treatment. Three non-medical volunteers used our AI system to assess ten
simulated stroke patients, including cases with likely large vessel occlusion
(LVO) strokes and stroke-like conditions, while we measured diagnostic
accuracy, completion times, user confidence, and expert physician review of the
AI-generated reports. The AI system correctly identified 84% of individual
stroke signs and detected 75% of likely LVOs, completing evaluations in just
over 6 minutes. Users reported high confidence (median 4.5/5) and ease of use
(mean 4.67/5). The system successfully identified 86% of actual strokes but
also incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert
physician reviewed the AI reports with videos, they identified the correct
diagnosis in 100% of cases, but felt confident enough to make preliminary
treatment decisions in only 40% of cases due to observed AI errors including
incorrect scoring and false information. While the current system's limitations
necessitate human oversight, ongoing rapid advancements in speech-to-speech AI
models suggest that future versions are poised to enable highly accurate
assessments. Achieving human-level voice interaction could transform emergency
medical care, putting expert-informed assessment capabilities in everyone's
hands.

</details>


### [35] [A visual analytics tool for taxonomy-based trajectory data exploration](https://arxiv.org/abs/2507.22899)
*Ivan A. Hanono Cozzetti,Ahmad Abdou*

Main category: cs.HC

TL;DR: 提出一种结合数据可视化和统计计算的多层次时空数据分析工具，通过机器学习分类移动对象，有效分析复杂运动模式。


<details>
  <summary>Details</summary>
Motivation: 解决时空数据分析中复杂和异构的运动模式带来的挑战。

Method: 使用机器学习模型将移动对象分类，结合数据可视化和统计计算方法，通过两个案例验证方法的有效性。

Result: 成功分类北极狐轨迹和热带气旋数据，揭示了不同行为模式下的统计特征。

Conclusion: 该方法详细解释复杂时空数据，为多领域应用提供了理论和实践蓝图。

Abstract: The analysis of spatio-temporal data presents significant challenges due to
the complexity and heterogeneity of movement patterns. This project proposes a
data analytics tool that combines data visualization and statistical
computation to facilitate spatio-temporal data analysis through a multi-level
approach. The tool categorizes moving objects into distinct taxonomies using
Machine Learning models, adding meaningful structure to the analysis. Two case
studies demonstrate the methodology's effectiveness. The first analyzed Arctic
fox trajectories, successfully identifying and labeling foxes with Geometric or
Kinematic-based behaviors, further categorized into Curvature and Acceleration
groups. Statistical indicators revealed that foxes with Acceleration-based
behavior showed constant, steady acceleration, while those with Curvature-based
behavior exhibited acceleration peaks and sudden deceleration. The second case
study examined tropical cyclone data, labeling trajectories with Speed,
Curvature, and hybrid Geometric-based behaviors through unique statistical
variables. Analysis of hybrid Geometric behavior (Curvature and Indentation
combined) identified specific angles with the highest impact on hurricane shape
and geometry. The proposed method and tool demonstrate that spatio-temporal
data, despite inherent complexity, can be analyzed and explained in detail,
providing a theoretical and practical blueprint applicable to multiple domains.

</details>


### [36] [Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants](https://arxiv.org/abs/2507.22900)
*Sergio Rojas-Galeano*

Main category: cs.HC

TL;DR: 研究探讨AI代码助手如何影响新手程序员在编程考试中的体验，发现AI工具有助于代码理解和提升信心，但也可能导致过度依赖和概念理解不足。


<details>
  <summary>Details</summary>
Motivation: 探究AI代码助手对新手程序员学习体验的影响，特别是在编程考试中的表现和感知。

Method: 通过两部分考试（有AI支持和无AI支持）以及20名学生的Likert量表和开放式回答收集数据。

Result: AI工具在初期开发中帮助理解代码和增强信心，但学生在无AI任务中表现出知识迁移困难，存在过度依赖和概念理解不足的问题。

Conclusion: 需设计教学策略，既能有效整合AI工具，又能强化基础编程技能。

Abstract: This exploratory study examines how AI code assistants shape novice
programmers' experiences during a two-part exam in an introductory programming
course. In the first part, students completed a programming task with access to
AI support; in the second, they extended their solutions without AI. We
collected Likert-scale and open-ended responses from 20 students to evaluate
their perceptions and challenges. Findings suggest that AI tools were perceived
as helpful for understanding code and increasing confidence, particularly
during initial development. However, students reported difficulties
transferring knowledge to unaided tasks, revealing possible overreliance and
gaps in conceptual understanding. These insights highlight the need for
pedagogical strategies that integrate AI meaningfully while reinforcing
foundational programming skills.

</details>


### [37] [Accelerated and Optimized Search of Imperceptible Color Vibration for Embedding Information into LCD images](https://arxiv.org/abs/2507.22901)
*Shingo Hattori,Takefumi Hiraki*

Main category: cs.HC

TL;DR: 提出了一种加速和优化在LCD图像中嵌入不可见颜色振动信息的方法，通过并行化搜索过程提升效率，并验证了其应用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因串行搜索颜色对而导致的高耗时问题，同时保持显示内容的美观性。

Method: 使用并行化搜索过程，通过数组表示移动量和条件提取操作，加速颜色对的搜索。

Result: 实现了快速颜色对搜索，并验证了在九种颜色图像中嵌入信息的可行性。

Conclusion: 该方法显著提升了搜索效率，并展示了颜色振动技术在信息嵌入中的实用价值。

Abstract: Large, high-resolution displays are installed throughout the city as public
displays. By superimposing invisible information on the images of these
displays, large numbers of devices with cameras and sensors can communicate
with the displays without prior pairing. Several applications have been
proposed, such as operating robots or communicating information to users by
displaying 2D codes on images. However, the display of 2D codes has the problem
of compromising the appearance of displayed content.
  Abe et al. proposed a method of communicating with devices by superimposing
invisible information using color vibration on images displayed on
off-the-shelf liquid-crystal displays (LCD). Using this method, we can embed
the information for devices in images without interfering with the displayed
content. Abe et al. uses a simple serial loop operation to search for color
pairs comprising a color vibration, which requires a very long processing time
due to the huge search space.
  In this paper, we propose an accelerated and optimized search method for
color pairs that constitute the imperceptible color vibration for embedding
information on LCD images. To achieve fast color pair search, we parallelized
the search process, which is previously done individually, by using arrays
representing the amount of movement and an operation to extract elements from
the array that satisfy the conditions. In addition, we investigate the amount
of information that can be superimposed on nine color images using the
imperceptible color vibration and clarify the applicability of embedding
information into images using the color vibration.

</details>


### [38] [Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting](https://arxiv.org/abs/2507.22902)
*Hashim Hayat,Maksim Kudrautsau,Evgeniy Makarov,Vlad Melnichenko,Tim Tsykunou,Piotr Varaksin,Matt Pavelle,Adam Z. Oskowitz*

Main category: cs.HC

TL;DR: 评估多Agent LLM AI系统作为AI医生在虚拟急诊中的表现，结果显示其诊断和治疗计划与人类医生高度一致。


<details>
  <summary>Details</summary>
Motivation: 全球医疗人员短缺和临床时间被行政负担占据的问题，AI可能成为解决方案。

Method: 回顾性比较多Agent AI系统与认证临床医生在500次远程急诊中的表现，通过盲审和专家评估。

Result: AI与医生在81%病例中诊断一致，治疗计划99.2%一致，无临床幻觉，部分AI表现优于人类。

Conclusion: AI在多Agent系统下可与人类医生比肩，有望缓解医疗人力资源短缺。

Abstract: Background: Globally we face a projected shortage of 11 million healthcare
practitioners by 2030, and administrative burden consumes 50% of clinical time.
Artificial intelligence (AI) has the potential to help alleviate these
problems. However, no end-to-end autonomous large language model (LLM)-based AI
system has been rigorously evaluated in real-world clinical practice. In this
study, we evaluated whether a multi-agent LLM-based AI framework can function
autonomously as an AI doctor in a virtual urgent care setting. Methods: We
retrospectively compared the performance of the multi-agent AI system Doctronic
and board-certified clinicians across 500 consecutive urgent-care telehealth
encounters. The primary end points: diagnostic concordance, treatment plan
consistency, and safety metrics, were assessed by blinded LLM-based
adjudication and expert human review. Results: The top diagnosis of Doctronic
and clinician matched in 81% of cases, and the treatment plan aligned in 99.2%
of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not
supported by clinical findings). In an expert review of discordant cases, AI
performance was superior in 36.1%, and human performance was superior in 9.3%;
the diagnoses were equivalent in the remaining cases. Conclusions: In this
first large-scale validation of an autonomous AI doctor, we demonstrated strong
diagnostic and treatment plan concordance with human clinicians, with AI
performance matching and in some cases exceeding that of practicing clinicians.
These findings indicate that multi-agent AI systems achieve comparable clinical
decision-making to human providers and offer a potential solution to healthcare
workforce shortages.

</details>


### [39] [A blessing or a burden? Exploring worker perspectives of using a social robot in a church](https://arxiv.org/abs/2507.22903)
*Andrew Blair,Peggy Gregory,Mary Ellen Foster*

Main category: cs.HC

TL;DR: 论文探讨了社交机器人在以社会效益为主要动机的组织（如教堂）中的应用，发现参与者对机器人使用反应不一，强调需要考虑机器人角色中的社会价值。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索机器人在非盈利组织（如教堂）中的应用潜力，关注社会效益而非经济利益。

Method: 与教堂合作，访谈15名利益相关者，采用反思性主题分析法分析数据。

Result: 参与者对机器人使用反应不一，强调教堂的共情责任和潜在后果，但认为信息提供和减轻琐碎任务是潜在用途。

Conclusion: 机器人引入需考虑社会价值，而不仅是经济利益，以确定其在组织中的合适角色。

Abstract: Recent technological advances have allowed robots to assist in the service
sector, and consequently accelerate job and sector transformation. Less
attention has been paid to the use of robots in real-world organisations where
social benefits, as opposed to profits, are the primary motivator. To explore
these opportunities, we have partnered with a working church and visitor
attraction. We conducted interviews with 15 participants from a range of
stakeholder groups within the church to understand worker perspectives of
introducing a social robot to the church and analysed the results using
reflexive thematic analysis. Findings indicate mixed responses to the use of a
robot, with participants highlighting the empathetic responsibility the church
has towards people and the potential for unintended consequences. However,
information provision and alleviation of menial or mundane tasks were
identified as potential use cases. This highlights the need to consider not
only the financial aspects of robot introduction, but also how social and
intangible values shape what roles a robot should take on within an
organisation.

</details>


### [40] [SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches](https://arxiv.org/abs/2507.22904)
*Ehsan Latif,Zirak Khan,Xiaoming Zhai*

Main category: cs.HC

TL;DR: SketchMind是一个基于认知科学的AI框架，用于评估和改进学生绘制的科学草图，通过多智能体协作实现个性化与透明的评估。


<details>
  <summary>Details</summary>
Motivation: 现有的草图评估方法缺乏可解释性、教学对齐性和跨认知层次的适应性，难以满足教育需求。

Method: SketchMind采用模块化智能体，包括评分解析、草图感知、认知对齐和迭代反馈，并结合Bloom认知层次进行评估。

Result: 在3,575个学生草图的测试中，SketchMind的平均准确率达到77.1%，显著优于基线模型，且人工评估反馈得分更高。

Conclusion: SketchMind展示了AI在教育中支持概念成长的潜力，代码与数据集将公开以促进未来研究。

Abstract: Scientific sketches (e.g., models) offer a powerful lens into students'
conceptual understanding, yet AI-powered automated assessment of such
free-form, visually diverse artifacts remains a critical challenge. Existing
solutions often treat sketch evaluation as either an image classification task
or monolithic vision-language models, which lack interpretability, pedagogical
alignment, and adaptability across cognitive levels. To address these
limitations, we present SketchMind, a cognitively grounded, multi-agent
framework for evaluating and improving student-drawn scientific sketches.
SketchMind comprises modular agents responsible for rubric parsing, sketch
perception, cognitive alignment, and iterative feedback with sketch
modification, enabling personalized and transparent evaluation. We evaluate
SketchMind on a curated dataset of 3,575 student-generated sketches across six
science assessment items with different highest order of Bloom's level that
require students to draw models to explain phenomena. Compared to baseline
GPT-4o performance without SRG (average accuracy: 55.6%), and with SRG
integration achieves 77.1% average accuracy (+21.4% average absolute gain). We
also demonstrate that multi-agent orchestration with SRG enhances SketchMind
performance, for example, GPT-4.1 gains an average 8.9% increase in sketch
prediction accuracy, outperforming single-agent pipelines across all items.
Human evaluators rated the feedback and co-created sketches generated by
\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5,
significantly higher than those of baseline models (e.g., 2.3 for GPT-4o).
Experts noted the system's potential to meaningfully support conceptual growth
through guided revision. Our code and (pending approval) dataset will be
released to support reproducibility and future research in AI-driven education.

</details>


### [41] [Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction](https://arxiv.org/abs/2507.22905)
*Qiaoqiao Ren,Tony Belpaeme*

Main category: cs.HC

TL;DR: 研究探讨了GPT-3.5、GPT-4和GPT-4o等LLMs是否能生成文化适应性触觉行为以传达情感，发现文化匹配和交互角色影响情感解码和适当性评价。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在机器人系统中生成文化和社交适宜触觉行为的潜力，填补该领域的研究空白。

Method: 生成12种情感的触觉描述，在三种文化背景下（中国、比利时、未指定）由90名参与者评估解码和适当性。

Result: 在匹配文化下，六种情感解码成功；人类到机器人的触觉行为评价更高；文化不匹配降低了解码准确性和适当性。

Conclusion: LLMs生成的触觉行为在文化匹配和交互角色上表现不一，需进一步优化以适应多样化情感和文化需求。

Abstract: As large language models (LLMs) become increasingly integrated into robotic
systems, their potential to generate socially and culturally appropriate
affective touch remains largely unexplored. This study investigates whether
LLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive
tactile behaviours to convey emotions in human-robot interaction. We produced
text based touch descriptions for 12 distinct emotions across three cultural
contexts (Chinese, Belgian, and unspecified), and examined their
interpretability in both robot-to-human and human-to-robot scenarios. A total
of 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified)
evaluated these LLM-generated tactile behaviours for emotional decoding and
perceived appropriateness. Results reveal that: (1) under matched cultural
conditions, participants successfully decoded six out of twelve emotions-mainly
socially oriented emotions such as love and Ekman emotions such as anger,
however, self-focused emotions like pride and embarrassment were more difficult
to interpret; (2) tactile behaviours were perceived as more appropriate when
directed from human to robot than from robot to human, revealing an asymmetry
in social expectations based on interaction roles; (3) behaviours interpreted
as aggressive (e.g., anger), overly intimate (e.g., love), or emotionally
ambiguous (i.e., not clearly decodable) were significantly more likely to be
rated as inappropriate; and (4) cultural mismatches reduced decoding accuracy
and increased the likelihood of behaviours being judged as inappropriate.

</details>


### [42] [Automated Label Placement on Maps via Large Language Models](https://arxiv.org/abs/2507.22952)
*Harry Shomer,Jiejun Xu*

Main category: cs.HC

TL;DR: 论文提出了一种基于大语言模型（LLMs）的自动标签放置新方法，通过数据编辑任务和上下文感知的空间标注，解决了标签放置的自动化难题。


<details>
  <summary>Details</summary>
Motivation: 标签放置是地图设计的关键，但现有自动化系统难以整合制图惯例和适应上下文。本文提出利用LLMs的上下文理解能力来改进这一问题。

Method: 通过检索增强生成（RAG）获取标签指南，整合到提示中，并利用LLMs生成标签坐标。使用MAPLE数据集评估不同LLM的性能。

Result: 实验表明，LLMs通过结构化提示和领域特定检索，能够生成符合专家制图标准的标签放置方案。

Conclusion: 该方法为AI辅助地图完成提供了可扩展框架，展示了基础模型在结构化数据编辑任务中的潜力。

Abstract: Label placement is a critical aspect of map design, serving as a form of
spatial annotation that directly impacts clarity and interpretability. Despite
its importance, label placement remains largely manual and difficult to scale,
as existing automated systems struggle to integrate cartographic conventions,
adapt to context, or interpret labeling instructions. In this work, we
introduce a new paradigm for automatic label placement (ALP) that formulates
the task as a data editing problem and leverages large language models (LLMs)
for context-aware spatial annotation. To support this direction, we curate
MAPLE, the first known benchmarking dataset for evaluating ALP on real-world
maps, encompassing diverse landmark types and label placement annotations from
open-source data. Our method retrieves labeling guidelines relevant to each
landmark type leveraging retrieval-augmented generation (RAG), integrates them
into prompts, and employs instruction-tuned LLMs to generate ideal label
coordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall
performance and generalization across different types of landmarks. This
includes both zero-shot and instruction-tuned performance. Our results
demonstrate that LLMs, when guided by structured prompts and domain-specific
retrieval, can learn to perform accurate spatial edits, aligning the generated
outputs with expert cartographic standards. Overall, our work presents a
scalable framework for AI-assisted map finishing and demonstrates the potential
of foundation models in structured data editing tasks. The code and data can be
found at https://github.com/HarryShomer/MAPLE.

</details>


### [43] [ChatVis: Large Language Model Agent for Generating Scientific Visualizations](https://arxiv.org/abs/2507.23096)
*Tom Peterka,Tanwi Mallick,Orcun Yildiz,David Lenz,Cory Quammen,Berk Geveci*

Main category: cs.HC

TL;DR: ChatVis是一个LLM助手，通过简化提示链、检索增强的提示生成和错误检查，帮助LLM生成ParaView科学可视化任务代码，无需微调LLM。测试显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在处理高度专业化的编程任务（如科学可视化）时仍存在困难，因此需要一种无需微调的方法来提升其能力。

Method: ChatVis采用链式思维提示简化、基于向量数据库的检索增强提示生成，以及迭代反馈错误检查，确保代码的正确性。

Result: 与未辅助的LLM相比，ChatVis在ParaView可视化任务上的所有指标均有显著改进。

Conclusion: ChatVis证明了通过辅助工具可以显著提升LLM在专业任务中的表现，而无需额外训练。

Abstract: Large language models (LLMs) are rapidly increasing in capability, but they
still struggle with highly specialized programming tasks such as scientific
visualization. We present an LLM assistant, ChatVis, that aids the LLM to
generate Python code for ParaView scientific visualization tasks, without the
need for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought
prompt simplification, retrieval-augmented prompt generation using a vector
database of documentation and code examples, and error checking with iterative
prompt feedback to correct errors until a visualization is produced. An
integral part of our approach is a benchmark suite of canonical visualization
tasks, ParaView regression tests, and scientific use cases that includes
comprehensive evaluation metrics. We evaluate our visualization assistant by
comparing results with a variety of top-performing unassisted LLMs. We find
that all the metrics are significantly improved with ChatVis.

</details>


### [44] [Accessibility Scout: Personalized Accessibility Scans of Built Environments](https://arxiv.org/abs/2507.23190)
*William Huang,Xia Su,Jon E. Froehlich,Yang Zhang*

Main category: cs.HC

TL;DR: 论文提出了一种基于大语言模型的系统Accessibility Scout，用于通过照片评估建筑环境的无障碍性，结合用户个性化需求与自动化技术。


<details>
  <summary>Details</summary>
Motivation: 传统手动评估无障碍性的方法耗时且难以扩展，而自动化方法通常忽略个体需求。通过结合大语言模型，旨在实现个性化与可扩展性的平衡。

Method: 开发了Accessibility Scout系统，利用照片扫描建筑环境，并通过人机协作逐步适应用户的个性化需求。

Result: 技术评估和用户研究表明，系统能生成超越传统无障碍标准（如ADA）的个性化评估。

Conclusion: 该研究为构建更具扩展性和个性化的物理世界无障碍评估提供了新方向，并探讨了未来改进的步骤。

Abstract: Assessing the accessibility of unfamiliar built environments is critical for
people with disabilities. However, manual assessments, performed by users or
their personal health professionals, are laborious and unscalable, while
automatic machine learning methods often neglect an individual user's unique
needs. Recent advances in Large Language Models (LLMs) enable novel approaches
to this problem, balancing personalization with scalability to enable more
adaptive and context-aware assessments of accessibility. We present
Accessibility Scout, an LLM-based accessibility scanning system that identifies
accessibility concerns from photos of built environments. With use,
Accessibility Scout becomes an increasingly capable "accessibility scout",
tailoring accessibility scans to an individual's mobility level, preferences,
and specific environmental interests through collaborative Human-AI
assessments. We present findings from three studies: a formative study with six
participants to inform the design of Accessibility Scout, a technical
evaluation of 500 images of built environments, and a user study with 10
participants of varying mobility. Results from our technical evaluation and
user study show that Accessibility Scout can generate personalized
accessibility scans that extend beyond traditional ADA considerations. Finally,
we conclude with a discussion on the implications of our work and future steps
for building more scalable and personalized accessibility assessments of the
physical world.

</details>


### [45] [Silent Impact: Tracking Tennis Shots from the Passive Arm](https://arxiv.org/abs/2507.23215)
*Junyong Park,Saelyne Yang,Sungho Jo*

Main category: cs.HC

TL;DR: 提出了名为Silent Impact的系统，通过传感器放在非主用手臂上分析网球击球动作，达到高分类准确率（88.2%）和检测F1分数（86.0%），并验证了其舒适性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有网球技术产品通常需在主用手臂或球拍上安装传感器，导致运动受限和不适，非主用手臂作为替代方案未被充分探索。

Method: 使用惯性测量单元传感器收集20名业余网球选手的非主用手臂数据，通过神经网络分类六种击球动作，并开发端到端原型系统。

Result: 分类准确率达88.2%，检测F1分数86.0%，用户研究表明参与者感到更轻松。

Conclusion: 非主用手臂是网球击球分析的有效且舒适的替代方案，推动了用户友好的体育分析技术发展。

Abstract: Wearable technology has transformed sports analytics, offering new dimensions
in enhancing player experience. Yet, many solutions involve cumbersome setups
that inhibit natural motion. In tennis, existing products require sensors on
the racket or dominant arm, causing distractions and discomfort. We propose
Silent Impact, a novel and user-friendly system that analyzes tennis shots
using a sensor placed on the passive arm. Collecting Inertial Measurement Unit
sensor data from 20 recreational tennis players, we developed neural networks
that exclusively utilize passive arm data to detect and classify six shots,
achieving a classification accuracy of 88.2% and a detection F1 score of 86.0%,
comparable to the dominant arm. These models were then incorporated into an
end-to-end prototype, which records passive arm motion through a smartwatch and
displays a summary of shots on a mobile app. User study (N=10) showed that
participants felt less burdened physically and mentally using Silent Impact on
the passive arm. Overall, our research establishes the passive arm as an
effective, comfortable alternative for tennis shot analysis, advancing
user-friendly sports analytics.

</details>


### [46] [Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System](https://arxiv.org/abs/2507.23298)
*Kazushi Kato,Koji Inoue,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.HC

TL;DR: 提出了一种实时预测点头时机和类型的模型，基于VAP扩展，结合多任务学习和预训练，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 非语言信息（如点头）对人类对话至关重要，现有语音对话系统需增强此类能力。

Method: 扩展VAP模型，预测多种点头类型，结合多任务学习和通用对话数据预训练，降低处理速率以实现实时运行。

Result: 多任务学习显著有效，降低处理速率不影响精度，主观评估优于传统同步点头方法。

Conclusion: 模型在实时性和准确性上表现优异，适用于专注聆听系统。

Abstract: In human dialogue, nonverbal information such as nodding and facial
expressions is as crucial as verbal information, and spoken dialogue systems
are also expected to express such nonverbal behaviors. We focus on nodding,
which is critical in an attentive listening system, and propose a model that
predicts both its timing and type in real time. The proposed model builds on
the voice activity projection (VAP) model, which predicts voice activity from
both listener and speaker audio. We extend it to prediction of various types of
nodding in a continuous and real-time manner unlike conventional models. In
addition, the proposed model incorporates multi-task learning with verbal
backchannel prediction and pretraining on general dialogue data. In the timing
and type prediction task, the effectiveness of multi-task learning was
significantly demonstrated. We confirmed that reducing the processing rate
enables real-time operation without a substantial drop in accuracy, and
integrated the model into an avatar attentive listening system. Subjective
evaluations showed that it outperformed the conventional method, which always
does nodding in sync with verbal backchannel. The code and trained models are
available at https://github.com/MaAI-Kyoto/MaAI.

</details>


### [47] [Automated Feedback on Student-Generated UML and ER Diagrams Using Large Language Models](https://arxiv.org/abs/2507.23470)
*Sebastian Gürtl,Gloria Schimetta,David Kerschbaumer,Michael Liut,Alexander Steinmaurer*

Main category: cs.HC

TL;DR: 该论文介绍了DUET，一个基于LLM的UML和ER图学习辅助工具，旨在通过文本化比较和反馈解决学习中的挑战，并在初步评估中显示出潜力。


<details>
  <summary>Details</summary>
Motivation: UML和ER图学习需要抽象思维和复杂理解，传统教学方法难以提供个性化反馈，DUET试图通过LLM技术解决这一问题。

Method: DUET通过多阶段LLM流程将学生与参考图转化为文本并比较，生成结构化反馈，支持自主学习和教学策略优化。

Result: 初步评估显示DUET在可访问性和扩展性方面有优势，但也存在可靠性和潜在滥用的问题，参与者提出了改进建议。

Conclusion: DUET为LLM在建模教育中的应用提供了方向，未来需进一步课堂集成和实证评估。

Abstract: UML and ER diagrams are foundational in computer science education but come
with challenges for learners due to the need for abstract thinking, contextual
understanding, and mastery of both syntax and semantics. These complexities are
difficult to address through traditional teaching methods, which often struggle
to provide scalable, personalized feedback, especially in large classes. We
introduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,
which converts a reference diagram and a student-submitted diagram into a
textual representation and provides structured feedback based on the
differences. It uses a multi-stage LLM pipeline to compare diagrams and
generate reflective feedback. Furthermore, the tool enables analytical insights
for educators, aiming to foster self-directed learning and inform instructional
strategies. We evaluated DUET through semi-structured interviews with six
participants, including two educators and four teaching assistants. They
identified strengths such as accessibility, scalability, and learning support
alongside limitations, including reliability and potential misuse. Participants
also suggested potential improvements, such as bulk upload functionality and
interactive clarification features. DUET presents a promising direction for
integrating LLMs into modeling education and offers a foundation for future
classroom integration and empirical evaluation.

</details>


### [48] [Digital literacy interventions can boost humans in discerning deepfakes](https://arxiv.org/abs/2507.23492)
*Dominique Geissler,Claire Robertson,Stefan Feuerriegel*

Main category: cs.HC

TL;DR: 该研究比较了五种数字素养干预措施在提升人们识别深度伪造（Deepfake）能力方面的效果，结果显示这些干预可提高13%的识别率，同时保持对真实信息的信任。


<details>
  <summary>Details</summary>
Motivation: 由于深度伪造可能损害机构信任并影响选举结果，而现有数字素养提升方法缺乏可扩展性和有效性，因此研究探索了多种干预措施的效果。

Method: 研究通过实验比较了五种干预措施：文本指南、视觉演示、游戏化练习、隐式学习以及AI生成过程的解释，共1200名美国参与者参与测试。

Result: 干预措施使深度伪造识别能力提高最多13个百分点，且未降低对真实图像的信任。

Conclusion: 该方法是可扩展的、适用于多样人群，且能有效提升深度伪造检测能力，同时保持对真实信息的信任。

Abstract: Deepfakes, i.e., images generated by artificial intelligence (AI), can erode
trust in institutions and compromise election outcomes, as people often
struggle to discern real images from deepfakes. Improving digital literacy can
help address these challenges, yet scalable and effective approaches remain
largely unexplored. Here, we compare the efficacy of five digital literacy
interventions to boost people's ability to discern deepfakes: (1) textual
guidance on common indicators of deepfakes; (2) visual demonstrations of these
indicators; (3) a gamified exercise for identifying deepfakes; (4) implicit
learning through repeated exposure and feedback; and (5) explanations of how
deepfakes are generated with the help of AI. We conducted an experiment with
N=1,200 participants from the United States to test the immediate and long-term
effectiveness of our interventions. Our results show that our interventions can
boost deepfake discernment by up to 13 percentage points while maintaining
trust in real images. Altogether, our approach is scalable, suitable for
diverse populations, and highly effective for boosting deepfake detection while
maintaining trust in truthful information.

</details>


### [49] [Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web](https://arxiv.org/abs/2507.23585)
*Sophia Liu,Shm Garanganao Almeda*

Main category: cs.HC

TL;DR: 论文提出“超文本摩擦力”概念，通过对比维基百科与Instagram Explore、Are.na与GenAI图像工具，探讨如何在算法主导的界面中恢复用户控制力。


<details>
  <summary>Details</summary>
Motivation: 算法驱动的界面往往牺牲用户代理权以追求效率和参与度，用户对内容和关系的构建失去控制。

Method: 通过对比分析不同系统的界面结构（如Wikipedia与Instagram Explore），提出“超文本摩擦力”作为设计原则。

Result: 超文本系统强调来源、关联思考和用户驱动的意义构建，而算法系统则隐藏过程、扁平化参与。

Conclusion: 论文贡献了两点：界面结构对代理权的比较分析，以及超文本价值观作为设计承诺，用于在算法化网络中恢复用户控制。

Abstract: Today's algorithm-driven interfaces, from recommendation feeds to GenAI
tools, often prioritize engagement and efficiency at the expense of user
agency. As systems take on more decision-making, users have less control over
what they see and how meaning or relationships between content are constructed.
This paper introduces "Hypertextual Friction," a conceptual design stance that
repositions classical hypertext principles--friction, traceability, and
structure--as actionable values for reclaiming agency in algorithmically
mediated environments. Through a comparative analysis of real-world
interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image
tools--we examine how different systems structure user experience, navigation,
and authorship. We show that hypertext systems emphasize provenance,
associative thinking, and user-driven meaning-making, while algorithmic systems
tend to obscure process and flatten participation. We contribute: (1) a
comparative analysis of how interface structures shape agency in user-driven
versus agent-driven systems, and (2) a conceptual stance that offers
hypertextual values as design commitments for reclaiming agency in an
increasingly algorithmic web.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [50] [Noise-Coded Illumination for Forensic and Photometric Video Analysis](https://arxiv.org/abs/2507.23002)
*Peter F. Michael,Zekun Hao,Serge Belongie,Abe Davis*

Main category: cs.GR

TL;DR: 本文提出了一种通过在场景照明中嵌入微妙的噪声调制来对抗视频伪造的方法，创建了一种信息不对称，有利于验证真实性。


<details>
  <summary>Details</summary>
Motivation: 随着视频伪造技术的普及，伪造视频越来越难与真实视频区分，本文旨在解决这一问题。

Method: 通过在场景照明中嵌入噪声调制，为录制的视频添加时间水印，编码未篡改场景的图像。

Result: 即使对手知道该方法，伪造具有编码的视频也变得更加困难，因为需要在信息劣势下解决更复杂的问题。

Conclusion: 该方法为高风险场景提供了有前途的保护手段，特别是在可以控制照明但无法控制摄像机的情况下。

Abstract: The proliferation of advanced tools for manipulating video has led to an arms
race, pitting those who wish to sow disinformation against those who want to
detect and expose it. Unfortunately, time favors the ill-intentioned in this
race, with fake videos growing increasingly difficult to distinguish from real
ones. At the root of this trend is a fundamental advantage held by those
manipulating media: equal access to a distribution of what we consider
authentic (i.e., "natural") video. In this paper, we show how coding very
subtle, noise-like modulations into the illumination of a scene can help combat
this advantage by creating an information asymmetry that favors verification.
Our approach effectively adds a temporal watermark to any video recorded under
coded illumination. However, rather than encoding a specific message, this
watermark encodes an image of the unmanipulated scene as it would appear lit
only by the coded illumination. We show that even when an adversary knows that
our technique is being used, creating a plausible coded fake video amounts to
solving a second, more difficult version of the original adversarial content
creation problem at an information disadvantage. This is a promising avenue for
protecting high-stakes settings like public events and interviews, where the
content on display is a likely target for manipulation, and while the
illumination can be controlled, the cameras capturing video cannot.

</details>


### [51] [XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding](https://arxiv.org/abs/2507.23777)
*Dian Chen,Yansong Qu,Xinyang Li,Ming Li,Shengchuan Zhang*

Main category: cs.GR

TL;DR: XSpecMesh是一种加速自回归网格生成模型的方法，通过轻量级多头推测解码并行预测多个标记，并结合验证和重采样策略以及蒸馏训练，实现1.7倍加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前自回归模型在生成高质量网格时需要大量标记预测，导致延迟高，亟需一种高效的加速方法。

Method: XSpecMesh采用轻量级多头推测解码并行预测标记，结合验证和重采样策略，并通过蒸馏训练对齐预测分布。

Result: 实验表明，该方法在保持生成质量的同时，实现1.7倍的加速效果。

Conclusion: XSpecMesh是一种高效且质量保持的网格生成加速方法，具有实际应用潜力。

Abstract: Current auto-regressive models can generate high-quality, topologically
precise meshes; however, they necessitate thousands-or even tens of
thousands-of next-token predictions during inference, resulting in substantial
latency. We introduce XSpecMesh, a quality-preserving acceleration method for
auto-regressive mesh generation models. XSpecMesh employs a lightweight,
multi-head speculative decoding scheme to predict multiple tokens in parallel
within a single forward pass, thereby accelerating inference. We further
propose a verification and resampling strategy: the backbone model verifies
each predicted token and resamples any tokens that do not meet the quality
criteria. In addition, we propose a distillation strategy that trains the
lightweight decoding heads by distilling from the backbone model, encouraging
their prediction distributions to align and improving the success rate of
speculative predictions. Extensive experiments demonstrate that our method
achieves a 1.7x speedup without sacrificing generation quality. Our code will
be released.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [52] [WiRM: Wireless Respiration Monitoring Using Conjugate Multiple Channel State Information and Fast Iterative Filtering in Wi-Fi Systems](https://arxiv.org/abs/2507.23419)
*James Rhodes,Lawrence Ong,Duy T. Ngo*

Main category: cs.ET

TL;DR: WiRM是一种两阶段的无线呼吸监测方法，通过相位净化技术和自适应多轨迹追踪提高呼吸率估计精度，并在嘈杂环境中表现出优异的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有呼吸监测方法大多仅关注呼吸率或呼吸波形，缺乏鲁棒性评估。WiRM旨在提供更精确的呼吸率估计和波形提取，并在嘈杂环境中验证其性能。

Method: WiRM采用两阶段方法：1）利用共轭乘法和自适应多轨迹追踪（AMTC）提高呼吸率估计；2）基于呼吸率优化波形提取。还开发了仿真工具模拟噪声环境。

Result: WiRM在呼吸率估计中平均降低38%的RMSE，波形提取的平均绝对相关性提升178.3%。在噪声环境下，其鲁棒性优于或与现有方法相当。

Conclusion: WiRM通过创新的两阶段设计和噪声评估工具，显著提升了无线呼吸监测的精度和鲁棒性，为实际应用提供了可靠解决方案。

Abstract: Monitoring respiratory health with the use of channel state information (CSI)
has shown promising results. Many existing methods focus on monitoring only the
respiratory rate, while others focus on monitoring the motion of the chest as a
patient breathes, which is referred to as the respiratory waveform. This paper
presents WiRM, a two-staged approach to contactless respiration monitoring. In
the first stage, WiRM improves upon existing respiratory rate estimation
techniques by using conjugate multiplication for phase sanitisation and
adaptive multi-trace carving (AMTC) for tracing how the respiratory rate
changes over time. When compared against three state-of-the-art methods, WiRM
has achieved an average reduction of $38\%$ in respiratory rate root mean
squared error (RMSE). In the second stage, WiRM uses this improved respiratory
rate estimate to inform the decomposition and selection of the respiratory
waveform from the CSI data. Remarkably, WiRM delivers a $178.3\%$ improvement
in average absolute correlation with the ground truth respiratory waveform.
Within the literature, it is difficult to compare the robustness of existing
algorithms in noisy environments. In this paper, we develop a purpose-built
simulation toolkit to evaluate the robustness of respiration monitoring
solutions under various noise conditions, including thermal, multiplicative,
and phase noise. Our results show that WiRM demonstrates improved or comparable
resilience to these common noise sources.

</details>


### [53] [SOME: Symmetric One-Hot Matching Elector -- A Lightweight Microsecond Decoder for Quantum Error Correction](https://arxiv.org/abs/2507.23618)
*Xinyi Guo,Geguang Miao,Shinichi Nishizawa,Hiromitsu Awano,Shinji Kimura,Takashi Sato*

Main category: cs.ET

TL;DR: 提出了一种名为SOME的新型解码器，通过将量子纠错解码任务重新表述为QUBO问题，显著降低了变量数量和拓扑复杂性，同时优化了解码时间。


<details>
  <summary>Details</summary>
Motivation: 传统量子纠错解码器（如MWPM和UF）在高阈值或快速解码方面表现优异，但拓扑复杂性较高；而基于伊辛模型的解码器虽然降低了复杂性，但解码时间较长。SOME旨在同时解决这两方面的问题。

Method: SOME将解码任务转化为QUBO问题（OHQ），其中变量表示翻转症状对的匹配情况，错误概率作为交互系数。通过对称单热编码构建自逆置换矩阵，并通过迭代优化最低总权重的矩阵来实现高效解码。

Result: SOME实现了变量数量减少99.9倍，解码时间从毫秒级缩短至微秒级，并在高达10.5%的物理错误率下保持性能，优于MWPM的最高阈值。

Conclusion: SOME通过创新的QUBO问题设计，显著提升了量子纠错解码的效率和性能，解决了传统方法的局限，为实际应用提供了更好的选择。

Abstract: Conventional quantum error correction (QEC) decoders such as Minimum-Weight
Perfect Matching (MWPM) and Union-Find (UF) offer high thresholds and fast
decoding, respectively, but both suffer from high topological complexity. In
contrast, Ising model-based decoders reduce topological complexity but demand
considerable decoding time. We propose the Symmetric One-Hot Matching Elector
(SOME), a novel decoder that reformulates the QEC decoding task as a Quadratic
Unconstrained Binary Optimization (QUBO) problem -- termed the One-Hot QUBO
(OHQ). Each variable in the QUBO represents whether a given pair of flipped
syndromes is matched, while the error probabilities between the pair are
encoded as interaction coefficients (weight). Constraints ensure that each
flipped syndrome is matched exactly once. Valid solutions of OHQ correspond to
self-inverse permutation matrices, characterized by symmetric one-hot encoding.
To solve the OHQ efficiently, SOME reformulates the decoding task as the
construction of permutation matrices that minimize the total weight. It
initializes each candidate matrix from one of the minimum-weight syndrome
pairs, then iteratively appends additional pairs in ascending order of weight,
and finally selects the permutation matrix with the lowest total energy. SOME
achieves up to a 99.9x reduction in variable count and reduces decoding times
from milliseconds to microseconds on a single-threaded commodity CPU. OHQ also
maintains performance up to a 10.5% physical error rate, surpassing the highest
known threshold of MWPM@.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [54] [H2SGEMM: Emulating FP32 GEMM on Ascend NPUs using FP16 Units with Precision Recovery and Cache-Aware Optimization](https://arxiv.org/abs/2507.23387)
*Weicheng Xue,Baisong Xu,Kai Yang,Yongxiang Liu,Dengdeng Fan,Pengxiang Xu,Yonghong Tian*

Main category: cs.DC

TL;DR: H2SGEMM算法利用FP16计算单元模拟FP32 GEMM，通过分解和误差补偿策略实现高性能和精度。


<details>
  <summary>Details</summary>
Motivation: 解决FP16矩阵引擎缺乏FP32支持的问题，同时保持高计算性能。

Method: 将FP32操作数分解为两个FP16值，并通过可调缩放补偿误差，采用缓存感知分块和双缓冲管道优化性能。

Result: 在Ascend 910A NPU上实现77%的FP32理论峰值性能，并恢复FP32精度。

Conclusion: H2SGEMM在缺乏FP32支持的硬件上高效且稳定地模拟FP32 GEMM。

Abstract: Low-precision matrix engines, such as FP16 cube, offer high throughput but
lack support for full-precision computation. In this work, we propose H2SGEMM,
a high-performance algorithm for emulating FP32 general matrix-matrix
multiplication (GEMM) using only FP16 computation units on a representative AI
accelerator. The method decomposes each FP32 operand into two FP16 values and
compensates for numerical errors through a tunable scaling strategy. A detailed
analysis of numerical errors, including underflow conditions and precision
loss, guides the selection of scaling parameters to preserve up to 22 bits of
mantissa accuracy. We further investigate the effect of computation order on
accuracy and demonstrate that a term-wise accumulation scheme improves
numerical stability over conventional FP32 GEMM in low-exponent regimes.
Finally, a cache-aware blocking strategy and double-buffered pipeline are
introduced to overlap memory transfers with computation, enabling H2SGEMM to
achieve up to 77% of the theoretical FP32-equivalent peak performance on Ascend
910A NPU lacking native FP32 support. Extensive numerical experiments confirm
that our method not only recovers the accuracy of native FP32 GEMM but also
exhibits superior numerical stability under certain conditions, due to its
structured and error-aware computation order.

</details>


### [55] [Towards a Testbed for Scalable FaaS Platforms](https://arxiv.org/abs/2507.23431)
*Trever Schirmer,David Bermbach*

Main category: cs.DC

TL;DR: 该论文提出了一个研究型测试平台，用于评估不同架构和技术对FaaS平台性能的影响。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解云平台架构如何影响其性能，尤其是面向可扩展性的FaaS平台。

Method: 开发了一个可灵活调整的研究型测试平台，用于快速评估不同架构和技术的影响。

Result: 测试平台能够有效评估FaaS平台的性能特征，尤其是可扩展性。

Conclusion: 该测试平台为研究FaaS平台性能提供了实用工具，有助于优化架构选择。

Abstract: Most cloud platforms have a Function-as-a-Service (FaaS) offering that
enables users to easily write highly scalable applications. To better
understand how the platform's architecture impacts its performance, we present
a research-focused testbed that can be adapted to quickly evaluate the impact
of different architectures and technologies on the characteristics of
scalability-focused FaaS platforms.

</details>


### [56] [Threshold-Driven Streaming Graph: Expansion and Rumor Spreading](https://arxiv.org/abs/2507.23533)
*Flora Angileri,Andrea Clementi,Emanuele Natale,Michele Salvi,Isabella Ziccardi*

Main category: cs.DC

TL;DR: 研究了RAES算法在动态图模型下的行为，证明了在节点更替的流模型中，每个快照图具有高概率的良好扩展性，并由此推得了谣言传播协议PUSH和PULL的完成时间对数上界。


<details>
  <summary>Details</summary>
Motivation: 解决RAES算法在静态图假设下的局限性，扩展至动态图模型，特别是节点更替的场景，以更好地应用在P2P网络中。

Method: 采用流式节点更替模型（滑动窗口模型），分析RAES算法在动态图中的行为，并通过数学证明展示其扩展性。

Result: 动态图中的每个快照图具有良好扩展性，且PUSH和PULL协议的完成时间在动态图中为对数级别。

Conclusion: RAES算法在动态图模型中依然有效，能够保持图的扩展性，并优化了谣言传播协议的效率。

Abstract: A randomized distributed algorithm called RAES was introduced in [Becchetti
et al., SODA 2020] to extract a bounded-degree expander from a dense $n$-vertex
expander graph $G = (V, E)$. The algorithm relies on a simple threshold-based
procedure. A key assumption in [Becchetti et al., SODA 2020] is that the input
graph $G$ is static - i.e., both its vertex set $V$ and edge set $E$ remain
unchanged throughout the process - while the analysis of RAES in dynamic models
is left as a major open question.
  In this work, we investigate the behavior of RAES under a dynamic graph model
induced by a streaming node-churn process (also known as the sliding window
model), where, at each discrete round, a new node joins the graph and the
oldest node departs. This process yields a bounded-degree dynamic graph
$\mathcal{G} =\{ G_t = (V_t, E_t) : t \in \mathbb{N}\}$ that captures essential
characteristics of peer-to-peer networks -- specifically, node churn and
threshold on the number of connections each node can manage. We prove that
every snapshot $G_t$ in the dynamic graph sequence has good expansion
properties with high probability. Furthermore, we leverage this property to
establish a logarithmic upper bound on the completion time of the well-known
PUSH and PULL rumor spreading protocols over the dynamic graph $\mathcal{G}$.

</details>


### [57] [The ArborX library: version 2.0](https://arxiv.org/abs/2507.23700)
*Andrey Prokopenko,Daniel Arndt,Damien Lebrun-Grandié,Bruno Turcksin*

Main category: cs.DC

TL;DR: ArborX 2.0库的概述，介绍其新接口、搜索数据结构等功能改进。


<details>
  <summary>Details</summary>
Motivation: 为了支持更广泛的用户问题，并提供更多功能和灵活性。

Method: 引入新接口、支持新搜索数据结构和用户回调功能，扩展算法集。

Result: 提升了库的性能和适用性，支持更多应用场景。

Conclusion: ArborX 2.0通过多项改进增强了功能性和灵活性。

Abstract: This paper provides an overview of the 2.0 release of the ArborX library, a
performance portable geometric search library based on Kokkos. We describe the
major changes in ArborX 2.0 including a new interface for the library to
support a wider range of user problems, new search data structures (brute
force, distributed), support for user functions to be executed on the results
(callbacks), and an expanded set of the supported algorithms (ray tracing,
clustering).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [58] [AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads](https://arxiv.org/abs/2507.23084)
*Taiyi Wang,Eiko Yoneki*

Main category: cs.DB

TL;DR: AutoIndexer是一个结合工作负载压缩、查询优化和专用RL模型的框架，用于高效选择数据库索引，显著降低搜索复杂度，减少查询执行时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的索引顾问在面对规模扩展时，由于动作空间指数级增长和大量试错，难以适应扩展的工作负载。

Method: 提出AutoIndexer框架，通过工作负载压缩、查询优化和专用RL模型，有效扩展索引选择的范围。

Result: 实验表明，AutoIndexer比无索引基准查询执行时间减少95%，比现有RL索引顾问节省20%的工作负载成本，调优时间减少50%。

Conclusion: AutoIndexer在大规模多样化工作负载中表现出实用性。

Abstract: Efficiently selecting indexes is fundamental to database performance
optimization, particularly for systems handling large-scale analytical
workloads. While deep reinforcement learning (DRL) has shown promise in
automating index selection through its ability to learn from experience, few
works address how these RL-based index advisors can adapt to scaling workloads
due to exponentially growing action spaces and heavy trial and error. To
address these challenges, we introduce AutoIndexer, a framework that combines
workload compression, query optimization, and specialized RL models to scale
index selection effectively. By operating on compressed workloads, AutoIndexer
substantially lowers search complexity without sacrificing much index quality.
Extensive evaluations show that it reduces end-to-end query execution time by
up to 95% versus non-indexed baselines. On average, it outperforms
state-of-the-art RL-based index advisors by approximately 20% in workload cost
savings while cutting tuning time by over 50%. These results affirm
AutoIndexer's practicality for large and diverse workloads.

</details>


### [59] [Jelly-Patch: a Fast Format for Recording Changes in RDF Datasets](https://arxiv.org/abs/2507.23499)
*Piotr Sowinski,Kacper Grzymkowski,Anastasiya Danilenka*

Main category: cs.DB

TL;DR: Jelly-Patch是一种高性能、压缩的二进制序列化格式，显著提升了RDF数据变化的序列化和解析效率。


<details>
  <summary>Details</summary>
Motivation: RDF系统中记录数据变化对于审计、增量备份、数据库复制和事件驱动工作流至关重要，但大规模和低延迟应用中更新量大会导致性能瓶颈。

Method: 提出Jelly-Patch，一种针对RDF数据集变化的高性能压缩二进制序列化格式，并通过基准测试与其他RDF Patch格式进行比较。

Result: Jelly-Patch在压缩率上提升了3.5-8.9倍，序列化和解析吞吐量分别提升了2.5倍和4.6倍。

Conclusion: Jelly-Patch在大规模和低延迟RDF系统中表现出显著性能优势。

Abstract: Recording data changes in RDF systems is a crucial capability, needed to
support auditing, incremental backups, database replication, and event-driven
workflows. In large-scale and low-latency RDF applications, the high volume and
frequency of updates can cause performance bottlenecks in the serialization and
transmission of changes. To alleviate this, we propose Jelly-Patch -- a
high-performance, compressed binary serialization format for changes in RDF
datasets. To evaluate its performance, we benchmark Jelly-Patch against
existing RDF Patch formats, using two datasets representing different use cases
(change data capture and IoT streams). Jelly-Patch is shown to achieve
3.5--8.9x better compression, and up to 2.5x and 4.6x higher throughput in
serialization and parsing, respectively. These significant advancements in
throughput and compression are expected to improve the performance of
large-scale and low-latency RDF systems.

</details>


### [60] [DataLens: Enhancing Dataset Discovery via Network Topologies](https://arxiv.org/abs/2507.23515)
*Anaïs Ollagnier,Aline Menin*

Main category: cs.DB

TL;DR: DataLens是一个基于网络的可视化平台，用于增强文本资源的发现，通过多视角探索和网络可视化提高搜索效率。


<details>
  <summary>Details</summary>
Motivation: 公开文本资源快速增长，但现有搜索方法依赖关键词和元数据，无法揭示资源间的联系。

Method: 结合分面搜索和高级可视化技术，提供网络可视化和链式视图，支持多角度探索。

Result: 用户研究表明，用户高度评价网络可视化工具，并提出改进建议以优化数据集搜索。

Conclusion: DataLens通过可视化技术提升了资源发现的效率和用户体验，未来可进一步优化。

Abstract: The rapid growth of publicly available textual resources, such as lexicons
and domain-specific corpora, presents challenges in efficiently identifying
relevant resources. While repositories are emerging, they often lack advanced
search and exploration features. Most search methods rely on keyword queries
and metadata filtering, which require prior knowledge and fail to reveal
connections between resources. To address this, we present DataLens, a
web-based platform that combines faceted search with advanced visualization
techniques to enhance resource discovery. DataLens offers network-based
visualizations, where the network structure can be adapted to suit the specific
analysis task. It also supports a chained views approach, enabling users to
explore data from multiple perspectives. A formative user study involving six
data practitioners revealed that users highly value visualization
tools-especially network-based exploration-and offered insights to help refine
our approach to better support dataset search.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [61] [Complexity-energy trade-off in programmable unitary interferometers](https://arxiv.org/abs/2507.22972)
*Nikita A. Nemkov,Stanislav S. Straupe*

Main category: physics.optics

TL;DR: 该论文讨论了相干多端口干涉仪在实现光子集成电路中矩阵乘法时的编程复杂性及其对能量效率的限制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了揭示当前干涉仪架构（如MZI和分束器网格）在实现矩阵乘法时面临的编程复杂性和能量效率问题背后的内在原因。

Method: 通过分析现有干涉仪架构的编程复杂性及其对输出能量的影响，论证了这些问题的内在性。

Result: 研究发现，编程复杂性是干涉仪架构的固有特性，而高效的编程算法通常会导致输出能量降低，进而影响准确性和能量效率。

Conclusion: 论文指出，需要新的架构设计以解决编程复杂性和能量效率之间的权衡问题。

Abstract: Coherent multiport interferometers are a promising approach to realize matrix
multiplication in integrated photonics. However, most known architectures -
such as MZI and beamsplitter meshes, as well as more general interferometers -
suffer from complicated procedures for mapping the matrix elements of the
desired transformation to specific phaseshifts in the device. We point out that
the high programming complexity is intrinsic, rather than accidental. At the
same time, we argue that interferometers admitting efficient programming
algorithms in general yield a much lower useful output energy, which ultimately
limits their accuracy and energy efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [KLLM: Fast LLM Inference with K-Means Quantization](https://arxiv.org/abs/2507.23035)
*Xueying Wu,Baijun Zhou,Zhihui Gao,Yuzhe Fu,Qilin Zheng,Yintao He,Hai Li*

Main category: cs.LG

TL;DR: KLLM是一个硬件-软件协同设计框架，通过K-Means量化和高效的异常值检测，显著提升了大型语言模型的推理速度和能效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的推理存在内存和计算需求高的问题，传统量化方法在低精度下精度损失严重，且非均匀量化难以直接执行。

Method: KLLM提出基于索引的计算方案，避免解量化和全精度计算，并设计Orizuru引擎在线检测激活异常值。

Result: 实验显示，KLLM在速度和能效上分别比A100 GPU和Atom提升了9.67x、7.03x和229.50x、150.21x。

Conclusion: KLLM通过协同设计，有效解决量化问题，显著提升了大型语言模型的推理性能。

Abstract: Large language model (LLM) inference poses significant challenges due to its
intensive memory and computation demands. Weight and activation quantization
(WAQ) offers a promising solution by reducing both memory footprint and
arithmetic complexity. However, two key challenges remain in the existing WAQ
designs. (1) Traditional WAQ designs rely on uniform integer-based quantization
for hardware efficiency, but this often results in significant accuracy
degradation at low precision. K-Means-based quantization, a non-uniform
quantization technique, achieves higher accuracy by matching the Gaussian-like
distributions of weights and activations in LLMs. However, its non-uniform
nature prevents direct execution on low-precision compute units, requiring
dequantization and floating-point matrix multiplications (MatMuls) during
inference. (2) Activation outliers further hinder effective low-precision WAQ.
Offline thresholding methods for outlier detection can lead to significant
model performance degradation, while existing online detection techniques
introduce substantial runtime overhead.
  To address the aforementioned challenges and fully unleash the potential of
WAQ with K-Means quantization for LLM inference, in this paper, we propose
KLLM, a hardware-software co-design framework. KLLM features an index-based
computation scheme for efficient execution of MatMuls and nonlinear operations
on K-Means-quantized data, which avoids most of the dequantization and
full-precision computations. Moreover, KLLM incorporates a novel outlier
detection engine, Orizuru, that efficiently identifies the top-$k$ largest and
smallest elements in the activation data stream during online inference.
  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,
7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the
A100 GPU and Atom, respectively.

</details>


### [63] [NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions](https://arxiv.org/abs/2507.23186)
*Peter Sharpe*

Main category: cs.LG

TL;DR: 该论文提出了一种名为NaN传播的方法，利用IEEE 754中的NaN浮点值特性，检测黑箱函数中的稀疏性，避免传统有限差分方法中的假阴性问题，显著提升了梯度计算的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 有限差分方法在检测黑箱函数的稀疏性时，由于梯度巧合为零会导致假阴性，进而影响梯度计算的准确性，但这些问题难以诊断。为了解决这一挑战，作者引入了NaN传播方法。

Method: 通过系统性地用NaN污染输入，并观察哪些输出变为NaN，该方法重建了保守的稀疏性模式，从而消除假阴性。

Result: 在航空航天机翼重量模型上的实验表明，该方法实现了1.52倍的加速，并检测到传统方法遗漏的数十个依赖关系。由于梯度计算是许多优化工作流的瓶颈，这是一个显著的改进。

Conclusion: 该技术利用IEEE 754的兼容性，无需修改现有黑箱代码即可跨编程语言和数学库工作，同时通过NaN负载编码等策略实现了优于现有黑箱稀疏性检测方法的性能。

Abstract: Sparsity detection in black-box functions enables significant computational
speedups in gradient-based optimization through Jacobian compression, but
existing finite-difference methods suffer from false negatives due to
coincidental zero gradients. These false negatives can silently corrupt
gradient calculations, leading to difficult-to-diagnose errors. We introduce
NaN-propagation, which exploits the universal contamination property of IEEE
754 Not-a-Number floating-point values to trace input-output dependencies
through floating-point numerical computations. By systematically contaminating
inputs with NaN and observing which outputs become NaN, the method reconstructs
conservative sparsity patterns that eliminate false negatives. We demonstrate
the approach on an aerospace wing weight model, achieving a 1.52x speedup while
detecting dozens of dependencies missed by conventional methods -- a
significant improvement since gradient computation is the bottleneck in many
optimization workflows. The technique leverages IEEE 754 compliance to work
across programming languages and math libraries without modifying existing
black-box codes. Advanced strategies including NaN payload encoding enable
faster-than-linear time complexity, improving upon existing black-box sparsity
detection methods. Practical algorithms are also proposed to mitigate
challenges from branching code execution common in engineering applications.

</details>


### [64] [Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform](https://arxiv.org/abs/2507.23562)
*Sirine Arfa,Bernhard Vogginger,Christian Mayr*

Main category: cs.LG

TL;DR: 该论文提出了一种基于脉冲神经网络（SNN）的强化学习算法，用于解决经典控制任务，并在SpiNNaker2神经形态芯片上实现了低功耗和低延迟。


<details>
  <summary>Details</summary>
Motivation: 旨在展示SNN在神经形态硬件上的潜力，特别是其低功耗和低延迟的优势，为机器人任务提供高效的解决方案。

Method: 采用Q-learning算法训练SNN，并将其量化为8位精度，部署在SpiNNaker2芯片上，与GTX 1650 GPU进行性能对比。

Result: SpiNNaker2在能耗上实现了32倍的降低，推理延迟与GPU相当，某些任务场景下表现更优。

Conclusion: SpiNNaker2展示了可扩展的低能耗神经形态计算潜力，为实时神经形态控制提供了可行的解决方案。

Abstract: Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power
consumption and low-latency inference on neuromorphic hardware for a wide range
of robotic tasks. In this work, we present an energy-efficient implementation
of a reinforcement learning (RL) algorithm using quantized SNNs to solve two
classical control tasks. The network is trained using the Q-learning algorithm,
then fine-tuned and quantized to low-bit (8-bit) precision for embedded
deployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative
advantage of SpiNNaker2 over conventional computing platforms, we analyze
inference latency, dynamic power consumption, and energy cost per inference for
our SNN models, comparing performance against a GTX 1650 GPU baseline. Our
results demonstrate SpiNNaker2's strong potential for scalable, low-energy
neuromorphic computing, achieving up to 32x reduction in energy consumption.
Inference latency remains on par with GPU-based execution, with improvements
observed in certain task settings, reinforcing SpiNNaker2's viability for
real-time neuromorphic control and making the neuromorphic approach a
compelling direction for efficient deep Q-learning.

</details>


### [65] [SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy](https://arxiv.org/abs/2507.23292)
*RJ Skerry-Ryan,Julian Salazar,Soroosh Mariooryad,David Kao,Daisy Stanton,Eric Battenberg,Matt Shannon,Ron J. Weiss,Robin Scheibler,Jonas Rothfuss,Tom Bagby*

Main category: cs.LG

TL;DR: 介绍了一个用于序列建模的神经网络层API和库，支持按层或按步执行，具有显式状态表示和流式处理能力。


<details>
  <summary>Details</summary>
Motivation: 旨在简化序列模型的创建，支持流式处理并减少常见错误。

Method: 通过定义显式状态和步进方法，实现层间和步进执行的等价性，兼容多种深度学习库。

Result: 提供了可组合的API和丰富层库，支持生产级模型的流式构建。

Conclusion: SequenceLayers通过显式状态和流式设计，提升了序列模型开发的效率和正确性。

Abstract: We introduce a neural network layer API and library for sequence modeling,
designed for easy creation of sequence models that can be executed both
layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,
autoregressive sampling). To achieve this, layers define an explicit
representation of their state over time (e.g., a Transformer KV cache, a
convolution buffer, an RNN hidden state), and a step method that evolves that
state, tested to give identical results to a stateless layer-wise invocation.
This and other aspects of the SequenceLayers contract enables complex models to
be immediately streamable, mitigates a wide range of common bugs arising in
both streaming and parallel sequence processing, and can be implemented in any
deep learning library. A composable and declarative API, along with a
comprehensive suite of layers and combinators, streamlines the construction of
production-scale models from simple streamable components while preserving
strong correctness guarantees. Our current implementations of SequenceLayers
(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.

</details>


### [66] [Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions](https://arxiv.org/abs/2507.23335)
*Qilin Zhou,Haipeng Wang,Zhengyuan Wei,W. K. Chan*

Main category: cs.LG

TL;DR: 论文提出了一种名为CostCert的新型、可扩展且精确的投票式认证防御方法，用于对抗对抗性补丁攻击，并显著优于现有方法PatchGuard。


<details>
  <summary>Details</summary>
Motivation: 现有认证恢复技术在面对对抗性补丁攻击时，无法精确保证真实标签在top-k预测中，且存在组合爆炸问题。因此，提出一种更高效的方法至关重要。

Method: 通过设计一种新颖的验证方法，CostCert无需进行两两比较或组合枚举，直接判断攻击预算是否足够干扰真实标签进入top-k预测。

Result: 实验表明，在补丁大小为96时，CostCert仍能保持57.3%的认证准确率，而PatchGuard已降至零。

Conclusion: CostCert是一种高效且精确的防御方法，显著提升了对抗性补丁攻击的防御能力。

Abstract: Patch robustness certification is an emerging verification approach for
defending against adversarial patch attacks with provable guarantees for deep
learning systems. Certified recovery techniques guarantee the prediction of the
sole true label of a certified sample. However, existing techniques, if
applicable to top-k predictions, commonly conduct pairwise comparisons on those
votes between labels, failing to certify the sole true label within the top k
prediction labels precisely due to the inflation on the number of votes
controlled by the attacker (i.e., attack budget); yet enumerating all
combinations of vote allocation suffers from the combinatorial explosion
problem. We propose CostCert, a novel, scalable, and precise voting-based
certified recovery defender. CostCert verifies the true label of a sample
within the top k predictions without pairwise comparisons and combinatorial
explosion through a novel design: whether the attack budget on the sample is
infeasible to cover the smallest total additional votes on top of the votes
uncontrollable by the attacker to exclude the true labels from the top k
prediction labels. Experiments show that CostCert significantly outperforms the
current state-of-the-art defender PatchGuard, such as retaining up to 57.3% in
certified accuracy when the patch size is 96, whereas PatchGuard has already
dropped to zero.

</details>


### [67] [LLM-Assisted Cheating Detection in Korean Language via Keystrokes](https://arxiv.org/abs/2507.22956)
*Dong Hyun Roh,Rajesh Kumar,An Ngo*

Main category: cs.LG

TL;DR: 该研究提出了一种基于击键动态的框架，用于检测韩语中LLM辅助作弊行为，填补了语言覆盖、认知上下文和LLM参与粒度等方面的研究空白。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决现有研究中语言覆盖、认知上下文和LLM参与粒度不足的问题，尤其是在韩语语境下。

Method: 研究采用69名参与者，在三种写作条件下完成任务（真实写作、改写ChatGPT回答、转录ChatGPT回答），涵盖布鲁姆分类法中的六个认知过程。使用可解释的时间和节奏特征，评估了多种分类器。

Result: 结果显示，时间特征在认知感知场景下表现良好，而节奏特征在跨认知场景中泛化能力更强。对比人类评估者，模型在检测真实和转录回答时表现更优。

Conclusion: 结论表明，击键动态技术能可靠检测不同认知需求和写作策略下的LLM辅助写作行为。

Abstract: This paper presents a keystroke-based framework for detecting LLM-assisted
cheating in Korean, addressing key gaps in prior research regarding language
coverage, cognitive context, and the granularity of LLM involvement. Our
proposed dataset includes 69 participants who completed writing tasks under
three conditions: Bona fide writing, paraphrasing ChatGPT responses, and
transcribing ChatGPT responses. Each task spans six cognitive processes defined
in Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and
create). We extract interpretable temporal and rhythmic features and evaluate
multiple classifiers under both Cognition-Aware and Cognition-Unaware settings.
Temporal features perform well under Cognition-Aware evaluation scenarios,
while rhythmic features generalize better under cross-cognition scenarios.
Moreover, detecting bona fide and transcribed responses was easier than
paraphrased ones for both the proposed models and human evaluators, with the
models significantly outperforming the humans. Our findings affirm that
keystroke dynamics facilitate reliable detection of LLM-assisted writing across
varying cognitive demands and writing strategies, including paraphrasing and
transcribing LLM-generated responses.

</details>


### [68] [Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System](https://arxiv.org/abs/2507.23756)
*Diana Mortagua*

Main category: cs.LG

TL;DR: 该研究通过考虑标注者的情绪和疲劳水平等内部因素，提出了一种基于知识推荐系统的查询-标注者配对策略，以减少标注错误并提高模型效率。


<details>
  <summary>Details</summary>
Motivation: 主动学习虽能减少标注数据需求，但仍需降低标注错误。现有策略忽略内部因素（如情绪、疲劳）对标注者生产力的影响，本研究旨在填补这一空白。

Method: 研究提出基于知识推荐系统的策略，结合标注者历史准确率、情绪和疲劳水平，以及查询实例信息，动态推荐最佳标注者。

Result: 实验表明，考虑内部因素显著减少了标注错误和模型不确定性，且在准确率和F1分数上有所提升。

Conclusion: 本研究为解决人类认知因素对主动学习的影响提供了初步探索，展示了内部因素的重要性。

Abstract: This study centers on overcoming the challenge of selecting the best
annotators for each query in Active Learning (AL), with the objective of
minimizing misclassifications. AL recognizes the challenges related to cost and
time when acquiring labeled data, and decreases the number of labeled data
needed. Nevertheless, there is still the necessity to reduce annotation errors,
aiming to be as efficient as possible, to achieve the expected accuracy faster.
Most strategies for query-annotator pairs do not consider internal factors that
affect productivity, such as mood, attention, motivation, and fatigue levels.
This work addresses this gap in the existing literature, by not only
considering how the internal factors influence annotators (mood and fatigue
levels) but also presenting a new query-annotator pair strategy, using a
Knowledge-Based Recommendation System (RS). The RS ranks the available
annotators, allowing to choose one or more to label the queried instance using
their past accuracy values, and their mood and fatigue levels, as well as
information about the instance queried. This work bases itself on existing
literature on mood and fatigue influence on human performance, simulating
annotators in a realistic manner, and predicting their performance with the RS.
The results show that considering past accuracy values, as well as mood and
fatigue levels reduces the number of annotation errors made by the annotators,
and the uncertainty of the model through its training, when compared to not
using internal factors. Accuracy and F1-score values were also better in the
proposed approach, despite not being as substantial as the aforementioned. The
methodologies and findings presented in this study begin to explore the open
challenge of human cognitive factors affecting AL.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [69] [Text-to-SQL Task-oriented Dialogue Ontology Construction](https://arxiv.org/abs/2507.23358)
*Renato Vukovic,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Hsien-Chin Lin,Shutong Feng,Nurul Lubis,Milica Gasic*

Main category: cs.CL

TL;DR: 该论文提出了一种名为TeQoDO的无监督方法，利用大型语言模型（LLM）的SQL编程能力和对话理论，从零开始构建面向任务的对话（TOD）本体，提高了可解释性和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型依赖参数化知识，缺乏可解释性和可信度。TOD系统通过外部数据库和显式本体结构解决了这一问题，但构建本体需要手动标注或监督训练。

Method: TeQoDO是一种基于Text-to-SQL的无监督方法，通过LLM自主构建TOD本体，结合预设的对话理论提示。

Result: TeQoDO在对话状态跟踪任务上表现优于迁移学习方法，并且能够扩展到构建更大规模的本体。

Conclusion: TeQoDO为利用本体增强LLM的可解释性提供了新的可能性。

Abstract: Large language models (LLMs) are widely used as general-purpose knowledge
sources, but they rely on parametric knowledge, limiting explainability and
trustworthiness. In task-oriented dialogue (TOD) systems, this separation is
explicit, using an external database structured by an explicit ontology to
ensure explainability and controllability. However, building such ontologies
requires manual labels or supervised training. We introduce TeQoDO: a
Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM
autonomously builds a TOD ontology from scratch without supervision using its
inherent SQL programming capabilities combined with dialogue theory provided in
the prompt. We show that TeQoDO outperforms transfer learning approaches, and
its constructed ontology is competitive on a downstream dialogue state tracking
task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also
scales to allow construction of much larger ontologies, which we investigate on
a Wikipedia and ArXiv dataset. We view this as a step towards broader
application of ontologies to increase LLM explainability.

</details>


### [70] [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)
*Md Talha Mohsin*

Main category: cs.CL

TL;DR: 本研究对五种主要LLM（GPT、Claude、Perplexity、Gemini和DeepSeek）在金融NLP任务中的表现进行了系统性比较，发现GPT表现最佳，其次是Claude和Perplexity，而Gemini和DeepSeek的输出一致性较低。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在金融NLP任务中表现优异，但不同模型间的系统性比较仍不足，因此本研究旨在填补这一空白。

Method: 通过人工标注、自动化词义指标（如ROUGE、余弦相似性、Jaccard）和模型行为诊断（提示级差异和模型间相似性）三种方法评估模型性能。

Result: GPT在连贯性、语义对齐和上下文相关性方面表现最佳，Gemini和DeepSeek的输出变异性较大。

Conclusion: LLM的输出对提示和源材料敏感，未来研究需进一步优化提示设计和模型训练方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide variety of Financial Natural Language Processing (FinNLP) tasks.
However, systematic comparisons among widely used LLMs remain underexplored.
Given the rapid advancement and growing influence of LLMs in financial
analysis, this study conducts a thorough comparative evaluation of five leading
LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the
'Magnificent Seven' technology companies. We create a set of domain-specific
prompts and then use three methodologies to evaluate model performance: human
annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,
Jaccard), and model behavior diagnostics (prompt-level variance and
across-model similarity). The results show that GPT gives the most coherent,
semantically aligned, and contextually relevant answers; followed by Claude and
Perplexity. Gemini and DeepSeek, on the other hand, have more variability and
less agreement. Also, the similarity and stability of outputs change from
company to company and over time, showing that they are sensitive to how
prompts are written and what source material is used.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [71] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: 本文介绍了一种基于大型语言模型（LLM）的智能体，用于与工业级ERP系统交互，通过自然语言查询生成可执行SQL语句。


<details>
  <summary>Details</summary>
Motivation: 提升企业资源规划（ERP）系统的交互效率，通过自然语言降低使用门槛。

Method: 提出了一种结合推理和批判阶段的双智能体架构，利用开源权重的LLM。

Result: 该智能体能可靠地将自然语言查询翻译为SQL语句。

Conclusion: 双智能体架构显著提高了查询生成的可靠性，为工业应用提供了实用工具。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [72] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 论文探讨了DRAI原则在领导级科学数据集上的应用，提出了一个两维准备度框架，以指导科学数据向AI训练的高效转化。


<details>
  <summary>Details</summary>
Motivation: 解决科学数据在训练基础模型时的预处理挑战，尤其是在高性能计算环境中。

Method: 分析四个典型领域的科学数据工作流，提出由数据准备级别和数据处理阶段组成的两维框架。

Result: 框架能够标准化跨领域、可扩展且可复现的科学AI训练数据准备。

Conclusion: 该框架为科学数据的AI训练提供了通用指导，并促进跨领域基础设施的发展。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [73] [Smart Video Capsule Endoscopy: Raw Image-Based Localization for Enhanced GI Tract Investigation](https://arxiv.org/abs/2507.23398)
*Oliver Bause,Julia Werner,Paul Palomero Bernardo,Oliver Bringmann*

Main category: eess.IV

TL;DR: 该论文提出了一种针对低功耗传感器边缘设备的轻量级神经网络，可以在Bayer图像上直接进行分类，避免RGB转换的能耗，实现高效图像分类。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络在资源受限的边缘设备上运行困难，且RGB图像转换消耗大量能量。论文旨在通过直接在Bayer图像上进行分类，减少能耗。

Method: 使用仅含63,000参数的CNN结合Viterbi解码，直接在Bayer图像上进行分类，并通过定制的PULPissimo芯片实现高效处理。

Result: 分类准确率达到93.06%，每张图像仅消耗5.31微焦耳，比传统视频胶囊节省89.9%能量。

Conclusion: 该方法显著降低了图像分类的能耗，适用于视频胶囊内窥镜等资源受限的应用场景。

Abstract: For many real-world applications involving low-power sensor edge devices deep
neural networks used for image classification might not be suitable. This is
due to their typically large model size and require- ment of operations often
exceeding the capabilities of such resource lim- ited devices. Furthermore,
camera sensors usually capture images with a Bayer color filter applied, which
are subsequently converted to RGB images that are commonly used for neural
network training. However, on resource-constrained devices, such conversions
demands their share of energy and optimally should be skipped if possible. This
work ad- dresses the need for hardware-suitable AI targeting sensor edge
devices by means of the Video Capsule Endoscopy, an important medical proce-
dure for the investigation of the small intestine, which is strongly limited by
its battery lifetime. Accurate organ classification is performed with a final
accuracy of 93.06% evaluated directly on Bayer images involv- ing a CNN with
only 63,000 parameters and time-series analysis in the form of Viterbi
decoding. Finally, the process of capturing images with a camera and raw image
processing is demonstrated with a customized PULPissimo System-on-Chip with a
RISC-V core and an ultra-low power hardware accelerator providing an
energy-efficient AI-based image clas- sification approach requiring just 5.31
{\mu}J per image. As a result, it is possible to save an average of 89.9% of
energy before entering the small intestine compared to classic video capsules.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [74] [Typing Tensor Calculus in 2-Categories (I)](https://arxiv.org/abs/1908.01212)
*Fatimah Rita Ahmadi*

Main category: math.CT

TL;DR: 论文提出了一种将线性代数计算形式化的方法，将矩阵视为矩阵范畴中的态射，并推广到半加性2-范畴以处理高秩张量，提供了无索引的线性代数框架。


<details>
  <summary>Details</summary>
Motivation: 旨在为高效算法开发和函数式编程语言提供一个适合的线性代数框架，同时支持并行化计算。

Method: 通过将矩阵视为矩阵范畴的态射，并延伸至半加性2-范畴来处理高秩张量，构建无索引的线性代数框架。

Result: 成功定义了一个包含矩阵和四索引张量的无索引线性代数框架，并进一步扩展到半加性2-范畴。

Conclusion: 该方法提供了一个类型化的线性代数框架，适用于函数式编程和并行计算，并能处理高秩张量。

Abstract: To formalize calculations in linear algebra for the development of efficient
algorithms and a framework suitable for functional programming languages and
faster parallelized computations, we adopt an approach that treats elements of
linear algebra, such as matrices, as morphisms in the category of matrices,
$\mathbf{Mat_{k}}$. This framework is further extended by generalizing the
results to arbitrary monoidal semiadditive categories. To enrich this
perspective and accommodate higher-rank matrices (tensors), we define
semiadditive 2-categories, where matrices $T_{ij}$ are represented as
1-morphisms, and tensors with four indices $T_{ijkl}$ as 2-morphisms. This
formalization provides an index-free, typed linear algebra framework that
includes matrices and tensors with up to four indices. Furthermore, we extend
the framework to monoidal semiadditive 2-categories and demonstrate detailed
operations and vectorization within the 2-category of 2Vec introduced by
Kapranov and Voevodsky.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [75] [Rational complex Bezier curves](https://arxiv.org/abs/2507.23485)
*A. Canton,L. Fernandez-Jambrina,M. J. Vazquez-Gallo*

Main category: math.NA

TL;DR: 本文提出了一种有理复Bézier曲线的形式化方法，扩展了CAD范式，通过引入复数值的控制多边形和权重，实现了更灵活的曲线设计和变换。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统CAD设计中曲线设计的局限性，作者希望通过引入复数值的控制多边形和权重，扩展曲线设计的能力，特别是在几何变换和曲线降阶方面。

Method: 通过将控制多边形和权重扩展为复数值，利用实平面和复数平面的投影变换群，实现了曲线的灵活变换。文中还通过多项式合成的方法验证了曲线的降阶条件。

Result: 该方法不仅在设计中实现了更灵活的几何变换（如几何反演），还在某些情况下降低了曲线的阶次，并通过公式验证了有理三次曲线是否为二次曲线。

Conclusion: 有理复Bézier曲线框架为CAD设计提供了新的工具，能够在保持简单性的同时实现更复杂的曲线设计和变换。

Abstract: In this paper we develop the formalism of rational complex Bezier curves.
This framework is a simple extension of the CAD paradigm, since it describes
arc of curves in terms of control polygons and weights, which are extended to
complex values. One of the major advantages of this extension is that we may
make use of two different groups of projective transformations. Besides the
group of projective transformations of the real plane, we have the group of
complex projective transformations. This allows us to apply useful
transformations like the geometric inversion to curves in design. In addition
to this, the use of the complex formulation allows to lower the degree of the
curves in some cases. This can be checked using the resultant of two
polynomials and provides a simple formula for determining whether a rational
cubic curve is a conic or not. Examples of application of the formalism to
classical curves are included.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [76] [Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving](https://arxiv.org/abs/2507.23042)
*Santosh Patapati,Trisanth Srinivasan*

Main category: cs.CV

TL;DR: NovaDrive是一种单分支视觉语言架构，通过处理多模态输入（图像、地图、深度和文本）和新型平滑损失函数，显著提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要在复杂环境中快速决策，现有方法在实时性和多模态数据融合方面存在不足，NovaDrive旨在解决这些问题。

Method: 采用单分支架构，结合两阶段交叉注意力模块对齐路点与地图，并优化图像和深度处理，使用平滑损失函数避免突变控制。基于LLaMA-3.2进行微调。

Result: 在nuScenes/Waymo子集上，成功率提升4%，路径效率提高0.11，碰撞频率降低1.4%，表明性能和安全性显著改善。

Conclusion: NovaDrive通过多模态融合和轻量设计提高了自动驾驶的效率和安全性，且具有扩展到其他领域的潜力。

Abstract: Autonomous vehicles must react in milliseconds while reasoning about road
geometry and traffic intent to navigate complex situations. We introduce
NovaDrive, a single-branch vision-language architecture that processes
front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a
single branch. A lightweight, two-stage cross-attention block first aligns
waypoint tokens with the HD map, then refines attention over fine-grained image
and depth patches. Coupled with a novel smoothness loss that discourages abrupt
steering and speed changes, this design eliminates the need for recurrent
memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language
backbone, enabling real-time inference. On the nuScenes / Waymo subset of the
MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts
path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from
2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations
confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention
fusion each contribute the most to these gains. Beyond safety, NovaDrive's
shorter routes (resulting from the novel smoothness loss) translate to lower
fuel or battery usage, pointing toward leaner, more easily updated driving
stacks. NovaDrive can be extended to other embodied-AI domains as well.

</details>


### [77] [Phi-Ground Tech Report: Advancing Perception in GUI Grounding](https://arxiv.org/abs/2507.23779)
*Miaosen Zhang,Ziqiang Xu,Jialiang Zhu,Qi Dai,Kai Qiu,Yifan Yang,Chong Luo,Tianyi Chen,Justin Wagle,Tim Franklin,Baining Guo*

Main category: cs.CV

TL;DR: 本文研究了多模态推理模型的GUI接地问题，提出了Phi-Ground模型家族，在多项基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 提升计算机使用代理（CUAs）的GUI接地能力是实现CUAs实际应用的核心挑战。

Method: 通过从数据收集到模型训练的详细分析，开发了Phi-Ground模型家族。

Result: Phi-Ground在多项基准测试中表现最佳，特别是在模型参数小于10B的情况下。

Conclusion: 本文的研究不仅改进了接地模型的设计，也为其他感知任务提供了有益的经验。

Abstract: With the development of multimodal reasoning models, Computer Use Agents
(CUAs), akin to Jarvis from \textit{"Iron Man"}, are becoming a reality. GUI
grounding is a core component for CUAs to execute actual actions, similar to
mechanical control in robotics, and it directly leads to the success or failure
of the system. It determines actions such as clicking and typing, as well as
related parameters like the coordinates for clicks. Current end-to-end
grounding models still achieve less than 65\% accuracy on challenging
benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from
being ready for deployment. % , as a single misclick can result in unacceptable
consequences. In this work, we conduct an empirical study on the training of
grounding models, examining details from data collection to model training.
Ultimately, we developed the \textbf{Phi-Ground} model family, which achieves
state-of-the-art performance across all five grounding benchmarks for models
under $10B$ parameters in agent settings. In the end-to-end model setting, our
model still achieves SOTA results with scores of \textit{\textbf{43.2}} on
ScreenSpot-pro and \textit{\textbf{27.2}} on UI-Vision. We believe that the
various details discussed in this paper, along with our successes and failures,
not only clarify the construction of grounding models but also benefit other
perception tasks. Project homepage:
\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}

</details>


### [78] [Consistent Point Matching](https://arxiv.org/abs/2507.23609)
*Halid Ziya Yerebakan,Gerardo Hermosillo Valadez*

Main category: cs.CV

TL;DR: 论文提出了一种将一致性启发式融入点匹配算法的改进方法，提升了医学图像解剖位置匹配的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 改进医学图像中点匹配的鲁棒性，尤其是在不同模态（如CT和MRI）和纵向数据集上的表现。

Method: 将一致性启发式融入点匹配算法，验证了其在多种内部和公共数据集上的表现。

Result: 在Deep Lesion Tracking数据集上超越了现有最佳结果，并能有效解决标志点定位问题。

Conclusion: 该方法无需机器学习模型或训练数据，实现了高效且可配置的医学图像导航。

Abstract: This study demonstrates that incorporating a consistency heuristic into the
point-matching algorithm \cite{yerebakan2023hierarchical} improves robustness
in matching anatomical locations across pairs of medical images. We validated
our approach on diverse longitudinal internal and public datasets spanning CT
and MRI modalities. Notably, it surpasses state-of-the-art results on the Deep
Lesion Tracking dataset. Additionally, we show that the method effectively
addresses landmark localization. The algorithm operates efficiently on standard
CPU hardware and allows configurable trade-offs between speed and robustness.
The method enables high-precision navigation between medical images without
requiring a machine learning model or training data.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [79] [Scalable contribution bounding to achieve privacy](https://arxiv.org/abs/2507.23432)
*Vincent Cohen-Addad,Alessandro Epasto,Jason Lee,Morteza Zadimoghaddam*

Main category: cs.DS

TL;DR: 论文提出了一种分布式算法，通过建模所有权结构为超图，并行处理用户提议的记录，确保数据隐私贡献不超限。


<details>
  <summary>Details</summary>
Motivation: 现代数据集中，记录可能有多所有者，现有序列算法计算量大、难以扩展，无法应对大规模数据挑战。

Method: 将所有权结构建模为超图，并行处理用户记录提议，仅当所有所有者一致同意时才添加记录。

Result: 算法高效、可扩展，最大化数据集规模，同时确保用户隐私贡献不超限。

Conclusion: 该分布式算法为大规模数据集中的用户级隐私提供了实用、高效的解决方案。

Abstract: In modern datasets, where single records can have multiple owners, enforcing
user-level differential privacy requires capping each user's total
contribution. This "contribution bounding" becomes a significant combinatorial
challenge. Existing sequential algorithms for this task are computationally
intensive and do not scale to the massive datasets prevalent today. To address
this scalability bottleneck, we propose a novel and efficient distributed
algorithm. Our approach models the complex ownership structure as a hypergraph,
where users are vertices and records are hyperedges. The algorithm proceeds in
rounds, allowing users to propose records in parallel. A record is added to the
final dataset only if all its owners unanimously agree, thereby ensuring that
no user's predefined contribution limit is violated. This method aims to
maximize the size of the resulting dataset for high utility while providing a
practical, scalable solution for implementing user-level privacy in large,
real-world systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [80] [Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance](https://arxiv.org/abs/2507.23088)
*Lalithkumar Seenivasan,Jiru Xu,Roger D. Soberanis Mukul,Hao Ding,Grayson Byrd,Yu-Chun Ku,Jose L. Porras,Masaru Ishii,Mathias Unberath*

Main category: cs.RO

TL;DR: 论文提出了一种新型的Perception Agent，通过结合语音交互的大型语言模型（LLMs）和基础模型，实现了更自然的人机交互，适用于动态手术环境中的实时辅助。


<details>
  <summary>Details</summary>
Motivation: 现有的AI手术辅助解决方案过于刚性，缺乏灵活性和自然交互能力，限制了其在动态手术环境中的实际应用潜力。

Method: 利用语音集成的大型语言模型（LLMs）、Segment Anything模型（SAM）和任意点跟踪基础模型，设计了一种具有记忆存储库和两种新机制的Perception Agent，能够通过直观交互分割已知和未知的手术场景元素。

Result: 定量分析表明，Perception Agent的性能与更耗费人力的手动提示策略相当；定性分析展示了其在分割新元素（如器械、移植体和纱布）方面的灵活性。

Conclusion: Perception Agent通过提供自然的人机交互并克服刚性，使基于AI的动态手术环境实时辅助更接近现实。

Abstract: Emerging surgical data science and robotics solutions, especially those
designed to provide assistance in situ, require natural human-machine
interfaces to fully unlock their potential in providing adaptive and intuitive
aid. Contemporary AI-driven solutions remain inherently rigid, offering limited
flexibility and restricting natural human-machine interaction in dynamic
surgical environments. These solutions rely heavily on extensive task-specific
pre-training, fixed object categories, and explicit manual-prompting. This work
introduces a novel Perception Agent that leverages speech-integrated
prompt-engineered large language models (LLMs), segment anything model (SAM),
and any-point tracking foundation models to enable a more natural human-machine
interaction in real-time intraoperative surgical assistance. Incorporating a
memory repository and two novel mechanisms for segmenting unseen elements,
Perception Agent offers the flexibility to segment both known and unseen
elements in the surgical scene through intuitive interaction. Incorporating the
ability to memorize novel elements for use in future surgeries, this work takes
a marked step towards human-machine symbiosis in surgical procedures. Through
quantitative analysis on a public dataset, we show that the performance of our
agent is on par with considerably more labor-intensive manual-prompting
strategies. Qualitatively, we show the flexibility of our agent in segmenting
novel elements (instruments, phantom grafts, and gauze) in a custom-curated
dataset. By offering natural human-machine interaction and overcoming rigidity,
our Perception Agent potentially brings AI-based real-time assistance in
dynamic surgical environments closer to reality.

</details>


### [81] [User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals](https://arxiv.org/abs/2507.23544)
*Ryo Miyoshi,Yuki Okafuji,Takuya Iwamoto,Junya Nakanishi,Jun Baba*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于多模态社交信号（如面部表情和声音）的用户体验（UX）评估方法，通过Transformer模型捕捉短期和长期的交互模式，实验结果表明其性能优于人工评估。


<details>
  <summary>Details</summary>
Motivation: 社会机器人的需求增长，需要根据用户状态调整其行为。准确评估人机交互（HRI）中的用户体验（UX）是实现这一目标的关键。现有的UX评估方法通常过于单一，本研究旨在通过多模态信号提供一个更全面的解决方案。

Method: 研究构建了一个UX数据集，并开发了一个基于Transformer的模型，利用面部表情和声音进行UX估计。采用多实例学习框架，捕捉短期和长期的交互模式，以更好地反映UX的时间动态。

Result: 实验结果显示，该方法在UX评估上优于第三方人工评估者。

Conclusion: 提出的多模态方法能够更全面地捕捉用户体验，为社交机器人的行为适应性提供了更准确的评估工具。

Abstract: In recent years, the demand for social robots has grown, requiring them to
adapt their behaviors based on users' states. Accurately assessing user
experience (UX) in human-robot interaction (HRI) is crucial for achieving this
adaptability. UX is a multi-faceted measure encompassing aspects such as
sentiment and engagement, yet existing methods often focus on these
individually. This study proposes a UX estimation method for HRI by leveraging
multimodal social signals. We construct a UX dataset and develop a
Transformer-based model that utilizes facial expressions and voice for
estimation. Unlike conventional models that rely on momentary observations, our
approach captures both short- and long-term interaction patterns using a
multi-instance learning framework. This enables the model to capture temporal
dynamics in UX, providing a more holistic representation. Experimental results
demonstrate that our method outperforms third-party human evaluators in UX
estimation.

</details>


### [82] [Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation](https://arxiv.org/abs/2507.23592)
*Haiyun Zhang,Stefano Dalla Gasperina,Saad N. Yousaf,Toshimitsu Tsuboi,Tetsuya Narita,Ashish D. Deshpande*

Main category: cs.RO

TL;DR: 论文提出了一种针对手部外骨骼的个性化校准框架，通过冗余关节传感和残差加权优化策略，显著提升了手部跟踪的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于用户手部解剖结构的差异和佩戴不一致，现有手部外骨骼在精确跟踪方面存在挑战，限制了其在精密任务中的应用。

Method: 提出了一种基于冗余关节传感和残差加权优化的虚拟链接参数估计方法，并使用数据驱动的方式调整成本函数权重。

Result: 在七名受试者中验证，该方法显著降低了关节和指尖跟踪误差，并通过虚拟手可视化验证了运动保真度的提升。

Conclusion: 该框架适用于具有闭环运动学和最小传感的外骨骼设计，为高保真遥操作和示范学习应用奠定了基础。

Abstract: Hand exoskeletons are critical tools for dexterous teleoperation and
immersive manipulation interfaces, but achieving accurate hand tracking remains
a challenge due to user-specific anatomical variability and donning
inconsistencies. These issues lead to kinematic misalignments that degrade
tracking performance and limit applicability in precision tasks. We propose a
subject-specific calibration framework for exoskeleton-based hand tracking that
uses redundant joint sensing and a residual-weighted optimization strategy to
estimate virtual link parameters. Implemented on the Maestro exoskeleton, our
method improves joint angle and fingertip position estimation across users with
varying hand geometries. We introduce a data-driven approach to empirically
tune cost function weights using motion capture ground truth, enabling more
accurate and consistent calibration across participants. Quantitative results
from seven subjects show substantial reductions in joint and fingertip tracking
errors compared to uncalibrated and evenly weighted models. Qualitative
visualizations using a Unity-based virtual hand further confirm improvements in
motion fidelity. The proposed framework generalizes across exoskeleton designs
with closed-loop kinematics and minimal sensing, and lays the foundation for
high-fidelity teleoperation and learning-from-demonstration applications.

</details>
