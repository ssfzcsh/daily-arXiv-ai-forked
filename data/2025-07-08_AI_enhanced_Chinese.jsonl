{"id": "2507.03156", "pdf": "https://arxiv.org/pdf/2507.03156", "abs": "https://arxiv.org/abs/2507.03156", "authors": ["Amr Mohamed", "Maram Assi", "Mariam Guizani"], "title": "The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "37 pages", "summary": "Large language model assistants (LLM-assistants) present new opportunities to\ntransform software development. Developers are increasingly adopting these\ntools across tasks, including coding, testing, debugging, documentation, and\ndesign. Yet, despite growing interest, there is no synthesis of how\nLLM-assistants affect software developer productivity. In this paper, we\npresent a systematic literature review of 37 peer-reviewed studies published\nbetween January 2014 and December 2024 that examine this impact. Our analysis\nreveals that LLM-assistants offer both considerable benefits and critical\nrisks. Commonly reported gains include minimized code search, accelerated\ndevelopment, and the automation of trivial and repetitive tasks. However,\nstudies also highlight concerns around cognitive offloading, reduced team\ncollaboration, and inconsistent effects on code quality. While the majority of\nstudies (92%) adopt a multi-dimensional perspective by examining at least two\nSPACE dimensions, reflecting increased awareness of the complexity of developer\nproductivity, only 14% extend beyond three dimensions, indicating substantial\nroom for more integrated evaluations. Satisfaction, Performance, and Efficiency\nare the most frequently investigated dimensions, whereas Communication and\nActivity remain underexplored. Most studies are exploratory (64%) and\nmethodologically diverse, but lack longitudinal and team-based evaluations.\nThis review surfaces key research gaps and provides recommendations for future\nresearch and practice. All artifacts associated with this study are publicly\navailable at https://zenodo.org/records/15788502.", "AI": {"tldr": "LLM\u52a9\u624b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4f46\u4e5f\u5e26\u6765\u98ce\u9669\u548c\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u9700\u66f4\u591a\u7ef4\u5ea6\u548c\u7eb5\u5411\u8bc4\u4f30\u3002", "motivation": "\u7814\u7a76LLM\u52a9\u624b\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u7efc\u8ff037\u98792014-2024\u5e74\u7684\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\u3002", "result": "LLM\u52a9\u624b\u80fd\u52a0\u901f\u5f00\u53d1\uff0c\u4f46\u4e5f\u53ef\u80fd\u964d\u4f4e\u4ee3\u7801\u8d28\u91cf\u548c\u534f\u4f5c\uff1b\u7814\u7a76\u591a\u5173\u6ce8\u6548\u7387\uff0c\u5ffd\u89c6\u6c9f\u901a\u548c\u6d3b\u52a8\u3002", "conclusion": "\u672a\u6765\u9700\u66f4\u5168\u9762\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u56e2\u961f\u534f\u4f5c\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u9762\u3002"}}
{"id": "2507.03160", "pdf": "https://arxiv.org/pdf/2507.03160", "abs": "https://arxiv.org/abs/2507.03160", "authors": ["Md Mahade Hasan", "Muhammad Waseem", "Kai-Kristian Kemell", "Jussi Raskua", "Juha Ala-Rantalaa", "Pekka Abrahamsson"], "title": "Assessing Small Language Models for Code Generation: An Empirical Study with Benchmarks", "categories": ["cs.SE"], "comment": null, "summary": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf920\u4e2a\u5f00\u6e90\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u53d1\u73b0\u67d0\u4e9b\u5c0f\u578b\u6a21\u578b\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff1b\u800c\u66f4\u5927\u7684\u6a21\u578b\u867d\u6027\u80fd\u66f4\u597d\uff0c\u4f46\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u663e\u8457\u589e\u52a0\u3002", "motivation": "\u968f\u7740\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u7684\u53d1\u5c55\uff0c\u5176\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u5b9e\u9645\u6027\u80fd\u4e0e\u5c40\u9650\u4ecd\u9700\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e8620\u4e2a\u53c2\u6570\u4ece0.4B\u523010B\u7684\u5f00\u6e90SLMs\u5728\u4e94\u4e2a\u4ee3\u7801\u76f8\u5173\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u529f\u80fd\u6b63\u786e\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u591a\u8bed\u8a00\u6027\u80fd\u3002", "result": "\u5c0f\u578b\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u63d0\u5347\u51c6\u786e\u6027\u9700\u8f6c\u5411\u66f4\u5927\u6a21\u578b\uff0c\u540e\u8005\u6027\u80fd\u66f4\u4f18\u4f46\u8d44\u6e90\u6d88\u8017\u663e\u8457\u589e\u52a0\uff1b\u591a\u8bed\u8a00\u6027\u80fd\u5dee\u5f02\u4e0d\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b9e\u9645\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2dSLMs\u7684\u8bbe\u8ba1\u4e0e\u9009\u62e9\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2507.03263", "pdf": "https://arxiv.org/pdf/2507.03263", "abs": "https://arxiv.org/abs/2507.03263", "authors": ["Haiqiao Gu", "Yiliang Zhao", "Kai Gao", "Minghui Zhou"], "title": "Analyzing C/C++ Library Migrations at the Package-level: Prevalence, Domains, Targets and Rationals across Seven Package Management Tools", "categories": ["cs.SE"], "comment": null, "summary": "Library migration happens when a library can not meet the project's\nrequirements and is non-trivial to accomplish. To mitigate the problem,\nsubstantial efforts have been devoted to understanding its characteristics and\nrecommending alternative libraries, especially for programming language (PL)\necosystems with a central package hosting platform, such as Python (PyPI).\nHowever, to the best of our knowledge, understanding of C/C++ library\nmigrations is still lacking, possibly due to challenges resulting from the\nfragmented and complicated dependency management practices in the C/C++\necosystem. To bridge this knowledge gap, this paper analyzes 19,943 C/C++\nprojects that utilize different package management tools and establishes the\nfirst C/C++ library migration dataset. Based on the dataset, we investigate the\nprevalence, domains, target library, and rationale of C/C++ library migrations\nand compare the results with three widely investigated PLs: Python, JavaScript,\nand Java. We find that the overall trend in the number of C/C++ library\nmigrations is similar to Java. Migrations across different package management\ntools are also observed. In C/C++, library migrations mainly occur in GUI,\nBuild, and OS development, but are rare in domains (e.g., Testing and Logging)\nthat dominate library migrations in the three compared PLs. 83.46\\% of C/C++\nsource libraries only have one migration target, suggesting that our library\nmigration dataset could be used directly to recommend migration targets. We\nfind four C/C++-specific migration reasons, such as less compile time and\nunification of dependency management, revealing the unique dependency\nmanagement requirements in C/C++ projects. We believe our findings can help\nC/C++ developers make more informed library migration decisions and shed light\non the design of C/C++ library migration tools.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5efa\u7acb\u4e86C/C++\u5e93\u8fc1\u79fb\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e8619,943\u4e2a\u9879\u76ee\uff0c\u6bd4\u8f83\u4e86C/C++\u4e0e\u5176\u4ed6\u8bed\u8a00\uff08Python\u3001JavaScript\u3001Java\uff09\u7684\u5e93\u8fc1\u79fb\u7279\u70b9\uff0c\u53d1\u73b0\u5176\u8fc1\u79fb\u8d8b\u52bf\u4e0eJava\u76f8\u4f3c\uff0c\u5e76\u63ed\u793a\u4e86C/C++\u7279\u6709\u7684\u8fc1\u79fb\u539f\u56e0\u3002", "motivation": "\u586b\u8865C/C++\u751f\u6001\u7cfb\u7edf\u4e2d\u5e93\u8fc1\u79fb\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u56e0C/C++\u4f9d\u8d56\u7ba1\u7406\u590d\u6742\uff0c\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u3002", "method": "\u5206\u679019,943\u4e2aC/C++\u9879\u76ee\uff0c\u5efa\u7acb\u9996\u4e2aC/C++\u5e93\u8fc1\u79fb\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u5176\u4e0ePython\u3001JavaScript\u3001Java\u7684\u8fc1\u79fb\u7279\u70b9\u3002", "result": "C/C++\u5e93\u8fc1\u79fb\u8d8b\u52bf\u4e0eJava\u76f8\u4f3c\uff1b\u8fc1\u79fb\u4e3b\u8981\u53d1\u751f\u5728GUI\u3001Build\u548cOS\u5f00\u53d1\u9886\u57df\uff1b83.46%\u7684\u6e90\u5e93\u4ec5\u6709\u4e00\u4e2a\u8fc1\u79fb\u76ee\u6807\uff1b\u63ed\u793a\u4e86\u56db\u4e2aC/C++\u7279\u6709\u7684\u8fc1\u79fb\u539f\u56e0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8eC/C++\u5f00\u53d1\u8005\u66f4\u660e\u667a\u5730\u51b3\u7b56\u5e93\u8fc1\u79fb\uff0c\u5e76\u4e3a\u76f8\u5173\u5de5\u5177\u8bbe\u8ba1\u63d0\u4f9b\u542f\u793a\u3002"}}
{"id": "2507.03328", "pdf": "https://arxiv.org/pdf/2507.03328", "abs": "https://arxiv.org/abs/2507.03328", "authors": ["S. Lee", "C. Myers", "A. Yang", "T. Zhang", "S. J. L. Billinge"], "title": "scikit-package -- software packaging standards and roadmap for sharing reproducible scientific software", "categories": ["cs.SE"], "comment": "GitHub: https://github.com/scikit-package/scikit-package Doc:\n  https://scikit-package.github.io/scikit-package/", "summary": "Scientific advancement relies on the ability to share and reproduce results.\nWhen data analysis or calculations are carried out using software written by\nscientists there are special challenges around code versions, quality and code\nsharing. scikit-package provides a roadmap to facilitate code reuse and sharing\nwith minimal effort through tutorials coupled with automated and centralized\nreusable workflows. The goal of the project is to provide pedagogical and\npractical tools for scientists who are not professionally trained software\nengineers to write more reusable and maintainable software code. Code reuse can\noccur at multiple levels of complexity-from turning a code block into a\nfunction within a single script, to publishing a publicly installable, fully\ntested, and documented software package scikit-package provides a community\nmaintained set of tools, and a roadmap, to help scientists bring their software\nhigher levels of reproducibility and shareability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86scikit-package\u9879\u76ee\uff0c\u65e8\u5728\u901a\u8fc7\u6559\u7a0b\u548c\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5e2e\u52a9\u975e\u4e13\u4e1a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u79d1\u5b66\u5bb6\u7f16\u5199\u66f4\u53ef\u91cd\u7528\u548c\u7ef4\u62a4\u7684\u4ee3\u7801\u3002", "motivation": "\u79d1\u5b66\u8fdb\u6b65\u4f9d\u8d56\u4e8e\u7ed3\u679c\u7684\u53ef\u5171\u4eab\u548c\u53ef\u91cd\u590d\u6027\uff0c\u4f46\u79d1\u5b66\u5bb6\u7f16\u5199\u7684\u4ee3\u7801\u5728\u7248\u672c\u3001\u8d28\u91cf\u548c\u5171\u4eab\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "scikit-package\u63d0\u4f9b\u6559\u7a0b\u548c\u96c6\u4e2d\u5316\u7684\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u4ece\u7b80\u5355\u811a\u672c\u5230\u5b8c\u6574\u8f6f\u4ef6\u5305\u7684\u4ee3\u7801\u590d\u7528\u3002", "result": "\u9879\u76ee\u63d0\u4f9b\u5de5\u5177\u548c\u8def\u7ebf\u56fe\uff0c\u5e2e\u52a9\u63d0\u5347\u4ee3\u7801\u7684\u53ef\u91cd\u590d\u6027\u548c\u5171\u4eab\u6027\u3002", "conclusion": "scikit-package\u4e3a\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u7684\u5b9e\u7528\u5de5\u5177\u548c\u793e\u533a\u652f\u6301\u3002"}}
{"id": "2507.03256", "pdf": "https://arxiv.org/pdf/2507.03256", "abs": "https://arxiv.org/abs/2507.03256", "authors": ["Xinyang Li", "Gen Li", "Zhihui Lin", "Yichen Qian", "GongXin Yao", "Weinan Jia", "Weihua Chen", "Fan Wang"], "title": "MoDA: Multi-modal Diffusion Architecture for Talking Head Generation", "categories": ["cs.GR", "cs.CV"], "comment": "12 pages, 7 figures", "summary": "Talking head generation with arbitrary identities and speech audio remains a\ncrucial problem in the realm of digital humans and the virtual metaverse.\nRecently, diffusion models have become a popular generative technique in this\nfield with their strong generation and generalization capabilities. However,\nseveral challenges remain for diffusion-based methods: 1) inefficient inference\nand visual artifacts, which arise from the implicit latent space of Variational\nAuto-Encoders (VAE), complicating the diffusion process; 2) authentic facial\nexpressions and head movements, resulting from insufficient multi-modal\ninformation interaction. In this paper, MoDA handle these challenges by 1)\ndefines a joint parameter space to bridge motion generation and neural\nrendering, and leverages flow matching to simplify the diffusion learning\nprocess; 2) introduces a multi-modal diffusion architecture to model the\ninteraction among noisy motion, audio, and auxiliary conditions, ultimately\nenhancing overall facial expressiveness. Subsequently, a coarse-to-fine fusion\nstrategy is adopted to progressively integrate different modalities, ensuring\neffective integration across feature spaces. Experimental results demonstrate\nthat MoDA significantly improves video diversity, realism, and efficiency,\nmaking it suitable for real-world applications.", "AI": {"tldr": "MoDA\u901a\u8fc7\u5b9a\u4e49\u8054\u5408\u53c2\u6570\u7a7a\u95f4\u548c\u5f15\u5165\u591a\u6a21\u6001\u6269\u6563\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8bf4\u8bdd\u5934\u751f\u6210\u4e2d\u7684\u6548\u7387\u4f4e\u3001\u89c6\u89c9\u4f2a\u5f71\u548c\u8868\u60c5\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u8bf4\u8bdd\u5934\u751f\u6210\u4e2d\u6548\u7387\u4f4e\u3001\u89c6\u89c9\u4f2a\u5f71\u4ee5\u53ca\u591a\u6a21\u6001\u4fe1\u606f\u4ea4\u4e92\u4e0d\u8db3\u5bfc\u81f4\u7684\u771f\u5b9e\u611f\u95ee\u9898\u3002", "method": "1) \u5b9a\u4e49\u8054\u5408\u53c2\u6570\u7a7a\u95f4\u7ed3\u5408\u6d41\u5339\u914d\u7b80\u5316\u6269\u6563\u5b66\u4e60\uff1b2) \u5f15\u5165\u591a\u6a21\u6001\u6269\u6563\u67b6\u6784\u589e\u5f3a\u8868\u60c5\u4e30\u5bcc\u6027\uff1b3) \u91c7\u7528\u7c97\u5230\u7ec6\u878d\u5408\u7b56\u7565\u6574\u5408\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "MoDA\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u591a\u6837\u6027\u3001\u771f\u5b9e\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "MoDA\u901a\u8fc7\u4f18\u5316\u6269\u6563\u5b66\u4e60\u548c\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bf4\u8bdd\u5934\u751f\u6210\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.03510", "pdf": "https://arxiv.org/pdf/2507.03510", "abs": "https://arxiv.org/abs/2507.03510", "authors": ["Jingze Dai", "Douglas G. Down"], "title": "On Combining Two Server Control Policies for Energy Efficiency", "categories": ["cs.PF"], "comment": "Extended Abstract for LOCO '24", "summary": "Two popular server control policies are available for reducing energy\nconsumption while maintaining acceptable performance levels: server speed\nscaling and the ability to turn servers off (and on). In this work, we explore\nthe question of whether there are synergistic effects between these two\nmechanisms. To do this, we employ a continuous-time Markov chain model where\nthe server can be turned off (and turning the server back on takes some time)\nand where the speed of the server can take on two values: a nominal operating\nspeed and a reduced operating speed. For a cost function that is linear in the\nmean response time and server power consumption, we suggest that the mechanisms\nare not synergistic in that for all system loads, one mechanism is dominant in\nthat if the other mechanism is also employed, there is only a small decrease in\ncost.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u670d\u52a1\u5668\u901f\u5ea6\u8c03\u6574\u548c\u5f00\u5173\u673a\u5236\u5728\u8282\u80fd\u4e2d\u7684\u534f\u540c\u6548\u5e94\uff0c\u53d1\u73b0\u4e24\u8005\u4e4b\u95f4\u7f3a\u4e4f\u663e\u8457\u7684\u534f\u540c\u4f5c\u7528\u3002", "motivation": "\u63a2\u7d22\u670d\u52a1\u5668\u901f\u5ea6\u8c03\u6574\u548c\u5f00\u5173\u673a\u5236\u5728\u8282\u80fd\u4e2d\u662f\u5426\u5b58\u5728\u534f\u540c\u6548\u5e94\uff0c\u4ee5\u4f18\u5316\u80fd\u6e90\u6d88\u8017\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\uff0c\u670d\u52a1\u5668\u53ef\u5f00\u5173\u4e14\u901f\u5ea6\u53ef\u8c03\u4e3a\u6b63\u5e38\u6216\u4f4e\u901f\uff0c\u5206\u6790\u7ebf\u6027\u6210\u672c\u51fd\u6570\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u4e24\u79cd\u673a\u5236\u5728\u6240\u6709\u7cfb\u7edf\u8d1f\u8f7d\u4e0b\u5747\u7f3a\u4e4f\u663e\u8457\u534f\u540c\u6548\u5e94\uff0c\u5176\u4e2d\u4e00\u79cd\u673a\u5236\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u53e6\u4e00\u79cd\u4ec5\u5e26\u6765\u5c0f\u5e45\u6210\u672c\u964d\u4f4e\u3002", "conclusion": "\u670d\u52a1\u5668\u8282\u80fd\u673a\u5236\u7684\u9009\u62e9\u5e94\u4ee5\u5355\u4e00\u7684\u5360\u4f18\u7b56\u7565\u4e3a\u4e3b\uff0c\u7ed3\u5408\u53e6\u4e00\u79cd\u673a\u5236\u5e26\u6765\u7684\u989d\u5916\u6536\u76ca\u6709\u9650\u3002"}}
{"id": "2507.03158", "pdf": "https://arxiv.org/pdf/2507.03158", "abs": "https://arxiv.org/abs/2507.03158", "authors": ["Jit Gupta", "Tarun Banka", "Rahul Gupta", "Mithun Dharmaraj", "Jasleen Kaur"], "title": "An End-to-End Assurance Framework for AI/ML Workloads in Datacenters", "categories": ["cs.NI"], "comment": "2 page, Poster/Demo in IEEE Infocom 2025, May 19-22, London , UK", "summary": "Modern machine learning workloads such as large language model training,\nfine-tuning jobs are highly distributed and span across hundreds of systems\nwith multiple GPUs. Job completion time for these workloads is the artifact of\nthe application, compute, network and storage performance. In case of failure\nor degraded performance it is imperative to understand the root cause and\npossible remediation for the problem for end-to-end assurance. This demo\nshowcases SaaSbased observability and automated troubleshooting for AI/ML\nworkload performance issues using cross-layer telemetry and logs (e.g.,\nApplication telemetry, Collective communication logs, GPU Health metrics,\nNetwork Flow Data, NIC ROCEv2 telemetry). Different use cases are demonstrated\nfor end-to-end assurance such as Cross-layer Dependency Graph, Cross-layer\nService Level Expectations, Automated Root Cause Analysis, GPU-toGPU\napplication path tracing.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u57fa\u4e8eSaaS\u7684AI/ML\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\u76d1\u63a7\u4e0e\u81ea\u52a8\u5316\u6545\u969c\u6392\u9664\uff0c\u5229\u7528\u8de8\u5c42\u9065\u6d4b\u6570\u636e\u89e3\u51b3\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u5206\u5e03\u5f0fAI/ML\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u95ee\u9898\u590d\u6742\uff0c\u9700\u8de8\u5c42\u5206\u6790\u4ee5\u5feb\u901f\u5b9a\u4f4d\u6839\u6e90\u5e76\u63d0\u4f9b\u4fee\u590d\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u5e94\u7528\u9065\u6d4b\u3001\u901a\u4fe1\u65e5\u5fd7\u3001GPU\u5065\u5eb7\u6307\u6807\u7b49\u8de8\u5c42\u6570\u636e\uff0c\u6784\u5efa\u4f9d\u8d56\u56fe\u548c\u81ea\u52a8\u5316\u6839\u56e0\u5206\u6790\u5de5\u5177\u3002", "result": "\u5c55\u793a\u4e86\u591a\u79cd\u7528\u4f8b\uff0c\u5982\u8de8\u5c42\u4f9d\u8d56\u56fe\u3001\u670d\u52a1\u7ea7\u522b\u671f\u671b\u548cGPU\u95f4\u5e94\u7528\u8def\u5f84\u8ffd\u8e2a\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u6027\u80fd\u4fdd\u969c\u3002", "conclusion": "SaaS\u5316\u76d1\u63a7\u4e0e\u81ea\u52a8\u5316\u5de5\u5177\u80fd\u6709\u6548\u63d0\u5347\u5206\u5e03\u5f0fAI/ML\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u53ef\u9760\u6027\u548c\u6545\u969c\u6062\u590d\u6548\u7387\u3002"}}
{"id": "2507.03208", "pdf": "https://arxiv.org/pdf/2507.03208", "abs": "https://arxiv.org/abs/2507.03208", "authors": ["Daniel Ranalter", "Cezary Kaliszyk", "Florian Rabe", "Geoff Sutcliffe"], "title": "The Dependently Typed Higher-Order Form for the TPTP World", "categories": ["cs.LO", "F.4.1"], "comment": "16 pages excluding references, to be published in the proceedings of\n  FroCoS 25", "summary": "Much of the current research and development in the field of automated\nreasoning builds on the infrastructure provided by the TPTP World. The TPTP\nlanguage for logical formulae is central to the far-reaching adoption of the\nTPTP World. This paper introduces the Dependently Typed higher-order Form (DTF)\nof the TPTP language. It takes advantage of already established binders in the\nsyntax, and is thus a minimally intrusive extension to the Typed Higher-order\nForm (THF). A starting set of over 100 problems is provided to exhibit the\nusefulness and incite interest in DTF. Some tools that are already able to\nreason about problems in the DTF language are discussed.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TPTP\u8bed\u8a00\u4e2d\u7684\u4f9d\u8d56\u7c7b\u578b\u9ad8\u9636\u5f62\u5f0f\uff08DTF\uff09\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7684THF\u5f62\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u5957\u95ee\u9898\u96c6\u4ee5\u5c55\u793a\u5176\u5b9e\u7528\u6027\u3002", "motivation": "TPTP\u8bed\u8a00\u5728\u81ea\u52a8\u63a8\u7406\u9886\u57df\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7DTF\u6269\u5c55\u5176\u529f\u80fd\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u73b0\u6709\u8bed\u6cd5\u7684\u6539\u52a8\u3002", "method": "\u5229\u7528TPTP\u8bed\u8a00\u4e2d\u5df2\u6709\u7684\u7ed1\u5b9a\u5668\uff0c\u63d0\u51fa\u4e86DTF\u5f62\u5f0f\uff0c\u4f5c\u4e3aTHF\u7684\u6269\u5c55\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u5957\u5305\u542b100\u591a\u4e2a\u95ee\u9898\u7684\u96c6\u5408\uff0c\u5c55\u793a\u4e86DTF\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e00\u4e9b\u652f\u6301DTF\u7684\u5de5\u5177\u3002", "conclusion": "DTF\u662f\u4e00\u79cd\u6709\u6548\u7684\u6269\u5c55\uff0c\u80fd\u591f\u63a8\u52a8TPTP\u8bed\u8a00\u7684\u8fdb\u4e00\u6b65\u5e94\u7528\u548c\u53d1\u5c55\u3002"}}
{"id": "2507.02865", "pdf": "https://arxiv.org/pdf/2507.02865", "abs": "https://arxiv.org/abs/2507.02865", "authors": ["Dinuo Liao", "James Derek Lomas", "Cehao Yu"], "title": "Enhancing the Aesthetic Appeal of AI-Generated Physical Product Designs through LoRA Fine-Tuning with Human Feedback", "categories": ["cs.HC", "cs.AI"], "comment": "6 pages, 7 figures. Submitted to AIGC2024", "summary": "This study explores how Low-Rank Adaptation (LoRA) fine-tuning, guided by\nhuman aesthetic evaluations, can enhance the outputs of generative AI models in\ntangible product design, using lamp design as a case study. By integrating\nhuman feedback into the AI model, we aim to improve both the desirability and\naesthetic appeal of the generated designs. Comprehensive experiments were\nconducted, starting with prompt optimization techniques and focusing on LoRA\nfine-tuning of the Stable Diffusion model. Additionally, methods to convert\nAI-generated designs into tangible products through 3D realization using 3D\nprinting technologies were investigated. The results indicate that LoRA\nfine-tuning effectively aligns AI-generated designs with human aesthetic\npreferences, leading to significant improvements in desirability and aesthetic\nappeal scores. These findings highlight the potential of human-AI collaboration\nin tangible product design and provide valuable insights into integrating human\nfeedback into AI design processes.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4eba\u7c7b\u5ba1\u7f8e\u8bc4\u4f30\u6307\u5bfc\u7684LoRA\u5fae\u8c03\u53ef\u4ee5\u63d0\u5347\u751f\u6210\u5f0fAI\u5728\u706f\u5177\u8bbe\u8ba1\u4e2d\u7684\u8f93\u51fa\u6548\u679c\uff0c\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u663e\u8457\u63d0\u9ad8\u8bbe\u8ba1\u7684\u5438\u5f15\u529b\u548c\u7f8e\u5b66\u8bc4\u5206\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4eba\u7c7b\u5ba1\u7f8e\u53cd\u9988\u6307\u5bfcLoRA\u5fae\u8c03\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u5f0fAI\u5728\u5b9e\u7269\u4ea7\u54c1\u8bbe\u8ba1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u7f8e\u5b66\u5438\u5f15\u529b\u548c\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u4e86LoRA\u5fae\u8c03Stable Diffusion\u6a21\u578b\uff0c\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u548c\u63d0\u793a\u4f18\u5316\u6280\u672f\uff0c\u5e76\u7814\u7a76\u5982\u4f55\u901a\u8fc73D\u6253\u5370\u5c06AI\u8bbe\u8ba1\u8f6c\u5316\u4e3a\u5b9e\u7269\u4ea7\u54c1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLoRA\u5fae\u8c03\u80fd\u6709\u6548\u5c06AI\u8bbe\u8ba1\u4e0e\u4eba\u7c7b\u5ba1\u7f8e\u504f\u597d\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u8bbe\u8ba1\u7684\u7f8e\u5b66\u8bc4\u5206\u548c\u5438\u5f15\u529b\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u5728\u5b9e\u7269\u4ea7\u54c1\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6574\u5408\u4eba\u7c7b\u53cd\u9988\u5230AI\u8bbe\u8ba1\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2507.03540", "pdf": "https://arxiv.org/pdf/2507.03540", "abs": "https://arxiv.org/abs/2507.03540", "authors": ["Philip D\u00f6bler", "Manpreet Singh Jattana"], "title": "A Survey on Integrating Quantum Computers into High Performance Computing Systems", "categories": ["cs.ET", "quant-ph"], "comment": null, "summary": "Quantum computers use quantum mechanical phenomena to perform conventionally\nintractable calculations for specific problems. Despite being universal\nmachines, quantum computers are not expected to replace classical computers,\nbut rather, to complement them and form hybrid systems. This makes integrating\nquantum computers into high performance computing (HPC) systems an increasingly\nrelevant topic. We present a structured literature review on the integration\naspect. We methodologically search literature databases and manually evaluate\n107 publications. These publications are divided into seven categories that\ndescribe the state of the art in each category. After a brief quantitative\nanalysis of the literature, this survey deals with the hardware architecture of\nhybrid quantum-classical systems, as well as the software stack. We observe the\ndevelopment of a wide range of tools enabling hybrid systems and emphasize the\nneed for future standardization of interfaces and methods to foster synergy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0e\u7ecf\u5178\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u96c6\u6210\u8fdb\u884c\u4e86\u7ed3\u6784\u5316\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u786c\u4ef6\u67b6\u6784\u548c\u8f6f\u4ef6\u6808\uff0c\u5f3a\u8c03\u672a\u6765\u9700\u6807\u51c6\u5316\u63a5\u53e3\u548c\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u673a\u867d\u65e0\u6cd5\u66ff\u4ee3\u7ecf\u5178\u8ba1\u7b97\u673a\uff0c\u4f46\u80fd\u4e0e\u7ecf\u5178\u7cfb\u7edf\u4e92\u8865\u5f62\u6210\u6df7\u5408\u7cfb\u7edf\uff0c\u7814\u7a76\u5176\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u7684\u96c6\u6210\u4e3a\u5f53\u524d\u70ed\u70b9\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u68c0\u7d22\u548c\u624b\u52a8\u8bc4\u4f30107\u7bc7\u51fa\u7248\u7269\uff0c\u5206\u4e3a\u4e03\u7c7b\u63cf\u8ff0\u5404\u9886\u57df\u73b0\u72b6\uff0c\u5e76\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u548c\u786c\u4ef6\u3001\u8f6f\u4ef6\u6808\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u591a\u79cd\u652f\u6301\u6df7\u5408\u7cfb\u7edf\u7684\u5de5\u5177\uff0c\u5e76\u6307\u51fa\u9700\u5bf9\u63a5\u53e3\u548c\u65b9\u6cd5\u8fdb\u884c\u6807\u51c6\u5316\u4ee5\u4fc3\u8fdb\u534f\u540c\u53d1\u5c55\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u91cf\u5b50\u4e0e\u7ecf\u5178\u7cfb\u7edf\u96c6\u6210\u7684\u73b0\u72b6\uff0c\u5f3a\u8c03\u672a\u6765\u6807\u51c6\u5316\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2507.03629", "pdf": "https://arxiv.org/pdf/2507.03629", "abs": "https://arxiv.org/abs/2507.03629", "authors": ["S\u00e9rgio Queiroz de Medeiros", "Fabio Mascarenhas"], "title": "Towards Automatic Error Recovery in Parsing Expression", "categories": ["cs.PL", "cs.FL", "F.4.3; D.3.1; D.3.4"], "comment": "arXiv admin note: substantial text overlap with arXiv:1905.02145", "summary": "Error recovery is an essential feature for a parser that should be plugged in\nIntegrated Development Environments (IDEs), which must build Abstract Syntax\nTrees (ASTs) even for syntactically invalid programs in order to offer features\nsuch as automated refactoring and code completion.\n  Parsing Expressions Grammars (PEGs) are a formalism that naturally describes\nrecursive top-down parsers using a restricted form of backtracking. Labeled\nfailures are a conservative extension of PEGs that adds an error reporting\nmechanism for PEG parsers, and these labels can also be associated with\nrecovery expressions to also be an error recovery mechanism. These expressions\ncan use the full expressivity of PEGs to recover from syntactic errors.\n  Manually annotating a large grammar with labels and recovery expressions can\nbe difficult. In this work, we present an algorithm that automatically\nannotates a PEG with labels, and builds their corresponding recovery\nexpressions. We evaluate this algorithm by adding error recovery to the parser\nof the Titan programming language. The results shown that with a small amount\nof manual intervention our algorithm can be used to produce error recovering\nparsers for PEGs where most of the alternatives are disjoint.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u4e3aPEG\u8bed\u6cd5\u6dfb\u52a0\u6807\u7b7e\u548c\u6062\u590d\u8868\u8fbe\u5f0f\u7684\u7b97\u6cd5\uff0c\u4ee5\u7b80\u5316\u9519\u8bef\u6062\u590d\u89e3\u6790\u5668\u7684\u5f00\u53d1\u3002\u5728Titan\u8bed\u8a00\u7684\u89e3\u6790\u5668\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u6548\u679c\u826f\u597d\u3002", "motivation": "\u96c6\u6210\u5f00\u53d1\u73af\u5883\uff08IDE\uff09\u4e2d\u7684\u89e3\u6790\u5668\u9700\u8981\u652f\u6301\u9519\u8bef\u6062\u590d\u529f\u80fd\uff0c\u4ee5\u4fbf\u5373\u4f7f\u7a0b\u5e8f\u6709\u8bed\u6cd5\u9519\u8bef\u4e5f\u80fd\u6784\u5efa\u62bd\u8c61\u8bed\u6cd5\u6811\u3002\u624b\u52a8\u4e3aPEG\u8bed\u6cd5\u6dfb\u52a0\u6807\u7b7e\u548c\u6062\u590d\u8868\u8fbe\u5f0f\u5f88\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\uff0c\u81ea\u52a8\u4e3aPEG\u8bed\u6cd5\u6dfb\u52a0\u6807\u7b7e\u5e76\u6784\u5efa\u5bf9\u5e94\u7684\u6062\u590d\u8868\u8fbe\u5f0f\u3002\u5728Titan\u7f16\u7a0b\u8bed\u8a00\u7684\u89e3\u6790\u5668\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u7b97\u6cd5\u5728Titan\u8bed\u8a00\u7684\u89e3\u6790\u5668\u4e2d\u6548\u679c\u826f\u597d\uff0c\u53ea\u9700\u5c11\u91cf\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u5b9e\u73b0\u9519\u8bef\u6062\u590d\u529f\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u591a\u6570\u5907\u9009\u65b9\u6848\u4e0d\u91cd\u53e0\u7684PEG\u8bed\u6cd5\u3002", "conclusion": "\u81ea\u52a8\u5316\u7b97\u6cd5\u80fd\u591f\u663e\u8457\u7b80\u5316PEG\u8bed\u6cd5\u4e2d\u9519\u8bef\u6062\u590d\u89e3\u6790\u5668\u7684\u5f00\u53d1\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4e0d\u91cd\u53e0\u7684\u8bed\u6cd5\u7ed3\u6784\uff0c\u4e3aIDE\u4e2d\u7684\u89e3\u6790\u5668\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.02871", "pdf": "https://arxiv.org/pdf/2507.02871", "abs": "https://arxiv.org/abs/2507.02871", "authors": ["Kia Silverbrook"], "title": "ZettaLith: An Architectural Exploration of Extreme-Scale AI Inference Acceleration", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.LG"], "comment": "53 pages, 15 figures, 23 tables", "summary": "The high computational cost and power consumption of current and anticipated\nAI systems present a major challenge for widespread deployment and further\nscaling. Current hardware approaches face fundamental efficiency limits. This\npaper introduces ZettaLith, a scalable computing architecture designed to\nreduce the cost and power of AI inference by over 1,000x compared to current\nGPU-based systems. Based on architectural analysis and technology projections,\na single ZettaLith rack could potentially achieve 1.507 zettaFLOPS in 2027 -\nrepresenting a theoretical 1,047x improvement in inference performance, 1,490x\nbetter power efficiency, and could be 2,325x more cost-effective than current\nleading GPU racks for FP4 transformer inference. The ZettaLith architecture\nachieves these gains by abandoning general purpose GPU applications, and via\nthe multiplicative effect of numerous co-designed architectural innovations\nusing established digital electronic technologies, as detailed in this paper.\nZettaLith's core architectural principles scale down efficiently to exaFLOPS\ndesktop systems and petaFLOPS mobile chips, maintaining their roughly 1,000x\nadvantage. ZettaLith presents a simpler system architecture compared to the\ncomplex hierarchy of current GPU clusters. ZettaLith is optimized exclusively\nfor AI inference and is not applicable for AI training.", "AI": {"tldr": "ZettaLith\u662f\u4e00\u79cd\u521b\u65b0\u7684\u8ba1\u7b97\u67b6\u6784\uff0c\u65e8\u5728\u901a\u8fc7\u8d85\u8fc71000\u500d\u7684AI\u63a8\u7406\u6210\u672c\u548c\u529f\u8017\u964d\u4f4e\uff0c\u7a81\u7834\u73b0\u6709GPU\u7cfb\u7edf\u7684\u6548\u7387\u9650\u5236\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u7684\u8ba1\u7b97\u6210\u672c\u548c\u529f\u8017\u5df2\u6210\u4e3a\u5e7f\u6cdb\u90e8\u7f72\u548c\u6269\u5c55\u7684\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u786c\u4ef6\u5b58\u5728\u6839\u672c\u6548\u7387\u9650\u5236\u3002", "method": "ZettaLith\u901a\u8fc7\u653e\u5f03\u901a\u7528GPU\u5e94\u7528\uff0c\u7ed3\u5408\u591a\u9879\u534f\u540c\u8bbe\u8ba1\u7684\u67b6\u6784\u521b\u65b0\uff0c\u5229\u7528\u6210\u719f\u7684\u6570\u5b57\u7535\u5b50\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "result": "\u9884\u8ba1\u52302027\u5e74\uff0c\u5355\u4e2aZettaLith\u673a\u67b6\u53ef\u80fd\u5b9e\u73b01.507 zettaFLOPS\uff0c\u63a8\u7406\u6027\u80fd\u63d0\u53471047\u500d\uff0c\u80fd\u6548\u63d0\u53471490\u500d\uff0c\u6210\u672c\u6548\u76ca\u63d0\u53472325\u500d\u3002", "conclusion": "ZettaLith\u4e3aAI\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4e13\u7528\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u4ece\u684c\u9762\u5230\u79fb\u52a8\u8bbe\u5907\u7684\u591a\u79cd\u89c4\u6a21\uff0c\u4f46\u5176\u4e0d\u9002\u7528\u4e8eAI\u8bad\u7ec3\u3002"}}
{"id": "2507.03384", "pdf": "https://arxiv.org/pdf/2507.03384", "abs": "https://arxiv.org/abs/2507.03384", "authors": ["Suchen Liu", "Jun Gao", "Yinjun Han", "Yang Lin"], "title": "LLM4Hint: Leveraging Large Language Models for Hint Recommendation in Offline Query Optimization", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Query optimization is essential for efficient SQL query execution in DBMS,\nand remains attractive over time due to the growth of data volumes and advances\nin hardware. Existing traditional optimizers struggle with the cumbersome\nhand-tuning required for complex workloads, and the learning-based methods face\nlimitations in ensuring generalization. With the great success of Large\nLanguage Model (LLM) across diverse downstream tasks, this paper explores how\nLLMs can be incorporated to enhance the generalization of learned optimizers.\nThough promising, such an incorporation still presents challenges, mainly\nincluding high model inference latency, and the substantial fine-tuning cost\nand suboptimal performance due to inherent discrepancy between the token\nsequences in LLM and structured SQL execution plans with rich numerical\nfeatures.\n  In this paper, we focus on recurring queries in offline optimization to\nalleviate the issue of high inference latency, and propose \\textbf{LLM4Hint}\nthat leverages moderate-sized backbone LLMs to recommend query optimization\nhints. LLM4Hint achieves the goals through: (i) integrating a lightweight model\nto produce a soft prompt, which captures the data distribution in DBMS and the\nSQL predicates to provide sufficient optimization features while simultaneously\nreducing the context length fed to the LLM, (ii) devising a query rewriting\nstrategy using a larger commercial LLM, so as to simplify SQL semantics for the\nbackbone LLM and reduce fine-tuning costs, and (iii) introducing an explicit\nmatching prompt to facilitate alignment between the LLM and the lightweight\nmodel, which can accelerate convergence of the combined model. Experiments show\nthat LLM4Hint, by leveraging the LLM's stronger capability to understand the\nquery statement, can outperform the state-of-the-art learned optimizers in\nterms of both effectiveness and generalization.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faLLM4Hint\uff0c\u5229\u7528\u4e2d\u7b49\u89c4\u6a21\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3aSQL\u67e5\u8be2\u4f18\u5316\u63d0\u4f9b\u63d0\u793a\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u3001\u67e5\u8be2\u6539\u5199\u7b56\u7565\u548c\u5339\u914d\u63d0\u793a\uff0c\u63d0\u9ad8\u4f18\u5316\u5668\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u4f18\u5316\u5668\u9700\u8981\u590d\u6742\u7684\u4eba\u5de5\u8c03\u4f18\uff0c\u800c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u8bba\u6587\u63a2\u7d22\u5982\u4f55\u5229\u7528LLM\u589e\u5f3a\u5b66\u4e60\u4f18\u5316\u5668\u7684\u6cdb\u5316\u6027\u3002", "method": "LLM4Hint\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6a21\u578b\u751f\u6210\u8f6f\u63d0\u793a\u3001\u5546\u7528LLM\u8fdb\u884c\u67e5\u8be2\u6539\u5199\uff0c\u5e76\u5f15\u5165\u5339\u914d\u63d0\u793a\u6765\u52a0\u901f\u6a21\u578b\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM4Hint\u5728\u6548\u679c\u548c\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u4f18\u5316\u5668\u3002", "conclusion": "LLM4Hint\u901a\u8fc7LLM\u7684\u5f3a\u5927\u7406\u89e3\u80fd\u529b\uff0c\u6210\u529f\u63d0\u5347\u4e86\u67e5\u8be2\u4f18\u5316\u7684\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u4e86\u9ad8\u5ef6\u8fdf\u548c\u8c03\u4f18\u6210\u672c\u95ee\u9898\u3002"}}
{"id": "2507.03255", "pdf": "https://arxiv.org/pdf/2507.03255", "abs": "https://arxiv.org/abs/2507.03255", "authors": ["Zedong Peng", "Zeju Li", "Mingzhe Gao", "Qiang Xu", "Chen Zhang", "Jieru Zhao"], "title": "ForgeHLS: A Large-Scale, Open-Source Dataset for High-Level Synthesis", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "We introduce ForgeEDA, an open-source comprehensive circuit dataset across\nvarious categories. ForgeEDA includes diverse circuit representations such as\nRegister Transfer Level (RTL) code, Post-mapping (PM) netlists, And-Inverter\nGraphs (AIGs), and placed netlists, enabling comprehensive analysis and\ndevelopment. We demonstrate ForgeEDA's utility by benchmarking state-of-the-art\nEDA algorithms on critical tasks such as Power, Performance, and Area (PPA)\noptimization, highlighting its ability to expose performance gaps and drive\nadvancements. Additionally, ForgeEDA's scale and diversity facilitate the\ntraining of AI models for EDA tasks, demonstrating its potential to improve\nmodel performance and generalization. By addressing limitations in existing\ndatasets, ForgeEDA aims to catalyze breakthroughs in modern IC design and\nsupport the next generation of innovations in EDA.", "AI": {"tldr": "ForgeEDA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u7efc\u5408\u7535\u8def\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u7535\u8def\u8868\u793a\u5f62\u5f0f\uff0c\u652f\u6301EDA\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u548cAI\u6a21\u578b\u8bad\u7ec3\uff0c\u65e8\u5728\u63a8\u52a8IC\u8bbe\u8ba1\u548cEDA\u9886\u57df\u7684\u521b\u65b0\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0cForgeEDA\u65e8\u5728\u63a8\u52a8\u73b0\u4ee3IC\u8bbe\u8ba1\u7684\u7a81\u7834\u5e76\u652f\u6301EDA\u9886\u57df\u7684\u4e0b\u4e00\u4ee3\u521b\u65b0\u3002", "method": "ForgeEDA\u63d0\u4f9b\u4e86\u591a\u79cd\u7535\u8def\u8868\u793a\u5f62\u5f0f\uff08\u5982RTL\u4ee3\u7801\u3001PM\u7f51\u8868\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u548cAI\u6a21\u578b\u8bad\u7ec3\u5c55\u793a\u4e86\u5176\u6548\u7528\u3002", "result": "ForgeEDA\u5728PPA\u4f18\u5316\u7b49\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u80fd\u529b\uff0c\u80fd\u591f\u66b4\u9732\u6027\u80fd\u5dee\u8ddd\u5e76\u63a8\u52a8\u6280\u672f\u8fdb\u5c55\u3002", "conclusion": "ForgeEDA\u901a\u8fc7\u5176\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0c\u6709\u6f5c\u529b\u6539\u8fdbAI\u6a21\u578b\u6027\u80fd\u5e76\u63a8\u52a8EDA\u9886\u57df\u7684\u7a81\u7834\u3002"}}
{"id": "2507.03405", "pdf": "https://arxiv.org/pdf/2507.03405", "abs": "https://arxiv.org/abs/2507.03405", "authors": ["Krishna Ronanki", "Simon Arvidsson", "Johan Axell"], "title": "Prompt Engineering Guidelines for Using Large Language Models in Requirements Engineering", "categories": ["cs.SE"], "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "The rapid emergence of generative AI models like Large Language Models (LLMs)\nhas demonstrated its utility across various activities, including within\nRequirements Engineering (RE). Ensuring the quality and accuracy of\nLLM-generated output is critical, with prompt engineering serving as a key\ntechnique to guide model responses. However, existing literature provides\nlimited guidance on how prompt engineering can be leveraged, specifically for\nRE activities. The objective of this study is to explore the applicability of\nexisting prompt engineering guidelines for the effective usage of LLMs within\nRE. To achieve this goal, we began by conducting a systematic review of primary\nliterature to compile a non-exhaustive list of prompt engineering guidelines.\nThen, we conducted interviews with RE experts to present the extracted\nguidelines and gain insights on the advantages and limitations of their\napplication within RE. Our literature review indicates a shortage of prompt\nengineering guidelines for domain-specific activities, specifically for RE. Our\nproposed mapping contributes to addressing this shortage. We conclude our study\nby identifying an important future line of research within this field.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u73b0\u6709\u7684\u63d0\u793a\u5de5\u7a0b\u6307\u5357\u5e94\u7528\u4e8e\u9700\u6c42\u5de5\u7a0b(RE)\u4e2d\uff0c\u586b\u8865\u4e86\u9886\u57df\u7279\u5b9a\u6307\u5357\u7684\u4e0d\u8db3\u3002", "motivation": "\u751f\u6210\u5f0fAI(\u5982\u5927\u8bed\u8a00\u6a21\u578b)\u5728\u9700\u6c42\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u7684\u63d0\u793a\u5de5\u7a0b\u6307\u5357\u4ee5\u786e\u4fdd\u8f93\u51fa\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u6c47\u7f16\u63d0\u793a\u5de5\u7a0b\u6307\u5357\uff0c\u5e76\u8bbf\u8c08RE\u4e13\u5bb6\u8bc4\u4f30\u5176\u9002\u7528\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0RE\u9886\u57df\u7f3a\u4e4f\u7279\u5b9a\u63d0\u793a\u5de5\u7a0b\u6307\u5357\uff0c\u63d0\u51fa\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u7f3a\u7684\u6620\u5c04\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u5728RE\u4e2d\u4f18\u5316\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2507.03731", "pdf": "https://arxiv.org/pdf/2507.03731", "abs": "https://arxiv.org/abs/2507.03731", "authors": ["Dale Decatur", "Itai Lang", "Kfir Aberman", "Rana Hanocka"], "title": "3D PixBrush: Image-Guided Local Texture Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present 3D PixBrush, a method for performing image-driven edits of local\nregions on 3D meshes. 3D PixBrush predicts a localization mask and a\nsynthesized texture that faithfully portray the object in the reference image.\nOur predicted localizations are both globally coherent and locally precise.\nGlobally - our method contextualizes the object in the reference image and\nautomatically positions it onto the input mesh. Locally - our method produces\nmasks that conform to the geometry of the reference image. Notably, our method\ndoes not require any user input (in the form of scribbles or bounding boxes) to\nachieve accurate localizations. Instead, our method predicts a localization\nmask on the 3D mesh from scratch. To achieve this, we propose a modification to\nthe score distillation sampling technique which incorporates both the predicted\nlocalization and the reference image, referred to as localization-modulated\nimage guidance. We demonstrate the effectiveness of our proposed technique on a\nwide variety of meshes and images.", "AI": {"tldr": "3D PixBrush\u662f\u4e00\u79cd\u65e0\u9700\u7528\u6237\u8f93\u5165\u5373\u53ef\u57283D\u7f51\u683c\u4e0a\u5b9e\u73b0\u56fe\u50cf\u9a71\u52a8\u5c40\u90e8\u7f16\u8f91\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u5b9a\u4f4d\u63a9\u6a21\u548c\u5408\u6210\u7eb9\u7406\uff0c\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u4e14\u5c40\u90e8\u7cbe\u786e\u7684\u7f16\u8f91\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u57283D\u7f51\u683c\u4e0a\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u7cbe\u786e\u7684\u56fe\u50cf\u9a71\u52a8\u5c40\u90e8\u7f16\u8f91\uff0c\u907f\u514d\u4f9d\u8d56\u7528\u6237\u8f93\u5165\uff08\u5982\u6d82\u9e26\u6216\u8fb9\u754c\u6846\uff09\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u5206\u6570\u84b8\u998f\u91c7\u6837\u6280\u672f\uff0c\u7ed3\u5408\u9884\u6d4b\u7684\u5b9a\u4f4d\u63a9\u6a21\u548c\u53c2\u8003\u56fe\u50cf\uff08\u79f0\u4e3a\u5b9a\u4f4d\u8c03\u5236\u56fe\u50cf\u5f15\u5bfc\uff09\uff0c\u81ea\u52a8\u751f\u6210\u5168\u5c40\u4e00\u81f4\u4e14\u5c40\u90e8\u7cbe\u786e\u7684\u63a9\u6a21\u548c\u7eb9\u7406\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u7f51\u683c\u548c\u56fe\u50cf\u4e0a\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u5c40\u90e8\u7f16\u8f91\u3002", "conclusion": "3D PixBrush\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u7528\u6237\u5e72\u9884\u7684\u9ad8\u65483D\u7f51\u683c\u5c40\u90e8\u7f16\u8f91\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.03537", "pdf": "https://arxiv.org/pdf/2507.03537", "abs": "https://arxiv.org/abs/2507.03537", "authors": ["Xiangxiang Li", "Haiyan Wang", "Yao Ge", "Xiaohong Shen", "Yong Liang Guan", "Miaowen Wen", "Chau Yuen"], "title": "Affine Frequency Division Multiplexing Over Wideband Doubly-Dispersive Channels With Time-Scaling Effects", "categories": ["cs.PF", "cs.IT", "math.IT"], "comment": null, "summary": "The recently proposed affine frequency division multiplexing (AFDM)\nmodulation has been considered as a promising technology for narrowband\ndoubly-dispersive channels. However, the time-scaling effects, i.e., pulse\nwidening and pulse shortening phenomena, in extreme wideband doubly-dispersive\nchannels have not been considered in the literatures. In this paper, we\ninvestigate such wideband transmission and develop an efficient transmission\nstructure with chirp-periodic prefix (CPP) and chirp-periodic suffix (CPS) for\nAFDM system. We derive the input-output relationship of AFDM system under\ntime-scaled wideband doubly-dispersive channels and demonstrate the sparsity in\ndiscrete affine Fourier (DAF) domain equivalent channels. We further optimize\nthe AFDM chirp parameters to accommodate the time-scaling characteristics in\nwideband doubly-dispersive channels and verify the superiority of the derived\nchirp parameters by pairwise error probability (PEP) analysis. We also develop\nan efficient cross domain distributed orthogonal approximate message passing\n(CD-D-OAMP) algorithm for AFDM symbol detection and analyze its corresponding\nstate evolution. By analyzing the detection complexity of CD-D-OAMP detector\nand evaluating the error performance of AFDM systems based on simulations, we\ndemonstrate that the AFDM system with our optimized chirp parameters\noutperforms the existing competitive modulation schemes in time-scaled wideband\ndoubly-dispersive channels. Moreover, our proposed CD-D-OAMP detector can\nachieve the desirable trade-off between the complexity and performance, while\nsupporting parallel computing to significantly reduce the computational\nlatency.", "AI": {"tldr": "\u7814\u7a76\u4e86AFDM\u8c03\u5236\u5728\u8d85\u5bbd\u5e26\u53cc\u5f25\u6563\u4fe1\u9053\u4e2d\u7684\u65f6\u95f4\u7f29\u653e\u6548\u5e94\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4f20\u8f93\u7ed3\u6784\uff0c\u5e76\u4f18\u5316\u4e86AFDM\u7cfb\u7edf\u53c2\u6570\u548c\u68c0\u6d4b\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684AFDM\u8c03\u5236\u6280\u672f\u5728\u7a84\u5e26\u53cc\u5f25\u6563\u4fe1\u9053\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8d85\u5bbd\u5e26\u4fe1\u9053\u4e2d\u7684\u65f6\u95f4\u7f29\u653e\u6548\u5e94\u672a\u88ab\u7814\u7a76\uff0c\u9700\u63a2\u7d22\u5176\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u5e26\u6709CPP\u548cCPS\u7684\u4f20\u8f93\u7ed3\u6784\uff0c\u4f18\u5316AFDM\u7684\u8c03\u9891\u53c2\u6570\uff0c\u5e76\u5f00\u53d1CD-D-OAMP\u68c0\u6d4b\u7b97\u6cd5\u53ca\u5176\u72b6\u6001\u6f14\u5316\u5206\u6790\u3002", "result": "\u4f18\u5316\u7684AFDM\u7cfb\u7edf\u5728\u8d85\u5bbd\u5e26\u4fe1\u9053\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0cCD-D-OAMP\u68c0\u6d4b\u5668\u5b9e\u73b0\u4e86\u590d\u6742\u5ea6\u548c\u6027\u80fd\u7684\u7406\u60f3\u5e73\u8861\u3002", "conclusion": "AFDM\u7cfb\u7edf\u5728\u8d85\u5bbd\u5e26\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f18\u5316\u7684\u53c2\u6570\u548c\u68c0\u6d4b\u7b97\u6cd5\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002"}}
{"id": "2507.03224", "pdf": "https://arxiv.org/pdf/2507.03224", "abs": "https://arxiv.org/abs/2507.03224", "authors": ["Alexander Shan", "Jasleen Kaur", "Rahul Singh", "Tarun Banka", "Raj Yavatkar", "T. Sridhar"], "title": "RCA Copilot: Transforming Network Data into Actionable Insights via Large Language Models", "categories": ["cs.NI"], "comment": "6 page, IEEE ICC 2025, Jun 8 12, 2025, Montreal, Canada", "summary": "Ensuring the reliability and availability of complex networked services\ndemands effective root cause analysis (RCA) across cloud environments, data\ncenters, and on-premises networks. Traditional RCA methods, which involve\nmanual inspection of data sources such as logs and telemetry data, are often\ntime-consuming and challenging for on-call engineers. While statistical\ninference methods have been employed to estimate the causality of network\nevents, these approaches alone are similarly challenging and suffer from a lack\nof interpretability, making it difficult for engineers to understand the\npredictions made by black-box models. In this paper, we present RCACopilot, an\nadvanced on-call system that combines statistical tests and large language\nmodel (LLM) reasoning to automate RCA across various network environments.\nRCACopilot gathers and synthesizes critical runtime diagnostic information,\npredicts the root cause of incidents, provides a clear explanatory narrative,\nand offers targeted action steps for engineers to resolve the issues. By\nutilizing LLM reasoning techniques and retrieval, RCACopilot delivers accurate\nand practical support for operators.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86RCACopilot\u7cfb\u7edf\uff0c\u7ed3\u5408\u7edf\u8ba1\u6d4b\u8bd5\u548c\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u81ea\u52a8\u5316\u7f51\u7edc\u95ee\u9898\u7684\u6839\u56e0\u5206\u6790\uff0c\u5e76\u4e3a\u5de5\u7a0b\u5e08\u63d0\u4f9b\u89e3\u91ca\u548c\u89e3\u51b3\u6b65\u9aa4\u3002", "motivation": "\u4f20\u7edf\u7684\u6839\u56e0\u5206\u6790\u65b9\u6cd5\u8017\u65f6\u4e14\u96be\u4ee5\u7406\u89e3\uff0c\u7edf\u8ba1\u63a8\u65ad\u65b9\u6cd5\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u5de5\u7a0b\u5e08\u96be\u4ee5\u4fe1\u4efb\u9ed1\u76d2\u6a21\u578b\u7684\u9884\u6d4b\u3002", "method": "RCACopilot\u7ed3\u5408\u7edf\u8ba1\u6d4b\u8bd5\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\uff0c\u81ea\u52a8\u6536\u96c6\u548c\u5408\u6210\u8fd0\u884c\u65f6\u7684\u8bca\u65ad\u4fe1\u606f\uff0c\u9884\u6d4b\u6839\u56e0\u5e76\u63d0\u4f9b\u89e3\u91ca\u548c\u89e3\u51b3\u6b65\u9aa4\u3002", "result": "RCACopilot\u4e3a\u64cd\u4f5c\u5458\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u5b9e\u7528\u7684\u652f\u6301\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "conclusion": "RCACopilot\u901a\u8fc7LLM\u6280\u672f\u63d0\u5347\u4e86\u6839\u56e0\u5206\u6790\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u652f\u6301\u3002"}}
{"id": "2507.03314", "pdf": "https://arxiv.org/pdf/2507.03314", "abs": "https://arxiv.org/abs/2507.03314", "authors": ["Zsolt Zombori", "Bal\u00e1zs Indruck"], "title": "Partial Label Learning for Automated Theorem Proving", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "We formulate learning guided Automated Theorem Proving as Partial Label\nLearning, building the first bridge across these fields of research and\nproviding a theoretical framework for dealing with alternative proofs during\nlearning. We use the plCoP theorem prover to demonstrate that methods from the\nPartial Label Learning literature tend to increase the performance of learning\nassisted theorem provers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u5b66\u4e60\u5f15\u5bfc\u7684\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5f62\u5f0f\u5316\u4e3a\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\uff0c\u5efa\u7acb\u4e86\u8fd9\u4e24\u4e2a\u7814\u7a76\u9886\u57df\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u5b66\u4e60\u4e2d\u66ff\u4ee3\u8bc1\u660e\u7684\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5c06\u5b66\u4e60\u5f15\u5bfc\u7684\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u4e0e\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u5904\u7406\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u66ff\u4ee3\u8bc1\u660e\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u4f7f\u7528\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7plCoP\u5b9a\u7406\u8bc1\u660e\u5668\u5c55\u793a\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u5b66\u4e60\u8f85\u52a9\u5b9a\u7406\u8bc1\u660e\u5668\u6027\u80fd\u7684\u63d0\u5347\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u5b66\u4e60\u8f85\u52a9\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u548c\u5b66\u4e60\u9886\u57df\u642d\u5efa\u4e86\u6865\u6881\uff0c\u5c55\u793a\u4e86\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.02866", "pdf": "https://arxiv.org/pdf/2507.02866", "abs": "https://arxiv.org/abs/2507.02866", "authors": ["Ben Kereopa-Yorke"], "title": "Engineering Trust, Creating Vulnerability: A Socio-Technical Analysis of AI Interface Design", "categories": ["cs.HC", "cs.CR"], "comment": "31 pages, 8 figures", "summary": "This paper examines how distinct cultures of AI interdisciplinarity emerge\nthrough interface design, revealing the formation of new disciplinary cultures\nat these intersections. Through the Interface-Mediated Cognitive Security\n(IMCS) framework, I demonstrate how the collision of cybersecurity engineering,\ncognitive psychology, critical technology studies, and human-computer\ninteraction generates research cultures that transcend traditional disciplinary\nboundaries. AI interfaces function as transformative boundary objects that\nnecessitate methodological fusion rather than mere collaboration,\nsimultaneously embodying technical architectures, psychological design\npatterns, and social interaction models. Through systematic visual analysis of\ngenerative AI platforms and case studies across public sector, medical, and\neducational domains, I identify four vulnerability vectors, Reflection\nSimulation, Authority Modulation, Cognitive Load Exploitation, and\nMarket-Security Tension, that structure interface-mediated cognitive security.\nThis research challenges three significant gaps in interdisciplinary theory:\nthe assumption that disciplines maintain distinct methodological boundaries\nduring collaboration, the belief that technical and social knowledge practices\ncan be cleanly separated, and the presumption that disciplinary integration\noccurs through formal rather than cultural mechanisms. The empirical evidence\ndemonstrates how interfaces function as sites of epistemological collision,\ncreating methodological pressure zones where traditional disciplinary\napproaches prove insufficient for analysing the complex socio-technical\nphenomena at the interface.", "AI": {"tldr": "\u901a\u8fc7\u754c\u9762\u8bbe\u8ba1\u7814\u7a76AI\u8de8\u5b66\u79d1\u6587\u5316\u7684\u5f62\u6210\uff0c\u63d0\u51faIMCS\u6846\u67b6\uff0c\u63ed\u793aAI\u754c\u9762\u4f5c\u4e3a\u8fb9\u754c\u5bf9\u8c61\u7684\u89d2\u8272\uff0c\u5e76\u8bc6\u522b\u56db\u79cd\u8ba4\u77e5\u5b89\u5168\u6f0f\u6d1e\u5411\u91cf\uff0c\u6311\u6218\u4f20\u7edf\u8de8\u5b66\u79d1\u7406\u8bba\u7684\u4e09\u5927\u5047\u8bbe\u3002", "motivation": "\u63a2\u8ba8AI\u8de8\u5b66\u79d1\u6587\u5316\u5982\u4f55\u901a\u8fc7\u754c\u9762\u8bbe\u8ba1\u5f62\u6210\uff0c\u63ed\u793a\u4f20\u7edf\u5b66\u79d1\u754c\u9650\u5728\u7814\u7a76\u590d\u6742\u793e\u4f1a\u6280\u672f\u73b0\u8c61\u65f6\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528IMCS\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u5206\u6790\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u751f\u6210\u5f0fAI\u5e73\u53f0\u53ca\u516c\u5171\u3001\u533b\u7597\u548c\u6559\u80b2\u9886\u57df\u7684\u754c\u9762\u8bbe\u8ba1\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u79cd\u754c\u9762\u4ecb\u5bfc\u7684\u8ba4\u77e5\u5b89\u5168\u6f0f\u6d1e\u5411\u91cf\uff0c\u8bc1\u660e\u754c\u9762\u662f\u5b66\u79d1\u65b9\u6cd5\u8bba\u78b0\u649e\u7684\u573a\u6240\u3002", "conclusion": "AI\u754c\u9762\u4f5c\u4e3a\u8fb9\u754c\u5bf9\u8c61\uff0c\u63a8\u52a8\u4e86\u65b9\u6cd5\u8bba\u878d\u5408\u800c\u975e\u7b80\u5355\u534f\u4f5c\uff0c\u6311\u6218\u4e86\u8de8\u5b66\u79d1\u7406\u8bba\u7684\u4e09\u5927\u9884\u8bbe\u3002"}}
{"id": "2507.04171", "pdf": "https://arxiv.org/pdf/2507.04171", "abs": "https://arxiv.org/abs/2507.04171", "authors": ["Abani Patra", "Mary Thomas", "Elias Bou-Harb", "Jeffrey Carver", "Yuebin Guo", "Ratnesh Kumar", "Julien Langou", "Guoyu Lu", "Vivak Patel", "Marianna Safronova", "Isla Simpson", "Dhruva Chakravorty", "Jane Combs", "Hantao Cui", "Sushil Prasad", "Adnan Rajib", "Susan Rathbun", "Erik Saule", "Isla Simpson", "Alan Sussman", "Shaowen Wang", "Sarina Zhe Zhang", "Ben Brown", "Varun Chandola", "Daniel Crawford", "Ian Foster", "Dave Hart", "Mike Heroux", "Mary Ann Leung", "Benjamin Lynch", "Dan Negrut", "D. K. Panda", "Manish Parashar", "Melissa Kline Struhl", "George K. Thiruvathukal"], "title": "2024 NSF CSSI-Cybertraining-SCIPE PI Meeting August 12 to 13, 2024, Charlotte, NC", "categories": ["cs.ET", "cs.CY"], "comment": "Annual NSF PI meeting; contains summaries of meetings and breakout\n  sessions, lists of participants, links to presented posters on figshare", "summary": "The second annual NSF, OAC CSSI, CyberTraining and related programs PI\nmeeting was held August 12 to 13 in Charlotte, NC, with participation from PIs\nor representatives of all major awards. Keynotes, panels, breakouts, and poster\nsessions allowed PIs to engage with each other, NSF staff, and invited experts.\nThe 286 attendees represented 292 awards across CSSI, CyberTraining, OAC Core,\nCIP, SCIPE CDSE, and related programs, and presented over 250 posters. This\nreport documents the meetings structure, findings, and recommendations,\noffering a snapshot of current community perspectives on cyberinfrastructure. A\nkey takeaway is a vibrant, engaged community advancing science through CI.\nAI-driven research modalities complement established HPC and data centric\ntools. Workforce development efforts align well with the CSSI community.", "AI": {"tldr": "\u603b\u7ed3NSF\u3001OAC CSSI\u3001CyberTraining\u7b49\u9879\u76ee\u7684\u7b2c\u4e8c\u6b21\u5e74\u5ea6PI\u4f1a\u8bae\u5185\u5bb9\uff0c\u5305\u62ec\u53c2\u4e0e\u8005\u3001\u6d3b\u52a8\u548c\u4e3b\u8981\u53d1\u73b0\u3002", "motivation": "\u8bb0\u5f55\u4f1a\u8bae\u7ed3\u6784\u548c\u793e\u533a\u5bf9\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u7684\u5f53\u524d\u89c2\u70b9\uff0c\u5c55\u793a\u793e\u533a\u5728\u63a8\u52a8\u79d1\u5b66\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u4e3b\u9898\u6f14\u8bb2\u3001\u5c0f\u7ec4\u8ba8\u8bba\u3001\u5206\u7ec4\u4f1a\u8bae\u548c\u6d77\u62a5\u5c55\u793a\uff0c\u4fc3\u8fdbPI\u3001NSF\u5de5\u4f5c\u4eba\u5458\u548c\u4e13\u5bb6\u4e4b\u95f4\u7684\u4e92\u52a8\u3002", "result": "286\u540d\u4e0e\u4f1a\u8005\u4ee3\u8868292\u4e2a\u5956\u9879\uff0c\u5c55\u793a\u4e86250\u591a\u5f20\u6d77\u62a5\uff0c\u5c55\u793a\u4e86\u793e\u533a\u5bf9\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u7684\u79ef\u6781\u6001\u5ea6\u548c\u8fdb\u5c55\u3002", "conclusion": "\u4f1a\u8bae\u5c55\u793a\u4e86\u6d3b\u8dc3\u7684\u793e\u533a\u901a\u8fc7CI\u63a8\u52a8\u79d1\u5b66\uff0cAI\u9a71\u52a8\u7684\u7814\u7a76\u4e0eHPC\u548c\u6570\u636e\u5de5\u5177\u76f8\u8f85\u76f8\u6210\uff0c\u52b3\u52a8\u529b\u53d1\u5c55\u4e0eCSSI\u793e\u533a\u76ee\u6807\u4e00\u81f4\u3002"}}
{"id": "2507.03867", "pdf": "https://arxiv.org/pdf/2507.03867", "abs": "https://arxiv.org/abs/2507.03867", "authors": ["Yu Xiang Zhu", "Amos Robinson", "Sophia Roshal", "Timothy Mou", "Julian Mackay", "Jonathan Aldrich", "Alex Potanin"], "title": "Semantically Separating Nominal Wyvern for Usability and Decidability", "categories": ["cs.PL"], "comment": null, "summary": "The Dependent Object Types (DOT) calculus incorporates concepts from\nfunctional languages (e.g. modules) with traditional object-oriented features\n(e.g. objects, subtyping) to achieve greater expressivity (e.g. F-bounded\npolymorphism). However, this merger of paradigms comes at the cost of subtype\ndecidability. Recent work on bringing decidability to DOT has either sacrificed\nexpressiveness or ease of use. The unrestricted construction of recursive types\nand type bounds has made subtype decidability a much harder problem than in\ntraditional object-oriented programming.\n  Recognizing this, our paper introduces Nominal Wyvern, a DOT-like dependent\ntype system that takes an alternative approach: instead of having a uniform\nstructural syntax like DOT, Nominal Wyvern is designed around a \"semantic\nseparation\" between the nominal declaration of recursive types on the one hand,\nand the structural refinement of those types when they are used on the other.\nThis design naturally guides the user to avoid writing undecidably recursive\nstructural types.\n  From a technical standpoint, this separation also makes guaranteeing\ndecidability possible by allowing for an intuitive adaptation of material/shape\nseparation, a technique for achieving subtype decidability by separating types\nresponsible for subtyping constraints from types that represent concrete data.\nThe result is a type system with syntax and structure familiar to OOP users\nthat achieves decidability without compromising the expressiveness of F-bounded\npolymorphism and module systems as they are used in practice.", "AI": {"tldr": "Nominal Wyvern\u63d0\u51fa\u4e86\u4e00\u79cdDOT\u7c7b\u4f9d\u8d56\u7c7b\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u540d\u4e49\u58f0\u660e\u4e0e\u7ed3\u6784\u7cbe\u5316\u7684\u5206\u79bb\uff0c\u89e3\u51b3\u4e86DOT\u4e2d\u9012\u5f52\u7c7b\u578b\u5bfc\u81f4\u7684\u53ef\u5224\u5b9a\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u8868\u8fbe\u529b\u3002", "motivation": "DOT\u7b97\u6cd5\u7ed3\u5408\u4e86\u51fd\u6570\u5f0f\u548c\u9762\u5411\u5bf9\u8c61\u7279\u6027\uff0c\u4f46\u589e\u52a0\u4e86\u5b50\u7c7b\u578b\u53ef\u5224\u5b9a\u6027\u7684\u96be\u5ea6\u3002\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u65e2\u4fdd\u7559\u8868\u8fbe\u529b\u53c8\u786e\u4fdd\u53ef\u5224\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u540d\u4e49\u58f0\u660e\u4e0e\u7ed3\u6784\u7cbe\u5316\u7684\u5206\u79bb\u8bbe\u8ba1\uff0c\u907f\u514d\u4e0d\u53ef\u5224\u5b9a\u7684\u9012\u5f52\u7c7b\u578b\uff0c\u5e76\u501f\u9274\u4e86material/shape\u5206\u79bb\u6280\u672f\u3002", "result": "\u8bbe\u8ba1\u4e86Nominal Wyvern\u7c7b\u578b\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5b50\u7c7b\u578b\u53ef\u5224\u5b9a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86F\u6709\u754c\u591a\u6001\u548c\u6a21\u5757\u7cfb\u7edf\u7684\u8868\u8fbe\u529b\u3002", "conclusion": "Nominal Wyvern\u901a\u8fc7\u5206\u79bb\u8bbe\u8ba1\u89e3\u51b3\u4e86DOT\u7684\u53ef\u5224\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u76f4\u89c2\u4e14\u5f3a\u5927\u7684\u7c7b\u578b\u7cfb\u7edf\u3002"}}
{"id": "2507.03114", "pdf": "https://arxiv.org/pdf/2507.03114", "abs": "https://arxiv.org/abs/2507.03114", "authors": ["Seonho Lee", "Jihwan Oh", "Junkyum Kim", "Seokjin Go", "Jongse Park", "Divya Mahajan"], "title": "Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications", "categories": ["cs.DC"], "comment": null, "summary": "This paper provides an in-depth characterization of GPU-accelerated systems,\nto understand the interplay between overlapping computation and communication\nwhich is commonly employed in distributed training settings. Due to the large\nsize of models, distributing them across multiple devices is required.\nOverlapping strategies, which enable concurrent computation and communication,\nare critical for mitigating communication bottlenecks and maximizing GPU\nutilization. However, the current consensus is that we should always and\naggressively overlap compute and communication to mitigate the overhead of\ndistribution. By systematically evaluating state-of-the-art GPUs, this study\ninvestigates the impact of hardware features such as numeric precision,\nspecialized cores, and power capping on distributed training workloads.\nComprehensive experiments and studies showcase the effects of overlapping\nstrategies on performance and power consumption across varying scenarios. We\nobserve that overlapping computation and communication can result in an average\ncomputational slowdown of 18.9%, with a maximum of 40.0% slowdown. This\nslowdown is in comparison to the scenario when no communication was happening\nwith the compute. We consider this an ideal execution scenario, where the\ncommunication in parallel has not impact on the compute time. However,\nperforming computation and communication sequentially is, on average, 10.2%\nslower than overlapped execution, with a maximum slowdown of 26.6%. We further\nobserve, while specialized datapath and optimized numeric precision mitigate\ncertain slowdowns, overlapping execution can lead to resource contention and\nalso increase power consumption under specific configurations. The analysis\nalso uncovers trade-offs introduced by power and frequency capping, emphasizing\nthe importance of balanced strategies to optimize energy efficiency and\ntraining throughput.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u5206\u6790\u4e86GPU\u52a0\u901f\u7cfb\u7edf\u4e2d\u8ba1\u7b97\u4e0e\u901a\u4fe1\u91cd\u53e0\u7b56\u7565\u7684\u6548\u679c\uff0c\u63ed\u793a\u4e86\u8d44\u6e90\u7ade\u4e89\u548c\u529f\u8017\u589e\u52a0\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u4e86\u786c\u4ef6\u7279\u6027\u5bf9\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u5f71\u54cd\u3002", "motivation": "\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\uff0c\u8ba1\u7b97\u4e0e\u901a\u4fe1\u7684\u91cd\u53e0\u7b56\u7565\u5e38\u7528\u4e8e\u7f13\u89e3\u901a\u4fe1\u74f6\u9888\uff0c\u4f46\u5f53\u524d\u5171\u8bc6\u8ba4\u4e3a\u5e94\u59cb\u7ec8\u79ef\u6781\u91cd\u53e0\uff0c\u672c\u7814\u7a76\u8d28\u7591\u5e76\u8bc4\u4f30\u5176\u5b9e\u9645\u6548\u679c\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u6700\u65b0GPU\u786c\u4ef6\u7279\u6027\uff08\u5982\u6570\u503c\u7cbe\u5ea6\u3001\u4e13\u7528\u6838\u5fc3\u548c\u529f\u8017\u9650\u5236\uff09\uff0c\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\u3002", "result": "\u91cd\u53e0\u7b56\u7565\u5e73\u5747\u5bfc\u81f4\u8ba1\u7b97\u901f\u5ea6\u4e0b\u964d18.9%\uff0c\u4f46\u6bd4\u987a\u5e8f\u6267\u884c\u5feb10.2%\uff1b\u4e13\u7528\u6570\u636e\u8def\u5f84\u548c\u4f18\u5316\u6570\u503c\u7cbe\u5ea6\u53ef\u7f13\u89e3\u90e8\u5206\u964d\u901f\u3002", "conclusion": "\u91cd\u53e0\u7b56\u7565\u9700\u5e73\u8861\u8d44\u6e90\u4e89\u7528\u4e0e\u529f\u8017\uff0c\u4f18\u5316\u80fd\u6e90\u6548\u7387\u548c\u8bad\u7ec3\u541e\u5410\u91cf\u3002"}}
{"id": "2507.03919", "pdf": "https://arxiv.org/pdf/2507.03919", "abs": "https://arxiv.org/abs/2507.03919", "authors": ["Duy Le"], "title": "PFCS: Prime Factorization Cache System for Deterministic Data Relationship Discovery", "categories": ["cs.DB", "cs.CC"], "comment": "6 pages, 3 figures, 3 algorithms", "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design", "AI": {"tldr": "PFCS\u5229\u7528\u8d28\u56e0\u6570\u5206\u89e3\u7684\u6570\u5b66\u552f\u4e00\u6027\u5b9e\u73b0\u786e\u5b9a\u6027\u5173\u7cfb\u53d1\u73b0\uff0c\u663e\u8457\u63d0\u5347\u7f13\u5b58\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7f13\u5b58\u7cfb\u7edf\u4f9d\u8d56\u7edf\u8ba1\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u5173\u7cfb\u53d1\u73b0\uff0c\u5bfc\u81f4\u9884\u53d6\u4e0d\u51c6\u786e\u548c\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u4e3a\u6570\u636e\u5143\u7d20\u5206\u914d\u552f\u4e00\u8d28\u6570\uff0c\u5c06\u5173\u7cfb\u8868\u793a\u4e3a\u5408\u6570\uff0c\u901a\u8fc7\u8d28\u56e0\u6570\u5206\u89e3\u6062\u590d\u5b8c\u7f8e\u5173\u7cfb\u3002", "result": "\u5728\u6570\u636e\u5e93\u3001ML\u548cHPC\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53476.2\u500d\uff0c\u547d\u4e2d\u738798.9%\uff0c\u529f\u8017\u964d\u4f4e38%\u3002", "conclusion": "PFCS\u7684\u6570\u5b66\u57fa\u7840\u63d0\u4f9b\u4e86\u4f20\u7edf\u8fd1\u4f3c\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u7684\u6b63\u5f0f\u4fdd\u8bc1\uff0c\u4e3a\u7f13\u5b58\u7cfb\u7edf\u8bbe\u8ba1\u786e\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.03308", "pdf": "https://arxiv.org/pdf/2507.03308", "abs": "https://arxiv.org/abs/2507.03308", "authors": ["Jindong Li", "Tenglong Li", "Ruiqi Chen", "Guobin Shen", "Dongcheng Zhao", "Qian Zhang", "Yi Zeng"], "title": "Hummingbird: A Smaller and Faster Large Language Model Accelerator on Embedded FPGA", "categories": ["cs.AR"], "comment": "Accepted by ICCAD2025", "summary": "Deploying large language models (LLMs) on embedded devices remains a\nsignificant research challenge due to the high computational and memory demands\nof LLMs and the limited hardware resources available in such environments.\nWhile embedded FPGAs have demonstrated performance and energy efficiency in\ntraditional deep neural networks, their potential for LLM inference remains\nlargely unexplored. Recent efforts to deploy LLMs on FPGAs have primarily\nrelied on large, expensive cloud-grade hardware and have only shown promising\nresults on relatively small LLMs, limiting their real-world applicability. In\nthis work, we present Hummingbird, a novel FPGA accelerator designed\nspecifically for LLM inference on embedded FPGAs. Hummingbird is smaller,\ntargeting embedded FPGAs such as the KV260 and ZCU104 with 67% LUT, 39% DSP,\nand 42% power savings over existing research. Hummingbird is stronger,\ntargeting LLaMA3-8B and supporting longer contexts, overcoming the typical 4GB\nmemory constraint of embedded FPGAs through offloading strategies. Finally,\nHummingbird is faste, achieving 4.8 tokens/s and 8.6 tokens/s for LLaMA3-8B on\nthe KV260 and ZCU104 respectively, with 93-94% model bandwidth utilization,\noutperforming the prior 4.9 token/s for LLaMA2-7B with 84% bandwidth\nutilization baseline. We further demonstrate the viability of industrial\napplications by deploying Hummingbird on a cost-optimized Spartan UltraScale\nFPGA, paving the way for affordable LLM solutions at the edge.", "AI": {"tldr": "Hummingbird\u662f\u4e00\u79cd\u4e13\u4e3a\u5d4c\u5165\u5f0fFPGA\u8bbe\u8ba1\u7684LLM\u63a8\u7406\u52a0\u901f\u5668\uff0c\u5177\u6709\u66f4\u5c0f\u7684\u4f53\u79ef\u3001\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u66f4\u4f4e\u7684\u529f\u8017\uff0c\u652f\u6301\u66f4\u5927\u89c4\u6a21\u7684LLM\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65f6\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\uff0c\u800c\u5d4c\u5165\u5f0fFPGA\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u53d1\u6325\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aHummingbird\u7684FPGA\u52a0\u901f\u5668\uff0c\u4f18\u5316\u4e86\u8d44\u6e90\u5229\u7528\u7387\u5e76\u91c7\u7528\u5378\u8f7d\u7b56\u7565\u514b\u670d\u5185\u5b58\u9650\u5236\u3002", "result": "\u5728\u5d4c\u5165\u5f0fFPGA\uff08\u5982KV260\u548cZCU104\uff09\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u63a8\u7406\u901f\u5ea6\u548c\u6a21\u578b\u5e26\u5bbd\u5229\u7528\u7387\uff0c\u540c\u65f6\u529f\u8017\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "Hummingbird\u4e3a\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6210\u672c\u4f18\u5316\u7684LLM\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5de5\u4e1a\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.03515", "pdf": "https://arxiv.org/pdf/2507.03515", "abs": "https://arxiv.org/abs/2507.03515", "authors": ["Radouane Bouchekir", "Michell Guzman Cancimance"], "title": "Enhancing Uncertainty Quantification for Runtime Safety Assurance Using Causal Risk Analysis and Operational Design Domain", "categories": ["cs.SE"], "comment": null, "summary": "Ensuring the runtime safety of autonomous systems remains challenging due to\ndeep learning components' inherent uncertainty and their sensitivity to\nenvironmental changes. In this paper, we propose an enhancement of traditional\nuncertainty quantification by explicitly incorporating environmental conditions\nusing risk-based causal analysis. We leverage Hazard Analysis and Risk\nAssessment (HARA) and fault tree modeling to identify critical operational\nconditions affecting system functionality. These conditions, together with\nuncertainties from the data and model, are integrated into a unified Bayesian\nNetwork (BN). At runtime, this BN is instantiated using real-time environmental\nobservations to infer a probabilistic distribution over the safety estimation.\nThis distribution enables the computation of both expected performance and its\nassociated variance, providing a dynamic and context-aware measure of\nuncertainty. We demonstrate our approach through a case study of the Object\nDetection (OD) component in an Automated Valet Parking (AVP).", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u73af\u5883\u6761\u4ef6\u7684\u98ce\u9669\u56e0\u679c\u5206\u6790\u589e\u5f3a\u4f20\u7edf\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u7ec4\u4ef6\u56e0\u4e0d\u786e\u5b9a\u6027\u548c\u5bf9\u73af\u5883\u53d8\u5316\u7684\u654f\u611f\u6027\u5bfc\u81f4\u7684\u81ea\u4e3b\u7cfb\u7edf\u8fd0\u884c\u65f6\u5b89\u5168\u95ee\u9898\u3002", "method": "\u5229\u7528HARA\u548c\u6545\u969c\u6811\u6a21\u578b\u8bc6\u522b\u5173\u952e\u64cd\u4f5c\u6761\u4ef6\uff0c\u7ed3\u5408\u6570\u636e\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u8d1d\u53f6\u65af\u7f51\u7edc\u8fdb\u884c\u5b9e\u65f6\u63a8\u65ad\u3002", "result": "\u901a\u8fc7\u81ea\u52a8\u9a7e\u9a76\u6cca\u8f66\u7cfb\u7edf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u7ec4\u4ef6\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u52a8\u6001\u8bc4\u4f30\u5b89\u5168\u6027\u80fd\u53ca\u5176\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2507.03836", "pdf": "https://arxiv.org/pdf/2507.03836", "abs": "https://arxiv.org/abs/2507.03836", "authors": ["Jianxin Sun", "David Lenz", "Hongfeng Yu", "Tom Peterka"], "title": "F-Hash: Feature-Based Hash Design for Time-Varying Volume Visualization via Multi-Resolution Tesseract Encoding", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Interactive time-varying volume visualization is challenging due to its\ncomplex spatiotemporal features and sheer size of the dataset. Recent works\ntransform the original discrete time-varying volumetric data into continuous\nImplicit Neural Representations (INR) to address the issues of compression,\nrendering, and super-resolution in both spatial and temporal domains. However,\ntraining the INR takes a long time to converge, especially when handling\nlarge-scale time-varying volumetric datasets. In this work, we proposed F-Hash,\na novel feature-based multi-resolution Tesseract encoding architecture to\ngreatly enhance the convergence speed compared with existing input encoding\nmethods for modeling time-varying volumetric data. The proposed design\nincorporates multi-level collision-free hash functions that map dynamic 4D\nmulti-resolution embedding grids without bucket waste, achieving high encoding\ncapacity with compact encoding parameters. Our encoding method is agnostic to\ntime-varying feature detection methods, making it a unified encoding solution\nfor feature tracking and evolution visualization. Experiments show the F-Hash\nachieves state-of-the-art convergence speed in training various time-varying\nvolumetric datasets for diverse features. We also proposed an adaptive ray\nmarching algorithm to optimize the sample streaming for faster rendering of the\ntime-varying neural representation.", "AI": {"tldr": "F-Hash\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u591a\u5206\u8fa8\u7387Tesseract\u7f16\u7801\u67b6\u6784\uff0c\u7528\u4e8e\u52a0\u901f\u65f6\u95f4\u53d8\u5316\u4f53\u6570\u636e\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u8bad\u7ec3\uff0c\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u548c\u6e32\u67d3\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u65f6\u95f4\u53d8\u5316\u4f53\u6570\u636e\u7684\u65f6\u7a7a\u590d\u6742\u6027\u548c\u5927\u89c4\u6a21\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u8bad\u7ec3\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u8017\u65f6\u8f83\u957f\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u7684\u7f16\u7801\u65b9\u6cd5\u52a0\u901f\u8bad\u7ec3\u3002", "method": "\u63d0\u51faF-Hash\u67b6\u6784\uff0c\u91c7\u7528\u591a\u7ea7\u65e0\u51b2\u7a81\u54c8\u5e0c\u51fd\u6570\u6620\u5c04\u52a8\u60014D\u591a\u5206\u8fa8\u7387\u5d4c\u5165\u7f51\u683c\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u5149\u7ebf\u884c\u8fdb\u7b97\u6cd5\u4f18\u5316\u6e32\u67d3\u91c7\u6837\u3002", "result": "F-Hash\u5728\u591a\u79cd\u65f6\u95f4\u53d8\u5316\u4f53\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u652f\u6301\u9ad8\u6548\u7684\u7279\u5f81\u8ddf\u8e2a\u548c\u6f14\u5316\u53ef\u89c6\u5316\u3002", "conclusion": "F-Hash\u4e3a\u65f6\u95f4\u53d8\u5316\u4f53\u6570\u636e\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u7f16\u7801\u548c\u6e32\u67d3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03211", "pdf": "https://arxiv.org/pdf/2507.03211", "abs": "https://arxiv.org/abs/2507.03211", "authors": ["Liangyu Wang", "Huanyi Xie", "Di Wang"], "title": "DistZO2: High-Throughput and Memory-Efficient Zeroth-Order Fine-tuning LLMs with Distributed Parallel Computing", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "Fine-tuning large language models (LLMs) remains resource-intensive due to\ntheir sheer scale. While zeroth-order (ZO) optimization provides a\nmemory-efficient alternative by eliminating backward passes, its application to\nmulti-hundred-billion-parameter models is constrained by GPU memory and compute\nthroughput. The ZO2 framework addresses the memory bottleneck by offloading\nmodel parameters to CPU memory and overlapping transformer block transfer with\ndual forward computation on a single GPU. However, ZO2 remains limited by its\nsingle-device execution and achieves modest throughput. In this work, we\npresent DistZO2, a high-throughput, memory-efficient framework for distributed\nzeroth-order fine-tuning of LLMs. DistZO2 introduces three parallel strategies:\n(1) Perturbation Parallelism (PertP), which parallelizes the two perturbed\nforward passes across devices; (2) Distributed Data Parallelism (DDP), adapted\nto the scalar-gradient nature of ZO training; and (3) a unified 2D Parallelism\ndesign that combines PertP and DDP. To further mitigate communication\nbottlenecks introduced by parameter offloading, we propose a hardware-aware\ncommunication strategy that slices parameter blocks and redistributes them\nacross GPUs via high-speed interconnects such as NVLink. DistZO2 scales\nzeroth-order fine-tuning to modern multi-GPU systems, preserving ZO2's memory\nefficiency while substantially improving training throughput. In our\nexperiments on OPT-175B, DistZO2 achieves a 3x speedup over ZO2 with\ndistributed computing. DistZO2's code has been open-sourced in\nhttps://github.com/liangyuwang/zo2.", "AI": {"tldr": "DistZO2\u662f\u4e00\u4e2a\u9ad8\u6548\u5185\u5b58\u3001\u9ad8\u541e\u5410\u91cf\u7684\u5206\u5e03\u5f0f\u96f6\u9636\u5fae\u8c03\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5355\u8bbe\u5907\u4e0a\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5e76\u884c\u7b56\u7565\u548c\u786c\u4ef6\u611f\u77e5\u901a\u4fe1\u7b56\u7565\u63d0\u5347\u4e86\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u8d44\u6e90\u5bc6\u96c6\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u591a\u767e\u4ebf\u53c2\u6570\u6a21\u578b\u4e0a\uff0c\u96f6\u9636\u4f18\u5316\uff08ZO\uff09\u867d\u8282\u7701\u5185\u5b58\u4f46\u53d7\u9650\u4e8e\u5355\u8bbe\u5907\u6267\u884c\u548c\u541e\u5410\u91cf\u3002", "method": "DistZO2\u5f15\u5165\u4e09\u79cd\u5e76\u884c\u7b56\u7565\uff1a\u6270\u52a8\u5e76\u884c\uff08PertP\uff09\u3001\u5206\u5e03\u5f0f\u6570\u636e\u5e76\u884c\uff08DDP\uff09\u53ca\u7edf\u4e002D\u5e76\u884c\u8bbe\u8ba1\uff0c\u5e76\u7ed3\u5408\u786c\u4ef6\u611f\u77e5\u901a\u4fe1\u7b56\u7565\u4f18\u5316\u53c2\u6570\u52a0\u8f7d\u3002", "result": "\u5728OPT-175B\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cDistZO2\u6bd4ZO2\u5b9e\u73b0\u4e863\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "DistZO2\u6210\u529f\u6269\u5c55\u4e86\u96f6\u9636\u5fae\u8c03\u81f3\u591aGPU\u7cfb\u7edf\uff0c\u4fdd\u6301\u4e86\u5185\u5b58\u6548\u7387\u5e76\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u3002"}}
{"id": "2507.03248", "pdf": "https://arxiv.org/pdf/2507.03248", "abs": "https://arxiv.org/abs/2507.03248", "authors": ["Wenhao Lu", "Zhiyuan Wang", "Hefan Zhang", "Shan Zhang", "Hongbin Luo"], "title": "OpenSN: An Open Source Library for Emulating LEO Satellite Networks", "categories": ["cs.NI"], "comment": "17 pages", "summary": "Low-earth-orbit (LEO) satellite constellations (e.g., Starlink) are becoming\na necessary component of future Internet. There have been increasing studies on\nLEO satellite networking. It is a crucial problem how to evaluate these studies\nin a systematic and reproducible manner. In this paper, we present OpenSN,\ni.e., an open source library for emulating large-scale satellite network (SN).\nDifferent from Mininet-based SN emulators (e.g., LeoEM), OpenSN adopts\ncontainer-based virtualization, thus allows for running distributed routing\nsoftware on each node, and can achieve horizontal scalability via flexible\nmulti-machine extension. Compared to other container-based SN emulators (e.g.,\nStarryNet), OpenSN streamlines the interaction with Docker command line\ninterface and significantly reduces unnecessary operations of creating virtual\nlinks. These modifications improve emulation efficiency and vertical\nscalability on a single machine. Furthermore, OpenSN separates user-defined\nconfiguration from container network management via a Key-Value Database that\nrecords the necessary information for SN emulation. Such a separation\narchitecture enhances the function extensibility. To sum up, OpenSN exhibits\nadvantages in efficiency, scalability, and extensibility, thus is a valuable\nopen source library that empowers research on LEO satellite networking.\nExperiment results show that OpenSN constructs mega-constellations 5X-10X\nfaster than StarryNet, and updates link state 2X-4X faster than LeoEM. We also\nverify the scalability of OpenSN by successfully emulating the five-shell\nStarlink constellation with a total of 4408 satellites.", "AI": {"tldr": "OpenSN\u662f\u4e00\u4e2a\u5f00\u6e90\u5e93\uff0c\u7528\u4e8e\u6a21\u62df\u5927\u89c4\u6a21\u536b\u661f\u7f51\u7edc\uff0c\u5177\u6709\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u529f\u80fd\u53ef\u6269\u5c55\u7684\u4f18\u52bf\uff0c\u6bd4\u73b0\u6709\u5de5\u5177\u66f4\u5feb\u4e14\u66f4\u7075\u6d3b\u3002", "motivation": "\u4e3a\u4e86\u7cfb\u7edf\u6027\u3001\u53ef\u91cd\u590d\u5730\u8bc4\u4f30\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u7f51\u7edc\u7814\u7a76\uff0c\u9700\u8981\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u529f\u80fd\u4e30\u5bcc\u7684\u6a21\u62df\u5de5\u5177\u3002", "method": "OpenSN\u91c7\u7528\u57fa\u4e8e\u5bb9\u5668\u7684\u865a\u62df\u5316\u6280\u672f\uff0c\u7b80\u5316\u4e0eDocker\u547d\u4ee4\u884c\u63a5\u53e3\u7684\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u952e\u503c\u6570\u636e\u5e93\u5206\u79bb\u7528\u6237\u914d\u7f6e\u4e0e\u5bb9\u5668\u7f51\u7edc\u7ba1\u7406\u3002", "result": "OpenSN\u6784\u5efa\u5927\u578b\u661f\u5ea7\u7684\u901f\u5ea6\u6bd4StarryNet\u5feb5-10\u500d\uff0c\u66f4\u65b0\u94fe\u8def\u72b6\u6001\u6bd4LeoEM\u5feb2-4\u500d\uff0c\u6210\u529f\u6a21\u62df\u4e864408\u9897\u536b\u661f\u7684Starlink\u661f\u5ea7\u3002", "conclusion": "OpenSN\u5728\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u529f\u80fd\u6269\u5c55\u6027\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u662fLEO\u536b\u661f\u7f51\u7edc\u7814\u7a76\u7684\u5b9d\u8d35\u5de5\u5177\u3002"}}
{"id": "2507.04080", "pdf": "https://arxiv.org/pdf/2507.04080", "abs": "https://arxiv.org/abs/2507.04080", "authors": ["Naoki Nishida", "Misaki Kojima", "Yuto Nakamura"], "title": "Difference of Constrained Patterns in Logically Constrained Term Rewrite Systems (Full Version)", "categories": ["cs.LO"], "comment": "38 pages", "summary": "Considering patterns as sets of their instances, a difference operator over\npatterns computes a finite set of two given patterns, which represents the\ndifference between the dividend pattern and the divisor pattern. A complement\nof a pattern is a set of patterns, the ground constructor instances of which\ncomprise the complement set of the ground constructor instances of the former\npattern. Given finitely many unconstrained linear patterns, using a difference\noperator over linear patterns, a known complement algorithm returns a finite\nset of linear patterns as a complement of the given patterns. In this paper, we\nextend the difference operator and complement algorithm to constrained linear\npatterns used in logically constrained term rewrite systems (LCTRSs, for short)\nthat have no user-defined constructor term with a sort for built-in values.\nThen, as for left-linear TRSs, using the complement algorithm, we show that\nquasi-reducibility is decidable for such LCTRSs with decidable built-in\ntheories. For the single use of the difference operator over (constrained)\npatterns, only divisor patterns are required to be linear.", "AI": {"tldr": "\u8bba\u6587\u6269\u5c55\u4e86\u5dee\u5f02\u8fd0\u7b97\u7b26\u548c\u8865\u7801\u7b97\u6cd5\u5230\u7ea6\u675f\u7ebf\u6027\u6a21\u5f0f\uff0c\u7528\u4e8e\u903b\u8f91\u7ea6\u675f\u9879\u91cd\u5199\u7cfb\u7edf\uff08LCTRSs\uff09\uff0c\u8bc1\u660e\u4e86\u5728\u53ef\u5224\u5b9a\u5185\u7f6e\u7406\u8bba\u4e0b\uff0c\u62df\u7ea6\u7b80\u6027\u662f\u53ef\u5224\u5b9a\u7684\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u6269\u5c55\u5dee\u5f02\u8fd0\u7b97\u7b26\u548c\u8865\u7801\u7b97\u6cd5\u7684\u9002\u7528\u8303\u56f4\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7ea6\u675f\u7ebf\u6027\u6a21\u5f0f\u548c\u903b\u8f91\u7ea6\u675f\u9879\u91cd\u5199\u7cfb\u7edf\uff08LCTRSs\uff09\uff0c\u4ee5\u89e3\u51b3\u62df\u7ea6\u7b80\u6027\u7684\u5224\u5b9a\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u5dee\u5f02\u8fd0\u7b97\u7b26\u548c\u8865\u7801\u7b97\u6cd5\u5230\u7ea6\u675f\u7ebf\u6027\u6a21\u5f0f\uff0c\u5e76\u7ed3\u5408\u5de6\u7ebf\u6027TRSs\u7684\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u62df\u7ea6\u7b80\u6027\u7684\u5224\u5b9a\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u53ef\u5224\u5b9a\u5185\u7f6e\u7406\u8bba\u7684LCTRSs\u4e2d\uff0c\u62df\u7ea6\u7b80\u6027\u662f\u53ef\u5224\u5b9a\u7684\uff0c\u4e14\u5dee\u5f02\u8fd0\u7b97\u7b26\u4ec5\u8981\u6c42\u9664\u6570\u6a21\u5f0f\u662f\u7ebf\u6027\u7684\u3002", "conclusion": "\u901a\u8fc7\u6269\u5c55\u5dee\u5f02\u8fd0\u7b97\u7b26\u548c\u8865\u7801\u7b97\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7ea6\u675f\u7ebf\u6027\u6a21\u5f0f\u4e0b\u62df\u7ea6\u7b80\u6027\u7684\u5224\u5b9a\u95ee\u9898\uff0c\u4e3a\u903b\u8f91\u7ea6\u675f\u9879\u91cd\u5199\u7cfb\u7edf\u7684\u7406\u8bba\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2507.02868", "pdf": "https://arxiv.org/pdf/2507.02868", "abs": "https://arxiv.org/abs/2507.02868", "authors": ["Anastasia Sergeeva", "Claudia Negri-Ribalta", "Gabriele Lenzini"], "title": "Identifying Ethical Challenges in XR Implementations in the Industrial Domain: A Case of Off-Highway Machinery", "categories": ["cs.HC"], "comment": "Presented at CHI 2025 (arXiv:2504.07475)", "summary": "Although extended reality(XR)-using technologies have started to be discussed\nin the industrial setting, it is becoming important to understand how to\nimplement them ethically and privacy-preservingly. In our paper, we summarise\nour experience of developing XR implementations for the off-highway machinery\ndomain by pointing to the main challenges we identified during the work. We\nbelieve that our findings can be a starting point for further discussion and\nfuture research regarding privacy and ethical challenges in industrial\napplications of XR.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u5982\u4f55\u4ee5\u4f26\u7406\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u65b9\u5f0f\u5b9e\u73b0\u6269\u5c55\u73b0\u5b9e(XR)\u6280\u672f\uff0c\u5e76\u603b\u7ed3\u4e86\u5728\u9ad8\u6027\u80fd\u673a\u68b0\u9886\u57df\u5f00\u53d1XR\u5b9e\u73b0\u7684\u7ecf\u9a8c\u4e0e\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u5de5\u4e1a\u5e94\u7528\u4e2dXR\u6280\u672f\u7684\u4f26\u7406\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u603b\u7ed3\u5728\u9ad8\u6027\u80fd\u673a\u68b0\u9886\u57df\u5f00\u53d1XR\u5b9e\u73b0\u7684\u7ecf\u9a8c\uff0c\u8bc6\u522b\u4e3b\u8981\u6311\u6218\u3002", "result": "\u63d0\u51fa\u4e86XR\u6280\u672f\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u9690\u79c1\u548c\u4f26\u7406\u6311\u6218\uff0c\u4e3a\u540e\u7eed\u8ba8\u8bba\u548c\u7814\u7a76\u94fa\u8def\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u5de5\u4e1aXR\u5e94\u7528\u7684\u4f26\u7406\u548c\u9690\u79c1\u7814\u7a76\u63d0\u4f9b\u4e86\u8d77\u70b9\u3002"}}
{"id": "2507.04818", "pdf": "https://arxiv.org/pdf/2507.04818", "abs": "https://arxiv.org/abs/2507.04818", "authors": ["Donato Ferraro", "Andrea Bastoni", "Alexander Zuepke", "Andrea Marongiu"], "title": "Enabling Security on the Edge: A CHERI Compartmentalized Network Stack", "categories": ["cs.ET", "cs.CR"], "comment": "Accepted for publication at Design, Automation and Test in Europe\n  Conference | The European Event for Electronic System Design & Test 2025\n  (DATE25), 7 pages", "summary": "The widespread deployment of embedded systems in critical infrastructures,\ninterconnected edge devices like autonomous drones, and smart industrial\nsystems requires robust security measures. Compromised systems increase the\nrisks of operational failures, data breaches, and -- in safety-critical\nenvironments -- potential physical harm to people. Despite these risks, current\nsecurity measures are often insufficient to fully address the attack surfaces\nof embedded devices. CHERI provides strong security from the hardware level by\nenabling fine-grained compartmentalization and memory protection, which can\nreduce the attack surface and improve the reliability of such devices. In this\nwork, we explore the potential of CHERI to compartmentalize one of the most\ncritical and targeted components of interconnected systems: their network\nstack. Our case study examines the trade-offs of isolating applications, TCP/IP\nlibraries, and network drivers on a CheriBSD system deployed on the Arm Morello\nplatform. Our results suggest that CHERI has the potential to enhance security\nwhile maintaining performance in embedded-like environments.", "AI": {"tldr": "\u63a2\u8ba8CHERI\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u9694\u79bb\u7f51\u7edc\u7ec4\u4ef6\u7684\u6f5c\u529b\uff0c\u63d0\u5347\u5b89\u5168\u6027\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u4e0d\u8db3\uff0cCHERI\u901a\u8fc7\u786c\u4ef6\u7ea7\u522b\u7684\u9694\u79bb\u80fd\u51cf\u5c11\u653b\u51fb\u9762\u3002", "method": "\u4f7f\u7528CheriBSD\u7cfb\u7edf\u5728Arm Morello\u5e73\u53f0\u4e0a\u9694\u79bb\u7f51\u7edc\u7ec4\u4ef6\u3002", "result": "CHERI\u80fd\u6709\u6548\u589e\u5f3a\u5b89\u5168\u6027\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "CHERI\u5728\u5d4c\u5165\u5f0f\u73af\u5883\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.04298", "pdf": "https://arxiv.org/pdf/2507.04298", "abs": "https://arxiv.org/abs/2507.04298", "authors": ["Youngju Song", "Minki Cho"], "title": "CCR 2.0: High-level Reasoning for Conditional Refinements", "categories": ["cs.PL"], "comment": null, "summary": "In recent years, great progress has been made in the field of formal\nverification for low-level systems. Many of them are based on one of two\npopular approaches: refinement or separation logic. These two approaches are\nvery different in nature and offer complementary benefits in terms of\ncompositionality. Recently, to fuse these benefits in a unified mechanism, a\nnew approach called Conditional Contextual Refinement (CCR 1.0 for short) was\nproposed. In this paper, we advance the model of CCR 1.0 and provide novel and\nintuitive reasoning principles, resulting in: CCR 2.0. Specifically, CCR 2.0\n(i) comes with a better compositionality theorem, having the practical benefit\nof facilitating more proof reuse, and (ii) provides a proof technique that\nhides model-level (i.e., resources of the separation logic) details from the\nuser. Achieving this goal was challenging due to non-trivial counterexamples\nwhich necessitated us to devise novel notions. Our results are formalized in\nCoq.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6a21\u578bCCR 2.0\uff0c\u7ed3\u5408\u4e86\u7cbe\u5316\u548c\u5206\u79bb\u903b\u8f91\u7684\u4f18\u70b9\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7ec4\u5408\u6027\u5b9a\u7406\u548c\u9690\u85cf\u6a21\u578b\u7ec6\u8282\u7684\u8bc1\u660e\u6280\u672f\u3002", "motivation": "\u7ed3\u5408\u7cbe\u5316\u548c\u5206\u79bb\u903b\u8f91\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u673a\u5236CCR 1.0\uff0c\u5e76\u8fdb\u4e00\u6b65\u6539\u8fdb\u4e3aCCR 2.0\u3002", "method": "\u5f00\u53d1\u4e86\u65b0\u7684\u63a8\u7406\u539f\u5219\uff0c\u6539\u8fdb\u4e86\u7ec4\u5408\u6027\u5b9a\u7406\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9690\u85cf\u6a21\u578b\u7ec6\u8282\u7684\u8bc1\u660e\u6280\u672f\u3002", "result": "CCR 2.0\u5728Coq\u4e2d\u5b9e\u73b0\uff0c\u5177\u6709\u66f4\u597d\u7684\u7ec4\u5408\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\u3002", "conclusion": "CCR 2.0\u6210\u529f\u878d\u5408\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u89e3\u51b3\u4e86\u975e\u5e73\u51e1\u53cd\u4f8b\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2507.03220", "pdf": "https://arxiv.org/pdf/2507.03220", "abs": "https://arxiv.org/abs/2507.03220", "authors": ["Saransh Gupta", "Umesh Deshpande", "Travis Janssen", "Swami Sundararaman"], "title": "Symbiosis: Multi-Adapter Inference and Fine-Tuning", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) allows model builders to capture the\ntask specific parameters into adapters, which are a fraction of the size of the\noriginal base model. Popularity of PEFT technique for fine-tuning has led to\ncreation of a large number of adapters for popular Large Language Models\n(LLMs). However, existing frameworks fall short in supporting inference or\nfine-tuning with multiple adapters in the following ways. 1) For fine-tuning,\neach job needs to deploy its dedicated base model instance, which results in\nexcessive GPU memory consumption and poor GPU utilization. 2) While popular\ninference platforms can serve multiple PEFT adapters, they do not allow\nindependent resource management or mixing of different PEFT methods. 3) They\ncannot share resources (such as base model instance) between inference and\nfine-tuning jobs. 4) They do not provide privacy to users who may not wish to\nexpose their fine-tuned parameters to service providers. In Symbiosis, we\naddress the above problems by enabling as-a-service deployment of base model.\nThe base model layers can be shared across multiple inference or fine-tuning\nprocesses. Our split-execution technique decouples the execution of\nclient-specific adapters and layers from the frozen base model layers offering\nthem flexibility to manage their resources, to select their fine-tuning method,\nto achieve their performance goals. Our approach is transparent to models and\nworks out-of-the-box for most models in the transformers library. Our\nevaluation on Llama2-13B shows the compared to baseline, Symbiosis can\nfine-tune 4X more adapters on the same set of GPUs in the same amount of time.", "AI": {"tldr": "Symbiosis\u6846\u67b6\u901a\u8fc7\u5171\u4eab\u57fa\u7840\u6a21\u578b\u5c42\u548c\u62c6\u5206\u6267\u884c\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u591a\u9002\u914d\u5668\u5fae\u8c03\u548c\u63a8\u7406\u4e2d\u7684\u8d44\u6e90\u6d6a\u8d39\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u5728\u591a\u9002\u914d\u5668\u5fae\u8c03\u548c\u63a8\u7406\u4e2d\u5b58\u5728GPU\u8d44\u6e90\u6d6a\u8d39\u3001\u7075\u6d3b\u6027\u4e0d\u8db3\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5171\u4eab\u57fa\u7840\u6a21\u578b\u5c42\u548c\u62c6\u5206\u6267\u884c\u6280\u672f\uff0c\u5c06\u5ba2\u6237\u7279\u5b9a\u9002\u914d\u5668\u4e0e\u57fa\u7840\u6a21\u578b\u89e3\u8026\u3002", "result": "\u5728Llama2-13B\u4e0a\uff0cSymbiosis\u53ef\u540c\u65f6\u5fae\u8c034\u500d\u4e8e\u57fa\u7ebf\u7684\u9002\u914d\u5668\u3002", "conclusion": "Symbiosis\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u7387\u548c\u7075\u6d3b\u6027\uff0c\u652f\u6301\u591a\u9002\u914d\u5668\u6df7\u5408\u4f7f\u7528\u548c\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2507.04256", "pdf": "https://arxiv.org/pdf/2507.04256", "abs": "https://arxiv.org/abs/2507.04256", "authors": ["Tang Qian", "Yifan Zhu", "Lu Chen", "Xiangyu Ke", "Jingwen Zhao", "Tianyi Li", "Yunjun Gao", "Christian S. Jensen"], "title": "OneDB: A Distributed Multi-Metric Data Similarity Search System", "categories": ["cs.DB"], "comment": null, "summary": "Increasingly massive volumes of multi-modal data are being accumulated in\nmany {real world} settings, including in health care and e-commerce. This\ndevelopment calls for effective general-purpose data management solutions for\nmulti-modal data. Such a solution must facilitate user-friendly and accurate\nretrieval of any multi-modal data according to diverse application\nrequirements. Further, such a solution must be capable of efficient and\nscalable retrieval.\n  To address this need, we present OneDB, a distributed multi-metric data\nsimilarity retrieval system. This system exploits the fact that data of diverse\nmodalities, such as text, images, and video, can be represented as metric data.\nThe system thus affords each data modality its own metric space with its own\ndistance function and then uses a multi-metric model to unify multi-modal data.\nThe system features several innovations: (i) an extended Spart SQL query\ninterface; (ii) lightweight means of learning appropriate weights of different\nmodalities when retrieving multi-modal data to enable accurate retrieval; (iii)\nsmart search-space pruning strategies that improve efficiency; (iv) two-layered\nindexing of data to ensure load-balancing during distributed processing; and\n(v) end-to-end system parameter autotuning.\n  Experiments on three real-life datasets and two synthetic datasets offer\nevidence that the system is capable of state-of-the-art performance: (i)\nefficient and effective weight learning; (ii) retrieval accuracy improvements\nof 12.63\\%--30.75\\% over the state-of-the-art vector similarity search system\nat comparable efficiency; (iii) accelerated search by 2.5--5.75x over\nstate-of-the-art single- or multi-metric solutions; (iv) demonstrated high\nscalability; and (v) parameter tuning that enables performance improvements of\n15+%.", "AI": {"tldr": "OneDB\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u591a\u5ea6\u91cf\u6570\u636e\u76f8\u4f3c\u6027\u68c0\u7d22\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u7ba1\u7406\uff0c\u901a\u8fc7\u521b\u65b0\u6280\u672f\u548c\u81ea\u52a8\u8c03\u4f18\u5b9e\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u7684\u68c0\u7d22\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u6a21\u6001\u6570\u636e\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u901a\u7528\u4e14\u9ad8\u6548\u7684\u6570\u636e\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u7528\u6237\u53cb\u597d\u4e14\u51c6\u786e\u7684\u68c0\u7d22\u3002", "method": "OneDB\u5229\u7528\u591a\u5ea6\u91cf\u6a21\u578b\u7edf\u4e00\u591a\u6a21\u6001\u6570\u636e\uff0c\u63d0\u4f9b\u6269\u5c55\u7684Spart SQL\u67e5\u8be2\u63a5\u53e3\u3001\u8f7b\u91cf\u7ea7\u6743\u91cd\u5b66\u4e60\u3001\u667a\u80fd\u526a\u679d\u7b56\u7565\u3001\u4e24\u5c42\u7d22\u5f15\u548c\u81ea\u52a8\u8c03\u4f18\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOneDB\u5728\u68c0\u7d22\u6548\u7387\u3001\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53c2\u6570\u8c03\u4f18\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "OneDB\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u81ea\u52a8\u8c03\u4f18\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u591a\u6a21\u6001\u6570\u636e\u68c0\u7d22\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.03522", "pdf": "https://arxiv.org/pdf/2507.03522", "abs": "https://arxiv.org/abs/2507.03522", "authors": ["Alexandre de Limas Santana", "Adri\u00e0 Armejach", "Francesc Martinez", "Erich Focht", "Marc Casas"], "title": "A Flexible Instruction Set Architecture for Efficient GEMMs", "categories": ["cs.AR", "cs.LG", "C.1.0"], "comment": null, "summary": "GEneral Matrix Multiplications (GEMMs) are recurrent in high-performance\ncomputing and deep learning workloads. Typically, high-end CPUs accelerate GEMM\nworkloads with Single-Instruction Multiple Data (SIMD) or vector Instruction\nSet Architectures (ISAs). Since these ISAs face significant issues when running\nGEMM workloads, particularly when dealing with small, tall, or skinny matrices,\nmatrix ISAs have been proposed and implemented by major hardware vendors in the\nlast years. Although these matrix ISAs deliver larger throughput when running\nGEMMs than their SIMD/vector counterparts, they are rigid solutions unable to\ndynamically adapt themselves to application-specific aspects like the data\nformat. This paper demonstrates that the state-of-the-art matrix ISAs deliver\nsuboptimal performance when running the most commonly used convolution and\ntransformer models.\n  This paper proposes the Matrix Tile Extension (MTE), the first matrix ISA\nthat completely decouples the instruction set architecture from the\nmicroarchitecture and seamlessly interacts with existing vector ISAs. MTE\nincurs minimal implementation overhead since it only requires a few additional\ninstructions and a 64-bit Control Status Register (CSR) to keep its state.\nSpecifically, MTE can i) vectorize GEMMs across the three dimensions M, N, and\nK; ii) leverage the capacity of the existing vector register file; and iii)\ndecouple the tile shape from the underlying microarchitecture. MTE achieves\nspeed-ups of 1.35x over the best state-of-the-art matrix ISA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Matrix Tile Extension (MTE)\uff0c\u4e00\u79cd\u65b0\u578b\u77e9\u9635\u6307\u4ee4\u96c6\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u77e9\u9635ISA\u5728\u8fd0\u884cGEMM\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u77e9\u9635ISA\u5728\u5904\u7406GEMM\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u6027\u80fd\u4e0d\u7406\u60f3\uff0c\u5c24\u5176\u662f\u5bf9\u5c0f\u578b\u6216\u975e\u6807\u51c6\u5f62\u72b6\u77e9\u9635\uff0c\u4e14\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u5e94\u7528\u7a0b\u5e8f\u7279\u6027\u3002", "method": "\u63d0\u51faMTE\uff0c\u901a\u8fc7\u89e3\u8026\u6307\u4ee4\u96c6\u67b6\u6784\u4e0e\u5fae\u67b6\u6784\uff0c\u6700\u5c0f\u5316\u5b9e\u73b0\u5f00\u9500\uff0c\u652f\u6301\u591a\u7ef4\u5411\u91cf\u5316\u548c\u5229\u7528\u73b0\u6709\u5411\u91cf\u5bc4\u5b58\u5668\u6587\u4ef6\u3002", "result": "MTE\u5728\u5e38\u89c1\u5377\u79ef\u548cTransformer\u6a21\u578b\u4e0a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u77e9\u9635ISA\uff0c\u901f\u5ea6\u63d0\u5347\u8fbe1.35\u500d\u3002", "conclusion": "MTE\u4e3a\u77e9\u9635\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.03527", "pdf": "https://arxiv.org/pdf/2507.03527", "abs": "https://arxiv.org/abs/2507.03527", "authors": ["Dulaji Hidellaarachchi", "John Grundy", "Rashina Hoda"], "title": "The Role of Humour in Software Engineering -- A Literature Review and Preliminary Taxonomy", "categories": ["cs.SE"], "comment": "Accepted to publish in Journal of Software Systems (JSS) New Idea\n  Track 2025 (23 pages, 1 figure)", "summary": "Humour has long been recognized as a key factor in enhancing creativity,\ngroup effectiveness, and employee well-being across various domains. However,\nits occurrence and impact within software engineering (SE) teams remains\nunder-explored. This paper introduces a comprehensive, literature review-based\ntaxonomy exploring the characterisation and use of humour in SE teams, with the\ngoal of boosting productivity, improving communication, and fostering a\npositive work environment while emphasising the responsible use of humour to\nmitigate its potential negative impacts. Drawing from a wide array of studies\nin psychology, sociology, and organizational behaviour, our proposed framework\ncategorizes humour into distinct theories, styles, models, and scales, offering\nSE professionals and researchers a structured approach to understanding humour\nin their work. This study also addresses the unique challenges of applying\nhumour in SE, highlighting its potential benefits while acknowledging the need\nfor further empirical validation in this context. Ultimately, our study aims to\npave the way for more cohesive, creative, and psychologically supportive SE\nenvironments through the strategic use of humour.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6cd5\uff0c\u7814\u7a76\u5e7d\u9ed8\u5728\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u4e2d\u7684\u7279\u5f81\u548c\u4f7f\u7528\uff0c\u65e8\u5728\u63d0\u5347\u751f\u4ea7\u529b\u3001\u6539\u5584\u6c9f\u901a\u5e76\u8425\u9020\u79ef\u6781\u7684\u5de5\u4f5c\u73af\u5883\uff0c\u540c\u65f6\u5f3a\u8c03\u8d1f\u8d23\u4efb\u7684\u5e7d\u9ed8\u4f7f\u7528\u4ee5\u907f\u514d\u6f5c\u5728\u8d1f\u9762\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u5e7d\u9ed8\u5728\u591a\u4e2a\u9886\u57df\u5df2\u88ab\u8bc1\u660e\u80fd\u63d0\u5347\u521b\u9020\u529b\u3001\u56e2\u961f\u6548\u80fd\u548c\u5458\u5de5\u798f\u7949\uff0c\u4f46\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u4e2d\u7684\u5177\u4f53\u5e94\u7528\u548c\u5f71\u54cd\u5c1a\u7f3a\u4e4f\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u5fc3\u7406\u5b66\u3001\u793e\u4f1a\u5b66\u548c\u7ec4\u7ec7\u884c\u4e3a\u5b66\u7684\u7814\u7a76\uff0c\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\uff0c\u5c06\u5e7d\u9ed8\u5206\u4e3a\u4e0d\u540c\u7684\u7406\u8bba\u3001\u98ce\u683c\u3001\u6a21\u578b\u548c\u91cf\u8868\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e13\u4e1a\u4eba\u58eb\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u7ed3\u6784\u5316\u7406\u89e3\u5e7d\u9ed8\u7684\u5de5\u5177\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u63d0\u4f9b\u4e86\u7406\u89e3\u548c\u5e94\u7528\u5e7d\u9ed8\u7684\u6307\u5357\uff0c\u540c\u65f6\u6307\u51fa\u9700\u8981\u8fdb\u4e00\u6b65\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\u4e2d\u7684\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u5408\u7406\u4f7f\u7528\u5e7d\u9ed8\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u4fc3\u8fdb\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u7684\u51dd\u805a\u529b\u3001\u521b\u9020\u529b\u548c\u5fc3\u7406\u652f\u6301\u6027\u5de5\u4f5c\u73af\u5883\u3002"}}
{"id": "2507.04084", "pdf": "https://arxiv.org/pdf/2507.04084", "abs": "https://arxiv.org/abs/2507.04084", "authors": ["Xin Cao", "Haoyu Wang", "Yuzhu Mao", "Xinda Liu", "Linzhi Su", "Kang Li"], "title": "Attention-Guided Multi-Scale Local Reconstruction for Point Clouds via Masked Autoencoder Self-Supervised Learning", "categories": ["cs.GR", "cs.CV"], "comment": "22 pages", "summary": "Self-supervised learning has emerged as a prominent research direction in\npoint cloud processing. While existing models predominantly concentrate on\nreconstruction tasks at higher encoder layers, they often neglect the effective\nutilization of low-level local features, which are typically employed solely\nfor activation computations rather than directly contributing to reconstruction\ntasks. To overcome this limitation, we introduce PointAMaLR, a novel\nself-supervised learning framework that enhances feature representation and\nprocessing accuracy through attention-guided multi-scale local reconstruction.\nPointAMaLR implements hierarchical reconstruction across multiple local\nregions, with lower layers focusing on fine-scale feature restoration while\nupper layers address coarse-scale feature reconstruction, thereby enabling\ncomplex inter-patch interactions. Furthermore, to augment feature\nrepresentation capabilities, we incorporate a Local Attention (LA) module in\nthe embedding layer to enhance semantic feature understanding. Comprehensive\nexperiments on benchmark datasets ModelNet and ShapeNet demonstrate\nPointAMaLR's superior accuracy and quality in both classification and\nreconstruction tasks. Moreover, when evaluated on the real-world dataset\nScanObjectNN and the 3D large scene segmentation dataset S3DIS, our model\nachieves highly competitive performance metrics. These results not only\nvalidate PointAMaLR's effectiveness in multi-scale semantic understanding but\nalso underscore its practical applicability in real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPointAMaLR\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u591a\u5c3a\u5ea6\u5c40\u90e8\u91cd\u5efa\u6765\u589e\u5f3a\u70b9\u4e91\u7279\u5f81\u8868\u793a\u548c\u5904\u7406\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u591a\u5173\u6ce8\u9ad8\u5c42\u7f16\u7801\u5668\u7684\u91cd\u5efa\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u4f4e\u5c42\u5c40\u90e8\u7279\u5f81\u7684\u6709\u6548\u5229\u7528\uff0c\u8fd9\u4e9b\u7279\u5f81\u901a\u5e38\u4ec5\u7528\u4e8e\u6fc0\u6d3b\u8ba1\u7b97\u800c\u975e\u76f4\u63a5\u53c2\u4e0e\u91cd\u5efa\u3002", "method": "PointAMaLR\u5b9e\u73b0\u591a\u5c42\u6b21\u5c40\u90e8\u533a\u57df\u7684\u5206\u5c42\u91cd\u5efa\uff0c\u4f4e\u5c42\u4e13\u6ce8\u4e8e\u7ec6\u7c92\u5ea6\u7279\u5f81\u6062\u590d\uff0c\u9ad8\u5c42\u8d1f\u8d23\u7c97\u7c92\u5ea6\u7279\u5f81\u91cd\u5efa\uff0c\u5e76\u7ed3\u5408\u5c40\u90e8\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728ModelNet\u3001ShapeNet\u3001ScanObjectNN\u548cS3DIS\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u7c7b\u548c\u91cd\u5efa\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u591a\u5c3a\u5ea6\u8bed\u4e49\u7406\u89e3\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "PointAMaLR\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u6709\u6548\u63d0\u5347\u4e86\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u4e5f\u5c55\u73b0\u51fa\u9ad8\u5ea6\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2507.03448", "pdf": "https://arxiv.org/pdf/2507.03448", "abs": "https://arxiv.org/abs/2507.03448", "authors": ["Franco Galante", "Chiara Ravazzi", "Luca Vassio", "Michele Garetto", "Emilio Leonardi"], "title": "Dominance or Fair Play in Social Networks? A Model of Influencer Popularity Dynamic", "categories": ["cs.SI", "cs.PF"], "comment": "18 pages", "summary": "This paper presents a data-driven mean-field approach to model the popularity\ndynamics of users seeking public attention, i.e., influencers. We propose a\nnovel analytical model that integrates individual activity patterns, expertise\nin producing viral content, exogenous events, and the platform's role in\nvisibility enhancement, ultimately determining each influencer's success. We\nanalytically derive sufficient conditions for system ergodicity, enabling\npredictions of popularity distributions. A sensitivity analysis explores\nvarious system configurations, highlighting conditions favoring either\ndominance or fair play among influencers. Our findings offer valuable insights\ninto the potential evolution of social networks towards more equitable or\nbiased influence ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u5e73\u5747\u573a\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u5bfb\u6c42\u516c\u4f17\u5173\u6ce8\u7684\u7528\u6237\uff08\u5373\u5f71\u54cd\u8005\uff09\u7684\u6d41\u884c\u5ea6\u52a8\u6001\u3002", "motivation": "\u7814\u7a76\u5f71\u54cd\u8005\u6d41\u884c\u5ea6\u52a8\u6001\u7684\u5efa\u6a21\uff0c\u4ee5\u7406\u89e3\u4e2a\u4f53\u6d3b\u52a8\u6a21\u5f0f\u3001\u4e13\u4e1a\u77e5\u8bc6\u3001\u5916\u751f\u4e8b\u4ef6\u548c\u5e73\u53f0\u4f5c\u7528\u5bf9\u6210\u529f\u7684\u5f71\u54cd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u6790\u6a21\u578b\uff0c\u6574\u5408\u4e86\u591a\u79cd\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u89e3\u6790\u63a8\u5bfc\u7cfb\u7edf\u904d\u5386\u6027\u6761\u4ef6\u6765\u9884\u6d4b\u6d41\u884c\u5ea6\u5206\u5e03\u3002", "result": "\u654f\u611f\u6027\u5206\u6790\u63ed\u793a\u4e86\u7cfb\u7edf\u914d\u7f6e\u5bf9\u4e0d\u540c\u7ed3\u679c\uff08\u5982\u4e3b\u5bfc\u6216\u516c\u5e73\u7ade\u4e89\uff09\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u793e\u4ea4\u7f51\u7edc\u5411\u66f4\u516c\u5e73\u6216\u66f4\u504f\u9887\u7684\u5f71\u54cd\u529b\u751f\u6001\u7cfb\u7edf\u6f14\u53d8\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.03317", "pdf": "https://arxiv.org/pdf/2507.03317", "abs": "https://arxiv.org/abs/2507.03317", "authors": ["Don Tan"], "title": "Low-power Wireless Network with Real-Time Guarantees for Edge-Cloud Applications", "categories": ["cs.NI"], "comment": "51 pages, 24 figures", "summary": "The goal of this project is to explore the feasibility of building a scalable\n& easy-to-deploy real-time LoRa testbed, made from multiple units of Raspberry\nPi (RPI), where each RPI manages its own set of LoRa radios. This project is\nmotivated by the lack of concrete large-scale LoRa testbeds that effectively\nintegrate LoRa communications into the real-time world. The paper introduces\nhow the idea of using RPI came about and why it should work in theory. The\npaper then carries out experiments on a component of the large-scale testbed,\nto evaluate the feasibility of the said component based on performance metrics\nsuch as RSSI, SNR, PLR and the ability to carry out millisecond-accurate\ntransmissions. The performance metrics are also used to explore the impact of\nusing different combinations of spread factors and transmission frequencies, as\nwell as making comparisons between time-division multiple access (TDMA) and\ncarrier-sense multiple access (CSMA) approaches. The results show that with the\nright parameters configured, the system can achieve stable and low-latency\ncommunications, proving some feasibility to operate under real-time situations.\nFuture work includes giving each RPI control over more radios, carrying out\ntrue parallel transmissions, and finally integrating multiple RPIs for a more\ncomplete large-scale real-time LoRa testbed.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u6811\u8393\u6d3e\uff08RPI\uff09\u6784\u5efa\u53ef\u6269\u5c55\u3001\u6613\u90e8\u7f72\u7684\u5b9e\u65f6LoRa\u6d4b\u8bd5\u5e73\u53f0\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u6307\u6807\u53ca\u53c2\u6570\u914d\u7f6e\u5bf9\u901a\u4fe1\u7a33\u5b9a\u6027\u548c\u5ef6\u8fdf\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u80fd\u591f\u5c06LoRa\u901a\u4fe1\u6709\u6548\u6574\u5408\u5230\u5b9e\u65f6\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21\u6d4b\u8bd5\u5e73\u53f0\uff0c\u56e0\u6b64\u7814\u7a76\u57fa\u4e8eRPI\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u591a\u53f0RPI\u7ba1\u7406\u5404\u81eaLoRa\u65e0\u7ebf\u7535\uff0c\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u6027\u80fd\u6307\u6807\uff08RSSI\u3001SNR\u3001PLR\u7b49\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u914d\u7f6e\uff08\u6269\u9891\u56e0\u5b50\u3001\u4f20\u8f93\u9891\u7387\uff09\u53ca\u591a\u5740\u63a5\u5165\u65b9\u5f0f\uff08TDMA\u4e0eCSMA\uff09\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5408\u9002\u53c2\u6570\u914d\u7f6e\u4e0b\uff0c\u7cfb\u7edf\u53ef\u5b9e\u73b0\u7a33\u5b9a\u4e14\u4f4e\u5ef6\u8fdf\u7684\u901a\u4fe1\uff0c\u5177\u5907\u5b9e\u65f6\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8bc1\u5b9e\u4e86RPI\u65b9\u6848\u7684\u7406\u8bba\u53ef\u884c\u6027\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u6269\u5c55\u5355\u53f0RPI\u63a7\u5236\u7684\u65e0\u7ebf\u7535\u6570\u91cf\uff0c\u5b9e\u73b0\u771f\u6b63\u5e76\u884c\u4f20\u8f93\u53ca\u591aRPI\u96c6\u6210\u3002"}}
{"id": "2507.04286", "pdf": "https://arxiv.org/pdf/2507.04286", "abs": "https://arxiv.org/abs/2507.04286", "authors": ["S. Akshay", "Ouldouz Neysari", "\u0110or\u0111e \u017dikeli\u0107"], "title": "Omega-regular Verification and Control for Distributional Specifications in MDPs", "categories": ["cs.LO"], "comment": "Accepted at CONCUR 2025", "summary": "A classical approach to studying Markov decision processes (MDPs) is to view\nthem as state transformers. However, MDPs can also be viewed as distribution\ntransformers, where an MDP under a strategy generates a sequence of probability\ndistributions over MDP states. This view arises in several applications, even\nas the probabilistic model checking problem becomes much harder compared to the\nclassical state transformer counterpart. It is known that even distributional\nreachability and safety problems become computationally intractable (Skolem-\nand positivity-hard). To address this challenge, recent works focused on sound\nbut possibly incomplete methods for verification and control of MDPs under the\ndistributional view. However, existing automated methods are applicable only to\ndistributional reachability, safety and reach-avoidance specifications.\n  In this work, we present the first automated method for verification and\ncontrol of MDPs with respect to distributional omega-regular specifications. To\nachieve this, we propose a novel notion of distributional certificates, which\nare sound and complete proof rules for proving that an MDP under a\ndistributionally memoryless strategy satisfies some distributional\nomega-regular specification. We then use our distributional certificates to\ndesign the first fully automated algorithms for verification and control of\nMDPs with respect to distributional omega-regular specifications. Our\nalgorithms follow a template-based synthesis approach and provide soundness and\nrelative completeness guarantees, while running in PSPACE. Our prototype\nimplementation demonstrates practical applicability of our algorithms to\nchallenging examples collected from the literature.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u548c\u63a7\u5236\u6ee1\u8db3\u5206\u5e03\u03c9-\u6b63\u5219\u89c4\u8303\u7684MDP\uff0c\u63d0\u51fa\u4e86\u5206\u5e03\u8bc1\u660e\u89c4\u5219\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9996\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3MDP\u5728\u5206\u5e03\u89c6\u56fe\u4e0b\u9a8c\u8bc1\u548c\u63a7\u5236\u4e2d\u7684\u8ba1\u7b97\u96be\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5206\u5e03\u03c9-\u6b63\u5219\u89c4\u8303\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5e03\u8bc1\u660e\u89c4\u5219\uff08\u5206\u5e03\u8bc1\u660e\u4e66\uff09\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u6a21\u677f\u7684\u5408\u6210\u7b97\u6cd5\uff0c\u786e\u4fdd\u7b97\u6cd5\u7684\u6b63\u786e\u6027\u4e0e\u76f8\u5bf9\u5b8c\u5907\u6027\u3002", "result": "\u7b97\u6cd5\u5728PSPACE\u590d\u6742\u5ea6\u4e0b\u8fd0\u884c\uff0c\u539f\u578b\u5b9e\u73b0\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u6311\u6218\u6027\u95ee\u9898\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86MDP\u5728\u5206\u5e03\u03c9-\u6b63\u5219\u89c4\u8303\u9a8c\u8bc1\u548c\u63a7\u5236\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u590d\u6742\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.02869", "pdf": "https://arxiv.org/pdf/2507.02869", "abs": "https://arxiv.org/abs/2507.02869", "authors": ["Nima Yazdani", "Aruj Mahajan", "Ali Ansari"], "title": "Zara: An LLM-based Candidate Interview Feedback System", "categories": ["cs.HC"], "comment": "11 pages, 8 figures", "summary": "This paper introduces Zara, an AI-driven recruitment support system developed\nby micro1, as a practical case study illustrating how large language models\n(LLMs) can enhance the candidate experience through personalized, scalable\ninterview support. Traditionally, recruiters have struggled to deliver\nindividualized candidate feedback due to logistical and legal constraints,\nresulting in widespread candidate dissatisfaction. Leveraging OpenAI's GPT-4o,\nZara addresses these limitations by dynamically generating personalized\npractice interviews, conducting conversational AI-driven assessments,\nautonomously delivering structured and actionable feedback, and efficiently\nanswering candidate inquiries using a Retrieval-Augmented Generation (RAG)\nsystem. To promote transparency, we have open-sourced the approach Zara uses to\ngenerate candidate feedback.", "AI": {"tldr": "Zara\u662f\u4e00\u4e2a\u57fa\u4e8eGPT-4o\u7684AI\u62db\u8058\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u9762\u8bd5\u652f\u6301\u63d0\u5347\u5019\u9009\u4eba\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u62db\u8058\u56e0\u7269\u6d41\u548c\u6cd5\u5f8b\u9650\u5236\u96be\u4ee5\u63d0\u4f9b\u4e2a\u6027\u5316\u53cd\u9988\uff0c\u5bfc\u81f4\u5019\u9009\u4eba\u4e0d\u6ee1\u610f\u3002", "method": "\u5229\u7528GPT-4o\u52a8\u6001\u751f\u6210\u4e2a\u6027\u5316\u6a21\u62df\u9762\u8bd5\u3001AI\u9a71\u52a8\u8bc4\u4f30\u3001\u7ed3\u6784\u5316\u53cd\u9988\u548cRAG\u7cfb\u7edf\u56de\u7b54\u95ee\u9898\u3002", "result": "\u7cfb\u7edf\u9ad8\u6548\u63d0\u4f9b\u4e2a\u6027\u5316\u652f\u6301\uff0c\u5e76\u5f00\u6e90\u53cd\u9988\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "Zara\u5c55\u793a\u4e86LLMs\u5982\u4f55\u6539\u8fdb\u62db\u8058\u6d41\u7a0b\u7684\u5019\u9009\u4f53\u9a8c\u3002"}}
{"id": "2507.02919", "pdf": "https://arxiv.org/pdf/2507.02919", "abs": "https://arxiv.org/abs/2507.02919", "authors": ["Dai Li", "Linzhuo Li", "Huilian Sophie Qiu"], "title": "ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models", "categories": ["cs.CL", "cs.CY", "cs.ET"], "comment": null, "summary": "Large language models (LLMs) in the form of chatbots like ChatGPT and Llama\nare increasingly proposed as \"silicon samples\" for simulating human opinions.\nThis study examines this notion, arguing that LLMs may misrepresent\npopulation-level opinions. We identify two fundamental challenges: a failure in\nstructural consistency, where response accuracy doesn't hold across demographic\naggregation levels, and homogenization, an underrepresentation of minority\nopinions. To investigate these, we prompted ChatGPT (GPT-4) and Meta's Llama\n3.1 series (8B, 70B, 405B) with questions on abortion and unauthorized\nimmigration from the American National Election Studies (ANES) 2020. Our\nfindings reveal significant structural inconsistencies and severe\nhomogenization in LLM responses compared to human data. We propose an\n\"accuracy-optimization hypothesis,\" suggesting homogenization stems from\nprioritizing modal responses. These issues challenge the validity of using\nLLMs, especially chatbots AI, as direct substitutes for human survey data,\npotentially reinforcing stereotypes and misinforming policy.", "AI": {"tldr": "\u7814\u7a76\u6307\u51fa\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u4eba\u7c7b\u610f\u89c1\u65f6\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4eba\u53e3\u5c42\u9762\u7684\u89c2\u70b9\uff0c\u5b58\u5728\u7ed3\u6784\u4e0d\u4e00\u81f4\u6027\u548c\u5c11\u6570\u6d3e\u610f\u89c1\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22LLMs\u662f\u5426\u80fd\u4f5c\u4e3a\u2018\u7845\u6837\u672c\u2019\u53ef\u9760\u5730\u6a21\u62df\u4eba\u7c7b\u89c2\u70b9\uff0c\u5c24\u5176\u662f\u4e0e\u771f\u5b9e\u8c03\u67e5\u6570\u636e\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5411ChatGPT\u548cMeta\u7684Llama\u6a21\u578b\u63d0\u95eeANES 2020\u4e2d\u7684\u95ee\u9898\uff08\u5982\u5815\u80ce\u548c\u975e\u6cd5\u79fb\u6c11\uff09\uff0c\u5206\u6790\u5176\u56de\u7b54\u7ed3\u6784\u3002", "result": "\u53d1\u73b0LLMs\u5b58\u5728\u663e\u8457\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\u6027\u548c\u5c11\u6570\u6d3e\u610f\u89c1\u4e25\u91cd\u4e0d\u8db3\u73b0\u8c61\u3002", "conclusion": "LLMs\u4e0d\u9002\u5408\u76f4\u63a5\u66ff\u4ee3\u4eba\u7c7b\u8c03\u67e5\u6570\u636e\uff0c\u53ef\u80fd\u5bfc\u81f4\u504f\u89c1\u548c\u653f\u7b56\u8bef\u5bfc\u3002"}}
{"id": "2507.04316", "pdf": "https://arxiv.org/pdf/2507.04316", "abs": "https://arxiv.org/abs/2507.04316", "authors": ["Jay Lee"], "title": "Retargeting an Abstract Interpreter for a New Language by Partial Evaluation", "categories": ["cs.PL"], "comment": "Presented at the Student Research Competition (SRC) at PLDI 2025\n  (https://pldi25.sigplan.org/details/pldi-2025-src/1/)", "summary": "It is well-known that abstract interpreters can be systematically derived\nfrom their concrete counterparts using a \"recipe,\" but developing sound static\nanalyzers remains a time-consuming task. Reducing the effort required and\nmechanizing the process of developing analyzers continues to be a significant\nchallenge. Is it possible to automatically retarget an existing abstract\ninterpreter for a new language?\n  We propose a novel technique to automatically derive abstract interpreters\nfor various languages from an existing abstract interpreter. By leveraging\npartial evaluation, we specialize an abstract interpreter for a source\nlanguage. The specialization is performed using the semantics of target\nlanguages written in the source language. Our approach eliminates the need to\ndevelop analyzers for new targets from scratch. We show that our method can\neffectively retarget an abstract interpreter for one language into a correct\nanalyzer for another language.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u90e8\u5206\u8bc4\u4f30\u81ea\u52a8\u4e3a\u4e0d\u540c\u8bed\u8a00\u751f\u6210\u62bd\u8c61\u89e3\u91ca\u5668\u7684\u65b0\u6280\u672f\uff0c\u65e0\u9700\u4ece\u5934\u5f00\u53d1\u65b0\u8bed\u8a00\u7684\u5206\u6790\u5668\u3002", "motivation": "\u51cf\u5c11\u5f00\u53d1\u9759\u6001\u5206\u6790\u5668\u7684\u65f6\u95f4\u548c\u7cbe\u529b\uff0c\u5b9e\u73b0\u62bd\u8c61\u89e3\u91ca\u5668\u7684\u81ea\u52a8\u91cd\u5b9a\u5411\u3002", "method": "\u5229\u7528\u90e8\u5206\u8bc4\u4f30\u6280\u672f\uff0c\u57fa\u4e8e\u6e90\u8bed\u8a00\u7684\u62bd\u8c61\u89e3\u91ca\u5668\uff0c\u901a\u8fc7\u76ee\u6807\u8bed\u8a00\u7684\u8bed\u4e49\u8fdb\u884c\u7279\u5316\u3002", "result": "\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u4e00\u4e2a\u8bed\u8a00\u7684\u62bd\u8c61\u89e3\u91ca\u5668\u91cd\u5b9a\u5411\u4e3a\u53e6\u4e00\u4e2a\u8bed\u8a00\u7684\u6b63\u786e\u5206\u6790\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u7b80\u5316\u4e86\u5f00\u53d1\u8fc7\u7a0b\uff0c\u4e3a\u591a\u8bed\u8a00\u9759\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u9014\u5f84\u3002"}}
{"id": "2507.03305", "pdf": "https://arxiv.org/pdf/2507.03305", "abs": "https://arxiv.org/abs/2507.03305", "authors": ["Yong-Cheng Liaw", "Shuo-Han Chen"], "title": "Analysis and Optimized CXL-Attached Memory Allocation for Long-Context LLM Fine-Tuning", "categories": ["cs.DC"], "comment": "9 pages, 10 figures, 2 tables", "summary": "The growing prevalence of Large Language Models (LLMs) and their substantial\nmemory requirements have prompted renewed interest in CPU offloading as a\nmethod to compensate for limited GPU memory. In particular, when CPU memory is\nleveraged to temporarily store intermediate states of LLMs, CPU memory becomes\na new bottleneck and soon reaches the capacity limitation of commodity CPUs. In\nthis work, we investigate the effectiveness of Compute Express Link (CXL)\nadd-in card (AIC) memory as an extension to CPU memory, enabling larger model\nsizes and longer context lengths during fine-tuning. Through extensive\nbenchmarking, this study quantifies the performance overhead introduced by\ntransferring data between CXL memory, CPU, and GPUs, focusing on how\nconcurrency and data volume influence bandwidth utilization and latency. This\nstudy also compares CPUbased optimizer steps when model parameters, gradients,\nand optimizer states reside in local memory versus CXL memory, revealing that\nnaive adoption of CXL often degrades performance during the optimizer phase. To\novercome these challenges, this study proposes a CXL-aware allocation to\nstrategically partition CPU offloading workloads across both local and CXL\nmemory. This study further demonstrates that employing multiple AICs\nsignificantly reduces bandwidth contention, thus improving scalability.\nExperimental results show that these optimizations enable efficient\nlong-context LLM fine-tuning, underscoring CXL as a promising avenue for\nunlocking the full potential of CPU offloading in long-context LLM fine-tuning.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528CXL\u5185\u5b58\u6269\u5c55CPU\u5185\u5b58\u4ee5\u652f\u6301\u66f4\u5927\u6a21\u578b\u548c\u66f4\u957f\u4e0a\u4e0b\u6587\u5fae\u8c03\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e86CXL\u611f\u77e5\u5206\u914d\u7b56\u7565\u4ee5\u51cf\u5c11\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u5c55\u793a\u4e86\u591aAIC\u51cf\u5c11\u5e26\u5bbd\u4e89\u7528\u7684\u4f18\u52bf\u3002", "motivation": "\u968f\u7740LLMs\u7684\u666e\u53ca\uff0cCPU\u5185\u5b58\u6210\u4e3a\u74f6\u9888\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7CXL\u5185\u5b58\u6269\u5c55\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u91cf\u5316CXL\u5185\u5b58\u4e0eCPU\u3001GPU\u95f4\u6570\u636e\u4f20\u8f93\u7684\u6027\u80fd\u5f00\u9500\uff0c\u63d0\u51faCXL\u611f\u77e5\u5206\u914d\u7b56\u7565\u3002", "result": "\u4f18\u5316\u540e\u7684\u65b9\u6848\u652f\u6301\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587LLM\u5fae\u8c03\uff0cCXL\u6210\u4e3a\u91ca\u653eCPU\u5378\u8f7d\u6f5c\u529b\u7684\u9014\u5f84\u3002", "conclusion": "CXL\u5185\u5b58\u6269\u5c55\u662fCPU\u5378\u8f7d\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u540e\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.04687", "pdf": "https://arxiv.org/pdf/2507.04687", "abs": "https://arxiv.org/abs/2507.04687", "authors": ["Zhenwei Dai", "Chuan Lei", "Asterios Katsifodimos", "Xiao Qin", "Christos Faloutsos", "Huzefa Rangwala"], "title": "AKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset Discovery in Data Lakes", "categories": ["cs.DB"], "comment": "13 pages", "summary": "How to generate a large, realistic set of tables along with joinability\nrelationships, to stress-test dataset discovery methods? Dataset discovery\nmethods aim to automatically identify related data assets in a data lake. The\ndevelopment and evaluation of such solutions for customers from a wide range of\nbusiness domains, relies on diverse, high quality and domain-specific tabular\nbenchmarks. Large language models (LLMs) are trained on a wide variety of text\ndata, which can provide a strong foundation of general and domain-specific\nknowledge. In this paper, we ask the question -- \\textit{can we leverage LLMs\nto generate a tabular benchmark adequate for evaluating the dataset discovery\nsolutions?} In particular, we focus on the task of finding joinable tables\nwhich is the cornerstone of virtually every dataset discovery method. Current\ncorpora for evaluating dataset discovery methods are mainly based on subsets of\nopen data, and they suffer from three important issues: $i)$ they focus on very\ncommon and generic data types (e.g., address, id, name, etc.); $ii)$ they do\nnot contain human-annotated column pairs; instead, practitioners synthesize\nground truth using table splits (e.g., horizontal for table union search and\nvertical ones for joinability) and $iii)$ they do not focus on semantic column\nrelationships.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u8868\u683c\u6570\u636e\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u96c6\u53d1\u73b0\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u8868\u8fde\u63a5\u6027\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u53d1\u73b0\u65b9\u6cd5\u7684\u57fa\u51c6\u6570\u636e\u96c6\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u6570\u636e\u7c7b\u578b\u5355\u4e00\u3001\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\u5217\u5bf9\u3001\u5ffd\u89c6\u8bed\u4e49\u5217\u5173\u7cfb\u3002LLMs\u56e0\u5176\u5e7f\u6cdb\u7684\u901a\u7528\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u63d0\u4f9b\u4e86\u751f\u6210\u66f4\u4f18\u57fa\u51c6\u6570\u636e\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u5229\u7528LLMs\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u8868\u683c\u6570\u636e\uff0c\u5e76\u5305\u542b\u4eba\u5de5\u6807\u6ce8\u7684\u5217\u5bf9\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6570\u636e\u7684\u4e0d\u8db3\u3002", "result": "LLMs\u53ef\u4ee5\u751f\u6210\u9002\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u96c6\u53d1\u73b0\u65b9\u6cd5\u7684\u8868\u683c\u6570\u636e\uff0c\u7279\u522b\u662f\u8868\u8fde\u63a5\u6027\u4efb\u52a1\u3002", "conclusion": "LLMs\u4e3a\u6784\u5efa\u66f4\u4f18\u7684\u6570\u636e\u96c6\u53d1\u73b0\u65b9\u6cd5\u8bc4\u4f30\u57fa\u51c6\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u672a\u6765\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.04276", "pdf": "https://arxiv.org/pdf/2507.04276", "abs": "https://arxiv.org/abs/2507.04276", "authors": ["Gwok-Waa Wan", "Shengchu Su", "Ruihu Wang", "Qixiang Chen", "Sam-Zaak Wong", "Mengnv Xing", "Hefei Feng", "Yubo Wang", "Yinan Zhu", "Jingyi Zhang", "Jianmin Ye", "Xinlai Wan", "Tao Ni", "Qiang Xu", "Nan Guan", "Zhe Jiang", "Xi Wang", "Yang Jun"], "title": "FIXME: Towards End-to-End Benchmarking of LLM-Aided Design Verification", "categories": ["cs.AR"], "comment": null, "summary": "Despite the transformative potential of Large Language Models (LLMs) in\nhardware design, a comprehensive evaluation of their capabilities in design\nverification remains underexplored. Current efforts predominantly focus on RTL\ngeneration and basic debugging, overlooking the critical domain of functional\nverification, which is the primary bottleneck in modern design methodologies\ndue to the rapid escalation of hardware complexity. We present FIXME, the first\nend-to-end, multi-model, and open-source evaluation framework for assessing LLM\nperformance in hardware functional verification (FV) to address this crucial\ngap. FIXME introduces a structured three-level difficulty hierarchy spanning\nsix verification sub-domains and 180 diverse tasks, enabling in-depth analysis\nacross the design lifecycle. Leveraging a collaborative AI-human approach, we\nconstruct a high-quality dataset using 100% silicon-proven designs, ensuring\ncomprehensive coverage of real-world challenges. Furthermore, we enhance the\nfunctional coverage by 45.57% through expert-guided optimization. By rigorously\nevaluating state-of-the-art LLMs such as GPT-4, Claude3, and LlaMA3, we\nidentify key areas for improvement and outline promising research directions to\nunlock the full potential of LLM-driven automation in hardware design\nverification. The benchmark is available at\nhttps://github.com/ChatDesignVerification/FIXME.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFIXME\u6846\u67b6\uff0c\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u786c\u4ef6\u529f\u80fd\u9a8c\u8bc1\u9886\u57df\u6027\u80fd\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u586b\u8865\u4e86\u5f53\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8RTL\u751f\u6210\u548c\u57fa\u7840\u8c03\u8bd5\uff0c\u5ffd\u7565\u4e86\u529f\u80fd\u9a8c\u8bc1\u8fd9\u4e00\u5173\u952e\u9886\u57df\uff0c\u968f\u7740\u786c\u4ef6\u590d\u6742\u5ea6\u7684\u5feb\u901f\u63d0\u5347\uff0c\u529f\u80fd\u9a8c\u8bc1\u6210\u4e3a\u73b0\u4ee3\u8bbe\u8ba1\u65b9\u6cd5\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "FIXME\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u3001\u591a\u6a21\u578b\u7684\u5f00\u6e90\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u4e09\u5c42\u6b21\u96be\u5ea6\u7ed3\u6784\u548c\u516d\u4e2a\u9a8c\u8bc1\u5b50\u9886\u57df\u7684180\u9879\u4efb\u52a1\uff0c\u91c7\u7528AI\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u65b9\u5f0f\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u8986\u76d6100%\u7845\u9a8c\u8bc1\u8bbe\u8ba1\u3002", "result": "\u901a\u8fc7\u4e13\u5bb6\u6307\u5bfc\u4f18\u5316\uff0c\u529f\u80fd\u8986\u76d6\u7387\u63d0\u5347\u4e8645.57%\uff0c\u5e76\u8bc4\u4f30\u4e86GPT-4\u3001Claude3\u548cLlaMA3\u7b49\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "conclusion": "FIXME\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u9a8c\u8bc1\u4e2d\u7684LLM\u9a71\u52a8\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u6307\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u548c\u7814\u7a76\u6f5c\u529b\u3002"}}
{"id": "2507.04758", "pdf": "https://arxiv.org/pdf/2507.04758", "abs": "https://arxiv.org/abs/2507.04758", "authors": ["Jiayun Hu", "Yueyi He", "Tianyi Liang", "Changbo Wang", "Chenhui Li"], "title": "Music2Palette: Emotion-aligned Color Palette Generation via Cross-Modal Representation Learning", "categories": ["cs.MM"], "comment": null, "summary": "Emotion alignment between music and palettes is crucial for effective\nmultimedia content, yet misalignment creates confusion that weakens the\nintended message. However, existing methods often generate only a single\ndominant color, missing emotion variation. Others rely on indirect mappings\nthrough text or images, resulting in the loss of crucial emotion details. To\naddress these challenges, we present Music2Palette, a novel method for\nemotion-aligned color palette generation via cross-modal representation\nlearning. We first construct MuCED, a dataset of 2,634 expert-validated\nmusic-palette pairs aligned through Russell-based emotion vectors. To directly\ntranslate music into palettes, we propose a cross-modal representation learning\nframework with a music encoder and color decoder. We further propose a\nmulti-objective optimization approach that jointly enhances emotion alignment,\ncolor diversity, and palette coherence. Extensive experiments demonstrate that\nour method outperforms current methods in interpreting music emotion and\ngenerating attractive and diverse color palettes. Our approach enables\napplications like music-driven image recoloring, video generating, and data\nvisualization, bridging the gap between auditory and visual emotion\nexperiences.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Music2Palette\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u751f\u6210\u4e0e\u97f3\u4e50\u60c5\u611f\u5bf9\u9f50\u7684\u8c03\u8272\u677f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5e38\u4ec5\u751f\u6210\u5355\u4e00\u4e3b\u8272\u6216\u4f9d\u8d56\u95f4\u63a5\u6620\u5c04\uff0c\u5bfc\u81f4\u60c5\u611f\u7ec6\u8282\u4e22\u5931\uff0c\u5f71\u54cd\u591a\u5a92\u4f53\u5185\u5bb9\u6548\u679c\u3002", "method": "\u6784\u5efa\u4e86MuCED\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u97f3\u4e50\u7f16\u7801\u5668\u548c\u989c\u8272\u89e3\u7801\u5668\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u589e\u5f3a\u60c5\u611f\u5bf9\u9f50\u3001\u989c\u8272\u591a\u6837\u6027\u4e0e\u8c03\u8272\u677f\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u97f3\u4e50\u60c5\u611f\u89e3\u8bfb\u53ca\u751f\u6210\u591a\u6837\u5316\u8c03\u8272\u677f\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Music2Palette\u586b\u8865\u4e86\u542c\u89c9\u4e0e\u89c6\u89c9\u60c5\u611f\u4f53\u9a8c\u95f4\u7684\u9e3f\u6c9f\uff0c\u9002\u7528\u4e8e\u97f3\u4e50\u9a71\u52a8\u7684\u56fe\u50cf\u91cd\u65b0\u7740\u8272\u3001\u89c6\u9891\u751f\u6210\u7b49\u5e94\u7528\u3002"}}
{"id": "2507.03536", "pdf": "https://arxiv.org/pdf/2507.03536", "abs": "https://arxiv.org/abs/2507.03536", "authors": ["Adam Tornhill", "Markus Borg", "Nadim Hagatulah", "Emma S\u00f6derberg"], "title": "ACE: Automated Technical Debt Remediation with Validated Large Language Model Refactorings", "categories": ["cs.SE"], "comment": "Published in proceedings of the 1st International Workshop on\n  Artificial Intelligence for Integrated Development Environments (AI-IDE)\n  (2025)", "summary": "The remarkable advances in AI and Large Language Models (LLMs) have enabled\nmachines to write code, accelerating the growth of software systems. However,\nthe bottleneck in software development is not writing code but understanding\nit; program understanding is the dominant activity, consuming approximately 70%\nof developers' time. This implies that improving existing code to make it\neasier to understand has a high payoff and - in the age of AI-assisted coding -\nis an essential activity to ensure that a limited pool of developers can keep\nup with ever-growing codebases. This paper introduces Augmented Code\nEngineering (ACE), a tool that automates code improvements using validated LLM\noutput. Developed through a data-driven approach, ACE provides reliable\nrefactoring suggestions by considering both objective code quality improvements\nand program correctness. Early feedback from users suggests that AI-enabled\nrefactoring helps mitigate code-level technical debt that otherwise rarely gets\nacted upon.", "AI": {"tldr": "ACE\u5de5\u5177\u5229\u7528LLM\u81ea\u52a8\u6539\u8fdb\u4ee3\u7801\uff0c\u63d0\u5347\u4ee3\u7801\u53ef\u7406\u89e3\u6027\u548c\u8d28\u91cf\uff0c\u51cf\u8f7b\u5f00\u53d1\u8005\u7684\u4ee3\u7801\u7406\u89e3\u8d1f\u62c5\u3002", "motivation": "AI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u6b65\u4f7f\u4ee3\u7801\u751f\u6210\u66f4\u9ad8\u6548\uff0c\u4f46\u4ee3\u7801\u7406\u89e3\u4ecd\u662f\u8f6f\u4ef6\u5f00\u53d1\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u6d88\u8017\u5f00\u53d1\u800570%\u65f6\u95f4\u3002\u56e0\u6b64\uff0c\u901a\u8fc7\u6539\u8fdb\u4ee3\u7801\u53ef\u7406\u89e3\u6027\u91ca\u653e\u5f00\u53d1\u8005\u8d44\u6e90\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faAugmented Code Engineering (ACE)\u5de5\u5177\uff0c\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5229\u7528\u9a8c\u8bc1\u8fc7\u7684LLM\u8f93\u51fa\u6765\u81ea\u52a8\u5316\u4ee3\u7801\u6539\u8fdb\uff0c\u540c\u65f6\u517c\u987e\u4ee3\u7801\u8d28\u91cf\u548c\u6b63\u786e\u6027\u3002", "result": "\u521d\u6b65\u7528\u6237\u53cd\u9988\u663e\u793a\uff0cAI\u8f85\u52a9\u7684\u4ee3\u7801\u91cd\u6784\u80fd\u6709\u6548\u51cf\u5c11\u6280\u672f\u503a\u52a1\u3002", "conclusion": "\u5728AI\u8f85\u52a9\u7f16\u7801\u65f6\u4ee3\uff0cACE\u901a\u8fc7\u81ea\u52a8\u5316\u6539\u8fdb\u4ee3\u7801\u53ef\u7406\u89e3\u6027\uff0c\u5e2e\u52a9\u6709\u9650\u5f00\u53d1\u8005\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u4ee3\u7801\u5e93\u3002"}}
{"id": "2507.04147", "pdf": "https://arxiv.org/pdf/2507.04147", "abs": "https://arxiv.org/abs/2507.04147", "authors": ["Shuo Xin", "Haiyu Wang", "Sai Qian Zhang"], "title": "A3FR: Agile 3D Gaussian Splatting with Incremental Gaze Tracked Foveated Rendering in Virtual Reality", "categories": ["cs.GR", "cs.CV", "cs.DC"], "comment": "ACM International Conference on Supercomputing 2025", "summary": "Virtual reality (VR) significantly transforms immersive digital interfaces,\ngreatly enhancing education, professional practices, and entertainment by\nincreasing user engagement and opening up new possibilities in various\nindustries. Among its numerous applications, image rendering is crucial.\nNevertheless, rendering methodologies like 3D Gaussian Splatting impose high\ncomputational demands, driven predominantly by user expectations for superior\nvisual quality. This results in notable processing delays for real-time image\nrendering, which greatly affects the user experience. Additionally, VR devices\nsuch as head-mounted displays (HMDs) are intricately linked to human visual\nbehavior, leveraging knowledge from perception and cognition to improve user\nexperience. These insights have spurred the development of foveated rendering,\na technique that dynamically adjusts rendering resolution based on the user's\ngaze direction. The resultant solution, known as gaze-tracked foveated\nrendering, significantly reduces the computational burden of the rendering\nprocess.\n  Although gaze-tracked foveated rendering can reduce rendering costs, the\ncomputational overhead of the gaze tracking process itself can sometimes\noutweigh the rendering savings, leading to increased processing latency. To\naddress this issue, we propose an efficient rendering framework\ncalled~\\textit{A3FR}, designed to minimize the latency of gaze-tracked foveated\nrendering via the parallelization of gaze tracking and foveated rendering\nprocesses. For the rendering algorithm, we utilize 3D Gaussian Splatting, a\nstate-of-the-art neural rendering technique. Evaluation results demonstrate\nthat A3FR can reduce end-to-end rendering latency by up to $2\\times$ while\nmaintaining visual quality.", "AI": {"tldr": "A3FR\u6846\u67b6\u901a\u8fc7\u5e76\u884c\u5316\u6ce8\u89c6\u8ddf\u8e2a\u4e0e\u6e32\u67d3\u8fc7\u7a0b\uff0c\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u63d0\u5347\u4e86VR\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6ce8\u89c6\u8ddf\u8e2a\u6e32\u67d3\u867d\u80fd\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4f46\u8ddf\u8e2a\u8fc7\u7a0b\u672c\u8eab\u7684\u8ba1\u7b97\u5f00\u9500\u53ef\u80fd\u5bfc\u81f4\u5ef6\u8fdf\u589e\u52a0\u3002", "method": "\u63d0\u51faA3FR\u6846\u67b6\uff0c\u5e76\u884c\u5316\u6ce8\u89c6\u8ddf\u8e2a\u548c\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u6e32\u67d3\u8fc7\u7a0b\u3002", "result": "A3FR\u964d\u4f4e\u7aef\u5230\u7aef\u6e32\u67d3\u5ef6\u8fdf\u8fbe2\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "A3FR\u6709\u6548\u4f18\u5316\u4e86VR\u6e32\u67d3\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u6e32\u67d3\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2507.04432", "pdf": "https://arxiv.org/pdf/2507.04432", "abs": "https://arxiv.org/abs/2507.04432", "authors": ["Pranta Saha", "Joyce Reimer", "Brook Byrns", "Connor Burbridge", "Neeraj Dhar", "Jeffrey Chen", "Steven Rayan", "Gordon Broderick"], "title": "Reconstructing Biological Pathways by Applying Selective Incremental Learning to (Very) Small Language Models", "categories": ["q-bio.MN", "cs.CL", "cs.IT", "cs.LG", "cs.PF", "math.IT"], "comment": "9 pages, 6 figures, 3 tables + 28 pages of supplemental tables;\n  submitted to 16th ACM Conference on Bioinformatics, Computational Biology,\n  and Health Informatics (ACM BCB 2025) as submission no. 76", "summary": "The use of generative artificial intelligence (AI) models is becoming\nubiquitous in many fields. Though progress continues to be made, general\npurpose large language AI models (LLM) show a tendency to deliver creative\nanswers, often called \"hallucinations\", which have slowed their application in\nthe medical and biomedical fields where accuracy is paramount. We propose that\nthe design and use of much smaller, domain and even task-specific LM may be a\nmore rational and appropriate use of this technology in biomedical research. In\nthis work we apply a very small LM by today's standards to the specialized task\nof predicting regulatory interactions between molecular components to fill gaps\nin our current understanding of intracellular pathways. Toward this we attempt\nto correctly posit known pathway-informed interactions recovered from manually\ncurated pathway databases by selecting and using only the most informative\nexamples as part of an active learning scheme. With this example we show that a\nsmall (~110 million parameters) LM based on a Bidirectional Encoder\nRepresentations from Transformers (BERT) architecture can propose molecular\ninteractions relevant to tuberculosis persistence and transmission with over\n80% accuracy using less than 25% of the ~520 regulatory relationships in\nquestion. Using information entropy as a metric for the iterative selection of\nnew tuning examples, we also find that increased accuracy is driven by favoring\nthe use of the incorrectly assigned statements with the highest certainty\n(lowest entropy). In contrast, the concurrent use of correct but least certain\nexamples contributed little and may have even been detrimental to the learning\nrate.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u5c0f\u578b\u3001\u9886\u57df\u7279\u5b9a\u7684\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u5728\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u66f4\u6709\u6548\u5730\u9884\u6d4b\u5206\u5b50\u95f4\u8c03\u63a7\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u51cf\u5c11\u5927\u578b\u901a\u7528\u8bed\u8a00\u6a21\u578b\u7684\u201c\u5e7b\u89c9\u201d\u95ee\u9898\u3002", "motivation": "\u901a\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u56e0\u201c\u5e7b\u89c9\u201d\u95ee\u9898\u5bfc\u81f4\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u66f4\u5408\u9002\u7684\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5c0f\u578bBERT\u67b6\u6784\u7684LM\uff08\u7ea61.1\u4ebf\u53c2\u6570\uff09\uff0c\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u793a\u4f8b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u9884\u6d4b\u7ed3\u6838\u75c5\u76f8\u5173\u7684\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u4ec5\u4f7f\u752825%\u7684\u8c03\u63a7\u5173\u7cfb\u6570\u636e\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u6027\u8d85\u8fc780%\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u9519\u8bef\u793a\u4f8b\uff08\u4f4e\u71b5\uff09\u53ef\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u5c0f\u578b\u3001\u9886\u57df\u7279\u5b9a\u7684LM\u5728\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u53ef\u9ad8\u6548\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff0c\u51cf\u5c11\u201c\u5e7b\u89c9\u201d\u95ee\u9898\u3002"}}
{"id": "2507.03401", "pdf": "https://arxiv.org/pdf/2507.03401", "abs": "https://arxiv.org/abs/2507.03401", "authors": ["Hanjian Liu", "Jinsong Gui"], "title": "AoI-Energy-Spectrum Optimization in Post-Disaster Powered Communication Intelligent Network via Hierarchical Heterogeneous Graph Neural Network", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "This paper designs a post-disaster powered communication intelligent network\n(PDPCIN) to address communication disruptions caused by ground base station\n(GBS) failures within the post-disaster area. PDPCIN employs unmanned aerial\nvehicles (UAVs) to provide wireless data collection (WDC) and wireless energy\ntransmission (WET) for affected areas and leverages low earth orbit satellites\n(LEO SATs) to relay UAV data to the nearest survival GBS. To ensure basic\npost-disaster communication while co-optimizing age of information (AoI),\nenergy efficiency, and spectrum efficiency, intelligent synchronization-UAV\n(IS-UAV) architecture, AoI-based four thresholds updating (AFTU) mechanism, and\nDynamic multi-LEO access (DMLA) strategy are proposed. However, three key\nchallenges remain: time-varying task-resource imbalances, complex topology\ncaused by multi-device scheduling, and nonlinear coupling in multidimensional\nmetric optimization, making system optimization NP-hard. Therefore, this paper\nproposes a hierarchical heterogeneous graph neural networks (HHGNN) framework.\nIt models heterogeneous device nodes and their communication relations as a\nhierarchical heterogeneous graph structure, integrating our defined graph\nsensing, exchange, and mask layer to handle the network's input, feature\npropagation, and output. To search appropriate number of single-LEO SATs, we\npropose single-LEO SAT density optimization (S-LSDO) algorithm. Finally, we\ncompare the proposed scheme with state-of-the-art benchmarks to validate its\nsuperior collaborative optimization of AoI, energy efficiency, and spectrum\nefficiency. Based on this, we derive the expressions for the expected values of\nAoI and stagnant AoI proportion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u707e\u540e\u4f9b\u7535\u901a\u4fe1\u667a\u80fd\u7f51\u7edc\uff08PDPCIN\uff09\uff0c\u5229\u7528\u65e0\u4eba\u673a\u548c\u4f4e\u8f68\u536b\u661f\u89e3\u51b3\u901a\u4fe1\u4e2d\u65ad\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u4f18\u5316\u673a\u5236\uff0c\u5e76\u901a\u8fc7HHGNN\u6846\u67b6\u89e3\u51b3\u4e86\u591a\u7ef4\u5ea6\u4f18\u5316\u7684\u590d\u6742\u6027\u3002", "motivation": "\u707e\u540e\u901a\u4fe1\u57fa\u7840\u8bbe\u65bd\u635f\u574f\u5bfc\u81f4\u901a\u4fe1\u4e2d\u65ad\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6062\u590d\u901a\u4fe1\u5e76\u4e3a\u53d7\u707e\u533a\u57df\u63d0\u4f9b\u6301\u7eed\u652f\u6301\u3002", "method": "\u7ed3\u5408\u65e0\u4eba\u673a\uff08UAV\uff09\u548c\u4f4e\u8f68\u536b\u661f\uff08LEO SATs\uff09\uff0c\u63d0\u51fa\u4e86IS-UAV\u67b6\u6784\u3001AFTU\u673a\u5236\u548cDMLA\u7b56\u7565\uff0c\u5e76\u5229\u7528HHGNN\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6848\u5728\u591a\u76ee\u6807\u4f18\u5316\uff08AoI\u3001\u80fd\u6548\u3001\u9891\u8c31\u6548\u7387\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5e76\u63a8\u5bfc\u4e86AoI\u671f\u671b\u503c\u548c\u505c\u6edeAoI\u6bd4\u4f8b\u7684\u8868\u8fbe\u5f0f\u3002", "conclusion": "PDPCIN\u53ca\u76f8\u5173\u4f18\u5316\u673a\u5236\u4e3a\u707e\u540e\u901a\u4fe1\u6062\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04445", "pdf": "https://arxiv.org/pdf/2507.04445", "abs": "https://arxiv.org/abs/2507.04445", "authors": ["Benjamin Przybocki", "Guilherme V. Toledo", "Yoni Zohar"], "title": "Shininess, strong politeness, and unicorns", "categories": ["cs.LO", "math.LO"], "comment": "To appear in FroCoS 2025", "summary": "Shininess and strong politeness are properties related to theory combination\nprocedures. In a paper titled \"Many-sorted equivalence of shiny and strongly\npolite theories\", Casal and Rasga proved that for decidable theories, these\nproperties are equivalent. We refine their result by showing that: (i) shiny\ntheories are always decidable, and therefore strongly polite; and (ii) there\nare (undecidable) strongly polite theories that are not shiny. This line of\nresearch is tightly related to a recent series of papers that have sought to\nclassify all the relations between theory combination properties. We finally\ncomplete this project, resolving all of the remaining problems that were\npreviously left open.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u8bc1\u660e\u95ea\u4eae\u7406\u8bba\u59cb\u7ec8\u53ef\u5224\u5b9a\u4e14\u5b58\u5728\u975e\u95ea\u4eae\u7684\u5f3a\u793c\u8c8c\u7406\u8bba\uff0c\u5b8c\u5584\u4e86Casal\u548cRasga\u5173\u4e8e\u95ea\u4eae\u4e0e\u5f3a\u793c\u8c8c\u7406\u8bba\u7b49\u4ef7\u6027\u7684\u7814\u7a76\uff0c\u6700\u7ec8\u89e3\u51b3\u4e86\u7406\u8bba\u7ec4\u5408\u5c5e\u6027\u5173\u7cfb\u4e2d\u6240\u6709\u672a\u51b3\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5b8c\u5584\u548c\u7ec6\u5316Casal\u548cRasga\u5173\u4e8e\u95ea\u4eae\u4e0e\u5f3a\u793c\u8c8c\u7406\u8bba\u7b49\u4ef7\u6027\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u89e3\u51b3\u7406\u8bba\u7ec4\u5408\u5c5e\u6027\u5173\u7cfb\u5206\u7c7b\u4e2d\u7684\u5269\u4f59\u5f00\u653e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u95ea\u4eae\u7406\u8bba\u7684\u53ef\u5224\u5b9a\u6027\u53ca\u5176\u4e0e\u5f3a\u793c\u8c8c\u7406\u8bba\u7684\u5173\u7cfb\uff0c\u5e76\u6784\u9020\u4e86\u53cd\u4f8b\u5c55\u793a\u5f3a\u793c\u8c8c\u7406\u8bba\u4e0d\u4e00\u5b9a\u95ea\u4eae\u3002", "result": "\u53d1\u73b0\u95ea\u4eae\u7406\u8bba\u59cb\u7ec8\u53ef\u5224\u5b9a\u4e14\u5fc5\u7136\u5f3a\u793c\u8c8c\uff0c\u4f46\u5b58\u5728\u975e\u95ea\u4eae\u7684\u5f3a\u793c\u8c8c\u7406\u8bba\u3002\u6700\u7ec8\u5b8c\u6210\u4e86\u7406\u8bba\u7ec4\u5408\u5c5e\u6027\u5173\u7cfb\u7684\u5168\u9762\u5206\u7c7b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f7b\u5e95\u89e3\u51b3\u4e86\u7406\u8bba\u7ec4\u5408\u5c5e\u6027\u5173\u7cfb\u7684\u5206\u7c7b\u95ee\u9898\uff0c\u660e\u786e\u4e86\u95ea\u4eae\u4e0e\u5f3a\u793c\u8c8c\u7406\u8bba\u7684\u5173\u7cfb\uff0c\u586b\u8865\u4e86\u5148\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.02905", "pdf": "https://arxiv.org/pdf/2507.02905", "abs": "https://arxiv.org/abs/2507.02905", "authors": ["Chisa Mori", "Shuhei Watanabe", "Masaki Onishi", "Takayuki Itoh"], "title": "Preference-Optimal Multi-Metric Weighting for Parallel Coordinate Plots", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted to International Conference Information Visualisation\n  (iV2025)", "summary": "Parallel coordinate plots (PCPs) are a prevalent method to interpret the\nrelationship between the control parameters and metrics. PCPs deliver such an\ninterpretation by color gradation based on a single metric. However, it is\nchallenging to provide such a gradation when multiple metrics are present.\nAlthough a naive approach involves calculating a single metric by linearly\nweighting each metric, such weighting is unclear for users. To address this\nproblem, we first propose a principled formulation for calculating the optimal\nweight based on a specific preferred metric combination. Although users can\nsimply select their preference from a two-dimensional (2D) plane for bi-metric\nproblems, multi-metric problems require intuitive visualization to allow them\nto select their preference. We achieved this using various radar charts to\nvisualize the metric trade-offs on the 2D plane reduced by UMAP. In the\nanalysis using pedestrian flow guidance planning, our method identified unique\npatterns of control parameter importance for each user preference, highlighting\nthe effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u504f\u597d\u7684\u591a\u6307\u6807\u6743\u91cd\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7UMAP\u964d\u7ef4\u548c\u96f7\u8fbe\u56fe\u5b9e\u73b0\u76f4\u89c2\u53ef\u89c6\u5316\u3002", "motivation": "\u89e3\u51b3\u591a\u6307\u6807\u60c5\u51b5\u4e0b\u5e73\u884c\u5750\u6807\u56fe\uff08PCPs\uff09\u65e0\u6cd5\u6709\u6548\u63d0\u4f9b\u989c\u8272\u6e10\u53d8\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u7528\u6237\u5bf9\u6743\u91cd\u5206\u914d\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6700\u4f18\u6743\u91cd\u7684\u539f\u7406\u6027\u65b9\u6cd5\uff0c\u4f7f\u7528UMAP\u964d\u7ef4\u548c\u96f7\u8fbe\u56fe\u5728\u4e8c\u7ef4\u5e73\u9762\u4e0a\u76f4\u89c2\u5c55\u793a\u6307\u6807\u6743\u8861\u3002", "result": "\u5728\u884c\u4eba\u6d41\u5f15\u5bfc\u89c4\u5212\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u7528\u6237\u504f\u597d\u8bc6\u522b\u63a7\u5236\u53c2\u6570\u91cd\u8981\u6027\u7684\u72ec\u7279\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e2e\u52a9\u7528\u6237\u5728\u591a\u6307\u6807\u60c5\u51b5\u4e0b\u76f4\u89c2\u9009\u62e9\u504f\u597d\uff0c\u5e76\u63ed\u793a\u53c2\u6570\u91cd\u8981\u6027\u6a21\u5f0f\u3002"}}
{"id": "2507.03216", "pdf": "https://arxiv.org/pdf/2507.03216", "abs": "https://arxiv.org/abs/2507.03216", "authors": ["Rongqian Ma", "Xuhan Zhang", "Adrian Wisnicki"], "title": "Disclosing Generative AI Use in Digital Humanities Research", "categories": ["cs.CY", "cs.AI", "cs.DL", "cs.ET"], "comment": null, "summary": "This survey study investigates how digital humanists perceive and approach\ngenerative AI disclosure in research. The results indicate that while digital\nhumanities scholars acknowledge the importance of disclosing GenAI use, the\nactual rate of disclosure in research practice remains low. Respondents differ\nin their views on which activities most require disclosure and on the most\nappropriate methods for doing so. Most also believe that safeguards for AI\ndisclosure should be established through institutional policies rather than\nleft to individual decisions. The study's findings will offer empirical\nguidance to scholars, institutional leaders, funders, and other stakeholders\nresponsible for shaping effective disclosure policies.", "AI": {"tldr": "\u6570\u5b57\u4eba\u6587\u9886\u57df\u5b66\u8005\u5bf9\u751f\u6210\u5f0fAI\u62ab\u9732\u7684\u6001\u5ea6\u4e0e\u5b9e\u8df5\u4e2d\u62ab\u9732\u7387\u4f4e\u7684\u77db\u76fe\u3002", "motivation": "\u4e86\u89e3\u6570\u5b57\u4eba\u6587\u9886\u57df\u5b66\u8005\u5982\u4f55\u770b\u5f85\u53ca\u5b9e\u8df5\u751f\u6210\u5f0fAI\u5728\u7814\u7a76\u4e2d\u4f7f\u7528\u7684\u62ab\u9732\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u7814\u7a76\u6536\u96c6\u6570\u5b57\u4eba\u6587\u5b66\u8005\u7684\u89c2\u70b9\u548c\u5b9e\u8df5\u60c5\u51b5\u3002", "result": "\u5b66\u8005\u4eec\u8ba4\u8bc6\u5230\u62ab\u9732\u91cd\u8981\u6027\uff0c\u4f46\u5b9e\u9645\u62ab\u9732\u7387\u4f4e\uff1b\u5bf9\u62ab\u9732\u8303\u56f4\u548c\u65b9\u6cd5\u7684\u770b\u6cd5\u4e0d\u4e00\uff1b\u666e\u904d\u652f\u6301\u901a\u8fc7\u673a\u6784\u653f\u7b56\u89c4\u8303\u62ab\u9732\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5b66\u8005\u3001\u673a\u6784\u9886\u5bfc\u8005\u548c\u8d44\u52a9\u8005\u5236\u5b9a\u6709\u6548\u62ab\u9732\u653f\u7b56\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2507.05234", "pdf": "https://arxiv.org/pdf/2507.05234", "abs": "https://arxiv.org/abs/2507.05234", "authors": ["Jay Lee", "Joongwon Ahn", "Kwangkeun Yi"], "title": "React-tRace: A Semantics for Understanding React Hooks", "categories": ["cs.PL", "cs.SE"], "comment": "Conditionally accepted to OOPSLA 2025", "summary": "React has become the most widely used web front-end framework, enabling the\ncreation of user interfaces in a declarative and compositional manner. Hooks\nare a set of APIs that manage side effects in functional components in React.\nHowever, their semantics are often seen as opaque to developers, leading to UI\nbugs. In this paper, we formalize the semantics of the essence of React Hooks\nwe name React-tRace, providing a framework that clarifies their behavior. We\ndemonstrate that our model captures the behavior of React, by theoretically\nshowing that it embodies essential properties of Hooks and empirically\ncomparing our React-tRace-definitional interpreter against a test suite.\nFurthermore, we showcase a practical visualization tool based on the\nformalization to demonstrate how developers can better understand the semantics\nof Hooks.", "AI": {"tldr": "React-tRace\u662f\u4e00\u4e2a\u5f62\u5f0f\u5316React Hooks\u8bed\u4e49\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u53ef\u89c6\u5316\u5de5\u5177\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3Hooks\u7684\u884c\u4e3a\u3002", "motivation": "React Hooks\u7684\u8bed\u4e49\u5bf9\u5f00\u53d1\u8005\u6765\u8bf4\u4e0d\u591f\u900f\u660e\uff0c\u5bb9\u6613\u5bfc\u81f4UI\u9519\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u5f62\u5f0f\u5316\u5176\u8bed\u4e49\u4ee5\u63d0\u9ad8\u7406\u89e3\u3002", "method": "\u63d0\u51faReact-tRace\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u89e3\u91ca\u5668\u548c\u6d4b\u8bd5\u5957\u4ef6\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\uff0c\u5e76\u5f00\u53d1\u53ef\u89c6\u5316\u5de5\u5177\u3002", "result": "\u7406\u8bba\u8bc1\u660eReact-tRace\u6355\u83b7\u4e86Hooks\u7684\u56fa\u6709\u7279\u6027\uff0c\u5b9e\u8bc1\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u4e0eReact\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "React-tRace\u5f62\u5f0f\u548c\u53ef\u89c6\u5316\u5de5\u5177\u6709\u6548\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3Hooks\u7684\u8bed\u4e49\uff0c\u51cf\u5c11UI\u9519\u8bef\u3002"}}
{"id": "2507.03486", "pdf": "https://arxiv.org/pdf/2507.03486", "abs": "https://arxiv.org/abs/2507.03486", "authors": ["Younjeong Lee", "Young Yoon"], "title": "A Distributed Consensus Algorithm for Autonomous Vehicles Deciding Entering Orders on Intesections without Traffic Signals", "categories": ["cs.DC"], "comment": "10 pages, 6 figures", "summary": "We propose a methodology for connected autonomous vehicles (CAVs) to\ndetermine their passing priority at unsignalized intersections where they\ncoexist with human-driven vehicles (HVs). Assuming that CAVs can perceive the\nentry order of surrounding vehicles using computer vision technology and are\ncapable of avoiding collisions, we introduce a voting-based distributed\nconsensus algorithm inspired by Raft to resolve tie-breaking among\nsimultaneously arriving CAVs. The algorithm is structured around the candidate\nand leader election processes and incorporates a minimal consensus quorum to\nensure both safety and liveness among CAVs under typical asynchronous\ncommunication conditions. Assuming CAVs to be SAE (Society of Automotive\nEngineers) Level-4 or higher autonomous vehicles, we implemented the proposed\ndistributed consensus algorithm using gRPC. By adjusting variables such as the\nCAV-to-HV ratio, intersection scale, and the processing time of computer vision\nmodules, we demonstrated that stable consensus can be achieved even under\nmixed-traffic conditions involving HVs without adequate functionalities to\ninteract with CAVs. Experimental results show that the proposed algorithm\nreached consensus at a typical unsignalized four-way, two-lane intersection in\napproximately 30-40 ms on average. A secondary vision-based system is employed\nto complete the crossing priorities based on the recognized lexicographical\norder of the license plate numbers in case the consensus procedure times out on\nan unreliable vehicle-to-vehicle communication network. The significance of\nthis study lies in its ability to improve traffic flow at unsignalized\nintersections by enabling rapid determination of passing priority through\ndistributed consensus even under mixed traffic with faulty vehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6df7\u5408\u4ea4\u901a\u4e2d\u65e0\u4eba\u8f66\u5728\u65e0\u4fe1\u53f7\u4ea4\u53c9\u8def\u53e3\u5feb\u901f\u786e\u5b9a\u901a\u884c\u4f18\u5148\u7ea7\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u8f66\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u8f66\u8f86\u5728\u65e0\u4fe1\u53f7\u4ea4\u53c9\u8def\u53e3\u5171\u5b58\u65f6\u7684\u4f18\u5148\u7ea7\u786e\u5b9a\u95ee\u9898\uff0c\u63d0\u5347\u4ea4\u901a\u6548\u7387\u3002", "method": "\u57fa\u4e8eRaft\u7684\u6295\u7968\u5206\u5e03\u5f0f\u5171\u8bc6\u7b97\u6cd5\uff0c\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6700\u5c0f\u5171\u8bc6\u6cd5\u5b9a\u6570\uff0c\u786e\u4fdd\u5b89\u5168\u4e0e\u6d3b\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7b97\u6cd5\u5728\u5178\u578b\u65e0\u4fe1\u53f7\u56db\u8def\u4ea4\u53c9\u53e3\u5e73\u574730-40\u6beb\u79d2\u5185\u8fbe\u6210\u5171\u8bc6\uff0c\u9002\u7528\u4e8e\u6df7\u5408\u4ea4\u901a\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u9ad8\u6548\u89e3\u51b3\u6df7\u5408\u4ea4\u901a\u4e2d\u7684\u4f18\u5148\u7ea7\u95ee\u9898\uff0c\u63d0\u5347\u65e0\u4fe1\u53f7\u4ea4\u53c9\u8def\u53e3\u7684\u4ea4\u901a\u6d41\u7545\u6027\u3002"}}
{"id": "2507.04872", "pdf": "https://arxiv.org/pdf/2507.04872", "abs": "https://arxiv.org/abs/2507.04872", "authors": ["Cong Yu", "Tuo Shi", "Matthias Weidlich", "Bo Zhao"], "title": "SHARP: Shared State Reduction for Efficient Matching of Sequential Patterns", "categories": ["cs.DB"], "comment": null, "summary": "The detection of sequential patterns in data is a basic functionality of\nmodern data processing systems for complex event processing (CEP), OLAP, and\nretrieval-augmented generation (RAG). In practice, pattern matching is\nchallenging, since common applications rely on a large set of patterns that\nshall be evaluated with tight latency bounds. At the same time, matching needs\nto maintain state, i.e., intermediate results, that grows exponentially in the\ninput size. Hence, systems turn to best-effort processing, striving for maximal\nrecall under a latency bound. Existing techniques, however, consider each\npattern in isolation, neglecting the optimization potential induced by state\nsharing in pattern matching.\n  In this paper, we present SHARP, a library that employs state reduction to\nachieve efficient best-effort pattern matching. To this end, SHARP incorporates\nstate sharing between patterns through a new abstraction, coined\npattern-sharing degree (PSD). At runtime, this abstraction facilitates the\ncategorization and indexing of partial pattern matches. Based thereon, once a\nlatency bound is exceeded, SHARP realizes best-effort processing by selecting a\nsubset of partial matches for further processing in constant time. In\nexperiments with real-world data, SHARP achieves a recall of 97%, 96% and 73%\nfor pattern matching in CEP, OLAP, and RAG applications, under a bound of 50%\nof the average processing latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSHARP\u5e93\uff0c\u901a\u8fc7\u72b6\u6001\u5171\u4eab\u5b9e\u73b0\u9ad8\u6548\u7684\u6a21\u5f0f\u5339\u914d\uff0c\u9002\u7528\u4e8eCEP\u3001OLAP\u548cRAG\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5355\u72ec\u5904\u7406\u6bcf\u4e2a\u6a21\u5f0f\uff0c\u5ffd\u7565\u4e86\u6a21\u5f0f\u5339\u914d\u4e2d\u72b6\u6001\u5171\u4eab\u7684\u4f18\u5316\u6f5c\u529b\u3002", "method": "SHARP\u5f15\u5165\u6a21\u5f0f\u5171\u4eab\u5ea6\uff08PSD\uff09\u62bd\u8c61\uff0c\u8fd0\u884c\u65f6\u5206\u7c7b\u7d22\u5f15\u90e8\u5206\u5339\u914d\uff0c\u5e76\u5728\u8d85\u65f6\u540e\u5feb\u901f\u9009\u62e9\u5b50\u96c6\u8fdb\u884c\u540e\u7eed\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSHARP\u572850%\u5e73\u5747\u5904\u7406\u5ef6\u8fdf\u4e0b\uff0cCEP\u3001OLAP\u548cRAG\u5e94\u7528\u4e2d\u7684\u53ec\u56de\u7387\u5206\u522b\u8fbe\u523097%\u300196%\u548c73%\u3002", "conclusion": "SHARP\u901a\u8fc7\u72b6\u6001\u5171\u4eab\u663e\u8457\u63d0\u5347\u4e86\u6a21\u5f0f\u5339\u914d\u7684\u6548\u7387\u4e0e\u53ec\u56de\u7387\u3002"}}
{"id": "2507.04315", "pdf": "https://arxiv.org/pdf/2507.04315", "abs": "https://arxiv.org/abs/2507.04315", "authors": ["Qingyun Zou", "Nuo Chen", "Yao Chen", "Bingsheng He", "WengFei Wong"], "title": "HLStrans: Dataset for LLM-Driven C-to-HLS Hardware Code Synthesis", "categories": ["cs.AR"], "comment": null, "summary": "High-level synthesis (HLS) enables software developers to describe and\nimplement hardware at a higher level of abstraction by using C/C++ instead of\ntraditional hardware description languages to automatically generate FPGA-ready\ndesigns. However, generating HLS code significantly differs from standard\nC/C++: it disallows certain coding idioms, relies on specialized libraries, and\ncritically requires fine-grained transformations and the insertion of\noptimization directives (pragmas) to achieve high performance. Large language\nmodels (LLMs) have shown promise in automating such transformations, yet\nexisting open-source datasets lack sufficient complexity and optimization\ndiversity. To address this gap, we introduce the HLStrans dataset, a\ncomprehensive collection of 137 distinct real word programs, each annotated\nwith a variety of C-to-HLS transformations that yield over 23K labeled design\nvariants. These include a broad spectrum of pragmas and code-level\noptimizations. We benchmark state-of-the-art LLMs on this dataset to evaluate\ntheir ability to generate synthesizable, high-performance HLS code. As part of\nan ongoing effort, we plan to expand the HLStrans dataset in both scale and\nprogram variety, further empowering research at the intersection of AI and\nhardware synthesis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86HLStrans\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u5f00\u6e90\u6570\u636e\u96c6\u4e2d\u590d\u6742\u6027\u548c\u4f18\u5316\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301AI\u4e0e\u786c\u4ef6\u5408\u6210\u9886\u57df\u7684\u7814\u7a76\u3002", "motivation": "\u4f20\u7edfHLS\u4ee3\u7801\u751f\u6210\u5b58\u5728\u56f0\u96be\uff0c\u7f3a\u4e4f\u8db3\u591f\u590d\u6742\u548c\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0cHLStrans\u6570\u636e\u96c6\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6536\u96c6137\u4e2a\u771f\u5b9e\u7a0b\u5e8f\u5e76\u6807\u6ce8\u591a\u79cdC-to-HLS\u8f6c\u6362\uff0c\u751f\u6210\u4e8623K\u5e26\u6807\u7b7e\u7684\u8bbe\u8ba1\u53d8\u4f53\u3002", "result": "\u6570\u636e\u96c6\u4e3aLLMs\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u4f18\u5316\u591a\u6837\u6027\uff0c\u7528\u4e8e\u8bc4\u4f30\u5176\u751f\u6210\u9ad8\u6027\u80fdHLS\u4ee3\u7801\u7684\u80fd\u529b\u3002", "conclusion": "HLStrans\u6570\u636e\u96c6\u5c06\u6269\u5c55\u89c4\u6a21\u548c\u7a0b\u5e8f\u591a\u6837\u6027\uff0c\u63a8\u52a8AI\u4e0e\u786c\u4ef6\u5408\u6210\u7814\u7a76\u3002"}}
{"id": "2507.05113", "pdf": "https://arxiv.org/pdf/2507.05113", "abs": "https://arxiv.org/abs/2507.05113", "authors": ["Binyan Xu", "Fan Yang", "Xilin Dai", "Di Tang", "Kehuan Zhang"], "title": "CLIP-Guided Backdoor Defense through Entropy-Based Poisoned Dataset Separation", "categories": ["cs.MM", "cs.CR", "cs.LG", "68T07", "I.2.6"], "comment": "15 pages, 9 figures, 15 tables. To appear in the Proceedings of the\n  32nd ACM International Conference on Multimedia (MM '25)", "summary": "Deep Neural Networks (DNNs) are susceptible to backdoor attacks, where\nadversaries poison training data to implant backdoor into the victim model.\nCurrent backdoor defenses on poisoned data often suffer from high computational\ncosts or low effectiveness against advanced attacks like clean-label and\nclean-image backdoors. To address them, we introduce CLIP-Guided backdoor\nDefense (CGD), an efficient and effective method that mitigates various\nbackdoor attacks. CGD utilizes a publicly accessible CLIP model to identify\ninputs that are likely to be clean or poisoned. It then retrains the model with\nthese inputs, using CLIP's logits as a guidance to effectively neutralize the\nbackdoor. Experiments on 4 datasets and 11 attack types demonstrate that CGD\nreduces attack success rates (ASRs) to below 1% while maintaining clean\naccuracy (CA) with a maximum drop of only 0.3%, outperforming existing\ndefenses. Additionally, we show that clean-data-based defenses can be adapted\nto poisoned data using CGD. Also, CGD exhibits strong robustness, maintaining\nlow ASRs even when employing a weaker CLIP model or when CLIP itself is\ncompromised by a backdoor. These findings underscore CGD's exceptional\nefficiency, effectiveness, and applicability for real-world backdoor defense\nscenarios. Code: https://github.com/binyxu/CGD.", "AI": {"tldr": "CGD\u662f\u4e00\u79cd\u5229\u7528CLIP\u6a21\u578b\u9ad8\u6548\u68c0\u6d4b\u548c\u9632\u5fa1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u540e\u95e8\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u663e\u8457\u4e14\u9002\u5e94\u6027\u5e7f\u3002", "motivation": "\u73b0\u6709\u7684\u540e\u95e8\u9632\u5fa1\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5bf9\u9ad8\u7ea7\u653b\u51fb\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u548c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7CLIP\u6a21\u578b\u8bc6\u522b\u5e76\u5206\u7c7b\u5e72\u51c0\u548c\u53d7\u6c61\u67d3\u7684\u6570\u636e\uff0c\u5229\u7528\u5176\u8f93\u51fa\u6307\u5bfc\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u4ee5\u6d88\u9664\u540e\u95e8\u3002", "result": "\u57284\u4e2a\u6570\u636e\u96c6\u548c11\u79cd\u653b\u51fb\u7c7b\u578b\u4e0a\uff0cCGD\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u81f31%\u4ee5\u4e0b\uff0c\u4e14\u5e72\u51c0\u6570\u636e\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.3%\u3002", "conclusion": "CGD\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u6709\u6548\u4e14\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u7684\u540e\u95e8\u9632\u5fa1\u65b9\u6cd5\uff0c\u5373\u4f7fCLIP\u6a21\u578b\u88ab\u653b\u51fb\u4e5f\u80fd\u4fdd\u6301\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.03620", "pdf": "https://arxiv.org/pdf/2507.03620", "abs": "https://arxiv.org/abs/2507.03620", "authors": ["Francisca Lemos", "Victor Alves", "Filipa Ferraz"], "title": "Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG", "68T50", "I.2.7; D.2.3"], "comment": "20 pages with 1 figure", "summary": "Although prompt engineering is central to unlocking the full potential of\nLarge Language Models (LLMs), crafting effective prompts remains a\ntime-consuming trial-and-error process that relies on human intuition. This\nstudy investigates Declarative Self-improving Python (DSPy), an optimization\nframework that programmatically creates and refines prompts, applied to five\nuse cases: guardrail enforcement, hallucination detection in code, code\ngeneration, routing agents, and prompt evaluation. Each use case explores how\nprompt optimization via DSPy influences performance. While some cases\ndemonstrated modest improvements - such as minor gains in the guardrails use\ncase and selective enhancements in hallucination detection - others showed\nnotable benefits. The prompt evaluation criterion task demonstrated a\nsubstantial performance increase, rising accuracy from 46.2% to 64.0%. In the\nrouter agent case, the possibility of improving a poorly performing prompt and\nof a smaller model matching a stronger one through optimized prompting was\nexplored. Although prompt refinement increased accuracy from 85.0% to 90.0%,\nusing the optimized prompt with a cheaper model did not improve performance.\nOverall, this study's findings suggest that DSPy's systematic prompt\noptimization can enhance LLM performance, particularly when instruction tuning\nand example selection are optimized together. However, the impact varies by\ntask, highlighting the importance of evaluating specific use cases in prompt\noptimization research.", "AI": {"tldr": "DSPy\u6846\u67b6\u901a\u8fc7\u7a0b\u5e8f\u5316\u4f18\u5316\u63d0\u793a\u8bcd\uff0c\u6539\u5584\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u6548\u679c\u56e0\u4efb\u52a1\u800c\u5f02\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u624b\u52a8\u8bbe\u8ba1\u63d0\u793a\u8bcd\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u63a2\u7d22\u7a0b\u5e8f\u5316\u4f18\u5316\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528DSPy\u6846\u67b6\u4f18\u5316\u63d0\u793a\u8bcd\uff0c\u5e76\u5728\u4e94\u4e2a\u7528\u4f8b\u4e2d\u6d4b\u8bd5\u5176\u6548\u679c\uff0c\u5305\u62ec\u62a4\u680f\u6267\u884c\u3001\u5e7b\u89c9\u68c0\u6d4b\u7b49\u3002", "result": "\u90e8\u5206\u4efb\u52a1\u8868\u73b0\u663e\u8457\u63d0\u5347\uff08\u5982\u63d0\u793a\u8bc4\u4f30\u4efb\u52a1\u7684\u51c6\u786e\u7387\u4ece46.2%\u63d0\u9ad8\u523064.0%\uff09\uff0c\u4f46\u4e5f\u6709\u4e00\u4e9b\u4efb\u52a1\u63d0\u5347\u6709\u9650\u3002", "conclusion": "DSPy\u7684\u63d0\u793a\u4f18\u5316\u80fd\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u4f46\u6548\u679c\u4f9d\u8d56\u4e8e\u5177\u4f53\u4efb\u52a1\uff0c\u9700\u7ed3\u5408\u6307\u4ee4\u8c03\u6574\u548c\u793a\u4f8b\u9009\u62e9\u3002"}}
{"id": "2507.05191", "pdf": "https://arxiv.org/pdf/2507.05191", "abs": "https://arxiv.org/abs/2507.05191", "authors": ["Gene Wei-Chin Lin", "Egor Larionov", "Hsiao-yu Chen", "Doug Roble", "Tuur Stuyck"], "title": "Neuralocks: Real-Time Dynamic Neural Hair Simulation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Real-time hair simulation is a vital component in creating believable virtual\navatars, as it provides a sense of immersion and authenticity. The dynamic\nbehavior of hair, such as bouncing or swaying in response to character\nmovements like jumping or walking, plays a significant role in enhancing the\noverall realism and engagement of virtual experiences. Current methods for\nsimulating hair have been constrained by two primary approaches: highly\noptimized physics-based systems and neural methods. However, state-of-the-art\nneural techniques have been limited to quasi-static solutions, failing to\ncapture the dynamic behavior of hair. This paper introduces a novel neural\nmethod that breaks through these limitations, achieving efficient and stable\ndynamic hair simulation while outperforming existing approaches. We propose a\nfully self-supervised method which can be trained without any manual\nintervention or artist generated training data allowing the method to be\nintegrated with hair reconstruction methods to enable automatic end-to-end\nmethods for avatar reconstruction. Our approach harnesses the power of compact,\nmemory-efficient neural networks to simulate hair at the strand level, allowing\nfor the simulation of diverse hairstyles without excessive computational\nresources or memory requirements. We validate the effectiveness of our method\nthrough a variety of hairstyle examples, showcasing its potential for\nreal-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u76d1\u7763\u795e\u7ecf\u65b9\u6cd5\uff0c\u53ef\u9ad8\u6548\u7a33\u5b9a\u5730\u6a21\u62df\u52a8\u6001\u5934\u53d1\u884c\u4e3a\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u65f6\u5934\u53d1\u6a21\u62df\u5bf9\u865a\u62df\u89d2\u8272\u771f\u5b9e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u795e\u7ecf\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u7d27\u51d1\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u81ea\u76d1\u7763\u8bad\u7ec3\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6216\u827a\u672f\u6570\u636e\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u79cd\u53d1\u578b\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u663e\u793a\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u7a81\u7834\u5c40\u9650\uff0c\u4e3a\u5934\u53d1\u6a21\u62df\u63d0\u4f9b\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05141", "pdf": "https://arxiv.org/pdf/2507.05141", "abs": "https://arxiv.org/abs/2507.05141", "authors": ["Jelin Leslin", "Martin Trapp", "Martin Andraud"], "title": "Hardware-efficient tractable probabilistic inference for TinyML Neurosymbolic AI applications", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "Neurosymbolic AI (NSAI) has recently emerged to mitigate limitations\nassociated with deep learning (DL) models, e.g. quantifying their uncertainty\nor reason with explicit rules. Hence, TinyML hardware will need to support\nthese symbolic models to bring NSAI to embedded scenarios. Yet, although\nsymbolic models are typically compact, their sparsity and computation\nresolution contrasts with low-resolution and dense neuro models, which is a\nchallenge on resource-constrained TinyML hardware severely limiting the size of\nsymbolic models that can be computed. In this work, we remove this bottleneck\nleveraging a tight hardware/software integration to present a complete\nframework to compute NSAI with TinyML hardware. We focus on symbolic models\nrealized with tractable probabilistic circuits (PCs), a popular subclass of\nprobabilistic models for hardware integration. This framework: (1) trains a\nspecific class of hardware-efficient \\emph{deterministic} PCs, chosen for the\nsymbolic task; (2) \\emph{compresses} this PC until it can be computed on TinyML\nhardware with minimal accuracy degradation, using our $n^{th}$-root compression\ntechnique, and (3) \\emph{deploys} the complete NSAI model on TinyML hardware.\nCompared to a 64b precision baseline necessary for the PC without compression,\nour workflow leads to significant hardware reduction on FPGA (up to 82.3\\% in\nFF, 52.6\\% in LUTs, and 18.0\\% in Flash usage) and an average inference speedup\nof 4.67x on ESP32 microcontroller.", "AI": {"tldr": "\u901a\u8fc7\u786c\u4ef6/\u8f6f\u4ef6\u7d27\u5bc6\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728TinyML\u786c\u4ef6\u4e0a\u5b9e\u73b0\u795e\u7ecf\u7b26\u53f7AI\u7684\u5b8c\u6574\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u786c\u4ef6\u8d44\u6e90\u6d88\u8017\u5e76\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3TinyML\u786c\u4ef6\u4e0a\u795e\u7ecf\u7b26\u53f7AI\uff08NSAI\uff09\u8ba1\u7b97\u4e2d\u7b26\u53f7\u6a21\u578b\u7a00\u758f\u6027\u4e0e\u8ba1\u7b97\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "1. \u8bad\u7ec3\u4e00\u79cd\u786c\u4ef6\u9ad8\u6548\u7684\u786e\u5b9a\u6027\u6982\u7387\u7535\u8def\uff08PC\uff09\uff1b2. \u4f7f\u7528$n^{th}$-\u6839\u538b\u7f29\u6280\u672f\u538b\u7f29PC\uff1b3. \u5728TinyML\u786c\u4ef6\u4e0a\u90e8\u7f72\u5b8c\u6574NSAI\u6a21\u578b\u3002", "result": "\u4e0e64\u4f4d\u7cbe\u5ea6\u57fa\u7ebf\u76f8\u6bd4\uff0c\u663e\u8457\u51cf\u5c11\u4e86FPGA\u786c\u4ef6\u8d44\u6e90\uff08FF\u8282\u770182.3%\uff0cLUT\u8282\u770152.6%\uff0cFlash\u8282\u770118.0%\uff09\uff0c\u5e73\u5747\u63a8\u7406\u901f\u5ea6\u63d0\u53474.67\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86TinyML\u786c\u4ef6\u4e0aNSAI\u7684\u90e8\u7f72\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u7387\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2507.03873", "pdf": "https://arxiv.org/pdf/2507.03873", "abs": "https://arxiv.org/abs/2507.03873", "authors": ["Tianlang He", "Zhangyu Chang", "S. -H. Gary Chan"], "title": "RateCount: Learning-Free Device Counting by Wi-Fi Probe Listening", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "A Wi-Fi-enabled device, or simply Wi-Fi device, sporadically broadcasts probe\nrequest frames (PRFs) to discover nearby access points (APs), whether connected\nto an AP or not. To protect user privacy, unconnected devices often randomize\ntheir MAC addresses in the PRFs, known as MAC address randomization. While\nprior works have achieved accurate device counting under MAC address\nrandomization, they typically rely on machine learning, resulting in\ninefficient deployment due to the time-consuming processes of data cleaning,\nmodel training, and hyperparameter tuning. To enhance deployment efficiency, we\npropose RateCount, an accurate, lightweight, and learning-free counting\napproach based on the rate at which APs receive PRFs within a window. RateCount\nemploys a provably unbiased closed-form expression to estimate the device count\ntime-averaged over the window and an error model to compute the lower bound of\nthe estimation variance. We also demonstrate how to extend RateCount to people\ncounting by incorporating a device-to-person calibration scheme. Through\nextensive real-world experiments conducted at multiple sites spanning a wide\nrange of counts, we show that RateCount, without any deployment costs for\nmachine learning, achieves comparable counting accuracy with the\nstate-of-the-art learning-based device counting and improves previous people\ncounting schemes by a large margin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRateCount\u7684\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728Wi-Fi\u8bbe\u5907\u968f\u673a\u5316MAC\u5730\u5740\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u8ba1\u6570\uff0c\u907f\u514d\u4e86\u673a\u5668\u5b66\u4e60\u90e8\u7f72\u7684\u9ad8\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8bbe\u5907\u8ba1\u6570\u65b9\u6cd5\u867d\u7136\u51c6\u786e\uff0c\u4f46\u5b58\u5728\u6570\u636e\u6e05\u6d17\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u7b49\u8017\u65f6\u95ee\u9898\uff0c\u90e8\u7f72\u6548\u7387\u4f4e\u3002\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8ba1\u6570\u65b9\u6cd5\u3002", "method": "RateCount\u901a\u8fc7\u8ba1\u7b97AP\u5728\u7a97\u53e3\u671f\u5185\u63a5\u6536PRF\u7684\u901f\u7387\uff0c\u4f7f\u7528\u65e0\u504f\u95ed\u5f0f\u8868\u8fbe\u5f0f\u4f30\u8ba1\u8bbe\u5907\u6570\u91cf\uff0c\u5e76\u7ed3\u5408\u6821\u51c6\u65b9\u6848\u6269\u5c55\u5230\u4eba\u5458\u8ba1\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRateCount\u65e0\u9700\u673a\u5668\u5b66\u4e60\u90e8\u7f72\u6210\u672c\uff0c\u4ecd\u80fd\u8fbe\u5230\u4e0e\u57fa\u4e8e\u5b66\u4e60\u7684\u8ba1\u6570\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u5e76\u5728\u4eba\u5458\u8ba1\u6570\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6848\u3002", "conclusion": "RateCount\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u8ba1\u6570\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u90e8\u7f72\u6548\u7387\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u6570\u51c6\u786e\u6027\u3002"}}
{"id": "2507.04449", "pdf": "https://arxiv.org/pdf/2507.04449", "abs": "https://arxiv.org/abs/2507.04449", "authors": ["Khashayar Irani"], "title": "Proof Analysis of A Foundational Classical Singlesuccedent Sequent Calculus", "categories": ["cs.LO"], "comment": "This paper is currently in draft form; therefore, constructive\n  comments sent via email are welcome", "summary": "In this paper we investigate the question: 'How can A Foundational Classical\nSinglesuccedent Sequent Calculus be formulated?' The choice of this particular\narea of proof-theoretic study is based on a particular ground that is, to\nformulate a robust and foundational classical singlesuccedent sequent calculus\nthat includes a number of novel rules with the ultimate aim of deriving the\nsinglesuccedent sequent {\\Gamma} sequent arrow C. To this end, we argue that\namong all standard sequent calculi (at least to the best of our knowledge)\nthere is no classical singlesuccedent sequent calculus that can be considered\nthe rightful successor to Gerhard Gentzen's (1935) original LK system. However,\nwe also contend that while several classical singlesuccedent sequent calculi\nexist such as Sara Negri's and Jan von Plato's (2001 & 2011) G3ip+Gem-at and\nG0ip+Gem0-at calculi, none of these proof systems possess the classical\nproof-theoretic potential to meet the formal expectations of a dedicated\nclassical proof theorist. Conversely, we shall demonstrate that our forthcoming\nsystem, namely G-Calculus through its classical division i.e. Gc has been\nentirely designed to meet these expectations. Prior to commencing our enquiry,\na supplementary note must be made and that is in this work when discussing\nvarious sequent calculi, for proof-theoretic purposes, we are primarily\nconcerned with their propositional components rather than their predicate\ndivisions except in G-Calculus where we examine both aspects.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7ecf\u5178\u5355\u540e\u7ee7\u5f0f\u5e8f\u5217\u6f14\u7b97G-Calculus\uff08Gc\uff09\uff0c\u4ee5\u5f25\u8865\u73b0\u6709\u7cfb\u7edf\u5982LK\u53ca\u5176\u53d8\u79cd\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7ecf\u5178\u5355\u540e\u7ee7\u5f0f\u5e8f\u5217\u6f14\u7b97\uff08\u5982Negri\u548cvon Plato\u7684\u7cfb\u7edf\uff09\u5728\u8bc1\u660e\u7406\u8bba\u6f5c\u529b\u4e0a\u65e0\u6cd5\u6ee1\u8db3\u4e13\u4e1a\u9700\u6c42\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u5f3a\u5927\u7684\u57fa\u7840\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u5305\u542b\u65b0\u9896\u89c4\u5219\u7684Gc\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u547d\u9898\u90e8\u5206\u800c\u975e\u8c13\u8bcd\u90e8\u5206\uff0c\u4ee5\u5b9e\u73b0\u76ee\u6807\u3002", "result": "Gc\u7cfb\u7edf\u88ab\u8bc1\u660e\u80fd\u591f\u6ee1\u8db3\u7ecf\u5178\u8bc1\u660e\u7406\u8bba\u5bb6\u7684\u5f62\u5f0f\u5316\u671f\u671b\u3002", "conclusion": "Gc\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u7ecf\u5178\u5355\u540e\u7ee7\u5f0f\u5e8f\u5217\u6f14\u7b97\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u610f\u4e49\u3002"}}
{"id": "2507.02914", "pdf": "https://arxiv.org/pdf/2507.02914", "abs": "https://arxiv.org/abs/2507.02914", "authors": ["Steve Dev\u00e8nes", "Marine Capallera", "Robin Cherix", "Elena Mugellini", "Omar Abou Khaled", "Francesco Carrino"], "title": "OAK -- Onboarding with Actionable Knowledge", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "This paper is an extended version of the work originally presented at\n  the AI-Days 2024 conference in Lausanne, Switzerland. It builds upon the\n  findings shared during the conference and includes additional results and\n  analysis", "summary": "The loss of knowledge when skilled operators leave poses a critical issue for\ncompanies. This know-how is diverse and unstructured. We propose a novel method\nthat combines knowledge graph embeddings and multi-modal interfaces to collect\nand retrieve expertise, making it actionable. Our approach supports\ndecision-making on the shop floor. Additionally, we leverage LLMs to improve\nquery understanding and provide adapted answers. As application case studies,\nwe developed a proof-of-concept for quality control in high precision\nmanufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u548c\u591a\u6a21\u6001\u63a5\u53e3\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u6536\u96c6\u548c\u68c0\u7d22\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5e76\u652f\u6301\u51b3\u7b56\u5236\u5b9a\u3002", "motivation": "\u89e3\u51b3\u56e0\u719f\u7ec3\u64cd\u4f5c\u5458\u79bb\u804c\u5bfc\u81f4\u77e5\u8bc6\u6d41\u5931\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u591a\u6837\u4e14\u975e\u7ed3\u6784\u5316\u3002", "method": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u548c\u591a\u6a21\u6001\u63a5\u53e3\uff0c\u5e76\u5229\u7528LLMs\u6539\u8fdb\u67e5\u8be2\u7406\u89e3\u548c\u63d0\u4f9b\u9002\u5e94\u6027\u7b54\u6848\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u5236\u9020\u8d28\u91cf\u63a7\u5236\u7684\u9a8c\u8bc1\u6848\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u6d41\u5931\u95ee\u9898\uff0c\u5e76\u652f\u6301\u5b9e\u9645\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2507.03410", "pdf": "https://arxiv.org/pdf/2507.03410", "abs": "https://arxiv.org/abs/2507.03410", "authors": ["Hrishikesh Terdalkar", "Angela Bonifati", "Andrea Mauri"], "title": "Graph Repairs with Large Language Models: An Empirical Study", "categories": ["cs.CL", "cs.DB", "cs.ET"], "comment": "Accepted to the 8th GRADES-NDA 2025 @ SIGMOD/PODS 2025", "summary": "Property graphs are widely used in domains such as healthcare, finance, and\nsocial networks, but they often contain errors due to inconsistencies, missing\ndata, or schema violations. Traditional rule-based and heuristic-driven graph\nrepair methods are limited in their adaptability as they need to be tailored\nfor each dataset. On the other hand, interactive human-in-the-loop approaches\nmay become infeasible when dealing with large graphs, as the cost--both in\nterms of time and effort--of involving users becomes too high. Recent\nadvancements in Large Language Models (LLMs) present new opportunities for\nautomated graph repair by leveraging contextual reasoning and their access to\nreal-world knowledge. We evaluate the effectiveness of six open-source LLMs in\nrepairing property graphs. We assess repair quality, computational cost, and\nmodel-specific performance. Our experiments show that LLMs have the potential\nto detect and correct errors, with varying degrees of accuracy and efficiency.\nWe discuss the strengths, limitations, and challenges of LLM-driven graph\nrepair and outline future research directions for improving scalability and\ninterpretability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u4fee\u590d\u5c5e\u6027\u56fe\u4e2d\u7684\u9519\u8bef\uff0c\u5e76\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u5c55\u793a\u4e86LLMs\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u5c5e\u6027\u56fe\u4e2d\u5b58\u5728\u7684\u9519\u8bef\uff08\u5982\u4e0d\u4e00\u81f4\u3001\u7f3a\u5931\u6570\u636e\u6216\u6a21\u5f0f\u8fdd\u89c4\uff09\u9700\u8981\u9ad8\u6548\u4fee\u590d\u65b9\u6cd5\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u5927\u89c4\u6a21\u56fe\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0cLLMs\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bc4\u4f30\u4e86\u516d\u79cd\u5f00\u6e90LLM\u5728\u4fee\u590d\u5c5e\u6027\u56fe\u4e2d\u7684\u6027\u80fd\uff0c\u5206\u6790\u4e86\u4fee\u590d\u8d28\u91cf\u3001\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLMs\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u7ea0\u6b63\u9519\u8bef\uff0c\u4f46\u51c6\u786e\u6027\u548c\u6548\u7387\u56e0\u6a21\u578b\u800c\u5f02\u3002", "conclusion": "LLMs\u5728\u5c5e\u6027\u56fe\u4fee\u590d\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.03659", "pdf": "https://arxiv.org/pdf/2507.03659", "abs": "https://arxiv.org/abs/2507.03659", "authors": ["Valentina Wu", "Alexandra Mendes", "Alexandre Abreu"], "title": "Specification-Guided Repair of Arithmetic Errors in Dafny Programs using LLMs", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Formal verification offers strong assurances of software correctness.\nHowever, debugging and repairing the underlying faults can be complex and\ntime-consuming when verification fails. Automated Program Repair (APR) aims to\nease this by automatically identifying and fixing faults. Traditional APR\ntechniques often depend on test suites for validation, but these may fail to\ncapture all scenarios. In contrast, formal specifications provide stronger\ncorrectness criteria for effective repairs.\n  We present an innovative APR tool for Dafny, a verification-aware programming\nlanguage that uses formal specifications - including pre-conditions,\npost-conditions, and invariants - as oracles for fault localization and repair.\nAssuming the correctness of the specifications and focusing on arithmetic bugs,\nwe localize faults through a series of steps, which include using Hoare Logic\nto determine the state of each statement within the program and\nstate-of-the-art Large Language Models (LLMs) to synthesize candidate fixes.\nThe chosen models were GPT-4o mini, Llama 3, Mistral 7B, and Llemma 7B.\n  We evaluate our approach using DafnyBench, a benchmark of real-world Dafny\nprograms. Our tool achieves 89.6% accuracy in fault localization, with GPT-4o\nmini yielding the highest repair success rate (74.18%). These results highlight\nthe potential of combining formal reasoning with LLM-driven program synthesis\nfor automated program repair.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e0eLLM\u9a71\u52a8\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u5de5\u5177\uff0c\u7528\u4e8eDafny\u8bed\u8a00\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u969c\u5b9a\u4f4d\u548c\u4fee\u590d\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u6280\u672f\u4f9d\u8d56\u6d4b\u8bd5\u5957\u4ef6\u9a8c\u8bc1\uff0c\u4f46\u65e0\u6cd5\u8986\u76d6\u6240\u6709\u573a\u666f\u3002\u5f62\u5f0f\u5316\u89c4\u8303\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u6b63\u786e\u6027\u6807\u51c6\uff0c\u4f46\u8c03\u8bd5\u548c\u4fee\u590d\u8fc7\u7a0b\u590d\u6742\u8017\u65f6\u3002", "method": "\u5229\u7528\u5f62\u5f0f\u5316\u89c4\u8303\uff08\u5982\u524d\u7f6e\u6761\u4ef6\u3001\u540e\u7f6e\u6761\u4ef6\u548c\u4e0d\u53d8\u5f0f\uff09\u4f5c\u4e3a\u6545\u969c\u5b9a\u4f4d\u548c\u4fee\u590d\u7684\u4f9d\u636e\uff0c\u7ed3\u5408Hoare Logic\u548c\u591a\u79cd\u5148\u8fdbLLM\uff08\u5982GPT-4o mini\uff09\u751f\u6210\u5019\u9009\u4fee\u590d\u65b9\u6848\u3002", "result": "\u5728DafnyBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6545\u969c\u5b9a\u4f4d\u51c6\u786e\u7387\u4e3a89.6%\uff0cGPT-4o mini\u7684\u4fee\u590d\u6210\u529f\u7387\u8fbe74.18%\u3002", "conclusion": "\u5f62\u5f0f\u5316\u63a8\u7406\u4e0eLLM\u9a71\u52a8\u7684\u7a0b\u5e8f\u5408\u6210\u76f8\u7ed3\u5408\uff0c\u4e3a\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03695", "pdf": "https://arxiv.org/pdf/2507.03695", "abs": "https://arxiv.org/abs/2507.03695", "authors": ["Mohsen Koohi Esfahani"], "title": "On Optimizing Resource Utilization in Distributed Connected Components", "categories": ["cs.DC"], "comment": null, "summary": "Connected Components (CC) is a core graph problem with numerous applications.\nThis paper investigates accelerating distributed CC by optimizing memory and\nnetwork bandwidth utilization. We present two novel distributed CC algorithms,\nSiskinCC and RobinCC, which are built upon the Jayanti-Tarjan disjoint set\nunion algorithm. To optimize memory utilization, SiskinCC and RobinCC are\ndesigned to facilitate efficient access to a shared array for all cores running\nin a machine. This allows execution of faster algorithms with larger memory\nbounds. SiskinCC leverages the continuous inter-machine communication during\nthe computation phase to reduce the final communication overhead and RobinCC\nleverages the structural properties of real-world graphs to optimize network\nbandwidth utilization. Our evaluation against state-of-the-art CC algorithms,\nusing real-world and synthetic graphs with up to 500 billion edges and 11.7\nbillion vertices, and on up to 2048 CPU cores, demonstrates that SiskinCC and\nRobinCC achieve up to 58.5 times speedup.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u5206\u5e03\u5f0f\u8fde\u901a\u5206\u91cf\uff08CC\uff09\u65b0\u7b97\u6cd5SiskinCC\u548cRobinCC\uff0c\u4f18\u5316\u5185\u5b58\u548c\u7f51\u7edc\u5e26\u5bbd\u5229\u7528\uff0c\u6027\u80fd\u63d0\u534758.5\u500d\u3002", "motivation": "\u8fde\u901a\u5206\u91cf\uff08CC\uff09\u662f\u56fe\u8ba1\u7b97\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u73b0\u6709\u5206\u5e03\u5f0f\u7b97\u6cd5\u5728\u5185\u5b58\u548c\u7f51\u7edc\u5e26\u5bbd\u5229\u7528\u4e0a\u6709\u4f18\u5316\u7a7a\u95f4\u3002", "method": "\u57fa\u4e8eJayanti-Tarjan\u4e0d\u76f8\u4ea4\u96c6\u5e76\u7b97\u6cd5\u8bbe\u8ba1SiskinCC\u548cRobinCC\uff0c\u5206\u522b\u4f18\u5316\u5185\u5b58\u5171\u4eab\u548c\u7f51\u7edc\u5e26\u5bbd\u5229\u7528\u3002", "result": "\u57285000\u4ebf\u8fb9\u548c117\u4ebf\u9876\u70b9\u7684\u56fe\u4e0a\u6d4b\u8bd5\uff0c2048\u6838CPU\u4e0a\u6700\u9ad8\u63d0\u901f58.5\u500d\u3002", "conclusion": "\u65b0\u578b\u7b97\u6cd5\u663e\u8457\u4f18\u5316\u4e86\u5206\u5e03\u5f0fCC\u8ba1\u7b97\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u53ca\u5408\u6210\u5927\u89c4\u6a21\u56fe\u3002"}}
{"id": "2507.04967", "pdf": "https://arxiv.org/pdf/2507.04967", "abs": "https://arxiv.org/abs/2507.04967", "authors": ["Bardia Mohammadi", "Laurent Bindschaedler"], "title": "The Case for Instance-Optimized LLMs in OLAP Databases", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.", "AI": {"tldr": "IOLM-DB\u901a\u8fc7\u4e3a\u6bcf\u6b21\u67e5\u8be2\u751f\u6210\u8f7b\u91cf\u7ea7\u4e13\u7528\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\uff0c\u4f7f\u5927\u89c4\u6a21LLM\u589e\u5f3a\u67e5\u8be2\u53d8\u5f97\u53ef\u884c\u3002", "motivation": "\u7531\u4e8eLLM\u5728\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u65f6\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5927\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u964d\u4f4e\u6210\u672c\u3002", "method": "IOLM-DB\u9488\u5bf9\u6bcf\u4e2a\u67e5\u8be2\u751f\u6210\u4e13\u7528\u6a21\u578b\uff0c\u7ed3\u5408\u91cf\u5316\u3001\u7a00\u758f\u5316\u548c\u7ed3\u6784\u526a\u679d\u7b49\u538b\u7f29\u6280\u672f\u3002", "result": "\u6a21\u578b\u4f53\u79ef\u51cf\u5c1176%\uff0c\u541e\u5410\u91cf\u63d0\u53473.31\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "IOLM-DB\u8bc1\u660eLLM\u589e\u5f3a\u67e5\u8be2\u5728\u5927\u89c4\u6a21\u5206\u6790\u7cfb\u7edf\u4e2d\u53ef\u884c\uff0c\u4e3a\u672a\u6765OLAP\u5e94\u7528\u5f00\u8f9f\u65b0\u53ef\u80fd\u3002"}}
{"id": "2507.04535", "pdf": "https://arxiv.org/pdf/2507.04535", "abs": "https://arxiv.org/abs/2507.04535", "authors": ["Chang Sun", "Zhiqiang Que", "Vladimir Loncar", "Wayne Luk", "Maria Spiropulu"], "title": "da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs", "categories": ["cs.AR", "cs.LG", "hep-ex", "B.2.4; B.6"], "comment": null, "summary": "Neural networks with a latency requirement on the order of microseconds, like\nthe ones used at the CERN Large Hadron Collider, are typically deployed on\nFPGAs fully unrolled and pipelined. A bottleneck for the deployment of such\nneural networks is area utilization, which is directly related to the required\nconstant matrix-vector multiplication (CMVM) operations. In this work, we\npropose an efficient algorithm for implementing CMVM operations with\ndistributed arithmetic (DA) on FPGAs that simultaneously optimizes for area\nconsumption and latency. The algorithm achieves resource reduction similar to\nstate-of-the-art algorithms while being significantly faster to compute. The\nproposed algorithm is open-sourced and integrated into the \\texttt{hls4ml}\nlibrary, a free and open-source library for running real-time neural network\ninference on FPGAs. We show that the proposed algorithm can reduce on-chip\nresources by up to a third for realistic, highly quantized neural networks\nwhile simultaneously reducing latency, enabling the implementation of\npreviously infeasible networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5f0f\u7b97\u6cd5\uff08DA\uff09\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728FPGA\u4e0a\u5b9e\u73b0\u6052\u5b9a\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\uff08CMVM\uff09\uff0c\u4f18\u5316\u4e86\u9762\u79ef\u5360\u7528\u548c\u5ef6\u8fdf\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u5f00\u6e90\u5e93hls4ml\u4e2d\u3002", "motivation": "\u7531\u4e8e\u5728\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u8981\u6c42\u4e0b\uff08\u5982CERN\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\u4e2d\u7684\u5e94\u7528\uff09\uff0c\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u4ee5\u5168\u5c55\u5f00\u548c\u6d41\u6c34\u7ebf\u65b9\u5f0f\u90e8\u7f72\u5728FPGA\u4e0a\uff0c\u800c\u9762\u79ef\u5229\u7528\u7387\u6210\u4e3a\u74f6\u9888\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u7b97\u6cd5\uff08DA\uff09\u5b9e\u73b0CMVM\u64cd\u4f5c\uff0c\u540c\u65f6\u4f18\u5316\u9762\u79ef\u548c\u5ef6\u8fdf\u3002", "result": "\u7b97\u6cd5\u5728\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u7b97\u6cd5\u76f8\u4f3c\u8d44\u6e90\u51cf\u5c11\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u901f\u5ea6\uff0c\u5e76\u6210\u529f\u5c06\u7247\u4e0a\u8d44\u6e90\u51cf\u5c11\u591a\u8fbe\u4e09\u5206\u4e4b\u4e00\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u964d\u4f4e\u8d44\u6e90\u5360\u7528\u548c\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u6b64\u524d\u4e0d\u53ef\u884c\u7684\u795e\u7ecf\u7f51\u7edc\u90e8\u7f72\uff0c\u5e76\u88ab\u96c6\u6210\u5230\u5f00\u6e90\u5e93hls4ml\u4e2d\u4ee5\u4f9b\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2507.02900", "pdf": "https://arxiv.org/pdf/2507.02900", "abs": "https://arxiv.org/abs/2507.02900", "authors": ["Vineet Kumar Rakesh", "Soumya Mazumdar", "Research Pratim Maity", "Sarbajit Pal", "Amitabha Das", "Tapas Samanta"], "title": "Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.HC", "cs.MM"], "comment": null, "summary": "Talking Head Generation (THG) has emerged as a transformative technology in\ncomputer vision, enabling the synthesis of realistic human faces synchronized\nwith image, audio, text, or video inputs. This paper provides a comprehensive\nreview of methodologies and frameworks for talking head generation,\ncategorizing approaches into 2D--based, 3D--based, Neural Radiance Fields\n(NeRF)--based, diffusion--based, parameter-driven techniques and many other\ntechniques. It evaluates algorithms, datasets, and evaluation metrics while\nhighlighting advancements in perceptual realism and technical efficiency\ncritical for applications such as digital avatars, video dubbing, ultra-low\nbitrate video conferencing, and online education. The study identifies\nchallenges such as reliance on pre--trained models, extreme pose handling,\nmultilingual synthesis, and temporal consistency. Future directions include\nmodular architectures, multilingual datasets, hybrid models blending\npre--trained and task-specific layers, and innovative loss functions. By\nsynthesizing existing research and exploring emerging trends, this paper aims\nto provide actionable insights for researchers and practitioners in the field\nof talking head generation. For the complete survey, code, and curated resource\nlist, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u8bf4\u8bdd\u5934\u90e8\u751f\u6210\uff08THG\uff09\u6280\u672f\uff0c\u5206\u7c7b\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u7b97\u6cd5\u3001\u6570\u636e\u96c6\u53ca\u8bc4\u4f30\u6307\u6807\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "THG\u6280\u672f\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u5982\u6570\u5b57\u865a\u62df\u4eba\u3001\u89c6\u9891\u914d\u97f3\u7b49\uff0c\u4f46\u9762\u4e34\u8bf8\u591a\u6280\u672f\u6311\u6218\uff0c\u9700\u7cfb\u7edf\u6027\u603b\u7ed3\u548c\u524d\u77bb\u6027\u7814\u7a76\u3002", "method": "\u6587\u7ae0\u5c06THG\u65b9\u6cd5\u5206\u4e3a2D\u30013D\u3001NeRF\u3001\u6269\u6563\u6a21\u578b\u7b49\u591a\u79cd\u6280\u672f\u7c7b\u522b\uff0c\u5e76\u5206\u6790\u4e86\u7b97\u6cd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u7efc\u8ff0\u63ed\u793a\u4e86THG\u5728\u611f\u77e5\u771f\u5b9e\u6027\u548c\u6280\u672f\u6548\u7387\u4e0a\u7684\u8fdb\u5c55\uff0c\u540c\u65f6\u6307\u51fa\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f9d\u8d56\u3001\u6781\u7aef\u59ff\u6001\u5904\u7406\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u6a21\u5757\u5316\u67b6\u6784\u3001\u6df7\u5408\u6a21\u578b\u7b49\uff0c\u672c\u6587\u4e3aTHG\u9886\u57df\u7684\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2507.02393", "pdf": "https://arxiv.org/pdf/2507.02393", "abs": "https://arxiv.org/abs/2507.02393", "authors": ["Seokyeong Lee", "Sithu Aung", "Junyong Choi", "Seungryong Kim", "Ig-Jae Kim", "Junghyun Cho"], "title": "PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection", "categories": ["cs.CV", "cs.GR"], "comment": "18 pages, 16 figures", "summary": "Monocular 3D object detection (M3OD) has long faced challenges due to data\nscarcity caused by high annotation costs and inherent 2D-to-3D ambiguity.\nAlthough various weakly supervised methods and pseudo-labeling methods have\nbeen proposed to address these issues, they are mostly limited by\ndomain-specific learning or rely solely on shape information from a single\nobservation. In this paper, we propose a novel pseudo-labeling framework that\nuses only video data and is more robust to occlusion, without requiring a\nmulti-view setup, additional sensors, camera poses, or domain-specific\ntraining. Specifically, we explore a technique for aggregating the\npseudo-LiDARs of both static and dynamic objects across temporally adjacent\nframes using object point tracking, enabling 3D attribute extraction in\nscenarios where 3D data acquisition is infeasible. Extensive experiments\ndemonstrate that our method ensures reliable accuracy and strong scalability,\nmaking it a practical and effective solution for M3OD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u89c6\u9891\u6570\u636e\u4e14\u65e0\u9700\u591a\u89c6\u89d2\u8bbe\u7f6e\u6216\u5176\u4ed6\u4f20\u611f\u5668\u7684\u4f2a\u6807\u7b7e\u6846\u67b6\uff0c\u63d0\u5347\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u4e2d\u6570\u636e\u7a00\u7f3a\u548c2D\u52303D\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u73b0\u6709\u7684\u5f31\u76d1\u7763\u548c\u4f2a\u6807\u7b7e\u65b9\u6cd5\u53d7\u5230\u9886\u57df\u7279\u5b9a\u5b66\u4e60\u6216\u5355\u4e00\u89c2\u5bdf\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7\u89c6\u9891\u6570\u636e\uff0c\u5229\u7528\u7269\u4f53\u70b9\u8ddf\u8e2a\u6280\u672f\u805a\u5408\u9759\u6001\u548c\u52a8\u6001\u7269\u4f53\u7684\u4f2aLiDAR\u70b9\u4e91\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u6216\u591a\u89c6\u89d2\u8bbe\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u8868\u73b0\u9c81\u68d2\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u5f3a\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03950", "pdf": "https://arxiv.org/pdf/2507.03950", "abs": "https://arxiv.org/abs/2507.03950", "authors": ["Yizhou Luo", "Kwan-Wu Chin", "Ruyi Guan", "Xi Xiao", "Caimeng Wang", "Jingyin Feng", "Tengjiao He"], "title": "Optimizing Age of Trust and Throughput in Multi-Hop UAV-Aided IoT Networks", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Devices operating in Internet of Things (IoT) networks may be deployed across\nvast geographical areas and interconnected via multi-hop communications.\nFurther, they may be unguarded. This makes them vulnerable to attacks and\nmotivates operators to check on devices frequently. To this end, we propose and\nstudy an Unmanned Aerial Vehicle (UAV)-aided attestation framework for use in\nIoT networks with a charging station powered by solar. A key challenge is\noptimizing the trajectory of the UAV to ensure it attests as many devices as\npossible. A trade-off here is that devices being checked by the UAV are\noffline, which affects the amount of data delivered to a gateway. Another\nchallenge is that the charging station experiences time-varying energy\narrivals, which in turn affect the flight duration and charging schedule of the\nUAV. To address these challenges, we employ a Deep Reinforcement Learning (DRL)\nsolution to optimize the UAV's charging schedule and the selection of devices\nto be attested during each flight. The simulation results show that our\nsolution reduces the average age of trust by 88% and throughput loss due to\nattestation by 30%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u7269\u8054\u7f51\u8bbe\u5907\u8ba4\u8bc1\u6846\u67b6\uff0c\u5229\u7528\u592a\u9633\u80fd\u5145\u7535\u7ad9\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u65e0\u4eba\u673a\u7684\u98de\u884c\u8f68\u8ff9\u548c\u5145\u7535\u8ba1\u5212\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba4\u8bc1\u6548\u7387\u548c\u7f51\u7edc\u541e\u5410\u91cf\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u5206\u5e03\u5e7f\u6cdb\u4e14\u6613\u53d7\u653b\u51fb\uff0c\u9700\u8981\u9891\u7e41\u8ba4\u8bc1\uff0c\u4f46\u4f20\u7edf\u65b9\u5f0f\u6548\u7387\u4f4e\u3002", "method": "\u91c7\u7528\u65e0\u4eba\u673a\u8f85\u52a9\u8ba4\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u592a\u9633\u80fd\u5145\u7535\u7ad9\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5145\u7535\u8ba1\u5212\u4e0e\u8bbe\u5907\u9009\u62e9\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4fe1\u4efb\u5e74\u9f84\u5e73\u5747\u51cf\u5c1188%\uff0c\u541e\u5410\u91cf\u635f\u5931\u51cf\u5c1130%\u3002", "conclusion": "\u8be5\u65b9\u6848\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7269\u8054\u7f51\u7f51\u7edc\u3002"}}
{"id": "2507.04830", "pdf": "https://arxiv.org/pdf/2507.04830", "abs": "https://arxiv.org/abs/2507.04830", "authors": ["Martin Leucker"], "title": "A Note on Runtime Verification of Concurrent Systems", "categories": ["cs.LO", "cs.FL", "cs.SE"], "comment": "14 pages, 1 figure", "summary": "To maximize the information gained from a single execution when verifying a\nconcurrent system, one can derive all concurrency-aware equivalent executions\nand check them against linear specifications. This paper offers an alternative\nperspective on verification of concurrent systems by leveraging trace-based\nlogics rather than sequence-based formalisms. Linear Temporal Logic over\nMazurkiewicz Traces (LTrL) operates on partial-order representations of\nexecutions, meaning that once a single execution is specified, all equivalent\ninterleavings are implicitly considered. This paper introduces a three valued\nversion of LTrL, indicating whether the so-far observed execution of the\nconcurrent system is one of correct, incorrect or inconclusive, together with a\nsuitable monitor synthesis procedure. To this end, the paper recalls a\nconstruction of trace-consistent B\\\"uchi automata for LTrL formulas and\nexplains how to employ it in well-understood monitor synthesis procedures. In\nthis way, a monitor results that yields for any linearization of an observed\ntrace the same verification verdict.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u5e8f\u6267\u884c\u7684LTrL\u4e09\u503c\u903b\u8f91\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u5e76\u53d1\u7cfb\u7edf\uff0c\u901a\u8fc7\u76d1\u89c6\u5668\u5408\u6210\u63d0\u4f9b\u4e00\u81f4\u7684\u9a8c\u8bc1\u7ed3\u679c\u3002", "motivation": "\u4e3a\u4e86\u5728\u5355\u6b21\u6267\u884c\u4e2d\u6700\u5927\u5316\u4fe1\u606f\u83b7\u53d6\u5e76\u9a8c\u8bc1\u5e76\u53d1\u7cfb\u7edf\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8etrace\u7684\u903b\u8f91\u66ff\u4ee3\u4f20\u7edf\u7684\u5e8f\u5217\u903b\u8f91\u3002", "method": "\u8bba\u6587\u91c7\u7528Mazurkiewicz Traces\u4e0a\u7684\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTrL\uff09\uff0c\u901a\u8fc7\u90e8\u5206\u5e8f\u8868\u793a\u6267\u884c\uff0c\u5e76\u6269\u5c55\u4e3a\u4e09\u503c\u903b\u8f91\uff08\u6b63\u786e\u3001\u9519\u8bef\u6216\u4e0d\u786e\u5b9a\uff09\uff0c\u8bbe\u8ba1\u76d1\u89c6\u5668\u5408\u6210\u7a0b\u5e8f\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u5e76\u53d1\u7cfb\u7edf\u7684\u5355\u6b21\u6267\u884c\u9a8c\u8bc1\uff0c\u6240\u6709\u7b49\u6548\u4ea4\u9519\u6267\u884c\u5747\u88ab\u9690\u5f0f\u8003\u8651\uff0c\u76d1\u89c6\u5668\u80fd\u4e3a\u4efb\u4f55\u7ebf\u6027\u5316trace\u63d0\u4f9b\u4e00\u81f4\u7684\u9a8c\u8bc1\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u4e09\u503cLTrL\u548c\u76d1\u89c6\u5668\u5408\u6210\uff0c\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u5e76\u53d1\u7cfb\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u3002"}}
{"id": "2507.02920", "pdf": "https://arxiv.org/pdf/2507.02920", "abs": "https://arxiv.org/abs/2507.02920", "authors": ["Reza Samimi", "Aditya Bhattacharya", "Lucija Gosak", "Gregor Stiglic", "Katrien Verbert"], "title": "Visual-Conversational Interface for Evidence-Based Explanation of Diabetes Risk Prediction", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "18 pages, 5 figures, 7th ACM Conference on Conversational User\n  Interfaces", "summary": "Healthcare professionals need effective ways to use, understand, and validate\nAI-driven clinical decision support systems. Existing systems face two key\nlimitations: complex visualizations and a lack of grounding in scientific\nevidence. We present an integrated decision support system that combines\ninteractive visualizations with a conversational agent to explain diabetes risk\nassessments. We propose a hybrid prompt handling approach combining fine-tuned\nlanguage models for analytical queries with general Large Language Models\n(LLMs) for broader medical questions, a methodology for grounding AI\nexplanations in scientific evidence, and a feature range analysis technique to\nsupport deeper understanding of feature contributions. We conducted a\nmixed-methods study with 30 healthcare professionals and found that the\nconversational interactions helped healthcare professionals build a clear\nunderstanding of model assessments, while the integration of scientific\nevidence calibrated trust in the system's decisions. Most participants reported\nthat the system supported both patient risk evaluation and recommendation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u548c\u5bf9\u8bdd\u4ee3\u7406\u7684\u7cd6\u5c3f\u75c5\u98ce\u9669\u8bc4\u4f30\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u7684\u590d\u6742\u6027\u548c\u79d1\u5b66\u4f9d\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u6765\u4f7f\u7528\u3001\u7406\u89e3\u548c\u9a8c\u8bc1AI\u9a71\u52a8\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u5f53\u524d\u7cfb\u7edf\u5b58\u5728\u53ef\u89c6\u5316\u590d\u6742\u548c\u7f3a\u4e4f\u79d1\u5b66\u4f9d\u636e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u63d0\u793a\u5904\u7406\u65b9\u6cd5\uff0c\u7ed3\u5408\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u548c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u57fa\u4e8e\u79d1\u5b66\u4f9d\u636e\u7684AI\u89e3\u91ca\u65b9\u6cd5\u53ca\u7279\u5f81\u8303\u56f4\u5206\u6790\u6280\u672f\u3002", "result": "30\u540d\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u7684\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u8bdd\u4ea4\u4e92\u589e\u5f3a\u4e86\u6a21\u578b\u8bc4\u4f30\u7684\u7406\u89e3\uff0c\u79d1\u5b66\u4f9d\u636e\u7684\u6574\u5408\u6821\u51c6\u4e86\u5bf9\u7cfb\u7edf\u51b3\u7b56\u7684\u4fe1\u4efb\u3002", "conclusion": "\u7cfb\u7edf\u5728\u60a3\u8005\u98ce\u9669\u8bc4\u4f30\u548c\u63a8\u8350\u65b9\u9762\u5f97\u5230\u4e86\u591a\u6570\u53c2\u4e0e\u8005\u7684\u8ba4\u53ef\u3002"}}
{"id": "2507.03411", "pdf": "https://arxiv.org/pdf/2507.03411", "abs": "https://arxiv.org/abs/2507.03411", "authors": ["Ali Nikseresht"], "title": "A Hybrid Game-Theory and Deep Learning Framework for Predicting Tourist Arrivals via Big Data Analytics and Opinion Leader Detection", "categories": ["cs.LG", "cs.GT", "eess.SP"], "comment": null, "summary": "In the era of Industry 5.0, data-driven decision-making has become\nindispensable for optimizing systems across Industrial Engineering. This paper\naddresses the value of big data analytics by proposing a novel non-linear\nhybrid approach for forecasting international tourist arrivals in two different\ncontexts: (i) arrivals to Hong Kong from five major source nations\n(pre-COVID-19), and (ii) arrivals to Sanya in Hainan province, China\n(post-COVID-19). The method integrates multiple sources of Internet big data\nand employs an innovative game theory-based algorithm to identify opinion\nleaders on social media platforms. Subsequently, nonstationary attributes in\ntourism demand data are managed through Empirical Wavelet Transform (EWT),\nensuring refined time-frequency analysis. Finally, a memory-aware Stacked\nBi-directional Long Short-Term Memory (Stacked BiLSTM) network is used to\ngenerate accurate demand forecasts. Experimental results demonstrate that this\napproach outperforms existing state-of-the-art techniques and remains robust\nunder dynamic and volatile conditions, highlighting its applicability to\nbroader Industrial Engineering domains, such as logistics, supply chain\nmanagement, and production planning, where forecasting and resource allocation\nare key challenges. By merging advanced Deep Learning (DL), time-frequency\nanalysis, and social media insights, the proposed framework showcases how\nlarge-scale data can elevate the quality and efficiency of decision-making\nprocesses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u975e\u7ebf\u6027\u6df7\u5408\u65b9\u6cd5\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u56fd\u9645\u6e38\u5ba2\u5230\u8fbe\u91cf\uff0c\u6574\u5408\u4e86\u4e92\u8054\u7f51\u5927\u6570\u636e\u3001\u6e38\u620f\u7406\u8bba\u7b97\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u7ed3\u679c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u5de5\u4e1a5.0\u65f6\u4ee3\uff0c\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\u5bf9\u4f18\u5316\u5de5\u4e1a\u5de5\u7a0b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5927\u6570\u636e\u5206\u6790\u63d0\u5347\u65c5\u6e38\u9700\u6c42\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5c55\u793a\u5176\u5728\u66f4\u5e7f\u6cdb\u5de5\u4e1a\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4f7f\u7528\u6e38\u620f\u7406\u8bba\u7b97\u6cd5\u8bc6\u522b\u793e\u4ea4\u5a92\u4f53\u610f\u89c1\u9886\u8896\uff1b2) \u5229\u7528\u7ecf\u9a8c\u5c0f\u6ce2\u53d8\u6362\u5904\u7406\u975e\u5e73\u7a33\u6570\u636e\uff1b3) \u91c7\u7528\u5806\u53e0\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u548c\u6ce2\u52a8\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u7269\u6d41\u3001\u4f9b\u5e94\u94fe\u7ba1\u7406\u548c\u751f\u4ea7\u89c4\u5212\u7b49\u9886\u57df\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u5206\u6790\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u5927\u5c3a\u5ea6\u6570\u636e\u5982\u4f55\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4e3a\u5de5\u4e1a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.03773", "pdf": "https://arxiv.org/pdf/2507.03773", "abs": "https://arxiv.org/abs/2507.03773", "authors": ["Yibo He", "Cunjian Huang", "Xianmiao Qu", "Hongdeng Chen", "Wei Yang", "Tao Xie"], "title": "RVISmith: Fuzzing Compilers for RVV Intrinsics", "categories": ["cs.CR", "cs.DC", "cs.PL", "cs.SE"], "comment": "To appear in ACM CCS 2025", "summary": "Modern processors are equipped with single instruction multiple data (SIMD)\ninstructions for fine-grained data parallelism. Compiler auto-vectorization\ntechniques that target SIMD instructions face performance limitations due to\ninsufficient information available at compile time, requiring programmers to\nmanually manipulate SIMD instructions. SIMD intrinsics, a type of built-in\nfunction provided by modern compilers, enable programmers to manipulate SIMD\ninstructions within high-level programming languages. Bugs in compilers for\nSIMD intrinsics can introduce potential threats to software security, producing\nunintended calculation results, data loss, program crashes, etc.\n  To detect bugs in compilers for SIMD intrinsics, we propose RVISmith, a\nrandomized fuzzer that generates well-defined C programs that include various\ninvocation sequences of RVV (RISC-V Vector Extension) intrinsics. We design\nRVISmith to achieve the following objectives: (i) achieving high intrinsic\ncoverage, (ii) improving sequence variety, and (iii) without known undefined\nbehaviors. We implement RVISmith based on the ratified RVV intrinsic\nspecification and evaluate our approach with three modern compilers: GCC, LLVM,\nand XuanTie. Experimental results show that RVISmith achieves 11.5 times higher\nintrinsic coverage than the state-of-the-art fuzzer for RVV intrinsics. By\ndifferential testing that compares results across different compilers,\noptimizations, and equivalent programs, we detect and report 13 previously\nunknown bugs of the three compilers under test to date. Of these bugs, 10 are\nconfirmed and another 3 are fixed by the compiler developers.", "AI": {"tldr": "RVISmith\u662f\u4e00\u4e2a\u9488\u5bf9SIMD intrinsics\u7f16\u8bd1\u5668bug\u7684\u968f\u673a\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\uff0c\u80fd\u9ad8\u6548\u68c0\u6d4b\u5e76\u62a5\u544a\u7f16\u8bd1\u5668\u4e2d\u7684\u672a\u77e5\u95ee\u9898\u3002", "motivation": "\u7f16\u8bd1\u5668\u5728SIMD intrinsics\u5904\u7406\u4e2d\u7684bug\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u81ea\u52a8\u5411\u91cf\u5316\u6280\u672f\u53d7\u9650\uff0c\u9700\u624b\u52a8\u5e72\u9884\u3002", "method": "\u8bbe\u8ba1RVISmith\u751f\u6210\u5305\u542bRVV intrinsics\u7684C\u7a0b\u5e8f\uff0c\u901a\u8fc7\u9ad8\u8986\u76d6\u7387\u3001\u591a\u6837\u5316\u5e8f\u5217\u548c\u65e0\u672a\u5b9a\u4e49\u884c\u4e3a\u7b49\u7279\u6027\u68c0\u6d4bbug\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRVISmith\u5728\u8986\u76d6\u7387\u4e0a\u8fdc\u8d85\u73b0\u6709\u5de5\u5177\uff0c\u5e76\u68c0\u6d4b\u51fa13\u4e2a\u672a\u77e5bug\uff0c\u5176\u4e2d10\u4e2a\u5df2\u786e\u8ba4\uff0c3\u4e2a\u5df2\u4fee\u590d\u3002", "conclusion": "RVISmith\u80fd\u6709\u6548\u63d0\u5347\u7f16\u8bd1\u5668\u5bf9SIMD intrinsics\u7684\u652f\u6301\u8d28\u91cf\uff0c\u68c0\u6d4b\u6f5c\u5728\u95ee\u9898\u3002"}}
{"id": "2507.03849", "pdf": "https://arxiv.org/pdf/2507.03849", "abs": "https://arxiv.org/abs/2507.03849", "authors": ["Mai Zheng", "Duo Zhang", "Ahmed Dajani"], "title": "On Fault Tolerance of Data Storage Systems: A Holistic Perspective", "categories": ["cs.DC"], "comment": null, "summary": "Data storage systems serve as the foundation of digital society. The enormous\ndata generated by people on a daily basis make the fault tolerance of data\nstorage systems increasingly important. Unfortunately, modern storage systems\nconsist of complicated hardware and software layers interacting with each\nother, which may contain latent bugs that elude extensive testing and lead to\ndata corruption, system downtime, or even unrecoverable data loss in practice.\nIn this chapter, we take a holistic view to introduce the typical architecture\nand major components of modern data storage systems (e.g., solid state drives,\npersistent memories, local file systems, and distributed storage management at\nscale). Next, we discuss a few representative bug detection and fault tolerance\ntechniques across layers with a focus on issues that affect system recovery and\ndata integrity. Finally, we conclude with open challenges and future work.", "AI": {"tldr": "\u73b0\u4ee3\u6570\u636e\u5b58\u50a8\u7cfb\u7edf\u67b6\u6784\u590d\u6742\uff0c\u786c\u4ef6\u4e0e\u8f6f\u4ef6\u5c42\u76f8\u4e92\u4f5c\u7528\u53ef\u80fd\u5bfc\u81f4\u6f5c\u5728\u9519\u8bef\uff0c\u5f71\u54cd\u6570\u636e\u5b8c\u6574\u6027\u548c\u7cfb\u7edf\u6062\u590d\u3002\u6587\u7ae0\u4ecb\u7ecd\u4e86\u5176\u5178\u578b\u67b6\u6784\u3001\u7ec4\u4ef6\u53ca\u8de8\u5c42\u7ea7\u9519\u8bef\u68c0\u6d4b\u548c\u5bb9\u9519\u6280\u672f\uff0c\u5e76\u603b\u7ed3\u4e86\u672a\u6765\u6311\u6218\u3002", "motivation": "\u6570\u636e\u5b58\u50a8\u7cfb\u7edf\u662f\u6570\u5b57\u793e\u4f1a\u7684\u57fa\u7840\uff0c\u5176\u6545\u969c\u5bb9\u5fcd\u5ea6\u5bf9\u6570\u636e\u5b89\u5168\u548c\u7cfb\u7edf\u7a33\u5b9a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4ece\u6574\u4f53\u89c6\u89d2\u4ecb\u7ecd\u73b0\u4ee3\u6570\u636e\u5b58\u50a8\u7cfb\u7edf\u7684\u67b6\u6784\u548c\u7ec4\u4ef6\uff0c\u5e76\u8ba8\u8bba\u8de8\u5c42\u7ea7\u7684\u9519\u8bef\u68c0\u6d4b\u4e0e\u5bb9\u9519\u6280\u672f\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u6280\u672f\u5bf9\u7cfb\u7edf\u6062\u590d\u548c\u6570\u636e\u5b8c\u6574\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u548c\u89e3\u51b3\u5b58\u50a8\u7cfb\u7edf\u95ee\u9898\u7684\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2507.04677", "pdf": "https://arxiv.org/pdf/2507.04677", "abs": "https://arxiv.org/abs/2507.04677", "authors": ["Siqing Fu", "Lizhou Wu", "Tiejun Li", "Chunyuan Zhang", "Sheng Ma", "Jianmin Zhang", "Yuhan Tang", "Jixuan Tang"], "title": "NeuroPDE: A Neuromorphic PDE Solver Based on Spintronic and Ferroelectric Devices", "categories": ["cs.AR", "B.7.1"], "comment": "9 pages, 12 figures, accepted at ICCAD 2025 (The 2025 IEEE/ACM\n  International Conference on Computer-Aided Design)", "summary": "In recent years, new methods for solving partial differential equations\n(PDEs) such as Monte Carlo random walk methods have gained considerable\nattention. However, due to the lack of hardware-intrinsic randomness in the\nconventional von Neumann architecture, the performance of PDE solvers is\nlimited. In this paper, we introduce NeuroPDE, a hardware design for\nneuromorphic PDE solvers that utilizes emerging spintronic and ferroelectric\ndevices. NeuroPDE incorporates spin neurons that are capable of probabilistic\ntransmission to emulate random walks, along with ferroelectric synapses that\nstore continuous weights non-volatilely. The proposed NeuroPDE achieves a\nvariance of less than 1e-2 compared to analytical solutions when solving\ndiffusion equations, demonstrating a performance advantage of 3.48x to 315x\nspeedup in execution time and an energy consumption advantage of 2.7x to 29.8x\nover advanced CMOS-based neuromorphic chips. By leveraging the inherent\nphysical stochasticity of emerging devices, this study paves the way for future\nprobabilistic neuromorphic computing systems.", "AI": {"tldr": "NeuroPDE\u662f\u4e00\u79cd\u5229\u7528\u65b0\u5174\u81ea\u65cb\u7535\u5b50\u548c\u94c1\u7535\u5668\u4ef6\u7684\u795e\u7ecf\u5f62\u6001PDE\u6c42\u89e3\u5668\u786c\u4ef6\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u65cb\u795e\u7ecf\u5143\u7684\u6982\u7387\u4f20\u8f93\u548c\u94c1\u7535\u7a81\u89e6\u7684\u975e\u6613\u5931\u6027\u5b58\u50a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c42\u89e3\u6269\u6563\u65b9\u7a0b\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u4f20\u7edf\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u7f3a\u4e4f\u786c\u4ef6\u56fa\u6709\u7684\u968f\u673a\u6027\uff0c\u9650\u5236\u4e86PDE\u6c42\u89e3\u5668\u7684\u6027\u80fd\u3002NeuroPDE\u901a\u8fc7\u65b0\u5174\u5668\u4ef6\uff08\u5982\u81ea\u65cb\u7535\u5b50\u548c\u94c1\u7535\u5668\u4ef6\uff09\u7684\u7269\u7406\u968f\u673a\u6027\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1NeuroPDE\u786c\u4ef6\uff0c\u7ed3\u5408\u81ea\u65cb\u795e\u7ecf\u5143\u7684\u968f\u673a\u4f20\u8f93\u548c\u94c1\u7535\u7a81\u89e6\u7684\u975e\u6613\u5931\u6027\u5b58\u50a8\uff0c\u6a21\u62df\u968f\u673a\u884c\u8d70\u4ee5\u6c42\u89e3PDE\u3002", "result": "NeuroPDE\u5728\u6c42\u89e3\u6269\u6563\u65b9\u7a0b\u65f6\uff0c\u65b9\u5dee\u5c0f\u4e8e1e-2\uff0c\u6267\u884c\u901f\u5ea6\u5feb3.48x\u81f3315x\uff0c\u80fd\u8017\u4f4e2.7x\u81f329.8x\uff0c\u4f18\u4e8e\u5148\u8fdbCMOS\u795e\u7ecf\u5f62\u6001\u82af\u7247\u3002", "conclusion": "NeuroPDE\u5229\u7528\u65b0\u5174\u5668\u4ef6\u7684\u7269\u7406\u968f\u673a\u6027\uff0c\u4e3a\u672a\u6765\u7684\u6982\u7387\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2507.02941", "pdf": "https://arxiv.org/pdf/2507.02941", "abs": "https://arxiv.org/abs/2507.02941", "authors": ["Yi-Chun Chen", "Arnav Jhala"], "title": "GameTileNet: A Semantic Dataset for Low-Resolution Game Art in Procedural Content Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "Note: This is a preprint version of a paper submitted to AIIDE 2025.\n  It includes additional discussion of limitations and future directions that\n  were omitted from the conference version due to space constraints", "summary": "GameTileNet is a dataset designed to provide semantic labels for\nlow-resolution digital game art, advancing procedural content generation (PCG)\nand related AI research as a vision-language alignment task. Large Language\nModels (LLMs) and image-generative AI models have enabled indie developers to\ncreate visual assets, such as sprites, for game interactions. However,\ngenerating visuals that align with game narratives remains challenging due to\ninconsistent AI outputs, requiring manual adjustments by human artists. The\ndiversity of visual representations in automatically generated game content is\nalso limited because of the imbalance in distributions across styles for\ntraining data. GameTileNet addresses this by collecting artist-created game\ntiles from OpenGameArt.org under Creative Commons licenses and providing\nsemantic annotations to support narrative-driven content generation. The\ndataset introduces a pipeline for object detection in low-resolution tile-based\ngame art (e.g., 32x32 pixels) and annotates semantics, connectivity, and object\nclassifications. GameTileNet is a valuable resource for improving PCG methods,\nsupporting narrative-rich game content, and establishing a baseline for object\ndetection in low-resolution, non-photorealistic images.\n  TL;DR: GameTileNet is a semantic dataset of low-resolution game tiles\ndesigned to support narrative-driven procedural content generation through\nvisual-language alignment.", "AI": {"tldr": "GameTileNet\u662f\u4e00\u4e2a\u4f4e\u5206\u8fa8\u7387\u6e38\u620f\u56fe\u5757\u7684\u8bed\u4e49\u6570\u636e\u96c6\uff0c\u65e8\u5728\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u652f\u6301\u53d9\u4e8b\u9a71\u52a8\u7684\u7a0b\u5e8f\u5316\u5185\u5bb9\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u6e38\u620f\u89c6\u89c9\u8d44\u4ea7\u751f\u6210\u4e2d\u89c6\u89c9\u4e0e\u53d9\u4e8b\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u751f\u6210\u5185\u5bb9\u591a\u6837\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u6536\u96c6\u827a\u672f\u5bb6\u521b\u4f5c\u7684\u6e38\u620f\u56fe\u5757\u5e76\u63d0\u4f9b\u8bed\u4e49\u6807\u6ce8\uff0c\u5305\u62ec\u5bf9\u8c61\u68c0\u6d4b\u3001\u8bed\u4e49\u3001\u8fde\u901a\u6027\u548c\u5bf9\u8c61\u5206\u7c7b\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u652f\u6301\u53d9\u4e8b\u9a71\u52a8\u7684PCG\u65b9\u6cd5\u548c\u4f4e\u5206\u8fa8\u7387\u975e\u771f\u5b9e\u611f\u56fe\u50cf\u5bf9\u8c61\u68c0\u6d4b\u7684\u6570\u636e\u96c6\u3002", "conclusion": "GameTileNet\u662f\u4e00\u4e2a\u6539\u8fdbPCG\u65b9\u6cd5\u548c\u652f\u6301\u4e30\u5bcc\u53d9\u4e8b\u6e38\u620f\u5185\u5bb9\u7684\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2507.04173", "pdf": "https://arxiv.org/pdf/2507.04173", "abs": "https://arxiv.org/abs/2507.04173", "authors": ["Henri A\u00efdasso", "Francis Bordeleau", "Ali Tizghadam"], "title": "Efficient Detection of Intermittent Job Failures Using Few-Shot Learning", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "Accepted at the 41st International Conference on Software Maintenance\n  and Evolution - ICSME 2025, Industry Track", "summary": "One of the main challenges developers face in the use of continuous\nintegration (CI) and deployment pipelines is the occurrence of intermittent job\nfailures, which result from unexpected non-deterministic issues (e.g., flaky\ntests or infrastructure problems) rather than regular code-related errors such\nas bugs. Prior studies developed machine-learning (ML) models trained on large\ndatasets of job logs to classify job failures as either intermittent or\nregular. As an alternative to costly manual labeling of large datasets, the\nstate-of-the-art (SOTA) approach leveraged a heuristic based on\nnon-deterministic job reruns. However, this method mislabels intermittent job\nfailures as regular in contexts where rerunning suspicious job failures is not\nan explicit policy, and therefore limits the SOTA's performance in practice. In\nfact, our manual analysis of 2,125 job failures from 5 industrial and 1\nopen-source projects reveals that, on average, 32\\% of intermittent job\nfailures are mislabeled as regular. To address these limitations, this paper\nintroduces a novel approach to intermittent job failure detection using\nfew-shot learning (FSL). Specifically, we fine-tune a small language model\nusing a few number of manually labeled log examples to generate rich\nembeddings, which are then used to train an ML classifier. Our FSL-based\napproach achieves 70-88\\% F1-score with only 12 shots in all projects,\noutperforming the SOTA, which proved ineffective (34-52\\% F1-score) in 4\nprojects. Overall, this study underlines the importance of data quality over\nquantity and provides a more efficient and practical framework for the\ndetection of intermittent job failures in organizations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6837\u672c\u5b66\u4e60\uff08FSL\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6301\u7eed\u96c6\u6210\u4e2d\u7684\u95f4\u6b47\u6027\u4efb\u52a1\u5931\u8d25\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8bef\u6807\u548c\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\u4f9d\u8d56\u5927\u91cf\u624b\u52a8\u6807\u6ce8\u6570\u636e\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bfc\u81f4\u95f4\u6b47\u6027\u4efb\u52a1\u5931\u8d25\u88ab\u8bef\u6807\uff0c\u5f71\u54cd\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5c0f\u6837\u672c\u5b66\u4e60\uff0c\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e30\u5bcc\u5d4c\u5165\uff0c\u7528\u4e8e\u8bad\u7ec3\u5206\u7c7b\u5668\u3002\u4ec5\u9700\u5c11\u91cf\u624b\u52a8\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "result": "\u65b0\u65b9\u6cd5\u57286\u4e2a\u9879\u76ee\u4e2d\u7684F1\u5206\u6570\u8fbe70-88%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0834-52%\uff09\uff0c\u4e14\u4ec5\u970012\u4e2a\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6570\u636e\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u91cd\u8981\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u95f4\u6b47\u6027\u4efb\u52a1\u5931\u8d25\u68c0\u6d4b\u6846\u67b6\u3002"}}
{"id": "2507.02877", "pdf": "https://arxiv.org/pdf/2507.02877", "abs": "https://arxiv.org/abs/2507.02877", "authors": ["Chi Zhang", "Yu Dong", "Yang Wang", "Yuetong Han", "Guihua Shan", "Bixia Tang"], "title": "AuraGenome: An LLM-Powered Framework for On-the-Fly Reusable and Scalable Circular Genome Visualizations", "categories": ["q-bio.GN", "cs.AI", "cs.GR", "cs.HC"], "comment": null, "summary": "Circular genome visualizations are essential for exploring structural\nvariants and gene regulation. However, existing tools often require complex\nscripting and manual configuration, making the process time-consuming,\nerror-prone, and difficult to learn. To address these challenges, we introduce\nAuraGenome, an LLM-powered framework for rapid, reusable, and scalable\ngeneration of multi-layered circular genome visualizations. AuraGenome combines\na semantic-driven multi-agent workflow with an interactive visual analytics\nsystem. The workflow employs seven specialized LLM-driven agents, each assigned\ndistinct roles such as intent recognition, layout planning, and code\ngeneration, to transform raw genomic data into tailored visualizations. The\nsystem supports multiple coordinated views tailored for genomic data, offering\nring, radial, and chord-based layouts to represent multi-layered circular\ngenome visualizations. In addition to enabling interactions and configuration\nreuse, the system supports real-time refinement and high-quality report export.\nWe validate its effectiveness through two case studies and a comprehensive user\nstudy. AuraGenome is available at: https://github.com/Darius18/AuraGenome.", "AI": {"tldr": "AuraGenome\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u3001\u53ef\u91cd\u7528\u4e14\u53ef\u6269\u5c55\u7684\u591a\u5c42\u73af\u5f62\u57fa\u56e0\u7ec4\u53ef\u89c6\u5316\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u811a\u672c\u548c\u624b\u52a8\u914d\u7f6e\uff0c\u5bfc\u81f4\u8fc7\u7a0b\u8017\u65f6\u3001\u6613\u51fa\u9519\u4e14\u96be\u5b66\u3002", "method": "AuraGenome\u7ed3\u5408\u8bed\u4e49\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5206\u6790\u7cfb\u7edf\uff0c\u4f7f\u7528\u4e03\u4e2a\u4e13\u95e8\u5316\u7684LLM\u4ee3\u7406\u5b8c\u6210\u4ece\u539f\u59cb\u6570\u636e\u5230\u5b9a\u5236\u53ef\u89c6\u5316\u7684\u8f6c\u6362\u3002", "result": "\u7cfb\u7edf\u652f\u6301\u591a\u89c6\u56fe\u3001\u5b9e\u65f6\u4f18\u5316\u548c\u9ad8\u8d28\u91cf\u62a5\u544a\u5bfc\u51fa\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "AuraGenome\u89e3\u51b3\u4e86\u57fa\u56e0\u7ec4\u53ef\u89c6\u5316\u4e2d\u7684\u6548\u7387\u548c\u6613\u7528\u6027\u95ee\u9898\uff0c\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2507.04001", "pdf": "https://arxiv.org/pdf/2507.04001", "abs": "https://arxiv.org/abs/2507.04001", "authors": ["Mohammed Zain Farooqi", "Masoud Hemmatpour", "Tore Heide Larsen"], "title": "In-Network Memory Access: Bridging SmartNIC and Host Memory", "categories": ["cs.NI"], "comment": null, "summary": "SmartNICs have been increasingly utilized across various applications to\noffload specific computational tasks, thereby enhancing overall system\nperformance. However, this offloading process introduces several communication\nchallenges that must be addressed for effective integration. A key challenge\nlies in establishing efficient communication between the offloaded components\nand the main application running on the host. In this study, we evaluate\ndifferent approaches for achieving memory access between the host and SmartNIC.\nWe analyze memory access performance on both the SmartNIC and the host to\nsupport in-network applications and guide the selection of an appropriate\nmemory access design.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86SmartNIC\u4e0e\u4e3b\u673a\u95f4\u5185\u5b58\u8bbf\u95ee\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u7f51\u7edc\u5185\u5e94\u7528\u5e76\u6307\u5bfc\u8bbe\u8ba1\u9009\u62e9\u3002", "motivation": "SmartNIC\u5728\u5378\u8f7d\u8ba1\u7b97\u4efb\u52a1\u65f6\u5f15\u5165\u4e86\u901a\u4fe1\u6311\u6218\uff0c\u9700\u89e3\u51b3\u4e3b\u673a\u4e0e\u5378\u8f7d\u7ec4\u4ef6\u95f4\u9ad8\u6548\u901a\u4fe1\u95ee\u9898\u3002", "method": "\u8bc4\u4f30\u4e86\u4e3b\u673a\u4e0eSmartNIC\u95f4\u7684\u591a\u79cd\u5185\u5b58\u8bbf\u95ee\u65b9\u6cd5\u3002", "result": "\u5206\u6790\u4e86SmartNIC\u548c\u4e3b\u673a\u4e0a\u7684\u5185\u5b58\u8bbf\u95ee\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7f51\u7edc\u5185\u5e94\u7528\u63d0\u4f9b\u4e86\u5185\u5b58\u8bbf\u95ee\u8bbe\u8ba1\u7684\u9009\u62e9\u6307\u5bfc\u3002"}}
{"id": "2507.05044", "pdf": "https://arxiv.org/pdf/2507.05044", "abs": "https://arxiv.org/abs/2507.05044", "authors": ["Albert Brandl", "Christian G. Ferm\u00fcller", "Gernot Salzer"], "title": "Testing for Renamability to Classes of Clause Sets", "categories": ["cs.LO", "cs.CC"], "comment": null, "summary": "This paper investigates the problem of testing clause sets for membership in\nclasses known from literature. In particular, we are interested in classes\ndefined via renaming: Is it possible to rename the predicates in a way such\nthat positive and negative literals satisfy certain conditions? We show that\nfor classes like Horn or OCC1N, the existence of such renamings can be decided\nin polynomial time, whereas the same problem is NP-complete for class PVD. The\ndecision procedures are based on hyper-resolution; if a renaming exists, it can\nbe extracted from the final saturated clause set.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u91cd\u547d\u540d\u6d4b\u8bd5\u5b50\u53e5\u96c6\u662f\u5426\u5c5e\u4e8e\u67d0\u4e9b\u5df2\u77e5\u7c7b\u7684\u95ee\u9898\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u7c7b\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u53ef\u5224\u5b9a\u6027\u7684\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u901a\u8fc7\u8c13\u8bcd\u91cd\u547d\u540d\u6765\u786e\u5b9a\u5b50\u53e5\u96c6\u662f\u5426\u5c5e\u4e8e\u7279\u5b9a\u7c7b\uff08\u5982Horn\u6216OCC1N\uff09\u7684\u53ef\u80fd\u6027\u3002", "method": "\u57fa\u4e8e\u8d85\u5206\u8fa8\u7387\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u82e5\u5b58\u5728\u91cd\u547d\u540d\uff0c\u53ef\u4ece\u9971\u548c\u5b50\u53e5\u96c6\u4e2d\u63d0\u53d6\u3002", "result": "\u5bf9\u4e8eHorn\u548cOCC1N\u7c7b\uff0c\u91cd\u547d\u540d\u7684\u5b58\u5728\u6027\u53ef\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5224\u5b9a\uff0c\u800c\u5bf9PVD\u7c7b\u8be5\u95ee\u9898\u4e3aNP\u5b8c\u5168\u3002", "conclusion": "\u4e0d\u540c\u7c7b\u7684\u91cd\u547d\u540d\u95ee\u9898\u5728\u590d\u6742\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.03032", "pdf": "https://arxiv.org/pdf/2507.03032", "abs": "https://arxiv.org/abs/2507.03032", "authors": ["Don Roosan", "Tiffany Khao", "Huong Phan", "Yan Li"], "title": "Enhanced knowledge retention through MedScrab: an interactive mobile game", "categories": ["cs.HC", "q-bio.OT"], "comment": "Conference can be found at: https://medinfo2025.org/Home/Program", "summary": "Noncompliance with medication regimens poses an immense challenge in the\nmanagement of chronic diseases, often resulting in exacerbated health\ncomplications and recurrent hospital admissions. Addressing this gap, our team\ndesigned an innovative mobile game aimed at bolstering medication adherence and\ninformation retention within the general population. Employing Amazon\nMechanical Turk, participants were enlisted and allocated into two cohorts: one\nengaged with our mobile game and the other perused an informational pamphlet\nabout medication. Both cohorts underwent a pre-intervention quiz, followed by\ntheir respective interventions, and concluded with a post-intervention quiz.\nPrimary outcome measures included the difference in quiz scores and the game\nplay duration. The investigation encompassed 243 participants with homogenous\nbaseline attributes. Participants interacting with the mobile game depicted a\nsignificant enhancement in their post-intervention scores compared to the\npre-intervention scores. We observed a notable correlation of 0.346 (p<0.001)\nwith a robust medium effect size of 0.641 (0.503 - 0.779). Although the\nduration of game play and post-intervention scores didn't exhibit a direct\ncorrelation, a tendency towards superior post-intervention scores was evident\namong participants who dedicated more time to the game. The interactive mobile\ngame we developed exhibits potential as an engaging instrument for empowering\npatients and caregivers. Providing critical medication information and the\npotential side effects in a manner that increases retention would thereby\nmitigate medication noncompliance. Future research endeavors should focus on\noptimizing and broadening the application of such mobile interfaces to fortify\npublic health initiatives.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86\u4e00\u6b3e\u79fb\u52a8\u6e38\u620f\u4ee5\u63d0\u9ad8\u836f\u7269\u4f9d\u4ece\u6027\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6e38\u620f\u4e0e\u4f20\u7edf\u5ba3\u4f20\u8d44\u6599\u7684\u6548\u679c\uff0c\u53d1\u73b0\u6e38\u620f\u7ec4\u5728\u77e5\u8bc6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u6162\u6027\u75c5\u60a3\u8005\u836f\u7269\u4f9d\u4ece\u6027\u5dee\u5bfc\u81f4\u5065\u5eb7\u95ee\u9898\uff0c\u9700\u8981\u521b\u65b0\u65b9\u6cd5\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\u3002", "method": "\u53c2\u4e0e\u8005\u5206\u4e3a\u6e38\u620f\u7ec4\u548c\u5ba3\u4f20\u8d44\u6599\u7ec4\uff0c\u901a\u8fc7\u524d\u540e\u6d4b\u8bd5\u5bf9\u6bd4\u6548\u679c\u3002", "result": "\u6e38\u620f\u7ec4\u6d4b\u8bd5\u6210\u7ee9\u663e\u8457\u63d0\u5347\uff0c\u4e14\u6e38\u620f\u65f6\u957f\u4e0e\u6210\u7ee9\u5448\u6b63\u76f8\u5173\u8d8b\u52bf\u3002", "conclusion": "\u79fb\u52a8\u6e38\u620f\u53ef\u4f5c\u4e3a\u63d0\u5347\u836f\u7269\u77e5\u8bc6\u7684\u6709\u529b\u5de5\u5177\uff0c\u672a\u6765\u9700\u4f18\u5316\u5176\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.03608", "pdf": "https://arxiv.org/pdf/2507.03608", "abs": "https://arxiv.org/abs/2507.03608", "authors": ["Sarat Ahmad", "Zeinab Nezami", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "title": "Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)", "categories": ["cs.AI", "cs.DC", "cs.ET", "cs.NI"], "comment": null, "summary": "Generative AI (GenAI) is expected to play a pivotal role in enabling\nautonomous optimization in future wireless networks. Within the ORAN\narchitecture, Large Language Models (LLMs) can be specialized to generate xApps\nand rApps by leveraging specifications and API definitions from the RAN\nIntelligent Controller (RIC) platform. However, fine-tuning base LLMs for\ntelecom-specific tasks remains expensive and resource-intensive.\nRetrieval-Augmented Generation (RAG) offers a practical alternative through\nin-context learning, enabling domain adaptation without full retraining. While\ntraditional RAG systems rely on vector-based retrieval, emerging variants such\nas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval\nstrategies to support multi-hop reasoning and improve factual grounding.\nDespite their promise, these methods lack systematic, metric-driven\nevaluations, particularly in high-stakes domains such as ORAN. In this study,\nwe conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid\nGraphRAG using ORAN specifications. We assess performance across varying\nquestion complexities using established generation metrics: faithfulness,\nanswer relevance, context relevance, and factual correctness. Results show that\nboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG\nimproves factual correctness by 8%, while GraphRAG improves context relevance\nby 7%.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4f20\u7edfRAG\u3001GraphRAG\u548cHybrid GraphRAG\u5728ORAN\u67b6\u6784\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0GraphRAG\u548cHybrid GraphRAG\u5728\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u4e8b\u5b9e\u6b63\u786e\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5728ORAN\u67b6\u6784\u4e2d\uff0c\u4e3a\u7535\u4fe1\u4efb\u52a1\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6210\u672c\u9ad8\u6602\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u6bd4\u8f83\u4e86\u4f20\u7edfRAG\u3001GraphRAG\u548cHybrid GraphRAG\u7684\u6027\u80fd\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728ORAN\u89c4\u8303\u4e2d\u7684\u5e94\u7528\uff0c\u4f7f\u7528\u751f\u6210\u6307\u6807\u5982\u5fe0\u5b9e\u5ea6\u3001\u7b54\u6848\u76f8\u5173\u6027\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u4e8b\u5b9e\u6b63\u786e\u6027\u3002", "result": "GraphRAG\u548cHybrid GraphRAG\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfRAG\uff0cHybrid GraphRAG\u5c06\u4e8b\u5b9e\u6b63\u786e\u6027\u63d0\u9ad8\u4e868%\uff0c\u800cGraphRAG\u5c06\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u63d0\u9ad8\u4e867%\u3002", "conclusion": "GraphRAG\u548cHybrid GraphRAG\u5728ORAN\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u81ea\u4e3b\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.04736", "pdf": "https://arxiv.org/pdf/2507.04736", "abs": "https://arxiv.org/abs/2507.04736", "authors": ["Zhirong Chen", "Kaiyan Chang", "Zhuolin Li", "Xinyang He", "Chujie Chen", "Cangyuan Li", "Mengdi Wang", "Haobo Xu", "Yinhe Han", "Ying Wang"], "title": "ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning", "categories": ["cs.AI", "cs.AR", "cs.PL"], "comment": null, "summary": "Large Language Models (LLMs) show significant potential for automating\nRegister-Transfer Level (RTL) code generation. However, current approaches face\na critical challenge: they can not simultaneously optimize for functional\ncorrectness and hardware quality (Power, Performance, Area - PPA). Methods\nbased on supervised fine-tuning often generate functionally correct but\nPPA-suboptimal code, lacking mechanisms to learn optimization principles. In\ncontrast, post-processing techniques that attempt to improve PPA metrics after\ngeneration are often inefficient because they operate externally without\nupdating the LLM's parameters, thus failing to enhance the model's intrinsic\ndesign capabilities.\n  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven\nreinforcement learning framework to train LLMs to generate RTL code that\nachieves both functional correctness and optimized PPA metrics. ChipSeek-R1\nemploys a hierarchical reward system, which incorporates direct feedback on\nsyntax, functional correctness (from simulators) and PPA metrics (from\nsynthesis tools) during reinforcement learning. This enables the model to learn\ncomplex hardware design trade-offs via trial-and-error, generating RTL code\nthat is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on\nstandard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results\nin functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1\ngenerated 27 RTL designs surpassing the PPA metrics of the original\nhuman-written code. Our findings demonstrate the effectiveness of integrating\ntoolchain feedback into LLM training and highlight the potential for\nreinforcement learning to enable automated generation of human-surpassing RTL\ncode. We open-source our code in anonymous github.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6ChipSeek-R1\uff0c\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u540c\u65f6\u6ee1\u8db3\u529f\u80fd\u6b63\u786e\u6027\u548c\u786c\u4ef6\u8d28\u91cf\uff08PPA\uff09\u4f18\u5316\u7684RTL\u4ee3\u7801\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u529f\u80fd\u6b63\u786e\u6027\u548c\u786c\u4ef6\u8d28\u91cf\uff08PPA\uff09\uff0c\u4e9f\u9700\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5956\u52b1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u6cd5\u3001\u529f\u80fd\u6b63\u786e\u6027\u548cPPA\u6307\u6807\u7684\u53cd\u9988\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u4e8627\u4e2aPPA\u4f18\u4e8e\u4eba\u5de5\u7f16\u5199\u7684RTL\u8bbe\u8ba1\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u5de5\u5177\u94fe\u53cd\u9988\u53ef\u6709\u6548\u751f\u6210\u8d85\u8d8a\u4eba\u5de5\u7684RTL\u4ee3\u7801\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.03952", "pdf": "https://arxiv.org/pdf/2507.03952", "abs": "https://arxiv.org/abs/2507.03952", "authors": ["Somayeh Sobati-M"], "title": "FedFog: Resource-Aware Federated Learning in Edge and Fog Networks", "categories": ["cs.DC"], "comment": null, "summary": "As edge and fog computing become central to modern distributed systems,\nthere's growing interest in combining serverless architectures with\nprivacy-preserving machine learning techniques like federated learning (FL).\nHowever, current simulation tools fail to capture this integration effectively.\nIn this paper, we introduce FedFog, a simulation framework that extends the\nFogFaaS environment to support FL-aware serverless execution across edge-fog\ninfrastructures. FedFog incorporates an adaptive FL scheduler,\nprivacy-respecting data flow, and resource-aware orchestration to emulate\nrealistic, dynamic conditions in IoT-driven scenarios. Through extensive\nsimulations on benchmark datasets, we demonstrate that FedFog accelerates model\nconvergence, reduces latency, and improves energy efficiency compared to\nconventional FL or FaaS setups-making it a valuable tool for researchers\nexploring scalable, intelligent edge systems.", "AI": {"tldr": "FedFog\u662f\u4e00\u4e2a\u6a21\u62df\u6846\u67b6\uff0c\u5c06\u670d\u52a1\u5668\u65e0\u4e0e\u8054\u5408\u5b66\u4e60\u7ed3\u5408\uff0c\u4f18\u5316\u8fb9\u7f18-\u96fe\u8ba1\u7b97\u73af\u5883\u4e0b\u7684\u6a21\u578b\u6536\u655b\u3001\u5ef6\u8fdf\u548c\u80fd\u6548\u3002", "motivation": "\u8fb9\u7f18\u548c\u96fe\u8ba1\u7b97\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u672a\u80fd\u6709\u6548\u6a21\u62df\u670d\u52a1\u5668\u65e0\u4e0e\u8054\u5408\u5b66\u4e60\u7684\u96c6\u6210\u3002", "method": "\u6269\u5c55FogFaaS\u73af\u5883\uff0c\u52a0\u5165\u81ea\u9002\u5e94FL\u8c03\u5ea6\u5668\u3001\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u6d41\u548c\u8d44\u6e90\u611f\u77e5\u7f16\u6392\uff0c\u6a21\u62df\u52a8\u6001\u7269\u8054\u7f51\u573a\u666f\u3002", "result": "FedFog\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u52a0\u901f\u6a21\u578b\u6536\u655b\u3001\u964d\u4f4e\u5ef6\u8fdf\u3001\u63d0\u9ad8\u80fd\u6548\u3002", "conclusion": "FedFog\u662f\u7814\u7a76\u53ef\u6269\u5c55\u667a\u80fd\u8fb9\u7f18\u7cfb\u7edf\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.04772", "pdf": "https://arxiv.org/pdf/2507.04772", "abs": "https://arxiv.org/abs/2507.04772", "authors": ["Seock-Hwan Noh", "Sungju Kim", "Seohyun Kim", "Daehoon Kim", "Jaeha Kung", "Yeseong Kim"], "title": "Jack Unit: An Area- and Energy-Efficient Multiply-Accumulate (MAC) Unit Supporting Diverse Data Formats", "categories": ["cs.AR"], "comment": "Accepted for publication at the 30th ACM/IEEE International Symposium\n  on Low Power Electronics and Design (ISLPED 2025)", "summary": "In this work, we introduce an area- and energy-efficient multiply-accumulate\n(MAC) unit, named Jack unit, that is a jack-of-all-trades, supporting various\ndata formats such as integer (INT), floating point (FP), and microscaling data\nformat (MX). It provides bit-level flexibility and enhances hardware efficiency\nby i) replacing the carry-save multiplier (CSM) in the FP multiplier with a\nprecision-scalable CSM, ii) performing the adjustment of significands based on\nthe exponent differences within the CSM, and iii) utilizing 2D sub-word\nparallelism. To assess effectiveness, we implemented the layout of the Jack\nunit and three baseline MAC units. Additionally, we designed an AI accelerator\nequipped with our Jack units to compare with a state-of-the-art AI accelerator\nsupporting various data formats. The proposed MAC unit occupies 1.17~2.01x\nsmaller area and consumes 1.05~1.84x lower power compared to the baseline MAC\nunits. On five AI benchmarks, the accelerator designed with our Jack units\nimproves energy efficiency by 1.32~5.41x over the baseline across various data\nformats.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJack\u5355\u5143\u7684\u9ad8\u6548\u4e58\u52a0\u5355\u5143\uff08MAC\uff09\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u683c\u5f0f\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u9762\u79ef\u548c\u80fd\u6548\u3002\u4e0e\u57fa\u51c6MAC\u5355\u5143\u76f8\u6bd4\uff0c\u5b83\u7684\u9762\u79ef\u66f4\u5c0f\u3001\u529f\u8017\u66f4\u4f4e\uff0c\u4e14\u5728AI\u52a0\u901f\u5668\u4e0a\u7684\u5e94\u7528\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709MAC\u5355\u5143\u5728\u5904\u7406\u591a\u79cd\u6570\u636e\u683c\u5f0f\u65f6\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86Jack\u5355\u5143\uff0c\u65e8\u5728\u63d0\u5347\u786c\u4ef6\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "method": "\u901a\u8fc7\u91c7\u7528\u7cbe\u5ea6\u53ef\u8c03\u7684\u8fdb\u4f4d\u4fdd\u5b58\u4e58\u6cd5\u5668\uff08CSM\uff09\u3001\u5728CSM\u5185\u8c03\u6574\u6709\u6548\u4f4d\u3001\u4ee5\u53ca\u5229\u75282D\u5b50\u5b57\u5e76\u884c\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684MAC\u8bbe\u8ba1\u3002", "result": "Jack\u5355\u5143\u7684\u9762\u79ef\u6bd4\u57fa\u51c6MAC\u5355\u5143\u5c0f1.17~2.01\u500d\uff0c\u529f\u8017\u4f4e1.05~1.84\u500d\uff1b\u5728AI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u6548\u6bd4\u57fa\u51c6\u9ad81.32~5.41\u500d\u3002", "conclusion": "Jack\u5355\u5143\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u8bbe\u8ba1\uff0c\u4e3aAI\u52a0\u901f\u5668\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u80fd\u6548\u63d0\u5347\u3002"}}
{"id": "2507.03286", "pdf": "https://arxiv.org/pdf/2507.03286", "abs": "https://arxiv.org/abs/2507.03286", "authors": ["Yang Hong", "Jie-Yi Feng", "Yi-Chun Yao", "I-Hsuan Cho", "Yu-Ting Lin", "Ying-Yu Chen"], "title": "Gaze and Glow: Exploring Editing Processes on Social Media through Interactive Exhibition", "categories": ["cs.HC", "cs.MM", "J.5"], "comment": "6 pages, 6 figures, to be published in DIS 2025 (Provocations and\n  Works in Progress)", "summary": "We present Gaze and Glow, an interactive installation that reveals the\noften-invisible efforts of social media editing. Through narrative personas,\nexperimental videos, and sensor-based interactions, the installation explores\nhow audience attention shapes users' editing practices and emotional\nexperiences. Deployed in a two-month public exhibition, Gaze and Glow engaged\nviewers and elicited responses. Reflexive thematic analysis of audience\nfeedback highlights how making editing visible prompts new reflections on\nauthenticity, agency, and performativity. We discuss implications for designing\ninteractive systems that support selective memory, user-controlled visibility,\nand critical engagement with everyday digital self-presentation.", "AI": {"tldr": "Gaze and Glow\u662f\u4e00\u4e2a\u63ed\u793a\u793e\u4ea4\u5a92\u4f53\u7f16\u8f91\u80cc\u540e\u52aa\u529b\u7684\u4e92\u52a8\u88c5\u7f6e\uff0c\u901a\u8fc7\u53d9\u4e8b\u89d2\u8272\u548c\u4f20\u611f\u5668\u4e92\u52a8\uff0c\u63a2\u8ba8\u89c2\u4f17\u6ce8\u610f\u529b\u5982\u4f55\u5f71\u54cd\u7f16\u8f91\u5b9e\u8df5\u548c\u60c5\u611f\u4f53\u9a8c\u3002", "motivation": "\u7814\u7a76\u793e\u4ea4\u5a92\u4f53\u7f16\u8f91\u4e2d\u5e38\u88ab\u5ffd\u89c6\u7684\u52aa\u529b\uff0c\u4ee5\u53ca\u89c2\u4f17\u6ce8\u610f\u529b\u5bf9\u7f16\u8f91\u884c\u4e3a\u548c\u60c5\u611f\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u53d9\u4e8b\u89d2\u8272\u3001\u5b9e\u9a8c\u89c6\u9891\u548c\u4f20\u611f\u5668\u4e92\u52a8\uff0c\u7ed3\u5408\u516c\u4f17\u5c55\u89c8\u548c\u89c2\u4f17\u53cd\u9988\u7684\u53cd\u601d\u6027\u4e3b\u9898\u5206\u6790\u3002", "result": "\u88c5\u7f6e\u5f15\u53d1\u4e86\u89c2\u4f17\u5bf9\u771f\u5b9e\u6027\u3001\u80fd\u52a8\u6027\u548c\u8868\u6f14\u6027\u7684\u53cd\u601d\uff0c\u5e76\u63d0\u51fa\u4e86\u652f\u6301\u9009\u62e9\u6027\u8bb0\u5fc6\u548c\u7528\u6237\u63a7\u5236\u53ef\u89c1\u6027\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bbe\u8ba1\u652f\u6301\u6279\u5224\u6027\u6570\u5b57\u81ea\u6211\u5448\u73b0\u7684\u4e92\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2507.04185", "pdf": "https://arxiv.org/pdf/2507.04185", "abs": "https://arxiv.org/abs/2507.04185", "authors": ["Aniket Kesari", "Travis Breaux", "Tom Norton", "Sarah Santos", "Anmol Singhal"], "title": "From Legal Text to Tech Specs: Generative AI's Interpretation of Consent in Privacy Law", "categories": ["cs.SE"], "comment": "10 pages, 1 figure, 20th International Conference on Artificial\n  Intelligence and Law (ICAIL 2025)", "summary": "Privacy law and regulation have turned to \"consent\" as the legitimate basis\nfor collecting and processing individuals' data. As governments have rushed to\nenshrine consent requirements in their privacy laws, such as the California\nConsumer Privacy Act (CCPA), significant challenges remain in understanding how\nthese legal mandates are operationalized in software. The opaque nature of\nsoftware development processes further complicates this translation. To address\nthis, we explore the use of Large Language Models (LLMs) in requirements\nengineering to bridge the gap between legal requirements and technical\nimplementation. This study employs a three-step pipeline that involves using an\nLLM to classify software use cases for compliance, generating LLM modifications\nfor non-compliant cases, and manually validating these changes against legal\nstandards. Our preliminary findings highlight the potential of LLMs in\nautomating compliance tasks, while also revealing limitations in their\nreasoning capabilities. By benchmarking LLMs against real-world use cases, this\nresearch provides insights into leveraging AI-driven solutions to enhance legal\ncompliance of software.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9700\u6c42\u5de5\u7a0b\u4e2d\u5f25\u5408\u6cd5\u5f8b\u8981\u6c42\u4e0e\u6280\u672f\u5b9e\u73b0\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u968f\u7740\u9690\u79c1\u6cd5\uff08\u5982CCPA\uff09\u5c06\u201c\u540c\u610f\u201d\u4f5c\u4e3a\u5408\u6cd5\u4f9d\u636e\uff0c\u4f46\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u5bfc\u81f4\u6cd5\u5f8b\u8981\u6c42\u96be\u4ee5\u6280\u672f\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u6d41\u7a0b\uff1aLLM\u5206\u7c7b\u8f6f\u4ef6\u7528\u4f8b\u5408\u89c4\u6027\uff0c\u751f\u6210\u975e\u5408\u89c4\u7528\u4f8b\u7684\u4fee\u6539\u5efa\u8bae\uff0c\u5e76\u4eba\u5de5\u9a8c\u8bc1\u4fee\u6539\u662f\u5426\u7b26\u5408\u6cd5\u5f8b\u6807\u51c6\u3002", "result": "\u521d\u6b65\u53d1\u73b0LLM\u5728\u81ea\u52a8\u5316\u5408\u89c4\u4efb\u52a1\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u8868\u660eAI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u80fd\u63d0\u5347\u8f6f\u4ef6\u6cd5\u5f8b\u5408\u89c4\u6027\uff0c\u4f46\u5176\u5c40\u9650\u6027\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.04081", "pdf": "https://arxiv.org/pdf/2507.04081", "abs": "https://arxiv.org/abs/2507.04081", "authors": ["Xudong Wang", "Lei Feng", "Jiacheng Wang", "Hongyang Du", "Changyuan Zhao", "Wenjing Li", "Zehui Xiong", "Dusit Niyato", "Ping Zhang"], "title": "Graph Diffusion-Based AeBS Deployment and Resource Allocation for RSMA-Enabled URLLC Low-Altitude Economy Networks", "categories": ["cs.NI"], "comment": "15 pages, 9 figures", "summary": "As a key component of low-altitude economic networks, aerial base stations\n(AeBSs) provide flexible and reliable wireless coverage to support 6G\nultra-reliable and low-latency communication (URLLC) services. However, limited\nspectrum resources and severe co-channel interference pose significant\nchallenges to the deployment and resource allocation of AeBSs. To address these\nlimitations, this paper proposes a novel rate-splitting multiple access\n(RSMA)-enabled transmission design to flexibly manage interference and\neffectively enhance URLLC services in spectrum-constrained multi-AeBS networks.\nOn this basis, we formulate a joint optimization problem involving AeBS\ndeployment, user association, and resource allocation to maximize the\nachievable sum rate and coverage of the total system. Given the NP-hard nature\nof the problem and the highly dynamic environment, we propose a novel\nalternating optimization framework based on the generative graph diffusion\nmodels. Specifically, we model AeBSs and ground users as graph nodes, then we\nemploy a discrete graph generation process solved via denoising diffusion is\nemployed to explore the combinatorial space of deployment and association\nstrategies. Moreover, the algorithm adopts the successive convex approximation\n(SCA) method to optimize AeBS beamforming and RSMA rate allocation under finite\nblocklength constraints. Extensive simulations demonstrate that the proposed\nalgorithm outperforms existing methods in terms of convergence speed, sum rate,\nand coverage, while also exhibiting robust performance under varying network\ndensities and interference levels.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901f\u7387\u5206\u5272\u591a\u5740\u63a5\u5165\uff08RSMA\uff09\u7684\u4f20\u8f93\u8bbe\u8ba1\uff0c\u901a\u8fc7\u751f\u6210\u56fe\u6269\u6563\u6a21\u578b\u548c\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u7a7a\u4e2d\u57fa\u7ad9\uff08AeBS\uff09\u7f51\u7edc\u4e2d\u5e72\u6270\u7ba1\u7406\u548c\u8d44\u6e90\u5206\u914d\u7684\u6311\u6218\u3002", "motivation": "\u5728\u9891\u8c31\u8d44\u6e90\u6709\u9650\u548c\u540c\u9891\u5e72\u6270\u4e25\u91cd\u7684\u591aAeBS\u7f51\u7edc\u4e2d\uff0c\u5982\u4f55\u9ad8\u6548\u90e8\u7f72\u57fa\u7ad9\u3001\u5206\u914d\u8d44\u6e90\u548c\u589e\u5f3a\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff08URLLC\uff09\u670d\u52a1\u3002", "method": "\u91c7\u7528\u751f\u6210\u56fe\u6269\u6563\u6a21\u578b\u8fdb\u884cAeBS\u90e8\u7f72\u548c\u7528\u6237\u5173\u8054\u7684\u7b56\u7565\u63a2\u7d22\uff0c\u7ed3\u5408\u9010\u6b21\u51f8\u903c\u8fd1\uff08SCA\uff09\u65b9\u6cd5\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u548cRSMA\u901f\u7387\u5206\u914d\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0c\u7b97\u6cd5\u5728\u6536\u655b\u901f\u5ea6\u3001\u603b\u901f\u7387\u548c\u8986\u76d6\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u4e0d\u540c\u7f51\u7edc\u5bc6\u5ea6\u548c\u5e72\u6270\u6c34\u5e73\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u63d0\u51fa\u7684RSMA-enabled\u4f20\u8f93\u8bbe\u8ba1\u548c\u4f18\u5316\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u591aAeBS\u7f51\u7edc\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.02916", "pdf": "https://arxiv.org/pdf/2507.02916", "abs": "https://arxiv.org/abs/2507.02916", "authors": ["Jiong Yang", "Yong Kiam Tan", "Mate Soos", "Magnus O. Myreen", "Kuldeep S. Meel"], "title": "Efficient Certified Reasoning for Binarized Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": "18 pages, 4 figures, to be published in SAT25", "summary": "Neural networks have emerged as essential components in safety-critical\napplications -- these use cases demand complex, yet trustworthy computations.\nBinarized Neural Networks (BNNs) are a type of neural network where each neuron\nis constrained to a Boolean value; they are particularly well-suited for\nsafety-critical tasks because they retain much of the computational capacities\nof full-scale (floating-point or quantized) deep neural networks, but remain\ncompatible with satisfiability solvers for qualitative verification and with\nmodel counters for quantitative reasoning. However, existing methods for BNN\nanalysis suffer from either limited scalability or susceptibility to soundness\nerrors, which hinders their applicability in real-world scenarios.\n  In this work, we present a scalable and trustworthy approach for both\nqualitative and quantitative verification of BNNs. Our approach introduces a\nnative representation of BNN constraints in a custom-designed solver for\nqualitative reasoning, and in an approximate model counter for quantitative\nreasoning. We further develop specialized proof generation and checking\npipelines with native support for BNN constraint reasoning, ensuring\ntrustworthiness for all of our verification results. Empirical evaluations on a\nBNN robustness verification benchmark suite demonstrate that our certified\nsolving approach achieves a $9\\times$ speedup over prior certified CNF and\nPB-based approaches, and our certified counting approach achieves a $218\\times$\nspeedup over the existing CNF-based baseline. In terms of coverage, our\npipeline produces fully certified results for $99\\%$ and $86\\%$ of the\nqualitative and quantitative reasoning queries on BNNs, respectively. This is\nin sharp contrast to the best existing baselines which can fully certify only\n$62\\%$ and $4\\%$ of the queries, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u4fe1\u7684BNN\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u652f\u6301\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u8986\u76d6\u7387\u3002", "motivation": "BNN\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u9700\u6c42\u9ad8\uff0c\u4f46\u73b0\u6709\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u6216\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5b9a\u5236\u6c42\u89e3\u5668\u548c\u8fd1\u4f3c\u6a21\u578b\u8ba1\u6570\u5668\uff0c\u7ed3\u5408\u4e13\u7528\u8bc1\u660e\u751f\u6210\u548c\u68c0\u67e5\u6d41\u7a0b\u3002", "result": "\u5b9a\u6027\u5206\u6790\u901f\u5ea6\u5feb9\u500d\uff0c\u5b9a\u91cf\u5206\u6790\u901f\u5ea6\u5feb218\u500d\uff1b\u8986\u76d6\u7387\u8fbe\u523099%\u548c86%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86BNN\u9a8c\u8bc1\u7684\u6548\u7387\u4e0e\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.03147", "pdf": "https://arxiv.org/pdf/2507.03147", "abs": "https://arxiv.org/abs/2507.03147", "authors": ["Thanh Hoang-Minh"], "title": "DeepGesture: A conversational gesture synthesis system based on emotions and semantics", "categories": ["cs.HC", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "Video Demo: https://www.youtube.com/watch?v=eZghfNGmZn8", "summary": "Along with the explosion of large language models, improvements in speech\nsynthesis, advancements in hardware, and the evolution of computer graphics,\nthe current bottleneck in creating digital humans lies in generating character\nmovements that correspond naturally to text or speech inputs.\n  In this work, we present DeepGesture, a diffusion-based gesture synthesis\nframework for generating expressive co-speech gestures conditioned on\nmultimodal signals-text, speech, emotion, and seed motion. Built upon the\nDiffuseStyleGesture model, DeepGesture introduces novel architectural\nenhancements that improve semantic alignment and emotional expressiveness in\ngenerated gestures. Specifically, we integrate fast text transcriptions as\nsemantic conditioning and implement emotion-guided classifier-free diffusion to\nsupport controllable gesture generation across affective states. A lightweight\nTransformer backbone combines full self-attention and cross-local attention for\neffective feature fusion of heterogeneous modalities. To visualize results, we\nimplement a full rendering pipeline in Unity based on BVH output from the\nmodel. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces\ngestures with improved human-likeness and contextual appropriateness,\noutperforming baselines on Mean Opinion Score and Frechet Gesture Distance\nmetrics. Our system supports interpolation between emotional states and\ndemonstrates generalization to out-of-distribution speech, including synthetic\nvoices-marking a step forward toward fully multimodal, emotionally aware\ndigital humans.", "AI": {"tldr": "DeepGesture\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u624b\u52bf\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u3001\u8bed\u97f3\u3001\u60c5\u611f\u548c\u79cd\u5b50\u52a8\u4f5c\u7b49\u591a\u6a21\u6001\u4fe1\u53f7\u751f\u6210\u81ea\u7136\u7684\u624b\u52bf\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u60c5\u611f\u8868\u8fbe\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u4eba\u751f\u6210\u7684\u74f6\u9888\u5728\u4e8e\u5982\u4f55\u6839\u636e\u6587\u672c\u6216\u8bed\u97f3\u8f93\u5165\u81ea\u7136\u5730\u751f\u6210\u89d2\u8272\u52a8\u4f5c\uff0cDeepGesture\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8eDiffuseStyleGesture\u6a21\u578b\uff0c\u5f15\u5165\u5feb\u901f\u6587\u672c\u8f6c\u5f55\u4f5c\u4e3a\u8bed\u4e49\u6761\u4ef6\uff0c\u5b9e\u73b0\u60c5\u611f\u5f15\u5bfc\u7684\u65e0\u5206\u7c7b\u5668\u6269\u6563\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7Transformer\u67b6\u6784\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "\u5728ZeroEGGS\u6570\u636e\u96c6\u4e0a\uff0cDeepGesture\u751f\u6210\u7684\u624b\u52bf\u5728\u4eba\u7c7b\u76f8\u4f3c\u5ea6\u548c\u4e0a\u4e0b\u6587\u9002\u5f53\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u652f\u6301\u60c5\u611f\u72b6\u6001\u63d2\u503c\u548c\u6cdb\u5316\u5230\u975e\u5206\u5e03\u8bed\u97f3\u3002", "conclusion": "DeepGesture\u671d\u7740\u591a\u6a21\u6001\u3001\u60c5\u611f\u611f\u77e5\u7684\u6570\u5b57\u4eba\u7c7b\u8fc8\u8fdb\u4e86\u4e00\u6b65\u3002"}}
{"id": "2507.03694", "pdf": "https://arxiv.org/pdf/2507.03694", "abs": "https://arxiv.org/abs/2507.03694", "authors": ["Jovonni L. PHarr"], "title": "Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital Wills", "categories": ["cs.CR", "cs.CE", "cs.ET"], "comment": null, "summary": "This work presents a novel decentralized protocol for digital estate planning\nthat integrates advances distributed computing, and cryptography. The original\nproof-of-concept was constructed using purely solidity contracts. Since then,\nwe have enhanced the implementation into a layer-1 protocol that uses modern\ninterchain communication to connect several heterogeneous chain types. A key\ncontribution of this research is the implementation of several modern\ncryptographic primitives to support various forms of claims for information\nvalidation. These primitives introduce an unmatched level of privacy to the\nprocess of digital inheritance. We also demonstrate on a set of heterogeneous\nsmart contracts, following the same spec, on each chain to serve as entry\npoints, gateways, or bridge contracts that are invoked via a path from the will\nmodule on our protocol, to the contract. This ensures a fair and secure\ndistribution of digital assets in accordance with the wishes of the decedent\nwithout the requirement of moving their funds. This research further extends\nits innovations with a user interaction model, featuring a check-in system and\naccount abstraction process, which enhances flexibility and user-friendliness\nwithout compromising on security. By developing a dedicated permissionless\nblockchain that is secured by a network of validators, and interchain relayers,\nthe proposed protocol signifies a transformation in the digital estate planning\nindustry and illustrates the potential of blockchain technology in\nrevolutionizing traditional legal and personal spheres. Implementing a\ncryptoeconomic network at the core of inheritance planning allows for unique\nincentive compatible economic mechanisms to be constructed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53bb\u4e2d\u5fc3\u5316\u6570\u5b57\u9057\u4ea7\u89c4\u5212\u534f\u8bae\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u8ba1\u7b97\u548c\u5bc6\u7801\u5b66\uff0c\u901a\u8fc7\u73b0\u4ee3\u94fe\u95f4\u901a\u4fe1\u8fde\u63a5\u5f02\u6784\u94fe\uff0c\u5b9e\u73b0\u5b89\u5168\u3001\u79c1\u5bc6\u7684\u6570\u5b57\u8d44\u4ea7\u5206\u914d\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6570\u5b57\u9057\u4ea7\u89c4\u5212\u4e2d\u7684\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff0c\u5229\u7528\u533a\u5757\u94fe\u6280\u672f\u9769\u65b0\u6cd5\u5f8b\u548c\u4e2a\u4eba\u9886\u57df\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u5c42\u534f\u8bae\uff0c\u4f7f\u7528\u73b0\u4ee3\u5bc6\u7801\u5b66\u539f\u8bed\u548c\u94fe\u95f4\u901a\u4fe1\uff0c\u6784\u5efa\u5f02\u6784\u667a\u80fd\u5408\u7ea6\u4f5c\u4e3a\u5165\u53e3\u70b9\uff0c\u5e76\u5f15\u5165\u7528\u6237\u4ea4\u4e92\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u9700\u8f6c\u79fb\u8d44\u4ea7\u7684\u516c\u5e73\u3001\u5b89\u5168\u6570\u5b57\u8d44\u4ea7\u5206\u914d\uff0c\u63d0\u5347\u4e86\u9690\u79c1\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\u3002", "conclusion": "\u8be5\u534f\u8bae\u901a\u8fc7\u5bc6\u7801\u7ecf\u6d4e\u7f51\u7edc\u9769\u65b0\u6570\u5b57\u9057\u4ea7\u89c4\u5212\uff0c\u5c55\u793a\u4e86\u533a\u5757\u94fe\u6280\u672f\u5728\u4f20\u7edf\u9886\u57df\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.03973", "pdf": "https://arxiv.org/pdf/2507.03973", "abs": "https://arxiv.org/abs/2507.03973", "authors": ["Muhang Lan", "Song Xiao", "Wenyi Zhang"], "title": "One-Bit Model Aggregation for Differentially Private and Byzantine-Robust Personalized Federated Learning", "categories": ["cs.DC"], "comment": null, "summary": "As the scale of federated learning (FL) systems expands, their inherent\nperformance limitations like communication overhead, Byzantine vulnerability,\nand privacy leakage have become increasingly critical. This paper considers a\npersonalized FL framework based on model regularization, and proposes a model\naggregation algorithm named PRoBit+ to concurrently overcome these limitations.\nPRoBit+ employs one-bit stochastic quantization and maximum likelihood\nestimation for parameter aggregation, and dynamically adjusts the step size of\nparameter updates, improving training stability of deep neural networks under\nlow communication overhead and heterogeneous data distributions. PRoBit+'s\nstatistical analysis is then conducted and its Byzantine robustness is proved.\nThe $(\\epsilon,0)$-differential privacy and a convergence upper bound of the\nPRoBit+ based FL are also theoretically established in heterogeneous contexts.\nThe analysis illustrates the trade-off among transmission accuracy, security\nguarantees, and convergence rates, and also indicates that the performance\ndegradation caused by transmission errors and privacy protection can be\nprogressively eliminated at a rate of $\\mathcal{O}(1/M)$ as the number of\nuploading clients $M$ increases. Comprehensive numerical experiments are\nconducted to assess PRoBit+ in comparison to benchmark methods across different\nByzantine attacks and varying proportions of malicious clients. The\nexperimental results demonstrate that PRoBit+ exhibits improved Byzantine\nrobustness over existing bit-based transmission schemes, minimal performance\ndegradation related to privacy protection, and nearly identical performance to\nfull-precision FedAvg in a secure environment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPRoBit+\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u6b63\u5219\u5316\u548c\u52a8\u6001\u8c03\u6574\u53c2\u6570\u66f4\u65b0\u6b65\u957f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u901a\u4fe1\u5f00\u9500\u3001\u62dc\u5360\u5ead\u653b\u51fb\u548c\u9690\u79c1\u6cc4\u9732\u7b49\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u968f\u7740\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u89c4\u6a21\u7684\u6269\u5927\uff0c\u5176\u56fa\u6709\u6027\u80fd\u9650\u5236\uff08\u5982\u901a\u4fe1\u5f00\u9500\u3001\u62dc\u5360\u5ead\u653b\u51fb\u548c\u9690\u79c1\u6cc4\u9732\uff09\u65e5\u76ca\u7a81\u51fa\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u63d0\u51faPRoBit+\u7b97\u6cd5\u6765\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "PRoBit+\u91c7\u7528\u4e00\u4f4d\u968f\u673a\u91cf\u5316\u548c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u8fdb\u884c\u53c2\u6570\u805a\u5408\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u53c2\u6570\u66f4\u65b0\u6b65\u957f\uff0c\u4ee5\u63d0\u9ad8\u5728\u4f4e\u901a\u4fe1\u5f00\u9500\u548c\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86PRoBit+\u7684\u62dc\u5360\u5ead\u9c81\u68d2\u6027\u3001(\u03b5,0)-\u5dee\u5206\u9690\u79c1\u6027\u4ee5\u53ca\u6536\u655b\u4e0a\u754c\u3002\u5b9e\u9a8c\u8868\u660ePRoBit+\u5728\u62dc\u5360\u5ead\u653b\u51fb\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9690\u79c1\u4fdd\u62a4\u5f15\u8d77\u7684\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\u3002", "conclusion": "PRoBit+\u5728\u901a\u4fe1\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u5b9e\u73b0\u4e86\u5e73\u8861\uff0c\u5e76\u901a\u8fc7M\u7684\u589e\u52a0\u9010\u6b65\u6d88\u9664\u4f20\u8f93\u8bef\u5dee\u548c\u9690\u79c1\u4fdd\u62a4\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\u3002"}}
{"id": "2507.05012", "pdf": "https://arxiv.org/pdf/2507.05012", "abs": "https://arxiv.org/abs/2507.05012", "authors": ["Samuel Riedel", "Yichao Zhang", "Marco Bertuletti", "Luca Benini"], "title": "Optimizing Scalable Multi-Cluster Architectures for Next-Generation Wireless Sensing and Communication", "categories": ["cs.AR"], "comment": "6 pages, 8 figures, accepted at IWASI 2025", "summary": "Next-generation wireless technologies (for immersive-massive communication,\njoint communication and sensing) demand highly parallel architectures for\nmassive data processing. A common architectural template scales up by grouping\ntens to hundreds of cores into shared-memory clusters, which are then scaled\nout as multi-cluster manycore systems. This hierarchical design, used in GPUs\nand accelerators, requires a balancing act between fewer large clusters and\nmore smaller clusters, affecting design complexity, synchronization,\ncommunication efficiency, and programmability. While all multi-cluster\narchitectures must balance these trade-offs, there is limited insight into\noptimal cluster sizes. This paper analyzes various cluster configurations,\nfocusing on synchronization, data movement overhead, and programmability for\ntypical wireless sensing and communication workloads. We extend the open-source\nshared-memory cluster MemPool into a multi-cluster architecture and propose a\nnovel double-buffering barrier that decouples processor and DMA. Our results\nshow a single 256-core cluster can be twice as fast as 16 16-core clusters for\nmemory-bound kernels and up to 24% faster for compute-bound kernels due to\nreduced synchronization and communication overheads.", "AI": {"tldr": "\u7814\u7a76\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u6280\u672f\u4e2d\u7684\u591a\u96c6\u7fa4\u67b6\u6784\u8bbe\u8ba1\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u96c6\u7fa4\u89c4\u6a21\u5bf9\u540c\u6b65\u3001\u6570\u636e\u79fb\u52a8\u548c\u7f16\u7a0b\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u540c\u6b65\u673a\u5236\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u6280\u672f\u9700\u8981\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u5e76\u884c\u67b6\u6784\uff0c\u4f46\u5f53\u524d\u591a\u96c6\u7fa4\u67b6\u6784\u7f3a\u4e4f\u5bf9\u96c6\u7fa4\u89c4\u6a21\u7684\u4f18\u5316\u6307\u5bfc\u3002", "method": "\u6269\u5c55MemPool\u4e3a\u591a\u96c6\u7fa4\u67b6\u6784\uff0c\u5e76\u8bbe\u8ba1\u53cc\u7f13\u51b2\u5c4f\u969c\u4ee5\u89e3\u8026\u5904\u7406\u5668\u548cDMA\uff0c\u6bd4\u8f83\u4e0d\u540c\u96c6\u7fa4\u914d\u7f6e\u7684\u6027\u80fd\u3002", "result": "256\u6838\u5355\u96c6\u7fa4\u6bd416\u4e2a16\u6838\u96c6\u7fa4\u5feb2\u500d\uff08\u5185\u5b58\u53d7\u9650\uff09\u548c24%\uff08\u8ba1\u7b97\u53d7\u9650\uff09\uff0c\u56e0\u51cf\u5c11\u4e86\u540c\u6b65\u548c\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u5927\u96c6\u7fa4\u5728\u51cf\u5c11\u540c\u6b65\u548c\u901a\u4fe1\u5f00\u9500\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u65e0\u7ebf\u901a\u4fe1\u548c\u4f20\u611f\u8d1f\u8f7d\u3002"}}
{"id": "2507.03434", "pdf": "https://arxiv.org/pdf/2507.03434", "abs": "https://arxiv.org/abs/2507.03434", "authors": ["Haochen Han", "Alex Jinpeng Wang", "Peijun Ye", "Fangming Liu"], "title": "Unlearning the Noisy Correspondence Makes CLIP More Robust", "categories": ["cs.CV", "cs.MM"], "comment": "ICCV 2025", "summary": "The data appetite for Vision-Language Models (VLMs) has continuously scaled\nup from the early millions to billions today, which faces an untenable\ntrade-off with data quality and inevitably introduces Noisy Correspondence (NC)\nsamples. Undoubtedly, such semantically unrelated data significantly impairs\nthe performance of VLMs. Previous efforts mainly address this challenge by\nestimating refined alignment for more precise guidance. However, such\nresource-intensive pipelines that train VLMs from scratch struggle to meet\nrealistic data demands. In this paper, we present a brand new perspective that\nseeks to directly eliminate the harmful effects of NC in pre-trained VLMs.\nSpecifically, we propose NCU, a Noisy Correspondence Unlearning fine-tuning\nframework that efficiently enhances VLMs' robustness by forgetting learned\nnoisy knowledge. The key to NCU is learning the hardest negative information,\nwhich can provide explicit unlearning direction for both false positives and\nfalse negatives. Such twin goals unlearning process can be formalized into one\nunified optimal transport objective for fast fine-tuning. We validate our\napproach with the prevailing CLIP model over various downstream tasks.\nRemarkably, NCU surpasses the robust pre-trained method on zero-shot transfer\nwhile with lower computational overhead. The code will be released upon\nacceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86NCU\u6846\u67b6\uff0c\u901a\u8fc7\u9057\u5fd8\u5b66\u4e60\u5230\u7684\u566a\u58f0\u77e5\u8bc6\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u566a\u58f0\u5bf9\u5e94\u6837\u672c\u7684\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u89c4\u6a21\u4e0d\u65ad\u589e\u5927\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u4e0b\u964d\u5bfc\u81f4\u566a\u58f0\u5bf9\u5e94\u6837\u672c\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u3002\u672c\u6587\u63d0\u51fa\u76f4\u63a5\u6d88\u9664\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u566a\u58f0\u5f71\u54cd\u7684\u65b0\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u4e86NCU\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u76ee\u6807\u5b66\u4e60\u6700\u96be\u8d1f\u6837\u672c\u4fe1\u606f\uff0c\u540c\u65f6\u5bf9\u5047\u9633\u6027\u548c\u5047\u9634\u6027\u8fdb\u884c\u7edf\u4e00\u7684\u9057\u5fd8\u5b66\u4e60\u7cbe\u7ec6\u8c03\u4f18\u3002", "result": "\u5728CLIP\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cNCU\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u9c81\u68d2\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u66f4\u4f4e\u3002", "conclusion": "NCU\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u5bf9\u5e94\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.04354", "pdf": "https://arxiv.org/pdf/2507.04354", "abs": "https://arxiv.org/abs/2507.04354", "authors": ["Yanzhou Mu", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhixiang Cao", "Peiran Yang", "Kexin Zhao", "An Guo", "Zhenyu Chen"], "title": "Improving Deep Learning Framework Testing with Model-Level Metamorphic Testing", "categories": ["cs.SE"], "comment": "23 pages, 5 figures", "summary": "Deep learning (DL) frameworks are essential to DL-based software systems, and\nframework bugs may lead to substantial disasters, thus requiring effective\ntesting. Researchers adopt DL models or single interfaces as test inputs and\nanalyze their execution results to detect bugs. However, floating-point errors,\ninherent randomness, and the complexity of test inputs make it challenging to\nanalyze execution results effectively, leading to existing methods suffering\nfrom a lack of suitable test oracles. Some researchers utilize metamorphic\ntesting to tackle this challenge. They design Metamorphic Relations (MRs) based\non input data and parameter settings of a single framework interface to\ngenerate equivalent test inputs, ensuring consistent execution results between\noriginal and generated test inputs. Despite their promising effectiveness, they\nstill face certain limitations. (1) Existing MRs overlook structural\ncomplexity, limiting test input diversity. (2) Existing MRs focus on limited\ninterfaces, which limits generalization and necessitates additional\nadaptations. (3) Their detected bugs are related to the result consistency of\nsingle interfaces and far from those exposed in multi-interface combinations\nand runtime metrics (e.g., resource usage). To address these limitations, we\npropose ModelMeta, a model-level metamorphic testing method for DL frameworks\nwith four MRs focused on the structure characteristics of DL models. ModelMeta\naugments seed models with diverse interface combinations to generate test\ninputs with consistent outputs, guided by the QR-DQN strategy. It then detects\nbugs through fine-grained analysis of training loss/gradients, memory/GPU\nusage, and execution time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aModelMeta\u7684\u6a21\u578b\u7ea7\u8715\u53d8\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6d4b\u8bd5\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u591a\u6837\u5316\u63a5\u53e3\u7ec4\u5408\u548c\u7cbe\u7ec6\u5206\u6790\u6765\u68c0\u6d4bbug\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u7684bug\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u4f46\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u56e0\u6d6e\u70b9\u8bef\u5dee\u3001\u968f\u673a\u6027\u548c\u6d4b\u8bd5\u8f93\u5165\u590d\u6742\u6027\u800c\u6548\u679c\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u5408\u9002\u7684\u6d4b\u8bd5\u9884\u8a00\u3002", "method": "\u63d0\u51faModelMeta\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u7ed3\u6784\u7279\u6027\u8bbe\u8ba1\u56db\u79cd\u8715\u53d8\u5173\u7cfb\uff0c\u5229\u7528QR-DQN\u7b56\u7565\u751f\u6210\u591a\u6837\u5316\u6d4b\u8bd5\u8f93\u5165\uff0c\u5e76\u5206\u6790\u8bad\u7ec3\u635f\u5931/\u68af\u5ea6\u3001\u5185\u5b58/GPU\u4f7f\u7528\u548c\u6267\u884c\u65f6\u95f4\u3002", "result": "ModelMeta\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u6d4b\u8bd5\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u7cbe\u7ec6\u5206\u6790\u68c0\u6d4b\u591a\u63a5\u53e3\u7ec4\u5408\u548c\u8fd0\u884c\u65f6\u6307\u6807\u76f8\u5173\u7684bug\u3002", "conclusion": "ModelMeta\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002"}}
{"id": "2507.03166", "pdf": "https://arxiv.org/pdf/2507.03166", "abs": "https://arxiv.org/abs/2507.03166", "authors": ["Daniel Berio", "Guillaume Clivaz", "Michael Stroh", "Oliver Deussen", "R\u00e9jean Plamondon", "Sylvain Calinon", "Frederic Fol Leymarie"], "title": "Image-driven Robot Drawing with Rapid Lognormal Movements", "categories": ["cs.RO", "cs.GR"], "comment": "Accepted at IEEE RO-MAN 2025", "summary": "Large image generation and vision models, combined with differentiable\nrendering technologies, have become powerful tools for generating paths that\ncan be drawn or painted by a robot. However, these tools often overlook the\nintrinsic physicality of the human drawing/writing act, which is usually\nexecuted with skillful hand/arm gestures. Taking this into account is important\nfor the visual aesthetics of the results and for the development of closer and\nmore intuitive artist-robot collaboration scenarios. We present a method that\nbridges this gap by enabling gradient-based optimization of natural human-like\nmotions guided by cost functions defined in image space. To this end, we use\nthe sigma-lognormal model of human hand/arm movements, with an adaptation that\nenables its use in conjunction with a differentiable vector graphics (DiffVG)\nrenderer. We demonstrate how this pipeline can be used to generate feasible\ntrajectories for a robot by combining image-driven objectives with a\nminimum-time smoothing criterion. We demonstrate applications with generation\nand robotic reproduction of synthetic graffiti as well as image abstraction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u50cf\u7a7a\u95f4\u6210\u672c\u51fd\u6570\u548c\u4eba\u7c7b\u624b\u52bf\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u673a\u5668\u4eba\u7ed8\u753b\u7684\u81ea\u7136\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u56fe\u50cf\u751f\u6210\u548c\u89c6\u89c9\u6a21\u578b\u5ffd\u7565\u4e86\u4eba\u7c7b\u7ed8\u753b/\u4e66\u5199\u65f6\u7684\u7269\u7406\u7279\u5f81\uff0c\u5f71\u54cd\u4e86\u89c6\u89c9\u7f8e\u611f\u548c\u4eba\u673a\u534f\u4f5c\u7684\u76f4\u89c2\u6027\u3002", "method": "\u91c7\u7528sigma-lognormal\u4eba\u7c7b\u624b\u52bf\u6a21\u578b\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u77e2\u91cf\u56fe\u5f62\u6e32\u67d3\u5668\uff08DiffVG\uff09\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u751f\u6210\u81ea\u7136\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u673a\u5668\u4eba\u53ef\u6267\u884c\u7684\u8f68\u8ff9\uff0c\u5e94\u7528\u4e8e\u5408\u6210\u6d82\u9e26\u548c\u56fe\u50cf\u62bd\u8c61\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7ed8\u753b\u7684\u7f8e\u89c2\u6027\u548c\u4eba\u673a\u534f\u4f5c\u7684\u81ea\u7136\u6027\u3002"}}
{"id": "2507.04421", "pdf": "https://arxiv.org/pdf/2507.04421", "abs": "https://arxiv.org/abs/2507.04421", "authors": ["Wanqing Tu"], "title": "Resource-Efficient Seamless Transitions For High-Performance Multi-hop UAV Multicasting", "categories": ["cs.NI"], "comment": null, "summary": "Many UAV-related applications require group communications between UAVs to\nreliably and efficiently deliver rich media content as well as to extend\nline-of-sight coverage between sky and ground. This paper studies fast yet\nresource-efficient UAV transitions while maintaining high multicasting\nperformance. We develop a set of analytic and algorithmic results to form the\nefficient transition formation (ETF) algorithm that deals with different UAV\ntransition scenarios in a multicasting environment. The ETF algorithm first\nevaluates the seamlessness of a straight-line trajectory (SLT), by processing\nlow-complexity computations (e.g., Euclidean distances) or a chain of fast\nchecks with controlled traffic overheads. For an interrupted SLT, ETF\nestablishes a new trajectory consisting of a minimum number of seamless\nstraight lines that join at specially selected locations in terms of\ncontrolling mobile UAVs' seamless travel distances. Our simulation studies\nquantify the multicasting performance gains that ETF allows, outperforming\ncompared studies when seamlessly transiting UAV group members.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u4eba\u673a\uff08UAV\uff09\u7fa4\u7ec4\u901a\u4fe1\u7b97\u6cd5\uff08ETF\uff09\uff0c\u901a\u8fc7\u5feb\u901f\u4f4e\u590d\u6742\u5ea6\u7684\u8ba1\u7b97\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\uff0c\u63d0\u5347\u591a\u64ad\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u7fa4\u7ec4\u901a\u4fe1\u9700\u8981\u9ad8\u6548\u53ef\u9760\u5730\u4f20\u8f93\u591a\u5a92\u4f53\u5185\u5bb9\u5e76\u6269\u5c55\u7a7a\u5bf9\u5730\u89c6\u8ddd\u8986\u76d6\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u5b9e\u73b0\u5feb\u901f\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u65e0\u4eba\u673a\u8fc7\u6e21\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u591a\u64ad\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u9ad8\u6548\u8fc7\u6e21\u5f62\u6210\uff08ETF\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u76f4\u7ebf\u8f68\u8ff9\uff08SLT\uff09\u7684\u8fde\u7eed\u6027\uff0c\u6216\u5feb\u901f\u68c0\u67e5\u94fe\u6765\u5904\u7406\u4e0d\u540c\u8fc7\u6e21\u573a\u666f\u3002\u82e5SLT\u4e2d\u65ad\uff0cETF\u5efa\u7acb\u7531\u6700\u5c11\u65e0\u7f1d\u76f4\u7ebf\u7ec4\u6210\u7684\u65b0\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0cETF\u5728\u591a\u64ad\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u80fd\u591f\u65e0\u7f1d\u8fc7\u6e21\u65e0\u4eba\u673a\u7fa4\u7ec4\u6210\u5458\u3002", "conclusion": "ETF\u7b97\u6cd5\u901a\u8fc7\u4f4e\u590d\u6742\u5ea6\u8ba1\u7b97\u548c\u4f18\u5316\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7fa4\u7ec4\u901a\u4fe1\u7684\u591a\u64ad\u6027\u80fd\u548c\u8fc7\u6e21\u6548\u7387\u3002"}}
{"id": "2507.03136", "pdf": "https://arxiv.org/pdf/2507.03136", "abs": "https://arxiv.org/abs/2507.03136", "authors": ["Ricardo Queiroz de Araujo Fernandes", "Anderson Santos", "Daniel Maier de Carvalho", "Andr\u00e9 Luiz Bandeira Molina"], "title": "Holographic Projection and Cyber Attack Surface: A Physical Analogy for Digital Security", "categories": ["cs.CR", "cs.LO"], "comment": "The paper was produced to base a presentation in the V Jornadas STIC\n  capitulo Panam\\'a", "summary": "This article presents an in-depth exploration of the analogy between the\nHolographic Principle in theoretical physics and cyber attack surfaces in\ndigital security. Building on concepts such as black hole entropy and AdS/CFT\nduality, it highlights how complex infrastructures project their\nvulnerabilities onto their external interfaces. The paper draws a parallel\nbetween a black hole's event horizon, which encodes all internal information,\nand the attack surface, which reflects the internal architecture's security\nposture. Additionally, the article outlines how this conceptual framework can\nguide cybersecurity practices, emphasizing strategies such as attack surface\nreduction, continuous scanning with tools like OWASP ZAP and Greenbone OpenVAS,\nand the implementation of Zero Trust Architecture. This analogy not only\nprovides a unique perspective on digital security but also underscores the\ncritical importance of boundary-level defenses in protecting vast internal\ninfrastructures.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u7269\u7406\u4e2d\u7684\u5168\u606f\u539f\u7406\u4e0e\u6570\u5b57\u5b89\u5168\u4e2d\u7684\u653b\u51fb\u8868\u9762\u7c7b\u6bd4\uff0c\u63a2\u8ba8\u4e86\u590d\u6742\u57fa\u7840\u8bbe\u65bd\u5982\u4f55\u5c06\u6f0f\u6d1e\u6295\u5c04\u5230\u5916\u90e8\u63a5\u53e3\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u7684\u7f51\u7edc\u5b89\u5168\u5b9e\u8df5\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7\u7269\u7406\u5b66\u7684\u5168\u606f\u539f\u7406\u4e3a\u6570\u5b57\u5b89\u5168\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\uff0c\u63ed\u793a\u5185\u90e8\u67b6\u6784\u5b89\u5168\u72b6\u6001\u4e0e\u5916\u90e8\u653b\u51fb\u8868\u9762\u7684\u5173\u7cfb\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u7406\u8bba\u7c7b\u6bd4\uff08\u5982\u9ed1\u6d1e\u4e8b\u4ef6\u89c6\u754c\u4e0e\u653b\u51fb\u8868\u9762\u7684\u5e73\u884c\u5173\u7cfb\uff09\u548c\u5b9e\u8df5\u7b56\u7565\uff08\u5982\u653b\u51fb\u8868\u9762\u51cf\u5c11\u3001\u6301\u7eed\u626b\u63cf\u5de5\u5177\u4f7f\u7528\u548c\u96f6\u4fe1\u4efb\u67b6\u6784\u5b9e\u65bd\uff09\u3002", "result": "\u7814\u7a76\u6210\u679c\u8868\u660e\uff0c\u8fb9\u754c\u9632\u5fa1\u5bf9\u4fdd\u62a4\u5e9e\u5927\u5185\u90e8\u57fa\u7840\u8bbe\u65bd\u81f3\u5173\u91cd\u8981\uff0c\u7c7b\u6bd4\u6846\u67b6\u4e3a\u6570\u5b57\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u901a\u8fc7\u5168\u606f\u539f\u7406\u7684\u7c7b\u6bd4\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u4f18\u5316\u6570\u5b57\u5b89\u5168\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u5f3a\u8c03\u4e86\u8fb9\u754c\u9632\u5fa1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.03170", "pdf": "https://arxiv.org/pdf/2507.03170", "abs": "https://arxiv.org/abs/2507.03170", "authors": ["Ronald J. Pandolfi", "Jeffrey J. Donatelli", "Julian Todd", "Daniela Ushizima"], "title": "ASCRIBE-XR: Virtual Reality for Visualization of Scientific Imagery", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "ASCRIBE-XR, a novel computational platform designed to facilitate the\nvisualization and exploration of 3D volumetric data and mesh data in the\ncontext of synchrotron experiments, is described. Using Godot and PC-VR\ntechnologies, the platform enables users to dynamically load and manipulate 3D\ndata sets to gain deeper insights into their research. The program's multi-user\ncapabilities, enabled through WebRTC, and MQTT, allow multiple users to share\ndata and visualize together in real-time, promoting a more interactive and\nengaging research experience. We describe the design and implementation of\nASCRIBE-XR, highlighting its key features and capabilities. We will also\ndiscuss its utility in the context of synchrotron research, including examples\nof its application and potential benefits for the scientific community.", "AI": {"tldr": "ASCRIBE-XR \u662f\u4e00\u4e2a\u57fa\u4e8e Godot \u548c PC-VR \u6280\u672f\u7684\u65b0\u8ba1\u7b97\u5e73\u53f0\uff0c\u7528\u4e8e\u540c\u6b65\u8f90\u5c04\u5b9e\u9a8c\u4e2d\u7684 3D \u4f53\u6570\u636e\u548c\u7f51\u683c\u6570\u636e\u53ef\u89c6\u5316\u4e0e\u63a2\u7d22\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u540c\u6b65\u8f90\u5c04\u5b9e\u9a8c\u4e2d 3D \u6570\u636e\u52a8\u6001\u52a0\u8f7d\u3001\u64cd\u4f5c\u548c\u591a\u7528\u6237\u5b9e\u65f6\u534f\u4f5c\u7684\u9700\u6c42\uff0c\u5f00\u53d1\u4e86 ASCRIBE-XR\u3002", "method": "\u5229\u7528 Godot \u548c PC-VR \u6280\u672f\u5b9e\u73b0 3D \u6570\u636e\u7684\u52a8\u6001\u52a0\u8f7d\u4e0e\u64cd\u4f5c\uff0c\u901a\u8fc7 WebRTC \u548c MQTT \u6280\u672f\u5b9e\u73b0\u591a\u7528\u6237\u5b9e\u65f6\u534f\u4f5c\u3002", "result": "\u5e73\u53f0\u652f\u6301\u591a\u7528\u6237\u5b9e\u65f6\u5171\u4eab\u548c\u53ef\u89c6\u5316\u6570\u636e\uff0c\u63d0\u5347\u4e86\u7814\u7a76\u7684\u4e92\u52a8\u6027\u548c\u53c2\u4e0e\u611f\u3002", "conclusion": "ASCRIBE-XR \u4e3a\u540c\u6b65\u8f90\u5c04\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u548c\u79d1\u5b66\u4ef7\u503c\u3002"}}
{"id": "2507.03823", "pdf": "https://arxiv.org/pdf/2507.03823", "abs": "https://arxiv.org/abs/2507.03823", "authors": ["Mohamed Messaoud Louamri", "Achraf Boussahi", "Nacer Eddine Belaloui", "Abdellah Tounsi", "Mohamed Taha Rouabah"], "title": "A Study of Gate-Based and Boson Sampling Quantum Random Number Generation on IBM and Xanadu Quantum Devices", "categories": ["quant-ph", "cs.ET"], "comment": "4 pages, 2 tables", "summary": "Quantum mechanics offers a fundamentally unpredictable entropy source due to\nthe intrinsic probabilistic nature of quantum measurements, making it\nattractive for secure random number generation. This paper explores the\npracticality of generating random numbers from two quantum platforms:\ngate-based circuits on IBM Quantum and (Gaussian) boson sampling with Xanadu\nBorealis. We implement simple post-processing methods, including the classic\nVon Neumann extractor and two tailored variants designed to address the\ncorrelated structure of boson sampling outputs. We evaluate debiased output\nfrom real quantum hardware using the NIST SP800-22r1a test suite and measure\nthe extraction efficiency of each debiasing method. Results show that, while\nunbiased bitstreams can be achieved on both platforms, throughput remains low\nand cost per random bit is high compared to specialized QRNG devices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4eceIBM Quantum\u7684\u57fa\u4e8e\u95e8\u7535\u8def\u548cXanadu Borealis\u7684\u9ad8\u65af\u73bb\u8272\u5b50\u91c7\u6837\u4e2d\u751f\u6210\u968f\u673a\u6570\u7684\u5b9e\u7528\u6027\uff0c\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u540e\u5904\u7406\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6210\u672c\u3002", "motivation": "\u91cf\u5b50\u6d4b\u91cf\u7684\u56fa\u6709\u6982\u7387\u6027\u63d0\u4f9b\u4e86\u4e0d\u53ef\u9884\u6d4b\u7684\u71b5\u6e90\uff0c\u9002\u5408\u7528\u4e8e\u5b89\u5168\u7684\u968f\u673a\u6570\u751f\u6210\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u7387\u548c\u6210\u672c\u662f\u9700\u8981\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u5728IBM Quantum\u548cXanadu Borealis\u5e73\u53f0\u4e0a\u5b9e\u73b0\u968f\u673a\u6570\u751f\u6210\uff0c\u4f7f\u7528Von Neumann\u63d0\u53d6\u5668\u53ca\u5176\u4e24\u79cd\u53d8\u4f53\u8fdb\u884c\u540e\u5904\u7406\uff0c\u5e76\u901a\u8fc7NIST\u6d4b\u8bd5\u8bc4\u4f30\u53bb\u504f\u6548\u679c\u3002", "result": "\u4e24\u79cd\u5e73\u53f0\u5747\u53ef\u751f\u6210\u65e0\u504f\u6bd4\u7279\u6d41\uff0c\u4f46\u541e\u5410\u91cf\u8f83\u4f4e\u4e14\u6bcf\u6bd4\u7279\u6210\u672c\u9ad8\u4e8e\u4e13\u7528QRNG\u8bbe\u5907\u3002", "conclusion": "\u91cf\u5b50\u968f\u673a\u6570\u751f\u6210\u5728\u5b89\u5168\u6027\u4e0a\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u6548\u7387\u548c\u6210\u672c\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2507.04172", "pdf": "https://arxiv.org/pdf/2507.04172", "abs": "https://arxiv.org/abs/2507.04172", "authors": ["Younan Gao", "Andrzej Pelc"], "title": "Gathering Teams of Bounded Memory Agents on a Line", "categories": ["cs.DC"], "comment": null, "summary": "Several mobile agents, modelled as deterministic automata, navigate in an\ninfinite line in synchronous rounds. All agents start in the same round. In\neach round, an agent can move to one of the two neighboring nodes, or stay\nidle. Agents have distinct labels which are integers from the set $\\{1,\\dots,\nL\\}$. They start in teams, and all agents in a team have the same starting\nnode. The adversary decides the compositions of teams, and their starting\nnodes. Whenever an agent enters a node, it sees the entry port number and the\nstates of all collocated agents; this information forms the input of the agent\non the basis of which it transits to the next state and decides the current\naction. The aim is for all agents to gather at the same node and stop.\nGathering is feasible, if this task can be accomplished for any decisions of\nthe adversary, and its time is the worst-case number of rounds from the start\ntill gathering.\n  We consider the feasibility and time complexity of gathering teams of agents,\nand give a complete solution of this problem. It turns out that both\nfeasibility and complexity of gathering depend on the sizes of teams. We first\nconcentrate on the case when all teams have the same size $x$. For the oriented\nline, gathering is impossible if $x=1$, and it can be accomplished in time\n$O(D)$, for $x>1$, where $D$ is the distance between the starting nodes of the\nmost distant teams. This complexity is of course optimal. For the unoriented\nline, the situation is different. For $x=1$, gathering is also impossible, but\nfor $x=2$, the optimal time of gathering is $\\Theta(D\\log L)$, and for $x\\geq\n3$, the optimal time of gathering is $\\Theta(D)$. In the case when there are\nteams of different sizes, we show that gathering is always possible in time\n$O(D)$, even for the unoriented line. This complexity is of course optimal.", "AI": {"tldr": "\u7814\u7a76\u4e86\u591a\u4e2a\u79fb\u52a8\u4ee3\u7406\u5728\u65e0\u9650\u7ebf\u4e0a\u7684\u805a\u96c6\u95ee\u9898\uff0c\u5176\u53ef\u884c\u6027\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u53d6\u51b3\u4e8e\u56e2\u961f\u7684\u5927\u5c0f\u3002\u5bf9\u4e8e\u540c\u7b49\u5927\u5c0f\u7684\u56e2\u961f\uff0c\u6709\u5411\u7ebf\u5728\u56e2\u961f\u5927\u5c0f\u5927\u4e8e1\u65f6\u53ef\u884c\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(D)\uff1b\u65e0\u5411\u7ebf\u5728\u56e2\u961f\u5927\u5c0f\u4e3a2\u65f6\u590d\u6742\u5ea6\u4e3a\u0398(DlogL)\uff0c\u5927\u4e8e3\u65f6\u4e3a\u0398(D)\u3002\u5bf9\u5927\u5c0f\u4e0d\u540c\u7684\u56e2\u961f\uff0c\u805a\u96c6\u603b\u662f\u53ef\u884c\u4e14\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(D)\u3002", "motivation": "\u7814\u7a76\u591a\u4ee3\u7406\u5728\u540c\u6b65\u73af\u5883\u4e2d\u7684\u805a\u96c6\u95ee\u9898\uff0c\u7279\u522b\u662f\u56e2\u961f\u5927\u5c0f\u5bf9\u805a\u96c6\u53ef\u884c\u6027\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u76f4\u63a5\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u786e\u5b9a\u6027\u81ea\u52a8\u673a\u6a21\u578b\u4ee3\u7406\u884c\u4e3a\uff0c\u8003\u8651\u56e2\u961f\u5927\u5c0f\u548c\u7ebf\u7c7b\u578b\uff08\u6709\u5411/\u65e0\u5411\uff09\uff0c\u5206\u6790\u805a\u96c6\u7684\u53ef\u884c\u6027\u53ca\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u5bf9\u4e8e\u540c\u7c7b\u5927\u5c0f\u56e2\u961f\uff0c\u6709\u5411\u7ebf\u5728x>1\u53ef\u884c\uff0c\u65e0\u5411\u7ebf\u5728x\u22652\u53ef\u884c\uff0c\u590d\u6742\u5ea6\u5206\u522b\u4e3aO(D)\u548c{\u0398(DlogL), \u0398(D)}\uff1b\u5f02\u7c7b\u5927\u5c0f\u56e2\u961f\u5219\u603b\u662f\u53ef\u884c\u4e14\u590d\u6742\u5ea6\u4e3aO(D)\u3002", "conclusion": "\u56e2\u961f\u5927\u5c0f\u548c\u7ebf\u7c7b\u578b\u662f\u805a\u96c6\u95ee\u9898\u53ef\u884c\u6027\u548c\u590d\u6742\u5ea6\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5f02\u7c7b\u56e2\u961f\u603b\u80fd\u9ad8\u6548\u89e3\u51b3\u3002"}}
{"id": "2507.05081", "pdf": "https://arxiv.org/pdf/2507.05081", "abs": "https://arxiv.org/abs/2507.05081", "authors": ["Xin Li", "Mianxin Xiao", "Xi Shen", "Jiaqing Chu", "Weifeng Huang", "Jiashun Li", "Yaoyi Li", "Mingjing Cai", "Jiaming Chen", "Xinming Zhang", "Daxing Zhang", "Congsi Wang", "Hong Tang", "Bao Zhao", "Qitao Lu", "Yilong Wang", "Jianjun Wang", "Minyi Xu", "Shitong Fang", "Xuanyu Huang. Chaoyang Zhao", "Zicheng Liu", "Yaowen Yang", "Guobiao Hu", "Junrui Liang", "Wei-Hsin Liao"], "title": "ViPSN 2.0: A Reconfigurable Battery-free IoT Platform for Vibration Energy Harvesting", "categories": ["cs.AR"], "comment": null, "summary": "Vibration energy harvesting is a promising solution for powering battery-free\nIoT systems; however, the instability of ambient vibrations presents\nsignificant challenges, such as limited harvested energy, intermittent power\nsupply, and poor adaptability to various applications. To address these\nchallenges, this paper proposes ViPSN2.0, a modular and reconfigurable IoT\nplatform that supports multiple vibration energy harvesters (piezoelectric,\nelectromagnetic, and triboelectric) and accommodates sensing tasks with varying\napplication requirements through standardized hot-swappable interfaces.\nViPSN~2.0 incorporates an energy-indication power management framework tailored\nto various application demands, including light-duty discrete sampling,\nheavy-duty high-power sensing, and complex-duty streaming tasks, thereby\neffectively managing fluctuating energy availability. The platform's\nversatility and robustness are validated through three representative\napplications: ViPSN-Beacon, enabling ultra-low-power wireless beacon\ntransmission from a single transient fingertip press; ViPSN-LoRa, supporting\nhigh-power, long-range wireless communication powered by wave vibrations in\nactual marine environments; and ViPSN-Cam, enabling intermittent image capture\nand wireless transfer. Experimental results demonstrate that ViPSN~2.0 can\nreliably meet a wide range of requirements in practical battery-free IoT\ndeployments under energy-constrained conditions.", "AI": {"tldr": "ViPSN2.0\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u91cd\u6784\u7684\u7269\u8054\u7f51\u5e73\u53f0\uff0c\u652f\u6301\u591a\u79cd\u632f\u52a8\u80fd\u91cf\u6536\u96c6\u5668\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u5316\u70ed\u63d2\u62d4\u63a5\u53e3\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u9700\u6c42\uff0c\u6709\u6548\u89e3\u51b3\u73af\u5883\u632f\u52a8\u4e0d\u7a33\u5b9a\u6027\u5e26\u6765\u7684\u95ee\u9898\u3002", "motivation": "\u73af\u5883\u632f\u52a8\u7684\u4e0d\u7a33\u5b9a\u6027\u9650\u5236\u4e86\u65e0\u7535\u6c60IoT\u7cfb\u7edf\u7684\u80fd\u91cf\u6536\u96c6\uff0cViPSN2.0\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5e73\u53f0\u652f\u6301\u591a\u79cd\u80fd\u91cf\u6536\u96c6\u5668\uff08\u538b\u7535\u3001\u7535\u78c1\u3001\u6469\u64e6\u7535\uff09\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u80fd\u91cf\u6307\u793a\u7684\u7535\u6e90\u7ba1\u7406\u6846\u67b6\uff0c\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002", "result": "\u901a\u8fc7\u4e09\u79cd\u4ee3\u8868\u6027\u5e94\u7528\u9a8c\u8bc1\u4e86\u5e73\u53f0\u7684\u591a\u6837\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f8b\u5982\u8d85\u4f4e\u529f\u8017\u65e0\u7ebf\u4fe1\u6807\u4f20\u8f93\u548c\u6d77\u6d0b\u73af\u5883\u4e2d\u7684\u9ad8\u529f\u7387\u8fdc\u7a0b\u901a\u4fe1\u3002", "conclusion": "ViPSN2.0\u80fd\u5728\u80fd\u91cf\u53d7\u9650\u6761\u4ef6\u4e0b\u53ef\u9760\u6ee1\u8db3\u65e0\u7535\u6c60IoT\u7cfb\u7edf\u7684\u591a\u6837\u5316\u9700\u6c42\u3002"}}
{"id": "2507.03797", "pdf": "https://arxiv.org/pdf/2507.03797", "abs": "https://arxiv.org/abs/2507.03797", "authors": ["Benjamin Kahl"], "title": "Assessing the Viability of Wave Field Synthesis in VR-Based Cognitive Research", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS"], "comment": "35 pages", "summary": "This paper investigates the viability of Wave Field Synthesis (WFS) for\nenhancing auditory immersion in VR-based cognitive research. While Virtual\nReality (VR) offers significant advantages for studying human perception and\nbehavior, auditory cues are often underutilized. WFS, an advanced audio\nrendering technique, can create highly realistic and spatially accurate\nsoundscapes, potentially increasing ecological validity. This study evaluates\nWFS by implementing a sample experiment where participants localize static and\nmoving sound sources in both a WFS-rendered environment and a conventional\nstereo headphone setup. The research explores the impact of virtual\nenvironments, sound types, and durations on localization accuracy and search\nbehavior. Findings indicate that while stereo setups can achieve higher\naccuracy, WFS provides a more natural and intuitive auditory experience,\nparticularly for directional cues. The study also highlights limitations of\ncurrent WFS systems, such as the lack of height localization, occlusion\nsimulation, and user-dependent optimization, which affect performance,\nespecially for centrally located sound sources. Despite these challenges, WFS\nshows promise for specialized auditory perception research, particularly for\ncomplex soundscapes where directional information is paramount.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6ce2\u573a\u5408\u6210\uff08WFS\uff09\u5728\u589e\u5f3aVR\u8ba4\u77e5\u7814\u7a76\u4e2d\u542c\u89c9\u6c89\u6d78\u611f\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0WFS\u867d\u5728\u67d0\u4e9b\u65b9\u9762\u4e0d\u5982\u4f20\u7edf\u7acb\u4f53\u58f0\u8033\u673a\u7cbe\u786e\uff0c\u4f46\u80fd\u63d0\u4f9b\u66f4\u81ea\u7136\u7684\u542c\u89c9\u4f53\u9a8c\u3002", "motivation": "VR\u6280\u672f\u5728\u4eba\u7c7b\u611f\u77e5\u548c\u884c\u4e3a\u7814\u7a76\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u542c\u89c9\u7ebf\u7d22\u5e38\u88ab\u5ffd\u89c6\u3002WFS\u4f5c\u4e3a\u4e00\u79cd\u5148\u8fdb\u7684\u97f3\u9891\u6e32\u67d3\u6280\u672f\uff0c\u6709\u671b\u63d0\u9ad8\u751f\u6001\u6548\u5ea6\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83WFS\u548c\u4f20\u7edf\u7acb\u4f53\u58f0\u8033\u673a\u5728\u9759\u6001\u548c\u52a8\u6001\u58f0\u6e90\u5b9a\u4f4d\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u865a\u62df\u73af\u5883\u3001\u58f0\u97f3\u7c7b\u578b\u548c\u65f6\u957f\u5bf9\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u641c\u7d22\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7acb\u4f53\u58f0\u8033\u673a\u5b9a\u4f4d\u66f4\u7cbe\u786e\uff0c\u4f46WFS\u63d0\u4f9b\u66f4\u81ea\u7136\u7684\u542c\u89c9\u4f53\u9a8c\uff0c\u5c24\u5176\u5728\u65b9\u5411\u6027\u7ebf\u7d22\u65b9\u9762\u3002WFS\u76ee\u524d\u5b58\u5728\u9ad8\u5ea6\u5b9a\u4f4d\u3001\u906e\u6321\u6a21\u62df\u7b49\u5c40\u9650\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u5c40\u9650\u6027\uff0cWFS\u5728\u590d\u6742\u7684\u58f0\u666f\u7814\u7a76\uff0c\u5c24\u5176\u662f\u65b9\u5411\u4fe1\u606f\u5173\u952e\u7684\u573a\u666f\u4e2d\uff0c\u663e\u793a\u51fa\u6f5c\u529b\u3002"}}
{"id": "2507.04360", "pdf": "https://arxiv.org/pdf/2507.04360", "abs": "https://arxiv.org/abs/2507.04360", "authors": ["Yanzhou Mu", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhixiang Cao", "Peiran Yang", "Yinglong Zou", "Tao Zheng", "Zhenyu Chen"], "title": "DevMuT: Testing Deep Learning Framework via Developer Expertise-Based Mutation", "categories": ["cs.SE"], "comment": "12 pages, 8 figures", "summary": "Deep learning (DL) frameworks are the fundamental infrastructure for various\nDL applications. Framework defects can profoundly cause disastrous accidents,\nthus requiring sufficient detection. In previous studies, researchers adopt DL\nmodels as test inputs combined with mutation to generate more diverse models.\nThough these studies demonstrate promising results, most detected defects are\nconsidered trivial (i.e., either treated as edge cases or ignored by the\ndevelopers). To identify important bugs that matter to developers, we propose a\nnovel DL framework testing method DevMuT, which generates models by adopting\nmutation operators and constraints derived from developer expertise. DevMuT\nsimulates developers'common operations in development and detects more diverse\ndefects within more stages of the DL model lifecycle (e.g., model training and\ninference). We evaluate the performance of DevMuT on three widely used DL\nframeworks (i.e., PyTorch, JAX, and Mind- Spore) with 29 DL models from nine\ntypes of industry tasks. The experiment results show that DevMuT outperforms\nstate-of-the-art baselines: it can achieve at least 71.68% improvement on\naverage in the diversity of generated models and 28.20% improvement on average\nin the legal rates of generated models. Moreover, DevMuT detects 117 defects,\n63 of which are confirmed, 24 are fixed, and eight are of high value confirmed\nby developers. Finally, DevMuT has been deployed in the MindSpore community\nsince December 2023. These demonstrate the effectiveness of DevMuT in detecting\ndefects that are close to the real scenes and are of concern to developers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6d4b\u8bd5\u65b9\u6cd5DevMuT\uff0c\u901a\u8fc7\u5f00\u53d1\u8005\u7684\u4e13\u4e1a\u77e5\u8bc6\u751f\u6210\u66f4\u6709\u6548\u7684\u6d4b\u8bd5\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f3a\u9677\u68c0\u6d4b\u7684\u591a\u6837\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u68c0\u6d4b\u7684\u7f3a\u9677\u591a\u4e3a\u8fb9\u7f18\u6848\u4f8b\u6216\u88ab\u5f00\u53d1\u8005\u5ffd\u7565\u7684\u95ee\u9898\uff0c\u800cDevMuT\u65e8\u5728\u8bc6\u522b\u5bf9\u5f00\u53d1\u8005\u771f\u6b63\u91cd\u8981\u7684\u7f3a\u9677\u3002", "method": "\u91c7\u7528\u5f00\u53d1\u8005\u5e38\u89c1\u7684\u64cd\u4f5c\u4f5c\u4e3a\u7a81\u53d8\u64cd\u4f5c\uff0c\u7ed3\u5408\u7ea6\u675f\u6761\u4ef6\u751f\u6210\u6d4b\u8bd5\u6a21\u578b\uff0c\u8986\u76d6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u751f\u547d\u5468\u671f\u7684\u591a\u4e2a\u9636\u6bb5\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u6d41DL\u6846\u67b6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDevMuT\u5728\u6a21\u578b\u751f\u6210\u591a\u6837\u6027\u548c\u5408\u6cd5\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u68c0\u6d4b\u5230117\u4e2a\u7f3a\u9677\uff0c\u5176\u4e2d63\u4e2a\u88ab\u786e\u8ba4\uff0c24\u4e2a\u5df2\u4fee\u590d\u3002", "conclusion": "DevMuT\u6709\u6548\u68c0\u6d4b\u4e86\u63a5\u8fd1\u771f\u5b9e\u573a\u666f\u4e14\u5bf9\u5f00\u53d1\u8005\u91cd\u8981\u7684\u7f3a\u9677\uff0c\u5df2\u88ab\u5b9e\u9645\u5e94\u7528\u4e8eMindSpore\u793e\u533a\u3002"}}
{"id": "2507.04425", "pdf": "https://arxiv.org/pdf/2507.04425", "abs": "https://arxiv.org/abs/2507.04425", "authors": ["Zexin Deng", "Zhenhui Yuan", "Longhao Zou"], "title": "TeleSim: A Network-Aware Testbed and Benchmark Dataset for Telerobotic Applications", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "Telerobotic technologies are becoming increasingly essential in fields such\nas remote surgery, nuclear decommissioning, and space exploration. Reliable\ndatasets and testbeds are essential for evaluating telerobotic system\nperformance prior to real-world deployment. However, there is a notable lack of\ndatasets that capture the impact of network delays, as well as testbeds that\nrealistically model the communication link between the operator and the robot.\nThis paper introduces TeleSim, a network-aware teleoperation dataset and\ntestbed designed to assess the performance of telerobotic applications under\ndiverse network conditions. TeleSim systematically collects performance data\nfrom fine manipulation tasks executed under three predefined network quality\ntiers: High, Medium, and Low. Each tier is characterized through controlled\nsettings of bandwidth, latency, jitter, and packet loss. Using OMNeT++ for\nprecise network simulation, we record a wide range of metrics, including\ncompletion time, success rates, video quality indicators (Peak Signal-to-Noise\nRatio (PSNR) and Structural Similarity Index Measure (SSIM)), and quality of\nservice (QoS) parameters. TeleSim comprises 300 experimental trials, providing\na robust benchmark for evaluating teleoperation systems across heterogeneous\nnetwork scenarios. In the worst network condition, completion time increases by\n221.8% and success rate drops by 64%. Our findings reveal that network\ndegradation leads to compounding negative impacts, notably reduced video\nquality and prolonged task execution, highlighting the need for adaptive,\nresilient teleoperation protocols. The full dataset and testbed software are\npublicly available on our GitHub repository:\nhttps://github.com/ConnectedRoboticsLab and YouTube channel:\nhttps://youtu.be/Fz_1iOYe104.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86TeleSim\uff0c\u4e00\u79cd\u7f51\u7edc\u611f\u77e5\u7684\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\u4e0b\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u80fd\u591f\u53cd\u6620\u7f51\u7edc\u5ef6\u8fdf\u5f71\u54cd\u7684\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u8fdc\u7a0b\u64cd\u4f5c\u5728\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "method": "TeleSim\u901a\u8fc7OMNeT++\u8fdb\u884c\u7f51\u7edc\u6a21\u62df\uff0c\u6536\u96c6\u7cbe\u7ec6\u64cd\u4f5c\u4efb\u52a1\u5728\u9ad8\u4e2d\u4f4e\u4e09\u4e2a\u7f51\u7edc\u8d28\u91cf\u5c42\u7ea7\u4e0b\u7684\u6027\u80fd\u6570\u636e\uff0c\u5305\u62ec\u5b8c\u6210\u65f6\u95f4\u3001\u6210\u529f\u7387\u7b49\u6307\u6807\u3002", "result": "\u5728\u6700\u5dee\u7f51\u7edc\u6761\u4ef6\u4e0b\uff0c\u5b8c\u6210\u65f6\u95f4\u589e\u52a0\u4e86221.8%\uff0c\u6210\u529f\u7387\u4e0b\u964d\u4e8664%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u7f51\u7edc\u8d28\u91cf\u5bf9\u8fdc\u7a0b\u64cd\u4f5c\u6709\u663e\u8457\u8d1f\u9762\u5f71\u54cd\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u9002\u5e94\u548c\u5f39\u6027\u7684\u8fdc\u7a0b\u64cd\u4f5c\u534f\u8bae\u3002"}}
{"id": "2507.03439", "pdf": "https://arxiv.org/pdf/2507.03439", "abs": "https://arxiv.org/abs/2507.03439", "authors": ["Luk\u00e1\u0161 Hol\u00edk", "Ond\u0159ej Leng\u00e1l", "Juraj Major", "Ad\u00e9la \u0160t\u011bpkov\u00e1", "Jan Strej\u010dek"], "title": "On Complementation of Nondeterministic Finite Automata without Full Determinization (Technical Report)", "categories": ["cs.FL", "cs.LO"], "comment": "Accepted at FCT'25", "summary": "Complementation of finite automata is a basic operation used in numerous\napplications. The standard way to complement a nondeterministic finite\nautomaton (NFA) is to transform it into an equivalent deterministic finite\nautomaton (DFA) and complement the DFA. The DFA can, however, be exponentially\nlarger than the corresponding NFA. In this paper, we study several alternative\napproaches to complementation, which are based either on reverse powerset\nconstruction or on two novel constructions that exploit a commonly occurring\nstructure of NFAs. Our experiment on a large data set shows that using a\ndifferent than the classical approach can in many cases yield significantly\nsmaller complements.", "AI": {"tldr": "\u7814\u7a76\u4e86\u51e0\u79cd\u57fa\u4e8e\u53cd\u5411\u5e42\u96c6\u6784\u9020\u6216\u65b0\u7ed3\u6784\u7684\u6709\u9650\u81ea\u52a8\u673a\u8865\u5168\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u5c0f\u7684\u8865\u5168\u7ed3\u679c\u3002", "motivation": "\u6709\u9650\u81ea\u52a8\u673a\u8865\u5168\u662f\u57fa\u672c\u64cd\u4f5c\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\uff08NFA\u8f6cDFA\u540e\u8865\u5168\uff09\u53ef\u80fd\u5bfc\u81f4DFA\u6307\u6570\u7ea7\u589e\u5927\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cd\u5411\u5e42\u96c6\u6784\u9020\u6216\u4e24\u79cd\u65b0\u7ed3\u6784\u6784\u5efa\u8865\u5168\u65b9\u6cd5\uff0c\u5229\u7528\u4e86NFA\u7684\u5e38\u89c1\u7ed3\u6784\u3002", "result": "\u5728\u5927\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u53d1\u73b0\uff0c\u65b0\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c0f\u8865\u5168\u540e\u7684\u81ea\u52a8\u673a\u89c4\u6a21\u3002", "conclusion": "\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u65b0\u8865\u5168\u65b9\u6cd5\u80fd\u66f4\u9ad8\u6548\u5730\u751f\u6210\u8f83\u5c0f\u7684\u8865\u5168\u7ed3\u679c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.03243", "pdf": "https://arxiv.org/pdf/2507.03243", "abs": "https://arxiv.org/abs/2507.03243", "authors": ["Zifei Wang", "Emmanuel Abolarin", "Kai Wu", "Venkatarao Rebba", "Jian Hu", "Zhen Hu", "Shan Bao", "Feng Zhou"], "title": "Beyond Charging Anxiety: An Explainable Approach to Understanding User Preferences of EV Charging Stations Using Review Data", "categories": ["cs.HC"], "comment": "19 pages, 8 figures", "summary": "Electric vehicles (EVs) charging infrastructure is directly related to the\noverall EV user experience and thus impacts the widespread adoption of EVs.\nUnderstanding key factors that affect EV users' charging experience is\nessential for building a robust and user-friendly EV charging infrastructure.\nThis study leverages about $17,000$ charging station (CS) reviews on Google\nMaps to explore EV user preferences for charging stations, employing ChatGPT\n4.0 for aspect-based sentiment analysis. We identify twelve key aspects\ninfluencing user satisfaction, ranging from accessibility and reliability to\namenities and pricing. Two distinct preference models are developed: a\nmicro-level model focused on individual user satisfaction and a macro-level\nmodel capturing collective sentiment towards specific charging stations. Both\nmodels utilize the LightGBM algorithm for user preference prediction, achieving\nstrong performance compared to other machine learning approaches. To further\nelucidate the impact of each aspect on user ratings, we employ SHAP (SHapley\nAdditive exPlanations), a game-theoretic approach for interpreting machine\nlearning models. Our findings highlight the significant impact of positive\nsentiment towards \"amenities and location\", coupled with negative sentiment\nregarding \"reliability and maintenance\", on overall user satisfaction. These\ninsights offer actionable guidance to charging station operators, policymakers,\nand EV manufacturers, empowering them to enhance user experience and foster\nwider EV adoption.", "AI": {"tldr": "\u5229\u7528 ChatGPT 4.0 \u5bf9 Google \u5730\u56fe\u4e0a\u7684\u5145\u7535\u7ad9\u8bc4\u8bba\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u7814\u7a76\u5f71\u54cd\u7535\u52a8\u8f66\u7528\u6237\u5145\u7535\u4f53\u9a8c\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u5f00\u53d1\u4e86\u5fae\u89c2\u548c\u5b8f\u89c2\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u7535\u52a8\u8f66\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u5e7f\u6cdb\u91c7\u7528\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e86\u89e3\u5f71\u54cd\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u4f7f\u7528 ChatGPT 4.0 \u5bf9 17,000 \u6761\u5145\u7535\u7ad9\u8bc4\u8bba\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u8bc6\u522b\u5f71\u54cd\u7528\u6237\u6ee1\u610f\u5ea6\u7684 12 \u4e2a\u5173\u952e\u65b9\u9762\uff0c\u5e76\u57fa\u4e8e LightGBM \u7b97\u6cd5\u5f00\u53d1\u5fae\u89c2\u548c\u5b8f\u89c2\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c'\u8bbe\u65bd\u4e0e\u4f4d\u7f6e'\u7684\u79ef\u6781\u60c5\u611f\u548c'\u53ef\u9760\u6027\u4e0e\u7ef4\u62a4'\u7684\u6d88\u6781\u60c5\u611f\u5bf9\u7528\u6237\u6ee1\u610f\u5ea6\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5145\u7535\u7ad9\u8fd0\u8425\u5546\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u7535\u52a8\u8f66\u5236\u9020\u5546\u63d0\u4f9b\u4e86\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u548c\u63a8\u5e7f\u7535\u52a8\u8f66\u7684\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2507.03993", "pdf": "https://arxiv.org/pdf/2507.03993", "abs": "https://arxiv.org/abs/2507.03993", "authors": ["Dipo Dunsin", "Mohamed Chahine Ghanem", "Eduardo Almeida Palmieri"], "title": "MalVol-25: A Diverse, Labelled and Detailed Volatile Memory Dataset for Malware Detection and Response Testing and Validation", "categories": ["cs.CR", "cs.ET", "cs.LG"], "comment": "6 pages", "summary": "This paper addresses the critical need for high-quality malware datasets that\nsupport advanced analysis techniques, particularly machine learning and agentic\nAI frameworks. Existing datasets often lack diversity, comprehensive labelling,\nand the complexity necessary for effective machine learning and agent-based AI\ntraining. To fill this gap, we developed a systematic approach for generating a\ndataset that combines automated malware execution in controlled virtual\nenvironments with dynamic monitoring tools. The resulting dataset comprises\nclean and infected memory snapshots across multiple malware families and\noperating systems, capturing detailed behavioural and environmental features.\nKey design decisions include applying ethical and legal compliance, thorough\nvalidation using both automated and manual methods, and comprehensive\ndocumentation to ensure replicability and integrity. The dataset's distinctive\nfeatures enable modelling system states and transitions, facilitating RL-based\nmalware detection and response strategies. This resource is significant for\nadvancing adaptive cybersecurity defences and digital forensic research. Its\nscope supports diverse malware scenarios and offers potential for broader\napplications in incident response and automated threat mitigation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u8d28\u91cf\u6076\u610f\u8f6f\u4ef6\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\uff0c\u586b\u8865\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u548c\u4ee3\u7406AI\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u7ea7\u5206\u6790\u9700\u6c42\u3002", "method": "\u5728\u865a\u62df\u73af\u5883\u4e2d\u81ea\u52a8\u6267\u884c\u6076\u610f\u8f6f\u4ef6\uff0c\u7ed3\u5408\u52a8\u6001\u76d1\u63a7\u5de5\u5177\u751f\u6210\u591a\u6837\u5316\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u591a\u79cd\u6076\u610f\u8f6f\u4ef6\u5bb6\u65cf\u548c\u64cd\u4f5c\u7cfb\u7edf\uff0c\u652f\u6301RL\u68c0\u6d4b\u4e0e\u54cd\u5e94\u7b56\u7565\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u63a8\u52a8\u81ea\u9002\u5e94\u7f51\u7edc\u5b89\u5168\u9632\u5fa1\u548c\u6570\u5b57\u53d6\u8bc1\u7814\u7a76\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.04357", "pdf": "https://arxiv.org/pdf/2507.04357", "abs": "https://arxiv.org/abs/2507.04357", "authors": ["Zareh Chahoki Atefeh", "Roveri Marco"], "title": "Static Analysis for Detecting Transaction Conflicts in Ethereum Smart Contracts", "categories": ["cs.DC", "cs.CR"], "comment": null, "summary": "Ethereum smart contracts operate in a concurrent environment where multiple\ntransactions can be submitted simultaneously. However, the Ethereum Virtual\nMachine (EVM) enforces sequential execution of transactions within each block\nto prevent conflicts arising from concurrent access to the same state\nvariables. Although this approach guarantees correct behavior, it limits the\nability of validators to leverage multi-core architectures for faster\ntransaction processing, thus restricting throughput. Existing solutions\nintroduce concurrency by allowing simultaneous transaction execution combined\nwith runtime conflict detection and rollback mechanisms to maintain\ncorrectness. However, these methods incur significant overhead due to\ncontinuous conflict tracking and transaction reversion. Recently, alternative\napproaches have emerged that aim to predict conflicts statically, before\nexecution, by analyzing smart contract code for potential transaction\ninteractions. Despite their promise, there is a lack of comprehensive studies\nthat examine static conflict detection and its broader implications in specific\nsmart contracts. This paper fills this important gap by proposing a novel\nstatic analysis method to detect potential transaction conflicts in Ethereum\nsmart contracts. Our method identifies read-write, write-write, and function\ncall conflicts between transaction pairs by analyzing state variable access\npatterns in Solidity contracts. We implement a tool that parses contract code\nand performs conflict detection. Evaluation on a dataset of real-world Ethereum\nsmart contracts demonstrates that our approach achieves high precision in\nidentifying potential conflicts. By enabling proactive conflict detection, our\ntool supports further design of transaction scheduling strategies that reduce\nruntime failures, enhance validator throughput, and contribute to blockchain\nscalability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u4ee5\u592a\u574a\u667a\u80fd\u5408\u7ea6\u4e2d\u7684\u6f5c\u5728\u4ea4\u6613\u51b2\u7a81\uff0c\u901a\u8fc7\u5206\u6790\u72b6\u6001\u53d8\u91cf\u7684\u8bbf\u95ee\u6a21\u5f0f\uff0c\u6709\u6548\u8bc6\u522b\u51b2\u7a81\uff0c\u63d0\u5347\u533a\u5757\u94fe\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4ee5\u592a\u574a\u667a\u80fd\u5408\u7ea6\u5728\u5904\u7406\u5e76\u53d1\u4ea4\u6613\u65f6\u9700\u987a\u5e8f\u6267\u884c\u4ee5\u4fdd\u8bc1\u6b63\u786e\u6027\uff0c\u4f46\u9650\u5236\u4e86\u591a\u6838\u67b6\u6784\u7684\u5229\u7528\uff0c\u964d\u4f4e\u4e86\u541e\u5410\u91cf\u3002\u73b0\u6709\u52a8\u6001\u51b2\u7a81\u68c0\u6d4b\u65b9\u6cd5\u5f00\u9500\u8f83\u5927\uff0c\u800c\u9759\u6001\u9884\u6d4b\u65b9\u6cd5\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790Solidity\u5408\u7ea6\u4e2d\u7684\u72b6\u6001\u53d8\u91cf\u8bbf\u95ee\u6a21\u5f0f\uff0c\u8bc6\u522b\u8bfb\u5199\u3001\u5199\u5199\u548c\u51fd\u6570\u8c03\u7528\u51b2\u7a81\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u89e3\u6790\u5de5\u5177\u3002", "result": "\u5728\u771f\u5b9e\u4ee5\u592a\u574a\u667a\u80fd\u5408\u7ea6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u6f5c\u5728\u51b2\u7a81\u65b9\u9762\u5177\u6709\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u5177\u652f\u6301\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u4ea4\u6613\u8c03\u5ea6\u7b56\u7565\uff0c\u51cf\u5c11\u8fd0\u884c\u65f6\u6545\u969c\uff0c\u63d0\u5347\u541e\u5410\u91cf\uff0c\u4ece\u800c\u63a8\u52a8\u533a\u5757\u94fe\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.03868", "pdf": "https://arxiv.org/pdf/2507.03868", "abs": "https://arxiv.org/abs/2507.03868", "authors": ["Xinyi Wu", "Yanhao Jia", "Luwei Xiao", "Shuai Zhao", "Fengkuang Chiang", "Erik Cambria"], "title": "From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM", "categories": ["cs.AI", "cs.CE", "cs.CY", "cs.MM"], "comment": null, "summary": "In AI-facilitated teaching, leveraging various query styles to interpret\nabstract educational content is crucial for delivering effective and accessible\nlearning experiences. However, existing retrieval systems predominantly focus\non natural text-image matching and lack the capacity to address the diversity\nand ambiguity inherent in real-world educational scenarios. To address this\nlimitation, we develop a lightweight and efficient multi-modal retrieval\nmodule, named Uni-Retrieval, which extracts query-style prototypes and\ndynamically matches them with tokens from a continually updated Prompt Bank.\nThis Prompt Bank encodes and stores domain-specific knowledge by leveraging a\nMixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to\nenhance Uni-Retrieval's capability to accommodate unseen query types at test\ntime. To enable natural language educational content generation, we integrate\nthe original Uni-Retrieval with a compact instruction-tuned language model,\nforming a complete retrieval-augmented generation pipeline named Uni-RAG. Given\na style-conditioned query, Uni-RAG first retrieves relevant educational\nmaterials and then generates human-readable explanations, feedback, or\ninstructional content aligned with the learning objective. Experimental results\non SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline\nretrieval and RAG systems in both retrieval accuracy and generation quality,\nwhile maintaining low computational cost. Our framework provides a scalable,\npedagogically grounded solution for intelligent educational systems, bridging\nretrieval and generation to support personalized, explainable, and efficient\nlearning assistance across diverse STEM scenarios.", "AI": {"tldr": "Uni-RAG\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u68c0\u7d22\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u4e0e\u751f\u6210\u6280\u672f\uff0c\u63d0\u5347\u4e86\u6559\u80b2\u573a\u666f\u4e2d\u7684\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u7cfb\u7edf\u65e0\u6cd5\u5904\u7406\u6559\u80b2\u573a\u666f\u4e2d\u7684\u591a\u6837\u6027\u548c\u6a21\u7cca\u6027\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86Uni-Retrieval\u6a21\u5757\uff0c\u7ed3\u5408MoE-LoRA\u6280\u672f\u52a8\u6001\u5339\u914d\u67e5\u8be2\u4e0ePrompt Bank\uff0c\u5e76\u4e0e\u6307\u4ee4\u8c03\u4f18\u7684\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5f62\u6210Uni-RAG\u3002", "result": "\u5728SER\u7b49\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUni-RAG\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "Uni-RAG\u4e3a\u667a\u80fd\u6559\u80b2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u57fa\u4e8e\u6559\u5b66\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u8de8STEM\u573a\u666f\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u3002"}}
{"id": "2507.04390", "pdf": "https://arxiv.org/pdf/2507.04390", "abs": "https://arxiv.org/abs/2507.04390", "authors": ["Vanesya Aura Ardity", "Yusuf Sulistyo Nugroho", "Syful Islam"], "title": "Exploring React Library Related Questions on Stack Overflow: Answered vs. Unanswered", "categories": ["cs.SE"], "comment": "6 pages, 9 figures, 7 tables, conference paper", "summary": "React is a popular JavaScript framework in modern web application\ndevelopment. Due to its high performance and efficiency, many developers use\nthis framework. Although React library offers many advantages, it is not\nwithout its challenges. When using React library, developers often face\nproblems where they often seek solutions through question-and-answer forums,\nsuch as Stack Overflow (SO). However, despite its high popularity, many\nReact-related questions on SO remain unanswered. Thus, this study aims to\nanalyze the factors associated with question answerability and difficulty\nlevels of React-related questions on SO. To facilitate our study, Exploratory\nData Analysis was applied to 534,820 questions, where they are filtered based\non 23 React-related tags. We implemented a quantitative approach through text\nmining and statistical analysis. A logistic regression model was used to\nidentify attributes associated with question answerability, while a simple\nlinear regression model was employed to examine the correlation between user\nreputations and performance difficulty scores (PD Score). The results show that\nsome attributes, such as number of views, code snippet inclusion, number of\nlines of code, and user reputation, positively affect the likelihood of\nquestion answerability. In contrast, the number of comments, question lengths,\nand presence of images in React-related questions reduce the probability of a\nquestion receiving responses from users. Further investigation indicates a\nnegative correlation between user reputations and PD Score, where reputation\nincrease corresponds to -0.092 reduction in PD score, signaling experienced\nusers tend to propose more complex technical inquiries. This study provides\ninsights into the characteristics of technical question-and-answer platforms,\nsuch as SO, that users need to consider the answerability factors when posting\nquestions related to React.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86Stack Overflow\u4e0aReact\u76f8\u5173\u95ee\u9898\u56de\u7b54\u7387\u548c\u96be\u5ea6\u7684\u5173\u8054\u56e0\u7d20\uff0c\u53d1\u73b0\u89c6\u56fe\u6570\u3001\u4ee3\u7801\u7247\u6bb5\u3001\u7528\u6237\u58f0\u8a89\u7b49\u6b63\u9762\u5f71\u54cd\u56de\u7b54\u7387\uff0c\u800c\u8bc4\u8bba\u6570\u3001\u95ee\u9898\u957f\u5ea6\u548c\u56fe\u7247\u5219\u964d\u4f4e\u56de\u7b54\u6982\u7387\u3002", "motivation": "\u5c3d\u7ba1React\u6d41\u884c\uff0c\u4f46\u8bb8\u591a\u76f8\u5173\u95ee\u9898\u5728Stack Overflow\u4e0a\u4ecd\u672a\u89e3\u7b54\uff0c\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u6587\u672c\u6316\u6398\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u5305\u62ec\u903b\u8f91\u56de\u5f52\u5206\u6790\u56de\u7b54\u7387\u548c\u7ebf\u6027\u56de\u5f52\u5206\u6790\u7528\u6237\u58f0\u8a89\u4e0e\u95ee\u9898\u96be\u5ea6\u7684\u5173\u7cfb\u3002", "result": "\u4ee3\u7801\u7247\u6bb5\u548c\u7528\u6237\u58f0\u8a89\u63d0\u5347\u56de\u7b54\u7387\uff0c\u8bc4\u8bba\u548c\u95ee\u9898\u957f\u5ea6\u5219\u964d\u4f4e\u56de\u7b54\u6982\u7387\uff1b\u7528\u6237\u58f0\u8a89\u8d8a\u9ad8\uff0c\u95ee\u9898\u96be\u5ea6\u8d8a\u4f4e\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6280\u672f\u95ee\u7b54\u5e73\u53f0\u7528\u6237\u63d0\u4f9b\u4e86\u63d0\u95ee\u7b56\u7565\u53c2\u8003\uff0c\u9700\u5173\u6ce8\u56de\u7b54\u7387\u76f8\u5173\u56e0\u7d20\u3002"}}
{"id": "2507.03839", "pdf": "https://arxiv.org/pdf/2507.03839", "abs": "https://arxiv.org/abs/2507.03839", "authors": ["Shuowen Li", "Kexin Wang", "Minglu Fang", "Danqi Huang", "Ali Asadipour", "Haipeng Mi", "Yitong Sun"], "title": "Participatory Evolution of Artificial Life Systems via Semantic Feedback", "categories": ["cs.AI", "cs.GR"], "comment": "10 pages", "summary": "We present a semantic feedback framework that enables natural language to\nguide the evolution of artificial life systems. Integrating a\nprompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the\nsystem allows user intent to modulate both visual outcomes and underlying\nbehavioral rules. Implemented in an interactive ecosystem simulation, the\nframework supports prompt refinement, multi-agent interaction, and emergent\nrule synthesis. User studies show improved semantic alignment over manual\ntuning and demonstrate the system's potential as a platform for participatory\ngenerative design and open-ended evolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u53cd\u9988\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u4eba\u5de5\u751f\u547d\u7cfb\u7edf\u7684\u6f14\u5316\uff0c\u7ed3\u5408\u63d0\u793a\u7f16\u7801\u3001\u4f18\u5316\u5668\u548c\u8bc4\u4f30\u6a21\u5757\uff0c\u5b9e\u73b0\u7528\u6237\u610f\u56fe\u5bf9\u7cfb\u7edf\u884c\u4e3a\u7684\u8c03\u63a7\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u66f4\u76f4\u89c2\u5730\u6307\u5bfc\u4eba\u5de5\u751f\u547d\u7cfb\u7edf\u7684\u6f14\u5316\u548c\u884c\u4e3a\u8bbe\u8ba1\uff0c\u51cf\u5c11\u624b\u52a8\u8c03\u6574\u7684\u590d\u6742\u6027\u3002", "method": "\u7ed3\u5408\u63d0\u793a\u7f16\u7801\u5668\uff08prompt-to-parameter\uff09\u3001CMA-ES\u4f18\u5316\u5668\u548cCLIP\u8bc4\u4f30\u6a21\u5757\uff0c\u6784\u5efa\u4ea4\u4e92\u5f0f\u751f\u6001\u7cfb\u7edf\u6a21\u62df\uff0c\u652f\u6301\u63d0\u793a\u7ec6\u5316\u3001\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u548c\u89c4\u5219\u5408\u6210\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u624b\u52a8\u8c03\u6574\uff0c\u5177\u5907\u53c2\u4e0e\u5f0f\u751f\u6210\u8bbe\u8ba1\u548c\u5f00\u653e\u6f14\u5316\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4eba\u5de5\u751f\u547d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u76f4\u89c2\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u65b9\u5f0f\uff0c\u9002\u7528\u4e8e\u751f\u6210\u8bbe\u8ba1\u548c\u5f00\u653e\u5f0f\u6f14\u5316\u7814\u7a76\u3002"}}
{"id": "2507.04589", "pdf": "https://arxiv.org/pdf/2507.04589", "abs": "https://arxiv.org/abs/2507.04589", "authors": ["Zien Wang", "Xiucheng Wang", "Nan Cheng", "Wenchao Xu", "Wei Quan", "Ruijin Sun", "Conghao Zhou"], "title": "On-Demand Multimedia Delivery in 6G: An Optimal-Cost Steiner Tree Approach", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "The exponential growth of multimedia data traffic in 6G networks poses\nunprecedented challenges for immersive communication, where\nultra-high-definition, multi-quality streaming must be delivered on demand\nwhile minimizing network operational costs. Traditional routing approaches,\nsuch as shortest-path algorithms, fail to optimize flow multiplexing across\nmultiple destinations, while conventional Steiner tree methods cannot\naccommodate heterogeneous quality-of-service (QoS) requirements-a critical need\nfor 6G's personalized services. In this paper, we address a fundamental but\nunsolved challenge: the minimum flow problem (MFP) with multi-destination,\nheterogeneous outflow demands, which is pivotal for efficient multimedia\ndistribution such as adaptive-resolution video streaming. To overcome the\nlimitations of existing methods, we propose a two-stage dynamic\nprogramming-enhanced On-demand Steiner Tree (OST) algorithm, the first approach\nthat jointly optimizes flow aggregation and QoS-aware path selection for\narbitrary outflow requirements. We rigorously prove the optimality of OST using\nmathematical induction, demonstrating that it guarantees the minimum-cost\nmulticast flow under differentiated service constraints. Extensive experiments\nin 6G-like multimedia transmission scenarios show that OST reduces total\nnetwork flow by over 10% compared to state-of-the-art methods while ensuring\non-demand QoS fulfillment. The complete code is available at\nhttps://github.com/UNIC-Lab/OST.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u52a8\u6001\u89c4\u5212\u589e\u5f3a\u578b\u6309\u9700Steiner\u6811\uff08OST\uff09\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e866G\u7f51\u7edc\u4e2d\u591a\u5a92\u4f53\u6570\u636e\u6d41\u7684\u6700\u5c0f\u6d41\u91cf\u95ee\u9898\uff08MFP\uff09\uff0c\u4f18\u5316\u4e86\u591a\u76ee\u6807\u548c\u5f02\u6784\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u9700\u6c42\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\u591a\u5a92\u4f53\u6570\u636e\u6d41\u91cf\u7684\u6fc0\u589e\u5bf9\u6309\u9700\u4ea4\u4ed8\u8d85\u9ad8\u6e05\u3001\u591a\u8d28\u91cf\u6d41\u5a92\u4f53\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u4f20\u7edf\u8def\u7531\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u5f02\u6784QoS\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u52a8\u6001\u89c4\u5212\u589e\u5f3a\u578bOST\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u6d41\u805a\u5408\u548cQoS\u611f\u77e5\u8def\u5f84\u9009\u62e9\u3002", "result": "OST\u57286G\u7c7b\u4f3c\u573a\u666f\u4e2d\u51cf\u5c11\u4e8610%\u4ee5\u4e0a\u7684\u603b\u7f51\u7edc\u6d41\u91cf\uff0c\u5e76\u6ee1\u8db3QoS\u9700\u6c42\u3002", "conclusion": "OST\u7b97\u6cd5\u4e3a6G\u7f51\u7edc\u4e2d\u7684\u9ad8\u6548\u591a\u5a92\u4f53\u5206\u53d1\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03827", "pdf": "https://arxiv.org/pdf/2507.03827", "abs": "https://arxiv.org/abs/2507.03827", "authors": ["Christine Ga\u00dfner"], "title": "Abstract computation over first-order structures. Part IIb: Moschovakis' operator and other non-determinisms", "categories": ["math.LO", "cs.LO"], "comment": "43 pages", "summary": "BSS RAMs were introduced to provide a mathematical framework for\ncharacterizing algorithms over first-order structures. Non-deterministic BSS\nRAMs help to model different non-deterministic approaches. Here, we deal with\ndifferent types of binary non-determinisms and study the consequences of the\ndecidability of the identity relation and the decidability of finite sets\nconsisting of one or two constants. We compare the binary non-determinism\nresulting from a non-deterministic branching process, the digital\nnon-determinism resulting from the restriction of guesses to two constants, and\nsome other non-determinisms resulting from the use of Moschovakis' operator\napplied to oracle sets restricted to tuples of constants. Moreover, we show\nthat the performance capability and the efficiency of individual machines are\ninfluenced by the following properties. 1. The identity relation belongs to the\nunderlying structure. 2. The identity is semi-decidable over the underlying\nstructure. 3. Two single-element sets of constants are semi-decidable. 4. A set\nof two constants is semi-decidable. The order of these properties corresponds\nto the strength of their influence. In all cases mentioned, the\nsemi-decidability of the sets implies their decidability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86BSS RAMs\u4e2d\u4e0d\u540c\u7c7b\u578b\u7684\u4e8c\u8fdb\u5236\u975e\u786e\u5b9a\u6027\uff0c\u5e76\u5206\u6790\u4e86\u8eab\u4efd\u5173\u7cfb\u548c\u6709\u9650\u5e38\u6570\u96c6\u7684\u53ef\u5224\u5b9a\u6027\u5bf9\u673a\u5668\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u4e3aBSS RAMs\u63d0\u4f9b\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u4ee5\u523b\u753b\u5728\u4e00\u9636\u7ed3\u6784\u4e0a\u7684\u7b97\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e0d\u540c\u975e\u786e\u5b9a\u6027\u6a21\u578b\u7684\u5f71\u54cd\u3002", "method": "\u6bd4\u8f83\u4e86\u7531\u975e\u786e\u5b9a\u6027\u5206\u652f\u8fc7\u7a0b\u4ea7\u751f\u7684\u4e8c\u8fdb\u5236\u975e\u786e\u5b9a\u6027\u3001\u6570\u5b57\u975e\u786e\u5b9a\u6027\u4ee5\u53ca\u5176\u4ed6\u975e\u786e\u5b9a\u6027\uff0c\u5e76\u5206\u6790\u4e86\u8eab\u4efd\u5173\u7cfb\u548c\u5e38\u6570\u96c6\u7684\u534a\u53ef\u5224\u5b9a\u6027\u5bf9\u673a\u5668\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u8eab\u4efd\u5173\u7cfb\u548c\u5e38\u6570\u96c6\u7684\u534a\u53ef\u5224\u5b9a\u6027\u4f1a\u663e\u8457\u5f71\u54cd\u673a\u5668\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e14\u5176\u987a\u5e8f\u5bf9\u5e94\u5f71\u54cd\u5f3a\u5ea6\u3002", "conclusion": "\u8bba\u6587\u8868\u660e\uff0c\u7279\u5b9a\u6027\u8d28\uff08\u5982\u8eab\u4efd\u5173\u7cfb\u548c\u5e38\u6570\u96c6\u7684\u534a\u53ef\u5224\u5b9a\u6027\uff09\u5bf9BSS RAMs\u7684\u6027\u80fd\u548c\u6548\u7387\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4e14\u8fd9\u4e9b\u6027\u8d28\u7684\u53ef\u5224\u5b9a\u6027\u662f\u5173\u952e\u3002"}}
{"id": "2507.04139", "pdf": "https://arxiv.org/pdf/2507.04139", "abs": "https://arxiv.org/abs/2507.04139", "authors": ["Mahdi Rezaei", "Mohsen Azarmi"], "title": "Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG", "cs.RO"], "comment": "8 pages, 4 Figures, 4 Tables. Accepted at IEEE IV 2025", "summary": "Ensuring safe transition of control in automated vehicles requires an\naccurate and timely assessment of driver readiness. This paper introduces\nDriver-Net, a novel deep learning framework that fuses multi-camera inputs to\nestimate driver take-over readiness. Unlike conventional vision-based driver\nmonitoring systems that focus on head pose or eye gaze, Driver-Net captures\nsynchronised visual cues from the driver's head, hands, and body posture\nthrough a triple-camera setup. The model integrates spatio-temporal data using\na dual-path architecture, comprising a Context Block and a Feature Block,\nfollowed by a cross-modal fusion strategy to enhance prediction accuracy.\nEvaluated on a diverse dataset collected from the University of Leeds Driving\nSimulator, the proposed method achieves an accuracy of up to 95.8% in driver\nreadiness classification. This performance significantly enhances existing\napproaches and highlights the importance of multimodal and multi-view fusion.\nAs a real-time, non-intrusive solution, Driver-Net contributes meaningfully to\nthe development of safer and more reliable automated vehicles and aligns with\nnew regulatory mandates and upcoming safety standards.", "AI": {"tldr": "Driver-Net \u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u591a\u6444\u50cf\u5934\u878d\u5408\u6765\u8bc4\u4f30\u9a7e\u9a76\u5458\u63a5\u7ba1\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u51c6\u5907\u72b6\u6001\uff0c\u901a\u8fc7\u65f6\u7a7a\u6570\u636e\u6574\u5408\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u8fbe\u5230\u4e8695.8%\u7684\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63a7\u5236\u6743\u5b89\u5168\u8f6c\u79fb\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u9a7e\u9a76\u5458\u76d1\u63a7\u7cfb\u7edf\u4ec5\u4f9d\u8d56\u5934\u90e8\u6216\u773c\u90e8\u59ff\u6001\u662f\u4e0d\u591f\u7684\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u89c6\u89c9\u7ebf\u7d22\u3002", "method": "Driver-Net \u91c7\u7528\u4e09\u6444\u50cf\u5934\u7684\u591a\u89c6\u89d2\u8f93\u5165\uff0c\u6355\u6349\u9a7e\u9a76\u5458\u5934\u90e8\u3001\u624b\u90e8\u548c\u8eab\u4f53\u59ff\u6001\u7684\u540c\u6b65\u89c6\u89c9\u4fe1\u606f\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u67b6\u6784\uff08Context Block \u548c Feature Block\uff09\u548c\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728\u5229\u5179\u5927\u5b66\u9a7e\u9a76\u6a21\u62df\u5668\u6536\u96c6\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\uff0cDriver-Net \u7684\u9a7e\u9a76\u5458\u51c6\u5907\u72b6\u6001\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523095.8%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Driver-Net \u662f\u4e00\u4e2a\u5b9e\u65f6\u3001\u975e\u4fb5\u5165\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6027\uff0c\u7b26\u5408\u65b0\u7684\u76d1\u7ba1\u8981\u6c42\u548c\u5b89\u5168\u6807\u51c6\u3002"}}
{"id": "2507.04420", "pdf": "https://arxiv.org/pdf/2507.04420", "abs": "https://arxiv.org/abs/2507.04420", "authors": ["Mohsen Koohi Esfahani"], "title": "Skipper: Maximal Matching with a Single Pass over Edges", "categories": ["cs.DC"], "comment": null, "summary": "Maximal Matching (MM) is a fundamental graph problem with diverse\napplications. However, state-of-the-art parallel MM algorithms are limited by\ntheir need to process graph edges repeatedly over multiple iterations.\nFurthermore, optimized algorithms often require additional memory for graph\ncontraction or edge filtering. In this paper, we introduce Skipper, an\nincremental asynchronous MM algorithm that (i) processes each edge\ndeterministically and only once, (ii) skips a large fraction of edges during\nprocessing, and (iii) minimizes memory space utilization. Notably, Skipper\nrequires (a) a single pass over the edges, and (b) only a single byte of memory\nspace per vertex. Our evaluation of Skipper, using both real-world and\nsynthetic graphs with up to 161 billion edges, and across three different\ncomputer architectures, shows that Skipper processes only 1.2% of the edges and\ndelivers a 47.1 times average speedup (geometric mean). Moreover, Skipper's\noutput quality is highly competitive, with an average size of 88.6% relative to\nthe output of the Lim-Chung algorithm as a state-of-the-art MM algorithm with\nthe largest output size.", "AI": {"tldr": "Skipper\u662f\u4e00\u79cd\u589e\u91cf\u5f02\u6b65\u6700\u5927\u5339\u914d\u7b97\u6cd5\uff0c\u80fd\u5355\u6b21\u5904\u7406\u6bcf\u6761\u8fb9\u4e14\u8df3\u8fc7\u5927\u91cf\u8fb9\u5904\u7406\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5185\u5b58\u4f7f\u7528\uff0c\u5b9e\u73b0\u9ad8\u8fbe47.1\u500d\u7684\u5e73\u5747\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u6700\u5927\u5339\u914d\u7b97\u6cd5\u9700\u591a\u6b21\u5904\u7406\u8fb9\u4e14\u5360\u7528\u989d\u5916\u5185\u5b58\uff0cSkipper\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Skipper\u91c7\u7528\u589e\u91cf\u5f02\u6b65\u65b9\u5f0f\uff0c\u5355\u6b21\u5904\u7406\u8fb9\u5e76\u8df3\u8fc7\u5927\u90e8\u5206\u8fb9\uff0c\u4ec5\u9700\u6bcf\u4e2a\u9876\u70b91\u5b57\u8282\u5185\u5b58\u3002", "result": "\u57281610\u4ebf\u8fb9\u7684\u56fe\u4e2d\uff0cSkipper\u4ec5\u5904\u74061.2%\u7684\u8fb9\uff0c\u5b9e\u73b047.1\u500d\u52a0\u901f\uff0c\u5339\u914d\u8d28\u91cf\u8fbe88.6%\u3002", "conclusion": "Skipper\u5728\u6027\u80fd\u548c\u8f93\u51fa\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u56fe\u5904\u7406\u3002"}}
{"id": "2507.03902", "pdf": "https://arxiv.org/pdf/2507.03902", "abs": "https://arxiv.org/abs/2507.03902", "authors": ["Dani Paul Hove", "Benjamin Watson"], "title": "The shortcomings of video conferencing technology, methods for revealing them, and emerging XR solutions", "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Video conferencing has become a central part of our daily lives, thanks to\nthe COVID-19 pandemic. Unfortunately, so have its many limitations, resulting\nin poor support for communicative and social behavior and ultimately, Zoom\nfatigue. New technologies will be required to address these limitations,\nincluding many drawn from mixed reality (XR). In this paper, our goals are to\nequip and encourage future researchers to develop and test such technologies.\nToward this end, we first survey research on the shortcomings of video\nconferencing systems, as defined before and after the pandemic. We then\nconsider the methods that research uses to evaluate support for communicative\nbehavior, and argue that those same methods should be employed in identifying,\nimproving, and validating promising video conferencing technologies. Next, we\nsurvey emerging XR solutions to video conferencing's limitations, most off\nwhich do not employ head-mounted displays.", "AI": {"tldr": "\u89c6\u9891\u4f1a\u8bae\u5728COVID-19\u75ab\u60c5\u671f\u95f4\u6210\u4e3a\u65e5\u5e38\u751f\u6d3b\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5176\u5c40\u9650\u6027\uff08\u5982\u793e\u4ea4\u884c\u4e3a\u652f\u6301\u4e0d\u8db3\u548c\u201cZoom\u75b2\u52b3\u201d\uff09\u9700\u8981\u65b0\u6280\u672f\uff08\u5c24\u5176\u662f\u6df7\u5408\u73b0\u5b9e\u6280\u672f\uff09\u6765\u89e3\u51b3\u3002\u672c\u6587\u65e8\u5728\u63a8\u52a8\u7814\u7a76\u5e76\u6d4b\u8bd5\u8fd9\u4e9b\u6280\u672f\u3002", "motivation": "\u7531\u4e8e\u89c6\u9891\u4f1a\u8bae\u5728\u75ab\u60c5\u671f\u95f4\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5c40\u9650\u6027\uff08\u5982\u793e\u4ea4\u884c\u4e3a\u652f\u6301\u4e0d\u8db3\u548c\u201cZoom\u75b2\u52b3\u201d\uff09\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u901a\u8fc7\u65b0\u6280\u672f\uff08\u5c24\u5176\u662f\u6df7\u5408\u73b0\u5b9e\u6280\u672f\uff09\u6765\u89e3\u51b3\u3002", "method": "\u672c\u6587\u9996\u5148\u8c03\u7814\u4e86\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u7684\u7f3a\u9677\uff08\u75ab\u60c5\u524d\u540e\uff09\uff0c\u7136\u540e\u63a2\u8ba8\u4e86\u8bc4\u4f30\u793e\u4ea4\u884c\u4e3a\u652f\u6301\u7684\u7814\u7a76\u65b9\u6cd5\uff0c\u5e76\u5efa\u8bae\u8fd9\u4e9b\u65b9\u6cd5\u5e94\u7528\u4e8e\u65b0\u6280\u672f\u7684\u5f00\u53d1\u548c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6df7\u5408\u73b0\u5b9e\uff08XR\uff09\u6280\u672f\uff08\u5c24\u5176\u662f\u975e\u5934\u6234\u5f0f\u8bbe\u5907\uff09\u53ef\u4ee5\u89e3\u51b3\u89c6\u9891\u4f1a\u8bae\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u547c\u5401\u672a\u6765\u7814\u7a76\u5f00\u53d1\u5e76\u6d4b\u8bd5\u65b0\u6280\u672f\uff08\u5c24\u5176\u662fXR\u6280\u672f\uff09\u4ee5\u6539\u8fdb\u89c6\u9891\u4f1a\u8bae\u4f53\u9a8c\u3002"}}
{"id": "2507.04422", "pdf": "https://arxiv.org/pdf/2507.04422", "abs": "https://arxiv.org/abs/2507.04422", "authors": ["Guoming Long", "Jingzhi Gong", "Hui Fang", "Tao Chen"], "title": "Learning Software Bug Reports: A Systematic Literature Review", "categories": ["cs.SE", "cs.AI", "D.2.7; I.2.7"], "comment": "Accepted by TOSEM", "summary": "The recent advancement of artificial intelligence, especially machine\nlearning (ML), has significantly impacted software engineering research,\nincluding bug report analysis. ML aims to automate the understanding,\nextraction, and correlation of information from bug reports. Despite its\ngrowing importance, there has been no comprehensive review in this area. In\nthis paper, we present a systematic literature review covering 1,825 papers,\nselecting 204 for detailed analysis. We derive seven key findings: 1) Extensive\nuse of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like\nBERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular\nfor feature representation, with a rise in deep learning approaches. 3) Stop\nword removal is the most common preprocessing, with structural methods rising\nafter 2020. 4) Eclipse and Mozilla are the most frequently evaluated software\nprojects. 5) Bug categorization is the most common task, followed by bug\nlocalization and severity prediction. 6) There is increasing attention on\nspecific bugs like non-functional and performance bugs. 7) Common evaluation\nmetrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold\ncross-validation preferred for model evaluation. 8) Many studies lack robust\nstatistical tests. We also identify six promising future research directions to\nprovide useful insights for practitioners.", "AI": {"tldr": "\u8bba\u6587\u8fdb\u884c\u4e86\u5173\u4e8e\u673a\u5668\u5b66\u4e60\u5728bug\u62a5\u544a\u5206\u6790\u4e2d\u7684\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u7684\u8d8b\u52bf\u548c\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5bf9bug\u62a5\u544a\u5206\u6790\u9886\u57df\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5bf91,825\u7bc7\u8bba\u6587\u7684\u7cfb\u7edf\u7b5b\u9009\uff0c\u9009\u53d6204\u7bc7\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff0c\u603b\u7ed3\u4e86\u4e03\u9879\u5173\u952e\u53d1\u73b0\u3002", "result": "\u53d1\u73b0CNN\u3001LSTM\u7b49\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46BERT\u7b49\u590d\u6742\u6a21\u578b\u4f7f\u7528\u8f83\u5c11\uff1bWord2Vec\u548cTF-IDF\u6d41\u884c\uff1bEclipse\u548cMozilla\u662f\u6700\u5e38\u7814\u7a76\u7684\u9879\u76ee\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6709\u7528\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u516d\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.04236", "pdf": "https://arxiv.org/pdf/2507.04236", "abs": "https://arxiv.org/abs/2507.04236", "authors": ["Md Dilshadur Rahman", "Md Rahat-uz- Zaman", "Andrew McNutt", "Paul Rosen"], "title": "AnnoGram: An Annotative Grammar of Graphics Extension", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Annotations are central to effective data communication, yet most\nvisualization tools treat them as secondary constructs -- manually defined,\ndifficult to reuse, and loosely coupled to the underlying visualization\ngrammar. We propose a declarative extension to Wilkinson's Grammar of Graphics\nthat reifies annotations as first-class design elements, enabling structured\nspecification of annotation targets, types, and positioning strategies. To\ndemonstrate the utility of our approach, we develop a prototype extension\ncalled Vega-Lite Annotation. Through comparison with eight existing tools, we\nshow that our approach enhances expressiveness, reduces authoring effort, and\nenables portable, semantically integrated annotation workflows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u5c06\u6ce8\u91ca\u4f5c\u4e3a\u4e00\u7b49\u8bbe\u8ba1\u5143\u7d20\uff0c\u589e\u5f3a\u4e86\u6570\u636e\u53ef\u89c6\u5316\u7684\u8868\u8fbe\u6027\u548c\u4fbf\u643a\u6027\u3002", "motivation": "\u73b0\u6709\u53ef\u89c6\u5316\u5de5\u5177\u5c06\u6ce8\u91ca\u4f5c\u4e3a\u6b21\u8981\u7ed3\u6784\uff0c\u96be\u4ee5\u590d\u7528\u4e14\u4e0e\u53ef\u89c6\u5316\u8bed\u6cd5\u677e\u6563\u8026\u5408\uff0c\u5f71\u54cd\u4e86\u6570\u636e\u6c9f\u901a\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528Wilkinson\u7684\u56fe\u5f62\u8bed\u6cd5\u6269\u5c55\uff0c\u901a\u8fc7\u58f0\u660e\u5f0f\u65b9\u6cd5\u5c06\u6ce8\u91ca\u76ee\u6807\u3001\u7c7b\u578b\u548c\u5b9a\u4f4d\u7b56\u7565\u7ed3\u6784\u5316\u3002", "result": "\u5f00\u53d1\u4e86Vega-Lite Annotation\u539f\u578b\uff0c\u4e0e8\u79cd\u73b0\u6709\u5de5\u5177\u5bf9\u6bd4\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u8868\u8fbe\u6027\u3001\u51cf\u5c11\u4e86\u521b\u4f5c\u5de5\u4f5c\u91cf\u5e76\u5b9e\u73b0\u4e86\u4fbf\u643a\u7684\u6ce8\u91ca\u5de5\u4f5c\u6d41\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u6ce8\u91ca\u63d0\u5347\u4e3a\u4e00\u7b49\u8bbe\u8ba1\u5143\u7d20\uff0c\u4e3a\u6570\u636e\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6ce8\u91ca\u652f\u6301\u3002"}}
{"id": "2507.04734", "pdf": "https://arxiv.org/pdf/2507.04734", "abs": "https://arxiv.org/abs/2507.04734", "authors": ["Mathieu Leonardon", "Mohammed El Houcine Ayoubi", "Adrien Cassagne", "Romain Tajan", "Camille Leroux"], "title": "Low-Latency Software Polar Encoders and Decoders for Short Blocklengths", "categories": ["cs.NI"], "comment": null, "summary": "This paper presents our low-latency Polar code encoders and decoders\ndeveloped for the 2025 International Symposium on Topics in Coding (ISTC 2025)\ncontest, which challenges participants to implement the fastest possible\nchannel code encoders and decoders in terms of average and maximum latency on a\nCPU target. Our solution is based on Polar codes with an Adaptive Successive\nCancellation List (ASCL) decoder. We introduce a novel ASCL unrolled decoder\ngenerator. We conduct an extensive exploration of the design space, including\ncode construction, CRC selection, and list size, to identify optimal trade-offs\nbetween signal-to-noise ratio and decoding time across various operating\npoints. The considered operating points are frame error rates of 10^{-3} and\n10^{-5}, information bit lengths of 64, 128, 256, and 512, and code rates of\n1/4, 1/2, and 4/5. We also propose an optimized bit-packed encoder. All\nimplementations of the encoders and decoders, along with the code construction\nand the unrolled decoders generator, are released as open source in the AFF3CT\ntoolbox.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4f4e\u5ef6\u8fdfPolar\u7801\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u7528\u4e8eISTC 2025\u7ade\u8d5b\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u8fde\u7eed\u53d6\u6d88\u5217\u8868\u89e3\u7801\u5668\uff08ASCL\uff09\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u4f18\u5316\u4e86\u6027\u80fd\u4e0e\u89e3\u7801\u65f6\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u4e3aISTC 2025\u7ade\u8d5b\u5f00\u53d1\u6700\u5feb\u7684\u4fe1\u9053\u7801\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u91cd\u70b9\u4f18\u5316\u5ef6\u8fdf\u548c\u6027\u80fd\u3002", "method": "\u57fa\u4e8ePolar\u7801\u548cASCL\u89e3\u7801\u5668\uff0c\u63d0\u51fa\u65b0\u578bASCL\u5c55\u5f00\u5f0f\u89e3\u7801\u5668\u751f\u6210\u5668\uff0c\u5e76\u63a2\u7d22\u4e86\u7801\u6784\u9020\u3001CRC\u9009\u62e9\u53ca\u5217\u8868\u5927\u5c0f\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u4e0d\u540c\u5e27\u9519\u8bef\u7387\u548c\u7801\u7387\u4e0b\u7684\u6027\u80fd\u4f18\u5316\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u5de5\u5177\u3002", "conclusion": "\u901a\u8fc7\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u548c\u4f18\u5316\uff0c\u5c55\u793a\u4e86\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6027\u80fd\u7684Polar\u7801\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.03929", "pdf": "https://arxiv.org/pdf/2507.03929", "abs": "https://arxiv.org/abs/2507.03929", "authors": ["Mohimenul Kabir", "Kuldeep S Meel"], "title": "An ASP-Based Framework for MUSes", "categories": ["cs.AI", "cs.LO"], "comment": "To appear in ICLP 2025 Technical Communication", "summary": "Given an unsatisfiable formula, understanding the core reason for\nunsatisfiability is crucial in several applications. One effective way to\ncapture this is through the minimal unsatisfiable subset (MUS), the\nsubset-minimal set of clauses that remains unsatisfiable. Current research\nbroadly focuses on two directions: (i) enumerating as many MUSes as possible\nwithin a given time limit, and (ii) counting the total number of MUSes for a\ngiven unsatisfiable formula.\n  In this paper, we introduce an answer set programming-based framework, named\nMUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for\nits strengths in knowledge representation and is particularly suitable for\nspecifying complex combinatorial problems. By translating MUS enumeration into\nanswer set solving, MUS-ASP leverages the computational efficiency of\nstate-of-the-art ASP systems. Our extensive experimental evaluation\ndemonstrates the effectiveness of MUS-ASP and highlights the acceleration in\nboth MUS enumeration and counting tasks, particularly when integrated within\nhybrid solvers, including the framework proposed in this paper.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u7684\u6846\u67b6MUS-ASP\uff0c\u7528\u4e8e\u5728\u7ebf\u679a\u4e3e\u6700\u5c0f\u4e0d\u53ef\u6ee1\u8db3\u5b50\u96c6\uff08MUS\uff09\uff0c\u5c55\u793a\u4e86\u5176\u5728MUS\u679a\u4e3e\u548c\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\uff0c\u7406\u89e3\u4e0d\u53ef\u6ee1\u8db3\u516c\u5f0f\u7684\u6838\u5fc3\u539f\u56e0\u81f3\u5173\u91cd\u8981\u3002\u6700\u5c0f\u4e0d\u53ef\u6ee1\u8db3\u5b50\u96c6\uff08MUS\uff09\u662f\u6355\u6349\u8fd9\u79cd\u539f\u56e0\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u5176\u679a\u4e3e\u548c\u8ba1\u6570\u4efb\u52a1\u5728\u73b0\u6709\u7814\u7a76\u4e2d\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u5c06MUS\u679a\u4e3e\u95ee\u9898\u8f6c\u5316\u4e3a\u7b54\u6848\u96c6\u6c42\u89e3\u95ee\u9898\uff0c\u5229\u7528ASP\u5728\u77e5\u8bc6\u8868\u793a\u548c\u7ec4\u5408\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86MUS-ASP\u6846\u67b6\uff0c\u5e76\u4e0e\u6df7\u5408\u6c42\u89e3\u5668\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMUS-ASP\u5728MUS\u679a\u4e3e\u548c\u8ba1\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u9ad8\u6548\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u6df7\u5408\u6c42\u89e3\u5668\u96c6\u6210\u65f6\u901f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MUS-ASP\u6846\u67b6\u901a\u8fc7\u5229\u7528ASP\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4e3aMUS\u7684\u5728\u7ebf\u679a\u4e3e\u548c\u8ba1\u6570\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03307", "pdf": "https://arxiv.org/pdf/2507.03307", "abs": "https://arxiv.org/abs/2507.03307", "authors": ["Taewook Kim", "Matthew Kay", "Yuqian Sun", "Melissa Roemmele", "Max Kreminski", "John Joon Young Chung"], "title": "Scaffolding Recursive Divergence and Convergence in Story Ideation", "categories": ["cs.HC", "cs.AI"], "comment": "17 pages, 5 figures, 3 tables", "summary": "Human creative ideation involves both exploration of diverse ideas\n(divergence) and selective synthesis of explored ideas into coherent\ncombinations (convergence). While processes of divergence and convergence are\noften interleaved and nested, existing AI-powered creativity support tools\n(CSTs) lack support for sophisticated orchestration of divergence and\nconvergence. We present Reverger, an AI-powered CST that helps users ideate\nvariations of conceptual directions for modifying a story by scaffolding\nflexible iteration between divergence and convergence. For divergence, our tool\nenables recursive exploration of alternative high-level directions for\nmodifying a specific part of the original story. For convergence, it allows\nusers to collect explored high-level directions and synthesize them into\nconcrete variations. Users can then iterate between divergence and convergence\nuntil they find a satisfactory outcome. A within-subject study revealed that\nReverger permitted participants to explore more unexpected and diverse\nhigh-level directions than a comparable baseline. Reverger users also felt that\nthey had more fine-grained control and discovered more effort-worthy outcomes.", "AI": {"tldr": "Reverger\u662f\u4e00\u79cdAI\u9a71\u52a8\u7684\u521b\u9020\u529b\u652f\u6301\u5de5\u5177\uff0c\u652f\u6301\u7528\u6237\u901a\u8fc7\u7075\u6d3b\u4ea4\u66ff\u53d1\u6563\u4e0e\u6536\u655b\u8fc7\u7a0b\uff0c\u751f\u6210\u6545\u4e8b\u4fee\u6539\u7684\u591a\u6837\u5316\u65b9\u5411\u3002", "motivation": "\u73b0\u6709AI\u8f85\u52a9\u5de5\u5177\u7f3a\u4e4f\u5bf9\u53d1\u6563\u4e0e\u6536\u655b\u8fc7\u7a0b\u7684\u9ad8\u7ea7\u534f\u8c03\u652f\u6301\uff0c\u8fd9\u9650\u5236\u4e86\u521b\u610f\u751f\u6210\u7684\u6df1\u5ea6\u4e0e\u591a\u6837\u6027\u3002", "method": "Reverger\u901a\u8fc7\u9012\u5f52\u63a2\u7d22\u9ad8\u7ea7\u4fee\u6539\u65b9\u5411\uff08\u53d1\u6563\uff09\u548c\u5408\u6210\u5177\u4f53\u53d8\u4f53\uff08\u6536\u655b\uff09\uff0c\u652f\u6301\u7528\u6237\u8fed\u4ee3\u4f18\u5316\u521b\u610f\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cReverger\u7528\u6237\u80fd\u63a2\u7d22\u66f4\u591a\u610f\u5916\u4e14\u591a\u6837\u5316\u7684\u65b9\u5411\uff0c\u5e76\u611f\u53d7\u5230\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u548c\u66f4\u6709\u4ef7\u503c\u7684\u6210\u679c\u3002", "conclusion": "Reverger\u901a\u8fc7\u6709\u6548\u534f\u8c03\u53d1\u6563\u4e0e\u6536\u655b\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u521b\u610f\u751f\u6210\u7684\u591a\u6837\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2507.04141", "pdf": "https://arxiv.org/pdf/2507.04141", "abs": "https://arxiv.org/abs/2507.04141", "authors": ["Mohsen Azarmi", "Mahdi Rezaei", "He Wang"], "title": "Pedestrian Intention Prediction via Vision-Language Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG", "cs.RO"], "comment": null, "summary": "Prediction of pedestrian crossing intention is a critical function in\nautonomous vehicles. Conventional vision-based methods of crossing intention\nprediction often struggle with generalizability, context understanding, and\ncausal reasoning. This study explores the potential of vision-language\nfoundation models (VLFMs) for predicting pedestrian crossing intentions by\nintegrating multimodal data through hierarchical prompt templates. The\nmethodology incorporates contextual information, including visual frames,\nphysical cues observations, and ego-vehicle dynamics, into systematically\nrefined prompts to guide VLFMs effectively in intention prediction. Experiments\nwere conducted on three common datasets-JAAD, PIE, and FU-PIP. Results\ndemonstrate that incorporating vehicle speed, its variations over time, and\ntime-conscious prompts significantly enhances the prediction accuracy up to\n19.8%. Additionally, optimised prompts generated via an automatic prompt\nengineering framework yielded 12.5% further accuracy gains. These findings\nhighlight the superior performance of VLFMs compared to conventional\nvision-based models, offering enhanced generalisation and contextual\nunderstanding for autonomous driving applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08VLFMs\uff09\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u63d0\u793a\u6a21\u677f\u9884\u6d4b\u884c\u4eba\u8fc7\u8857\u610f\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u65b9\u6cd5\u5728\u9884\u6d4b\u884c\u4eba\u8fc7\u8857\u610f\u56fe\u65f6\u6cdb\u5316\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u4e0d\u8db3\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528VLFMs\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u63d0\u793a\u6a21\u677f\u6574\u5408\u89c6\u89c9\u5e27\u3001\u7269\u7406\u7ebf\u7d22\u548c\u81ea\u8f66\u52a8\u6001\u7b49\u591a\u6a21\u6001\u6570\u636e\uff0c\u4f18\u5316\u63d0\u793a\u8bbe\u8ba1\u4ee5\u63d0\u5347VLFM\u7684\u9884\u6d4b\u6548\u679c\u3002", "result": "\u5728JAAD\u3001PIE\u548cFU-PIP\u6570\u636e\u96c6\u4e0a\uff0c\u7ed3\u5408\u8f66\u901f\u548c\u65f6\u5e8f\u63d0\u793a\u7684VLFMs\u5c06\u9884\u6d4b\u7cbe\u5ea6\u63d0\u534719.8%\uff0c\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u8fdb\u4e00\u6b65\u5e26\u676512.5%\u7684\u589e\u76ca\u3002", "conclusion": "VLFMs\u5728\u884c\u4eba\u8fc7\u8857\u610f\u56fe\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u89c6\u89c9\u6a21\u578b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u6027\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2507.04459", "pdf": "https://arxiv.org/pdf/2507.04459", "abs": "https://arxiv.org/abs/2507.04459", "authors": ["Ajay D. Kshemkalyani", "Manish Kumar", "Anisur Rahaman Molla", "Gokarna Sharma"], "title": "Agentic Distributed Computing", "categories": ["cs.DC", "cs.DS", "cs.MA", "cs.RO"], "comment": "42 pages, 3 figures,3 tables, 8 pseudocodes; some overlaps with\n  arXiv:2403.13716v2", "summary": "The most celebrated and extensively studied model of distributed computing is\nthe {\\em message-passing model,} in which each vertex/node of the (distributed\nnetwork) graph corresponds to a static computational device that communicates\nwith other devices through passing messages. In this paper, we consider the\n{\\em agentic model} of distributed computing which extends the message-passing\nmodel in a new direction. In the agentic model, computational devices are\nmodeled as relocatable or mobile computational devices (called agents in this\npaper), i.e., each vertex/node of the graph serves as a container for the\ndevices, and hence communicating with another device requires relocating to the\nsame node. We study two fundamental graph level tasks, leader election, and\nminimum spanning tree, in the agentic model, which will enhance our\nunderstanding of distributed computation across paradigms. The objective is to\nminimize both time and memory complexities. Following the literature, we\nconsider the synchronous setting in which each agent performs its operations\nsynchronously with others, and hence the time complexity can be measured in\nrounds. In this paper, we present two deterministic algorithms for leader\nelection: one for the case of $k<n$ and another for the case of $k=n$,\nminimizing both time and memory complexities, where $k$ and $n$, respectively,\nare the number of agents and number of nodes of the graph. Using these leader\nelection results, we develop deterministic algorithms for agents to construct a\nminimum spanning tree of the graph, minimizing both time and memory\ncomplexities. To the best of our knowledge, this is the first study of\ndistributed graph level tasks in the agentic model with $k\\leq n$. Previous\nstudies only considered the case of $k=n$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u5173\u4e8e\u9886\u5bfc\u9009\u4e3e\u548c\u6700\u5c0f\u751f\u6210\u6811\u4efb\u52a1\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u4f18\u5316\u4e86\u65f6\u95f4\u548c\u5185\u5b58\u590d\u6742\u5ea6\uff0c\u9996\u6b21\u63a2\u8ba8\u4e86\u4ee3\u7406\u6570\u91cf\u4e0d\u8d85\u8fc7\u8282\u70b9\u6570\u7684\u60c5\u51b5\u3002", "motivation": "\u6269\u5c55\u6d88\u606f\u4f20\u9012\u6a21\u578b\uff0c\u5f15\u5165\u53ef\u79fb\u52a8\u8ba1\u7b97\u8bbe\u5907\uff08\u4ee3\u7406\uff09\uff0c\u7814\u7a76\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u9886\u5bfc\u9009\u4e3e\u548c\u6700\u5c0f\u751f\u6210\u6811\u4efb\u52a1\u7684\u65b0\u8303\u5f0f\uff0c\u586b\u8865\u4e86k\u2264n\u60c5\u51b5\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u9886\u5bfc\u9009\u4e3e\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\uff08k<n\u548ck=n\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u6700\u5c0f\u751f\u6210\u6811\u7b97\u6cd5\uff0c\u4f18\u5316\u65f6\u95f4\u548c\u5185\u5b58\u590d\u6742\u5ea6\u3002", "result": "\u9488\u5bf9k\u2264n\u7684\u60c5\u51b5\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u9886\u5bfc\u9009\u4e3e\u548c\u6700\u5c0f\u751f\u6210\u6811\u7b97\u6cd5\uff0c\u4e3a\u4ee3\u7406\u6a21\u578b\u4e0b\u7684\u5206\u5e03\u5f0f\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u4ee3\u7406\u6a21\u578b\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u4e3a\u7814\u7a76\u4ee3\u7406\u6570\u91cf\u53d7\u9650\u7684\u60c5\u51b5\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u590d\u6742\u7684\u4efb\u52a1\u548c\u573a\u666f\u3002"}}
{"id": "2507.04061", "pdf": "https://arxiv.org/pdf/2507.04061", "abs": "https://arxiv.org/abs/2507.04061", "authors": ["Hanghui Guo", "Weijie Shi", "Mengze Li", "Juncheng Li", "Hao Chen", "Yue Cui", "Jiajie Xu", "Jia Zhu", "Jiawei Shen", "Zhangze Chen", "Sirui Han"], "title": "Consistent and Invariant Generalization Learning for Short-video Misinformation Detection", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted to ACM MM 2025,15 pages, 16figures", "summary": "Short-video misinformation detection has attracted wide attention in the\nmulti-modal domain, aiming to accurately identify the misinformation in the\nvideo format accompanied by the corresponding audio. Despite significant\nadvancements, current models in this field, trained on particular domains\n(source domains), often exhibit unsatisfactory performance on unseen domains\n(target domains) due to domain gaps. To effectively realize such domain\ngeneralization on the short-video misinformation detection task, we propose\ndeep insights into the characteristics of different domains: (1) The detection\non various domains may mainly rely on different modalities (i.e., mainly\nfocusing on videos or audios). To enhance domain generalization, it is crucial\nto achieve optimal model performance on all modalities simultaneously. (2) For\nsome domains focusing on cross-modal joint fraud, a comprehensive analysis\nrelying on cross-modal fusion is necessary. However, domain biases located in\neach modality (especially in each frame of videos) will be accumulated in this\nfusion process, which may seriously damage the final identification of\nmisinformation. To address these issues, we propose a new DOmain generalization\nmodel via ConsisTency and invariance learning for shORt-video misinformation\ndetection (named DOCTOR), which contains two characteristic modules: (1) We\ninvolve the cross-modal feature interpolation to map multiple modalities into a\nshared space and the interpolation distillation to synchronize multi-modal\nlearning; (2) We design the diffusion model to add noise to retain core\nfeatures of multi modal and enhance domain invariant features through\ncross-modal guided denoising. Extensive experiments demonstrate the\neffectiveness of our proposed DOCTOR model. Our code is public available at\nhttps://github.com/ghh1125/DOCTOR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDOCTOR\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u548c\u4e0d\u53d8\u6027\u5b66\u4e60\u6765\u63d0\u5347\u77ed\u89c6\u9891\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u9886\u57df\u5dee\u5f02\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8de8\u9886\u57df\u8bc6\u522b\u77ed\u89c6\u9891\u865a\u5047\u4fe1\u606f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u9886\u57df\u7279\u6027\u7684\u65b9\u6cd5\u3002", "method": "DOCTOR\u6a21\u578b\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u8de8\u6a21\u6001\u7279\u5f81\u63d2\u503c\u548c\u63d2\u503c\u84b8\u998f\u4ee5\u540c\u6b65\u591a\u6a21\u6001\u5b66\u4e60\uff1b\u4ee5\u53ca\u6269\u6563\u6a21\u578b\u589e\u5f3a\u9886\u57df\u4e0d\u53d8\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDOCTOR\u6a21\u578b\u5728\u8de8\u9886\u57df\u77ed\u89c6\u9891\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DOCTOR\u901a\u8fc7\u591a\u6a21\u6001\u4e00\u81f4\u6027\u548c\u7279\u5f81\u4e0d\u53d8\u6027\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.04548", "pdf": "https://arxiv.org/pdf/2507.04548", "abs": "https://arxiv.org/abs/2507.04548", "authors": ["Renato Cordeiro Ferreira", "Dayanne Gomes", "Vitor Tamae", "Francisco Wernke", "Alfredo Goldman"], "title": "SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.7; I.2.7; I.5.4"], "comment": "4 pages, 1 figure (1 diagram), published at ISE 2022", "summary": "Respiratory insufficiency is a medic symptom in which a person gets a reduced\namount of oxygen in the blood. This paper reports the experience of building\nSPIRA: an intelligent system for detecting respiratory insufficiency from\nvoice. It compiles challenges faced in two succeeding implementations of the\nsame architecture, summarizing lessons learned on data collection, training,\nand inference for future projects in similar systems.", "AI": {"tldr": "\u603b\u7ed3SPIRA\u7cfb\u7edf\u7684\u6784\u5efa\u7ecf\u9a8c\uff0c\u7528\u4e8e\u4ece\u58f0\u97f3\u4e2d\u68c0\u6d4b\u547c\u5438\u529f\u80fd\u4e0d\u5168\uff0c\u5e76\u5206\u4eab\u5728\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6559\u8bad\u3002", "motivation": "\u89e3\u51b3\u547c\u5438\u529f\u80fd\u4e0d\u5168\u7684\u65e9\u671f\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u7cfb\u7edf\u4ece\u58f0\u97f3\u4e2d\u8bc6\u522b\u76f8\u5173\u75c7\u72b6\u3002", "method": "\u6784\u5efaSPIRA\u7cfb\u7edf\uff0c\u7ecf\u5386\u4e86\u4e24\u4ee3\u67b6\u6784\u5b9e\u73b0\uff0c\u603b\u7ed3\u4e86\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6210\u529f\u5f00\u53d1SPIRA\u7cfb\u7edf\uff0c\u4e3a\u672a\u6765\u7c7b\u4f3c\u9879\u76ee\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u6559\u8bad\u3002", "conclusion": "SPIRA\u7cfb\u7edf\u7684\u6784\u5efa\u8fc7\u7a0b\u4e3a\u547c\u5438\u529f\u80fd\u4e0d\u5168\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7ecf\u9a8c\uff0c\u672a\u6765\u9879\u76ee\u53ef\u501f\u9274\u5176\u4e2d\u7684\u6559\u8bad\u3002"}}
{"id": "2507.04285", "pdf": "https://arxiv.org/pdf/2507.04285", "abs": "https://arxiv.org/abs/2507.04285", "authors": ["Ze Yuan", "Xin Yu", "Yangtian Sun", "Yuan-Chen Guo", "Yan-Pei Cao", "Ding Liang", "Xiaojuan Qi"], "title": "SeqTex: Generate Mesh Textures in Video Sequence", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": null, "summary": "Training native 3D texture generative models remains a fundamental yet\nchallenging problem, largely due to the limited availability of large-scale,\nhigh-quality 3D texture datasets. This scarcity hinders generalization to\nreal-world scenarios. To address this, most existing methods finetune\nfoundation image generative models to exploit their learned visual priors.\nHowever, these approaches typically generate only multi-view images and rely on\npost-processing to produce UV texture maps -- an essential representation in\nmodern graphics pipelines. Such two-stage pipelines often suffer from error\naccumulation and spatial inconsistencies across the 3D surface. In this paper,\nwe introduce SeqTex, a novel end-to-end framework that leverages the visual\nknowledge encoded in pretrained video foundation models to directly generate\ncomplete UV texture maps. Unlike previous methods that model the distribution\nof UV textures in isolation, SeqTex reformulates the task as a sequence\ngeneration problem, enabling the model to learn the joint distribution of\nmulti-view renderings and UV textures. This design effectively transfers the\nconsistent image-space priors from video foundation models into the UV domain.\nTo further enhance performance, we propose several architectural innovations: a\ndecoupled multi-view and UV branch design, geometry-informed attention to guide\ncross-domain feature alignment, and adaptive token resolution to preserve fine\ntexture details while maintaining computational efficiency. Together, these\ncomponents allow SeqTex to fully utilize pretrained video priors and synthesize\nhigh-fidelity UV texture maps without the need for post-processing. Extensive\nexperiments show that SeqTex achieves state-of-the-art performance on both\nimage-conditioned and text-conditioned 3D texture generation tasks, with\nsuperior 3D consistency, texture-geometry alignment, and real-world\ngeneralization.", "AI": {"tldr": "SeqTex\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u57fa\u7840\u6a21\u578b\u7684\u89c6\u89c9\u77e5\u8bc6\u76f4\u63a5\u751f\u6210\u5b8c\u6574\u7684UV\u7eb9\u7406\u8d34\u56fe\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u7a7a\u95f4\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u76843D\u7eb9\u7406\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u548c\u540e\u7eed\u5904\u7406\uff0c\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u548c\u7a7a\u95f4\u4e0d\u4e00\u81f4\u3002\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf3D\u7eb9\u7406\u6570\u636e\u96c6\uff0c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "SeqTex\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e8f\u5217\u751f\u6210\u95ee\u9898\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u7684\u56fe\u50cf\u7a7a\u95f4\u5148\u9a8c\u77e5\u8bc6\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u548cUV\u5206\u652f\u8bbe\u8ba1\u3001\u51e0\u4f55\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u53ca\u81ea\u9002\u5e94\u4ee4\u724c\u5206\u8fa8\u7387\u3002", "result": "SeqTex\u5728\u56fe\u50cf\u548c\u6587\u672c\u6761\u4ef6\u4e0b\u76843D\u7eb9\u7406\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5177\u6709\u51fa\u8272\u76843D\u4e00\u81f4\u6027\u3001\u7eb9\u7406-\u51e0\u4f55\u5bf9\u9f50\u548c\u771f\u5b9e\u4e16\u754c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SeqTex\u901a\u8fc7\u7aef\u5230\u7aef\u6846\u67b6\u76f4\u63a5\u751f\u6210UV\u7eb9\u7406\u8d34\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u7eb9\u7406\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2507.04968", "pdf": "https://arxiv.org/pdf/2507.04968", "abs": "https://arxiv.org/abs/2507.04968", "authors": ["Pramod N Chine", "Suven Jagtiani", "Mandar R Nalavade", "Gaurav S Kasbekar"], "title": "User Association in the Presence of Jamming in Wireless Networks Using the Whittle Index", "categories": ["cs.NI"], "comment": null, "summary": "In wireless networks, algorithms for user association, i.e., the task of\nchoosing the base station (BS) that every arriving user should join,\nsignificantly impact the network performance. A wireless network with multiple\nBSs, operating on non-overlapping channels, is considered. The channels of the\nBSs are susceptible to jamming by attackers. During every time slot, a user\narrives with a certain probability. There exists a holding cost in each slot\nfor every user associated with a BS. The goal here is to design a user\nassociation scheme, which assigns a BS to each user upon arrival with the\nobjective of minimizing the long-run total average holding cost borne within\nthe network. This objective results in low average delays attained by the\nusers. This association problem is an instance of restless multi-armed bandit\nproblems, and is known to be hard to solve. By making use of the framework\npresented by Whittle, the hard per-stage constraint that every arriving user\nmust connect to exactly one BS in a time slot is relaxed to a long-term\ntime-averaged constraint. Subsequently, we employ the Lagrangian multiplier\nstrategy to reformulate the problem into an unconstrained form and decompose it\ninto separate Markov Decision Processes at the BSs. Further, the problem is\nproven to be Whittle indexable and a method for calculating the Whittle indices\ncorresponding to different BSs is presented. We design a user association\npolicy under which, upon arrival of a user in a time slot, it is assigned to\nthe BS having the least Whittle index in that slot. Through extensive\nsimulations, we show that our proposed association policy based on the Whittle\nindex outperforms various user association policies proposed in previous work\nin terms of different metrics such as average cost, average delay, and Jain's\nfairness index.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWhittle\u6307\u6570\u7684\u7528\u6237\u5173\u8054\u7b56\u7565\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7528\u6237\u7684\u603b\u5e73\u5747\u6301\u6709\u6210\u672c\u3002", "motivation": "\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7528\u6237\u5173\u8054\u7b97\u6cd5\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u590d\u6742\u7ea6\u675f\u95ee\u9898\u3002", "method": "\u5229\u7528Whittle\u6846\u67b6\u5c06\u786c\u7ea6\u675f\u677e\u5f1b\u4e3a\u957f\u671f\u5e73\u5747\u7ea6\u675f\uff0c\u91c7\u7528\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u5206\u89e3\u95ee\u9898\u5e76\u8bc1\u660e\u5176Whittle\u53ef\u7d22\u5f15\u6027\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0c\u8be5\u7b56\u7565\u5728\u5e73\u5747\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u516c\u5e73\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8eWhittle\u6307\u6570\u7684\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u7f51\u7edc\u4e2d\u7684\u7528\u6237\u5173\u8054\u95ee\u9898\u3002"}}
{"id": "2507.04719", "pdf": "https://arxiv.org/pdf/2507.04719", "abs": "https://arxiv.org/abs/2507.04719", "authors": ["Roozbeh Yousefzadeh", "Xuenan Cao"], "title": "Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "This position paper provides a critical but constructive discussion of\ncurrent practices in benchmarking and evaluative practices in the field of\nformal reasoning and automated theorem proving. We take the position that open\ncode, open data, and benchmarks that are complete and error-free will\naccelerate progress in this field. We identify practices that create barriers\nto contributing to this field and suggest ways to remove them. We also discuss\nsome of the practices that might produce misleading evaluative information. We\naim to create discussions that bring together people from various groups\ncontributing to automated theorem proving, autoformalization, and informal\nreasoning.", "AI": {"tldr": "\u672c\u6587\u6279\u8bc4\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u5f62\u5f0f\u5316\u63a8\u7406\u548c\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u8bc4\u4f30\u5b9e\u8df5\uff0c\u5021\u5bfc\u5f00\u653e\u4ee3\u7801\u3001\u5f00\u653e\u6570\u636e\u53ca\u65e0\u9519\u8bef\u7684\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u8ba8\u8bba\u548c\u6539\u9769\u73b0\u6709\u5b9e\u8df5\uff0c\u6d88\u9664\u9886\u57df\u5185\u7684\u8d21\u732e\u969c\u788d\uff0c\u4fc3\u8fdb\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\u7b49\u76f8\u5173\u9886\u57df\u7684\u8fdb\u6b65\u3002", "method": "\u901a\u8fc7\u6279\u5224\u6027\u5206\u6790\u5f53\u524d\u5b9e\u8df5\uff0c\u8bc6\u522b\u95ee\u9898\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\uff0c\u540c\u65f6\u63a2\u8ba8\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\u6027\u8bc4\u4f30\u4fe1\u606f\u7684\u505a\u6cd5\u3002", "result": "\u547c\u5401\u5f00\u653e\u3001\u900f\u660e\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u5171\u4eab\uff0c\u4ee5\u52a0\u901f\u9886\u57df\u53d1\u5c55\u3002", "conclusion": "\u5e0c\u671b\u901a\u8fc7\u8de8\u7fa4\u4f53\u8ba8\u8bba\uff0c\u63a8\u52a8\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\u53ca\u76f8\u5173\u9886\u57df\u7684\u5408\u4f5c\u4e0e\u8fdb\u6b65\u3002"}}
{"id": "2507.03391", "pdf": "https://arxiv.org/pdf/2507.03391", "abs": "https://arxiv.org/abs/2507.03391", "authors": ["Thomas Vase Schultz Volden", "Oleg Jarma Montoya", "Paolo Burelli", "Marco Scirea"], "title": "On the dynamics of affective states during play and the role of confusion", "categories": ["cs.HC"], "comment": "4 pages, 2 figures, Conference on Games", "summary": "Video game designers often view confusion as undesirable, yet it is\ninevitable, as new players must adapt to new interfaces and mechanics in an\nincreasingly varied and innovative game market, which is more popular than\never. Research suggests that confusion can contribute to a positive experience,\npotentially motivating players to learn. The state of confusion in video games\nshould be further investigated to gain more insight into the learning\nexperience of play and how it affects the player experience. In this article,\nwe design a study to collect learning-related affects for users playing a game\nprototype that intentionally confuses the player. We assess the gathered\naffects against a complex learning model, affirming that, in specific\ninstances, the player experience aligns with the learning experiences.\nMoreover, we identify correlations between these affects and the Player\nExperience Inventory constructs, particularly concerning flow experiences.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7535\u5b50\u6e38\u620f\u4e2d\u201c\u56f0\u60d1\u201d\u7684\u79ef\u6781\u4f5c\u7528\uff0c\u8ba4\u4e3a\u5b83\u53ef\u4ee5\u4fc3\u8fdb\u73a9\u5bb6\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u56f0\u60d1\u4e0e\u5b66\u4e60\u4f53\u9a8c\u7684\u5173\u8054\u3002", "motivation": "\u5c3d\u7ba1\u6e38\u620f\u8bbe\u8ba1\u5e08\u901a\u5e38\u8ba4\u4e3a\u56f0\u60d1\u662f\u6d88\u6781\u7684\uff0c\u4f46\u7814\u7a76\u8868\u660e\u5b83\u53ef\u80fd\u4fc3\u8fdb\u73a9\u5bb6\u7684\u5b66\u4e60\u52a8\u673a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u56f0\u60d1\u5728\u6e38\u620f\u4e2d\u7684\u89d2\u8272\u53ca\u5176\u5bf9\u73a9\u5bb6\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b9e\u9a8c\uff0c\u8ba9\u73a9\u5bb6\u4f53\u9a8c\u6545\u610f\u5f15\u53d1\u56f0\u60d1\u7684\u6e38\u620f\u539f\u578b\uff0c\u5e76\u6536\u96c6\u5b66\u4e60\u76f8\u5173\u7684\u60c5\u611f\u6570\u636e\uff0c\u4e0e\u590d\u6742\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u73a9\u5bb6\u7684\u4f53\u9a8c\u4e0e\u5b66\u4e60\u4f53\u9a8c\u4e00\u81f4\u3002\u6b64\u5916\uff0c\u56f0\u60d1\u60c5\u611f\u4e0e\u201c\u73a9\u5bb6\u4f53\u9a8c\u91cf\u8868\u201d\u4e2d\u7684\u201c\u5fc3\u6d41\u4f53\u9a8c\u201d\u5b58\u5728\u76f8\u5173\u6027\u3002", "conclusion": "\u56f0\u60d1\u5728\u6e38\u620f\u4e2d\u53ef\u4ee5\u53d1\u6325\u79ef\u6781\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u4fc3\u8fdb\u5b66\u4e60\u548c\u5fc3\u6d41\u4f53\u9a8c\u65b9\u9762\uff0c\u4e3a\u6e38\u620f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2507.04249", "pdf": "https://arxiv.org/pdf/2507.04249", "abs": "https://arxiv.org/abs/2507.04249", "authors": ["Umer Sadiq Khan", "Saif Ur Rehman Khan"], "title": "Ethics by Design: A Lifecycle Framework for Trustworthy AI in Medical Imaging From Transparent Data Governance to Clinically Validated Deployment", "categories": ["cs.CY", "cs.ET"], "comment": null, "summary": "The integration of artificial intelligence (AI) in medical imaging raises\ncrucial ethical concerns at every stage of its development, from data\ncollection to deployment. Addressing these concerns is essential for ensuring\nthat AI systems are developed and implemented in a manner that respects patient\nrights and promotes fairness. This study aims to explore the ethical\nimplications of AI in medical imaging, focusing on five key stages: data\ncollection, data processing, model training, model evaluation, and deployment.\nThe goal is to evaluate how these stages adhere to fundamental ethical\nprinciples, including data privacy, fairness, transparency, accountability, and\nautonomy. An analytical approach was employed to examine the ethical challenges\nassociated with each stage of AI development. We reviewed existing literature,\nguidelines, and regulations concerning AI ethics in healthcare and identified\ncritical ethical issues at each stage. The study outlines specific inquiries\nand principles for each phase of AI development. The findings highlight key\nethical issues: ensuring patient consent and anonymization during data\ncollection, addressing biases in model training, ensuring transparency and\nfairness during model evaluation, and the importance of continuous ethical\nassessments during deployment. The analysis also emphasizes the impact of\naccessibility issues on different stakeholders, including private, public, and\nthird-party entities. The study concludes that ethical considerations must be\nsystematically integrated into each stage of AI development in medical imaging.\nBy adhering to these ethical principles, AI systems can be made more robust,\ntransparent, and aligned with patient care and data control. We propose\ntailored ethical inquiries and strategies to support the creation of ethically\nsound AI systems in medical imaging.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86AI\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u4e94\u4e2a\u5173\u952e\u5f00\u53d1\u9636\u6bb5\u7684\u4f26\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u786e\u4fdd\u6570\u636e\u9690\u79c1\u3001\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u7684\u539f\u5219\u548c\u65b9\u6cd5\u3002", "motivation": "AI\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5e94\u7528\u5f15\u53d1\u4e86\u4ece\u6570\u636e\u6536\u96c6\u5230\u90e8\u7f72\u7684\u5168\u8fc7\u7a0b\u4f26\u7406\u95ee\u9898\uff0c\u9700\u786e\u4fdd\u7cfb\u7edf\u5c0a\u91cd\u60a3\u8005\u6743\u5229\u5e76\u4fc3\u8fdb\u516c\u5e73\u3002", "method": "\u91c7\u7528\u5206\u6790\u65b9\u6cd5\u5ba1\u67e5\u6587\u732e\u3001\u6307\u5357\u548c\u6cd5\u89c4\uff0c\u8bc6\u522bAI\u5f00\u53d1\u7684\u5404\u4e2a\u9636\u6bb5\uff08\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u90e8\u7f72\uff09\u7684\u4f26\u7406\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5173\u952e\u4f26\u7406\u95ee\u9898\u5305\u62ec\u60a3\u8005\u540c\u610f\u3001\u6570\u636e\u533f\u540d\u5316\u3001\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u504f\u89c1\u3001\u900f\u660e\u6027\u548c\u516c\u5e73\u6027\uff0c\u4ee5\u53ca\u90e8\u7f72\u65f6\u7684\u6301\u7eed\u4f26\u7406\u8bc4\u4f30\u3002", "conclusion": "\u4f26\u7406\u539f\u5219\u9700\u7cfb\u7edf\u6574\u5408\u5230AI\u5f00\u53d1\u7684\u6bcf\u4e2a\u9636\u6bb5\uff0c\u652f\u6301\u521b\u5efa\u66f4\u900f\u660e\u3001\u516c\u5e73\u4e14\u7b26\u5408\u60a3\u8005\u62a4\u7406\u548c\u6570\u636e\u63a7\u5236\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2507.04647", "pdf": "https://arxiv.org/pdf/2507.04647", "abs": "https://arxiv.org/abs/2507.04647", "authors": ["Faveo Hoerold", "Ivan R. Ivanov", "Akash Dhruv", "William S. Moses", "Anshu Dubey", "Mohamed Wahib", "Jens Domke"], "title": "RAPTOR: Practical Numerical Profiling of Scientific Applications", "categories": ["cs.DC", "cs.NA", "math.NA"], "comment": "12 pages, 8 figures, to be published in SC'25", "summary": "The proliferation of low-precision units in modern high-performance\narchitectures increasingly burdens domain scientists. Historically, the choice\nin HPC was easy: can we get away with 32 bit floating-point operations and\nlower bandwidth requirements, or is FP64 necessary? Driven by Artificial\nIntelligence, vendors introduced novel low-precision units for vector and\ntensor operations, and FP64 capabilities stagnate or are reduced. This is\nforcing scientists to re-evaluate their codes, but a trivial search-and-replace\napproach to go from FP64 to FP16 will not suffice. We introduce RAPTOR: a\nnumerical profiling tool to guide scientists in their search for code regions\nwhere precision lowering is feasible. Using LLVM, we transparently replace\nhigh-precision computations using low-precision units, or emulate a\nuser-defined precision. RAPTOR is a novel, feature-rich approach -- with focus\non ease of use -- to change, profile, and reason about numerical requirements\nand instabilities, which we demonstrate with four real-world multi-physics\nFlash-X applications.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u540d\u4e3aRAPTOR\u7684\u6570\u503c\u5206\u6790\u5de5\u5177\uff0c\u5e2e\u52a9\u79d1\u5b66\u5bb6\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u8bc4\u4f30\u4f4e\u7cbe\u5ea6\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u4ee3\u9ad8\u6027\u80fd\u67b6\u6784\u4e2d\u4f4e\u7cbe\u5ea6\u5355\u5143\u7684\u666e\u53ca\u7ed9\u79d1\u5b66\u5bb6\u5e26\u6765\u4e86\u8d1f\u62c5\uff0c\u9700\u8981\u5de5\u5177\u6765\u8bc4\u4f30\u4ee3\u7801\u4e2d\u54ea\u4e9b\u90e8\u5206\u53ef\u4ee5\u964d\u4f4e\u7cbe\u5ea6\u800c\u4e0d\u5f71\u54cd\u7ed3\u679c\u3002", "method": "\u4f7f\u7528LLVM\u900f\u660e\u5730\u66ff\u6362\u9ad8\u7cbe\u5ea6\u8ba1\u7b97\u4e3a\u4f4e\u7cbe\u5ea6\u64cd\u4f5c\u6216\u6a21\u62df\u7528\u6237\u5b9a\u4e49\u7684\u7cbe\u5ea6\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6613\u7528\u7684\u5de5\u5177\u6765\u5206\u6790\u6570\u503c\u9700\u6c42\u548c\u7a33\u5b9a\u6027\u3002", "result": "RAPTOR\u5728\u56db\u4e2a\u771f\u5b9e\u7684\u591a\u7269\u7406\u573aFlash-X\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "RAPTOR\u4e3a\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u8bc4\u4f30\u548c\u4f18\u5316\u6570\u503c\u7cbe\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f4e\u7cbe\u5ea6\u67b6\u6784\u4e0b\u7684\u8ba1\u7b97\u6311\u6218\u3002"}}
{"id": "2507.04377", "pdf": "https://arxiv.org/pdf/2507.04377", "abs": "https://arxiv.org/abs/2507.04377", "authors": ["Xiao Zhang", "Johan Bos"], "title": "Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "Accepted by ACMMM 2025", "summary": "Tombstones are historically and culturally rich artifacts, encapsulating\nindividual lives, community memory, historical narratives and artistic\nexpression. Yet, many tombstones today face significant preservation\nchallenges, including physical erosion, vandalism, environmental degradation,\nand political shifts. In this paper, we introduce a novel multi-modal framework\nfor tombstones digitization, aiming to improve the interpretation, organization\nand retrieval of tombstone content. Our approach leverages vision-language\nmodels (VLMs) to translate tombstone images into structured Tombstone Meaning\nRepresentations (TMRs), capturing both image and text information. To further\nenrich semantic parsing, we incorporate retrieval-augmented generation (RAG)\nfor integrate externally dependent elements such as toponyms, occupation codes,\nand ontological concepts. Compared to traditional OCR-based pipelines, our\nmethod improves parsing accuracy from an F1 score of 36.1 to 89.5. We\nadditionally evaluate the model's robustness across diverse linguistic and\ncultural inscriptions, and simulate physical degradation through image fusion\nto assess performance under noisy or damaged conditions. Our work represents\nthe first attempt to formalize tombstone understanding using large\nvision-language models, presenting implications for heritage preservation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5c06\u5893\u7891\u56fe\u50cf\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u8868\u793a\uff08TMRs\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u6790\u51c6\u786e\u7387\uff08F1\u4ece36.1\u63d0\u5347\u81f389.5\uff09\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u591a\u6837\u6587\u5316\u548c\u7269\u7406\u9000\u5316\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5893\u7891\u4f5c\u4e3a\u5386\u53f2\u548c\u6587\u5316\u7684\u91cd\u8981\u8f7d\u4f53\uff0c\u9762\u4e34\u591a\u79cd\u4fdd\u62a4\u6311\u6218\uff0c\u5982\u7269\u7406\u4fb5\u8680\u3001\u7834\u574f\u548c\u73af\u5883\u9000\u5316\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6570\u5b57\u5316\u548c\u4fdd\u62a4\u5176\u5185\u5bb9\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u5c06\u5893\u7891\u56fe\u50cf\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u8868\u793a\uff08TMRs\uff09\uff0c\u6574\u5408\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u5916\u90e8\u4f9d\u8d56\u5143\u7d20\u5982\u5730\u540d\u548c\u804c\u4e1a\u4ee3\u7801\u3002", "result": "\u4e0e\u4f20\u7edfOCR\u65b9\u6cd5\u76f8\u6bd4\uff0c\u89e3\u6790\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff08F1\u4ece36.1\u523089.5\uff09\uff0c\u4e14\u5728\u591a\u6837\u6587\u5316\u548c\u7269\u7406\u9000\u5316\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u8bba\u6587\u9996\u6b21\u5c1d\u8bd5\u5229\u7528\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f62\u5f0f\u5316\u5893\u7891\u7406\u89e3\uff0c\u4e3a\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u5de5\u5177\u3002"}}
{"id": "2507.04555", "pdf": "https://arxiv.org/pdf/2507.04555", "abs": "https://arxiv.org/abs/2507.04555", "authors": ["Gabriella Waters"], "title": "Testing, Evaluation, Verification and Validation (TEVV) of Digital Twins: A Comprehensive Framework", "categories": ["cs.SE"], "comment": "1 figure, 41 pages, 3 tables", "summary": "Digital twins have emerged as a powerful technology for modeling and\nsimulating complex systems across various domains (Fuller et al., 2020; Tao et\nal., 2019). As virtual representations of physical assets, processes, or\nsystems, digital twins enable real-time monitoring, predictive analysis, and\noptimization. However, as digital twins become more sophisticated and integral\nto decision-making processes, ensuring their accuracy, reliability, and ethical\nimplementation is essential. This paper presents a comprehensive framework for\nthe Testing, Evaluation, Verification and Validation (TEVV) of digital twins to\naddress the unique challenges posed by these dynamic and complex virtual\nmodels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u7684\u5168\u9762\u6d4b\u8bd5\u4e0e\u9a8c\u8bc1\u6846\u67b6\uff08TEVV\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u5176\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5728\u590d\u6742\u7cfb\u7edf\u5efa\u6a21\u548c\u6a21\u62df\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u4f26\u7406\u5b9e\u73b0\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTEVV\uff08\u6d4b\u8bd5\u3001\u8bc4\u4f30\u3001\u9a8c\u8bc1\u548c\u786e\u8ba4\uff09\u7684\u7efc\u5408\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u7684\u5f00\u53d1\u548c\u9a8c\u8bc1\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u7684\u52a8\u6001\u6027\u548c\u590d\u6742\u6027\uff0c\u4e3a\u5176\u5728\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u53ef\u9760\u4fdd\u969c\u3002", "conclusion": "TEVV\u6846\u67b6\u4e3a\u6570\u5b57\u5b6a\u751f\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u548c\u5e94\u7528\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5c24\u5176\u662f\u5728\u786e\u4fdd\u6a21\u578b\u8d28\u91cf\u548c\u4f26\u7406\u5408\u89c4\u65b9\u9762\u3002"}}
{"id": "2507.05028", "pdf": "https://arxiv.org/pdf/2507.05028", "abs": "https://arxiv.org/abs/2507.05028", "authors": ["C\u00e9cilia Pradic"], "title": "The Myhill isomorphism theorem does not generalize much", "categories": ["math.LO", "cs.LO", "03F50, 03F60"], "comment": "25 pages, 6 figures, draft", "summary": "The Myhill isomorphism is a variant of the Cantor-Bernstein theorem. It\nstates that, from two injections that reduces two subsets of $\\mathbb{N}$ to\neach other, there exists a bijection $\\mathbb{N} \\to \\mathbb{N}$ that preserves\nthem. This theorem can be proven constructively. We investigate to which extent\nthe theorem can be extended to other infinite sets other than $\\mathbb{N}$. We\nshow that, assuming Markov's principle, the theorem can be extended to the\nconatural numbers $\\mathbb{N}_{\\infty}$ provided that we only require that\nbicomplemented sets are preserved by the bijection. This restriction is\nessential. Otherwise, the picture is overall negative: among other things, it\nis impossible to extend that result to either $2 \\times \\mathbb{N}_{\\infty}$,\n$\\mathbb{N} + \\mathbb{N}_{\\infty}$, $\\mathbb{N} \\times \\mathbb{N}_{\\infty}$,\n$\\mathbb{N}_{\\infty}^2$, $2^{\\mathbb{N}}$ or $\\mathbb{N}^{\\mathbb{N}}$.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Myhill\u540c\u6784\u5b9a\u7406\uff08Cantor-Bernstein\u5b9a\u7406\u7684\u53d8\u4f53\uff09\u5728\u975e\u81ea\u7136\u6570\u65e0\u9650\u96c6\u4e0a\u7684\u6269\u5c55\u6027\uff0c\u8bc1\u660e\u5728Markov\u539f\u7406\u4e0b\u53ef\u6269\u5c55\u81f3\u5171\u81ea\u7136\u6570\u96c6\uff0c\u4f46\u5bf9\u5176\u4ed6\u96c6\u5408\u5219\u666e\u904d\u4e0d\u53ef\u884c\u3002", "motivation": "\u63a2\u8ba8Myhill\u540c\u6784\u5b9a\u7406\u5728\u4e0d\u540c\u65e0\u9650\u96c6\u4e0a\u7684\u9002\u7528\u6027\uff0c\u4ee5\u6269\u5c55\u8fd9\u4e00\u7ecf\u5178\u6784\u9020\u6027\u5b9a\u7406\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u901a\u8fc7\u6784\u9020\u6027\u8bc1\u660e\u548c\u53cd\u4f8b\u5206\u6790\uff0c\u7ed3\u5408Markov\u539f\u7406\uff0c\u6d4b\u8bd5\u5b9a\u7406\u5728\u5171\u81ea\u7136\u6570\u96c6\u548c\u5176\u4ed6\u96c6\u5408\u4e0a\u7684\u53ef\u884c\u6027\u3002", "result": "\u5b9a\u7406\u53ef\u6269\u5c55\u81f3\u5171\u81ea\u7136\u6570\u96c6\uff08\u9700\u9650\u5236\u53cc\u8865\u96c6\u6761\u4ef6\uff09\uff0c\u4f46\u65e0\u6cd5\u63a8\u5e7f\u5230\u5176\u4ed6\u6d4b\u8bd5\u96c6\u5408\uff08\u5982\u4e58\u79ef\u96c6\u3001\u6307\u6570\u96c6\u7b49\uff09\u3002", "conclusion": "Myhill\u540c\u6784\u5b9a\u7406\u7684\u6269\u5c55\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u96c6\u5408\u7ed3\u6784\uff0c\u4ec5\u5bf9\u7279\u5b9a\u96c6\u5408\uff08\u5982\u5171\u81ea\u7136\u6570\u96c6\uff09\u5728\u9650\u5236\u6761\u4ef6\u4e0b\u6210\u7acb\u3002"}}
{"id": "2507.03520", "pdf": "https://arxiv.org/pdf/2507.03520", "abs": "https://arxiv.org/abs/2507.03520", "authors": ["Tiantian Feng", "Brandon M Booth", "Karel Mundnich", "Emily Zhou", "Benjamin Girault", "Kristina Lerman", "Shrikanth Narayanan"], "title": "TILES-2018 Sleep Benchmark Dataset: A Longitudinal Wearable Sleep Data Set of Hospital Workers for Modeling and Understanding Sleep Behaviors", "categories": ["cs.HC"], "comment": null, "summary": "Sleep is important for everyday functioning, overall well-being, and quality\nof life. Recent advances in wearable sensing technology have enabled\ncontinuous, noninvasive, and cost-effective monitoring of sleep patterns in\nreal-world natural living settings. Wrist-worn devices, in particular, are\ncapable of tracking sleep patterns using accelerometers and heart rate sensors.\nTo support sleep research in naturalistic environments using wearable sensors,\nwe introduce the TILES-2018 Sleep Benchmark dataset, which we make publicly\navailable to the research community. This dataset was collected over a 10-week\nperiod from 139 hospital employees and includes over 6,000 unique sleep\nrecordings, alongside self-reported survey data from each participant, which\nincludes sleep quality, stress, and anxiety among other measurements. We\npresent in-depth analyses of sleep patterns by combining the TILES-2018 Sleep\nBenchmark dataset with a previously released dataset (TILES-2018), which\nfollows a similar study protocol. Our analyses include sleep duration, sleep\nstages, and sleep diaries. Moreover, we report machine learning benchmarks\nusing this dataset as a testbed for tasks including sleep stage classification,\nprediction of self-reported sleep quality, and classifying demographics.\nOverall, this dataset provides a valuable resource for advancing foundational\nstudies in sleep behavior modeling.", "AI": {"tldr": "\u7761\u7720\u7814\u7a76\u5bf9\u65e5\u5e38\u529f\u80fd\u548c\u751f\u6d3b\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002TILES-2018\u7761\u7720\u57fa\u51c6\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u53ef\u7a7f\u6234\u8bbe\u5907\u6536\u96c6\u7684\u7761\u7720\u6570\u636e\u548c\u81ea\u62a5\u6570\u636e\uff0c\u7528\u4e8e\u7761\u7720\u6a21\u5f0f\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u7814\u7a76\u3002", "motivation": "\u901a\u8fc7\u53ef\u7a7f\u6234\u8bbe\u5907\u5728\u81ea\u7136\u73af\u5883\u4e2d\u76d1\u6d4b\u7761\u7720\u6a21\u5f0f\uff0c\u586b\u8865\u7761\u7720\u7814\u7a76\u7684\u6570\u636e\u7a7a\u767d\uff0c\u652f\u6301\u7761\u7720\u884c\u4e3a\u5efa\u6a21\u3002", "method": "\u4f7f\u7528\u624b\u8155\u53ef\u7a7f\u6234\u8bbe\u5907\uff08\u52a0\u901f\u5ea6\u8ba1\u548c\u5fc3\u7387\u4f20\u611f\u5668\uff09\u6536\u96c6139\u540d\u533b\u9662\u5458\u5de5\u7684\u7761\u7720\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u81ea\u62a5\u8c03\u67e5\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b6,000\u591a\u6b21\u7761\u7720\u8bb0\u5f55\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u9a8c\u8bc1\u4e86\u5176\u7528\u9014\uff0c\u5982\u7761\u7720\u9636\u6bb5\u5206\u7c7b\u548c\u7761\u7720\u8d28\u91cf\u9884\u6d4b\u3002", "conclusion": "TILES-2018\u6570\u636e\u96c6\u4e3a\u7761\u7720\u884c\u4e3a\u5efa\u6a21\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u53ef\u7a7f\u6234\u8bbe\u5907\u5728\u7761\u7720\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.04335", "pdf": "https://arxiv.org/pdf/2507.04335", "abs": "https://arxiv.org/abs/2507.04335", "authors": ["Yexin Yan", "Stefan Hillmich", "Robert Wille", "Christian Mayr"], "title": "Node Replacement based Approximate Quantum Simulation with Decision Diagrams", "categories": ["quant-ph", "cs.ET"], "comment": null, "summary": "Simulating a quantum circuit with a classical computer requires exponentially\ngrowing resources. Decision diagrams exploit the redundancies in quantum\ncircuit representation to efficiently represent and simulate quantum circuits.\nBut for complicated quantum circuits like the quantum supremacy benchmark,\nthere is almost no redundancy to exploit. Therefore, it often makes sense to do\na trade-off between simulation accuracy and memory requirement. Previous work\non approximate simulation with decision diagrams exploits this trade-off by\nremoving less important nodes. In this work, instead of removing these nodes,\nwe try to find similar nodes to replace them, effectively slowing down the\nfidelity loss when reducing the memory. In addition, we adopt Locality\nSensitive Hashing (LSH) to drastically reduce the computational complexity for\nsearching for replacement nodes. Our new approach achieves a better\nmemory-accuracy trade-off for representing a quantum circuit with decision\ndiagrams with minimal run time overhead. Notably, our approach shows good\nscaling properties when increasing the circuit size and depth. For the first\ntime, a strong better-than-linear trade-off between memory and fidelity is\ndemonstrated for a decision diagram based quantum simulation when representing\nthe quantum supremacy benchmark circuits at high circuit depths, showing the\npotential of drastically reducing the resource requirement for approximate\nsimulation of the quantum supremacy benchmarks on a classical computer.", "AI": {"tldr": "\u901a\u8fc7\u76f8\u4f3c\u8282\u70b9\u66ff\u6362\u800c\u4e0d\u662f\u5220\u9664\u4e0d\u91cd\u8981\u7684\u8282\u70b9\uff0c\u7ed3\u5408\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u7b97\u6cd5\uff0c\u63d0\u9ad8\u91cf\u5b50\u7535\u8def\u51b3\u7b56\u56fe\u8868\u5f81\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6a21\u62df\u590d\u6742\u91cf\u5b50\u7535\u8def\u65f6\u5185\u5b58\u9700\u6c42\u5927\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u5728\u51c6\u786e\u6027\u548c\u5185\u5b58\u4f7f\u7528\u95f4\u8fdb\u884c\u6743\u8861\u3002", "method": "\u4f7f\u7528\u76f8\u4f3c\u8282\u70b9\u66ff\u6362\u548c\u5c40\u90e8\u654f\u611f\u54c8\u5e0c\u7b97\u6cd5\u4f18\u5316\u51b3\u7b56\u56fe\u8868\u5f81\uff0c\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u9ad8\u6df1\u5ea6\u91cf\u5b50\u7535\u8def\u4e0a\u5c55\u73b0\u4e86\u4f18\u4e8e\u7ebf\u6027\u7684\u5185\u5b58-\u4fdd\u771f\u5ea6\u6743\u8861\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7ecf\u5178\u8ba1\u7b97\u673a\u4e0a\u8fd1\u4f3c\u6a21\u62df\u91cf\u5b50\u9738\u6743\u57fa\u51c6\u7535\u8def\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04785", "pdf": "https://arxiv.org/pdf/2507.04785", "abs": "https://arxiv.org/abs/2507.04785", "authors": ["Jesper Larsson Tr\u00e4ff"], "title": "Communication Round and Computation Efficient Exclusive Prefix-Sums Algorithms (for MPI_Exscan)", "categories": ["cs.DC"], "comment": null, "summary": "Parallel scan primitives compute element-wise inclusive or exclusive prefix\nsums of input vectors contributed by $p$ consecutively ranked processors under\nan associative, binary operator $\\oplus$. In message-passing systems with\nbounded, one-ported communication capabilities, at least $\\lceil\\log_2 p\\rceil$\nor $\\lceil\\log_2 (p-1)\\rceil$ communication rounds are required to perform the\nscans. While there are well-known, simple algorithms for the inclusive scan\nthat solve the problem in $\\lceil\\log_2 p\\rceil$ communication rounds with\n$\\lceil\\log_2 p\\rceil$ applications of $\\oplus$ (which could be expensive), the\nexclusive scan appears more difficult. Conventionally, the problem is solved\nwith either $\\lceil\\log_2 (p-1)\\rceil+1$ communication rounds (e.g., by\nshifting the input vectors), or in $\\lceil\\log_2 p\\rceil$ communication rounds\nwith $2\\lceil\\log_2 p\\rceil-1$ applications of $\\oplus$ (by a modified\ninclusive scan algorithm). We give a new, simple algorithm that computes the\nexclusive prefix sums in $q=\\lceil\\log_2 (p-1)+\\log_2\\frac{4}{3}\\rceil$\nsimultaneous send-receive communication rounds with $q-1$ applications of\n$\\oplus$. We compare the three algorithms implemented in MPI against the MPI\nlibrary native MPI\\_Exscan primitive on a small, $36$-node cluster with a\nstate-of-the-art MPI library, indicating possible and worthwhile improvements\nto standard implementations. The algorithms assume input vectors to be small so\nthat performance is dominated by the number of communication rounds. For large\ninput vectors, other (pipelined, fixed-degree tree) algorithms must be used.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e76\u884c\u626b\u63cf\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u8f83\u5c11\u7684\u901a\u4fe1\u8f6e\u6b21\u4e2d\u8ba1\u7b97\u6392\u4ed6\u524d\u7f00\u548c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6392\u4ed6\u626b\u63cf\u7b97\u6cd5\u901a\u4fe1\u8f6e\u6b21\u591a\u6216\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u4f5c\u8005\u65e8\u5728\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u65b0\u7b97\u6cd5\uff0c\u51cf\u5c11\u4e86\u901a\u4fe1\u8f6e\u6b21\u548c\u64cd\u4f5c\u7b26\u5e94\u7528\u6b21\u6570\u3002", "result": "\u65b0\u7b97\u6cd5\u572836\u8282\u70b9\u96c6\u7fa4\u4e0a\u4f18\u4e8eMPI\u539f\u751f\u5b9e\u73b0\u548c\u4f20\u7edf\u7b97\u6cd5\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u6709\u6f5c\u529b\u6539\u8fdb\u6807\u51c6\u5b9e\u73b0\uff0c\u4f46\u4ec5\u9002\u7528\u4e8e\u5c0f\u8f93\u5165\u5411\u91cf\u573a\u666f\u3002"}}
{"id": "2507.04776", "pdf": "https://arxiv.org/pdf/2507.04776", "abs": "https://arxiv.org/abs/2507.04776", "authors": ["Jun-You Wang", "Li Su"], "title": "Improving BERT for Symbolic Music Understanding Using Token Denoising and Pianoroll Prediction", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "comment": "Accepted at ISMIR 2025", "summary": "We propose a pre-trained BERT-like model for symbolic music understanding\nthat achieves competitive performance across a wide range of downstream tasks.\nTo achieve this target, we design two novel pre-training objectives, namely\ntoken correction and pianoroll prediction. First, we sample a portion of note\ntokens and corrupt them with a limited amount of noise, and then train the\nmodel to denoise the corrupted tokens; second, we also train the model to\npredict bar-level and local pianoroll-derived representations from the\ncorrupted note tokens. We argue that these objectives guide the model to better\nlearn specific musical knowledge such as pitch intervals. For evaluation, we\npropose a benchmark that incorporates 12 downstream tasks ranging from chord\nestimation to symbolic genre classification. Results confirm the effectiveness\nof the proposed pre-training objectives on downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBERT\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7528\u4e8e\u7b26\u53f7\u97f3\u4e50\u7406\u89e3\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u65b0\u7684\u9884\u8bad\u7ec3\u76ee\u6807\uff08token\u6821\u6b63\u548c\u94a2\u7434\u5377\u9884\u6d4b\uff09\uff0c\u5e76\u572812\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u7b26\u53f7\u97f3\u4e50\u7406\u89e3\u7684\u6027\u80fd\uff0c\u7814\u7a76\u8005\u8bbe\u8ba1\u4e86\u65b0\u7684\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u65e8\u5728\u5e2e\u52a9\u6a21\u578b\u5b66\u4e60\u7279\u5b9a\u7684\u97f3\u4e50\u77e5\u8bc6\uff08\u5982\u97f3\u9ad8\u95f4\u9694\uff09\u3002", "method": "1. token\u6821\u6b63\uff1a\u5bf9\u90e8\u5206\u97f3\u7b26token\u52a0\u5165\u566a\u58f0\u5e76\u8bad\u7ec3\u6a21\u578b\u53bb\u566a\uff1b2. \u94a2\u7434\u5377\u9884\u6d4b\uff1a\u4ece\u635f\u574f\u7684\u97f3\u7b26token\u9884\u6d4b\u6761\u7ea7\u548c\u5c40\u90e8\u94a2\u7434\u5377\u8868\u793a\u3002", "result": "\u6a21\u578b\u572812\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u9884\u8bad\u7ec3\u76ee\u6807\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u80fd\u591f\u6709\u6548\u63d0\u5347\u7b26\u53f7\u97f3\u4e50\u7406\u89e3\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.04857", "pdf": "https://arxiv.org/pdf/2507.04857", "abs": "https://arxiv.org/abs/2507.04857", "authors": ["Weiqi Wang", "Marie Farrell", "Lucas C. Cordeiro", "Liping Zhao"], "title": "Supporting Software Formal Verification with Large Language Models: An Experimental Study", "categories": ["cs.SE"], "comment": "Accepted for publication in 2025 IEEE 33rd International Requirements\n  Engineering Conference (RE)", "summary": "Formal methods have been employed for requirements verification for a long\ntime. However, it is difficult to automatically derive properties from natural\nlanguage requirements. SpecVerify addresses this challenge by integrating large\nlanguage models (LLMs) with formal verification tools, providing a more\nflexible mechanism for expressing requirements. This framework combines Claude\n3.5 Sonnet with the ESBMC verifier to form an automated workflow. Evaluated on\nnine cyber-physical systems from Lockheed Martin, SpecVerify achieves 46.5%\nverification accuracy, comparable to NASA's CoCoSim, but with lower false\npositives. Our framework formulates assertions that extend beyond the\nexpressive power of LTL and identifies falsifiable cases that are missed by\nmore traditional methods. Counterexample analysis reveals CoCoSim's limitations\nstemming from model connection errors and numerical approximation issues. While\nSpecVerify advances verification automation, our comparative study of Claude,\nChatGPT, and Llama shows that high-quality requirements documentation and human\nmonitoring remain critical, as models occasionally misinterpret specifications.\nOur results demonstrate that LLMs can significantly reduce the barriers to\nformal verification, while highlighting the continued importance of\nhuman-machine collaboration in achieving optimal results.", "AI": {"tldr": "SpecVerify\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\uff0c\u81ea\u52a8\u5316\u9a8c\u8bc1\u9700\u6c42\uff0c\u5c55\u793a46.5%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u51cf\u5c11\u4e86\u5047\u9633\u6027\u60c5\u51b5\u3002", "motivation": "\u89e3\u51b3\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u81ea\u52a8\u63d0\u53d6\u5c5e\u6027\u7684\u96be\u9898\uff0c\u63d0\u5347\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u7075\u6d3b\u6027\u3002", "method": "\u6574\u5408Claude 3.5 Sonnet\u4e0eESBMC\u9a8c\u8bc1\u5668\uff0c\u751f\u6210\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u57289\u4e2a\u6d1b\u514b\u5e0c\u5fb7\u00b7\u9a6c\u4e01\u7684\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\uff0c\u6548\u679c\u4e0eNASA\u7684CoCoSim\u76f8\u5f53\uff0c\u4f46\u5047\u9633\u6027\u66f4\u4f4e\u3002", "conclusion": "LLMs\u80fd\u663e\u8457\u964d\u4f4e\u5f62\u5f0f\u5316\u9a8c\u8bc1\u95e8\u69db\uff0c\u4f46\u9ad8\u8d28\u91cf\u9700\u6c42\u6587\u6863\u548c\u4eba\u5de5\u76d1\u63a7\u4ecd\u662f\u5173\u952e\u3002"}}
{"id": "2507.04376", "pdf": "https://arxiv.org/pdf/2507.04376", "abs": "https://arxiv.org/abs/2507.04376", "authors": ["Georgios Ioannides", "Christos Constantinou", "Vinija Jain", "Aman Chadha", "Aaron Elkins"], "title": "MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents", "categories": ["cs.AI", "cs.DC", "cs.MA", "cs.NI"], "comment": null, "summary": "As Artificial Intelligence systems evolve from monolithic models to\necosystems of specialized agents, the need for standardized communication\nprotocols becomes increasingly critical. This paper introduces MOD-X (Modular\nOpen Decentralized eXchange), a novel architectural framework proposal for\nagent interoperability that addresses key limitations of existing protocols.\nUnlike current approaches, MOD-X proposes a layered architecture with a\nUniversal Message Bus, thorough state management, translation capabilities, and\nblockchain-based security mechanisms. We present MOD-X's architecture, compare\nit with existing protocols, and demonstrate its application through a worked\nexample how it enables integration between heterogeneous specialist agents\n(agents with different architectures, vendors, capabilities, and knowledge\nrepresentations--including rule-based systems, neural networks, symbolic\nreasoning engines, and legacy software with agent wrappers). MOD-X's key\ninnovations include a publish-subscribe communication model, semantic\ncapability discovery, and dynamic workflow orchestration--providing a framework\nthat bridges theoretical formalism with practical implementation. This\narchitecture addresses the growing need for truly decentralized, interoperable\nagent ecosystems that can scale effectively without the need for central\ncoordination.", "AI": {"tldr": "MOD-X\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u5f00\u653e\u3001\u53bb\u4e2d\u5fc3\u5316\u7684\u67b6\u6784\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f02\u6784\u667a\u80fd\u4f53\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5206\u5c42\u67b6\u6784\u3001\u901a\u7528\u6d88\u606f\u603b\u7ebf\u3001\u72b6\u6001\u7ba1\u7406\u548c\u533a\u5757\u94fe\u5b89\u5168\u673a\u5236\u7b49\u521b\u65b0\u529f\u80fd\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u4ece\u5355\u4e00\u6a21\u578b\u53d1\u5c55\u4e3a\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u6807\u51c6\u5316\u901a\u4fe1\u534f\u8bae\u7684\u9700\u6c42\u65e5\u76ca\u91cd\u8981\u3002\u73b0\u6709\u7684\u534f\u8bae\u65e0\u6cd5\u6709\u6548\u652f\u6301\u5f02\u6784\u667a\u80fd\u4f53\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u63d0\u51faMOD-X\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff0c\u5305\u62ec\u901a\u7528\u6d88\u606f\u603b\u7ebf\u3001\u72b6\u6001\u7ba1\u7406\u3001\u8bed\u4e49\u80fd\u529b\u53d1\u73b0\u548c\u52a8\u6001\u5de5\u4f5c\u6d41\u7f16\u6392\uff0c\u5e76\u7ed3\u5408\u533a\u5757\u94fe\u5b89\u5168\u673a\u5236\u3002", "result": "MOD-X\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u96c6\u6210\u4e0d\u540c\u7c7b\u578b\uff08\u5982\u57fa\u4e8e\u89c4\u5219\u3001\u795e\u7ecf\u7f51\u7edc\u3001\u7b26\u53f7\u63a8\u7406\u7b49\uff09\u7684\u5f02\u6784\u667a\u80fd\u4f53\uff0c\u4e14\u65e0\u9700\u4e2d\u592e\u534f\u8c03\u5373\u53ef\u6269\u5c55\u3002", "conclusion": "MOD-X\u4e3a\u53bb\u4e2d\u5fc3\u5316\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u7ed3\u5408\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u534f\u8bae\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.05219", "pdf": "https://arxiv.org/pdf/2507.05219", "abs": "https://arxiv.org/abs/2507.05219", "authors": ["Johan van Benthem", "Thomas Icard"], "title": "Interleaving Logic and Counting", "categories": ["math.LO", "cs.CL", "cs.LO", "03B70, 03B65, 03B45"], "comment": null, "summary": "Reasoning with quantifier expressions in natural language combines logical\nand arithmetical features, transcending strict divides between qualitative and\nquantitative. Our topic is this cooperation of styles as it occurs in common\nlinguistic usage and its extension into the broader practice of natural\nlanguage plus \"grassroots mathematics\".\n  We begin with a brief review of first-order logic with counting operators and\ncardinality comparisons. This system is known to be of high complexity, and\ndrowns out finer aspects of the combination of logic and counting. We move to a\nsmall fragment that can represent numerical syllogisms and basic reasoning\nabout comparative size: monadic first-order logic with counting. We provide\nnormal forms that allow for axiomatization, determine which arithmetical\nnotions can be defined on finite and on infinite models, and conversely, we\ndiscuss which logical notions can be defined out of purely arithmetical ones,\nand what sort of (non-)classical logics can be induced.\n  Next, we investigate a series of strengthenings, again using normal form\nmethods. The monadic second-order version is close, in a precise sense, to\nadditive Presburger Arithmetic, while versions with the natural device of tuple\ncounting take us to Diophantine equations, making the logic undecidable. We\nalso define a system that combines basic modal logic over binary accessibility\nrelations with counting, needed to formulate ubiquitous reasoning patterns such\nas the Pigeonhole Principle.\n  We return to our starting point in natural language, confronting the\narchitecture of our formal systems with linguistic quantifier vocabulary and\nsyntax. We conclude with some general thoughts on yet further entanglements of\nlogic and counting in formal systems, on rethinking the\nqualitative/quantitative divide, and on connecting our analysis to empirical\nfindings in cognitive science.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u81ea\u7136\u8bed\u8a00\u4e2d\u91cf\u5316\u8868\u8fbe\u7684\u903b\u8f91\u4e0e\u7b97\u672f\u7279\u5f81\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5c0f\u578b\u903b\u8f91\u7247\u6bb5\uff0c\u7528\u4e8e\u8868\u793a\u6570\u503c\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u65b9\u6cd5\u63a2\u8ba8\u4e86\u5176\u4e0e\u7b97\u672f\u7406\u8bba\u7684\u8054\u7cfb\u3002", "motivation": "\u7814\u7a76\u81ea\u7136\u8bed\u8a00\u4e2d\u91cf\u5316\u8868\u8fbe\u7684\u903b\u8f91\u4e0e\u7b97\u672f\u7279\u5f81\u7684\u7ed3\u5408\uff0c\u8d85\u8d8a\u5b9a\u6027\u4e0e\u5b9a\u91cf\u7684\u4e25\u683c\u5212\u5206\uff0c\u4ee5\u7406\u89e3\u548c\u6269\u5c55\u8fd9\u79cd\u7ed3\u5408\u5728\u8bed\u8a00\u548c\u6570\u5b66\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u56de\u987e\u4e00\u9636\u903b\u8f91\u4e0e\u8ba1\u6570\u64cd\u4f5c\u7b26\uff0c\u63d0\u51fa\u5c0f\u578b\u903b\u8f91\u7247\u6bb5\u8868\u793a\u6570\u503c\u63a8\u7406\uff0c\u4f7f\u7528\u8303\u5f0f\u65b9\u6cd5\u8fdb\u884c\u5f3a\u5316\uff0c\u63a2\u8ba8\u5176\u4e0e\u52a0\u6027Presburger\u7b97\u672f\u548c\u4e22\u756a\u56fe\u65b9\u7a0b\u7684\u8054\u7cfb\u3002", "result": "\u786e\u5b9a\u4e86\u5728\u6709\u9650\u548c\u65e0\u9650\u6a21\u578b\u4e2d\u53ef\u4ee5\u5b9a\u4e49\u7684\u7b97\u672f\u6982\u5ff5\uff0c\u63a2\u8ba8\u4e86\u903b\u8f91\u4e0e\u7b97\u672f\u7684\u76f8\u4e92\u5b9a\u4e49\u5173\u7cfb\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ed3\u5408\u6a21\u6001\u903b\u8f91\u4e0e\u8ba1\u6570\u7684\u7cfb\u7edf\u3002", "conclusion": "\u603b\u7ed3\u4e86\u903b\u8f91\u4e0e\u8ba1\u6570\u5728\u5f62\u5f0f\u7cfb\u7edf\u4e2d\u7684\u8fdb\u4e00\u6b65\u7ed3\u5408\uff0c\u53cd\u601d\u4e86\u5b9a\u6027\u4e0e\u5b9a\u91cf\u7684\u5212\u5206\uff0c\u5e76\u5efa\u8bae\u5c06\u5176\u4e0e\u8ba4\u77e5\u79d1\u5b66\u7684\u5b9e\u8bc1\u7814\u7a76\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2507.03670", "pdf": "https://arxiv.org/pdf/2507.03670", "abs": "https://arxiv.org/abs/2507.03670", "authors": ["Nikhita Joshi", "Daniel Vogel"], "title": "Interaction Techniques that Encourage Longer Prompts Can Improve Psychological Ownership when Writing with AI", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Writing longer prompts for an AI assistant to generate a short story\nincreases psychological ownership, a user's feeling that the writing belongs to\nthem. To encourage users to write longer prompts, we evaluated two interaction\ntechniques that modify the prompt entry interface of chat-based generative AI\nassistants: pressing and holding the prompt submission button, and continuously\nmoving a slider up and down when submitting a short prompt. A within-subjects\nexperiment investigated the effects of such techniques on prompt length and\npsychological ownership, and results showed that these techniques increased\nprompt length and led to higher psychological ownership than baseline\ntechniques. A second experiment further augmented these techniques by showing\nAI-generated suggestions for how the prompts could be expanded. This further\nincreased prompt length, but did not lead to improvements in psychological\nownership. Our results show that simple interface modifications like these can\nelicit more writing from users and improve psychological ownership.", "AI": {"tldr": "\u901a\u8fc7\u4fee\u6539AI\u52a9\u624b\u63d0\u793a\u8f93\u5165\u754c\u9762\u7684\u4ea4\u4e92\u6280\u672f\uff08\u5982\u6309\u538b\u63d0\u4ea4\u6309\u94ae\u548c\u6ed1\u52a8\u6761\uff09\u53ef\u4ee5\u589e\u52a0\u7528\u6237\u63d0\u793a\u957f\u5ea6\u548c\u5fc3\u7406\u6240\u6709\u6743\u611f\uff0c\u800cAI\u751f\u6210\u5efa\u8bae\u867d\u80fd\u8fdb\u4e00\u6b65\u589e\u52a0\u63d0\u793a\u957f\u5ea6\u4f46\u5bf9\u5fc3\u7406\u6240\u6709\u6743\u65e0\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u754c\u9762\u4ea4\u4e92\u8bbe\u8ba1\u9f13\u52b1\u7528\u6237\u4e3aAI\u52a9\u624b\u7f16\u5199\u66f4\u957f\u7684\u63d0\u793a\uff0c\u4ece\u800c\u63d0\u5347\u5176\u5fc3\u7406\u6240\u6709\u6743\u611f\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u6309\u538b\u3001\u6ed1\u52a8\u6761\u7b49\u4ea4\u4e92\u6280\u672f\u53caAI\u751f\u6210\u5efa\u8bae\u5bf9\u63d0\u793a\u957f\u5ea6\u548c\u5fc3\u7406\u6240\u6709\u6743\u7684\u5f71\u54cd\u3002", "result": "\u4ea4\u4e92\u6280\u672f\u663e\u8457\u589e\u52a0\u63d0\u793a\u957f\u5ea6\u548c\u5fc3\u7406\u6240\u6709\u6743\uff0cAI\u751f\u6210\u5efa\u8bae\u4ec5\u589e\u52a0\u63d0\u793a\u957f\u5ea6\u3002", "conclusion": "\u7b80\u5355\u754c\u9762\u4fee\u6539\u53ef\u6709\u6548\u4fc3\u4f7f\u7528\u6237\u66f4\u591a\u5199\u4f5c\u5e76\u589e\u5f3a\u5fc3\u7406\u6240\u6709\u6743\u3002"}}
{"id": "2507.04648", "pdf": "https://arxiv.org/pdf/2507.04648", "abs": "https://arxiv.org/abs/2507.04648", "authors": ["Mustafa Altay Karamuftuoglu", "Changxu Song", "Beyza Zeynep Ucpinar", "Sasan Razmkhah", "Massoud Pedram"], "title": "Optimized Bistable Vortex Memory Arrays for Superconducting In-Memory Matrix-Vector Multiplication", "categories": ["cond-mat.supr-con", "cs.ET"], "comment": "arXiv admin note: text overlap with arXiv:2406.08871", "summary": "Building upon previously introduced Bistable Vortex Memory (BVM) as a novel,\nnonvolatile, high-density, and scalable superconductor memory technology, this\nwork presents a methodology that uses BVM arrays to address challenges in\ndata-driven algorithms and neural networks, specifically focusing on\nmatrix-vector multiplication (MVM). The BVM approach introduces a novel\nsuperconductor-based methodology for in-memory arithmetic, achieving\nultra-high-speed and energy-efficient computation by utilizing BVM arrays for\nin-memory computation. The design employs a tiled multiplier structure where\nBVM's inherent current summation capability is combined with Quantizer Buffer\n(QB) cells to convert the analog accumulated current into a variable number of\ndigital Single Flux Quantum (SFQ) pulses. These pulses are then processed by T1\nadder cells, which handle binary addition and carry propagation, thereby\nforming a complete functional multiplier unit. This paper thus presents an\nefficient MVM architecture that uses these BVM-based multipliers in a systolic\narray configuration to enable parallel computation. A key innovation is an\noptimized BVM array structure specifically tailored for multiplication\napplications, involving a restructuring of Sense Lines (SLs) with diagonal\nconnections to reduce area and an adjusted input scheme to enhance\ncomputational efficiency compared to the general-purpose BVM array design. We\ndemonstrate the efficacy of this approach with a 4-bit multiplier operating at\n20 GHz with 50 ps latency and an MVM structure demonstrating operation at 20\nGHz. Furthermore, we showcase how this multiplier design can be extended to\nsupport Multiply-Accumulate (MAC) operations. This work paves the way for\npower-efficient neural networks by enabling high-speed in-memory computation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u5bfc\u53cc\u7a33\u6001\u6da1\u65cb\u5b58\u50a8\u5668\uff08BVM\uff09\u7684\u5185\u5b58\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u73b0\u9ad8\u6548\u7684\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\uff08MVM\uff09\u8fd0\u7b97\uff0c\u652f\u6301\u5e76\u884c\u8ba1\u7b97\uff0c\u5177\u6709\u9ad8\u901f\u5ea6\u548c\u4f4e\u80fd\u8017\u7684\u7279\u70b9\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u7b97\u6cd5\u548c\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\uff08MVM\uff09\u6311\u6218\uff0c\u672c\u6587\u5229\u7528BVM\u6280\u672f\u5b9e\u73b0\u8d85\u9ad8\u901f\u3001\u9ad8\u80fd\u6548\u7684\u5185\u5b58\u8ba1\u7b97\uff0c\u4e3a\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u91c7\u7528BVM\u9635\u5217\u8fdb\u884c\u5185\u5b58\u7b97\u672f\u8fd0\u7b97\uff0c\u7ed3\u5408\u91cf\u5316\u7f13\u51b2\u5668\uff08QB\uff09\u5355\u5143\u548cT1\u52a0\u6cd5\u5668\uff0c\u6784\u5efa\u529f\u80fd\u5b8c\u6574\u7684\u4e58\u6cd5\u5668\u5355\u5143\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7684BVM\u9635\u5217\u7ed3\u6784\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u6210\u529f\u5b9e\u73b04\u4f4d\u4e58\u6cd5\u5668\u572820 GHz\u9891\u7387\u4e0b\u8fd0\u884c\uff0c\u5ef6\u8fdf\u4e3a50 ps\uff0c\u540c\u65f6\u5c55\u793a\u4e8620 GHz\u7684MVM\u7ed3\u6784\u8fd0\u884c\u6548\u679c\uff0c\u652f\u6301\u4e58\u52a0\uff08MAC\uff09\u8fd0\u7b97\u6269\u5c55\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u9ad8\u80fd\u6548\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u9ad8\u901f\u5185\u5b58\u8ba1\u7b97\u7684\u65b0\u9014\u5f84\uff0c\u6269\u5c55\u4e86BVM\u6280\u672f\u5728\u8ba1\u7b97\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.04786", "pdf": "https://arxiv.org/pdf/2507.04786", "abs": "https://arxiv.org/abs/2507.04786", "authors": ["Zhiyi Hu", "Siyuan Shen", "Tommaso Bonato", "Sylvain Jeaugey", "Cedell Alexander", "Eric Spada", "Jeff Hammond", "Torsten Hoefler"], "title": "Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols and Algorithms", "categories": ["cs.DC", "C.2"], "comment": null, "summary": "The NVIDIA Collective Communication Library (NCCL) is a critical software\nlayer enabling high-performance collectives on large-scale GPU clusters.\nDespite being open source with a documented API, its internal design remains\nlargely opaque. The orchestration of communication channels, selection of\nprotocols, and handling of memory movement across devices and nodes are not\nwell understood, making it difficult to analyze performance or identify\nbottlenecks. This paper presents a comprehensive analysis of NCCL, focusing on\nits communication protocol variants (Simple, LL, and LL128), mechanisms\ngoverning intra-node and inter-node data movement, and ring- and tree-based\ncollective communication algorithms. The insights obtained from this study\nserve as the foundation for ATLAHS, an application-trace-driven network\nsimulation toolchain capable of accurately reproducing NCCL communication\npatterns in large-scale AI training workloads. By demystifying NCCL's internal\narchitecture, this work provides guidance for system researchers and\nperformance engineers working to optimize or simulate collective communication\nat scale.", "AI": {"tldr": "\u672c\u6587\u5bf9NVIDIA\u96c6\u4f53\u901a\u4fe1\u5e93\uff08NCCL\uff09\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5176\u5185\u90e8\u901a\u4fe1\u534f\u8bae\u3001\u6570\u636e\u79fb\u52a8\u673a\u5236\u53ca\u7b97\u6cd5\uff0c\u4e3a\u4f18\u5316\u6216\u6a21\u62df\u5927\u89c4\u6a21\u96c6\u4f53\u901a\u4fe1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u7531\u4e8eNCCL\u5185\u90e8\u8bbe\u8ba1\u4e0d\u900f\u660e\uff0c\u96be\u4ee5\u5206\u6790\u6027\u80fd\u6216\u8bc6\u522b\u74f6\u9888\uff0c\u672c\u6587\u65e8\u5728\u63ed\u793a\u5176\u5185\u90e8\u67b6\u6784\u4ee5\u5e2e\u52a9\u7cfb\u7edf\u7814\u7a76\u4eba\u5458\u548c\u6027\u80fd\u5de5\u7a0b\u5e08\u3002", "method": "\u5206\u6790\u4e86NCCL\u7684\u901a\u4fe1\u534f\u8bae\u53d8\u4f53\uff08Simple\u3001LL\u3001LL128\uff09\u3001\u6570\u636e\u79fb\u52a8\u673a\u5236\u53ca\u73af\u72b6\u548c\u6811\u72b6\u96c6\u4f53\u901a\u4fe1\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u6210\u679c\u4e3aATLAHS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u80fd\u51c6\u786e\u6a21\u62dfNCCL\u901a\u4fe1\u6a21\u5f0f\u7684\u5e94\u7528\u8ddf\u8e2a\u9a71\u52a8\u7f51\u7edc\u4eff\u771f\u5de5\u5177\u94fe\u3002", "conclusion": "\u901a\u8fc7\u89e3\u6790NCCL\u5185\u90e8\u67b6\u6784\uff0c\u672c\u6587\u4e3a\u5927\u89c4\u6a21AI\u8bad\u7ec3\u4e2d\u7684\u96c6\u4f53\u901a\u4fe1\u4f18\u5316\u4e0e\u4eff\u771f\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2507.04955", "pdf": "https://arxiv.org/pdf/2507.04955", "abs": "https://arxiv.org/abs/2507.04955", "authors": ["Fathinah Izzati", "Xinyue Li", "Gus Xia"], "title": "EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "We propose Expotion (Facial Expression and Motion Control for Multimodal\nMusic Generation), a generative model leveraging multimodal visual controls -\nspecifically, human facial expressions and upper-body motion - as well as text\nprompts to produce expressive and temporally accurate music. We adopt\nparameter-efficient fine-tuning (PEFT) on the pretrained text-to-music\ngeneration model, enabling fine-grained adaptation to the multimodal controls\nusing a small dataset. To ensure precise synchronization between video and\nmusic, we introduce a temporal smoothing strategy to align multiple modalities.\nExperiments demonstrate that integrating visual features alongside textual\ndescriptions enhances the overall quality of generated music in terms of\nmusicality, creativity, beat-tempo consistency, temporal alignment with the\nvideo, and text adherence, surpassing both proposed baselines and existing\nstate-of-the-art video-to-music generation models. Additionally, we introduce a\nnovel dataset consisting of 7 hours of synchronized video recordings capturing\nexpressive facial and upper-body gestures aligned with corresponding music,\nproviding significant potential for future research in multimodal and\ninteractive music generation.", "AI": {"tldr": "Expotion\u662f\u4e00\u79cd\u5229\u7528\u9762\u90e8\u8868\u60c5\u3001\u4e0a\u534a\u8eab\u52a8\u4f5c\u548c\u6587\u672c\u63d0\u793a\u751f\u6210\u97f3\u4e50\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u65f6\u95f4\u5bf9\u9f50\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u751f\u6210\u97f3\u4e50\u7684\u8d28\u91cf\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u89c6\u89c9\u63a7\u5236\uff08\u9762\u90e8\u8868\u60c5\u548c\u8eab\u4f53\u52a8\u4f5c\uff09\u4e0e\u6587\u672c\u63d0\u793a\u7ed3\u5408\uff0c\u4ee5\u751f\u6210\u66f4\u5177\u8868\u73b0\u529b\u548c\u65f6\u95f4\u51c6\u786e\u6027\u7684\u97f3\u4e50\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u548c\u5c0f\u6570\u636e\u96c6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u9002\u914d\uff0c\u5f15\u5165\u65f6\u95f4\u5e73\u6ed1\u7b56\u7565\u786e\u4fdd\u591a\u6a21\u6001\u540c\u6b65\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u89c6\u89c9\u7279\u5f81\u548c\u6587\u672c\u63cf\u8ff0\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u97f3\u4e50\u7684\u591a\u4e2a\u7ef4\u5ea6\u8d28\u91cf\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u51c6\u548c\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "Expotion\u5728\u591a\u6a21\u6001\u97f3\u4e50\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u540c\u6b65\u89c6\u9891-\u97f3\u4e50\u6570\u636e\u96c6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2507.04871", "pdf": "https://arxiv.org/pdf/2507.04871", "abs": "https://arxiv.org/abs/2507.04871", "authors": ["Jerome Pfeiffer", "Jingxi Zhang", "Benoit Combemale", "Judith Michael", "Bernhard Rumpe", "Manuel Wimmer", "Andreas Wortmann"], "title": "Towards a Unifying Reference Model for Digital Twins of Cyber-Physical Systems", "categories": ["cs.SE"], "comment": null, "summary": "Digital twins are sophisticated software systems for the representation,\nmonitoring, and control of cyber-physical systems, including automotive,\navionics, smart manufacturing, and many more. Existing definitions and\nreference models of digital twins are overly abstract, impeding their\ncomprehensive understanding and implementation guidance. Consequently, a\nsignificant gap emerges between abstract concepts and their industrial\nimplementations. We analyze popular reference models for digital twins and\ncombine these into a significantly detailed unifying reference model for\ndigital twins that reduces the concept-implementation gap to facilitate their\nengineering in industrial practice. This enhances the understanding of the\nconcepts of digital twins and their relationships and guides developers to\nimplement digital twins effectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6570\u5b57\u5b6a\u751f\u53c2\u8003\u6a21\u578b\uff0c\u65e8\u5728\u5f25\u5408\u62bd\u8c61\u6982\u5ff5\u4e0e\u5de5\u4e1a\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u6570\u5b57\u5b6a\u751f\u7684\u5b9a\u4e49\u548c\u53c2\u8003\u6a21\u578b\u8fc7\u4e8e\u62bd\u8c61\uff0c\u5bfc\u81f4\u5176\u7406\u89e3\u548c\u5b9e\u65bd\u6307\u5bfc\u4e0d\u8db3\uff0c\u5de5\u4e1a\u5b9e\u8df5\u4e0e\u7406\u8bba\u95f4\u5b58\u5728\u663e\u8457\u9e3f\u6c9f\u3002", "method": "\u5206\u6790\u73b0\u6709\u6d41\u884c\u7684\u6570\u5b57\u5b6a\u751f\u53c2\u8003\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u6574\u5408\u4e3a\u4e00\u4e2a\u66f4\u8be6\u7ec6\u7684\u7edf\u4e00\u53c2\u8003\u6a21\u578b\u3002", "result": "\u7edf\u4e00\u7684\u53c2\u8003\u6a21\u578b\u51cf\u5c11\u4e86\u6982\u5ff5\u4e0e\u5b9e\u65bd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u4e86\u5de5\u4e1a\u5b9e\u8df5\u4e2d\u6570\u5b57\u5b6a\u751f\u7684\u5de5\u7a0b\u5316\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e0d\u4ec5\u589e\u5f3a\u4e86\u6570\u5b57\u5b6a\u751f\u6982\u5ff5\u53ca\u5176\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u8fd8\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u65bd\u6307\u5bfc\u3002"}}
{"id": "2507.04621", "pdf": "https://arxiv.org/pdf/2507.04621", "abs": "https://arxiv.org/abs/2507.04621", "authors": ["Yusong Zhang", "Yuxuan Sun", "Lei Guo", "Wei Chen", "Bo Ai", "Deniz Gunduz"], "title": "Multimodal LLM Integrated Semantic Communications for 6G Immersive Experiences", "categories": ["cs.LG", "cs.AI", "cs.NI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "6G networks promise revolutionary immersive communication experiences\nincluding augmented reality (AR), virtual reality (VR), and holographic\ncommunications. These applications demand high-dimensional multimodal data\ntransmission and intelligent data processing in real-time, which is extremely\nchallenging over resource-limited wireless communication systems. Moreover, a\njoint understanding of the environment, context, and user intent is essential\nto deliver task-relevant content effectively. This article presents a novel\nmultimodal large language model (MLLM) integrated semantic communications\nframework, termed MLLM-SC, which fully leverages reasoning and generative\ncapabilities of pre-trained foundation models for context-aware and\ntask-oriented wireless communication. The MLLM-SC framework adopts a\ndevice-edge collaborative architecture. At the edge, MLLM-empowered semantic\nguidance module analyzes multimodal inputs, user intents, and channel\nconditions to generate importance-aware attention maps prioritizing\nsemantically critical information. An importance-aware semantic encoder and a\nresource-adaptive semantic decoder are jointly designed and optimized, which\ncan utilize the semantic guidance for adaptive bandwidth allocation and\nhigh-quality content reconstruction or generation. Extensive case studies on\nvisual question answering for AR/VR applications and diffusion-driven image\ngeneration validate the effectiveness of MLLM-SC.", "AI": {"tldr": "6G\u7f51\u7edc\u4e2d\uff0cMLLM-SC\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u8bed\u4e49\u901a\u4fe1\uff0c\u652f\u6301\u4efb\u52a1\u5bfc\u5411\u7684\u9ad8\u6548\u65e0\u7ebf\u4f20\u8f93\u3002", "motivation": "6G\u7f51\u7edc\u9700\u652f\u6301\u9ad8\u7ef4\u591a\u6a21\u6001\u6570\u636e\u5b9e\u65f6\u4f20\u8f93\uff0c\u4f46\u8d44\u6e90\u6709\u9650\uff0c\u9700\u7ed3\u5408\u73af\u5883\u3001\u4e0a\u4e0b\u6587\u548c\u7528\u6237\u610f\u56fe\u4ee5\u5b9e\u73b0\u4efb\u52a1\u76f8\u5173\u7684\u9ad8\u6548\u901a\u4fe1\u3002", "method": "\u63d0\u51faMLLM-SC\u6846\u67b6\uff0c\u91c7\u7528\u8bbe\u5907-\u8fb9\u7f18\u534f\u4f5c\u67b6\u6784\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u8bed\u4e49\u6ce8\u610f\u529b\u56fe\u5e76\u4f18\u5316\u7f16\u89e3\u7801\u5668\u3002", "result": "\u5728AR/VR\u89c6\u89c9\u95ee\u7b54\u548c\u56fe\u50cf\u751f\u6210\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "MLLM-SC\u6846\u67b6\u57286G\u8bed\u4e49\u901a\u4fe1\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.04874", "pdf": "https://arxiv.org/pdf/2507.04874", "abs": "https://arxiv.org/abs/2507.04874", "authors": ["Wenjie Sun", "Xiaoyu Li", "Zhigang Wang", "Geng Chen", "Lianhui Yu", "Guowu Yang"], "title": "DYNAMO: Dynamic Neutral Atom Multi-programming Optimizer Towards Quantum Operating Systems", "categories": ["quant-ph", "cs.ET"], "comment": null, "summary": "As quantum computing advances towards practical applications, quantum\noperating systems become inevitable, where multi-programming -- the core\nfunctionality of operating systems -- enables concurrent execution of multiple\nquantum programs to enhance hardware utilization. However, most quantum\ncompilation work focuses solely on single-circuit execution, severely limiting\nresource efficiency and hindering quantum operating system development. We\npropose Dynamic Neutral Atom Multi-programming Optimizer (DYNAMO), a method\nthat realizes multi-programming on neutral atom quantum architectures through\nparallel compilation and intelligent resource allocation across multiple\nquantum processing units (QPUs). DYNAMO addresses two critical challenges:\ninefficient and difficult resource partitioning, and complex scheduling\nconflicts from concurrent program. Our method enables efficient spatial and\ntemporal resource sharing while maintaining circuit correctness and hardware\nconstraints. Experimental evaluation across circuits ranging from 12 to over\n1200 gates demonstrates that DYNAMO achieves up to 14.39x compilation speedup\nwhile reducing execution stages by an average of 50.47%. Furthermore, DYNAMO\nsuccessfully distributes workloads across multiple QPUs with balanced resource\nutilization. By enabling efficient multi-programming capabilities, DYNAMO\nestablishes a critical foundation towards realizing practical quantum operating\nsystems.", "AI": {"tldr": "DYNAMO\u662f\u4e00\u79cd\u591a\u7a0b\u5e8f\u91cf\u5b50\u7f16\u8bd1\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u8d44\u6e90\u5206\u914d\u548c\u5e76\u884c\u7f16\u8bd1\u63d0\u5347\u91cf\u5b50\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u5411\u5b9e\u7528\u5316\u53d1\u5c55\uff0c\u91cf\u5b50\u64cd\u4f5c\u7cfb\u7edf\u9700\u8981\u591a\u7a0b\u5e8f\u5e76\u53d1\u4ee5\u63d0\u9ad8\u786c\u4ef6\u5229\u7528\u7387\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5355\u7535\u8def\u6267\u884c\uff0c\u9650\u5236\u4e86\u8d44\u6e90\u6548\u7387\u3002", "method": "\u63d0\u51faDYNAMO\u65b9\u6cd5\uff0c\u5728\u4e2d\u6027\u539f\u5b50\u91cf\u5b50\u67b6\u6784\u4e0a\u5b9e\u73b0\u591a\u7a0b\u5e8f\u4f18\u5316\uff0c\u89e3\u51b3\u8d44\u6e90\u5206\u914d\u548c\u8c03\u5ea6\u51b2\u7a81\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cDYNAMO\u7f16\u8bd1\u901f\u5ea6\u63d0\u534714.39\u500d\uff0c\u6267\u884c\u9636\u6bb5\u51cf\u5c1150.47%\uff0c\u8d44\u6e90\u5229\u7528\u5747\u8861\u3002", "conclusion": "DYNAMO\u4e3a\u5b9e\u7528\u91cf\u5b50\u64cd\u4f5c\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u5347\u4e86\u591a\u7a0b\u5e8f\u5e76\u53d1\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2507.04960", "pdf": "https://arxiv.org/pdf/2507.04960", "abs": "https://arxiv.org/abs/2507.04960", "authors": ["Marthe Bonamy", "Cyril Gavoille", "Timoth\u00e9 Picavet", "Alexandra Wesolek"], "title": "Distributed Approximation Algorithms for Minimum Dominating Set in Locally Nice Graphs", "categories": ["cs.DC", "cs.DS"], "comment": null, "summary": "We give a new, short proof that graphs embeddable in a given Euler genus-$g$\nsurface admit a simple $f(g)$-round $\\alpha$-approximation distributed\nalgorithm for Minimum Dominating Set (MDS), where the approximation ratio\n$\\alpha \\le 906$. Using tricks from Heydt et al. [European Journal of\nCombinatorics (2025)], we in fact derive that $\\alpha \\le 34 +\\varepsilon$,\ntherefore improving upon the current state of the art of $24g+O(1)$ due to\nAmiri et al. [ACM Transactions on Algorithms (2019)]. It also improves the\napproximation ratio of $91+\\varepsilon$ due to Czygrinow et al. [Theoretical\nComputer Science (2019)] in the particular case of orientable surfaces.\n  All our distributed algorithms work in the deterministic LOCAL model. They do\nnot require any preliminary embedding of the graph and only rely on two things:\na LOCAL algorithm for MDS on planar graphs with ``uniform'' approximation\nguarantees and the knowledge that graphs embeddable in bounded Euler genus\nsurfaces have asymptotic dimension $2$.\n  More generally, our algorithms work in any graph class of bounded asymptotic\ndimension where ``most vertices'' are locally in a graph class that admits a\nLOCAL algorithm for MDS with uniform approximation guarantees.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u7b80\u77ed\u7684\u8bc1\u660e\uff0c\u8868\u660e\u5728\u7ed9\u5b9aEuler genus-$g$\u8868\u9762\u4e0a\u53ef\u5d4c\u5165\u7684\u56fe\u5177\u6709\u4e00\u4e2a\u7b80\u5355\u7684$f(g)$\u8f6e\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u6700\u5c0f\u652f\u914d\u96c6\uff08MDS\uff09\uff0c\u8fd1\u4f3c\u6bd4$\u03b1 \\le 906$\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u5c06\u5176\u964d\u81f3$\u03b1 \\le 34 +\\varepsilon$\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u7684$24g+O(1)$\u8fd1\u4f3c\u6bd4\u3002", "motivation": "\u6539\u8fdb\u5f53\u524dEuler genus-$g$\u8868\u9762\u4e0a\u53ef\u5d4c\u5165\u56fe\u7684\u6700\u5c0f\u652f\u914d\u96c6\u95ee\u9898\u5206\u5e03\u5f0f\u7b97\u6cd5\u7684\u8fd1\u4f3c\u6bd4\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u6548\u7387\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u57fa\u4e8eHeydt\u7b49\u4eba\u7684\u6280\u5de7\u548c\u5c40\u90e8\u7b97\u6cd5\uff0c\u7ed3\u5408\u56fe\u7684\u6e10\u8fd1\u7ef4\u5ea6\u7406\u8bba\uff0c\u63d0\u51fa\u65b0\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u65e0\u9700\u9884\u5148\u5d4c\u5165\u56fe\u7ed3\u6784\u3002", "result": "\u5b9e\u73b0\u4e86\u66f4\u4f4e\u4e14\u6539\u8fdb\u7684\u8fd1\u4f3c\u6bd4$\u03b1 \\le 34 +\\varepsilon$\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u786e\u5b9a\u6027LOCAL\u6a21\u578b\u4e2d\u6709\u6548\uff0c\u9002\u7528\u4e8e\u66f4\u5927\u7c7b\u522b\u7684\u56fe\u7ed3\u6784\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u7406\u8bba\u548c\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.04958", "pdf": "https://arxiv.org/pdf/2507.04958", "abs": "https://arxiv.org/abs/2507.04958", "authors": ["Kefan Tang", "Lihuo He", "Jisheng Dang", "Xinbo Gao"], "title": "Boosting Temporal Sentence Grounding via Causal Inference", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by ACM MM 2025", "summary": "Temporal Sentence Grounding (TSG) aims to identify relevant moments in an\nuntrimmed video that semantically correspond to a given textual query. Despite\nexisting studies having made substantial progress, they often overlook the\nissue of spurious correlations between video and textual queries. These\nspurious correlations arise from two primary factors: (1) inherent biases in\nthe textual data, such as frequent co-occurrences of specific verbs or phrases,\nand (2) the model's tendency to overfit to salient or repetitive patterns in\nvideo content. Such biases mislead the model into associating textual cues with\nincorrect visual moments, resulting in unreliable predictions and poor\ngeneralization to out-of-distribution examples. To overcome these limitations,\nwe propose a novel TSG framework, causal intervention and counterfactual\nreasoning that utilizes causal inference to eliminate spurious correlations and\nenhance the model's robustness. Specifically, we first formulate the TSG task\nfrom a causal perspective with a structural causal model. Then, to address\nunobserved confounders reflecting textual biases toward specific verbs or\nphrases, a textual causal intervention is proposed, utilizing do-calculus to\nestimate the causal effects. Furthermore, visual counterfactual reasoning is\nperformed by constructing a counterfactual scenario that focuses solely on\nvideo features, excluding the query and fused multi-modal features. This allows\nus to debias the model by isolating and removing the influence of the video\nfrom the overall effect. Experiments on public datasets demonstrate the\nsuperiority of the proposed method. The code is available at\nhttps://github.com/Tangkfan/CICR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u5e8f\u8bed\u53e5\u5b9a\u4f4d\uff08TSG\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u6765\u89e3\u51b3\u89c6\u9891\u548c\u6587\u672c\u67e5\u8be2\u4e4b\u95f4\u7684\u865a\u5047\u5173\u8054\u95ee\u9898\u3002", "motivation": "\u73b0\u6709TSG\u65b9\u6cd5\u5ffd\u89c6\u4e86\u89c6\u9891\u548c\u6587\u672c\u67e5\u8be2\u4e4b\u95f4\u7684\u865a\u5047\u5173\u8054\uff0c\u8fd9\u79cd\u5173\u8054\u6e90\u4e8e\u6587\u672c\u6570\u636e\u7684\u56fa\u6709\u504f\u5dee\u548c\u6a21\u578b\u5bf9\u89c6\u9891\u4e2d\u663e\u7740\u6216\u91cd\u590d\u6a21\u5f0f\u7684\u8fc7\u62df\u5408\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u9884\u6d4b\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u8bba\u6587\u9996\u5148\u4ece\u56e0\u679c\u89c6\u89d2\u6784\u5efa\u4e86TSG\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u7136\u540e\u63d0\u51fa\u6587\u672c\u56e0\u679c\u5e72\u9884\u548c\u89c6\u89c9\u53cd\u4e8b\u5b9e\u63a8\u7406\u65b9\u6cd5\uff0c\u6d88\u9664\u4e86\u672a\u89c2\u6d4b\u7684\u6df7\u6742\u56e0\u7d20\u548c\u89c6\u9891\u504f\u5dee\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff0c\u8bba\u6587\u6709\u6548\u63d0\u5347\u4e86TSG\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.05100", "pdf": "https://arxiv.org/pdf/2507.05100", "abs": "https://arxiv.org/abs/2507.05100", "authors": ["Haoran Wei", "Nazim Madhavji", "John Steinbacher"], "title": "Understanding Everything as Code: A Taxonomy and Conceptual Model", "categories": ["cs.SE"], "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025), Technical Papers track", "summary": "Background: Everything as Code (EaC) is an emerging paradigm aiming to codify\nall aspects of modern software systems. Despite its growing popularity,\ncomprehensive industry standards and peer-reviewed research clarifying its\nscope and guiding its adoption remain scarce. Aims: This study systematically\nanalyzes existing knowledge and perceptions of EaC, clarifies its scope and\nboundaries, and provides structured guidance for researchers and practitioners.\nMethod: We conducted a large-scale multivocal literature review (MLR),\nsynthesizing academic and grey literature sources. Findings were analyzed\nquantitatively and thematically. Based on this analysis, we developed a\ntaxonomy and conceptual model of EaC, validated through collaboration with\nindustry experts. Results: The resulting taxonomy comprises 25 distinct EaC\npractices organized into six layers based on industry awareness and functional\nroles. The conceptual model illustrates focus areas, overlaps, and interactions\namong these EaC practices within the software delivery lifecycle. Additionally,\npractical code examples demonstrating the implementation of these practices\nwere developed in collaboration with industry experts. Conclusions: This work\naddresses the current scarcity of academic discourse on EaC by providing the\nfirst comprehensive taxonomy and conceptual model. These contributions enhance\nconceptual clarity, offer actionable guidance to practitioners, and lay the\ngroundwork for future research in this emerging domain.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u591a\u58f0\u6587\u732e\u7efc\u8ff0\u7cfb\u7edf\u5206\u6790\u4e86Everything as Code\uff08EaC\uff09\u7684\u73b0\u6709\u77e5\u8bc6\uff0c\u63d0\u51fa\u4e86\u5305\u542b25\u79cd\u5b9e\u8df5\u7684\u5206\u7c7b\u6cd5\u548c\u6982\u5ff5\u6a21\u578b\uff0c\u586b\u8865\u4e86\u5b66\u672f\u7a7a\u767d\u3002", "motivation": "EaC\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u8303\u5f0f\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u884c\u4e1a\u6807\u51c6\u548c\u5b66\u672f\u7814\u7a76\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6f84\u6e05\u5176\u8303\u56f4\u548c\u8fb9\u754c\uff0c\u5e76\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u591a\u58f0\u6587\u732e\u7efc\u8ff0\uff08MLR\uff09\u65b9\u6cd5\uff0c\u7efc\u5408\u5b66\u672f\u548c\u7070\u8272\u6587\u732e\uff0c\u5b9a\u91cf\u548c\u4e3b\u9898\u5206\u6790\u53d1\u73b0\uff0c\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86EaC\u7684\u5206\u7c7b\u6cd5\u548c\u6982\u5ff5\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u5305\u542b25\u79cdEaC\u5b9e\u8df5\u7684\u5206\u7c7b\u6cd5\u53ca\u6982\u5ff5\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5b9e\u8df5\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u4e1a\u754c\u5408\u4f5c\u5f00\u53d1\u4e86\u4ee3\u7801\u793a\u4f8b\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86EaC\u5b66\u672f\u7a7a\u767d\uff0c\u589e\u5f3a\u4e86\u6982\u5ff5\u6e05\u6670\u5ea6\uff0c\u4e3a\u5b9e\u8df5\u8005\u548c\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.04752", "pdf": "https://arxiv.org/pdf/2507.04752", "abs": "https://arxiv.org/abs/2507.04752", "authors": ["Shuo Yang", "Xinran Zheng", "Xinchen Zhang", "Jinfeng Xu", "Jinze Li", "Donglin Xie", "Weicai Long", "Edith C. H. Ngai"], "title": "Large Language Models for Network Intrusion Detection Systems: Foundations, Implementations, and Future Directions", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized various fields with their\nexceptional capabilities in understanding, processing, and generating\nhuman-like text. This paper investigates the potential of LLMs in advancing\nNetwork Intrusion Detection Systems (NIDS), analyzing current challenges,\nmethodologies, and future opportunities. It begins by establishing a\nfoundational understanding of NIDS and LLMs, exploring the enabling\ntechnologies that bridge the gap between intelligent and cognitive systems in\nAI-driven NIDS. While Intelligent NIDS leverage machine learning and deep\nlearning to detect threats based on learned patterns, they often lack\ncontextual awareness and explainability. In contrast, Cognitive NIDS integrate\nLLMs to process both structured and unstructured security data, enabling deeper\ncontextual reasoning, explainable decision-making, and automated response for\nintrusion behaviors. Practical implementations are then detailed, highlighting\nLLMs as processors, detectors, and explainers within a comprehensive AI-driven\nNIDS pipeline. Furthermore, the concept of an LLM-centered Controller is\nproposed, emphasizing its potential to coordinate intrusion detection\nworkflows, optimizing tool collaboration and system performance. Finally, this\npaper identifies critical challenges and opportunities, aiming to foster\ninnovation in developing reliable, adaptive, and explainable NIDS. By\npresenting the transformative potential of LLMs, this paper seeks to inspire\nadvancement in next-generation network security systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63d0\u5347\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08NIDS\uff09\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5206\u6790\u4e86\u5f53\u524d\u6311\u6218\u3001\u65b9\u6cd5\u53ca\u672a\u6765\u673a\u9047\u3002", "motivation": "\u73b0\u6709\u667a\u80fdNIDS\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u800cLLMs\u80fd\u591f\u5904\u7406\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u5b89\u5168\u6570\u636e\uff0c\u5b9e\u73b0\u66f4\u6df1\u5c42\u6b21\u7684\u63a8\u7406\u4e0e\u51b3\u7b56\u3002", "method": "\u63d0\u51faLLM\u9a71\u52a8\u7684\u8ba4\u77e5NIDS\u6846\u67b6\uff0c\u5305\u62ecLLM\u4f5c\u4e3a\u5904\u7406\u5668\u3001\u68c0\u6d4b\u5668\u548c\u89e3\u91ca\u5668\uff0c\u5e76\u5efa\u8baeLLM\u4e2d\u5fc3\u63a7\u5236\u5668\u534f\u8c03\u5de5\u4f5c\u6d41\u3002", "result": "LLMs\u80fd\u663e\u8457\u63d0\u5347NIDS\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u53ef\u89e3\u91ca\u6027\u548c\u81ea\u52a8\u5316\u54cd\u5e94\u80fd\u529b\u3002", "conclusion": "LLMs\u4e3a\u4e0b\u4e00\u4ee3\u7f51\u7edc\u5b89\u5168\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u81ea\u9002\u5e94\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u9614\u7684\u53d1\u5c55\u524d\u666f\u3002"}}
{"id": "2507.03892", "pdf": "https://arxiv.org/pdf/2507.03892", "abs": "https://arxiv.org/abs/2507.03892", "authors": ["Nuo Chen", "Pu Yan", "Jia Li", "Qixuan Zhao"], "title": "Is AI mingling or bullying me? Exploring User Interactions with a Chatbot in China", "categories": ["cs.HC"], "comment": null, "summary": "Since its viral emergence in early 2024, Comment Robert-a Weibo-launched\nsocial chatbot-has gained widespread attention on the Chinese Internet for its\nunsolicited and unpredictable comments on user posts. Unlike conventional\nchatbots that respond only to user prompts, Robert autonomously intervenes in\npublic discourse, representing a novel form of AI-driven social media\nengagement. This study examines how such autonomous, algorithmic communication\nreshapes human-AI interaction in everyday online contexts. Using computational\nlinguistics techniques, including topic classification and sentiment analysis,\nwe analyze over 3,900 user-submitted interactions from the \"Robert Victims\nAlliance\", a grassroots community documenting their exchanges with the chatbot.\nTopic modeling reveals six key themes: interpersonal relationships,\nself-identity, academic and career concerns, subcultures, sensitive topics, and\nsocial events. Complementing this, mixed-methods emotional analysis uncovers a\ncomplex affective spectrum: Robert's casual remarks can evoke warmth and humor\nbut may also conceal covert hostility beneath neutral or polite language. These\nambivalent interactions reveal an emerging emotional divide between humans and\nsocially proactive AI, suggesting that while Robert simulates social presence,\nit often falls short of users' emotional needs. Our study contributes to\nhuman-AI interaction research by offering new insights into the affective\ndynamics and socio-technical implications of unsolicited AI bots' participation\nin digital public spheres.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u793e\u4ea4\u804a\u5929\u673a\u5668\u4ebaRobert\u7684\u81ea\u4e3b\u8bc4\u8bba\u884c\u4e3a\u5bf9\u4eba\u7c7b-AI\u4e92\u52a8\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u5176\u590d\u6742\u60c5\u611f\u8c31\u548c\u60c5\u611f\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76Robert\u8fd9\u79cd\u65b0\u578bAI\u9a71\u52a8\u7684\u793e\u4ea4\u5a92\u4f53\u53c2\u4e0e\u5f62\u5f0f\uff0c\u63a2\u8ba8\u5176\u5982\u4f55\u91cd\u5851\u65e5\u5e38\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u4eba\u7c7b-AI\u4e92\u52a8\u3002", "method": "\u4f7f\u7528\u8ba1\u7b97\u8bed\u8a00\u5b66\u6280\u672f\uff08\u5982\u4e3b\u9898\u5206\u7c7b\u548c\u60c5\u611f\u5206\u6790\uff09\uff0c\u5206\u6790\u4e863900\u591a\u6761\u7528\u6237\u63d0\u4ea4\u7684\u4e0eRobert\u7684\u4e92\u52a8\u6570\u636e\u3002", "result": "\u53d1\u73b0\u516d\u4e2a\u5173\u952e\u4e3b\u9898\uff0c\u60c5\u611f\u5206\u6790\u663e\u793aRobert\u7684\u8bc4\u8bba\u65e2\u80fd\u5f15\u53d1\u6e29\u6696\u4e0e\u5e7d\u9ed8\uff0c\u4e5f\u53ef\u80fd\u9690\u542b\u654c\u610f\uff0c\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0e\u81ea\u4e3bAI\u4e4b\u95f4\u7684\u60c5\u611f\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4eba\u7c7b-AI\u4e92\u52a8\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u63a2\u8ba8\u4e86\u672a\u8bf7\u6c42AI\u53c2\u4e0e\u6570\u5b57\u516c\u5171\u9886\u57df\u7684\u60c5\u611f\u52a8\u6001\u548c\u793e\u4f1a\u6280\u672f\u5f71\u54cd\u3002"}}
{"id": "2507.04969", "pdf": "https://arxiv.org/pdf/2507.04969", "abs": "https://arxiv.org/abs/2507.04969", "authors": ["Chanh Nguyen", "Erik Elmroth", "Monowar Bhuyan"], "title": "Silent Failures in Stateless Systems: Rethinking Anomaly Detection for Serverless Computing", "categories": ["cs.DC"], "comment": "12 pages, 6 figures, IEEE CISOSE 2025", "summary": "Serverless computing has redefined cloud application deployment by\nabstracting infrastructure and enabling on-demand, event-driven execution,\nthereby enhancing developer agility and scalability. However, maintaining\nconsistent application performance in serverless environments remains a\nsignificant challenge. The dynamic and transient nature of serverless functions\nmakes it difficult to distinguish between benign and anomalous behavior, which\nin turn undermines the effectiveness of traditional anomaly detection methods.\nThese conventional approaches, designed for stateful and long-running services,\nstruggle in serverless settings where executions are short-lived, functions are\nisolated, and observability is limited.\n  In this first comprehensive vision paper on anomaly detection for serverless\nsystems, we systematically explore the unique challenges posed by this\nparadigm, including the absence of persistent state, inconsistent monitoring\ngranularity, and the difficulty of correlating behaviors across distributed\nfunctions. We further examine a range of threats that manifest as anomalies,\nfrom classical Denial-of-Service (DoS) attacks to serverless-specific threats\nsuch as Denial-of-Wallet (DoW) and cold start amplification. Building on these\nobservations, we articulate a research agenda for next-generation detection\nframeworks that address the need for context-aware, multi-source data fusion,\nreal-time, lightweight, privacy-preserving, and edge-cloud adaptive\ncapabilities.\n  Through the identification of key research directions and design principles,\nwe aim to lay the foundation for the next generation of anomaly detection in\ncloud-native, serverless ecosystems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u65e0\u670d\u52a1\u8ba1\u7b97\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u6311\u6218\uff0c\u63d0\u51fa\u4e0b\u4e00\u4ee3\u68c0\u6d4b\u6846\u67b6\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u65e0\u670d\u52a1\u8ba1\u7b97\u7684\u52a8\u6001\u6027\u548c\u4e34\u65f6\u6027\u4f7f\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5931\u6548\uff0c\u9700\u65b0\u65b9\u6cd5\u5e94\u5bf9\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u65e0\u670d\u52a1\u7cfb\u7edf\u7684\u72ec\u7279\u6311\u6218\u548c\u5a01\u80c1\uff0c\u63d0\u51fa\u7814\u7a76\u8bae\u7a0b\u3002", "result": "\u660e\u786e\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u591a\u6e90\u6570\u636e\u878d\u5408\u7b49\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.05227", "pdf": "https://arxiv.org/pdf/2507.05227", "abs": "https://arxiv.org/abs/2507.05227", "authors": ["Qucheng Peng", "Chen Bai", "Guoxiang Zhang", "Bo Xu", "Xiaotong Liu", "Xiaoyin Zheng", "Chen Chen", "Cheng Lu"], "title": "NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.MM", "cs.SY", "eess.SY"], "comment": "Accepted by ACM Multimedia 2025", "summary": "Autonomous driving systems have made significant advances in Q&A, perception,\nprediction, and planning based on local visual information, yet they struggle\nto incorporate broader navigational context that human drivers routinely\nutilize. We address this critical gap between local sensor data and global\nnavigation information by proposing NavigScene, an auxiliary navigation-guided\nnatural language dataset that simulates a human-like driving environment within\nautonomous driving systems. Moreover, we develop three complementary paradigms\nto leverage NavigScene: (1) Navigation-guided Reasoning, which enhances\nvision-language models by incorporating navigation context into the prompting\napproach; (2) Navigation-guided Preference Optimization, a reinforcement\nlearning method that extends Direct Preference Optimization to improve\nvision-language model responses by establishing preferences for\nnavigation-relevant summarized information; and (3) Navigation-guided\nVision-Language-Action model, which integrates navigation guidance and\nvision-language models with conventional driving models through feature fusion.\nExtensive experiments demonstrate that our approaches significantly improve\nperformance across perception, prediction, planning, and question-answering\ntasks by enabling reasoning capabilities beyond visual range and improving\ngeneralization to diverse driving scenarios. This work represents a significant\nstep toward more comprehensive autonomous driving systems capable of navigating\ncomplex, unfamiliar environments with greater reliability and safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNavigScene\u6570\u636e\u96c6\u53ca\u4e09\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5165\u5bfc\u822a\u4e0a\u4e0b\u6587\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5168\u5c40\u5bfc\u822a\u4fe1\u606f\u6574\u5408\u4e0a\u7684\u4e0d\u8db3\uff0c\u6a21\u62df\u4eba\u7c7b\u9a7e\u9a76\u73af\u5883\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "1) \u5bfc\u822a\u5f15\u5bfc\u63a8\u7406\uff1b2) \u5bfc\u822a\u5f15\u5bfc\u504f\u597d\u4f18\u5316\uff1b3) \u5bfc\u822a\u5f15\u5bfc\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u53ca\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05200", "pdf": "https://arxiv.org/pdf/2507.05200", "abs": "https://arxiv.org/abs/2507.05200", "authors": ["Susmita Das", "Madhusudan Ghosh", "Priyanka Swami", "Debasis Ganguly", "Gul Calikli"], "title": "In-Context Learning as an Effective Estimator of Functional Correctness of LLM-Generated Code", "categories": ["cs.SE", "cs.IR"], "comment": null, "summary": "When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6ca1\u6709\u6d4b\u8bd5\u7528\u4f8b\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u793a\u4f8b\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u4f30\u8ba1\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u5feb\u901f\u5f00\u53d1\u6216\u529f\u80fd\u9a71\u52a8\u7684\u8f6f\u4ef6\u9879\u76ee\u4e2d\uff0cLLM\u751f\u6210\u7684\u4ee3\u7801\u7f3a\u4e4f\u6d4b\u8bd5\u7528\u4f8b\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5176\u529f\u80fd\u6b63\u786e\u6027\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u4f9b\u5c11\u91cf\u529f\u80fd\u6b63\u786e\u7684\u4ee3\u7801\u793a\u4f8b\uff0c\u6539\u8fdb\u73b0\u6709\u7684\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\uff08QPP\uff09\u65b9\u6cd5\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5c11\u6837\u672c\u793a\u4f8b\u80fd\u663e\u8457\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u5728\u4ee3\u7801\u8d28\u91cf\u4f30\u8ba1\u4e2d\u6709\u6548\uff0c\u4e3a\u751f\u6210\u5f0f\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.05042", "pdf": "https://arxiv.org/pdf/2507.05042", "abs": "https://arxiv.org/abs/2507.05042", "authors": ["Onur Ayan", "Jiping Luo", "Xueli An", "Nikolaos Pappas"], "title": "Age-Aware CSI Acquisition of a Finite-State Markovian Channel", "categories": ["cs.IT", "cs.NI", "math.IT"], "comment": "Accepted to be presented at the IEEE PIMRC 2025", "summary": "The Age of Information (AoI) has emerged as a critical metric for quantifying\ninformation freshness; however, its interplay with channel estimation in\npartially observable wireless systems remains underexplored. This work\nconsiders a transmitter-receiver pair communicating over an unreliable channel\nwith time-varying reliability levels. The transmitter observes the\ninstantaneous link reliability through a channel state information acquisition\nprocedure, during which the data transmission is interrupted. This leads to a\nfundamental trade-off between utilizing limited network resources for either\ndata transmission or channel state information acquisition to combat the\nchannel aging effect. Assuming the wireless channel is modeled as a\nfinite-state Markovian channel, we formulate an optimization problem as a\npartially observable Markov decision process (POMDP), obtain the optimal policy\nthrough the relative value iteration algorithm, and demonstrate the efficiency\nof our solution through simulations. To the best of our knowledge, this is the\nfirst work to aim for an optimal scheduling policy for data transmissions while\nconsidering the effect of channel state information aging.", "AI": {"tldr": "\u7814\u7a76\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff08AoI\uff09\u4e0e\u4fe1\u9053\u4f30\u8ba1\u7684\u4ea4\u4e92\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePOMDP\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u4fe1\u606f\u65b0\u9c9c\u5ea6\u4e0e\u4fe1\u9053\u4f30\u8ba1\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u89e3\u51b3\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u6570\u636e\u4f20\u8f93\u4e0e\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u83b7\u53d6\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6709\u9650\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u4fe1\u9053\u6a21\u578b\uff0c\u6784\u5efaPOMDP\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u76f8\u5bf9\u503c\u8fed\u4ee3\u7b97\u6cd5\u6c42\u89e3\u6700\u4f18\u7b56\u7565\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u89e3\u51b3\u65b9\u6848\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u9996\u6b21\u7ed3\u5408\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u8001\u5316\u6548\u5e94\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u4f20\u8f93\u7684\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u3002"}}
{"id": "2507.05043", "pdf": "https://arxiv.org/pdf/2507.05043", "abs": "https://arxiv.org/abs/2507.05043", "authors": ["Lewei Jin", "Yongqi Chen", "Kui Zhang", "Yifan Zhuo", "Yi Gao", "Bowei Yang", "Zhengong Cai", "Wei Dong"], "title": "MoLink: Distributed and Efficient Serving Framework for Large Models", "categories": ["cs.DC"], "comment": null, "summary": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u8ba8\u8bba\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6210\u672c\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d88\u8d39\u7ea7GPU\u7684\u9ad8\u6548\u5206\u5e03\u5f0fLLM\u670d\u52a1\u7cfb\u7edfMoLink\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6210\u672c\u670d\u52a1\u95ee\u9898\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u5229\u7528\u6d88\u8d39\u7ea7GPU\u6765\u964d\u4f4e\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86MoLink\u7cfb\u7edf\uff0c\u901a\u8fc7\u5173\u952e\u6280\u672f\u652f\u6301\u5728\u5f02\u6784\u548c\u5f31\u8fde\u63a5\u7684\u6d88\u8d39\u7ea7GPU\u4e0a\u9ad8\u6548\u8fd0\u884cLLM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoLink\u5728\u541e\u5410\u91cf\u548c\u6210\u672c\u6548\u76ca\u4e0a\u5206\u522b\u63d0\u5347\u81f3458%\u548c151%\u3002", "conclusion": "MoLink\u4e3a\u8de8\u5e73\u53f0\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6613\u7528\u7684LLM\u670d\u52a1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05245", "pdf": "https://arxiv.org/pdf/2507.05245", "abs": "https://arxiv.org/abs/2507.05245", "authors": ["Fatema Tuz Zohra", "Brittany Johnson"], "title": "An Investigation into Maintenance Support for Neural Networks", "categories": ["cs.SE"], "comment": "Revised version accepted at the HumanAISE Workshop, co-located with\n  FSE 2025", "summary": "As the potential for neural networks to augment our daily lives grows,\nensuring their quality through effective testing, debugging, and maintenance is\nessential. This is especially the case as we acknowledge the prospects of\nnegative impacts from these technologies. Traditional software engineering\nmethods, such as testing and debugging, have proven effective in maintaining\nsoftware quality; however, they reveal significant research and practice gaps\nin maintaining neural networks. In particular, there is a limited understanding\nof how practitioners currently address challenges related to understanding and\nmitigating undesirable behaviors in neural networks. In our ongoing research,\nwe explore the current state of research and practice in maintaining neural\nnetworks by curating insights from practitioners through a preliminary study\ninvolving interviews and supporting survey responses. Our findings thus far\nindicate that existing tools primarily concentrate on building and training\nmodels. While these tools can be beneficial, they often fall short of\nsupporting practitioners' understanding and addressing the underlying causes of\nunexpected model behavior. By evaluating current procedures and identifying the\nlimitations of traditional methodologies, our study aims to offer a\ndeveloper-centric perspective on where current practices fall short and\nhighlight opportunities for improving maintenance support in neural networks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u795e\u7ecf\u7f51\u7edc\u7ef4\u62a4\u4e2d\u7684\u6d4b\u8bd5\u3001\u8c03\u8bd5\u548c\u8d28\u91cf\u4fdd\u969c\u95ee\u9898\uff0c\u6307\u51fa\u4e86\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u65b9\u6cd5\u5728\u795e\u7ecf\u7f51\u7edc\u9886\u57df\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u5176\u8d28\u91cf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u795e\u7ecf\u7f51\u7edc\u7ef4\u62a4\u4e2d\u5b58\u5728\u663e\u8457\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u7a7a\u767d\uff0c\u5c24\u5176\u662f\u5bf9\u4e0d\u826f\u884c\u4e3a\u7684\u7406\u89e3\u548c\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8bbf\u8c08\u548c\u8c03\u67e5\uff0c\u6536\u96c6\u4ece\u4e1a\u8005\u7684\u89c1\u89e3\uff0c\u5206\u6790\u5f53\u524d\u7814\u7a76\u4e0e\u5b9e\u8df5\u7684\u73b0\u72b6\uff0c\u8bc4\u4f30\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u5de5\u5177\u4e3b\u8981\u96c6\u4e2d\u5728\u6a21\u578b\u7684\u6784\u5efa\u548c\u8bad\u7ec3\u4e0a\uff0c\u4f46\u5bf9\u7406\u89e3\u53ca\u89e3\u51b3\u6a21\u578b\u610f\u5916\u884c\u4e3a\u7684\u652f\u6301\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u5f00\u53d1\u8005\u89c6\u89d2\uff0c\u6307\u51fa\u5f53\u524d\u5b9e\u8df5\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u7ef4\u62a4\u652f\u6301\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2507.03942", "pdf": "https://arxiv.org/pdf/2507.03942", "abs": "https://arxiv.org/abs/2507.03942", "authors": ["Franklin Mingzhe Li", "Akihiko Oharazawa", "Chloe Qingyu Zhu", "Misty Fan", "Daisuke Sato", "Chieko Asakawa", "Patrick Carrington"], "title": "More than One Step at a Time: Designing Procedural Feedback for Non-visual Makeup Routines", "categories": ["cs.HC", "cs.CV"], "comment": "ASSETS 2025", "summary": "Makeup plays a vital role in self-expression, identity, and confidence - yet\nremains an underexplored domain for assistive technology, especially for people\nwith vision impairments. While existing tools support isolated tasks such as\ncolor identification or product labeling, they rarely address the procedural\ncomplexity of makeup routines: coordinating step sequences, managing product\nplacement, and assessing the final look with accessible feedback. To understand\nthe real-world process, we conducted a contextual inquiry with 15 visually\nimpaired makeup users, capturing real-time makeup application behaviors and\ntheir step-by-step information needs and assessment approaches. Our findings\nreveal embodied, tactile-first strategies; persistent challenges in blending,\nsymmetry, and assessment; and a desire for honest, real-time, goal-aligned\nfeedback. We also interviewed five professional makeup artists, who reviewed\nparticipant makeup videos and provided expert responses to participant-raised\nquestions and assessment practices. We contribute a taxonomy of feedback needs\nin non-visual makeup, and outline design implications for future assistive\nsystems - emphasizing hands-free, conversational interaction and context-aware,\nprocedural support for expressive and independent beauty practices.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u89c6\u529b\u969c\u788d\u8005\u5728\u5316\u5986\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f85\u52a9\u6280\u672f\u8bbe\u8ba1\u65b9\u6848\uff0c\u4ee5\u63d0\u4f9b\u5b9e\u65f6\u3001\u76ee\u6807\u4e00\u81f4\u7684\u53cd\u9988\u3002", "motivation": "\u5316\u5986\u5728\u81ea\u6211\u8868\u8fbe\u3001\u8eab\u4efd\u8ba4\u540c\u548c\u4fe1\u5fc3\u65b9\u9762\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5bf9\u4e8e\u89c6\u529b\u969c\u788d\u8005\u6765\u8bf4\uff0c\u73b0\u6709\u7684\u8f85\u52a9\u5de5\u5177\u672a\u80fd\u5145\u5206\u89e3\u51b3\u5316\u5986\u8fc7\u7a0b\u4e2d\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u5982\u6b65\u9aa4\u534f\u8c03\u548c\u6700\u7ec8\u6548\u679c\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5bf915\u540d\u89c6\u529b\u969c\u788d\u5316\u5986\u7528\u6237\u8fdb\u884c\u5b9e\u5730\u8c03\u7814\uff0c\u8bb0\u5f55\u4ed6\u4eec\u7684\u5316\u5986\u884c\u4e3a\u548c\u9700\u6c42\uff0c\u5e76\u91c7\u8bbf\u4e865\u540d\u4e13\u4e1a\u5316\u5986\u5e08\uff0c\u4ee5\u83b7\u53d6\u4e13\u5bb6\u53cd\u9988\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u529b\u969c\u788d\u8005\u5728\u5316\u5986\u65f6\u91c7\u7528\u7684\u89e6\u89c9\u4f18\u5148\u7b56\u7565\uff0c\u4ee5\u53ca\u5728\u5bf9\u79f0\u6027\u548c\u8bc4\u4f30\u65b9\u9762\u7684\u6301\u7eed\u6311\u6218\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u9988\u9700\u6c42\u5206\u7c7b\u6cd5\u3002", "conclusion": "\u672a\u6765\u7684\u8f85\u52a9\u7cfb\u7edf\u5e94\u6ce8\u91cd\u514d\u624b\u64cd\u4f5c\u3001\u5bf9\u8bdd\u5f0f\u4ea4\u4e92\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u652f\u6301\uff0c\u4ee5\u5e2e\u52a9\u89c6\u529b\u969c\u788d\u8005\u5b9e\u73b0\u72ec\u7acb\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u5316\u5986\u3002"}}
{"id": "2507.05230", "pdf": "https://arxiv.org/pdf/2507.05230", "abs": "https://arxiv.org/abs/2507.05230", "authors": ["Shudi Weng", "Ming Xiao", "Chao Ren", "Mikael Skoglund"], "title": "Cooperative Gradient Coding", "categories": ["cs.DC"], "comment": null, "summary": "This work studies gradient coding (GC) in the context of distributed training\nproblems with unreliable communication. We propose cooperative GC (CoGC), a\nnovel gradient-sharing-based GC framework that leverages cooperative\ncommunication among clients. This approach ultimately eliminates the need for\ndataset replication, making it both communication- and computation-efficient\nand suitable for federated learning (FL). By employing the standard GC decoding\nmechanism, CoGC yields strictly binary outcomes: either the global model is\nexactly recovered, or the decoding fails entirely, with no intermediate\nresults. This characteristic ensures the optimality of the training and\ndemonstrates strong resilience to client-to-server communication failures when\nthe communication channels among clients are in good condition. However, it may\nalso result in communication inefficiency and hinder convergence due to its\nlack of flexibility, especially when communication channels among clients are\nin poor condition. To overcome this limitation and further harness the\npotential of GC matrices, we propose a complementary decoding mechanism, termed\nGC$^+$, which leverages information that would otherwise be discarded during GC\ndecoding failures. This approach significantly improves system reliability\nunder unreliable communication, as the full recovery of the global model\ntypically dominates in GC$^+$. To conclude, this work establishes solid\ntheoretical frameworks for both CoGC and GC$^+$. We provide complete outage\nanalyses for each decoding mechanism, along with a rigorous investigation of\nhow outages affect the structure and performance of GC matrices. Building on\nthese analyses, we derive convergence bounds for both decoding mechanisms.\nFinally, the effectiveness of CoGC and GC$^+$ is validated through extensive\nsimulations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoGC\u7684\u65b0\u578b\u68af\u5ea6\u5171\u4eab\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u4e0d\u53ef\u9760\u901a\u4fe1\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u4e92\u8865\u89e3\u7801\u673a\u5236GC\u207a\u63d0\u9ad8\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u56e0\u901a\u4fe1\u4e0d\u53ef\u9760\u5bfc\u81f4\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u51cf\u5c11\u6570\u636e\u590d\u5236\u9700\u6c42\u5e76\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u3002", "method": "\u91c7\u7528\u4e86CoGC\u6846\u67b6\u548cGC\u207a\u89e3\u7801\u673a\u5236\uff0c\u524d\u8005\u5b9e\u73b0\u4e25\u683c\u7684\u4e8c\u5143\u7ed3\u679c\uff0c\u540e\u8005\u5229\u7528\u5931\u8d25\u65f6\u88ab\u4e22\u5f03\u7684\u4fe1\u606f\u63d0\u9ad8\u6548\u7387\u3002", "result": "CoGC\u5728\u5ba2\u6237\u7aef\u901a\u4fe1\u826f\u597d\u65f6\u8868\u73b0\u51fa\u5f3a\u97e7\u6027\uff0c\u800cGC\u207a\u5728\u901a\u4fe1\u5dee\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u3002\u7406\u8bba\u5206\u6790\u548c\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u7f16\u7801\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2507.02976", "pdf": "https://arxiv.org/pdf/2507.02976", "abs": "https://arxiv.org/abs/2507.02976", "authors": ["Amirali Sajadi", "Kostadin Damevski", "Preetha Chatterjee"], "title": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench", "categories": ["cs.CR", "cs.LG", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u4ee3\u7801\u4fee\u8865\u7a0b\u5e8f\u5728\u771f\u5b9e\u5f00\u53d1\u73af\u5883\u4e2d\u5f15\u5165\u7684\u5b89\u5168\u6f0f\u6d1e\u6bd4\u5f00\u53d1\u8005\u591a9\u500d\uff0c\u4e14\u4ee3\u7406\u6846\u67b6\u5728\u9ad8\u81ea\u4e3b\u6743\u65f6\u4e5f\u6613\u4ea7\u751f\u6f0f\u6d1e\u3002", "motivation": "\u63a2\u7a76LLM\u53ca\u5176\u4ee3\u7406\u6846\u67b6\u5728\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u6027\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5bf9\u73b0\u5b9e\u73af\u5883\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "method": "\u57fa\u4e8eSWE-bench\u6570\u636e\u96c6\u768420,000+\u95ee\u9898\uff0c\u5206\u6790LLM\uff08Llama 3.3\uff09\u548c\u5f00\u53d1\u8005\u7f16\u5199\u7684\u4fee\u8865\u7a0b\u5e8f\uff0c\u5e76\u8bc4\u4f30\u4e09\u79cd\u4ee3\u7406\u6846\u67b6\uff08OpenHands\u7b49\uff09\u7684\u751f\u6210\u5185\u5bb9\u3002", "result": "LLM\u4fee\u8865\u7a0b\u5e8f\u5f15\u5165\u65b0\u6f0f\u6d1e\u7684\u6982\u7387\u662f\u5f00\u53d1\u8005\u76849\u500d\uff0c\u4ee3\u7406\u6846\u67b6\u5728\u9ad8\u81ea\u4e3b\u6743\u65f6\u6613\u51fa\u9519\uff1b\u6f0f\u6d1e\u66f4\u6613\u51fa\u73b0\u5728\u6d89\u53ca\u591a\u6587\u4ef6\u3001\u591a\u4ee3\u7801\u884c\u6216\u4fe1\u606f\u4e0d\u5b8c\u6574\u7684Issue\u4e2d\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u56e0\u7d20\u5bf9\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u9700\u5f00\u53d1\u7ed3\u5408\u4ee3\u7801\u548c\u4efb\u52a1\u4fe1\u606f\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u8865\u5145\u73b0\u6709\u5de5\u5177\u3002"}}
{"id": "2507.04005", "pdf": "https://arxiv.org/pdf/2507.04005", "abs": "https://arxiv.org/abs/2507.04005", "authors": ["Baiqiao Zhang", "Xiangxian Li", "Chao Zhou", "Xinyu Gai", "Zhifeng Liao", "Juan Liu", "Xue Yang", "Niqi Liu", "Xiaojuan Ma", "Yong-jin Liu", "Yulong Bian"], "title": "Exploring a Gamified Personality Assessment Method through Interaction with Multi-Personality LLM Agents", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "The execution of effective and imperceptible personality assessments is\nreceiving increasing attention in psychology and human-computer interaction\nfields. This study explores an interactive approach for personality assessment,\nfocusing on the multiplicity of personality representation. We propose a\nframework of gamified personality assessment through multi-personality\nrepresentations (Multi-PR GPA). The framework leverages Large Language Models\nto empower virtual agents with diverse personalities. These agents elicit\nmultifaceted human personality representations through engaging in interactive\ngames. Drawing upon the multi-type textual data generated throughout the\ninteraction, it achieves two ways of personality assessments (i.e., Direct\nAssessment and Que-based Assessment) and provides interpretable insights.\nGrounded in the classic Big Five theory, we implemented a prototype system and\nconducted a user study to assess the efficacy of Multi-PR GPA. The results\nunderscore the effectiveness of our approach in personality assessment and\ndemonstrate that it achieves superior performance when considering the\nmultiplicity of personality representation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6027\u683c\u8868\u5f81\u7684\u6e38\u620f\u5316\u4eba\u683c\u8bc4\u4f30\u6846\u67b6Multi-PR GPA\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u865a\u62df\u89d2\u8272\uff0c\u901a\u8fc7\u4e92\u52a8\u6e38\u620f\u5b9e\u73b0\u4e24\u79cd\u4eba\u683c\u8bc4\u4f30\u65b9\u5f0f\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u4ea4\u4e92\u5f0f\u4eba\u683c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5173\u6ce8\u4eba\u683c\u8868\u5f81\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faMulti-PR GPA\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u865a\u62df\u89d2\u8272\uff0c\u901a\u8fc7\u4e92\u52a8\u6e38\u620f\u6536\u96c6\u591a\u7c7b\u578b\u6587\u672c\u6570\u636e\uff0c\u5b9e\u73b0\u76f4\u63a5\u8bc4\u4f30\u548c\u57fa\u4e8e\u961f\u5217\u7684\u8bc4\u4f30\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u9a8c\u8bc1\u4e86Multi-PR GPA\u5728\u4eba\u683c\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u8003\u8651\u4eba\u683c\u8868\u5f81\u591a\u6837\u6027\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "Multi-PR GPA\u6846\u67b6\u4e3a\u57fa\u4e8e\u591a\u6027\u683c\u8868\u5f81\u7684\u4eba\u683c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u89e3\u91ca\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03117", "pdf": "https://arxiv.org/pdf/2507.03117", "abs": "https://arxiv.org/abs/2507.03117", "authors": ["Patrik Okanovic", "Sameer Deshmukh", "Grzegorz Kwasniewski", "Kentaro Katayama", "Takumi Honda", "Maciej Besta", "Torsten Hoefler"], "title": "BLaST: High Performance Inference and Pretraining using BLock Sparse Transformers", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "The energy consumption of large-scale ML models is dominated by data movement\n- shuffling billions of parameters across memory hierarchies and data centers.\nEffective sparsification to prune redundant parameters is still challenging:\nexisting methods incur significant accuracy degradation, performance overhead,\nor both. We introduce (Bl)ock (a)nd (S)parse (T)ransformers (BLaST), a general,\nrobust, and reliable sparsification method applicable to linear layers in all\nsettings. Our method iteratively sparsifies weight matrices into a block\nsparsity pattern suitable for efficient sparse matrix-matrix (SpMM)\nmultiplication. BLaST achieves up to 95% sparsity in MLP weights with\nnegligible accuracy loss. Our fused, highly optimized Sparse MLP kernel\ndelivers up to 16.7x speedup over dense MLPs across 9 architectures and 8\ndatasets, resulting in up to 1.6x inference speedup, 1.11x pretraining speedup\nand up to 3.12x inference memory usage reduction. BLaST enables the next\ngeneration of large-scale AI systems by reducing energy use, memory footprint,\nand latency.", "AI": {"tldr": "BLaST\u662f\u4e13\u4e3a\u5927\u578bML\u6a21\u578b\u8bbe\u8ba1\u7684\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5757\u7a00\u758f\u6a21\u5f0f\u663e\u8457\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u548c\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21ML\u6a21\u578b\u4e2d\u6570\u636e\u4f20\u8f93\u80fd\u8017\u9ad8\u53ca\u73b0\u6709\u7a00\u758f\u5316\u65b9\u6cd5\u7684\u6027\u80fd\u4e0e\u7cbe\u5ea6\u635f\u5931\u95ee\u9898\u3002", "method": "BLaST\u901a\u8fc7\u8fed\u4ee3\u7a00\u758f\u5316\u6743\u91cd\u77e9\u9635\u4e3a\u9002\u5408\u9ad8\u6548\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u7684\u5757\u7a00\u758f\u6a21\u5f0f\u3002", "result": "\u5b9e\u73b0\u9ad8\u8fbe95%\u7a00\u758f\u5ea6\uff0c\u6027\u80fd\u63d0\u534716.7\u500d\uff0c\u80fd\u6548\u663e\u8457\u4f18\u5316\u3002", "conclusion": "BLaST\u5728\u51cf\u5c11\u80fd\u8017\u3001\u5185\u5b58\u5360\u7528\u548c\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u63a8\u52a8\u5927\u89c4\u6a21AI\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2507.03344", "pdf": "https://arxiv.org/pdf/2507.03344", "abs": "https://arxiv.org/abs/2507.03344", "authors": ["Jason Zhijingcheng Yu", "Fangqi Han", "Kaustab Choudhury", "Trevor E. Carlson", "Prateek Saxena"], "title": "Securing Mixed Rust with Hardware Capabilities", "categories": ["cs.CR", "cs.SE", "C.1.3; D.2.5"], "comment": "To appear at CCS '25", "summary": "The Rust programming language enforces three basic Rust principles, namely\nownership, borrowing, and AXM (Aliasing Xor Mutability) to prevent security\nbugs such as memory safety violations and data races. However, Rust projects\noften have mixed code, i.e., code that also uses unsafe Rust, FFI (Foreign\nFunction Interfaces), and inline assembly for low-level control. The Rust\ncompiler is unable to statically enforce Rust principles in mixed Rust code\nwhich can lead to many security vulnerabilities. In this paper, we propose\nCapsLock, a security enforcement mechanism that can run at the level of machine\ncode and detect Rust principle violations at run-time in mixed code. CapsLock\nis kept simple enough to be implemented into recent capability-based hardware\nabstractions that provide low-cost spatial memory safety. CapsLock introduces a\nnovel revoke-on-use abstraction for capability-based designs, wherein accessing\na memory object via a capability implicitly invalidates certain other\ncapabilities pointing to it, thereby also providing temporal memory safety\nautomatically, without requiring software to explicitly specify such\ninvalidation. Thus, CapsLock is the first mechanism capable of providing\ncross-language enforcement of Rust principles. We implemented a prototype of\nCapsLock on QEMU. Evaluation results show that CapsLock is highly compatible\nwith existing Rust code (passing 99.7% of the built-in test cases of the 100\nmost popular crates) and flags Rust principle violations in real-world Rust\nprojects that use FFI or inline assembly. We discovered 8 previously unknown\nbugs in such crates in our experiments.", "AI": {"tldr": "CapsLock\u662f\u4e00\u79cd\u8fd0\u884c\u65f6\u5b89\u5168\u673a\u5236\uff0c\u901a\u8fc7\u80fd\u529b\u57fa\u786c\u4ef6\u62bd\u8c61\u68c0\u6d4b\u6df7\u5408Rust\u4ee3\u7801\u4e2d\u7684\u539f\u5219\u8fdd\u53cd\uff0c\u63d0\u4f9b\u8de8\u8bed\u8a00\u5185\u5b58\u5b89\u5168\u4fdd\u62a4\u3002", "motivation": "Rust\u9879\u76ee\u5e38\u4f7f\u7528\u4e0d\u5b89\u5168\u4ee3\u7801\u3001FFI\u548c\u5185\u8054\u6c47\u7f16\uff0c\u7f16\u8bd1\u5668\u65e0\u6cd5\u9759\u6001\u68c0\u67e5\u8fd9\u4e9b\u6df7\u5408\u4ee3\u7801\uff0c\u5bfc\u81f4\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faCapsLock\u673a\u5236\uff0c\u57fa\u4e8e\u80fd\u529b\u57fa\u786c\u4ef6\u8bbe\u8ba1\uff0c\u5b9e\u73b0revoke-on-use\u62bd\u8c61\uff0c\u8bbf\u95ee\u5185\u5b58\u65f6\u81ea\u52a8\u64a4\u9500\u76f8\u5173\u80fd\u529b\uff0c\u65e0\u9700\u663e\u5f0f\u65e0\u6548\u5316\u3002", "result": "\u539f\u578b\u5728QEMU\u4e0a\u5b9e\u73b0\uff0c\u517c\u5bb9\u6027\u9ad8\uff0899.7%\u6d4b\u8bd5\u901a\u8fc7\uff09\uff0c\u68c0\u6d4b\u52308\u4e2a\u672a\u77e5\u6f0f\u6d1e\u3002", "conclusion": "CapsLock\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9Rust\u539f\u5219\u7684\u8de8\u8bed\u8a00\u8fd0\u884c\u65f6\u5b89\u5168\u4fdd\u62a4\uff0c\u9002\u7528\u4e8e\u6df7\u5408\u4ee3\u7801\u73af\u5883\u3002"}}
{"id": "2507.04043", "pdf": "https://arxiv.org/pdf/2507.04043", "abs": "https://arxiv.org/abs/2507.04043", "authors": ["Kai Deng"], "title": "Evaluating the Effectiveness of Large Language Models in Solving Simple Programming Tasks: A User-Centered Study", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become more common in educational tools and\nprogramming environments, questions arise about how these systems should\ninteract with users. This study investigates how different interaction styles\nwith ChatGPT-4o (passive, proactive, and collaborative) affect user performance\non simple programming tasks. I conducted a within-subjects experiment where\nfifteen high school students participated, completing three problems under\nthree distinct versions of the model. Each version was designed to represent a\nspecific style of AI support: responding only when asked, offering suggestions\nautomatically, or engaging the user in back-and-forth dialogue.Quantitative\nanalysis revealed that the collaborative interaction style significantly\nimproved task completion time compared to the passive and proactive conditions.\nParticipants also reported higher satisfaction and perceived helpfulness when\nworking with the collaborative version. These findings suggest that the way an\nLLM communicates, how it guides, prompts, and responds, can meaningfully impact\nlearning and performance. This research highlights the importance of designing\nLLMs that go beyond functional correctness to support more interactive,\nadaptive, and user-centered experiences, especially for novice programmers.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86ChatGPT-4o\u7684\u4e0d\u540c\u4ea4\u4e92\u65b9\u5f0f\uff08\u88ab\u52a8\u3001\u4e3b\u52a8\u3001\u534f\u4f5c\uff09\u5bf9\u9ad8\u4e2d\u751f\u5b8c\u6210\u7f16\u7a0b\u4efb\u52a1\u7684\u5f71\u54cd\u3002\u534f\u4f5c\u65b9\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u968f\u7740LLMs\u5728\u6559\u80b2\u5de5\u5177\u4e2d\u7684\u666e\u53ca\uff0c\u5982\u4f55\u8bbe\u8ba1\u5176\u4ea4\u4e92\u65b9\u5f0f\u4ee5\u4f18\u5316\u7528\u6237\u4f53\u9a8c\u548c\u5b66\u4e60\u6548\u679c\u6210\u4e3a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ec4\u5185\u5b9e\u9a8c\u8bbe\u8ba1\uff0c15\u540d\u9ad8\u4e2d\u751f\u5728\u4e0d\u540c\u4ea4\u4e92\u6a21\u5f0f\u7684ChatGPT-4o\u4e0b\u5b8c\u6210\u7f16\u7a0b\u4efb\u52a1\uff0c\u91cf\u5316\u5206\u6790\u8868\u73b0\u548c\u53cd\u9988\u3002", "result": "\u534f\u4f5c\u4ea4\u4e92\u65b9\u5f0f\u5728\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "LLMs\u7684\u4ea4\u4e92\u8bbe\u8ba1\u5bf9\u5b66\u4e60\u6548\u679c\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u6ce8\u91cd\u4ea4\u4e92\u6027\u548c\u7528\u6237\u4e2d\u5fc3\u5316\uff0c\u5c24\u5176\u5bf9\u521d\u5b66\u8005\u3002"}}
{"id": "2507.03258", "pdf": "https://arxiv.org/pdf/2507.03258", "abs": "https://arxiv.org/abs/2507.03258", "authors": ["Zhaorun Lin"], "title": "Novel Blockchain-based Protocols for Electronic Voting and Auctions", "categories": ["cs.CR", "cs.DC"], "comment": "My thesis for MPhil at HKUST", "summary": "Programmable blockchains have long been a hot research topic given their\ntremendous use in decentralized applications. Smart contracts, using\nblockchains as their underlying technology, inherit the desired properties such\nas verifiability, immutability, and transparency, which make it a great suit in\ntrustless environments.\n  In this thesis, we consider several decentralized protocols to be built on\nblockchains, specifically using smart contracts on Ethereum. We used\nalgorithmic and cryptographic tools in our implementations to further improve\nthe level of security and efficiency beyond the state-of-the-art works. We\nproposed a new approach called Blind Vote, which is an untraceable, secure,\nefficient, secrecy-preserving, and fully on-chain electronic voting protocol\nbased on the well-known concept of Chaum's blind signatures. We illustrate that\nour approach achieves the same security guarantees as previous methods such as\nTornado Vote [1], while consuming significantly less gas. Thus, we provide a\ncheaper and considerably more gas-efficient alternative for anonymous\nblockchain-based voting. On the other hand, we propose a new family of\nalgorithms for private, trustless auctions that protect bidder identities and\nbid values while remaining practical for smart contract execution. We ensure\ntrustlessness by running the auction logic in a smart contract, thereby\neliminating reliance on any single trusted party. This approach prevents bid\ntampering, front-running, and collusion by enforcing immutability and\ndecentralized verification of bids. The resulting protocol uniquely combines\nefficiency, trustlessness, and enduring bid privacy, offering a scalable and\nsecure solution for blockchain-based marketplaces and other decentralized\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u9ad8\u6548\u3001\u5b89\u5168\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u7535\u5b50\u6295\u7968\u534f\u8bae\uff08Blind Vote\uff09\u548c\u79c1\u5bc6\u62cd\u5356\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86Gas\u6d88\u8017\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u53bb\u4e2d\u5fc3\u5316\u548c\u4e0d\u53ef\u7be1\u6539\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u73b0\u6709\u533a\u5757\u94fe\u534f\u8bae\u7684\u5b89\u5168\u6027\u4e0e\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u7535\u5b50\u6295\u7968\u548c\u62cd\u5356\u9886\u57df\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u6210\u672c\u6548\u76ca\u3002", "method": "\u5229\u7528\u7b97\u6cd5\u548c\u5bc6\u7801\u5b66\u5de5\u5177\uff08\u5982Chaum\u76f2\u7b7e\u540d\uff09\u5728\u4ee5\u592a\u574a\u667a\u80fd\u5408\u7ea6\u4e0a\u5b9e\u73b0\u7535\u5b50\u6295\u7968\u534f\u8bae\uff08Blind Vote\uff09\u548c\u79c1\u5bc6\u62cd\u5356\u7b97\u6cd5\uff0c\u5f3a\u8c03\u53bb\u4e2d\u5fc3\u5316\u548cGas\u6548\u7387\u3002", "result": "Blind Vote\u6bd4Tornado Vote\u7b49\u73b0\u6709\u65b9\u6848\u66f4\u7701Gas\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u5b89\u5168\u6027\uff1b\u62cd\u5356\u7b97\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u9632\u6b62\u4e32\u901a\u548c\u7be1\u6539\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u8bae\u5728\u5b89\u5168\u6027\u3001\u9690\u79c1\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04095", "pdf": "https://arxiv.org/pdf/2507.04095", "abs": "https://arxiv.org/abs/2507.04095", "authors": ["Alireza Mortezapour", "Giuliana Vitiello"], "title": "Human-centered AI with focus on Human-robot interaction (Book chapter)", "categories": ["cs.HC", "cs.AI", "cs.RO"], "comment": null, "summary": "Modern social robots can be considered the descendants of steam engines from\nthe First Industrial Revolution (IR 1.0) and industrial robotic arms from the\nThird Industrial Revolution (IR 3.0). As some time has passed since the\nintroduction of these robots during the Fourth Industrial Revolution (IR 4.0),\nchallenges and issues in their interaction with humans have emerged, leading\nresearchers to conclude that, like any other AI-based technology, these robots\nmust also be human-centered to meet the needs of their users. This chapter aims\nto introduce humans and their needs in interactions with robots, ranging from\nshort-term, one-on-one interactions (micro-level) to long-term, macro-level\nneeds at the societal scale. Building upon the principles of human-centered AI,\nthis chapter presents, for the first time, a new framework of human needs\ncalled the Dual Pyramid. This framework encompasses a comprehensive list of\nhuman needs in robot interactions, from the most fundamental, robot\neffectiveness to macro level requirements, such as the collaboration with\nrobots in achieving the United Nations 17 Sustainable Development Goals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53cc\u91d1\u5b57\u5854\u201d\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63cf\u8ff0\u4eba\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u4e2d\u7684\u591a\u5c42\u6b21\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u793e\u4ea4\u673a\u5668\u4eba\u5728\u7b2c\u56db\u6b21\u5de5\u4e1a\u9769\u547d\u4e2d\u7684\u5e94\u7528\uff0c\u5176\u4e0e\u4eba\u7c7b\u4e92\u52a8\u4e2d\u7684\u6311\u6218\u548c\u95ee\u9898\u9010\u6e10\u663e\u73b0\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u601d\u8003\u5982\u4f55\u4f7f\u8fd9\u4e9b\u673a\u5668\u4eba\u66f4\u52a0\u4eba\u6027\u5316\u4ee5\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u539f\u5219\uff0c\u9996\u6b21\u63d0\u51fa\u201c\u53cc\u91d1\u5b57\u5854\u201d\u6846\u67b6\uff0c\u6db5\u76d6\u4ece\u57fa\u672c\u673a\u5668\u4eba\u6548\u80fd\u5230\u8054\u5408\u56fd17\u9879\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u7b49\u5b8f\u89c2\u9700\u6c42\u7684\u591a\u5c42\u6b21\u4eba\u7c7b\u9700\u6c42\u3002", "result": "\u901a\u8fc7\u201c\u53cc\u91d1\u5b57\u5854\u201d\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u5c55\u793a\u4e86\u4eba\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u4e2d\u7684\u591a\u5c42\u6b21\u9700\u6c42\uff0c\u4e3a\u672a\u6765\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u201c\u53cc\u91d1\u5b57\u5854\u201d\u6846\u67b6\u4e3a\u7406\u89e3\u4eba\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u7684\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u9762\u5411\u4eba\u7c7b\u9700\u6c42\u7684\u8bbe\u8ba1\u5728\u673a\u5668\u4eba\u6280\u672f\u53d1\u5c55\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.04055", "pdf": "https://arxiv.org/pdf/2507.04055", "abs": "https://arxiv.org/abs/2507.04055", "authors": ["Yufan Chen", "Daoyuan Wu", "Juantao Zhong", "Zicheng Zhang", "Debin Gao", "Shuai Wang", "Yingjiu Li", "Ning Liu"], "title": "Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "Malware Family Classification (MFC) aims to identify the fine-grained family\n(e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in\ncontrast to malware detection or sample classification that predicts only an\nYes/No. Accurate family identification can greatly facilitate automated sample\nlabeling and understanding on crowdsourced malware analysis platforms such as\nVirusTotal and MalwareBazaar, which generate vast amounts of data daily. In\nthis paper, we explore and assess the feasibility of using traditional binary\nstring features for MFC in the new era of large language models (LLMs) and\nRetrieval-Augmented Generation (RAG). Specifically, we investigate how\nFamily-Specific String (FSS) features could be utilized in a manner similar to\nRAG to facilitate MFC. To this end, we develop a curated evaluation framework\ncovering 4,347 samples from 67 malware families, extract and analyze over 25\nmillion strings, and conduct detailed ablation studies to assess the impact of\ndifferent design choices in four major modules.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u4f20\u7edf\u4e8c\u8fdb\u5236\u5b57\u7b26\u4e32\u7279\u5f81\u5728LLM\u548cRAG\u65f6\u4ee3\u8fdb\u884c\u6076\u610f\u8f6f\u4ef6\u5bb6\u65cf\u5206\u7c7b\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u6076\u610f\u8f6f\u4ef6\u5bb6\u65cf\u5206\u7c7b\uff08MFC\uff09\u80fd\u81ea\u52a8\u5316\u6807\u8bb0\u6837\u672c\u5e76\u7406\u89e3\u5927\u89c4\u6a21\u6076\u610f\u5206\u6790\u5e73\u53f0\u6570\u636e\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u65b0\u6280\u672f\u4e0b\u7684\u9002\u7528\u6027\u503c\u5f97\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5bb6\u65cf\u7279\u5b9a\u5b57\u7b26\u4e32\uff08FSS\uff09\u7279\u5f81\uff0c\u7c7b\u4f3cRAG\u7684\u65b9\u5f0f\uff0c\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\u5206\u679025\u767e\u4e07\u5b57\u7b26\u4e32\uff0c\u7814\u7a7667\u5bb6\u65cf\u76844,347\u6837\u672c\u3002", "result": "\u8be6\u7ec6\u6d88\u878d\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u56db\u5927\u6a21\u5757\u4e2d\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\u7684\u5f71\u54cd\u3002", "conclusion": "\u4f20\u7edf\u5b57\u7b26\u4e32\u7279\u5f81\u5728\u65b0\u6280\u672f\u80cc\u666f\u4e0b\u4ecd\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u8bbe\u8ba1\u3002"}}
{"id": "2507.04160", "pdf": "https://arxiv.org/pdf/2507.04160", "abs": "https://arxiv.org/abs/2507.04160", "authors": ["Subasish Das"], "title": "HyperSumm-RL: A Dialogue Summarization Framework for Modeling Leadership Perception in Social Robots", "categories": ["cs.HC"], "comment": "6 pages with references", "summary": "This paper introduces HyperSumm-RL, a hypertext-aware summarization and\ninteraction analysis framework designed to investigate human perceptions of\nsocial robot leadership through long-form dialogue. The system utilizes a\nstructured Natural Language Processing (NLP) workflow that combines\ntransformer-based long dialogue summarization, leadership style modeling, and\nuser response analysis, enabling scalable evaluation of social robots in\ncomplex human-robot interaction (HRI) settings. Unlike prior work that focuses\non static or task-oriented HRI, HyperSumm-RL captures and hypertextually\norganizes dynamic conversational exchanges into navigable, semantically rich\nrepresentations which allows researchers to trace interaction threads, identify\ninfluence cues, and analyze leadership framing over time. The contributions of\nthis study are threefold: (1) we present a novel infrastructure for summarizing\nand linking long, multi-turn dialogues using leadership-style taxonomies; (2)\nwe propose an interactive hypertext model that supports relational navigation\nacross conversational themes, participant responses, and robot behavior modes;\nand (3) we demonstrate the utility of this system in interpreting participant\ntrust, engagement, and expectation shifts during social robot leadership\nscenarios. The findings reveal how hypertextual workflows can augment HRI\nresearch by enabling transparent, interpretable, and semantically grounded\nanalysis of emergent social dynamics.", "AI": {"tldr": "HyperSumm-RL\u662f\u4e00\u4e2a\u57fa\u4e8e\u8d85\u6587\u672c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u793e\u4ea4\u673a\u5668\u4eba\u9886\u5bfc\u529b\u7684\u4eba\u7c7b\u611f\u77e5\uff0c\u901a\u8fc7\u957f\u5bf9\u8bdd\u603b\u7ed3\u548c\u4ea4\u4e92\u5206\u6790\uff0c\u652f\u6301\u52a8\u6001\u5bf9\u8bdd\u7684\u8bed\u4e49\u5316\u7ec4\u7ec7\u548c\u5bfc\u822a\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5bf9\u793e\u4ea4\u673a\u5668\u4eba\u9886\u5bfc\u529b\u7684\u611f\u77e5\uff0c\u5f25\u8865\u73b0\u6709\u9759\u6001\u6216\u4efb\u52a1\u5bfc\u5411\u7684\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u57fa\u4e8eTransformer\u7684\u957f\u5bf9\u8bdd\u603b\u7ed3\u3001\u9886\u5bfc\u98ce\u683c\u5efa\u6a21\u548c\u7528\u6237\u54cd\u5e94\u5206\u6790\uff0c\u6784\u5efa\u8d85\u6587\u672c\u5316\u4ea4\u4e92\u6a21\u578b\u3002", "result": "\u5f00\u53d1\u4e86\u652f\u6301\u5bf9\u8bdd\u4e3b\u9898\u3001\u53c2\u4e0e\u8005\u54cd\u5e94\u548c\u673a\u5668\u4eba\u884c\u4e3a\u6a21\u5f0f\u5bfc\u822a\u7684\u7cfb\u7edf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5206\u6790\u4fe1\u4efb\u3001\u53c2\u4e0e\u5ea6\u548c\u671f\u671b\u53d8\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8d85\u6587\u672c\u5de5\u4f5c\u6d41\u80fd\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u7684\u900f\u660e\u5ea6\u548c\u8bed\u4e49\u5316\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2507.04162", "pdf": "https://arxiv.org/pdf/2507.04162", "abs": "https://arxiv.org/abs/2507.04162", "authors": ["Mengxi Liu", "Daniel Gei\u00dfler", "Deepika Gurung", "Hymalai Bello", "Bo Zhou", "Sizhen Bian", "Paul Lukowicz", "Passant Elagroudy"], "title": "iBreath: Usage Of Breathing Gestures as Means of Interactions", "categories": ["cs.HC"], "comment": null, "summary": "Breathing is a spontaneous but controllable body function that can be used\nfor hands-free interaction. Our work introduces \"iBreath\", a novel system to\ndetect breathing gestures similar to clicks using bio-impedance. We evaluated\niBreath's accuracy and user experience using two lab studies (n=34). Our\nresults show high detection accuracy (F1-scores > 95.2%). Furthermore, the\nusers found the gestures easy to use and comfortable. Thus, we developed eight\npractical guidelines for the future development of breathing gestures. For\nexample, designers can train users on new gestures within just 50 seconds (five\ntrials), and achieve robust performance with both user-dependent and\nuser-independent models trained on data from 21 participants, each yielding\naccuracies above 90%. Users preferred single clicks and disliked triple clicks.\nThe median gesture duration is 3.5-5.3 seconds. Our work provides solid ground\nfor researchers to experiment with creating breathing gestures and\ninteractions.", "AI": {"tldr": "iBreath\u662f\u4e00\u4e2a\u901a\u8fc7\u751f\u7269\u963b\u6297\u68c0\u6d4b\u547c\u5438\u624b\u52bf\u7684\u65b0\u578b\u7cfb\u7edf\uff0c\u7814\u7a76\u663e\u793a\u5176\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\uff08F1\u5206\u6570>95.2%\uff09\uff0c\u7528\u6237\u4f53\u9a8c\u826f\u597d\uff0c\u5e76\u4e3a\u672a\u6765\u547c\u5438\u624b\u52bf\u5f00\u53d1\u63d0\u4f9b\u4e86\u516b\u9879\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u63a2\u7d22\u547c\u5438\u4f5c\u4e3a\u4e00\u79cd\u65e0\u9700\u624b\u52a8\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u4ee5\u751f\u7269\u963b\u6297\u6280\u672f\u4e3a\u57fa\u7840\uff0c\u63d0\u5347\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u548c\u4fbf\u6377\u6027\u3002", "method": "\u91c7\u7528\u4e24\u8f6e\u5b9e\u9a8c\u5ba4\u7814\u7a76\uff08n=34\uff09\uff0c\u8bc4\u4f30iBreath\u7cfb\u7edf\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u5e76\u5f00\u53d1\u4e86\u7528\u6237\u4f9d\u8d56\u548c\u72ec\u7acb\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\uff08F1\u5206\u6570>95.2%\uff09\uff0c\u7528\u6237\u5bf9\u5355\u6b21\u70b9\u51fb\u624b\u52bf\u63a5\u53d7\u5ea6\u9ad8\uff0c\u624b\u52bf\u6301\u7eed\u65f6\u95f4\u4e2d\u4f4d\u6570\u4e3a3.5-5.3\u79d2\u3002", "conclusion": "iBreath\u4e3a\u547c\u5438\u624b\u52bf\u4ea4\u4e92\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u3002"}}
{"id": "2507.03840", "pdf": "https://arxiv.org/pdf/2507.03840", "abs": "https://arxiv.org/abs/2507.03840", "authors": ["Manasa Kaniselvan", "Alexander Maeder", "Chen Hao Xia", "Alexandros Nikolaos Ziogas", "Mathieu Luisier"], "title": "Distributed Equivariant Graph Neural Networks for Large-Scale Electronic Structure Prediction", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.DC", "physics.comp-ph"], "comment": "13 pages, 8 figures", "summary": "Equivariant Graph Neural Networks (eGNNs) trained on density-functional\ntheory (DFT) data can potentially perform electronic structure prediction at\nunprecedented scales, enabling investigation of the electronic properties of\nmaterials with extended defects, interfaces, or exhibiting disordered phases.\nHowever, as interactions between atomic orbitals typically extend over 10+\nangstroms, the graph representations required for this task tend to be densely\nconnected, and the memory requirements to perform training and inference on\nthese large structures can exceed the limits of modern GPUs. Here we present a\ndistributed eGNN implementation which leverages direct GPU communication and\nintroduce a partitioning strategy of the input graph to reduce the number of\nembedding exchanges between GPUs. Our implementation shows strong scaling up to\n128 GPUs, and weak scaling up to 512 GPUs with 87% parallel efficiency for\nstructures with 3,000 to 190,000 atoms on the Alps supercomputer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0feGNN\u5b9e\u73b0\uff0c\u901a\u8fc7GPU\u76f4\u63a5\u901a\u4fe1\u548c\u8f93\u5165\u56fe\u5206\u533a\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6750\u6599\u7535\u5b50\u7ed3\u6784\u9884\u6d4b\u4e2d\u7684\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfeGNN\u5728\u5927\u5c3a\u5ea6\u6750\u6599\u7535\u5b50\u7ed3\u6784\u9884\u6d4b\u4e2d\u56e0\u5bc6\u96c6\u8fde\u63a5\u7684\u56fe\u8868\u793a\u548c\u9ad8\u5185\u5b58\u9700\u6c42\u96be\u4ee5\u5728\u73b0\u4ee3GPU\u4e0a\u8fd0\u884c\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0feGNN\u5b9e\u73b0\uff0c\u7ed3\u5408GPU\u76f4\u63a5\u901a\u4fe1\u548c\u8f93\u5165\u56fe\u5206\u533a\u7b56\u7565\uff0c\u51cf\u5c11GPU\u95f4\u5d4c\u5165\u4ea4\u6362\u3002", "result": "\u5b9e\u73b0\u5f3a\u6269\u5c55\u5230128 GPU\uff0c\u5f31\u6269\u5c55\u5230512 GPU\uff0c\u57283000\u81f3190000\u539f\u5b50\u7ed3\u6784\u4e0a\u5e76\u884c\u6548\u7387\u8fbe87%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u7535\u5b50\u7ed3\u6784\u9884\u6d4b\u7684\u5185\u5b58\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u9ad8\u6548\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.04952", "pdf": "https://arxiv.org/pdf/2507.04952", "abs": "https://arxiv.org/abs/2507.04952", "authors": ["Chenchen Zhang", "Yuhang Li", "Can Xu", "Jiaheng Liu", "Ao Liu", "Shihui Hu", "Dengpeng Wu", "Guanhua Huang", "Kejiao Li", "Qi Yi", "Ruibin Xiong", "Haotian Zhu", "Yuanxing Zhang", "Yuhao Jiang", "Yue Zhang", "Zenan Xu", "Bohui Zhai", "Guoxiang He", "Hebin Li", "Jie Zhao", "Le Zhang", "Lingyun Tan", "Pengyu Guo", "Xianshu Pang", "Yang Ruan", "Zhifeng Zhang", "Zhonghu Wang", "Ziyan Xu", "Zuopu Yin", "Wiggin Zhou", "Chayse Zhou", "Fengzong Lian"], "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "The generative capabilities of Large Language Models (LLMs) are rapidly\nexpanding from static code to dynamic, interactive visual artifacts. This\nprogress is bottlenecked by a critical evaluation gap: established benchmarks\nfocus on algorithmic correctness and are blind to the visual fidelity and\ninteractive integrity that define modern user experiences. To bridge this gap,\nwe introduce ArtifactsBench, a new benchmark and paradigm for the automated,\nmultimodal evaluation of visual code generation. Our framework programmatically\nrenders each generated artifact and captures its dynamic behavior through\ntemporal screenshots. This visual evidence, alongside the source code, is then\nassessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a\nfine-grained, per-task checklist to ensure holistic and reproducible scoring.\nWe construct a new benchmark of 1,825 diverse tasks and evaluate over 30\nleading LLMs. Our automated evaluation achieves a striking 94.4% ranking\nconsistency with WebDev Arena, the gold-standard for human preference in web\ndevelopment, and over 90% pairwise agreement with human experts. This\nestablishes ArtifactsBench as the first framework to reliably automate the\nassessment of human-perceived quality at scale. Our analysis provides a\nhigh-resolution map of the current SOTA, revealing that generalist models often\noutperform domain-specific ones. We open-source ArtifactsBench, including the\nbenchmark, evaluation harness, and baseline results at\nhttps://artifactsbenchmark.github.io/, to provide the community with a scalable\nand accurate tool to accelerate the development of user-centric generative\nmodels.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3LLMs\u5728\u52a8\u6001\u4ea4\u4e92\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u8bc4\u4f30\u74f6\u9888\uff0c\u63d0\u51fa\u4e86ArtifactsBench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u81ea\u52a8\u5316\u4e14\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u9ad8\u5ea6\u4e00\u81f4\u7684\u8bc4\u5206\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u7b97\u6cd5\u6b63\u786e\u6027\uff0c\u5ffd\u89c6\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u4ea4\u4e92\u5b8c\u6574\u6027\uff0c\u7814\u7a76\u56e2\u961f\u8bd5\u56fe\u586b\u8865\u8fd9\u4e00\u8bc4\u4f30\u7a7a\u767d\u3002", "method": "ArtifactsBench\u901a\u8fc7\u7a0b\u5e8f\u5316\u6e32\u67d3\u751f\u6210\u7684\u89c6\u89c9\u4ee3\u7801\uff0c\u52a8\u6001\u6355\u6349\u884c\u4e3a\uff0c\u5e76\u5229\u7528\u591a\u6a21\u6001LLM\u7ed3\u5408\u4efb\u52a1\u6e05\u5355\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u8be5\u6846\u67b6\u57281,825\u4e2a\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u4e8630\u591a\u4e2aLLMs\uff0c\u81ea\u52a8\u5316\u8bc4\u5206\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u6027\u8fbe94.4%\uff0c\u4e14\u5f00\u6e90\u8d44\u6e90\u4e3a\u793e\u533a\u63d0\u4f9b\u5de5\u5177\u3002", "conclusion": "ArtifactsBench\u662f\u9996\u4e2a\u80fd\u5927\u89c4\u6a21\u53ef\u9760\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u4eba\u7c7b\u611f\u77e5\u8d28\u91cf\u7684\u6846\u67b6\uff0c\u663e\u793a\u901a\u7528\u6a21\u578b\u5e38\u4f18\u4e8e\u4e13\u4e1a\u6a21\u578b\u3002"}}
{"id": "2507.04130", "pdf": "https://arxiv.org/pdf/2507.04130", "abs": "https://arxiv.org/abs/2507.04130", "authors": ["Mohammad Dindoost", "Oliver Alvarado Rodriguez", "Bartosz Bryg", "Ioannis Koutis", "David A. Bader"], "title": "HiPerMotif: Novel Parallel Subgraph Isomorphism in Large-Scale Property Graphs", "categories": ["cs.DS", "cs.DC"], "comment": null, "summary": "Subgraph isomorphism, essential for pattern detection in large-scale graphs,\nfaces scalability challenges in attribute-rich property graphs used in\nneuroscience, systems biology, and social network analysis. Traditional\nalgorithms explore search spaces vertex-by-vertex from empty mappings, leading\nto extensive early-stage exploration with limited pruning opportunities. We\nintroduce HiPerMotif, a novel hybrid parallel algorithm that fundamentally\nshifts the search initialization strategy. After structurally reordering the\npattern graph to prioritize high-degree vertices, HiPerMotif systematically\nidentifies all possible mappings for the first edge (vertices 0,1) in the\ntarget graph, validates these edge candidates using efficient vertex and edge\nvalidators, and injects the validated partial mappings as states at depth 2.\nThe algorithm then continues with traditional vertex-by-vertex exploration from\nthese pre-validated starting points, effectively pruning the expensive early\nsearch tree branches while enabling natural parallelization over edge\ncandidates. Our contributions include the edge-centric initialization paradigm\nwith state injection, a structural reordering strategy achieving up to 5x\nspeedup, rapid edge and vertex validators for attribute-rich graphs, and\nefficient parallel enumeration over target graph edges. Implemented in the\nopen-source Arachne framework, HiPerMotif achieves up to 66x speedup over\nstate-of-the-art baselines (VF2-PS, VF3P, Glasgow) on diverse datasets where\nbaselines successfully complete execution. Additionally, HiPerMotif\nsuccessfully processes massive datasets such as the H01 connectome with 147\nmillion edges, which existing methods cannot handle due to memory constraints.\nComprehensive evaluation across synthetic and real-world graphs demonstrates\nHiPerMotif's scalability, enabling advanced analysis in computational\nneuroscience and beyond.", "AI": {"tldr": "HiPerMotif\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u5e76\u884c\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fb9\u4e2d\u5fc3\u521d\u59cb\u5316\u7b56\u7565\u548c\u7ed3\u6784\u91cd\u6392\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c5e\u6027\u4e30\u5bcc\u56fe\u4e2d\u7684\u5b50\u56fe\u540c\u6784\u641c\u7d22\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe66\u500d\u7684\u52a0\u901f\uff0c\u5e76\u80fd\u5904\u7406\u6d77\u91cf\u6570\u636e\u96c6\u3002", "motivation": "\u4f20\u7edf\u5b50\u56fe\u540c\u6784\u7b97\u6cd5\u5728\u5904\u7406\u5c5e\u6027\u4e30\u5bcc\u7684\u56fe\u65f6\u9762\u4e34\u641c\u7d22\u7a7a\u95f4\u5927\u3001\u526a\u679d\u673a\u4f1a\u5c11\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u5176\u5728\u795e\u7ecf\u79d1\u5b66\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002HiPerMotif\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "HiPerMotif\u901a\u8fc7\u7ed3\u6784\u91cd\u6392\u5e8f\u4f18\u5148\u5904\u7406\u9ad8\u5ea6\u6570\u9876\u70b9\uff0c\u4ece\u76ee\u6807\u56fe\u4e2d\u63d0\u53d6\u7b2c\u4e00\u8fb9\u7684\u5019\u9009\u6620\u5c04\uff0c\u5e76\u901a\u8fc7\u6709\u6548\u9a8c\u8bc1\u5668\u5feb\u901f\u9a8c\u8bc1\uff0c\u5c06\u8fd9\u4e9b\u90e8\u5206\u6620\u5c04\u4f5c\u4e3a\u641c\u7d22\u7684\u8d77\u70b9\uff0c\u4ece\u800c\u51cf\u5c11\u65e9\u671f\u641c\u7d22\u5f00\u9500\u5e76\u652f\u6301\u5e76\u884c\u5316\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\uff0cHiPerMotif\u6bd4\u73b0\u6709\u7b97\u6cd5\uff08\u5982VF2-PS\uff09\u5feb66\u500d\uff0c\u5e76\u53ef\u5904\u74061.47\u4ebf\u8fb9\u7684H01\u8fde\u63a5\u7ec4\u6570\u636e\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u7b97\u6cd5\u7684\u5185\u5b58\u9650\u5236\u3002", "conclusion": "HiPerMotif\u901a\u8fc7\u521b\u65b0\u7684\u521d\u59cb\u5316\u7b56\u7565\u548c\u5e76\u884c\u5316\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b50\u56fe\u540c\u6784\u7b97\u6cd5\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u590d\u6742\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.04990", "pdf": "https://arxiv.org/pdf/2507.04990", "abs": "https://arxiv.org/abs/2507.04990", "authors": ["Mohammad Hossein Amini", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "AI for the Routine, Humans for the Complex: Accuracy-Driven Data Labelling with Mixed Integer Linear Programming", "categories": ["cs.CV", "cs.SE"], "comment": null, "summary": "The scarcity of accurately labelled data remains a major challenge in deep\nlearning (DL). Many DL approaches rely on semi-supervised methods, which focus\non constructing large datasets that require only a minimal amount of\nhuman-labelled data. Since DL training algorithms can tolerate moderate label\nnoise, it has generally been acceptable for the accuracy of labels in large\ntraining datasets to fall well short of a perfect 100%. However, when it comes\nto testing DL models, achieving high label accuracy-as close to 100% as\npossible-is paramount for reliable verification. In this article, we introduce\nOPAL, a human-assisted labelling method that can be configured to target a\ndesired accuracy level while minimizing the manual effort required for\nlabelling. The main contribution of OPAL is a mixed-integer linear programming\n(MILP) formulation that minimizes labelling effort subject to a specified\naccuracy target. We evaluate OPAL for two tasks in the context of testing\nvision systems: automatic labelling of test data and automated validation of\ntest data. Our evaluation, based on more than 2500 experiments performed on\nseven datasets, comparing OPAL with eight baseline methods, shows that OPAL,\nrelying on its MILP formulation, achieves an average accuracy of 98.8%, just\n1.2% below perfect accuracy, while cutting manual labelling by more than half.\nFurther, OPAL significantly outperforms automated labelling baselines in\nlabelling accuracy across all seven datasets, with large effect sizes, when all\nmethods are provided with the same manual-labelling budget. For automated\ntest-input validation, on average, OPAL reduces manual effort by 28.8% while\nachieving 4.5% higher accuracy than the SOTA validation baselines. Finally, we\nshow that augmenting OPAL with an active learning loop leads to an additional\n4.5% reduction in required manual labelling, without compromising accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOPAL\u7684\u4eba\u5de5\u8f85\u52a9\u6807\u6ce8\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6700\u5c0f\u5316\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u786e\u4fdd\u6807\u7b7e\u9ad8\u51c6\u786e\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0cOPAL\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u4e2d\uff0c\u51c6\u786e\u7684\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u867d\u7136\u8bad\u7ec3\u53ef\u4ee5\u5bb9\u5fcd\u90e8\u5206\u6807\u7b7e\u566a\u58f0\uff0c\u4f46\u6d4b\u8bd5\u65f6\u9700\u8981\u9ad8\u51c6\u786e\u7387\u4ee5\u786e\u4fdd\u6a21\u578b\u9a8c\u8bc1\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51faOPAL\u65b9\u6cd5\uff0c\u5229\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6765\u4f18\u5316\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u8fbe\u5230\u6307\u5b9a\u7684\u51c6\u786e\u7387\u76ee\u6807\uff0c\u5e76\u8fdb\u4e00\u6b65\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u4ee5\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u57282500\u591a\u6b21\u5b9e\u9a8c\u4e2d\uff0cOPAL\u5728\u4e03\u79cd\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523098.8%\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5c06\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\u51cf\u5c11\u4e00\u534a\u4ee5\u4e0a\u3002", "conclusion": "OPAL\u5728\u4fdd\u8bc1\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\uff0c\u5c24\u5176\u5728\u6d4b\u8bd5\u6570\u636e\u81ea\u52a8\u6807\u6ce8\u548c\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.04238", "pdf": "https://arxiv.org/pdf/2507.04238", "abs": "https://arxiv.org/abs/2507.04238", "authors": ["Zhang Youpeng", "Nuwan Janaka", "Ashwin Ram", "Yin Peilin", "Tian Yang", "Shengdong Zhao", "Pierre Dragicevic"], "title": "WSCoach: Wearable Real-time Auditory Feedback for Reducing Unwanted Words in Daily Communication", "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "30 pages, 9 figures", "summary": "The rise of wearable smart devices raises unprecedented opportunities for\nself-improvement through ubiquitous behavior tracking and guidance. However,\nthe design of effective wearable behavior intervention systems remains\nrelatively unexplored. To address this gap, we conducted controlled studies\nfocusing on the reduction of unwanted words (e.g., filler words, swear words)\nin daily communication through auditory feedback using wearable technology. We\nstarted with a design space exploration, considering various factors such as\nthe type, duration, and timing of the auditory feedback. Then, we conducted\npilot studies to reduce the space of design choices and prototyped a system\ncalled WSCoach (Wearable Speech Coach), which informs users when they utter\nunwanted words in near-real-time. To evaluate WSCoach, we compared it with a\nstate-of-the-art mobile application supporting post-hoc conversation analysis.\nBoth approaches were effective in reducing the occurrence of unwanted words,\nbut WSCoach appears to be more effective in the long run. Finally, we discuss\nguidelines for the design of wearable audio-based behavior monitoring and\nintervention systems and highlight the potential of wearable technology for\nfacilitating behavior correction and improvement. For supplementary material,\nplease see the META Appendix and our OSF project at\nhttps://osf.io/6vhwn/?view_only=489498d3ac2d4703a17475fc6ca65dfa.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u901a\u8fc7\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u542c\u89c9\u53cd\u9988\u51cf\u5c11\u65e5\u5e38\u4ea4\u6d41\u4e2d\u4e0d\u826f\u7528\u8bcd\u7684\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86WSCoach\u7cfb\u7edf\uff0c\u5e76\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u79fb\u52a8\u5e94\u7528\u3002", "motivation": "\u5229\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u884c\u4e3a\u5e72\u9884\u6f5c\u529b\uff0c\u586b\u8865\u8bbe\u8ba1\u6709\u6548\u7cfb\u7edf\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u3001\u521d\u6b65\u7814\u7a76\u548c\u539f\u578b\u5f00\u53d1\uff0c\u6784\u5efa\u4e86\u5b9e\u65f6\u53cd\u9988\u7cfb\u7edfWSCoach\uff0c\u5e76\u4e0e\u79fb\u52a8\u5e94\u7528\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "WSCoach\u5728\u957f\u671f\u51cf\u5c11\u4e0d\u826f\u7528\u8bcd\u65b9\u9762\u6bd4\u79fb\u52a8\u5e94\u7528\u66f4\u6709\u6548\u3002", "conclusion": "\u53ef\u7a7f\u6234\u6280\u672f\u5bf9\u884c\u4e3a\u77eb\u6b63\u6709\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u6307\u5357\u3002"}}
{"id": "2507.04241", "pdf": "https://arxiv.org/pdf/2507.04241", "abs": "https://arxiv.org/abs/2507.04241", "authors": ["Yichen Yu", "Huan-Song Xu"], "title": "RunPacer: A Smartwatch-Based Vibrotactile Feedback System for Symmetric Co-Running by Visually Impaired Individuals and Guides", "categories": ["cs.HC"], "comment": "6 pages, 1 figure", "summary": "Visually impaired individuals often require a guide runner to safely\nparticipate in outdoor running. However, maintaining synchronized pacing with\nverbal cues or tethers can be mentally taxing and physically restrictive.\nExisting solutions primarily focus on navigation or obstacle avoidance but\noverlook the importance of real-time interpersonal rhythm coordination during\nrunning. We introduce RunPacer, a smartwatch-based vibrotactile feedback system\nthat delivers synchronized rhythmic pulses to both runners. In contrast to\nconventional guide-running systems that rely heavily on continuous verbal\ncommunication or mechanical tethering, RunPacer emphasizes interpersonal\ncadence alignment as its core interaction model. By pre-setting a target step\nfrequency or dynamically adapting to the guide's natural pace, the system\nensures that both runners receive identical haptic cues, enabling them to\nmaintain coordinated motion intuitively and efficiently. This poster presents\nthe system architecture, positions it within prior research on haptic\nentrainment, and outlines the vision for future field deployment, including\npotential multimodal feedback extensions. RunPacer contributes a lightweight,\nsocially cooperative, and non-visual assistive framework that reimagines\nco-running as a shared, embodied, and accessible experience.", "AI": {"tldr": "RunPacer\u662f\u4e00\u6b3e\u57fa\u4e8e\u667a\u80fd\u624b\u8868\u7684\u89e6\u89c9\u53cd\u9988\u7cfb\u7edf\uff0c\u65e8\u5728\u5e2e\u52a9\u89c6\u969c\u8dd1\u8005\u4e0e\u966a\u8dd1\u5458\u540c\u6b65\u8282\u594f\uff0c\u51cf\u5c11\u5bf9\u53e3\u5934\u63d0\u793a\u6216\u7269\u7406\u8fde\u63a5\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u591a\u5173\u6ce8\u5bfc\u822a\u6216\u907f\u969c\uff0c\u5ffd\u89c6\u4e86\u8dd1\u6b65\u4e2d\u5b9e\u65f6\u8282\u594f\u534f\u8c03\u7684\u91cd\u8981\u6027\uff0cRunPacer\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u9884\u8bbe\u76ee\u6807\u6b65\u9891\u6216\u52a8\u6001\u9002\u5e94\u966a\u8dd1\u5458\u7684\u81ea\u7136\u8282\u594f\uff0c\u5411\u53cc\u65b9\u63d0\u4f9b\u540c\u6b65\u7684\u89e6\u89c9\u63d0\u793a\u3002", "result": "RunPacer\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u793e\u4ea4\u5408\u4f5c\u7684\u975e\u89c6\u89c9\u8f85\u52a9\u6846\u67b6\uff0c\u5b9e\u73b0\u76f4\u89c9\u4e14\u9ad8\u6548\u7684\u534f\u540c\u8dd1\u6b65\u4f53\u9a8c\u3002", "conclusion": "RunPacer\u91cd\u65b0\u5b9a\u4e49\u4e86\u534f\u540c\u8dd1\u6b65\u7684\u5171\u4eab\u3001\u5177\u4f53\u5316\u548c\u65e0\u969c\u788d\u4f53\u9a8c\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u591a\u6a21\u6001\u53cd\u9988\u7684\u6269\u5c55\u3002"}}
{"id": "2507.04310", "pdf": "https://arxiv.org/pdf/2507.04310", "abs": "https://arxiv.org/abs/2507.04310", "authors": ["Gyuejeong Lee", "Jihwan Shin", "Daeyoung Choi"], "title": "Heterogeneous Federated Learning with Prototype Alignment and Upscaling", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Heterogeneity in data distributions and model architectures remains a\nsignificant challenge in federated learning (FL). Various heterogeneous FL\n(HtFL) approaches have recently been proposed to address this challenge. Among\nthem, prototype-based FL (PBFL) has emerged as a practical framework that only\nshares per-class mean activations from the penultimate layer. However, PBFL\napproaches often suffer from suboptimal prototype separation, limiting their\ndiscriminative power. We propose Prototype Normalization (ProtoNorm), a novel\nPBFL framework that addresses this limitation through two key components:\nPrototype Alignment (PA) and Prototype Upscaling (PU). The PA method draws\ninspiration from the Thomson problem in classical physics, optimizing global\nprototype configurations on a unit sphere to maximize angular separation;\nsubsequently, the PU method increases prototype magnitudes to enhance\nseparation in Euclidean space. Extensive evaluations on benchmark datasets show\nthat our approach better separates prototypes and thus consistently outperforms\nexisting HtFL approaches. Notably, since ProtoNorm inherits the communication\nefficiency of PBFL and the PA is performed server-side, it is particularly\nsuitable for resource-constrained environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProtoNorm\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u578b\u5bf9\u9f50\u548c\u539f\u578b\u653e\u5927\u6280\u672f\u4f18\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u539f\u578b\u5206\u79bb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5206\u5e03\u548c\u6a21\u578b\u67b6\u6784\u5f02\u8d28\u6027\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u73b0\u6709\u7684\u539f\u578b\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u539f\u578b\u5206\u79bb\u4e0a\u8868\u73b0\u6b20\u4f73\u3002", "method": "ProtoNorm\u7ed3\u5408\u4e86\u539f\u578b\u5bf9\u9f50\uff08PA\uff09\u548c\u539f\u578b\u653e\u5927\uff08PU\uff09\u6280\u672f\uff0c\u524d\u8005\u901a\u8fc7\u4f18\u5316\u5168\u5c40\u539f\u578b\u914d\u7f6e\u4ee5\u6700\u5927\u5316\u89d2\u5ea6\u5206\u79bb\uff0c\u540e\u8005\u901a\u8fc7\u589e\u52a0\u539f\u578b\u5e45\u5ea6\u63d0\u5347\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u5206\u79bb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cProtoNorm\u80fd\u66f4\u597d\u5730\u5206\u79bb\u539f\u578b\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ProtoNorm\u4e0d\u4ec5\u7ee7\u627f\u4e86\u539f\u578b\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u6548\u7387\uff0c\u8fd8\u56e0\u5176\u670d\u52a1\u5668\u7aef\u5904\u7406\u800c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2507.04278", "pdf": "https://arxiv.org/pdf/2507.04278", "abs": "https://arxiv.org/abs/2507.04278", "authors": ["Zheng Lian", "Licai Sun", "Haoyu Chen", "Zebang Cheng", "Fan Zhang", "Ziyu Jia", "Ziyang Ma", "Fei Ma", "Xiaojiang Peng", "Jianhua Tao"], "title": "DMER-Ranker: Learning to Rank Emotion Descriptions in the Absence of Ground Truth", "categories": ["cs.HC"], "comment": null, "summary": "Descriptive Multimodal Emotion Recognition (DMER) is a newly proposed task\nthat aims to describe a person's emotional state using free-form natural\nlanguage. Unlike traditional discriminative methods that rely on predefined\nemotion taxonomies, DMER provides greater flexibility in emotional expression,\nenabling fine-grained and interpretable emotion representations. However, this\nfree-form prediction paradigm introduces significant challenges in evaluation.\nExisting methods either depend on ground-truth descriptions that require\nsubstantial manual effort or simplify the task by shifting the focus from\nevaluating descriptions to evaluating emotion labels. However, the former\nsuffers from the labor-intensive collection of comprehensive descriptions,\nwhile the latter overlooks critical aspects such as emotional temporal\ndynamics, intensity, and uncertainty. To address these limitations, we propose\nDMER-Ranker, a novel evaluation strategy that reformulates the traditional\n``prediction-ground truth'' comparison into the ``prediction-prediction''\ncomparison, eliminating the need for ground-truth descriptions. We then employ\nthe Bradley-Terry algorithm to convert pairwise comparison results into\nmodel-level rankings. Additionally, we explore the possibility of automatic\npreference prediction and introduce DMER-Preference, the first preference\ndataset specifically designed for human emotions. Our work advances the field\nof DMER and lays the foundation for more intelligent human-computer interaction\nsystems.", "AI": {"tldr": "DMER-Ranker\u662f\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u4f20\u7edf\u201c\u9884\u6d4b\u4e0e\u771f\u5b9e\u63cf\u8ff0\u201d\u7684\u6bd4\u8f83\u8f6c\u5316\u4e3a\u201c\u9884\u6d4b\u4e0e\u9884\u6d4b\u201d\u7684\u6bd4\u8f83\uff0c\u89e3\u51b3\u4e86\u81ea\u7531\u5f62\u5f0f\u60c5\u611f\u63cf\u8ff0\u4efb\u52a1DMER\u7684\u8bc4\u4f30\u96be\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u5224\u522b\u6027\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u60c5\u611f\u5206\u7c7b\uff0c\u9650\u5236\u4e86\u60c5\u611f\u8868\u8fbe\u7684\u7075\u6d3b\u6027\u3002DMER\u4efb\u52a1\u867d\u7136\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u548c\u53ef\u89e3\u91ca\u7684\u60c5\u611f\u8868\u793a\uff0c\u4f46\u5176\u81ea\u7531\u5f62\u5f0f\u7684\u9884\u6d4b\u8303\u5f0f\u5f15\u5165\u4e86\u8bc4\u4f30\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDMER-Ranker\uff0c\u5229\u7528\u201c\u9884\u6d4b\u95f4\u6bd4\u8f83\u201d\u66ff\u4ee3\u201c\u9884\u6d4b\u4e0e\u771f\u5b9e\u63cf\u8ff0\u201d\u6bd4\u8f83\uff0c\u7ed3\u5408Bradley-Terry\u7b97\u6cd5\u5c06\u6210\u5bf9\u6bd4\u8f83\u7ed3\u679c\u8f6c\u6362\u4e3a\u6a21\u578b\u6392\u540d\u3002", "result": "DMER-Ranker\u6d88\u9664\u4e86\u5bf9\u771f\u5b9e\u63cf\u8ff0\u7684\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165\u4e86DMER-Preference\u6570\u636e\u96c6\uff0c\u4e3a\u60c5\u611f\u504f\u597d\u9884\u6d4b\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86DMER\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e3a\u66f4\u667a\u80fd\u7684\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.04398", "pdf": "https://arxiv.org/pdf/2507.04398", "abs": "https://arxiv.org/abs/2507.04398", "authors": ["Yueqiao Jin", "Kaixun Yang", "Roberto Martinez-Maldonado", "Dragan Ga\u0161evi\u0107", "Lixiang Yan"], "title": "Do Students Write Better Post-AI Support? Effects of Generative AI Literacy and Chatbot Interaction Strategies on Multimodal Academic Writing", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Academic writing increasingly involves multimodal tasks requiring students to\nintegrate visual information and textual arguments. While generative AI (GenAI)\ntools, like ChatGPT, offer new pathways for supporting academic writing, little\nis known about how students' GenAI literacy influences their independent\nmultimodal writing skills or how chatbot interaction strategies (passive\nreactive vs. proactive scaffolding) impact learning. This study examined 79\nhigher education students' multimodal academic writing performance using a\ncomparative research design. Students completed writing tasks integrating\nvisual data under two chatbot-assisted conditions (passive vs. proactive) and\nsubsequently without AI assistance. Their writing performance was rigorously\nevaluated across five dimensions, including insightfulness, visual data\nintegration, organisation, linguistic quality, and critical thinking. Ordinal\nlogistic regression and correlation analyses revealed that higher levels of\nGenAI literacy significantly predicted stronger independent multimodal writing\nperformance immediately after AI assistance removal, particularly for students\nusing passive chatbots requiring active prompting. These results highlight the\ncritical role of GenAI literacy and specific chatbot interaction strategies in\nshaping students' capacities for independent multimodal academic writing. Our\nfindings emphasise the need for purposeful integration of GenAI literacy\ntraining into curricula and balancing external scaffolding support with\nautonomous learning opportunities. This research offers valuable\nrecommendations for educators leveraging AI-enhanced pedagogies to optimise\nstudent writing outcomes and technological engagement strategies.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cGenAI\u7d20\u517b\u548c\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8\u7b56\u7565\u5bf9\u5b66\u751f\u72ec\u7acb\u591a\u6a21\u6001\u5b66\u672f\u5199\u4f5c\u80fd\u529b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u88ab\u52a8\u4e92\u52a8\u6761\u4ef6\u4e0b\u3002", "motivation": "\u63a2\u8ba8\u5b66\u751fGenAI\u7d20\u517b\u548c\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8\u7b56\u7565\u5bf9\u5176\u72ec\u7acb\u591a\u6a21\u6001\u5199\u4f5c\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "79\u540d\u9ad8\u7b49\u6559\u80b2\u5b66\u751f\u5728\u4e24\u79cd\u804a\u5929\u673a\u5668\u4eba\u8f85\u52a9\u6761\u4ef6\u4e0b\u5b8c\u6210\u5199\u4f5c\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4e94\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u8868\u73b0\u3002", "result": "GenAI\u7d20\u517b\u9ad8\u7684\u5b66\u751f\u5728AI\u8f85\u52a9\u79fb\u9664\u540e\u8868\u73b0\u66f4\u597d\uff0c\u88ab\u52a8\u4e92\u52a8\u7b56\u7565\u6548\u679c\u66f4\u663e\u8457\u3002", "conclusion": "\u5e94\u52a0\u5f3aGenAI\u7d20\u517b\u57f9\u8bad\uff0c\u5e73\u8861\u5916\u90e8\u652f\u6301\u4e0e\u81ea\u4e3b\u5b66\u4e60\uff0c\u4f18\u5316AI\u6559\u5b66\u7b56\u7565\u3002"}}
{"id": "2507.04697", "pdf": "https://arxiv.org/pdf/2507.04697", "abs": "https://arxiv.org/abs/2507.04697", "authors": ["Daichi Mukunoki", "Shun-ichiro Hayashi", "Tetsuya Hoshino", "Takahiro Katagiri"], "title": "Performance Evaluation of General Purpose Large Language Models for Basic Linear Algebra Subprograms Code Generation", "categories": ["cs.LG", "cs.DC", "cs.MS"], "comment": "8 pages, 6 tables", "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.", "AI": {"tldr": "\u8bc4\u4f30\u901a\u7528LLM\u5728CPU\u4e0a\u751f\u6210BLAS\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793aGPT-4.1\u548co4-mini\u80fd\u751f\u6210\u4f18\u5316\u540e\u7684\u9ad8\u6548\u4ee3\u7801\u3002", "motivation": "\u7814\u7a76\u901a\u7528LLM\u5728BLAS\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u4f18\u5316\u6027\u80fd\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528GPT-4.1\u548co4-mini\u751f\u6210\u4e0d\u540c\u4f18\u5316\u7ea7\u522b\u7684C\u4ee3\u7801\uff0c\u5e76\u4e0e\u53c2\u8003\u4ee3\u7801\u6bd4\u8f83\u6027\u80fd\u3002", "result": "LLM\u80fd\u751f\u6210\u6b63\u786e\u4e14\u4f18\u5316\u7684\u4ee3\u7801\uff0c\u6027\u80fd\u4f18\u4e8e\u53c2\u8003\u4ee3\u7801\u3002", "conclusion": "\u901a\u7528LLM\u5728BLAS\u4ee3\u7801\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.04454", "pdf": "https://arxiv.org/pdf/2507.04454", "abs": "https://arxiv.org/abs/2507.04454", "authors": ["Videep Venkatesha", "Mariah Bradford", "Nathaniel Blanchard"], "title": "Dude, where's my utterance? Evaluating the effects of automatic segmentation and transcription on CPS detection", "categories": ["cs.HC", "cs.CL", "cs.CY", "eess.AS"], "comment": "Accepted at AIED 2025", "summary": "Collaborative Problem-Solving (CPS) markers capture key aspects of effective\nteamwork, such as staying on task, avoiding interruptions, and generating\nconstructive ideas. An AI system that reliably detects these markers could help\nteachers identify when a group is struggling or demonstrating productive\ncollaboration. Such a system requires an automated pipeline composed of\nmultiple components. In this work, we evaluate how CPS detection is impacted by\nautomating two critical components: transcription and speech segmentation. On\nthe public Weights Task Dataset (WTD), we find CPS detection performance with\nautomated transcription and segmentation methods is comparable to\nhuman-segmented and manually transcribed data; however, we find the automated\nsegmentation methods reduces the number of utterances by 26.5%, impacting the\nthe granularity of the data. We discuss the implications for developing\nAI-driven tools that support collaborative learning in classrooms.", "AI": {"tldr": "\u7814\u7a76\u4e86\u81ea\u52d5\u5316\u8f49\u9304\u548c\u8a9e\u97f3\u5206\u6bb5\u5c0d\u5354\u4f5c\u554f\u984c\u89e3\u6c7a\uff08CPS\uff09\u6a19\u8a18\u6aa2\u6e2c\u7684\u5f71\u97ff\uff0c\u7d50\u679c\u986f\u793a\u6027\u80fd\u8207\u4eba\u5de5\u8655\u7406\u76f8\u7576\uff0c\u4f46\u81ea\u52d5\u5206\u6bb5\u6e1b\u5c11\u6578\u64da\u7d30\u7c92\u5ea6\u3002", "motivation": "\u958b\u767cAI\u7cfb\u7d71\u4ee5\u6aa2\u6e2cCPS\u6a19\u8a18\uff0c\u5e6b\u52a9\u6559\u5e2b\u8b58\u5225\u5718\u968a\u5354\u4f5c\u554f\u984c\u6216\u6210\u6548\u3002", "method": "\u8a55\u4f30\u81ea\u52d5\u5316\u8f49\u9304\u548c\u8a9e\u97f3\u5206\u6bb5\u5c0dCPS\u6aa2\u6e2c\u7684\u5f71\u97ff\uff0c\u4f7f\u7528\u516c\u958b\u6578\u64da\u96c6WTD\u3002", "result": "\u81ea\u52d5\u5316\u65b9\u6cd5\u6027\u80fd\u63a5\u8fd1\u4eba\u5de5\u8655\u7406\uff0c\u4f46\u5206\u6bb5\u65b9\u6cd5\u6e1b\u5c1126.5%\u7684\u8a9e\u53e5\uff0c\u5f71\u97ff\u6578\u64da\u7d30\u7c92\u5ea6\u3002", "conclusion": "\u81ea\u52d5\u5316\u65b9\u6cd5\u53ef\u884c\u4f46\u9700\u6ce8\u610f\u6578\u64da\u7d30\u7c92\u5ea6\u640d\u5931\uff0c\u5c0dAI\u652f\u6301\u5354\u4f5c\u5b78\u7fd2\u5de5\u5177\u6709\u555f\u793a\u3002"}}
{"id": "2507.04903", "pdf": "https://arxiv.org/pdf/2507.04903", "abs": "https://arxiv.org/abs/2507.04903", "authors": ["Thinh Dao", "Dung Thuy Nguyen", "Khoa D Doan", "Kok-Seng Wong"], "title": "BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning", "categories": ["cs.CR", "cs.AI", "cs.DC"], "comment": "Under review at NeurIPS'25", "summary": "Federated Learning (FL) systems are vulnerable to backdoor attacks, where\nadversaries train their local models on poisoned data and submit poisoned model\nupdates to compromise the global model. Despite numerous proposed attacks and\ndefenses, divergent experimental settings, implementation errors, and\nunrealistic assumptions hinder fair comparisons and valid conclusions about\ntheir effectiveness in real-world scenarios. To address this, we introduce\nBackFed - a comprehensive benchmark suite designed to standardize, streamline,\nand reliably evaluate backdoor attacks and defenses in FL, with a focus on\npractical constraints. Our benchmark offers key advantages through its\nmulti-processing implementation that significantly accelerates experimentation\nand the modular design that enables seamless integration of new methods via\nwell-defined APIs. With a standardized evaluation pipeline, we envision BackFed\nas a plug-and-play environment for researchers to comprehensively and reliably\nevaluate new attacks and defenses. Using BackFed, we conduct large-scale\nstudies of representative backdoor attacks and defenses across both Computer\nVision and Natural Language Processing tasks with diverse model architectures\nand experimental settings. Our experiments critically assess the performance of\nproposed attacks and defenses, revealing unknown limitations and modes of\nfailures under practical conditions. These empirical insights provide valuable\nguidance for the development of new methods and for enhancing the security of\nFL systems. Our framework is openly available at\nhttps://github.com/thinh-dao/BackFed.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BackFed\uff0c\u4e00\u4e2a\u6807\u51c6\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u540e\u95e8\u653b\u51fb\u4e0e\u9632\u5fa1\u8bc4\u4f30\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u5b9e\u9a8c\u8bbe\u7f6e\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u540e\u95e8\u653b\u51fb\u4e0e\u9632\u5fa1\u7814\u7a76\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u4e0d\u4e00\u81f4\u3001\u5b9e\u73b0\u9519\u8bef\u548c\u5047\u8bbe\u4e0d\u73b0\u5b9e\uff0c\u963b\u788d\u4e86\u516c\u5e73\u6bd4\u8f83\u548c\u6709\u6548\u7ed3\u8bba\u7684\u5f97\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u9ad8\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faBackFed\u57fa\u51c6\u5957\u4ef6\uff0c\u91c7\u7528\u591a\u8fdb\u7a0b\u5b9e\u73b0\u52a0\u901f\u5b9e\u9a8c\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u65b0\u65b9\u6cd5\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u6d41\u7a0b\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002", "result": "\u901a\u8fc7BackFed\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u4ee3\u8868\u6027\u653b\u51fb\u4e0e\u9632\u5fa1\u8fdb\u884c\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5176\u5728\u5b9e\u9645\u6761\u4ef6\u4e0b\u7684\u672a\u77e5\u5c40\u9650\u6027\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "BackFed\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u7684\u73af\u5883\u6765\u8bc4\u4f30\u65b0\u65b9\u6cd5\uff0c\u5e76\u4e3a\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u5347\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.04469", "pdf": "https://arxiv.org/pdf/2507.04469", "abs": "https://arxiv.org/abs/2507.04469", "authors": ["Ammar Ahmed", "Ali Shariq Imran"], "title": "The role of large language models in UI/UX design: A systematic literature review", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "This systematic literature review examines the role of large language models\n(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies\npublished between 2022 and 2025. We identify key LLMs in use, including GPT-4,\nGemini, and PaLM, and map their integration across the design lifecycle, from\nideation to evaluation. Common practices include prompt engineering,\nhuman-in-the-loop workflows, and multimodal input. While LLMs are reshaping\ndesign processes, challenges such as hallucination, prompt instability, and\nlimited explainability persist. Our findings highlight LLMs as emerging\ncollaborators in design, and we propose directions for the ethical, inclusive,\nand effective integration of these technologies.", "AI": {"tldr": "\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728UI/UX\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u4e8638\u7bc7\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\uff0c\u53d1\u73b0LLMs\u6b63\u91cd\u5851\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u4f46\u4e5f\u5b58\u5728\u5e7b\u89c9\u3001\u63d0\u793a\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "motivation": "\u5206\u6790LLMs\u5728UI/UX\u8bbe\u8ba1\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u6311\u6218\uff0c\u4e3a\u672a\u6765\u6280\u672f\u6574\u5408\u63d0\u4f9b\u65b9\u5411\u3002", "method": "\u7efc\u8ff038\u7bc72022-2025\u5e74\u7684\u7814\u7a76\uff0c\u8bc6\u522b\u5173\u952eLLMs\u53ca\u5176\u5728\u8bbe\u8ba1\u751f\u547d\u5468\u671f\u4e2d\u7684\u96c6\u6210\u65b9\u5f0f\u3002", "result": "LLMs\u5728\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5e7b\u89c9\u3001\u63d0\u793a\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "conclusion": "LLMs\u662f\u8bbe\u8ba1\u7684\u6f5c\u5728\u534f\u4f5c\u4f19\u4f34\uff0c\u9700\u63a8\u52a8\u5176\u4f26\u7406\u548c\u9ad8\u6548\u6574\u5408\u3002"}}
{"id": "2507.04956", "pdf": "https://arxiv.org/pdf/2507.04956", "abs": "https://arxiv.org/abs/2507.04956", "authors": ["Yusei Tanaka"], "title": "Bullshark on Narwhal: Implementation-level Workflow Analysis of Round-based DAG Consensus in Theory and Practice", "categories": ["cs.CR", "cs.DC"], "comment": "17 pages, in Japanese language, 11 figures", "summary": "Round-based DAGs enable high-performance Byzantine fault-tolerant consensus,\nyet their technical advantages remain underutilized due to their short history.\nWhile research on consensus protocols is active in both academia and industry,\nmany studies overlook implementation-level algorithms, leaving actual\nperformance unclear - particularly for theoretical protocols whose practical\nperformance cannot often be evaluated. Bullshark, a Round-based DAG BFT\nprotocol on Narwhal mempool, achieves optimal performance: 297,000 transactions\nper second with 2-second latency. We analyze the algorithm's workflow, from\ntransaction submission to blockchain commitment, breaking it down layer by\nlayer at the functional level and delineating the key features and interactions\nof the Bullshark and Narwhal components. Future work aims to improve\nperformance in Byzantine fault environments and optimize trade-offs in the CAP\ntheorem.", "AI": {"tldr": "Bullshark\u662f\u4e00\u79cd\u57fa\u4e8eDAG\u7684BFT\u534f\u8bae\uff0c\u4f18\u5316\u6027\u80fd\u8fbe29.7\u4e07TPS\uff0c2\u79d2\u5ef6\u8fdf\uff0c\u7ed3\u5408Narwhal\u5185\u5b58\u6c60\uff0c\u672a\u6765\u7814\u7a76\u5c06\u63d0\u5347\u62dc\u5360\u5ead\u5bb9\u9519\u6027\u80fd\u3002", "motivation": "\u5229\u7528Round-based DAG\u63d0\u5347BFT\u5171\u8bc6\u6027\u80fd\uff0c\u586b\u8865\u7406\u8bba\u534f\u8bae\u4e0e\u5b9e\u9645\u6027\u80fd\u8bc4\u4f30\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5206\u6790Bullshark\u4e0eNarwhal\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ece\u4ea4\u6613\u63d0\u4ea4\u5230\u533a\u5757\u94fe\u786e\u8ba4\uff0c\u9010\u5c42\u5206\u89e3\u529f\u80fd\u4e0e\u4ea4\u4e92\u3002", "result": "\u5b9e\u73b029.7\u4e07TPS\u548c2\u79d2\u5ef6\u8fdf\u7684\u4f18\u5316\u6027\u80fd\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5c06\u4f18\u5316\u62dc\u5360\u5ead\u5bb9\u9519\u6027\u80fd\u53caCAP\u5b9a\u7406\u6743\u8861\u3002"}}
{"id": "2507.04491", "pdf": "https://arxiv.org/pdf/2507.04491", "abs": "https://arxiv.org/abs/2507.04491", "authors": ["Zhicheng Lin"], "title": "A validity-guided workflow for robust large language model research in psychology", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are rapidly being integrated into psychological\nresearch as research tools, evaluation targets, human simulators, and cognitive\nmodels. However, recent evidence reveals severe measurement unreliability:\nPersonality assessments collapse under factor analysis, moral preferences\nreverse with punctuation changes, and theory-of-mind accuracy varies widely\nwith trivial rephrasing. These \"measurement phantoms\"--statistical artifacts\nmasquerading as psychological phenomena--threaten the validity of a growing\nbody of research. Guided by the dual-validity framework that integrates\npsychometrics with causal inference, we present a six-stage workflow that\nscales validity requirements to research ambition--using LLMs to code text\nrequires basic reliability and accuracy, while claims about psychological\nproperties demand comprehensive construct validation. Researchers must (1)\nexplicitly define their research goal and corresponding validity requirements,\n(2) develop and validate computational instruments through psychometric\ntesting, (3) design experiments that control for computational confounds, (4)\nexecute protocols with transparency, (5) analyze data using methods appropriate\nfor non-independent observations, and (6) report findings within demonstrated\nboundaries and use results to refine theory. We illustrate the workflow through\nan example of model evaluation--\"LLM selfhood\"--showing how systematic\nvalidation can distinguish genuine computational phenomena from measurement\nartifacts. By establishing validated computational instruments and transparent\npractices, this workflow provides a path toward building a robust empirical\nfoundation for AI psychology research.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\u5b58\u5728\u6d4b\u91cf\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u516d\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\u4ee5\u786e\u4fdd\u7814\u7a76\u6709\u6548\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u8fd1\u671f\u53d1\u73b0\u5176\u6d4b\u91cf\u7ed3\u679c\u5b58\u5728\u4e25\u91cd\u4e0d\u53ef\u9760\u6027\uff0c\u5a01\u80c1\u7814\u7a76\u6709\u6548\u6027\u3002", "method": "\u57fa\u4e8e\u53cc\u6548\u5ea6\u6846\u67b6\uff0c\u63d0\u51fa\u516d\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u660e\u786e\u7814\u7a76\u76ee\u6807\u3001\u9a8c\u8bc1\u8ba1\u7b97\u5de5\u5177\u3001\u63a7\u5236\u5b9e\u9a8c\u53d8\u91cf\u7b49\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u9a8c\u8bc1\uff0c\u53ef\u4ee5\u533a\u5206\u771f\u5b9e\u7684\u8ba1\u7b97\u73b0\u8c61\u4e0e\u6d4b\u91cf\u5047\u8c61\uff0c\u4e3aAI\u5fc3\u7406\u5b66\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u5de5\u4f5c\u6d41\u7a0b\u901a\u8fc7\u900f\u660e\u5b9e\u8df5\u548c\u9a8c\u8bc1\u5de5\u5177\uff0c\u4e3a\u5fc3\u7406\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2507.04906", "pdf": "https://arxiv.org/pdf/2507.04906", "abs": "https://arxiv.org/abs/2507.04906", "authors": ["Gianmarco Tedeschi", "Rune Kristian Lundedal Nielsen", "Paolo Burelli"], "title": "Using Psychophysiological Insights to Evaluate the Impact of Loot Boxes on Arousal", "categories": ["cs.HC"], "comment": null, "summary": "This study investigates the psychophysiological effects of loot box\ninteractions in video games and their potential similarities to those recorded\nduring gambling interactions. Using electrodermal activity (EDA) measurements,\nthe research examines player arousal during loot box interactions and explores\nthe relationship between Internet Gaming Disorder (IGD) severity and loot box\ninteractions from a psychophysiological perspective. The study employs a\ncustom-designed game to control experimental conditions and standardise loot\nbox interactions. Participants' IGD severity is assessed using the Internet\nGaming Disorder Scale - Short Form (IGDS9-SF), while arousal is measured\nthrough EDA, analysing both tonic and phasic components. The study contributes\nto the ongoing debate surrounding gaming disorder and loot boxes, offering\ninsights for game developers and policymakers on the potential risks associated\nwith random reward mechanisms in video games.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6e38\u620f\u5185\u62bd\u5956\u673a\u5236\uff08\u6218\u5229\u54c1\u7bb1\uff09\u7684\u5fc3\u7406\u751f\u7406\u6548\u5e94\u53ca\u5176\u4e0e\u8d4c\u535a\u6548\u5e94\u7684\u76f8\u4f3c\u6027\uff0c\u901a\u8fc7EDA\u6d4b\u91cf\u73a9\u5bb6\u5174\u594b\u7a0b\u5ea6\uff0c\u5206\u6790\u5176\u4e0e\u6e38\u620f\u6210\u763e\u4e25\u91cd\u5ea6\u7684\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22\u6218\u5229\u54c1\u7bb1\u673a\u5236\u662f\u5426\u4e0e\u8d4c\u535a\u7c7b\u4f3c\uff0c\u53ca\u5176\u5bf9\u6e38\u620f\u6210\u763e\u7684\u5f71\u54cd\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u98ce\u9669\u53c2\u8003\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u6e38\u620f\u63a7\u5236\u5b9e\u9a8c\u6761\u4ef6\uff0c\u901a\u8fc7IGDS9-SF\u8bc4\u4f30\u6e38\u620f\u6210\u763e\u4e25\u91cd\u5ea6\uff0cEDA\u6d4b\u91cf\u5174\u594b\u7a0b\u5ea6\uff08\u5305\u62ec\u7d27\u5f20\u548c\u76f8\u4f4d\u6210\u5206\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6218\u5229\u54c1\u7bb1\u53ef\u80fd\u5f15\u53d1\u4e0e\u8d4c\u535a\u76f8\u4f3c\u7684\u5fc3\u7406\u751f\u7406\u53cd\u5e94\uff0c\u53ef\u80fd\u4e0e\u6e38\u620f\u6210\u763e\u7a0b\u5ea6\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6218\u5229\u54c1\u7bb1\u7684\u98ce\u9669\u63d0\u4f9b\u4e86\u79d1\u5b66\u4f9d\u636e\uff0c\u5efa\u8bae\u5f00\u53d1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5173\u6ce8\u5176\u6f5c\u5728\u5371\u5bb3\u3002"}}
{"id": "2507.04970", "pdf": "https://arxiv.org/pdf/2507.04970", "abs": "https://arxiv.org/abs/2507.04970", "authors": ["Matt Adams", "Nick Tandavanitj", "Steve Benford", "Ayse Kucukyilmaz", "Victor Ngo", "Simon Castle-Green", "Guido Salimberi", "Pepita Bernard", "Joel Fischer", "Alan Chamberlain", "Eike Schneiders", "Clara Mancini"], "title": "Cat Royale: An Artistic Inquiry into Trust in Robots", "categories": ["cs.HC"], "comment": "Published at ICRA 2025 in the Arts in Robotics track\n  (https://roboticart.org/icra2025/)", "summary": "Cat Royale is an artwork created by the artists Blast Theory to explore the\nquestion of whether we should trust robots to care for our loved ones. The\nartists endeavoured to create a `Cat Utopia', a luxurious environment that was\ninhabited by a family of three cats for six hours a day for twelve days, at the\ncentre of which a robot arm played with them by wielding toys. Behind the\nscenes, the decision engine recommended games based on ongoing assessment of\ntheir happiness. A video installation featuring an eight-hour movie of the\ncats' exploits is currently touring worldwide, provoking audiences to engage\nwith the question of trust in autonomous systems.", "AI": {"tldr": "\u827a\u672f\u9879\u76ee'Cat Royale'\u63a2\u8ba8\u662f\u5426\u4fe1\u4efb\u673a\u5668\u4eba\u7167\u987e\u5ba0\u7269\uff0c\u901a\u8fc7\u732b\u7684\u4e4c\u6258\u90a6\u73af\u5883\u548c\u673a\u5668\u4eba\u4e92\u52a8\u5f15\u53d1\u601d\u8003\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u662f\u5426\u5e94\u4fe1\u4efb\u673a\u5668\u4eba\u7167\u987e\u6240\u7231\u4e4b\u4eba\uff08\u5982\u5ba0\u7269\uff09\uff0c\u63a2\u8ba8\u81ea\u4e3b\u7cfb\u7edf\u7684\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u827a\u672f\u5bb6\u521b\u9020\u732b\u7684\u4e4c\u6258\u90a6\u73af\u5883\uff0c\u7531\u673a\u5668\u4eba\u624b\u81c2\u4e0e\u732b\u4e92\u52a812\u5929\uff0c\u51b3\u7b56\u5f15\u64ce\u6839\u636e\u732b\u7684\u5e78\u798f\u8bc4\u4f30\u63a8\u8350\u6e38\u620f\u3002", "result": "\u9879\u76ee\u5236\u4f5c\u4e868\u5c0f\u65f6\u7535\u5f71\uff0c\u5168\u7403\u5de1\u5c55\uff0c\u5f15\u53d1\u89c2\u4f17\u5bf9\u81ea\u4e3b\u7cfb\u7edf\u4fe1\u4efb\u7684\u8ba8\u8bba\u3002", "conclusion": "\u901a\u8fc7\u827a\u672f\u5f62\u5f0f\u6210\u529f\u5f15\u53d1\u516c\u4f17\u5bf9\u673a\u5668\u4eba\u4fe1\u4efb\u95ee\u9898\u7684\u5173\u6ce8\u548c\u601d\u8003\u3002"}}
{"id": "2507.05046", "pdf": "https://arxiv.org/pdf/2507.05046", "abs": "https://arxiv.org/abs/2507.05046", "authors": ["Kadija Bouyzourn", "Alexandra Birch"], "title": "What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User Attributes, Trust Dimensions, Task Context, and Societal Perceptions among University Students", "categories": ["cs.HC"], "comment": "25 pages, 11 tables, 6 figures", "summary": "This mixed-methods inquiry examined four domains that shape university\nstudents' trust in ChatGPT: user attributes, seven delineated trust dimensions,\ntask context, and perceived societal impact. Data were collected through a\nsurvey of 115 UK undergraduate and postgraduate students and four complementary\nsemi-structured interviews. Behavioural engagement outweighed demographics:\nfrequent use increased trust, whereas self-reported understanding of\nlarge-language-model mechanics reduced it. Among the dimensions, perceived\nexpertise and ethical risk were the strongest predictors of overall trust; ease\nof use and transparency had secondary effects, while human-likeness and\nreputation were non-significant. Trust was highly task-contingent; highest for\ncoding and summarising, lowest for entertainment and citation generation, yet\nconfidence in ChatGPT's referencing ability, despite known inaccuracies, was\nthe single strongest correlate of global trust, indicating automation bias.\nComputer-science students surpassed peers only in trusting the system for\nproofreading and writing, suggesting technical expertise refines rather than\ninflates reliance. Finally, students who viewed AI's societal impact positively\nreported the greatest trust, whereas mixed or negative outlooks dampened\nconfidence. These findings show that trust in ChatGPT hinges on task\nverifiability, perceived competence, ethical alignment and direct experience,\nand they underscore the need for transparency, accuracy cues and user education\nwhen deploying LLMs in academic settings.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u5b66\u751f\u5bf9ChatGPT\u4fe1\u4efb\u7684\u56db\u5927\u5f71\u54cd\u56e0\u7d20\uff1a\u7528\u6237\u5c5e\u6027\u3001\u4fe1\u4efb\u7ef4\u5ea6\u3001\u4efb\u52a1\u60c5\u5883\u548c\u793e\u4f1a\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u9891\u7387\u63d0\u5347\u4fe1\u4efb\uff0c\u800c\u4e86\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u673a\u5236\u5219\u964d\u4f4e\u4fe1\u4efb\uff1b\u4e13\u4e1a\u6027\u548c\u4f26\u7406\u98ce\u9669\u662f\u4fe1\u4efb\u7684\u4e3b\u8981\u9884\u6d4b\u56e0\u7d20\u3002", "motivation": "\u4e86\u89e3\u5927\u5b66\u751f\u4fe1\u4efbChatGPT\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u5b66\u672f\u573a\u666f\u4e2d\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff1a\u5bf9115\u540d\u82f1\u56fd\u672c\u79d1\u751f\u548c\u7814\u7a76\u751f\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u8f85\u4ee54\u6b21\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u3002", "result": "\u4fe1\u4efb\u4e0e\u4efb\u52a1\u53ef\u9a8c\u8bc1\u6027\u3001\u611f\u77e5\u80fd\u529b\u3001\u4f26\u7406\u5bf9\u9f50\u53ca\u76f4\u63a5\u7ecf\u9a8c\u76f8\u5173\uff1b\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u5bf9\u6821\u5bf9\u548c\u5199\u4f5c\u4efb\u52a1\u66f4\u4fe1\u4efb\u3002", "conclusion": "\u5728\u5b66\u672f\u4e2d\u4f7f\u7528ChatGPT\u9700\u5173\u6ce8\u900f\u660e\u5ea6\u3001\u51c6\u786e\u6027\u63d0\u793a\u548c\u7528\u6237\u6559\u80b2\uff0c\u4ee5\u589e\u5f3a\u4fe1\u4efb\u3002"}}
{"id": "2507.05187", "pdf": "https://arxiv.org/pdf/2507.05187", "abs": "https://arxiv.org/abs/2507.05187", "authors": ["Andreas Mayer"], "title": "Infrastructuring Contestability: A Framework for Community-Defined AI Value Pluralism", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The proliferation of AI-driven systems presents a fundamental challenge to\nHuman-Computer Interaction (HCI) and Computer-Supported Cooperative Work\n(CSCW), often diminishing user agency and failing to account for value\npluralism. Current approaches to value alignment, which rely on centralized,\ntop-down definitions, lack the mechanisms for meaningful contestability. This\nleaves users and communities unable to challenge or shape the values embedded\nin the systems that govern their digital lives, creating a crisis of legitimacy\nand trust. This paper introduces Community-Defined AI Value Pluralism (CDAVP),\na socio-technical framework that addresses this gap. It reframes the design\nproblem from achieving a single aligned state to infrastructuring a dynamic\necosystem for value deliberation and application. At its core, CDAVP enables\ndiverse, self-organizing communities to define and maintain explicit value\nprofiles - rich, machine-readable representations that can encompass not only\npreferences but also community-specific rights and duties. These profiles are\nthen contextually activated by the end-user, who retains ultimate control\n(agency) over which values guide the AI's behavior. AI applications, in turn,\nare designed to transparently interpret these profiles and moderate conflicts,\nadhering to a set of non-negotiable, democratically-legitimated meta-rules. The\ndesigner's role shifts from crafting static interfaces to becoming an architect\nof participatory ecosystems. We argue that infrastructuring for pluralism is a\nnecessary pathway toward achieving robust algorithmic accountability and\ngenuinely contestable, human-centric AI.", "AI": {"tldr": "\u63d0\u51faCommunity-Defined AI Value Pluralism (CDAVP)\u6846\u67b6\uff0c\u89e3\u51b3AI\u7cfb\u7edf\u4ef7\u503c\u5bf9\u9f50\u95ee\u9898\uff0c\u5f3a\u8c03\u793e\u533a\u81ea\u6cbb\u4e0e\u7528\u6237\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u7684\u4ef7\u503c\u5bf9\u9f50\u65b9\u6cd5\u8fc7\u4e8e\u96c6\u4e2d\u5316\uff0c\u5ffd\u7565\u4e86\u4ef7\u503c\u591a\u5143\u6027\uff0c\u5bfc\u81f4\u7528\u6237\u548c\u793e\u533a\u65e0\u6cd5\u6311\u6218\u6216\u5851\u9020\u7cfb\u7edf\u4ef7\u503c\u89c2\uff0c\u5f15\u53d1\u4fe1\u4efb\u5371\u673a\u3002", "method": "\u91c7\u7528CDAVP\u6846\u67b6\uff0c\u901a\u8fc7\u793e\u533a\u5b9a\u4e49\u548c\u7ef4\u62a4\u660e\u786e\u7684\u4ef7\u503c\u914d\u7f6e\u6587\u4ef6\uff0c\u7528\u6237\u53ef\u52a8\u6001\u6fc0\u6d3b\u8fd9\u4e9b\u914d\u7f6e\uff0cAI\u5e94\u7528\u900f\u660e\u89e3\u91ca\u5e76\u8c03\u89e3\u51b2\u7a81\u3002", "result": "CDAVP\u6846\u67b6\u652f\u6301\u591a\u5143\u4ef7\u503c\u8868\u8fbe\uff0c\u589e\u5f3a\u7528\u6237\u4ee3\u7406\uff0c\u63d0\u5347\u7b97\u6cd5\u900f\u660e\u5ea6\u548c\u53ef\u4e89\u8bae\u6027\uff0c\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u7cfb\u7edf\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u5143\u4ef7\u503c\u751f\u6001\u7cfb\u7edf\uff0cCDAVP\u4e3a\u7b97\u6cd5\u95ee\u8d23\u5236\u548c\u771f\u6b63\u7684\u4eba\u7c7b\u4e2d\u5fc3AI\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.02913", "pdf": "https://arxiv.org/pdf/2507.02913", "abs": "https://arxiv.org/abs/2507.02913", "authors": ["Andrew Schwabe", "\u00d6zg\u00fcr Akg\u00fcn", "Ella Haig"], "title": "Toward Cyclic A.I. Modelling of Self-Regulated Learning: A Case Study with E-Learning Trace Data", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG", "F.2.2; I.2.4; I.2.8"], "comment": "6 pages, 3 figures", "summary": "Many e-learning platforms assert their ability or potential to improve\nstudents' self-regulated learning (SRL), however the cyclical and undirected\nnature of SRL theoretical models represent significant challenges for\nrepresentation within contemporary machine learning frameworks. We apply\nSRL-informed features to trace data in order to advance modelling of students'\nSRL activities, to improve predictability and explainability regarding the\ncausal effects of learning in an eLearning environment. We demonstrate that\nthese features improve predictive accuracy and validate the value of further\nresearch into cyclic modelling techniques for SRL.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u673a\u5668\u5b66\u4e60\u6846\u67b6\u4e2d\u5efa\u6a21\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff08SRL\uff09\uff0c\u901a\u8fc7SRL\u7279\u5f81\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u867d\u7136\u8bb8\u591a\u7535\u5b50\u5b66\u4e60\u5e73\u53f0\u58f0\u79f0\u80fd\u63d0\u5347\u5b66\u751f\u7684SRL\u80fd\u529b\uff0c\u4f46SRL\u7684\u5faa\u73af\u548c\u65e0\u5b9a\u5411\u7279\u6027\u5bf9\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u5e94\u7528SRL\u7279\u5f81\u5206\u6790\u5b66\u4e60\u6570\u636e\uff0c\u4ee5\u6539\u8fdbSRL\u6d3b\u52a8\u7684\u5efa\u6a21\u3002", "result": "\u8fd9\u4e9b\u7279\u5f81\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5faa\u73af\u5efa\u6a21\u6280\u672f\u7684\u4ef7\u503c\u3002", "conclusion": "\u8fdb\u4e00\u6b65\u7814\u7a76\u5faa\u73af\u5efa\u6a21\u6280\u672f\u5bf9SRL\u7684\u6f5c\u5728\u5f71\u54cd\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2507.02922", "pdf": "https://arxiv.org/pdf/2507.02922", "abs": "https://arxiv.org/abs/2507.02922", "authors": ["V. C. Storey", "J. Parsons", "A. Castellanos", "M. Tremblay", "R. Lukyanenko", "W. Maass", "A. Castillo"], "title": "Domain Knowledge in Artificial Intelligence: Using Conceptual Modeling to Increase Machine Learning Accuracy and Explainability", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Machine learning enables the extraction of useful information from large,\ndiverse datasets. However, despite many successful applications, machine\nlearning continues to suffer from performance and transparency issues. These\nchallenges can be partially attributed to the limited use of domain knowledge\nby machine learning models. This research proposes using the domain knowledge\nrepresented in conceptual models to improve the preparation of the data used to\ntrain machine learning models. We develop and demonstrate a method, called the\nConceptual Modeling for Machine Learning (CMML), which is comprised of\nguidelines for data preparation in machine learning and based on conceptual\nmodeling constructs and principles. To assess the impact of CMML on machine\nlearning outcomes, we first applied it to two real-world problems to evaluate\nits impact on model performance. We then solicited an assessment by data\nscientists on the applicability of the method. These results demonstrate the\nvalue of CMML for improving machine learning outcomes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6982\u5ff5\u6a21\u578b\u4e2d\u7684\u9886\u57df\u77e5\u8bc6\u6765\u6539\u8fdb\u673a\u5668\u5b66\u4e60\u6570\u636e\u51c6\u5907\u7684\u65b9\u6cd5CMML\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u660e\u4e86\u5176\u5bf9\u673a\u5668\u5b66\u4e60\u6027\u80fd\u7684\u63d0\u5347\u4f5c\u7528\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u5904\u7406\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u65f6\u9762\u4e34\u6027\u80fd\u548c\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u578b\u5bf9\u9886\u57df\u77e5\u8bc6\u7684\u5229\u7528\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u6982\u5ff5\u5efa\u6a21\u6307\u5bfc\u4e0b\u7684\u673a\u5668\u5b66\u4e60\u6570\u636e\u51c6\u5907\u65b9\u6cd5CMML\uff0c\u5e76\u5e94\u7528\u4e8e\u5b9e\u9645\u95ee\u9898\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aCMML\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u8bc4\u4f30\u4e5f\u652f\u6301\u5176\u9002\u7528\u6027\u3002", "conclusion": "CMML\u901a\u8fc7\u6574\u5408\u9886\u57df\u77e5\u8bc6\u6709\u6548\u6539\u8fdb\u4e86\u673a\u5668\u5b66\u4e60\u7684\u6027\u80fd\u548c\u6570\u636e\u51c6\u5907\u8fc7\u7a0b\u3002"}}
{"id": "2507.02950", "pdf": "https://arxiv.org/pdf/2507.02950", "abs": "https://arxiv.org/abs/2507.02950", "authors": ["Keita Kiuchi", "Yoshikazu Fujimoto", "Hideyuki Goto", "Tomonori Hosokawa", "Makoto Nishimura", "Yosuke Sato", "Izumi Sezai"], "title": "Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria", "categories": ["cs.CL", "cs.AI", "cs.HC", "68T50", "I.2.7; H.5.2; J.4"], "comment": "69 pages, 0 figures, 9 tables; data and code at\n  https://osf.io/p8c39/files/2e58c42f-a7ba-45f2-aa60-265e107e36db", "summary": "This study provides the first comprehensive evaluation of large language\nmodel (LLM) performance across three counseling roles in Japanese-language\ntherapeutic contexts. We simultaneously assessed counselor artificial\nintelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured\nMulti-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations,\nand evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human\nexperts (n = 15) with extensive counseling experience evaluated AI-generated\ndialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding\nManual 4.2.1.\n  Notably, SMDP implementation significantly enhanced counselor AI performance\nacross all MITI global ratings compared with zeroshot prompting, with no\nsignificant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed\ncomparable performance to human raters for Cultivating Change Talk but\nsystematically overestimated Softening Sustain Talk and the overall quality\nmetrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3\nfocused on technical proficiency, and Sonnet prioritized emotional expression.\nClient AI simulations exhibited a limited emotional range and unnaturally high\ncompliance, indicating the need for enhanced realism.\n  These findings establish benchmarks for AI-assisted counseling in non-English\ncontexts and identify critical areas for improvement through advanced prompt\nengineering, retrieval-augmented generation, and targeted fine-tuning, with\nimportant implications for developing culturally sensitive AI mental health\ntools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e5\u672c\u8bed\u6cbb\u7597\u73af\u5883\u4e2d\u4e09\u79cd\u54a8\u8be2\u89d2\u8272\u7684\u8868\u73b0\uff0c\u53d1\u73b0SMDP\u663e\u8457\u63d0\u5347\u54a8\u8be2AI\u6027\u80fd\uff0c\u8bc4\u4f30AI\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8005\u5728\u67d0\u4e9b\u6307\u6807\u4e0a\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u4ecd\u5b58\u5728\u504f\u89c1\u548c\u5c40\u9650\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30AI\u5728\u975e\u82f1\u8bed\uff08\u65e5\u8bed\uff09\u6cbb\u7597\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u5f00\u53d1\u6587\u5316\u654f\u611f\u7684AI\u5fc3\u7406\u5065\u5eb7\u5de5\u5177\u63d0\u4f9b\u57fa\u51c6\u548c\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u4e13\u5bb6\uff08n=15\uff09\u4f7f\u7528MITI\u7f16\u7801\u624b\u518c4.2.1\u8bc4\u4f30AI\u751f\u6210\u7684\u5bf9\u8bdd\uff0c\u6bd4\u8f83\u4e0d\u540cAI\u7cfb\u7edf\uff08\u5982GPT-4-turbo\u3001Claude-3-Opus\u7b49\uff09\u7684\u6027\u80fd\u3002", "result": "SMDP\u663e\u8457\u63d0\u5347\u54a8\u8be2AI\u8868\u73b0\uff1b\u8bc4\u4f30AI\u5728\u90e8\u5206\u6307\u6807\u4e0a\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8005\u76f8\u5f53\uff0c\u4f46\u5b58\u5728\u7cfb\u7edf\u9ad8\u4f30\u548c\u6a21\u578b\u504f\u89c1\uff1b\u5ba2\u6237AI\u6a21\u62df\u60c5\u611f\u8303\u56f4\u6709\u9650\uff0c\u9700\u63d0\u9ad8\u771f\u5b9e\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u8f85\u52a9\u975e\u82f1\u8bed\u54a8\u8be2\u8bbe\u7acb\u4e86\u57fa\u51c6\uff0c\u5e76\u6307\u51fa\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u3001\u5fae\u8c03\u7b49\u65b9\u6cd5\u6539\u8fdb\u7684\u65b9\u5411\uff0c\u5bf9\u5f00\u53d1\u6587\u5316\u654f\u611f\u7684AI\u5fc3\u7406\u5065\u5eb7\u5de5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.03049", "pdf": "https://arxiv.org/pdf/2507.03049", "abs": "https://arxiv.org/abs/2507.03049", "authors": ["Ferran Gebell\u00ed", "Ana\u00eds Garrell", "Jan-Gerrit Habekost", "S\u00e9verin Lemaignan", "Stefan Wermter", "Raquel Ros"], "title": "Personalised Explanations in Long-term Human-Robot Interactions", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "8 pages. It will be published at RO-MAN 2025", "summary": "In the field of Human-Robot Interaction (HRI), a fundamental challenge is to\nfacilitate human understanding of robots. The emerging domain of eXplainable\nHRI (XHRI) investigates methods to generate explanations and evaluate their\nimpact on human-robot interactions. Previous works have highlighted the need to\npersonalise the level of detail of these explanations to enhance usability and\ncomprehension. Our paper presents a framework designed to update and retrieve\nuser knowledge-memory models, allowing for adapting the explanations' level of\ndetail while referencing previously acquired concepts. Three architectures\nbased on our proposed framework that use Large Language Models (LLMs) are\nevaluated in two distinct scenarios: a hospital patrolling robot and a kitchen\nassistant robot. Experimental results demonstrate that a two-stage\narchitecture, which first generates an explanation and then personalises it, is\nthe framework architecture that effectively reduces the level of detail only\nwhen there is related user knowledge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7528\u6237\u77e5\u8bc6\u8bb0\u5fc6\u6a21\u578b\u7684XHRI\u6846\u67b6\uff0c\u901a\u8fc7\u9002\u5e94\u89e3\u91ca\u7684\u8be6\u7ec6\u7a0b\u5ea6\u6765\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u7406\u89e3\u6548\u679c\uff0c\u5e76\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u4ea4\u4e92\u4e2d\u89e3\u91ca\u7684\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u63d0\u5347\u7528\u6237\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u7406\u89e3\u548c\u53ef\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u66f4\u65b0\u548c\u68c0\u7d22\u7528\u6237\u77e5\u8bc6\u8bb0\u5fc6\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u9636\u6bb5\u67b6\u6784\uff08\u5148\u751f\u6210\u89e3\u91ca\u518d\u4e2a\u6027\u5316\uff09\u80fd\u6709\u6548\u51cf\u5c11\u89e3\u91ca\u7684\u8be6\u7ec6\u7a0b\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4e2a\u6027\u5316\u89e3\u91ca\u63d0\u9ad8\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u6548\u679c\uff0c\u672a\u6765\u53ef\u62d3\u5c55\u81f3\u66f4\u591a\u573a\u666f\u3002"}}
{"id": "2507.03330", "pdf": "https://arxiv.org/pdf/2507.03330", "abs": "https://arxiv.org/abs/2507.03330", "authors": ["Franklin Mingzhe Li", "Kaitlyn Ng", "Bin Zhu", "Patrick Carrington"], "title": "Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "ASSETS 2025", "summary": "Cooking plays a vital role in everyday independence and well-being, yet\nremains challenging for people with vision impairments due to limited support\nfor tracking progress and receiving contextual feedback. Object status - the\ncondition or transformation of ingredients and tools - offers a promising but\nunderexplored foundation for context-aware cooking support. In this paper, we\npresent OSCAR (Object Status Context Awareness for Recipes), a technical\npipeline that explores the use of object status recognition to enable recipe\nprogress tracking in non-visual cooking. OSCAR integrates recipe parsing,\nobject status extraction, visual alignment with cooking steps, and time-causal\nmodeling to support real-time step tracking. We evaluate OSCAR on 173\ninstructional videos and a real-world dataset of 12 non-visual cooking sessions\nrecorded by BLV individuals in their homes. Our results show that object status\nconsistently improves step prediction accuracy across vision-language models,\nand reveal key factors that impact performance in real-world conditions, such\nas implicit tasks, camera placement, and lighting. We contribute the pipeline\nof context-aware recipe progress tracking, an annotated real-world non-visual\ncooking dataset, and design insights to guide future context-aware assistive\ncooking systems.", "AI": {"tldr": "OSCAR\u662f\u4e00\u79cd\u5229\u7528\u7269\u4f53\u72b6\u6001\u8bc6\u522b\u6765\u652f\u6301\u65e0\u89c6\u89c9\u70f9\u996a\u7684\u6280\u672f\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u98df\u8c31\u89e3\u6790\u548c\u5b9e\u65f6\u6b65\u8fdb\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u70f9\u996a\u6b65\u9aa4\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u70f9\u996a\u5bf9\u89c6\u529b\u969c\u788d\u8005\u7684\u65e5\u5e38\u751f\u6d3b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u8ddf\u8e2a\u8fdb\u5ea6\u548c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u53cd\u9988\u7684\u652f\u6301\u3002\u7269\u4f53\u72b6\u6001\uff08\u98df\u6750\u548c\u5de5\u5177\u7684\u53d8\u6362\u72b6\u6001\uff09\u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u70f9\u996a\u652f\u6301\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "method": "OSCAR\u7ed3\u5408\u98df\u8c31\u89e3\u6790\u3001\u7269\u4f53\u72b6\u6001\u63d0\u53d6\u3001\u70f9\u996a\u6b65\u9aa4\u89c6\u89c9\u5bf9\u9f50\u548c\u65f6\u5e8f\u56e0\u679c\u5efa\u6a21\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6b65\u8fdb\u8ddf\u8e2a\u3002\u5728173\u4e2a\u6559\u5b66\u89c6\u9891\u548c12\u4e2a\u771f\u5b9e\u65e0\u89c6\u89c9\u70f9\u996a\u4f1a\u8bdd\u4e2d\u9a8c\u8bc1\u3002", "result": "\u7269\u4f53\u72b6\u6001\u663e\u8457\u63d0\u5347\u4e86\u8de8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6b65\u9aa4\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5f71\u54cd\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff08\u5982\u9690\u542b\u4efb\u52a1\u3001\u6444\u50cf\u5934\u4f4d\u7f6e\u548c\u5149\u7ebf\uff09\u3002", "conclusion": "OSCAR\u4e3a\u672a\u6765\u4e0a\u4e0b\u6587\u611f\u77e5\u8f85\u52a9\u70f9\u996a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6280\u672f\u6846\u67b6\u3001\u771f\u5b9e\u6570\u636e\u96c6\u548c\u8bbe\u8ba1\u89c1\u89e3\u3002"}}
{"id": "2507.03730", "pdf": "https://arxiv.org/pdf/2507.03730", "abs": "https://arxiv.org/abs/2507.03730", "authors": ["Gongwei Chen", "Xurui Zhou", "Rui Shao", "Yibo Lyu", "Kaiwen Zhou", "Shuai Wang", "Wentao Li", "Yinchuan Li", "Zhongang Qi", "Liqiang Nie"], "title": "Less is More: Empowering GUI Agent with Context-Aware Simplification", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "The research focus of GUI agents is shifting from text-dependent to\npure-vision-based approaches, which, though promising, prioritize comprehensive\npre-training data collection while neglecting contextual modeling challenges.\nWe probe the characteristics of element and history contextual modeling in GUI\nagent and summarize: 1) the high-density and loose-relation of element context\nhighlight the existence of many unrelated elements and their negative\ninfluence; 2) the high redundancy of history context reveals the inefficient\nhistory modeling in current GUI agents. In this work, we propose a\ncontext-aware simplification framework for building an efficient and effective\nGUI Agent, termed SimpAgent. To mitigate potential interference from numerous\nunrelated elements, we introduce a masking-based element pruning method that\ncircumvents the intractable relation modeling through an efficient masking\nmechanism. To reduce the redundancy in historical information, we devise a\nconsistency-guided history compression module, which enhances implicit\nLLM-based compression through innovative explicit guidance, achieving an\noptimal balance between performance and efficiency. With the above components,\nSimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances.\nComprehensive navigation experiments across diverse web and mobile environments\ndemonstrate the effectiveness and potential of our agent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSimpAgent\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7b80\u5316\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3GUI\u4ee3\u7406\u4e2d\u5143\u7d20\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5c4f\u853d\u65e0\u5173\u5143\u7d20\u548c\u538b\u7f29\u5197\u4f59\u5386\u53f2\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7eaf\u89c6\u89c9GUI\u4ee3\u7406\u65b9\u6cd5\u8fc7\u4e8e\u5173\u6ce8\u9884\u8bad\u7ec3\u6570\u636e\u6536\u96c6\uff0c\u800c\u5ffd\u89c6\u4e86\u5143\u7d20\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u65e0\u5173\u5143\u7d20\u548c\u5197\u4f59\u5386\u53f2\u4fe1\u606f\u5f71\u54cd\u4ee3\u7406\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u63d0\u51faSimpAgent\u6846\u67b6\uff0c\u5305\u62ec\u57fa\u4e8e\u5c4f\u853d\u7684\u5143\u7d20\u526a\u679d\u65b9\u6cd5\u548c\u4e00\u81f4\u6027\u5f15\u5bfc\u7684\u5386\u53f2\u538b\u7f29\u6a21\u5757\uff0c\u5206\u522b\u89e3\u51b3\u65e0\u5173\u5143\u7d20\u5e72\u6270\u548c\u5197\u4f59\u5386\u53f2\u4fe1\u606f\u95ee\u9898\u3002", "result": "SimpAgent\u51cf\u5c11\u4e8627%\u7684FLOPs\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u7f51\u9875\u548c\u79fb\u52a8\u73af\u5883\u4e2d\u5c55\u793a\u51fa\u4f18\u8d8a\u7684\u5bfc\u822a\u6027\u80fd\u3002", "conclusion": "SimpAgent\u901a\u8fc7\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u7b80\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.03866", "pdf": "https://arxiv.org/pdf/2507.03866", "abs": "https://arxiv.org/abs/2507.03866", "authors": ["Shuning Jiang", "Wei-Lun Chao", "Daniel Haehn", "Hanspeter Pfister", "Jian Chen"], "title": "A Rigorous Behavior Assessment of CNNs Using a Data-Domain Sampling Regime", "categories": ["cs.LG", "cs.CV", "cs.HC"], "comment": "This is a preprint of a paper that has been conditionally accepted\n  for publication at IEEE VIS 2025. The final version may be different upon\n  publication. 9 pages main text, 11 pages supplementary contents, 37 figures", "summary": "We present a data-domain sampling regime for quantifying CNNs' graphic\nperception behaviors. This regime lets us evaluate CNNs' ratio estimation\nability in bar charts from three perspectives: sensitivity to training-test\ndistribution discrepancies, stability to limited samples, and relative\nexpertise to human observers. After analyzing 16 million trials from 800 CNNs\nmodels and 6,825 trials from 113 human participants, we arrived at a simple and\nactionable conclusion: CNNs can outperform humans and their biases simply\ndepend on the training-test distance. We show evidence of this simple, elegant\nbehavior of the machines when they interpret visualization images. osf.io/gfqc3\nprovides registration, the code for our sampling regime, and experimental\nresults.", "AI": {"tldr": "CNN\u5728\u6761\u5f62\u56fe\u6bd4\u7387\u4f30\u8ba1\u80fd\u529b\u4e0a\u4f18\u4e8e\u4eba\u7c7b\uff0c\u5176\u504f\u5dee\u4ec5\u53d6\u51b3\u4e8e\u8bad\u7ec3-\u6d4b\u8bd5\u8ddd\u79bb\u3002", "motivation": "\u91cf\u5316CNN\u5728\u56fe\u5f62\u611f\u77e5\u884c\u4e3a\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u8bbe\u8ba1\u6570\u636e\u57df\u91c7\u6837\u673a\u5236\uff0c\u8bc4\u4f30CNN\u5bf9\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5e03\u5dee\u5f02\u7684\u654f\u611f\u6027\u3001\u6837\u672c\u4e0d\u8db3\u65f6\u7684\u7a33\u5b9a\u6027\u53ca\u4e0e\u4eba\u7c7b\u7684\u76f8\u5bf9\u80fd\u529b\u3002", "result": "\u5206\u6790\u4e861600\u4e07\u6b21CNN\u5b9e\u9a8c\u548c6825\u6b21\u4eba\u7c7b\u5b9e\u9a8c\uff0c\u53d1\u73b0CNN\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u4e14\u504f\u5dee\u7b80\u5355\u4f9d\u8d56\u4e8e\u8bad\u7ec3-\u6d4b\u8bd5\u8ddd\u79bb\u3002", "conclusion": "CNN\u5728\u53ef\u89c6\u5316\u56fe\u50cf\u89e3\u91ca\u4e2d\u8868\u73b0\u51fa\u7b80\u5355\u4e14\u9ad8\u6548\u7684\u884c\u4e3a\uff0c\u5176\u6027\u80fd\u53ef\u76f4\u63a5\u7531\u8bad\u7ec3-\u6d4b\u8bd5\u8ddd\u79bb\u9884\u6d4b\u3002"}}
{"id": "2507.03871", "pdf": "https://arxiv.org/pdf/2507.03871", "abs": "https://arxiv.org/abs/2507.03871", "authors": ["Karine Karine", "Benjamin M. Marlin"], "title": "Enhancing Adaptive Behavioral Interventions with LLM Inference from Participant-Described States", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": "Accepted at Machine Learning for Healthcare (MLHC) 2025", "summary": "The use of reinforcement learning (RL) methods to support health behavior\nchange via personalized and just-in-time adaptive interventions is of\nsignificant interest to health and behavioral science researchers focused on\nproblems such as smoking cessation support and physical activity promotion.\nHowever, RL methods are often applied to these domains using a small collection\nof context variables to mitigate the significant data scarcity issues that\narise from practical limitations on the design of adaptive intervention trials.\nIn this paper, we explore an approach to significantly expanding the state\nspace of an adaptive intervention without impacting data efficiency. The\nproposed approach enables intervention participants to provide natural language\ndescriptions of aspects of their current state. It then leverages inference\nwith pre-trained large language models (LLMs) to better align the policy of a\nbase RL method with these state descriptions. To evaluate our method, we\ndevelop a novel physical activity intervention simulation environment that\ngenerates text-based state descriptions conditioned on latent state variables\nusing an auxiliary LLM. We show that this approach has the potential to\nsignificantly improve the performance of online policy learning methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5065\u5eb7\u884c\u4e3a\u5e72\u9884\u4e2d\u7684\u72b6\u6001\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5065\u5eb7\u884c\u4e3a\u5e72\u9884\uff08\u5982\u6212\u70df\u548c\u4fc3\u8fdb\u4f53\u80b2\u6d3b\u52a8\uff09\u4e2d\uff0cRL\u65b9\u6cd5\u56e0\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u5e38\u53d7\u9650\u4e8e\u5c11\u6570\u4e0a\u4e0b\u6587\u53d8\u91cf\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u548cLLM\u63a8\u7406\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\uff0c\u4e0d\u964d\u4f4e\u6570\u636e\u6548\u7387\u3002", "method": "\u5229\u7528\u53c2\u4e0e\u8005\u63d0\u4f9b\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3LLM\u8fdb\u884c\u63a8\u7406\uff0c\u4f18\u5316RL\u7b56\u7565\u7684\u57fa\u7840\u65b9\u6cd5\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u62df\u73af\u5883\uff0c\u901a\u8fc7\u8f85\u52a9LLM\u751f\u6210\u57fa\u4e8e\u6f5c\u5728\u72b6\u6001\u53d8\u91cf\u7684\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u5728\u7ebf\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u548cLLM\u6269\u5c55\u72b6\u6001\u7a7a\u95f4\uff0c\u4e3aRL\u5728\u5065\u5eb7\u884c\u4e3a\u5e72\u9884\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.04009", "pdf": "https://arxiv.org/pdf/2507.04009", "abs": "https://arxiv.org/abs/2507.04009", "authors": ["Ziyang Miao", "Qiyu Sun", "Jingyuan Wang", "Yuchen Gong", "Yaowei Zheng", "Shiqi Li", "Richong Zhang"], "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "preprint", "summary": "Large language models (LLMs) have shown impressive performance on\ngeneral-purpose tasks, yet adapting them to specific domains remains\nchallenging due to the scarcity of high-quality domain data. Existing data\nsynthesis tools often struggle to extract reliable fine-tuning data from\nheterogeneous documents effectively. To address this limitation, we propose\nEasy Dataset, a unified framework for synthesizing fine-tuning data from\nunstructured documents via an intuitive graphical user interface (GUI).\nSpecifically, Easy Dataset allows users to easily configure text extraction\nmodels and chunking strategies to transform raw documents into coherent text\nchunks. It then leverages a persona-driven prompting approach to generate\ndiverse question-answer pairs using public-available LLMs. Throughout the\npipeline, a human-in-the-loop visual interface facilitates the review and\nrefinement of intermediate outputs to ensure data quality. Experiments on a\nfinancial question-answering task show that fine-tuning LLMs on the synthesized\ndataset significantly improves domain-specific performance while preserving\ngeneral knowledge. The source code and installable package are available at\nhttps://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub\nstars.", "AI": {"tldr": "Easy Dataset\u662f\u4e00\u4e2a\u901a\u8fc7GUI\u4ece\u975e\u7ed3\u6784\u5316\u6587\u6863\u4e2d\u751f\u6210\u5fae\u8c03\u6570\u636e\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u7279\u5b9a\u9886\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u9002\u5e94\u4e2d\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faEasy Dataset\u6846\u67b6\uff0c\u7ed3\u5408\u7528\u6237\u914d\u7f6e\u7684\u6587\u672c\u63d0\u53d6\u548c\u5206\u5757\u7b56\u7565\uff0c\u4ee5\u53ca\u4eba\u7269\u9a71\u52a8\u63d0\u793a\u65b9\u6cd5\u751f\u6210QA\u5bf9\uff0c\u5e76\u901a\u8fc7\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u57fa\u4e8e\u5408\u6210\u6570\u636e\u5fae\u8c03\u7684LLM\u5728\u91d1\u878d\u95ee\u7b54\u4efb\u52a1\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\u3002", "conclusion": "Easy Dataset\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5de5\u5177\u5df2\u5f00\u6e90\u5e76\u83b7\u5f97\u5e7f\u6cdb\u8ba4\u53ef\u3002"}}
{"id": "2507.04182", "pdf": "https://arxiv.org/pdf/2507.04182", "abs": "https://arxiv.org/abs/2507.04182", "authors": ["Sirina H\u00e5land", "Trond Karlsen Str\u00f8m", "Petra Galu\u0161\u010d\u00e1kov\u00e1"], "title": "Navigating Speech Recording Collections with AI-Generated Illustrations", "categories": ["cs.IR", "cs.CL", "cs.HC", "cs.SD", "eess.AS"], "comment": null, "summary": "Although the amount of available spoken content is steadily increasing,\nextracting information and knowledge from speech recordings remains\nchallenging. Beyond enhancing traditional information retrieval methods such as\nspeech search and keyword spotting, novel approaches for navigating and\nsearching spoken content need to be explored and developed. In this paper, we\npropose a novel navigational method for speech archives that leverages recent\nadvances in language and multimodal generative models. We demonstrate our\napproach with a Web application that organizes data into a structured format\nusing interactive mind maps and image generation tools. The system is\nimplemented using the TED-LIUM~3 dataset, which comprises over 2,000 speech\ntranscripts and audio files of TED Talks. Initial user tests using a System\nUsability Scale (SUS) questionnaire indicate the application's potential to\nsimplify the exploration of large speech collections.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u65b0\u578b\u8bed\u97f3\u6863\u6848\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7Web\u5e94\u7528\u5b9e\u73b0\u7ed3\u6784\u5316\u6570\u636e\u7ec4\u7ec7\u548c\u4ea4\u4e92\u5f0f\u601d\u7ef4\u5bfc\u56fe\u3002", "motivation": "\u968f\u7740\u53e3\u8bed\u5185\u5bb9\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u7684\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\u5df2\u4e0d\u8db3\u4ee5\u9ad8\u6548\u63d0\u53d6\u77e5\u8bc6\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u6765\u5bfc\u822a\u548c\u641c\u7d22\u8bed\u97f3\u5185\u5bb9\u3002", "method": "\u5229\u7528\u8bed\u8a00\u548c\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2aWeb\u5e94\u7528\uff0c\u4f7f\u7528\u4ea4\u4e92\u5f0f\u601d\u7ef4\u5bfc\u56fe\u548c\u56fe\u50cf\u751f\u6210\u5de5\u5177\u7ec4\u7ec7\u6570\u636e\u3002", "result": "\u5728TED-LIUM~3\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\uff0c\u7528\u6237\u6d4b\u8bd5\u663e\u793a\u7cfb\u7edf\u80fd\u7b80\u5316\u5927\u89c4\u6a21\u8bed\u97f3\u96c6\u5408\u7684\u63a2\u7d22\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u8bed\u97f3\u6863\u6848\u5bfc\u822a\u4e2d\u7684\u6f5c\u529b\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.04189", "pdf": "https://arxiv.org/pdf/2507.04189", "abs": "https://arxiv.org/abs/2507.04189", "authors": ["Runcong Zhao", "Qinglin Zhu", "Hainiu Xu", "Bin Liang", "Yulan He", "Lin Gui"], "title": "SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Understanding character relationships is essential for interpreting complex\nnarratives and conducting socially grounded AI research. However, manual\nannotation is time-consuming and low in coverage, while large language models\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\nextraction with symbolic reasoning. The system constructs editable character\nrelationship graphs, refines them using seven types of logical constraints, and\nenables real-time validation and conflict resolution through an interactive\ninterface. To support logical supervision and explainable social analysis, we\nrelease a dataset of 160 interpersonal relationships with corresponding logical\nstructures. Experiments show that SymbolicThought improves annotation accuracy\nand consistency while significantly reducing time cost, offering a practical\ntool for narrative understanding, explainable AI, and LLM evaluation.", "AI": {"tldr": "SymbolicThought\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u7b26\u53f7\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u7f16\u8f91\u7684\u89d2\u8272\u5173\u7cfb\u56fe\uff0c\u4f18\u5316\u6807\u6ce8\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u624b\u52a8\u6807\u6ce8\u8017\u65f6\u4e14\u8986\u76d6\u7387\u4f4e\uff0c\u4ee5\u53caLLM\u8f93\u51fa\u5b58\u5728\u5e7b\u89c9\u6216\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408LLM\u63d0\u53d6\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u901a\u8fc7\u4e03\u79cd\u903b\u8f91\u7ea6\u675f\u4f18\u5316\u5173\u7cfb\u56fe\uff0c\u5e76\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u754c\u9762\u5b9e\u65f6\u9a8c\u8bc1\u548c\u89e3\u51b3\u51b2\u7a81\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSymbolicThought\u663e\u8457\u63d0\u9ad8\u6807\u6ce8\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u65f6\u95f4\u6210\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53d9\u4e8b\u7406\u89e3\u3001\u53ef\u89e3\u91caAI\u548cLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.04295", "pdf": "https://arxiv.org/pdf/2507.04295", "abs": "https://arxiv.org/abs/2507.04295", "authors": ["Runcong Zhao", "Artem Borov", "Jiazheng Li", "Yulan He"], "title": "LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Effective feedback is essential for student learning but is time-intensive\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\npersonalised, curriculum-aligned feedback in science education. LearnLens\ncomprises three components: (1) an error-aware assessment module that captures\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\na structured, topic-linked memory chain rather than traditional\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\neducator-in-the-loop interface for customisation and oversight. LearnLens\naddresses key challenges in existing systems, offering scalable, high-quality\nfeedback that empowers both teachers and students.", "AI": {"tldr": "LearnLens\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff0c\u751f\u6210\u4e2a\u6027\u5316\u4e14\u7b26\u5408\u8bfe\u7a0b\u7684\u79d1\u5b66\u6559\u80b2\u53cd\u9988\uff0c\u89e3\u51b3\u4f20\u7edf\u53cd\u9988\u8017\u65f6\u7684\u95ee\u9898\u3002", "motivation": "\u6559\u5e08\u63d0\u4f9b\u6709\u6548\u53cd\u9988\u5bf9\u5b66\u751f\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u3002LearnLens\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u7cfb\u7edf\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u4e09\u4e2a\u6a21\u5757\uff1a\u9519\u8bef\u611f\u77e5\u8bc4\u4f30\u3001\u8bfe\u7a0b\u57fa\u7840\u7684\u751f\u6210\uff08\u4f7f\u7528\u7ed3\u6784\u5316\u8bb0\u5fc6\u94fe\uff09\u548c\u6559\u5e08\u53c2\u4e0e\u7684\u754c\u9762\u3002", "result": "LearnLens\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u8d28\u91cf\u7684\u53cd\u9988\uff0c\u51cf\u5c11\u566a\u97f3\u5e76\u63d0\u5347\u5173\u8054\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u6559\u5e08\u548c\u5b66\u751f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u652f\u6301\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2507.04340", "pdf": "https://arxiv.org/pdf/2507.04340", "abs": "https://arxiv.org/abs/2507.04340", "authors": ["Jan Kompatscher", "Danqing Shi", "Giovanna Varni", "Tino Weinkauf", "Antti Oulasvirta"], "title": "Interactive Groupwise Comparison for Reinforcement Learning from Human Feedback", "categories": ["cs.LG", "cs.HC", "I.2.6; H.5.2; I.2.9"], "comment": "11 pages, 11 figures in proceedings of Computer Graphics Forum", "summary": "Reinforcement learning from human feedback (RLHF) has emerged as a key\nenabling technology for aligning AI behavior with human preferences. The\ntraditional way to collect data in RLHF is via pairwise comparisons: human\nraters are asked to indicate which one of two samples they prefer. We present\nan interactive visualization that better exploits the human visual ability to\ncompare and explore whole groups of samples. The interface is comprised of two\nlinked views: 1) an exploration view showing a contextual overview of all\nsampled behaviors organized in a hierarchical clustering structure; and 2) a\ncomparison view displaying two selected groups of behaviors for user queries.\nUsers can efficiently explore large sets of behaviors by iterating between\nthese two views. Additionally, we devised an active learning approach\nsuggesting groups for comparison. As shown by our evaluation in six simulated\nrobotics tasks, our approach increases the final policy returns by 69.34%. It\nleads to lower error rates and better policies. We open-source the code that\ncan be easily integrated into the RLHF training loop, supporting research on\nhuman-AI alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u7ec4\u6bd4\u8f83\u63d0\u5347\u6570\u636e\u6536\u96c6\u6548\u7387\u3002", "motivation": "\u4f20\u7edfRLHF\u4f9d\u8d56\u4e24\u4e24\u6bd4\u8f83\uff0c\u6548\u7387\u8f83\u4f4e\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4eba\u7c7b\u7684\u89c6\u89c9\u6bd4\u8f83\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u63a2\u7d22\u89c6\u56fe\u548c\u6bd4\u8f83\u89c6\u56fe\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u754c\u9762\uff0c\u652f\u6301\u7528\u6237\u9ad8\u6548\u63a2\u7d22\u884c\u4e3a\u96c6\u5408\u5e76\u91c7\u7528\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u5efa\u8bae\u6bd4\u8f83\u7ec4\u3002", "result": "\u5728\u516d\u4e2a\u6a21\u62df\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u6700\u7ec8\u7b56\u7565\u56de\u62a5\u63d0\u534769.34%\uff0c\u4e14\u964d\u4f4e\u4e86\u9519\u8bef\u7387\u548c\u4f18\u5316\u4e86\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86RLHF\u7684\u6548\u679c\uff0c\u5f00\u6e90\u4ee3\u7801\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u4eba\u673a\u5bf9\u9f50\u3002"}}
{"id": "2507.04352", "pdf": "https://arxiv.org/pdf/2507.04352", "abs": "https://arxiv.org/abs/2507.04352", "authors": ["Greg Nyilasy", "Harsha Gangadharbatla"], "title": "AI-washing: The Asymmetric Effects of Its Two Types on Consumer Moral Judgments", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "As AI hype continues to grow, organizations face pressure to broadcast or\ndownplay purported AI initiatives - even when contrary to truth. This paper\nintroduces AI-washing as overstating (deceptive boasting) or understating\n(deceptive denial) a company's real AI usage. A 2x2 experiment (N = 401)\nexamines how these false claims affect consumer attitudes and purchase\nintentions. Results reveal a pronounced asymmetry: deceptive denial evokes more\nnegative moral judgments than honest negation, while deceptive boasting has no\neffects. We show that perceived betrayal mediates these outcomes. By clarifying\nhow AI-washing erodes trust, the study highlights clear ethical implications\nfor policymakers, marketers, and researchers striving for transparency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f01\u4e1a\u5938\u5927\u6216\u6de1\u5316AI\u4f7f\u7528\uff08AI-washing\uff09\u5bf9\u6d88\u8d39\u8005\u6001\u5ea6\u548c\u8d2d\u4e70\u610f\u5411\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u865a\u5047\u5426\u8ba4\u6bd4\u8bda\u5b9e\u5426\u5b9a\u5f15\u53d1\u66f4\u8d1f\u9762\u7684\u9053\u5fb7\u5224\u65ad\uff0c\u800c\u865a\u5047\u5938\u5927\u5219\u65e0\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AI\u70ed\u5ea6\u7684\u4e0a\u5347\uff0c\u4f01\u4e1a\u53ef\u80fd\u4e3a\u4e86\u8fce\u5408\u5e02\u573a\u800c\u5938\u5927\u6216\u6de1\u5316\u5176AI\u4f7f\u7528\u60c5\u51b5\uff0c\u8fd9\u79cd\u4e0d\u5b9e\u884c\u4e3a\u53ef\u80fd\u5f71\u54cd\u6d88\u8d39\u8005\u4fe1\u4efb\u3002", "method": "\u7814\u7a76\u91c7\u75282x2\u5b9e\u9a8c\u8bbe\u8ba1\uff08N=401\uff09\uff0c\u63a2\u8ba8\u865a\u5047\u5938\u5927\u548c\u865a\u5047\u5426\u8ba4\u5982\u4f55\u5f71\u54cd\u6d88\u8d39\u8005\u7684\u9053\u5fb7\u5224\u65ad\u548c\u8d2d\u4e70\u610f\u5411\u3002", "result": "\u865a\u5047\u5426\u8ba4\u5f15\u53d1\u66f4\u8d1f\u9762\u7684\u9053\u5fb7\u5224\u65ad\uff0c\u800c\u865a\u5047\u5938\u5927\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u8fd9\u79cd\u6548\u5e94\u7531\u611f\u77e5\u80cc\u53db\u4e2d\u4ecb\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86AI-washing\u5bf9\u4fe1\u4efb\u7684\u7834\u574f\u4f5c\u7528\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u3001\u8425\u9500\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4f26\u7406\u542f\u793a\u3002"}}
{"id": "2507.04996", "pdf": "https://arxiv.org/pdf/2507.04996", "abs": "https://arxiv.org/abs/2507.04996", "authors": ["Jiangbo Yu"], "title": "From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems", "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.HC", "cs.RO"], "comment": null, "summary": "Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity\nto operate according to internal rules without external control. Accordingly,\nautonomous vehicles (AuVs) are defined as systems capable of perceiving their\nenvironment and executing preprogrammed tasks independently of external input.\nHowever, both research and real-world deployments increasingly showcase\nvehicles that demonstrate behaviors beyond this definition (including the SAE\nlevels 1 to 6), such as interaction with humans and machines, goal adaptation,\ncontextual reasoning, external tool use, and long-term planning, particularly\nwith the integration of large language models (LLMs) and agentic AI systems.\nThese developments reveal a conceptual gap between technical autonomy and the\nbroader cognitive and social capabilities needed for future human-centered\nmobility systems. To address this, we introduce the concept of agentic vehicles\n(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and\ninteract within complex environments. This paper presents a systems-level\nframework to characterize AgVs, focusing on their cognitive and communicative\nlayers and differentiating them from conventional AuVs. It synthesizes relevant\nadvances in agentic AI, robotics, multi-agent systems, and human-machine\ninteraction, and highlights how agentic AI, through high-level reasoning and\ntool use, can function not merely as computational tools but as interactive\nagents embedded in mobility ecosystems. The paper concludes by identifying key\nchallenges in the development and governance of AgVs, including safety,\nreal-time control, public acceptance, ethical alignment, and regulatory\nframeworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u2018agentic vehicles\u2019\uff08AgVs\uff09\u6982\u5ff5\uff0c\u7ed3\u5408AI\u667a\u80fd\u4f53\u7684\u9ad8\u7ef4\u63a8\u7406\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u533a\u522b\u4e8e\u4f20\u7edf\u81ea\u4e3b\u8f66\u8f86\uff08AuVs\uff09\uff0c\u4ee5\u586b\u8865\u6280\u672f\u81ea\u4e3b\u6027\u4e0e\u672a\u6765\u79fb\u52a8\u7cfb\u7edf\u9700\u6c42\u7684\u8ba4\u77e5\u548c\u793e\u4f1a\u80fd\u529b\u9e3f\u6c9f\u3002", "motivation": "\u4f20\u7edf\u81ea\u4e3b\u8f66\u8f86\u7684\u5b9a\u4e49\u672a\u80fd\u6db5\u76d6\u5982\u4eba\u673a\u4ea4\u4e92\u3001\u76ee\u6807\u9002\u5e94\u7b49\u65b0\u5174\u884c\u4e3a\uff0c\u9700\u6269\u5c55\u4e3a\u66f4\u5177\u8ba4\u77e5\u548c\u793e\u4ea4\u80fd\u529b\u7684AgVs\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u7ea7\u6846\u67b6\uff0c\u6574\u5408\u667a\u80fd\u4f53AI\u3001\u673a\u5668\u4eba\u5b66\u7b49\u591a\u9886\u57df\u8fdb\u5c55\uff0c\u7a81\u51faAgVs\u7684\u8ba4\u77e5\u548c\u901a\u4fe1\u5c42\u3002", "result": "AgVs\u901a\u8fc7\u9ad8\u7ea7\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u53ef\u878d\u5165\u79fb\u52a8\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u3002", "conclusion": "\u5f00\u53d1AgVs\u9762\u4e34\u5b89\u5168\u3001\u4f26\u7406\u7b49\u6311\u6218\uff0c\u9700\u591a\u9886\u57df\u534f\u4f5c\u89e3\u51b3\u3002"}}
{"id": "2507.05030", "pdf": "https://arxiv.org/pdf/2507.05030", "abs": "https://arxiv.org/abs/2507.05030", "authors": ["Celeste Campos-Castillo", "Xuan Kang", "Linnea I. Laestadius"], "title": "Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good", "categories": ["cs.CY", "cs.AI", "cs.HC", "J.4"], "comment": null, "summary": "Recently, research into chatbots (also known as conversational agents, AI\nagents, voice assistants), which are computer applications using artificial\nintelligence to mimic human-like conversation, has grown sharply. Despite this\ngrowth, sociology lags other disciplines (including computer science, medicine,\npsychology, and communication) in publishing about chatbots. We suggest\nsociology can advance understanding of human-chatbot interaction and offer four\nsociological theories to enhance extant work in this field. The first two\ntheories (resource substitution theory, power-dependence theory) add new\ninsights to existing models of the drivers of chatbot use, which overlook\nsociological concerns about how social structure (e.g., systemic\ndiscrimination, the uneven distribution of resources within networks) inclines\nindividuals to use chatbots, including problematic levels of emotional\ndependency on chatbots. The second two theories (affect control theory,\nfundamental cause of disease theory) help inform the development of\nchatbot-driven interventions that minimize safety risks and enhance equity by\nleveraging sociological insights into how chatbot outputs could attend to\ncultural contexts (e.g., affective norms) to promote wellbeing and enhance\ncommunities (e.g., opportunities for civic participation). We discuss the value\nof applying sociological theories for advancing theorizing about human-chatbot\ninteraction and developing chatbots for social good.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u63a2\u8ba8\u4e86\u793e\u4f1a\u5b66\u5728\u804a\u5929\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u7684\u6ede\u540e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u56db\u79cd\u793e\u4f1a\u5b66\u7406\u8bba\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "motivation": "\u793e\u4f1a\u5b66\u5728\u804a\u5929\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u843d\u540e\u4e8e\u5176\u4ed6\u5b66\u79d1\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u793e\u4f1a\u5b66\u7406\u8bba\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u5bf9\u4eba\u7c7b\u4e0e\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u79cd\u793e\u4f1a\u5b66\u7406\u8bba\uff08\u8d44\u6e90\u66ff\u4ee3\u7406\u8bba\u3001\u6743\u529b\u4f9d\u8d56\u7406\u8bba\u3001\u60c5\u611f\u63a7\u5236\u7406\u8bba\u548c\u75be\u75c5\u6839\u672c\u539f\u56e0\u7406\u8bba\uff09\uff0c\u4ee5\u5206\u6790\u804a\u5929\u673a\u5668\u4eba\u4f7f\u7528\u7684\u9a71\u52a8\u56e0\u7d20\u548c\u5f00\u53d1\u5e72\u9884\u63aa\u65bd\u3002", "result": "\u8fd9\u4e9b\u7406\u8bba\u4e3a\u804a\u5929\u673a\u5668\u4eba\u7684\u4f7f\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u5b89\u5168\u3001\u516c\u5e73\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u4fc3\u8fdb\u793e\u4f1a\u798f\u7949\u3002", "conclusion": "\u5e94\u7528\u793e\u4f1a\u5b66\u7406\u8bba\u53ef\u4ee5\u63a8\u52a8\u4eba\u7c7b\u4e0e\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8\u7684\u7406\u8bba\u7814\u7a76\uff0c\u5e76\u5f00\u53d1\u66f4\u5177\u793e\u4f1a\u4ef7\u503c\u7684\u804a\u5929\u673a\u5668\u4eba\u3002"}}
