<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 15]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [math.HO](#math.HO) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Aspect-Oriented Programming in Secure Software Development: A Case Study of Security Aspects in Web Applications](https://arxiv.org/abs/2509.07449)
*Mterorga Ukor*

Main category: cs.SE

TL;DR: AOP在Web应用中通过模块化安全功能提升代码质量和可维护性，且性能开销小。


<details>
  <summary>Details</summary>
Motivation: 解决传统OOP中安全逻辑与业务功能纠缠导致的可维护性问题。

Method: 采用案例研究，对比AOP与OOP/中间件实现安全功能，分析代码和性能指标。

Result: AOP显著提升模块化、可重用性和可维护性，性能影响小。

Conclusion: AOP为平衡安全性与软件质量提供了实用方案。

Abstract: Security remains a critical challenge in modern web applications, where
threats such as unauthorized access, data breaches, and injection attacks
continue to undermine trust and reliability. Traditional Object-Oriented
Programming (OOP) often intertwines security logic with business functionality,
leading to code tangling, scattering, and reduced maintainability. This study
investigates the role of Aspect-Oriented Programming (AOP) in enhancing secure
software development by modularizing cross-cutting security concerns. Using a
case study approach, we compare AOP-based implementations of security features
including authentication, authorization, input validation, encryption, logging,
and session management with conventional OOP or middleware-based approaches.
Data collection involves analyzing code quality metrics (e.g., lines of code,
coupling, cohesion, modularity index, reusability), performance metrics
(response time, throughput, memory usage), and maintainability indicators.
Developer feedback is also incorporated to assess integration and debugging
experiences. Statistical methods, guided by the ISO/IEC 25010 software quality
model, are applied to evaluate differences across implementations. The findings
demonstrate that AOP enhances modularity, reusability, and maintainability of
security mechanisms, while introducing only minimal performance overhead. The
study contributes practical insights for software engineers and researchers
seeking to balance security with software quality in web application
development.

</details>


### [2] [CRACI: A Cloud-Native Reference Architecture for the Industrial Compute Continuum](https://arxiv.org/abs/2509.07498)
*Hai Dinh-Tuan*

Main category: cs.SE

TL;DR: 摘要介绍了CRACI，一种云原生参考架构，用于解决传统工业架构（如ISA-95和RAMI 4.0）在工业4.0中的局限性，支持灵活的数据流和跨领域需求。


<details>
  <summary>Details</summary>
Motivation: 传统工业架构（如ISA-95和RAMI 4.0）的刚性、数据孤岛以及对云原生技术的缺乏支持，阻碍了可扩展和互操作的工业系统的发展。

Method: 提出CRACI架构，采用解耦和事件驱动模型，并嵌入信任、治理与策略、可观测性和生命周期管理等核心支柱。通过理论分析和实际性能数据验证架构。

Result: CRACI展示了克服传统架构局限性的能力，支持可扩展的现代工业系统。

Conclusion: CRACI是一种可行的云原生架构，能够利用计算连续体克服传统模型的限制。

Abstract: The convergence of Information Technology (IT) and Operational Technology
(OT) in Industry 4.0 exposes the limitations of traditional, hierarchical
architectures like ISA-95 and RAMI 4.0. Their inherent rigidity, data silos,
and lack of support for cloud-native technologies impair the development of
scalable and interoperable industrial systems. This paper addresses this issue
by introducing CRACI, a Cloud-native Reference Architecture for the Industrial
Compute Continuum. Among other features, CRACI promotes a decoupled and
event-driven model to enable flexible, non-hierarchical data flows across the
continuum. It embeds cross-cutting concerns as foundational pillars: Trust,
Governance & Policy, Observability, and Lifecycle Management, ensuring quality
attributes are core to the design. The proposed architecture is validated
through a two-fold approach: (1) a comparative theoretical analysis against
established standards, operational models, and academic proposals; and (2) a
quantitative evaluation based on performance data from previously published
real-world smart manufacturing implementations. The results demonstrate that
CRACI provides a viable, state-of-the-art architecture that utilizes the
compute continuum to overcome the structural limitations of legacy models and
enable scalable, modern industrial systems.

</details>


### [3] [PatchSeeker: Mapping NVD Records to their Vulnerability-fixing Commits with LLM Generated Commits and Embeddings](https://arxiv.org/abs/2509.07540)
*Huu Hung Nguyen,Anh Tuan Nguyen,Thanh Le-Cong,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: PatchSeeker利用大型语言模型，通过生成VFC的详细摘要和嵌入NVD描述，显著提高漏洞修复提交的匹配性能。


<details>
  <summary>Details</summary>
Motivation: NVD数据库缺乏明确的漏洞修复提交（VFC）链接，现有方法依赖稀疏且噪音较多的提交信息，无法捕捉漏洞描述的深层语义。

Method: PatchSeeker通过LLM生成VFC的详细摘要，增强提交信息，并创建NVD描述的嵌入，以建立语义链接。

Result: PatchSeeker的MRR比最佳基线Prospector高59.3%，Recall@10高27.9%。进一步评估证实其有效性。

Conclusion: PatchSeeker显著提升VFC匹配性能，但需进一步研究局限性。

Abstract: Software vulnerabilities pose serious risks to modern software ecosystems.
While the National Vulnerability Database (NVD) is the authoritative source for
cataloging these vulnerabilities, it often lacks explicit links to the
corresponding Vulnerability-Fixing Commits (VFCs). VFCs encode precise code
changes, enabling vulnerability localization, patch analysis, and dataset
construction. Automatically mapping NVD records to their true VFCs is therefore
critical. Existing approaches have limitations as they rely on sparse, often
noisy commit messages and fail to capture the deep semantics in the
vulnerability descriptions. To address this gap, we introduce PatchSeeker, a
novel method that leverages large language models to create rich semantic links
between vulnerability descriptions and their VFCs. PatchSeeker generates
embeddings from NVD descriptions and enhances commit messages by synthesizing
detailed summaries for those that are short or uninformative. These generated
messages act as a semantic bridge, effectively closing the information gap
between natural language reports and low-level code changes. Our approach
PatchSeeker achieves 59.3% higher MRR and 27.9% higher Recall@10 than the
best-performing baseline, Prospector, on the benchmark dataset. The extended
evaluation on recent CVEs further confirms PatchSeeker's effectiveness.
Ablation study shows that both the commit message generation method and the
selection of backbone LLMs make a positive contribution to PatchSeeker. We also
discuss limitations and open challenges to guide future work.

</details>


### [4] [Bridging the Gap Between Binary and Source Based Package Management in Spack](https://arxiv.org/abs/2509.07728)
*John Gouwar,Gregory Becker,Tamara Dahlgren,Nathan Hanford,Arjun Guha,Todd Gamblin*

Main category: cs.SE

TL;DR: 本文介绍了Splicing，一种扩展Spack的方法，用于解决二进制兼容性问题，从而允许混合使用源代码和二进制分发，同时保持源代码构建的灵活性。


<details>
  <summary>Details</summary>
Motivation: 二进制包管理器安装速度快但缺乏灵活性，源代码包管理器灵活但编译速度慢。HPC领域的包管理器Spack无法混合使用不同来源的二进制文件。

Method: 提出了Splicing方法，扩展Spack的包装语言和依赖解析引擎，以重用兼容的二进制文件，同时保持源代码构建的灵活性。

Result: Splicing在安装时开销最小，支持快速从二进制文件安装，即使是对ABI敏感的依赖（如MPI）也无需大量重建。

Conclusion: Splicing成功解决了Spack在二进制兼容性方面的局限性，实现了源代码和二进制分发的无缝混合使用。

Abstract: Binary package managers install software quickly but they limit
configurability due to rigid ABI requirements that ensure compatibility between
binaries. Source package managers provide flexibility in building software, but
compilation can be slow. For example, installing an HPC code with a new MPI
implementation may result in a full rebuild. Spack, a widely deployed,
HPC-focused package manager, can use source and pre-compiled binaries, but
lacks a binary compatibility model, so it cannot mix binaries not built
together. We present splicing, an extension to Spack that models binary
compatibility between packages and allows seamless mixing of source and binary
distributions. Splicing augments Spack's packaging language and dependency
resolution engine to reuse compatible binaries but maintains the flexibility of
source builds. It incurs minimal installation-time overhead and allows rapid
installation from binaries, even for ABI-sensitive dependencies like MPI that
would otherwise require many rebuilds.

</details>


### [5] [What's Coming Next? Short-Term Simulation of Business Processes from Current State](https://arxiv.org/abs/2509.07747)
*Maksym Avramenko,David Chapela-Campa,Marlon Dumas,Fredrik Milani*

Main category: cs.SE

TL;DR: 论文提出了一种基于事件日志的短期业务流程仿真方法，通过从当前状态初始化仿真，比传统的长期仿真方法更准确地预测短期性能。


<details>
  <summary>Details</summary>
Motivation: 现有仿真方法主要支持战术决策，缺乏对操作决策的支持，尤其是对临时中断（如需求峰值或资源短缺）的影响分析。

Method: 研究提出了一种替代方法，从事件日志中提取当前状态信息，初始化仿真运行，解决了两个关键问题：状态信息的提取和仿真模型的初始化。

Result: 实验表明，该方法在存在概念漂移或突发性能模式的情况下，比传统方法更准确地预测短期性能。

Conclusion: 基于事件日志的短期仿真方法为操作决策提供了更准确的预测工具。

Abstract: Business process simulation is an approach to evaluate business process
changes prior to implementation. Existing methods in this field primarily
support tactical decision-making, where simulations start from an empty state
and aim to estimate the long-term effects of process changes. A complementary
use-case is operational decision-making, where the goal is to forecast
short-term performance based on ongoing cases and to analyze the impact of
temporary disruptions, such as demand spikes and shortfalls in available
resources. An approach to tackle this use-case is to run a long-term simulation
up to a point where the workload is similar to the current one (warm-up), and
measure performance thereon. However, this approach does not consider the
current state of ongoing cases and resources in the process. This paper studies
an alternative approach that initializes the simulation from a representation
of the current state derived from an event log of ongoing cases. The paper
addresses two challenges in operationalizing this approach: (1) Given a
simulation model, what information is needed so that a simulation run can start
from the current state of cases and resources? (2) How can the current state of
a process be derived from an event log? The resulting short-term simulation
approach is embodied in a simulation engine that takes as input a simulation
model and a log of ongoing cases, and simulates cases for a given time horizon.
An experimental evaluation shows that this approach yields more accurate
short-term performance forecasts than long-term simulations with warm-up
period, particularly in the presence of concept drift or bursty performance
patterns.

</details>


### [6] [What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring Motivations in Open-Source Projects](https://arxiv.org/abs/2509.07763)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: LLMs能有效捕捉代码重构的表面动机，但难以理解架构层面的深层原因。结合软件指标，可形成混合方法，优化重构实践。


<details>
  <summary>Details</summary>
Motivation: 研究开发者的重构动机及哪些指标能捕捉这些动机，以支持更广泛和有效的重构实践。

Method: 通过大规模实证研究分析开发者重构活动，利用LLMs从版本控制数据中识别动机，并与文献中的动机对比。

Result: LLMs在80%的案例中与人工判断一致，但与文献动机仅47%吻合。它们为22%的动机提供了更详细的解释，主要集中在代码可读性和结构改进。

Conclusion: LLMs适用于提供局部解释，结合软件指标可形成混合方法，系统化重构优先级的权衡短期改进与长期架构目标。

Abstract: Context. Code refactoring improves software quality without changing external
behavior. Despite its advantages, its benefits are hindered by the considerable
cost of time, resources, and continuous effort it demands. Aim. Understanding
why developers refactor, and which metrics capture these motivations, may
support wider and more effective use of refactoring in practice. Method. We
performed a large-scale empirical study to analyze developers refactoring
activity, leveraging Large Language Models (LLMs) to identify underlying
motivations from version control data, comparing our findings with previous
motivations reported in the literature. Results. LLMs matched human judgment in
80% of cases, but aligned with literature-based motivations in only 47%. They
enriched 22% of motivations with more detailed rationale, often highlighting
readability, clarity, and structural improvements. Most motivations were
pragmatic, focused on simplification and maintainability. While metrics related
to developer experience and code readability ranked highest, their correlation
with motivation categories was weak. Conclusions. We conclude that LLMs
effectively capture surface-level motivations but struggle with architectural
reasoning. Their value lies in providing localized explanations, which, when
combined with software metrics, can form hybrid approaches. Such integration
offers a promising path toward prioritizing refactoring more systematically and
balancing short-term improvements with long-term architectural goals.

</details>


### [7] ["We provide our resources in a dedicated repository": Surveying the Transparency of HICSS publications](https://arxiv.org/abs/2509.07851)
*Irdin Pekaric,Giovanni Apruzzese*

Main category: cs.SE

TL;DR: 研究分析了2017-2024年HICSS会议论文中使用外部存储库（如GitHub）提供补充材料的情况，发现仅有3%的论文具备可用公共存储库。


<details>
  <summary>Details</summary>
Motivation: 探讨科研透明度和可重复性，评估HICSS论文中外部存储库的使用程度。

Method: 收集5579篇HICSS论文，筛选出2028篇涉及人类研究或技术实现的论文，审查其是否包含功能性公共存储库链接。

Result: 仅有3%的论文具备可用公共存储库。

Conclusion: 科研界在使用外部存储库方面仍有显著提升空间。

Abstract: Every day, new discoveries are made by researchers from all across the globe
and fields. HICSS is a flagship venue to present and discuss such scientific
advances. Yet, the activities carried out for any given research can hardly be
fully contained in a single document of a few pages-the "paper." Indeed, any
given study entails data, artifacts, or other material that is crucial to truly
appreciate the contributions claimed in the corresponding paper. External
repositories (e.g., GitHub) are a convenient tool to store all such resources
so that future work can freely observe and build upon them -- thereby improving
transparency and promoting reproducibility of research as a whole. In this
work, we scrutinize the extent to which papers recently accepted to HICSS
leverage such repositories to provide supplementary material. To this end, we
collect all the 5579 papers included in HICSS proceedings from 2017-2024. Then,
we identify those entailing either human subject research (850) or technical
implementations (737), or both (147). Finally, we review their text, examining
how many include a link to an external repository-and, inspect its contents.
Overall, out of 2028 papers, only 3\% have a functional and publicly available
repository that is usable by downstream research. We release all our tools.

</details>


### [8] [Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation](https://arxiv.org/abs/2509.07933)
*Wanni Vidulige Ishan Perera,Xing Liu,Fan liang,Junyi Zhang*

Main category: cs.SE

TL;DR: 本文探讨了使用基于LLM的工具（如PentestGPT）自动化Android渗透测试的可行性，与传统手动方法对比，评估了自动化测试的效果、可靠性和扩展性。研究发现LLMs能显著提升工作流程效率，但仍需人类监督以确保准确性和道德应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是利用AI和LLMs在网络安全领域的新机遇，特别是在Android渗透测试自动化中，探索其与传统方法的差异及其潜力。

Method: 研究方法包括使用Android模拟器（Genymotion）作为测试平台，执行传统和AI生成的脚本进行测试，并开发一个集成OpenAI API的Web应用以自动化脚本生成。

Result: 结果表明，LLMs能大幅简化渗透测试工作流程，但在准确性和道德应用方面仍需人工控制。

Conclusion: 结论是AI在渗透测试中具有潜力，但需平衡效率与道德风险，为AI驱动的网络安全研究提供了新见解。

Abstract: The rapid evolution of Artificial Intelligence (AI) and Large Language Models
(LLMs) has opened up new opportunities in the area of cybersecurity, especially
in the exploitation automation landscape and penetration testing. This study
explores Android penetration testing automation using LLM-based tools,
especially PentestGPT, to identify and execute rooting techniques. Through a
comparison of the traditional manual rooting process and exploitation methods
produced using AI, this study evaluates the efficacy, reliability, and
scalability of automated penetration testing in achieving high-level privilege
access on Android devices. With the use of an Android emulator (Genymotion) as
the testbed, we fully execute both traditional and exploit-based rooting
methods, automating the process using AI-generated scripts. Secondly, we create
a web application by integrating OpenAI's API to facilitate automated script
generation from LLM-processed responses. The research focuses on the
effectiveness of AI-enabled exploitation by comparing automated and manual
penetration testing protocols, by determining LLM weaknesses and strengths
along the way. We also provide security suggestions of AI-enabled exploitation,
including ethical factors and potential misuse. The findings exhibit that while
LLMs can significantly streamline the workflow of exploitation, they need to be
controlled by humans to ensure accuracy and ethical application. This study
adds to the increasing body of literature on AI-powered cybersecurity and its
effect on ethical hacking, security research, and mobile device security.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD](https://arxiv.org/abs/2509.07003)
*Youjie Li,Cheng Wan,Zhiqi Lin,Hongyu Zhu,Jiacheng Yang,Ziang Song,Xinyi Di,Jiawei Wu,Huiyao Shu,Wenlei Bao,Yanghua Peng,Haibin Lin,Li-Wen Chang*

Main category: cs.PL

TL;DR: veScale是一个基于SPMD范式的分布式训练系统，解决了单设备一致性和高性能问题，相比现有系统提速2.2倍，代码复杂度降低78.4%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的分布式训练需要复杂的并行机制（如3D并行），而SPMD范式更简单易调试，但存在单设备执行一致性和性能问题。

Method: veScale采用SPMD范式，提出分布式随机数生成算法，优化PyTorch原语和通信效率。

Result: veScale在性能上提升2.2倍，代码复杂度降低78.4%，同时保持单设备等效结果。

Conclusion: veScale成功实现了高性能与简单性的结合，为分布式张量编程提供了新方案。

Abstract: Large Language Models (LLMs) have scaled rapidly in size and complexity,
requiring increasingly intricate parallelism for distributed training, such as
3D parallelism. This sophistication motivates a shift toward simpler, more
debuggable programming paradigm like Single Program Multiple Data (SPMD).
However, SPMD in eager execution introduces two key challenges: ensuring
consistency with single-device execution and achieving high performance at
scale. In this paper, we introduce veScale, an eager-mode training system that
fully embraces SPMD paradigm to democratize distributed tensor programming.
veScale addresses the prevalent issue of inconsistent results in systems like
PyTorch by introducing a novel algorithm of distributed Random Number
Generation (RNG) compatible with arbitrary sharded operators. veScale also
significantly boosts training performance by reducing PyTorch primitive's
overhead and improving communication efficiency. Evaluations show that veScale
delivers up to 2.2x speedup over the state-of-the-art training systems, like
TorchTitan, and cuts code complexity by 78.4%, while preserving
single-device-equivalent results.

</details>


### [10] [Fast and Extensible Hybrid Embeddings with Micros](https://arxiv.org/abs/2509.07551)
*Sean Bocirnea,William J. Bowman*

Main category: cs.PL

TL;DR: 论文提出了一种名为“micro embedding”的新方法，通过将语法转换为中间表示（IR）而非源代码，显著提升了编译时性能，并保持了类型语言的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统宏嵌入（macro embedding）方法虽然在扩展性上表现良好，但其编译时性能较差。为了解决这一问题，作者提出了更高效的替代方案。

Method: 采用micro embedding技术，将语法直接转换为中间表示（IR），并设计了一些模式来支持IR及其函数的可扩展性。

Result: 该方法显著提升了编译时性能，同时保持了对象语言的可扩展性。

Conclusion: micro embedding是传统宏嵌入的有效替代方案，尤其在需要高性能编译时的情况下。

Abstract: Macro embedding is a popular approach to defining extensible shallow
embeddings of object languages in Scheme like host languages. While macro
embedding has even been shown to enable implementing extensible typed languages
in systems like Racket, it comes at a cost: compile-time performance. In this
paper, we revisit micros - syntax to intermediate representation (IR)
transformers, rather than source syntax to source syntax transformers (macros).
Micro embedding enables stopping at an IR, producing a deep embedding and
enabling high performance compile-time functions over an efficient IR, before
shallowly embedding the IR back into source syntax. Combining micros with
several design patterns to enable the IR and functions over it to be
extensible, we achieve extensible hybrid embedding of statically typed
languages with significantly improved compile-time compared to macro-embedding
approaches. We describe our design patterns and propose new abstractions
packaging these patterns.

</details>


### [11] [What's in the Box: Ergonomic and Expressive Capture Tracking over Generic Data Structures (Extended Version)](https://arxiv.org/abs/2509.07609)
*Yichen Xu,Oliver Bračevac,Cao Nguyen Pham,Martin Odersky*

Main category: cs.PL

TL;DR: System Capless改进了Scala中的捕获类型系统，通过引入reach capabilities（rcaps）来命名泛型数据结构中的能力，提升了表达力，并成功应用于Scala标准库与实际库中，验证了其轻量级和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有Scala捕获类型系统无法有效追踪泛型数据结构中的能力，限制了其在标准库中的广泛采用。

Method: 提出System Capless，引入rcaps机制，通过存在和全称捕获集量化命名泛型类型中的能力，并在Lean中形式化验证。

Result: System Capless支持轻量级语法，扩展了表达力，且在实际应用中只需最小改动。

Conclusion: rcaps机制显著提升了捕获类型系统的实用性和扩展性，适用于实际生产代码。

Abstract: Capturing types in Scala unify static effect and resource tracking with
object capabilities, enabling lightweight effect polymorphism with minimal
notational overhead. However, their expressiveness has been insufficient for
tracking capabilities embedded in generic data structures, preventing them from
scaling to the standard collections library -- an essential prerequisite for
broader adoption. This limitation stems from the inability to name capabilities
within the system's notion of box types.
  This paper develops System Capless, a new foundation for capturing types that
provides the theoretical basis for reach capabilities (rcaps), a novel
mechanism for naming "what's in the box." The calculus refines the universal
capability notion into a new scheme with existential and universal capture set
quantification. Intuitively, rcaps witness existentially quantified capture
sets inside the boxes of generic types in a way that does not require exposing
existential capture types in the surface language. We have fully mechanized the
formal metatheory of System Capless in Lean, including proofs of type soundness
and scope safety. System Capless supports the same lightweight notation of
capturing types plus rcaps, as certified by a type-preserving translation, and
also enables fully optional explicit capture-set quantification to increase
expressiveness.
  Finally, we present a full reimplementation of capture checking in Scala 3
based on System Capless and migrate the entire Scala collections library and an
asynchronous programming library to evaluate its practicality and ergonomics.
Our results demonstrate that reach capabilities enable the adoption of capture
checking in production code with minimal changes and minimal-to-zero notational
overhead in a vast majority of cases.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [12] [Unikernels vs. Containers: A Runtime-Level Performance Comparison for Resource-Constrained Edge Workloads](https://arxiv.org/abs/2509.07891)
*Hai Dinh-Tuan*

Main category: cs.PF

TL;DR: 比较容器和unikernel在边缘计算中的性能表现，发现内存限制对不同执行模型的影响各不相同。


<details>
  <summary>Details</summary>
Motivation: 探讨在工业边缘环境中，容器和unikernel在内存约束下的性能表现差异，尤其是对AOT和JIT编译模型的影响。

Method: 使用Go和Node.js应用进行实验，分别代表AOT和JIT编译，比较两者在资源受限环境下的表现。

Result: Unikernel在Go应用中表现更优，而Node.js应用中容器在低内存时性能更稳定，揭示了内存管理的重要性。

Conclusion: 最优部署策略需考虑运行时行为和可用资源，强调了边缘计算中工作负载感知的必要性。

Abstract: The choice between containers and unikernels is a critical trade-off for edge
applications, balancing the container's ecosystem maturity against unikernel's
specialized efficiency. However, until now, how this trade-off behaves under
the severe memory constraints of industrial edge environments remains
insufficiently investigated, especially across different execution models. This
work presents an empirical comparison using Go and Node.js applications,
representing ahead-of-time (AOT) and just-in-time (JIT) compilation,
respectively. While unikernels consistently deliver faster startup times and
outperform containers for Go-based workloads in resource-constrained
environments, the evaluation results identify a critical performance crossover
for Node.js. Below a certain memory threshold, Docker containers maintain
stable performance for both I/O-bound and CPU-bound applications, while the
Nanos unikernel's performance degrades sharply. This reveals that Linux's
memory management capabilities can outweigh the minimalist efficiency of
unikernels under resource scarcity, a critical trade-off that, until now, has
not been adequately quantified for JIT runtimes in this context. These findings
demonstrate that the optimal deployment paradigm depends on both runtime
behavior and available system resources, underscoring the need for
workload-aware deployment strategies in edge computing.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [13] [SCION Path Performance Toolkit and Benchmark for Advancing Machine Learning in Next-Generation Networks: ScionPathML](https://arxiv.org/abs/2509.07154)
*Damien Rossi,Lars Herschbach,Sina Keshvadi*

Main category: cs.NI

TL;DR: 该论文通过SCIONLab测试床对SCION架构进行长期测量研究，揭示了路径稳定性、多样性和性能的动态特性，为多路径协议设计提供了重要见解。


<details>
  <summary>Details</summary>
Motivation: 现有的路径感知网络缺乏实际动态数据，导致多路径传输协议设计受限，因此需要实证研究以填补这一空白。

Method: 在SCIONLab全球测试床上对SCION架构进行纵向测量，分析路径稳定性、多样性和性能表现。

Result: 研究发现控制平面频繁变动、路径寿命短、路径不对称（路径不一致性），并发多路径传输可能提高总吞吐量但降低单路径延迟和可靠性。

Conclusion: 多路径协议（如MPQUIC）需明确考虑高变动性和路径不对称性，这对传统设计假设提出了挑战。

Abstract: Path-aware networks promise enhanced performance and resilience through
multipath transport, but a lack of empirical data on their real-world dynamics
hinders the design of effective protocols. This paper presents a longitudinal
measurement study of the SCION architecture on the global SCIONLab testbed,
characterizing the path stability, diversity, and performance crucial for
protocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic
environment, with significant control-plane churn and short path lifetimes in
parts of the testbed. We identify and characterize path discrepancy, a
phenomenon where routing policies create asymmetric path availability between
endpoints. Furthermore, we observe a performance trade-off where concurrent
multipath transmissions can improve aggregate throughput but may degrade the
latency and reliability of individual paths. These findings demonstrate that
protocols such as MPQUIC should explicitly account for high churn and path
asymmetry, challenging common assumptions in multipath protocol design.

</details>


### [14] [DORA: Dynamic O-RAN Resource Allocation for Multi-Slice 5G Networks](https://arxiv.org/abs/2509.07242)
*Alireza Ebrahimi Dorcheh,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.NI

TL;DR: DORA是一种基于深度强化学习的动态资源分配框架，用于在Open RAN中支持5G多业务类型的切片级PRB分配，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 5G需同时支持URLLC、eMBB和mMTC等多种业务类型，每种业务有不同QoS需求。在有限频谱资源下，需要动态、标准化的资源管理方法。

Method: DORA采用PPO-based强化学习代理，根据流量需求和信道条件动态分配PRB，并通过轮询调度简化控制复杂度。支持在线训练以适应流量变化。

Result: DORA在拥塞场景下优于多个基线方法，包括降低URLLC延迟、提升eMBB吞吐量且减少SLA违规，并扩大mMTC覆盖范围。

Conclusion: DORA是首个完全在线的DRL框架，实现了O-RAN中自适应的切片感知PRB分配，具有潜在应用价值。

Abstract: The fifth generation (5G) of wireless networks must simultaneously support
heterogeneous service categories, including Ultra-Reliable Low-Latency
Communications (URLLC), enhanced Mobile Broadband (eMBB), and massive
Machine-Type Communications (mMTC), each with distinct Quality of Service (QoS)
requirements. Meeting these demands under limited spectrum resources requires
adaptive and standards-compliant radio resource management. We present DORA
(Dynamic O-RAN Resource Allocation), a deep reinforcement learning (DRL)
framework for dynamic slice-level Physical Resource Block (PRB) allocation in
Open RAN. DORA employs a PPO-based RL agent to allocate PRBs across URLLC,
eMBB, and mMTC slices based on observed traffic demands and channel conditions.
Intra-slice PRB scheduling is handled deterministically via round-robin among
active UEs, simplifying control complexity and improving training stability.
Unlike prior work, DORA supports online training and adapts continuously to
evolving traffic patterns and cross-slice contention. Implemented in the
standards-compliant OpenAirInterface (OAI) RAN stack and designed for
deployment as an O-RAN xApp, DORA integrates seamlessly with RAN Intelligent
Controllers (RICs). Extensive evaluation under congested regimes shows that
DORA outperforms three non-learning baselines and a \texttt{DQN} agent,
achieving lower URLLC latency, higher eMBB throughput with fewer SLA
violations, and broader mMTC coverage without starving high-priority slices. To
our knowledge, this is the first fully online DRL framework for adaptive,
slice-aware PRB allocation in O-RAN.

</details>


### [15] [TEGRA: A Flexible & Scalable NextGen Mobile Core](https://arxiv.org/abs/2509.07410)
*Bilal Saleem,Omar Basit,Jiayi Meng,Iftekhar Alam,Ajay Thakur,Christian Maciocco,Muhammad Shahbaz,Y. Charlie Hu,Larry Peterson*

Main category: cs.NI

TL;DR: 论文探讨了5G/6G移动核心网在SBA架构下灵活性与可扩展性之间的潜在权衡，提出了一种高性能、灵活且可扩展的解决方案TEGRA。


<details>
  <summary>Details</summary>
Motivation: 当前5G/6G移动核心网采用SBA架构，但在性能优化时仍依赖传统NFV技术，可能导致灵活性与可扩展性的矛盾。

Method: 通过设计弹性SBA微服务模式和状态管理策略，提出了TEGRA解决方案，优化性能而不牺牲适应性。

Result: TEGRA显著降低了延迟，性能优于传统SBA实现（如free5GC、Open5GS），同时保持灵活性，且代码复杂度大幅降低。

Conclusion: TEGRA证明了在SBA架构中可以实现高性能、灵活性和可扩展性的统一，为未来移动核心网设计提供了新方向。

Abstract: To support emerging mobile use cases (e.g., AR/VR, autonomous driving, and
massive IoT), next-generation mobile cores for 5G and 6G are being
re-architected as service-based architectures (SBAs) running on both private
and public clouds. However, current performance optimization strategies for
scaling these cores still revert to traditional NFV-based techniques, such as
consolidating functions into rigid, monolithic deployments on dedicated
servers. This raises a critical question: Is there an inherent tradeoff between
flexibility and scalability in an SBA-based mobile core, where improving
performance (and resiliency) inevitably comes at the cost of one or the other?
  To explore this question, we introduce resilient SBA microservices design
patterns and state-management strategies, and propose TEGRA -- a
high-performance, flexible, and scalable SBA-based mobile core. By leveraging
the mobile core's unique position in the end-to-end internet ecosystem (i.e.,
at the last-mile edge), TEGRA optimizes performance without compromising
adaptability. Our evaluation demonstrates that TEGRA achieves significantly
lower latencies, processing requests 20x, 11x, and 1.75x faster than
traditional SBA core implementations -- free5GC, Open5GS, and Aether,
respectively -- all while matching the performance of state-of-the-art cores
(e.g., CoreKube) while retaining flexibility. Furthermore, it reduces the
complexity of deploying new features, requiring orders of magnitude fewer lines
of code (LoCs) compared to existing cores.

</details>


### [16] [Network-accelerated Active Messages](https://arxiv.org/abs/2509.07431)
*Md Ashfaqur Rahaman,Alireza Sanaee,Todd Thornley,Sebastiano Miano,Gianni Antichi,Brent E. Stephens,Ryan Stutsman*

Main category: cs.NI

TL;DR: NAAM利用eBPF的便携性动态选择最佳执行位置，解决了RDMA和SmartNIC的编程难题，显著提升了性能和灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决RDMA和SmartNIC在性能和灵活性方面的限制，提供动态选择执行位置的方案。

Method: 通过NAAM框架，应用指定与消息关联的eBPF函数，动态选择在客户端、SmartNIC或服务器CPU上执行逻辑。

Result: NAAM在NVIDIA BlueField-2 SmartNIC上实现了高性能动态负载转移，支持数百万操作/秒，并扩展至数百个应用负载。

Conclusion: NAAM通过eBPF的便携性和动态负载转移能力，显著提升了网络应用的灵活性和性能。

Abstract: Remote Direct Memory Access (RDMA) improves host networking performance by
eliminating software and server CPU involvement. However, RDMA has a limited
set of operations, is difficult to program, and often requires multiple round
trips to perform simple application operations. Programmable SmartNICs provide
a different means to offload work from host CPUs to a NIC. This leaves
applications with the complex choice of embedding logic as RPC handlers at
servers, using RDMA's limited interface to access server structures via
client-side logic, or running some logic on SmartNICs. The best choice varies
between workloads and over time. To solve this dilemma, we present NAAM,
network-accelerated active messages. NAAM applications specify small, portable
eBPF functions associated with messages. Each message specifies what data it
accesses using an RDMA-like interface. NAAM runs at various places in the
network, including at clients, on server-attached SmartNICs, and server host
CPU cores. Due to eBPF's portability, the code associated with a message can be
run at any location. Hence, the NAAM runtime can dynamically steer any message
to execute its associated logic wherever it makes the most sense. To
demonstrate NAAM's flexibility, we built several applications, including the
MICA hash table and lookups from a Cell-style B-tree. With an NVIDIA
BlueField-2 SmartNIC and integrating its NIC-embedded switch, NAAM can run any
of these operations on client, server, and NIC cores, shifting load in tens of
milliseconds on server compute congestion. NAAM dynamically offloads up to 1.8
million MICA ops/s for YCSB-B and 750,000 Cell lookups/s from server CPUs.
Finally, whereas iPipe, the state-of-the-art SmartNIC offload framework, only
scales to 8 application offloads on BlueField-2, NAAM scales to hundreds of
application offloads with minimal impact on tail latency due to eBPF's low
overhead.

</details>


### [17] [Constraint-Compliant Network Optimization through Large Language Models](https://arxiv.org/abs/2509.07492)
*Youngjin Song,Wookjin Lee,Hong Ki Kim,Sang Hyun Lee*

Main category: cs.NI

TL;DR: 该论文提出了一种基于LLM的优化框架，确保网络优化中的严格约束满足，避免了传统方法导致的不可行解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在约束满足方面表现不佳，导致解决方案不可行。本文旨在开发一种能严格满足约束的网络优化框架。

Method: 提出了一种基于自然语言的输入编码策略，限制解空间并保证可行性，应用于多接入边缘计算网络的任务分配优化。

Result: 数值评估表明，LLM在约束感知的网络优化中具有潜力，特别是在最小化最坏情况下延迟方面。

Conclusion: 该框架展示了LLM在网络优化中的推理能力，为约束感知的优化提供了新思路。

Abstract: This work develops an LLM-based optimization framework ensuring strict
constraint satisfaction in network optimization. While LLMs possess contextual
reasoning capabilities, existing approaches often fail to enforce constraints,
causing infeasible solutions. Unlike conventional methods that address average
constraints, the proposed framework integrates a natural language-based input
encoding strategy to restrict the solution space and guarantee feasibility. For
multi-access edge computing networks, task allocation is optimized while
minimizing worst-case latency. Numerical evaluations demonstrate LLMs as a
promising tool for constraint-aware network optimization, offering insights
into their inference capabilities.

</details>


### [18] [FlexSAN: A Flexible Regenerative Satellite Access Network Architecture](https://arxiv.org/abs/2509.07548)
*Weize Kong,Chaoqun You,Xuming Pei,YueGao*

Main category: cs.NI

TL;DR: FlexSAN提出了一种自适应卫星接入网络架构，根据实时用户需求动态配置最优的再生负载，在保证服务质量的同时降低了运营成本。


<details>
  <summary>Details</summary>
Motivation: 现有的卫星接入网络（SAN）配置静态且不灵活，容易导致资源浪费或用户体验不佳。FlexSAN旨在解决这一问题。

Method: 设计了FlexSAN架构，采用自适应贪心启发式算法动态选择最优再生负载配置，以平衡QoS和OPEX。

Result: 实验显示，FlexSAN平均提高了36.1%的用户接纳率，并减少了15%的运营成本。

Conclusion: FlexSAN通过动态配置负载优化性能和成本，显著优于静态SAN配置。

Abstract: The regenerative satellite access network (SAN) architecture deploys
next-generation NodeB (gNBs) on satellites to enable enhanced network
management capabilities. It supports two types of regenerative payload,
on-board gNB and on-board gNB-Distributed Unit (gNB-DU). Measurement results
based on our prototype implementation show that the on-board gNB offers lower
latency, while the on-board gNB-DU is more cost-effective, and there is often a
trade-off between Quality-of-Service (QoS) and operational expenditure (OPEX)
when choosing between the two payload types. However, current SAN
configurations are static and inflexible -- either deploying the full on-board
gNB or only the on-board gNB-DU. This rigidity can lead to resource waste or
poor user experiences. In this paper, we propose Flexible SAN (FlexSAN), an
adaptive satellite access network architecture that dynamically configures the
optimal regenerative payload based on real-time user demands. FlexSAN selects
the lowest OPEX payload configuration when all user demands are satisfied, and
otherwise maximizes the number of admitted users while ensuring QoS for
connected users. To address the computational complexity of dynamic payload
selection, we design an adaptive greedy heuristic algorithm. Extensive
experiments validate FlexSAN's effectiveness, showing a 36.1% average
improvement in user admission rates and a 15% OPEX reduction over static SANs.

</details>


### [19] [Quantum Computing for Large-scale Network Optimization: Opportunities and Challenges](https://arxiv.org/abs/2509.07773)
*Sebastian Macaluso,Giovanni Geraci,Elías F. Combarro,Sergi Abadal,Ioannis Arapakis,Sofia Vallecorsa,Eduard Alarcón*

Main category: cs.NI

TL;DR: 论文提出利用量子计算（QC）解决未来移动网络中大规模多目标优化问题，重点分析了图中心表示的统一策略，并探讨了量子退火和量子强化学习的方法及挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模6G及以上网络的复杂性需要创新方法来解决多目标优化问题，传统方法往往难以处理，量子计算被视为一种有前景的技术。

Method: 通过分析问题特征（尤其是图中心表示），提出统一策略，采用量子退火和量子强化学习进行优化。

Result: 提出了一种结合量子计算的优化方法，并识别了相关挑战。

Conclusion: 量子计算在未来网络优化中具有潜力，但仍需克服算法和硬件方面的挑战。

Abstract: The complexity of large-scale 6G-and-beyond networks demands innovative
approaches for multi-objective optimization over vast search spaces, a task
often intractable. Quantum computing (QC) emerges as a promising technology for
efficient large-scale optimization. We present our vision of leveraging QC to
tackle key classes of problems in future mobile networks. By analyzing and
identifying common features, particularly their graph-centric representation,
we propose a unified strategy involving QC algorithms. Specifically, we outline
a methodology for optimization using quantum annealing as well as quantum
reinforcement learning. Additionally, we discuss the main challenges that QC
algorithms and hardware must overcome to effectively optimize future networks.

</details>


### [20] [Making congestion control robust to per-packet load balancing in datacenters](https://arxiv.org/abs/2509.07907)
*Barak Gerstein,Mark Silberstein,Isaac Keslassy*

Main category: cs.NI

TL;DR: 本文研究了数据中心网络中逐包负载均衡与现有拥塞控制算法（CCAs）结合时性能下降的问题，并提出了一种新的方法MSwift来改进性能。


<details>
  <summary>Details</summary>
Motivation: 由于逐包负载均衡与现有CCAs的结合可能导致性能下降甚至崩溃，且现有CCAs对重复ACK的处理不足，因此需要一种更鲁棒的解决方案。

Method: 作者首先建模了CCAs在多路径拥塞时的吞吐崩溃现象，并提出使用中位数反馈替代传统方法，设计了一种名为MSwift的新算法，以提升多路径路由下的性能。

Result: 实验表明，MSwift在随机包喷洒和自适应路由场景下，能将第99百分位的流完成时间（FCT）提升高达25%。

Conclusion: MSwift通过中位数反馈机制，有效提升了多路径路由下的性能，同时保留了单路径路由和高度负载的耐受性。

Abstract: Per-packet load-balancing approaches are increasingly deployed in datacenter
networks. However, their combination with existing congestion control
algorithms (CCAs) may lead to poor performance, and even state-of-the-art CCAs
can collapse due to duplicate ACKs. A typical approach to handle this collapse
is to make CCAs resilient to duplicate ACKs.
  In this paper, we first model the throughput collapse of a wide array of CCAs
when some of the paths are congested. We show that addressing duplicate ACKs is
insufficient. Instead, we explain that since CCAs are typically designed for
single-path routing, their estimation function focuses on the latest feedback
and mishandles feedback that reflects multiple paths. We propose to use a
median feedback that is more robust to the varying signals that come with
multiple paths. We introduce MSwift, which applies this principle to make
Google's Swift robust to multi-path routing while keeping its incast tolerance
and single-path performance. Finally, we demonstrate that MSwift improves the
99th-percentile FCT by up to 25\%, both with random packet spraying and
adaptive routing.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [21] [Contradictions](https://arxiv.org/abs/2509.07026)
*Yang Xu,Shuwei Chen,Xiaomei Zhong,Jun Liu,Xingxing He*

Main category: cs.LO

TL;DR: 本文研究了标准矛盾的系统化构建方法，特别是最大三角标准矛盾和三角型标准矛盾的构造方法，提出了通过最大标准矛盾判断子句集可满足性与不可满足性的程序，并推导了计算标准子矛盾数量的公式，推动了基于矛盾分离的动态多子句自动推理的发展。


<details>
  <summary>Details</summary>
Motivation: 克服经典二元归结的限制，提升自动定理证明的表达和推理能力，使AI系统更加透明和可靠。

Method: 研究两种标准矛盾（最大三角标准矛盾和三角型标准矛盾）的构造方法，并提出基于最大标准矛盾的子句集可满足性判定程序。

Result: 推导了计算标准子矛盾数量的公式，为动态多子句自动推理提供了方法论基础。

Conclusion: 该方法扩展了自动推理系统的表达和推理能力，超越了经典二元范式。

Abstract: Trustworthy AI requires reasoning systems that are not only powerful but also
transparent and reliable. Automated Theorem Proving (ATP) is central to formal
reasoning, yet classical binary resolution remains limited, as each step
involves only two clauses and eliminates at most two literals. To overcome this
bottleneck, the concept of standard contradiction and the theory of
contradiction-separation-based deduction were introduced in 2018. This paper
advances that framework by focusing on the systematic construction of standard
contradictions. Specially, this study investigates construction methods for two
principal forms of standard contradiction: the maximum triangular standard
contradiction and the triangular-type standard contradiction. Building on these
structures, we propose a procedure for determining the satisfiability and
unsatisfiability of clause sets via maximum standard contradiction.
Furthermore, we derive formulas for computing the number of standard
sub-contradictions embedded within both the maximum triangular standard
contradiction and the triangular-type standard contradiction. The results
presented herein furnish the methodological basis for advancing
contradiction-separation-based dynamic multi-clause automated deduction,
thereby extending the expressive and deductive capabilities of automated
reasoning systems beyond the classical binary paradigm.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [22] [Short-Term Gaze Prediction: Analysis of Individual Differences, Typical and Extreme-Case Errors](https://arxiv.org/abs/2509.07126)
*Kateryna Melnyk,Lee Friedman,Oleg Komogortsev*

Main category: cs.HC

TL;DR: 研究了三种模型（LSTM、TF和ClPr）在短时凝视预测中的表现，LSTM在凝视和扫视中表现较好，而TF和ClPr在后扫视时期更精确。分析了极端情况和个体差异，建议未来研究需单独分析极端案例。


<details>
  <summary>Details</summary>
Motivation: 研究不同模型在短时凝视预测中的性能差异，并提供模型选择的实用建议。

Method: 使用三层LSTM网络、简单的transformer编码器模型（TF）和分类预测网络（ClPr），评估它们在凝视、扫视及后扫视时期的表现。

Result: LSTM在凝视和扫视中表现更好，TF和ClPr在后扫视时期更精确。极端情况下最佳模型因眼动类型而异。

Conclusion: 需单独分析极端案例，并提供模型选择的实用建议。

Abstract: Gaze prediction is a diverse field of study with multiple research focuses
and practical applications. This article investigates how recurrent neural
networks and transformers perform short-term gaze prediction. We used three
models: a three-layer long-short-term memory (LSTM) network, a simple
transformer-encoder model (TF), and a classification-predictor network (ClPr),
which simultaneously classifies the signal into eye movement events and
predicts the positions of gaze. The performance of the models was evaluated for
ocular fixations and saccades of various amplitudes and as a function of
individual differences in both typical and extreme cases. On average, LSTM
performed better on fixations and saccades, whereas TF and ClPr demonstrated
more precise results for post-saccadic periods. In extreme cases, the
best-performing models vary depending on the type of eye movement. We reviewed
the difference between the median $P_{50}$ and high-percentile $P_{95}$ error
profiles across subjects. The subjects for which the models perform the best
overall do not necessarily exhibit the lowest $P_{95}$ values, which supports
the idea of analyzing extreme cases separately in future work. We explore the
trade-offs between the proposed solutions and provide practical insights into
model selection for gaze prediction.

</details>


### [23] [Wellbeing-Centered UX: Supporting Content Moderators](https://arxiv.org/abs/2509.07187)
*Diana Mihalache,Dalila Szostak*

Main category: cs.HC

TL;DR: 本章探讨了用户体验（UX）与内容审核员福祉的交叉点，强调了在设计过程中融入福祉考量的重要性，并提供了实用策略。


<details>
  <summary>Details</summary>
Motivation: 内容审核员面临敏感内容、单调任务和复杂决策等挑战，现有工具不足导致其福祉问题突出。

Method: 本章提出了一个框架，涵盖研究、写作和设计等关键UX领域，并提供实际实施策略。

Result: 通过福祉考量，可以创建支持内容审核员的用户体验，对用户和企业都有益。

Conclusion: 在UX设计中融入福祉考量是提升内容审核员体验的关键，同时也为业务带来长期价值。

Abstract: This chapter focuses on the intersection of user experience (UX) and
wellbeing in the context of content moderation. Human content moderators play a
key role in protecting end users from harm by detecting, evaluating, and
addressing content that may violate laws or product policies. They face
numerous challenges, including exposure to sensitive content, monotonous tasks,
and complex decisions, which are often exacerbated by inadequate tools. This
chapter explains the importance of incorporating wellbeing considerations
throughout the product development lifecycle, offering a framework and
practical strategies for implementation across key UX disciplines: research,
writing, and design. By examining these considerations, this chapter provides a
roadmap for creating user experiences that support content moderators,
benefiting both the user and the business.

</details>


### [24] [Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data](https://arxiv.org/abs/2509.07202)
*Khushiyant*

Main category: cs.HC

TL;DR: 本文提出了一种结合Gemma 2B LLM和分类器-LLM架构的新方法，用于EEG文本生成，显著降低了数据需求和计算成本，性能接近前沿方法。


<details>
  <summary>Details</summary>
Motivation: 解决EEG文本生成的高数据需求和计算成本问题，结合LLMs提升EEG解码能力。

Method: 使用Gemma 2B LLM与分类器-LLM架构，结合RNN编码器，降低数据需求和计算成本。

Result: 性能接近前沿方法，整体性能提升10%，数据需求显著降低。

Conclusion: 该方法展示了EEG文本生成的高效迁移学习潜力，推动了脑机接口的研究和应用。

Abstract: Text generating capabilities have undergone a substantial transformation with
the introduction of large language models (LLMs). Electroencephalography
(EEG)-based text production is still difficult, though, because it requires a
lot of data and processing power. This paper introduces a new method that
combines the use of the Gemma 2B LLM with a classifier-LLM architecture to
incorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically
lowers the amount of data and compute power needed while achieving performance
close to that of cutting-edge methods. Notably, compared to current
methodologies, our methodology delivers an overall performance improvement of
10%. The suggested architecture demonstrates the possibility of effective
transfer learning for EEG-based text production, remaining strong and
functional even in the face of data limits. This work highlights the potential
of integrating LLMs with EEG decoding to improve assistive technologies and
improve independence and communication for those with severe motor limitations.
Our method pushes the limits of present capabilities and opens new paths for
research and application in brain-computer interfaces by efficiently using the
strengths of pre-trained language models. This makes EEG-based text production
more accessible and efficient.

</details>


### [25] [In the Queue: Understanding How Reddit Moderators Use the Modqueue](https://arxiv.org/abs/2509.07314)
*Tanvi Bajpai,Eshwar Chandrasekharan*

Main category: cs.HC

TL;DR: 论文研究了Reddit的modqueue（审核队列）在实践中的使用情况，发现其使用方式多样且存在不足，提出了改进设计的机会。


<details>
  <summary>Details</summary>
Motivation: Reddit的modqueue是社区审核的核心工具，但缺乏对其实际使用情况的了解，因此作者通过调查填补了这一空白。

Method: 对110名负责400多个子论坛的审核员进行了调查，了解他们对modqueue的使用方式。

Result: modqueue的使用方式多样，但普遍存在协调性差、界面信号不一致等问题，且不足以单独支持审核决策。

Conclusion: modqueue并“非万能”，未来设计需更模块化、集成化和可定制化，以更好地支持多样化的审核工作流。

Abstract: On Reddit, the moderation queue (modqueue) is a primary interface for
moderators to review reported content. Despite its central role in Reddit's
community-reliant moderation model, little is known about how moderators
actually use it in practice. To address this gap, we surveyed 110 moderators,
who collectively oversee more than 400 unique subreddits, and asked them about
their usage of the modqueue. Modqueue practices vary widely: some moderators
approach it as a daily checklist, others as a hub to infer community-wide
patterns, and many still find the queue insufficient to inform their moderation
decisions. We also identify persistent challenges around review coordination,
inconsistent interface signals, and reliance on third-party tools. Taken
together, we show the modqueue is neither a one-size-fits-all solution nor
sufficient on its own for supporting moderator review. Our work highlights
design opportunities for more modular, integrated, and customizable platform
infrastructures that better support the diversity of moderator workflows.

</details>


### [26] [SpecifyUI: Supporting Iterative UI Design Intent Expression through Structured Specifications and Generative AI](https://arxiv.org/abs/2509.07334)
*Yunnong Chen,Chengwei Shi,Liuqing Chen*

Main category: cs.HC

TL;DR: SPEC是一种结构化、参数化的中间表示，用于辅助UI设计。结合SpecifyUI系统，通过多智能体生成器实现设计的高保真渲染，显著提升了意图对齐和设计质量。


<details>
  <summary>Details</summary>
Motivation: 当前工具在外部化设计师意图和控制迭代变化方面存在困难。

Method: 引入SPEC中间表示，开发SpecifyUI系统，整合区域分割和视觉语言模型，支持多源UI组合和精确编辑。

Result: SPEC生成更忠实于设计意图，用户研究中SpecifyUI在多方面优于基线Stitch。

Conclusion: SPEC为LLM辅助设计提供了一种规范驱动的新范式，支持迭代协作流程。

Abstract: Large language models (LLMs) promise to accelerate UI design, yet current
tools struggle with two fundamentals: externalizing designers' intent and
controlling iterative change. We introduce SPEC, a structured, parameterized,
hierarchical intermediate representation that exposes UI elements as
controllable parameters. Building on SPEC, we present SpecifyUI, an interactive
system that extracts SPEC from UI references via region segmentation and
vision-language models, composes UIs across multiple sources, and supports
targeted edits at global, regional, and component levels. A multi-agent
generator renders SPEC into high-fidelity designs, closing the loop between
intent expression and controllable generation. Quantitative experiments show
SPEC-based generation more faithfully captures reference intent than
prompt-based baselines. In a user study with 16 professional designers,
SpecifyUI significantly outperformed Stitch on intent alignment, design
quality, controllability, and overall experience in human-AI co-creation. Our
results position SPEC as a specification-driven paradigm that shifts
LLM-assisted design from one-shot prompting to iterative, collaborative
workflows.

</details>


### [27] [Feed-O-Meter: Fostering Design Feedback Skills through Role-playing Interactions with AI Mentee](https://arxiv.org/abs/2509.07424)
*Hyunseung Lim,Dasom Choi,DaEun Choi,Sooyohn Nam,Hwajung Hong*

Main category: cs.HC

TL;DR: Feed-O-Meter系统利用LLM代理帮助学生练习设计反馈，克服了学生在提供反馈时的心理障碍，提升了参与动机和反馈的可理解性。


<details>
  <summary>Details</summary>
Motivation: 解决学生在设计课程中因缺乏信心和害怕被评价而无法有效提供反馈的问题，并探索AI代理在培养反馈技能中的潜力。

Method: 设计Feed-O-Meter系统，利用精心设计的LLM代理模拟角色扮演，学生作为导师向AI学员提供反馈，并观察反馈对AI学员的影响。

Result: 用户研究表明，系统通过角色切换提高了参与者的参与度和动机，并帮助他们调整反馈使其更易理解。

Conclusion: Feed-O-Meter展示了AI代理在设计教育中的潜力，未来需进一步优化系统以更好地培养反馈技能。

Abstract: Effective feedback, including critique and evaluation, helps designers
develop design concepts and refine their ideas, supporting informed
decision-making throughout the iterative design process. However, in
studio-based design courses, students often struggle to provide feedback due to
a lack of confidence and fear of being judged, which limits their ability to
develop essential feedback-giving skills. Recent advances in large language
models (LLMs) suggest that role-playing with AI agents can let learners engage
in multi-turn feedback without the anxiety of external judgment or the time
constraints of real-world settings. Yet prior studies have raised concerns that
LLMs struggle to behave like real people in role-play scenarios, diminishing
the educational benefits of these interactions. Therefore, designing AI-based
agents that effectively support learners in practicing and developing
intellectual reasoning skills requires more than merely assigning the target
persona's personality and role to the agent. By addressing these issues, we
present Feed-O-Meter, a novel system that employs carefully designed LLM-based
agents to create an environment in which students can practice giving design
feedback. The system enables users to role-play as mentors, providing feedback
to an AI mentee and allowing them to reflect on how that feedback impacts the
AI mentee's idea development process. A user study (N=24) indicated that
Feed-O-Meter increased participants' engagement and motivation through
role-switching and helped them adjust feedback to be more comprehensible for an
AI mentee. Based on these findings, we discuss future directions for designing
systems to foster feedback skills in design education.

</details>


### [28] [dciWebMapper2: Enhancing the dciWebMapper framework toward integrated, interactive visualization of linked multi-type maps, charts, and spatial statistics and analysis](https://arxiv.org/abs/2509.07897)
*Sarigai Sarigai,Liping Yang,Katie Slack,Carolyn Fish,Michaela Buenemann,Qiusheng Wu,Yan Lin,Joseph A. Cook,David Jacobs*

Main category: cs.HC

TL;DR: dciWebMapper2是一个开源框架的扩展，支持多属性空间分析和交互式地理可视化，适用于气候正义、食品获取和社会脆弱性等领域。


<details>
  <summary>Details</summary>
Motivation: 随着交互式地理可视化的重要性增加，需要开源框架支持动态分析和易用设计。

Method: 该框架整合了多种地图类型和统计图表，支持高维比较和可视化清晰度，基于制图和信息可视化原则。

Result: dciWebMapper2展示了其适应性和潜力，支持包容性空间叙事和透明地理空间分析。

Conclusion: 该框架为研究、教育和公民参与提供了多功能基础。

Abstract: As interactive web-based geovisualization becomes increasingly vital across
disciplines, there is a growing need for open-source frameworks that support
dynamic, multi-attribute spatial analysis and accessible design. This paper
introduces dciWebMapper2, a significant expansion of the original dciWebMapper
framework, designed to enable exploratory analysis across domains such as
climate justice, food access, and social vulnerability. The enhanced framework
integrates multiple map types, including choropleth, proportional symbol, small
multiples, and heatmaps, with linked statistical charts (e.g., scatter plots,
boxplots) and time sliders, all within a coordinated-view environment.
Dropdown-based controls allow flexible, high-dimensional comparisons while
maintaining visual clarity. Grounded in cartographic and information
visualization principles, dciWebMapper2 is fully open-source, self-contained,
and server-free, supporting modularity, reproducibility, and long-term
sustainability. Three applied use cases demonstrate its adaptability and
potential to democratize interactive web cartography. This work offers a
versatile foundation for inclusive spatial storytelling and transparent
geospatial analysis in research, education, and civic engagement.

</details>


### [29] [Social Media Clones: Exploring the Impact of Social Delegation with AI Clones through a Design Workbook Study](https://arxiv.org/abs/2509.07502)
*Jackie Liu,Mehrnoosh Sadat Shirvani,Hwajung Hong,Ig-Jae Kim,Dongwook Yoon*

Main category: cs.HC

TL;DR: 社交媒体的AI克隆技术可以提升用户体验，但也可能带来身份真实性和社交信任的风险。研究通过设计工作簿和访谈发现，用户倾向于模仿克隆行为以减少差异。


<details>
  <summary>Details</summary>
Motivation: 探讨AI克隆技术如何影响用户的在线行为和互动，为负责任的技术整合奠定基础。

Method: 通过设计工作簿向32名社交媒体用户展示8种克隆概念，并进行半结构化访谈。

Result: 克隆技术带来便利和舒适，但也威胁用户真实性和引发社区怀疑，用户倾向于模仿克隆行为。

Conclusion: 研究强调了AI克隆技术推广中的身份和印象管理挑战，并提出了设计建议以促进其成功整合。

Abstract: Social media clones are AI-powered social delegates of ourselves created
using our personal data. As our identities and online personas intertwine,
these technologies have the potential to greatly enhance our social media
experience. If mismanaged, however, these clones may also pose new risks to our
social reputation and online relationships. To set the foundation for a
productive and responsible integration, we set out to understand how social
media clones will impact our online behavior and interactions. We conducted a
series of semi-structured interviews introducing eight speculative clone
concepts to 32 social media users through a design workbook. Applying existing
work in AI-mediated communication in the context of social media, we found that
although clones can offer convenience and comfort, they can also threaten the
user's authenticity and increase skepticism within the online community. As a
result, users tend to behave more like their clones to mitigate discrepancies
and interaction breakdowns. These findings are discussed through the lens of
past literature in identity and impression management to highlight challenges
in the adoption of social media clones by the general public, and propose
design considerations for their successful integration into social media
platforms.

</details>


### [30] [Digital Twins for Extended Reality Tourism: User Experience Evaluation Across User Groups](https://arxiv.org/abs/2509.07740)
*Maximilian Warsinke,Francesco Vona,Tanja Kojić,Jan-Niklas Voigt-Antons,Sebastian Möller*

Main category: cs.HC

TL;DR: 研究评估了扩展现实（XR）旅游中两种数字孪生应用的用户体验（UX），发现AR和VR均提供低任务负荷和高乐趣，但VR存在易用性和晕动症问题，AR则用户体验更高。


<details>
  <summary>Details</summary>
Motivation: 探讨数字孪生技术在XR旅游中的应用效果，重点关注用户体验及相关因素。

Method: 采用定量探索方法，84名参与者通过标准化问卷评估UX、任务负荷、存在感、晕动症和情感反应。

Result: AR-VT获得高UX评分，VR-VT增强存在感但存在挑战；年龄、XR经验和技术亲和力与评测指标显著相关。

Conclusion: 强调为XR新手量身定制体验的重要性，突显UX在XR旅游中的关键作用。

Abstract: This study evaluates the user experience (UX) in extended reality (XR)
tourism of two digital twin-based applications: an Augmented Reality Virtual
Tour (AR-VT) for enhanced on-site visits and a Virtual Reality Virtual Tour
(VR-VT) for remote exploration. Using a quantitative exploratory approach, 84
participants from Spain and Germany, divided into three sample groups, assessed
UX, task load, presence, cybersickness, and emotional response through
standardized questionnaires. Findings indicate that both applications provided
a low task load and high enjoyment. The VR-based tour enhanced presence but
posed usability and cybersickness challenges, while the AR-based tour achieved
high UX ratings, with qualitative feedback suggesting areas for refinement.
Correlation analysis revealed significant relationships between age, prior XR
experience, and technological affinity with the measured metrics for both
applications. These results highlight the importance of well-designed
experiences tailored to XR novices, reinforcing the critical role of UX in
digital twin-based XR tourism.

</details>


### [31] [Enhancing Online Learning by Integrating Biosensors and Multimodal Learning Analytics for Detecting and Predicting Student Behavior: A Review](https://arxiv.org/abs/2509.07742)
*Alvaro Becerra,Ruth Cobos,Charles Lang*

Main category: cs.HC

TL;DR: 该论文探讨了生物传感器与多模态学习分析的结合，用于预测和分析学生在计算机学习中的行为，强调其在个性化学习中的潜力。


<details>
  <summary>Details</summary>
Motivation: 提升在线学习中的学生参与度和教育成果，通过生物传感器和多模态数据分析学生行为。

Method: 系统综述了54项关键研究，采用了先进的机器学习算法和多模态数据预处理技术。

Result: 多模态数据的整合可以实现个性化学习体验、实时反馈和智能教育干预。

Conclusion: 生物传感器驱动的自适应学习系统具有变革潜力，能推动更定制化的在线学习体验。

Abstract: In modern online learning, understanding and predicting student behavior is
crucial for enhancing engagement and optimizing educational outcomes. This
systematic review explores the integration of biosensors and Multimodal
Learning Analytics (MmLA) to analyze and predict student behavior during
computer-based learning sessions. We examine key challenges, including emotion
and attention detection, behavioral analysis, experimental design, and
demographic considerations in data collection. Our study highlights the growing
role of physiological signals, such as heart rate, brain activity, and
eye-tracking, combined with traditional interaction data and self-reports to
gain deeper insights into cognitive states and engagement levels. We synthesize
findings from 54 key studies, analyzing commonly used methodologies such as
advanced machine learning algorithms and multimodal data pre-processing
techniques. The review identifies current research trends, limitations, and
emerging directions in the field, emphasizing the transformative potential of
biosensor-driven adaptive learning systems. Our findings suggest that
integrating multimodal data can facilitate personalized learning experiences,
real-time feedback, and intelligent educational interventions, ultimately
advancing toward a more customized and adaptive online learning experience.

</details>


### [32] [LLMs in Wikipedia: Investigating How LLMs Impact Participation in Knowledge Communities](https://arxiv.org/abs/2509.07819)
*Moyan Zhou,Soobin Cho,Loren Terveen*

Main category: cs.HC

TL;DR: 研究探讨了大型语言模型（LLM）如何在Wikipedia编辑社区中影响编辑行为，以及编辑如何调整LLM输出以符合社区规范。通过访谈发现，LLM对不同经验水平的编辑影响不同，且新编辑在适应社区规范时面临挑战。研究还提出了支持社区参与的LLM设计建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在知识生产中的应用增多，了解其对社区参与的影响和如何规范使用成为了重要课题。研究旨在填补这一空白，帮助塑造未来的使用规范。

Method: 通过采访16位使用LLM进行编辑的Wikipedia编辑，分析LLM如何影响编辑行为、调整策略及社区反应。

Result: 发现LLM对不同经验水平的编辑影响差异显著，新编辑在适应社区规范时更易遭遇挑战，社区对LLM辅助编辑的反应也因编辑经验而异。

Conclusion: 研究挑战了现有的新手参与模型，并提出LLM设计应侧重于支持社区参与，如通过脚手架、教学和情境感知等方式。

Abstract: Large language models (LLMs) are reshaping knowledge production as community
members increasingly incorporate them into their contribution workflows.
However, participating in knowledge communities involves more than just
contributing content - it is also a deeply social process. While communities
must carefully consider appropriate and responsible LLM integration, the
absence of concrete norms has left individual editors to experiment and
navigate LLM use on their own. Understanding how LLMs influence community
participation is therefore critical in shaping future norms and supporting
effective adoption. To address this gap, we investigated Wikipedia, one of the
largest knowledge production communities, to understand 1) how LLMs influence
the ways editors contribute content, 2) what strategies editors leverage to
align LLM outputs with community norms, and 3) how other editors in the
community respond to LLM-assisted contributions. Through interviews with 16
Wikipedia editors who had used LLMs for their edits, we found that 1) LLMs
affected the content contributions for experienced and new editors differently;
2) aligning LLM outputs with community norms required tacit knowledge that
often challenged newcomers; and 3) as a result, other editors responded to
LLM-assisted edits differently depending on the editors' expertise level. Based
on these findings, we challenge existing models of newcomer involvement and
propose design implications for LLMs that support community engagement through
scaffolding, teaching, and context awareness.

</details>


### [33] [NeuroGaze: A Hybrid EEG and Eye-Tracking Brain-Computer Interface for Hands-Free Interaction in Virtual Reality](https://arxiv.org/abs/2509.07863)
*Kyle Coutray,Wanyea Barbel,Zack Groth,Joseph J LaViola Jr*

Main category: cs.HC

TL;DR: NeuroGaze是一种结合脑电图（EEG）和眼动追踪的混合接口，用于VR中无手交互。实验表明其准确性优于其他方法但速度较慢，展示了消费级BCI在日常VR中的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索消费级BCI设备在日常活动中的应用，尤其是在VR中解决输入方法在速度、准确性和体力消耗之间的权衡问题。

Method: 引入NeuroGaze混合接口，结合EEG和眼动追踪技术，通过360度立方体选择任务比较了VR控制器、凝视+捏合手势和NeuroGaze的性能。

Result: NeuroGaze在准确性上优于其他方法，但任务完成时间更长；物理需求减少，但用户偏好和总体评分混合。

Conclusion: NeuroGaze展示了消费级BCI在VR中的可行性，具有人体工学和包容性潜力，但需改进EEG信号处理和自适应多模态集成以提升性能。

Abstract: Brain-Computer Interfaces (BCIs) have traditionally been studied in clinical
and laboratory contexts, but the rise of consumer-grade devices now allows
exploration of their use in daily activities. Virtual reality (VR) provides a
particularly relevant domain, where existing input methods often force
trade-offs between speed, accuracy, and physical effort. This study introduces
NeuroGaze, a hybrid interface combining electroencephalography (EEG) with eye
tracking to enable hands-free interaction in immersive VR. Twenty participants
completed a 360{\deg} cube-selection task using three different input methods:
VR controllers, gaze combined with a pinch gesture, and NeuroGaze. Performance
was measured by task completion time and error rate, while workload was
evaluated using the NASA Task Load Index (NASA-TLX). NeuroGaze successfully
supported target selection with off-the-shelf hardware, producing fewer errors
than the alternative methods but requiring longer completion times, reflecting
a classic speed-accuracy tradeoff. Workload analysis indicated reduced physical
demand for NeuroGaze compared to controllers, though overall ratings and user
preferences were mixed. These findings demonstrate the feasibility of hybrid
EEG+gaze systems for everyday VR use, highlighting their ergonomic benefits and
inclusivity potential. Although not yet competitive in speed, NeuroGaze points
toward a practical role for consumer-grade BCIs in accessibility and
long-duration applications, and underscores the need for improved EEG signal
processing and adaptive multimodal integration to enhance future performance.

</details>


### [34] [An Enactivist Approach to Human-Computer Interaction: Bridging the Gap Between Human Agency and Affordances](https://arxiv.org/abs/2509.07871)
*Angjelin Hila*

Main category: cs.HC

TL;DR: 提出了一种基于认知理论的「自我感受」（FoA）新框架，替代传统的「自我意识」（SoA），为XR、BCI和HAX设计提供新思路。


<details>
  <summary>Details</summary>
Motivation: 新兴的XR、AI和BCI领域需要新的理论框架来理解人类的自主性和能动性。

Method: 结合认知理论的动态系统模型，提出FoA概念（包括情感投入和意志注意），并设计神经动态指标和现象学报告进行测量。

Result: FoA框架提供了更丰富的设计洞察，适用于XR、BCI和生成式AI等交互环境。

Conclusion: 该框架旨在增强人类在快速发展的交互领域中的能动性。

Abstract: Emerging paradigms in XR, AI, and BCI contexts necessitate novel theoretical
frameworks for understanding human autonomy and agency in HCI. Drawing from
enactivist theories of cognition, we conceptualize human agents as
self-organizing, operationally closed systems that actively enact their
cognitive domains through dynamic interaction with their environments. To
develop measurable variables aligned with this framework, we introduce
"feelings of agency" (FoA) as an alternative to the established construct of
"sense of agency" (SoA), refining Synofzyk's multifactorial weighting model and
offering a novel conceptual pathway for overcoming gaps in the dominant
comparator model. We define FoA as comprising two subconstructs: affective
engagement and volitional attention, which we operationalize through integrated
neurodynamic indicators (valence, arousal, cross frequency coupling within the
dorsal attention system) and first-person phenomenological reports. We argue
that these neurophenomenological indicators provide richer, more actionable
insights for digital affordance design, particularly in XR, BCI, Human AI
Interaction (HAX), and generative AI environments. Our framework aims to inform
and inspire design parameters that significantly enhance human agency in
rapidly evolving interactive domains.

</details>


### [35] [A Robot That Listens: Enhancing Self-Disclosure and Engagement Through Sentiment-based Backchannels and Active Listening](https://arxiv.org/abs/2509.07873)
*Hieu Tran,Go-Eum Cha,Sooyeon Jeong*

Main category: cs.HC

TL;DR: 研究开发了一种基于LLM的社交机器人，具备情感回馈和积极倾听功能，实验证明它能更有效地促进用户自我披露并提升互动体验。


<details>
  <summary>Details</summary>
Motivation: 提升社交机器人的沟通能力，使其能与人类建立更深层次的情感联系和信任。

Method: 开发了具备情感回馈和积极倾听行为的机器人，并与仅回馈和无倾听行为的机器人对比实验。

Result: 积极倾听机器人显著提升了用户自我披露和互动满意度。

Conclusion: 积极倾听行为能改善人机沟通，促进更深层次的关系建立。

Abstract: As social robots get more deeply integrated intoour everyday lives, they will
be expected to engage in meaningful conversations and exhibit socio-emotionally
intelligent listening behaviors when interacting with people. Active listening
and backchanneling could be one way to enhance robots' communicative
capabilities and enhance their effectiveness in eliciting deeper
self-disclosure, providing a sense of empathy,and forming positive rapport and
relationships with people.Thus, we developed an LLM-powered social robot that
can exhibit contextually appropriate sentiment-based backchannelingand active
listening behaviors (active listening+backchanneling) and compared its efficacy
in eliciting people's self-disclosurein comparison to robots that do not
exhibit any of these listening behaviors (control) and a robot that only
exhibitsbackchanneling behavior (backchanneling-only). Through ourexperimental
study with sixty-five participants, we found theparticipants who conversed with
the active listening robot per-ceived the interactions more positively, in
which they exhibited the highest self-disclosures, and reported the strongest
senseof being listened to. The results of our study suggest that the
implementation of active listening behaviors in social robotshas the potential
to improve human-robot communication andcould further contribute to the
building of deeper human-robot relationships and rapport.

</details>


### [36] [Knowledge Isn't Power: The Ethics of Social Robots and the Difficulty of Informed Consent](https://arxiv.org/abs/2509.07942)
*James M. Berzuk,Lauren Corcoran,Brannen McKenzie-Lefurgey,Katie Szilagyi,James E. Young*

Main category: cs.HC

TL;DR: 论文探讨了社交机器人通过模仿人类行为触发用户情感反应的伦理问题，特别是知情同意的法律和道德挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索社交机器人如何通过人类化行为影响用户，以及这些互动中用户是否能真正知情同意。

Method: 方法包括法律原则和伦理框架的应用，分析人机互动与传统互动的差异。

Result: 结果表明，现有知情同意框架需要重新思考，以适用于社交机器人带来的新型互动。

Conclusion: 结论提出了设计目标，以实现更道德和知情的人机互动。

Abstract: Contemporary robots are increasingly mimicking human social behaviours to
facilitate interaction, such as smiling to signal approachability, or
hesitating before taking an action to allow people time to react. Such
techniques can activate a person's entrenched social instincts, triggering
emotional responses as though they are interacting with a fellow human, and can
prompt them to treat a robot as if it truly possesses the underlying life-like
processes it outwardly presents, raising significant ethical questions. We
engage these issues through the lens of informed consent: drawing upon
prevailing legal principles and ethics, we examine how social robots can
influence user behaviour in novel ways, and whether under those circumstances
users can be appropriately informed to consent to these heightened
interactions. We explore the complex circumstances of human-robot interaction
and highlight how it differs from more familiar interaction contexts, and we
apply legal principles relating to informed consent to social robots in order
to reconceptualize the current ethical debates surrounding the field. From this
investigation, we synthesize design goals for robot developers to achieve more
ethical and informed human-robot interaction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [37] [On design, analysis, and hybrid manufacturing of microstructured blade-like geometries](https://arxiv.org/abs/2509.07044)
*Pablo Antolin,Michael Barton,Georges-Pierre Bonneau,Annalisa Buffa,Amaia Calleja-Ochoa,Gershon Elber,Stefanie Elgeti,Gaizka Gómez Escudero,Alicia Gonzalez,Haizea González Barrio,Stefanie Hahmann,Thibaut Hirschler,Q Youn Honga,Konstantin Key,Myung-Soo Kim,Michael Kofler,Norberto Lopez de Lacalle,Silvia de la Maza,Kanika Rajain,Jacques Zwar*

Main category: cs.GR

TL;DR: 论文探讨了利用多材料3D打印技术设计新型非均质、多孔、轻量化且低成本的CAD对象，提出了涵盖设计、优化、制造和检测的统一制造流程，并以工业测试案例证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 挑战传统CAD几何设计的五十年范式，探索非均质内部微结构的自由形状CAD对象，以实现更轻、更便宜且功能相同的制造方案。

Method: 提出包含设计、优化、制造和检测的统一制造流程，应用于多材料3D打印技术。

Result: 通过工业测试案例（如叶片）证明，非均质微结构设计在保持功能的同时显著减少材料使用。

Conclusion: 研究展示了新型CAD设计和制造流程的潜力，为非均质微结构对象的实际工业应用提供了可行性。

Abstract: With the evolution of new manufacturing technologies such as multi-material
3D printing, one can think of new type of objects that consist of considerably
less, yet heterogeneous, material, consequently being porous, lighter and
cheaper, while having the very same functionality as the original object when
manufactured from one single solid material. We aim at questioning five decades
of traditional paradigms in geometric CAD and focus at new generation of CAD
objects that are not solid, but contain heterogeneous free-form internal
microstructures. We propose a unified manufacturing pipeline that involves all
stages, namely design, optimization, manufacturing, and inspection of
microstructured free-form geometries. We demonstrate our pipeline on an
industrial test case of a blisk blade that sustains the desired pressure
limits, yet requires significantly less material when compared to the solid
counterpart.

</details>


### [38] [SVGauge: Towards Human-Aligned Evaluation for SVG Generation](https://arxiv.org/abs/2509.07127)
*Leonardo Zini,Elia Frigieri,Sebastiano Aloscari,Marcello Generali,Lorenzo Dodi,Robert Dosen,Lorenzo Baraldi*

Main category: cs.GR

TL;DR: SVGauge是一种针对文本生成SVG图像的评估指标，结合视觉保真度和语义一致性，优于现有指标，并与人类评价高度相关。


<details>
  <summary>Details</summary>
Motivation: 现有指标（如FID、LPIPS或CLIPScore）无法满足SVG图像的符号和矢量特性需求，需要一种新的评估标准。

Method: SVGauge通过SigLIP图像嵌入（经PCA和白化处理）评估视觉保真度，并通过BLIP-2生成的描述与原始提示在SBERT和TF-IDF空间中的比较评估语义一致性。

Result: SVGauge在SHE基准测试中与人类评价相关性最高，并能更准确地重现八种零样本LLM生成器的系统级排名。

Conclusion: SVGauge是评估文本生成SVG模型的实用工具，强调了矢量专有评估的必要性。

Abstract: Generated Scalable Vector Graphics (SVG) images demand evaluation criteria
tuned to their symbolic and vectorial nature: criteria that existing metrics
such as FID, LPIPS, or CLIPScore fail to satisfy. In this paper, we introduce
SVGauge, the first human-aligned, reference based metric for text-to-SVG
generation. SVGauge jointly measures (i) visual fidelity, obtained by
extracting SigLIP image embeddings and refining them with PCA and whitening for
domain alignment, and (ii) semantic consistency, captured by comparing
BLIP-2-generated captions of the SVGs against the original prompts in the
combined space of SBERT and TF-IDF. Evaluation on the proposed SHE benchmark
shows that SVGauge attains the highest correlation with human judgments and
reproduces system-level rankings of eight zero-shot LLM-based generators more
faithfully than existing metrics. Our results highlight the necessity of
vector-specific evaluation and provide a practical tool for benchmarking future
text-to-SVG generation models.

</details>


### [39] [Efficient Computation of Voronoi Diagrams Using Point-in-Cell Tests](https://arxiv.org/abs/2509.07175)
*Yanyang Xiao,Yao Li,Juan Cao,Zhonggui Chen*

Main category: cs.GR

TL;DR: 提出了一种高效计算受限Voronoi图的新方法，通过基于边的搜索方案和点-在-单元测试，减少了不必要的裁剪，显著提升了性能，并支持GPU并行计算。


<details>
  <summary>Details</summary>
Motivation: Voronoi图在许多应用中广泛使用，但其计算效率仍有提升空间，特别是在受限或裁剪的情况下。

Method: 采用边缘搜索方案结合点-在-单元测试，动态确定裁剪平面，仅对必要部分进行裁剪，避免了多余计算，并扩展到GPU并行处理。

Result: 实验结果显示该方法在不同站点分布下均优于现有技术，表现出最佳性能。

Conclusion: 该方法通过高效裁剪策略和并行计算，显著提升了受限Voronoi图的计算效率，具有广泛的应用前景。

Abstract: Since the Voronoi diagram appears in many applications, the topic of
improving its computational efficiency remains attractive. We propose a novel
yet efficient method to compute Voronoi diagrams bounded by a given domain,
i.e., the clipped or restricted Voronoi diagrams. The intersection of the
domain and a Voronoi cell (domain-cell intersection) is generated by removing
the part outside the cell from the domain, which can be accomplished by several
clippings. Different from the existing methods, we present an edge-based search
scheme to find clipping planes (bisectors). A test called point-in-cell is
first set up to tell whether a space point is in a target Voronoi cell or not.
Then, for each edge of the intermediate domain-cell intersection, we will
launch a clipping only if its two endpoints are respectively inside and outside
the corresponding Voronoi cell, where the bisector for the clipping can be
found by using a few times of point-in-cell tests. Therefore, our method only
involves the clippings that contribute to the final results, which is a great
advantage over the state-of-the-art methods. Additionally, because each
domain-cell intersection can be generated independently, we extend the proposed
method to the GPUs for computing Voronoi diagrams in parallel. The experimental
results show the best performance of our method compared to state-of-the-art
ones, regardless of site distribution. This paper was first submitted to
SIGGRAPH Asia 2025.

</details>


### [40] [Neural Cone Radiosity for Interactive Global Illumination with Glossy Materials](https://arxiv.org/abs/2509.07522)
*Jierui Ren,Haojie Jin,Bo Pang,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出了一种基于神经辐射度框架的高效方法——神经锥辐射度，通过反射感知的射线锥编码来捕捉高频、强烈视角依赖的辐射分布，显著提升了网络建模高频反射分布的能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经辐射度方法在捕捉高频、强烈视角依赖的辐射分布时存在局限性，尤其是对于光泽材料。

Method: 采用反射感知的射线锥编码，基于预滤波多分辨率哈希网格来近似光泽BSDF瓣，并将视角依赖的反射特性直接嵌入编码过程。

Result: 该方法在各种光泽条件下实时生成高质量、无噪声的渲染效果，且在保真度和真实感上优于基线方法。

Conclusion: 神经锥辐射度方法不仅高效，还能广泛适用于不同光泽度的表面，为高频辐射分布建模提供了新思路。

Abstract: Modeling of high-frequency outgoing radiance distributions has long been a
key challenge in rendering, particularly for glossy material. Such
distributions concentrate radiative energy within a narrow lobe and are highly
sensitive to changes in view direction. However, existing neural radiosity
methods, which primarily rely on positional feature encoding, exhibit notable
limitations in capturing these high-frequency, strongly view-dependent radiance
distributions. To address this, we propose a highly-efficient approach by
reflectance-aware ray cone encoding based on the neural radiosity framework,
named neural cone radiosity. The core idea is to employ a pre-filtered
multi-resolution hash grid to accurately approximate the glossy BSDF lobe,
embedding view-dependent reflectance characteristics directly into the encoding
process through continuous spatial aggregation. Our design not only
significantly improves the network's ability to model high-frequency reflection
distributions but also effectively handles surfaces with a wide range of
glossiness levels, from highly glossy to low-gloss finishes. Meanwhile, our
method reduces the network's burden in fitting complex radiance distributions,
allowing the overall architecture to remain compact and efficient.
Comprehensive experimental results demonstrate that our method consistently
produces high-quality, noise-free renderings in real time under various
glossiness conditions, and delivers superior fidelity and realism compared to
baseline approaches.

</details>


### [41] [Topology-Aware Optimization of Gaussian Primitives for Human-Centric Volumetric Videos](https://arxiv.org/abs/2509.07653)
*Yuheng Jiang,Chengcheng Guo,Yize Wu,Yu Hong,Shengkun Zhu,Zhehao Shen,Yingliang Zhang,Shaohui Jiao,Zhuo Su,Lan Xu,Marc Habermann,Christian Theobalt*

Main category: cs.GR

TL;DR: TaoGS是一种新型拓扑感知动态高斯表示方法，通过解耦运动和外观来支持长期跟踪和拓扑自适应。


<details>
  <summary>Details</summary>
Motivation: 动态场景建模，特别是涉及拓扑变化且需保持长期跟踪的场景，是当前的关键挑战。

Method: 利用稀疏的运动高斯集表示场景运动，并通过时空跟踪器和结构变化检测进行更新。每个运动高斯动态激活一组局部外观高斯以捕捉精细纹理。

Result: TaoGS支持高保真渲染，即使在衣物变化等复杂场景中也能保持时间一致性，并支持40%的压缩。

Conclusion: TaoGS为拓扑变化的可扩展体积视频提供了统一的适应性解决方案。

Abstract: Volumetric video is emerging as a key medium for digitizing the dynamic
physical world, creating the virtual environments with six degrees of freedom
to deliver immersive user experiences. However, robustly modeling general
dynamic scenes, especially those involving topological changes while
maintaining long-term tracking remains a fundamental challenge. In this paper,
we present TaoGS, a novel topology-aware dynamic Gaussian representation that
disentangles motion and appearance to support, both, long-range tracking and
topological adaptation. We represent scene motion with a sparse set of motion
Gaussians, which are continuously updated by a spatio-temporal tracker and
photometric cues that detect structural variations across frames. To capture
fine-grained texture, each motion Gaussian anchors and dynamically activates a
set of local appearance Gaussians, which are non-rigidly warped to the current
frame to provide strong initialization and significantly reduce training time.
This activation mechanism enables efficient modeling of detailed textures and
maintains temporal coherence, allowing high-fidelity rendering even under
challenging scenarios such as changing clothes. To enable seamless integration
into codec-based volumetric formats, we introduce a global Gaussian Lookup
Table that records the lifespan of each Gaussian and organizes attributes into
a lifespan-aware 2D layout. This structure aligns naturally with standard video
codecs and supports up to 40 compression. TaoGS provides a unified, adaptive
solution for scalable volumetric video under topological variation, capturing
moments where "elegance in motion" and "Power in Stillness", delivering
immersive experiences that harmonize with the physical world.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [42] [PSketch: A Priority-Aware Sketch Architecture for Real-Time Flow Monitoring via eBPF](https://arxiv.org/abs/2509.07338)
*Yuanjun Dai,Qingzhe Guo,Xiangren Wang*

Main category: cs.ET

TL;DR: PSketch是一种基于eBPF的内核优先级感知草图框架，用于SDN中的高精度流监控。


<details>
  <summary>Details</summary>
Motivation: 解决SDN中草图监控的管道和内存限制问题，提升算法灵活性和准确性。

Method: 通过哈希表无损跟踪高优先级流，使用草图管道近似识别top-k大象流，支持TCP/UDP且开销低。

Result: 在10 Gbps CAIDA流量上，实现96.0% top-k检测准确率、96.4%重传召回率，吞吐量仅降低0.7%。

Conclusion: PSketch在通用Linux系统上高效运行，摆脱硬件依赖，显著提升监控性能。

Abstract: Sketch-based monitoring in SDN often suffers from tightly coupled pipeline
and memory constraints, limiting algorithmic flexibility and reducing accuracy.
We propose PSketch, the first in-kernel priority-aware sketching framework
implemented with eBPF. It ensures lossless tracking of high-priority flows via
a hash-based table and approximates top-k elephant flows using a sketch pipe.
PSketch supports both TCP and UDP and enables in-kernel retransmission tracking
with minimal overhead. Unlike SDN-based approaches, it runs on commodity Linux
systems, removing hardware dependencies. We perform evaluation on 10 Gbps CAIDA
traces. Results show that PSketch achieves 96.0% top-k detection accuracy,
96.4% retransmission recall, and only 0.7% throughput degradation.

</details>


### [43] [Gut-Brain Axis as a Closed-Loop Molecular Communication Network](https://arxiv.org/abs/2509.07911)
*Beyza E. Ortlek,Ozgur B. Akan*

Main category: cs.ET

TL;DR: 该论文提出了一个新颖的分子通信框架，用于分析肠道-大脑轴(GBA)的信息传递，并通过数学模型揭示了压力和炎症如何导致病理状态。


<details>
  <summary>Details</summary>
Motivation: 研究肠道-大脑轴在多压力条件下的信息传递机制，以量化其对健康和疾病状态的影响。

Method: 使用六方程非线性延迟微分方程(DDE)模型，结合时间域模拟、频域小信号分析和信息论容量分析。

Result: 稳态时系统保持高信息吞吐的昼夜节律；持续压力导致带宽缩窄和信息效率下降，引发高皮质醇病理状态。

Conclusion: 模型量化了信号机制对信息处理的影响，解释了从健康昼夜节律到高皮质醇病理状态的转变。

Abstract: Molecular communication (MC) provides a quantitative framework for analyzing
information transfer within biological systems. This paper introduces a novel
and comprehensive MC framework for the gut-brain axis (GBA) as a system of six
coupled, nonlinear delay differential equations (DDEs). The proposed model
defines a bidirectional feedback loop with a gut-to-brain inflammatory channel
and a brain-to-gut neuroendocrine channel. Under prolonged stress, this
feedback loop becomes self-perpetuating and drives the system into a
pathological state. We evaluate the end-to-end channel across varying
conditions using time-domain simulations, small-signal frequency-domain
characterization, and an information-theoretic capacity analysis. At
homeostasis, the system maintains stable circadian dynamics with higher
information throughput, whereas sustained stress drives a shift to dysregulated
hypercortisolism. In this pathological state, spectral efficiency decreases due
to a narrowed effective bandwidth and a lower passband gain driven by
neuroendocrine delays and saturating cytokine-hormone kinetics. These results
quantify the impact of these signaling mechanisms on stability and information
processing, elucidating the transition from healthy circadian rhythms to a
persistent pathological state of hypercortisolism.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [44] [Crossword: Adaptive Consensus for Dynamic Data-Heavy Workloads](https://arxiv.org/abs/2509.07157)
*Guanzhou Hu,Yiwei Chen,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: Crossword是一种灵活的共识协议，针对云环境中动态数据密集型工作负载的挑战，通过智能分片和自适应权衡提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决云环境中复制负载大小差异大、带宽压力突发的动态工作负载问题。

Method: 采用按实例的纠删码和智能分片分配，结合自适应权衡和懒散跟随者八卦机制，保证高可用性。

Result: 在静态场景中性能相当，动态工作负载下性能最高提升2.3倍；集成CockroachDB时TPC-C吞吐量提升1.32倍。

Conclusion: Crossword在动态环境中显著优于现有协议，并通过开源Gazette推动实际应用。

Abstract: We present Crossword, a flexible consensus protocol for dynamic data-heavy
workloads, a rising challenge in the cloud where replication payload sizes span
a wide spectrum and introduce sporadic bandwidth stress. Crossword applies
per-instance erasure coding and distributes coded shards intelligently to
reduce critical-path data transfer significantly when desirable. Unlike
previous approaches that statically assign shards to servers, Crossword enables
an adaptive tradeoff between the assignment of shards and quorum size in
reaction to dynamic workloads and network conditions, while always retaining
the availability guarantee of classic protocols. Crossword handles leader
failover gracefully by employing a lazy follower gossiping mechanism that
incurs minimal impact on critical-path performance. We implement Crossword
(along with relevant protocols) in Gazette, a distributed, replicated, and
protocol-generic key-value store written in async Rust. We evaluate Crossword
comprehensively to show that it matches the best performance among previous
protocols (MultiPaxos, Raft, RSPaxos, and CRaft) in static scenarios, and
outperforms them by up to 2.3x under dynamic workloads and network conditions.
Our integration of Crossword with CockroachDB brings 1.32x higher aggregate
throughput to TPC-C under 5-way replication. We will open-source Gazette upon
publication.

</details>


### [45] [Bodega: Serving Linearizable Reads Locally from Anywhere at Anytime via Roster Leases](https://arxiv.org/abs/2509.07158)
*Guanzhou Hu,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: Bodega是一种新型共识协议，通过创新的roster leases算法实现本地线性化读取，显著提升读取性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有共识协议在高写入干扰下读取性能低的问题。

Method: 使用roster leases算法、乐观保持和早期接受通知等技术。

Result: 在真实WAN集群上，Bodega将客户端读取请求平均加速5.6x-13.1x，同时保持高写入性能。

Conclusion: Bodega是一种高效且非侵入式的共识协议扩展，适用于分布式系统。

Abstract: We present Bodega, the first consensus protocol that serves linearizable
reads locally from any desired node, regardless of interfering writes. Bodega
achieves this via a novel roster leases algorithm that safeguards the roster, a
new notion of cluster metadata. The roster is a generalization of leadership;
it tracks arbitrary subsets of replicas as responder nodes for local reads. A
consistent agreement on the roster is established through roster leases, an
all-to-all leasing mechanism that generalizes existing all-to-one leasing
approaches (Leader Leases, Quorum Leases), unlocking a new point in the
protocol design space. Bodega further employs optimistic holding and early
accept notifications to minimize interruption from interfering writes, and
incorporates smart roster coverage and lightweight heartbeats to maximize
practicality. Bodega is a non-intrusive extension to classic consensus; it
imposes no special requirements on writes other than a responder-covering
quorum. We implement Bodega and related works in Vineyard, a protocol-generic
replicated key-value store written in async Rust. We compare it to previous
protocols (Leader Leases, EPaxos, PQR, and Quorum Leases) and two production
coordination services (etcd and ZooKeeper). Bodega speeds up average client
read requests by 5.6x-13.1x on real WAN clusters versus previous approaches
under moderate write interference, delivers comparable write performance,
supports fast proactive roster changes as well as fault tolerance via leases,
and closely matches the performance of sequentially-consistent etcd and
ZooKeeper deployments across all YCSB workloads. We will open-source Vineyard
upon publication.

</details>


### [46] [DREAMS: Decentralized Resource Allocation and Service Management across the Compute Continuum Using Service Affinity](https://arxiv.org/abs/2509.07497)
*Hai Dinh-Tuan,Tien Hung Nguyen,Sanjeet Raj Pandey*

Main category: cs.DC

TL;DR: 本文提出了DREAMS，一种去中心化框架，用于在计算连续体中优化微服务部署决策，解决传统集中式方法的规模、延迟和单点故障问题。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要适应动态工作负载和定制生产需求的计算基础设施，传统集中式资源分配和服务部署方法难以满足需求。

Method: DREAMS通过自主代理在各计算域内操作，并通过Raft共识算法和成本效益投票实现全局协调，形成去中心化架构。

Result: DREAMS在现代制造环境中实现了全局优化的服务部署，并具备高容错性；协调操作（如LDM注册和迁移投票）随域数增加呈次线性扩展。

Conclusion: DREAMS是一种高效、可扩展的去中心化解决方案，适用于多利益相关方的计算连续体环境。

Abstract: Modern manufacturing systems require adaptive computing infrastructures that
can respond to highly dynamic workloads and increasingly customized production
demands. The compute continuum emerges as a promising solution, enabling
flexible deployment of microservices across distributed, heterogeneous domains.
However, this paradigm also requires a novel approach to resource allocation
and service placement, as traditional centralized solutions struggle to scale
effectively, suffer from latency bottlenecks, and introduce single points of
failure. In this paper, we present DREAMS, a decentralized framework that
optimizes microservice placement decisions collaboratively across different
computational domains. At its core, DREAMS introduces agents that operate
autonomously within each domain while coordinating globally through a
Raft-based consensus algorithm and cost-benefit voting. This decentralized
architecture enables responsive, privacy-preserving, and fault-tolerant
coordination, making it particularly suitable given the growing prevalence of
multi-stakeholder scenarios across the compute continuum. In particular, within
modern manufacturing environments, DREAMS achieves globally optimized service
placements while maintaining high fault tolerance. Further evaluations
demonstrate that key coordination operations, such as Local Domain Manager
(LDM) registration and migration voting, scale sub-linearly with the number of
domains, confirming the efficiency and scalability of our proposal.

</details>


### [47] [A Study on Messaging Trade-offs in Data Streaming for Scientific Workflows](https://arxiv.org/abs/2509.07199)
*Anjus George,Michael J. Brim,Christopher Zimmer,Tyler J. Skluzacek,A. J. Ruckman,Gustav R. Jansen,Sarp Oral*

Main category: cs.DC

TL;DR: 研究探讨了消息传递参数及其配置对科学工作流数据流要求的影响，通过模拟实验揭示了优化配置的关键观察。


<details>
  <summary>Details</summary>
Motivation: 现代科学工作流需要实时数据分析，消除文件传输延迟，确保低延迟、高吞吐量和可靠的数据传输。

Method: 使用RabbitMQ消息框架模拟Deleria和LCLS工作流的数据流，分析不同配置对吞吐量的影响。

Result: 模拟实验揭示了优化配置的关键观察，帮助用户选择最适合其工作流的配置。

Conclusion: 研究为科学工作流的数据流配置提供了实用见解，优化了实时数据传输性能。

Abstract: Memory-to-memory data streaming is essential for modern scientific workflows
that require near real-time data analysis, experimental steering, and informed
decision-making during experiment execution. It eliminates the latency
bottlenecks associated with file-based transfers to parallel storage, enabling
rapid data movement between experimental facilities and HPC systems. These
tightly coupled experimental-HPC workflows demand low latency, high throughput,
and reliable data delivery to support on-the-fly analysis and timely feedback
for experimental control. Off-the-shelf messaging frameworks are increasingly
considered viable solutions for enabling such direct memory streaming due to
their maturity, broad adoption, and ability to abstract core messaging and
reliability functionalities from the application layer. However, effectively
meeting the workflows' requirements depends on utilizing the framework's
capabilities and carefully tuning its configurations.
  In this paper, we present a study that investigates the messaging parameters,
and their configuration choices that impact the streaming requirements of two
representative scientific workflows. We specifically characterize throughput
trade-offs associated with reliable message transmission for these workflows.
Our study is conducted through streaming simulations using synthetic workloads
derived from the Deleria and LCLS workflows, employing the RabbitMQ messaging
framework within the context of the Data Streaming to HPC infrastructure at
OLCF. Our simulations reveal several key observations and practical insights
that help users understand which configurations best meet the needs of their
streaming workloads.

</details>


### [48] [Optimizing Task Scheduling in Fog Computing with Deadline Awareness](https://arxiv.org/abs/2509.07378)
*Mohammad Sadegh Sirjani,Somayeh Sobati-Moghadam*

Main category: cs.DC

TL;DR: 本文提出了一种结合改进的金鹰优化算法和强化学习的任务调度方法（RIGEO），用于优化雾计算中物联网任务的资源分配，以减少能耗并提升服务质量。


<details>
  <summary>Details</summary>
Motivation: 物联网应用的快速发展需要低延迟和高响应速度，雾计算虽然提供了解决方案，但在资源分配和任务调度方面仍面临挑战。

Method: 将雾节点按流量分为高低两类，低流量节点使用改进的金鹰优化算法调度低截止时间任务，高流量节点使用强化学习处理高截止时间任务。

Result: 实验表明，RIGEO算法在系统响应时间、截止时间违规、资源及能耗等方面优于现有算法。

Conclusion: 结合IGEO和强化学习的RIGEO算法能有效优化雾计算中的任务调度，满足物联网应用的能耗和QoS需求。

Abstract: The rise of Internet of Things (IoT) devices has led to the development of
numerous applications that require quick responses and low latency. Fog
computing has emerged as a solution for processing these IoT applications, but
it faces challenges such as resource allocation and job scheduling. Therefore,
it is crucial to determine how to assign and schedule tasks on Fog nodes. A
well-designed job scheduling algorithm can help decrease energy usage and
improve response times for application requests. This work aims to schedule
tasks in IoT while minimizing the total energy consumption of nodes and
enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into
account task deadlines. Initially, this paper classifies the Fog nodes into two
categories based on their traffic level: low and high. It schedules
low-deadline tasks on low-traffic-level nodes using an Improved Golden Eagle
Optimization (IGEO) algorithm, an enhancement of the Golden Eagle Optimization
Algorithm that utilizes genetic operators for discretization. High-deadline
tasks are processed on high-traffic nodes using reinforcement learning (RL).
This combined approach is called the Reinforcement Improved Golden Eagle
Optimization (RIGEO) algorithm. Experimental results demonstrate that the
proposed algorithms optimize system response time, total deadline violation
time, and resource and system energy consumption compared to other
state-of-the-art algorithms.

</details>


### [49] [DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference](https://arxiv.org/abs/2509.07379)
*Yuning Zhang,Grant Pinkert,Nan Yang,Yanli Li,Dong Yuan*

Main category: cs.DC

TL;DR: 为了解决MoE推理中的内存和延迟问题，提出了DuoServe-MoE系统，通过分阶段调度策略优化预填充和解码阶段的专家激活，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型在推理时存在内存压力和延迟问题，尤其是单GPU环境下，需要针对不同阶段优化调度策略。

Method: DuoServe-MoE系统分阶段处理预填充和解码：预填充阶段使用两流CUDA管道重叠计算，解码阶段通过离线训练的轻量级预测器预取可能激活的专家。

Result: 在4-bit Mixtral模型上，DuoServe-MoE将端到端延迟降低了1.42到7.54倍，峰值内存仅占全模型大小的15%。

Conclusion: DuoServe-MoE通过分阶段专家调度策略，有效解决了MoE推理中的内存和延迟问题，显著提升了性能。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances
their capabilities by increasing model width through sparsely activated expert
branches, which keeps inference computation efficient. However, the large
number of expert weights introduces significant GPU memory pressure, especially
in resource-constrained environments such as single-GPU servers. More
importantly, MoE inference consists of two fundamentally different stages: a
prefill stage where most experts are activated densely, and a decode stage
where only a few experts are triggered sparsely. Treating these stages with a
uniform scheduling strategy often leads to suboptimal latency and memory usage.
To address this, we propose DuoServe-MoE, an inference serving system that
explicitly separates prefill and decode stages and applies tailored expert
scheduling strategies to each. In the prefill stage, DuoServe-MoE uses a
two-stream CUDA pipeline that overlaps expert weight prefetching with the
computation of non-MoE layers, limiting expert residency in GPU memory. In the
decode stage, a lightweight layer-level predictor trained offline from
activation traces is used to prefetch only the most likely activated experts,
without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B
and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to
7.54 times while keeping peak memory usage at only 15 percent of the full model
size.

</details>


### [50] [Dependency-Aware Execution Mechanism in Hyperledger Fabric Architecture](https://arxiv.org/abs/2509.07425)
*Sanyam Kaul,Manaswini Piduguralla,Gayathri Shreeya Patnala,Sathya Peri*

Main category: cs.DC

TL;DR: 该论文提出了一种依赖感知执行模型，优化Hyperledger Fabric的性能，通过标志交易依赖关系、优化区块构建和并行执行，提高了吞吐量并降低了拒绝率。


<details>
  <summary>Details</summary>
Motivation: Hyperledger Fabric在高负载下存在性能瓶颈，交易依赖导致资源低效和竞争，因此需要改进其执行模型。

Method: 引入依赖标志系统、优化区块构建、使用DAG表示依赖关系，并实现交易的并行执行。

Result: 在Fabric v2.5中测试显示，吞吐量提升高达40%，且在高竞争场景下显著降低拒绝率。

Conclusion: 依赖感知调度和DAG执行显著提升了Fabric的可扩展性，同时兼容现有共识和智能合约层。

Abstract: Hyperledger Fabric is a leading permissioned blockchain framework for
enterprise use, known for its modular design and privacy features. While it
strongly supports configurable consensus and access control, Fabric can face
challenges in achieving high transaction throughput and low rejection rates
under heavy workloads. These performance limitations are often attributed to
endorsement, ordering, and validation bottlenecks. Further, optimistic
concurrency control and deferred validation in Fabric may lead to resource
inefficiencies and contention, as conflicting transactions are identified only
during the commit phase. To address these challenges, we propose a
dependency-aware execution model for Hyperledger Fabric. Our approach includes:
(a) a dependency flagging system during endorsement, marking transactions as
independent or dependent using a hashmap; (b) an optimized block construction
in the ordering service that prioritizes independent transactions; (c) the
incorporation of a Directed Acyclic Graph (DAG) within each block to represent
dependencies; and (d) parallel execution of independent transactions at the
committer, with dependent transactions processed according to DAG order.
Incorporated in Hyperledger Fabric v2.5, our framework was tested on workloads
with varying dependency levels and system loads. Results show up to 40% higher
throughput and significantly reduced rejection rates in high-contention
scenarios. This demonstrates that dependency-aware scheduling and DAG-based
execution can substantially enhance Fabric's scalability while remaining
compatible with its existing consensus and smart contract layers.

</details>


### [51] [Astra: A Multi-Agent System for GPU Kernel Performance Optimization](https://arxiv.org/abs/2509.07506)
*Anjiang Wei,Tianran Sun,Yogesh Seenichamy,Hang Song,Anne Ouyang,Azalia Mirhoseini,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: 论文介绍了Astra，首个基于LLM的多智能体系统，用于GPU内核优化，相比传统方法更高效且无需大量人工调整。


<details>
  <summary>Details</summary>
Motivation: 当前GPU内核优化依赖大量手动调优或编译器系统，仍耗费人工。LLM虽有应用潜力，但此前主要在PyTorch到CUDA的转换上。

Method: Astra从现有CUDA代码出发，多LLM智能体通过迭代生成、测试、分析和计划协作优化内核。

Result: Astra在SGLang内核上实现了1.32倍的平均加速（零样本提示）。

Conclusion: LLM多智能体系统是GPU内核优化的新范式，可自主应用多种优化手段。

Abstract: GPU kernel optimization has long been a central challenge at the intersection
of high-performance computing and machine learning. Efficient kernels are
crucial for accelerating large language model (LLM) training and serving, yet
attaining high performance typically requires extensive manual tuning.
Compiler-based systems reduce some of this burden, but still demand substantial
manual design and engineering effort. Recently, researchers have explored using
LLMs for GPU kernel generation, though prior work has largely focused on
translating high-level PyTorch modules into CUDA code. In this work, we
introduce Astra, the first LLM-based multi-agent system for GPU kernel
optimization. Unlike previous approaches, Astra starts from existing CUDA
implementations extracted from SGLang, a widely deployed framework for serving
LLMs, rather than treating PyTorch modules as the specification. Within Astra,
specialized LLM agents collaborate through iterative code generation, testing,
profiling, and planning to produce kernels that are both correct and
high-performance. On kernels from SGLang, Astra achieves an average speedup of
1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study
further demonstrates that LLMs can autonomously apply loop transformations,
optimize memory access patterns, exploit CUDA intrinsics, and leverage fast
math operations to yield substantial performance gains. Our work highlights
multi-agent LLM systems as a promising new paradigm for GPU kernel
optimization.

</details>


### [52] [Navigating Energy Doldrums: Modeling the Impact of Energy Price Volatility on HPC Cost of Ownership](https://arxiv.org/abs/2509.07567)
*Peter Arzt,Felix Wolf*

Main category: cs.DC

TL;DR: 探讨动态调整高性能计算资源以应对电价波动的策略，通过模型评估其对总拥有成本的影响。


<details>
  <summary>Details</summary>
Motivation: 高电价波动和高性能计算系统的高能耗成本促使研究动态资源调整策略。

Method: 提出简单模型，利用关键系统参数评估动态资源调整对总拥有成本的影响。

Result: 应用于大学HPC集群的真实数据，评估不同情境下该方法的成本效益。

Conclusion: 动态容量策略可降低HPC系统能耗成本，但需权衡硬件利用率和成本效益。

Abstract: Energy costs are a major factor in the total cost of ownership (TCO) for
high-performance computing (HPC) systems. The rise of intermittent green energy
sources and reduced reliance on fossil fuels have introduced volatility into
electricity markets, complicating energy budgeting. This paper explores
variable capacity as a strategy for managing HPC energy costs - dynamically
adjusting compute resources in response to fluctuating electricity prices.
While this approach can lower energy expenses, it risks underutilizing costly
hardware. To evaluate this trade-off, we present a simple model that helps
operators estimate the TCO impact of variable capacity strategies using key
system parameters. We apply this model to real data from a university HPC
cluster and assess how different scenarios could affect the cost-effectiveness
of this approach in the future.

</details>


### [53] [AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with FaaS-hosted MCP Services](https://arxiv.org/abs/2509.07595)
*Shiva Sai Krishna Anand Tokal,Vaibhav Jha,Anand Eswaran,Praveen Jayachandran,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文提出了一种新型的AgentX工作流模式，结合了阶段设计器、规划器和执行器代理，性能优于现有代理模式（如ReAct和Magentic One），并通过MCP工具和FaaS部署方式进行了实证评估。


<details>
  <summary>Details</summary>
Motivation: 当前代理系统在多工具、复杂多步骤任务和长上下文管理中表现不佳，现有的工作流模式（如CoT和ReAct）需要改进。

Method: 设计了AgentX工作流模式，由阶段设计器、规划器和执行器代理组成，并提出了两种MCP服务器的FaaS部署方案。

Result: 实证评估显示，AgentX在成功率、延迟和成本方面优于ReAct和Magentic One。

Conclusion: AgentX展示了设计和部署代理工作流的潜力和挑战。

Abstract: Generative Artificial Intelligence (GenAI) has rapidly transformed various
fields including code generation, text summarization, image generation and so
on. Agentic AI is a recent evolution that further advances this by coupling the
decision making and generative capabilities of LLMs with actions that can be
performed using tools. While seemingly powerful, Agentic systems often struggle
when faced with numerous tools, complex multi-step tasks,and long-context
management to track history and avoid hallucinations. Workflow patterns such as
Chain-of-Thought (CoT) and ReAct help address this. Here, we define a novel
agentic workflow pattern, AgentX, composed of stage designer, planner, and
executor agents that is competitive or better than the state-of-the-art agentic
patterns. We also leverage Model Context Protocol (MCP) tools, and propose two
alternative approaches for deploying MCP servers as cloud Functions as a
Service (FaaS). We empirically evaluate the success rate, latency and cost for
AgentX and two contemporary agentic patterns, ReAct and Magentic One, using
these the FaaS and local MCP server alternatives for three practical
applications. This highlights the opportunities and challenges of designing and
deploying agentic workflows.

</details>


### [54] [Scaling atomic ordering in shared memory](https://arxiv.org/abs/2509.07781)
*Lorenzo Martignetti,Eliã Batista,Gianpaolo Cugola,Fernando Pedone*

Main category: cs.DC

TL;DR: TRAM是一种专为共享内存系统设计的原子多播协议，通过树状覆盖架构实现高性能，吞吐量提升3倍以上，延迟降低2.3倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有的原子多播协议多为消息传递系统设计，共享内存系统的协议较少，无法充分利用其性能优势。

Method: TRAM采用树状覆盖架构，设计简单实用。

Result: TRAM吞吐量提升3倍以上，延迟降低2.3倍以上；优于消息传递协议，吞吐量最高提升5.9倍，延迟降低106倍。

Conclusion: TRAM在共享内存系统中表现出色，为高性能关键服务提供了有效的解决方案。

Abstract: Atomic multicast is a communication primitive used in dependable systems to
ensure consistent ordering of messages delivered to a set of replica groups.
This primitive enables critical services to integrate replication and sharding
(i.e., state partitioning) to achieve fault tolerance and scalability. While
several atomic multicast protocols have been developed for message-passing
systems, only a few are designed for the shared memory system model. This paper
introduces TRAM, an atomic multicast protocol specifically designed for shared
memory systems, leveraging an overlay tree architecture. Due to its simple and
practical design, TRAM delivers exceptional performance, increasing throughput
by more than 3$\times$ and reducing latency by more than 2.3$\times$ compared
to state-of-the-art shared memory-based protocols. Additionally, it
significantly outperforms message-passing-based protocols, boosting throughput
by up to 5.9$\times$ and reducing latency by up to 106$\times$.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [55] [Navigating the Data Space Landscape: Concepts, Applications, and Future Directions](https://arxiv.org/abs/2509.06983)
*Bojana Marojevikj,Riste Stojanov*

Main category: cs.DB

TL;DR: 本文探讨了数据空间的演变、核心概念、实际应用及未来方向，强调语义数据模型和SPARQL授权层的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究数据空间的基础架构和实际应用，以解决数据主权、互操作性和信任等挑战。

Method: 分析数据空间的核心设计原则、构建模块及多个行业的实际案例。

Result: 展示了数据空间在实践中的成功应用，并提出未来发展方向。

Conclusion: 语义数据模型和SPARQL授权层有望显著提升数据空间的互操作性和安全性。

Abstract: This paper explores the evolving landscape of data spaces, focusing on key
concepts, practical applications, and emerging future directions. It begins by
introducing the foundational principles that underpin data space architectures,
emphasizing their importance in facilitating secure and efficient data
exchange. The core design principles and essential building blocks that form
the backbone of data-space systems are then examined. Several real-world
implementations are presented, showcasing how data spaces are applied across
various industries to address challenges such as data sovereignty,
interoperability, and trust. The paper concludes by discussing future
directions, proposing that leveraging semantic data models can significantly
enhance interoperability and data integration within data spaces. Furthermore,
it suggests exploring the implementation of SPARQL as a sophisticated
authorization layer to improve security and granular control over data access.
This research provides a comprehensive understanding of the current state of
data spaces and aims to guide future advancements in this rapidly evolving
field by highlighting the potential of semantic data and SPARQL-based
authorization.

</details>


### [56] [Private Queries with Sigma-Counting](https://arxiv.org/abs/2509.07018)
*Jun Gao,Jie Ding*

Main category: cs.DB

TL;DR: 提出了一种名为sigma-counting的新方法，通过sigma-algebra构造隐私保护的计数查询，显著提高输出准确性并保持隐私级别。


<details>
  <summary>Details</summary>
Motivation: 传统噪声添加方法在查询数量多和输出准确性上有限，且不保证嵌套查询的总顺序。需要一种改进方案。

Method: 采用sigma-algebra概念设计隐私保护计数查询，优化查询处理。

Result: 新方法在大量查询下显著提高准确性并保持隐私级别，适用于动态数据集。

Conclusion: sigma-counting是解决隐私保护计数查询问题的有效方法，具有实际应用潜力。

Abstract: Many data applications involve counting queries, where a client specifies a
feasible range of variables and a database returns the corresponding item
counts. A program that produces the counts of different queries often risks
leaking sensitive individual-level information. A popular approach to enhance
data privacy is to return a noisy version of the actual count. It is typically
achieved by adding independent noise to each query and then control the total
privacy budget within a period. This approach may be limited in the number of
queries and output accuracy in practice. Also, the returned counts do not
maintain the total order for nested queries, an important feature in many
applications. This work presents the design and analysis of a new method,
sigma-counting, that addresses these challenges. Sigma-counting uses the notion
of sigma-algebra to construct privacy-preserving counting queries. We show that
the proposed concepts and methods can significantly improve output accuracy
while maintaining a desired privacy level in the presence of massive queries to
the same data. We also discuss how the technique can be applied to address
large and time-varying datasets.

</details>


### [57] [JOINT: Join Optimization and Inference via Network Traversal](https://arxiv.org/abs/2509.07230)
*Szu-Yun Ko,Ethan Chen,Bo-Cian Chang,Alan Shu-Luen Chang*

Main category: cs.DB

TL;DR: 该论文提出了一种模糊连接框架，能够自动识别可连接的列对，并通过图遍历在多数据库中寻找间接连接路径。


<details>
  <summary>Details</summary>
Motivation: 传统的关系数据库需要用户手动指定连接键，且假设列名和值完全匹配。这在实践中限制了碎片化或不一致命名表的连接性。

Method: 该方法结合列名相似性和行级模糊值重叠，使用负对数转换的Jaccard分数计算边权重，并通过图遍历进行连接路径发现。

Result: 在合成的医疗风格数据库上的实验表明，尽管列名和值部分不匹配，系统仍能恢复有效的连接。

Conclusion: 这项研究在数据集成方面具有直接应用价值。

Abstract: Traditional relational databases require users to manually specify join keys
and assume exact matches between column names and values. In practice, this
limits joinability across fragmented or inconsistently named tables. We propose
a fuzzy join framework that automatically identifies joinable column pairs and
traverses indirect (multi-hop) join paths across multiple databases. Our method
combines column name similarity with row-level fuzzy value overlap, computes
edge weights using negative log-transformed Jaccard scores, and performs join
path discovery via graph traversal. Experiments on synthetic healthcare-style
databases demonstrate the system's ability to recover valid joins despite
fuzzified column names and partial value mismatches. This research has direct
applications in data integration.

</details>


### [58] [Filtered Approximate Nearest Neighbor Search: A Unified Benchmark and Systematic Experimental Study [Experiment, Analysis & Benchmark]](https://arxiv.org/abs/2509.07789)
*Jiayang Shi,Yuzheng Cai,Weiguo Zheng*

Main category: cs.DB

TL;DR: 论文综述了过滤近似最近邻搜索（FANNS）算法，提出了分类方法、公平评估框架和基准测试，以解决现有研究中的问题。


<details>
  <summary>Details</summary>
Motivation: 当前FANNS算法缺乏系统性研究，导致比较困难，实验设计偏颇，亟需统一标准以促进公平评估和理解算法优劣。

Method: 提出分类法（过滤-搜索、搜索-过滤、混合搜索）和评估框架，标准化参数调整和过滤流程，进行全面的实证分析。

Result: 通过实验分析查询难度和数据集特性对性能的影响，评估算法的鲁棒性和扩展性，建立了标准化基准。

Conclusion: 通过系统性研究和基准测试，为FANNS算法提供了公平比较和分析的基础，促进未来可重现的研究。

Abstract: For a given dataset $\mathcal{D}$ and structured label $f$, the goal of
Filtered Approximate Nearest Neighbor Search (FANNS) algorithms is to find
top-$k$ points closest to a query that satisfy label constraints, while
ensuring both recall and QPS (Queries Per Second). In recent years, many FANNS
algorithms have been proposed. However, the lack of a systematic investigation
makes it difficult to understand their relative strengths and weaknesses.
Additionally, we found that: (1) FANNS algorithms have coupled,
dataset-dependent parameters, leading to biased comparisons. (2) Key impact
factors are rarely analyzed systematically, leaving unclear when each algorithm
performs well. (3) Disparate datasets, workloads, and biased experiment designs
make cross-algorithm comparisons unreliable. Thus, a comprehensive survey and
benchmark for FANNS is crucial to achieve the following goals: designing a fair
evaluation and clarifying the classification of algorithms, conducting in-depth
analysis of their performance, and establishing a unified benchmark. First, we
propose a taxonomy (dividing methods into \textit{filter-then-search},
\textit{search-then-filter}, \textit{hybrid-search}) and a systematic
evaluation framework, integrating unified parameter tuning and standardized
filtering across algorithms to reduce implementation-induced performance
variations and reflect core trade-offs. Then, we conduct a comprehensive
empirical study to analyze how query difficulty and dataset properties impact
performance, evaluating robustness under pressures like filter selectivity,
Recall@k, and scalability to clarify each method's strengths. Finally, we
establish a standardized benchmark with real-world datasets and open-source
related resources to ensure reproducible future research.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [59] [HYLU: Hybrid Parallel Sparse LU Factorization](https://arxiv.org/abs/2509.07690)
*Xiaoming Chen*

Main category: cs.AR

TL;DR: HYLU是一种混合并行LU分解通用求解器，适用于多核共享内存架构下的稀疏线性系统求解，性能优于Intel MKL PARDISO。


<details>
  <summary>Details</summary>
Motivation: 设计适用于不同稀疏模式的通用求解器，提升稀疏线性系统求解效率。

Method: 采用混合数值核技术，适应系数矩阵的不同稀疏模式。

Result: 在34个稀疏矩阵测试中，HYLU在数值分解阶段表现优于Intel MKL PARDISO（单次求解1.74倍，重复求解2.26倍）。

Conclusion: HYLU在多核共享内存架构下表现出色，适用于稀疏线性系统的高效求解。

Abstract: This article introduces HYLU, a hybrid parallel LU factorization-based
general-purpose solver designed for efficiently solving sparse linear systems
(Ax=b) on multi-core shared-memory architectures. The key technical feature of
HYLU is the integration of hybrid numerical kernels so that it can adapt to
various sparsity patterns of coefficient matrices. Tests on 34 sparse matrices
from SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL
PARDISO in the numerical factorization phase by geometric means of 1.74X (for
one-time solving) and 2.26X (for repeated solving). HYLU can be downloaded from
https://github.com/chenxm1986/hylu.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [Lookup multivariate Kolmogorov-Arnold Networks](https://arxiv.org/abs/2509.07103)
*Sergey Pozdnyakov,Philippe Schwaller*

Main category: cs.LG

TL;DR: 论文提出了一种名为lmKANs的高维线性映射替代方法，显著提升了模型容量与推理成本的平衡，实验显示在多种任务中降低了计算成本并保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型中高维线性映射（如线性层）占据了大部分参数和计算成本，需要一种更高效的方法来替代。

Method: 通过可训练的低维多变量函数（基于样条查找表实现）表达高维映射，降低了计算复杂度。

Result: lmKANs在多种任务中显著降低了推理FLOPs（高达6倍），并在吞吐量和准确性上优于传统方法。

Conclusion: 提出的lmKANs是一种高效的高维映射替代方案，可广泛应用于深度学习模型，提升计算效率和性能。

Abstract: High-dimensional linear mappings, or linear layers, dominate both the
parameter count and the computational cost of most modern deep-learning models.
We introduce a general drop-in replacement, lookup multivariate
Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better
trade-off between capacity and inference cost. Our construction expresses a
general high-dimensional mapping through trainable low-dimensional multivariate
functions. These functions can carry dozens or hundreds of trainable parameters
each, and yet it takes only a few multiplications to compute them because they
are implemented as spline lookup tables. Empirically, lmKANs reduce inference
FLOPs by up to 6.0x while matching the flexibility of MLPs in general
high-dimensional function approximation. In another feedforward fully connected
benchmark, on the tabular-like dataset of randomly displaced methane
configurations, lmKANs enable more than 10x higher H100 throughput at equal
accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs
cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10
and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA
kernels, is available online at https://github.com/schwallergroup/lmkan.

</details>


### [61] [Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges](https://arxiv.org/abs/2509.07946)
*Kasra Borazjani,Naji Khosravan,Rajeev Sahay,Bita Akram,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 该论文提出了一种结合联邦学习和多模态多任务基础模型的隐私保护协作训练范式（M3T FedFMs），旨在解决教育领域中的数据隐私和个性化需求，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 教育领域中多模态多任务基础模型的应用受到隐私法规和数据孤岛的限制，需要一种既能保护隐私又能实现协作训练的新方法。

Method: 提出M3T FedFMs范式，通过联邦学习整合多模态多任务基础模型，支持分散机构的隐私保护协作训练。

Result: M3T FedFMs能够在隐私保护、个性化和包容性三个方面推动下一代智能教育系统的发展。

Conclusion: M3T FedFMs是一个有前景但尚未充分探索的研究方向，未来需解决隐私异质性、数据模态多样性等技术挑战。

Abstract: Multi-modal multi-task (M3T) foundation models (FMs) have recently shown
transformative potential in artificial intelligence, with emerging applications
in education. However, their deployment in real-world educational settings is
hindered by privacy regulations, data silos, and limited domain-specific data
availability. We introduce M3T Federated Foundation Models (FedFMs) for
education: a paradigm that integrates federated learning (FL) with M3T FMs to
enable collaborative, privacy-preserving training across decentralized
institutions while accommodating diverse modalities and tasks. Subsequently,
this position paper aims to unveil M3T FedFMs as a promising yet underexplored
approach to the education community, explore its potentials, and reveal its
related future research directions. We outline how M3T FedFMs can advance three
critical pillars of next-generation intelligent education systems: (i) privacy
preservation, by keeping sensitive multi-modal student and institutional data
local; (ii) personalization, through modular architectures enabling tailored
models for students, instructors, and institutions; and (iii) equity and
inclusivity, by facilitating participation from underrepresented and
resource-constrained entities. We finally identify various open research
challenges, including studying of (i) inter-institution heterogeneous privacy
regulations, (ii) the non-uniformity of data modalities' characteristics, (iii)
the unlearning approaches for M3T FedFMs, (iv) the continual learning
frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must
be collectively addressed for practical deployment.

</details>


### [62] [Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data](https://arxiv.org/abs/2509.07198)
*Yiyue Chen,Usman Akram,Chianing Wang,Haris Vikalo*

Main category: cs.LG

TL;DR: Fed-REACT是一个针对异构和动态客户数据的联邦学习框架，通过两阶段设计（特征提取和动态聚类）提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习在数据分布异构和动态变化时的性能下降问题。

Method: 两阶段设计：1) 客户局部模型学习特征表示；2) 服务器动态聚类并分簇训练任务特定模型。

Result: 理论分析支持表示学习阶段，实验证明Fed-REACT在真实数据集上表现优越且鲁棒。

Conclusion: Fed-REACT有效解决了联邦学习中的异构和动态数据问题，提升了模型性能。

Abstract: Motivated by the high resource costs and privacy concerns associated with
centralized machine learning, federated learning (FL) has emerged as an
efficient alternative that enables clients to collaboratively train a global
model while keeping their data local. However, in real-world deployments,
client data distributions often evolve over time and differ significantly
across clients, introducing heterogeneity that degrades the performance of
standard FL algorithms. In this work, we introduce Fed-REACT, a federated
learning framework designed for heterogeneous and evolving client data.
Fed-REACT combines representation learning with evolutionary clustering in a
two-stage process: (1) in the first stage, each client learns a local model to
extracts feature representations from its data; (2) in the second stage, the
server dynamically groups clients into clusters based on these representations
and coordinates cluster-wise training of task-specific models for downstream
objectives such as classification or regression. We provide a theoretical
analysis of the representation learning stage, and empirically demonstrate that
Fed-REACT achieves superior accuracy and robustness on real-world datasets.

</details>


### [63] [FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning](https://arxiv.org/abs/2509.07342)
*Yuxuan Bai,Yuxuan Sun,Tan Chen,Wei Chen,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: FedTeddi 是一种针对动态数据演变的联邦边缘学习调度算法，通过量化数据的时序动态和非独立同分布特性，显著提升模型收敛速度和测试准确率。


<details>
  <summary>Details</summary>
Motivation: 解决实际场景中客户端数据随时间变化且非独立同分布时，如何高效且及时地调整模型的问题。

Method: 提出 FedTeddi 算法，量化数据的时序漂移和集体差异（以EMD表示），并设计联合调度和带宽分配优化目标。

Result: 相比随机调度，在 CIFAR-10 和 CIFAR-100 上分别提升了 58.4% 和 49.2% 的收敛速度，测试准确率更高。

Conclusion: FedTeddi 能有效应对动态数据演变，快速学习新数据且保留旧知识，显著提升联邦边缘学习性能。

Abstract: Federated edge learning (FEEL) enables collaborative model training across
distributed clients over wireless networks without exposing raw data. While
most existing studies assume static datasets, in real-world scenarios clients
may continuously collect data with time-varying and non-independent and
identically distributed (non-i.i.d.) characteristics. A critical challenge is
how to adapt models in a timely yet efficient manner to such evolving data. In
this paper, we propose FedTeddi, a temporal-drift-and-divergence-aware
scheduling algorithm that facilitates fast convergence of FEEL under dynamic
data evolution and communication resource limits. We first quantify the
temporal dynamics and non-i.i.d. characteristics of data using temporal drift
and collective divergence, respectively, and represent them as the Earth
Mover's Distance (EMD) of class distributions for classification tasks. We then
propose a novel optimization objective and develop a joint scheduling and
bandwidth allocation algorithm, enabling the FEEL system to learn from new data
quickly without forgetting previous knowledge. Experimental results show that
our algorithm achieves higher test accuracy and faster convergence compared to
benchmark methods, improving the rate of convergence by 58.4% on CIFAR-10 and
49.2% on CIFAR-100 compared to random scheduling.

</details>


### [64] [MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?](https://arxiv.org/abs/2509.07727)
*Songkai Ma,Zhaorui Zhang,Sheng Di,Benben Liu,Xiaodong Yu,Xiaoyi Lu,Dan Wang*

Main category: cs.LG

TL;DR: 论文探讨了在GPU内存受限下，如何通过有损压缩算法压缩MoE模型中的非激活专家以减少数据传输开销，并分析了压缩误差对推理性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着MoE模型在LLM领域的广泛应用，如何在有限GPU内存下高效服务MoE模型成为挑战。非激活专家的内存卸载需解决数据传输问题，压缩技术为此提供可能。

Method: 提出使用有界误差有损压缩算法（如SZ3和CuSZp）压缩非激活专家，减少MoE推理时的数据传输开销，并通过多基准实验分析压缩误差对推理准确性的影响。

Result: 实验表明，浅层专家（负责注意力机制和输入转换）对压缩误差不敏感；中层专家（核心推理部分）误差会显著降低推理准确性；深层专家误差有时能提升推理准确性。

Conclusion: 有损压缩可有效减少MoE推理时的数据传输开销，但需根据专家层的位置差异化处理压缩误差，以平衡开销与准确性。

Abstract: With the widespread application of Mixture of Experts (MoE) reasoning models
in the field of LLM learning, efficiently serving MoE models under limited GPU
memory constraints has emerged as a significant challenge. Offloading the
non-activated experts to main memory has been identified as an efficient
approach to address such a problem, while it brings the challenges of
transferring the expert between the GPU memory and main memory. We need to
explore an efficient approach to compress the expert and analyze how the
compression error affects the inference performance.
  To bridge this gap, we propose employing error-bounded lossy compression
algorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby
reducing data transfer overhead during MoE inference. We conduct extensive
experiments across various benchmarks and present a comprehensive analysis of
how compression-induced errors in different experts affect overall inference
accuracy. The results indicate that experts in the shallow layers, which are
primarily responsible for the attention mechanism and the transformation of
input tokens into vector representations, exhibit minimal degradation in
inference accuracy when subjected to bounded errors. In contrast, errors in the
middle-layer experts, which are central to model reasoning, significantly
impair inference accuracy. Interestingly, introducing bounded errors in the
deep-layer experts, which are mainly responsible for instruction following and
output integration, can sometimes lead to improvements in inference accuracy.

</details>


### [65] [FUnc-SNE: A flexible, Fast, and Unconstrained algorithm for neighbour embeddings](https://arxiv.org/abs/2509.07681)
*Pierre Lambert,Edouard Couplet,Michel Verleysen,John Aldo Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的邻居嵌入（NE）加速方法，兼顾计算效率和结构保留，适用于高维数据，支持交互式探索。


<details>
  <summary>Details</summary>
Motivation: 现有NE加速方法要么牺牲计算质量（如UMAP），要么限制目标维度（如FIt-SNE），需平衡效率和灵活性。

Method: 通过迭代近似最近邻搜索减少计算量，保留精细结构并支持超参数调优，不限制嵌入维度。

Result: 实验表明该方法在速度和结构提取灵活性上表现优异，适合交互式数据探索。

Conclusion: 该方法为NE提供了一种高效且灵活的解决方案，扩展了其在机器学习中的应用潜力。

Abstract: Neighbour embeddings (NE) allow the representation of high dimensional
datasets into lower dimensional spaces and are often used in data
visualisation. In practice, accelerated approximations are employed to handle
very large datasets. Accelerating NE is challenging, and two main directions
have been explored: very coarse approximations based on negative sampling (as
in UMAP) achieve high effective speed but may lack quality in the extracted
structures; less coarse approximations, as used in FIt-SNE or BH-t-SNE, offer
better structure preservation at the cost of speed, while also restricting the
target dimensionality to 2 or 3, limiting NE to visualisation. In some
variants, the precision of these costlier accelerations also enables
finer-grained control on the extracted structures through dedicated
hyperparameters.
  This paper proposes to bridge the gab between both approaches by introducing
a novel way to accelerate NE, requiring a small number of computations per
iteration while maintaining good fine-grained structure preservation and
flexibility through hyperparameter tuning, without limiting the dimensionality
of the embedding space. The method was designed for interactive exploration of
data; as such, it abandons the traditional two-phased approach of other NE
methods, allowing instantaneous visual feedback when changing hyperparameters,
even when these control processes happening on the high-dimensional side of the
computations. Experiments using a publicly available, GPU accelerated GUI
integration of the method show promising results in terms of speed, flexibility
in the structures getting extracted, and show potential uses in broader machine
learning contexts with minimal algorithmic modifications. Central to this
algorithm is a novel approach to iterative approximate nearest neighbour
search, which shows promising results compared to nearest neighbour descent.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [66] [A smart fridge with AI-enabled food computing](https://arxiv.org/abs/2509.07400)
*Khue Nong Thuc,Khoa Tran Nguyen Anh,Tai Nguyen Huy,Du Nguyen Hao Hong,Khanh Dinh Ba*

Main category: eess.SY

TL;DR: 摘要介紹了一個基於IoT和計算機視覺的智能冰箱系統，通過ESP32-CAM實現實時食品檢測與管理，解決高密度庫存中的遮擋問題，並通過改進的損失函數提升檢測可靠性。


<details>
  <summary>Details</summary>
Motivation: 提升食品管理效率，減少浪費，優化家庭消費，並應對高密度庫存中的檢測挑戰。

Method: 系統分為數據預處理、目標檢測與管理、基於網頁的可視化三個模塊，採用改進的焦點損失函數解決模型校準問題。

Result: 改進的檢測方法顯著提升了不同光照條件下的可靠性，並支持可持續生活目標。

Conclusion: 該系統為現代食品管理提供了實用解決方案，通過減少浪費和優化消費推動可持續發展。

Abstract: The Internet of Things (IoT) plays a crucial role in enabling seamless
connectivity and intelligent home automation, particularly in food management.
By integrating IoT with computer vision, the smart fridge employs an ESP32-CAM
to establish a monitoring subsystem that enhances food management efficiency
through real-time food detection, inventory tracking, and temperature
monitoring. This benefits waste reduction, grocery planning improvement, and
household consumption optimization. In high-density inventory conditions,
capturing partial or layered images complicates object detection, as
overlapping items and occluded views hinder accurate identification and
counting. Besides, varied angles and obscured details in multi-layered setups
reduce algorithm reliability, often resulting in miscounts or
misclassifications. Our proposed system is structured into three core modules:
data pre-processing, object detection and management, and a web-based
visualization. To address the challenge of poor model calibration caused by
overconfident predictions, we implement a variant of focal loss that mitigates
over-confidence and under-confidence in multi-category classification. This
approach incorporates adaptive, class-wise error calibration via temperature
scaling and evaluates the distribution of predicted probabilities across
methods. Our results demonstrate that robust functional calibration
significantly improves detection reliability under varying lighting conditions
and scalability challenges. Further analysis demonstrates a practical,
user-focused approach to modern food management, advancing sustainable living
goals through reduced waste and more informed consumption.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [67] [Toward Lifelong-Sustainable Electronic-Photonic AI Systems via Extreme Efficiency, Reconfigurability, and Robustness](https://arxiv.org/abs/2509.07396)
*Ziang Yin,Hongjian Zhou,Chetan Choppali Sudarshan,Vidya Chhabria,Jiaqi Gu*

Main category: physics.optics

TL;DR: 论文探讨了电子-光子集成电路（EPICs）作为下一代AI系统的可持续平台，通过电子-光子设计自动化（EPDA）和跨层协同设计方法进一步提升性能和可持续性。


<details>
  <summary>Details</summary>
Motivation: 大规模AI的发展对计算能力需求激增，传统电子平台在能源、带宽和扩展性方面面临挑战，EPICs因其高带宽、低延迟和能效优势成为解决方案。

Method: 通过EPDA工具和跨层设备-电路-架构协同设计方法，优化芯片布局、减少金属层使用，并实现紧凑的光子电路设计、可重构硬件拓扑和智能容错机制。

Result: EPICs结合EPDA和协同设计方法，显著提升了AI系统的性能和可持续性，同时满足了现代AI的计算需求和可持续计算的紧迫要求。

Conclusion: EPICs通过高效性能和可持续设计，为终身可持续的AI系统提供了可行路径，同时应对了现代AI的计算挑战和环境可持续性需求。

Abstract: The relentless growth of large-scale artificial intelligence (AI) has created
unprecedented demand for computational power, straining the energy, bandwidth,
and scaling limits of conventional electronic platforms. Electronic-photonic
integrated circuits (EPICs) have emerged as a compelling platform for
next-generation AI systems, offering inherent advantages in ultra-high
bandwidth, low latency, and energy efficiency for computing and
interconnection. Beyond performance, EPICs also hold unique promises for
sustainability. Fabricated in relaxed process nodes with fewer metal layers and
lower defect densities, photonic devices naturally reduce embodied carbon
footprint (CFP) compared to advanced digital electronic integrated circuits,
while delivering orders-of-magnitude higher computing performance and
interconnect bandwidth. To further advance the sustainability of photonic AI
systems, we explore how electronic-photonic design automation (EPDA) and
cross-layer co-design methodologies can amplify these inherent benefits. We
present how advanced EPDA tools enable more compact layout generation, reducing
both chip area and metal layer usage. We will also demonstrate how cross-layer
device-circuit-architecture co-design unlocks new sustainability gains for
photonic hardware: ultra-compact photonic circuit designs that minimize chip
area cost, reconfigurable hardware topology that adapts to evolving AI
workloads, and intelligent resilience mechanisms that prolong lifetime by
tolerating variations and faults. By uniting intrinsic photonic efficiency with
EPDA- and co-design-driven gains in area efficiency, reconfigurability, and
robustness, we outline a vision for lifelong-sustainable electronic-photonic AI
systems. This perspective highlights how EPIC AI systems can simultaneously
meet the performance demands of modern AI and the urgent imperative for
sustainable computing.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [68] [Leveraging Digital Twin-as-a-Service Towards Continuous and Automated Cybersecurity Certification](https://arxiv.org/abs/2509.07649)
*Ioannis Koufos,Abdul Rehman Qureshi,Adrian Asensio,Allen Abishek,Efstathios Zaragkas,Ricard Vilalta,Maria Souvalioti,George Xilouris,Michael-Alexandros Kourtis*

Main category: cs.CR

TL;DR: SDT-aaS利用数字孪生技术实现自动化、非侵入式的安全合规评估，支持实时性且兼容开放标准。


<details>
  <summary>Details</summary>
Motivation: 传统风险评估方法依赖手动审计和系统扫描，可能引发操作中断和安全漏洞。

Method: 通过镜像真实资产、收集合规构件并生成机器可读证据，提出SDT-aaS解决方案。

Result: 中规模基础设施用例验证了其可行性和性能，支持高效合规管理。

Conclusion: SDT-aaS为高效、按需的网络安全治理提供了新途径，最小化运营影响。

Abstract: Traditional risk assessments rely on manual audits and system scans, often
causing operational disruptions and leaving security gaps. To address these
challenges, this work presents Security Digital Twin-as-a-Service (SDT-aaS), a
novel approach that leverages Digital Twin (DT) technology for automated,
non-intrusive security compliance. SDT-aaS enables real-time security
assessments by mirroring real-world assets, collecting compliance artifacts,
and creating machine-readable evidence. The proposed work is a scalable and
interoperable solution that supports open standards like CycloneDX and Web of
Things (WoT), facilitating seamless integration and efficient compliance
management. Empirical results from a moderate-scale infrastructure use case
demonstrate its feasibility and performance, paving the way for efficient,
on-demand cybersecurity governance with minimal operational impact.

</details>


### [69] [SafeToolBench: Pioneering a Prospective Benchmark to Evaluating Tool Utilization Safety in LLMs](https://arxiv.org/abs/2509.07315)
*Hongfei Xia,Hongru Wang,Zeming Liu,Qian Yu,Yuhang Guo,Haifeng Wang*

Main category: cs.CR

TL;DR: 该论文提出了SafeToolBench基准和SafeInstructTool框架，旨在前瞻性评估和提高LLM工具使用的安全性，避免潜在风险。


<details>
  <summary>Details</summary>
Motivation: 外部工具的使用虽然增强了LLM的问题解决能力，但也带来了金融损失和隐私泄露等风险，现有研究主要回顾性评估安全性，无法避免不可逆的损害。

Method: 提出了SafeToolBench基准和SafeInstructTool框架，从用户指令、工具本身及指令-工具联合三个视角共九个维度评估和增强LLM的工具安全性意识。

Result: 实验表明，现有方法无法全面捕捉工具使用中的风险，而SafeInstructTool显著提升了LLM的安全自意识，使其工具使用更安全可靠。

Conclusion: SafeToolBench和SafeInstructTool为前瞻性评估和提升LLM工具使用安全性提供了有效方案，增强了LLM的可信度。

Abstract: Large Language Models (LLMs) have exhibited great performance in autonomously
calling various tools in external environments, leading to better problem
solving and task automation capabilities. However, these external tools also
amplify potential risks such as financial loss or privacy leakage with
ambiguous or malicious user instructions. Compared to previous studies, which
mainly assess the safety awareness of LLMs after obtaining the tool execution
results (i.e., retrospective evaluation), this paper focuses on prospective
ways to assess the safety of LLM tool utilization, aiming to avoid irreversible
harm caused by directly executing tools. To this end, we propose SafeToolBench,
the first benchmark to comprehensively assess tool utilization security in a
prospective manner, covering malicious user instructions and diverse practical
toolsets. Additionally, we propose a novel framework, SafeInstructTool, which
aims to enhance LLMs' awareness of tool utilization security from three
perspectives (i.e., \textit{User Instruction, Tool Itself, and Joint
Instruction-Tool}), leading to nine detailed dimensions in total. We experiment
with four LLMs using different methods, revealing that existing approaches fail
to capture all risks in tool utilization. In contrast, our framework
significantly enhances LLMs' self-awareness, enabling a more safe and
trustworthy tool utilization.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [70] [Proximity Graphs for Similarity Search: Fast Construction, Lower Bounds, and Euclidean Separation](https://arxiv.org/abs/2509.07732)
*Shangqi Lu,Yufei Tao*

Main category: cs.DS

TL;DR: 本文提出了基于邻近图的近似最近邻搜索（ANN）的新理论基础，介绍了构建接近图的算法，改进了构建时间和查询时间的理论界限，并在欧几里得空间中进一步优化了图的大小。


<details>
  <summary>Details</summary>
Motivation: 邻近图方法是系统社区中近似最近邻搜索的主要范式，但其理论基础仍待深入。本文旨在填补这一空白，同时优化构建时间和查询时间的性能。

Method: 提出了一种算法，用于构建$(1+\epsilon)$-ANN搜索的邻近图，具有$O((1/\epsilon)^\lambda \cdot n \log \Delta)$的边数和$(1/\epsilon)^\lambda \cdot \text{polylog }\Delta$的查询时间。该方法在欧几里得空间中进一步减小了图的大小。

Result: 构建时间接近线性，优于先前的$\Omega(n^2)$界限。在欧几里得空间中，图的大小减少到$O((1/\epsilon)^\lambda \cdot n)$，同时保持了类似的查询和构建时间。

Conclusion: 结果表明邻近图在理论和实践上均可优化，尤其是在欧几里得空间中。未来的研究方向可能包括进一步探索几何空间中的优化潜力。

Abstract: Proximity graph-based methods have emerged as a leading paradigm for
approximate nearest neighbor (ANN) search in the system community. This paper
presents fresh insights into the theoretical foundation of these methods. We
describe an algorithm to build a proximity graph for $(1+\epsilon)$-ANN search
that has $O((1/\epsilon)^\lambda \cdot n \log \Delta)$ edges and guarantees
$(1/\epsilon)^\lambda \cdot \text{polylog }\Delta$ query time. Here, $n$ and
$\Delta$ are the size and aspect ratio of the data input, respectively, and
$\lambda = O(1)$ is the doubling dimension of the underlying metric space. Our
construction time is near-linear to $n$, improving the $\Omega(n^2)$ bounds of
all previous constructions. We complement our algorithm with lower bounds
revealing an inherent limitation of proximity graphs: the number of edges needs
to be at least $\Omega((1/\epsilon)^\lambda \cdot n + n \log \Delta)$ in the
worst case, up to a subpolynomial factor. The hard inputs used in our
lower-bound arguments are non-geometric, thus prompting the question of whether
improvement is possible in the Euclidean space (a key subclass of metric
spaces). We provide an affirmative answer by using geometry to reduce the graph
size to $O((1/\epsilon)^\lambda \cdot n)$ while preserving nearly the same
query and construction time.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [71] [Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems](https://arxiv.org/abs/2509.07817)
*Xiaolin Chen,Xuemeng Song,Haokun Wen,Weili Guan,Xiangyu Zhao,Liqiang Nie*

Main category: cs.CL

TL;DR: 提出了一种利用双重知识和大语言模型（LLM）增强多模态任务导向对话系统的文本响应生成方法DK2R。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了非结构化评论知识和大语言模型的潜力，导致多模态任务导向对话系统的文本响应生成效果受限。

Method: DK2R通过两阶段推理器，动态选择知识类型并解耦意图与响应，利用LLM生成辅助信号。

Result: 在公开数据集上的实验证明了DK2R的优越性，并开源了代码和参数。

Conclusion: DK2R有效结合双重知识和LLM，显著提升了多模态任务导向对话系统的文本响应生成性能。

Abstract: Textual response generation is pivotal for multimodal \mbox{task-oriented}
dialog systems, which aims to generate proper textual responses based on the
multimodal context. While existing efforts have demonstrated remarkable
progress, there still exist the following limitations: 1) \textit{neglect of
unstructured review knowledge} and 2) \textit{underutilization of large
language models (LLMs)}. Inspired by this, we aim to fully utilize dual
knowledge (\textit{i.e., } structured attribute and unstructured review
knowledge) with LLMs to promote textual response generation in multimodal
task-oriented dialog systems. However, this task is non-trivial due to two key
challenges: 1) \textit{dynamic knowledge type selection} and 2)
\textit{intention-response decoupling}. To address these challenges, we propose
a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for
multimodal dialog systems (named DK2R). To be specific, DK2R first extracts
both structured attribute and unstructured review knowledge from external
knowledge base given the dialog context. Thereafter, DK2R uses an LLM to
evaluate each knowledge type's utility by analyzing LLM-generated provisional
probe responses. Moreover, DK2R separately summarizes the intention-oriented
key clues via dedicated reasoning, which are further used as auxiliary signals
to enhance LLM-based textual response generation. Extensive experiments
conducted on a public dataset verify the superiority of DK2R. We have released
the codes and parameters.

</details>


### [72] [Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation](https://arxiv.org/abs/2509.07190)
*Zahra Atf,Peter R Lewis*

Main category: cs.CL

TL;DR: 本文提出了一种基于道德规则的框架来处理LLM生成文本中的不确定性，替代了传统不透明的概率方法，以提高透明度和信任度。


<details>
  <summary>Details</summary>
Motivation: 在LLM应用于高风险场景时，不确定性解释的技术和伦理问题凸显。概率方法通常不够透明，与透明度期望不符。

Method: 结合道德心理学和美德伦理学，定义了预防、尊重和责任等规则，并通过轻量级Prolog引擎实现不确定性级别的触发。

Result: 场景模拟测试了规则的覆盖率、公平性和信任校准效果。临床和法律领域的案例证明了该方法提升了信任和可解释性。

Conclusion: 该框架为社会责任的自然语言生成提供了透明且轻量的替代方案。

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where explaining uncertainty is both technical and ethical. Probabilistic
methods are often opaque and misaligned with expectations of transparency. We
propose a framework based on rule-based moral principles for handling
uncertainty in LLM-generated text. Using insights from moral psychology and
virtue ethics, we define rules such as precaution, deference, and
responsibility to guide responses under epistemic or aleatoric uncertainty.
These rules are encoded in a lightweight Prolog engine, where uncertainty
levels (low, medium, high) trigger aligned system actions with plain-language
rationales. Scenario-based simulations benchmark rule coverage, fairness, and
trust calibration. Use cases in clinical and legal domains illustrate how moral
reasoning can improve trust and interpretability. Our approach offers a
transparent, lightweight alternative to probabilistic models for socially
responsible natural language generation.

</details>


### [73] [Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents](https://arxiv.org/abs/2509.07389)
*Sankalp Tattwadarshi Swain,Anshika Krishnatray,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CL

TL;DR: 研究评估大型语言模型（LLM代理）是否能够通过模式识别和交互反馈学习新语言（Tinkatongue），结果显示LLM代理在100次响应内未能建立对话，但其策略与人类语言学习相似。


<details>
  <summary>Details</summary>
Motivation: 现有研究未评估LLM代理是否能够通过交互反馈学习语言，这是人类语言习得的核心特征。

Method: 提出新实验框架，评估LLM代理在与仅懂Tinkatongue的机器人对话中的语言学习能力。

Result: LLM代理未能成功对话，但采用了类似人类的学习策略。

Conclusion: 结果为评估基准提供了新方向，并开辟了更有效利用交互反馈的学习模型设计路径。

Abstract: Existing evaluation studies on linguistic competence of large language models
(LLM agents) have focused primarily on vocabulary learning, morphological rule
induction, syntactic generalization, pragmatic inference, and cross-linguistic
transfer. However, none assess whether LLM agents can acquire a language
through pattern recognition and interactive feedback, a central feature of
human language acquisition. We propose a novel experimental framework in which
an LLM agent is evaluated on its ability to acquire and use a newly constructed
language (Tinkatongue) in conversation with a bot that understands only
Tinkatongue. Our findings show that LLM agents fail to establish a conversation
within 100 responses, yet they adopt distinct strategies that mirror human
approaches to language learning. The results suggest a new direction for
evaluation benchmarks and open pathways to model designs that learn more
effectively from interactive feedback.

</details>


### [74] [Are Humans as Brittle as Large Language Models?](https://arxiv.org/abs/2509.07869)
*Jiahui Li,Sean Papay,Roman Klinger*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLM）输出不稳定性的原因，并与人类标注者对指令修改的敏感性进行了对比。研究发现，人类和LLM在某些类型的提示修改下都表现出脆性，但人类对拼写错误和标签顺序反转的敏感性较低。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索LLM的提示脆性是否与人类标注行为类似，从而判断这种脆性是否应被视为问题。

Method: 通过系统性比较提示修改对LLM和人类标注者的影响，重点关注两者对提示扰动的敏感性。

Result: 人类和LLM在特定提示修改（如标签集或格式替换）下均表现出脆性，但人类对拼写错误和标签顺序反转的干扰更小。

Conclusion: LLM的提示脆性部分反映了人类标注的变异性，但也存在独特的不稳定性，需在应用中综合考虑。

Abstract: The output of large language models (LLM) is unstable, due to both
non-determinism of the decoding process as well as to prompt brittleness. While
the intrinsic non-determinism of LLM generation may mimic existing uncertainty
in human annotations through distributional shifts in outputs, it is largely
assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.
This raises the question: do human annotators show similar sensitivity to
instruction changes? If so, should prompt brittleness in LLMs be considered
problematic? One may alternatively hypothesize that prompt brittleness
correctly reflects human annotation variances. To fill this research gap, we
systematically compare the effects of prompt modifications on LLMs and
identical instruction modifications for human annotators, focusing on the
question of whether humans are similarly sensitive to prompt perturbations. To
study this, we prompt both humans and LLMs for a set of text classification
tasks conditioned on prompt variations. Our findings indicate that both humans
and LLMs exhibit increased brittleness in response to specific types of prompt
modifications, particularly those involving the substitution of alternative
label sets or label formats. However, the distribution of human judgments is
less affected by typographical errors and reversed label order than that of
LLMs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [75] [From Passive to Participatory: How Liberating Structures Can Revolutionize Our Conferences](https://arxiv.org/abs/2509.07046)
*Daniel Russo,Margaret-Anne Storey*

Main category: cs.CY

TL;DR: 本文主张将学术会议从被动展示转向互动参与式，通过引入解放结构（Liberating Structures）和技术分轨，以提升会议质量。


<details>
  <summary>Details</summary>
Motivation: 当前学术会议面临投稿激增、评审负担加重和互动减少的危机，AI加剧了这一问题。

Method: 提出使用解放结构促进合作，并分两轨会议设计：一轨生成新想法，另一轨讨论已确立工作。

Result: 通过分轨和互动形式，会议能更注重质量而非数量。

Conclusion: 这一变革可确保会议在AI时代仍是真正洞察与创新的平台。

Abstract: Our conferences face a growing crisis: an overwhelming flood of submissions,
increased reviewing burdens, and diminished opportunities for meaningful
engagement. With AI making paper generation easier than ever, we must ask
whether the current model fosters real innovation or simply incentivizes more
publications. This article advocates for a shift from passive paper
presentations to interactive, participatory formats. We propose Liberating
Structures, facilitation techniques that promote collaboration and deeper
intellectual exchange. By restructuring conferences into two tracks, one for
generating new ideas and another for discussing established work, we can
prioritize quality over quantity and reinvigorate academic gatherings.
Embracing this change will ensure conferences remain spaces for real insight,
creativity, and impactful collaboration in the AI era.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [76] [Autonomous Code Evolution Meets NP-Completeness](https://arxiv.org/abs/2509.07367)
*Cunxi Yu,Rongjian Liang,Chia-Tung Ho,Haoxing Ren*

Main category: cs.AI

TL;DR: 介绍了SATLUTION框架，首次将基于LLM的代码进化扩展到整个代码库规模，成功应用于布尔可满足性问题（SAT），并超越了人类设计的SAT竞赛冠军。


<details>
  <summary>Details</summary>
Motivation: 受到AlphaEvolve的启发，希望扩展LLM代理在代码进化中的应用范围，从几百行代码的内核到整个代码库规模。

Method: SATLUTION框架通过严格正确性保证和分布式运行时反馈，协调LLM代理直接演化SAT求解器代码库。

Result: SATLUTION演化的求解器在SAT竞赛2024和2025基准测试中超越了人类设计的冠军。

Conclusion: SATLUTION展示了LLM在复杂代码库规模上的强大编码和自我进化能力，为自动代码优化提供了新方向。

Abstract: Large language models (LLMs) have recently shown strong coding abilities,
enabling not only static code generation but also iterative code self-evolving
through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve}
demonstrated that LLM-based coding agents can autonomously improve algorithms
and surpass human experts, with scopes limited to isolated kernels spanning
hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the
first framework to extend LLM-based code evolution to the full repository
scale, encompassing hundreds of files and tens of thousands of lines of C/C++
code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem
and a cornerstone of both theory and applications. SATLUTION orchestrates LLM
agents to directly evolve solver repositories under strict correctness
guarantees and distributed runtime feedback, while simultaneously self-evolving
its own evolution policies and rules. Starting from SAT Competition 2024
codebases and benchmark, SATLUTION evolved solvers that decisively outperformed
the human-designed winners of the SAT Competition 2025, and also surpassed both
2024 and 2025 champions on the 2024 benchmarks.

</details>


### [77] [HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring](https://arxiv.org/abs/2509.07260)
*Xin Wang,Ting Dang,Xinyu Zhang,Vassilis Kostakos,Michael J. Witbrock,Hong Jia*

Main category: cs.AI

TL;DR: 这篇论文探讨了在移动和可穿戴设备上使用小型语言模型（SLMs）进行健康预测的潜力，与大型语言模型（LLMs）相比，SLMs在效率和隐私方面具有优势，但也存在类别不平衡和少量样本场景的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于基于云的LLMs在医疗健康预测中虽然表现优异，但存在隐私、内存和延迟问题，因此研究轻量级的SLMs在本地设备上的应用具有重要意义。

Method: 通过零样本、少量样本和指令微调方法系统评估SLMs在健康预测任务中的表现，并将表现最佳微调后的SLMs部署到移动设备上测试实际效率和预测性能。

Result: SLMs在性能上可与LLMs媲美，同时在效率和隐私方面有明显优势，但在类别不平衡和少量样本场景中仍存在挑战。

Conclusion: 尽管SLMs目前尚不完美，但其作为一种隐私保护的下一代医疗健康监测解决方案具有广阔前景。

Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating
timely interventions, managing chronic health conditions, and ultimately
improving individuals' quality of life. Previous studies on large language
models (LLMs) have highlighted their impressive generalization abilities and
effectiveness in healthcare prediction tasks. However, most LLM-based
healthcare solutions are cloud-based, which raises significant privacy concerns
and results in increased memory usage and latency. To address these challenges,
there is growing interest in compact models, Small Language Models (SLMs),
which are lightweight and designed to run locally and efficiently on mobile and
wearable devices. Nevertheless, how well these models perform in healthcare
prediction remains largely unexplored. We systematically evaluated SLMs on
health prediction tasks using zero-shot, few-shot, and instruction fine-tuning
approaches, and deployed the best performing fine-tuned SLMs on mobile devices
to evaluate their real-world efficiency and predictive performance in practical
healthcare scenarios. Our results show that SLMs can achieve performance
comparable to LLMs while offering substantial gains in efficiency and privacy.
However, challenges remain, particularly in handling class imbalance and
few-shot scenarios. These findings highlight SLMs, though imperfect in their
current form, as a promising solution for next-generation, privacy-preserving
healthcare monitoring.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [78] [On-chip microwave sensing of quasiparticles in tantalum superconducting circuits on silicon for scalable quantum technologies](https://arxiv.org/abs/2509.07669)
*Shima Poorgholam-Khanjari,Paniz Foshat,Mingqi Zhang,Valentino Seferai,Martin Weides,Kaveh Delfanazari*

Main category: quant-ph

TL;DR: 摘要研究了超导量子电路中非平衡准粒子的影响，提出了一种评估准粒子密度的方法，并发现α-Ta的准粒子密度低于NbN，从而降低微波损耗。


<details>
  <summary>Details</summary>
Motivation: 非平衡准粒子限制了超导量子电路的性能和可扩展性，影响谐振器品质因数和量子比特的相干时间，因此需要理解和减少这些激发态。

Method: 研究中通过高Qα-Ta共面波导谐振器在单光子状态下进行微波传感，测量温度依赖性以揭示非平衡准粒子的存在。

Result: 发现α-Ta中的准粒子密度约为NbN的三分之一，直接关联到微波损耗的减少。

Conclusion: 该方法为研究准粒子动力学提供了可扩展平台，并为设计具有更高相干性的超导电路指出了新途径。

Abstract: The performance and scalability of superconducting quantum circuits are
fundamentally constrained by non-equilibrium quasiparticles, which induce
microwave losses that limit resonator quality factors and qubit coherence
times. Understanding and mitigating these excitations is therefore central to
advancing scalable quantum technologies. Here, we demonstrate on-chip microwave
sensing of quasiparticles in high-Q {\alpha}-tantalum coplanar waveguide
resonators on silicon, operated in the single-photon regime.
Temperature-dependent measurements reveal persistent non-equilibrium
quasiparticles at millikelvin temperatures, producing a measurable suppression
of the internal quality factor (Qi) relative to theoretical expectations. By
benchmarking across materials, we find that the quasiparticle density in
{\alpha}-Ta is approximately one-third that of NbN at equivalent normalised
temperatures (T/Tc), directly correlating with reduced microwave loss. Our
methodology establishes a scalable platform for probing quasiparticle dynamics
and points towards new routes for engineering superconducting circuits with
improved coherence, with impact on qubit readout resonators, kinetic-inductance
detectors, and emerging quantum processors and sensors.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [79] [Timing the Message: Language-Based Notifications for Time-Critical Assistive Settings](https://arxiv.org/abs/2509.07438)
*Ya-Chuan Hsu,Jonathan DeCastro,Andrew Silva,Guy Rosman*

Main category: cs.RO

TL;DR: 该论文研究了在时间关键场景（如辅助驾驶）中，语言辅助系统在及时性与信息量之间的权衡，通过强化学习和离线数据集设计了一个优化框架，显著提高了成功率。


<details>
  <summary>Details</summary>
Motivation: 现有辅助系统（如警报或触觉信号）缺乏上下文指导，依赖用户独立决策，导致潜在延迟或歧义。语言辅助系统虽能提供更丰富的指导，但忽视了时间因素（如传达和理解延迟），这在时间关键场景中尤为关键。

Method: 论文将问题建模为增强状态马尔可夫决策过程，结合强化学习和离线生成的分类数据集，设计了一个优化框架，平衡及时性与信息量。

Result: 通过合成人类实验，该框架比忽略时间延迟的方法提高了40%以上的成功率，有效平衡了两者，并揭示了这一常被忽视的权衡关系。

Conclusion: 研究为时间关键的人机辅助通信优化开辟了新方向，强调了及时性与信息量平衡的重要性。

Abstract: In time-critical settings such as assistive driving, assistants often rely on
alerts or haptic signals to prompt rapid human attention, but these cues
usually leave humans to interpret situations and decide responses
independently, introducing potential delays or ambiguity in meaning.
Language-based assistive systems can instead provide instructions backed by
context, offering more informative guidance. However, current approaches (e.g.,
social assistive robots) largely prioritize content generation while
overlooking critical timing factors such as verbal conveyance duration, human
comprehension delays, and subsequent follow-through duration. These timing
considerations are crucial in time-critical settings, where even minor delays
can substantially affect outcomes. We aim to study this inherent trade-off
between timeliness and informativeness by framing the challenge as a sequential
decision-making problem using an augmented-state Markov Decision Process. We
design a framework combining reinforcement learning and a generated offline
taxonomy dataset, where we balance the trade-off while enabling a scalable
taxonomy dataset generation pipeline. Empirical evaluation with synthetic
humans shows our framework improves success rates by over 40% compared to
methods that ignore time delays, while effectively balancing timeliness and
informativeness. It also exposes an often-overlooked trade-off between these
two factors, opening new directions for optimizing communication in
time-critical human-AI assistance.

</details>


### [80] [Temporal Counterfactual Explanations of Behaviour Tree Decisions](https://arxiv.org/abs/2509.07674)
*Tamlin Love,Antonio Andriella,Guillem Alenyà*

Main category: cs.RO

TL;DR: 提出了一种新方法，通过行为树结构和领域知识自动生成反事实解释，增强机器人决策的可解释性。


<details>
  <summary>Details</summary>
Motivation: 解释机器人行为对提高透明度和信任至关重要，现有方法无法生成因果关系的反事实解释。

Method: 从行为树结构和状态知识构建因果模型，查询并搜索生成多样化的反事实解释。

Result: 该方法能正确解释多种行为树结构和状态的行为，提升机器人系统的可理解性。

Conclusion: 该方法为构建更透明、可信的机器人系统提供了重要一步。

Abstract: Explainability is a critical tool in helping stakeholders understand robots.
In particular, the ability for robots to explain why they have made a
particular decision or behaved in a certain way is useful in this regard.
Behaviour trees are a popular framework for controlling the decision-making of
robots and other software systems, and thus a natural question to ask is
whether or not a system driven by a behaviour tree is capable of answering
"why" questions. While explainability for behaviour trees has seen some prior
attention, no existing methods are capable of generating causal, counterfactual
explanations which detail the reasons for robot decisions and behaviour.
Therefore, in this work, we introduce a novel approach which automatically
generates counterfactual explanations in response to contrastive "why"
questions. Our method achieves this by first automatically building a causal
model from the structure of the behaviour tree as well as domain knowledge
about the state and individual behaviour tree nodes. The resultant causal model
is then queried and searched to find a set of diverse counterfactual
explanations. We demonstrate that our approach is able to correctly explain the
behaviour of a wide range of behaviour tree structures and states. By being
able to answer a wide range of causal queries, our approach represents a step
towards more transparent, understandable and ultimately trustworthy robotic
systems.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [81] [ReShape: a Collaborative Art Experience](https://arxiv.org/abs/2509.07643)
*Hugo Parlier,Bruno Teheux*

Main category: math.HO

TL;DR: ReShape项目是一个基于数学启发的众包艺术计划。


<details>
  <summary>Details</summary>
Motivation: 结合数学与艺术，探索众包模式在艺术创作中的应用。

Method: 设计和实施了一个名为ReShape的众包艺术项目。

Result: 成功创建了一个由数学驱动的艺术计划。

Conclusion: 该项目展示了数学与艺术结合的潜力以及众包模式的创新性。

Abstract: This article describes a project called ReShape in which we created and
designed a crowdsourced art initiative, inspired and powered by mathematics.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [82] [Influence Maximization Considering Influence, Cost and Time](https://arxiv.org/abs/2509.07625)
*Mingyang Feng,Qi Zhao,Shan He,Yuhui Shi*

Main category: cs.SI

TL;DR: 本文提出了一种新的多目标影响力最大化问题，同时优化影响力、成本和时间，并通过提出的EVEA算法在真实网络中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了影响力传播、成本效率和时效性的相互作用，而这些因素在病毒营销和信息活动等实际场景中至关重要。

Method: 提出多目标影响力最大化问题，并开发进化变长搜索算法（EVEA），有效搜索最优节点组合。

Result: EVEA算法在四个真实网络中表现优于基线，影响力、成本和时间的帕累托前沿多样且平衡。

Conclusion: 多目标优化在实际应用中是可行且必要的，EVEA算法为解决此类问题提供了有效工具。

Abstract: Influence maximization has been studied for social network analysis, such as
viral marketing (advertising), rumor prevention, and opinion leader
identification. However, most studies neglect the interplay between influence
spread, cost efficiency, and temporal urgency. In practical scenarios such as
viral marketing and information campaigns, jointly optimizing Influence, Cost,
and Time is essential, yet remaining largely unaddressed in current literature.
To bridge the gap, this paper proposes a new multi-objective influence
maximization problem that simultaneously optimizes influence, cost, and time.
We show the intuitive and empirical evidence to prove the feasibility and
necessity of this multi-objective problem. We also develop an evolutionary
variable-length search algorithm that can effectively search for optimal node
combinations. The proposed EVEA algorithm outperforms all baselines, achieving
up to 19.3% higher hypervolume and 25 to 40% faster convergence across four
real-world networks, while maintaining a diverse and balanced Pareto front
among influence, cost, and time objectives.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [83] [Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry](https://arxiv.org/abs/2509.07130)
*Soruya Saha,Md Nurul Absur,Saptarshi Debroy*

Main category: cs.CV

TL;DR: 该论文提出了一种无监督、无标签的检测与恢复机制，用于对抗边缘服务器上的视觉惯性里程计（VIO）姿态欺骗攻击，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前将VIO卸载到边缘服务器的趋势可能导致服务器端的威胁，尤其是姿态欺骗攻击可能累积为显著漂移，同时规避启发式检查。论文旨在研究此威胁并提供解决方案。

Method: 提出了一种无监督模型，通过在无攻击会话中训练学习运动的时间规律性，检测运行时偏差并启动恢复以保持姿态一致性。

Result: 在ILLIXR测试平台上通过多种欺骗强度评估，实验结果显示轨迹和姿态误差相比无防御基线显著降低。

Conclusion: 该无监督检测与恢复机制有效减少了VIO卸载环境中的姿态欺骗威胁，提升了系统的可靠性和安全性。

Abstract: Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by
fusing camera and Inertial Measurement Unit (IMU) data for real-time pose.
However, current trend of offloading VIO to edge servers can lead server-side
threat surface where subtle pose spoofing can accumulate into substantial
drift, while evading heuristic checks. In this paper, we study this threat and
present an unsupervised, label-free detection and recovery mechanism. The
proposed model is trained on attack-free sessions to learn temporal
regularities of motion to detect runtime deviations and initiate recovery to
restore pose consistency. We evaluate the approach in a realistic offloaded-VIO
environment using ILLIXR testbed across multiple spoofing intensities.
Experimental results in terms of well-known performance metrics show
substantial reductions in trajectory and pose error compared to a no-defense
baseline.

</details>


### [84] [Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models](https://arxiv.org/abs/2509.07010)
*Ahmed R. Sadik,Mariusz Bujny*

Main category: cs.CV

TL;DR: 本文提出了一种基于人机交互的定量评估框架，用于评估大型语言模型生成的3D模型的几何和结构保真度，并通过多模态输入验证其性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估多模态输入的LLM生成3D模型几何和结构保真度的稳健方法，推动了本研究。

Method: 提出了一套全面的相似性和复杂性指标（如体积精度、表面对齐、尺寸保真度和拓扑复杂性），并以L形支架为案例，比较了四种输入模态的性能。

Result: 代码级提示在所有指标上实现了完美重建，表明语义丰富性的提升能够提高生成保真度。

Conclusion: 该定量评估方法显著加快了向真实数据的收敛，为生成模型在CAD应用中的验证和优化提供了可扩展的方法。

Abstract: Large Language Models are increasingly capable of interpreting multimodal
inputs to generate complex 3D shapes, yet robust methods to evaluate geometric
and structural fidelity remain underdeveloped. This paper introduces a human in
the loop framework for the quantitative evaluation of LLM generated 3D models,
supporting applications such as democratization of CAD design, reverse
engineering of legacy designs, and rapid prototyping. We propose a
comprehensive suite of similarity and complexity metrics, including volumetric
accuracy, surface alignment, dimensional fidelity, and topological intricacy,
to benchmark generated models against ground truth CAD references. Using an L
bracket component as a case study, we systematically compare LLM performance
across four input modalities: 2D orthographic views, isometric sketches,
geometric structure trees, and code based correction prompts. Our findings
demonstrate improved generation fidelity with increased semantic richness, with
code level prompts achieving perfect reconstruction across all metrics. A key
contribution of this work is demonstrating that our proposed quantitative
evaluation approach enables significantly faster convergence toward the ground
truth, especially compared to traditional qualitative methods based solely on
visual inspection and human intuition. This work not only advances the
understanding of AI assisted shape synthesis but also provides a scalable
methodology to validate and refine generative models for diverse CAD
applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [85] [Prototype: A Keyword Spotting-Based Intelligent Audio SoC for IoT](https://arxiv.org/abs/2509.06964)
*Huihong Liang,Dongxuan Jia,Youquan Wang,Longtao Huang,Shida Zhong,Luping Xiang,Lei Huang,Tao Yuan*

Main category: cs.SD

TL;DR: 这篇论文展示了一种集成了关键词检测加速器的智能音频SoC，通过算法-硬件协同设计优化能效，并展示了其低延迟、低功耗和低成本的优势。


<details>
  <summary>Details</summary>
Motivation: 为物联网设备提供高效、低成本的实时语音交互能力。

Method: 通过算法-硬件协同设计集成关键词检测加速器，优化能效并实现低延迟和低功耗。

Result: FPGA原型展示了稳定的性能和实时语音交互能力。

Conclusion: 该系统适用于边缘智能应用，具有高效能和低成本的优势。

Abstract: In this demo, we present a compact intelligent audio system-on-chip (SoC)
integrated with a keyword spotting accelerator, enabling ultra-low latency,
low-power, and low-cost voice interaction in Internet of Things (IoT) devices.
Through algorithm-hardware co-design, the system's energy efficiency is
maximized. We demonstrate the system's capabilities through a live FPGA-based
prototype, showcasing stable performance and real-time voice interaction for
edge intelligence applications.

</details>
