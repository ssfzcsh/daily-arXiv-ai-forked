<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.HC](#cs.HC) [Total: 13]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 4]
- [eess.SP](#eess.SP) [Total: 1]
- [math.AT](#math.AT) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation](https://arxiv.org/abs/2506.19045)
*Ahmadreza Saboor Yaraghi,Golnaz Gharachorlu,Sakina Fatima,Lionel C. Briand,Ruiyuan Wan,Ruifeng Gao*

Main category: cs.SE

TL;DR: 提出了一种基于LLM的静态系统测试代码故障定位（TCFL）方法，无需执行测试用例，通过修剪执行轨迹和错误消息提示LLM，显著提高了效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 由于现有故障定位方法依赖重复执行，对非确定性失败或高执行成本场景不适用，且忽略了测试代码本身的故障。

Method: 提出一种完全静态的LLM驱动方法，使用三种新算法修剪执行轨迹，结合错误消息提示LLM排序潜在故障位置。

Result: 最佳修剪轨迹与实际轨迹F1分数约90%，推理时间减少34%，块级定位在Top-3命中率达81%。

Conclusion: 该方法高效且实用，适用于大型系统测试脚本，无需访问被测系统源代码。

Abstract: Fault localization (FL) is a critical step in debugging which typically
relies on repeated executions to pinpoint faulty code regions. However,
repeated executions can be impractical in the presence of non-deterministic
failures or high execution costs. While recent efforts have leveraged Large
Language Models (LLMs) to aid execution-free FL, these have primarily focused
on identifying faults in the system under test (SUT) rather than in the often
complex system test code. However, the latter is also important as, in
practice, many failures are triggered by faulty test code. To overcome these
challenges, we introduce a fully static, LLM-driven approach for system test
code fault localization (TCFL) that does not require executing the test case.
Our method uses a single failure execution log to estimate the test's execution
trace through three novel algorithms that identify only code statements likely
involved in the failure. This pruned trace, combined with the error message, is
used to prompt the LLM to rank potential faulty locations. Our black-box,
system-level approach requires no access to the SUT source code and is
applicable to large test scripts that assess full system behavior. We evaluate
our technique at function, block, and line levels using an industrial dataset
of faulty test cases not previously used in pre-training LLMs. Results show
that our best estimated trace closely match actual traces, with an F1 score of
around 90%. Additionally, pruning the complex system test code reduces the
LLM's inference time by up to 34% without any loss in FL performance. Our
results further suggest that block-level TCFL offers a practical balance,
narrowing the search space while preserving useful context, achieving an 81%
hit rate at top-3 (Hit@3).

</details>


### [2] [Dataset of Yul Contracts to Support Solidity Compiler Research](https://arxiv.org/abs/2506.19153)
*Krzysztof Fonal*

Main category: cs.SE

TL;DR: YulCode是首个公开的专注于Yul语言的智能合约数据集，包含348,840个实例，约135,013个唯一合约，为低级别智能合约分析提供了丰富资源。


<details>
  <summary>Details</summary>
Motivation: 填补当前智能合约数据集中缺乏专门针对Yul语言的空白，支持低级别智能合约研究。

Method: 通过编译已部署在以太坊主网的Solidity源代码生成Yul合约实例。

Result: 生成了直接代表真实去中心化应用的YulCode数据集。

Conclusion: YulCode为机器学习、形式验证等任务提供了新资源，推动了低级别智能合约研究的发展。

Abstract: The YulCode dataset presents a comprehensive collection of 348,840 Yul-based
smart contract instances, comprising approximately 135,013 unique contracts.
These contracts were generated through the compilation of Solidity source files
that have been deployed on the Ethereum mainnet, making the dataset directly
representative of real-world decentralized applications. YulCode provides a
rich foundation for a variety of research and development tasks, including but
not limited to machine learning applications, formal verification, optimization
analysis, and software engineering tool evaluation in the context of low-level
smart contract code. To the best of our knowledge at the time of writing,
YulCode is the first and only publicly available dataset that focuses
specifically on Yul, an intermediate language designed for the Ethereum Virtual
Machine (EVM). As such, it fills a critical gap in the current ecosystem of
smart contract datasets and opens new avenues for research and tooling aimed at
low-level contract analysis and generation.

</details>


### [3] [Generating and Understanding Tests via Path-Aware Symbolic Execution with LLMs](https://arxiv.org/abs/2506.19287)
*Yaoxuan Wu,Xiaojie Zhou,Ahmad Humayun,Muhammad Ali Gulzar,Miryung Kim*

Main category: cs.SE

TL;DR: PALM 结合了符号路径枚举和大型语言模型（LLM）辅助的测试生成，提供了一种新的测试生成方法，并通过交互式前端帮助用户理解路径覆盖和测试效果。


<details>
  <summary>Details</summary>
Motivation: 传统的符号执行在测试生成中受限于对目标代码和库函数的约束建模能力，而LLM虽能生成多样化测试输入，但缺乏系统化路径枚举能力。PALM旨在结合两者的优势。

Method: PALM 通过静态枚举程序路径（AST级分析），将其转换为可执行变体（嵌入断言指定目标路径），并利用LLM生成测试输入，避免了传统的SMT公式翻译。

Result: 用户研究显示，PALM的交互式前端帮助用户更直观地理解路径覆盖，并验证测试效果。

Conclusion: PALM 提供了一种更高效的测试生成方式，结合了符号执行和LLM的优势，并通过可视化增强了用户对测试覆盖的理解。

Abstract: Symbolic execution is a widely used technique for test generation, offering
systematic exploration of program paths through constraint solving. However, it
is fundamentally constrained by the capability to model the target code
including library functions in terms of symbolic constraint and the capability
of underlying constraint solvers. As a result, many paths involving complex
features remain unanalyzed or insufficiently modeled. Recent advances in large
language models (LLMs) have shown promise in generating diverse and valid test
inputs. Yet, LLMs lack mechanisms for systematically enumerating program paths
and often fail to cover subtle corner cases. We observe that directly prompting
an LLM with the full program leads to missed coverage of interesting paths. In
this paper, we present PALM, a test generation system that combines symbolic
path enumeration with LLM-assisted test generation. PALM statically enumerates
possible paths through AST-level analysis and transforms each into an
executable variant with embedded assertions that specify the target path. This
avoids the need to translate path constraints into SMT formulae, by instead
constructing program variants that LLM can interpret. Importantly, PALM is the
first to provide an interactive frontend that visualizes path coverage
alongside generated tests, assembling tests based on the specific paths they
exercise. A user study with 12 participants demonstrates that PALM's frontend
helps users better understand path coverage and identify which paths are
actually exercised by PALM-generated tests, through verification and
visualization of their path profiles.

</details>


### [4] [What Makes the Best Decomposition? Investigating Binary Decomposition Under FCG Variance](https://arxiv.org/abs/2506.19425)
*Ang Jia,He Jiang,Zhilei Ren,Xiaochen Li,Ming Fan,Ting Liu*

Main category: cs.SE

TL;DR: 论文研究了二进制分解中函数调用图（FCGs）因编译设置不同而产生的变异性，分析了其对现有分解方法的挑战，并提出了一种识别最优分解的方法。


<details>
  <summary>Details</summary>
Motivation: 现有二进制分解方法依赖于函数调用关系的相似性，但不同编译设置下FCGs的变异性较大，尤其是在函数内联决策多样化的情况下，影响了分解的准确性。

Method: 通过17种编译器、6种优化和4种架构构建数据集，分析FCGs的变化和映射关系，评估现有方法的性能，并提出最优分解识别方法。

Result: 研究发现FCGs的大小变化显著，但仍可通过三种映射关系关联；现有方法在跨编译器评估中面临挑战；提出的最优分解方法表现优于现有方法。

Conclusion: FCGs的变异性对二进制分解方法提出了新的挑战，最优分解方法为解决这一问题提供了新思路。

Abstract: Binary decomposition, which decomposes binary files into modules, plays a
critical role in binary reuse detection. Existing binary decomposition works
either apply anchor-based methods by extending anchor functions to generate
modules, or apply clustering-based methods by using clustering algorithms to
group binary functions, which all rely on that reused code shares similar
function call relationships. However, we find that function call graphs (FCGs)
vary a lot when using different compilation settings, especially with diverse
function inlining decisions.
  In this work, we conduct the first systematic empirical study on the variance
of FCGs compiled by various compilation settings and explore its effect on
binary decomposition methods. We first construct a dataset compiled by 17
compilers, using 6 optimizations to 4 architectures and analyze the changes and
mappings of the FCGs. We find that the size of FCGs changes dramatically, while
the FCGs are still linked by three different kinds of mappings. Then we
evaluate the existing works under the FCG variance, and results show that
existing works are facing great challenges when conducting cross-compiler
evaluation with diverse optimization settings. Finally, we propose a method to
identify the optimal decomposition and compare the existing decomposition works
with the optimal decomposition. Existing works either suffer from low coverage
or cannot generate stable community similarities.

</details>


### [5] [LLM-based Multi-Agent System for Intelligent Refactoring of Haskell Code](https://arxiv.org/abs/2506.19481)
*Shahbaz Siddeeq,Muhammad Waseem,Zeeshan Rasheed,Md Mahade Hasan,Jussi Rasku,Mika Saari,Henri Terho,Kalle Makela,Kai-Kristian Kemell,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 研究提出了一种基于大型语言模型（LLM）的多代理系统，用于自动化Haskell代码的重构，通过实验验证了其在降低代码复杂度、提升代码质量和性能效率方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 软件重构在开发与维护中至关重要，但当前过程仍依赖人工分析，效率低下且易引入缺陷，需要自动化解决方案。

Method: 设计了基于LLM的多代理系统，包含代码分析、重构执行、验证和调试等角色，并在开源Haskell代码库上进行测试。

Result: 实验结果显示，系统平均降低代码复杂度11.03%，提升代码质量22.46%，提高性能效率13.27%，并优化内存分配14.57%。

Conclusion: LLM多代理系统可有效管理函数式编程语言的重构任务，增强可维护性并支持自动化开发流程。

Abstract: Refactoring is a constant activity in software development and maintenance.
Scale and maintain software systems are based on code refactoring. However,
this process is still labor intensive, as it requires programmers to analyze
the codebases in detail to avoid introducing new defects. In this research, we
put forward a large language model (LLM)-based multi-agent system to automate
the refactoring process on Haskell code. The objective of this research is to
evaluate the effect of LLM-based agents in performing structured and
semantically accurate refactoring on Haskell code. Our proposed multi-agent
system based on specialized agents with distinct roles, including code
analysis, refactoring execution, verification, and debugging. To test the
effectiveness and practical applicability of the multi-agent system, we
conducted evaluations using different open-source Haskell codebases. The
results of the experiments carried out showed that the proposed LLM-based
multi-agent system could average 11.03% decreased complexity in code, an
improvement of 22.46% in overall code quality, and increase performance
efficiency by an average of 13.27%. Furthermore, memory allocation was
optimized by up to 14.57%. These results highlight the ability of LLM-based
multi-agent in managing refactoring tasks targeted toward functional
programming paradigms. Our findings hint that LLM-based multi-agent systems
integration into the refactoring of functional programming languages can
enhance maintainability and support automated development workflows.

</details>


### [6] [Integrating Pair Programming as a Work Practice](https://arxiv.org/abs/2506.19511)
*Nina Haugland Andersen,Anastasiia Tkalich,Nils Brede Moe,Darja Smite,Asgaut Mjølne Söderbom,Ola Hast,Viktoria Stray*

Main category: cs.SE

TL;DR: 摘要探讨了影响结对编程（PP）采用和持续参与的多重因素，并强调了实践适应性和团队集体学习的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究PP的采用和持续参与因素，因为尽管其益处明确，但在团队中的采用仍不一致。

Method: 在挪威一家成熟的敏捷公司进行案例研究，通过两轮访谈收集数据并进行主题分析。

Result: 发现PP的参与受多种因素影响，包括实践对日常工作的贡献感知、公司态度、资源等。

Conclusion: 长期参与PP需实践效益的验证，并根据团队特点调整。研究对希望将PP融入工作流的实践者有益。

Abstract: Context: Pair programming (PP) is more relevant than ever. As modern systems
grow in complexity, knowledge sharing and collaboration across teams have
become essential. However, despite well-documented benefits of PP, its adoption
remains inconsistent across software teams. Objective: This study aims to
understand the factors that facilitate or hinder team members' adoption as well
as lasting engagement in PP. Method: We have conducted an exploratory
single-case study in a mature agile company in Norway. We collected data
through two rounds of interviews with team members in different roles and
performed a thematic analysis of the interviews. Results: Our key finding is
that multiple factors, related to the perceptions of how PP contributes to
daily work, efforts associated with engaging in PP sessions, company and team
attitudes, resources, infrastructure, and task characteristics, affect PP
engagement. Conclusion: Long-term engagement in PP requires expected benefits
with the practice being confirmed in firsthand experiences. Adapting the
practice to each unique team, with insights drawn from collective learning, is
also beneficial. Our findings will be beneficial for software practitioners
seeking to make PP an integrated part of their team's workflow.

</details>


### [7] [Lost in Translation? Converting RegExes for Log Parsing into Dynatrace Pattern Language](https://arxiv.org/abs/2506.19539)
*Julian Fragner,Christian Macho,Bernhard Dieber,Martin Pinzger*

Main category: cs.SE

TL;DR: Reptile工具结合规则和GPT-4技术，将正则表达式自动转换为DPL模式，显著提升日志分析平台迁移效率。


<details>
  <summary>Details</summary>
Motivation: 解决企业从正则表达式迁移到DPL模式语言时的高成本和易出错问题。

Method: 结合规则转换和GPT-4优化，处理无法完全转换的情况。

Result: 成功转换73.7%的正则表达式，优化后F1-score和MCC超过0.91。

Conclusion: Reptile为公司迁移到现代日志分析平台提供了高效且可靠的解决方案。

Abstract: Log files provide valuable information for detecting and diagnosing problems
in enterprise software applications and data centers. Several log analytics
tools and platforms were developed to help filter and extract information from
logs, typically using regular expressions (RegExes). Recent commercial log
analytics platforms provide domain-specific languages specifically designed for
log parsing, such as Grok or the Dynatrace Pattern Language (DPL). However,
users who want to migrate to these platforms must manually convert their
RegExes into the new pattern language, which is costly and error-prone. In this
work, we present Reptile, which combines a rule-based approach for converting
RegExes into DPL patterns with a best-effort approach for cases where a full
conversion is impossible. Furthermore, it integrates GPT-4 to optimize the
obtained DPL patterns. The evaluation with 946 RegExes collected from a large
company shows that Reptile safely converted 73.7% of them. The evaluation of
Reptile's pattern optimization with 23 real-world RegExes showed an F1-score
and MCC above 0.91. These results are promising and have ample practical
implications for companies that migrate to a modern log analytics platform,
such as Dynatrace.

</details>


### [8] [Simulating the Waterfall Model: A Systematic Review](https://arxiv.org/abs/2506.19653)
*Antonios Saravanos*

Main category: cs.SE

TL;DR: 该论文通过系统映射研究分析了瀑布模型在计算模拟中的表现，发现尽管瀑布模型在当前以敏捷方法为主的软件设计中仍存在，但其模拟研究较少，且多采用离散事件模拟和开源工具。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨瀑布模型在学术文献中的模拟表现，尽管敏捷方法主导现代软件设计，瀑布模型仍被用于混合开发方法中，但其模拟研究缺乏关注。

Method: 通过结构化搜索学术数据库，筛选出68篇2000-2024年的相关研究，并从模拟方法、工具、地理与时间趋势以及对Royce原始模型的忠实度四个维度进行分析。

Result: 结果显示离散事件模拟是最常见的方法，研究中更多使用开源工具，且几乎没有研究完全遵循Royce的原始模型。

Conclusion: 研究表明瀑布模型的模拟虽小众但有学术价值，未来研究应关注更易用的建模工具及其与现代混合实践的融合。

Abstract: This systematic mapping study examines how the Waterfall Model has been
represented in computational simulations within peer-reviewed literature. While
Agile methodologies dominate contemporary software design practices, the
Waterfall Model persists, particularly, within hybrid approaches that fuse
structured, sequential workflows with the adaptability of agile practices.
Despite its continued presence, little attention has been given to how the
Waterfall Model is simulated in research contexts. A structured search of major
academic databases identified 68 peer-reviewed studies published between 2000
and 2024. After applying inclusion criteria, selected studies were analyzed
across four dimensions: (1) simulation methodologies (e.g., discrete-event
simulation, system dynamics), (2) platforms and tools (e.g., Simphony.NET,
SimPy), (3) geographic and temporal trends, and (4) fidelity to Royce's
original seven-phase model. Discrete-event simulation was most commonly used,
reflecting the model's sequential nature. Early work relied on proprietary
platforms, while recent studies increasingly use open-source, Python-based
tools. No studies fully implemented Royce's original formulation, most employed
adaptations. These findings suggest that although niche, simulation of the
Waterfall Model is present in academic discourse. This work highlights the need
for accessible modeling tools and calls for future research that integrates the
waterfall software process model with modern hybrid practices.

</details>


### [9] [Adaptive Request Scheduling for CodeLLM Serving with SLA Guarantees](https://arxiv.org/abs/2506.19677)
*Shi Chang,Boyuan Chen,Kishanthan Thangarajah,Hanan Lutfiyya,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SABER是一种动态批处理策略，通过实时调整决策提升CodeLLM在资源受限环境中的服务性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM服务系统中静态批处理配置无法适应请求速率波动或异构工作负载的问题，进而导致SLA违规和性能不稳定的挑战。

Method: 提出SABER，动态预测每个请求的SLA可行性并实时调整批处理决策。

Result: 相比最佳静态配置，SABER提升吞吐量达26%，降低延迟变异性达45%。

Conclusion: SLA感知的自适应调度是确保CodeLLM高性能服务的关键。

Abstract: Code Large Language Models (CodeLLMs) are increasingly integrated into modern
software development workflows, yet efficiently serving them in
resource-constrained, self-hosted environments remains a significant challenge.
Existing LLM serving systems employs Continuous Batching for throughput
improvement. However, they rely on static batch size configurations that cannot
adapt to fluctuating request rates or heterogeneous workloads, leading to
frequent SLA (Service Level Agreement) violations and unstable performance. In
this study, We propose SABER, a dynamic batching strategy that predicts
per-request SLA feasibility and adjusts decisions in real time. SABER improves
goodput by up to 26% over the best static configurations and reduces latency
variability by up to 45%, all without manual tuning or service restarts. Our
results demonstrate that SLA-aware, adaptive scheduling is key to robust,
high-performance CodeLLM serving.

</details>


### [10] [Exploring Developer Experience Factors in Software Ecosystems](https://arxiv.org/abs/2506.19757)
*Rodrigo Oliveira Zacarias,Léo Carvalho Ramos Antunes,Márcio de Oliveira Barros,Rodrigo Pereira dos Santos,Patricia Lago*

Main category: cs.SE

TL;DR: 该研究通过系统性映射研究和德尔菲研究，识别了影响开发者体验（DX）的关键因素，包括财务成本、技术资源、市场进入壁垒和财务收益，为促进软件生态系统的成功提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 开发者体验对软件生态系统的成功至关重要，但目前缺乏对其关键因素的清晰认识。研究旨在填补这一空白，帮助提升开发者的持续参与度。

Method: 采用系统性映射研究分析29篇文献，并结合德尔菲研究评估27个DX因素对21名第三方开发者的影响。

Result: 识别出财务成本、技术资源、低市场进入壁垒和财务收益是影响开发者采纳和贡献的关键因素。

Conclusion: 开发者体验对软件生态系统的可持续发展至关重要，研究结果为研究者与实践者提供了实用建议。

Abstract: Context: Developer experience (DX) plays a key role in developers'
performance and their continued involvement in a software ecosystem (SECO)
platform. While researchers and practitioners have recognized several factors
affecting DX in SECO platforms, a clear roadmap of the most influential factors
is still missing. This is particularly important given the direct impact on
developers' interest in SECO and their ongoing engagement with the common
technological platform. Goal: This work aims to identify key DX factors and
understand how they influence third-party developers' decisions to adopt and
keep contributing to a SECO. Methods: We conducted a systematic mapping study
(SMS), analyzing 29 studies to assess the state-of-the-art of DX in SECO.
Additionally, we conducted a Delphi study to evaluate the influence of 27 DX
factors (identified in our SMS) from the perspective of 21 third-party
developers to adopt and keep contributing to a SECO. Results: The factors that
most strongly influence developers' adoption and ongoing contributions to a
SECO are: financial costs for using the platform, desired technical resources
for development, low barriers to entry into the applications market, and more
financial gains. Conclusion: DX is essential for the success and sustainability
of SECO. Our set of DX factors provides valuable insights and recommendations
for researchers and practitioners to address key DX concerns from the
perspective of third-party developers.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Mix-of-Language-Experts Architecture for Multilingual Programming](https://arxiv.org/abs/2506.18923)
*Yifan Zong,Yuntian Deng,Pengyu Nie*

Main category: cs.PL

TL;DR: MoLE架构通过混合语言专家模块平衡多语言编程的效率与专业化，优于单模型和独立模型。


<details>
  <summary>Details</summary>
Motivation: 解决多语言编程任务中单模型效率与多模型专业化之间的权衡问题。

Method: 引入MoLE架构，包含基础模型、共享LoRA模块和语言特定LoRA模块，优化时联合训练，推理时自动路由。

Result: MoLE在参数效率和准确率上优于独立LoRA和共享LLM。

Conclusion: MoLE为多语言编程任务提供了高效且专业的解决方案。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
aiding developers with tasks like code comprehension, generation, and
translation. Supporting multilingual programming -- i.e., coding tasks across
multiple programming languages -- typically requires either (1) finetuning a
single LLM across all programming languages, which is cost-efficient but
sacrifices language-specific specialization and performance, or (2) finetuning
separate LLMs for each programming language, which allows for specialization
but is computationally expensive and storage-intensive due to the duplication
of parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel
architecture that balances efficiency and specialization for multilingual
programming. MoLE is composed of a base model, a shared LoRA (low-rank
adaptation) module, and a collection of language-specific LoRA modules. These
modules are jointly optimized during the finetuning process, enabling effective
knowledge sharing and specialization across programming languages. During
inference, MoLE automatically routes to the language-specific LoRA module
corresponding to the programming language of the code token being generated.
Our experiments demonstrate that MoLE achieves greater parameter efficiency
compared to training separate language-specific LoRAs, while outperforming a
single shared LLM finetuned for all programming languages in terms of accuracy.

</details>


### [12] [The Autonomous Data Language -- Concepts, Design and Formal Verification](https://arxiv.org/abs/2506.19457)
*Tom T. P. Franken,Thomas Neele,Jan Friso Groote*

Main category: cs.PL

TL;DR: 提出了一种新的并行编程范式——数据自主范式，强调数据元素自主计算，并通过AuDaLa语言实现，证明了其适合形式化验证。


<details>
  <summary>Details</summary>
Motivation: 现有的并行语言主要关注处理器和线程，导致数据与内存处理困难，偏离算法初衷。

Method: 提出数据自主范式，开发AuDaLa语言并提供形式化定义（类型系统和操作语义），通过示例展示其自然性。

Result: AuDaLa语言实现了数据自主协作的高度并行编程，且适合形式化验证并行程序。

Conclusion: 数据自主范式为并行编程提供了新思路，AuDaLa语言展示了其可行性和优势。

Abstract: Nowadays, the main advances in computational power are due to parallelism.
However, most parallel languages have been designed with a focus on processors
and threads. This makes dealing with data and memory in programs hard, which
distances the implementation from its original algorithm. We propose a new
paradigm for parallel programming, the data-autonomous paradigm, where
computation is performed by autonomous data elements. Programs in this paradigm
are focused on making the data collaborate in a highly parallel fashion. We
furthermore present AuDaLa, the first data autonomous programming language, and
provide a full formalisation that includes a type system and operational
semantics. Programming in AuDaLa is very natural, as illustrated by examples,
albeit in a style very different from sequential and contemporary parallel
programming. Additionally, it lends itself for the formal verification of
parallel programs, which we demonstrate.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [13] [WiLLM: An Open Wireless LLM Communication System](https://arxiv.org/abs/2506.19030)
*Boyi Liu,Yongguang Lu,Jianguo Zhao,Qiang Yang,Wen Wu,Lin Chen,Jagmohan Chauhan,Jun Zhang*

Main category: cs.NI

TL;DR: WiLLM是首个专为移动LLM服务设计的开源无线系统，通过核心网络部署LLM、创新的网络切片架构和多项新技术，解决了当前方案的局限性，并提供了数据集和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有无线基础设施难以应对LLM的快速发展，需要为移动LLM服务设计新的架构。

Method: 提出WiLLM系统，采用核心网络部署LLM、'Tree-Branch-Fruit'网络切片扩展、双层面切片架构和应用层隧道机制等技术。

Result: WiLLM实现了细粒度资源分配、旧设备兼容性和灵活部署，并发布了首个LLM无线通信数据集和基准测试。

Conclusion: WiLLM为移动LLM服务提供了开放性平台，促进了跨层优化和AI与电信的融合。

Abstract: The rapid evolution of LLMs threatens to overwhelm existing wireless
infrastructure, necessitating architectural innovations for burgeoning mobile
LLM services. This paper introduces WiLLM, the first open-source wireless
system specifically designed for these services. First, we establish a new
paradigm by deploying LLMs in core networks (CNs) with abundant GPUs. This
enables distributed inference services, strategically positioning LLM inference
at the convergence of backbone bandwidth and the cellular network's edge.
Second, we propose an innovative "Tree-Branch-Fruit" extension to the
conventional network slicing architecture. This specialized design allows
telecom operators to monetize LLM services through slice subscriptions while
maintaining infrastructure ownership. Finally, to realize this vision, WiLLM
addresses critical limitations in current solutions with several novel
capabilities. It features enhanced slice orchestration through a dual-layer
slicing architecture, enabling coordinated multi-UE-multi-slice scheduling for
finer-grained resource allocation. To ensure universal compatibility, an
application-layer tunneling mechanism allows legacy devices without native
slicing to access LLM slice services without hardware upgrades. Furthermore,
its dual-mode scheduling and cross-layer APIs support flexible deployment from
CNs to servers. Built on OpenAirInterface, WiLLM extends this established
framework, lowering the adoption barrier for researchers. We also release the
first LLM wireless communication dataset with 1,649,996 records and
synchronized 58-dimensional metrics, alongside two benchmarks. A case study
with smart glasses demonstrates practical viability for resource-constrained
devices. WiLLM aims to foster an open platform for cross-layer optimization and
AI-telecom convergence. The code, datasets, and hardware details are available
at https://openwillm.github.io.

</details>


### [14] [Enhancing Evacuation Safety: Detecting Post-Nuclear Event Radiation Levels in an Urban Area](https://arxiv.org/abs/2506.19044)
*Ellis Duncalfe,Milena Radenkovic*

Main category: cs.NI

TL;DR: 论文探讨了在核爆炸后城市中，如何利用延迟容忍网络（DTN）传播辐射数据以支持应急响应和疏散安全。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决核爆炸后通信基础设施损坏导致的数据传播难题，强调及时辐射数据对减少伤亡的重要性。

Method: 利用ONE模拟器构建伪真实的动态核爆炸后场景，集成应急人员、无人机和民用设备的辐射传感数据作为DTN节点，评估Epidemic和PRoPHET两种路由协议的效率。

Result: 两种协议均实现高消息送达率，PRoPHET网络开销较低但延迟较高。

Conclusion: DTN解决方案能在基础设施严重损坏时有效传播关键辐射数据，支持应急响应和疏散安全。

Abstract: The detonation of an improvised nuclear device (IND) in an urban area would
cause catastrophic damage, followed by hazardous radioactive fallout. Timely
dissemination of radiation data is crucial for evacuation and casualty
reduction. However, conventional communication infrastructure is likely to be
severely disrupted. This study designs and builds a pseudorealistic,
geospatially and temporally dynamic post-nuclear event (PNE) scenario using the
Opportunistic Network Environment (ONE) simulator. It integrates radiation
sensing by emergency responders, unmanned aerial vehicles (UAVs), and civilian
devices as dynamic nodes within Delay-Tolerant Networks (DTNs). The performance
of two DTN routing protocols, Epidemic and PRoPHET, was evaluated across
multiple PNE phases. Both protocols achieve high message delivery rates, with
PRoPHET exhibiting lower network overhead but higher latency. Findings
demonstrate the potential of DTN-based solutions to support emergency response
and evacuation safety by ensuring critical radiation data propagation despite
severe infrastructure damage.

</details>


### [15] [A Study on E2E Performance Improvement of Platooning Using Outdoor LiFi](https://arxiv.org/abs/2506.19304)
*Zhiyi Zhu,Eiji Takimoto,Patrick Finnerty,Chikara Ohta*

Main category: cs.NI

TL;DR: 提出了一种基于LiFi和C-V2X的混合通信架构，以减少自动驾驶车队中的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 传统C-V2X架构在自动驾驶车队长度增加时会导致端到端延迟增加，影响通信效率。

Method: 通过模拟整合LiFi和C-V2X，提出多领导者车辆的混合架构，利用LiFi实现高速低延迟通信。

Result: 该架构减少了端到端延迟，提升了通信效率。

Conclusion: 混合LiFi和C-V2X的架构有效解决了自动驾驶车队中的通信延迟问题。

Abstract: Platooning within autonomous vehicles has proven effective in addressing
driver shortages and reducing fuel consumption. However, as platooning lengths
increase, traditional C-V2X (cellular vehicle-to-everything) architectures are
susceptible to end-to-end (E2E) latency increases. This is due to the necessity
of relaying information through multiple hops from the leader vehicle to the
last vehicle. To address this problem, this paper proposes a hybrid
communication architecture based on a simulation that integrates light fidelity
(LiFi) and C-V2X. The proposed architecture introduces multiple-leader vehicles
equipped with outdoor LiFi communication nodes in platoons to achieve
high-speed and low-delay communication between leader vehicles, which reduces
E2E delay.

</details>


### [16] [Fractality of Wireless Mesh Networks: Dimensional Effects on Network Performance](https://arxiv.org/abs/2506.19366)
*Marat Zaidyn,Sayat Akhtanov,Dana Turlykozhayeva,Symbat Temesheva,Almat Akhmetali,Alisher Skabylov,Nurzhan Ussipov*

Main category: cs.NI

TL;DR: 论文提出了一种基于分形几何的无线网状网络（WMN）拓扑构建算法，能够通过调整分形维度精确控制空间自相似性。通过仿真验证，高分形维度的拓扑在同等条件下表现出更高的鲁棒性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统的网络模型通常假设节点均匀或随机分布，无法真实反映实际部署中复杂的层次空间模式。本研究旨在填补这一空白，探索分形几何在WMN设计中的应用潜力。

Method: 提出了一种新颖算法，通过系统调整分形维度生成具有不同空间复杂度的网络布局。使用NS-3仿真评估了吞吐量、延迟、抖动和分组交付率等关键性能指标。

Result: 实验表明，高分形维度的拓扑在相同条件下优于经典的随机、小世界和无标度网络模型，表现出更强的鲁棒性和更高的吞吐量。

Conclusion: 分形几何为设计可扩展且高效的WMN架构提供了新的范式，验证了其在网络性能优化中的潜力。

Abstract: Wireless mesh networks (WMNs) depend on the spatial distribution of nodes,
which directly influences connectivity, routing efficiency, and overall network
performance. Conventional models typically assume uniform or random node
placement, which inadequately represent the complex, hierarchical spatial
patterns observed in practical deployments. In this study, we present a novel
algorithm that constructs WMN topologies with tunable fractal dimensions,
allowing precise control over spatial self-similarity. By systematically
varying the fractal dimension, the algorithm generates network layouts spanning
a continuum of spatial complexities, ranging from sparse fragmented clusters to
dense, cohesive structures. Through NS-3 simulations, Key performance metrics
including throughput, latency, jitter, and packet delivery ratio were evaluated
across a range of fractal dimensions. Comparative evaluations against classical
random, small-world, and scale-free network models reveal that high-dimensional
fractal topologies achieve enhanced resilience and throughput under equivalent
conditions. These findings demonstrate the potential of fractal geometry as a
design paradigm for scalable and efficient WMN architectures.

</details>


### [17] [CORMO-RAN: Lossless Migration of xApps in O-RAN](https://arxiv.org/abs/2506.19760)
*Antonio Calagna,Stefano Maxenti,Leonardo Bonati,Salvatore D'Oro,Tommaso Melodia,Carla Fabiana Chiasserini*

Main category: cs.NI

TL;DR: CORMO-RAN是一种动态管理计算资源的编排器，用于在低流量时期节省能源，同时确保xApps的可用性和性能。


<details>
  <summary>Details</summary>
Motivation: 在密集区域的低流量时期，RAN的计算资源可能未充分利用，导致能源浪费。CORMO-RAN旨在解决这一问题。

Method: 通过数据驱动的编排器动态激活计算节点，支持无损迁移xApps，并考虑状态大小和时序约束的多样性。

Result: 在O-RAN 5G测试平台上，CORMO-RAN节能效率高达64%，优于现有方法。

Conclusion: CORMO-RAN能有效平衡服务可用性、扩展性和能源消耗，适用于动态RAN环境。

Abstract: Open Radio Access Network (RAN) is a key paradigm to attain unprecedented
flexibility of the RAN via disaggregation and Artificial Intelligence
(AI)-based applications called xApps. In dense areas with many active RAN
nodes, compute resources are engineered to support potentially hundreds of
xApps monitoring and controlling the RAN to achieve operator's intents.
However, such resources might become underutilized during low-traffic periods,
where most cells are sleeping and, given the reduced RAN complexity, only a few
xApps are needed for its control. In this paper, we propose CORMO-RAN, a
data-driven orchestrator that dynamically activates compute nodes based on xApp
load to save energy, and performs lossless migration of xApps from nodes to be
turned off to active ones while ensuring xApp availability during migration.
CORMO-RAN tackles the trade-off among service availability, scalability, and
energy consumption while (i) preserving xApps' internal state to prevent RAN
performance degradation during migration; (ii) accounting for xApp diversity in
state size and timing constraints; and (iii) implementing several migration
strategies and providing guidelines on best strategies to use based on resource
availability and requirements. We prototype CORMO-RAN as an rApp, and
experimentally evaluate it on an O-RAN private 5G testbed hosted on a Red Hat
OpenShift cluster with commercial radio units. Results demonstrate that
CORMO-RAN is effective in minimizing energy consumption of the RAN Intelligent
Controller (RIC) cluster, yielding up to 64% energy saving when compared to
existing approaches.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [18] [A Survey of Multi-sensor Fusion Perception for Embodied AI: Background, Methods, Challenges and Prospects](https://arxiv.org/abs/2506.19769)
*Shulan Ruan,Rongwei Wang,Xuchen Shen,Huijie Liu,Baihui Xiao,Jun Shi,Kun Zhang,Zhenya Huang,Yu Liu,Enhong Chen,You He*

Main category: cs.MM

TL;DR: 本文提出一种任务无关的多传感器融合感知（MSFP）综述，涵盖多模态、多智能体、时间序列和多模态LLM融合方法，弥补现有综述的局限性，并为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 现有MSFP综述多为单任务或单视角，限制了跨领域研究的直接受益。本文旨在从任务无关和多技术视角组织MSFP研究，推动更广泛的应用和发展。

Method: 1. 介绍MSFP背景；2. 综述多模态与多智能体融合方法；3. 分析时间序列融合方法；4. 探讨多模态LLM融合方法；5. 讨论开放挑战与未来方向。

Result: 本文系统梳理了MSFP的多样化技术路径，填补了现有综述的不足，并为跨领域研究提供统一框架。

Conclusion: 本文为MSFP研究提供了全面且任务无关的视角，有助于推动未来技术进步和应用拓展。

Abstract: Multi-sensor fusion perception (MSFP) is a key technology for embodied AI,
which can serve a variety of downstream tasks (e.g., 3D object detection and
semantic segmentation) and application scenarios (e.g., autonomous driving and
swarm robotics). Recently, impressive achievements on AI-based MSFP methods
have been reviewed in relevant surveys. However, we observe that the existing
surveys have some limitations after a rigorous and detailed investigation. For
one thing, most surveys are oriented to a single task or research field, such
as 3D object detection or autonomous driving. Therefore, researchers in other
related tasks often find it difficult to benefit directly. For another, most
surveys only introduce MSFP from a single perspective of multi-modal fusion,
while lacking consideration of the diversity of MSFP methods, such as
multi-view fusion and time-series fusion. To this end, in this paper, we hope
to organize MSFP research from a task-agnostic perspective, where methods are
reported from various technical views. Specifically, we first introduce the
background of MSFP. Next, we review multi-modal and multi-agent fusion methods.
A step further, time-series fusion methods are analyzed. In the era of LLM, we
also investigate multimodal LLM fusion methods. Finally, we discuss open
challenges and future directions for MSFP. We hope this survey can help
researchers understand the important progress in MSFP and provide possible
insights for future research.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [19] [FO-Query Enumeration over SLP-Compressed Structures of Bounded Degree](https://arxiv.org/abs/2506.19421)
*Markus Lohrey,Sebastian Maneth,Markus L. Schmid*

Main category: cs.LO

TL;DR: 该论文扩展了在有界度关系结构上进行一阶查询的结果集枚举方法，支持通过SLP压缩形式表示的结构，并在预处理时间与SLP大小成线性关系的情况下实现恒定延迟。


<details>
  <summary>Details</summary>
Motivation: 研究如何在压缩形式（SLP）表示的有界度关系结构上高效枚举一阶查询结果集。

Method: 提出一种算法，针对满足顶点条件的SLP表示的结构，实现恒定延迟和线性预处理时间的枚举。

Result: 算法在固定公式下，预处理时间与SLP大小成线性关系，且枚举延迟恒定。

Conclusion: 该工作为压缩结构上的查询处理提供了高效解决方案。

Abstract: Enumerating the result set of a first-order query over a relational structure
of bounded degree can be done with linear preprocessing and constant delay. In
this work, we extend this result towards the compressed perspective where the
structure is given in a potentially highly compressed form by a straight-line
program (SLP). Our main result is an algorithm that enumerates the result set
of a first-order query over a structure of bounded degree that is represented
by an SLP satisfying the so-called apex condition. For a fixed formula, the
enumeration algorithm has constant delay and needs a preprocessing time that is
linear in the size of the SLP.

</details>


### [20] [Time-Sensitive Importance Splitting](https://arxiv.org/abs/2506.19568)
*Gabriel Dengler,Carlos E. Budde,Laura Carnevali,Arnd Hartmanns*

Main category: cs.LO

TL;DR: 提出了一个基于时间敏感重要性函数的重要性分裂方法，用于非马尔可夫模型的罕见事件模拟，解决了现有方法的局限性，并通过实验验证了其潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在需要系统时间行为信息时面临实际或理论限制，本文旨在通过扩展重要性分裂方法来解决这些问题。

Method: 使用时间敏感重要性函数进行反向可达性搜索，考虑计时器的上下界，以引导路径生成至罕见事件。

Result: 开发了原型实现，并通过可靠性工程的例子展示了其估计罕见事件概率的潜力。

Conclusion: 该方法为非马尔可夫模型的罕见事件模拟提供了新的解决方案，初步实验证明其有效性。

Abstract: State-of-the-art methods for rare event simulation of non-Markovian models
face practical or theoretical limits if observing the event of interest
requires prior knowledge or information on the timed behavior of the system. In
this paper, we attack both limits by extending importance splitting with a
time-sensitive importance function. To this end, we perform backwards
reachability search from the target states, considering information about the
lower and upper bounds of the active timers in order to steer the generation of
paths towards the rare event. We have developed a prototype implementation of
the approach for input/output stochastic automata within the Modest Toolset.
Preliminary experiments show the potential of the approach in estimating rare
event probabilities for an example from reliability engineering.

</details>


### [21] [Homomorphism Indistinguishability and Game Comonads for Restricted Conjunction and Requantification](https://arxiv.org/abs/2506.19746)
*Georg Schindling*

Main category: cs.LO

TL;DR: 该论文探讨了同态不可区分性的框架，扩展了有限变量计数逻辑中限制重定量的逻辑，引入了可重用性的路径和树分解方法，并通过博弈和范畴理论统一了逻辑、组合和范畴视角。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过同态不可区分性框架，揭示图类的组合性质与逻辑片段结构之间的联系，并扩展有限模型理论中受限逻辑的层级分。

Method: 方法包括引入可重用性的路径和树分解、开发基于博弈的刻画、构造新的伴随函子以封装受限重定量游戏，以及结合范畴理论分析图分解。

Result: 结果表明，具有可重用性约束的路径宽度和树宽度类是同态区分封闭的，且伴随函子的余代数与路径/树分解具有紧密对应关系。

Conclusion: 结论将可重用性概念在逻辑、组合和范畴视角下统一，为图分解和有限模型理论提供了新的理论工具和联系。

Abstract: The notion of homomorphism indistinguishability offers a combinatorial
framework for characterizing equivalence relations of graphs, in particular
equivalences in counting logics within finite model theory. That is, for
certain graph classes, two structures agree on all homomorphism counts from the
class if and only if they satisfy the same sentences in a corresponding logic.
This perspective often reveals connections between the combinatorial properties
of graph classes and the syntactic structure of logical fragments. In this
work, we extend this perspective to logics with restricted requantification,
refining the stratification of logical resources in finite-variable counting
logics. Specifically, we generalize Lov\'asz-type theorems for these logics
with either restricted conjunction or bounded quantifier-rank and present new
combinatorial proofs of existing results. To this end, we introduce novel path
and tree decompositions that incorporate the concept of reusability and develop
characterizations based on pursuit-evasion games. Leveraging this framework, we
establish that classes of bounded pathwidth and treewidth with reusability
constraints are homomorphism distinguishing closed. Finally, we develop a
comonadic perspective on requantification by constructing new comonads that
encapsulate restricted-reusability pebble games. We show a tight correspondence
between their coalgebras and path/tree decompositions, yielding categorical
characterizations of reusability in graph decompositions. This unifies logical,
combinatorial, and categorical perspectives on the notion of reusability.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [22] [UniMind: Unleashing the Power of LLMs for Unified Multi-Task Brain Decoding](https://arxiv.org/abs/2506.18962)
*Weiheng Lu,Chunfeng Song,Jiamin Wu,Pengyu Zhu,Yuchen Zhou,Weijian Mai,Qihao Zheng,Wanli Ouyang*

Main category: cs.HC

TL;DR: 论文提出了UniMind，一种基于EEG信号的通用基础模型，通过结合大语言模型来解决多任务脑解码中的泛化性问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于EEG的脑解码模型在泛化性和任务适应能力上表现不佳，主要由于任务间的异质性。UniMind旨在解决这一问题。

Method: UniMind设计了Neuro-Language Connector来桥接神经信号与语言模型的模态差异，并通过Task-aware Query Selection模块动态生成任务相关的查询标记。

Result: 在十个数据集上的实验显示，UniMind比现有多任务解码模型平均提升12%的性能，并提供了有价值的神经科学见解。

Conclusion: UniMind通过结合语言模型成功解决了EEG解码中的泛化性问题，为多任务脑解码提供了新的解决方案。

Abstract: Decoding human brain activity from electroencephalography (EEG) signals is a
central challenge at the intersection of neuroscience and artificial
intelligence, enabling diverse applications in mental state assessment,
clinical monitoring, and human-machine interaction. Recent efforts have
extensively explored EEG-based brain foundation models for generalized brain
decoding, employing large-scale training on multiple datasets. However, most of
these attempts struggle with generalizability and fail to achieve satisfactory
performance without task-specific tuning due to pronounced inherent
heterogeneity among decoding tasks. To address these challenges, we present
UniMind, a general-purpose EEG foundation model for unified multi-task brain
decoding by uniquely unleashing the power of large language models to
comprehend complex neural patterns. UniMind offers several advantages. First,
we design a Neuro-Language Connector to bridge the modality gap between neural
signals and large language models, distilling and transforming the
spatiotemporal neural patterns of EEG data into representations understandable
by language models. Second, a Task-aware Query Selection module is proposed to
inject task-awareness into the cross-modal alignment by dynamically generating
task-adaptive query tokens, enabling learning of task-relevant neural patterns
across diverse tasks. Extensive experiments across ten datasets demonstrate
that UniMind substantially outperforms state-of-the-art multi-task decoding
models, with an average gain of 12 percent, while also offering valuable
neuroscientific insights into neural functional correlations across tasks. The
code will be made publicly available.

</details>


### [23] [Raise Awareness of the Environmental Impacts of Retail Food Products: A User-Centered Scenario-Based Approach](https://arxiv.org/abs/2506.19017)
*Lorenzo Porcelli,Francesco Palmieri*

Main category: cs.HC

TL;DR: 论文摘要讨论了气候变化加剧及其对人类的影响，提出了一种用户界面设计，通过提高购物时对环境影响的意识来促进公民参与气候行动。


<details>
  <summary>Details</summary>
Motivation: 气候变化主要由人类活动引起，导致灾害频发和经济损失增加，亟需公民参与和意识提升来缓解其影响。

Method: 采用以用户为中心的场景化设计方法开发界面，并加入游戏化元素以提高公民参与的积极性。

Result: 界面设计成功提高了用户对购物中环境影响的认识，并通过游戏化手段促进了公民气候行动的参与。

Conclusion: 用户界面设计结合游戏化元素是提升公民气候行动参与的有效方式，有助于缓解气候变化的影响。

Abstract: The climate is warming rapidly, and atmospheric concentrations of greenhouse
gases (GHGs) are at their highest levels ever recorded. As a result of these
climate changes, caused mainly by human activities, disasters have increased
fivefold over the past 50 years, causing death and economic loss. Civic
engagement and awareness are essential to mitigate climate change and its
impacts. In this work, we proposed a user interface that makes users aware of
the environmental impact of the food products they buy when shopping. A
user-centered scenario-based design was followed in the development of the
interface. Gamification elements were added to increase civic participation in
climate action.

</details>


### [24] [Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education](https://arxiv.org/abs/2506.19107)
*Ruiwei Xiao,Xinying Hou,Runlong Ye,Majeed Kazemitabaar,Nicholas Diana,Michael Liut,John Stamper*

Main category: cs.HC

TL;DR: 论文提出了一种名为“教学提示”的新概念，旨在帮助学生更有效地使用大语言模型（LLM）提升学习效果，并通过交互式系统和场景教学进行干预，取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在教育中的广泛应用，学生的误用可能影响学习效果，因此研究如何通过有效的提示技术促进学习。

Method: 通过教师调查（N=36）设计交互式系统，结合场景教学，对学生（N=22）进行干预并评估效果。

Result: 学生使用LLM的教学提示技能显著提升，对系统的态度积极，未来使用意愿增强。

Conclusion: 研究提出了教学提示的理论框架、教师态度洞察及有效干预设计，具有课堂推广潜力。

Abstract: With the proliferation of large language model (LLM) applications since 2022,
their use in education has sparked both excitement and concern. Recent studies
consistently highlight students' (mis)use of LLMs can hinder learning outcomes.
This work aims to teach students how to effectively prompt LLMs to improve
their learning. We first proposed pedagogical prompting, a
theoretically-grounded new concept to elicit learning-oriented responses from
LLMs. To move from concept design to a proof-of-concept learning intervention
in real educational settings, we selected early undergraduate CS education
(CS1/CS2) as the example context. We began with a formative survey study with
instructors (N=36) teaching early-stage undergraduate-level CS courses to
inform the instructional design based on classroom needs. Based on their
insights, we designed and developed a learning intervention through an
interactive system with scenario-based instruction to train pedagogical
prompting skills. Finally, we evaluated its instructional effectiveness through
a user study with CS novice students (N=22) using pre/post-tests. Through mixed
methods analyses, our results indicate significant improvements in learners'
LLM-based pedagogical help-seeking skills, along with positive attitudes toward
the system and increased willingness to use pedagogical prompts in the future.
Our contributions include (1) a theoretical framework of pedagogical prompting;
(2) empirical insights into current instructor attitudes toward pedagogical
prompting; and (3) a learning intervention design with an interactive learning
tool and scenario-based instruction leading to promising results on teaching
LLM-based help-seeking. Our approach is scalable for broader implementation in
classrooms and has the potential to be integrated into tools like ChatGPT as an
on-boarding experience to encourage learning-oriented use of generative AI.

</details>


### [25] [Smart Glasses for CVI: Co-Designing Extended Reality Solutions to Support Environmental Perception by People with Cerebral Visual Impairment](https://arxiv.org/abs/2506.19210)
*Bhanuka Gamage,Nicola McDowell,Dijana Kovacic,Leona Holloway,Thanh-Toan Do,Nicholas Price,Arthur Lowery,Kim Marriott*

Main category: cs.HC

TL;DR: 智能眼镜通过协助定位物体、阅读文本、识别人物等方式，为脑视觉障碍（CVI）患者提供支持，填补了辅助技术研究中的空白。


<details>
  <summary>Details</summary>
Motivation: CVI是视力障碍的主要原因，但在辅助技术研究中代表性不足，影响复杂环境中的视觉处理能力。

Method: 采用双钻石设计框架，包括日记研究、创意工作坊和迭代开发会话，使用Apple Vision Pro进行测试。

Result: 智能眼镜显著帮助CVI患者解决物体定位、文本阅读、人物识别等关键挑战。

Conclusion: 研究及时探索了智能眼镜与CVI需求的结合点，为未来技术发展提供了方向。

Abstract: Cerebral Visual Impairment (CVI) is the set to be the leading cause of vision
impairment, yet remains underrepresented in assistive technology research.
Unlike ocular conditions, CVI affects higher-order visual processing-impacting
object recognition, facial perception, and attention in complex environments.
This paper presents a co-design study with two adults with CVI investigating
how smart glasses, i.e. head-mounted extended reality displays, can support
understanding and interaction with the immediate environment. Guided by the
Double Diamond design framework, we conducted a two-week diary study, two
ideation workshops, and ten iterative development sessions using the Apple
Vision Pro. Our findings demonstrate that smart glasses can meaningfully
address key challenges in locating objects, reading text, recognising people,
engaging in conversations, and managing sensory stress. With the rapid
advancement of smart glasses and increasing recognition of CVI as a distinct
form of vision impairment, this research addresses a timely and under-explored
intersection of technology and need.

</details>


### [26] [HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps](https://arxiv.org/abs/2506.19268)
*Timoteo Kelly,Abdulkadir Korkmaz,Samuel Mallet,Connor Souders,Sadra Aliakbarpour,Praveen Rao*

Main category: cs.HC

TL;DR: HARPT是一个大规模标注的移动健康应用商店评论语料库，用于推动用户隐私和信任研究。


<details>
  <summary>Details</summary>
Motivation: 为研究用户在健康应用中的隐私和信任问题提供数据支持。

Method: 采用基于规则的过滤、迭代手动标注、数据增强和弱监督的多种方法构建语料库。

Result: 标注了480,000条评论，提供了性能良好的分类模型基准。

Conclusion: HARPT作为公开资源，支持健康信息学、网络安全和自然语言处理的研究。

Abstract: We present HARPT, a large-scale annotated corpus of mobile health app store
reviews aimed at advancing research in user privacy and trust. The dataset
comprises over 480,000 user reviews labeled into seven categories that capture
critical aspects of trust in applications, trust in providers and privacy
concerns. Creating HARPT required addressing multiple complexities, such as
defining a nuanced label schema, isolating relevant content from large volumes
of noisy data, and designing an annotation strategy that balanced scalability
with accuracy. This strategy integrated rule-based filtering, iterative manual
labeling with review, targeted data augmentation, and weak supervision using
transformer-based classifiers to accelerate coverage. In parallel, a carefully
curated subset of 7,000 reviews was manually annotated to support model
development and evaluation. We benchmark a broad range of classification
models, demonstrating that strong performance is achievable and providing a
baseline for future research. HARPT is released as a public resource to support
work in health informatics, cybersecurity, and natural language processing.

</details>


### [27] [OpticalAging: Real-time Presbyopia Simulation for Inclusive Design via Tunable Lenses](https://arxiv.org/abs/2506.19307)
*Qing Zhang,Zixiong Su,Yoshihito Kondoh,Kazunori Asada,Thad Starner,Kai Kunze,Yuta Itoh,Jun Rekimoto*

Main category: cs.HC

TL;DR: 提出一种名为OpticalAging的光学透射模拟方法，模拟老花眼视觉体验，旨在提升人们对老花眼的认知和同理心。


<details>
  <summary>Details</summary>
Motivation: 当前对老花眼的抽象认知与实际感知挑战之间存在差距，希望通过模拟工具弥补这一不足。

Method: 利用可调透镜动态模拟老花眼视觉，并结合用户研究验证效果。

Result: 定量数据显示模拟显著改变了近点视力；定性结果表明参与者对老花眼的认知和同理心提升。

Conclusion: OpticalAging可作为补充工具，用于年龄包容性设计，但需结合直接用户参与。

Abstract: Presbyopia, a common age-related vision condition affecting most people as
they age, often remains inadequately understood by those unaffected. To help
bridge the gap between abstract accessibility knowledge and a more grounded
appreciation of perceptual challenges, this study presents OpticalAging, an
optical see-through simulation approach. Unlike VR-based methods, OpticalAging
uses dynamically controlled tunable lenses to simulate the first-person visual
perspective of presbyopia's distance-dependent blur during real-world
interaction, aiming to enhance awareness. While acknowledging critiques
regarding simulation's limitations in fully capturing lived experience, we
position this tool as a complement to user-centered methods. Our user study (N
= 19, 18-35 years old) provides validation: quantitative measurements show
statistically significant changes in near points across three age modes (40s,
50s, 60s), while qualitative results suggest increases in reported
understanding and empathy among participants. The integration of our tool into
a design task showcases its potential applicability within age-inclusive design
workflows when used critically alongside direct user engagement.

</details>


### [28] [Can theory-driven learning analytics dashboard enhance human-AI collaboration in writing learning? Insights from an empirical experiment](https://arxiv.org/abs/2506.19364)
*Angxuan Chen,Jingjing Lian,Xinran Kuang,Jiyou Jia*

Main category: cs.HC

TL;DR: 研究表明，基于理论的学习分析仪表板（LAD）能提升学生在写作任务中与生成式AI（GenAI）的合作效果，增强写作知识和自我调节学习（SRL）能力，但也可能增加测试焦虑和认知负担。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过LAD解决教育中过度依赖GenAI及浅层学习的问题，以改善人机协作写作任务中的学习效果。

Method: 基于Zimmerman的SRL框架，LAD提供实时反馈；采用准实验设计，52名研究生分为实验组（使用LAD）和对照组，通过知识测试、问卷和对话数据分析效果。

Result: 实验组在写作知识和SRL技能上显著提升，但测试焦虑和认知负担增加；与GenAI的对话更具反思性和评价性。

Conclusion: 需设计互补GenAI的干预措施，确保技术真正促进学习而非削弱学习过程。

Abstract: The integration of Generative AI (GenAI) into education has raised concerns
about over-reliance and superficial learning, particularly in writing tasks in
higher education. This study explores whether a theory-driven learning
analytics dashboard (LAD) can enhance human-AI collaboration in the academic
writing task by improving writing knowledge gains, fostering self-regulated
learning (SRL) skills and building different human-AI dialogue characteristics.
Grounded in Zimmerman's SRL framework, the LAD provided real-time feedback on
learners' goal-setting, writing processes and reflection, while monitoring the
quality of learner-AI interactions. A quasi-experiment was conducted involving
52 postgraduate students divided into an experimental group (EG) using the LAD
to a control group (CG) without it in a human-AI collaborative writing task.
Pre- and post- knowledge tests, questionnaires measuring SRL and cognitive
load, and students' dialogue data with GenAI were collected and analyzed.
Results showed that the EG achieved significantly higher writing knowledge
gains and improved SRL skills, particularly in self-efficacy and cognitive
strategies. However, the EG also reported increased test anxiety and cognitive
load, possibly due to heightened metacognitive awareness. Epistemic Network
Analysis revealed that the EG engaged in more reflective, evaluative
interactions with GenAI, while the CG focused on more transactional and
information-seeking exchanges. These findings contribute to the growing body of
literature on the educational use of GenAI and highlight the importance of
designing interventions that complement GenAI tools, ensuring that technology
enhances rather than undermines the learning process.

</details>


### [29] [Integrating AIs With Body Tracking Technology for Human Behaviour Analysis: Challenges and Opportunities](https://arxiv.org/abs/2506.19430)
*Adrien Coppens,Valérie Maquil*

Main category: cs.HC

TL;DR: 论文讨论了如何利用深度相机和AI技术实现人体行为分析的自动化，以及由此带来的机会与挑战。


<details>
  <summary>Details</summary>
Motivation: 利用低成本深度相机和AI技术，构建无需额外设备的交互系统和用户研究工具。

Method: 结合现成的AI识别模块，构建远程协作系统，优化人体行为追踪流程。

Result: 成功开发了一个跨墙式显示屏的远程协作系统，展示了AI组件的潜力。

Conclusion: 深度相机与AI技术的结合为行为分析提供了新机会，但也需解决组件协调和工程化挑战。

Abstract: The automated analysis of human behaviour provides many opportunities for the
creation of interactive systems and the post-experiment investigations for user
studies. Commodity depth cameras offer reasonable body tracking accuracy at a
low price point, without the need for users to wear or hold any extra
equipment. The resulting systems typically perform body tracking through a
dedicated machine learning model, but they can be enhanced with additional AI
components providing extra capabilities. This leads to opportunities but also
challenges, for example regarding the orchestration of such AI components and
the engineering of the resulting tracking pipeline. In this paper, we discuss
these elements, based on our experience with the creation of a remote
collaboration system across distant wall-sized displays, that we built using
existing and readily available building blocks, including AI-based recognition
models.

</details>


### [30] [5 Days, 5 Stories: Using Technology to Promote Empathy in the Workplace](https://arxiv.org/abs/2506.19495)
*Russell Beale,Eugenia Sergueeva*

Main category: cs.HC

TL;DR: 研究探讨了数字故事分享平台In Your Shoes在提升职场共情能力中的作用，发现其虽未显著改变共情特质，但增强了情境共情和职场关系。


<details>
  <summary>Details</summary>
Motivation: 共情是职场合作和沟通的关键，但培养共情技能仍具挑战。研究旨在探索数字工具如何支持职场中的共情发展。

Method: 采用混合研究方法，通过一周的平台干预，量化评估共情商数（EQ），并定性分析参与者体验。

Result: 量化结果显示共情特质无显著变化，但定性结果表明平台促进了情境共情、自我反思和职场关系。

Conclusion: 异步、结构化的数字叙事工具对职场共情有潜力，为设计情感智能职场技术提供了启示。

Abstract: Empathy is widely recognized as a vital attribute for effective collaboration
and communication in the workplace, yet developing empathic skills and
fostering it among colleagues remains a challenge. This study explores the
potential of a collaborative digital storytelling platform - In Your Shoes -
designed to promote empathic listening and interpersonal understanding through
the structured exchange of personal narratives. A one-week intervention was
conducted with employees from multiple organizations using the platform.
Employing a mixed methods approach, we assessed quantitative changes in empathy
using the Empathy Quotient (EQ) and qualitatively analyzed participant
experiences through grounded theory. While quantitative analysis revealed no
statistically significant shift in dispositional empathy, qualitative findings
suggested the tool facilitated situational empathy, prompted self-reflection,
improved emotional resonance, and enhanced workplace relationships.
Participants reported feelings of psychological safety, connection, and, in
some cases, therapeutic benefits from sharing and responding to stories. These
results highlight the promise of asynchronous, structured narrative-based
digital tools for supporting empathic engagement in professional settings,
offering insights for the design of emotionally intelligent workplace
technologies.

</details>


### [31] [Examination of Eye-Tracking, Head-Gaze, and Controller-Based Ray-casting in TMT-VR: Performance and Usability Across Adulthood](https://arxiv.org/abs/2506.19519)
*Panagiotis Kourtesis,Evgenia Giatzoglou,Panagiotis Vorias,Katerina Alkisti Gounari,Eleni Orfanidou,Chrysanthi Nega*

Main category: cs.HC

TL;DR: VR输入方式在神经心理学测试中的比较：眼动追踪在空间精度上最优，头部凝视在速度和错误率上表现最好，手柄则表现较差。年龄是影响行为的主要因素。


<details>
  <summary>Details</summary>
Motivation: 研究虚拟现实（VR）不同输入方式在神经心理学测试中的表现差异，以及年龄对测试结果的影响。

Method: 77名健康志愿者（年轻和中年）使用三种VR输入方式（眼动追踪、头部凝视和手柄）完成VR版的Trail-Making Test（TMT），分析完成时间、空间精度和错误率。

Result: 年轻人在速度和精度上表现更好；眼动追踪在空间精度上占优，头部凝视在速度上表现最佳，手柄表现最差。主观评分整体较高。

Conclusion: VR-TMT是一种准确、吸引人且适应性强的评估工具，但需要根据不同年龄制定标准。头部凝视在认知负荷较高时表现最佳。

Abstract: Virtual reality (VR) can enrich neuropsychological testing, yet the ergonomic
trade-offs of its input modes remain under-examined. Seventy-seven healthy
volunteers-young (19-29 y) and middle-aged (27-56 y)-completed a VR
Trail-Making Test with three pointing methods: eye-tracking, head-gaze, and a
six-degree-of-freedom hand controller. Completion time, spatial accuracy, and
error counts for the simple (Trail A) and alternating (Trail B) sequences were
analysed in 3 x 2 x 2 mixed-model ANOVAs; post-trial scales captured usability
(SUS), user experience (UEQ-S), and acceptability. Age dominated behaviour:
younger adults were reliably faster, more precise, and less error-prone.
Against this backdrop, input modality mattered. Eye-tracking yielded the best
spatial accuracy and shortened Trail A time relative to manual control;
head-gaze matched eye-tracking on Trail A speed and became the quickest, least
error-prone option on Trail B. Controllers lagged on every metric. Subjective
ratings were high across the board, with only a small usability dip in
middle-aged low-gamers. Overall, gaze-based ray-casting clearly outperformed
manual pointing, but optimal choice depended on task demands: eye-tracking
maximised spatial precision, whereas head-gaze offered calibration-free
enhanced speed and error-avoidance under heavier cognitive load. TMT-VR appears
to be accurate, engaging, and ergonomically adaptable assessment, yet it
requires age-specific-stratified norms.

</details>


### [32] [Beyond Wellbeing Apps: Co-Designing Immersive, Embodied, and Collective Digital Wellbeing Interventions for Healthcare Professionals](https://arxiv.org/abs/2506.19524)
*Zheyuan Zhang,Jingjing Sun,Dorian Peters,Rafael A. Calvo*

Main category: cs.HC

TL;DR: 医疗保健专业人员（HCPs）面临日益增加的压力与职业倦怠。技术化的健康干预为他们提供了灵活的解决方案，但新兴技术（如VR、AR等）在HCPs中的接受度尚缺乏研究。本文通过两阶段共同设计研究，探索了HCPs对健康技术的偏好与需求。


<details>
  <summary>Details</summary>
Motivation: 随着HCPs的工作压力增加，传统健康干预（如移动/web应用）之外的新兴技术（如VR/AR）尚未在HCPs群体中得到充分研究。因此，本文旨在了解HCPs对这些技术的接受度与偏好。

Method: 研究采用两阶段共同设计方法，招募26名HCPs参与创意生成、概念评估、原型测试和设计迭代。

Result: HCPs尤其重视技术对心理健康的支持潜力，尤其是沉浸式、具身化和集体体验。研究还提出满足自主性、能力和关联性需求的设计建议。

Conclusion: 新兴健康技术对HCPs的心理健康干预具有潜力，但需通过满足用户核心需求来维持参与度。研究为未来技术设计提供了实用指导。

Abstract: Healthcare professionals (HCPs) face increasing levels of stress and burnout.
Technological wellbeing interventions provide accessible and flexible support
for HCPs. While most studies have focused on mobile- and web-based programs,
alternative technologies like virtual reality (VR), augmented reality (AR),
tangible interfaces, and embodied technologies are emerging as engaging and
effective tools for wellbeing interventions. However, there is still a lack of
research on how such technologies are perceived among HCPs. This study explored
HCPs' perceptions and preferences for various types of wellbeing technologies,
by conducting a 2-phase co-design study involving 26 HCPs in idea generation,
concept evaluation, prototype testing, and design iteration. From our findings,
HCPs highly valued the potential of technologies to support mental health with
immersive, embodied, and collective experiences. Furthermore, we provided
design recommendations for wellbeing technologies for HCPs that sustain user
engagement by meeting their needs for autonomy, competence, and relatedness in
the experiences.

</details>


### [33] [Filters of Identity: AR Beauty and the Algorithmic Politics of the Digital Body](https://arxiv.org/abs/2506.19611)
*Miriam Doh,Corinna Canali,Nuria Oliver*

Main category: cs.HC

TL;DR: 该论文将AR美颜滤镜置于HCI中身体政治的讨论中，认为其非中立工具，而是强化种族、性别和能力主义美标准的技术治理手段。


<details>
  <summary>Details</summary>
Motivation: 探讨AR美颜滤镜如何通过命名惯例、算法偏见和平台治理，隐形地施加美学规范，从而强化不平等标准。

Method: 通过理论分析和批判性思考，揭示滤镜对美学规范和数字化的影响。

Result: 论证了AR美颜滤镜作为治理技术对美学标准的隐形影响，并提出透明化干预的解决方案。

Conclusion: 呼吁重新思考算法美学和数字体现，以透明化干预应对AR美颜滤镜的不平等影响。

Abstract: This position paper situates AR beauty filters within the broader debate on
Body Politics in HCI. We argue that these filters are not neutral tools but
technologies of governance that reinforce racialized, gendered, and ableist
beauty standards. Through naming conventions, algorithmic bias, and platform
governance, they impose aesthetic norms while concealing their influence. To
address these challenges, we advocate for transparency-driven interventions and
a critical rethinking of algorithmic aesthetics and digital embodiment.

</details>


### [34] [Varif.ai to Vary and Verify User-Driven Diversity in Scalable Image Generation](https://arxiv.org/abs/2506.19644)
*M. Michelessa,J. Ng,C. Hurter,B. Y. Lim*

Main category: cs.HC

TL;DR: 论文提出了Varif.ai系统，通过迭代生成、验证和调整属性，支持用户驱动的图像生成多样性控制。


<details>
  <summary>Details</summary>
Motivation: 图像生成的多样性对公平性和创意构思至关重要，但现有模型难以满足用户的个性化多样性需求。

Method: 结合文本到图像模型和大语言模型，Varif.ai迭代生成图像、验证属性覆盖并调整属性。

Result: 用户研究表明，Varif.ai比基线方法更有效，能更轻松地实现多样性目标。

Conclusion: Varif.ai支持用户控制图像生成多样性，适用于创意构思和可扩展的图像生成。

Abstract: Diversity in image generation is essential to ensure fair representations and
support creativity in ideation. Hence, many text-to-image models have
implemented diversification mechanisms. Yet, after a few iterations of
generation, a lack of diversity becomes apparent, because each user has their
own diversity goals (e.g., different colors, brands of cars), and there are
diverse attributions to be specified. To support user-driven diversity control,
we propose Varif.ai that employs text-to-image and Large Language Models to
iteratively i) (re)generate a set of images, ii) verify if user-specified
attributes have sufficient coverage, and iii) vary existing or new attributes.
Through an elicitation study, we uncovered user needs for diversity in image
generation. A pilot validation showed that Varif.ai made achieving diverse
image sets easier. In a controlled evaluation with 20 participants, Varif.ai
proved more effective than baseline methods across various scenarios. Thus,
this supports user control of diversity in image generation for creative
ideation and scalable image generation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [35] [Am I Playing Better Now? The Effects of G-SYNC in 60Hz Gameplay](https://arxiv.org/abs/2506.19084)
*Maryam Riahi,Benjamin Watson*

Main category: cs.GR

TL;DR: G-SYNC技术在60Hz刷新率下对资深玩家的游戏性能有提升，特别是在高难度游戏中，但对情感和体验的影响有限。


<details>
  <summary>Details</summary>
Motivation: 探究G-SYNC技术在60Hz刷新率下的效果，以补充之前针对30Hz的研究。

Method: 在60Hz刷新率下测试G-SYNC对玩家性能和情绪的影响，并与30Hz的结果进行对比。

Result: 60Hz下G-SYNC的效果较30Hz有所减弱，但仍能提升资深玩家的表现，特别是在高难度游戏中；对情绪和体验的影响不明显。

Conclusion: G-SYNC在更高刷新率下仍有一定价值，但其效果因刷新率和游戏难度而异。

Abstract: G-SYNC technology matches formerly regular display refreshes to irregular
frame updates, improving frame rates and interactive latency. In a previous
study of gaming at the 30Hz frame rates common on consoles, players of
Battlefield 4 were unable to discern when G-SYNC was in use, but scored higher
with G-SYNC and were affected emotionally. We build on that study with the
first examination of G-SYNC's effects at the 60Hz frame rate more common in PC
gaming and on emerging consoles. Though G-SYNC's effects are less at 60Hz than
they were at 30Hz, G-SYNC can still improve the performance of veteran players,
particularly when games are challenging. G-SYNC's effects on emotion and
experience were limited.

</details>


### [36] [SOF: Sorted Opacity Fields for Fast Unbounded Surface Reconstruction](https://arxiv.org/abs/2506.19139)
*Lukas Radl,Felix Windisch,Thomas Deixelberger,Jozef Hladky,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Main category: cs.GR

TL;DR: SOF方法通过分层重新排序和鲁棒的高斯深度公式，提升了3D高斯表示下场景重建的精度和速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖近似深度估计和全局排序启发式，导致重建网格的准确性和质量受限。

Method: 引入分层重新排序和鲁棒的高斯深度公式，结合不透明度场的水平集正则化器，并行化Marching Tetrahedra算法。

Result: SOF在定量评估中展现出更高重建精度，处理时间减少超三倍。

Conclusion: SOF将高效的高斯渲染转化为同等高效几何提取，推动了技术发展。

Abstract: Recent advances in 3D Gaussian representations have significantly improved
the quality and efficiency of image-based scene reconstruction. Their explicit
nature facilitates real-time rendering and fast optimization, yet extracting
accurate surfaces - particularly in large-scale, unbounded environments -
remains a difficult task. Many existing methods rely on approximate depth
estimates and global sorting heuristics, which can introduce artifacts and
limit the fidelity of the reconstructed mesh. In this paper, we present Sorted
Opacity Fields (SOF), a method designed to recover detailed surfaces from 3D
Gaussians with both speed and precision. Our approach improves upon prior work
by introducing hierarchical resorting and a robust formulation of Gaussian
depth, which better aligns with the level-set. To enhance mesh quality, we
incorporate a level-set regularizer operating on the opacity field and
introduce losses that encourage geometrically-consistent primitive shapes. In
addition, we develop a parallelized Marching Tetrahedra algorithm tailored to
our opacity formulation, reducing meshing time by up to an order of magnitude.
As demonstrated by our quantitative evaluation, SOF achieves higher
reconstruction accuracy while cutting total processing time by more than a
factor of three. These results mark a step forward in turning efficient
Gaussian-based rendering into equally efficient geometry extraction.

</details>


### [37] [Style Transfer: A Decade Survey](https://arxiv.org/abs/2506.19278)
*Tianshan Zhang,Hao Tang*

Main category: cs.GR

TL;DR: 论文综述了人工智能生成内容（AIGC）在视觉艺术中的发展，重点分析了变分自编码器（VAE）、生成对抗网络（GANs）和扩散模型（Diffusion Models）的作用及其艺术与技术的结合。


<details>
  <summary>Details</summary>
Motivation: 探讨AIGC技术在视觉艺术创作中的应用及其对艺术表达的影响，填补技术机制与美学理解之间的空白。

Method: 通过对过去十年500多篇研究论文的系统性回顾，分析了VAE、GANs和扩散模型的演进，并提出多维评估框架。

Result: 揭示了AIGC系统的变革能力和当前局限，强调其对未来创意实践的深远影响。

Conclusion: 论文为人工智能与艺术表达的融合提供了统一视角，并指出了未来研究的关键挑战和方向。

Abstract: The revolutionary advancement of Artificial Intelligence Generated Content
(AIGC) has fundamentally transformed the landscape of visual content creation
and artistic expression. While remarkable progress has been made in image
generation and style transfer, the underlying mechanisms and aesthetic
implications of these technologies remain insufficiently understood. This paper
presents a comprehensive survey of AIGC technologies in visual arts, tracing
their evolution from early algorithmic frameworks to contemporary deep
generative models. We identify three pivotal paradigms: Variational
Autoencoders (VAE), Generative Adversarial Networks (GANs), and Diffusion
Models, and examine their roles in bridging the gap between human creativity
and machine synthesis. To support our analysis, we systematically review over
500 research papers published in the past decade, spanning both foundational
developments and state-of-the-art innovations. Furthermore, we propose a
multidimensional evaluation framework that incorporates Technical Innovation,
Artistic Merit, Visual Quality, Computational Efficiency, and Creative
Potential. Our findings reveal both the transformative capacities and current
limitations of AIGC systems, emphasizing their profound impact on the future of
creative practices. Through this extensive synthesis, we offer a unified
perspective on the convergence of artificial intelligence and artistic
expression, while outlining key challenges and promising directions for future
research in this rapidly evolving field.

</details>


### [38] [Continuous Indexed Points for Multivariate Volume Visualization](https://arxiv.org/abs/2506.19400)
*Liang Zhou,Xinyi Gou,Daniel Weiskopf*

Main category: cs.GR

TL;DR: 提出了连续索引点方法，用于改进多元体积数据的可视化，通过局部线性拟合和主成分分析，实现了更高效的多元数据相关性分析。


<details>
  <summary>Details</summary>
Motivation: 为了提升多元体积数据的可视化效果，尤其是捕捉局部相关性，需要一种能够线性结构表示的方法。

Method: 采用主成分分析进行局部线性拟合，并结合层次空间数据结构加速；通过连续索引点在平行坐标中可视化局部线性信息。

Result: 新方法能够有效分析多元数据的局部相关性，支持交互式探索，并通过案例研究和专家验证了其有效性。

Conclusion: 连续索引点方法为多元体积数据的可视化提供了高效且交互式的解决方案，具有实际应用价值。

Abstract: We introduce continuous indexed points for improved multivariate volume
visualization. Indexed points represent linear structures in parallel
coordinates and can be used to encode local correlation of multivariate
(including multifield, multifaceted, and multiattribute) volume data. First, we
perform local linear fitting in the spatial neighborhood of each volume sample
using principal component analysis, accelerated by hierarchical spatial data
structures. This local linear information is then visualized as continuous
indexed points in parallel coordinates: a density representation of indexed
points in a continuous domain. With our new method, multivariate volume data
can be analyzed using the eigenvector information from local spatial
embeddings. We utilize both 1-flat and 2-flat indexed points, allowing us to
identify correlations between two variables and even three variables,
respectively. An interactive occlusion shading model facilitates good spatial
perception of the volume rendering of volumetric correlation characteristics.
Interactive exploration is supported by specifically designed multivariate
transfer function widgets working in the image plane of parallel coordinates.
We show that our generic technique works for multi-attribute datasets. The
effectiveness and usefulness of our new method is demonstrated through a case
study, an expert user study, and domain expert feedback.

</details>


### [39] [Virtual Memory for 3D Gaussian Splatting](https://arxiv.org/abs/2506.19415)
*Jonathan Haberl,Philipp Fleck,Clemens Arth*

Main category: cs.GR

TL;DR: 提出了一种利用虚拟内存技术渲染大型复杂3D高斯泼溅场景的方法，有效减少内存使用并加速渲染。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯泼溅技术在渲染大规模复杂场景时的内存和性能问题。

Method: 利用虚拟内存和虚拟纹理技术动态识别和流式传输可见高斯分布到GPU，实时渲染。

Result: 方法显著减少内存使用并加速渲染，适用于桌面和移动设备。

Conclusion: 该方法通过虚拟内存和细节层级技术，有效提升了大规模3D高斯泼溅场景的渲染效率。

Abstract: 3D Gaussian Splatting represents a breakthrough in the field of novel view
synthesis. It establishes Gaussians as core rendering primitives for highly
accurate real-world environment reconstruction. Recent advances have
drastically increased the size of scenes that can be created. In this work, we
present a method for rendering large and complex 3D Gaussian Splatting scenes
using virtual memory. By leveraging well-established virtual memory and virtual
texturing techniques, our approach efficiently identifies visible Gaussians and
dynamically streams them to the GPU just in time for real-time rendering.
Selecting only the necessary Gaussians for both storage and rendering results
in reduced memory usage and effectively accelerates rendering, especially for
highly complex scenes. Furthermore, we demonstrate how level of detail can be
integrated into our proposed method to further enhance rendering speed for
large-scale scenes. With an optimized implementation, we highlight key
practical considerations and thoroughly evaluate the proposed technique and its
impact on desktop and mobile devices.

</details>


### [40] [Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders](https://arxiv.org/abs/2506.19708)
*Matyas Bohacek,Thomas Fel,Maneesh Agrawala,Ekdeep Singh Lubana*

Main category: cs.GR

TL;DR: 该论文提出了一种系统方法，用于识别生成式图像模型中的“概念盲点”，即训练数据中存在但在模型生成中缺失或误表示的概念。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式图像模型在大规模数据集上表现优异，但某些简单概念（如人手或四个一组的物体）仍无法正确生成。这些失败模式是否反映结构性限制尚不明确。

Method: 利用稀疏自编码器（SAEs）提取可解释的概念嵌入，构建包含32,000个概念的RA-SAE，以定量比较真实图像与生成图像中的概念分布。

Result: 研究发现了一些被抑制的盲点（如鸟食器、DVD盘）和过表达的盲点（如木质背景）。同时，还发现了模型对训练数据的特定视觉模板的复制现象。

Conclusion: 论文提出了一种理论框架，通过评估生成模型与数据生成过程的概念一致性，系统性地识别概念盲点。

Abstract: Despite their impressive performance, generative image models trained on
large-scale datasets frequently fail to produce images with seemingly simple
concepts -- e.g., human hands or objects appearing in groups of four -- that
are reasonably expected to appear in the training data. These failure modes
have largely been documented anecdotally, leaving open the question of whether
they reflect idiosyncratic anomalies or more structural limitations of these
models. To address this, we introduce a systematic approach for identifying and
characterizing "conceptual blindspots" -- concepts present in the training data
but absent or misrepresented in a model's generations. Our method leverages
sparse autoencoders (SAEs) to extract interpretable concept embeddings,
enabling a quantitative comparison of concept prevalence between real and
generated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with
32,000 concepts -- the largest such SAE to date -- enabling fine-grained
analysis of conceptual disparities. Applied to four popular generative models
(Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals
specific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces
on documents) and exaggerated blindspots (e.g., wood background texture and
palm trees). At the individual datapoint level, we further isolate memorization
artifacts -- instances where models reproduce highly specific visual templates
seen during training. Overall, we propose a theoretically grounded framework
for systematically identifying conceptual blindspots in generative models by
assessing their conceptual fidelity with respect to the underlying
data-generating process.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [41] [TRMAC: A Time-Reversal-based MAC Protocol for Wireless Networks within Computing Packages](https://arxiv.org/abs/2506.19487)
*Ama Bandara,Abhijit Das,Fatima Rodriguez-Galan,Eduard Alarcon,Sergi Abadal*

Main category: cs.ET

TL;DR: 该论文提出了TRMAC协议，利用时间反演的空间聚焦能力实现共享频道的并行传输，解决了高密度流量下可扩展MAC的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着小芯片集成和多核架构的普及，传统互连技术的局限性促使无线片上通信成为新选择，但可扩展的MAC在高密度流量下仍具挑战性。

Method: TRMAC利用片上无线信道的准确定性特性，通过预表征信道冲激响应和能量阈值协调访问，无需正交资源分配或集中仲裁。

Result: 仿真显示TRMAC在单一频道上实现与多频道协议相当的吞吐量和低延迟，适用于未来无线片上网络。

Conclusion: TRMAC为下一代计算平台的MAC协议设计提供了新方向，结合底层信道物理特性实现高效、低复杂度的通信。

Abstract: As chiplet-based integration and many-core architectures become the norm in
high-performance computing, on-chip wireless communication has emerged as a
compelling alternative to traditional interconnects. However, scalable Medium
Access Control (MAC) remains a fundamental challenge, particularly under dense
traffic and limited spectral resources. This paper presents TRMAC, a novel
cross-layer MAC protocol that exploits the spatial focusing capability of Time
Reversal (TR) to enable multiple parallel transmissions over a shared frequency
channel. By leveraging the quasi-deterministic nature of on-chip wireless
channels, TRMAC pre-characterizes channel impulse responses to coordinate
access using energy-based thresholds, eliminating the need for orthogonal
resource allocation or centralized arbitration. Through detailed physical-layer
simulation and system-level evaluation on diverse traffic, TRMAC demonstrates
comparable or superior performance to existing multi-channel MAC protocols,
achieving low latency, high throughput, and strong scalability across hundreds
of cores. TRMAC provides a low-complexity, high-efficiency solution for future
Wireless Networks-on-Chip (WNoCs), particularly in chiplet-based systems where
spatial reuse and modularity are critical. With simulations we prove that TRMAC
can be utilized for parallel transmissions with a single frequency channel with
a similar throughput and latency as in using multiple frequency bands omitting
the need for complex transceivers. This work establishes a new design direction
for MAC protocols that are tightly integrated with the underlying channel
physics to meet the demands of next-generation computing platforms.

</details>


### [42] [Experimental Assessment of Neural 3D Reconstruction for Small UAV-based Applications](https://arxiv.org/abs/2506.19491)
*Genís Castillo Gómez-Raya,Álmos Veres-Vitályos,Filip Lemic,Pablo Royo,Mario Montagud,Sergi Fernández,Sergi Abadal,Xavier Costa-Pérez*

Main category: cs.ET

TL;DR: 论文提出了一种结合神经3D重建（N3DR）和小型无人机的创新方法，以提高在受限环境中对静态小物体进行高精度3D重建的能力。


<details>
  <summary>Details</summary>
Motivation: 随着无人机的小型化，其在室内和难以到达区域的部署潜力增加，但飞行动力学和功耗等问题限制了其自主性和任务能力。

Method: 设计并实现了一个基于N3DR的流程，利用Instant-ngp、Nerfacto和Splatfacto等先进模型，通过小型无人机拍摄的图像提升3D重建质量。

Result: 实验结果显示，N3DR增强的流程显著提高了重建质量，适用于高精度3D映射和异常检测。

Conclusion: 研究表明N3DR在提升小型无人机系统能力方面具有巨大潜力。

Abstract: The increasing miniaturization of Unmanned Aerial Vehicles (UAVs) has
expanded their deployment potential to indoor and hard-to-reach areas. However,
this trend introduces distinct challenges, particularly in terms of flight
dynamics and power consumption, which limit the UAVs' autonomy and mission
capabilities. This paper presents a novel approach to overcoming these
limitations by integrating Neural 3D Reconstruction (N3DR) with small UAV
systems for fine-grained 3-Dimensional (3D) digital reconstruction of small
static objects. Specifically, we design, implement, and evaluate an N3DR-based
pipeline that leverages advanced models, i.e., Instant-ngp, Nerfacto, and
Splatfacto, to improve the quality of 3D reconstructions using images of the
object captured by a fleet of small UAVs. We assess the performance of the
considered models using various imagery and pointcloud metrics, comparing them
against the baseline Structure from Motion (SfM) algorithm. The experimental
results demonstrate that the N3DR-enhanced pipeline significantly improves
reconstruction quality, making it feasible for small UAVs to support
high-precision 3D mapping and anomaly detection in constrained environments. In
more general terms, our results highlight the potential of N3DR in advancing
the capabilities of miniaturized UAV systems.

</details>


### [43] [The receptron is a nonlinear threshold logic gate with intrinsic multi-dimensional selective capabilities for analog inputs](https://arxiv.org/abs/2506.19642)
*B. Paroli,F. Borghi,M. A. C. Potenza,P. Milani*

Main category: cs.ET

TL;DR: 论文提出了一种称为'receptron'的广义阈值逻辑门模型，通过输入依赖的权重函数显著提升分类性能，并展示了其在多维空间中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统阈值逻辑门（TLGs）的线性限制了分类能力，需依赖网络完成复杂任务。因此，提出'receptron'模型以增强单单元的分类性能。

Method: 'receptron'采用非线性输入依赖权重函数，特别适用于3D空间中立方域内的模拟输入，并可扩展至多维情况。

Result: 实验证明，'receptron'在模拟输入中具有选择性激活特性，能够高效处理大量输入，适用于高选择性需求的边缘应用。

Conclusion: 'receptron'网络代表了一种新型设备，无需复杂训练即可实现高分类能力，适合边缘计算应用。

Abstract: Threshold logic gates (TLGs) have been proposed as artificial counterparts of
biological neurons with classification capabilities based on a linear predictor
function combining a set of weights with the feature vector. The linearity of
TLGs limits their classification capabilities requiring the use of networks for
the accomplishment of complex tasks. A generalization of the TLG model called
receptron, characterized by input-dependent weight functions allows for a
significant enhancement of classification performances even with the use of a
single unit. Here we formally demonstrate that a receptron, characterized by
nonlinear input-dependent weight functions, exhibit intrinsic selective
activation properties for analog inputs, when the input vector is within cubic
domains in a 3D space. The proposed model can be extended to the n-dimensional
case for multidimensional applications. Our results suggest that
receptron-based networks can represent a new class of devices capable to manage
a large number of analog inputs, for edge applications requiring high
selectivity and classification capabilities without the burden of complex
training.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [44] [Survey of HPC in US Research Institutions](https://arxiv.org/abs/2506.19019)
*Peng Shu,Junhao Chen,Zhengliang Liu,Huaqin Zhao,Xinliang Li,Tianming Liu*

Main category: cs.DC

TL;DR: 该论文调查了美国大学的高性能计算（HPC）资源状况，对比国家实验室和工业界的超级计算机，发现大学HPC的增长速度远低于后者，提出了改进的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI和数据密集型科学的快速发展，大学HPC资源不足问题日益突出，影响了学术研究的竞争力。

Method: 通过分析50多所顶尖研究机构的HPC能力、架构设计、治理模式及能效，对比国家实验室和工业界的系统。

Result: 大学HPC的增长速度（CAGR≈18%）远低于国家（≈43%）和工业界（≈78%），GPU密集型工作负载加剧了能力差距。

Conclusion: 需要采用联合计算、闲置GPU利用和成本分担等模式，以及新兴的去中心化强化学习技术，以提升大学HPC的可持续性和公平性。

Abstract: The rapid growth of AI, data-intensive science, and digital twin technologies
has driven an unprecedented demand for high-performance computing (HPC) across
the research ecosystem. While national laboratories and industrial hyperscalers
have invested heavily in exascale and GPU-centric architectures,
university-operated HPC systems remain comparatively under-resourced. This
survey presents a comprehensive assessment of the HPC landscape across U.S.
universities, benchmarking their capabilities against Department of Energy
(DOE) leadership-class systems and industrial AI infrastructures. We examine
over 50 premier research institutions, analyzing compute capacity,
architectural design, governance models, and energy efficiency. Our findings
reveal that university clusters, though vital for academic research, exhibit
significantly lower growth trajectories (CAGR $\approx$ 18%) than their
national ($\approx$ 43%) and industrial ($\approx$ 78%) counterparts. The
increasing skew toward GPU-dense AI workloads has widened the capability gap,
highlighting the need for federated computing, idle-GPU harvesting, and
cost-sharing models. We also identify emerging paradigms, such as decentralized
reinforcement learning, as promising opportunities for democratizing AI
training within campus environments. Ultimately, this work provides actionable
insights for academic leaders, funding agencies, and technology partners to
ensure more equitable and sustainable HPC access in support of national
research priorities.

</details>


### [45] [Vertex addition to a ball graph with application to reliability and area coverage in autonomous swarms](https://arxiv.org/abs/2506.19197)
*Calum Buchanan,Puck Rombach,James Bagrow,Hamid R. Ossareh*

Main category: cs.DC

TL;DR: 本文提出了一种算法，通过重新定位或添加顶点来优化单位球图的可靠性和区域覆盖，同时避免了顶点聚集问题。


<details>
  <summary>Details</summary>
Motivation: 单位球图用于建模空间网络（如自主集群中的通信网络），但顶点或边可能不可靠。设计高可靠性的集群结构是关键。

Method: 扩展了先前的立方时间复杂度算法，通过重新定位或添加顶点来最大化可靠性，同时确保顶点分布均匀，避免聚集。

Result: 新算法能够在保证高可靠性的同时，优化区域覆盖，优于基于Fruchterman-Reingold算法的改进方法。

Conclusion: 该方法为设计既可靠又分布均匀的单位球图提供了有效工具，适用于集群任务中的通信和区域覆盖需求。

Abstract: A unit ball graph consists of a set of vertices, labeled by points in
Euclidean space, and edges joining all pairs of points within distance $1$.
These geometric graphs are used to model a variety of spatial networks,
including communication networks between agents in an autonomous swarm. In such
an application, vertices and/or edges of the graph may not be perfectly
reliable; an agent may experience failure or a communication link rendered
inoperable. With the goal of designing robust swarm formations, or unit ball
graphs with high reliability (probability of connectedness), in a preliminary
conference paper we provided an algorithm with cubic time complexity to
determine all possible changes to a unit ball graph by repositioning a single
vertex. Using this algorithm and Monte Carlo simulations, one obtains an
efficient method to modify a unit ball graph by moving a single vertex to a
location which maximizes the reliability. Another important consideration in
many swarm missions is area coverage, yet highly reliable ball graphs often
contain clusters of vertices. Here, we generalize our previous algorithm to
improve area coverage as well as reliability. Our algorithm determines a
location to add or move a vertex within a unit ball graph which maximizes the
reliability, under the constraint that no other vertices of the graph be within
some fixed distance. We compare this method of obtaining graphs with high
reliability and evenly distributed area coverage to another method which uses a
modified Fruchterman-Reingold algorithm for ball graphs.

</details>


### [46] [Shelby: Decentralized Storage Designed to Serve](https://arxiv.org/abs/2506.19233)
*Guy Goren,Andrew Hariri,Timothy D. R. Hartley,Ravi Kappiyoor,Alexander Spiegelman,David Zmick*

Main category: cs.DC

TL;DR: Shelby是一个高性能的去中心化存储协议，旨在满足视频流、大规模数据分析或AI训练等高需求工作负载。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化存储协议在吞吐量、延迟、成本效益和可用性方面不足，导致Web3数据密集型应用仍依赖集中式基础设施。

Method: 采用控制与数据平面分离、低复制开销的纠删码、最小化修复带宽，并通过专用骨干网连接RPC和存储节点；付费读取激励性能，引入新型审计协议。

Result: Shelby实现了高吞吐、低延迟的Web2级性能，同时保持去中心化特性，适用于生产规模的读取密集型Web3应用。

Conclusion: Shelby填补了去中心化存储在高性能需求领域的空白，为Web3数据密集型应用提供了可行的去中心化解决方案。

Abstract: Existing decentralized storage protocols fall short of the service required
by real-world applications. Their throughput, latency, cost-effectiveness, and
availability are insufficient for demanding workloads such as video streaming,
large-scale data analytics, or AI training. As a result, Web3 data-intensive
applications are predominantly dependent on centralized infrastructure.
  Shelby is a high-performance decentralized storage protocol designed to meet
demanding needs. It achieves fast, reliable access to large volumes of data
while preserving decentralization guarantees. The architecture reflects lessons
from Web2 systems: it separates control and data planes, uses erasure coding
with low replication overhead and minimal repair bandwidth, and operates over a
dedicated backbone connecting RPC and storage nodes. Reads are paid, which
incentivizes good performance. Shelby also introduces a novel auditing protocol
that provides strong cryptoeconomic guarantees without compromising
performance, a common limitation of other decentralized solutions. The result
is a decentralized system that brings Web2-grade performance to
production-scale, read-intensive Web3 applications.

</details>


### [47] [The Autonomy of the Lightning Network: A Mathematical and Economic Proof of Structural Decoupling from BTC](https://arxiv.org/abs/2506.19333)
*Craig Steven Wright*

Main category: cs.DC

TL;DR: 该研究分析了闪电网络作为比特币第二层解决方案的特性，发现其在高交易量下形成流动性枢纽垄断，具有不透明、系统性脆弱等隐患，偏离比特币初衷。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨闪电网络作为比特币的扩展方案，是否真正实现了去中心化和安全性，并揭示其潜在的系统性风险。

Method: 通过数学模型、博弈论证明和复杂性分析，研究闪电网络在交易量增加时的行为和成本结构，以及流动性枢纽的形成。

Result: 研究发现闪电网络在高交易量下会形成流动性枢纽垄断，带来租金提取、不透明性和系统性脆弱性，且通道关闭变得经济上不可行。

Conclusion: 结论是闪电网络并非简单扩展比特币，而是形成了一种具有影子银行特性的合成金融系统，缺乏储备纪律、透明度和可执行性保障。

Abstract: This paper presents a formal analysis of the Lightning Network as a monetary
system structurally diverging from Bitcoin's base-layer settlement model. We
demonstrate that under increasing transaction demand, BTC transaction fees rise
superlinearly due to throughput constraints, while Lightning Network routing
costs approach a bounded asymptote. Using mathematical modeling, game-theoretic
proofs, and complexity analysis, we show that Lightning enables indefinite
off-chain operation via the emergence of liquidity hub oligopolies. These hubs
exhibit properties of unregulated financial intermediaries, including rent
extraction, opacity, and systemic fragility. Strategic agent models show that
channel closure becomes economically infeasible, and routing problems approach
hardness limits in P-Space complexity. We conclude that Lightning does not
merely extend Bitcoin, but constitutes a synthetic financial system with
shadowbank characteristics, lacking reserve discipline, transparency, or
enforceable settlement guarantees.

</details>


### [48] [A Heuristic Algorithm for Shortest Path Search](https://arxiv.org/abs/2506.19349)
*Huashan Yu,Xiaolin Wang,Yingwei Luo*

Main category: cs.DC

TL;DR: 本文提出了一种新颖的最短路径搜索方法，通过动态步进和遍历优化启发式算法，显著提升了并行SSSP算法的效率和性能，实现了2.5到5.83倍的加速。


<details>
  <summary>Details</summary>
Motivation: 解决单源最短路径（SSSP）问题在并行算法中的挑战，开发更快、实用且工作高效的并行算法。

Method: 引入了动态步进启发式和遍历优化启发式，以减少路径扩展和同步的开销，并开发了一种实用的SSSP算法。

Result: 在73个真实和合成图上评估，算法相比五种最先进实现的加速比达到2.5到5.83倍。

Conclusion: 所提方法显著提升了SSSP算法的性能，适用于多种图结构。

Abstract: The Single-Source Shortest Path (SSSP) problem is well-known for the
challenges in developing fast, practical, and work-efficient parallel
algorithms. This work introduces a novel shortest path search method. It allows
paths with different lengths to be extended in parallel at the cost of almost
negligible repeated relaxations. A dynamic-stepping heuristic is proposed for
the method to efficiently reduce the extended paths and the synchronizations. A
traversal-optimization heuristic is proposed to improve the method by
efficiently reducing the created paths and alleviating the load imbalance.
Based on the method, the two heuristics are used to develop a practical SSSP
algorithm, which tactfully reduces workload and overhead. The heuristics and
the algorithm were evaluated on 73 real-world and synthetic graphs. The
algorithm was also compared with five state-of-the-art SSSP implementations. On
each GAP benchmark suite graph except Road, its speedup to the best achieved by
these five implementations is 2.5x to 5.83x.

</details>


### [49] [Computing Tree Structures in Anonymous Graphs via Mobile Agents](https://arxiv.org/abs/2506.19365)
*Prabhat Kumar Chand,Manish Kumar,Anisur Rahaman Molla*

Main category: cs.DC

TL;DR: 本文研究了在基于代理的网络中构造最小生成树（MST）和广度优先搜索（BFS）树的问题，提出了一种高效的确定性算法，显著改进了时间和内存效率。


<details>
  <summary>Details</summary>
Motivation: 传统分布式计算中，MST和BFS树构造主要在静态节点的消息传递模型中研究。本文旨在探索移动代理在匿名图中的构造问题，以解决动态环境下的计算挑战。

Method: 采用同步模型，代理无先验图知识，通过确定性算法构造BFS和MST树，选举领导并优化时间和内存使用。

Result: BFS树构造时间为O(min(DΔ, m log n) + n log n + Δ log² n)，MST构造时间为O(n log n + Δ log² n)，内存为O(log n)比特每代理。

Conclusion: 算法显著提升了效率和内存使用，实现了近乎线性时间复杂度的领导者选举和MST构造，适用于动态代理网络。

Abstract: Minimum Spanning Tree (MST) and Breadth-First Search (BFS) tree constructions
are classical problems in distributed computing, traditionally studied in the
message-passing model, where static nodes communicate via messages. This paper
investigates MST and BFS tree construction in an agent-based network, where
mobile agents explore a graph and compute. Each node hosts one agent, and
communication occurs when agents meet at a node. We consider $n$ agents
initially dispersed (one per node) in an anonymous, arbitrary $n$-node,
$m$-edge graph $G$. The goal is to construct the BFS and MST trees from this
configuration such that each tree edge is known to at least one of its
endpoints, while minimizing time and memory per agent. We work in a synchronous
model and assume agents have no prior knowledge of any graph parameters such as
$n$, $m$, $D$, $\Delta$ (graph diameter and maximum degree). Prior work solves
BFS in $O(D\Delta)$ rounds with $O(\log n)$ bits per agent, assuming the root
is known. We give a deterministic algorithm that constructs the BFS tree in
$O(\min(D\Delta, m\log n) + n\log n + \Delta \log^2 n)$ rounds using $O(\log
n)$ bits per agent without root knowledge. To determine the root, we solve
leader election and MST construction. We elect a leader and construct the MST
in $O(n\log n + \Delta \log^2 n)$ rounds, with $O(\log n)$ bits per agent.
Prior MST algorithms require $O(m + n\log n)$ rounds and $\max(\Delta, \log n)
\log n$ bits. Our results significantly improve memory efficiency and time,
achieving nearly linear-time leader election and MST. Agents are assumed to
know $\lambda$, the maximum identifier, bounded by a polynomial in $n$.

</details>


### [50] [Towards an Introspective Dynamic Model of Globally Distributed Computing Infrastructures](https://arxiv.org/abs/2506.19578)
*Ozgur O. Kilic,David K. Park,Yihui Ren,Tatiana Korchuganova,Sairam Sri Vatsavai,Joseph Boudreau,Tasnuva Chowdhury,Shengyu Feng,Raees Khan,Jaehyung Kim,Scott Klasky,Tadashi Maeno,Paul Nilsson,Verena Ingrid Martinez Outschoorn,Norbert Podhorszki,Frédéric Suter,Wei Yang,Yiming Yang,Shinjae Yoo,Alexei Klimentov,Adolfy Hoisie*

Main category: cs.DC

TL;DR: 本文提出了一种基于真实数据的交互式系统，用于评估和改进大规模科学合作中的工作流和数据管理策略。


<details>
  <summary>Details</summary>
Motivation: 大规模科学合作如ATLAS、Belle II等产生海量数据，现有数据管理和分配策略效率不足，缺乏动态评估模型。

Method: 利用PanDA系统的工作执行记录，分析关键性能指标，并开发生成式AI模型模拟负载时间序列。

Result: 确定了排队时间、错误率等关键指标，并构建了包含显性和隐性特征的生成模型。

Conclusion: 该方法为优化数据分配和负载管理提供了动态评估工具，有望提升科学合作的效率。

Abstract: Large-scale scientific collaborations like ATLAS, Belle II, CMS, DUNE, and
others involve hundreds of research institutes and thousands of researchers
spread across the globe. These experiments generate petabytes of data, with
volumes soon expected to reach exabytes. Consequently, there is a growing need
for computation, including structured data processing from raw data to
consumer-ready derived data, extensive Monte Carlo simulation campaigns, and a
wide range of end-user analysis. To manage these computational and storage
demands, centralized workflow and data management systems are implemented.
However, decisions regarding data placement and payload allocation are often
made disjointly and via heuristic means. A significant obstacle in adopting
more effective heuristic or AI-driven solutions is the absence of a quick and
reliable introspective dynamic model to evaluate and refine alternative
approaches. In this study, we aim to develop such an interactive system using
real-world data. By examining job execution records from the PanDA workflow
management system, we have pinpointed key performance indicators such as
queuing time, error rate, and the extent of remote data access. The dataset
includes five months of activity. Additionally, we are creating a generative AI
model to simulate time series of payloads, which incorporate visible features
like category, event count, and submitting group, as well as hidden features
like the total computational load-derived from existing PanDA records and
computing site capabilities. These hidden features, which are not visible to
job allocators, whether heuristic or AI-driven, influence factors such as
queuing times and data movement.

</details>


### [51] [PS-WL: A Probability-Sensitive Wear Leveling scheme for SSD array scaling](https://arxiv.org/abs/2506.19660)
*Shuhang Xu,Yunfei Gu,Linhui Liu,Chentao Wu*

Main category: cs.DC

TL;DR: 针对SSD阵列扩展中传统磨损均衡（WL）范式的问题，论文提出了一种概率敏感的磨损均衡方案（PS-WL），通过平衡故障风险而非磨损，显著降低了阵列故障风险并减少性能开销。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统WL范式因忽略磨损与故障概率的非线性关系而导致老化磁盘过早失效的问题。

Method: 提出PS-WL方案，引入基于故障概率的“有效寿命”模型，并采用PID控制器进行磨损均衡，同时通过保守区域限制热数据迁移以减少性能开销。

Result: 仿真结果表明PS-WL在降低性能开销的同时，有效降低了阵列的整体故障风险。

Conclusion: PS-WL通过直接优化可靠性，设计出更安全、高效且稳定的可扩展存储系统。

Abstract: As flash-based Solid State Drive (SSD) arrays become essential to modern data
centers, scaling these arrays to meet explosive data growth is a frequent and
critical operation. However, the conventional wear-leveling (WL) paradigm
applied during scaling suffers from a fundamental flaw: it ignores the
non-linear relationship between wear and failure probability, potentially
pushing the most vulnerable, aged disks towards premature failure. To address
this critical issue at its root, we propose the Probability-Sensitive Wear
Leveling (PS-WL) scheme, which shifts the optimization goal from balancing wear
to directly balancing failure risk. At its core, PS-WL introduces an "effective
lifetime" model derived from a realistic failure probability to more accurately
assess disk lifetime. This model guides a PID controller for wear leveling
operation, with a conservative zone minimizes performance overhead by
restricting warm data migration. Comprehensive simulations validate the
superiority of PS-WL over state-of-the-art methods. The results demonstrate
that our approach significantly reduces performance overhead while, most
critically, consistently and effectively lowering the aggregated array failure
risk across diverse system configurations and workloads. This proves that by
directly optimizing for reliability, PS-WL builds a scalable storage system
that is, by design, fundamentally safer, more efficient, and more stable.

</details>


### [52] [Formalization and security analysis of the Bridgeless protocol](https://arxiv.org/abs/2506.19730)
*Orestis Alpos,Oleg Fomenko,Dimitris Karakostas,Oleksandr Kurbatov,Andrey Sabelnikov*

Main category: cs.DC

TL;DR: 本文对无桥协议的安全性进行了形式化证明，该协议支持多链间的代币桥接。


<details>
  <summary>Details</summary>
Motivation: 无桥协议旨在实现跨链代币转移，并由验证者确保交易的安全性和活跃性。

Method: 通过形式化子协议，并定义验证者在源链和目标链上的交互条件。

Result: 协议在所有支持的链（如EVM兼容链、Zano和比特币链）上保持安全性和活跃性。

Conclusion: 形式化证明表明无桥协议在多链环境下是安全且可靠的。

Abstract: This paper formalizes the proves the security of the Bridgeless protocol, a
protocol able to bridge tokens between various chains. The Bridgeless protocol
is run by a set of validators, responsible for verifying deposit transactions
on the source chain and generating the corresponding withdrawals on the target
chain. The protocol is designed to be chain-agnostic and the validators
interact with each supported chain via a chain client. It currently supports
EVM-compatible chains, the Zano, and the Bitcoin chains. The paper formalizes
all involved subprotocols and describes the conditions under which the protocol
maintains safety and liveness.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [53] [SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications](https://arxiv.org/abs/2506.18951)
*Jinyang Li,Xiaolong Li,Ge Qu,Per Jacobsson,Bowen Qin,Binyuan Hui,Shuzheng Si,Nan Huo,Xiaohan Xu,Yue Zhang,Ziwei Tang,Yuanshuai Li,Florensia Widjaja,Xintong Zhu,Feige Zhou,Yongfeng Huang,Yannis Papakonstantinou,Fatma Ozcan,Chenhao Ma,Reynold Cheng*

Main category: cs.DB

TL;DR: 论文提出了BIRD-CRITIC基准和Six-Gym训练环境，用于评估和提升LLMs在SQL问题调试上的能力，并通过Bird-Fixer开源代理展示了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在SQL问题调试任务上缺乏严格评估，且开源模型能力不足，需要提升本地开发能力并保护数据隐私。

Method: 引入BIRD-CRITIC基准和Six-Gym训练环境，采用SQL-Rewind策略和f-Plan Boosting方法生成高质量训练数据。

Result: Bird-Fixer在BIRD-CRITIC基准上表现优于主流专有模型，成功率达38.11%（PG）和29.65%（Multi）。

Conclusion: 该研究为SQL问题调试提供了新基准和训练方法，显著推动了开源模型在此领域的发展。

Abstract: Resolution of complex SQL issues persists as a significant bottleneck in
real-world database applications. Current Large Language Models (LLMs), while
adept at text-to-SQL translation, have not been rigorously evaluated on the
more challenging task of debugging SQL issues. To address this gap, we
introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530
PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks
(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within
new environments to facilitate rigorous evaluation. Baseline evaluations
underscore the task's complexity, with the leading reasoning model O3-Mini
achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on
BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks
is crucial for empowering local development while safeguarding data privacy.
Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for
elevating open-source model capabilities for SQL issue debugging. This
environment leverages SQL-Rewind strategy, which automatically generates
executable issue-solution datasets by reverse-engineering issues from verified
SQLs. However, popular trajectory-based fine-tuning methods do not explore
substantial supervisory signals. We further propose f-Plan Boosting, which
extracts high-level debugging plans from SQL solutions, enabling teacher LLMs
to produce 73.7% more successful trajectories for training. We integrate these
components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,
Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on
BIRD-CRITIC-Multi, surpassing leading proprietary models such as
Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing
sophisticated SQL-debugging capabilities. The leaderboard and source code are
available: https://bird-critic.github.io/

</details>


### [54] [Higher-Order Graph Databases](https://arxiv.org/abs/2506.19661)
*Maciej Besta,Shriram Chandran,Jakub Cudak,Patrick Iff,Marcin Copik,Robert Gerstenberger,Tomasz Szydlo,Jürgen Müller,Torsten Hoefler*

Main category: cs.DB

TL;DR: 该论文提出了一种新型的高阶图数据库（HO-GDBs），通过提升和降低范式扩展传统图数据库，支持超图、节点元组等复杂结构，并在理论和实现上验证了其正确性、可扩展性和高性能。


<details>
  <summary>Details</summary>
Motivation: 当前图数据库系统无法支持高阶交互（如子图计数、多元建模等），限制了大规模图数据分析的能力。

Method: 提出HO-GDBs，采用提升和降低范式，实现轻量级、模块化且可并行化的原型系统，支持超图等复杂结构的统一API。

Result: 原型系统在大规模HO OLTP和OLAP任务中表现良好，提升图神经网络在GDB中的准确性达44%，并保障低延迟和高吞吐。

Conclusion: HO-GDBs通过扩展传统图数据库，显著提升了高阶数据分析的效率与能力，具备广泛适用性。

Abstract: Recent advances in graph databases (GDBs) have been driving interest in
large-scale analytics, yet current systems fail to support higher-order (HO)
interactions beyond first-order (one-hop) relations, which are crucial for
tasks such as subgraph counting, polyadic modeling, and HO graph learning. We
address this by introducing a new class of systems, higher-order graph
databases (HO-GDBs) that use lifting and lowering paradigms to seamlessly
extend traditional GDBs with HO. We provide a theoretical analysis of OLTP and
OLAP queries, ensuring correctness, scalability, and ACID compliance. We
implement a lightweight, modular, and parallelizable HO-GDB prototype that
offers native support for hypergraphs, node-tuples, subgraphs, and other HO
structures under a unified API. The prototype scales to large HO OLTP & OLAP
workloads and shows how HO improves analytical tasks, for example enhancing
accuracy of graph neural networks within a GDB by 44%. Our work ensures low
latency and high query throughput, and generalizes both ACID-compliant and
eventually consistent systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [55] [MEDEA: A Design-Time Multi-Objective Manager for Energy-Efficient DNN Inference on Heterogeneous Ultra-Low Power Platforms](https://arxiv.org/abs/2506.19067)
*Hossein Taji,José Miranda,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: MEDEA是一种新型的设计时多目标管理器，用于在异构超低功耗平台上实现高效DNN推理，通过动态电压频率调整、内存感知分块等技术，实现高达38%的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 随着设备端AI需求的增长，如何在资源受限的超低功耗平台上高效执行DNN应用成为关键挑战。

Method: MEDEA结合了内核级DVFS、内存感知分块和基于时间约束的优化策略，以最小化能耗并满足应用截止时间。

Result: 在生物医学癫痫检测案例中，MEDEA相比现有方法能耗降低38%，且满足所有时间和内存需求。

Conclusion: MEDEA通过集成多种优化技术，显著提升了异构超低功耗平台上DNN推理的能效。

Abstract: The growing demand for on-device AI necessitates energy-efficient execution
of DNN based applications on resource-constrained ultra-low power (ULP)
platforms. Heterogeneous architectures, combining specialized processing
elements (PEs), have emerged as a key solution for achieving the required
performance and energy efficiency. However, optimizing energy while executing
applications on these platforms requires efficiently managing platform
resources like PEs, power features, and memory footprint, all while adhering to
critical application deadlines. This paper presents MEDEA, a novel design-time
multi-objective manager for energy-efficient DNN inference on Heterogeneous ULP
(HULP) platforms. MEDEA uniquely integrates: kernel-level dynamic voltage and
frequency scaling (DVFS) for dynamic energy adaptation; kernel-level
granularity scheduling, suitable for specialized accelerators; memory-aware
adaptive tiling to navigate severe memory constraints; and all within a timing
constraint-based optimization strategy, which minimizes energy based on
application deadline. To showcase practical viability, we evaluate MEDEA on
HEEPtimize, a heterogeneous ULP platform (22 nm, FPGA-prototyped) featuring a
RISC-V processor besides Near-Memory Computing (NMC) and Coarse-Grained
Reconfigurable Array (CGRA) accelerators. Experimental results, using a
biomedical seizure detection case study, demonstrate that MEDEA achieves
overall energy reductions of up to 38% compared to representative
state-of-the-art methods, while consistently meeting all timing and memory
requirements. This effectiveness is attributed to its integrated features, with
our analysis showing that kernel-level DVFS alone can be responsible for over
31% of the energy savings in specific scenarios.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [56] [Can AI support student engagement in classroom activities in higher education?](https://arxiv.org/abs/2506.18941)
*Neha Rani,Sharan Majumder,Ishan Bhardwaj,Pedro Guillermo Feijoo Garcia*

Main category: cs.CY

TL;DR: 研究探讨了在大规模计算机科学课堂中，利用基于LLM的AI模型（如ChatGPT）提升学生学习内容参与度的潜力。


<details>
  <summary>Details</summary>
Motivation: 大规模课堂中学生与教师及学习材料互动不足，亟需技术手段改善参与度。

Method: 在软件工程课程中设计了使用ChatGPT的课堂活动，并进行了有无CAI工具的对比实验。

Result: ChatGPT在课堂活动中有效提升了学生对学习内容的参与度。

Conclusion: 基于LLM的AI工具（如ChatGPT）在大规模课堂中具有提升学生参与度的潜力。

Abstract: Lucrative career prospects and creative opportunities often attract students
to enroll in computer science majors and pursue advanced studies in the field.
Consequently, there has been a significant surge in enrollment in computer
science courses, resulting in large class sizes that can range from hundreds to
even thousands of students. A common challenge in such large classrooms is the
lack of engagement between students and both the instructor and the learning
material. However, with advancements in technology and improvements in large
language models (LLMs), there is a considerable opportunity to utilize
LLM-based AI models, such as conversational artificial intelligence (CAI), to
enhance student engagement with learning content in large classes. To explore
the potential of CAI to support engagement, especially with learning content,
we designed an activity in a software Engineering course (with a large class
size) where students used CAI for an in-class activity. We conducted a
within-subject investigation in a large classroom at a US university where we
compared student engagement during an in-class activity that used CAI tool vs.
one without CAI tool. The CAI tool we used was ChatGPT due to its widespread
popularity and familiarity. Our results indicate that CAI (ChatGPT) has the
potential to support engagement with learning content during in-class
activities, especially in large class sizes. We further discuss the
implications of our findings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [57] [Emotion Detection on User Front-Facing App Interfaces for Enhanced Schedule Optimization: A Machine Learning Approach](https://arxiv.org/abs/2506.19280)
*Feiting Yang,Antoine Moevus,Steve Lévesque*

Main category: cs.AI

TL;DR: 论文探讨了在日历应用中集成情绪检测技术，通过生物特征和行为数据实现动态用户界面，提升用户体验。比较发现行为方法更优，GRU网络在生物特征中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 通过情绪识别增强人机交互，提升日历应用的用户体验和生产效率。

Method: 结合生物特征（心率数据用LSTM和GRU处理）和行为分析（鼠标、键盘活动用机器学习分类）两种方法检测情绪。

Result: 行为方法准确性更高（鼠标交互达90%），GRU在生物特征中表现优于LSTM（价预测准确率84.38%）。

Conclusion: 行为方法更稳定准确，GRU在情绪维度预测中效果显著，为情绪识别应用提供了实用参考。

Abstract: Human-Computer Interaction (HCI) has evolved significantly to incorporate
emotion recognition capabilities, creating unprecedented opportunities for
adaptive and personalized user experiences. This paper explores the integration
of emotion detection into calendar applications, enabling user interfaces to
dynamically respond to users' emotional states and stress levels, thereby
enhancing both productivity and engagement. We present and evaluate two
complementary approaches to emotion detection: a biometric-based method
utilizing heart rate (HR) data extracted from electrocardiogram (ECG) signals
processed through Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)
neural networks to predict the emotional dimensions of Valence, Arousal, and
Dominance; and a behavioral method analyzing computer activity through multiple
machine learning models to classify emotions based on fine-grained user
interactions such as mouse movements, clicks, and keystroke patterns. Our
comparative analysis, from real-world datasets, reveals that while both
approaches demonstrate effectiveness, the computer activity-based method
delivers superior consistency and accuracy, particularly for mouse-related
interactions, which achieved approximately 90\% accuracy. Furthermore, GRU
networks outperformed LSTM models in the biometric approach, with Valence
prediction reaching 84.38\% accuracy.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [58] [Network Structures as an Attack Surface: Topology-Based Privacy Leakage in Federated Learning](https://arxiv.org/abs/2506.19260)
*Murtaza Rangwala,Richard O. Sinnott,Rajkumar Buyya*

Main category: cs.CR

TL;DR: 该论文首次全面分析了联邦学习系统中基于网络拓扑的隐私泄漏问题，揭示了攻击者如何利用不同程度的网络结构知识推断敏感数据分布，并提出了有效的防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有隐私研究主要关注梯度攻击，而网络拓扑知识的隐私影响尚未充分研究，因此需要填补这一空白。

Method: 通过分析4,720个攻击实例，研究了六种不同的对抗知识场景（包括完整和部分拓扑知识），并提出了三种攻击向量。同时，提出了结构噪声注入作为防御机制。

Result: 实验表明，部分拓扑知识场景下攻击效率可超过安全阈值，而提出的防御机制能将攻击减少高达51.4%。

Conclusion: 网络拓扑是联邦学习系统中的基本隐私漏洞，但通过拓扑感知的防御机制可以有效缓解。

Abstract: Federated learning systems increasingly rely on diverse network topologies to
address scalability and organizational constraints. While existing privacy
research focuses on gradient-based attacks, the privacy implications of network
topology knowledge remain critically understudied. We conduct the first
comprehensive analysis of topology-based privacy leakage across realistic
adversarial knowledge scenarios, demonstrating that adversaries with varying
degrees of structural knowledge can infer sensitive data distribution patterns
even under strong differential privacy guarantees. Through systematic
evaluation of 4,720 attack instances, we analyze six distinct adversarial
knowledge scenarios: complete topology knowledge and five partial knowledge
configurations reflecting real-world deployment constraints. We propose three
complementary attack vectors: communication pattern analysis, parameter
magnitude profiling, and structural position correlation, achieving success
rates of 84.1%, 65.0%, and 47.2% under complete knowledge conditions.
Critically, we find that 80% of realistic partial knowledge scenarios maintain
attack effectiveness above security thresholds, with certain partial knowledge
configurations achieving performance superior to the baseline complete
knowledge scenario. To address these vulnerabilities, we propose and
empirically validate structural noise injection as a complementary defense
mechanism across 808 configurations, demonstrating up to 51.4% additional
attack reduction when properly layered with existing privacy techniques. These
results establish that network topology represents a fundamental privacy
vulnerability in federated learning systems while providing practical pathways
for mitigation through topology-aware defense mechanisms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [HI-SQL: Optimizing Text-to-SQL Systems through Dynamic Hint Integration](https://arxiv.org/abs/2506.18916)
*Ganesh Parab,Zishan Ahmad,Dagnachew Birru*

Main category: cs.LG

TL;DR: HI-SQL提出了一种基于历史查询日志的提示生成机制，显著提升了复杂SQL查询的准确性，同时降低了计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决现有Text-to-SQL系统中多步管道带来的高计算成本、高延迟和错误传播问题。

Method: 利用历史查询日志生成上下文提示，指导SQL生成，避免多步管道的复杂性。

Result: 在多个基准数据集上显著提高了LLM生成的查询准确性，同时优化了计算效率和延迟。

Conclusion: HI-SQL为增强Text-to-SQL系统提供了一种高效且实用的解决方案。

Abstract: Text-to-SQL generation bridges the gap between natural language and
databases, enabling users to query data without requiring SQL expertise. While
large language models (LLMs) have significantly advanced the field, challenges
remain in handling complex queries that involve multi-table joins, nested
conditions, and intricate operations. Existing methods often rely on multi-step
pipelines that incur high computational costs, increase latency, and are prone
to error propagation. To address these limitations, we propose HI-SQL, a
pipeline that incorporates a novel hint generation mechanism utilizing
historical query logs to guide SQL generation. By analyzing prior queries, our
method generates contextual hints that focus on handling the complexities of
multi-table and nested operations. These hints are seamlessly integrated into
the SQL generation process, eliminating the need for costly multi-step
approaches and reducing reliance on human-crafted prompts. Experimental
evaluations on multiple benchmark datasets demonstrate that our approach
significantly improves query accuracy of LLM-generated queries while ensuring
efficiency in terms of LLM calls and latency, offering a robust and practical
solution for enhancing Text-to-SQL systems.

</details>


### [60] [Local Learning Rules for Out-of-Equilibrium Physical Generative Models](https://arxiv.org/abs/2506.19136)
*Cyrill Bösch,Geoffrey Roeder,Marc Serra-Garcia,Ryan P. Adams*

Main category: cs.LG

TL;DR: 该论文提出了一种通过学习局部规则来驱动基于分数的生成模型（SGMs）的非平衡协议的方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何从系统动力学或力测量中直接学习SGMs的驱动协议参数梯度，从而更高效地实现生成任务。

Method: 研究方法是通过局部学习规则计算驱动协议的参数梯度，并在一组非线性过阻尼振荡器网络中实现SGMs，应用于高斯混合采样和MNIST数据集图像生成。

Result: 结果表明，该方法能够成功驱动SGMs实现目标生成任务，包括2D高斯混合采样和MNIST数据集的0和1图像生成。

Conclusion: 论文结论表明，局部学习规则可以有效用于SGMs的非平衡驱动协议学习，为复杂系统的生成任务提供了新思路。

Abstract: We show that the out-of-equilibrium driving protocol of score-based
generative models (SGMs) can be learned via a local learning rule. The gradient
with respect to the parameters of the driving protocol are computed directly
from force measurements or from observed system dynamics. As a demonstration,
we implement an SGM in a network of driven, nonlinear, overdamped oscillators
coupled to a thermal bath. We first apply it to the problem of sampling from a
mixture of two Gaussians in 2D. Finally, we train a network of 10x10
oscillators to sample images of 0s and 1s from the MNIST dataset.

</details>


### [61] [A Batch-Insensitive Dynamic GNN Approach to Address Temporal Discontinuity in Graph Streams](https://arxiv.org/abs/2506.19282)
*Yang Zhou,Xiaoning Ren*

Main category: cs.LG

TL;DR: BADGNN提出了一种新的批处理无关框架，通过时间Lipschitz正则化和自适应注意力调整，解决了MDGNNs训练中因大批次导致的时间连续性问题。


<details>
  <summary>Details</summary>
Motivation: 动态图中保持时间连续性很重要，但大批次训练的MDGNNs会破坏事件序列，导致时间信息丢失和参数收敛困难。

Method: 提出了BADGNN框架，包括时间Lipschitz正则化（TLR）和自适应注意力调整（A3）两个核心组件。

Result: 在三个基准数据集上，BADGNN实现了高性能，支持更大的批次和更快的训练速度。

Conclusion: BADGNN有效解决了时间连续性问题，提升了训练效率和模型性能。

Abstract: In dynamic graphs, preserving temporal continuity is critical. However,
Memory-based Dynamic Graph Neural Networks (MDGNNs) trained with large batches
often disrupt event sequences, leading to temporal information loss. This
discontinuity not only deteriorates temporal modeling but also hinders
optimization by increasing the difficulty of parameter convergence. Our
theoretical study quantifies this through a Lipschitz upper bound, showing that
large batch sizes enlarge the parameter search space. In response, we propose
BADGNN, a novel batch-agnostic framework consisting of two core components: (1)
Temporal Lipschitz Regularization (TLR) to control parameter search space
expansion, and (2) Adaptive Attention Adjustment (A3) to alleviate attention
distortion induced by both regularization and batching. Empirical results on
three benchmark datasets show that BADGNN maintains strong performance while
enabling significantly larger batch sizes and faster training compared to TGN.
Our code is available at Code:
https://anonymous.4open.science/r/TGN_Lipichitz-C033/.

</details>


### [62] [GradualDiff-Fed: A Federated Learning Specialized Framework for Large Language Model](https://arxiv.org/abs/2506.19164)
*Amir Faiyaz,Tara Salman*

Main category: cs.LG

TL;DR: 论文提出了一种名为Gradual Diff-Fed的联邦学习框架，专注于高效调优大型语言模型（LLMs），通过仅传输模型权重差异显著降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，如何在隐私保护的前提下高效调优这些模型成为关键挑战，尤其是如何减少通信开销并保持性能。

Method: 提出Gradual Diff-Fed框架，通过仅传输模型权重差异而非完整模型来优化联邦学习中的通信效率。

Result: 实验表明，该框架在保持与集中训练相当性能的同时，显著减少了通信开销。

Conclusion: Gradual Diff-Fed为在隐私保护场景下高效调优大型语言模型提供了一种可行的解决方案。

Abstract: The rapid proliferation of large language models (LLMs) has created an
unprecedented demand for fine-tuning models for specialized domains, such as
medical science. While federated learning (FL) offers a decentralized and
privacy-preserving approach to collaboratively fine-tune LLMs without sharing
raw data, it presents significant challenges, particularly in performance and
managing large model sizes efficiently. In this paper, we introduce
GradualDiff-Fed, an FL framework designed explicitly for LLMs, and their
challenge of handling the high parameter size. GradualDiff-Fed reduces
communication costs by transmitting only the difference of model weights rather
than the entire model during training rounds. Such an approach significantly
improves scalability and communication efficiency, making it more feasible to
fine-tune LLMs across distributed clients without compromising performance. Our
evaluation demonstrates that GradualDiff-Fed achieves performance on par with
centralized training while drastically reducing communication overhead. These
results highlight the potential of GradualDiff-Fed as an efficient solution for
fine-tuning large models from distributed data in privacy-preserving settings
without comprising performance.

</details>


### [63] [Private Model Personalization Revisited](https://arxiv.org/abs/2506.19220)
*Conor Snedeker,Xinyu Zhou,Raef Bassily*

Main category: cs.LG

TL;DR: 该论文研究了在用户级别差分隐私（DP）下的模型个性化问题，提出了一种私有、高效的去中心化学习算法，以在异构数据中恢复共享嵌入和低维表示，并在隐私保护和效用之间取得了改进。


<details>
  <summary>Details</summary>
Motivation: 在去中心化学习中，用户数据的统计异质性使得模型个性化变得复杂，同时需要在隐私保护（差分隐私）下实现高效学习。现有方法在隐私保护和适用范围上存在限制，因此需要提出更通用的解决方案。

Method: 提出了一种基于FedRep算法的私有、高效的去中心化学习算法，学习共享嵌入和低维表示。算法利用了Johnson-Lindenstrauss变换降低维度，并在隐私保护下改进了效用保证。

Result: 在更广泛的用户分布（亚高斯分布）下实现了隐私保护，并在某些参数条件下改进了隐私误差。在二元分类任务中，进一步实现了与维度无关的风险边界。

Conclusion: 论文证明了在隐私保护下实现高效模型个性化的可能性，改进了现有方法的局限，并在理论和实际应用中取得了显著进展。

Abstract: We study model personalization under user-level differential privacy (DP) in
the shared representation framework. In this problem, there are $n$ users whose
data is statistically heterogeneous, and their optimal parameters share an
unknown embedding $U^* \in\mathbb{R}^{d\times k}$ that maps the user parameters
in $\mathbb{R}^d$ to low-dimensional representations in $\mathbb{R}^k$, where
$k\ll d$. Our goal is to privately recover the shared embedding and the local
low-dimensional representations with small excess risk in the federated
setting. We propose a private, efficient federated learning algorithm to learn
the shared embedding based on the FedRep algorithm in [CHM+21]. Unlike
[CHM+21], our algorithm satisfies differential privacy, and our results hold
for the case of noisy labels. In contrast to prior work on private model
personalization [JRS+21], our utility guarantees hold under a larger class of
users' distributions (sub-Gaussian instead of Gaussian distributions).
Additionally, in natural parameter regimes, we improve the privacy error term
in [JRS+21] by a factor of $\widetilde{O}(dk)$. Next, we consider the binary
classification setting. We present an information-theoretic construction to
privately learn the shared embedding and derive a margin-based accuracy
guarantee that is independent of $d$. Our method utilizes the
Johnson-Lindenstrauss transform to reduce the effective dimensions of the
shared embedding and the users' data. This result shows that
dimension-independent risk bounds are possible in this setting under a margin
loss.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [64] [Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation](https://arxiv.org/abs/2506.19352)
*Jisu Shin,Juhyun Oh,Eunsu Kim,Hoyun Song,Alice Oh*

Main category: cs.CL

TL;DR: 该论文提出了一种细粒度的评估框架，用于量化大型语言模型（LLM）在角色一致性上的表现，解决了现有方法在长文本生成中难以捕捉细微角色偏离的问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法通常为整个响应分配单一分数，难以捕捉长文本生成中细微的角色不一致问题，影响模型可靠性。

Method: 提出了一种原子级评估框架，包含三个关键指标，用于衡量角色对齐和一致性的细粒度表现。

Result: 实验证明，该框架能有效检测之前方法忽略的角色不一致问题，并揭示了任务结构和角色吸引力对模型适应性的影响。

Conclusion: 该框架为角色一致性的评估提供了更精确的方法，突出了在保持角色表达一致性方面的挑战。

Abstract: Ensuring persona fidelity in large language models (LLMs) is essential for
maintaining coherent and engaging human-AI interactions. However, LLMs often
exhibit Out-of-Character (OOC) behavior, where generated responses deviate from
an assigned persona, leading to inconsistencies that affect model reliability.
Existing evaluation methods typically assign single scores to entire responses,
struggling to capture subtle persona misalignment, particularly in long-form
text generation. To address this limitation, we propose an atomic-level
evaluation framework that quantifies persona fidelity at a finer granularity.
Our three key metrics measure the degree of persona alignment and consistency
within and across generations. Our approach enables a more precise and
realistic assessment of persona fidelity by identifying subtle deviations that
real users would encounter. Through our experiments, we demonstrate that our
framework effectively detects persona inconsistencies that prior methods
overlook. By analyzing persona fidelity across diverse tasks and personality
types, we reveal how task structure and persona desirability influence model
adaptability, highlighting challenges in maintaining consistent persona
expression.

</details>


### [65] [Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning](https://arxiv.org/abs/2506.19484)
*Russell Beale*

Main category: cs.CL

TL;DR: 论文回顾了大型语言模型（LLM）在高等教育中的应用，结合教育理论分析了其优势与不足，并提出了改进策略。


<details>
  <summary>Details</summary>
Motivation: 探讨如何将LLM驱动的对话系统更有效地融入教育实践，弥补理论与技术之间的差距。

Method: 综合文献与教育理论（如对话教学法），分析LLM的提示策略和检索增强生成（RAG）如何支持个性化学习。

Result: 发现LLM在知识共建和人性化互动方面的不足，并提出改进提示设计和检索机制的建议。

Conclusion: 论文旨在通过理论指导和技术优化，提升LLM在教育中的实用性和教学效果。

Abstract: Large Language Models (LLMs) are rapidly transforming education by enabling
rich conversational learning experiences. This article provides a comprehensive
review of how LLM-based conversational agents are being used in higher
education, with extensions to secondary and lifelong learning contexts. We
synthesize existing literature on LLMs in education and theories of
conversational and dialogic pedagogy - including Vygotsky's sociocultural
learning (scaffolding and the Zone of Proximal Development), the Socratic
method, and Laurillard's conversational framework - and examine how prompting
strategies and retrieval-augmented generation (RAG) can align LLM behaviors
with these pedagogical theories, and how it can support personalized, adaptive
learning. We map educational theories to LLM capabilities, highlighting where
LLM-driven dialogue supports established learning principles and where it
challenges or falls short of traditional pedagogical assumptions. Notable gaps
in applying prior theories to LLMs are identified, such as the models tendency
to provide direct answers instead of fostering co-construction of knowledge,
and the need to account for the constant availability and broad but non-human
expertise of LLM tutors. In response, we propose practical strategies to better
align LLM interactions with sound pedagogy - for example, designing prompts
that encourage Socratic questioning, scaffolded guidance, and student
reflection, as well as integrating retrieval mechanisms to ensure accuracy and
contextual relevance. Our aim is to bridge the gap between educational theory
and the emerging practice of AI-driven conversational learning, offering
insights and tools for making LLM-based dialogues more educationally productive
and theory-aligned.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [66] [In-Memory Sorting-Searching with Cayley Tree](https://arxiv.org/abs/2506.19379)
*Subrata Paul,Sukanta Das,Biplab K Sikdar*

Main category: cs.FL

TL;DR: 提出了一种新型内存计算模型，利用Cayley树结构减少CPU工作负载，适用于数据密集型任务。


<details>
  <summary>Details</summary>
Motivation: 降低CPU在数据密集型计算中的参与度，提升效率。

Method: 通过Cayley树结构实现内存计算，支持搜索、最大值/最小值计算和排序，硬件实现包括传统和新型内存架构，并在FPGA上验证。

Result: 内存搜索和极值计算的时间复杂度为O(log n)，排序为O(n log n)，优于现有设计。

Conclusion: 该内存计算模型高效且可行，为数据密集型任务提供了新解决方案。

Abstract: This work proposes a computing model to reduce the workload of CPU. It relies
on the data intensive computation in memory, where the data reside, and
effectively realizes an in-memory computing (IMC) platform. Each memory word,
with additional logic, acts as a tiny processing element which forms the node
of a Cayley tree. The Cayley tree in turn defines the framework for solving the
data intensive computational problems. It finds the solutions for in-memory
searching, computing the max (min) in-memory and in-memory sorting while
reducing the involvement of CPU. The worst case time complexities of the IMC
based solutions for in-memory searching and computing max (min) in-memory are
$\mathcal{O}\log{n}$. Such solutions are independent of the order of elements
in the list. The worst case time complexity of in-memory sorting, on the other
hand, is $\mathcal{O}(n\log{n})$. Two types of hardware implementations of the
IMC platform are proposed. One is based on the existing/conventional memory
architecture, and the other one is on a newly defined memory architecture. The
solutions are further implemented in FPGA platform to prove the effectiveness
of the IMC architecture while comparing with the state-of-the art designs.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [67] [Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices and Tensors](https://arxiv.org/abs/2506.19175)
*Benjamin Brock,Willow Ahrens,Hameer Abbasi,Timothy A. Davis,Juni Kim,James Kitchen,Spencer Patty,Isaac Virshup,Erik Welch*

Main category: cs.MS

TL;DR: Binsparse是一种跨平台的二进制稀疏矩阵和张量存储格式，通过JSON描述符和二进制数组实现高效存储，显著减少文件大小和解析时间。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的稀疏矩阵存储格式（Matrix Market和FROSTT）基于ASCII文本，导致文件大、解析慢，成为性能瓶颈。Binsparse旨在提供一种高效、模块化的二进制格式解决这一问题。

Method: Binsparse由JSON描述符和二进制数组组成，支持多种现代二进制容器（如HDF5、Zarr、NPZ），并提供了跨语言和框架的参考实现。

Result: 在SuiteSparse和FROSTT数据集上，Binsparse的HDF5 CSR格式无压缩时平均减少2.4倍文件大小，压缩时达7.5倍；解析速度在无压缩时提升26.5倍（读取）和31倍（写入）。

Conclusion: Binsparse提供了一种高效、通用的二进制稀疏矩阵和张量存储解决方案，显著提升了存储和解析性能。

Abstract: Sparse matrices and tensors are ubiquitous throughout multiple subfields of
computing. The widespread usage of sparse data has inspired many in-memory and
on-disk storage formats, but the only widely adopted storage specifications are
the Matrix Market and FROSTT file formats, which both use ASCII text. Due to
the inefficiency of text storage, these files typically have larger file sizes
and longer parsing times than binary storage formats, which directly store an
in-memory representation to disk. This can be a major bottleneck; since sparse
computation is often bandwidth-bound, the cost of loading or storing a matrix
to disk often exceeds the cost of performing a sparse computation. While it is
common practice for practitioners to develop their own, custom, non-portable
binary formats for high-performance sparse matrix storage, there is currently
no cross-platform binary sparse matrix storage format. We present Binsparse, a
cross-platform binary sparse matrix and tensor format specification. Binsparse
is a modular, embeddable format, consisting of a JSON descriptor, which
describes the matrix or tensor dimensions, type, and format, and a series of
binary arrays, which can be stored in all modern binary containers, such as
HDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse
spanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our
Binsparse format on every matrix in the SuiteSparse Matrix Collection and a
selection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format
shows file size reductions of 2.4x on average without compression and 7.5x with
compression. We evaluate our parser's read/write performance against a
state-of-the-art Matrix Market parser, demonstrating warm cache mean read
speedups of 26.5x without compression and 2.6x with compression, and write
speedups of 31x without compression and 1.4x with compression.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [68] [NIC-RobustBench: A Comprehensive Open-Source Toolkit for Neural Image Compression and Robustness Analysis](https://arxiv.org/abs/2506.19051)
*Georgii Bychkov,Khaled Abud,Egor Kovalev,Alexander Gushchin,Dmitriy Vatolin,Anastasia Antsiferova*

Main category: eess.IV

TL;DR: NIC-RobustBench是首个用于评估神经图像压缩（NIC）鲁棒性和对抗防御效能的开源框架，涵盖了广泛的编解码器，并支持扩展。


<details>
  <summary>Details</summary>
Motivation: 随着JPEG AI标准的发布，评估NIC方法的鲁棒性变得尤为重要，而此前的研究仅限于有限的编解码器和攻击类型。

Method: 开发了NIC-RobustBench框架，包含最多编解码器的NIC库，并可扩展，用于评估鲁棒性和RD性能。

Result: 通过NIC-RobustBench框架，对NIC的鲁棒性进行了全面分析。

Conclusion: 该框架为NIC鲁棒性研究提供了重要工具，填补了此前研究的空白。

Abstract: Adversarial robustness of neural networks is an increasingly important area
of research, combining studies on computer vision models, large language models
(LLMs), and others. With the release of JPEG AI -- the first standard for
end-to-end neural image compression (NIC) methods -- the question of evaluating
NIC robustness has become critically significant. However, previous research
has been limited to a narrow range of codecs and attacks. To address this, we
present \textbf{NIC-RobustBench}, the first open-source framework to evaluate
NIC robustness and adversarial defenses' efficiency, in addition to comparing
Rate-Distortion (RD) performance. The framework includes the largest number of
codecs among all known NIC libraries and is easily scalable. The paper
demonstrates a comprehensive overview of the NIC-RobustBench framework and
employs it to analyze NIC robustness. Our code is available online at
https://github.com/msu-video-group/NIC-RobustBench.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [69] [PEVLM: Parallel Encoding for Vision-Language Models](https://arxiv.org/abs/2506.19651)
*Letian Kang,Shixian Luo,Yiqiang Li,Xiaoyang Yu,Shenxuan Zhou,Yong Wu*

Main category: cs.CV

TL;DR: PEVLM是一种并行编码策略，旨在提升视觉语言模型（VLMs）的长视频理解效率，通过分块和共享机制降低计算复杂度，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制的二次复杂度限制了VLMs在长视频理解中的应用，需要一种无需微调的高效解决方案。

Method: 提出PEVLM，将输入分块处理，保留全注意力位置嵌入，并通过对齐注意力权重模拟全注意力分布，降低计算复杂度。

Result: 在LongVideoBench上，PEVLM提升了8.37%的准确率，计算速度提升7.47倍，端到端延迟降低40%。

Conclusion: PEVLM在低延迟、长上下文视频理解中表现出色，适用于自动驾驶等实际应用。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance in
video-language tasks, yet their application to long video understanding remains
constrained by the quadratic complexity of standard attention mechanisms. In
this paper, we propose \textbf{PEVLM}, a parallel encoding strategy
specifically designed to improve the prefill efficiency of VLMs without
requiring model finetuning. PEVLM partitions the input into block-wise segments
with a shared sink, preserves full-attention positional embeddings, and aligns
attention weights to mimic full-attention distributions. This design reduces
attention computation from $O((T \times N)^2)$ to $O(T \times N)$ while
maintaining high accuracy. Extensive experiments on the LongVideoBench
benchmark show that PEVLM achieves up to 8.37\% accuracy improvement over
existing inference-efficient methods and delivers up to 7.47x speedup in
attention computation and 40\% reduction in end-to-end latency. Under strict
latency constraints, PEVLM significantly outperforms baselines, raising
accuracy from 23.26\% to 61.03\%. These results highlight PEVLM's effectiveness
for low-latency, long-context video understanding, making it well-suited for
real-world applications such as autonomous driving.

</details>


### [70] [Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition](https://arxiv.org/abs/2506.19079)
*Iosif Tsangko,Andreas Triantafyllopoulos,Adem Abdelmoula,Adria Mallol-Ragolta,Bjoern W. Schuller*

Main category: cs.CV

TL;DR: 研究探讨了基础视觉语言模型（VLMs）在情感计算中依赖的视觉线索，揭示其行为模式及潜在风险。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在情感识别中依赖的视觉线索是否基于心理学基础或浅层学习，以评估其可靠性和公平性。

Method: 通过在AffectNet数据集上测试不同规模的VLMs，分析其性能变化及模型内部推理过程。

Result: 发现牙齿可见性是关键线索，最佳模型（GPT-4o）依赖面部属性（如眉毛位置）进行情感推理。

Conclusion: 虽然模型表现出高一致性，但也揭示了捷径学习、偏见和公平性风险，尤其在敏感领域。

Abstract: Foundation Models (FMs) are rapidly transforming Affective Computing (AC),
with Vision Language Models (VLMs) now capable of recognising emotions in zero
shot settings. This paper probes a critical but underexplored question: what
visual cues do these models rely on to infer affect, and are these cues
psychologically grounded or superficially learnt? We benchmark varying scale
VLMs on a teeth annotated subset of AffectNet dataset and find consistent
performance shifts depending on the presence of visible teeth. Through
structured introspection of, the best-performing model, i.e., GPT-4o, we show
that facial attributes like eyebrow position drive much of its affective
reasoning, revealing a high degree of internal consistency in its
valence-arousal predictions. These patterns highlight the emergent nature of
FMs behaviour, but also reveal risks: shortcut learning, bias, and fairness
issues especially in sensitive domains like mental health and education.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [71] [Robotics Under Construction: Challenges on Job Sites](https://arxiv.org/abs/2506.19597)
*Haruki Uchiito,Akhilesh Bhat,Koji Kusaka,Xiaoya Zhang,Hiraku Kinjo,Honoka Uehara,Motoki Koyama,Shinji Natsume*

Main category: cs.RO

TL;DR: 为解决建筑业劳动力短缺和生产率停滞问题，本文提出了一种基于CD110R-3履带运输车的自主负载运输系统，作为无人建筑工地的初步尝试。该系统整合了自主导航、车队管理和GNSS定位，初步探索了动态环境适应算法。


<details>
  <summary>Details</summary>
Motivation: 建筑业面临劳动力短缺和生产率停滞，自动化成为可持续基础设施发展的关键。

Method: 基于CD110R-3履带运输车，集成自主导航、车队管理和GNSS定位，并开始研究外部传感器感知与建图系统。

Result: 初步结果揭示了在变化地形导航、施工环境下感知及传感器布置优化等方面的挑战。

Conclusion: 展望未来，构建协作自主代理动态适应工地条件的生态系统是目标，本文为机器人驱动的建筑自动化提供了基础性见解，并指出了关键技术发展方向。

Abstract: As labor shortages and productivity stagnation increasingly challenge the
construction industry, automation has become essential for sustainable
infrastructure development. This paper presents an autonomous payload
transportation system as an initial step toward fully unmanned construction
sites. Our system, based on the CD110R-3 crawler carrier, integrates autonomous
navigation, fleet management, and GNSS-based localization to facilitate
material transport in construction site environments. While the current system
does not yet incorporate dynamic environment adaptation algorithms, we have
begun fundamental investigations into external-sensor based perception and
mapping system. Preliminary results highlight the potential challenges,
including navigation in evolving terrain, environmental perception under
construction-specific conditions, and sensor placement optimization for
improving autonomy and efficiency. Looking forward, we envision a construction
ecosystem where collaborative autonomous agents dynamically adapt to site
conditions, optimizing workflow and reducing human intervention. This paper
provides foundational insights into the future of robotics-driven construction
automation and identifies critical areas for further technological development.

</details>


### [72] [Probabilistic modelling and safety assurance of an agriculture robot providing light-treatment](https://arxiv.org/abs/2506.19620)
*Mustafa Adam,Kangfeng Ye,David A. Anisi,Ana Cavalcanti,Jim Woodcock,Robert Morris*

Main category: cs.RO

TL;DR: 该论文提出了一个农业机器人安全保证框架，通过概率建模和风险分析，量化风险缓解措施的效果，帮助设计和决策。


<details>
  <summary>Details</summary>
Motivation: 农民对农业机器人的信任取决于其可靠性、鲁棒性和安全性，因此需要开发一个安全保证框架以检测、跟踪和避开障碍物和人类。

Method: 使用危险识别和风险评估矩阵，通过三个状态机捕获机器人平台、传感器和感知系统及人类行为，利用PRISM概率模型检查器自动生成和分析概率模型。

Result: 量化了风险缓解措施的效果，展示了不同设计概念（如高性能物体检测系统或更复杂的警告系统）对风险降低的影响。

Conclusion: 该框架不仅适用于初始概念开发阶段，还可用于实施、部署和操作阶段，为农业机器人的安全设计提供重要参考。

Abstract: Continued adoption of agricultural robots postulates the farmer's trust in
the reliability, robustness and safety of the new technology. This motivates
our work on safety assurance of agricultural robots, particularly their ability
to detect, track and avoid obstacles and humans. This paper considers a
probabilistic modelling and risk analysis framework for use in the early
development phases. Starting off with hazard identification and a risk
assessment matrix, the behaviour of the mobile robot platform, sensor and
perception system, and any humans present are captured using three state
machines. An auto-generated probabilistic model is then solved and analysed
using the probabilistic model checker PRISM. The result provides unique insight
into fundamental development and engineering aspects by quantifying the effect
of the risk mitigation actions and risk reduction associated with distinct
design concepts. These include implications of adopting a higher performance
and more expensive Object Detection System or opting for a more elaborate
warning system to increase human awareness. Although this paper mainly focuses
on the initial concept-development phase, the proposed safety assurance
framework can also be used during implementation, and subsequent deployment and
operation phases.

</details>


### [73] [A Verification Methodology for Safety Assurance of Robotic Autonomous Systems](https://arxiv.org/abs/2506.19622)
*Mustafa Adam,David A. Anisi,Pedro Ribeiro*

Main category: cs.RO

TL;DR: 该论文提出了一种用于自主农业机器人安全保证的验证工作流程，涵盖从概念设计到运行时验证的整个开发周期，通过系统性的危害分析和风险评估来识别潜在风险，并验证安全控制器是否满足要求。


<details>
  <summary>Details</summary>
Motivation: 在共享的人类环境中部署自主机器人（如农业场景）需要严格的安全保证，以确保功能可靠性和符合法规。这些系统需在动态、非结构化的环境中安全运行，并与人类安全互动。

Method: 论文提出了一套完整的验证工作流程，包括危害分析、风险评估、安全控制器建模及其行为验证，以确保控制器满足安全要求。

Result: 该方法在农业机器人上的应用表明，它能够有效验证安全关键属性，并早期发现设计问题，从而助力开发更安全的自主系统。

Conclusion: 该验证方法为自主农业机器人提供了可靠的安全保证，并通过系统性分析和验证提升了整体安全性。

Abstract: Autonomous robots deployed in shared human environments, such as agricultural
settings, require rigorous safety assurance to meet both functional reliability
and regulatory compliance. These systems must operate in dynamic, unstructured
environments, interact safely with humans, and respond effectively to a wide
range of potential hazards. This paper presents a verification workflow for the
safety assurance of an autonomous agricultural robot, covering the entire
development life-cycle, from concept study and design to runtime verification.
The outlined methodology begins with a systematic hazard analysis and risk
assessment to identify potential risks and derive corresponding safety
requirements. A formal model of the safety controller is then developed to
capture its behaviour and verify that the controller satisfies the specified
safety properties with respect to these requirements. The proposed approach is
demonstrated on a field robot operating in an agricultural setting. The results
show that the methodology can be effectively used to verify safety-critical
properties and facilitate the early identification of design issues,
contributing to the development of safer robots and autonomous systems.

</details>


### [74] [Preserving Sense of Agency: User Preferences for Robot Autonomy and User Control across Household Tasks](https://arxiv.org/abs/2506.19202)
*Claire Yang,Heer Patel,Max Kleiman-Weiner,Maya Cakmak*

Main category: cs.RO

TL;DR: 研究探讨了用户对辅助机器人自主性的偏好，发现用户的控制感主要受机器人是否自主行动和第三方参与操作的影响。高自主性机器人会降低用户的控制感，但用户自行编程的机器人能保留控制感。高风险任务中，用户更偏好高控制权。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解用户对辅助机器人自主性的偏好及其对用户控制感的影响，以优化机器人设计。

Method: 研究方法为通过实验让参与者评估四种不同自主性水平下控制感，并对不同家庭任务中的机器人偏好进行排序。

Result: 结果显示用户控制感受机器人自主性和第三方参与的影响；用户自行编程的机器人能保留控制感，高风险任务中用户更偏好高控制权。

Conclusion: 结论是机器人设计需平衡自主性与用户控制感，特别是在高风险任务中应根据用户偏好调整自主性水平。

Abstract: Roboticists often design with the assumption that assistive robots should be
fully autonomous. However, it remains unclear whether users prefer highly
autonomous robots, as prior work in assistive robotics suggests otherwise. High
robot autonomy can reduce the user's sense of agency, which represents feeling
in control of one's environment. How much control do users, in fact, want over
the actions of robots used for in-home assistance? We investigate how robot
autonomy levels affect users' sense of agency and the autonomy level they
prefer in contexts with varying risks. Our study asked participants to rate
their sense of agency as robot users across four distinct autonomy levels and
ranked their robot preferences with respect to various household tasks. Our
findings revealed that participants' sense of agency was primarily influenced
by two factors: (1) whether the robot acts autonomously, and (2) whether a
third party is involved in the robot's programming or operation. Notably, an
end-user programmed robot highly preserved users' sense of agency, even though
it acts autonomously. However, in high-risk settings, e.g., preparing a snack
for a child with allergies, they preferred robots that prioritized their
control significantly more. Additional contextual factors, such as trust in a
third party operator, also shaped their preferences.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [75] [Experimental Assessment of A Framework for In-body RF-backscattering Localization](https://arxiv.org/abs/2506.19499)
*Noa Jie Vives Zaguirre,Oscar Lasierra,Filip Lemic,Gerard Calvo Bartra,Pablo José Galván Calderón,Gines Garcia-Aviles,Sergi Abadal,Xavier Costa-Pérez*

Main category: eess.SP

TL;DR: 本研究探讨了基于射频反向散射的体内定位方法，用于胃肠道诊断和治疗。与传统侵入性方法相比，该方法显示出潜力，但受生物组织衰减和外部干扰影响。


<details>
  <summary>Details</summary>
Motivation: 传统胃肠道诊断方法如成像和内窥镜检查具有侵入性和分辨率限制，因此需要创新的非侵入性替代方案。

Method: 研究采用射频反向散射技术，通过实验评估在空气、鸡肉和猪肉组织中的谐波生成和接收性能。

Result: 结果显示，反向散射设备的位置、天线选择和增益设置显著影响性能，生物组织密度增加会导致更大衰减。外部干扰和塑料外壳也会影响传播。

Conclusion: 研究强调了干扰抑制和改进的传播模型对提升性能的重要性。

Abstract: Localization of in-body devices is beneficial for Gastrointestinal (GI)
diagnosis and targeted treatment. Traditional methods such as imaging and
endoscopy are invasive and limited in resolution, highlighting the need for
innovative alternatives. This study presents an experimental framework for
Radio Frequency (RF)-backscatter-based in-body localization, inspired by the
ReMix approach, and evaluates its performance in real-world conditions. The
experimental setup includes an in-body backscatter device and various off-body
antenna configurations to investigate harmonic generation and reception in air,
chicken and pork tissues. The results indicate that optimal backscatter device
positioning, antenna selection, and gain settings significantly impact
performance, with denser biological tissues leading to greater attenuation. The
study also highlights challenges such as external interference and plastic
enclosures affecting propagation. The findings emphasize the importance of
interference mitigation and refined propagation models to enhance performance.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [76] [Hypercubical manifolds in homotopy type theory](https://arxiv.org/abs/2506.19402)
*Samuel Mimram,Émile Oleon*

Main category: math.AT

TL;DR: 该论文在几何构造和证明的同伦类型理论框架下，引入了对应于超立方流形的类型，并证明其满足预期性质，同时提出高维推广。


<details>
  <summary>Details</summary>
Motivation: 研究目的是在几何构造的同伦类型理论中，模拟超立方流形并验证其性质，进而推广到更高维的构造。

Method: 通过同伦类型理论，定义了超立方流形类型，并基于组合计算和flattening lemma验证其性质，同时提出高维推广。

Result: 成功定义并验证了超立方流形类型的性质，提出了更高维的近似构造，逼近Q的解环化。

Conclusion: 同伦类型理论能有效模拟复杂几何构造，超立方流形的定义和推广为后续研究提供了新工具和方法。

Abstract: Homotopy type theory is a logical setting in which one can perform geometric
constructions and proofs in a synthetic way. Namely, types can be interpreted
as spaces up to homotopy, and proofs as homotopy invariant constructions. In
this context, we introduce a type which corresponds to the hypercubical
manifold, a space first introduced by Poincar\'e in 1895. Its importance stems
from the fact that it provides an approximation of the group Q of quaternionic
units, in the sense of being the first step of a cellular resolution of Q. In
order to ensure the validity of our definition, we show that it satisfies the
expected property: it is the homotopy quotient of the 3-sphere by the expected
action of Q. This is non-trivial and requires performing subtle combinatorial
computations based on the flattening lemma, thus illustrating the effective
nature of homotopy type theory. Finally, based on the previous construction, we
introduce new higher-dimensional generalizations of this manifold, which
correspond to better cellular approximations of Q, converging toward a
delooping of Q.

</details>
