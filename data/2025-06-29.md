<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 11]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CV](#cs.CV) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Domain Knowledge in Requirements Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.20754)
*Marina Araújo,Júlia Araújo,Romeu Oliveira,Lucas Romao,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文通过系统映射研究，综述了如何将领域知识有效融入需求工程（RE）的现有方法、技术和工具，总结了75篇相关论文的主要发现，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 领域知识是需求工程成功的关键，但目前缺乏对其如何有效使用的系统性总结。本文旨在填补这一空白。

Method: 采用混合搜索策略（数据库搜索与迭代的向后和向前滚雪球法）进行系统映射研究。

Result: 分析了75篇论文，总结了主要需求类型、常见质量属性以及领域知识形式化、获取和长期维护的挑战。

Conclusion: 研究为知识驱动的需求工程提供了概念和方法基础，并指出了未来发展的方向。

Abstract: [Context] Domain knowledge is recognized as a key component for the success
of Requirements Engineering (RE), as it provides the conceptual support needed
to understand the system context, ensure alignment with stakeholder needs, and
reduce ambiguity in requirements specification. Despite its relevance, the
scientific literature still lacks a systematic consolidation of how domain
knowledge can be effectively used and operationalized in RE. [Goal] This paper
addresses this gap by offering a comprehensive overview of existing
contributions, including methods, techniques, and tools to incorporate domain
knowledge into RE practices. [Method] We conducted a systematic mapping study
using a hybrid search strategy that combines database searches with iterative
backward and forward snowballing. [Results] In total, we found 75 papers that
met our inclusion criteria. The analysis highlights the main types of
requirements addressed, the most frequently considered quality attributes, and
recurring challenges in the formalization, acquisition, and long-term
maintenance of domain knowledge. The results provide support for researchers
and practitioners in identifying established approaches and unresolved issues.
The study also outlines promising directions for future research, emphasizing
the development of scalable, automated, and sustainable solutions to integrate
domain knowledge into RE processes. [Conclusion] The study contributes by
providing a comprehensive overview that helps to build a conceptual and
methodological foundation for knowledge-driven requirements engineering.

</details>


### [2] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文通过系统映射研究分析了敏捷管理在机器学习（ML）赋能系统中的应用现状，总结了八个关键主题和主要挑战。


<details>
  <summary>Details</summary>
Motivation: 机器学习开发的动态特性对传统项目管理提出了挑战，而敏捷方法的灵活性似乎适合解决这些问题。然而，如何在ML赋能系统中有效应用敏捷方法尚不明确。

Method: 采用混合搜索策略（数据库搜索与前后向雪球迭代结合）进行系统映射研究。

Result: 研究识别了27篇论文，总结了八个框架和主题，并指出ML任务中的努力估计是主要挑战。

Conclusion: 研究为领域现状提供了映射，并指出需要更多实证研究验证现有成果。

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [3] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Main category: cs.SE

TL;DR: 提出了一种用户友好的方法，利用Python和rdflib库简化Neo4j数据库与OWL的集成，解决了传统方法需要描述逻辑知识的限制，以FDA FAERS数据库为例展示了自动化生成类和公理的过程。


<details>
  <summary>Details</summary>
Motivation: 随着数据和知识的快速扩张，传统方法需要复杂的描述逻辑知识，限制了Neo4j与OWL的无缝集成，亟需一种更易用的方法。

Method: 使用Python和rdflib库开发脚本，自动从Neo4j数据库中生成OWL的类和公理，以FDA FAERS数据库为案例。

Result: 成功实现了一种无需深入描述逻辑知识即可集成Neo4j与OWL的方法，提升了知识图谱的生成效率。

Conclusion: 该方法为快速增长的不良药物事件数据集提供了实用的本体生成解决方案，有助于改善药物安全监测和公共卫生决策。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [4] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 论文探讨了检索增强生成（RAG）系统在现实场景中的应用，评估了五个领域的RAG系统，总结了十二个关键教训。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在事实准确性和上下文相关性方面的局限性，并通过实证研究填补RAG系统开发与评估的空白。

Method: 开发了五个领域的RAG应用，结合多语言OCR、语义检索和领域适配LLMs，并通过100名用户的网络评估进行六维分析。

Result: 用户评估显示系统在易用性、相关性、透明度等方面表现良好，同时总结了技术和伦理挑战。

Conclusion: RAG系统在现实应用中具有潜力，但需解决技术和伦理问题以提高可靠性和可用性。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [5] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Main category: cs.SE

TL;DR: 论文提出一种结合强化学习（RL）和人类指导的方法，用于开发复杂模型转换（MT）序列。人类指导能显著提升RL性能，即使建议存在不确定性。


<details>
  <summary>Details</summary>
Motivation: 复杂模型转换（如模型同步、自动修复）的开发容易出错且难以手动完成。RL虽然适合解决此类问题，但在复杂场景中性能不足。人类指导可以弥补这一缺陷。

Method: 提出一种技术框架，将用户定义的MT映射到RL原语，并通过RL程序执行以寻找最优MT序列。人类建议（即使不确定）被整合到RL过程中。

Result: 评估表明，人类指导显著提升RL性能，并更高效地开发复杂MT序列。不确定性和及时性之间的权衡优化了结果。

Conclusion: 该方法为RL驱动的“人在环路”工程方法迈出一步，证明人类指导对复杂MT开发的重要性。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [6] [Boosting Vulnerability Detection with Inter-function Multilateral Association Insights](https://arxiv.org/abs/2506.21014)
*Shaojian Qiu,Mengyang Huang,Jiahao Cheng*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vulnerability detection is a crucial yet challenging technique for ensuring
the security of software systems. Currently, most deep learning-based
vulnerability detection methods focus on stand-alone functions, neglecting the
complex inter-function interrelations, particularly the multilateral
associations. This oversight can fail to detect vulnerabilities in these
interrelations. To address this gap, we present an Inter-Function Multilateral
Association analysis framework for Vulnerability Detection (IFMA-VD). The
cornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and
utilizing hyperedge convolution to extract multilateral association features.
Specifically, we first parse functions into a code property graph to generate
intra-function features. Following this, we construct a code behavior
hypergraph by segmenting the program dependency graph to isolate and encode
behavioral features into hyperedges. Finally, we utilize a hypergraph network
to capture the multilateral association knowledge for augmenting vulnerability
detection. We evaluate IFMA-VD on three widely used vulnerability datasets and
demonstrate improvements in F-measure and Recall compared to baseline methods.
Additionally, we illustrate that multilateral association features can boost
code feature representation and validate the effectiveness of IFMA-VD on
real-world datasets.

</details>


### [7] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: 论文针对AI4RE领域公开标记数据集稀缺问题，提出Synthline v1方法，通过优化生成策略和提示技术提升合成需求数据质量，并在多项分类任务中验证其效果，部分任务性能甚至超越人工数据。


<details>
  <summary>Details</summary>
Motivation: 解决AI4RE领域公开标记数据集不足的问题，探索利用大语言模型生成高质量合成需求数据的系统性方法。

Method: 提出Synthline v1，扩展了之前版本，结合多样本提示、自动提示优化（PACE）和生成后筛选技术，评估其对四项分类任务数据质量的影响。

Result: 多样本提示显著提升数据多样性和实用性，PACE对功能分类效果显著但其他任务表现不一，合成数据在安全和缺陷分类任务中超越人工数据。

Conclusion: 系统性合成生成是解决数据集稀缺的有效途径，AI4RE领域可通过优化合成数据质量实现性能提升。

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [8] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Main category: cs.SE

TL;DR: 该论文提出了一种名为 $T^3$ 的创新框架，将大型语言模型（LLMs）的推理能力与树搜索结合，显著提高了自动程序修复（APR）的精度。


<details>
  <summary>Details</summary>
Motivation: 自动程序修复（APR）是软件开发与维护的核心技术，但由于其复杂的逻辑和多步推理需求，目前链式思考（CoT）技术的应用不足。

Method: 系统评估了几种常见CoT技术在APR任务中的表现，并提出$T^3$框架，将LLMs的推理能力与树搜索结合。

Result: $T^3$框架有效提高了生成候选修复方案的精度，并为APR任务中的样本选择和修复策略优化提供了指导。

Conclusion: $T^3$为高效自动化调试建立了坚实框架，展示了LLMs和CoT技术在APR领域的潜力。

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


### [9] [KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks](https://arxiv.org/abs/2506.21266)
*Daniil Karol,Elizaveta Artser,Ilya Vlasov,Yaroslav Golubev,Hieke Keuning,Anastasiia Birillo*

Main category: cs.SE

TL;DR: 论文介绍了一个名为KOALA的工具，用于收集学生在JetBrains IDE中解决编程任务时的代码快照和功能使用数据，解决了现有工具的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的数据收集工具在收集学生编程任务数据时存在局限性，如无法控制代码粒度、不收集特定编程环境事件等。这促使研究者开发了KOALA工具。

Method: KOALA是一个高度可配置的工具插件，可安装在JetBrains IDE中。它可以配置任务、启用或禁用IDE功能（如代码补全）、运行调查，并收集代码快照、IDE操作（如运行和调试）以及其他未在以往工作中收集的数据（如快捷键使用和文件切换）。

Result: 通过KOALA，研究者在两门课程中收集了28名学生的数据，展示了一些有趣的洞察。

Conclusion: KOALA是一个方便且高度可配置的工具，能够有效收集学生编程任务的详细数据，为研究和教育提供了宝贵的资源。

Abstract: Collecting data of students solving programming tasks is incredibly valuable
for researchers and educators. It allows verifying that the students correctly
apply the features and concepts they are taught, or finding students'
misconceptions. However, existing data collection tools have limitations, e.g.,
no control over the granularity of the collected code, not collecting the
specific events of the programming environment used, and overall being hard to
configure.
  To overcome these limitations, we propose KOALA, a convenient and highly
configurable tool for collecting code snapshots and feature usage from students
solving programming tasks in JetBrains IDEs. The plugin can be installed in
IDEs and configured to provide the students with the necessary tasks, enable or
disable certain IDE features like code completion, and run surveys. During
problem solving, the plugin collects code snapshots at the configured
granularity, all IDE actions like running and debugging, as well as some data
not collected in prior works, like employed hotkeys and switching focus between
files. The collected data is sent to the server that comes with the tool, where
it is stored and can be converted to the standardized ProgSnap2 format. To
showcase the tool, we collected data from 28 students solving tasks in two
courses within the IDE, highlighting some insights from this data.

</details>


### [10] [Exploring Micro Frontends: A Case Study Application in E-Commerce](https://arxiv.org/abs/2506.21297)
*Ricardo Hideki Hangai Kojo,Luiz Fernando Corte Real,Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman*

Main category: cs.SE

TL;DR: 论文探讨了微前端架构的适用性，通过实际案例展示其在特定场景下的优势与局限性。


<details>
  <summary>Details</summary>
Motivation: 研究微前端架构是否值得在工业中采用，特别是在面临技术债务和团队协作需求的场景下。

Method: 结合学术和灰色文献调查微前端的现状，并在一个手工艺品市场平台中实现该架构，最后通过开发者问卷评估效果。

Result: 微前端的采用解决了技术债务和团队独立性问题，但并非唯一解决方案；其便利性更多源于已有的微服务基础架构和团队协作经验。

Conclusion: 微前端在特定场景（如已有微服务基础设施）下是合适选择，但并非所有情况都必要。

Abstract: In the micro frontends architectural style, the frontend is divided into
smaller components, which can range from a simple button to an entire page. The
goal is to improve scalability, resilience, and team independence, albeit at
the cost of increased complexity and infrastructure demands. This paper seeks
to understand when it is worth adopting micro frontends, particularly in the
context of industry. To achieve this, we conducted an investigation into the
state of the art of micro frontends, based on both academic and gray
literature. We then implemented this architectural style in a marketplace for
handcrafted products, which already used microservices. Finally, we evaluated
the implementation through a semi-open questionnaire with the developers. At
the studied marketplace company, the need for architectural change arose due to
the tight coupling between their main system (a Java monolith) and a dedicated
frontend system. Additionally, there were deprecated technologies and poor
developer experience. To address these issues, the micro frontends architecture
was adopted, along with the API Gateway and Backend for Frontend patterns, and
technologies such as Svelte and Fastify. Although the adoption of Micro
Frontends was successful, it was not strictly necessary to meet the company's
needs. According to the analysis of the mixed questionnaire responses, other
alternatives, such as a monolithic frontend, could have achieved comparable
results. What made adopting micro frontends the most convenient choice in the
company's context was the monolith strangulation and microservices adoption,
which facilitated implementation through infrastructure reuse and knowledge
sharing between teams.

</details>


### [11] [An object-centric core metamodel for IoT-enhanced event logs](https://arxiv.org/abs/2506.21300)
*Yannis Bertrand,Christian Imenkamp,Lukas Malburg,Matthias Ehrendorfer,Marco Franceschetti,Joscha Grüger,Francesco Leotta,Jürgen Mangler,Ronny Seiger,Agnes Koschmider,Stefanie Rinderle-Ma,Barbara Weber,Estefania Serral*

Main category: cs.SE

TL;DR: 论文提出了一种融合物联网（IoT）与业务流程数据的核心模型，以解决现有数据模型碎片化问题，并通过Python原型验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 物联网技术与业务流程的结合产生了大量数据，但IoT数据与传统流程数据的特性差异导致集成困难，现有数据模型碎片化阻碍了协作与数据共享。

Method: 提出一个核心模型，综合现有数据模型的关键特征，并基于常见需求设计，通过Python原型进行评估。

Result: 核心模型成功满足常见需求，促进了流程挖掘领域的数据共享与协作。

Conclusion: 核心模型通过整合现有模型的优点，解决了IoT与业务流程数据集成的问题，为流程挖掘领域提供了统一的解决方案。

Abstract: Advances in Internet-of-Things (IoT) technologies have prompted the
integration of IoT devices with business processes (BPs) in many organizations
across various sectors, such as manufacturing, healthcare and smart spaces. The
proliferation of IoT devices leads to the generation of large amounts of IoT
data providing a window on the physical context of BPs, which facilitates the
discovery of new insights about BPs using process mining (PM) techniques.
However, to achieve these benefits, IoT data need to be combined with
traditional process (event) data, which is challenging due to the very
different characteristics of IoT and process data, for instance in terms of
granularity levels. Recently, several data models were proposed to integrate
IoT data with process data, each focusing on different aspects of data
integration based on different assumptions and requirements. This fragmentation
hampers data exchange and collaboration in the field of PM, e.g., making it
tedious for researchers to share data. In this paper, we present a core model
synthesizing the most important features of existing data models. As the core
model is based on common requirements, it greatly facilitates data sharing and
collaboration in the field. A prototypical Python implementation is used to
evaluate the model against various use cases and demonstrate that it satisfies
these common requirements.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [12] [Drift-Adaptive Slicing-Based Resource Management for Cooperative ISAC Networks](https://arxiv.org/abs/2506.20762)
*Shisheng Hu,Jie Gao,Xue Qin,Conghao Zhou,Xinyu Huang,Mushu Li,Mingcheng He,Xuemin Shen*

Main category: cs.NI

TL;DR: 提出了一种新颖的基于切片的漂移自适应资源管理方案，用于协作式综合感知与通信（ISAC）网络，通过数字孪生技术优化资源分配，提升服务满意度和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 传统的资源管理方案难以应对移动设备和感知目标的非静态空间分布，导致规划决策失效，需要一种自适应方法来优化资源分配。

Method: 采用网络切片技术，分别提供感知和通信服务；构建数字孪生模型，开发漂移自适应统计模型和仿真功能，优化决策和验证。

Result: 与基准方案相比，服务满意度比率提升了18%，资源消耗减少了13.1%。

Conclusion: 提出的漂移自适应切片资源管理方案能有效提升ISAC网络的性能，为动态环境下的资源分配提供了新思路。

Abstract: In this paper, we propose a novel drift-adaptive slicing-based resource
management scheme for cooperative integrated sensing and communication (ISAC)
networks. Particularly, we establish two network slices to provide sensing and
communication services, respectively. In the large-timescale planning for the
slices, we partition the sensing region of interest (RoI) of each mobile device
and reserve network resources accordingly, facilitating low-complexity
distance-based sensing target assignment in small timescales. To cope with the
non-stationary spatial distributions of mobile devices and sensing targets,
which can result in the drift in modeling the distributions and ineffective
planning decisions, we construct digital twins (DTs) of the slices. In each DT,
a drift-adaptive statistical model and an emulation function are developed for
the spatial distributions in the corresponding slice, which facilitates
closed-form decision-making and efficient validation of a planning decision,
respectively. Numerical results show that the proposed drift-adaptive
slicing-based resource management scheme can increase the service satisfaction
ratio by up to 18% and reduce resource consumption by up to 13.1% when compared
with benchmark schemes.

</details>


### [13] [Flowcut Switching: High-Performance Adaptive Routing with In-Order Delivery Guarantees](https://arxiv.org/abs/2506.21406)
*Tommaso Bonato,Daniele De Sensi,Salvatore Di Girolamo,Abdulla Bataineh,David Hewson,Duncan Roweth,Torsten Hoefler*

Main category: cs.NI

TL;DR: 提出了一种名为flowcut switching的新型自适应路由算法，旨在解决高性能计算中网络延迟和乱序数据包问题。


<details>
  <summary>Details</summary>
Motivation: 现有自适应路由算法在乱序数据包传输时可能导致性能下降或CPU利用率增加，尤其在非突发流量（如RDMA）场景下表现不佳。

Method: flowcut switching算法通过保证数据包按序传输，解决了现有算法（如flowlet switching）在非突发性流量和乱序问题上的不足。

Result: 该方法在各种网络条件下均能保证数据包的按序传输，适用于RDMA等非突发性流量场景。

Conclusion: flowcut switching是一种高效的自适应路由算法，能够显著提升网络性能，尤其是在高性能计算环境中。

Abstract: Network latency severely impacts the performance of applications running on
supercomputers. Adaptive routing algorithms route packets over different
available paths to reduce latency and improve network utilization. However, if
a switch routes packets belonging to the same network flow on different paths,
they might arrive at the destination out-of-order due to differences in the
latency of these paths. For some transport protocols like TCP, QUIC, and RoCE,
out-of-order (OOO) packets might cause large performance drops or significantly
increase CPU utilization. In this work, we propose flowcut switching, a new
adaptive routing algorithm that provides high-performance in-order packet
delivery. Differently from existing solutions like flowlet switching, which are
based on the assumption of bursty traffic and that might still reorder packets,
flowcut switching guarantees in-order delivery under any network conditions,
and is effective also for non-bursty traffic, as it is often the case for RDMA.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [14] [E-FreeM2: Efficient Training-Free Multi-Scale and Cross-Modal News Verification via MLLMs](https://arxiv.org/abs/2506.20944)
*Van-Hoang Phan,Long-Khanh Pham,Dang Vu,Anh-Duy Tran,Minh-Son Dao*

Main category: cs.MM

TL;DR: 提出了一种无需训练的检索式多模态事实验证系统，结合视觉语言模型和大语言模型，通过动态检索可信数据源来评估信息可信度，有效应对传统训练模型的漏洞。


<details>
  <summary>Details</summary>
Motivation: 移动和无线网络中错误信息的快速传播带来了关键的安全挑战，需要一种更可靠的方法来验证信息。

Method: 利用预训练的视觉语言模型和大语言模型，结合动态检索可信数据源，设计轻量级系统，实现无需训练的事实验证。

Result: 在两个事实检查基准测试中取得了SOTA结果，证明了其在错误信息检测和抗攻击方面的有效性。

Conclusion: 该系统具有在移动和无线通信环境中提升安全性的潜力，特别是在对抗攻击和轻量级部署方面表现优异。

Abstract: The rapid spread of misinformation in mobile and wireless networks presents
critical security challenges. This study introduces a training-free,
retrieval-based multimodal fact verification system that leverages pretrained
vision-language models and large language models for credibility assessment. By
dynamically retrieving and cross-referencing trusted data sources, our approach
mitigates vulnerabilities of traditional training-based models, such as
adversarial attacks and data poisoning. Additionally, its lightweight design
enables seamless edge device integration without extensive on-device
processing. Experiments on two fact-checking benchmarks achieve SOTA results,
confirming its effectiveness in misinformation detection and its robustness
against various attack vectors, highlighting its potential to enhance security
in mobile and wireless communication environments.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [15] [Pebble Games and Algebraic Proof Systems](https://arxiv.org/abs/2506.21149)
*Lisa-Marie Jaser,Jacobo Toran*

Main category: cs.LO

TL;DR: 该论文研究了pebble游戏与代数证明系统之间的强连接，证明了pebble游戏的策略与代数证明系统中的度与大小之间存在对应关系。


<details>
  <summary>Details</summary>
Motivation: 探索pebble游戏与代数证明系统之间的联系，揭示两者之间的结构相似性。

Method: 通过分析pebble公式的否定，将pebble游戏的策略与代数证明系统的度、大小和空间复杂度进行对应。

Result: 证明了pebble游戏的策略与代数证明系统的参数之间存在直接对应关系，并得出了代数证明系统之间的度分离和强规模-度权衡。

Conclusion: 该研究为pebble游戏与代数证明系统之间的联系提供了新的理论支持，并展示了这些联系如何用于证明代数证明系统的差异性。

Abstract: Analyzing refutations of the well known 0pebbling formulas Peb$(G)$ we prove
some new strong connections between pebble games and algebraic proof system,
showing that there is a parallelism between the reversible, black and
black-white pebbling games on one side, and the three algebraic proof systems
Nullstellensatz, Monomial Calculus and Polynomial Calculus on the other side.
In particular we prove that for any DAG $G$ with a single sink, if there is a
Monomial Calculus refutation for Peb$(G)$ having simultaneously degree $s$ and
size $t$ then there is a black pebbling strategy on $G$ with space $s$ and time
$t+s$. Also if there is a black pebbling strategy for $G$ with space $s$ and
time $t$ it is possible to extract from it a MC refutation for Peb$(G)$ having
simultaneously degree $s$ and size $ts$. These results are analogous to those
proven in {deRezende et al.21} for the case of reversible pebbling and
Nullstellensatz. Using them we prove degree separations between NS, MC and PC,
as well as strong degree-size tradeoffs for MC.
  We also notice that for any directed acyclic graph $G$ the space needed in a
pebbling strategy on $G$, for the three versions of the game, reversible, black
and black-white, exactly matches the variable space complexity of a refutation
of the corresponding pebbling formula Peb$(G)$ in each of the algebraic proof
systems NS, MC and PC. Using known pebbling bounds on graphs, this connection
implies separations between the corresponding variable space measures.

</details>


### [16] [Deciding Robust Instances of an Escape Problem for Dynamical Systems in Euclidean Space](https://arxiv.org/abs/2506.21481)
*Eike Neumann*

Main category: cs.LO

TL;DR: 研究在实数计算的比特模型中，如何判断一个点是否在连续映射的迭代下逃离一个闭集的问题，提出了一个完备的决策方法。


<details>
  <summary>Details</summary>
Motivation: 解决在实数计算的比特模型下，对点的逃逸行为进行决策的问题，特别是在函数扰动下保持稳健性的场景。

Method: 提出了一个完备的决策方法，其终止集包含所有其他决策方法的终止集，适用于一般连续函数，并扩展到仿射线性系统和二次复多项式。

Result: 该方法在所有问题实例的集合中是稠密的，且在复动力学的双曲猜想下，可以解决Mandelbrot集的可计算性问题。

Conclusion: 提出的方法在理论上具有完备性和广泛适用性，尤其在复杂动力学的特定问题中展示了强大的潜力。

Abstract: We study the problem of deciding whether a point escapes a closed subset of
$\mathbb{R}^d$ under the iteration of a continuous map $f \colon \mathbb{R}^d
\to \mathbb{R}^d$ in the bit-model of real computation. We give a sound partial
decision method for this problem which is complete in the sense that its
halting set contains the halting set of all sound partial decision methods for
the problem. Equivalently, our decision method terminates on all problem
instances whose answer is robust under all sufficiently small perturbations of
the function. We further show that the halting set of our algorithm is dense in
the set of all problem instances. While our algorithm applies to general
continuous functions, we demonstrate that it also yields complete decision
methods for much more rigid function families: affine linear systems and
quadratic complex polynomials. In the latter case, completeness is subject to
the density of hyperbolicity conjecture in complex dynamics. This in particular
yields an alternative proof of Hertling's (2004) conditional answer to a
question raised by Penrose (1989) regarding the computability of the Mandelbrot
set.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [17] [Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots](https://arxiv.org/abs/2506.20748)
*Jingshu Li,Zicheng Zhu,Renwen Zhang,Yi-Chieh Lee*

Main category: cs.HC

TL;DR: 研究了聊天机器人的人形化特征如何影响人类对其的共情行为，发现人类身份和情感表达能促进共情行为，共情是关键中介因素。


<details>
  <summary>Details</summary>
Motivation: 探索人类帮助聊天机器人的动机因素，填补研究空白。

Method: 基于CASA框架，设计在线实验（N=244），测量参与者对聊天机器人的共情行为和意图。

Result: 聊天机器人的人形化特征（身份和情感表达）显著增加人类的共情行为和意图。

Conclusion: 共情是理解人类对聊天机器人亲社会行为的关键，并有助于促进此类行为。

Abstract: Chatbots are increasingly integrated into people's lives and are widely used
to help people. Recently, there has also been growing interest in the reverse
direction-humans help chatbots-due to a wide range of benefits including better
chatbot performance, human well-being, and collaborative outcomes. However,
little research has explored the factors that motivate people to help chatbots.
To address this gap, we draw on the Computers Are Social Actors (CASA)
framework to examine how chatbot anthropomorphism-including human-like
identity, emotional expression, and non-verbal expression-influences human
empathy toward chatbots and their subsequent prosocial behaviors and
intentions. We also explore people's own interpretations of their prosocial
behaviors toward chatbots. We conducted an online experiment (N = 244) in which
chatbots made mistakes in a collaborative image labeling task and explained the
reasons to participants. We then measured participants' prosocial behaviors and
intentions toward the chatbots. Our findings revealed that human identity and
emotional expression of chatbots increased participants' prosocial behavior and
intention toward chatbots, with empathy mediating these effects. Qualitative
analysis further identified two motivations for participants' prosocial
behaviors: empathy for the chatbot and perceiving the chatbot as human-like. We
discuss the implications of these results for understanding and promoting human
prosocial behaviors toward chatbots.

</details>


### [18] ["TikTok, Do Your Thing": User Reactions to Social Surveillance in the Public Sphere](https://arxiv.org/abs/2506.20884)
*Meira Gilbert,Miranda Wei,Lindah Kotut*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: ''TikTok, Do Your Thing'' is a viral trend where users attempt to identify
strangers they see in public via information crowd-sourcing. The trend started
as early as 2021 and users typically engage with it for romantic purposes
(similar to a ''Missed Connections'' personal advertisement). This practice
includes acts of surveillance and identification in the public sphere, although
by peers rather than governments or corporations. To understand users'
reactions to this trend we conducted a qualitative analysis of 60 TikTok videos
and 1,901 user comments. Of the 60 videos reviewed, we find 19 individuals were
successfully identified. We also find that while there were comments expressing
disapproval (n=310), more than double the number expressed support (n=883).
Supportive comments demonstrated genuine interest and empathy, reflecting
evolving conceptions of community and algorithmic engagement. On the other
hand, disapproving comments highlighted concerns about inappropriate
relationships, stalking, consent, and gendered double standards. We discuss
these insights in relation to the normalization of interpersonal surveillance,
online stalking, and as an evolution of social surveillance to offer a new
perspective on user perceptions surrounding interpersonal surveillance and
identification in the public sphere.

</details>


### [19] [Effect of Haptic Feedback on Avoidance Behavior and Visual Exploration in Dynamic VR Pedestrian Environment](https://arxiv.org/abs/2506.20952)
*Kyosuke Ishibashi,Atsushi Saito,Zin Y. Tun,Lucas Ray,Megan C. Coram,Akihiro Sakurai,Allison M. Okamura,Ko Yamamoto*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Human crowd simulation in virtual reality (VR) is a powerful tool with
potential applications including emergency evacuation training and assessment
of building layout. While haptic feedback in VR enhances immersive experience,
its effect on walking behavior in dense and dynamic pedestrian flows is
unknown. Through a user study, we investigated how haptic feedback changes user
walking motion in crowded pedestrian flows in VR. The results indicate that
haptic feedback changed users' collision avoidance movements, as measured by
increased walking trajectory length and change in pelvis angle. The
displacements of users' lateral position and pelvis angle were also increased
in the instantaneous response to a collision with a non-player character (NPC),
even when the NPC was inside the field of view. Haptic feedback also enhanced
users' awareness and visual exploration when an NPC approached from the side
and back. Furthermore, variation in walking speed was increased by the haptic
feedback. These results suggested that the haptic feedback enhanced users'
sensitivity to a collision in VR environment.

</details>


### [20] [Follow the user meaningfully and product growth will follow: A mixed methods case study tying UX Point of View & Growth leading to measurable impact](https://arxiv.org/abs/2506.21195)
*Neha Raghuvanshi*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Have you wondered how cross-functional teams balance between maximizing value
that users derive and business growth leading to win-win situations? This case
study shows how User Experience Research (UXR) and Data Science teams used
mixed methods research to strategically influence Product Led Growth (PLG) for
a Password Manager used by million+ users, thus allowing our users, internal
teams, and business to win. The audience will take away practical
lessons/techniques related to leveraging mixed methods to: a. Maximize user
value while meeting business growth goals b. Influence cross-functional teams
c. Measure user and business impact This case study can be easily tied to the
UXR Point of view pyramid (POV) [2] that represents a methodological approach
to construct a POV and further dives into actioning POV to create measurable
user and business impact.

</details>


### [21] [Subtitled Media Adaptations for People with Aphasia: Ongoing Accessibility Barriers and Emerging Design Practices](https://arxiv.org/abs/2506.21201)
*Zihao You,Michael Crabb*

Main category: cs.HC

TL;DR: 摘要呼吁关注字幕对复杂可访问性需求人群的排斥问题，提出个性化字幕解决方案，并以失语症患者为例，强调需要包容性媒体研究与实践。


<details>
  <summary>Details</summary>
Motivation: 当前的字幕通用方案无法满足多样化需求，特别是失语症患者理解字幕的困难，亟需个性化改进。

Method: 通过开发原型工具和方法，将失语症患者的意见融入系统设计过程，以实现包容性媒体解决方案。

Result: 提出了一种更具包容性的媒体研究框架，强调个性化字幕设计的必要性。

Conclusion: 包容性设计和用户参与是改善媒体可访问性的关键，尤其是针对失语症等特殊需求群体。

Abstract: The consumption of subtitles via TVs, laptops and smartphones has the
potential to marginalize people based on their complex accessibility needs. The
current one-size-fits-all approach to this accessibility aid is no longer fit
for purpose and work is required to look at how it can be adapted to be
personalised for individual users based on individual context, content, and
consumption habits. People with Aphasia, for example, encounter significant
challenges in understanding subtitle texts.
  We see our work as a call to action for more inclusive practices, focusing on
how the thoughts and opinions of people with aphasia can be included in media
research. Our work investigates how to develop future media solutions for
people with aphasia to create a more inclusive media viewing environment. We
believe the key to this is appropriate prototyping tools and methods to allow
equitable inclusion in the system design process.

</details>


### [22] [Multimodal LLMs for Visualization Reconstruction and Understanding](https://arxiv.org/abs/2506.21319)
*Can Liu,Chunlin Da,Xiaoxiao Long,Yuxiao Yang,Yu Zhang,Yong Wang*

Main category: cs.HC

TL;DR: 论文提出了一种针对可视化理解的多模态大型模型，通过结合图表图像及其向量化表示，提升了数据提取和图表重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大型模型在自然图像理解方面表现良好，但在可视化理解中表现不佳，主要原因是它们无法解码数据到视觉的映射规则和提取结构化信息。

Method: 论文提出了一个新的数据集，并训练了专门用于可视化理解的多模态大型模型。方法结合了图表图像及其向量化表示、编码方案和数据特征。

Result: 实验结果表明，该方法在数据提取准确性和图表重建质量上均有显著提升。

Conclusion: 通过向量化表示和专门训练的多模态模型，可以更有效地理解和重建可视化内容。

Abstract: Visualizations are crucial for data communication, yet understanding them
requires comprehension of both visual elements and their underlying data
relationships. Current multimodal large models, while effective in natural
image understanding, struggle with visualization due to their inability to
decode the data-to-visual mapping rules and extract structured information. To
address these challenges, we present a novel dataset and train multimodal
visualization LLMs specifically designed for understanding. Our approach
combines chart images with their corresponding vectorized representations,
encoding schemes, and data features. The proposed vector format enables compact
and accurate reconstruction of visualization content. Experimental results
demonstrate significant improvements in both data extraction accuracy and chart
reconstruction quality.

</details>


### [23] [An evaluation of level of detail degradation in head-mounted display peripheries](https://arxiv.org/abs/2506.21441)
*Benjamin Watson,Neff Walker,Larry F Hodges,Martin Reddy*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A paradigm for the design of systems that manage level of detail in virtual
environments is proposed. As an example of the prototyping step in this
paradigm, a user study was performed to evaluate the effectiveness of high
detail insets used with head-mounted displays. Ten subjects were given a simple
search task that required the location and identification of a single target
object. All subjects used seven different displays (the independent variable),
varying in inset size and peripheral detail, to perform this task. Frame rate,
target location, subject input method, and order of display use were all
controlled. Primary dependent measures were search time on trials with correct
identification, and the percentage of all trials correctly identified. ANOVAs
of the results showed that insetless, high detail displays did not lead to
significantly different search times or accuracies than displays with insets.
In fact, only the insetless, low detail display returned significantly
different results. Further research is being performed to examine the effect of
varying task complexity, inset size, and level of detail.

</details>


### [24] ["Who Should I Believe?": User Interpretation and Decision-Making When a Family Healthcare Robot Contradicts Human Memory](https://arxiv.org/abs/2506.21322)
*Hong Wang,Natalia Calvo-Barajas,Katie Winkle,Ginevra Castellano*

Main category: cs.HC

TL;DR: 论文研究了机器人透明度和社交性对用户信任和决策的影响，结果显示了高透明度机器人如何改变用户对其信息差异的解释，并指出了用户过度依赖机器人的风险。


<details>
  <summary>Details</summary>
Motivation: 随着智能医疗机器人在家庭环境中的部署日益普及，当机器人的信息与用户记忆不一致时，会影响用户信任和决策，因此研究透明度和社交性的影响至关重要。

Method: 研究采用2x2的受试者间在线实验，176名参与者观看Furhat机器人在虚构场景中建议用药时间的视频，通过对比不同透明度和社交性水平的影响。

Result: 低透明度机器人导致用户倾向于认为自己记忆错误，而高透明度机器人则让用户更可能将差异归因于外部因素；用户表现出对机器人的过度信任。

Conclusion: 研究强调了机器人透明度和系统访问控制的重要性，以及在医疗等敏感领域中用户过度依赖机器人的潜在风险。

Abstract: Advancements in robotic capabilities for providing physical assistance,
psychological support, and daily health management are making the deployment of
intelligent healthcare robots in home environments increasingly feasible in the
near future. However, challenges arise when the information provided by these
robots contradicts users' memory, raising concerns about user trust and
decision-making. This paper presents a study that examines how varying a
robot's level of transparency and sociability influences user interpretation,
decision-making and perceived trust when faced with conflicting information
from a robot. In a 2 x 2 between-subjects online study, 176 participants
watched videos of a Furhat robot acting as a family healthcare assistant and
suggesting a fictional user to take medication at a different time from that
remembered by the user. Results indicate that robot transparency influenced
users' interpretation of information discrepancies: with a low transparency
robot, the most frequent assumption was that the user had not correctly
remembered the time, while with the high transparency robot, participants were
more likely to attribute the discrepancy to external factors, such as a partner
or another household member modifying the robot's information. Additionally,
participants exhibited a tendency toward overtrust, often prioritizing the
robot's recommendations over the user's memory, even when suspecting system
malfunctions or third-party interference. These findings highlight the impact
of transparency mechanisms in robotic systems, the complexity and importance
associated with system access control for multi-user robots deployed in home
environments, and the potential risks of users' over reliance on robots in
sensitive domains such as healthcare.

</details>


### [25] [Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles](https://arxiv.org/abs/2506.21456)
*Benjamin Watson,Neff Walker,Larry F Hodges*

Main category: cs.HC

TL;DR: 研究表明，在外围区域降低细节水平（LOD）对头动追踪大视场显示有效，本文提出了一种心理学模型，探讨了眼/头运动权衡，并指导外围降质显示的设计。实验发现，中心高细节区域的形状对搜索性能影响不大，而其面积显著影响性能，尤其是当水平和垂直视角至少30度时，性能与未降质显示无显著差异，验证了模型。


<details>
  <summary>Details</summary>
Motivation: 探讨外围降质显示的有效性及其设计依据。

Method: 提出基于眼/头运动权衡的心理学模型，并通过实验评估中心高细节区域的形状和面积对搜索性能的影响。

Result: 中心区域的形状对性能无显著影响，但面积（至少30度视角）显著影响性能，验证了模型。

Conclusion: 外围降质显示设计应关注中心区域的面积而非形状，且视角需至少30度以确保性能。

Abstract: Previous work has demonstrated the utility of reductions in the level of
detail (LOD) in the periphery of head-tracked, large field of view displays.
This paper provides a psychophysically based model, centered around an eye/head
movement tradeoff, that explains the effectiveness of peripheral degradation
and suggests how peripherally degraded displays should be designed. An
experiment evaluating the effect on search performance of the shape and area of
the high detail central area (inset) in peripherally degraded displays was
performed, results indicated that inset shape is not a significant factor in
performance. Inset area, however, was significant: performance with displays
subtending at least 30 degrees of horizontal and vertical angle was not
significantly different from performance with an undegraded display. These
results agreed with the proposed model.

</details>


### [26] [A Systematic Review of Human-AI Co-Creativity](https://arxiv.org/abs/2506.21333)
*Saloni Singh,Koen Hndriks,Drik Heylen,Kim Baraka*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The co creativity community is making significant progress in developing more
sophisticated and tailored systems to support and enhance human creativity.
Design considerations from prior work can serve as a valuable and efficient
foundation for future systems. To support this effort, we conducted a
systematic literature review of 62 papers on co-creative systems. These papers
cover a diverse range of applications, including visual arts, design, and
writing, where the AI acts not just as a tool but as an active collaborator in
the creative process. From this review, we identified several key dimensions
relevant to system design: phase of the creative process, creative task,
proactive behavior of the system, user control, system embodiment, and AI model
type. Our findings suggest that systems offering high user control lead to
greater satisfaction, trust, and a stronger sense of ownership over creative
outcomes. Furthermore, proactive systems, when adaptive and context sensitive,
can enhance collaboration. We also extracted 24 design considerations,
highlighting the value of encouraging users to externalize their thoughts and
of increasing the system's social presence and transparency to foster trust.
Despite recent advancements, important gaps remain, such as limited support for
early creative phases like problem clarification, and challenges related to
user adaptation to AI systems.

</details>


### [27] [Lightweight Fingernail Haptic Device: Unobstructed Fingerpad Force and Vibration Feedback for Enhanced Virtual Dexterous Manipulation](https://arxiv.org/abs/2506.21417)
*Yunxiu Xu,Siyu Wang,Shoichi Hasegawa*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study presents a lightweight, wearable fingertip haptic device that
provides physics-based haptic feedback for dexterous manipulation in virtual
environments without hindering real-world interactions. The device, designed
with thin strings and actuators attached to the fingernails, ensures minimal
weight (1.55 g per finger) and preserves finger flexibility. Integrating the
software with a physics engine renders multiple types of haptic feedback (grip
force, collision, and sliding vibration feedback). We evaluated the device's
performance in pressure perception, slip feedback, typical dexterous
manipulation tasks, and daily operations, and we gathered user experience
through subjective assessments. Our results show that participants could
perceive and respond to pressure and vibration feedback. Through dexterous
manipulation experiments, we further demonstrated that these minimal haptic
cues significantly improved virtual task efficiency, showcasing how lightweight
haptic feedback can enhance manipulation performance without complex
mechanisms. The device's ability to preserve tactile sensations and minimize
hindrance to real-world operations is a key advantage over glove-type haptic
devices. This research offers a potential solution for designing haptic
interfaces that balance lightweight construction, haptic feedback for dexterous
manipulation, and daily wearability.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [28] [Generative Blocks World: Moving Things Around in Pictures](https://arxiv.org/abs/2506.20703)
*Vaibhav Vavilala,Seemandhar Jain,Rahul Vasanth,D. A. Forsyth,Anand Bhattad*

Main category: cs.GR

TL;DR: 论文提出了Generative Blocks World方法，通过操纵简单的几何抽象来生成和编辑图像场景，实现高保真性和可编辑性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在编辑图像场景时难以兼顾几何一致性和纹理一致性，限制了编辑的灵活性和图像的真实性。

Method: 将场景表示为凸3D基元的组合，允许不同数量的基元表示同一场景；基于流的方法在深度和纹理提示的条件下生成图像。

Result: 方法在视觉保真度、可编辑性和组合泛化性上优于现有工作。

Conclusion: Generative Blocks World通过几何和纹理的联合优化，显著提升了图像编辑的效果和灵活性。

Abstract: We describe Generative Blocks World to interact with the scene of a generated
image by manipulating simple geometric abstractions. Our method represents
scenes as assemblies of convex 3D primitives, and the same scene can be
represented by different numbers of primitives, allowing an editor to move
either whole structures or small details. Once the scene geometry has been
edited, the image is generated by a flow-based method which is conditioned on
depth and a texture hint. Our texture hint takes into account the modified 3D
primitives, exceeding texture-consistency provided by existing key-value
caching techniques. These texture hints (a) allow accurate object and camera
moves and (b) largely preserve the identity of objects depicted. Quantitative
and qualitative experiments demonstrate that our approach outperforms prior
works in visual fidelity, editability, and compositional generalization.

</details>


### [29] [3DGH: 3D Head Generation with Composable Hair and Face](https://arxiv.org/abs/2506.20875)
*Chengan He,Junxuan Li,Tobias Kirschstein,Artem Sevastopolsky,Shunsuke Saito,Qingyang Tan,Javier Romero,Chen Cao,Holly Rushmeier,Giljoo Nam*

Main category: cs.GR

TL;DR: 3DGH是一种无条件生成3D人头部的模型，可将头发和面部组件分离建模，使用基于3D高斯泼溅的新数据表示和双生成器架构，实现高效的头发和面部合成与编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的模型通常将头发和面部纠缠在一起建模，限制了灵活性。3DGH旨在通过分离建模和引入可变形头发几何体，实现更灵活的头发和面部合成与编辑。

Method: 提出基于模板的3D高斯泼溅数据表示，设计双生成器GAN架构，并使用交叉注意力机制建模头发和面部的相关性。通过精心设计的训练目标稳定训练并促成分离。

Result: 实验验证了3DGH的设计选择，在无条件全头部图像合成和可组合的3D发型编辑任务中，性能优于现有方法。

Conclusion: 3DGH通过分离头发和面部建模，实现了高效且灵活的3D人头生成与编辑，为相关领域提供了新的解决方案。

Abstract: We present 3DGH, an unconditional generative model for 3D human heads with
composable hair and face components. Unlike previous work that entangles the
modeling of hair and face, we propose to separate them using a novel data
representation with template-based 3D Gaussian Splatting, in which deformable
hair geometry is introduced to capture the geometric variations across
different hairstyles. Based on this data representation, we design a 3D
GAN-based architecture with dual generators and employ a cross-attention
mechanism to model the inherent correlation between hair and face. The model is
trained on synthetic renderings using carefully designed objectives to
stabilize training and facilitate hair-face separation. We conduct extensive
experiments to validate the design choice of 3DGH, and evaluate it both
qualitatively and quantitatively by comparing with several state-of-the-art 3D
GAN methods, demonstrating its effectiveness in unconditional full-head image
synthesis and composable 3D hairstyle editing. More details will be available
on our project page: https://c-he.github.io/projects/3dgh/.

</details>


### [30] [Data Visualization for Improving Financial Literacy: A Systematic Review](https://arxiv.org/abs/2506.20901)
*Meng Du,Robert Amor,Kwan-Liu Ma,Burkhard C. Wünsche*

Main category: cs.GR

TL;DR: 金融素养通过数据可视化工具提升，本文系统综述了37篇研究，涵盖可视化工具在金融教育中的五大应用领域及其有效性。


<details>
  <summary>Details</summary>
Motivation: 金融素养对个人的财务决策至关重要，但许多人理解财务概念存在困难。数据可视化可以简化概念，提升学习和理解效果。

Method: 系统性综述37篇研究，分类为五大领域：可视图的时空演进、工具使用动机、财务主题与教学方法、工具与技术类型、教学干预效果评估。

Result: 研究为金融教育中的可视化工具提供了实用见解，并指出了未来的研究方向和机会。

Conclusion: 数据可视化在提升金融素养中具有重要价值，本文为教育者和专业人士提供了设计和应用视觉工具的有效方法。

Abstract: Financial literacy empowers individuals to make informed and effective
financial decisions, improving their overall financial well-being and security.
However, for many people understanding financial concepts can be daunting and
only half of US adults are considered financially literate. Data visualization
simplifies these concepts, making them accessible and engaging for learners of
all ages. This systematic review analyzes 37 research papers exploring the use
of data visualization and visual analytics in financial education and literacy
enhancement. We classify these studies into five key areas: (1) the evolution
of visualization use across time and space, (2) motivations for using
visualization tools, (3) the financial topics addressed and instructional
approaches used, (4) the types of tools and technologies applied, and (5) how
the effectiveness of teaching interventions was evaluated. Furthermore, we
identify research gaps and highlight opportunities for advancing financial
literacy. Our findings offer practical insights for educators and professionals
to effectively utilize or design visual tools for financial literacy.

</details>


### [31] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.

</details>


### [32] [FairyGen: Storied Cartoon Video from a Single Child-Drawn Character](https://arxiv.org/abs/2506.21272)
*Jiayi Zheng,Xiaodong Cun*

Main category: cs.GR

TL;DR: FairyGen是一个从单张儿童绘画生成故事驱动卡通视频的系统，保留了原画的独特艺术风格，并提供了连贯的叙事和高质量视觉表现。


<details>
  <summary>Details</summary>
Motivation: 现有的故事生成方法通常只关注角色一致性和基本动作，无法同时保持艺术风格和叙事连贯性。FairyGen旨在解决这一问题。

Method: 系统通过四个步骤实现：(1) 使用MLLM生成结构化故事板；(2) 风格传播适配器保持角色视觉风格；(3) 镜头设计模块增强视觉多样性；(4) 3D代理和运动定制适配器生成逼真动画。

Result: 实验表明，FairyGen能生成风格一致、叙事连贯且视觉多样化的动画，适合个性化故事制作。

Conclusion: FairyGen在保留原画风格的同时，提供高质量的叙事动画，具有广泛的应用潜力。

Abstract: We propose FairyGen, an automatic system for generating story-driven cartoon
videos from a single child's drawing, while faithfully preserving its unique
artistic style. Unlike previous storytelling methods that primarily focus on
character consistency and basic motion, FairyGen explicitly disentangles
character modeling from stylized background generation and incorporates
cinematic shot design to support expressive and coherent storytelling. Given a
single character sketch, we first employ an MLLM to generate a structured
storyboard with shot-level descriptions that specify environment settings,
character actions, and camera perspectives. To ensure visual consistency, we
introduce a style propagation adapter that captures the character's visual
style and applies it to the background, faithfully retaining the character's
full visual identity while synthesizing style-consistent scenes. A shot design
module further enhances visual diversity and cinematic quality through frame
cropping and multi-view synthesis based on the storyboard. To animate the
story, we reconstruct a 3D proxy of the character to derive physically
plausible motion sequences, which are then used to fine-tune an MMDiT-based
image-to-video diffusion model. We further propose a two-stage motion
customization adapter: the first stage learns appearance features from
temporally unordered frames, disentangling identity from motion; the second
stage models temporal dynamics using a timestep-shift strategy with frozen
identity weights. Once trained, FairyGen directly renders diverse and coherent
video scenes aligned with the storyboard. Extensive experiments demonstrate
that our system produces animations that are stylistically faithful,
narratively structured natural motion, highlighting its potential for
personalized and engaging story animation. The code will be available at
https://github.com/GVCLab/FairyGen

</details>


### [33] [IDGraphs: Intrusion Detection and Analysis Using Stream Compositing](https://arxiv.org/abs/2506.21425)
*Pin Ren,Yan Gao,Zhichun Li,Yan Chen,Benjamin Watson*

Main category: cs.GR

TL;DR: IDGraphs是一个交互式可视化系统，用于检测和分析网络中的入侵和异常流量，解决了现有系统在交互式检查、蠕虫传播分析和相关攻击发现方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着高速路由器的流量增长，现有入侵检测系统在交互式检查入侵、分析蠕虫传播模式和发现相关攻击方面能力有限，亟需改进。

Method: IDGraphs通过将流量数据按时间轴和未成功连接数汇总，并使用Histographs技术可视化数据频率，用户可通过交互式查询和分析高亮显示的流量模式。

Result: 该系统成功应用于包含1.79亿流量记录的1.16TB数据集，检测到端口扫描、蠕虫爆发、SYN洪泛等攻击和异常。

Conclusion: IDGraphs提供了一种高效的可视化方法，显著提升了入侵检测的准确性和交互性，尤其适用于分布式攻击的识别。

Abstract: Traffic anomalies and attacks are commonplace in today's networks and
identifying them rapidly and accurately is critical for large network
operators. For a statistical intrusion detection system (IDS), it is crucial to
detect at the flow-level for accurate detection and mitigation. However,
existing IDS systems offer only limited support for 1) interactively examining
detected intrusions and anomalies, 2) analyzing worm propagation patterns, 3)
and discovering correlated attacks. These problems are becoming even more acute
as the traffic on today's high-speed routers continues to grow.
  IDGraphs is an interactive visualization system for intrusion detection that
addresses these challenges. The central visualization in the system is a
flow-level trace plotted with time on the horizontal axis and aggregated number
of unsuccessful connections on the vertical axis. We then summarize a stack of
tens or hundreds of thousands of these traces using the Histographs [RW05]
technique, which maps data frequency at each pixel to brightness. Users may
then interactively query the summary view, performing analysis by highlighting
subsets of the traces. For example, brushing a linked correlation matrix view
highlights traces with similar patterns, revealing distributed attacks that are
difficult to detect using standard statistical analysis.
  We apply IDGraphs system to a real network router data-set with 179M
flow-level records representing a total traffic of 1.16TB. The system
successfully detects and analyzes a variety of attacks and anomalies, including
port scanning, worm outbreaks, stealthy TCP SYN floodings, and some distributed
attacks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [34] [Scalable GPU Performance Variability Analysis framework](https://arxiv.org/abs/2506.20674)
*Ankur Lahiry,Ayush Pokharel,Seth Ockerman,Amal Gueroudji,Line Pouchard,Tanzima Z. Islam*

Main category: cs.DC

TL;DR: 论文提出了一种分布式数据分析框架，用于高效分析大规模GPU性能日志，解决了现有工具在处理高复杂度和大容量数据时的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有的GPU性能日志分析工具因内存和时间消耗过大，难以集成到自动化工作流中，限制了性能分析的及时性和效率。

Method: 该系统通过将数据集分片为可并行分析的独立单元，利用MPI跨节点并发处理，降低了单节点内存压力，避免了中心化瓶颈。

Result: 框架成功应用于实际HPC和AI工作负载的Nsight Compute日志，能够诊断性能变异性，并揭示内存传输延迟对GPU内核行为的影响。

Conclusion: 该分布式框架显著提升了大规模性能分析的效率，为高复杂度和大容量数据的实时探索提供了可行方案。

Abstract: Analyzing large-scale performance logs from GPU profilers often requires
terabytes of memory and hours of runtime, even for basic summaries. These
constraints prevent timely insight and hinder the integration of performance
analytics into automated workflows. Existing analysis tools typically process
data sequentially, making them ill-suited for HPC workflows with growing trace
complexity and volume. We introduce a distributed data analysis framework that
scales with dataset size and compute availability. Rather than treating the
dataset as a single entity, our system partitions it into independently
analyzable shards and processes them concurrently across MPI ranks. This design
reduces per-node memory pressure, avoids central bottlenecks, and enables
low-latency exploration of high-dimensional trace data. We apply the framework
to end-to-end Nsight Compute traces from real HPC and AI workloads, demonstrate
its ability to diagnose performance variability, and uncover the impact of
memory transfer latency on GPU kernel behavior.

</details>


### [35] [ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data](https://arxiv.org/abs/2506.20673)
*Yongqian Sun,Xijie Pan,Xiao Xiong,Lei Tao,Jiaju Wang,Shenglin Zhang,Yuan Yuan,Yuqi Li,Kunlin Jian*

Main category: cs.DC

TL;DR: ClusterRCA提出了一种新框架，通过多模态数据定位HPC系统中的故障节点和类型，结合分类器和图方法，实验显示高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 网络故障诊断对HPC系统至关重要，现有方法因数据异构性和准确性不足无法直接应用。

Method: 利用多模态数据提取拓扑连接的NIC对特征，结合分类器和图方法构建故障图并进行随机游走。

Result: 在顶级HPC设备商数据集上，ClusterRCA表现出高准确性和跨场景鲁棒性。

Conclusion: ClusterRCA为HPC网络故障诊断提供了一种有效且通用的解决方案。

Abstract: Network failure diagnosis is challenging yet critical for high-performance
computing (HPC) systems. Existing methods cannot be directly applied to HPC
scenarios due to data heterogeneity and lack of accuracy. This paper proposes a
novel framework, called ClusterRCA, to localize culprit nodes and determine
failure types by leveraging multimodal data. ClusterRCA extracts features from
topologically connected network interface controller (NIC) pairs to analyze the
diverse, multimodal data in HPC systems. To accurately localize culprit nodes
and determine failure types, ClusterRCA combines classifier-based and
graph-based approaches. A failure graph is constructed based on the output of
the state classifier, and then it performs a customized random walk on the
graph to localize the root cause. Experiments on datasets collected by a
top-tier global HPC device vendor show ClusterRCA achieves high accuracy in
diagnosing network failure for HPC systems. ClusterRCA also maintains robust
performance across different application scenarios.

</details>


### [36] [Utility-Driven Speculative Decoding for Mixture-of-Experts](https://arxiv.org/abs/2506.20675)
*Anish Saxena,Po-An Tsai,Hritvik Taneja,Aamer Jaleel,Moinuddin Qureshi*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: GPU memory bandwidth is the main bottleneck for low-latency Large Language
Model (LLM) inference. Speculative decoding leverages idle GPU compute by using
a lightweight drafter to propose K tokens, which the LLM verifies in parallel,
boosting token throughput. In conventional dense LLMs, all model weights are
fetched each iteration, so speculation adds no latency overhead. Emerging
Mixture of Experts (MoE) models activate only a subset of weights per token,
greatly reducing data movement. However, we show that speculation is
ineffective for MoEs: draft tokens collectively activate more weights,
increasing data movement and verification time by 2-3x. When token throughput
gains fail to offset this overhead, speculation causes slowdowns up to 1.5x,
making it infeasible. Even when useful, the optimal K varies by task, model,
and even between requests and iterations. Thus, despite widespread use in dense
LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables
speculation to avoid slowdowns and dynamically tunes K to accelerate MoE
serving. Cascade uses a lightweight metric, speculation utility, the ratio of
token gains to verification cost, which shows iteration-level locality,
enabling periodic decisions via short test and longer set phases. For each
request, Cascade disables speculation if utility drops below one during
testing, and when utility exceeds one, tests multiple K-values to choose the
utility-maximizing K for the set phase. We implement Cascade in vLLM and
evaluate it on five popular MoEs with workloads spanning code, math,
extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and
improves throughput by 7-14% over static K, making speculative decoding
practical for MoEs.

</details>


### [37] [ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks](https://arxiv.org/abs/2506.20938)
*Joshua H. Davis,Daniel Nichols,Ishan Khillan,Abhinav Bhatele*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: GPGPU architectures have become significantly diverse in recent years, which
has led to an emergence of a variety of specialized programming models and
software stacks to support them. While portable execution models exist, they
still require significant developer effort to port to and optimize for
different hardware architectures. Recent advances in large language models
(LLMs) can help us reduce some of this programmer burden. In this paper, we
present a novel benchmark and testing framework, ParEval-Repo, which can be
used to evaluate the efficacy of LLM-based approaches in automatically
translating entire codebases across GPGPU execution models. ParEval-Repo
includes several scientific computing and AI mini-applications in a range of
programming models, and levels of repository complexity. We use ParEval-Repo to
evaluate a range of state-of-the-art open-source and commercial LLMs, with both
a non-agentic and a top-down agentic approach. We assess code generated by the
LLMs and approaches in terms of compilability, functional correctness,
categories of build errors, and the cost of translation in terms of the number
of inference tokens. Our results demonstrate that LLM translation of scientific
applications is feasible for small programs but difficulty with generating
functional build systems and cross-file dependencies pose challenges in scaling
to larger codebases.

</details>


### [38] [Portable High-Performance Kernel Generation for a Computational Fluid Dynamics Code with DaCe](https://arxiv.org/abs/2506.20994)
*Måns I. Andersson,Martin Karp,Niclas Jansson,Stefano Markidis*

Main category: cs.DC

TL;DR: 论文研究了如何利用DaCe框架自动生成高性能代码，以应对HPC系统中硬件多样性的挑战，并以CFD中的Neko求解器为例，展示了代码的可移植性和性能。


<details>
  <summary>Details</summary>
Motivation: 随着HPC加速器的多样化，开发兼容多种硬件架构的高性能代码成为挑战，DaCe框架旨在解决这一问题。

Method: 利用DaCe的Stateful Dataflow Multigraph (SDFG)表示，自动生成针对多核处理器和加速器的高性能代码。

Result: 实验表明，生成的代码在Nvidia和AMD GPU上具有可移植性和高性能。

Conclusion: 自动代码生成是确保大规模科学应用长期可持续性的可行方案。

Abstract: With the emergence of new high-performance computing (HPC) accelerators, such
as Nvidia and AMD GPUs, efficiently targeting diverse hardware architectures
has become a major challenge for HPC application developers. The increasing
hardware diversity in HPC systems often necessitates the development of
architecture-specific code, hindering the sustainability of large-scale
scientific applications. In this work, we leverage DaCe, a data-centric
parallel programming framework, to automate the generation of high-performance
kernels. DaCe enables automatic code generation for multicore processors and
various accelerators, reducing the burden on developers who would otherwise
need to rewrite code for each new architecture. Our study demonstrates DaCe's
capabilities by applying its automatic code generation to a critical
computational kernel used in Computational Fluid Dynamics (CFD). Specifically,
we focus on Neko, a Fortran-based solver that employs the spectral-element
method, which relies on small tensor operations. We detail the formulation of
this computational kernel using DaCe's Stateful Dataflow Multigraph (SDFG)
representation and discuss how this approach facilitates high-performance code
generation. Additionally, we outline the workflow for seamlessly integrating
DaCe's generated code into the Neko solver. Our results highlight the
portability and performance of the generated code across multiple platforms,
including Nvidia GH200, Nvidia A100, and AMD MI250X GPUs, with competitive
performance results. By demonstrating the potential of automatic code
generation, we emphasise the feasibility of using portable solutions to ensure
the long-term sustainability of large-scale scientific applications.

</details>


### [39] [Bridding OT and PaaS in Edge-to-Cloud Continuum](https://arxiv.org/abs/2506.21072)
*Carlos J Barrios,Yves Denneulin*

Main category: cs.DC

TL;DR: OTPaaS提供了一个高效管理数据的框架，提升响应时间、安全性及可靠性，适用于边缘和云环境。


<details>
  <summary>Details</summary>
Motivation: 为工业转型和数据主权提供高效、安全的数据管理解决方案。

Method: 采用平台即服务模型，成功部署并整合边缘和云环境组件。

Result: 实现了响应时间、安全性及可靠性的提升。

Conclusion: OTPaaS为工业转型提供了高效、灵活的解决方案，解决了关键挑战。

Abstract: The Operational Technology Platform as a Service (OTPaaS) initiative provides
a structured framework for the efficient management and storage of data. It
ensures excellent response times while improving security, reliability, data
and technology sovereignty, robustness, and energy efficiency, which are
crucial for industrial transformation and data sovereignty. This paper
illustrates successful deployment, adaptable application management, and
various integration components catering to Edge and Cloud environments. It
leverages the advantages of the Platform as a Service model and highlights key
challenges that have been addressed for specific use cases.

</details>


### [40] [BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient LLM Services](https://arxiv.org/abs/2506.21033)
*Zhaojiacheng Zhou,Hongze Liu,Shijing Yuan,Hanning Zhang,Jiong Lou,Chentao Wu,Jie Li*

Main category: cs.DC

TL;DR: 提出基于区块链的外部知识框架，解决LLM幻觉问题，通过分布式知识孤岛共享确保数据安全和知识质量。


<details>
  <summary>Details</summary>
Motivation: LLM的幻觉问题日益受到关注，但分散的知识孤岛因隐私和安全问题难以共享，亟需解决方案。

Method: 利用区块链技术，将知识提炼为提示，引入声誉机制和交叉验证，设计查询生成框架。

Result: 实验表明，该框架在区块链环境下实现了高效的LLM知识共享。

Conclusion: 区块链技术能有效协调知识孤岛，提升LLM知识检索的可靠性和安全性。

Abstract: The hallucination problem of Large Language Models (LLMs) has increasingly
drawn attention. Augmenting LLMs with external knowledge is a promising
solution to address this issue. However, due to privacy and security concerns,
a vast amount of downstream task-related knowledge remains dispersed and
isolated across various "silos," making it difficult to access. To bridge this
knowledge gap, we propose a blockchain-based external knowledge framework that
coordinates multiple knowledge silos to provide reliable foundational knowledge
for large model retrieval while ensuring data security. Technically, we distill
knowledge from local data into prompts and execute transactions and records on
the blockchain. Additionally, we introduce a reputation mechanism and
cross-validation to ensure knowledge quality and provide incentives for
participation. Furthermore, we design a query generation framework that
provides a direct API interface for large model retrieval. To evaluate the
performance of our proposed framework, we conducted extensive experiments on
various knowledge sources. The results demonstrate that the proposed framework
achieves efficient LLM service knowledge sharing in blockchain environments.

</details>


### [41] [Enabling Bitcoin Smart Contracts on the Internet Computer](https://arxiv.org/abs/2506.21327)
*Ryan Croote,Islam El-Ashi,Thomas Locher,Yvonne-Anne Pignolet*

Main category: cs.DC

TL;DR: 提出了一种在Internet Computer（IC）上执行比特币智能合约的新架构，避免了传统桥接的安全风险，并实现了快速最终化和低成本。


<details>
  <summary>Details</summary>
Motivation: 比特币自身可编程性有限，现有方法多依赖桥接机制，存在安全隐患。本文探索一种无需桥接的直接交互方式。

Method: 直接在IC区块链上执行比特币智能合约，通过新颖机制协调比特币的随机性和IC的不可逆状态变化。

Result: 实际测试显示，该架构能在几秒内完成最终化且成本低廉，支持了此前不现实的复杂去中心化应用。

Conclusion: 该架构为比特币智能合约提供了一种高效安全的解决方案，具有独立研究价值和应用潜力。

Abstract: There is growing interest in providing programmatic access to the value
locked in Bitcoin, which famously offers limited programmability itself.
Various approaches have been put forth in recent years, with the vast majority
of proposed mechanisms either building new functionality on top of Bitcoin or
leveraging a bridging mechanism to enable smart contracts that make use of
``wrapped'' bitcoins on entirely different platforms.
  In this work, an architecture is presented that follows a different approach.
The architecture enables the execution of Turing-complete Bitcoin smart
contracts on the Internet Computer (IC), a blockchain platform for hosting and
executing decentralized applications. Instead of using a bridge, IC and Bitcoin
nodes interact directly, eliminating potential security risks that the use of a
bridge entails. This integration requires novel concepts, in particular to
reconcile the probabilistic nature of Bitcoin with the irreversibility of
finalized state changes on the IC, which may be of independent interest.
  In addition to the presentation of the architecture, we provide evaluation
results based on measurements of the Bitcoin integration running on mainnet.
The evaluation results demonstrate that, with finalization in a few seconds and
low execution costs, this integration enables complex Bitcoin-based
decentralized applications that were not practically feasible or economically
viable before.

</details>


### [42] [Carbon-Aware Microservice Deployment for Optimal User Experience on a Budget](https://arxiv.org/abs/2506.21422)
*Kevin Kreutz,Philipp Wiesner,Monica Vitali*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The carbon footprint of data centers has recently become a critical concern.
So far, most carbon-aware strategies have focused on leveraging the flexibility
of scheduling decisions for batch processing by shifting the time and location
of workload executions. However, such approaches cannot be applied to
service-oriented cloud applications, since they have to be reachable at every
point in time and often at low latencies. We propose a carbon-aware approach
for operating microservices under hourly carbon budgets. By choosing the most
appropriate version and horizontal scaleout for each microservice, our strategy
maximizes user experience and revenue while staying within budget constraints.
Experiments across various application configurations and carbon budgets
demonstrate that the approach adapts properly to changing workloads and carbon
intensities.

</details>


### [43] [exa-AMD: A Scalable Workflow for Accelerating AI-Assisted Materials Discovery and Design](https://arxiv.org/abs/2506.21449)
*Maxim Moraru,Weiyi Xia,Zhuo Ye,Feng Zhang,Yongxin Yao,Ying Wai Li,Cai-Zhuang Wang*

Main category: cs.DC

TL;DR: exa-AMD是一个基于Python的应用程序，整合AI/ML工具、材料数据库和量子力学计算，加速功能材料的发现与设计。


<details>
  <summary>Details</summary>
Motivation: 旨在通过高性能工作流加速功能材料的发现与设计，提升研究效率。

Method: 利用任务并行编程库Parsl，实现从笔记本电脑到超级计算机的灵活任务执行，解耦工作流逻辑与执行配置。

Result: 研究者无需为不同系统重新实现工作流，即可实现高效扩展。

Conclusion: exa-AMD为材料科学研究提供了高效、灵活和可扩展的解决方案。

Abstract: exa-AMD is a Python-based application designed to accelerate the discovery
and design of functional materials by integrating AI/ML tools, materials
databases, and quantum mechanical calculations into scalable, high-performance
workflows. The execution model of exa-AMD relies on Parsl, a task-parallel
programming library that enables a flexible execution of tasks on any computing
resource from laptops to supercomputers. By using Parsl, exa-AMD is able to
decouple the workflow logic from execution configuration, thereby empowering
researchers to scale their workflows without having to reimplement them for
each system.

</details>


### [44] [Efficient and Reuseable Cloud Configuration Search Using Discovery Spaces](https://arxiv.org/abs/2506.21467)
*Michael Johnston,Burkhard Ringlein,Christoph Hagleitner,Alessandro Pomponio,Vassilis Vassiliadis,Christian Pinto,Srikumar Venugopal*

Main category: cs.DC

TL;DR: 提出了一种名为Discovery Space的抽象方法，用于形式化描述云资源配置问题，并支持高效、安全地探索大规模搜索空间。


<details>
  <summary>Details</summary>
Motivation: 为了解决在大量云资源配置选项中快速找到满足服务级别协议且成本最低的最优配置的问题。

Method: 提出了Discovery Space抽象，支持结构化和分布式的搜索空间探索，并实现了具体实现，适用于多种工作负载。

Result: 该方法支持优化器间安全共享数据，提升搜索效率，并能在相似搜索空间中实现90%以上的搜索加速。

Conclusion: Discovery Space为大规模配置搜索提供了有效工具，显著提升了搜索效率和知识复用能力。

Abstract: Finding the optimal set of cloud resources to deploy a given workload at
minimal cost while meeting a defined service level agreement is an active area
of research. Combining tens of parameters applicable across a large selection
of compute, storage, and services offered by cloud providers with similar
numbers of application-specific parameters leads to configuration spaces with
millions of deployment options.
  In this paper, we propose Discovery Space, an abstraction that formalizes the
description of workload configuration problems, and exhibits a set of
characteristics required for structured, robust and distributed investigations
of large search spaces. We describe a concrete implementation of the Discovery
Space abstraction and show that it is generalizable across a diverse set of
workloads such as Large Language Model inference and Big Data Analytics.
  We demonstrate that our approach enables safe, transparent sharing of data
between executions of best-of-breed optimizers increasing the efficiency of
optimal configuration detection in large search spaces. We also demonstrate how
Discovery Spaces enable transfer and reuse of knowledge across similar search
spaces, enabling configuration search speed-ups of over 90%.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [45] [Condensed Representation of RDF and its Application on Graph Versioning](https://arxiv.org/abs/2506.21203)
*Jey Puget Gil,Emmanuel Coquery,John Samuel,Gilles Gesquiere*

Main category: cs.DB

TL;DR: 该论文提出了一种关于知识图谱演化的凝缩表示方法，用于管理不断变化的数据。


<details>
  <summary>Details</summary>
Motivation: 研究知识图谱中的演化现象有助于理解实体间关系并预测未来趋势，但目前缺乏高效的数据组织方法。

Method: 提出并形式化了演化图谱的凝缩表示方法。

Result: 该方法能够更高效地管理和利用不断更新的知识图谱数据。

Conclusion: 凝缩表示方法为知识图谱管理提供了实用工具，便于数据分析和利用。

Abstract: The study of the evolving phenomena in a domain helps to understand the
relationships between entities at different points in time and predict future
trends. These phenomena, often complex, can be represented using knowledge
graphs, which have the capability to model heterogeneous data from multiple
sources. Nowadays, a considerable amount of sources delivering periodic updates
to knowledge graphs in various domains is openly available. The evolution of
data is of interest to knowledge graph management systems, and therefore it is
crucial to organize these constantly evolving data to make them easily
accessible and exploitable for analyzes. In this article, we will present and
formalize the condensed representation of these evolving graphs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [46] [Post-Quantum and Blockchain-Based Attestation for Trusted FPGAs in B5G Networks](https://arxiv.org/abs/2506.21073)
*Ilias Papalamprou,Nikolaos Fotos,Nikolaos Chatzivasileiadis,Anna Angelogianni,Dimosthenis Masouros,Dimitrios Soudris*

Main category: cs.AR

TL;DR: 论文提出了一种结合远程认证和量子安全加密算法的硬件-软件混合方案，以安全配置FPGA，并利用区块链确保安全性。


<details>
  <summary>Details</summary>
Motivation: 5G及更高网络的发展要求更高的性能，但FPGA部署在非保护环境中容易受到攻击，尤其是量子计算的威胁。

Method: 采用远程认证和量子安全加密算法（PQC），并集成区块链存储安全证据。

Result: 在两种FPGA家族中测试显示，相比非PQC方法，性能开销仅为2%。

Conclusion: 该方案能有效提升FPGA的安全性，适应量子计算时代的挑战。

Abstract: The advent of 5G and beyond has brought increased performance networks,
facilitating the deployment of services closer to the user. To meet performance
requirements such services require specialized hardware, such as Field
Programmable Gate Arrays (FPGAs). However, FPGAs are often deployed in
unprotected environments, leaving the user's applications vulnerable to
multiple attacks. With the rise of quantum computing, which threatens the
integrity of widely-used cryptographic algorithms, the need for a robust
security infrastructure is even more crucial. In this paper we introduce a
hybrid hardware-software solution utilizing remote attestation to securely
configure FPGAs, while integrating Post-Quantum Cryptographic (PQC) algorithms
for enhanced security. Additionally, to enable trustworthiness across the whole
edge computing continuum, our solution integrates a blockchain infrastructure,
ensuring the secure storage of any security evidence. We evaluate the proposed
secure configuration process under different PQC algorithms in two FPGA
families, showcasing only 2% overheard compared to the non PQC approach.

</details>


### [47] [Accelerating GNN Training through Locality-aware Dropout and Merge](https://arxiv.org/abs/2506.21414)
*Gongjian Sun,Mingyu Yan,Dengke Han,Runzhen Xue,Duo Wang,Xiaochun Ye,Dongrui Fan*

Main category: cs.AR

TL;DR: LiGNN是一种基于硬件的解决方案，通过改进数据局部性来加速GNN训练，采用丢弃与合并技术，显著提升性能并减少DRAM访问。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN加速器虽然利用了片上内存和数据访问调度策略，但仍需从DRAM中访问不规则地址的特征，导致性能下降。LiGNN通过改进数据局部性来解决这一问题。

Method: LiGNN在邻居聚合过程中应用局部感知的特征丢弃机制和DRAM行级的内存访问合并策略，以减少不规则DRAM访问。

Result: 在0.5的丢弃率下，LiGNN性能提升1.48~3.02倍，减少34%~55%的DRAM访问，降低59%~82%的DRAM行激活，同时保持模型精度。

Conclusion: LiGNN通过硬件优化有效提升了GNN训练效率，同时兼顾模型精度，为GNN加速提供了新思路。

Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in graph
learning and are widely adopted across various critical domains. However, the
irregular connectivity between vertices leads to inefficient neighbor
aggregation, resulting in substantial irregular and coarse-grained DRAM
accesses. This lack of data locality presents significant challenges for
execution platforms, ultimately degrading performance. While previous
accelerator designs have leveraged on-chip memory and data access scheduling
strategies to address this issue, they still inevitably access features at
irregular addresses from DRAM. In this work, we propose LiGNN, a hardware-based
solution that improves data locality by applying dropout and merge techniques
during neighbor aggregation to accelerate GNN training. Unlike conventional
algorithm-level dropout methods that primarily aim to improve accuracy while
overlooking hardware costs, LiGNN introduces a locality-aware feature dropout
mechanism. This approach selectively drops node features with data locality
awareness, effectively reducing irregular DRAM accesses without compromising
model accuracy. Moreover, by leveraging detailed knowledge of memory layout and
organization-including critical alignment constraints-LiGNN strategically
merges memory accesses during neighbor aggregation at the DRAM row level,
guided by GNN-level semantics. This optimization significantly improves data
locality with minimal additional cost. Under the commonly adopted 0.5 dropout
rate, LiGNN outperforms state-of-the-art methods, delivering a 1.48~3.02x
speedup, reducing DRAM accesses by 34%~55%, and lowering DRAM row activations
by 59%~82%, all while maintaining model accuracy.

</details>


### [48] [OptGM: An Optimized Gate Merging Method to Mitigate NBTI in Digital Circuits](https://arxiv.org/abs/2506.21487)
*Maryam Ghane,Amir M. Hajisadeghi,Hamid R. Zarandi*

Main category: cs.AR

TL;DR: OptGM是一种优化的门合并方法，用于减少数字电路中的NBTI效应。它通过识别关键节点并合并相关门电路，显著降低了NBTI关键晶体管数量和延迟退化，同时提升了性能成本比。


<details>
  <summary>Details</summary>
Motivation: 负偏压温度不稳定性（NBTI）会导致数字电路性能退化，特别是PMOS晶体管。OptGM旨在通过优化合并门电路来缓解这一问题。

Method: 1. 识别信号概率超过阈值的关键节点。2. 合并驱动关键节点和被其驱动的门电路为复杂门，消除关键节点。3. 在组合和时序基准电路上进行评估。

Result: NBTI关键晶体管数量减少89.29%，延迟退化减少23.87%，总晶体管数量减少6.47%，性能成本比提升12.8%，面积开销极小。

Conclusion: OptGM有效缓解了NBTI问题，显著提升了电路性能，同时保持了较低的实现成本。

Abstract: This paper presents OptGM, an optimized gate merging method designed to
mitigate negative bias temperature instability (NBTI) in digital circuits.
First, the proposed approach effectively identifies NBTI-critical internal
nodes, defined as those with a signal probability exceeding a predefined
threshold. Next, based on the proposed optimized algorithm, the sensitizer gate
(which drives the critical node) and the sensitive gate (which is fed by it)
are merged into a new complex gate. This complex gate preserves the original
logic while eliminating NBTI-critical nodes. Finally, to evaluate the
effectiveness of OptGM, we assess it on several combinational and sequential
benchmark circuits. Simulation results demonstrate that, on average, the number
of NBTI-critical transistors (i.e., PMOS transistors connected to critical
nodes), NBTI-induced delay degradation, and the total transistor count are
reduced by 89.29%, 23.87%, and 6.47%, respectively. Furthermore, OptGM enhances
performance per cost (PPC) by 12.8% on average, with minimal area overhead.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [49] [ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers](https://arxiv.org/abs/2506.21537)
*Nicholas S. DiBrita,Jason Han,Tirthak Patel*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Research in quantum machine learning has recently proliferated due to the
potential of quantum computing to accelerate machine learning. An area of
machine learning that has not yet been explored is neural ordinary differential
equation (neural ODE) based residual neural networks (ResNets), which aim to
improve the effectiveness of neural networks using the principles of ordinary
differential equations. In this work, we present our insights about why analog
Rydberg atom quantum computers are especially well-suited for ResNets. We also
introduce ResQ, a novel framework to optimize the dynamics of Rydberg atom
quantum computers to solve classification problems in machine learning using
analog quantum neural ODEs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [50] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


### [51] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Main category: cs.SD

TL;DR: 论文研究了Adapter的不同配置对两种AI音乐模型（MusicGen和Mustango）在低资源音乐类型（如印度斯坦古典音乐和土耳其Makam音乐）上的效果，发现了卷积和Transformer适配器的优缺点，并探讨了计算资源与性能之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 由于微调大规模音乐生成模型（如MusicGen和Mustango）需要大量计算资源，参数高效微调（PEFT）技术成为一种有潜力的替代方案。然而，适配器的设计选择多样，缺乏明确的指导，因此需要研究如何优化适配器配置以适应低资源音乐类型。

Method: 通过实验研究两种AI音乐模型（MusicGen和Mustango）在不同适配器配置下的表现，包括卷积和Transformer结构的适配器，并分析了它们在计算资源需求上的差异。

Result: 卷积适配器在捕捉局部音乐细节（如装饰音和短旋律）方面表现更优，而Transformer适配器更适合保持长距离依赖关系。此外，中等规模的适配器（40M参数）在表达力和生成质量之间取得最佳平衡。Mustango生成多样性更好但计算资源需求高且训练时间长，而MusicGen训练更快且效率更高，但生成内容稍显冗余。

Conclusion: 研究发现，适配器的选择应根据具体需求决定，不同任务和资源条件需要不同的适配器配置，中等规模的适配器可能是一种折衷方案。Mustango和MusicGen各有优劣，需根据实际应用场景选择模型。

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [52] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Main category: cs.AI

TL;DR: 开发了一个交互式可解释AI系统IXAII，整合了四种方法，为用户提供定制化解释，提升透明度。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法多为静态且忽略用户视角，限制了其有效性。

Method: 开发了交互式系统IXAII，结合LIME、SHAP、Anchors和DiCE四种方法，为不同用户群体提供定制化解释和可视化选项。

Result: 用户和专家评价显示IXAII能有效提升透明度。

Conclusion: IXAII整合交互性和多种解释方法，为人机交互和AI可解释性提供了新视角。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [53] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević,Ravi Hammond,Tobias Gessler,Anisoara Calinescu,Jonathan Cook,Matteo Gallici,Andrei Lupu,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: 为解决人类与AI协调中的评估难题，研究团队提出了AH2AC2挑战，并开发了基于大规模数据的‘人类代理’作为评估工具，同时开源了少量游戏数据以促进数据效率方法的研究。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，人类与AI的无缝协调是重要但尚未解决的挑战，Hanabi游戏因其复杂性成为理想测试平台，但人类评估的高成本和难复现限制了其应用。

Method: 研究通过AH2AC2挑战，利用大规模人类游戏数据训练‘人类代理’作为评估伙伴，并开源少量数据鼓励数据效率方法。

Result: 提出了两玩家和三玩家Hanabi场景的基线结果，并通过受控系统提供代理以公平评估。

Conclusion: AH2AC2为人类-AI协调研究提供了低成本、可复现的评估框架，推动了这一领域的进步。

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [54] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding,Renyu Zhang,Xinyu Feng,Chengye Xie,Zheng Zhang,Yanting Zhang*

Main category: cs.AI

TL;DR: 论文提出PsyLite，一种基于InternLM2.5-7B-chat的轻量级心理咨询大语言模型，通过混合蒸馏数据微调和ORPO偏好优化增强能力，并在多项评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI心理咨询模型在对话安全、场景处理及轻量化部署方面存在不足，需改进。

Method: 采用两阶段训练策略（混合蒸馏数据微调+ORPO偏好优化）和条件RAG技术，结合低硬件部署方案。

Result: PsyLite在多项评估中超越基线模型，心理辅导专业性提升47.6%，对话安全性提升2.4%，且仅需5GB内存。

Conclusion: PsyLite为资源受限环境下的心理咨询应用提供了可行方案。

Abstract: With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [55] [Hierarchical Sub-action Tree for Continuous Sign Language Recognition](https://arxiv.org/abs/2506.20947)
*Dejie Yang,Zhu Xu,Xinjie Gao,Yang Liu*

Main category: cs.CV

TL;DR: 提出HST-CSLR方法，通过构建层次化子动作树（HST）结合大语言模型的文本知识，优化连续手语识别中的视觉与文本模态对齐。


<details>
  <summary>Details</summary>
Motivation: 现有研究在连续手语识别（CSLR）中因标注数据和跨模态对齐不足而受限，需更高效利用文本知识。

Method: 构建HST结构，逐步对齐视觉与文本模态，并引入对比增强以减少计算复杂度。

Result: 在四个数据集上实验验证HST-CSLR的有效性。

Conclusion: HST-CSLR通过层次化结构更高效利用文本知识，提升CSLR性能。

Abstract: Continuous sign language recognition (CSLR) aims to transcribe untrimmed
videos into glosses, which are typically textual words. Recent studies indicate
that the lack of large datasets and precise annotations has become a bottleneck
for CSLR due to insufficient training data. To address this, some works have
developed cross-modal solutions to align visual and textual modalities.
However, they typically extract textual features from glosses without fully
utilizing their knowledge. In this paper, we propose the Hierarchical
Sub-action Tree (HST), termed HST-CSLR, to efficiently combine gloss knowledge
with visual representation learning. By incorporating gloss-specific knowledge
from large language models, our approach leverages textual information more
effectively. Specifically, we construct an HST for textual information
representation, aligning visual and textual modalities step-by-step and
benefiting from the tree structure to reduce computational complexity.
Additionally, we impose a contrastive alignment enhancement to bridge the gap
between the two modalities. Experiments on four datasets (PHOENIX-2014,
PHOENIX-2014T, CSL-Daily, and Sign Language Gesture) demonstrate the
effectiveness of our HST-CSLR.

</details>


### [56] [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552)
*Yutong Bai,Danny Tran,Amir Bar,Yann LeCun,Trevor Darrell,Jitendra Malik*

Main category: cs.CV

TL;DR: 这篇论文提出了一种通过人体动作预测第一人称视角视频（PEVA）的模型，结合了3D身体姿态和自回归条件扩散变换器，并在Nymeria数据集上进行了训练，展示了对复杂环境的模拟能力。


<details>
  <summary>Details</summary>
Motivation: 旨在探索如何通过人体动作（以3D姿态表示）预测第一人称视角视频，模拟人类行为对环境的影响，从而为模拟复杂现实环境和具身智能体行为提供基础。

Method: 利用3D身体姿态轨迹作为条件输入，结合关节层次结构，训练了一个自回归条件扩散变换器模型，并在大规模数据集Nymeria上进行训练。

Result: 设计了分层评估协议，验证了模型在具身预测和控制能力方面的表现，展示了模型对复杂现实环境的模拟能力。

Conclusion: 该研究是首次尝试从人类视角通过视频预测建模复杂现实环境和具身智能体行为，为后续相关研究提供了基础。

Abstract: We train models to Predict Ego-centric Video from human Actions (PEVA), given
the past video and an action represented by the relative 3D body pose. By
conditioning on kinematic pose trajectories, structured by the joint hierarchy
of the body, our model learns to simulate how physical human actions shape the
environment from a first-person point of view. We train an auto-regressive
conditional diffusion transformer on Nymeria, a large-scale dataset of
real-world egocentric video and body pose capture. We further design a
hierarchical evaluation protocol with increasingly challenging tasks, enabling
a comprehensive analysis of the model's embodied prediction and control
abilities. Our work represents an initial attempt to tackle the challenges of
modeling complex real-world environments and embodied agent behaviors with
video prediction from the perspective of a human.

</details>


### [57] [How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?](https://arxiv.org/abs/2506.20795)
*Stephanie Käs,Anton Burenko,Louis Markert,Onur Alp Culha,Dennis Mack,Timm Linder,Bastian Leibe*

Main category: cs.CV

TL;DR: 研究比较了V-JEPA、Gemini Flash 2.0和HD-GCN在动态全身手势识别中的表现，其中HD-GCN表现最佳，但V-JEPA通过简单分类头接近其性能，Gemini在零样本设置中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用通用视觉模型（VFMs）和视觉语言模型（VLMs）减少手势识别系统的复杂性，特别是在嘈杂的生产环境中。

Method: 比较了V-JEPA、Gemini Flash 2.0和HD-GCN三种模型在NUGGET数据集上的表现，分析了各自的输入表示和性能。

Result: HD-GCN表现最好，V-JEPA通过简单分类头接近其性能，而Gemini在零样本设置中难以区分手势。

Conclusion: V-JEPA有望通过共享多任务模型减少系统复杂性，但需进一步研究适合手势识别的输入表示。

Abstract: Gestures enable non-verbal human-robot communication, especially in noisy
environments like agile production. Traditional deep learning-based gesture
recognition relies on task-specific architectures using images, videos, or
skeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)
and Vision Language Models (VLMs) with their strong generalization abilities
offer potential to reduce system complexity by replacing dedicated
task-specific modules. This study investigates adapting such models for
dynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art
VFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing
skeleton-based approach). We introduce NUGGET, a dataset tailored for
human-robot communication in intralogistics environments, to evaluate the
different gesture recognition approaches. In our experiments, HD-GCN achieves
best performance, but V-JEPA comes close with a simple, task-specific
classification head - thus paving a possible way towards reducing system
complexity, by using it as a shared multi-task model. In contrast, Gemini
struggles to differentiate gestures based solely on textual descriptions in the
zero-shot setting, highlighting the need of further research on suitable input
representations for gestures.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [58] [MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models](https://arxiv.org/abs/2506.20686)
*Hoa La,Ahan Gupta,Alex Morehead,Jianlin Cheng,Minjia Zhang*

Main category: q-bio.BM

TL;DR: 本文介绍了MegaFold，一个跨平台系统，用于加速AlphaFold3（AF3）训练，通过缓存、高效内核和深度融合解决了计算和内存瓶颈，显著提升了训练效率和序列长度支持。


<details>
  <summary>Details</summary>
Motivation: AlphaFold3等蛋白质结构预测模型虽然推动了生物分子建模的发展，但其计算和内存需求高，导致训练难以扩展。MegaFold旨在解决这些瓶颈。

Method: MegaFold通过以下方法优化训练：提前缓存以减少GPU闲置时间；基于Triton的高效EvoAttention内核；深度融合常见小算子。

Result: 在NVIDIA H200和AMD MI250 GPU上，MegaFold将AF3训练的内存使用降低1.23倍，训练时间分别提升1.73倍和1.62倍，支持1.35倍更长的序列长度。

Conclusion: MegaFold显著提升了现代蛋白质折叠模型的可扩展性和训练效率，并已开源。

Abstract: Protein structure prediction models such as AlphaFold3 (AF3) push the
frontier of biomolecular modeling by incorporating science-informed
architectural changes to the transformer architecture. However, these advances
come at a steep system cost, introducing: compute- and memory-intensive
operators, 2D attention mechanisms, and retrieval-augmented data pipelines,
which collectively hinder the scalability of AF3 training. In this work, we
present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold
tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle
time from the retrieval-augmented data pipeline, Triton-based kernels for
memory-efficient EvoAttention on heterogeneous devices, and deep fusion for
common and critical small operators in AF3. Evaluation on both NVIDIA H200 and
AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by
up to 1.23$\times$ and improves per-iteration training time by up-to
1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables
training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines
without running out-of-memory, significantly improving the scalability of
modern protein folding models. We open source our code at
https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [59] [Metadata Enrichment of Long Text Documents using Large Language Models](https://arxiv.org/abs/2506.20918)
*Manika Lamba,You Peng,Sophie Nikolov,Glen Layne-Worthey,J. Stephen Downie*

Main category: cs.DL

TL;DR: 论文通过结合人工与大语言模型，丰富了HathiTrust数字图书馆中1920-2020年英文长文本的元数据，为计算社会科学等领域提供了宝贵资源。


<details>
  <summary>Details</summary>
Motivation: 提升数字图书馆中元数据的丰富性和可用性，以支持跨学科研究。

Method: 结合人工标注和大语言模型对元数据进行语义增强。

Result: 提供更丰富的元数据访问点，显著提升搜索效果和资源可访问性。

Conclusion: 大语言模型在元数据增强中效果显著，特别适用于现有元数据缺失严重的数字仓库。

Abstract: In this project, we semantically enriched and enhanced the metadata of long
text documents, theses and dissertations, retrieved from the HathiTrust Digital
Library in English published from 1920 to 2020 through a combination of manual
efforts and large language models. This dataset provides a valuable resource
for advancing research in areas such as computational social science, digital
humanities, and information science. Our paper shows that enriching metadata
using LLMs is particularly beneficial for digital repositories by introducing
additional metadata access points that may not have originally been foreseen to
accommodate various content types. This approach is particularly effective for
repositories that have significant missing data in their existing metadata
fields, enhancing search results and improving the accessibility of the digital
repository.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [60] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文讨论了如何通过工程实践提升大规模文本嵌入基准（MTEB）的可重复性和可扩展性，包括数据集验证、自动化测试和设计选择等。


<details>
  <summary>Details</summary>
Motivation: 确保MTEB的持续可重复性和可扩展性，以保持其在文本嵌入模型评估中的标准和相关性。

Method: 采用稳健的持续集成管道验证数据集完整性、自动化测试执行和评估结果的普适性。详细说明了设计选择以增强可重复性。

Result: 成功扩展了MTEB的覆盖范围，同时保持了质量和领域相关性。

Conclusion: 本文的经验为机器学习评估框架的可重复性和可用性提供了宝贵见解，尤其适用于基准维护者。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [61] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: LLM生成的研究想法在执行后评分显著低于人类专家想法，揭示了当前LLM在生成真正有效研究想法上的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成的研究想法在执行后是否优于人类专家想法，以验证其在实际研究中的有效性。

Method: 招募43位专家实施随机分配的研究想法（LLM生成或人类撰写），记录执行过程并进行盲审评分。

Result: 执行后，LLM想法的评分显著下降，与人类想法的差距缩小，部分指标甚至反转。

Conclusion: 当前LLM在生成真正有效的研究想法上仍有局限性，且仅基于想法阶段的评估可能不够准确。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [62] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文提出了通过扩展机器人格清单（MPI）和引入特定属性控制（SAC）框架，以16PF模型为基础，实现了对LLM人格特质的精细控制和动态调节。


<details>
  <summary>Details</summary>
Motivation: 为了满足LLMs在交互中表现出更细腻且可控的人格特质的需求，现有的基于Big Five模型的方法在维度和强度控制方面存在不足。

Method: 扩展MPI至16PF模型，开发SAC框架，通过形容词语义锚定和五个强度因子（频率、深度、阈值、努力和意愿）动态调节特质强度。

Result: 实验表明，连续谱系的强度调节相比二元切换更能实现一致可控的人格表达，且特质强度的变化会系统影响相关特质，表明LLMs内化了多维人格结构。

Conclusion: 该研究为人类与机器的交互提供了更细腻可控的方法，推动了LLMs在医疗、教育等领域的应用，使其更具人性化。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [63] [From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting](https://arxiv.org/abs/2506.21246)
*Giorgos Demosthenous,Chryssis Georgiou,Eliada Polydorou*

Main category: q-fin.PM

TL;DR: 研究探索了数据源多样性对加密货币预测模型性能的影响，通过整合多种数据类型，提出了一种新特征降维算法，发现数据多样性显著提升模型预测能力。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场高度波动，传统单一数据源预测模型表现有限，研究旨在探讨多数据源整合是否提升预测准确性。

Method: 引入了Crypto100指数，并提出新特征降维算法，从技术指标、链上数据、情感指标等多样数据中筛选关键特征。

Result: 实验表明，数据源多样性显著提高模型预测性能，尤其是链上数据对短长期预测均至关重要，传统市场指数对长期预测更相关。

Conclusion: 多数据源整合能显著提升加密货币预测模型的准确性和稳健性，为市场驱动因素提供了新见解。

Abstract: This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [64] [Rational Miner Behaviour, Protocol Stability, and Time Preference: An Austrian and Game-Theoretic Analysis of Bitcoin's Incentive Environment](https://arxiv.org/abs/2506.20965)
*Craig Steven Wright*

Main category: econ.GN

TL;DR: 该论文结合奥地利资本理论和重复博弈理论，分析区块链系统中不同制度条件下矿工的战略行为。研究发现，协议规则可变时会提高时间偏好，破坏长期规划和合作均衡。不变协议则能恢复战略一致性和可持续网络均衡。


<details>
  <summary>Details</summary>
Motivation: 探讨区块链系统中矿工行为如何受协议可变性影响，以及如何通过不变协议恢复战略一致性和可持续性。

Method: 采用形式化的博弈论分析和奥地利经济学原理，研究可变协议对矿工激励的影响。

Result: 可变协议导致矿工从生产性投资转向政治寻租和影响力博弈，而不变协议（如比特币原始协议）能支持可计算性和低时间偏好。

Conclusion: 协议不可变性是恢复战略一致性、企业家信心和可持续网络均衡的关键。

Abstract: This paper integrates Austrian capital theory with repeated game theory to
examine strategic miner behaviour under different institutional conditions in
blockchain systems. It shows that when protocol rules are mutable, effective
time preference rises, undermining rational long-term planning and cooperative
equilibria. Using formal game-theoretic analysis and Austrian economic
principles, the paper demonstrates how mutable protocols shift miner incentives
from productive investment to political rent-seeking and influence games. The
original Bitcoin protocol is interpreted as an institutional anchor: a fixed
rule-set enabling calculability and low time preference. Drawing on the work of
Bohm-Bawerk, Mises, and Hayek, the argument is made that protocol immutability
is essential for restoring strategic coherence, entrepreneurial confidence, and
sustainable network equilibrium.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [65] [Diophantine Equations over $\mathbb Z$: Universal Bounds and Parallel Formalization](https://arxiv.org/abs/2506.20909)
*Jonas Bayer,Marco David,Malte Hassler,Yuri Matiyasevich,Dierk Schleicher*

Main category: math.NT

TL;DR: 该论文研究Diophantine方程的复杂性边界，并探索与形式化定理证明并行开发数学证明的方法。通过显式通用对（ν, δ）为整数未知数取得新边界，并通过证明助手Isabelle进行形式化验证。


<details>
  <summary>Details</summary>
Motivation: 研究Hilbert第十问题中Diophantine方程的子类不可判定性，并为数学实践探索形式化验证的新方法。

Method: 开发显式通用对（ν, δ）以界定Diophantine方程的复杂性，并利用Isabelle进行并行形式化验证。

Result: 获得新的复杂性边界，并通过形式化验证证实结果的正确性。

Conclusion: 该研究不仅推进了Diophantine方程的理论，还为21世纪数学实践中的形式化验证提供了新思路。

Abstract: This paper explores multiple closely related themes: bounding the complexity
of Diophantine equations over the integers and developing mathematical proofs
in parallel with formal theorem provers.
  Hilbert's Tenth Problem (H10) asks about the decidability of Diophantine
equations and has been answered negatively by Davis, Putnam, Robinson and
Matiyasevich. It is natural to ask for which subclasses of Diophantine
equations H10 remains undecidable. Such subclasses can be defined in terms of
universal pairs: bounds on the number of variables $\nu$ and degree $\delta$
such that all Diophantine equations can be rewritten in at most this
complexity. Our work develops explicit universal pairs $(\nu, \delta)$ for
integer unknowns, achieving new bounds that cannot be obtained by naive
translations from known results over $\mathbb N$.
  In parallel, we have conducted a formal verification of our results using the
proof assistant Isabelle. While formal proof verification has traditionally
been applied a posteriori to known results, this project integrates
formalization into the discovery and development process. In a final section,
we describe key insights gained from this unusual approach and its implications
for mathematical practice. Our work contributes both to the study of
Diophantine equations and to the broader question of how mathematics is
conducted in the 21st century.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [67] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng,Meng Zhang,Ming Tang*

Main category: cs.LG

TL;DR: 本文分析了联邦学习（FL）在概念漂移下的性能问题，提出了一种基于信息理论的算法来缓解性能下降。通过建模为马尔可夫链并引入静态泛化误差评估模型，证明了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据通常是动态的且分布会变化，导致性能下降（概念漂移）。本文旨在研究FL在概念漂移下的表现并提出解决方案。

Method: 将概念漂移建模为马尔可夫链，提出静态泛化误差，并设计基于KL散度和互信息的正则化算法以适应漂移。还研究了三种漂移模式的影响。

Result: 实验结果验证了理论分析，表明漂移模式显著影响性能，且所提算法优于现有方法。

Conclusion: 本文提出的算法能有效适应概念漂移，提升FL的长期性能，同时揭示了性能与成本的权衡。

Abstract: Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [68] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim,Brian Godwin S. Lim,Argel A. Bandala,John Anthony C. Jose,Timothy Scott C. Chu,Edwin Sybingco*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新颖的图时序卷积网络（AGTCNet），用于运动想象脑电图（MI-EEG）分类，显著提升了分类性能，并在模型大小、推理时间和输入信号长度上实现了优化。


<details>
  <summary>Details</summary>
Motivation: 脑机接口（BCI）技术虽然在帮助运动障碍者方面具有巨大潜力，但因神经活动的复杂性和变异性，开发适用于不同个体和不同会话的BCI系统仍具挑战性。现有方法未能充分捕捉多通道脑电图信号中的时空依赖性。

Method: 论文提出了AGTCNet模型，利用脑电图电极的拓扑结构作为归纳偏置，并结合图卷积注意力网络（GCAT）来联合学习时空脑电图表示。

Result: 在多个数据集上，AGTCNet显著优于现有方法，达到了最先进的性能。模型大小减少了49.87%，推理时间快了64.65%，且在主体独立和主体特定的分类任务中均表现出色。

Conclusion: AGTCNet通过其紧凑的架构和高效的表现，为BCI部署提供了实用且有效的解决方案，提升了分类准确性并优化了计算资源。

Abstract: Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [69] [Inside Job: Defending Kubernetes Clusters Against Network Misconfigurations](https://arxiv.org/abs/2506.21134)
*Jacopo Bufalino,Jose Luis Martin-Navarro,Mario Di Francesco,Tuomas Aura*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Kubernetes has emerged as the de facto standard for container orchestration.
Unfortunately, its increasing popularity has also made it an attractive target
for malicious actors. Despite extensive research on securing Kubernetes, little
attention has been paid to the impact of network configuration on the security
of application deployments. This paper addresses this gap by conducting a
comprehensive analysis of network misconfigurations in a Kubernetes cluster
with specific reference to lateral movement. Accordingly, we carried out an
extensive evaluation of 287 open-source applications belonging to six different
organizations, ranging from IT companies and public entities to non-profits. As
a result, we identified 634 misconfigurations, well beyond what could be found
by solutions in the state of the art. We responsibly disclosed our findings to
the concerned organizations and engaged in a discussion to assess their
severity. As of now, misconfigurations affecting more than thirty applications
have been fixed with the mitigations we proposed.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [70] [Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing](https://arxiv.org/abs/2506.20782)
*Marc Bara*

Main category: cs.NE

TL;DR: 本文首次提出了将脉冲神经网络（SNN）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架，填补了该领域的研究空白，并为可持续的大规模InSAR处理提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测数据量的指数增长（如NISAR任务预计两年内生成100PB数据），高效能耗处理变得至关重要。SNN凭借其事件驱动计算模型，有望在保持精度的同时节省30-100倍能耗。

Method: 开发了针对相位解缠的脉冲编码方案，提出了利用相位解缠空间传播特性的SNN架构，并进行了计算复杂度和收敛性的理论分析。

Result: 证实了SNN的时空动态特性自然地建模了相位解缠所需的空间连续性约束，为SNN与SAR干涉仪交叉研究开辟了新方向。

Conclusion: 该框架不仅为现有算法提供了补充，还可能推动更可持续的大规模InSAR处理技术的发展。

Abstract: We present the first theoretical framework for applying spiking neural
networks (SNNs) to synthetic aperture radar (SAR) interferometric phase
unwrapping. Despite extensive research in both domains, our comprehensive
literature review confirms that SNNs have never been applied to phase
unwrapping, representing a significant gap in current methodologies. As Earth
observation data volumes continue to grow exponentially (with missions like
NISAR expected to generate 100PB in two years) energy-efficient processing
becomes critical for sustainable data center operations. SNNs, with their
event-driven computation model, offer potential energy savings of 30-100x
compared to conventional approaches while maintaining comparable accuracy. We
develop spike encoding schemes specifically designed for wrapped phase data,
propose SNN architectures that leverage the spatial propagation nature of phase
unwrapping, and provide theoretical analysis of computational complexity and
convergence properties. Our framework demonstrates how the temporal dynamics
inherent in SNNs can naturally model the spatial continuity constraints
fundamental to phase unwrapping. This work opens a new research direction at
the intersection of neuromorphic computing and SAR interferometry, offering a
complementary approach to existing algorithms that could enable more
sustainable large-scale InSAR processing.

</details>


### [71] [Brain2Model Transfer: Training sensory and decision models with human neural activity as a teacher](https://arxiv.org/abs/2506.20834)
*Tomas Gallo Aquino,Victoria Liu,Habiba Azab,Raissa Mathura,Andrew J Watrous,Eleonora Bartoli,Benjamin Y Hayden,Paul Sajda,Sameer A Sheth,Nuttida Rungratsameetaweemana*

Main category: cs.NE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transfer learning enhances the training of novel sensory and decision models
by employing rich feature representations from large, pre-trained teacher
models. Cognitive neuroscience shows that the human brain creates
low-dimensional, abstract representations for efficient sensorimotor coding.
Importantly, the brain can learn these representations with significantly fewer
data points and less computational power than artificial models require. We
introduce Brain2Model Transfer Learning (B2M), a framework where neural
activity from human sensory and decision-making tasks acts as the teacher model
for training artificial neural networks. We propose two B2M strategies: (1)
Brain Contrastive Transfer, which aligns brain activity and network activations
through a contrastive objective; and (2) Brain Latent Transfer, which projects
latent dynamics from similar cognitive tasks onto student networks via
supervised regression of brain-derived features. We validate B2M in
memory-based decision-making with a recurrent neural network and scene
reconstruction for autonomous driving with a variational autoencoder. The
results show that student networks benefiting from brain-based transfer
converge faster and achieve higher predictive accuracy than networks trained in
isolation. Our findings indicate that the brain's representations are valuable
for artificial learners, paving the way for more efficient learning of complex
decision-making representations, which would be costly or slow through purely
artificial training.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [72] [Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions](https://arxiv.org/abs/2506.20677)
*Shrinivass Arunachalam Balasubramanian*

Main category: cs.DS

TL;DR: 提出了一种新的自适应混合排序范式，通过实时监测输入数据模式自动选择最有效的排序算法（计数排序、基数排序或快速排序）。该架构使用特征提取模块和决策引擎，实验证明其性能优于传统静态排序算法。


<details>
  <summary>Details</summary>
Motivation: 传统的排序算法无法在所有数据分布下均表现最优，因此需要一种能够根据数据类型和特征动态选择最优算法的解决方案。

Method: 通过特征提取模块计算数据量、值范围和熵等参数，再利用决策引擎（包括有限状态机和XGBoost分类器）选择最优排序算法。针对不同情况使用计数排序、基数排序或快速排序。

Result: 实验结果表明，该自适应混合排序框架在执行时间、灵活性和效率上显著优于传统静态排序算法。

Conclusion: 该框架具有可扩展性和广泛适用性，适用于大数据分析、边缘计算和硬件受限系统等多种场景。

Abstract: Sorting is an essential operation in computer science with direct
consequences on the performance of large scale data systems, real-time systems,
and embedded computation. However, no sorting algorithm is optimal under all
distributions of data. The new adaptive hybrid sorting paradigm proposed in
this paper is the paradigm that automatically selects the most effective
sorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time
monitoring of patterns in input data. The architecture begins by having a
feature extraction module to compute significant parameters such as data
volume, value range and entropy. These parameters are sent to a decision engine
involving Finite State Machine and XGBoost classifier to aid smart and
effective in choosing the optimal sorting strategy. It implements Counting Sort
on small key ranges, Radix Sort on large range structured input with
low-entropy keys and QuickSort on general purpose sorting. The experimental
findings of both synthetic and real life dataset confirm that the proposed
solution is actually inclined to excel significantly by comparison in execution
time, flexibility and the efficiency of conventional static sorting algorithms.
The proposed framework provides a scalable, high perhaps and applicable to a
wide range of data processing operations like big data analytics, edge
computing, and systems with hardware limitations.

</details>


### [73] [Practical and Accurate Local Edge Differentially Private Graph Algorithms](https://arxiv.org/abs/2506.20828)
*Pranay Mundra,Charalampos Papamanthou,Julian Shun,Quanquan C. Liu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rise of massive networks across diverse domains necessitates
sophisticated graph analytics, often involving sensitive data and raising
privacy concerns. This paper addresses these challenges using local
differential privacy (LDP), which enforces privacy at the individual level,
where no third-party entity is trusted, unlike centralized models that assume a
trusted curator. We introduce novel LDP algorithms for two fundamental graph
statistics: k-core decomposition and triangle counting. Our approach leverages
input-dependent private graph properties, specifically the degeneracy and
maximum degree of the graph, to improve theoretical utility. Unlike prior
methods, our error bounds are determined by the maximum degree rather than the
total number of edges, resulting in significantly tighter guarantees. For
triangle counting, we improve upon the work of Imola, Murakami, and
Chaudhury~\cite{IMC21locally, IMC21communication}, which bounds error in terms
of edge count. Instead, our algorithm achieves bounds based on graph degeneracy
by leveraging a private out-degree orientation, a refined variant of Eden et
al.'s randomized response technique~\cite{ELRS23, and a novel analysis,
yielding stronger guarantees than prior work. Beyond theoretical gains, we are
the first to evaluate local DP algorithms in a distributed simulation, unlike
prior work tested on a single processor. Experiments on real-world graphs show
substantial accuracy gains: our k-core decomposition achieves errors within 3x
of exact values, far outperforming the 131x error in the baseline of Dhulipala
et al.~\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative
approximation errors by up to six orders of magnitude, while maintaining
competitive runtime.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [74] [RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation](https://arxiv.org/abs/2506.20817)
*Ali Tourani,Fatemeh Nazary,Yashar Deldjoo*

Main category: cs.IR

TL;DR: 论文提出一种多模态电影推荐系统，结合LLM生成的剧情描述和预告片视觉嵌入，通过数据增强和融合策略提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 解决电影推荐中因元数据有限而导致推荐效果不佳的问题。

Method: 采用Retrieval-Augmented Generation (RAG)和协同过滤，结合LLM生成文本和视觉嵌入，利用PCA和CCA进行多模态融合。

Result: CCA融合显著提升召回率，LLM驱动的重排序进一步改善NDCG。

Conclusion: 发布框架以促进多模态推荐技术的研究，适用于冷启动和特定领域场景。

Abstract: This paper addresses the challenge of developing multimodal recommender
systems for the movie domain, where limited metadata (e.g., title, genre) often
hinders the generation of robust recommendations. We introduce a resource that
combines LLM-generated plot descriptions with trailer-derived visual embeddings
in a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and
collaborative filtering. Central to our approach is a data augmentation step
that transforms sparse metadata into richer textual signals, alongside fusion
strategies (e.g., PCA, CCA) that integrate visual cues. Experimental
evaluations demonstrate that CCA-based fusion significantly boosts recall
compared to unimodal baselines, while an LLM-driven re-ranking step further
improves NDCG, particularly in scenarios with limited textual data. By
releasing this framework, we invite further exploration of multi-modal
recommendation techniques tailored to cold-start, novelty-focused, and
domain-specific settings. All code, data, and detailed documentation are
publicly available at: https://github.com/RecSys-lab/RAG-VisualRec

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [75] [Benchmarking and Parallelization of Electrostatic Particle-In-Cell for low-temperature Plasma Simulation by particle-thread Binding](https://arxiv.org/abs/2506.21524)
*Libn Varghese,Bhaskar Chaudhury,Miral Shah,Mainak Bandyopadhyay*

Main category: physics.comp-ph

TL;DR: 本文提出了一种新颖的粒子-线程绑定策略，显著提升了PIC模拟中电荷沉积子程序的可扩展性和性能，同时保持传统数据结构，仅需最小修改。


<details>
  <summary>Details</summary>
Motivation: 针对2D和3D设备尺度PIC模拟中电荷沉积子程序因频繁的粒子-网格交互成为性能瓶颈的问题，传统方法因生成私有网格而面临可扩展性挑战。

Method: 采用粒子-线程绑定策略，仅需每个节点或共享内存系统中四个私有网格，通过附加功能和标志确保网格数据结构完全可访问，并避免对同单元粒子的同时访问。

Result: 在共享内存和分布式内存系统（1000核）上的性能评估显示该方法具有出色的可扩展性和较低硬件依赖性。

Conclusion: 该方法显著提升了PIC模拟的可扩展性和性能，同时保持了代码的简洁性和兼容性。

Abstract: The Particle-In-Cell (PIC) method for plasma simulation tracks particle phase
space information using particle and grid data structures. High computational
costs in 2D and 3D device-scale PIC simulations necessitate parallelization,
with the Charge Deposition (CD) subroutine often becoming a bottleneck due to
frequent particle-grid interactions. Conventional methods mitigate dependencies
by generating private grids for each core, but this approach faces scalability
issues. We propose a novel approach based on a particle-thread binding strategy
that requires only four private grids per node in distributed memory systems or
four private grids in shared memory systems, enhancing CD scalability and
performance while maintaining conventional data structures and requiring
minimal changes to existing PIC codes. This method ensures complete
accessibility of grid data structure for concurrent threads and avoids
simultaneous access to particles within the same cell using additional
functions and flags. Performance evaluations using a PIC benchmark for
low-temperature partially magnetized E x B discharge simulation on a shared
memory as well as a distributed memory system (1000 cores) demonstrate the
method's scalability, and additionally, we show the method has little hardware
dependency.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [76] [When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact](https://arxiv.org/abs/2506.20442)
*Tianyao Shi,Ritbik Kumar,Inez Hua,Yi Ding*

Main category: cs.CY

TL;DR: 本文首次全面分析了计算系统对生物多样性的影响，提出了两个新指标（EBI和OBI）以及FABRIC建模框架，强调了在可持续计算中需要同时考虑生物多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的计算可持续性研究多关注碳和水，忽视了生物多样性的影响，因此需要开发新的度量和方法来填补这一空白。

Method: 提出了Embodied Biodiversity Index (EBI)和Operational Biodiversity Index (OBI)两个新指标，并开发了FABRIC建模框架，用于量化计算工作负载对生物多样性的影响。

Result: 评估结果显示，生物多样性应成为可持续计算设计和优化中的重要考量因素。

Conclusion: 该研究为计算领域引入生物多样性影响度量提供了初步框架，并强调了其在可持续性研究中的重要性。

Abstract: Biodiversity loss is a critical planetary boundary, yet its connection to
computing remains largely unexamined. Prior sustainability efforts in computing
have focused on carbon and water, overlooking biodiversity due to the lack of
appropriate metrics and modeling frameworks. This paper presents the first
end-to-end analysis of biodiversity impact from computing systems. We introduce
two new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity
Index (OBI)--to quantify biodiversity impact across the lifecycle, and present
FABRIC, a modeling framework that links computing workloads to biodiversity
impacts. Our evaluation highlights the need to consider biodiversity alongside
carbon and water in sustainable computing design and optimization. The code is
available at https://github.com/TianyaoShi/FABRIC.

</details>
