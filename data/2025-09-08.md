<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 8]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.AI](#cs.AI) [Total: 5]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [math.LO](#math.LO) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 3]
- [physics.ins-det](#physics.ins-det) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Comparative Evaluation of Large Language Models for Test-Skeleton Generation](https://arxiv.org/abs/2509.04644)
*Subhang Boorlagadda,Nitya Naga Sai Atluri,Muhammet Mustafa Olmez,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 论文研究了使用大语言模型（LLM）自动生成测试骨架的可行性，比较了四种模型在生成RSpec测试骨架时的性能，发现DeepSeek表现最佳，而GPT-4输出完整但一致性较差。


<details>
  <summary>Details</summary>
Motivation: 测试骨架在测试驱动开发（TDD）中非常重要，但手动编写耗时且容易出错。研究旨在探索LLM在这一任务中的实用性。

Method: 评估了GPT-4、DeepSeek-Chat、Llama4-Maverick和Gemma2-9B四种LLM生成RSpec测试骨架的能力，采用静态分析和专家盲审衡量质量。

Result: DeepSeek生成的骨架最易维护且结构良好，GPT-4输出完整但与传统测试实践不一致。提示设计和上下文输入是影响质量的关键因素。

Conclusion: LLM可用于自动生成测试骨架，但效果因模型和设计而异，提示设计和上下文优化对提升质量至关重要。

Abstract: This paper explores the use of Large Language Models (LLMs) to automate the
generation of test skeletons -- structural templates that outline unit test
coverage without implementing full test logic. Test skeletons are especially
important in test-driven development (TDD), where they provide an early
framework for systematic verification. Traditionally authored manually, their
creation can be time-consuming and error-prone, particularly in educational or
large-scale development settings. We evaluate four LLMs -- GPT-4,
DeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate
RSpec skeletons for a real-world Ruby class developed in a university software
engineering course. Each model's output is assessed using static analysis and a
blind expert review to measure structural correctness, clarity,
maintainability, and conformance to testing best practices. The study reveals
key differences in how models interpret code structure and testing conventions,
offering insights into the practical challenges of using LLMs for automated
test scaffolding. Our results show that DeepSeek generated the most
maintainable and well-structured skeletons, while GPT-4 produced more complete
but conventionally inconsistent output. The study reveals prompt design and
contextual input as key quality factors.

</details>


### [2] [Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)](https://arxiv.org/abs/2509.04721)
*Abhishek Dey,Saurabh Srivastava,Gaurav Singh,Robert G. Pettit*

Main category: cs.SE

TL;DR: PICO-TINYML-BENCHMARK是一个模块化且平台无关的框架，用于评估TinyML模型在资源受限嵌入式系统中的实时性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决TinyML模型在嵌入式系统中的实际性能评估问题，为优化部署提供依据。

Method: 通过评估推理延迟、CPU利用率、内存效率和预测稳定性等关键指标，对比两种平台（BeagleBone AI64和Raspberry Pi 4）上三个代表性TinyML模型的性能。

Result: BeagleBone AI64在AI任务中表现稳定，而Raspberry Pi 4在资源效率和成本效益上更优。

Conclusion: 研究结果为TinyML部署提供了实用指导，填补了理论进展与实际应用之间的差距。

Abstract: This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic
framework for benchmarking the real-time performance of TinyML models on
resource-constrained embedded systems. Evaluating key metrics such as inference
latency, CPU utilization, memory efficiency, and prediction stability, the
framework provides insights into computational trade-offs and platform-specific
optimizations. We benchmark three representative TinyML models -- Gesture
Classification, Keyword Spotting, and MobileNet V2 -- on two widely adopted
platforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.
Results reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent
inference latency for AI-specific tasks, while the Raspberry Pi 4 excels in
resource efficiency and cost-effectiveness. These findings offer actionable
guidance for optimizing TinyML deployments, bridging the gap between
theoretical advancements and practical applications in embedded systems.

</details>


### [3] [NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation](https://arxiv.org/abs/2509.04763)
*Tiancheng Jin,Shangzhou Xia,Jianjun Zhao*

Main category: cs.SE

TL;DR: NovaQ是一个用于量子程序的多样性引导测试框架，通过生成多样化的量子状态输入并结合新颖性评估模块，有效提升了测试覆盖率和错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，确保量子程序的可靠性变得越来越重要。由于量子程序的特殊性，传统的测试方法可能不足以覆盖其复杂行为。

Method: NovaQ结合了基于分布的测试用例生成器和新颖性驱动的评估模块。生成器通过变异电路参数产生多样化的量子状态输入，评估器则根据内部电路状态指标（如幅度、相位和纠缠）量化行为新颖性。

Result: 实验结果显示，NovaQ在多样性和错误检测方面优于现有基线方法，适用于不同规模和复杂度的量子程序。

Conclusion: NovaQ通过多样性引导的测试策略，显著提升了量子程序的测试覆盖率和可靠性。

Abstract: Quantum programs are designed to run on quantum computers, leveraging quantum
circuits to solve problems that are intractable for classical machines. As
quantum computing advances, ensuring the reliability of quantum programs has
become increasingly important. This paper introduces NovaQ, a diversity-guided
testing framework for quantum programs. NovaQ combines a distribution-based
test case generator with a novelty-driven evaluation module. The generator
produces diverse quantum state inputs by mutating circuit parameters, while the
evaluator quantifies behavioral novelty based on internal circuit state
metrics, including magnitude, phase, and entanglement. By selecting inputs that
map to infrequently covered regions in the metric space, NovaQ effectively
explores under-tested program behaviors. We evaluate NovaQ on quantum programs
of varying sizes and complexities. Experimental results show that NovaQ
consistently achieves higher test input diversity and detects more bugs than
existing baseline approaches.

</details>


### [4] [Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation](https://arxiv.org/abs/2509.04810)
*Yogev Cohen,Dudi Ohayon,Romy Somkin,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.SE

TL;DR: 利用大型语言模型（LLMs）生成合成数据，以解决新兴编程语言中标记数据不足的问题，从而训练代码审查推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现代开发流程中，自动化决定代码变更是否需要人工审查对维护软件质量至关重要。然而，新兴编程语言和框架缺乏足够的标记数据来训练监督模型。

Method: 通过LLMs将资源丰富语言的代码变更转换为新兴语言的等效变更，生成合成训练数据。同时训练监督分类器并评估其性能。

Result: 实验表明，LLM生成的合成数据能有效提升审查推荐系统的性能，缩小与真实标记数据的差距。

Conclusion: 该方法为扩展自动化代码审查能力提供了可扩展的途径，特别是在缺乏标记数据的快速演变的技朧栈中。

Abstract: Automating the decision of whether a code change requires manual review is
vital for maintaining software quality in modern development workflows.
However, the emergence of new programming languages and frameworks creates a
critical bottleneck: while large volumes of unlabelled code are readily
available, there is an insufficient amount of labelled data to train supervised
models for review classification. We address this challenge by leveraging Large
Language Models (LLMs) to translate code changes from well-resourced languages
into equivalent changes in underrepresented or emerging languages, generating
synthetic training data where labelled examples are scarce. We assume that
although LLMs have learned the syntax and semantics of new languages from
available unlabelled code, they have yet to fully grasp which code changes are
considered significant or review-worthy within these emerging ecosystems. To
overcome this, we use LLMs to generate synthetic change examples and train
supervised classifiers on them. We systematically compare the performance of
these classifiers against models trained on real labelled data. Our experiments
across multiple GitHub repositories and language pairs demonstrate that
LLM-generated synthetic data can effectively bootstrap review recommendation
systems, narrowing the performance gap even in low-resource settings. This
approach provides a scalable pathway to extend automated code review
capabilities to rapidly evolving technology stacks, even in the absence of
annotated data.

</details>


### [5] [Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining](https://arxiv.org/abs/2509.04877)
*Maryam Khan,Muhammad Azeem Akbar,Jussi Kasurinen*

Main category: cs.SE

TL;DR: 该博士研究通过分析400个GitHub项目，验证了大型语言模型（LLM）在软件工程教育中的激励因素（如学习动力）和阻碍因素（如抄袭问题），为后续框架开发奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（如ChatGPT）在教育中的广泛应用，需要系统研究其责任性整合，以避免潜在问题并发挥优势。

Method: 采用试点仓库挖掘方法，分析GitHub项目的README文件和议题讨论，识别已通过文献综述合成的激励与阻碍因素。

Result: 激励因素（如学习动力）和阻碍因素（如抄袭问题）均显著存在，但部分阻碍因素（如课程设计问题）未体现。

Conclusion: 研究初步验证了激励/阻碍因素分类，揭示了实践与研究间的差距，并为未来框架开发提供了基础。

Abstract: Context: Large Language Models (LLMs) such as ChatGPT are increasingly
adopted in software engineering (SE) education, offering both opportunities and
challenges. Their adoption requires systematic investigation to ensure
responsible integration into curricula. Objective: This doctoral research aims
to develop a validated framework for integrating LLMs into SE education through
a multi-phase process, including taxonomies development, empirical
investigation, and case studies. This paper presents the first empirical step.
Method: We conducted a pilot repository mining study of 400 GitHub projects,
analyzing README files and issues discussions to identify the presence of
motivator and demotivator previously synthesized in our literature review [ 8]
study. Results: Motivators such as engagement and motivation (227 hits),
software engineering process understanding (133 hits), and programming
assistance and debugging support (97 hits) were strongly represented.
Demotivators, including plagiarism and IP concerns (385 hits), security,
privacy and data integrity (87 hits), and over-reliance on AI in learning (39
hits), also appeared prominently. In contrast, demotivators such as challenges
in evaluating learning outcomes and difficulty in curriculum redesign recorded
no hits across the repositories. Conclusion: The study provides early empirical
validation of motivators/demotivators taxonomies with respect to their themes,
highlights research practice gaps, and lays the foundation for developing a
comprehensive framework to guide the responsible adoption of LLMs in SE
education.

</details>


### [6] [FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage](https://arxiv.org/abs/2509.04967)
*Kai Feng,Jeremy Singer,Angelos K Marnerides*

Main category: cs.SE

TL;DR: FuzzRDUCC是一种新的模糊测试框架，通过数据流分析增强覆盖率，结合符号执行重建定义-使用链，显著提升漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试依赖控制流边缘覆盖率，可能忽略隐藏漏洞。数据流分析的加入能更好地揭示程序内部数据传播路径，提高测试效果。

Method: FuzzRDUCC利用符号执行直接从二进制可执行文件重建定义-使用链，并通过启发式算法选择关键数据流路径，减少计算开销。

Result: 在binutils基准测试中，FuzzRDUCC能够发现未被其他先进模糊测试工具检测到的独特崩溃。

Conclusion: FuzzRDUCC是一种可行的下一代漏洞检测和发现方案，尤其在数据流路径分析方面表现突出。

Abstract: Binary-only fuzzing often struggles with achieving thorough code coverage and
uncovering hidden vulnerabilities due to limited insight into a program's
internal dataflows. Traditional grey-box fuzzers guide test case generation
primarily using control flow edge coverage, which can overlook bugs not easily
exposed through control flow analysis alone. We argue that integrating dataflow
analysis into the fuzzing process can enhance its effectiveness by revealing
how data propagates through the program, thereby enabling the exploration of
execution paths that control flow-based methods might miss. In this context, we
introduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution
to reconstruct definition-use (def-use) chains directly from binary
executables. FuzzRDUCC identifies crucial dataflow paths and exposes security
vulnerabilities without incurring excessive computational overhead, due to a
novel heuristic algorithm that selects relevant def-use chains without
affecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using
the binutils benchmark and demonstrate that it can identify unique crashes not
found by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible
solution for next generation vulnerability detection and discovery mechanisms.

</details>


### [7] [GenAI-based test case generation and execution in SDV platform](https://arxiv.org/abs/2509.05112)
*Denesa Zyberaj,Lukasz Mazur,Nenad Petrovic,Pankhuri Verma,Pascal Hirmer,Dirk Slama,Xiangwei Cheng,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一种基于GenAI的自动化测试用例生成方法，结合大语言模型和视觉语言模型，将自然语言需求和系统图转换为结构化的Gherkin测试用例。方法通过车辆信号规范建模标准化信号定义，提升兼容性并简化集成。测试用例在数字汽车环境中快速验证，但GenAI流程和平台限制仍需人工干预。


<details>
  <summary>Details</summary>
Motivation: 为了提高汽车软件测试的效率，减少人工编写测试用例的工作量，并实现跨子系统兼容性。

Method: 利用大语言模型和视觉语言模型将需求与系统图转换为Gherkin测试用例，结合车辆信号规范建模标准化信号定义，并在数字汽车平台中执行测试。

Result: 通过儿童存在检测系统用例验证，显著减少人工测试用例编写工作量，并快速执行生成的测试用例。

Conclusion: 尽管自动化程度高，但由于GenAI和平台限制，仍需人工干预。

Abstract: This paper introduces a GenAI-driven approach for automated test case
generation, leveraging Large Language Models and Vision-Language Models to
translate natural language requirements and system diagrams into structured
Gherkin test cases. The methodology integrates Vehicle Signal Specification
modeling to standardize vehicle signal definitions, improve compatibility
across automotive subsystems, and streamline integration with third-party
testing tools. Generated test cases are executed within the digital.auto
playground, an open and vendor-neutral environment designed to facilitate rapid
validation of software-defined vehicle functionalities. We evaluate our
approach using the Child Presence Detection System use case, demonstrating
substantial reductions in manual test specification effort and rapid execution
of generated tests. Despite significant automation, the generation of test
cases and test scripts still requires manual intervention due to current
limitations in the GenAI pipeline and constraints of the digital.auto platform.

</details>


### [8] [AI Agents for Web Testing: A Case Study in the Wild](https://arxiv.org/abs/2509.05197)
*Naimeng Ye,Xiao Yu,Ruize Xu,Tianyi Peng,Zhou Yu*

Main category: cs.SE

TL;DR: WebProber是一个基于AI代理的网页测试框架，通过模拟真实用户行为发现传统测试工具遗漏的可用性问题。


<details>
  <summary>Details</summary>
Motivation: 传统网页测试方法难以捕捉复杂的用户行为，导致可用性问题未被发现，而大型语言模型和AI代理为解决这一问题提供了新可能。

Method: 开发WebProber框架，通过输入URL自动探索网站、模拟用户交互、识别问题并生成报告。

Result: 在120个学术个人网站的测试中，WebProber发现29个可用性问题，许多是传统工具未能检测到的。

Conclusion: 基于AI代理的测试是未来用户中心测试框架的有前景方向。

Abstract: Automated web testing plays a critical role in ensuring high-quality user
experiences and delivering business value. Traditional approaches primarily
focus on code coverage and load testing, but often fall short of capturing
complex user behaviors, leaving many usability issues undetected. The emergence
of large language models (LLM) and AI agents opens new possibilities for web
testing by enabling human-like interaction with websites and a general
awareness of common usability problems. In this work, we present WebProber, a
prototype AI agent-based web testing framework. Given a URL, WebProber
autonomously explores the website, simulating real user interactions,
identifying bugs and usability issues, and producing a human-readable report.
We evaluate WebProber through a case study of 120 academic personal websites,
where it uncovered 29 usability issues--many of which were missed by
traditional tools. Our findings highlight agent-based testing as a promising
direction while outlining directions for developing next-generation,
user-centered testing frameworks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [A Large-Scale Study of Floating-Point Usage in Statically Typed Languages](https://arxiv.org/abs/2509.04936)
*Andrea Gilot,Tobias Wrigstad,Eva Darulova*

Main category: cs.PL

TL;DR: 这是一篇关于浮点算术在静态类型语言中大规模实证研究的论文，旨在填补对现实世界代码中浮点运算使用情况的知识空白。


<details>
  <summary>Details</summary>
Motivation: 理解现实世界浮点代码的真实面貌，以推动静态和动态分析或程序修复技术在实践中的应用。

Method: 采用随机抽样和基于内在属性的过滤方法，避免偏见，并通过关键词搜索和代码解析识别浮点运算和使用构造（如循环）。

Result: 研究发现，浮点算术广泛使用，同时文献中用于评估自动推理技术的基准在某些方面具有代表性，但并非所有方面。

Conclusion: 这项研究及数据集有助于未来浮点算术技术的设计和评估，以更贴近实际用户的需求。

Abstract: Reasoning about floating-point arithmetic is notoriously hard. While static
and dynamic analysis techniques or program repair have made significant
progress, more work is still needed to make them relevant to real-world code.
On the critical path to that goal is understanding what real-world
floating-point code looks like. To close that knowledge gap, this paper
presents the first large-scale empirical study of floating-point arithmetic
usage in statically typed languages across public GitHub repositories. We
follow state-of the art mining practices including random sampling and
filtering based on only intrinsic properties to avoid bias, and identify
floating-point usage by searching for keywords in the source code, and
programming language constructs (e.g., loops) by parsing the code. Our
evaluation supports the claim often made in papers that floating-point
arithmetic is widely used. Comparing statistics such as size and usage of
certain constructs and functions, we find that benchmarks used in literature to
evaluate automated reasoning techniques for floating-point arithmetic are in
certain aspects representative of 'real-world' code, but not in all. We aim for
our study and dataset to help future techniques for floating-point arithmetic
to be designed and evaluated to match actual users' expectations.

</details>


### [10] [AI-Assisted Modeling: DSL-Driven AI Interactions](https://arxiv.org/abs/2509.05160)
*Steven Smyth,Daniel Busch,Moez Ben Haj Hmida,Edward A. Lee,Bernhard Steffen*

Main category: cs.PL

TL;DR: AI辅助编程通过透明化和即时图形可视化提升开发效率和验证能力。


<details>
  <summary>Details</summary>
Motivation: 增强AI生成代码的透明度和验证能力，以提升软件开发的性能和质量。

Method: 结合领域特定建模技术和即时图形可视化，支持多种输入方式（编程、自然语言、语音），并分步提供反馈。

Result: 开发了一个Visual Studio Code扩展原型，展示如何通过新方法改进建模、可视化和验证。

Conclusion: 该方法在AI辅助编程中显著提升了代码生成和验证的效率，具有广泛的应用潜力。

Abstract: AI-assisted programming greatly increases software development performance.
We enhance this potential by integrating transparency through domain-specific
modeling techniques and providing instantaneous, graphical visualizations that
accurately represent the semantics of AI-generated code. This approach
facilitates visual inspection and formal verification, such as model checking.
  Formal models can be developed using programming, natural language prompts,
voice commands, and stage-wise refinement, with immediate feedback after each
transformation step. This support can be tailored to specific domains or
intended purposes, improving both code generation and subsequent validation
processes.
  To demonstrate the effectiveness of this approach, we have developed a
prototype as a Visual Studio Code extension for the Lingua Franca language.
This prototype showcases the potential for novel domain-specific modeling
practices, offering an advancement in how models are created, visualized, and
verified.

</details>


### [11] [Non-Termination Proving: 100 Million LoC and Beyond](https://arxiv.org/abs/2509.05293)
*Julien Vanegue,Jules Villard,Peter O'Hearn,Azalea Raad*

Main category: cs.PL

TL;DR: Pulse Infinite 是一款工具，采用证明技术检测大型程序中的非终止（发散）问题，适用于大规模代码库。


<details>
  <summary>Details</summary>
Motivation: 现有的工具仅适用于小型代码库（数十至数百行代码），而实际企业代码库可能达到数千万甚至数亿行，需要一种能应对大规模代码的解决方案。

Method: Pulse Infinite 采用组合和欠近似的方法，组合性支持规模扩展，欠近似确保发散证明的可靠性。

Result: 该工具在超过一亿行的 C、C++ 和 Hack 代码中应用，发现 30 多个未知问题，成为检测真实代码库发散问题的新标杆。

Conclusion: Pulse Infinite 实现了对大规模代码库的高效分析，显著提升了非终止问题检测的实用性。

Abstract: We report on our tool, Pulse Infinite, that uses proof techniques to show
non-termination (divergence) in large programs. Pulse Infinite works
compositionally and under-approximately: the former supports scale, and the
latter ensures soundness for proving divergence. Prior work focused on small
benchmarks in the tens or hundreds of lines of code (LoC), and scale limits
their practicality: a single company may have tens of millions, or even
hundreds of millions of LoC or more. We report on applying Pulse Infinite to
over a hundred million lines of open-source and proprietary software written in
C, C++, and Hack, identifying over 30 previously unknown issues, establishing a
new state of the art for detecting divergence in real-world codebases.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [12] [High Performance Matrix Multiplication](https://arxiv.org/abs/2509.04594)
*Ethan Davis*

Main category: cs.PF

TL;DR: 比较了五种矩阵乘法算法（CuBLAS、CUDA、BLAS、OpenMP和C++ Threads）的性能，发现对于至少10,000×10,000的方阵，性能排序为CuBLAS > CUDA > BLAS > OpenMP > C++ Threads。


<details>
  <summary>Details</summary>
Motivation: 矩阵乘法是深度学习、科学模拟和视频图形等高性能技术的基础。研究不同算法在高性能计算中的表现，有助于优化这些应用。

Method: 使用CuBLAS、CUDA、BLAS、OpenMP和C++ Threads五种算法，对大规模方阵（N ≥ 10,000）进行乘法运算，并以FLOPS衡量性能。

Result: 统计显著性显示（p值 < 5e-12），性能排序为CuBLAS > CUDA > BLAS > OpenMP > C++ Threads。

Conclusion: CuBLAS在矩阵乘法中表现最优，为高性能计算提供了明确的选择依据。

Abstract: Matrix multiplication is the foundation from much of the success from high
performance technologies like deep learning, scientific simulations, and video
graphics. High level programming languages like Python and R rely on highly
optimized low level libraries for performing core linear algebra operations
like matrix multiplication from Basic Linear Algebra Subprograms (BLAS). This
paper compares the performance of five different matrix multiplication
algorithms using CuBLAS, CUDA, BLAS, OpenMP, and C++ Threads. We find
statistical significance with a p-value below 5e-12 to support the hypothesis
that for square $N \times N$ matrices where $N$ is at least 10,000 then the in
order performance as measured in floating point operations per second (FLOPS)
for these matrix multiplication algorithms is CuBLAS, CUDA, BLAS, OpenMP, and
C++ Threads.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [13] [NEXUS: Efficient and Scalable Multi-Cell mmWave Baseband Processing with Heterogeneous Compute](https://arxiv.org/abs/2509.04625)
*Zhenzhou Qi,Chung-Hsuan Tung,Zhihui Gao,Tingjun Chen*

Main category: cs.NI

TL;DR: NEXUS是一个在单服务器上实现实时、虚拟化多小区毫米波基带处理的系统，通过异构计算资源和创新的调度框架提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 5G NR的高速发展，尤其是毫米波频谱的应用，对基带处理的灵活性、可扩展性和效率提出了更高要求，多小区部署中的计算资源分配问题亟待解决。

Method: NEXUS整合了软件数字信号处理管道和硬件加速的LDPC解码，并利用虚拟功能（VFs）在多个CPU核心间共享ACC100 eASIC。采用随机森林模型预测单小区资源分配，开发轻量级竞争模型调度多小区资源。

Result: NEXUS支持最多16个并发小区，总吞吐量达5.37Gbps，同时将多小区调度的搜索空间减少多个数量级。

Conclusion: NEXUS证明了虚拟化、资源感知的基带处理对下一代vRAN系统的实用性和高效性。

Abstract: The rapid adoption of 5G New Radio (NR), particularly in the millimeter-wave
(mmWave) spectrum, imposes stringent demands on the flexibility, scalability,
and efficiency of baseband processing. While virtualized Radio Access Networks
(vRANs) enable dynamic spectrum sharing across cells, compute resource
allocation for baseband processing, especially in multi-cell deployments with
heterogeneous workloads, remains underexplored. In this paper, we present
NEXUS, the first system to realize real-time, virtualized multi-cell mmWave
baseband processing on a single server with heterogeneous compute resources.
NEXUS integrates software-based digital signal processing pipelines with
hardware-accelerated LDPC decoding, and introduces a novel framework for
sharing Intel's ACC100 eASIC across multiple CPU cores via virtual functions
(VFs). For single-cell operation, NEXUS employs a random forest (RAF)-based
model that predicts the most energy-efficient resource allocation for the given
cell configuration with microsecond-level inference latency and high accuracy.
For multi-cell scenarios, NEXUS introduces a power-aware scheduler that
incorporates a lightweight contention model to adjust resource allocation
strategies under concurrent execution. Through extensive evaluation across
various Frequency Range 2 (FR2) cell configurations, we show that NEXUS
supports up to 16 concurrent cells under full load, achieving 5.37Gbps
aggregate throughput, while reducing the multi-cell scheduling search space by
orders of magnitude. These results demonstrate that virtualized, resource-aware
baseband processing is both practical and efficient for next-generation vRAN
systems.

</details>


### [14] [Path Dynamics in a Deployed Path-Aware Network: A Measurement Study of SCIONLab](https://arxiv.org/abs/2509.04695)
*Lars Herschbach,Damien Rossi,Sina Keshvadi*

Main category: cs.NI

TL;DR: 该论文通过全球SCIONLab测试床对SCION架构进行纵向测量研究，发现路径稳定性、多样性和性能对多路径协议设计至关重要，揭示了高动态性和路径不对称性对多路径QUIC等协议的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了填补关于路径感知网络实时动态数据的空白，以支持更有效的多路径协议设计。

Method: 方法是在全球SCIONLab测试床上对SCION架构进行纵向测量研究，分析路径稳定性、多样性和性能。

Result: 结果显示测试床具有高动态性，控制平面变化大，路径寿命短，存在路径不对称现象，并发多路径传输可能影响单一路径的延迟和可靠性。

Conclusion: 结论指出多路径协议设计需明确考虑高动态性和路径不对称性，挑战了现有设计中的常见假设。

Abstract: Path-aware networks promise enhanced performance and resilience through
multipath transport, but a lack of empirical data on their real-world dynamics
hinders the design of effective protocols. This paper presents a longitudinal
measurement study of the SCION architecture on the global SCIONLab testbed,
characterizing the path stability, diversity, and performance crucial for
protocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic
environment, with significant control-plane churn and short path lifetimes in
parts of the testbed. We identify and characterize path discrepancy, a
phenomenon where routing policies create asymmetric path availability between
endpoints. Furthermore, we observe a performance trade-off where concurrent
multipath transmissions can improve aggregate throughput but may degrade the
latency and reliability of individual paths. These findings demonstrate that
protocols such as MPQUIC should explicitly account for high churn and path
asymmetry, challenging common assumptions in multipath protocol design.

</details>


### [15] [Where Have All the Firewalls Gone? Security Consequences of Residential IPv6 Transition](https://arxiv.org/abs/2509.04792)
*Erik Rye,Dave Levin,Robert Beverly*

Main category: cs.NI

TL;DR: 研究探讨了IPv6的普及是否增加了住宅网络对IoT僵尸网络的攻击风险，并通过大规模IPv6扫描发现IPv6比IPv4更容易让IoT设备暴露在攻击下。


<details>
  <summary>Details</summary>
Motivation: 随着互联网向IPv6过渡，住宅网络不再依赖NAT，这可能导致更多设备暴露在攻击下，研究旨在验证这一假设。

Method: 采用一种适合低资源设备的大规模IPv6扫描方法，对比IPv6和IPv4住宅网络中可公开访问的设备数量。

Result: 扫描结果显示，IPv6网络中暴露的设备（如打印机、iPhone、智能灯）比IPv4更多，表明NAT曾是互联网的默认防火墙。

Conclusion: IPv6的过渡使住宅网络更容易受到攻击，NAT的缺失导致更多设备暴露在IoT僵尸网络的威胁下。

Abstract: IPv4 NAT has limited the spread of IoT botnets considerably by
default-denying bots' incoming connection requests to in-home devices unless
the owner has explicitly allowed them. As the Internet transitions to majority
IPv6, however, residential connections no longer require the use of NAT. This
paper therefore asks: has the transition from IPv4 to IPv6 ultimately made
residential networks more vulnerable to attack, thereby empowering the next
generation of IPv6-based IoT botnets? To answer this question, we introduce a
large-scale IPv6 scanning methodology that, unlike those that rely on AI, can
be run on low-resource devices common in IoT botnets. We use this methodology
to perform the largest-scale measurement of IPv6 residential networks to date,
and compare which devices are publicly accessible to comparable IPv4 networks.
We were able to receive responses from 14.0M distinct IPv6 addresses inside of
residential networks (i.e., not the external-facing gateway), in 2,436 ASes
across 118 countries. These responses come from protocols commonly exploited by
IoT botnets (including telnet and FTP), as well as protocols typically
associated with end-user devices (including iPhone-Sync and IPP). Comparing to
IPv4, we show that we are able to reach more printers, iPhones, and smart
lights over IPv6 than full IPv4-wide scans could. Collectively, our results
show that NAT has indeed acted as the de facto firewall of the Internet, and
the v4-to-v6 transition of residential networks is opening up new devices to
attack.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [16] [REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts](https://arxiv.org/abs/2509.04844)
*Xinkui Lin,Yongxiu Xu,Minghao Tang,Shilong Zhang,Hongbo Xu,Hao Xu,Yubin Wang*

Main category: cs.MM

TL;DR: 论文提出了一个名为REMOTE的多模态关系提取框架，结合多级最优传输和专家混合机制，能够同时提取文本实体和视觉对象之间的模态内和模态间关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只能提取单一类型的关系三元组，且无法动态捕捉跨模态交互，导致计算冗余。

Method: 引入专家混合机制动态选择最优交互特征，并使用多级最优传输融合模块保留低层信息。

Result: REMOTE在多个公共数据集上表现优异，实现了最先进的性能。

Conclusion: REMOTE框架有效解决了多模态关系提取的多样性问题，提升了性能。

Abstract: Multimodal relation extraction (MRE) is a crucial task in the fields of
Knowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge
graph construction. However, existing methods are typically limited to
extracting a single type of relational triplet, which restricts their ability
to extract triplets beyond the specified types. Directly combining these
methods fails to capture dynamic cross-modal interactions and introduces
significant computational redundancy. Therefore, we propose a novel
\textit{unified multimodal Relation Extraction framework with Multilevel
Optimal Transport and mixture-of-Experts}, termed REMOTE, which can
simultaneously extract intra-modal and inter-modal relations between textual
entities and visual objects. To dynamically select optimal interaction features
for different types of relational triplets, we introduce mixture-of-experts
mechanism, ensuring the most relevant modality information is utilized.
Additionally, considering that the inherent property of multilayer sequential
encoding in existing encoders often leads to the loss of low-level information,
we adopt a multilevel optimal transport fusion module to preserve low-level
features while maintaining multilayer encoding, yielding more expressive
representations. Correspondingly, we also create a Unified Multimodal Relation
Extraction (UMRE) dataset to evaluate the effectiveness of our framework,
encompassing diverse cases where the head and tail entities can originate from
either text or image. Extensive experiments show that REMOTE effectively
extracts various types of relational triplets and achieves state-of-the-art
performanc on almost all metrics across two other public MRE datasets. We
release our resources at https://github.com/Nikol-coder/REMOTE.

</details>


### [17] [An Emotion Recognition Framework via Cross-modal Alignment of EEG and Eye Movement Data](https://arxiv.org/abs/2509.04938)
*Jianlu Wang,Yanan Wang,Tong Liu*

Main category: cs.MM

TL;DR: 论文提出了一种基于跨模态注意力机制的混合架构，用于实现EEG和眼动数据的多模态对齐，以提高情感识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于单模态数据的情感识别系统难以捕捉复杂的情感状态，因此需要一种更有效的多模态方法。

Method: 提出了一种混合架构，利用跨模态注意力机制对齐EEG和眼动数据。

Result: 在SEED-IV数据集上实验取得了90.62%的准确率。

Conclusion: 该方法为利用多模态数据进行情感识别提供了有前景的基础。

Abstract: Emotion recognition is essential for applications in affective computing and
behavioral prediction, but conventional systems relying on single-modality data
often fail to capture the complexity of affective states. To address this
limitation, we propose an emotion recognition framework that achieves accurate
multimodal alignment of Electroencephalogram (EEG) and eye movement data
through a hybrid architecture based on cross-modal attention mechanism.
Experiments on the SEED-IV dataset demonstrate that our method achieve 90.62%
accuracy. This work provides a promising foundation for leveraging multimodal
data in emotion recognition

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [Forall-Exists Relational Verification by Filtering to Forall-Forall](https://arxiv.org/abs/2509.04777)
*Ramana Nagasamudram,Anindya Banerjee,David A. Naumann*

Main category: cs.LO

TL;DR: 本文提出了一种通过过滤器-充分性转换验证∀∃属性的方法，利用产品程序的∀∀属性间接证明原始程序的∀∃性质，并开发了相应的逻辑和工具支持。


<details>
  <summary>Details</summary>
Motivation: 现有研究和工具多集中在处理∀∀属性（如2-safety），但对涉及非确定性的关键∀∃属性缺乏有效支持。

Method: 通过过滤器-充分性转换在产品程序中添加断言，将∀∃问题转化为∀∀问题，并设计了支持直接翻译的逻辑（bicoms）。

Result: 成功验证了论文中的所有示例，证明该方法可通过标准工具实现自动验证。

Conclusion: 该方法扩展了现有工具的能力，使得用户能够利用常规断言语言和处理∀∀属性的工具来验证∀∃属性。

Abstract: Relational verification encompasses research directions such as reasoning
about data abstraction, reasoning about security and privacy, secure
compilation, and functional specificaton of tensor programs, among others.
Several relational Hoare logics exist, with accompanying tool support for
compositional reasoning of $\forall\forall$ (2-safety) properties and,
generally, k-safety properties of product programs. In contrast, few logics and
tools exist for reasoning about $\forall\exists$ properties which are critical
in the context of nondeterminism.
  This paper's primary contribution is a methodology for verifying a
$\forall\exists$ judgment by way of a novel filter-adequacy transformation.
This transformation adds assertions to a product program in such a way that the
desired $\forall\exists$ property (of a pair of underlying unary programs) is
implied by a $\forall\forall$ property of the transformed product. The paper
develops a program logic for the basic $\forall\exists$ judgement extended with
assertion failures; develops bicoms, a form of product programs that represents
pairs of executions and that caters for direct translation of $\forall\forall$
properties to unary correctness; proves (using the logic) a soundness theorem
that says successful $\forall\forall$ verification of a transformed bicom
implies the $\forall\exists$ spec for its underlying unary commands; and
implements a proof of principle prototype for auto-active relational
verification which has been used to verify all examples in the paper. The
methodology thereby enables a user to work with ordinary assertions and
assumptions, and a standard assertion language, so that existing tools
including auto-active verifiers can be used.

</details>


### [19] [Higher order differential calculus in mathlib](https://arxiv.org/abs/2509.04922)
*Sébastien Gouëzel*

Main category: cs.LO

TL;DR: 本文介绍了在Lean数学库mathlib中开发的高阶微分计算库，其特点包括支持任意标量域、在定义域而非全空间上工作，并整合解析函数到更广泛的平滑函数中。


<details>
  <summary>Details</summary>
Motivation: 为了支持广泛的应用场景，文章对传统教材定义进行了多项扩展，包括标量域的灵活性、定义域的限制以及函数类型的整合。

Method: 通过数学和形式化角度解决广义化带来的挑战，开发了高阶微分计算库。

Result: 成功实现了一个适应性强、功能广泛的高阶微分计算工具，解决了广义化引入的问题。

Conclusion: 该库为数学和形式化验证领域提供了更灵活和强大的工具，展示了广义化方法的可行性。

Abstract: We report on the higher-order differential calculus library developed inside
the Lean mathematical library mathlib. To support a broad range of
applications, we depart in several ways from standard textbook definitions: we
allow arbitrary fields of scalars, we work with functions defined on domains
rather than full spaces, and we integrate analytic functions in the broader
scale of smooth functions. These generalizations introduce significant
challenges, which we address from both the mathematical and the formalization
perspectives.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [20] [Computational Cognitive Modeling to understand the effects of Racializing AI on Human-AI cooperation with PigChase Task](https://arxiv.org/abs/2509.04636)
*Swapnika Dulam,Christopher L Dancy*

Main category: cs.HC

TL;DR: 研究了AI系统训练数据的种族化对人类与AI合作的影响，发现不同种族参与者对种族化AI代理的态度存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探讨人类与AI合作中，训练数据的种族化如何影响互动，填补相关研究空白。

Method: 通过Pig Chase游戏实验和认知模型（ACT-R）分析参与者行为及策略。

Result: 不同种族参与者对种族化AI代理有显著不同态度，非白人对白人化AI代理更积极。

Conclusion: 需进一步研究种族化对AI互动的影响，以完善人类与AI合作的理解。

Abstract: Despite the continued anthropomorphization of AI systems, the potential
impact of racialization during human-AI interaction is understudied. This study
explores how human-AI cooperation may be impacted by the belief that data used
to train an AI system is racialized, that is, it was trained on data from a
specific group of people. During this study, participants completed a human-AI
cooperation task using the Pig Chase game. Participants of different
self-identified demographics interacted with AI agents whose perceived racial
identities were manipulated, allowing us to assess how sociocultural
perspectives influence the decision-making of participants in the game. After
the game, participants completed a survey questionnaire to explain the
strategies they used while playing the game and to understand the perceived
intelligence of their AI teammates. Statistical analysis of task behavior data
revealed a statistically significant effect of the participant's demographic,
as well as the interaction between this self-identified demographic and the
treatment condition (i.e., the perceived demographic of the agent). The results
indicated that Non-White participants viewed AI agents racialized as White in a
positive way compared to AI agents racialized as Black. Both Black and White
participants viewed the AI agent in the control treatment in a negative way. A
baseline cognitive model of the task using ACT-R cognitive architecture was
used to understand a cognitive-level, process-based explanation of the
participants' perspectives based on results found from the study. This model
helps us better understand the factors affecting the decision-making strategies
of the game participants. Results from analysis of these data, as well as
cognitive modeling, indicate a need to expand understanding of the ways
racialization (whether implicit or explicit) impacts interaction with AI
systems.

</details>


### [21] [Transforming Fashion with AI: A Comparative Study of Large Language Models in Apparel Design](https://arxiv.org/abs/2509.04705)
*Nusrat Jahan Lamia,Sadia Afrin Mim*

Main category: cs.HC

TL;DR: 该研究探讨了三大LLM（OpenAI、GeminiAI、Deepseek）在纺织制造多阶段（如可持续选择、生产规划等）中的应用，评估了它们在服装设计复制能力上的表现，并创建了定制数据集进行深入分析。


<details>
  <summary>Details</summary>
Motivation: 探索AI在纺织制造和服装设计中的实际应用潜力，帮助设计师提前评估效果并进行修改和预测。

Method: 通过定制数据集测试三大LLM在织物图像生成和分类中的表现，比较不同模型在设计和分类任务中的能力。

Result: OpenAI在织物构造和分类中表现最佳（LPIPS约0.2，分类准确率80%），而Deepseek表现较差。所有模型在复杂织物识别和设计生成中均存在困难。

Conclusion: AI在纺织制造中具有一定潜力，但过度依赖可能抑制人类创造力。模型在文本推荐方面表现良好，但在复杂图像任务中仍需改进。

Abstract: Fashion has evolved from handcrafted designs to automated production over the
years, where AI has added another dimension to it. Nowadays, practically every
industry uses artificial models to automate their operations. To explore their
role, we examined three prominent LLMs (OpenAI, GeminiAI, Deepseek) in multiple
stages of textile manufacturing (e.g., sustainable choice, cost effectiveness,
production planning, etc.). We assessed the models' ability to replicate
garment design using certain parameters (fabric construction, shade, weave,
silhouette, etc.). We compared the models in terms of different body types and
functional purposes (e.g., fashionwear, sportswear) so that designers could
evaluate effectiveness before developing actual patterns, make necessary
modifications, and conduct fashion forecasting beforehand. To facilitate deeper
analysis, we created a custom dataset specifically for fabric image generation
and classification. Our analysis revealed that, in terms of fabric
construction, the OpenAI DALL-E model integrated with ChatGPT outperformed
other models, achieving a lower LPIPS (Learned Perceptual Image Patch
Similarity) score of approximately $0.2$. In fabric classification from images,
we found OpenAI offered the best results by breaking down certain factors
(e.g., breathability, moisture-wicking, and tactile comfort), achieving
approximately $80\%$ accuracy for base construction and $55\%$ for detailed
construction. However, our results indicate that Deepseek faced significant
challenges in generating and recognizing fabric images. Overall, all the models
struggled to recognize complex fabric constructions and intricate designs from
images, and relying too much on AI might hinder human creativity. We also
observed that all three models performed effectively in providing
recommendations and insights for fabric design in textual form.

</details>


### [22] [SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching](https://arxiv.org/abs/2509.04752)
*Melik Ozolcer,Sang Won Bae*

Main category: cs.HC

TL;DR: SePA是一种结合个性化机器学习与检索增强生成的LLM健康教练系统，通过预测模型和检索模块提供个性化建议，优于通用基线，并在专家研究中表现良好。


<details>
  <summary>Details</summary>
Motivation: 开发一种结合个性化预测和可靠检索的健康教练系统，以提供更精准、可信的健康建议。

Method: 结合个性化预测模型（基于可穿戴数据）和检索增强生成（使用专家审阅的网络内容），并通过交叉验证和专家研究评估效果。

Result: 个性化模型优于通用基线，检索建议更受专家青睐（Cliff's δ=0.3，p=0.05）；同时平衡了响应质量与速度。

Conclusion: SePA为下一代可信个人健康信息系统提供了透明且高效的解决方案。

Abstract: This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM
health coaching system that integrates personalized machine learning and
retrieval-augmented generation to deliver adaptive, evidence-based guidance.
SePA combines: (1) Individualized models predicting daily stress, soreness, and
injury risk from wearable sensor data (28 users, 1260 data points); and (2) A
retrieval module that grounds LLM-generated feedback in expert-vetted web
content to ensure contextual relevance and reliability. Our predictive models,
evaluated with rolling-origin cross-validation and group k-fold
cross-validation show that personalized models outperform generalized
baselines. In a pilot expert study (n=4), SePA's retrieval-based advice was
preferred over a non-retrieval baseline, yielding meaningful practical effect
(Cliff's $\delta$=0.3, p=0.05). We also quantify latency performance trade-offs
between response quality and speed, offering a transparent blueprint for
next-generation, trustworthy personal health informatics systems.

</details>


### [23] [Evaluating Idle Animation Believability: a User Perspective](https://arxiv.org/abs/2509.05023)
*Eneko Atxa Landa,Elena Lazkano,Igor Rodriguez,Itsaso Rodríguez-Moreno,Itziar Irigoien*

Main category: cs.HC

TL;DR: 论文探讨了虚拟角色空闲动画的生成问题，指出无论是演员表演还是真实记录的空闲动画，用户都无法区分其真实性，且手工制作和记录的动画感知不同，这简化了动画录制过程。


<details>
  <summary>Details</summary>
Motivation: 高质量的空闲动画对虚拟角色的真实感至关重要，但目前缺乏广泛可用的数据集，且录制过程复杂昂贵。

Method: 通过研究手工制作和记录的空闲动画的感知差异，探讨了录制过程的简化可能。

Result: 用户无法区分演员表演和真实记录的空闲动画的真实性，且手工制作和记录的动画感知不同。

Conclusion: 演员可以被告知表演动作，从而简化录制过程，为未来录制空闲动画数据集提供帮助。

Abstract: Animating realistic avatars requires using high quality animations for every
possible state the avatar can be in. This includes actions like walking or
running, but also subtle movements that convey emotions and personality. Idle
animations, such as standing, breathing or looking around, are crucial for
realism and believability. In games and virtual applications, these are often
handcrafted or recorded with actors, but this is costly. Furthermore, recording
realistic idle animations can be very complex, because the actor must not know
they are being recorded in order to make genuine movements. For this reasons
idle animation datasets are not widely available. Nevertheless, this paper
concludes that both acted and genuine idle animations are perceived as real,
and that users are not able to distinguish between them. It also states that
handmade and recorded idle animations are perceived differently. These two
conclusions mean that recording idle animations should be easier than it is
thought to be, meaning that actors can be specifically told to act the
movements, significantly simplifying the recording process. These conclusions
should help future efforts to record idle animation datasets. Finally, we also
publish ReActIdle, a 3 dimensional idle animation dataset containing both real
and acted idle motions.

</details>


### [24] [Long-Term Experiences From Working with Extended Reality in the Wild](https://arxiv.org/abs/2509.05067)
*Verena Biener,Florian Jack Winston,Dieter Schmalstieg,Alexander Plopski*

Main category: cs.HC

TL;DR: 研究探讨了Apple Vision Pro（AVP）作为生产力工具的长期使用情况，发现尽管XR能提升效率，但仍存在应用程序不足、工作流整合困难等问题。


<details>
  <summary>Details</summary>
Motivation: 尽管XR设备作为生产力工具被广泛宣传，但缺乏在日常环境中长期使用的研究。

Method: 通过采访十位AVP用户，了解其使用体验和局限性。

Result: 用户认为XR能提高生产力，但存在应用不足、工作流整合困难等问题。

Conclusion: XR作为生产力工具仍有改进空间，需解决应用和工作流整合问题。

Abstract: Extended Reality (XR) is increasingly used as a productivity tool and recent
commercial XR devices have even been specifically designed as productivity
tools, or, at least, are heavily advertised for such purposes, such as the
Apple Vision Pro (AVP), which has now been available for more than one year. In
spite of what marketing suggests, research still lacks an understanding of the
long-term usage of such devices in ecologically valid everyday settings, as
most studies are conducted in very controlled environments. Therefore, we
conducted interviews with ten AVP users to better understand how experienced
users engage with the device, and which limitations persist. Our participants
report that XR can increase productivity and that they got used to the device
after some time. Yet, a range of limitations persist that might hinder the
widespread use of XR as a productivity tool, such as a lack of native
applications, difficulties when integrating XR into current workflows, and
limited possibilities to adapt and customize the XR experience.

</details>


### [25] [Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination](https://arxiv.org/abs/2509.05145)
*Błażej Kotowski,Nicholas Evans,Behzad Haki,Frederic Font,Sergi Jordà*

Main category: cs.HC

TL;DR: 论文探讨了GrooveTransformer系统在艺术场景中的多稳定性及其成因，并通过VCE方法分析了其在DMI设计中的价值。


<details>
  <summary>Details</summary>
Motivation: 研究GrooveTransformer系统如何在不同的艺术场景中展现出多稳定性，以及这种多稳定性是如何产生的。

Method: 采用Variational Cross- Examination（VCE）框架，分析系统在三个艺术场景中的应用。

Result: 发现系统的多稳定性源于系统不变性的功能、跨学科合作及其开发的情境性。

Conclusion: VCE是描述和分析DMI设计的有效方法，揭示了技术与用户及环境之间的相互塑造关系。

Abstract: This paper investigates GrooveTransformer, a real-time rhythm generation
system, through the postphenomenological framework of Variational
Cross-Examination (VCE). By reflecting on its deployment across three distinct
artistic contexts, we identify three stabilities: an autonomous drum
accompaniment generator, a rhythmic control voltage sequencer in Eurorack
format, and a rhythm driver for a harmonic accompaniment system. The
versatility of its applications was not an explicit goal from the outset of the
project. Thus, we ask: how did this multistability emerge? Through VCE, we
identify three key contributors to its emergence: the affordances of system
invariants, the interdisciplinary collaboration, and the situated nature of its
development. We conclude by reflecting on the viability of VCE as a descriptive
and analytical method for Digital Musical Instrument (DMI) design, emphasizing
its value in uncovering how technologies mediate, co-shape, and are co-shaped
by users and contexts.

</details>


### [26] [Transition of car-based human-mobility in the pandemic era: Data insight from a cross-border region in Europe](https://arxiv.org/abs/2509.05166)
*Sujit Kumar Sikder,Jyotirmaya Ijaradar,Hao Li,Hichem Omrani*

Main category: cs.HC

TL;DR: 研究分析了德国与邻国在COVID疫情前后的车辆交通量时空变化，提出了一种可重复的工作流计算交通流多维变量，并为碳净零转型提供了进一步研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着开放式数据的兴起，实时交通数据成为支持前瞻性决策的重要资源。研究旨在利用此类数据探索人类汽车出行行为的时空变化。

Method: 使用卢森堡等国的交通计数数据集，开发了一个可重复的工作流计算交通流多维变量，并进行了数据协调以处理缺失数据。

Result: 研究报告了疫情前后（2016-2018与2019-2021）德国与邻国之间的个体汽车出行行为变化。

Conclusion: 研究强调了在碳净零转型背景下，未来需进一步研究道路网络级别的插值和小规模方法，以确定污染热点及其与功能性地用模式的因果关系。

Abstract: Many transport authorities are collecting and publishing almost real-time
road traffic data to meet the growing trend of massive open data, a vital
resource for foresight decision support systems considering deep data insights.
Using such a traffic count dataset, we explored the spatio-temporal transitions
in the cross-country road traffic volumes in the context of modelling
behavioural transitions in car-based human mobility. We developed a
reproducible workflow for computing multi-dimensional variables of traffic
flow. This study reports on individual car-based daily travel behaviour
detected, before (2016-2018) and during the COVID pandemic (2019-2021), between
Germany and neighbouring countries (Luxembourg, France and Belgium). In
relevance to the net-zero carbon transition, further study should shed light on
the interpolation and downscaling approaches at the comprehensive road-network
level for identifying pollution hot spots, causal link to functional landuse
patterns and calculation of spatial influence area. In the case of Luxembourg,
the Bridges and Roads Authority has installed a large digital traffic
observatory infrastructure through the adoption of sensor-based IoT
technologies, like other European member states. Since 2016, they have provided
high-performance data processing and published open data on the country's road
traffic. The dataset contains an hourly traffic count for different vehicle
types, daily for representative observation points, followed by a major road
network. The original dataset contains significant missing entries, so
comprehensive data harmonization was performed.

</details>


### [27] [Conversational AI increases political knowledge as effectively as self-directed internet search](https://arxiv.org/abs/2509.05219)
*Lennart Luettgau,Hannah Rose Kirk,Kobi Hackenburg,Jessica Bergs,Henry Davidson,Henry Ogden,Divya Siddarth,Saffron Huang,Christopher Summerfield*

Main category: cs.HC

TL;DR: 研究表明，32%的聊天机器人用户在2024年英国大选前使用对话式AI获取政治信息，但对话式AI的使用并未增加公众对政治错误信息的信任，效果与自主互联网搜索相当。


<details>
  <summary>Details</summary>
Motivation: 随着对话式AI在信息检索中的普及，人们担忧其在政治领域可能传播偏见或虚假信息，影响选民决策。然而，这种担忧的实际影响尚不明确。

Method: 通过英国全国代表性调查（N=2,499）和随机对照实验（N=2,858），分析对话式AI的使用情况及对政治知识的影响。

Result: 32%的聊天机器人用户在选举前使用AI获取政治信息；对话式AI提升政治知识（增强真实信息信念，减少错误信息信念）的效果与互联网搜索相当。

Conclusion: 尽管对话式AI在英国政治信息检索中的使用增加，但其可能不会导致公众对政治错误信息的信任上升。

Abstract: Conversational AI systems are increasingly being used in place of traditional
search engines to help users complete information-seeking tasks. This has
raised concerns in the political domain, where biased or hallucinated outputs
could misinform voters or distort public opinion. However, in spite of these
concerns, the extent to which conversational AI is used for political
information-seeking, as well the potential impact of this use on users'
political knowledge, remains uncertain. Here, we address these questions:
First, in a representative national survey of the UK public (N = 2,499), we
find that in the week before the 2024 election as many as 32% of chatbot users
- and 13% of eligible UK voters - have used conversational AI to seek political
information relevant to their electoral choice. Second, in a series of
randomised controlled trials (N = 2,858 total) we find that across issues,
models, and prompting strategies, conversations with AI increase political
knowledge (increase belief in true information and decrease belief in
misinformation) to the same extent as self-directed internet search. Taken
together, our results suggest that although people in the UK are increasingly
turning to conversational AI for information about politics, this shift may not
lead to increased public belief in political misinformation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [28] [Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments](https://arxiv.org/abs/2509.04481)
*Yi-Chun Chen,Arnav Jhala*

Main category: cs.GR

TL;DR: 论文提出了一种轻量级流程，将短文本故事转换为2D游戏场景序列，结合了语言模型和程序生成技术。


<details>
  <summary>Details</summary>
Motivation: 解决叙事文本与可玩视觉环境连接的问题，推动程序化内容生成的发展。

Method: 系统从LLM生成的故事中提取关键时间帧和空间谓词（三元组），利用GameTileNet数据集检索视觉资源，并通过元胞自动机生成地形和对象布局。

Result: 在十个多样化故事中评估，验证了瓦片对象匹配、功能层对齐和空间约束满足的效果。

Conclusion: 该原型为叙事驱动的场景生成提供了可扩展方案，并为未来的多帧连续性、符号追踪和多智能体协作研究奠定了基础。

Abstract: Recent advances in large language models(LLMs) enable compelling story
generation, but connecting narrative text to playable visual environments
remains an open challenge in procedural content generation(PCG). We present a
lightweight pipeline that transforms short narrative prompts into a sequence of
2D tile-based game scenes, reflecting the temporal structure of stories. Given
an LLM-generated narrative, our system identifies three key time frames,
extracts spatial predicates in the form of "Object-Relation-Object" triples,
and retrieves visual assets using affordance-aware semantic embeddings from the
GameTileNet dataset. A layered terrain is generated using Cellular Automata,
and objects are placed using spatial rules grounded in the predicate structure.
We evaluated our system in ten diverse stories, analyzing tile-object matching,
affordance-layer alignment, and spatial constraint satisfaction across frames.
This prototype offers a scalable approach to narrative-driven scene generation
and lays the foundation for future work on multi-frame continuity, symbolic
tracking, and multi-agent coordination in story-centered PCG.

</details>


### [29] [Fidelity-preserving enhancement of ptychography with foundational text-to-image models](https://arxiv.org/abs/2509.04513)
*Ming Du,Volker Rose,Junjing Deng,Dileep Singh,Si Chen,Mathew J. Cherukara*

Main category: cs.GR

TL;DR: 本文提出了一种结合物理模型相位检索与文本引导图像编辑的插件（PnP）框架，利用扩散模型提升图像质量并减少伪影。


<details>
  <summary>Details</summary>
Motivation: 解决ptychographic相位检索中存在的网格病理和多层串扰等伪影问题，提升重建图像质量。

Method: 采用交替方向乘子法（ADMM）整合数据保真度与伪影去除子问题，结合文本引导的扩散图像编辑技术（LEDITS++）。

Result: 在模拟和实验数据集上表现出显著的伪影抑制和结构保真度提升，通过PSNR等指标验证。

Conclusion: 结合文本引导生成模型与模型驱动的相位检索算法，为高质量衍射成像提供了可迁移且保真的方法。

Abstract: Ptychographic phase retrieval enables high-resolution imaging of complex
samples but often suffers from artifacts such as grid pathology and multislice
crosstalk, which degrade reconstructed images. We propose a plug-and-play (PnP)
framework that integrates physics model-based phase retrieval with text-guided
image editing using foundational diffusion models. By employing the alternating
direction method of multipliers (ADMM), our approach ensures consensus between
data fidelity and artifact removal subproblems, maintaining physics consistency
while enhancing image quality. Artifact removal is achieved using a text-guided
diffusion image editing method (LEDITS++) with a pre-trained foundational
diffusion model, allowing users to specify artifacts for removal in natural
language. Demonstrations on simulated and experimental datasets show
significant improvements in artifact suppression and structural fidelity,
validated by metrics such as peak signal-to-noise ratio (PSNR) and diffraction
pattern consistency. This work highlights the combination of text-guided
generative models and model-based phase retrieval algorithms as a transferable
and fidelity-preserving method for high-quality diffraction imaging.

</details>


### [30] [Improved 3D Scene Stylization via Text-Guided Generative Image Editing with Region-Based Control](https://arxiv.org/abs/2509.05285)
*Haruo Fujiwara,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.GR

TL;DR: 本文提出了一种改进文本驱动3D场景编辑和风格化的方法，通过重新训练3D表示并结合多视图风格化图像，解决了风格一致性和视角一致性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现高质量风格化和视角一致性方面存在不足，尤其是在不同区域或对象上应用语义一致的风格时。

Method: 采用参考式注意力共享机制增强风格对齐，利用多深度图网格提升视角一致性，并通过加权损失函数实现多区域风格控制。

Result: 实验证明，该方法显著提升了3D风格化的效果，支持多区域风格混合。

Conclusion: 该方法在保持风格和视角一致性的同时，提供了灵活的区域控制风格迁移能力。

Abstract: Recent advances in text-driven 3D scene editing and stylization, which
leverage the powerful capabilities of 2D generative models, have demonstrated
promising outcomes. However, challenges remain in ensuring high-quality
stylization and view consistency simultaneously. Moreover, applying style
consistently to different regions or objects in the scene with semantic
correspondence is a challenging task. To address these limitations, we
introduce techniques that enhance the quality of 3D stylization while
maintaining view consistency and providing optional region-controlled style
transfer. Our method achieves stylization by re-training an initial 3D
representation using stylized multi-view 2D images of the source views.
Therefore, ensuring both style consistency and view consistency of stylized
multi-view images is crucial. We achieve this by extending the style-aligned
depth-conditioned view generation framework, replacing the fully shared
attention mechanism with a single reference-based attention-sharing mechanism,
which effectively aligns style across different viewpoints. Additionally,
inspired by recent 3D inpainting methods, we utilize a grid of multiple depth
maps as a single-image reference to further strengthen view consistency among
stylized images. Finally, we propose Multi-Region Importance-Weighted Sliced
Wasserstein Distance Loss, allowing styles to be applied to distinct image
regions using segmentation masks from off-the-shelf models. We demonstrate that
this optional feature enhances the faithfulness of style transfer and enables
the mixing of different styles across distinct regions of the scene.
Experimental evaluations, both qualitative and quantitative, demonstrate that
our pipeline effectively improves the results of text-driven 3D stylization.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [31] [Analyzing Gait Adaptation with Hemiplegia Simulation Suits and Digital Twins](https://arxiv.org/abs/2509.05116)
*Jialin Chen,Jeremie Clos,Dominic Price,Praminda Caleb-Solly*

Main category: cs.ET

TL;DR: 研究通过健康参与者穿戴模拟特定疾病的服装，在受控环境中研究步态变化，以支持康复机器人的快速原型设计。论文分析了偏瘫模拟服装对步态的影响，并利用数字孪生模型和机器学习方法评估其效果。


<details>
  <summary>Details</summary>
Motivation: 早期测试康复机器人原型时直接接触用户存在安全隐患，因此需要一种安全的替代方法。

Method: 使用Vicon运动捕捉系统和Delsys Trigno EMG/IMU传感器收集四种步行条件下的生物力学数据，并集成到数字孪生模型中进行分析。

Result: 模拟服装显著改变了运动和肌肉激活模式，使用者需要更突然的动作来补偿。研究还确定了最能捕捉步态动态和人与助行器互动的关键特征和传感器类型。

Conclusion: 模拟特定疾病的服装是研究步态变化的安全有效方法，同时也为数字孪生框架提供了有价值的步态数据。

Abstract: To advance the development of assistive and rehabilitation robots, it is
essential to conduct experiments early in the design cycle. However, testing
early prototypes directly with users can pose safety risks. To address this, we
explore the use of condition-specific simulation suits worn by healthy
participants in controlled environments as a means to study gait changes
associated with various impairments and support rapid prototyping. This paper
presents a study analyzing the impact of a hemiplegia simulation suit on gait.
We collected biomechanical data using a Vicon motion capture system and Delsys
Trigno EMG and IMU sensors under four walking conditions: with and without a
rollator, and with and without the simulation suit. The gait data was
integrated into a digital twin model, enabling machine learning analyses to
detect the use of the simulation suit and rollator, identify turning behavior,
and evaluate how the suit affects gait over time. Our findings show that the
simulation suit significantly alters movement and muscle activation patterns,
prompting users to compensate with more abrupt motions. We also identify key
features and sensor modalities that are most informative for accurately
capturing gait dynamics and modeling human-rollator interaction within the
digital twin framework.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs](https://arxiv.org/abs/2509.04719)
*Han Liang,Jiahui Zhou,Zicheng Zhou,Xiaoxi Zhang,Xu Chen*

Main category: cs.DC

TL;DR: STADI是一种新颖的框架，通过时空自适应调度加速异构多GPU环境中的扩散模型推理，减少延迟45%并提升资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散并行推理方法在异构多GPU环境下资源利用率低和负载不平衡的问题。

Method: 采用混合调度器，结合计算感知的步骤分配器和弹性补丁并行机制，优化时空维度的负载分配。

Result: 实验表明STADI显著提升负载均衡，减少端到端推理延迟45%，并改善异构GPU的资源利用率。

Conclusion: STADI在异构环境中高效加速扩散模型推理，具有实际应用价值。

Abstract: The escalating adoption of diffusion models for applications such as image
generation demands efficient parallel inference techniques to manage their
substantial computational cost. However, existing diffusion parallelism
inference schemes often underutilize resources in heterogeneous multi-GPU
environments, where varying hardware capabilities or background tasks cause
workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion
Inference (STADI), a novel framework to accelerate diffusion model inference in
such settings. At its core is a hybrid scheduler that orchestrates fine-grained
parallelism across both temporal and spatial dimensions. Temporally, STADI
introduces a novel computation-aware step allocator applied after warmup
phases, using a least-common-multiple-minimizing quantization technique to
reduce denoising steps on slower GPUs and execution synchronization. To further
minimize GPU idle periods, STADI executes an elastic patch parallelism
mechanism that allocates variably sized image patches to GPUs according to
their computational capability, ensuring balanced workload distribution through
a complementary spatial mechanism. Extensive experiments on both
load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,
demonstrating improved load balancing and mitigation of performance
bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion
inference framework, our method significantly reduces end-to-end inference
latency by up to 45% and significantly improves resource utilization on
heterogeneous GPUs.

</details>


### [33] [VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving](https://arxiv.org/abs/2509.04827)
*Jiahuan Yu,Aryan Taneja,Junfeng Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: VoltanaLLM是一种基于控制理论的SLO感知、高效节能LLM服务系统，通过联合设计频率缩放和请求路由，实现高达36.3%的节能，同时保持近乎完美的SLO达标率。


<details>
  <summary>Details</summary>
Motivation: 现代LLM服务系统的能源成本持续攀升，对可持续和成本效益部署提出了挑战。

Method: VoltanaLLM结合反馈驱动的频率控制器和状态空间路由器，动态调整GPU频率并优化路由决策，以最小化能源消耗。

Result: 在多个先进LLM和真实数据集上的实验显示，VoltanaLLM实现高达36.3%的节能，同时维持高SLO达标率。

Conclusion: VoltanaLLM为可持续和智能LLM服务提供了有效解决方案。

Abstract: Modern Large Language Model (LLM) serving systems increasingly support
interactive applications, like real-time chat assistants, code generation
tools, and agentic workflows. However, the soaring energy cost of LLM inference
presents a growing challenge for sustainable and cost-effective deployment.
This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM
serving, built from a control theory perspective. VoltanaLLM co-designs
frequency scaling and request routing in emerging prefill/decode disaggregated
architectures, leveraging their decoupled execution to enable fine-grained
phase-specific control. It consists of a feedback-driven frequency controller
that dynamically adapts GPU frequency for prefill and decode phases, and a
state-space router that explores routing decisions across frequency-scaled
instances to minimize energy under latency constraints. We implement VoltanaLLM
in SGLang and evaluate its performance over multiple state-of-the-art LLMs and
real-world datasets. The results demonstrate that VoltanaLLM achieves up to
36.3% energy savings while maintaining near-perfect SLO attainment rate, paving
the way for sustainable and intelligent LLM serving.

</details>


### [34] [Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization](https://arxiv.org/abs/2509.05216)
*Mengjiao Han,Andres Sewell,Joseph Insley,Janet Knowles,Victor A. Mateevitsi,Michael E. Papka,Steve Petruzza,Silvio Rizzi*

Main category: cs.DC

TL;DR: 本文提出了3D高斯散点（3D-GS）的多GPU扩展方法，用于科学可视化。通过多GPU训练后端，提升了大规模数据集的训练效率和高分辨率重建能力。


<details>
  <summary>Details</summary>
Motivation: 为了实现对大规模科学数据的高分辨率实时可视化，克服单GPU在处理高复杂度数据集时的性能瓶颈。

Method: 在3D-GS的基础上，引入多GPU训练后端（Grendel-GS），分布式优化处理数据，提升训练速度和扩展性。

Result: 实验表明，使用四GPU处理Kingsnake数据集（4M高斯）时提速5.6倍；Miranda数据集（18M高斯）在单GPU无法完成的任务下也能成功训练。

Conclusion: 本研究为将3D-GS集成到高性能科学工作流中奠定了基础，支持复杂模拟的实时可视化。

Abstract: We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)
pipeline for scientific visualization. Building on previous work that
demonstrated high-fidelity isosurface reconstruction using Gaussian primitives,
we incorporate a multi-GPU training backend adapted from Grendel-GS to enable
scalable processing of large datasets. By distributing optimization across
GPUs, our method improves training throughput and supports high-resolution
reconstructions that exceed single-GPU capacity. In our experiments, the system
achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs
compared to a single-GPU baseline, and successfully trains the Miranda dataset
(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays
the groundwork for integrating 3D-GS into HPC-based scientific workflows,
enabling real-time post hoc and in situ visualization of complex simulations.

</details>


### [35] [Dynamic reconfiguration for malleable applications using RMA](https://arxiv.org/abs/2509.05248)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo*

Main category: cs.DC

TL;DR: 研究基于MPI远程内存访问(RMA)的单边通信方法，动态调整可扩展应用规模，与传统集体通信方法对比，性能相当但初始化成本较高。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过单边通信方法优化动态调整应用规模时的数据重新分配，减少对应用执行的影响。

Method: 将基于RMA的单边通信方法集成到MaM库中，并与传统集体通信方法对比，同时扩展Wait Drains策略以支持高效后台重新配置。

Result: 性能与传统方法相当，但高初始化成本限制了其优势。

Conclusion: 单边通信方法在动态调整中有潜力，但需降低初始化成本以发挥更大优势。

Abstract: This paper investigates the novel one-sided communication methods based on
remote memory access (RMA) operations in MPI for dynamic resizing of malleable
applications, enabling data redistribution with minimal impact on application
execution. After their integration into the MaM library, these methods are
compared with traditional collective-based approaches. In addition, the
existing strategy Wait Drains is extended to support efficient background
reconfiguration. Results show comparable performance, though high
initialization costs currently limit their advantage.

</details>


### [36] [Scaling Performance of Large Language Model Pretraining](https://arxiv.org/abs/2509.05258)
*Alexander Interrante-Grant,Carla Varela-Rosa,Suhaas Narayan,Chris Connelly,Albert Reuther*

Main category: cs.DC

TL;DR: 这篇论文旨在揭示大规模语言模型预训练流程的核心内容，特别是分布式训练、大规模数据集管理以及数据并行扩展的技术优化。


<details>
  <summary>Details</summary>
Motivation: 当前，大规模语言模型的训练需要巨大的计算资源，但公开文献中关于其扩展性能和训练优化的信息稀缺。作者希望通过本文为研究者提供实际指导。

Method: 论文重点讨论了分布式训练的技术、跨数百个节点管理大规模数据集的方法，以及如何通过数据并行充分利用GPU计算能力。

Result: 论文提供了实用建议和技术细节，帮助优化大规模语言模型的训练效率。

Conclusion: 通过本文的研究，可以为未来的大规模语言模型训练提供重要的技术参考，填补了公开文献中的空白。

Abstract: Large language models (LLMs) show best-in-class performance across a wide
range of natural language processing applications. Training these models is an
extremely computationally expensive task; frontier Artificial Intelligence (AI)
research companies are investing billions of dollars into supercomputing
infrastructure to train progressively larger models on increasingly massive
datasets. Unfortunately, information about the scaling performance and training
considerations of these large training pipelines is scarce in public
literature. Working with large-scale datasets and models can be complex and
practical recommendations are scarce in the public literature for tuning
training performance when scaling up large language models. In this paper, we
aim to demystify the large language model pretraining pipeline somewhat - in
particular with respect to distributed training, managing large datasets across
hundreds of nodes, and scaling up data parallelism with an emphasis on fully
leveraging available GPU compute capacity.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [37] [Schema Inference for Tabular Data Repositories Using Large Language Models](https://arxiv.org/abs/2509.04632)
*Zhenyu Wu,Jiaoyan Chen,Norman W. Paton*

Main category: cs.DB

TL;DR: SI-LLM（基于大型语言模型的模式推断）通过列标题和单元格值推断表格数据的简明概念模式，性能优于或与现有最佳方法相当。


<details>
  <summary>Details</summary>
Motivation: 由于异构数据源的表示不一致和元数据稀疏，处理此类表格数据具有挑战性。现有研究在模式推断方面仍有不足。

Method: SI-LLM仅利用列标题和单元格值，推断出包含层次化实体类型、属性和类型间关系的概念模式。

Result: 在两个数据集上的广泛评估表明，SI-LLM在端到端结果和各步骤性能上均优于或与现有最佳方法相当。

Conclusion: SI-LLM为稀疏元数据下的表格数据模式推断提供了有效解决方案，所有资源和代码均已开源。

Abstract: Minimally curated tabular data often contain representational inconsistencies
across heterogeneous sources, and are accompanied by sparse metadata. Working
with such data is intimidating. While prior work has advanced dataset discovery
and exploration, schema inference remains difficult when metadata are limited.
We present SI-LLM (Schema Inference using Large Language Models), which infers
a concise conceptual schema for tabular data using only column headers and cell
values. The inferred schema comprises hierarchical entity types, attributes,
and inter-type relationships. In extensive evaluation on two datasets from web
tables and open data, SI-LLM achieves promising end-to-end results, as well as
better or comparable results to state-of-the-art methods at each step. All
source code, full prompts, and datasets of SI-LLM are available at
https://github.com/PierreWoL/SILLM.

</details>


### [38] [Efficient Exact Resistance Distance Computation on Small-Treewidth Graphs: a Labelling Approach](https://arxiv.org/abs/2509.05129)
*Meihao Liao,Yueyang Pan,Rong-Hua Li,Guoren Wang*

Main category: cs.DB

TL;DR: 提出了一种基于树分解的索引方法TreeIndex，用于高效计算小树宽图（如道路网络）上的电阻距离，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有随机游走方法在小树宽图上效率低下且只能提供近似解，而最短路径距离通过树分解实现了高效计算，但电阻距离的直接推广因矩阵操作成本高而不切实际。

Method: 通过分析电阻距离的割性质，并结合树分解，提出了一种紧凑的标签结构TreeIndex，其构建时间和标签大小均与小树宽图的实际参数相关。

Result: TreeIndex在小树宽图（如美国道路网络）上表现卓越，构建时间短且查询速度快，首次实现了对大规模图的可扩展精确计算。

Conclusion: TreeIndex实现了电阻距离的高效精确计算，为解决小树宽图上的该类问题提供了有效工具。

Abstract: Resistance distance computation is a fundamental problem in graph analysis,
yet existing random walk-based methods are limited to approximate solutions and
suffer from poor efficiency on small-treewidth graphs (e.g., road networks). In
contrast, shortest-path distance computation achieves remarkable efficiency on
such graphs by leveraging cut properties and tree decompositions. Motivated by
this disparity, we first analyze the cut property of resistance distance. While
a direct generalization proves impractical due to costly matrix operations, we
overcome this limitation by integrating tree decompositions, revealing that the
resistance distance $r(s,t)$ depends only on labels along the paths from $s$
and $t$ to the root of the decomposition. This insight enables compact
labelling structures. Based on this, we propose \treeindex, a novel index
method that constructs a resistance distance labelling of size $O(n \cdot
h_{\mathcal{G}})$ in $O(n \cdot h_{\mathcal{G}}^2 \cdot d_{\max})$ time, where
$h_{\mathcal{G}}$ (tree height) and $d_{\max}$ (maximum degree) behave as small
constants in many real-world small-treewidth graphs (e.g., road networks). Our
labelling supports exact single-pair queries in $O(h_{\mathcal{G}})$ time and
single-source queries in $O(n \cdot h_{\mathcal{G}})$ time. Extensive
experiments show that TreeIndex substantially outperforms state-of-the-art
approaches. For instance, on the full USA road network, it constructs a $405$
GB labelling in $7$ hours (single-threaded) and answers exact single-pair
queries in $10^{-3}$ seconds and single-source queries in $190$ seconds--the
first exact method scalable to such large graphs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [39] [Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization](https://arxiv.org/abs/2509.04646)
*Philippe J. Giabbanelli,Ameeta Agrawal*

Main category: cs.AI

TL;DR: 本文提出了一种框架，利用大型语言模型（LLMs）为不同健康领域的利益相关者定制仿真模型的解释，弥补了现有方法无法满足多样化需求的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的仿真模型（如基于代理的模型）在支持健康决策方面潜力巨大，但由于模型复杂性和解释的通用性不足，利益相关者难以充分利用。

Method: 采用混合方法设计，首先收集利益相关者的需求和风格偏好，然后优化LLMs以生成定制化输出，并通过多指标评估改进。

Result: 框架成功识别了利益相关者的需求，并优化了LLMs的生成能力，以提供更具针对性的解释。

Conclusion: 该框架为健康仿真模型的定制解释提供了可行方案，有望提升决策支持的效果。

Abstract: Modeling & Simulation (M&S) approaches such as agent-based models hold
significant potential to support decision-making activities in health, with
recent examples including the adoption of vaccines, and a vast literature on
healthy eating behaviors and physical activity behaviors. These models are
potentially usable by different stakeholder groups, as they support
policy-makers to estimate the consequences of potential interventions and they
can guide individuals in making healthy choices in complex environments.
However, this potential may not be fully realized because of the models'
complexity, which makes them inaccessible to the stakeholders who could benefit
the most. While Large Language Models (LLMs) can translate simulation outputs
and the design of models into text, current approaches typically rely on
one-size-fits-all summaries that fail to reflect the varied informational needs
and stylistic preferences of clinicians, policymakers, patients, caregivers,
and health advocates. This limitation stems from a fundamental gap: we lack a
systematic understanding of what these stakeholders need from explanations and
how to tailor them accordingly. To address this gap, we present a step-by-step
framework to identify stakeholder needs and guide LLMs in generating tailored
explanations of health simulations. Our procedure uses a mixed-methods design
by first eliciting the explanation needs and stylistic preferences of diverse
health stakeholders, then optimizing the ability of LLMs to generate tailored
outputs (e.g., via controllable attribute tuning), and then evaluating through
a comprehensive range of metrics to further improve the tailored generation of
summaries.

</details>


### [40] [Maestro: Joint Graph & Config Optimization for Reliable AI Agents](https://arxiv.org/abs/2509.04642)
*Wenxiao Wang,Priyatham Kattakinda,Soheil Feizi*

Main category: cs.AI

TL;DR: Maestro是一个整体优化框架，通过联合搜索图结构和节点配置来提升LLM代理的性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有优化器仅调整节点配置，而忽略了图结构的优化，导致结构性失败模式无法解决。

Method: Maestro框架同时优化图结构和节点配置，利用文本反馈提高样本效率，并针对特定失败模式。

Result: 在IFBench和HotpotQA基准测试中，Maestro平均优于其他优化器4.86%-12%，且在提示优化方面也有显著提升。

Conclusion: 联合优化图结构和配置能解决仅优化提示无法解决的问题，显著提升LLM代理的表现。

Abstract: Building reliable LLM agents requires decisions at two levels: the graph
(which modules exist and how information flows) and the configuration of each
node (models, prompts, tools, control knobs). Most existing optimizers tune
configurations while holding the graph fixed, leaving structural failure modes
unaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for
LLM agents that jointly searches over graphs and configurations to maximize
agent quality, subject to explicit rollout/token budgets. Beyond numeric
metrics, Maestro leverages reflective textual feedback from traces to
prioritize edits, improving sample efficiency and targeting specific failure
modes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses
leading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,
4.9%, and 4.86%, respectively; even when restricted to prompt-only
optimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these
results with far fewer rollouts than GEPA. We further show large gains on two
applications (interviewer & RAG agents), highlighting that joint graph &
configuration search addresses structural failure modes that prompt tuning
alone cannot fix.

</details>


### [41] [An Approach to Grounding AI Model Evaluations in Human-derived Criteria](https://arxiv.org/abs/2509.04676)
*Sasha Mitts*

Main category: cs.AI

TL;DR: 该论文提出了一种结合人类评估标准的新方法，以增强AI模型行为的可解释性和适用性。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试难以捕捉AI模型的细微能力，特别是在物理世界建模领域。

Method: 基于Perception Test和OpenEQA基准，通过深度访谈和大规模调查，识别关键认知技能。

Result: 参与者认为AI缺乏解释性和同理心技能，但对AI表现有高期望。

Conclusion: 研究强调用户中心评估的重要性，为未来AI模型评估提供了可行框架。

Abstract: In the rapidly evolving field of artificial intelligence (AI), traditional
benchmarks can fall short in attempting to capture the nuanced capabilities of
AI models. We focus on the case of physical world modeling and propose a novel
approach to augment existing benchmarks with human-derived evaluation criteria,
aiming to enhance the interpretability and applicability of model behaviors.
Grounding our study in the Perception Test and OpenEQA benchmarks, we conducted
in-depth interviews and large-scale surveys to identify key cognitive skills,
such as Prioritization, Memorizing, Discerning, and Contextualizing, that are
critical for both AI and human reasoning. Our findings reveal that participants
perceive AI as lacking in interpretive and empathetic skills yet hold high
expectations for AI performance. By integrating insights from our findings into
benchmark design, we offer a framework for developing more human-aligned means
of defining and measuring progress. This work underscores the importance of
user-centered evaluation in AI development, providing actionable guidelines for
researchers and practitioners aiming to align AI capabilities with human
cognitive processes. Our approach both enhances current benchmarking practices
and sets the stage for future advancements in AI model evaluation.

</details>


### [42] [TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models](https://arxiv.org/abs/2509.04809)
*Haechang Kim,Hao Chen,Can Li,Jong Min Lee*

Main category: cs.AI

TL;DR: TalkToAgent是一个多代理大型语言模型框架，旨在通过交互式自然语言解释提升强化学习策略的透明度，弥补XRL方法与用户需求之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有XRL方法的解释结果不够直观且工具覆盖不全面，导致用户难以理解复杂的强化学习策略。

Method: 提出TalkToAgent框架，包含五个专用LLM代理，通过自然语言交互自动匹配用户查询与XRL工具，并提供状态变量、预期结果或反事实解释。

Result: 在四重水箱过程控制问题上验证了TalkToAgent的高精度任务映射能力，且反事实生成的失败率显著降低。

Conclusion: TalkToAgent能有效解释和情境化智能体的行为，提升了透明度和用户理解能力。

Abstract: Explainable Reinforcement Learning (XRL) has emerged as a promising approach
in improving the transparency of Reinforcement Learning (RL) agents. However,
there remains a gap between complex RL policies and domain experts, due to the
limited comprehensibility of XRL results and isolated coverage of current XRL
approaches that leave users uncertain about which tools to employ. To address
these challenges, we introduce TalkToAgent, a multi-agent Large Language Models
(LLM) framework that delivers interactive, natural language explanations for RL
policies. The architecture with five specialized LLM agents (Coordinator,
Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically
map user queries to relevant XRL tools and clarify an agent's actions in terms
of either key state variables, expected outcomes, or counterfactual
explanations. Moreover, our approach extends previous counterfactual
explanations by deriving alternative scenarios from qualitative behavioral
descriptions, or even new rule-based policies. We validated TalkToAgent on
quadruple-tank process control problem, a well-known nonlinear control
benchmark. Results demonstrated that TalkToAgent successfully mapped user
queries into XRL tasks with high accuracy, and coder-debugger interactions
minimized failures in counterfactual generation. Furthermore, qualitative
evaluation confirmed that TalkToAgent effectively interpreted agent's actions
and contextualized their meaning within the problem domain.

</details>


### [43] [SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing](https://arxiv.org/abs/2509.04908)
*Hongyi Jing,Jiafu Chen,Chen Rao,Ziqiang Dang,Jiajie Teng,Tianyi Chu,Juncheng Mo,Shuo Fang,Huaizhong Lin,Rui Lv,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: SparkUI-Parser提出了一个新颖的端到端框架，通过连续建模坐标和引入拒绝机制，解决了现有MLLMs在GUI解析中的精度和速度问题，并在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在GUI解析中存在坐标离散建模导致的低精度和慢推理速度，且无法解析整个界面，限制了其广泛应用。

Method: 提出了基于预训练MLLM的连续坐标建模框架，结合令牌路由器和坐标解码器，并引入了改进的匈牙利匹配算法作为拒绝机制。

Result: 实验证明，SparkUI-Parser在多个基准测试中显著优于现有方法，提高了精度和推理速度。

Conclusion: SparkUI-Parser为GUI解析提供了一种高效且鲁棒的解决方案，支持更广泛的界面解析需求。

Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have
made great progress. However, the following challenges still exist in prior
methods: 1) They model discrete coordinates based on text autoregressive
mechanism, which results in lower grounding accuracy and slower inference
speed. 2) They can only locate predefined sets of elements and are not capable
of parsing the entire interface, which hampers the broad application and
support for downstream tasks. To address the above issues, we propose
SparkUI-Parser, a novel end-to-end framework where higher localization
precision and fine-grained parsing capability of the entire interface are
simultaneously achieved. Specifically, instead of using probability-based
discrete modeling, we perform continuous modeling of coordinates based on a
pre-trained Multimodal Large Language Model (MLLM) with an additional token
router and coordinate decoder. This effectively mitigates the limitations
inherent in the discrete output characteristics and the token-by-token
generation process of MLLMs, consequently boosting both the accuracy and the
inference speed. To further enhance robustness, a rejection mechanism based on
a modified Hungarian matching algorithm is introduced, which empowers the model
to identify and reject non-existent elements, thereby reducing false positives.
Moreover, we present ScreenParse, a rigorously constructed benchmark to
systematically assess structural perception capabilities of GUI models across
diverse scenarios. Extensive experiments demonstrate that our approach
consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,
CAGUI-Grounding and ScreenParse benchmarks. The resources are available at
https://github.com/antgroup/SparkUI-Parser.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [44] [Distributed-HISQ: A Distributed Quantum Control Architecture](https://arxiv.org/abs/2509.04798)
*Yilun Zhao,Kangding Zhao,Peng Zhou,Dingdong Liu,Tingyu Luo,Yuzhen Zheng,Peng Luo,Shun Hu,Jin Lin,Cheng Guo,Yinhe Han,Ying Wang,Mingtang Deng,Junjie Wu,X. Fu*

Main category: quant-ph

TL;DR: 论文提出了Distributed-HISQ，解决了量子控制架构的可扩展性问题，通过HISQ指令集和BISP同步协议，显著提升了执行效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 量子比特数量不断增加，分布式量子控制架构不可避免，但现有同步策略性能开销大，量子指令集设计缺乏统一性，限制了系统的可重构性和扩展性。

Method: 提出Distributed-HISQ，包括HISQ（硬件无关的通用指令集）和BISP（基于预订的同步协议），并在商用量子控制系统上实现验证。

Result: BISP显著减少了同步开销，程序执行时间平均减少22.8%，保真度提升约5倍。

Conclusion: Distributed-HISQ通过统一的指令集和高效的同步协议，为量子控制架构的可扩展性和性能提供了有效解决方案。

Abstract: The design of a scalable Quantum Control Architecture (QCA) faces two primary
challenges. First, the continuous growth in qubit counts has rendered
distributed QCA inevitable, yet the nondeterministic latencies inherent in
feedback loops demand cycleaccurate synchronization across multiple
controllers. Existing synchronization strategies -- whether lock-step or
demand-driven -- introduce significant performance penalties. Second, existing
quantum instruction set architectures are polarized, being either too abstract
or too granular. This lack of a unifying design necessitates recurrent hardware
customization for each new control requirement, which limits the system's
reconfigurability and impedes the path toward a scalable and unified digital
microarchitecture.
  Addressing these challenges, we propose Distributed-HISQ, featuring: (i)
HISQ, A universal instruction set that redefines quantum control with a
hardware-agnostic design. By decoupling from quantum operation semantics, HISQ
provides a unified language for control sequences, enabling a single
microarchitecture to support various control methods and enhancing system
reconfigurability. (ii) BISP, a booking-based synchronization protocol that can
potentially achieve zero-cycle synchronization overhead. The feasibility and
adaptability of Distributed-HISQ are validated through its implementation on a
commercial quantum control system targeting superconducting qubits. We
performed a comprehensive evaluation using a customized quantum software stack.
Our results show that BISP effectively synchronizes multiple control boards,
leading to a 22.8% reduction in average program execution time and a
$\sim$5$\times$ reduction in infidelity when compared to an existing lock-step
synchronization scheme.

</details>


### [45] [Histogram Driven Amplitude Embedding for Qubit Efficient Quantum Image Compression](https://arxiv.org/abs/2509.04849)
*Sahil Tomar,Sandeep Kumar*

Main category: quant-ph

TL;DR: 本文提出了一种利用近期量子设备压缩彩色图像的紧凑且硬件高效方法，通过分块和直方图编码实现高效量子态嵌入，显著优于传统像素级编码。


<details>
  <summary>Details</summary>
Motivation: 解决传统图像压缩方法在量子设备上的资源效率问题，探索在NISQ时代量子硬件上实现高效图像压缩的可行性。

Method: 将图像分块为固定大小的bixels，计算每块总强度，构建全局直方图并将其归一化平方根编码为量子态振幅，使用PennyLane在IBM量子硬件上执行。

Result: 实验结果表明，仅需5-7个量子比特即可实现高质量重建，资源效率显著优于传统方法。

Conclusion: 该方法在NISQ时代量子设备上具有实际应用潜力，通过调节直方图分箱数平衡保真度与资源使用。

Abstract: This work introduces a compact and hardware efficient method for compressing
color images using near term quantum devices. The approach segments the image
into fixed size blocks called bixels, and computes the total intensity within
each block. A global histogram with B bins is then constructed from these block
intensities, and the normalized square roots of the bin counts are encoded as
amplitudes into an n qubit quantum state. Amplitude embedding is performed
using PennyLane and executed on real IBM Quantum hardware. The resulting state
is measured to reconstruct the histogram, enabling approximate recovery of
block intensities and full image reassembly. The method maintains a constant
qubit requirement based solely on the number of histogram bins, independent of
the resolution of the image. By adjusting B, users can control the trade off
between fidelity and resource usage. Empirical results demonstrate high quality
reconstructions using as few as 5 to 7 qubits, significantly outperforming
conventional pixel level encodings in terms of qubit efficiency and validating
the practical application of the method for current NISQ era quantum systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [46] [Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks](https://arxiv.org/abs/2509.05273)
*Jason Gardner,Ayan Dutta,Swapnoneel Roy,O. Patrick Kreidl,Ladislau Boloni*

Main category: cs.LG

TL;DR: 论文通过系统比较7种DRL算法的能源消耗、碳排放和成本，揭示了算法效率的显著差异，并指出在不牺牲性能的情况下如何减少环境影响和经济成本。


<details>
  <summary>Details</summary>
Motivation: 研究DRL算法的环境与经济成本，填补能源效率研究的空白。

Method: 在10款Atari 2600游戏上训练7种DRL算法各100万步，实时测量能耗并计算碳排放和成本。

Result: 不同算法效率差异大，如ARS比DQN节能24%，QR-DQN比RecurrentPPO减少68%碳排放和成本。

Conclusion: 为开发高效且可持续的DRL实践提供依据，倡导未来算法设计中考虑可持续性。

Abstract: The growing computational demands of deep reinforcement learning (DRL) have
raised concerns about the environmental and economic costs of training
large-scale models. While algorithmic efficiency in terms of learning
performance has been extensively studied, the energy requirements, greenhouse
gas emissions, and monetary costs of DRL algorithms remain largely unexplored.
In this work, we present a systematic benchmarking study of the energy
consumption of seven state-of-the-art DRL algorithms, namely DQN, TRPO, A2C,
ARS, PPO, RecurrentPPO, and QR-DQN, implemented using Stable Baselines. Each
algorithm was trained for one million steps each on ten Atari 2600 games, and
power consumption was measured in real-time to estimate total energy usage,
CO2-Equivalent emissions, and electricity cost based on the U.S. national
average electricity price. Our results reveal substantial variation in energy
efficiency and training cost across algorithms, with some achieving comparable
performance while consuming up to 24% less energy (ARS vs. DQN), emitting
nearly 68% less CO2, and incurring almost 68% lower monetary cost (QR-DQN vs.
RecurrentPPO) than less efficient counterparts. We further analyze the
trade-offs between learning performance, training time, energy use, and
financial cost, highlighting cases where algorithmic choices can mitigate
environmental and economic impact without sacrificing learning performance.
This study provides actionable insights for developing energy-aware and
cost-efficient DRL practices and establishes a foundation for incorporating
sustainability considerations into future algorithmic design and evaluation.

</details>


### [47] [An Efficient Subspace Algorithm for Federated Learning on Heterogeneous Data](https://arxiv.org/abs/2509.05213)
*Jiaojiao Zhang,Yuqi Xu,Kun Yuan*

Main category: cs.LG

TL;DR: 提出了FedSub算法，通过子空间投影降低联邦学习在异构数据上的通信、计算和内存成本，同时缓解客户端漂移问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在大规模深度神经网络中因数据异构性导致的客户端漂移问题，以及高通信、计算和内存成本。

Method: 利用子空间投影确保客户端更新在低维子空间内，并结合低维对偶变量缓解客户端漂移。

Result: 收敛分析揭示了步长和子空间投影矩阵的影响，实验证明了算法的效率。

Conclusion: FedSub是一个高效的联邦学习算法，适用于异构数据场景。

Abstract: This work addresses the key challenges of applying federated learning to
large-scale deep neural networks, particularly the issue of client drift due to
data heterogeneity across clients and the high costs of communication,
computation, and memory. We propose FedSub, an efficient subspace algorithm for
federated learning on heterogeneous data. Specifically, FedSub utilizes
subspace projection to guarantee local updates of each client within
low-dimensional subspaces, thereby reducing communication, computation, and
memory costs. Additionally, it incorporates low-dimensional dual variables to
mitigate client drift. We provide convergence analysis that reveals the impact
of key factors such as step size and subspace projection matrices on
convergence. Experimental results demonstrate its efficiency.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [48] [Memristor-Based Neural Network Accelerators for Space Applications: Enhancing Performance with Temporal Averaging and SIRENs](https://arxiv.org/abs/2509.04506)
*Zacharia A. Rudge,Dominik Dold,Moritz Fieback,Dario Izzo,Said Hamdioui*

Main category: eess.SY

TL;DR: 该论文探讨了基于忆阻器的神经网络在航天器上的应用潜力，通过技术改进实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 忆阻器因其高能效和抗辐射特性，适合用于航天器上的人工智能加速器，但其非理想性导致性能下降，亟需解决方案。

Method: 采用位切片、NN层的时间平均和周期性激活函数等技术，对忆阻器神经网络进行优化。

Result: 初始性能从0.07和0.3分别提升至0.01和0.007，接近现有技术的最优水平（0.003-0.005和0.003）。

Conclusion: 忆阻器在航天应用中有巨大潜力，未来的技术和神经网络改进将进一步缩小性能差距。

Abstract: Memristors are an emerging technology that enables artificial intelligence
(AI) accelerators with high energy efficiency and radiation robustness --
properties that are vital for the deployment of AI on-board spacecraft.
However, space applications require reliable and precise computations, while
memristive devices suffer from non-idealities, such as device variability,
conductance drifts, and device faults. Thus, porting neural networks (NNs) to
memristive devices often faces the challenge of severe performance degradation.
In this work, we show in simulations that memristor-based NNs achieve
competitive performance levels on on-board tasks, such as navigation \& control
and geodesy of asteroids. Through bit-slicing, temporal averaging of NN layers,
and periodic activation functions, we improve initial results from around
$0.07$ to $0.01$ and $0.3$ to $0.007$ for both tasks using RRAM devices, coming
close to state-of-the-art levels ($0.003-0.005$ and $0.003$, respectively). Our
results demonstrate the potential of memristors for on-board space
applications, and we are convinced that future technology and NN improvements
will further close the performance gap to fully unlock the benefits of
memristors.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [49] [Quantum Fourier Transform Based Denoising: Unitary Filtering for Enhanced Speech Clarity](https://arxiv.org/abs/2509.04851)
*Rajeshwar Tripathi,Sahil Tomar,Sandeep Kumar,Monika Aggarwal*

Main category: cs.SD

TL;DR: 该论文提出了一种量子启发的去噪框架，将量子傅里叶变换（QFT）融入传统音频增强流程中，相比传统的FFT方法，QFT提供了全局相位一致性和能量保持效果，从而提升语音和噪声的区分能力。实验表明，QFT在去噪性能上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于FFT的去噪方法在处理低信噪比和非平稳噪声时的局限性，探索量子信号处理技术在音频增强中的应用潜力。

Method: 将Wiener和谱减法中的FFT替换为QFT操作符，保持超参数一致以公平比较。在不同信噪比条件下对干净语音、合成音和噪声混合进行实验。

Result: 实验结果显示，QFT去噪在信噪比上提升最高达15 dB，且减少了伪影生成，尤其在低信噪比和非平稳噪声场景下表现稳健，且无需额外计算开销。

Conclusion: QFT为量子增强的语音处理提供了一种可扩展的解决方案，展示了量子信号处理在音频增强中的潜力。

Abstract: This paper introduces a quantum-inspired denoising framework that integrates
the Quantum Fourier Transform (QFT) into classical audio enhancement pipelines.
Unlike conventional Fast Fourier Transform (FFT) based methods, QFT provides a
unitary transformation with global phase coherence and energy preservation,
enabling improved discrimination between speech and noise. The proposed
approach replaces FFT in Wiener and spectral subtraction filters with a QFT
operator, ensuring consistent hyperparameter settings for fair comparison.
Experiments on clean speech, synthetic tones, and noisy mixtures across diverse
signal to noise ratio (SNR) conditions, demonstrate statistically significant
gains in SNR, with up to 15 dB improvement and reduced artifact generation.
Results confirm that QFT based denoising offers robustness under low SNR and
nonstationary noise scenarios without additional computational overhead,
highlighting its potential as a scalable pathway toward quantum-enhanced speech
processing.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [50] [Subvarieties of pointed Abelian l-groups](https://arxiv.org/abs/2509.05044)
*Filip Jankovec*

Main category: math.LO

TL;DR: 该论文完全分类了由链生成的指向阿贝尔格序群的子簇和子拟簇，通过两种方法：格群论分析和MV代数的Γ函子连接，提供了结构的全面描述。


<details>
  <summary>Details</summary>
Motivation: 研究指向阿贝尔格序群的子簇和子拟簇结构，通过不同方法提供完整的分类，并推广至MV代数领域。

Method: 1. 使用格群论方法分析词典积和根基，识别子簇格中的不可约成员；2. 通过MV代数的Γ函子连接模型理论，推广分类结果。

Result: 获得了子簇格的完整描述，并推广了Komori和Gispert的先驱性分类结果。

Conclusion: 论文通过两种互补方法，为指向阿贝尔格序群的结构提供了全面的分类和理论支持。

Abstract: This paper provides a complete classification of the subvarieties and
subquasivarieties of pointed Abelian lattice-ordered groups ($\ell$-groups)
that are generated by chains. We present two complementary approaches to
achieve this classification.
  First, using purely $\ell$-group-theoretic methods, we analyze the structure
of lexicographic products and radicals to identify all join-irreducible members
of the lattice of subvarieties of positively pointed $\ell$-groups. We provide
a novel equational basis for each of these subvarieties, leading to a complete
description of the entire subvariety lattice. As a direct application, our
$\ell$-group-theoretic classification yields an alternative, self-contained
proof of Komori's celebrated classification of subvarieties of MV-algebras.
  Second, we explore the connection to MV-algebras via Mundici's $\Gamma$
functor. We prove that this functor preserves universal classes, a result of
independent model-theoretic interest. This allows us to lift the classification
of universal classes of MV-chains, due to Gispert, to a complete classification
of universal classes of totally ordered pointed Abelian $\ell$-groups. As a
direct consequence, we obtain a full description of the corresponding lattice
of subquasivarieties. These results offer a comprehensive structural
understanding of one of the most fundamental classes of ordered algebraic
structures.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [51] [Partializations of Markov categories](https://arxiv.org/abs/2509.05094)
*Areeb Shah Mohammed*

Main category: math.CT

TL;DR: 该论文构建了一种从特定类型的马尔可夫类别（称为可部分化马尔可夫类别）推导出的部分核的CD类别，扩展了传统的部分态射模型（如p-类别、支配类别等）到非确定性/非笛卡尔设置。


<details>
  <summary>Details</summary>
Motivation: 研究如何在非确定性环境下推广传统的部分态射模型，并保留概率论中的重要性质（如正性、条件性、Kolmogorov积等）。

Method: 利用可部分化马尔可夫类别构造部分核的CD类别，提出适用于部分映射的Kolmogorov积概念，并讨论概率幺半群的偏代数。

Result: 成功构建了部分核的CD类别，并验证了其在概率论中的关键性质。主要应用包括标准Borel空间与马尔可夫核类别的部分化。

Conclusion: 该研究为非确定性环境下的部分态射提供了新的理论框架，扩展了概率论中的部分映射应用场景。

Abstract: The present work develops a construction of a CD category of partial kernels
from a particular type of Markov category called a partializable Markov
category. These are a generalization of earlier models of categories of partial
morphisms such as p-categories, dominical categories, restriction categories,
etc. to a non-deterministic/non-cartesian setting. Here all morphisms are
quasi-total, with a natural poset enrichment corresponding to one morphism
being a restriction of the other. Furthermore, various properties important to
categorical probability are preserved, such as positivity, representability,
conditionals, Kolmogorov products, and splittings of idempotents. We
additionally discuss an alternative notion of Kolmogorov product suitable for
partial maps, as well as partial algebras for probability monads.
  The primary example is that of the partialization of the category of standard
Borel spaces and Markov kernels. Other examples include variants where the
distributions are finitely supported, or where one considers multivalued maps
instead.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [52] [L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning](https://arxiv.org/abs/2509.04884)
*Raul Singh,Nicolo Brunello,Vincenzo Scotti,Mark James Carman*

Main category: cs.CL

TL;DR: L1RA是一种新技术，通过动态调整低秩适配器的秩分配来优化资源利用，提高LLM微调效率。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs微调的高计算需求在资源受限时存在挑战，L1RA旨在优化资源分配以减少开销。

Method: 使用L1正则化动态修剪和重新分配低秩适配器的秩，确保在给定秩预算下优化性能。

Result: 实验表明L1RA在性能相当或更好时，计算开销更低，并提供模型改进的诊断信息。

Conclusion: L1RA是一种高效的LLM微调技术，特别适用于资源受限场景。

Abstract: The ability of Large Language Models (LLMs) to solve complex tasks has made
them crucial in the development of AI-based applications. However, the high
computational requirements to fine-tune these LLMs on downstream tasks pose
significant challenges, particularly when resources are limited. In response to
this challenge, we introduce L1RA, a novel technique aimed at dynamically
distributing the rank of low-rank adapters during fine-tuning using LoRA. Given
a rank budget (i.e., total sum of adapters rank), L1RA leverages L1
regularisation to prune redundant ranks and redistribute them across adapters,
thereby optimising resource utilisation. Through a series of comprehensive
experiments, we empirically demonstrate that L1RA maintains comparable or even
reduced computational overhead compared to other LoRA variants, including the
vanilla approach, while achieving same or better performances. Moreover, the
post-training analysis of rank distribution unveiled insights into the specific
model components requiring the most adaptation to align with the task
objective: the feed-forward layers and the attention output projection. These
results highlight the efficacy of L1RA in not only enhancing the efficiency of
LLM fine-tuning, but also in providing valuable diagnostic information for
model refinement and customisation. In conclusion, L1RA stands as a promising
technique for advancing the performance and interpretability of LLM adaptation,
particularly in scenarios where computational resources are constrained.

</details>


### [53] [Evaluating NL2SQL via SQL2NL](https://arxiv.org/abs/2509.04657)
*Mohammadtaher Safarzadeh,Afshin Oroojlooyjadid,Dan Roth*

Main category: cs.CL

TL;DR: 本文提出了一种基于SQL2NL的模式对齐复述框架，用于评估NL2SQL模型在语言多样性下的稳健性，发现现有模型表现脆弱，性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能系统或可控地评估NL2SQL模型在语言多样性下的泛化能力，因此需要开发新的评估方法。

Method: 提出模式对齐的复述框架，利用SQL2NL自动生成语义相同但词汇多样的查询，以隔离评估语言变异的影响。

Result: 实验显示，现有模型在语言变异下性能显著下降（如LLaMa3.3-70B下降10.23%），且与查询复杂度、数据集和领域相关。

Conclusion: 强调需要开发明确衡量语言泛化的评估框架，以确保模型在真实场景中的可靠性能。

Abstract: Robust evaluation in the presence of linguistic variation is key to
understanding the generalization capabilities of Natural Language to SQL
(NL2SQL) models, yet existing benchmarks rarely address this factor in a
systematic or controlled manner. We propose a novel schema-aligned paraphrasing
framework that leverages SQL-to-NL (SQL2NL) to automatically generate
semantically equivalent, lexically diverse queries while maintaining alignment
with the original schema and intent. This enables the first targeted evaluation
of NL2SQL robustness to linguistic variation in isolation-distinct from prior
work that primarily investigates ambiguity or schema perturbations. Our
analysis reveals that state-of-the-art models are far more brittle than
standard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop
in execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries,
while LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to
42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We
also find that robustness degradation varies significantly with query
complexity, dataset, and domain -- highlighting the need for evaluation
frameworks that explicitly measure linguistic generalization to ensure reliable
performance in real-world settings.

</details>


### [54] [Combine Virtual Reality and Machine-Learning to Identify the Presence of Dyslexia: A Cross-Linguistic Approach](https://arxiv.org/abs/2509.04510)
*Michele Materazzini,Gianluca Morciano,Jose Manuel Alcalde-Llergo,Enrique Yeguas-Bolivar,Giuseppe Calabro,Andrea Zingoni,Juri Taborri*

Main category: cs.CL

TL;DR: 该研究探讨了如何利用虚拟现实(VR)和人工智能(AI)预测意大利和西班牙大学生是否患有阅读障碍，并通过机器学习(ML)算法分析VR数据。


<details>
  <summary>Details</summary>
Motivation: 探讨VR和AI技术在阅读障碍诊断中的潜在应用，以提供一种新型的支持工具。

Method: 参与者完成基于VR的阅读表现和自尊测试，通过t检验和Mann Whitney检验进行初步分析，随后训练监督ML模型进行分类。

Result: ML模型在意大利、西班牙和合并组中的分类准确率分别为87.5%、66.6%和75.0%，显示VR和ML在捕捉任务完成速度差异方面有效。

Conclusion: VR和ML可作为阅读障碍评估的支持工具，但语言因素可能影响分类准确性。

Abstract: This study explores the use of virtual reality (VR) and artificial
intelligence (AI) to predict the presence of dyslexia in Italian and Spanish
university students. In particular, the research investigates whether
VR-derived data from Silent Reading (SR) tests and self-esteem assessments can
differentiate between students that are affected by dyslexia and students that
are not, employing machine learning (ML) algorithms. Participants completed
VR-based tasks measuring reading performance and self-esteem. A preliminary
statistical analysis (t tests and Mann Whitney tests) on these data was
performed, to compare the obtained scores between individuals with and without
dyslexia, revealing significant differences in completion time for the SR test,
but not in accuracy, nor in self esteem. Then, supervised ML models were
trained and tested, demonstrating an ability to classify the presence/absence
of dyslexia with an accuracy of 87.5 per cent for Italian, 66.6 per cent for
Spanish, and 75.0 per cent for the pooled group. These findings suggest that VR
and ML can effectively be used as supporting tools for assessing dyslexia,
particularly by capturing differences in task completion speed, but
language-specific factors may influence classification accuracy.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [55] [UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis](https://arxiv.org/abs/2509.04624)
*Ali Khanpour,Tianyi Wang,Afra Vahidi-Shams,Wim Ectors,Farzam Nakhaie,Amirhossein Taheri,Christian Claudel*

Main category: cs.CV

TL;DR: 提出一种基于无人机的交通监控系统，实现高效车辆检测、分类和行为分析，解决传统监控的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统交通监控系统覆盖范围有限，适应性差，无法满足现代城市交通需求。

Method: 利用多尺度多角度模板匹配、卡尔曼滤波和单应性校准处理无人机航拍视频。

Result: 系统检测精度达91.8%，F1分数90.5%，并能分类车辆及检测交通违规行为。

Conclusion: 系统具有高扩展性和准确性，适合作为智能城市的独立交通监控方案。

Abstract: Traffic congestion and violations pose significant challenges for urban
mobility and road safety. Traditional traffic monitoring systems, such as fixed
cameras and sensor-based methods, are often constrained by limited coverage,
low adaptability, and poor scalability. To address these challenges, this paper
introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance
system capable of accurate vehicle detection, classification, tracking, and
behavioral analysis in real-world, unconstrained urban environments. The system
leverages multi-scale and multi-angle template matching, Kalman filtering, and
homography-based calibration to process aerial video data collected from
altitudes of approximately 200 meters. A case study in urban area demonstrates
robust performance, achieving a detection precision of 91.8%, an F1-score of
90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.
Beyond precise detection, the system classifies five vehicle types and
automatically detects critical traffic violations, including unsafe lane
changes, illegal double parking, and crosswalk obstructions, through the fusion
of geofencing, motion filtering, and trajectory deviation analysis. The
integrated analytics module supports origin-destination tracking, vehicle count
visualization, inter-class correlation analysis, and heatmap-based congestion
modeling. Additionally, the system enables entry-exit trajectory profiling,
vehicle density estimation across road segments, and movement direction
logging, supporting comprehensive multi-scale urban mobility analytics.
Experimental results confirms the system's scalability, accuracy, and practical
relevance, highlighting its potential as an enforcement-aware,
infrastructure-independent traffic monitoring solution for next-generation
smart cities.

</details>


### [56] [Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper](https://arxiv.org/abs/2509.04957)
*Gehui Chen,Guan'an Wang,Xiaowen Huang,Jitao Sang*

Main category: cs.CV

TL;DR: MFM-Mapper通过融合双视觉编码器特征和使用GPT-2替代线性映射，显著提升了视频到音频生成的语义和时间一致性，同时训练效率更高，仅需16%的训练资源。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频生成模型训练资源消耗大，且依赖预训练基础模型。MFM-Mapper旨在通过改进映射机制，提升特征对齐和训练效率。

Method: 提出MFM-Mapper，融合双视觉编码器特征，使用GPT-2替代线性映射，增强跨模态特征对齐。

Result: MFM-Mapper在语义和时间一致性上表现更好，训练效率显著提升，仅需16%的训练资源。

Conclusion: MFM-Mapper在视频到音频生成任务中实现了高效且性能优越的特征映射。

Abstract: Recent Video-to-Audio (V2A) generation relies on extracting semantic and
temporal features from video to condition generative models. Training these
models from scratch is resource intensive. Consequently, leveraging foundation
models (FMs) has gained traction due to their cross-modal knowledge transfer
and generalization capabilities. One prior work has explored fine-tuning a
lightweight mapper network to connect a pre-trained visual encoder with a
text-to-audio generation model for V2A. Inspired by this, we introduce the
Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper
approach, MFM-Mapper benefits from richer semantic and temporal information by
fusing features from dual visual encoders. Furthermore, by replacing a linear
mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels
between cross-modal features mapping and autoregressive translation tasks. Our
MFM-Mapper exhibits remarkable training efficiency. It achieves better
performance in semantic and temporal consistency with fewer training consuming,
requiring only 16\% of the training scale compared to previous mapper-based
work, yet achieves competitive performance with models trained on a much larger
scale.

</details>


### [57] [SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models](https://arxiv.org/abs/2509.04889)
*Dominik Pegler,David Steyrl,Mengfan Zhang,Alexander Karner,Jozsef Arato,Frank Scharnowski,Filip Melinscak*

Main category: cs.CV

TL;DR: 该研究探讨预训练计算机视觉模型是否能准确预测蜘蛛相关图像的恐惧等级，采用三种模型进行迁移学习，预测313张标准化图像的人类恐惧评分，平均绝对误差为10.1-11.0。研究发现数据集大小对性能有显著影响，模型的预测基于蜘蛛相关特征，表明可解释模型在情感治疗技术中的潜力。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉的进步为临床应用（如计算机化暴露疗法）提供了新机会。本研究旨在验证预训练计算机视觉模型是否能准确预测恐惧等级，为开发适应性情感治疗系统奠定基础。

Method: 研究采用三种预训练模型，通过迁移学习预测蜘蛛相关图像的恐惧评分（0-100）。使用313张标准化图像进行交叉验证，并分析学习曲线和模型可解释性。

Result: 模型平均绝对误差为10.1-11.0。学习曲线显示数据集大小显著影响性能，但增加数据量后无显著提升。可解释性分析表明模型依赖蜘蛛相关特征。错误分析发现某些视觉条件（如远视图和人工蜘蛛）误差较高。

Conclusion: 研究证明了可解释计算机视觉模型在预测恐惧等级中的潜力，强调了模型可解释性和足够数据集大小对开发有效情感治疗技术的重要性。

Abstract: Advances in computer vision have opened new avenues for clinical
applications, particularly in computerized exposure therapy where visual
stimuli can be dynamically adjusted based on patient responses. As a critical
step toward such adaptive systems, we investigated whether pretrained computer
vision models can accurately predict fear levels from spider-related images. We
adapted three diverse models using transfer learning to predict human fear
ratings (on a 0-100 scale) from a standardized dataset of 313 images. The
models were evaluated using cross-validation, achieving an average mean
absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis
revealed that reducing the dataset size significantly harmed performance,
though further increases yielded no substantial gains. Explainability
assessments showed the models' predictions were based on spider-related
features. A category-wise error analysis further identified visual conditions
associated with higher errors (e.g., distant views and artificial/painted
spiders). These findings demonstrate the potential of explainable computer
vision models in predicting fear ratings, highlighting the importance of both
model explainability and a sufficient dataset size for developing effective
emotion-aware therapeutic technologies.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [58] [HEEPidermis: a versatile SoC for BioZ recording](https://arxiv.org/abs/2509.04528)
*Juan Sapriza,Beatrice Grassano,Alessio Naclerio,Filippo Quadri,Tommaso Terzano,David Mallasén,Davide Schiavone,Robin Leplae,Jérémie Moullet,Alexandre Levisse,Christoph Müller,Mariagrazia Graziano,Matías Miguez,David Atienza*

Main category: physics.ins-det

TL;DR: HEEPidermis是一个集成了组织阻抗测量所有关键模块的SoC，包括电流DAC、VCO-based ADC和RISC-V CPU，支持闭环操作和高效长期记录。


<details>
  <summary>Details</summary>
Motivation: 现有组织阻抗测量设备体积大或功能单一，限制了研究和实验的可能性，需要更灵活的解决方案。

Method: 开发了一个集成了模拟前端和数字后端的SoC，采用开源设计和TSMC 65 nm工艺制造。

Result: HEEPidermis实现了高效、灵活的阻抗测量，支持闭环操作和长期记录。

Conclusion: HEEPidermis为生物阻抗研究提供了小型化、高效的硬件平台，推动了该领域的发展。

Abstract: Biological impedance (BioZ) is an information-packed modality that allows for
non-invasive monitoring of health and emotional state. Currently, most research
involving tissue impedance is based on bulky or fixed-purpose hardware, which
limits the scope of research and the possibilities of experiments. In this
work, we present HEEPidermis: a System-on-Chip (SoC) which integrates all the
blocks needed for tissue impedance measurement, including two 8-bit,
arbitrary-signal current DACs, two VCO-based ADCs, and a RISC-V CPU to enable
on-chip feature extraction for closed-loop operation. An event-based
sub-sampler improves storage and energy efficiency for long-term recording. In
addition to the versatile SoC, the digital back-end and behavioral models of
the analog front-end are open-source, allowing fast system-level simulations or
repurposing. The SoC was taped out on TSMC 65 nm LP process.

</details>
