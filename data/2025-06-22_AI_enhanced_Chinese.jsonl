{"id": "2506.14866", "pdf": "https://arxiv.org/pdf/2506.14866", "abs": "https://arxiv.org/abs/2506.14866", "authors": ["Thomas Kuntz", "Agatha Duzan", "Hao Zhao", "Francesco Croce", "Zico Kolter", "Nicolas Flammarion", "Maksym Andriushchenko"], "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86OS-Harm\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u8986\u76d6\u4e09\u7c7b\u5371\u5bb3\u884c\u4e3a\uff0c\u5e76\u6d4b\u8bd5\u591a\u79cd\u524d\u6cbf\u6a21\u578b\u7684\u5b89\u5168\u6027\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08\u901a\u8fc7\u622a\u56fe\u6216\u65e0\u969c\u788d\u6811\u4ea4\u4e92\uff09\u5b89\u5168\u6027\u88ab\u5ffd\u89c6\uff0c\u4f46\u5176\u6f5c\u5728\u5371\u5bb3\u884c\u4e3a\u8bc4\u4f30\u5bf9\u5e7f\u6cdb\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8eOSWorld\u73af\u5883\u6784\u5efaOS-Harm\u57fa\u51c6\uff0c\u8bbe\u8ba1150\u4e2a\u4efb\u52a1\u6d4b\u8bd5\u4e09\u7c7b\u5371\u5bb3\uff08\u6ee5\u7528\u3001\u63d0\u793a\u6ce8\u5165\u3001\u6a21\u578b\u8bef\u884c\u4e3a\uff09\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u5316\u8bc4\u5224\u65b9\u6cd5\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\u6240\u6709\u524d\u6cbf\u6a21\u578b\uff08\u5982o4-mini\u3001Claude 3.7\u7b49\uff09\u6613\u88ab\u6ee5\u7528\u3001\u6613\u53d7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4e14\u5076\u5c14\u6267\u884c\u4e0d\u5b89\u5168\u884c\u4e3a\u3002\u81ea\u52a8\u5316\u8bc4\u5224\u4e0e\u4eba\u5de5\u6807\u6ce8\u4e00\u81f4\u6027\u9ad8\uff08F1\u5206\u65700.76/0.79\uff09\u3002", "conclusion": "OS-Harm\u586b\u8865\u4e86\u8ba1\u7b97\u673a\u4ee3\u7406\u5b89\u5168\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2506.15084", "pdf": "https://arxiv.org/pdf/2506.15084", "abs": "https://arxiv.org/abs/2506.15084", "authors": ["Weiqi Lu", "Yongqiang Tian", "Xiaohan Zhong", "Haoyang Ma", "Zhenyang Xu", "Shing-Chi Cheung", "Chengnian Sun"], "title": "An Empirical Study of Bugs in Data Visualization Libraries", "categories": ["cs.SE", "cs.CV", "cs.HC"], "comment": "Proc. ACM Softw. Eng. 2, FSE", "summary": "Data visualization (DataViz) libraries play a crucial role in presentation,\ndata analysis, and application development, underscoring the importance of\ntheir accuracy in transforming data into visual representations. Incorrect\nvisualizations can adversely impact user experience, distort information\nconveyance, and influence user perception and decision-making processes. Visual\nbugs in these libraries can be particularly insidious as they may not cause\nobvious errors like crashes, but instead mislead users of the underlying data\ngraphically, resulting in wrong decision making. Consequently, a good\nunderstanding of the unique characteristics of bugs in DataViz libraries is\nessential for researchers and developers to detect and fix bugs in DataViz\nlibraries.\n  This study presents the first comprehensive analysis of bugs in DataViz\nlibraries, examining 564 bugs collected from five widely-used libraries. Our\nstudy systematically analyzes their symptoms and root causes, and provides a\ndetailed taxonomy. We found that incorrect/inaccurate plots are pervasive in\nDataViz libraries and incorrect graphic computation is the major root cause,\nwhich necessitates further automated testing methods for DataViz libraries.\nMoreover, we identified eight key steps to trigger such bugs and two test\noracles specific to DataViz libraries, which may inspire future research in\ndesigning effective automated testing techniques. Furthermore, with the recent\nadvancements in Vision Language Models (VLMs), we explored the feasibility of\napplying these models to detect incorrect/inaccurate plots. The results show\nthat the effectiveness of VLMs in bug detection varies from 29% to 57%,\ndepending on the prompts, and adding more information in prompts does not\nnecessarily increase the effectiveness. More findings can be found in our\nmanuscript.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u5206\u6790\u4e86\u6570\u636e\u53ef\u89c6\u5316\u5e93\u4e2d\u7684\u9519\u8bef\uff0c\u7814\u7a76\u4e86564\u4e2a\u6765\u81ea\u4e94\u4e2a\u5e38\u7528\u5e93\u7684bug\uff0c\u7cfb\u7edf\u6027\u5206\u6790\u4e86\u5176\u75c7\u72b6\u548c\u6839\u6e90\uff0c\u5e76\u63d0\u51fa\u5206\u7c7b\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u9519\u8bef\u7684\u56fe\u5f62\u8ba1\u7b97\u662f\u4e3b\u8981\u6839\u6e90\uff0c\u4e14\u63d0\u51fa\u4e86\u516b\u4e2a\u89e6\u53d1\u6b65\u9aa4\u548c\u4e24\u4e2a\u6d4b\u8bd5\u9884\u8a00\u3002\u6b64\u5916\uff0c\u63a2\u7d22\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u68c0\u6d4b\u9519\u8bef\u56fe\u5f62\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u5176\u6548\u679c\u572829%\u81f357%\u4e4b\u95f4\u3002", "motivation": "\u6570\u636e\u53ef\u89c6\u5316\u5e93\uff08DataViz\uff09\u7684\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u9519\u8bef\u7684\u53ef\u89c6\u5316\u53ef\u80fd\u8bef\u5bfc\u7528\u6237\u51b3\u7b56\u3002\u7136\u800c\uff0c\u5bf9\u5176bug\u7279\u6027\u7684\u4e86\u89e3\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u7814\u7a76\u6536\u96c6\u5e76\u5206\u6790\u4e86564\u4e2a\u6765\u81ea\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684DataViz\u5e93\u7684bug\uff0c\u7cfb\u7edf\u6027\u5206\u6790\u4e86\u75c7\u72b6\u3001\u6839\u6e90\u5e76\u63d0\u51fa\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u63a2\u7d22\u4e86VLM\u5728\u68c0\u6d4b\u9519\u8bef\u56fe\u5f62\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9519\u8bef\u7684\u56fe\u5f62\u8ba1\u7b97\u662f\u4e3b\u8981\u6839\u6e90\uff0c\u63d0\u51fa\u4e86\u516b\u4e2a\u89e6\u53d1\u6b65\u9aa4\u548c\u4e24\u4e2a\u6d4b\u8bd5\u9884\u8a00\u3002VLM\u7684\u68c0\u6d4b\u6548\u679c\u572829%\u81f357%\u4e4b\u95f4\uff0c\u63d0\u793a\u4fe1\u606f\u8d8a\u591a\u4e0d\u4e00\u5b9a\u6548\u679c\u8d8a\u597d\u3002", "conclusion": "\u7814\u7a76\u4e3aDataViz\u5e93\u7684bug\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86VLM\u5728\u8fd9\u4e00\u9886\u57df\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2506.15088", "pdf": "https://arxiv.org/pdf/2506.15088", "abs": "https://arxiv.org/abs/2506.15088", "authors": ["Miao Miao"], "title": "Program Feature-based Fuzzing Benchmarking", "categories": ["cs.SE"], "comment": null, "summary": "Fuzzing is a powerful software testing technique renowned for its\neffectiveness in identifying software vulnerabilities. Traditional fuzzing\nevaluations typically focus on overall fuzzer performance across a set of\ntarget programs, yet few benchmarks consider how fine-grained program features\ninfluence fuzzing effectiveness. To bridge this gap, we introduce a novel\nbenchmark designed to generate programs with configurable, fine-grained program\nfeatures to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing\nstudies, extracting 7 program features related to control-flow and data-flow\nthat can impact fuzzer performance. Using these features, we generated a\nbenchmark consisting of 153 programs controlled by 10 fine-grained configurable\nparameters. We evaluated 11 popular fuzzers using this benchmark. The results\nindicate that fuzzer performance varies significantly based on the program\nfeatures and their strengths, highlighting the importance of incorporating\nprogram characteristics into fuzzing evaluations.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u53ef\u914d\u7f6e\u7ec6\u7c92\u5ea6\u7a0b\u5e8f\u7279\u5f81\u7684\u7a0b\u5e8f\uff0c\u4ee5\u6539\u8fdb\u6a21\u7cca\u6d4b\u8bd5\u7684\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u6a21\u7cca\u6d4b\u8bd5\u8bc4\u4f30\u901a\u5e38\u5173\u6ce8\u6574\u4f53\u6027\u80fd\uff0c\u800c\u5ffd\u89c6\u4e86\u7ec6\u7c92\u5ea6\u7a0b\u5e8f\u7279\u5f81\u5bf9\u6a21\u7cca\u6d4b\u8bd5\u6548\u679c\u7684\u5f71\u54cd\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u679025\u7bc7\u7070\u8272\u6a21\u7cca\u6d4b\u8bd5\u7814\u7a76\uff0c\u63d0\u53d67\u4e2a\u4e0e\u63a7\u5236\u548c\u6570\u636e\u6d41\u76f8\u5173\u7684\u7a0b\u5e8f\u7279\u5f81\uff0c\u751f\u6210\u5305\u542b153\u4e2a\u7a0b\u5e8f\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5e76\u7528\u5176\u8bc4\u4f3011\u4e2a\u6d41\u884c\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u3002", "result": "\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u7684\u6027\u80fd\u56e0\u7a0b\u5e8f\u7279\u5f81\u53ca\u5176\u5f3a\u5ea6\u800c\u5f02\uff0c\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30\u4e2d\u8003\u8651\u7a0b\u5e8f\u7279\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u7a0b\u5e8f\u7279\u5f81\u5bf9\u6a21\u7cca\u6d4b\u8bd5\u6548\u679c\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u6a21\u7cca\u6d4b\u8bd5\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.15098", "pdf": "https://arxiv.org/pdf/2506.15098", "abs": "https://arxiv.org/abs/2506.15098", "authors": ["Haosheng Zuo", "Feifei Niu", "Chuanyi Li"], "title": "Enhancement Report Approval Prediction: A Comparative Study of Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Enhancement reports (ERs) serve as a critical communication channel between\nusers and developers, capturing valuable suggestions for software improvement.\nHowever, manually processing these reports is resource-intensive, leading to\ndelays and potential loss of valuable insights. To address this challenge,\nenhancement report approval prediction (ERAP) has emerged as a research focus,\nleveraging machine learning techniques to automate decision-making. While\ntraditional approaches have employed feature-based classifiers and deep\nlearning models, recent advancements in large language models (LLM) present new\nopportunities for enhancing prediction accuracy. This study systematically\nevaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and\nXLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1\n8B Instruct and DeepSeek-V3 for decoder models) against traditional methods\n(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)\nIncorporating creator profiles increases unfine-tuned decoder-only models'\noverall accuracy by 10.8 percent though it may introduce bias; (2) LoRA\nfine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79\npercent accuracy and significantly enhancing recall for approved reports (76.1\npercent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5\npercent under strict chronological evaluation and effectively addressing class\nimbalance issues. These findings establish LLM as a superior solution for ERAP,\ndemonstrating their potential to streamline software maintenance workflows and\nimprove decision-making in real-world development environments. We also\ninvestigated and summarized the ER cases where the large models underperformed,\nproviding valuable directions for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u9884\u6d4b\u8f6f\u4ef6\u6539\u8fdb\u62a5\u544a\uff08ERs\uff09\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0LLM\u5728\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7ed3\u5408\u521b\u4f5c\u8005\u6863\u6848\u548cLoRA\u5fae\u8c03\u540e\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u5904\u7406\u8f6f\u4ef6\u6539\u8fdb\u62a5\u544a\u6548\u7387\u4f4e\u4e14\u6613\u4e22\u5931\u6709\u4ef7\u503c\u7684\u4fe1\u606f\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7LLM\u6539\u8fdb\u81ea\u52a8\u51b3\u7b56\u7684\u51c6\u786e\u7387\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e8618\u79cdLLM\u53d8\u4f53\uff08\u5305\u62ec\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u6a21\u578b\uff09\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\uff08CNN/LSTM-BERT/GloVe\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u63a2\u7a76\u4e86\u521b\u4f5c\u8005\u6863\u6848\u548cLoRA\u5fae\u8c03\u7684\u6548\u679c\u3002", "result": "LLM\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cLoRA\u5fae\u8c03\u7684Llama 3.1 8B Instruct\u8fbe\u523079%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u6539\u5584\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "LLM\u4e3aERAP\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u4e0d\u8db3\u573a\u666f\u3002"}}
{"id": "2506.15183", "pdf": "https://arxiv.org/pdf/2506.15183", "abs": "https://arxiv.org/abs/2506.15183", "authors": ["Xingyu Chen", "Xinmin Fang", "Shuting Zhang", "Xinyu Zhang", "Liang He", "Zhengxiong Li"], "title": "You Only Render Once: Enhancing Energy and Computation Efficiency of Mobile Virtual Reality", "categories": ["cs.GR"], "comment": null, "summary": "Mobile Virtual Reality (VR) is essential to achieving convenient and\nimmersive human-computer interaction and realizing emerging applications such\nas Metaverse. However, existing VR technologies require two separate renderings\nof binocular images, causing a significant bottleneck for mobile devices with\nlimited computing capability and power supply. This paper proposes an approach\nto rendering optimization for mobile VR called EffVR. By utilizing the\nper-pixel attribute, EffVR can generate binocular VR images from the monocular\nimage through genuinely one rendering, saving half the computation over\nconventional approaches. Our evaluation indicates that, compared with the\nstate-of-art, EffVRcan save 27% power consumption on average while achieving\nhigh binocular image quality (0.9679 SSIM and 34.09 PSNR) in mobile VR\napplications. Additionally, EffVR can increase the frame rate by 115.2%. These\nresults corroborate EffVRsuperior computation/energy-saving performance, paving\nthe road to a sustainable mobile VR. The source code, demo video, android app,\nand more are released anonymously at https://yoro-vr.github.io/", "AI": {"tldr": "EffVR\u901a\u8fc7\u5355\u76ee\u56fe\u50cf\u751f\u6210\u53cc\u76eeVR\u56fe\u50cf\uff0c\u8282\u770150%\u8ba1\u7b97\u91cf\uff0c\u5e73\u5747\u964d\u4f4e27%\u529f\u8017\u5e76\u63d0\u9ad8\u5e27\u7387115.2%\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8VR\u4e2d\u53cc\u76ee\u56fe\u50cf\u5206\u79bb\u6e32\u67d3\u5bfc\u81f4\u7684\u6027\u80fd\u548c\u80fd\u8017\u74f6\u9888\u95ee\u9898\u3002", "method": "\u5229\u7528\u9010\u50cf\u7d20\u5c5e\u6027\u4ece\u5355\u76ee\u56fe\u50cf\u751f\u6210\u53cc\u76eeVR\u56fe\u50cf\uff0c\u4ec5\u9700\u4e00\u6b21\u6e32\u67d3\u3002", "result": "\u9ad8\u6548\u8282\u80fd\uff0c\u56fe\u50cf\u8d28\u91cf\u9ad8\uff08SSIM 0.9679\uff0cPSNR 34.09\uff09\u3002", "conclusion": "EffVR\u4e3a\u53ef\u6301\u7eed\u79fb\u52a8VR\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15135", "pdf": "https://arxiv.org/pdf/2506.15135", "abs": "https://arxiv.org/abs/2506.15135", "authors": ["Zhengqun Koo"], "title": "Towards Bug-Free Distributed Go Programs", "categories": ["cs.SE", "cs.LO", "cs.PL", "F.3.1; F.1.2"], "comment": "Version 1. B.Comp. Dissertation", "summary": "Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u9759\u6001\u8bc1\u660e\u4f7f\u7528Go\u8bed\u8a00\u5b50\u96c6\u7684\u5206\u5e03\u5f0f\u7a0b\u5e8f\u4e2d\u4e0d\u5b58\u5728\u901a\u4fe1\u7ade\u4e89\u3002", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5e76\u53d1\u6027\u96be\u4ee5\u63a8\u7406\uff0c\u901a\u4fe1\u7ade\u4e89\u4f1a\u5bfc\u81f4\u63a5\u6536\u8005\u6536\u5230\u9519\u8bef\u6d88\u606f\u6216\u65e0\u6d88\u606f\uff0c\u6025\u9700\u9759\u6001\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6269\u5c55happens-before\u987a\u5e8f\u5230\u7f13\u51b2\u548c\u975e\u7f13\u51b2\u901a\u9053\uff0c\u9759\u6001\u5206\u6790\u5206\u5e03\u5f0f\u7a0b\u5e8f\u7684\u6267\u884c\u903b\u8f91\u3002", "result": "\u6846\u67b6\u80fd\u591f\u8bc1\u660e\u5206\u5e03\u5f0f\u7a0b\u5e8f\u4e2d\u901a\u4fe1\u7ade\u4e89\u7684\u7f3a\u5931\u3002", "conclusion": "\u6269\u5c55\u7684happens-before\u987a\u5e8f\u9002\u7528\u4e8e\u9759\u6001\u9a8c\u8bc1\u901a\u4fe1\u7ade\u4e89\uff0c\u9002\u7528\u4e8eGo\u8bed\u8a00\u7684\u5206\u5e03\u5f0f\u7f16\u7a0b\u3002"}}
{"id": "2506.14775", "pdf": "https://arxiv.org/pdf/2506.14775", "abs": "https://arxiv.org/abs/2506.14775", "authors": ["Tobias Labarta", "Nhi Hoang", "Katharina Weitz", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "title": "See What I Mean? CUE: A Cognitive Model of Understanding Explanations", "categories": ["cs.HC", "cs.AI", "cs.LG", "68T05, 91E30, 68U35, 62P10", "H.5.2; I.2.6; H.1.2; I.4.9; K.4.2"], "comment": "10 pages, 5 figures (main text), 4 tables, 455-participant user study", "summary": "As machine learning systems increasingly inform critical decisions, the need\nfor human-understandable explanations grows. Current evaluations of Explainable\nAI (XAI) often prioritize technical fidelity over cognitive accessibility which\ncritically affects users, in particular those with visual impairments. We\npropose CUE, a model for Cognitive Understanding of Explanations, linking\nexplanation properties to cognitive sub-processes: legibility (perception),\nreadability (comprehension), and interpretability (interpretation). In a study\n(N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we\nfound comparable task performance but lower confidence/effort for visually\nimpaired users. Unlike expected, these gaps were not mitigated and sometimes\nworsened by accessibility-focused color maps like Cividis. These results\nchallenge assumptions about perceptual optimization and support the need for\nadaptive XAI interfaces. They also validate CUE by demonstrating that altering\nexplanation legibility affects understandability. We contribute: (1) a\nformalized cognitive model for explanation understanding, (2) an integrated\ndefinition of human-centered explanation properties, and (3) empirical evidence\nmotivating accessible, user-tailored XAI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CUE\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u89e3\u91ca\u5c5e\u6027\u4e0e\u8ba4\u77e5\u5b50\u8fc7\u7a0b\uff08\u53ef\u8bfb\u6027\u3001\u53ef\u7406\u89e3\u6027\u3001\u53ef\u89e3\u91ca\u6027\uff09\u5173\u8054\uff0c\u8bc4\u4f30\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7684\u8ba4\u77e5\u53ef\u8bbf\u95ee\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u89c6\u89c9\u969c\u788d\u7528\u6237\u5728\u7406\u89e3\u70ed\u56fe\u89e3\u91ca\u65f6\u5b58\u5728\u4fe1\u5fc3\u548c\u52aa\u529b\u964d\u4f4e\u7684\u95ee\u9898\uff0c\u5373\u4f7f\u4f7f\u7528\u65e0\u969c\u788d\u989c\u8272\u6620\u5c04\uff08\u5982Cividis\uff09\u4e5f\u672a\u80fd\u6539\u5584\u3002", "motivation": "\u5f53\u524d\u5bf9\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7684\u8bc4\u4f30\u5e38\u5ffd\u7565\u8ba4\u77e5\u53ef\u8bbf\u95ee\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u89c6\u89c9\u969c\u788d\u7528\u6237\u7684\u9002\u7528\u6027\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7CUE\u6a21\u578b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f18\u5316XAI\u7684\u4eba\u7c7b\u4e2d\u5fc3\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faCUE\u6a21\u578b\uff0c\u5c06\u89e3\u91ca\u5c5e\u6027\uff08\u53ef\u8bfb\u6027\u3001\u53ef\u7406\u89e3\u6027\u3001\u53ef\u89e3\u91ca\u6027\uff09\u4e0e\u8ba4\u77e5\u5b50\u8fc7\u7a0b\u5173\u8054\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u4e0d\u540c\u989c\u8272\u6620\u5c04\uff08BWR\u3001Cividis\u3001Coolwarm\uff09\u5bf9\u89c6\u89c9\u969c\u788d\u7528\u6237\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u4efb\u52a1\u8868\u73b0\u76f8\u8fd1\uff0c\u4f46\u89c6\u89c9\u969c\u788d\u7528\u6237\u7684\u4fe1\u5fc3\u548c\u52aa\u529b\u663e\u8457\u964d\u4f4e\uff1b\u65e0\u969c\u788d\u989c\u8272\u6620\u5c04\uff08\u5982Cividis\uff09\u672a\u80fd\u6539\u5584\u8fd9\u4e00\u5dee\u8ddd\uff0c\u751a\u81f3\u6709\u65f6\u52a0\u5267\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u611f\u77e5\u4f18\u5316\u7684\u5047\u8bbe\uff0c\u652f\u6301\u81ea\u9002\u5e94\u7684XAI\u754c\u9762\u8bbe\u8ba1\uff0c\u5e76\u9a8c\u8bc1\u4e86CUE\u6a21\u578b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.15523", "pdf": "https://arxiv.org/pdf/2506.15523", "abs": "https://arxiv.org/abs/2506.15523", "authors": ["Jiaqi Sun", "Dingyu Yang", "Shiyou Qian", "Jian Cao", "Guangtao Xue"], "title": "Atys: An Efficient Profiling Framework for Identifying Hotspot Functions in Large-scale Cloud Microservices", "categories": ["cs.PF"], "comment": null, "summary": "To handle the high volume of requests, large-scale services are comprised of\nthousands of instances deployed in clouds. These services utilize diverse\nprogramming languages and are distributed across various nodes as encapsulated\ncontainers. Given their vast scale, even minor performance enhancements can\nlead to significant cost reductions. In this paper, we introduce Atys1, an\nefficient profiling framework specifically designed to identify hotspot\nfunctions within large-scale distributed services. Atys presents four key\nfeatures. First, it implements a language-agnostic adaptation mechanism for\nmultilingual microservices. Second, a two-level aggregation method is\nintroduced to provide a comprehensive overview of flamegraphs. Third, we\npropose a function selective pruning (FSP) strategy to enhance the efficiency\nof aggregating profiling results. Finally, we develop a frequency dynamic\nadjustment (FDA) scheme that dynamically modifies sampling frequency based on\nservice status, effectively minimizing profiling cost while maintaining\naccuracy. Cluster-scale experiments on two benchmarks show that the FSP\nstrategy achieves a 6.8% reduction in time with a mere 0.58% mean average\npercentage error (MAPE) in stack traces aggregation. Additionally, the FDA\nscheme ensures that the mean squared error (MSE) remains on par with that at\nhigh sampling rates, while achieving an 87.6% reduction in cost.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Atys1\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u5206\u6790\u6846\u67b6\uff0c\u65e8\u5728\u8bc6\u522b\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u670d\u52a1\u4e2d\u7684\u70ed\u70b9\u51fd\u6570\uff0c\u5177\u5907\u591a\u8bed\u8a00\u9002\u914d\u3001\u4e24\u7ea7\u805a\u5408\u3001\u9009\u62e9\u6027\u526a\u679d\u548c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u9891\u7387\u7b49\u7279\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5206\u6790\u6210\u672c\u3002", "motivation": "\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u670d\u52a1\u4e2d\uff0c\u5373\u4f7f\u5fae\u5c0f\u7684\u6027\u80fd\u63d0\u5347\u4e5f\u80fd\u5e26\u6765\u663e\u8457\u7684\u6210\u672c\u8282\u7ea6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u9ad8\u6548\u7684\u6846\u67b6\u6765\u8bc6\u522b\u70ed\u70b9\u51fd\u6570\u3002", "method": "Atys1\u91c7\u7528\u4e86\u591a\u8bed\u8a00\u9002\u914d\u673a\u5236\u3001\u4e24\u7ea7\u805a\u5408\u65b9\u6cd5\u3001\u51fd\u6570\u9009\u62e9\u6027\u526a\u679d\u7b56\u7565\u548c\u52a8\u6001\u8c03\u6574\u91c7\u6837\u9891\u7387\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFSP\u7b56\u7565\u51cf\u5c11\u4e866.8%\u7684\u65f6\u95f4\uff0cMAPE\u4ec5\u4e3a0.58%\uff1bFDA\u65b9\u6848\u5728\u4fdd\u6301\u9ad8\u91c7\u6837\u7387\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u6210\u672c\u964d\u4f4e\u4e8687.6%\u3002", "conclusion": "Atys1\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u6210\u672c\u4f18\u5316\u7684\u5206\u6790\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u670d\u52a1\u7684\u6027\u80fd\u4f18\u5316\u3002"}}
{"id": "2506.14803", "pdf": "https://arxiv.org/pdf/2506.14803", "abs": "https://arxiv.org/abs/2506.14803", "authors": ["Arbind Agrahari Baniya", "Tsz-Kwan Lee", "Peter W. Eklund", "Sunil Aryal"], "title": "Omnidirectional Video Super-Resolution using Deep Learning", "categories": ["cs.MM", "cs.CV", "cs.LG"], "comment": null, "summary": "Omnidirectional Videos (or 360{\\deg} videos) are widely used in Virtual\nReality (VR) to facilitate immersive and interactive viewing experiences.\nHowever, the limited spatial resolution in 360{\\deg} videos does not allow for\neach degree of view to be represented with adequate pixels, limiting the visual\nquality offered in the immersive experience. Deep learning Video\nSuper-Resolution (VSR) techniques used for conventional videos could provide a\npromising software-based solution; however, these techniques do not tackle the\ndistortion present in equirectangular projections of 360{\\deg} video signals.\nAn additional obstacle is the limited availability of 360{\\deg} video datasets\nfor study. To address these issues, this paper creates a novel 360{\\deg} Video\nDataset (360VDS) with a study of the extensibility of conventional VSR models\nto 360{\\deg} videos. This paper further proposes a novel deep learning model\nfor 360{\\deg} Video Super-Resolution (360{\\deg} VSR), called Spherical Signal\nSuper-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent\nmodelling with an attention mechanism, unbound from conventional VSR techniques\nlike alignment. With a purpose-built feature extractor and a novel loss\nfunction addressing spherical distortion, S3PO outperforms most\nstate-of-the-art conventional VSR models and 360{\\deg}~specific\nsuper-resolution models on 360{\\deg} video datasets. A step-wise ablation study\nis presented to understand and demonstrate the impact of the chosen\narchitectural sub-components, targeted training and optimisation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9360\u5ea6\u89c6\u9891\u7684\u8d85\u5206\u8fa8\u7387\u6280\u672fS3PO\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728360\u5ea6\u89c6\u9891\u4e2d\u7684\u5931\u771f\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684360VDS\u6570\u636e\u96c6\u3002", "motivation": "360\u5ea6\u89c6\u9891\u5728VR\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\uff0c\u5f71\u54cd\u89c6\u89c9\u8d28\u91cf\u3002\u4f20\u7edf\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406360\u5ea6\u89c6\u9891\u7684\u6295\u5f71\u5931\u771f\uff0c\u4e14\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86S3PO\u6a21\u578b\uff0c\u91c7\u7528\u5faa\u73af\u5efa\u6a21\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bbe\u8ba1\u4e86\u9488\u5bf9\u7403\u5f62\u5931\u771f\u7684\u7279\u5f81\u63d0\u53d6\u5668\u548c\u635f\u5931\u51fd\u6570\u3002", "result": "S3PO\u5728360\u5ea6\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u4f20\u7edf\u548c360\u5ea6\u4e13\u7528\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u3002", "conclusion": "S3PO\u6210\u529f\u89e3\u51b3\u4e86360\u5ea6\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.15174", "pdf": "https://arxiv.org/pdf/2506.15174", "abs": "https://arxiv.org/abs/2506.15174", "authors": ["Hossein Albakri", "Kazem Cheshmi"], "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in GPUs", "categories": ["cs.PL"], "comment": null, "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aenumerate-and-sparse-coarsen\u7684\u7f16\u8bd1\u5668\u53d8\u6362\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728GPU\u4e0a\u52a0\u901f\u7a00\u758f\u77e9\u9635-\u77e9\u9635\u4e58\u6cd5\uff08SPMM\uff09\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u6570\u636e\u7ed3\u6784\u5bfc\u81f4\u7684\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u4f4e\u6548\u95ee\u9898\u3002", "motivation": "\u7a00\u758f\u6570\u636e\u7ed3\u6784\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u5e38\u7528\u4ee5\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u4f46\u4f1a\u5bfc\u81f4\u5185\u5b58\u8bbf\u95ee\u4e0d\u89c4\u5219\uff0c\u5f71\u54cdGPU\u8d44\u6e90\u7684\u9ad8\u6548\u5229\u7528\u3002", "method": "\u901a\u8fc7\u65b0\u7684\u7f16\u8bd1\u5668\u53d8\u6362\u65b9\u6cd5\uff0c\u589e\u52a0\u4e86\u5bc4\u5b58\u5668\u548c\u7f13\u5b58\u4e2d\u7684\u6570\u636e\u91cd\u7528\uff0c\u540c\u65f6\u4f18\u5316\u4e86GPU\u8ba1\u7b97\u8d44\u6e90\u7684\u8d1f\u8f7d\u5747\u8861\u3002", "result": "\u5728A100 GPU\u4e0a\uff0cSPMM\u7684\u6027\u80fd\u63d0\u5347\u4e861.84\u500d\u81f32.27\u500d\uff0c\u76f8\u8f83\u4e8ecuBLAS\u548ccuSPARSE\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86GPU\u4e0a\u7a00\u758f\u77e9\u9635\u8ba1\u7b97\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5377\u79ef\u548c\u53d8\u6362\u5668\u6a21\u578b\u3002"}}
{"id": "2506.14987", "pdf": "https://arxiv.org/pdf/2506.14987", "abs": "https://arxiv.org/abs/2506.14987", "authors": ["Eman Alqudah", "Ashfaq Khokhar"], "title": "CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in Industrial URLLC", "categories": ["cs.NI", "cs.LG"], "comment": "This paper has been submitted to IEEEGLOBE2025 on April 15, 2025", "summary": "Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a\nCNN-based dynamic priority prediction mechanism for improved interference\ncoordination in multi-cell, multi-channel networks. Unlike LDP's static\npriorities, our approach uses a Convolutional Neural Network and graph coloring\nto adaptively assign link priorities based on real-time traffic, transmission\nopportunities, and network conditions. Assuming that first training phase is\nperformed offline, our approach introduced minimal overhead, while enabling\nmore efficient resource allocation, boosting network capacity, SINR, and\nschedulability. Simulation results show SINR gains of up to 113\\%, 94\\%, and\n49\\% over LDP across three network configurations, highlighting its\neffectiveness for complex URLLC scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u52a8\u6001\u4f18\u5148\u7ea7\u9884\u6d4b\u673a\u5236\uff0c\u7528\u4e8e\u589e\u5f3aLDP\u7b97\u6cd5\uff0c\u4ee5\u5728\u591a\u5c0f\u533a\u3001\u591a\u901a\u9053\u7f51\u7edc\u4e2d\u63d0\u9ad8\u5e72\u6270\u534f\u8c03\u6548\u7387\u3002", "motivation": "\u786e\u4fdd\u5927\u89c4\u6a21\u5de5\u4e1a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff08URLLC\uff09\u7684\u5206\u7ec4\u7ea7\u901a\u4fe1\u8d28\u91cf\u662f\u5173\u952e\u3002", "method": "\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u56fe\u7740\u8272\u6280\u672f\u52a8\u6001\u5206\u914d\u94fe\u8def\u4f18\u5148\u7ea7\uff0c\u5b9e\u65f6\u8003\u8651\u6d41\u91cf\u3001\u4f20\u8f93\u673a\u4f1a\u548c\u7f51\u7edc\u6761\u4ef6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e09\u79cd\u7f51\u7edc\u914d\u7f6e\u4e2dSINR\u589e\u76ca\u5206\u522b\u8fbe\u5230113%\u300194%\u548c49%\uff0c\u663e\u8457\u4f18\u4e8eLDP\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742URLLC\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u9ad8\u6548\u5206\u914d\u8d44\u6e90\u5e76\u63d0\u5347\u7f51\u7edc\u5bb9\u91cf\u3002"}}
{"id": "2506.14771", "pdf": "https://arxiv.org/pdf/2506.14771", "abs": "https://arxiv.org/abs/2506.14771", "authors": ["Mengyuan Wang", "Yang Liu", "Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "title": "Empirical Studies of Large Scale Environment Scanning by Consumer Electronics", "categories": ["eess.IV", "cs.CV", "cs.ET", "cs.MM"], "comment": "Accepted by IEEE Consumer Electronics Magazine", "summary": "This paper presents an empirical evaluation of the Matterport Pro3, a\nconsumer-grade 3D scanning device, for large-scale environment reconstruction.\nWe conduct detailed scanning (1,099 scanning points) of a six-floor building\n(17,567 square meters) and assess the device's effectiveness, limitations, and\nperformance enhancements in diverse scenarios. Challenges encountered during\nthe scanning are addressed through proposed solutions, while we also explore\nadvanced methods to overcome them more effectively. Comparative analysis with\nanother consumer-grade device (iPhone) highlights the Pro3's balance between\ncost-effectiveness and performance. The Matterport Pro3 achieves a denser point\ncloud with 1,877,324 points compared to the iPhone's 506,961 points and higher\nalignment accuracy with an RMSE of 0.0118 meters. The cloud-to-cloud (C2C)\naverage distance error between the two point cloud models is 0.0408 meters,\nwith a standard deviation of 0.0715 meters. The study demonstrates the Pro3's\nability to generate high-quality 3D models suitable for large-scale\napplications, leveraging features such as LiDAR and advanced alignment\ntechniques.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86Matterport Pro3\u5728\u5927\u89c4\u6a21\u73af\u5883\u91cd\u5efa\u4e2d\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u5176\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u6d88\u8d39\u7ea73D\u626b\u63cf\u8bbe\u5907Matterport Pro3\u5728\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\u6781\u9650\u3002", "method": "\u901a\u8fc7\u516d\u5c42\u5efa\u7b51\uff0817,567\u5e73\u65b9\u7c73\uff09\u76841,099\u6b21\u626b\u63cf\u70b9\uff0c\u5bf9\u6bd4iPhone\u8bbe\u5907\uff0c\u5206\u6790Pro3\u7684\u70b9\u4e91\u5bc6\u5ea6\u548c\u5bf9\u9f50\u7cbe\u5ea6\u3002", "result": "Pro3\u751f\u6210\u7684\u70b9\u4e91\u66f4\u5bc6\u96c6\uff081,877,324\u70b9vs 506,961\u70b9\uff09\uff0c\u5bf9\u9f50\u8bef\u5dee\u66f4\u4f4e\uff08RMSE 0.0118\u7c73\uff09\u3002", "conclusion": "Matterport Pro3\u5728\u6210\u672c\u6548\u76ca\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u5408\u5927\u89c4\u6a213D\u5efa\u6a21\u5e94\u7528\u3002"}}
{"id": "2506.15006", "pdf": "https://arxiv.org/pdf/2506.15006", "abs": "https://arxiv.org/abs/2506.15006", "authors": ["Jesmin Jahan Tithi", "Hanjiang Wu", "Avishaii Abuhatzera", "Fabrizio Petrini"], "title": "Scaling Intelligence: Designing Data Centers for Next-Gen Language Models", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.ET", "cs.PF"], "comment": "14 pages, submitted to SC25 for review", "summary": "The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8\ntrillion parameters - demands a radical rethinking of data center architecture\nto ensure scalability, efficiency, and cost-effectiveness. Our work provides a\ncomprehensive co-design framework that jointly explores FLOPS, HBM bandwidth\nand capacity, multiple network topologies (two-tier vs. FullFlat optical), the\nsize of the scale-out domain, and popular parallelism/optimization strategies\nused in LLMs. We introduce and evaluate FullFlat network architectures, which\nprovide uniform high-bandwidth, low-latency connectivity between all nodes, and\ndemonstrate their transformative impact on performance and scalability. Through\ndetailed sensitivity analyses, we quantify the benefits of overlapping compute\nand communication, leveraging hardware-accelerated collectives, wider scale-out\ndomains, and larger memory capacity. Our study spans both sparse (mixture of\nexperts) and dense transformer-based LLMs, revealing how system design choices\naffect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens\nper sec / Peak flops of the hardware) and overall throughput. For the co-design\nstudy, we extended and validated a performance modeling tool capable of\npredicting LLM runtime within 10% of real-world measurements. Our findings\noffer actionable insights and a practical roadmap for designing AI data centers\nthat can efficiently support trillion-parameter models, reduce optimization\ncomplexity, and sustain the rapid evolution of AI capabilities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u8054\u5408\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\u4e2d\u7684\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e86FullFlat\u7f51\u7edc\u67b6\u6784\u7684\u4f18\u52bf\u53ca\u5176\u5bf9\u541e\u5410\u91cf\u548c\u6a21\u578b\u5229\u7528\u7387\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\uff09\u53c2\u6570\u89c4\u6a21\u7684\u7206\u70b8\u6027\u589e\u957f\uff08\u9ad8\u8fbe1.8\u4e07\u4ebf\uff09\uff0c\u4f20\u7edf\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\u9762\u4e34\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u7684\u6311\u6218\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u8054\u5408\u63a2\u7d22FLOPS\u3001HBM\u5e26\u5bbd\u4e0e\u5bb9\u91cf\u3001\u591a\u79cd\u7f51\u7edc\u62d3\u6251\uff08\u5982FullFlat\u5149\u7f51\u7edc\uff09\u3001\u6269\u5c55\u57df\u5927\u5c0f\u4ee5\u53ca\u5e76\u884c\u5316/\u4f18\u5316\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6027\u80fd\u5efa\u6a21\u5de5\u5177\uff0c\u5e76\u8fdb\u884c\u8be6\u7ec6\u7684\u654f\u611f\u6027\u5206\u6790\u3002", "result": "FullFlat\u7f51\u7edc\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u91cf\u5316\u4e86\u8ba1\u7b97\u4e0e\u901a\u4fe1\u91cd\u53e0\u3001\u786c\u4ef6\u52a0\u901f\u96c6\u4f53\u64cd\u4f5c\u3001\u66f4\u5927\u6269\u5c55\u57df\u548c\u5185\u5b58\u5bb9\u91cf\u7684\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9ad8\u6548\u652f\u6301\u4e07\u4ebf\u53c2\u6570\u6a21\u578b\u7684\u6570\u636e\u4e2d\u5fc3\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u7ebf\u56fe\uff0c\u964d\u4f4e\u4e86\u4f18\u5316\u590d\u6742\u5ea6\uff0c\u63a8\u52a8\u4e86AI\u80fd\u529b\u7684\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2506.14772", "pdf": "https://arxiv.org/pdf/2506.14772", "abs": "https://arxiv.org/abs/2506.14772", "authors": ["Jakob De Moor", "Hans Weytjens", "Johannes De Smedt", "Jochen De Weerdt"], "title": "SimBank: from Simulation to Solution in Prescriptive Process Monitoring", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Prescriptive Process Monitoring (PresPM) is an emerging area within Process\nMining, focused on optimizing processes through real-time interventions for\neffective decision-making. PresPM holds significant promise for organizations\nseeking enhanced operational performance. However, the current literature faces\ntwo key limitations: a lack of extensive comparisons between techniques and\ninsufficient evaluation approaches. To address these gaps, we introduce\nSimBank: a simulator designed for accurate benchmarking of PresPM methods.\nModeled after a bank's loan application process, SimBank enables extensive\ncomparisons of both online and offline PresPM methods. It incorporates a\nvariety of intervention optimization problems with differing levels of\ncomplexity and supports experiments on key causal machine learning challenges,\nsuch as assessing a method's robustness to confounding in data. SimBank\nadditionally offers a comprehensive evaluation capability: for each test case,\nit can generate the true outcome under each intervention action, which is not\npossible using recorded datasets. The simulator incorporates parallel\nactivities and loops, drawing from common logs to generate cases that closely\nresemble real-life process instances. Our proof of concept demonstrates\nSimBank's benchmarking capabilities through experiments with various PresPM\nmethods across different interventions, highlighting its value as a publicly\navailable simulator for advancing research and practice in PresPM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SimBank\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u7cbe\u786e\u6bd4\u8f83\u548c\u8bc4\u4f30Prescriptive Process Monitoring (PresPM)\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u6280\u672f\u6bd4\u8f83\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dPresPM\u7814\u7a76\u7f3a\u4e4f\u6280\u672f\u95f4\u7684\u5e7f\u6cdb\u6bd4\u8f83\u548c\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0cSimBank\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u94f6\u884c\u7684\u8d37\u6b3e\u7533\u8bf7\u6d41\u7a0b\uff0cSimBank\u652f\u6301\u5728\u7ebf\u548c\u79bb\u7ebfPresPM\u65b9\u6cd5\u7684\u6bd4\u8f83\uff0c\u6db5\u76d6\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u5e72\u9884\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u7b49\u3002", "result": "SimBank\u80fd\u591f\u751f\u6210\u771f\u5b9e\u5e72\u9884\u7ed3\u679c\uff0c\u652f\u6301\u5bf9PresPM\u65b9\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "SimBank\u4e3aPresPM\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u516c\u5f00\u53ef\u7528\u7684\u6a21\u62df\u5e73\u53f0\uff0c\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.14851", "pdf": "https://arxiv.org/pdf/2506.14851", "abs": "https://arxiv.org/abs/2506.14851", "authors": ["Yifei Liu", "Zuo Gan", "Zhenghao Gan", "Weiye Wang", "Chen Chen", "Yizhou Shan", "Xusheng Chen", "Zhenhua Han", "Yifei Zhu", "Shixuan Sun", "Minyi Guo"], "title": "Efficient Serving of LLM Applications with Probabilistic Demand Modeling", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": null, "summary": "Applications based on Large Language Models (LLMs) contains a series of tasks\nto address real-world problems with boosted capability, which have dynamic\ndemand volumes on diverse backends. Existing serving systems treat the resource\ndemands of LLM applications as a blackbox, compromising end-to-end efficiency\ndue to improper queuing order and backend warm up latency. We find that the\nresource demands of LLM applications can be modeled in a general and accurate\nmanner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which\nleverages PDGraph for efficient serving of LLM applications. Confronting\nprobabilistic demand description, Hermes applies the Gittins policy to\ndetermine the scheduling order that can minimize the average application\ncompletion time. It also uses the PDGraph model to help prewarm cold backends\nat proper moments. Experiments with diverse LLM applications confirm that\nHermes can effectively improve the application serving efficiency, reducing the\naverage completion time by over 70% and the P95 completion time by over 80%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Hermes\u7cfb\u7edf\uff0c\u901a\u8fc7\u6982\u7387\u9700\u6c42\u56fe\uff08PDGraph\uff09\u6a21\u578b\u4f18\u5316LLM\u5e94\u7528\u7684\u8d44\u6e90\u8c03\u5ea6\u548c\u9884\u70ed\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5c06LLM\u5e94\u7528\u7684\u8d44\u6e90\u9700\u6c42\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faPDGraph\u6a21\u578b\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1Hermes\u7cfb\u7edf\uff0c\u91c7\u7528Gittins\u7b56\u7565\u8c03\u5ea6\u4efb\u52a1\u5e76\u7ed3\u5408\u9884\u9884\u70ed\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHermes\u80fd\u51cf\u5c11\u5e73\u5747\u5b8c\u6210\u65f6\u95f470%\u4ee5\u4e0a\uff0cP95\u5b8c\u6210\u65f6\u95f480%\u4ee5\u4e0a\u3002", "conclusion": "Hermes\u4e3aLLM\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u670d\u52a1\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u548c\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2506.15290", "pdf": "https://arxiv.org/pdf/2506.15290", "abs": "https://arxiv.org/abs/2506.15290", "authors": ["Andela Ilic", "Jiaxi Jiang", "Paul Streli", "Xintong Liu", "Christian Holz"], "title": "Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.HC", "68T07, 68T45, 68U01", "I.2; I.3; I.4; I.5"], "comment": "Accepted by IJCAI 2025", "summary": "Motion capture using sparse inertial sensors has shown great promise due to\nits portability and lack of occlusion issues compared to camera-based tracking.\nExisting approaches typically assume that IMU sensors are tightly attached to\nthe human body. However, this assumption often does not hold in real-world\nscenarios. In this paper, we present a new task of full-body human pose\nestimation using sparse, loosely attached IMU sensors. To solve this task, we\nsimulate IMU recordings from an existing garment-aware human motion dataset. We\ndeveloped transformer-based diffusion models to synthesize loose IMU data and\nestimate human poses based on this challenging loose IMU data. In addition, we\nshow that incorporating garment-related parameters while training the model on\nsimulated loose data effectively maintains expressiveness and enhances the\nability to capture variations introduced by looser or tighter garments.\nExperiments show that our proposed diffusion methods trained on simulated and\nsynthetic data outperformed the state-of-the-art methods quantitatively and\nqualitatively, opening up a promising direction for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u7a00\u758f\u3001\u677e\u6563\u7a7f\u6234\u7684IMU\u4f20\u611f\u5668\u8fdb\u884c\u5168\u8eab\u59ff\u6001\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\u5904\u7406\u677e\u6563IMU\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u8863\u7269\u53c2\u6570\u63d0\u5347\u8868\u73b0\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709IMU\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u4f20\u611f\u5668\u7d27\u8d34\u8eab\u4f53\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u8fd9\u4e00\u5047\u8bbe\u5f80\u5f80\u4e0d\u6210\u7acb\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u677e\u6563\u7a7f\u6234IMU\u4f20\u611f\u5668\u7684\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\u3002", "method": "\u4ece\u73b0\u6709\u7684\u8863\u7269\u611f\u77e5\u8fd0\u52a8\u6570\u636e\u96c6\u4e2d\u6a21\u62df\u677e\u6563IMU\u8bb0\u5f55\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8eTransformer\u7684\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u5408\u6210\u677e\u6563IMU\u6570\u636e\u5e76\u4f30\u8ba1\u59ff\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u677e\u6563IMU\u4f20\u611f\u5668\u7684\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u7ed3\u5408\u8863\u7269\u53c2\u6570\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\u3002"}}
{"id": "2506.15295", "pdf": "https://arxiv.org/pdf/2506.15295", "abs": "https://arxiv.org/abs/2506.15295", "authors": ["Massimo Bartoletti", "Enrico Lipparini"], "title": "A theory of Lending Protocols in DeFi", "categories": ["cs.GT", "cs.CR", "cs.LO"], "comment": null, "summary": "Lending protocols are one of the main applications of Decentralized Finance\n(DeFi), enabling crypto-assets loan markets with a total value estimated in the\ntens of billions of dollars. Unlike traditional lending systems, these\nprotocols operate without relying on trusted authorities or off-chain\nenforcement mechanisms. To achieve key economic goals such as stability of the\nloan market, they devise instead trustless on-chain mechanisms, such as\nrewarding liquidators who repay the loans of under-collateralized borrowers by\nawarding them part of the borrower's collateral. The complexity of these\nincentive mechanisms, combined with their entanglement in low-level\nimplementation details, makes it challenging to precisely assess the structural\nand economic properties of lending protocols, as well as to analyze user\nstrategies and attacks. Crucially, since participation is open to anyone, any\nweaknesses in the incentive mechanism may give rise to unintended emergent\nbehaviours, or even enable adversarial strategies aimed at making profits to\nthe detriment of legit users, or at undermining the stability of the protocol.\nIn this work, we propose a formal model of lending protocols that captures the\nessential features of mainstream platforms, enabling us to identify and prove\nkey properties related to their economic and strategic dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u53bb\u4e2d\u5fc3\u5316\u501f\u8d37\u534f\u8bae\u7684\u7ecf\u6d4e\u548c\u7b56\u7565\u52a8\u6001\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u501f\u8d37\u534f\u8bae\u867d\u5927\u4f46\u590d\u6742\uff0c\u5176\u6fc0\u52b1\u673a\u5236\u53ef\u80fd\u5bfc\u81f4\u610f\u60f3\u4e0d\u5230\u7684\u884c\u4e3a\u6216\u653b\u51fb\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u5206\u6790\u5de5\u5177\u3002", "method": "\u5efa\u7acb\u4e00\u4e2a\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u6355\u6349\u4e3b\u6d41\u5e73\u53f0\u7684\u6838\u5fc3\u7279\u5f81\u3002", "result": "\u8bc6\u522b\u5e76\u8bc1\u660e\u4e86\u4e0e\u534f\u8bae\u7ecf\u6d4e\u548c\u7b56\u7565\u52a8\u6001\u76f8\u5173\u7684\u5173\u952e\u5c5e\u6027\u3002", "conclusion": "\u5f62\u5f0f\u5316\u6a21\u578b\u5e2e\u52a9\u7406\u89e3\u501f\u8d37\u534f\u8bae\u7684\u884c\u4e3a\u548c\u6f5c\u5728\u6f0f\u6d1e\u3002"}}
{"id": "2506.14777", "pdf": "https://arxiv.org/pdf/2506.14777", "abs": "https://arxiv.org/abs/2506.14777", "authors": ["Jules Leguy", "Pierre-Antoine Jean", "Felipe Torres Figueroa", "S\u00e9bastien Harispe"], "title": "WebXAII: an open-source web framework to study human-XAI interaction", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "This article introduces WebXAII, an open-source web framework designed to\nfacilitate research on human interaction with eXplainable Artificial\nIntelligence (XAI) systems. The field of XAI is rapidly expanding, driven by\nthe growing societal implications of the widespread adoption of AI (and in\nparticular machine learning) across diverse applications. Researchers who study\nthe interaction between humans and XAI techniques typically develop ad hoc\ninterfaces in order to conduct their studies. These interfaces are usually not\nshared alongside the results of the studies, which limits their reusability and\nthe reproducibility of experiments. In response, we design and implement\nWebXAII, a web-based platform that can embody full experimental protocols,\nmeaning that it can present all aspects of the experiment to human participants\nand record their responses. The experimental protocols are translated into a\ncomposite architecture of generic views and modules, which offers a lot of\nflexibility. The architecture is defined in a structured configuration file, so\nthat protocols can be implemented with minimal programming skills. We\ndemonstrate that WebXAII can effectively embody relevant protocols, by\nreproducing the protocol of a state-of-the-art study of the literature. The\nframework is available at https://github.com/PAJEAN/WebXAII.", "AI": {"tldr": "WebXAII\u662f\u4e00\u4e2a\u5f00\u6e90\u7684web\u6846\u67b6\uff0c\u65e8\u5728\u4fc3\u8fdb\u4eba\u7c7b\u4e0e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7cfb\u7edf\u4ea4\u4e92\u7684\u7814\u7a76\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u5b9e\u9a8c\u63a5\u53e3\u4e0d\u53ef\u91cd\u7528\u548c\u5b9e\u9a8c\u96be\u4ee5\u590d\u73b0\u7684\u95ee\u9898\u3002", "motivation": "XAI\u9886\u57df\u8fc5\u901f\u6269\u5f20\uff0c\u4f46\u7814\u7a76\u4eba\u5458\u901a\u5e38\u5f00\u53d1\u4e34\u65f6\u63a5\u53e3\u8fdb\u884c\u7814\u7a76\uff0c\u8fd9\u4e9b\u63a5\u53e3\u672a\u5171\u4eab\uff0c\u9650\u5236\u4e86\u5b9e\u9a8c\u7684\u53ef\u91cd\u7528\u6027\u548c\u590d\u73b0\u6027\u3002", "method": "\u8bbe\u8ba1\u548c\u5b9e\u73b0WebXAII\uff0c\u4e00\u4e2a\u57fa\u4e8eweb\u7684\u5e73\u53f0\uff0c\u80fd\u591f\u4f53\u73b0\u5b8c\u6574\u7684\u5b9e\u9a8c\u534f\u8bae\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u914d\u7f6e\u6587\u4ef6\u5b9a\u4e49\u7075\u6d3b\u7684\u5b9e\u9a8c\u67b6\u6784\u3002", "result": "WebXAII\u6210\u529f\u590d\u73b0\u4e86\u6587\u732e\u4e2d\u7684\u5148\u8fdb\u7814\u7a76\u534f\u8bae\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "WebXAII\u4e3aXAI\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u91cd\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5b9e\u9a8c\u7684\u5e7f\u6cdb\u590d\u73b0\u548c\u6539\u8fdb\u3002"}}
{"id": "2506.14852", "pdf": "https://arxiv.org/pdf/2506.14852", "abs": "https://arxiv.org/abs/2506.14852", "authors": ["Qizheng Zhang", "Michael Wornow", "Kunle Olukotun"], "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG", "cs.PF"], "comment": "23 pages", "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aagentic plan caching\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u3001\u5b58\u50a8\u3001\u9002\u914d\u548c\u91cd\u7528\u7ed3\u6784\u5316\u8ba1\u5212\u6a21\u677f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u5e94\u7528\u7684\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684LLM\u7f13\u5b58\u6280\u672f\uff08\u5982\u4e0a\u4e0b\u6587\u7f13\u5b58\u548c\u8bed\u4e49\u7f13\u5b58\uff09\u4e3b\u8981\u9488\u5bf9\u804a\u5929\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u6216\u73af\u5883\u4e0a\u4e0b\u6587\u7684\u667a\u80fd\u4ee3\u7406\u5e94\u7528\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7f13\u5b58\u65b9\u6cd5\u3002", "method": "\u4ece\u5df2\u5b8c\u6210\u7684\u4efb\u52a1\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u8ba1\u5212\u6a21\u677f\uff0c\u901a\u8fc7\u5173\u952e\u8bcd\u63d0\u53d6\u5339\u914d\u65b0\u8bf7\u6c42\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u5c06\u6a21\u677f\u9002\u914d\u5230\u5177\u4f53\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u667a\u80fd\u4ee3\u7406\u5e94\u7528\u4e2d\uff0c\u8be5\u7cfb\u7edf\u5e73\u5747\u964d\u4f4e\u4e8646.62%\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "conclusion": "agentic plan caching\u4e3a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4e92\u8865\u7684\u7f13\u5b58\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15424", "pdf": "https://arxiv.org/pdf/2506.15424", "abs": "https://arxiv.org/abs/2506.15424", "authors": ["Michael Mendler", "Marc Pouzet"], "title": "PSM: Policy Synchronised Deterministic Memory", "categories": ["cs.PL"], "comment": "This report summarises work on coding the theory of\n  policy-synchronised memory (see https://rdcu.be/erBwl) in Haskell. This was\n  developed for a graduate level course on Functional Reactive Programming\n  taught at Bamberg University by the first author during 2020-2023. An early\n  version of the PSM library had been presented at the SYNCHRON Workshop\n  (Aussois, France), November 2019", "summary": "Concurrency and determinacy do not go well with each other when resources\nmust be shared. Haskell provides parallel programming abstractions such as IVar\nand LVar in the Par monad and concurrent abstractions such as MVar and TVar in\nthe in IO and STM monads, respectively. The former are determinate but have no\ndestructive updates and the latter have destructive updates but do not\nguarantee determinacy. Programming patterns that are both concurrent and\ndeterminate, such as those provided by Kahn or Berry require memory\nabstractions at a higher level than is currently available. In this paper we\ndescribe a new type context PSM for policy synchronised memory in Haskell. Like\nSTM and IO, the computations in PSM can access persistent state and, as a\nside-effect, update the memory in imperative style. Like the Par and IO monads,\nPSM supports concurrent threads and shared state. However, in contrast to IO,\nour PSM contexts are race-free since concurrent accesses are policy coordinated\nwhich guarantees determinacy.Well-typed transactions in the PSM context can\naccommodate abstract data structures that are imperative, concurrently\nshareable and still behave deterministically, by construction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Haskell\u7c7b\u578b\u4e0a\u4e0b\u6587PSM\uff08\u7b56\u7565\u540c\u6b65\u5185\u5b58\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u5e76\u53d1\u7f16\u7a0b\u4e0e\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u901a\u8fc7\u7b56\u7565\u534f\u8c03\u5171\u4eab\u5185\u5b58\u7684\u8bbf\u95ee\uff0c\u5b9e\u73b0\u65e0\u7ade\u4e89\u4e14\u786e\u5b9a\u6027\u7684\u5e76\u53d1\u7f16\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684Haskell\u5e76\u53d1\u62bd\u8c61\uff08\u5982IVar\u3001LVar\u3001MVar\u3001TVar\uff09\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u786e\u5b9a\u6027\u548c\u7834\u574f\u6027\u66f4\u65b0\u7684\u9700\u6c42\uff0c\u800cKahn\u6216Berry\u7b49\u9ad8\u5c42\u6b21\u7684\u5e76\u53d1\u786e\u5b9a\u6027\u6a21\u5f0f\u7f3a\u4e4f\u5185\u5b58\u62bd\u8c61\u652f\u6301\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86PSM\uff08\u7b56\u7565\u540c\u6b65\u5185\u5b58\uff09\u7c7b\u578b\u4e0a\u4e0b\u6587\uff0c\u652f\u6301\u5e76\u53d1\u7ebf\u7a0b\u548c\u5171\u4eab\u72b6\u6001\uff0c\u901a\u8fc7\u7b56\u7565\u534f\u8c03\u786e\u4fdd\u65e0\u7ade\u4e89\u8bbf\u95ee\uff0c\u4ece\u800c\u4fdd\u8bc1\u786e\u5b9a\u6027\u3002", "result": "PSM\u6210\u529f\u5b9e\u73b0\u4e86\u65e2\u652f\u6301\u7834\u574f\u6027\u66f4\u65b0\u53c8\u4fdd\u6301\u786e\u5b9a\u6027\u7684\u5e76\u53d1\u7f16\u7a0b\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u7c7b\u578b\u7cfb\u7edf\u4fdd\u8bc1\u4e86\u62bd\u8c61\u6570\u636e\u7ed3\u6784\u7684\u786e\u5b9a\u6027\u884c\u4e3a\u3002", "conclusion": "PSM\u4e3aHaskell\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u5e76\u53d1\u7f16\u7a0b\u62bd\u8c61\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u786e\u5b9a\u6027\u7684\u5171\u4eab\u5185\u5b58\u8bbf\u95ee\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u5e76\u53d1\u548c\u786e\u5b9a\u6027\u7684\u573a\u666f\u3002"}}
{"id": "2506.15011", "pdf": "https://arxiv.org/pdf/2506.15011", "abs": "https://arxiv.org/abs/2506.15011", "authors": ["Eman Alqudah", "Ashfaq Khokhar"], "title": "GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC", "categories": ["cs.NI", "cs.LG"], "comment": "This paper has been submitted to IEEE MASS 2025 on May 7, 2025", "summary": "Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph\nConvolutional Network (GCN) integrated with a Deep Q-Network (DQN)\nreinforcement learning framework for improved interference coordination in\nmulti-cell, multi-channel networks. Unlike LDP's static priorities, our\napproach dynamically learns link priorities based on real-time traffic demand,\nnetwork topology, remaining transmission opportunities, and interference\npatterns. The GCN captures spatial dependencies, while the DQN enables adaptive\nscheduling decisions through reward-guided exploration. Simulation results show\nthat our GCN-DQN model achieves mean SINR improvements of 179.6\\%, 197.4\\%, and\n175.2\\% over LDP across three network configurations. Additionally, the GCN-DQN\nmodel demonstrates mean SINR improvements of 31.5\\%, 53.0\\%, and 84.7\\% over\nour previous CNN-based approach across the same configurations. These results\nunderscore the effectiveness of our GCN-DQN model in addressing complex URLLC\nrequirements with minimal overhead and superior network performance.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7ed3\u5408GCN\u548cDQN\u6539\u8fdb\u4e86LDP\u7b97\u6cd5\uff0c\u52a8\u6001\u5b66\u4e60\u94fe\u8def\u4f18\u5148\u7ea7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u786e\u4fdd\u5927\u89c4\u6a21\u5de5\u4e1a\u65e0\u7ebf\u7f51\u7edc\u4e2dURLLC\u7684\u5305\u7ea7\u901a\u4fe1\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165GCN\u4e0eDQN\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u52a8\u6001\u5b66\u4e60\u94fe\u8def\u4f18\u5148\u7ea7\uff0c\u57fa\u4e8e\u5b9e\u65f6\u6d41\u91cf\u9700\u6c42\u3001\u7f51\u7edc\u62d3\u6251\u7b49\u3002", "result": "GCN-DQN\u5728\u4e09\u79cd\u7f51\u7edc\u914d\u7f6e\u4e0b\uff0cSINR\u5e73\u5747\u63d0\u5347\u663e\u8457\uff08179.6%~197.4%\uff09\uff0c\u4f18\u4e8eLDP\u548cCNN\u65b9\u6cd5\u3002", "conclusion": "GCN-DQN\u6a21\u578b\u80fd\u9ad8\u6548\u6ee1\u8db3URLLC\u9700\u6c42\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u5f00\u9500\u5c0f\u3002"}}
{"id": "2506.14928", "pdf": "https://arxiv.org/pdf/2506.14928", "abs": "https://arxiv.org/abs/2506.14928", "authors": ["Dyk Chung Nguyen", "Thomas Chetaille", "Yuan-Hang Zhang", "Yuriy V. Pershin", "Massimiliano Di Ventra"], "title": "On the solvable-unsolvable transition due to noise-induced chaos in digital memcomputing", "categories": ["nlin.CD", "cs.ET"], "comment": null, "summary": "Digital memcomputing machines (DMMs) have been designed to solve complex\ncombinatorial optimization problems. Since DMMs are fundamentally classical\ndynamical systems, their ordinary differential equations (ODEs) can be\nefficiently simulated on modern computers. This provides a unique platform to\nstudy their performance under various conditions. An aspect that has received\nlittle attention so far is how their performance is affected by the numerical\nerrors in the solution of their ODEs and the physical noise they would be\nnaturally subject to if built in hardware. Here, we analyze these two aspects\nin detail by varying the integration time step (numerical noise) and adding\nstochastic perturbations (physical noise) into the equations of DMMs. We are\nparticularly interested in understanding how noise induces a chaotic transition\nthat marks the shift from successful problem-solving to failure in these\nsystems. Our study includes an analysis of power spectra and Lyapunov exponents\ndepending on the noise strength. The results reveal a correlation between the\ninstance solvability and the sign of the ensemble averaged mean largest\nLyapunov exponent. Interestingly, we find a regime in which DMMs with positive\nmean largest Lyapunov exponents still exhibit solvability. Furthermore, the\npower spectra provide additional information about our system by distinguishing\nbetween regular behavior (peaks) and chaotic behavior (broadband spectrum).\nTherefore, power spectra could be utilized to control whether a DMM operates in\nthe optimal dynamical regime. Overall, we find that the qualitative effects of\nnumerical and physical noise are mostly similar, despite their fundamentally\ndifferent origin.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6570\u5b57\u8bb0\u5fc6\u8ba1\u7b97\u5668\uff08DMMs\uff09\u5728\u6570\u503c\u8bef\u5dee\u548c\u7269\u7406\u566a\u58f0\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u566a\u58f0\u4f1a\u5f15\u53d1\u6df7\u6c8c\u8f6c\u53d8\uff0c\u5f71\u54cd\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u5e73\u5747\u6700\u5927Lyapunov\u6307\u6570\u4e3a\u6b63\uff0cDMMs\u4ecd\u80fd\u89e3\u51b3\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8DMMs\u5728\u6570\u503c\u566a\u58f0\u548c\u7269\u7406\u566a\u58f0\u4e0b\u7684\u8868\u73b0\u53ca\u5176\u5bf9\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u6b64\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6539\u53d8\u79ef\u5206\u65f6\u95f4\u6b65\u957f\uff08\u6570\u503c\u566a\u58f0\uff09\u548c\u52a0\u5165\u968f\u673a\u6270\u52a8\uff08\u7269\u7406\u566a\u58f0\uff09\u6765\u6a21\u62dfDMMs\u7684\u884c\u4e3a\uff0c\u5206\u6790\u529f\u7387\u8c31\u548cLyapunov\u6307\u6570\u3002", "result": "\u53d1\u73b0\u566a\u58f0\u5f3a\u5ea6\u4e0e\u6df7\u6c8c\u884c\u4e3a\u76f8\u5173\uff0c\u4e14\u5b58\u5728\u5373\u4f7fLyapunov\u6307\u6570\u4e3a\u6b63\u4ecd\u80fd\u89e3\u51b3\u95ee\u9898\u7684\u673a\u5236\u3002\u529f\u7387\u8c31\u53ef\u7528\u4e8e\u533a\u5206\u548c\u4f18\u5316DMMs\u7684\u8fd0\u884c\u72b6\u6001\u3002", "conclusion": "\u6570\u503c\u548c\u7269\u7406\u566a\u58f0\u5bf9DMMs\u7684\u5f71\u54cd\u76f8\u4f3c\uff0c\u529f\u7387\u8c31\u53ef\u5e2e\u52a9\u63a7\u5236\u5176\u6700\u4f73\u52a8\u6001\u8fd0\u884c\u72b6\u6001\u3002"}}
{"id": "2506.15066", "pdf": "https://arxiv.org/pdf/2506.15066", "abs": "https://arxiv.org/abs/2506.15066", "authors": ["Jianmin Ye", "Tianyang Liu", "Qi Tian", "Shengchu Su", "Zhe Jiang", "Xi Wang"], "title": "ChatModel: Automating Reference Model Design and Verification with LLMs", "categories": ["cs.AR", "cs.MA"], "comment": null, "summary": "As the complexity of integrated circuit designs continues to escalate, the\nfunctional verification becomes increasingly challenging. Reference models,\ncritical for accelerating the verification process, are themselves becoming\nmore intricate and time-consuming to develop. Despite the promise shown by\nlarge language models (LLMs) in code programming, effectively generating\ncomplex reference models remains a significant hurdle. To address these\nchallenges, we introduce ChatModel, the first LLM-aided agile reference model\ngeneration and verification platform. ChatModel streamlines the transition from\ndesign specifications to fully functional reference models by integrating\ndesign standardization and hierarchical agile modeling. Employing a\nbuilding-block generation strategy, it not only enhances the design\ncapabilities of LLMs for reference models but also significantly boosts\nverification efficiency. We evaluated ChatModel on 300 designs of varying\ncomplexity, demonstrating substantial improvements in both efficiency and\nquality of reference model generation. ChatModel achieved a peak performance\nimprovement of 55.02% compared to alternative methods, with notable\nenhancements in generation stability, and delivered a 9.18x increase in its\ncapacity to produce reference model designs. Furthermore, it accelerated the\niterative process of reference model design and validation by an average of\n5.90x compared to traditional approaches. These results highlight the potential\nof ChatModel to significantly advance the automation of reference model\ngeneration and validation.", "AI": {"tldr": "ChatModel\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u8003\u6a21\u578b\u751f\u6210\u4e0e\u9a8c\u8bc1\u5e73\u53f0\uff0c\u901a\u8fc7\u8bbe\u8ba1\u6807\u51c6\u5316\u548c\u5206\u5c42\u654f\u6377\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53c2\u8003\u6a21\u578b\u7684\u751f\u6210\u6548\u7387\u548c\u9a8c\u8bc1\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u7684\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u529f\u80fd\u6027\u9a8c\u8bc1\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\uff0c\u53c2\u8003\u6a21\u578b\u7684\u5f00\u53d1\u4e5f\u65e5\u76ca\u590d\u6742\u548c\u8017\u65f6\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7a0b\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u751f\u6210\u590d\u6742\u53c2\u8003\u6a21\u578b\u4ecd\u5b58\u5728\u56f0\u96be\u3002", "method": "ChatModel\u91c7\u7528\u8bbe\u8ba1\u6807\u51c6\u5316\u548c\u5206\u5c42\u654f\u6377\u5efa\u6a21\uff0c\u7ed3\u5408\u6a21\u5757\u5316\u751f\u6210\u7b56\u7565\uff0c\u4f18\u5316\u4e86\u53c2\u8003\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728300\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u8bbe\u8ba1\u4e0a\u6d4b\u8bd5\uff0cChatModel\u7684\u5cf0\u503c\u6027\u80fd\u63d0\u534755.02%\uff0c\u751f\u6210\u7a33\u5b9a\u6027\u663e\u8457\u63d0\u9ad8\uff0c\u53c2\u8003\u6a21\u578b\u8bbe\u8ba1\u80fd\u529b\u63d0\u53479.18\u500d\uff0c\u8fed\u4ee3\u8fc7\u7a0b\u52a0\u901f5.90\u500d\u3002", "conclusion": "ChatModel\u80fd\u591f\u663e\u8457\u63a8\u8fdb\u53c2\u8003\u6a21\u578b\u751f\u6210\u548c\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316\uff0c\u5c55\u73b0\u4e86\u5728\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u9a8c\u8bc1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.15172", "pdf": "https://arxiv.org/pdf/2506.15172", "abs": "https://arxiv.org/abs/2506.15172", "authors": ["Maria Spichkova", "Kevin Iwan", "Madeleine Zwart", "Hina Lee", "Yuwon Yoon", "Xiaohan Qin"], "title": "Advanced approach for Agile/Scrum Process: RetroAI++", "categories": ["cs.SE"], "comment": "Preprint. Accepted to the 29th International Conference on\n  Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025).\n  Final version to be published by Elsevier (In Press)", "summary": "In Agile/Scrum software development, sprint planning and retrospective\nanalysis are the key elements of project management. The aim of our work is to\nsupport software developers in these activities. In this paper, we present our\nprototype tool RetroAI++, based on emerging intelligent technologies. In our\nRetroAI++ prototype, we aim to automate and refine the practical application of\nAgile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI\ninsights, our prototype aims to automate and refine the many processes involved\nin the Sprint Planning, Development and Retrospective stages of Agile/Scrum\ndevelopment projects, offering intelligent suggestions for sprint organisation\nas well as meaningful insights for retrospective reflection.", "AI": {"tldr": "RetroAI++\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u6280\u672f\u7684\u539f\u578b\u5de5\u5177\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u548c\u4f18\u5316Agile/Scrum\u5f00\u53d1\u4e2d\u7684\u51b2\u523a\u8ba1\u5212\u548c\u56de\u987e\u5206\u6790\u3002", "motivation": "\u652f\u6301\u8f6f\u4ef6\u5f00\u53d1\u8005\u5728\u51b2\u523a\u8ba1\u5212\u548c\u56de\u987e\u5206\u6790\u4e2d\u66f4\u9ad8\u6548\u5730\u5e94\u7528Agile/Scrum\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u81ea\u52a8\u5316\u51b2\u523a\u8ba1\u5212\u3001\u5f00\u53d1\u548c\u56de\u987e\u9636\u6bb5\u7684\u591a\u9879\u6d41\u7a0b\uff0c\u5e76\u63d0\u4f9b\u667a\u80fd\u5efa\u8bae\u548c\u6709\u610f\u4e49\u7684\u5206\u6790\u3002", "result": "\u5f00\u53d1\u4e86RetroAI++\u539f\u578b\u5de5\u5177\uff0c\u80fd\u591f\u4e3a\u51b2\u523a\u7ec4\u7ec7\u548c\u56de\u987e\u53cd\u601d\u63d0\u4f9b\u667a\u80fd\u5316\u652f\u6301\u3002", "conclusion": "RetroAI++\u6709\u6f5c\u529b\u663e\u8457\u63d0\u5347Agile/Scrum\u5f00\u53d1\u4e2d\u7684\u9879\u76ee\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2506.15312", "pdf": "https://arxiv.org/pdf/2506.15312", "abs": "https://arxiv.org/abs/2506.15312", "authors": ["Han Wu", "Junyao Li", "Kangbo Zhao", "Sen Zhang", "Yukai Shi", "Liang Lin"], "title": "One-shot Face Sketch Synthesis in the Wild via Generative Diffusion Prior and Instruction Tuning", "categories": ["cs.GR", "cs.CR", "cs.CV", "cs.CY"], "comment": "We propose a novel framework for face sketch synthesis, where merely\n  a single pair of samples suffices to enable in-the-wild face sketch synthesis", "summary": "Face sketch synthesis is a technique aimed at converting face photos into\nsketches. Existing face sketch synthesis research mainly relies on training\nwith numerous photo-sketch sample pairs from existing datasets. However, these\nlarge-scale discriminative learning methods will have to face problems such as\ndata scarcity and high human labor costs. Once the training data becomes\nscarce, their generative performance significantly degrades. In this paper, we\npropose a one-shot face sketch synthesis method based on diffusion models. We\noptimize text instructions on a diffusion model using face photo-sketch image\npairs. Then, the instructions derived through gradient-based optimization are\nused for inference. To simulate real-world scenarios more accurately and\nevaluate method effectiveness more comprehensively, we introduce a new\nbenchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark\nconsists of 400 pairs of face photo-sketch images, including sketches with\ndifferent styles and photos with different backgrounds, ages, sexes,\nexpressions, illumination, etc. For a solid out-of-distribution evaluation, we\nselect only one pair of images for training at each time, with the rest used\nfor inference. Extensive experiments demonstrate that the proposed method can\nconvert various photos into realistic and highly consistent sketches in a\none-shot context. Compared to other methods, our approach offers greater\nconvenience and broader applicability. The dataset will be available at:\nhttps://github.com/HanWu3125/OS-Sketch", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u6b21\u4eba\u8138\u7d20\u63cf\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6587\u672c\u6307\u4ee4\uff0c\u5b9e\u73b0\u9ad8\u4e00\u81f4\u6027\u7684\u7d20\u63cf\u751f\u6210\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6OS-Sketch\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u548c\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u7684\u6587\u672c\u6307\u4ee4\uff0c\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u5b9e\u73b0\u7d20\u63cf\u5408\u6210\u3002", "result": "\u5728\u5355\u6b21\u8bad\u7ec3\u4e0b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u4e00\u81f4\u7684\u4eba\u8138\u7d20\u63cf\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u4fbf\u5229\u6027\u3002"}}
{"id": "2506.14799", "pdf": "https://arxiv.org/pdf/2506.14799", "abs": "https://arxiv.org/abs/2506.14799", "authors": ["Evdoxia Taka", "Debadyuti Bhattacharya", "Joanne Garde-Hansen", "Sanjay Sharma", "Tanaya Guha"], "title": "Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in AI has enabled automated analysis of complex media content\nat scale and generate actionable insights regarding character representation\nalong such dimensions as gender and age. Past work focused on quantifying\nrepresentation from audio/video/text using various ML models, but without\nhaving the audience in the loop. We ask, even if character distribution along\ndemographic dimensions are available, how useful are they to the general\npublic? Do they actually trust the numbers generated by AI models? Our work\naddresses these questions through a user study, while proposing a new AI-based\ncharacter representation and visualization tool. Our tool based on the\nContrastive Language Image Pretraining (CLIP) foundation model to analyze\nvisual screen data to quantify character representation across dimensions of\nage and gender. We also designed effective visualizations suitable for\npresenting such analytics to lay audience. Next, we conducted a user study to\nseek empirical evidence on the usefulness and trustworthiness of the\nAI-generated results for carefully chosen movies presented in the form of our\nvisualizations. We note that participants were able to understand the analytics\nfrom our visualization, and deemed the tool `overall useful'. Participants also\nindicated a need for more detailed visualizations to include more demographic\ncategories and contextual information of the characters. Participants' trust in\nAI-based gender and age models is seen to be moderate to low, although they\nwere not against the use of AI in this context. Our tool including code,\nbenchmarking, and data from the user study can be found here:\nhttps://anonymous.4open.science/r/Character-Representation-Media-FF7B", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u751f\u6210\u7684\u89d2\u8272\u4ee3\u8868\u6027\u6570\u636e\u5bf9\u516c\u4f17\u7684\u5b9e\u7528\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eCLIP\u6a21\u578b\u7684\u53ef\u89c6\u5316\u5de5\u5177\u3002", "motivation": "\u7814\u7a76AI\u751f\u6210\u7684\u6027\u522b\u548c\u5e74\u9f84\u89d2\u8272\u4ee3\u8868\u6027\u6570\u636e\u5bf9\u516c\u4f17\u7684\u5b9e\u7528\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u4f7f\u7528CLIP\u6a21\u578b\u5206\u6790\u89c6\u89c9\u6570\u636e\uff0c\u8bbe\u8ba1\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u53c2\u4e0e\u8005\u8ba4\u4e3a\u5de5\u5177\u6574\u4f53\u6709\u7528\uff0c\u4f46\u5bf9AI\u6a21\u578b\u7684\u4fe1\u4efb\u5ea6\u4e2d\u4f4e\uff0c\u5e0c\u671b\u53ef\u89c6\u5316\u66f4\u8be6\u7ec6\u3002", "conclusion": "AI\u751f\u6210\u7684\u89d2\u8272\u6570\u636e\u5206\u6790\u5bf9\u516c\u4f17\u6709\u4ef7\u503c\uff0c\u4f46\u9700\u63d0\u5347\u900f\u660e\u5ea6\u548c\u53ef\u89c6\u5316\u7ec6\u8282\u3002"}}
{"id": "2506.14937", "pdf": "https://arxiv.org/pdf/2506.14937", "abs": "https://arxiv.org/abs/2506.14937", "authors": ["Luan Gon\u00e7alves Miranda", "Pedro Ivo da Cruz", "Murilo Bellezoni Loiola"], "title": "Determina\u00e7\u00e3o Autom\u00e1tica de Limiar de Detec\u00e7\u00e3o de Ataques em Redes de Computadores Utilizando Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI", "cs.PF"], "comment": "This work was accepted at SBrT 2022 (Brazilian Symposium on\n  Telecommunications and Signal Processing), though it was not included in the\n  official proceedings. in Portuguese language", "summary": "Currently, digital security mechanisms like Anomaly Detection Systems using\nAutoencoders (AE) show great potential for bypassing problems intrinsic to the\ndata, such as data imbalance. Because AE use a non-trivial and nonstandardized\nseparation threshold to classify the extracted reconstruction error, the\ndefinition of this threshold directly impacts the performance of the detection\nprocess. Thus, this work proposes the automatic definition of this threshold\nusing some machine learning algorithms. For this, three algorithms were\nevaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u81ea\u52a8\u5b9a\u4e49\u81ea\u7f16\u7801\u5668\u7684\u91cd\u5efa\u8bef\u5dee\u9608\u503c\uff0c\u4ee5\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u7f16\u7801\u5668\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u91cd\u5efa\u8bef\u5dee\u9608\u503c\u7684\u975e\u6807\u51c6\u5316\u95ee\u9898\uff0c\u4ece\u800c\u4f18\u5316\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff1aK-\u6700\u8fd1\u90bb\u3001K-\u5747\u503c\u548c\u652f\u6301\u5411\u91cf\u673a\uff0c\u7528\u4e8e\u81ea\u52a8\u5b9a\u4e49\u9608\u503c\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u76ee\u6807\u662f\u6539\u5584\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u5e94\u7528\uff0c\u53ef\u4ee5\u81ea\u52a8\u4f18\u5316\u81ea\u7f16\u7801\u5668\u7684\u91cd\u5efa\u8bef\u5dee\u9608\u503c\uff0c\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2506.14805", "pdf": "https://arxiv.org/pdf/2506.14805", "abs": "https://arxiv.org/abs/2506.14805", "authors": ["Yang Yao", "Lingyu Li", "Jiaxin Song", "Chiyu Chen", "Zhenqi He", "Yixu Wang", "Xin Wang", "Tianle Gu", "Jie Li", "Yan Teng", "Yingchun Wang"], "title": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "As Multimodal Large Language Models (MLLMs) continue to evolve, their\ncognitive and reasoning capabilities have seen remarkable progress. However,\nchallenges in visual fine-grained perception and commonsense causal inference\npersist. This paper introduces Argus Inspection, a multimodal benchmark with\ntwo levels of difficulty, emphasizing detailed visual recognition while\nincorporating real-world commonsense understanding to evaluate causal reasoning\nabilities. Expanding on it, we present the Eye of Panoptes framework, which\nintegrates a binary parametric Sigmoid metric with an indicator function,\nenabling a more holistic evaluation of MLLMs' responses in opinion-based\nreasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the\nhighest performance in visual fine-grained reasoning reaches only 0.46,\nhighlighting considerable potential for enhancement. Our research offers\nvaluable perspectives for the continued refinement of MLLMs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Argus Inspection\u591a\u6a21\u6001\u57fa\u51c6\u548cEye of Panoptes\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30MLLMs\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u6a21\u578b\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u5f53\u524dMLLMs\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u548c\u5e38\u8bc6\u56e0\u679c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faArgus Inspection\u57fa\u51c6\u548cEye of Panoptes\u6846\u67b6\uff0c\u7ed3\u5408Sigmoid\u5ea6\u91cf\u4e0e\u6307\u793a\u51fd\u6570\uff0c\u8bc4\u4f30MLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e3b\u6d41MLLMs\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u6700\u4f73\u8868\u73b0\u4ec5\u4e3a0.46\uff0c\u6539\u8fdb\u7a7a\u95f4\u5927\u3002", "conclusion": "\u7814\u7a76\u4e3aMLLMs\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2506.15316", "pdf": "https://arxiv.org/pdf/2506.15316", "abs": "https://arxiv.org/abs/2506.15316", "authors": ["Benoit Tain", "Raphael Millet", "Romain Lemaire", "Michal Szczepanski", "Laurent Alacoque", "Emmanuel Pluchart", "Sylvain Choisnet", "Rohit Prasad", "Jerome Chossat", "Pascal Pierunek", "Pascal Vivet", "Sebastien Thuries"], "title": "J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor", "categories": ["cs.AR", "cs.AI"], "comment": "Preprint from ISLPED 2025. 979-8-3315-2710-5/25/$31.00\n  \\c{opyright}2025 IEEE", "summary": "This paper presents J3DAI, a tiny deep neural network-based hardware\naccelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial\nintelligence (AI) chip integrating a Deep Neural Network (DNN)-based\naccelerator. The DNN accelerator is designed to efficiently perform neural\nnetwork tasks such as image classification and segmentation. This paper focuses\non the digital system of J3DAI, highlighting its Performance-Power-Area (PPA)\ncharacteristics and showcasing advanced edge AI capabilities on a CMOS image\nsensor. To support hardware, we utilized the Aidge comprehensive software\nframework, which enables the programming of both the host processor and the DNN\naccelerator. Aidge supports post-training quantization, significantly reducing\nmemory footprint and computational complexity, making it crucial for deploying\nmodels on resource-constrained hardware like J3DAI. Our experimental results\ndemonstrate the versatility and efficiency of this innovative design in the\nfield of edge AI, showcasing its potential to handle both simple and\ncomputationally intensive tasks. Future work will focus on further optimizing\nthe architecture and exploring new applications to fully leverage the\ncapabilities of J3DAI. As edge AI continues to grow in importance, innovations\nlike J3DAI will play a crucial role in enabling real-time, low-latency, and\nenergy-efficient AI processing at the edge.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86J3DAI\uff0c\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u7528\u4e8e3D\u5806\u53e0CMOS\u56fe\u50cf\u4f20\u611f\u5668\uff0c\u5177\u5907AI\u82af\u7247\u96c6\u6210\u529f\u80fd\uff0c\u4e13\u6ce8\u4e8e\u8fb9\u7f18AI\u7684\u9ad8\u6548\u5904\u7406\u3002", "motivation": "\u968f\u7740\u8fb9\u7f18AI\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u9ad8\u6548\u3001\u5b9e\u65f6\u7684AI\u5904\u7406\u89e3\u51b3\u65b9\u6848\u3002J3DAI\u65e8\u5728\u6ee1\u8db3\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u80fd\u6548\u9700\u6c42\u3002", "method": "\u91c7\u7528Aidge\u8f6f\u4ef6\u6846\u67b6\u652f\u6301\u786c\u4ef6\u5f00\u53d1\uff0c\u5305\u62ec\u540e\u8bad\u7ec3\u91cf\u5316\u4ee5\u51cf\u5c11\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u4e13\u6ce8\u4e8e\u6570\u5b57\u7cfb\u7edf\u8bbe\u8ba1\u53ca\u5176PPA\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86J3DAI\u5728\u8fb9\u7f18AI\u4e2d\u7684\u591a\u529f\u80fd\u6027\u548c\u9ad8\u6548\u6027\uff0c\u80fd\u591f\u5904\u7406\u7b80\u5355\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u3002", "conclusion": "J3DAI\u5728\u8fb9\u7f18AI\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u4f18\u5316\u67b6\u6784\u5e76\u63a2\u7d22\u65b0\u5e94\u7528\u3002"}}
{"id": "2506.14981", "pdf": "https://arxiv.org/pdf/2506.14981", "abs": "https://arxiv.org/abs/2506.14981", "authors": ["Hailiang Zhang", "Dieu My T. Nguyen", "Christine Smit", "Mahabal Hegde"], "title": "Zarr-Based Chunk-Level Cumulative Sums in Reduced Dimensions", "categories": ["cs.DC", "cs.DS"], "comment": null, "summary": "Data analysis on massive multi-dimensional data, such as high-resolution\nlarge-region time averaging or area averaging for geospatial data, often\ninvolves calculations over a significant number of data points. While\nperforming calculations in scalable and flexible distributed or cloud\nenvironments is a viable option, a full scan of large data volumes still serves\nas a computationally intensive bottleneck, leading to significant cost. This\npaper introduces a generic and comprehensive method to address these\ncomputational challenges. This method generates a small, size-tunable\nsupplementary dataset that stores the cumulative sums along specific subset\ndimensions on top of the raw data. This minor addition unlocks rapid and cheap\nhigh-resolution large-region data analysis, making calculations over large\nnumbers of data points feasible with small instances or even microservices in\nthe cloud. This method is general-purpose, but is particularly well-suited for\ndata stored in chunked, cloud-optimized formats and for services running in\ndistributed or cloud environments. We present a Zarr extension proposal to\nintegrate the specifications of this method and facilitate its straightforward\nimplementation in general-purpose software applications. Benchmark tests\ndemonstrate that this method, implemented in Amazon Web services (AWS),\nsignificantly outperforms the brute-force approach used in on-premises\nservices. With just 5% supplemental storage, this method achieves a performance\nthat is 3-4 orders of magnitude (~10,000 times) faster than the brute-force\napproach, while incurring significantly reduced computational costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u5c0f\u578b\u8865\u5145\u6570\u636e\u96c6\u5b58\u50a8\u7d2f\u79ef\u548c\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u591a\u7ef4\u6570\u636e\u5206\u6790\u7684\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u7ef4\u6570\u636e\u5168\u626b\u63cf\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u751f\u6210\u5b58\u50a8\u7279\u5b9a\u7ef4\u5ea6\u7d2f\u79ef\u548c\u7684\u5c0f\u578b\u8865\u5145\u6570\u636e\u96c6\u3002", "result": "\u5728AWS\u4e0a\u5b9e\u73b0\uff0c\u6027\u80fd\u6bd4\u66b4\u529b\u65b9\u6cd5\u5feb3-4\u4e2a\u6570\u91cf\u7ea7\uff0c\u5b58\u50a8\u589e\u52a0\u4ec55%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u6210\u672c\u4f4e\uff0c\u7279\u522b\u9002\u5408\u4e91\u73af\u5883\u4e2d\u7684\u6570\u636e\u5206\u6790\u3002"}}
{"id": "2506.15227", "pdf": "https://arxiv.org/pdf/2506.15227", "abs": "https://arxiv.org/abs/2506.15227", "authors": ["Quanjun Zhang", "Chunrong Fang", "Siqi Gu", "Ye Shang", "Zhenyu Chen", "Liang Xiao"], "title": "Large Language Models for Unit Testing: A Systematic Literature Review", "categories": ["cs.SE"], "comment": null, "summary": "Unit testing is a fundamental practice in modern software engineering, with\nthe aim of ensuring the correctness, maintainability, and reliability of\nindividual software components. Very recently, with the advances in Large\nLanguage Models (LLMs), a rapidly growing body of research has leveraged LLMs\nto automate various unit testing tasks, demonstrating remarkable performance\nand significantly reducing manual effort. However, due to ongoing explorations\nin the LLM-based unit testing field, it is challenging for researchers to\nunderstand existing achievements, open challenges, and future opportunities.\nThis paper presents the first systematic literature review on the application\nof LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant\npapers from the perspectives of both unit testing and LLMs. We first categorize\nexisting unit testing tasks that benefit from LLMs, e.g., test generation and\noracle generation. We then discuss several critical aspects of integrating LLMs\ninto unit testing research, including model usage, adaptation strategies, and\nhybrid approaches. We further summarize key challenges that remain unresolved\nand outline promising directions to guide future research in this area.\nOverall, our paper provides a systematic overview of the research landscape to\nthe unit testing community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research. Our artifacts are\npublicly available at the GitHub repository:\nhttps://github.com/iSEngLab/AwesomeLLM4UT.", "AI": {"tldr": "\u672c\u6587\u5bf9\u622a\u81f32025\u5e743\u6708\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5355\u5143\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u7814\u7a76\u73b0\u72b6\u3001\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740LLMs\u7684\u53d1\u5c55\uff0c\u5176\u5728\u5355\u5143\u6d4b\u8bd5\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\u8fc5\u901f\u589e\u957f\uff0c\u4f46\u7814\u7a76\u9886\u57df\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u6027\u603b\u7ed3\uff0c\u4e9f\u9700\u7efc\u8ff0\u4ee5\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5206\u6790\u76f8\u5173\u8bba\u6587\uff0c\u4ece\u5355\u5143\u6d4b\u8bd5\u548cLLMs\u4e24\u4e2a\u89d2\u5ea6\u5206\u7c7b\u73b0\u6709\u4efb\u52a1\uff08\u5982\u6d4b\u8bd5\u751f\u6210\u4e0e\u65ad\u8a00\u751f\u6210\uff09\uff0c\u5e76\u63a2\u8ba8\u6a21\u578b\u4f7f\u7528\u3001\u9002\u5e94\u7b56\u7565\u53ca\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u603b\u7ed3\u4e86LLMs\u5728\u5355\u5143\u6d4b\u8bd5\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u6f5c\u5728\u7814\u7a76\u65b9\u5411\uff0c\u63d0\u4f9b\u4e86\u7814\u7a76\u73b0\u72b6\u7684\u5168\u9762\u6982\u8ff0\u3002", "conclusion": "\u672c\u6587\u4e3a\u5355\u5143\u6d4b\u8bd5\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u7406\u89e3\u73b0\u6709\u6210\u679c\u5e76\u63a8\u52a8\u672a\u6765\u7814\u7a76\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.15684", "pdf": "https://arxiv.org/pdf/2506.15684", "abs": "https://arxiv.org/abs/2506.15684", "authors": ["Qingming Liu", "Zhen Liu", "Dinghuai Zhang", "Kui Jia"], "title": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Technical Report (21 pages, 21 figures)", "summary": "Generating high-quality and photorealistic 3D assets remains a longstanding\nchallenge in 3D vision and computer graphics. Although state-of-the-art\ngenerative models, such as diffusion models, have made significant progress in\n3D generation, they often fall short of human-designed content due to limited\nability to follow instructions, align with human preferences, or produce\nrealistic textures, geometries, and physical attributes. In this paper, we\nintroduce Nabla-R2D3, a highly effective and sample-efficient reinforcement\nlearning alignment framework for 3D-native diffusion models using 2D rewards.\nBuilt upon the recently proposed Nabla-GFlowNet method, which matches the score\nfunction to reward gradients in a principled manner for reward finetuning, our\nNabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D\nreward signals. Extensive experiments show that, unlike vanilla finetuning\nbaselines which either struggle to converge or suffer from reward hacking,\nNabla-R2D3 consistently achieves higher rewards and reduced prior forgetting\nwithin a few finetuning steps.", "AI": {"tldr": "\u63d0\u51fa\u4e86Nabla-R2D3\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u76843D\u6269\u6563\u6a21\u578b\u5bf9\u9f50\u6846\u67b6\uff0c\u5229\u75282D\u5956\u52b1\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u76843D\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u5728\u751f\u6210\u8d28\u91cf\u3001\u6307\u4ee4\u9075\u5faa\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eNabla-GFlowNet\u65b9\u6cd5\uff0c\u63d0\u51faNabla-R2D3\u6846\u67b6\uff0c\u901a\u8fc72D\u5956\u52b1\u4fe1\u53f7\u5bf93D\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cNabla-R2D3\u5728\u66f4\u5c11\u7684\u4f18\u5316\u6b65\u9aa4\u5185\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\uff0c\u5e76\u51cf\u5c11\u5148\u9a8c\u9057\u5fd8\u3002", "conclusion": "Nabla-R2D3\u662f\u63d0\u53473D\u751f\u6210\u6a21\u578b\u8d28\u91cf\u548c\u6548\u7387\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2506.14809", "pdf": "https://arxiv.org/pdf/2506.14809", "abs": "https://arxiv.org/abs/2506.14809", "authors": ["Peng Jiang", "Vinicius Cezar Monteiro de Lira", "Antonio Maiorino"], "title": "Impact of a Deployed LLM Survey Creation Tool through the IS Success Model", "categories": ["cs.HC", "cs.LG", "I.2; H.4"], "comment": null, "summary": "Surveys are a cornerstone of Information Systems (IS) research, yet creating\nhigh-quality surveys remains labor-intensive, requiring both domain expertise\nand methodological rigor. With the evolution of large language models (LLMs),\nnew opportunities emerge to automate survey generation. This paper presents the\nreal-world deployment of an LLM-powered system designed to accelerate data\ncollection while maintaining survey quality. Deploying such systems in\nproduction introduces real-world complexity, including diverse user needs and\nquality control. We evaluate the system using the DeLone and McLean IS Success\nModel to understand how generative AI can reshape a core IS method. This study\nmakes three key contributions. To our knowledge, this is the first application\nof the IS Success Model to a generative AI system for survey creation. In\naddition, we propose a hybrid evaluation framework combining automated and\nhuman assessments. Finally, we implement safeguards that mitigate\npost-deployment risks and support responsible integration into IS workflows.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u8c03\u67e5\u95ee\u5377\u7684\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408DeLone\u548cMcLean\u7684IS\u6210\u529f\u6a21\u578b\u8bc4\u4f30\u5176\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u95ee\u5377\u8bbe\u8ba1\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u548c\u65f6\u95f4\uff0cLLM\u4e3a\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u95ee\u5377\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408IS\u6210\u529f\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u91c7\u7528\u81ea\u52a8\u4e0e\u4eba\u5de5\u7ed3\u5408\u7684\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u52a0\u901f\u6570\u636e\u6536\u96c6\u5e76\u4fdd\u6301\u95ee\u5377\u8d28\u91cf\uff0c\u540c\u65f6\u901a\u8fc7\u5b89\u5168\u63aa\u65bd\u964d\u4f4e\u4e86\u90e8\u7f72\u98ce\u9669\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u751f\u6210\u5f0fAI\u53ef\u4ee5\u91cd\u5851IS\u6838\u5fc3\u65b9\u6cd5\uff0c\u5e76\u53ef\u901a\u8fc7\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\u548c\u4fdd\u969c\u63aa\u65bd\u5b9e\u73b0\u8d1f\u8d23\u4efb\u7684\u5e94\u7528\u3002"}}
{"id": "2506.14824", "pdf": "https://arxiv.org/pdf/2506.14824", "abs": "https://arxiv.org/abs/2506.14824", "authors": ["Yao Zhang", "Hewei Gao", "Haokun Chen", "Weiguo Li", "Yunpu Ma", "Volker Tresp"], "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.MM"], "comment": "12 pages, 3 figures", "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.", "AI": {"tldr": "FedNano \u662f\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u670d\u52a1\u5668\u4e0a\u96c6\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u5ba2\u6237\u7aef\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684 NanoEdge \u6a21\u5757\uff0c\u89e3\u51b3\u4e86 MLLMs \u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u90e8\u7f72\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u5206\u5e03\u5f0f\u591a\u6a21\u6001\u6570\u636e\u548c\u4e25\u683c\u7684\u9690\u79c1\u8981\u6c42\uff0c\u73b0\u6709\u7684 MLLMs \u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u9762\u4e34\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u9ad8\u3001\u5ba2\u6237\u7aef\u80fd\u529b\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "FedNano \u5728\u670d\u52a1\u5668\u4e2d\u592e\u5316 LLM\uff0c\u5ba2\u6237\u7aef\u90e8\u7f72\u8f7b\u91cf\u7ea7 NanoEdge \u6a21\u5757\uff0c\u5305\u542b\u6a21\u6001\u7279\u5b9a\u7684\u7f16\u7801\u5668\u3001\u8fde\u63a5\u5668\u548c\u4f4e\u79e9\u9002\u914d\u5668\uff0c\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u548c\u901a\u4fe1\u5f00\u9500\u3002", "result": "FedNano \u51cf\u5c11\u4e86 95% \u7684\u5ba2\u6237\u7aef\u5b58\u50a8\u9700\u6c42\uff0c\u901a\u4fe1\u5f00\u9500\u964d\u81f3\u6a21\u578b\u53c2\u6570\u7684 0.01%\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "FedNano \u5f25\u5408\u4e86 MLLMs \u89c4\u6a21\u4e0e\u8054\u90a6\u5b66\u4e60\u53ef\u884c\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5f0f\u591a\u6a21\u6001 AI \u7cfb\u7edf\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15028", "pdf": "https://arxiv.org/pdf/2506.15028", "abs": "https://arxiv.org/abs/2506.15028", "authors": ["Gargi Mitra", "Mohammadreza Hallajiyan", "Inji Kim", "Athish Pranav Dharmalingam", "Mohammed Elnawawy", "Shahrear Iqbal", "Karthik Pattabiraman", "Homa Alemzadeh"], "title": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices", "categories": ["cs.CR", "cs.ET", "cs.LG"], "comment": "32 pages, 6 figures, 6 tables", "summary": "The integration of AI/ML into medical devices is rapidly transforming\nhealthcare by enhancing diagnostic and treatment facilities. However, this\nadvancement also introduces serious cybersecurity risks due to the use of\ncomplex and often opaque models, extensive interconnectivity, interoperability\nwith third-party peripheral devices, Internet connectivity, and vulnerabilities\nin the underlying technologies. These factors contribute to a broad attack\nsurface and make threat prevention, detection, and mitigation challenging.\nGiven the highly safety-critical nature of these devices, a cyberattack on\nthese devices can cause the ML models to mispredict, thereby posing significant\nsafety risks to patients. Therefore, ensuring the security of these devices\nfrom the time of design is essential. This paper underscores the urgency of\naddressing the cybersecurity challenges in ML-enabled medical devices at the\npre-market phase. We begin by analyzing publicly available data on device\nrecalls and adverse events, and known vulnerabilities, to understand the threat\nlandscape of AI/ML-enabled medical devices and their repercussions on patient\nsafety. Building on this analysis, we introduce a suite of tools and techniques\ndesigned by us to assist security analysts in conducting comprehensive\npremarket risk assessments. Our work aims to empower manufacturers to embed\ncybersecurity as a core design principle in AI/ML-enabled medical devices,\nthereby making them safe for patients.", "AI": {"tldr": "AI/ML \u5728\u533b\u7597\u8bbe\u5907\u4e2d\u7684\u6574\u5408\u5e26\u6765\u5de8\u5927\u597d\u5904\uff0c\u4f46\u4e5f\u5f15\u5165\u4e25\u91cd\u7684\u7f51\u7edc\u5b89\u5168\u98ce\u9669\u3002\u672c\u6587\u5f3a\u8c03\u9700\u5728\u4e0a\u5e02\u524d\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u51fa\u5de5\u5177\u548c\u65b9\u6cd5\u4ee5\u5e2e\u52a9\u5b89\u5168\u5206\u6790\u5e08\u8fdb\u884c\u5168\u9762\u7684\u98ce\u9669\u8bc4\u4f30\u3002", "motivation": "\u533b\u7597\u8bbe\u5907\u7684AI/ML\u96c6\u6210\u589e\u52a0\u4e86\u5b89\u5168\u548c\u9690\u79c1\u98ce\u9669\uff0c\u4e9f\u9700\u4ece\u8bbe\u8ba1\u9636\u6bb5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u4fdd\u62a4\u60a3\u8005\u5b89\u5168\u3002", "method": "\u5206\u6790\u516c\u5f00\u7684\u8bbe\u5907\u53ec\u56de\u3001\u4e0d\u826f\u4e8b\u4ef6\u548c\u5df2\u77e5\u6f0f\u6d1e\u6570\u636e\uff0c\u5f00\u53d1\u5de5\u5177\u548c\u6280\u672f\u4ee5\u652f\u6301\u4e0a\u5e02\u524d\u98ce\u9669\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u4e00\u5957\u5de5\u5177\uff0c\u5e2e\u52a9\u5236\u9020\u5546\u5c06\u7f51\u7edc\u5b89\u5168\u4f5c\u4e3aAI/ML\u533b\u7597\u8bbe\u5907\u7684\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\u3002", "conclusion": "\u5728\u533b\u7597\u8bbe\u5907\u7684AI/ML\u96c6\u6210\u4e2d\uff0c\u4ece\u8bbe\u8ba1\u9636\u6bb5\u5d4c\u5165\u7f51\u7edc\u5b89\u5168\u662f\u786e\u4fdd\u60a3\u8005\u5b89\u5168\u7684\u5173\u952e\u3002"}}
{"id": "2506.15440", "pdf": "https://arxiv.org/pdf/2506.15440", "abs": "https://arxiv.org/abs/2506.15440", "authors": ["Omar Numan", "Gaurav Singh", "Kazybek Adam", "Jelin Leslin", "Aleksi Korsman", "Otto Simola", "Marko Kosunen", "Jussi Ryyn\u00e4nen", "Martin Andraud"], "title": "Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration", "categories": ["cs.AR"], "comment": "This work has been submitted to the IEEE for possible publication. 12\n  pages, 10 figures, 2 tables", "summary": "Developing accurate and reliable Compute-In-Memory (CIM) architectures is\nbecoming a key research focus to accelerate Artificial Intelligence (AI) tasks\non hardware, particularly Deep Neural Networks (DNNs). In that regard, there\nhas been significant interest in analog and mixed-signal CIM architectures\naimed at increasing the efficiency of data storage and computation to handle\nthe massive amount of data needed by DNNs. Specifically, resistive mixed-signal\nCIM cores are pushed by recent progresses in emerging Non-Volatile Memory\n(eNVM) solutions. Yet, mixed-signal CIM computing cores still face several\nintegration and reliability challenges that hinder their large-scale adoption\ninto end-to-end AI computing systems. In terms of integration, resistive and\neNVM-based CIM cores need to be integrated with a control processor to realize\nend-to-end AI acceleration. Moreover, SRAM-based CIM architectures are still\nmore efficient and easier to program than their eNVM counterparts. In terms of\nreliability, analog circuits are more susceptible to variations, leading to\ncomputation errors and degraded accuracy. This work addresses these two\nchallenges by proposing a self-calibrated mixed-signal CIM accelerator SoC,\nfabricated in 22-nm FDSOI technology. The integration is facilitated by (1) the\nCIM architecture, combining the density and ease of SRAM-based weight storage\nwith multi-bit computation using linear resistors, and (2) an open-source\nprogramming and testing strategy for CIM systems. The accuracy and reliability\nare enabled through an automated RISC-V controlled on-chip calibration,\nallowing us to improve the compute SNR by 25 to 45% across multiple columns to\nreach 18-24 dB. To showcase further integration possibilities, we show how our\nproof-of-concept SoC can be extended to recent high-density linear resistor\ntechnologies for enhanced computing performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6821\u51c6\u6df7\u5408\u4fe1\u53f7\u8ba1\u7b97\u5185\u5b58\u50a8\u5668\uff08CIM\uff09\u52a0\u901f\u5668SoC\uff0c\u4ee5\u89e3\u51b3CIM\u67b6\u6784\u5728\u96c6\u6210\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u6df7\u5408\u4fe1\u53f7CIM\u67b6\u6784\u5728AI\u4efb\u52a1\u4e2d\u5177\u6709\u9ad8\u6548\u6027\uff0c\u4f46\u9762\u4e34\u96c6\u6210\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u963b\u788d\u5176\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u91c7\u752822nm FDSOI\u6280\u672f\u5b9e\u73b0SoC\uff0c\u7ed3\u5408SRAM\u5bc6\u5ea6\u4e0e\u7ebf\u6027\u7535\u963b\u591a\u6bd4\u7279\u8ba1\u7b97\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u7f16\u7a0b\u548c\u6d4b\u8bd5\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u81ea\u52a8\u6821\u51c6\u63d0\u5347\u8ba1\u7b97\u4fe1\u566a\u6bd425-45%\uff0c\u8fbe\u523018-24dB\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3aCIM\u67b6\u6784\u7684\u5927\u89c4\u6a21\u96c6\u6210\u548c\u9ad8\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2506.15114", "pdf": "https://arxiv.org/pdf/2506.15114", "abs": "https://arxiv.org/abs/2506.15114", "authors": ["Youjia Li", "Robert Latham", "Robert Ross", "Ankit Agrawal", "Alok Choudhary", "Wei-Keng Liao"], "title": "Parallel Data Object Creation: Towards Scalable Metadata Management in High-Performance I/O Library", "categories": ["cs.DC"], "comment": null, "summary": "High-level I/O libraries, such as HDF5 and PnetCDF, are commonly used by\nlarge-scale scientific applications to perform I/O tasks in parallel. These I/O\nlibraries store the metadata such as data types and dimensionality along with\nthe raw data in the same files. While these libraries are well-optimized for\nconcurrent access to the raw data, they are designed neither to handle a large\nnumber of data objects efficiently nor to create different data objects\nindependently by multiple processes, as they require applications to call data\nobject creation APIs collectively with consistent metadata among all processes.\nApplications that process data gathered from remote sensors, such as particle\ncollision experiments in high-energy physics, may generate data of different\nsizes from different sensors and desire to store them as separate data objects.\nFor such applications, the I/O library's requirement on collective data object\ncreation can become very expensive, as the cost of metadata consistency check\nincreases with the metadata volume as well as the number of processes. To\naddress this limitation, using PnetCDF as an experimental platform, we\ninvestigate solutions in this paper that abide the netCDF file format, as well\nas propose a new file header format that enables independent data object\ncreation. The proposed file header consists of two sections, an index table and\na list of metadata blocks. The index table contains the reference to the\nmetadata blocks and each block stores metadata of objects that can be created\ncollectively or independently. The new design achieves a scalable performance,\ncutting data object creation times by up to 582x when running on 4096 MPI\nprocesses to create 5,684,800 data objects in parallel. Additionally, the new\nmethod reduces the memory footprints, with each process requiring an amount of\nmemory space inversely proportional to the number of processes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6587\u4ef6\u5934\u90e8\u683c\u5f0f\uff0c\u652f\u6301\u5e76\u884cI/O\u5e93\u4e2d\u72ec\u7acb\u521b\u5efa\u6570\u636e\u5bf9\u8c61\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5e76\u884cI/O\u5e93\u5982HDF5\u548cPnetCDF\u5728\u96c6\u4f53\u521b\u5efa\u6570\u636e\u5bf9\u8c61\u65f6\u6548\u7387\u4f4e\u4e14\u5185\u5b58\u5360\u7528\u9ad8\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u8981\u72ec\u7acb\u521b\u5efa\u5927\u91cf\u6570\u636e\u5bf9\u8c61\u7684\u9700\u6c42\u3002", "method": "\u4ee5PnetCDF\u4e3a\u5b9e\u9a8c\u5e73\u53f0\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u6587\u4ef6\u5934\u90e8\u683c\u5f0f\uff0c\u5305\u542b\u7d22\u5f15\u8868\u548c\u5143\u6570\u636e\u5757\u5217\u8868\uff0c\u652f\u6301\u72ec\u7acb\u6570\u636e\u5bf9\u8c61\u521b\u5efa\u3002", "result": "\u65b0\u8bbe\u8ba1\u5c06\u6570\u636e\u5bf9\u8c61\u521b\u5efa\u65f6\u95f4\u51cf\u5c11\u4e86582\u500d\uff084096\u4e2aMPI\u8fdb\u7a0b\u521b\u5efa5,684,800\u4e2a\u5bf9\u8c61\uff09\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u5360\u7528\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u5e76\u884cI/O\u5e93\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.15453", "pdf": "https://arxiv.org/pdf/2506.15453", "abs": "https://arxiv.org/abs/2506.15453", "authors": ["Yusuf Sulistyo Nugroho", "Farah Danisha Salam", "Brittany Reid", "Raula Gaikovina Kula", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Uncovering Intention through LLM-Driven Code Snippet Description Generation", "categories": ["cs.SE", "cs.AI"], "comment": "6 pages, 3 figures, 4 tables, conference paper", "summary": "Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f00\u53d1\u8005\u5e38\u7528\u7684\u4ee3\u7801\u7247\u6bb5\u63cf\u8ff0\u7c7b\u578b\uff0c\u5e76\u8bc4\u4f30\u4e86LLM\uff08\u5982Llama\uff09\u5728\u751f\u6210\u63cf\u8ff0\u65b9\u9762\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0c55.5%\u7684\u539f\u59cb\u63cf\u8ff0\u57fa\u4e8e\u793a\u4f8b\u4f7f\u7528\uff0cLLM\u7684\u6b63\u786e\u8bc6\u522b\u7387\u4e3a79.75%\uff0c\u751f\u6210\u63cf\u8ff0\u7684\u5e73\u5747\u76f8\u4f3c\u5f97\u5206\u4e3a0.7173\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5174\u8d77\uff0c\u7814\u7a76\u5f00\u53d1\u8005\u5e38\u7528\u7684\u4ee3\u7801\u7247\u6bb5\u63cf\u8ff0\u7c7b\u578b\uff0c\u5e76\u8bc4\u4f30LLM\u5728\u652f\u6301\u63cf\u8ff0\u751f\u6210\u65b9\u9762\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528NPM\u4ee3\u7801\u7247\u6bb5\u6570\u636e\u96c6\uff08185,412\u4e2a\u5305\uff0c1,024,579\u4e2a\u4ee3\u7801\u7247\u6bb5\uff09\uff0c\u624b\u52a8\u5206\u7c7b400\u4e2a\u6837\u672c\u63cf\u8ff0\uff0c\u5e76\u5229\u7528LLM\u751f\u6210\u63cf\u8ff0\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "55.5%\u7684\u539f\u59cb\u63cf\u8ff0\u4e3a\u793a\u4f8b\u4f7f\u7528\uff0cLLM\u6b63\u786e\u8bc6\u522b\u7387\u4e3a79.75%\uff1b\u751f\u6210\u63cf\u8ff0\u7684\u5e73\u5747\u76f8\u4f3c\u5f97\u5206\u4e3a0.7173\uff0c\u663e\u793a\u76f8\u5173\u6027\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u4ee3\u7801\u7247\u6bb5\u7684\u6587\u6863\u610f\u56fe\u53ef\u80fd\u56e0\u4efb\u52a1\u7c7b\u578b\u800c\u5f02\uff0cLLM\u5728\u63cf\u8ff0\u751f\u6210\u65b9\u9762\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u4ecd\u9700\u63d0\u5347\u7ec6\u8282\u548c\u76f8\u5173\u6027\u3002"}}
{"id": "2506.15571", "pdf": "https://arxiv.org/pdf/2506.15571", "abs": "https://arxiv.org/abs/2506.15571", "authors": ["Le Vu Anh", "Nguyen Viet Anh", "Mehmet Dik", "Tu Nguyen Thi Ngoc"], "title": "MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh Smoothing", "categories": ["cs.LG", "cs.GR"], "comment": "9 pages, 8 figures, 4 tables", "summary": "Real-time mesh smoothing at scale remains a formidable challenge: classical\nRicci-flow solvers demand costly global updates, while greedy heuristics suffer\nfrom slow convergence or brittle tuning. We present MicroRicci, the first truly\nself-tuning, local Ricci-flow solver that borrows ideas from coding theory and\npacks them into just 1K + 200 parameters. Its primary core is a greedy\nsyndrome-decoding step that pinpoints and corrects the largest curvature error\nin O(E) time, augmented by two tiny neural modules that adaptively choose\nvertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,\nMicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),\ntightens curvature spread from 0.19 to 0.185, and achieves a remarkable\nUV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per\niteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration\nover state-of-the-art methods. MicroRicci's combination of linear-time updates,\nautomatic hyperparameter adaptation, and high-quality geometric and perceptual\nresults makes it well suited for real-time, resource-limited applications in\ngraphics, simulation, and related fields.", "AI": {"tldr": "MicroRicci\u662f\u4e00\u79cd\u81ea\u8c03\u8282\u7684\u5c40\u90e8Ricci-flow\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u7f16\u7801\u7406\u8bba\u548c\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u683c\u5e73\u6ed1\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684Ricci-flow\u6c42\u89e3\u5668\u56e0\u5168\u5c40\u66f4\u65b0\u6210\u672c\u9ad8\u3001\u542f\u53d1\u5f0f\u65b9\u6cd5\u6536\u655b\u6162\u6216\u53c2\u6570\u8c03\u6574\u8106\u5f31\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5927\u89c4\u6a21\u7f51\u683c\u5e73\u6ed1\u7684\u9700\u6c42\u3002MicroRicci\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MicroRicci\u91c7\u7528\u8d2a\u5a6a\u7684\u7efc\u5408\u5f81\u89e3\u7801\u6b65\u9aa4\u5feb\u901f\u4fee\u6b63\u6700\u5927\u66f2\u7387\u8bef\u5dee\uff08O(E)\u65f6\u95f4\uff09\uff0c\u5e76\u5229\u7528\u4e24\u4e2a\u5fae\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u52a8\u6001\u9009\u62e9\u9876\u70b9\u548c\u6b65\u957f\u3002", "result": "\u5728110\u4e2aSJTU-TMQA\u7f51\u683c\u6d4b\u8bd5\u4e2d\uff0cMicroRicci\u5c06\u8fed\u4ee3\u6b21\u6570\u4ece950\u6b21\u964d\u81f3400\u6b21\uff082.4\u500d\u52a0\u901f\uff09\uff0c\u66f2\u7387\u5206\u5e03\u4ece0.19\u7f29\u5c0f\u81f30.185\uff0cUV\u7578\u53d8-MOS\u76f8\u5173\u6027\u8fbe-0.93\u3002", "conclusion": "MicroRicci\u56e0\u5176\u7ebf\u6027\u65f6\u95f4\u66f4\u65b0\u3001\u81ea\u52a8\u8d85\u53c2\u6570\u9002\u5e94\u548c\u9ad8\u8d28\u91cf\u51e0\u4f55\u4e0e\u611f\u77e5\u7ed3\u679c\uff0c\u9002\u7528\u4e8e\u56fe\u5f62\u3001\u4eff\u771f\u7b49\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2506.14820", "pdf": "https://arxiv.org/pdf/2506.14820", "abs": "https://arxiv.org/abs/2506.14820", "authors": ["Hyeon Jeon", "Hyunwook Lee", "Yun-Hsin Kuo", "Taehyun Yang", "Daniel Archambault", "Sungahn Ko", "Takanori Fujiwara", "Kwan-Liu Ma", "Jinwook Seo"], "title": "Navigating High-Dimensional Backstage: A Guide for Exploring Literature for the Reliable Use of Dimensionality Reduction", "categories": ["cs.HC", "cs.LG"], "comment": "EG/VGTC EuroVis 2025 Short paper", "summary": "Visual analytics using dimensionality reduction (DR) can easily be unreliable\nfor various reasons, e.g., inherent distortions in representing the original\ndata. The literature has thus proposed a wide range of methodologies to make\nDR-based visual analytics reliable. However, the diversity and extensiveness of\nthe literature can leave novice analysts and researchers uncertain about where\nto begin and proceed. To address this problem, we propose a guide for reading\npapers for reliable visual analytics with DR. Relying on the previous\nclassification of the relevant literature, our guide helps both practitioners\nto (1) assess their current DR expertise and (2) identify papers that will\nfurther enhance their understanding. Interview studies with three experts in DR\nand data visualizations validate the significance, comprehensiveness, and\nusefulness of our guide.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4efd\u6307\u5357\uff0c\u5e2e\u52a9\u65b0\u624b\u6cd5\u5e08\u548c\u7814\u7a76\u8005\u5728\u57fa\u4e8e\u964d\u7ef4\u7684\u53ef\u89c6\u5316\u5206\u6790\u4e2d\u9009\u62e9\u548c\u9605\u8bfb\u6587\u732e\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u9a8c\u8bc1\u4e86\u6307\u5357\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u964d\u7ef4\u53ef\u89c6\u5316\u5206\u6790\u4e2d\u7684\u4e0d\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5e2e\u52a9\u521d\u5b66\u8005\u89e3\u51b3\u6587\u732e\u9009\u62e9\u548c\u7406\u89e3\u4e0a\u7684\u56f0\u60d1\u3002", "method": "\u57fa\u4e8e\u5df2\u6709\u6587\u732e\u5206\u7c7b\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4efd\u9605\u8bfb\u6307\u5357\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u9a8c\u8bc1\u3002", "result": "\u6307\u5357\u5f97\u5230\u4e86\u4e13\u5bb6\u8ba4\u53ef\uff0c\u5177\u6709\u663e\u8457\u6027\u3001\u5168\u9762\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6307\u5357\u4e3a\u521d\u5b66\u8005\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u964d\u7ef4\u53ef\u89c6\u5316\u5206\u6790\u5b66\u4e60\u8def\u5f84\u3002"}}
{"id": "2506.15154", "pdf": "https://arxiv.org/pdf/2506.15154", "abs": "https://arxiv.org/abs/2506.15154", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS", "68T10 (Primary), 68T50 (Secondary)", "H.5.5; H.5.1; I.2.7"], "comment": "14 pages, 2 figures, Accepted to AIMC 2025", "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u97f3\u4e50\u5b57\u5e55\u6a21\u578bSonicVerse\uff0c\u901a\u8fc7\u7ed3\u5408\u5b57\u5e55\u751f\u6210\u4e0e\u8f85\u52a9\u97f3\u4e50\u7279\u5f81\u68c0\u6d4b\u4efb\u52a1\uff0c\u751f\u6210\u4e30\u5bcc\u4e14\u63cf\u8ff0\u6027\u7684\u97f3\u4e50\u7247\u6bb5\u63cf\u8ff0\u3002", "motivation": "\u4e3a\u4e30\u5bcc\u97f3\u4e50\u6570\u636e\u5e93\u5e76\u63a8\u52a8\u97f3\u4e50AI\u7814\u7a76\uff0c\u9700\u751f\u6210\u51c6\u786e\u53cd\u6620\u97f3\u4e50\u7279\u6027\u7684\u8be6\u7ec6\u5b57\u5e55\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6295\u5f71\u7684\u67b6\u6784\uff0c\u5c06\u97f3\u9891\u8f93\u5165\u8f6c\u6362\u4e3a\u8bed\u8a00\u6807\u8bb0\uff0c\u540c\u65f6\u901a\u8fc7\u8f85\u52a9\u5934\u68c0\u6d4b\u97f3\u4e50\u7279\u5f81\uff0c\u5e76\u5c06\u7279\u5f81\u6295\u5f71\u4e3a\u8bed\u8a00\u6807\u8bb0\u4ee5\u589e\u5f3a\u5b57\u5e55\u8f93\u5165\u3002\u6269\u5c55MusicBench\u6570\u636e\u96c6\uff0c\u4f7f\u7528MIRFLEX\u63d0\u53d6\u97f3\u4e50\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u6574\u5408\u97f3\u4e50\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u751f\u6210\u5b57\u5e55\u7684\u8d28\u91cf\u4e0e\u7ec6\u8282\u3002", "conclusion": "SonicVerse\u4e0d\u4ec5\u80fd\u751f\u6210\u77ed\u97f3\u4e50\u7247\u6bb5\u7684\u8be6\u7ec6\u63cf\u8ff0\uff0c\u8fd8\u80fd\u4e3a\u957f\u97f3\u4e50\u7247\u6bb5\u751f\u6210\u65f6\u95f4\u4fe1\u606f\u63cf\u8ff0\uff0c\u5c55\u793a\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u97f3\u4e50\u5b57\u5e55\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.15070", "pdf": "https://arxiv.org/pdf/2506.15070", "abs": "https://arxiv.org/abs/2506.15070", "authors": ["Rasha Karakchi", "Rye Stahle-Smith", "Nishant Chinnasami", "Tiffany Yu"], "title": "Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine", "categories": ["cs.CR", "cs.ET"], "comment": "This is submitted to the ACM/IEEE Symposium on Edge Computing (SEC\n  2025)", "summary": "The exponential growth of Internet of Things (IoT) applications has\nintensified the demand for efficient, high-throughput, and energy-efficient\ndata processing at the edge. Conventional CPU-centric encryption methods suffer\nfrom performance bottlenecks and excessive data movement, especially in\nlatency-sensitive and resource-constrained environments. In this paper, we\npresent SPiME, a lightweight, scalable, and FPGA-compatible Secure\nProcessor-in-Memory Encryption architecture that integrates the Advanced\nEncryption Standard (AES-128) directly into a Processing-in-Memory (PiM)\nframework. SPiME is designed as a modular array of parallel PiM units, each\ncombining an AES core with a minimal control unit to enable distributed\nin-place encryption with minimal overhead. The architecture is fully\nimplemented in Verilog and tested on multiple AMD UltraScale and UltraScale+\nFPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units\nwhile maintaining less than 5\\% utilization of key FPGA resources on high-end\ndevices. It delivers over 25~Gbps in sustained encryption throughput with\npredictable, low-latency performance. The design's portability,\nconfigurability, and resource efficiency make it a compelling solution for\nsecure edge computing, embedded cryptographic systems, and customizable\nhardware accelerators.", "AI": {"tldr": "SPiME\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u4e14\u517c\u5bb9FPGA\u7684\u5b89\u5168\u5b58\u50a8\u5668\u5185\u5904\u7406\u52a0\u5bc6\u67b6\u6784\uff0c\u901a\u8fc7\u96c6\u6210AES-128\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u52a0\u5bc6\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u548c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u3002", "motivation": "\u7269\u8054\u7f51\u5e94\u7528\u7684\u5feb\u901f\u589e\u957f\u5bf9\u8fb9\u7f18\u8ba1\u7b97\u7684\u9ad8\u6548\u3001\u9ad8\u541e\u5410\u91cf\u548c\u8282\u80fd\u6570\u636e\u5904\u7406\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u4f20\u7edfCPU\u52a0\u5bc6\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u74f6\u9888\u548c\u6570\u636e\u79fb\u52a8\u8fc7\u591a\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u5e76\u884c\u5b58\u50a8\u5668\u5185\u5904\u7406\u5355\u5143\u9635\u5217\uff0c\u6bcf\u4e2a\u5355\u5143\u7ed3\u5408AES\u6838\u5fc3\u548c\u6700\u5c0f\u63a7\u5236\u5355\u5143\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u5c31\u5730\u52a0\u5bc6\uff0c\u5e76\u5728FPGA\u4e0a\u5b9e\u73b0\u548c\u6d4b\u8bd5\u3002", "result": "SPiME\u53ef\u4ee5\u5728\u9ad8\u7aefFPGA\u4e0a\u6269\u5c55\u5230\u8d85\u8fc74000\u4e2a\u5e76\u884c\u5355\u5143\uff0c\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e8e5%\uff0c\u63d0\u4f9b\u6301\u7eed\u52a0\u5bc6\u541e\u5410\u91cf\u8d85\u8fc725 Gbps\uff0c\u4e14\u5ef6\u8fdf\u4f4e\u4e14\u53ef\u9884\u6d4b\u3002", "conclusion": "SPiME\u7684\u4fbf\u643a\u6027\u3001\u53ef\u914d\u7f6e\u6027\u548c\u8d44\u6e90\u6548\u7387\u4f7f\u5176\u6210\u4e3a\u5b89\u5168\u8fb9\u7f18\u8ba1\u7b97\u3001\u5d4c\u5165\u5f0f\u52a0\u5bc6\u7cfb\u7edf\u548c\u53ef\u5b9a\u5236\u786c\u4ef6\u52a0\u901f\u5668\u7684\u7406\u60f3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15601", "pdf": "https://arxiv.org/pdf/2506.15601", "abs": "https://arxiv.org/abs/2506.15601", "authors": ["Donghyun Gouk", "Seungkwan Kang", "Seungjun Lee", "Jiseon Kim", "Kyungkuk Nam", "Eojin Ryu", "Sangwon Lee", "Dongpyung Kim", "Junhyeok Jang", "Hanyeoreum Bae", "Myoungsoo Jung"], "title": "CXL-GPU: Pushing GPU Memory Boundaries with the Integration of CXL Technologies", "categories": ["cs.AR"], "comment": null, "summary": "This work introduces a GPU storage expansion solution utilizing CXL,\nfeaturing a novel GPU system design with multiple CXL root ports for\nintegrating diverse storage media (DRAMs and/or SSDs). We developed and\nsiliconized a custom CXL controller integrated at the hardware RTL level,\nachieving two-digit nanosecond roundtrip latency, the first in the field. This\nstudy also includes speculative read and deterministic store mechanisms to\nefficiently manage read and write operations to hide the endpoint's backend\nmedia latency variation. Performance evaluations reveal our approach\nsignificantly outperforms existing methods, marking a substantial advancement\nin GPU storage technology.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCXL\u7684GPU\u5b58\u50a8\u6269\u5c55\u65b9\u6848\uff0c\u521b\u65b0\u8bbe\u8ba1\u591aCXL\u6839\u7aef\u53e3\u96c6\u6210\u591a\u79cd\u5b58\u50a8\u4ecb\u8d28\uff0c\u9996\u6b21\u5b9e\u73b0\u4e24\u4f4d\u6570\u7eb3\u79d2\u7ea7\u5ef6\u8fdf\uff0c\u901a\u8fc7\u63a8\u6d4b\u8bfb\u548c\u786e\u5b9a\u6027\u5199\u673a\u5236\u4f18\u5316\u8bfb\u5199\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3GPU\u5b58\u50a8\u6269\u5c55\u7684\u5ef6\u8fdf\u548c\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u5347\u5b58\u50a8\u6280\u672f\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u591aCXL\u6839\u7aef\u53e3\u7684GPU\u7cfb\u7edf\uff0c\u5f00\u53d1\u5b9a\u5236CXL\u63a7\u5236\u5668\uff0c\u96c6\u6210\u63a8\u6d4b\u8bfb\u548c\u786e\u5b9a\u6027\u5199\u673a\u5236\u3002", "result": "\u5b9e\u73b0\u4e24\u4f4d\u6570\u7eb3\u79d2\u7ea7\u5ef6\u8fdf\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3aGPU\u5b58\u50a8\u6280\u672f\u5e26\u6765\u91cd\u5927\u8fdb\u6b65\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.15155", "pdf": "https://arxiv.org/pdf/2506.15155", "abs": "https://arxiv.org/abs/2506.15155", "authors": ["Jiale Xu", "Rui Zhang", "Yi Xiong", "Cong Guo", "Zihan Liu", "Yangjie Zhou", "Weiming Hu", "Hao Wu", "Changxu Shao", "Ziqing Wang", "Yongjie Yuan", "Junping Zhao", "Minyi Guo", "Jingwen Leng"], "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving", "categories": ["cs.DC"], "comment": null, "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.", "AI": {"tldr": "eLLM\u63d0\u51fa\u4e86\u4e00\u79cd\u5f39\u6027\u5185\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u5f20\u91cf\u62bd\u8c61\u548c\u52a8\u6001\u5185\u5b58\u8c03\u6574\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u52a8\u6001\u5185\u5b58\u7ba1\u7406\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u6279\u5904\u7406\u89c4\u6a21\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u4e2d\u5fc3\u90e8\u7f72\u65f6\uff0c\u52a8\u6001\u6fc0\u6d3b\u548cKV\u7f13\u5b58\u7684\u5185\u5b58\u7ba1\u7406\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u541e\u5410\u91cf\u4e0b\u964d\uff0ceLLM\u65e8\u5728\u4f18\u5316\u8fd9\u4e00\u95ee\u9898\u3002", "method": "eLLM\u91c7\u7528\u865a\u62df\u5f20\u91cf\u62bd\u8c61\u3001\u5f39\u6027\u5185\u5b58\u673a\u5236\u548c\u8f7b\u91cf\u7ea7\u8c03\u5ea6\u7b56\u7565\uff0c\u52a8\u6001\u7ba1\u7406\u5185\u5b58\u5e76\u5e73\u8861\u6027\u80fd\u3002", "result": "eLLM\u7684\u541e\u5410\u91cf\u63d0\u5347\u4e862.32\u500d\uff0c\u652f\u6301128K-token\u8f93\u5165\u7684\u6279\u5904\u7406\u89c4\u6a21\u6269\u5927\u4e863\u500d\u3002", "conclusion": "eLLM\u901a\u8fc7\u7edf\u4e00\u548c\u5f39\u6027\u7684\u5185\u5b58\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u8fd0\u884c\u65f6\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.15655", "pdf": "https://arxiv.org/pdf/2506.15655", "abs": "https://arxiv.org/abs/2506.15655", "authors": ["Yilin Zhang", "Xinran Zhao", "Zora Zhiruo Wang", "Chenyang Yang", "Jiayi Wei", "Tongshuang Wu"], "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree", "categories": ["cs.SE"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become essential for large-scale\ncode generation, grounding predictions in external code corpora to improve\nactuality. However, a critical yet underexplored aspect of RAG pipelines is\nchunking -- the process of dividing documents into retrievable units. Existing\nline-based chunking heuristics often break semantic structures, splitting\nfunctions or merging unrelated code, which can degrade generation quality. We\npropose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method\nthat recursively breaks large AST nodes into smaller chunks and merges sibling\nnodes while respecting size limits. This approach generates self-contained,\nsemantically coherent units across programming languages and tasks, improving\nperformance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3\npoints on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.\nOur work highlights the importance of structure-aware chunking for scaling\nretrieval-enhanced code intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u7684\u4ee3\u7801\u5206\u5757\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u884c\u7684\u5206\u5757\u65b9\u6cd5\u4f1a\u7834\u574f\u4ee3\u7801\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u5206\u5757\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u62bd\u8c61\u8bed\u6cd5\u6811\u9012\u5f52\u5206\u89e3\u5927\u578bAST\u8282\u70b9\uff0c\u5e76\u5728\u5927\u5c0f\u9650\u5236\u5185\u5408\u5e76\u5144\u5f1f\u8282\u70b9\uff0c\u751f\u6210\u8bed\u4e49\u8fde\u8d2f\u7684\u5206\u5757\u5355\u5143\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982\u5728RepoEval\u68c0\u7d22\u4e2dRecall@5\u63d0\u53474.3\u70b9\uff0c\u5728SWE-bench\u751f\u6210\u4e2dPass@1\u63d0\u53472.67\u70b9\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u5206\u5757\u5bf9\u6269\u5c55\u68c0\u7d22\u589e\u5f3a\u4ee3\u7801\u667a\u80fd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.14829", "pdf": "https://arxiv.org/pdf/2506.14829", "abs": "https://arxiv.org/abs/2506.14829", "authors": ["Aditya Majumdar", "Wenbo Zhang", "Kashvi Prawal", "Amulya Yadav"], "title": "The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects\nfocus on harnessing AI to address societal issues in areas such as healthcare,\nsocial justice, etc. Unfortunately, despite growing interest in AI4SI,\nachieving tangible, on-the-ground impact remains a significant challenge. For\nexample, identifying and engaging motivated collaborators who are willing to\nco-design and deploy AI based solutions in real-world settings is often\ndifficult. Even when such partnerships are established, many AI4SI projects\n\"fail\" to progress beyond the proof-of-concept stage, and hence, are unable to\ntransition to at-scale production-level solutions. Furthermore, the unique\nchallenges faced by AI4SI researchers are not always fully recognized within\nthe broader AI community, where such work is sometimes viewed as primarily\napplied and not aligning with the traditional criteria for novelty emphasized\nin core AI venues. This paper attempts to shine a light on the diverse\nchallenges faced in AI4SI research by diagnosing a multitude of factors that\nprevent AI4SI partnerships from achieving real-world impact on the ground.\nDrawing on semi-structured interviews with six leading AI4SI researchers -\ncomplemented by the authors' own lived experiences in conducting AI4SI research\n- this paper attempts to understand the day-to-day difficulties faced in\ndeveloping and deploying socially impactful AI solutions. Through thematic\nanalysis, we identify structural and organizational, communication,\ncollaboration, and operational challenges as key barriers to deployment. While\nthere are no easy fixes, we synthesize best practices and actionable strategies\ndrawn from these interviews and our own work in this space. In doing so, we\nhope this paper serves as a practical reference guide for AI4SI researchers and\npartner organizations seeking to engage more effectively in socially impactful\nAI collaborations.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86AI4SI\u9879\u76ee\u5982\u4f55\u5229\u7528AI\u89e3\u51b3\u793e\u4f1a\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u9762\u4e34\u5b9e\u9645\u843d\u5730\u56f0\u96be\u3001\u5408\u4f5c\u6311\u6218\u4ee5\u53ca\u5b66\u672f\u8ba4\u53ef\u5ea6\u4f4e\u7684\u56f0\u5883\uff0c\u5e76\u901a\u8fc7\u91c7\u8bbf\u548c\u5b9e\u8df5\u7ecf\u9a8c\u5206\u6790\u4e86\u969c\u788d\u6240\u5728\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u8bc6\u522bAI4SI\u9879\u76ee\u4e2d\u963b\u788d\u5b9e\u9645\u5f71\u54cd\u7684\u6311\u6218\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5408\u4f5c\u7ec4\u7ec7\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\u3002", "method": "\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u4f5c\u8005\u81ea\u8eab\u7ecf\u9a8c\u8fdb\u884c\u4e3b\u9898\u5206\u6790\u3002", "result": "\u53d1\u73b0\u7ed3\u6784\u6027\u3001\u7ec4\u7ec7\u6027\u3001\u6c9f\u901a\u3001\u534f\u4f5c\u548c\u64cd\u4f5c\u5c42\u9762\u7684\u6311\u6218\u662f\u4e3b\u8981\u969c\u788d\uff0c\u5e76\u603b\u7ed3\u4e86\u5e94\u5bf9\u7b56\u7565\u3002", "conclusion": "\u8bba\u6587\u4e3aAI4SI\u7814\u7a76\u8005\u4e0e\u5408\u4f5c\u4f19\u4f34\u63d0\u4f9b\u4e86\u5e94\u5bf9\u6311\u6218\u7684\u5b9e\u7528\u53c2\u8003\u3002"}}
{"id": "2506.15228", "pdf": "https://arxiv.org/pdf/2506.15228", "abs": "https://arxiv.org/abs/2506.15228", "authors": ["Yufeng Zhang", "Wenrui Dai", "Hang Yu", "Shizhan Liu", "Junhui Hou", "Jianguo Li", "Weiyao Lin"], "title": "ABC: Adaptive BayesNet Structure Learning for Computational Scalable Multi-task Image Compression", "categories": ["eess.IV", "cs.MM"], "comment": null, "summary": "Neural Image Compression (NIC) has revolutionized image compression with its\nsuperior rate-distortion performance and multi-task capabilities, supporting\nboth human visual perception and machine vision tasks. However, its widespread\nadoption is hindered by substantial computational demands. While existing\napproaches attempt to address this challenge through module-specific\noptimizations or pre-defined complexity levels, they lack comprehensive control\nover computational complexity. We present ABC (Adaptive BayesNet structure\nlearning for computational scalable multi-task image Compression), a novel,\ncomprehensive framework that achieves computational scalability across all NIC\ncomponents through Bayesian network (BayesNet) structure learning. ABC\nintroduces three key innovations: (i) a heterogeneous bipartite BayesNet\n(inter-node structure) for managing neural backbone computations; (ii) a\nhomogeneous multipartite BayesNet (intra-node structure) for optimizing\nautoregressive unit processing; and (iii) an adaptive control module that\ndynamically adjusts the BayesNet structure based on device capabilities, input\ndata complexity, and downstream task requirements. Experiments demonstrate that\nABC enables full computational scalability with better complexity adaptivity\nand broader complexity control span, while maintaining competitive compression\nperformance. Furthermore, the framework's versatility allows integration with\nvarious NIC architectures that employ BayesNet representations, making it a\nrobust solution for ensuring computational scalability in NIC applications.\nCode is available in https://github.com/worldlife123/cbench_BaSIC.", "AI": {"tldr": "ABC\u6846\u67b6\u901a\u8fc7\u8d1d\u53f6\u65af\u7f51\u7edc\u7ed3\u6784\u5b66\u4e60\u5b9e\u73b0\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u7684\u5168\u9762\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\uff08NIC\uff09\u56e0\u9ad8\u8ba1\u7b97\u9700\u6c42\u800c\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faABC\u6846\u67b6\uff0c\u5305\u542b\u5f02\u6784\u4e8c\u5206\u8d1d\u53f6\u65af\u7f51\u7edc\uff08\u7ba1\u7406\u8ba1\u7b97\uff09\u3001\u540c\u6784\u591a\u5206\u8d1d\u53f6\u65af\u7f51\u7edc\uff08\u4f18\u5316\u5904\u7406\uff09\u548c\u81ea\u9002\u5e94\u63a7\u5236\u6a21\u5757\u3002", "result": "ABC\u5b9e\u73b0\u4e86\u5168\u9762\u7684\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\uff0c\u4fdd\u6301\u538b\u7f29\u6027\u80fd\u7684\u540c\u65f6\uff0c\u9002\u5e94\u6027\u66f4\u5f3a\u3002", "conclusion": "ABC\u662f\u4e00\u79cd\u591a\u529f\u80fd\u3001\u8ba1\u7b97\u53ef\u6269\u5c55\u7684NIC\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u67b6\u6784\u3002"}}
{"id": "2506.15613", "pdf": "https://arxiv.org/pdf/2506.15613", "abs": "https://arxiv.org/abs/2506.15613", "authors": ["Miryeong Kwon", "Donghyun Gouk", "Junhyeok Jang", "Jinwoo Baek", "Hyunwoo You", "Sangyoon Ji", "Hongjoo Jung", "Junseok Moon", "Seungkwan Kang", "Seungjun Lee", "Myoungsoo Jung"], "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and Instruction Annotation", "categories": ["cs.AR"], "comment": null, "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528Compute Express Link (CXL)\u5c06\u57fa\u4e8ePCIe\u7684\u5757\u5b58\u50a8\u8f6c\u53d8\u4e3a\u53ef\u6269\u5c55\u7684\u5b57\u8282\u53ef\u5bfb\u5740\u5de5\u4f5c\u5185\u5b58\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u7f13\u5b58\u6027\u548cType 3\u7ec8\u7aef\u8bbe\u5907\uff08CXL-SSD\uff09\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u8bbe\u8ba1\u548c\u6a21\u62df\u9a8c\u8bc1\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5c06\u5757\u5b58\u50a8\u9002\u914d\u5230CXL\u5185\u5b58\u4e2d\u5fc3\u6a21\u578b\u7684\u6311\u6218\uff0c\u63a8\u52a8\u5b58\u50a8\u4e0e\u5185\u5b58\u7684\u878d\u5408\u3002", "method": "\u91c7\u7528\u7f13\u5b58\u6027\u4f5c\u4e3a\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u63d0\u51faType 3\u7ec8\u7aef\u8bbe\u5907CXL-SSD\uff0c\u5e76\u5728FPGA\u5e73\u53f0\u4e0a\u6784\u5efa\u539f\u578b\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\uff08Determinism\uff09\u548c\u53ef\u7f13\u51b2\u6027\uff08Bufferability\uff09\u6807\u6ce8\u673a\u5236\u4f18\u5316\u6027\u80fd\u3002", "result": "CXL-SSD\u6027\u80fd\u6bd4PCIe\u5185\u5b58\u6269\u5c55\u5668\u63d0\u534710.9\u500d\uff0c\u6807\u6ce8\u673a\u5236\u8fdb\u4e00\u6b65\u964d\u4f4e\u5ef6\u8fdf5.4\u500d\uff0c\u5728\u5c40\u90e8\u6027\u9ad8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u63a5\u8fd1DRAM\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5757\u5b58\u50a8\u4e0eCXL\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u5185\u5b58\u5b58\u50a8\u878d\u5408\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.15418", "pdf": "https://arxiv.org/pdf/2506.15418", "abs": "https://arxiv.org/abs/2506.15418", "authors": ["Nick Brown"], "title": "RISC-V for HPC: An update of where we are and main action points", "categories": ["cs.DC"], "comment": "Extended abstract accepted to the RISC-V Summit Europe 2025", "summary": "This extended abstract is submitted on behalf of the RISC-V HPC SIG who have\nbeen undertaking an analysis to explore the current state and limitations of\nthe RISC-V ecosystem for HPC. Whilst it is right to celebrate that there has\nbeen great progress made in recent years, we also highlight limitations and\nwhere effort should be focussed.", "AI": {"tldr": "RISC-V HPC SIG\u5206\u6790\u4e86RISC-V\u751f\u6001\u7cfb\u7edf\u5728HPC\u9886\u57df\u7684\u73b0\u72b6\u548c\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1RISC-V\u5728HPC\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u5173\u6ce8\u5176\u5c40\u9650\u6027\u4ee5\u63a8\u52a8\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524d\u751f\u6001\u7cfb\u7edf\uff0c\u8bc6\u522b\u5173\u952e\u95ee\u9898\u548c\u672a\u6765\u52aa\u529b\u65b9\u5411\u3002", "result": "\u660e\u786e\u4e86RISC-V\u5728HPC\u4e2d\u7684\u8fdb\u5c55\u548c\u5f85\u89e3\u51b3\u7684\u74f6\u9888\u3002", "conclusion": "\u9700\u8981\u96c6\u4e2d\u7cbe\u529b\u89e3\u51b3RISC-V\u751f\u6001\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u4ee5\u652f\u6301HPC\u5e94\u7528\u3002"}}
{"id": "2506.15648", "pdf": "https://arxiv.org/pdf/2506.15648", "abs": "https://arxiv.org/abs/2506.15648", "authors": ["Georgios Androutsopoulos", "Antonio Bianchi"], "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses", "categories": ["cs.CR", "cs.LG", "cs.SE"], "comment": null, "summary": "Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.", "AI": {"tldr": "deepSURF\u662f\u4e00\u4e2a\u7ed3\u5408\u9759\u6001\u5206\u6790\u548cLLM\u5f15\u5bfc\u7684\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\uff0c\u7528\u4e8e\u68c0\u6d4bRust\u5e93\u4e2d\u7684\u5185\u5b58\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u9488\u5bf9unsafe\u4ee3\u7801\u3002\u901a\u8fc7\u5904\u7406\u6cdb\u578b\u548c\u52a8\u6001\u589e\u5f3a\u6a21\u7cca\u6d4b\u8bd5\uff0c\u5de5\u5177\u572827\u4e2aRust\u5e93\u4e2d\u53d1\u73b020\u4e2a\u5df2\u77e5\u548c6\u4e2a\u65b0\u6f0f\u6d1e\uff0c\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4bRust\u5185\u5b58\u6f0f\u6d1e\u7684\u5de5\u5177\u80fd\u529b\u6709\u9650\uff0c\u672a\u80fd\u5145\u5206\u5904\u7406Rust\u7279\u6709\u7c7b\u578b\u6216\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u3002", "method": "deepSURF\u96c6\u6210\u9759\u6001\u5206\u6790\u548cLLM\u5f15\u5bfc\u7684\u6a21\u7cca\u6d4b\u8bd5\u751f\u6210\uff0c\u901a\u8fc7\u66ff\u6362\u6cdb\u578b\u548c\u52a8\u6001\u751f\u6210\u6a21\u7cca\u6d4b\u8bd5\u4ee3\u7801\u6765\u6a21\u62df\u7528\u6237\u884c\u4e3a\u3002", "result": "\u572827\u4e2aRust\u5e93\u4e2d\u6210\u529f\u53d1\u73b020\u4e2a\u5df2\u77e5\u548c6\u4e2a\u65b0\u6f0f\u6d1e\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "conclusion": "deepSURF\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86Rust\u5185\u5b58\u6f0f\u6d1e\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2506.14948", "pdf": "https://arxiv.org/pdf/2506.14948", "abs": "https://arxiv.org/abs/2506.14948", "authors": ["Mohna Chakraborty", "Lu Wang", "David Jurgens"], "title": "Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework", "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in domains requiring\nmoral understanding, yet their reasoning often remains shallow, and misaligned\nwith human reasoning. Unlike humans, whose moral reasoning integrates\ncontextual trade-offs, value systems, and ethical theories, LLMs often rely on\nsurface patterns, leading to biased decisions in morally and ethically complex\nscenarios. To address this gap, we present a value-grounded framework for\nevaluating and distilling structured moral reasoning in LLMs. We benchmark 12\nopen-source models across four moral datasets using a taxonomy of prompts\ngrounded in value systems, ethical theories, and cognitive reasoning\nstrategies. Our evaluation is guided by four questions: (1) Does reasoning\nimprove LLM decision-making over direct prompting? (2) Which types of\nvalue/ethical frameworks most effectively guide LLM reasoning? (3) Which\ncognitive reasoning strategies lead to better moral performance? (4) Can\nsmall-sized LLMs acquire moral competence through distillation? We find that\nprompting with explicit moral structure consistently improves accuracy and\ncoherence, with first-principles reasoning and Schwartz's + care-ethics\nscaffolds yielding the strongest gains. Furthermore, our supervised\ndistillation approach transfers moral competence from large to small models\nwithout additional inference cost. Together, our results offer a scalable path\ntoward interpretable and value-grounded models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ef7\u503c\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u63d0\u70bc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u7ed3\u6784\u5316\u9053\u5fb7\u63a8\u7406\uff0c\u53d1\u73b0\u663e\u5f0f\u9053\u5fb7\u7ed3\u6784\u548c\u7279\u5b9a\u63a8\u7406\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b0\u5c0f\u578b\u6a21\u578b\u7684\u9053\u5fb7\u80fd\u529b\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u9700\u8981\u9053\u5fb7\u7406\u89e3\u7684\u9886\u57df\u4e2d\u8868\u73b0\u80a4\u6d45\uff0c\u4e14\u4e0e\u4eba\u7c7b\u63a8\u7406\u4e0d\u4e00\u81f4\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4ef7\u503c\u7cfb\u7edf\u7684\u63d0\u793a\u5206\u7c7b\u6cd5\uff0c\u5bf912\u4e2a\u5f00\u6e90\u6a21\u578b\u5728\u56db\u4e2a\u9053\u5fb7\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u76d1\u7763\u84b8\u998f\u65b9\u6cd5\u3002", "result": "\u663e\u5f0f\u9053\u5fb7\u7ed3\u6784\u63d0\u793a\uff08\u5982\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u7406\u548c\u7279\u5b9a\u4f26\u7406\u6846\u67b6\uff09\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u84b8\u998f\u65b9\u6cd5\u6210\u529f\u5c06\u9053\u5fb7\u80fd\u529b\u4ece\u5927\u6a21\u578b\u8fc1\u79fb\u5230\u5c0f\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u53ef\u89e3\u91ca\u4e14\u57fa\u4e8e\u4ef7\u503c\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2506.15276", "pdf": "https://arxiv.org/pdf/2506.15276", "abs": "https://arxiv.org/abs/2506.15276", "authors": ["Jun Zhu", "Xinfeng Zhang", "Lv Tang", "JunHao Jiang"], "title": "MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": null, "summary": "Implicit Neural representations (INRs) have emerged as a promising approach\nfor video compression, and have achieved comparable performance to the\nstate-of-the-art codecs such as H.266/VVC. However, existing INR-based methods\nstruggle to effectively represent detail-intensive and fast-changing video\ncontent. This limitation mainly stems from the underutilization of internal\nnetwork features and the absence of video-specific considerations in network\ndesign. To address these challenges, we propose a multi-scale feature fusion\nframework, MSNeRV, for neural video representation. In the encoding stage, we\nenhance temporal consistency by employing temporal windows, and divide the\nvideo into multiple Groups of Pictures (GoPs), where a GoP-level grid is used\nfor background representation. Additionally, we design a multi-scale spatial\ndecoder with a scale-adaptive loss function to integrate multi-resolution and\nmulti-frequency information. To further improve feature extraction, we\nintroduce a multi-scale feature block that fully leverages hidden features. We\nevaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and\ncompression. Experimental results demonstrate that our model exhibits superior\nrepresentation capability among INR-based approaches and surpasses VTM-23.7\n(Random Access) in dynamic scenarios in terms of compression efficiency.", "AI": {"tldr": "MSNeRV\u662f\u7528\u4e8e\u795e\u7ecf\u89c6\u9891\u8868\u793a\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709INR\u65b9\u6cd5\u5728\u7ec6\u8282\u5bc6\u96c6\u548c\u5feb\u901f\u53d8\u5316\u5185\u5bb9\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eINR\u7684\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\u5728\u7ec6\u8282\u5bc6\u96c6\u548c\u5feb\u901f\u53d8\u5316\u5185\u5bb9\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u672a\u5145\u5206\u5229\u7528\u7f51\u7edc\u5185\u90e8\u7279\u5f81\u4e14\u8bbe\u8ba1\u7f3a\u4e4f\u89c6\u9891\u7279\u5b9a\u8003\u8651\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6846\u67b6MSNeRV\uff0c\u5305\u62ec\u65f6\u95f4\u7a97\u53e3\u589e\u5f3a\u65f6\u5e8f\u4e00\u81f4\u6027\u3001GoP\u7ea7\u7f51\u683c\u8868\u793a\u80cc\u666f\u3001\u591a\u5c3a\u5ea6\u7a7a\u95f4\u89e3\u7801\u5668\u53ca\u81ea\u9002\u5e94\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMSNeRV\u5728INR\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u538b\u7f29\u6548\u7387\u5728\u52a8\u6001\u573a\u666f\u4e2d\u8d85\u8fc7VTM-23.7\u3002", "conclusion": "MSNeRV\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8868\u793a\u548c\u538b\u7f29\u80fd\u529b\u3002"}}
{"id": "2506.15634", "pdf": "https://arxiv.org/pdf/2506.15634", "abs": "https://arxiv.org/abs/2506.15634", "authors": ["Hasnain A. Ziad", "Alexander C. Bodoh", "Ashiq A. Sakib"], "title": "SR-NCL: an Area-/Energy-Efficient Resilient NCL Architecture Based on Selective Redundancy", "categories": ["cs.AR", "B.2.6; B.2.7; B.2.8"], "comment": "5 pages. Accepted for publication in the Proceedings of IEEE ISCAS\n  2025", "summary": "Duplication-based redundancy schemes have proven to be effective in designing\nfully-resilient Quasi-delay Insensitive (QDI) asynchronous circuits. The\ncomplete resiliency, however, is accompanied by significant energy, latency,\nand area overhead. This paper presents a novel error-tolerant Null Convention\nLogic (NCL) architecture based on selective redundancy. Results demonstrate the\nefficacy of the proposed method in terms of area and energy utilization as\ncompared to existing duplication-based NCL designs, targeting an image\nprocessing application.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9009\u62e9\u6027\u5197\u4f59\u7684\u65b0\u578b\u5bb9\u9519NCL\u67b6\u6784\uff0c\u76f8\u6bd4\u4f20\u7edf\u91cd\u590d\u5197\u4f59\u8bbe\u8ba1\uff0c\u5728\u9762\u79ef\u548c\u80fd\u8017\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91cd\u590d\u5197\u4f59\u7684QDI\u5f02\u6b65\u7535\u8def\u8bbe\u8ba1\u867d\u7136\u5b8c\u5168\u5bb9\u9519\uff0c\u4f46\u5e26\u6765\u663e\u8457\u7684\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u9762\u79ef\u5f00\u9500\u3002", "method": "\u91c7\u7528\u9009\u62e9\u6027\u5197\u4f59\u8bbe\u8ba1\u7684\u65b0\u578bNCL\u67b6\u6784\u3002", "result": "\u5728\u56fe\u50cf\u5904\u7406\u5e94\u7528\u4e2d\uff0c\u65b0\u67b6\u6784\u5728\u9762\u79ef\u548c\u80fd\u8017\u4e0a\u4f18\u4e8e\u4f20\u7edf\u8bbe\u8ba1\u3002", "conclusion": "\u9009\u62e9\u6027\u5197\u4f59\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u5bb9\u9519\u8bbe\u8ba1\u65b9\u6848\u3002"}}
{"id": "2506.15437", "pdf": "https://arxiv.org/pdf/2506.15437", "abs": "https://arxiv.org/abs/2506.15437", "authors": ["Nick Brown", "Jake Davies", "Felix LeClair"], "title": "Exploring Fast Fourier Transforms on the Tenstorrent Wormhole", "categories": ["cs.DC"], "comment": "Author accepted version of paper submitted to RISC-V for HPC ISC\n  workshop 2025", "summary": "Whilst numerous areas of computing have adopted the RISC-V Instruction Set\nArchitecture (ISA) wholesale in recent years, it is yet to become widespread in\nHPC. RISC-V accelerators offer a compelling option where the HPC community can\nbenefit from the specialisation offered by the open nature of the standard but\nwithout the extensive ecosystem changes required when adopting RISC-V CPUs. In\nthis paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT)\nalgorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon\nTenstorrent's Tensix architecture, this technology decouples the movement of\ndata from compute, potentially offering increased control to the programmer.\nExploring different optimisation techniques to address the bottlenecks inherent\nin data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is\nslower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around\n8 times less power and consumes around 2.8 times less energy than the CPU when\ncomputing the Fourier transform.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06Cooley-Tukey FFT\u7b97\u6cd5\u79fb\u690d\u5230Tenstorrent Wormhole RISC-V\u52a0\u901f\u5668\u4e0a\u7684\u6548\u679c\uff0c\u5c3d\u7ba1\u6027\u80fd\u4e0d\u5982\u9ad8\u7aefCPU\uff0c\u4f46\u80fd\u8017\u663e\u8457\u964d\u4f4e\u3002", "motivation": "\u5229\u7528RISC-V\u52a0\u901f\u5668\u7684\u5f00\u653e\u6027\u548c\u4e13\u4e1a\u5316\u4f18\u52bf\uff0c\u4e3aHPC\u9886\u57df\u63d0\u4f9b\u9ad8\u6548\u80fd\u4f4e\u529f\u8017\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u6570\u636e\u79fb\u52a8\u6280\u672f\uff0c\u5c06FFT\u7b97\u6cd5\u79fb\u690d\u5230Tenstorrent Wormhole RISC-V\u52a0\u901f\u5668\u4e0a\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "Wormhole n300\u5728\u529f\u8017\u548c\u80fd\u8017\u4e0a\u5206\u522b\u6bd424\u6838Xeon Platinum\u4f4e8\u500d\u548c2.8\u500d\uff0c\u4f46\u6027\u80fd\u8f83\u6162\u3002", "conclusion": "RISC-V\u52a0\u901f\u5668\u5728HPC\u4e2d\u5177\u6709\u6f5c\u5728\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u80fd\u6548\u6bd4\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.15008", "pdf": "https://arxiv.org/pdf/2506.15008", "abs": "https://arxiv.org/abs/2506.15008", "authors": ["Richa Gupta", "Alexander Htet Kyaw"], "title": "Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output", "categories": ["cs.HC", "cs.AI"], "comment": "15 Pages, 6 figures, CAAD Futures 2025", "summary": "Generative AI, specifically text-to-image models, have revolutionized\ninterior architectural design by enabling the rapid translation of conceptual\nideas into visual representations from simple text prompts. While generative AI\ncan produce visually appealing images they often lack actionable data for\ndesigners In this work, we propose a novel pipeline that integrates DALL-E 3\nwith a materials dataset to enrich AI-generated designs with sustainability\nmetrics and material usage insights. After the model generates an interior\ndesign image, a post-processing module identifies the top ten materials present\nand pairs them with carbon dioxide equivalent (CO2e) values from a general\nmaterials dictionary. This approach allows designers to immediately evaluate\nenvironmental impacts and refine prompts accordingly. We evaluate the system\nthrough three user tests: (1) no mention of sustainability to the user prior to\nthe prompting process with generative AI, (2) sustainability goals communicated\nto the user before prompting, and (3) sustainability goals communicated along\nwith quantitative CO2e data included in the generative AI outputs. Our\nqualitative and quantitative analyses reveal that the introduction of\nsustainability metrics in the third test leads to more informed design\ndecisions, however, it can also trigger decision fatigue and lower overall\nsatisfaction. Nevertheless, the majority of participants reported incorporating\nsustainability principles into their workflows in the third test, underscoring\nthe potential of integrated metrics to guide more ecologically responsible\npractices. Our findings showcase the importance of balancing design freedom\nwith practical constraints, offering a clear path toward holistic, data-driven\nsolutions in AI-assisted architectural design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DALL-E 3\u548c\u6750\u6599\u6570\u636e\u96c6\u7684\u7ba1\u9053\uff0c\u4e3aAI\u751f\u6210\u7684\u5185\u9970\u8bbe\u8ba1\u6dfb\u52a0\u53ef\u6301\u7eed\u6027\u6307\u6807\u548c\u6750\u6599\u4f7f\u7528\u6d1e\u5bdf\uff0c\u4ee5\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u8bc4\u4f30\u73af\u5883\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0fAI\u80fd\u5feb\u901f\u751f\u6210\u89c6\u89c9\u6548\u679c\uff0c\u4f46\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u7684\u6570\u636e\u652f\u6301\u8bbe\u8ba1\u51b3\u7b56\uff0c\u5c24\u5176\u662f\u5728\u53ef\u6301\u7eed\u6027\u65b9\u9762\u3002", "method": "\u901a\u8fc7\u540e\u5904\u7406\u6a21\u5757\u8bc6\u522bAI\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u6750\u6599\uff0c\u5e76\u4e0eCO2e\u503c\u914d\u5bf9\uff0c\u7528\u6237\u6d4b\u8bd5\u6bd4\u8f83\u4e86\u4e0d\u540c\u63d0\u793a\u65b9\u5f0f\u5bf9\u8bbe\u8ba1\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "result": "\u5f15\u5165\u53ef\u6301\u7eed\u6027\u6307\u6807\uff08\u5982CO2e\u6570\u636e\uff09\u80fd\u5f15\u5bfc\u66f4\u73af\u4fdd\u7684\u8bbe\u8ba1\u51b3\u7b56\uff0c\u4f46\u53ef\u80fd\u5f15\u53d1\u51b3\u7b56\u75b2\u52b3\u5e76\u964d\u4f4e\u6ee1\u610f\u5ea6\u3002", "conclusion": "\u5e73\u8861\u8bbe\u8ba1\u81ea\u7531\u4e0e\u53ef\u6301\u7eed\u6027\u7ea6\u675f\u662f\u5173\u952e\uff0c\u5c55\u793a\u4e86AI\u8f85\u52a9\u8bbe\u8ba1\u4e2d\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.15298", "pdf": "https://arxiv.org/pdf/2506.15298", "abs": "https://arxiv.org/abs/2506.15298", "authors": ["Xinqi Fan", "Jingting Li", "John See", "Moi Hoon Yap", "Wen-Huang Cheng", "Xiaobai Li", "Xiaopeng Hong", "Su-Jing Wang", "Adrian K. Davision"], "title": "MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering", "categories": ["cs.CV", "cs.MM"], "comment": "Micro-Expression Grand Challenge (MEGC) at ACM MM 2025", "summary": "Facial micro-expressions (MEs) are involuntary movements of the face that\noccur spontaneously when a person experiences an emotion but attempts to\nsuppress or repress the facial expression, typically found in a high-stakes\nenvironment. In recent years, substantial advancements have been made in the\nareas of ME recognition, spotting, and generation. However, conventional\napproaches that treat spotting and recognition as separate tasks are\nsuboptimal, particularly for analyzing long-duration videos in realistic\nsettings. Concurrently, the emergence of multimodal large language models\n(MLLMs) and large vision-language models (LVLMs) offers promising new avenues\nfor enhancing ME analysis through their powerful multimodal reasoning\ncapabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that\nreflect these evolving research directions: (1) ME spot-then-recognize\n(ME-STR), which integrates ME spotting and subsequent recognition in a unified\nsequential pipeline; and (2) ME visual question answering (ME-VQA), which\nexplores ME understanding through visual question answering, leveraging MLLMs\nor LVLMs to address diverse question types related to MEs. All participating\nalgorithms are required to run on this test set and submit their results on a\nleaderboard. More details are available at https://megc2025.github.io.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u9762\u90e8\u5fae\u8868\u60c5\uff08MEs\uff09\u7684\u8bc6\u522b\u3001\u5b9a\u4f4d\u548c\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u63d0\u51fa\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u589e\u5f3aME\u5206\u6790\uff0c\u5e76\u4ecb\u7ecd\u4e86MEGC 2025\u7684\u4e24\u9879\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5fae\u8868\u60c5\u5b9a\u4f4d\u548c\u8bc6\u522b\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7edf\u4e00\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u9879\u4efb\u52a1\uff1aME-STR\uff08\u7edf\u4e00\u5e8f\u5217\u7ba1\u9053\uff09\u548cME-VQA\uff08\u89c6\u89c9\u95ee\u7b54\uff09\uff0c\u5229\u7528MLLMs\u548cLVLMs\u63d0\u5347ME\u5206\u6790\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u65b0\u65b9\u6cd5\u63d0\u5347\u5fae\u8868\u60c5\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u957f\u671f\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8bba\u6587\u5c55\u793a\u4e86\u5229\u7528\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4e3a\u5fae\u8868\u60c5\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.14830", "pdf": "https://arxiv.org/pdf/2506.14830", "abs": "https://arxiv.org/abs/2506.14830", "authors": ["Zhizhao Wen", "Ruoxin Zhang", "Chao Wang"], "title": "Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": "Source code available; Accepted by 2025 6th International Conference\n  on Electronic Communication and Artificial Intelligence; 5 pages; 7 figures", "summary": "Aiming at the critical role of SSD health state prediction in data\nreliability assurance, this study proposes a hybrid BiGRU-MHA model that\nincorporates a multi-head attention mechanism to enhance the accuracy and\nstability of storage device health classification. The model innovatively\nintegrates temporal feature extraction and key information focusing\ncapabilities. Specifically, it leverages the bidirectional timing modeling\nadvantages of the BiGRU network to capture both forward and backward\ndependencies of SSD degradation features. Simultaneously, the multi-head\nattention mechanism dynamically assigns feature weights, improving the model's\nsensitivity to critical health indicators. Experimental results show that the\nproposed model achieves classification accuracies of 92.70% on the training set\nand 92.44% on the test set, with a minimal performance gap of only 0.26%,\ndemonstrating excellent generalization ability. Further analysis using the\nreceiver operating characteristic (ROC) curve shows an area under the curve\n(AUC) of 0.94 on the test set, confirming the model's robust binary\nclassification performance. This work not only presents a new technical\napproach for SSD health prediction but also addresses the generalization\nbottleneck of traditional models, offering a verifiable method with practical\nvalue for preventive maintenance of industrial-grade storage systems. The\nresults show the model can significantly reduce data loss risks by providing\nearly failure warnings and help optimize maintenance costs, supporting\nintelligent decision-making in building reliable storage systems for cloud\ncomputing data centers and edge storage environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6ce8\u610f\u529b\u673a\u5236\u7684BiGRU-MHA\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8SSD\u5065\u5eb7\u72b6\u6001\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u4e3a92.70%\u548c92.44%\uff0c\u5177\u6709\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u3002AUC\u6307\u6807\u4e3a0.94\uff0c\u8bc1\u660e\u5176\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u7684\u9c81\u68d2\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9SSD\u5065\u5eb7\u72b6\u6001\u9884\u6d4b\u5728\u6570\u636e\u53ef\u9760\u6027\u4fdd\u969c\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4f20\u7edf\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u53cc\u5411\u65f6\u5e8f\u5efa\u6a21\u4f18\u52bf\u7684BiGRU\u7f51\u7edc\u548c\u591a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u52a8\u6001\u5206\u914d\u7279\u5f81\u6743\u91cd\uff0c\u63d0\u9ad8\u5bf9\u5173\u952e\u5065\u5eb7\u6307\u6807\u7684\u654f\u611f\u6027\u3002", "result": "\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0cAUC\u4e3a0.94\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u7ea7\u5b58\u50a8\u7cfb\u7edf\u7684\u9884\u9632\u6027\u7ef4\u62a4\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3aSSD\u5065\u5eb7\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u6280\u672f\u65b9\u6848\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u6570\u636e\u4e22\u5931\u98ce\u9669\uff0c\u652f\u6301\u4e91\u8ba1\u7b97\u6570\u636e\u4e2d\u5fc3\u548c\u8fb9\u7f18\u5b58\u50a8\u73af\u5883\u7684\u667a\u80fd\u51b3\u7b56\u3002"}}
{"id": "2506.15454", "pdf": "https://arxiv.org/pdf/2506.15454", "abs": "https://arxiv.org/abs/2506.15454", "authors": ["Nizar ALHafez", "Ahmad Kurdi"], "title": "Parallel Paradigms in Modern HPC: A Comparative Analysis of MPI, OpenMP, and CUDA", "categories": ["cs.DC"], "comment": "10 pages", "summary": "This paper presents a comprehensive comparison of three dominant parallel\nprogramming models in High Performance Computing (HPC): Message Passing\nInterface (MPI), Open Multi-Processing (OpenMP), and Compute Unified Device\nArchitecture (CUDA). Selecting optimal programming approaches for modern\nheterogeneous HPC architectures has become increasingly critical. We\nsystematically analyze these models across multiple dimensions: architectural\nfoundations, performance characteristics, domain-specific suitability,\nprogramming complexity, and recent advancements. We examine each model's\nstrengths, weaknesses, and optimization techniques. Our investigation\ndemonstrates that MPI excels in distributed memory environments with\nnear-linear scalability for communication-intensive applications, but faces\ncommunication overhead challenges. OpenMP provides strong performance and\nusability in shared-memory systems and loop-centric tasks, though it is limited\nby shared memory contention. CUDA offers substantial performance gains for\ndata-parallel GPU workloads, but is restricted to NVIDIA GPUs and requires\nspecialized expertise. Performance evaluations across scientific simulations,\nmachine learning, and data analytics reveal that hybrid approaches combining\ntwo or more models often yield optimal results in heterogeneous environments.\nThe paper also discusses implementation challenges, optimization best\npractices, and emerging trends such as performance portability frameworks,\ntask-based programming, and the convergence of HPC and Big Data. This research\nhelps developers and researchers make informed decisions when selecting\nprogramming models for modern HPC applications, emphasizing that the best\nchoice depends on application requirements, hardware, and development\nconstraints.", "AI": {"tldr": "\u672c\u6587\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u7684\u4e09\u79cd\u4e3b\u8981\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\uff08MPI\u3001OpenMP\u548cCUDA\uff09\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u52bf\u3001\u52a3\u52bf\u53ca\u9002\u7528\u573a\u666f\uff0c\u5e76\u6307\u51fa\u6df7\u5408\u6a21\u578b\u5728\u5f02\u6784\u73af\u5883\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u7684\u5f02\u6784\u5316\uff0c\u9009\u62e9\u5408\u9002\u7684\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u57fa\u4e8e\u5e94\u7528\u9700\u6c42\u548c\u786c\u4ef6\u9650\u5236\u505a\u51fa\u66f4\u660e\u667a\u7684\u9009\u62e9\u3002", "method": "\u901a\u8fc7\u5bf9MPI\u3001OpenMP\u548cCUDA\u5728\u67b6\u6784\u57fa\u7840\u3001\u6027\u80fd\u7279\u70b9\u3001\u9886\u57df\u9002\u7528\u6027\u3001\u7f16\u7a0b\u590d\u6742\u6027\u548c\u6700\u65b0\u8fdb\u5c55\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u7ed3\u5408\u79d1\u5b66\u6a21\u62df\u3001\u673a\u5668\u5b66\u4e60\u548c\u6570\u636e\u5206\u6790\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "result": "MPI\u9002\u7528\u4e8e\u901a\u4fe1\u5bc6\u96c6\u578b\u5206\u5e03\u5f0f\u5185\u5b58\u73af\u5883\uff0c\u4f46\u9762\u4e34\u901a\u4fe1\u5f00\u9500\uff1bOpenMP\u5728\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\u53ca\u5faa\u73af\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u53d7\u9650\u4e8e\u5171\u4eab\u5185\u5b58\u4e89\u7528\uff1bCUDA\u5728\u6570\u636e\u5e76\u884cGPU\u4efb\u52a1\u4e2d\u6027\u80fd\u663e\u8457\uff0c\u4f46\u4ec5\u9002\u7528\u4e8eNVIDIA GPU\u4e14\u9700\u4e13\u4e1a\u77e5\u8bc6\u3002\u6df7\u5408\u6a21\u578b\u5728\u5f02\u6784\u73af\u5883\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u9009\u62e9\u6700\u4f18\u7f16\u7a0b\u6a21\u578b\u9700\u7efc\u5408\u8003\u8651\u5e94\u7528\u9700\u6c42\u3001\u786c\u4ef6\u9650\u5236\u548c\u5f00\u53d1\u7ea6\u675f\u3002\u6df7\u5408\u6a21\u578b\u548c\u65b0\u5174\u6280\u672f\uff08\u5982\u6027\u80fd\u53ef\u79fb\u690d\u6846\u67b6\uff09\u4e3a\u672a\u6765HPC\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u591a\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.15047", "pdf": "https://arxiv.org/pdf/2506.15047", "abs": "https://arxiv.org/abs/2506.15047", "authors": ["Jiayue Melissa Shi", "Dong Whi Yoo", "Keran Wang", "Violeta J. Rodriguez", "Ravi Karkar", "Koustuv Saha"], "title": "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Family caregivers of individuals with Alzheimer's Disease and Related\nDementia (AD/ADRD) face significant emotional and logistical challenges that\nplace them at heightened risk for stress, anxiety, and depression. Although\nrecent advances in generative AI -- particularly large language models (LLMs)\n-- offer new opportunities to support mental health, little is known about how\ncaregivers perceive and engage with such technologies. To address this gap, we\ndeveloped Carey, a GPT-4o-based chatbot designed to provide informational and\nemotional support to AD/ADRD caregivers. Using Carey as a technology probe, we\nconducted semi-structured interviews with 16 family caregivers following\nscenario-driven interactions grounded in common caregiving stressors. Through\ninductive coding and reflexive thematic analysis, we surface a systemic\nunderstanding of caregiver needs and expectations across six themes --\non-demand information access, emotional support, safe space for disclosure,\ncrisis management, personalization, and data privacy. For each of these themes,\nwe also identified the nuanced tensions in the caregivers' desires and\nconcerns. We present a mapping of caregiver needs, AI chatbot's strengths,\ngaps, and design recommendations. Our findings offer theoretical and practical\ninsights to inform the design of proactive, trustworthy, and caregiver-centered\nAI systems that better support the evolving mental health needs of AD/ADRD\ncaregivers.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8eGPT-4o\u7684\u804a\u5929\u673a\u5668\u4ebaCarey\uff0c\u7528\u4e8e\u652f\u6301AD/ADRD\u5bb6\u5ead\u7167\u987e\u8005\u7684\u5fc3\u7406\u5065\u5eb7\uff0c\u901a\u8fc7\u8bbf\u8c08\u63ed\u793a\u4e86\u516d\u9879\u9700\u6c42\u548cAI\u804a\u5929\u673a\u5668\u4eba\u7684\u4f18\u52a3\u52bf\uff0c\u63d0\u51fa\u4e86\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "AD/ADRD\u5bb6\u5ead\u7167\u987e\u8005\u9762\u4e34\u5de8\u5927\u7684\u60c5\u611f\u548c\u5b9e\u9645\u6311\u6218\uff0c\u800c\u751f\u6210\u5f0fAI\uff08\u5982LLMs\uff09\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u65b9\u9762\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f00\u53d1Carey\u804a\u5929\u673a\u5668\u4eba\u4f5c\u4e3a\u6280\u672f\u63a2\u9488\uff0c\u5bf916\u540d\u7167\u987e\u8005\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u91c7\u7528\u5f52\u7eb3\u7f16\u7801\u548c\u4e3b\u9898\u5206\u6790\u6cd5\u3002", "result": "\u63ed\u793a\u4e86\u7167\u987e\u8005\u7684\u516d\u5927\u9700\u6c42\u53ca\u5176\u4e0eAI\u804a\u5929\u673a\u5668\u4eba\u7684\u5339\u914d\u4e0e\u77db\u76fe\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u66f4\u8d34\u8fd1\u7167\u987e\u8005\u9700\u6c42\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2506.15677", "pdf": "https://arxiv.org/pdf/2506.15677", "abs": "https://arxiv.org/abs/2506.15677", "authors": ["Yining Hong", "Rui Sun", "Bingxuan Li", "Xingcheng Yao", "Maxine Wu", "Alexander Chien", "Da Yin", "Ying Nian Wu", "Zhecan James Wang", "Kai-Wei Chang"], "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MM", "cs.RO"], "comment": null, "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bAI\u4ee3\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5b9e\u4f53\u611f\u77e5\u4e0e\u7f51\u7edc\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u9700\u8981\u8de8\u9886\u57df\u667a\u80fd\u7684\u4efb\u52a1\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u9886\u57df\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u5b9e\u4f53\u4e16\u754c\u4e0e\u6570\u5b57\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u89e3\u51b3\u8de8\u9886\u57df\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u2018Embodied Web Agents\u2019\u4efb\u52a1\u73af\u5883\uff0c\u6574\u5408\u4e863D\u6a21\u62df\u73af\u5883\u4e0e\u529f\u80fd\u6027\u7f51\u7edc\u63a5\u53e3\uff0c\u5e76\u6784\u5efa\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709AI\u7cfb\u7edf\u5728\u8de8\u9886\u57df\u4efb\u52a1\u4e0a\u4e0e\u4eba\u7c7b\u80fd\u529b\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7ed3\u5408\u5b9e\u4f53\u8ba4\u77e5\u4e0e\u7f51\u7edc\u77e5\u8bc6\u8bbf\u95ee\u7684\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6311\u6218\u4e0e\u673a\u9047\u3002"}}
{"id": "2506.15461", "pdf": "https://arxiv.org/pdf/2506.15461", "abs": "https://arxiv.org/abs/2506.15461", "authors": ["Nikolay Blagoev", "O\u011fuzhan Ersoy", "Lydia Yiyu Chen"], "title": "All is Not Lost: LLM Recovery without Checkpoints", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.", "AI": {"tldr": "CheckFree \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u90bb\u8fd1\u9636\u6bb5\u6765\u66ff\u4ee3\u5931\u8d25\u9636\u6bb5\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6216\u5b58\u50a8\u3002\u6269\u5c55\u7248 CheckFree+ \u652f\u6301\u65e0\u5e8f\u6d41\u6c34\u7ebf\u6267\u884c\uff0c\u53ef\u5bb9\u5fcd\u9996\u5c3e\u9636\u6bb5\u5d29\u6e83\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u548c\u4f4e\u6027\u80fd\u8ba1\u7b97\u8282\u70b9\u4e0a\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u964d\u4f4e\u6210\u672c\u5e76\u4fc3\u8fdb\u6a21\u578b\u6c11\u4e3b\u5316\uff0c\u4f46\u8282\u70b9\u6545\u969c\u548c\u8c03\u5ea6\u7b56\u7565\u5bfc\u81f4\u9636\u6bb5\u4e22\u5931\uff0c\u4f20\u7edf\u6062\u590d\u65b9\u6cd5\u6548\u7387\u4f4e\u3002", "method": "CheckFree \u901a\u8fc7\u52a0\u6743\u5e73\u5747\u90bb\u8fd1\u9636\u6bb5\u66ff\u4ee3\u5931\u8d25\u9636\u6bb5\uff1bCheckFree+ \u6269\u5c55\u4e86\u65e0\u5e8f\u6d41\u6c34\u7ebf\u6267\u884c\uff0c\u901a\u8fc7\u590d\u5236\u6743\u91cd\u548c\u90bb\u8fd1\u9636\u6bb5\u6a21\u4eff\u884c\u4e3a\u6765\u6062\u590d\u9996\u5c3e\u9636\u6bb5\u3002", "result": "\u5728\u4f4e\u81f3\u4e2d\u7b49\u6545\u969c\u7387\uff085-10%\uff09\u4e0b\uff0cCheckFree \u548c CheckFree+ \u5728\u6536\u655b\u65f6\u95f4\u4e0a\u4f18\u4e8e\u68c0\u67e5\u70b9\u548c\u5197\u4f59\u8ba1\u7b97\uff0c\u63d0\u5347\u8d85\u8fc7 12%\u3002", "conclusion": "CheckFree \u548c CheckFree+ \u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6545\u969c\u6062\u590d\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2506.15129", "pdf": "https://arxiv.org/pdf/2506.15129", "abs": "https://arxiv.org/abs/2506.15129", "authors": ["Paul Murrell"], "title": "Data Verbalisation: What is Text Doing in a Data Visualisation?", "categories": ["cs.HC", "stat.OT"], "comment": "43 pages (including appendix), 20 figures", "summary": "This article discusses the role that text elements play in a data\nvisualisation. We argue that there is a need for a simple, coherent explanation\nof text elements similar to the understanding that already exists for non-text\nelements like bars, points, and lines. We explore examples of how text is used\nwithin a data visualisation and use existing knowledge and assessment\ntechniques to evaluate when text is effective and when it is not. The result is\na framework that aims to be easy to understand and easy to apply in order to\nunderstand the purpose and effectiveness of the text elements in any data\nvisualisation.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u6587\u672c\u5143\u7d20\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u53ef\u89c6\u5316\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u975e\u6587\u672c\u5143\u7d20\uff08\u5982\u67f1\u72b6\u56fe\u3001\u70b9\u548c\u7ebf\uff09\uff0c\u7f3a\u4e4f\u5bf9\u6587\u672c\u5143\u7d20\u7684\u7cfb\u7edf\u7406\u89e3\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u77e5\u8bc6\u548c\u8bc4\u4f30\u6280\u672f\uff0c\u63a2\u7a76\u6587\u672c\u5728\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u4f7f\u7528\u6848\u4f8b\uff0c\u8bc4\u4f30\u5176\u6709\u6548\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b80\u5355\u6613\u7528\u7684\u6846\u67b6\uff0c\u5e2e\u52a9\u7406\u89e3\u548c\u5e94\u7528\u6587\u672c\u5143\u7d20\u7684\u76ee\u7684\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u6587\u672c\u5143\u7d20\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u53ef\u89c6\u5316\u7684\u6574\u4f53\u6548\u679c\u3002"}}
{"id": "2506.15488", "pdf": "https://arxiv.org/pdf/2506.15488", "abs": "https://arxiv.org/abs/2506.15488", "authors": ["Hussam Al Daas", "Grey Ballard", "Laura Grigori", "Suraj Kumar", "Kathryn Rouse", "Mathieu V\u00e9rit\u00e9"], "title": "Minimizing Communication for Parallel Symmetric Tensor Times Same Vector Computation", "categories": ["cs.DC"], "comment": "19 pages, 1 figure", "summary": "In this article, we focus on the parallel communication cost of multiplying\nthe same vector along two modes of a $3$-dimensional symmetric tensor. This is\na key computation in the higher-order power method for determining eigenpairs\nof a $3$-dimensional symmetric tensor and in gradient-based methods for\ncomputing a symmetric CP decomposition. We establish communication lower bounds\nthat determine how much data movement is required to perform the specified\ncomputation in parallel. The core idea of the proof relies on extending a key\ngeometric inequality for $3$-dimensional symmetric computations. We demonstrate\nthat the communication lower bounds are tight by presenting an optimal\nalgorithm where the data distribution is a natural extension of the triangle\nblock partition scheme for symmetric matrices to 3-dimensional symmetric\ntensors.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u4e09\u7ef4\u5bf9\u79f0\u5f20\u91cf\u4e0a\u5e76\u884c\u8ba1\u7b97\u5411\u91cf\u4e58\u6cd5\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u51fa\u4e86\u7d27\u81f4\u7684\u901a\u4fe1\u4e0b\u9650\uff0c\u5e76\u5c55\u793a\u4e86\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u89e3\u51b3\u5728\u5bf9\u79f0\u5f20\u91cf\u8ba1\u7b97\u4e2d\u7684\u5e76\u884c\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u8fd9\u5bf9\u9ad8\u9636\u5e42\u6cd5\u548c\u5bf9\u79f0CP\u5206\u89e3\u7684\u8ba1\u7b97\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u4e09\u7ef4\u5bf9\u79f0\u8ba1\u7b97\u7684\u5173\u952e\u51e0\u4f55\u4e0d\u7b49\u5f0f\uff0c\u5efa\u7acb\u4e86\u901a\u4fe1\u4e0b\u9650\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u79f0\u77e9\u9635\u4e09\u89d2\u5f62\u5757\u5206\u533a\u65b9\u6848\u7684\u6700\u4f18\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u901a\u4fe1\u4e0b\u9650\u7684\u7d27\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u5206\u5e03\u7684\u6700\u4f18\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\u3002", "conclusion": "\u672c\u6587\u4e3a\u5bf9\u79f0\u5f20\u91cf\u8ba1\u7b97\u4e2d\u7684\u901a\u4fe1\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15189", "pdf": "https://arxiv.org/pdf/2506.15189", "abs": "https://arxiv.org/abs/2506.15189", "authors": ["Yikan Wang"], "title": "Accessible Gesture-Driven Augmented Reality Interaction System", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Augmented reality (AR) offers immersive interaction but remains inaccessible\nfor users with motor impairments or limited dexterity due to reliance on\nprecise input methods. This study proposes a gesture-based interaction system\nfor AR environments, leveraging deep learning to recognize hand and body\ngestures from wearable sensors and cameras, adapting interfaces to user\ncapabilities. The system employs vision transformers (ViTs), temporal\nconvolutional networks (TCNs), and graph attention networks (GATs) for gesture\nprocessing, with federated learning ensuring privacy-preserving model training\nacross diverse users. Reinforcement learning optimizes interface elements like\nmenu layouts and interaction modes. Experiments demonstrate a 20% improvement\nin task completion efficiency and a 25% increase in user satisfaction for\nmotor-impaired users compared to baseline AR systems. This approach enhances AR\naccessibility and scalability. Keywords: Deep learning, Federated learning,\nGesture recognition, Augmented reality, Accessibility, Human-computer\ninteraction", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684AR\u624b\u52bf\u4ea4\u4e92\u7cfb\u7edf\uff0c\u63d0\u5347\u8fd0\u52a8\u969c\u788d\u7528\u6237\u7684\u4f53\u9a8c\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709AR\u7cfb\u7edf\u4f9d\u8d56\u7cbe\u786e\u8f93\u5165\uff0c\u5bf9\u8fd0\u52a8\u969c\u788d\u7528\u6237\u4e0d\u53cb\u597d\u3002", "method": "\u7ed3\u5408ViTs\u3001TCNs\u548cGATs\u5904\u7406\u624b\u52bf\uff0c\u8054\u90a6\u5b66\u4e60\u4fdd\u62a4\u9690\u79c1\uff0c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u754c\u9762\u3002", "result": "\u4efb\u52a1\u6548\u7387\u63d0\u534720%\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u63d0\u9ad825%\u3002", "conclusion": "\u7cfb\u7edf\u663e\u8457\u63d0\u5347AR\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2506.15537", "pdf": "https://arxiv.org/pdf/2506.15537", "abs": "https://arxiv.org/abs/2506.15537", "authors": ["Polina Shpilker", "Line Pouchard"], "title": "Automatic Metadata Capture and Processing for High-Performance Workflows", "categories": ["cs.DC"], "comment": null, "summary": "Modern workflows run on increasingly heterogeneous computing architectures\nand with this heterogeneity comes additional complexity. We aim to apply the\nFAIR principles for research reproducibility by developing software to collect\nmetadata annotations for workflows run on HPC systems. We experiment with two\npossible formats to uniformly store these metadata, and reorganize the\ncollected metadata to be as easy to use as possible for researchers studying\ntheir workflow performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f00\u53d1\u8f6f\u4ef6\u6536\u96c6HPC\u7cfb\u7edf\u4e0a\u8fd0\u884c\u7684\u5de5\u4f5c\u6d41\u5143\u6570\u636e\u6ce8\u91ca\uff0c\u5e76\u5b9e\u9a8c\u4e86\u4e24\u79cd\u5b58\u50a8\u683c\u5f0f\uff0c\u4ee5\u4f18\u5316\u5143\u6570\u636e\u7684\u4f7f\u7528\u4fbf\u6377\u6027\u3002", "motivation": "\u73b0\u4ee3\u5de5\u4f5c\u6d41\u8fd0\u884c\u5728\u8d8a\u6765\u8d8a\u5f02\u6784\u7684\u8ba1\u7b97\u67b6\u6784\u4e0a\uff0c\u8fd9\u79cd\u5f02\u6784\u6027\u5e26\u6765\u4e86\u590d\u6742\u6027\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5e94\u7528FAIR\u539f\u5219\uff08\u53ef\u67e5\u627e\u3001\u53ef\u8bbf\u95ee\u3001\u53ef\u4e92\u64cd\u4f5c\u3001\u53ef\u91cd\u7528\uff09\u6765\u63d0\u9ad8\u7814\u7a76\u53ef\u91cd\u590d\u6027\u3002", "method": "\u5f00\u53d1\u8f6f\u4ef6\u4ee5\u6536\u96c6HPC\u7cfb\u7edf\u4e0a\u5de5\u4f5c\u6d41\u7684\u5143\u6570\u636e\u6ce8\u91ca\uff0c\u5e76\u5b9e\u9a8c\u4e24\u79cd\u7edf\u4e00\u7684\u5b58\u50a8\u683c\u5f0f\uff0c\u540c\u65f6\u91cd\u65b0\u7ec4\u7ec7\u5143\u6570\u636e\u4ee5\u4fbf\u4e8e\u7814\u7a76\u8005\u5206\u6790\u5de5\u4f5c\u6d41\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u5143\u6570\u636e\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u7814\u7a76\u8005\u4f7f\u7528\u3002", "conclusion": "\u901a\u8fc7FAIR\u539f\u5219\u548c\u5143\u6570\u636e\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u5f02\u6784\u8ba1\u7b97\u73af\u5883\u4e0b\u5de5\u4f5c\u6d41\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u6027\u80fd\u5206\u6790\u7684\u4fbf\u6377\u6027\u3002"}}
{"id": "2506.15293", "pdf": "https://arxiv.org/pdf/2506.15293", "abs": "https://arxiv.org/abs/2506.15293", "authors": ["Francesco Chiossi", "Julian Rasch", "Robin Welsch", "Albrecht Schmidt", "Florian Michahelles"], "title": "Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces", "categories": ["cs.HC", "cs.RO"], "comment": "9 pages", "summary": "As robots enter collaborative workspaces, ensuring mutual understanding\nbetween human workers and robotic systems becomes a prerequisite for trust,\nsafety, and efficiency. In this position paper, we draw on the cooperation\nscenario of the AIMotive project in which a human and a cobot jointly perform\nassembly tasks to argue for a structured approach to intent communication.\nBuilding on the Situation Awareness-based Agent Transparency (SAT) framework\nand the notion of task abstraction levels, we propose a multidimensional design\nspace that maps intent content (SAT1, SAT3), planning horizon (operational to\nstrategic), and modality (visual, auditory, haptic). We illustrate how this\nspace can guide the design of multimodal communication strategies tailored to\ndynamic collaborative work contexts. With this paper, we lay the conceptual\nfoundation for a future design toolkit aimed at supporting transparent\nhuman-robot interaction in the workplace. We highlight key open questions and\ndesign challenges, and propose a shared agenda for multimodal, adaptive, and\ntrustworthy robotic collaboration in hybrid work environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ef4\u5ea6\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u7528\u4e8e\u6307\u5bfc\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u610f\u56fe\u901a\u4fe1\u8bbe\u8ba1\uff0c\u4ee5\u63d0\u9ad8\u900f\u660e\u5ea6\u3001\u4fe1\u4efb\u548c\u6548\u7387\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u8fdb\u5165\u534f\u4f5c\u5de5\u4f5c\u7a7a\u95f4\uff0c\u786e\u4fdd\u4eba\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u76f8\u4e92\u7406\u89e3\u6210\u4e3a\u4fe1\u4efb\u3001\u5b89\u5168\u548c\u6548\u7387\u7684\u524d\u63d0\u3002", "method": "\u57fa\u4e8eSAT\u6846\u67b6\u548c\u4efb\u52a1\u62bd\u8c61\u5c42\u6b21\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u7ef4\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u6db5\u76d6\u610f\u56fe\u5185\u5bb9\u3001\u89c4\u5212\u8303\u56f4\u548c\u901a\u4fe1\u6a21\u6001\u3002", "result": "\u63d0\u51fa\u4e86\u672a\u6765\u8bbe\u8ba1\u5de5\u5177\u5305\u7684\u6982\u5ff5\u57fa\u7840\uff0c\u652f\u6301\u900f\u660e\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u5e76\u63d0\u51fa\u4e86\u5171\u4eab\u7814\u7a76\u8bae\u7a0b\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6df7\u5408\u5de5\u4f5c\u73af\u5883\u4e2d\u591a\u6a21\u6001\u3001\u81ea\u9002\u5e94\u548c\u53ef\u4fe1\u8d56\u7684\u673a\u5668\u4eba\u534f\u4f5c\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u5f00\u653e\u95ee\u9898\u548c\u8bbe\u8ba1\u6311\u6218\u3002"}}
{"id": "2506.15595", "pdf": "https://arxiv.org/pdf/2506.15595", "abs": "https://arxiv.org/abs/2506.15595", "authors": ["Kunming Zhang", "Hanlong Liao", "Guoming Tang"], "title": "LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale Heterogeneous Clusters", "categories": ["cs.DC"], "comment": "12 pages, 19 figures,7 tables", "summary": "Parallel computing with multiple GPUs has become the dominant paradigm for\nmachine learning tasks, especially those of large language models (LLMs). To\nreduce the latency incurred by inter-GPU communication, a common practice for\nparallel tasks has been to allocate GPUs based on their physical proximity.\nHowever, this long-standing assumption has notable limitations, particularly in\nlarge-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs\nis irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU\ndispatching system based on global perspectives. To tackle the difficulty of\nstoring massive GPU topology information, LiteGD adopts a computation-aware\ndesign that leverages a lightweight Transformer network trained on sampled\ndata. Our customized design for network structure ensures both transferability\nand scalability. LiteGD also employs a bidirectional tree search approach to\nfind the optimal GPU dispatching in the data generated in the previous step,\nwhich can identify near-optimal solutions while reducing search overhead. We\nimplement and evaluate LiteGD in both real and simulated GPU clusters with\nhomogeneous and heterogeneous interconnects, respectively. Experimental results\ndemonstrate that LiteGD consistently achieves high GPU bandwidth efficacy\n(approximately 90\\%) across various cluster configurations and 80\\% in\nreal-world H100 cluster, significantly outperforming conventional default and\ninterconnect topology-aware dispatching methods, particularly in large-scale\nheterogeneous environments.", "AI": {"tldr": "LiteGD\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u52a8\u6001GPU\u8c03\u5ea6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5168\u5c40\u89c6\u89d2\u4f18\u5316\u591aGPU\u5e76\u884c\u8ba1\u7b97\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u5f02\u6784GPU\u96c6\u7fa4\u4e2d\uff0c\u4f20\u7edf\u57fa\u4e8e\u7269\u7406\u90bb\u8fd1\u6027\u7684GPU\u5206\u914d\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0cLiteGD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u611f\u77e5\u8bbe\u8ba1\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7Transformer\u7f51\u7edc\u5904\u7406GPU\u62d3\u6251\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u53cc\u5411\u6811\u641c\u7d22\u4f18\u5316\u8c03\u5ea6\u3002", "result": "LiteGD\u5728\u591a\u79cd\u96c6\u7fa4\u914d\u7f6e\u4e2d\u5b9e\u73b0\u9ad8\u5e26\u5bbd\u6548\u7387\uff08\u7ea690%\uff09\uff0c\u5728\u771f\u5b9eH100\u96c6\u7fa4\u4e2d\u8fbe\u523080%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "LiteGD\u5728\u5927\u578b\u5f02\u6784\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u591aGPU\u5e76\u884c\u8ba1\u7b97\u63d0\u4f9b\u9ad8\u6548\u8c03\u5ea6\u65b9\u6848\u3002"}}
{"id": "2506.15294", "pdf": "https://arxiv.org/pdf/2506.15294", "abs": "https://arxiv.org/abs/2506.15294", "authors": ["Jonas Lau", "Annie Tran"], "title": "UXR Point of View on Product Feature Prioritization Prior To Multi-Million Engineering Commitments", "categories": ["cs.HC"], "comment": null, "summary": "This paper discusses a popular UX research activity, feature prioritization,\nusing the User Experience Research Point of View (UXR PoV) Playbook framework.\nWe describe an application of multinomial logistic regression, frequently\nmarketed as MaxDiff, for prioritizing product features in consumer product\ndevelopment. It addresses challenges of traditional surveying techniques. We\npropose a solution using MaxDiff to generate a reliable preference list with a\nreasonable sample size. We also adapt the MaxDiff method to reduce the number\nof survey responses in half, making it less tedious from the survey takers'\nperspective. We present a case study using the adapted MaxDiff method for\ntablet feature prioritization research involving users with disabilities.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u4e00\u79cd\u901a\u8fc7UXR PoV\u6846\u67b6\u548cMaxDiff\u65b9\u6cd5\u8fdb\u884c\u529f\u80fd\u4f18\u5148\u6392\u5e8f\u7684\u7814\u7a76\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8c03\u67e5\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u5728\u6b8b\u75be\u4eba\u5e73\u677f\u529f\u80fd\u4f18\u5148\u6392\u5e8f\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u8c03\u67e5\u65b9\u6cd5\u5728\u529f\u80fd\u4f18\u5148\u6392\u5e8f\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210\u504f\u597d\u5217\u8868\u3002", "method": "\u91c7\u7528MaxDiff\uff08\u591a\u9879\u903b\u8f91\u56de\u5f52\uff09\u65b9\u6cd5\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u6539\u8fdb\u4ee5\u51cf\u5c11\u8c03\u67e5\u54cd\u5e94\u6570\u91cf\u3002", "result": "\u6539\u8fdb\u540e\u7684MaxDiff\u65b9\u6cd5\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u6837\u672c\u751f\u6210\u53ef\u9760\u7684\u504f\u597d\u5217\u8868\uff0c\u5e76\u964d\u4f4e\u4e86\u53c2\u4e0e\u8005\u7684\u75b2\u52b3\u611f\u3002", "conclusion": "MaxDiff\u65b9\u6cd5\u5728\u529f\u80fd\u4f18\u5148\u6392\u5e8f\u4e2d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u51cf\u5c11\u8c03\u67e5\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2506.14911", "pdf": "https://arxiv.org/pdf/2506.14911", "abs": "https://arxiv.org/abs/2506.14911", "authors": ["Ganyu Wang", "Boyu Wang", "Bin Gu", "Charles Ling"], "title": "Event-Driven Online Vertical Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": "Published as a conference paper at ICLR 2025", "summary": "Online learning is more adaptable to real-world scenarios in Vertical\nFederated Learning (VFL) compared to offline learning. However, integrating\nonline learning into VFL presents challenges due to the unique nature of VFL,\nwhere clients possess non-intersecting feature sets for the same sample. In\nreal-world scenarios, the clients may not receive data streaming for the\ndisjoint features for the same entity synchronously. Instead, the data are\ntypically generated by an \\emph{event} relevant to only a subset of clients. We\nare the first to identify these challenges in online VFL, which have been\noverlooked by previous research. To address these challenges, we proposed an\nevent-driven online VFL framework. In this framework, only a subset of clients\nwere activated during each event, while the remaining clients passively\ncollaborated in the learning process. Furthermore, we incorporated\n\\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by\nonline learning problems with non-convex models within a non-stationary\nenvironment. We conducted a comprehensive regret analysis of our proposed\nframework, specifically examining the DLR under non-convex conditions with\nevent-driven online VFL. Extensive experiments demonstrated that our proposed\nframework was more stable than the existing online VFL framework under\nnon-stationary data conditions while also significantly reducing communication\nand computation costs.", "AI": {"tldr": "\u5728\u7ebf\u5b66\u4e60\u5728\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u4e2d\u6bd4\u79bb\u7ebf\u5b66\u4e60\u66f4\u5177\u9002\u5e94\u6027\uff0c\u4f46\u5176\u5728VFL\u4e2d\u7684\u6574\u5408\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u9a71\u52a8\u7684\u5728\u7ebfVFL\u6846\u67b6\uff0c\u4ec5\u6fc0\u6d3b\u90e8\u5206\u5ba2\u6237\u7aef\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u5c40\u90e8\u9057\u61be\uff08DLR\uff09\u89e3\u51b3\u975e\u51f8\u548c\u975e\u7a33\u6001\u73af\u5883\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u66f4\u7a33\u5b9a\u4e14\u964d\u4f4e\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u4e2d\u7684\u5728\u7ebf\u5b66\u4e60\u56e0\u5ba2\u6237\u7aef\u7279\u5f81\u96c6\u4e0d\u91cd\u53e0\u4e14\u6570\u636e\u5f02\u6b65\u5230\u8fbe\u800c\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u65b0\u7684\u6846\u67b6\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u9a71\u52a8\u7684\u5728\u7ebfVFL\u6846\u67b6\uff0c\u4ec5\u6fc0\u6d3b\u90e8\u5206\u5ba2\u6237\u7aef\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u5c40\u90e8\u9057\u61be\uff08DLR\uff09\u4ee5\u5e94\u5bf9\u975e\u51f8\u548c\u975e\u7a33\u6001\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u975e\u7a33\u6001\u6570\u636e\u6761\u4ef6\u4e0b\u6bd4\u73b0\u6709\u6846\u67b6\u66f4\u7a33\u5b9a\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VFL\u4e2d\u5728\u7ebf\u5b66\u4e60\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.15314", "pdf": "https://arxiv.org/pdf/2506.15314", "abs": "https://arxiv.org/abs/2506.15314", "authors": ["Jason Dong", "Anna Wu"], "title": "Case Study for Developing a UXR Point of View for FinOps Product Innovation", "categories": ["cs.HC"], "comment": null, "summary": "In the dynamic landscape of Cloud financial management, we are sharing a case\nstudy exploring the development of a User Experience Research (UXR) Point of\nView (PoV) to drive FinOps product innovation. We demonstrate how qualitative\nand quantitative research methods working together to navigate the challenges\nof understanding customer needs, aligning cross-functional teams, and\nprioritizing limited resources. Through a multi-phased research approach, the\nresearch team identifies opportunities, quantifies pain points, and segments\ndiverse customer cohorts. This culminated in a UXR PoV that informed the\ncreation of a differentiated product strategy, a 'one-stop shop' dashboard\nempowering FinOps practitioners with actionable insights and tools. This case\nstudy highlights the power of mixed-methods research in uncovering actionable\ninsights that drive impactful product innovation.", "AI": {"tldr": "\u5728\u4e91\u8d22\u52a1\u7ba1\u7406\u9886\u57df\uff0c\u901a\u8fc7\u5b9a\u6027\u5b9a\u91cf\u6df7\u5408\u7814\u7a76\u65b9\u6cd5\uff0c\u7814\u7a76\u56e2\u961f\u8bc6\u522b\u673a\u4f1a\u3001\u91cf\u5316\u75db\u70b9\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u7ad9\u5f0f\u4eea\u8868\u76d8\uff0c\u4e3aFinOps\u5b9e\u8df5\u8005\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u4e91\u8d22\u52a1\u7ba1\u7406\u4e2d\u7ed3\u5408\u7528\u6237\u7814\u7a76\u4ee5\u63a8\u52a8FinOps\u4ea7\u54c1\u521b\u65b0\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u5b9a\u6027\u5b9a\u91cf\u6df7\u5408\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u6790\u5ba2\u6237\u9700\u6c42\u5e76\u534f\u8c03\u8de8\u804c\u80fd\u56e2\u961f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u7ad9\u5f0f\u4eea\u8868\u76d8\uff0c\u4e3aFinOps\u5b9e\u8df5\u8005\u63d0\u4f9b\u53ef\u64cd\u4f5c\u5de5\u5177\u3002", "conclusion": "\u6df7\u5408\u7814\u7a76\u65b9\u6cd5\u80fd\u6709\u6548\u63ed\u793a\u63a8\u52a8\u4ea7\u54c1\u521b\u65b0\u7684\u5173\u952e\u6d1e\u5bdf\u3002"}}
{"id": "2506.15325", "pdf": "https://arxiv.org/pdf/2506.15325", "abs": "https://arxiv.org/abs/2506.15325", "authors": ["Festus Adedoyin", "Huseyin Dogan"], "title": "Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook", "categories": ["cs.HC"], "comment": null, "summary": "Advancements in Artificial Intelligence (AI) have significantly transformed\nthe financial industry, enabling the development of more personalised and\nadaptable financial products and services. This research paper explores various\ninstances where Human-Centred AI (HCAI) has facilitated these advancements,\ndrawing from contemporary studies and industry progress. The paper examines how\nthe application of HCAI-powered data analytics, machine learning, and natural\nlanguage processing enables financial institutions to gain a deeper\nunderstanding of their customers' unique needs, preferences, and behavioural\npatterns. This, in turn, allows for the creation of tailored financial\nsolutions that address individual consumer requirements, ultimately enhancing\noverall user experience and satisfaction. Additionally, the study highlights\nthe integration of AI-powered robo-advisory services, which offer customised\ninvestment recommendations and portfolio management tailored to diverse risk\nprofiles and investment goals. Moreover, the paper underscores the role of AI\nin strengthening fraud detection, risk assessment, and regulatory compliance,\nleading to a more secure and adaptable financial landscape. The findings of\nthis research demonstrate the substantial impact of Human-Centred AI on the\nfinancial industry, offering a strategic framework for financial institutions\nto leverage these technologies. By incorporating a User Experience Research\n(UXR) Point of View (PoV), financial institutions can ensure that AI-driven\nsolutions align with user needs and business objectives.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\uff08HCAI\uff09\u5982\u4f55\u63a8\u52a8\u91d1\u878d\u884c\u4e1a\u7684\u4e2a\u6027\u5316\u670d\u52a1\uff0c\u5305\u62ec\u6570\u636e\u5206\u6790\u3001\u673a\u5668\u4eba\u54a8\u8be2\u548c\u98ce\u9669\u7ba1\u7406\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76HCAI\u5728\u91d1\u878d\u884c\u4e1a\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u6ee1\u610f\u5ea6\u3002", "method": "\u901a\u8fc7\u5f53\u4ee3\u7814\u7a76\u548c\u884c\u4e1a\u8fdb\u5c55\uff0c\u5206\u6790HCAI\u5728\u6570\u636e\u5206\u6790\u3001\u673a\u5668\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "HCAI\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u6027\u5316\u91d1\u878d\u670d\u52a1\u7684\u8d28\u91cf\uff0c\u589e\u5f3a\u4e86\u6b3a\u8bc8\u68c0\u6d4b\u548c\u98ce\u9669\u7ba1\u7406\u3002", "conclusion": "\u7ed3\u5408\u7528\u6237\u4f53\u9a8c\u7814\u7a76\uff0c\u91d1\u878d\u884c\u4e1a\u53ef\u8fdb\u4e00\u6b65\u5229\u7528HCAI\u6280\u672f\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002"}}
{"id": "2506.15264", "pdf": "https://arxiv.org/pdf/2506.15264", "abs": "https://arxiv.org/abs/2506.15264", "authors": ["M\u00e9lanie Cambus", "Darya Melnyk", "Tijana Milentijevi\u0107", "Stefan Schmid"], "title": "Centroid Approximation for Byzantine-Tolerant Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": "19 pages, 10 figures", "summary": "Federated learning allows each client to keep its data locally when training\nmachine learning models in a distributed setting. Significant recent research\nestablished the requirements that the input must satisfy in order to guarantee\nconvergence of the training loop. This line of work uses averaging as the\naggregation rule for the training models. In particular, we are interested in\nwhether federated learning is robust to Byzantine behavior, and observe and\ninvestigate a tradeoff between the average/centroid and the validity conditions\nfrom distributed computing. We show that the various validity conditions alone\ndo not guarantee a good approximation of the average. Furthermore, we show that\nreaching good approximation does not give good results in experimental settings\ndue to possible Byzantine outliers. Our main contribution is the first lower\nbound of $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ on the centroid approximation under\nbox validity that is often considered in the literature, where $n$ is the\nnumber of clients, $t$ the upper bound on the number of Byzantine faults, and\n$d$ is the dimension of the machine learning model. We complement this lower\nbound by an upper bound of $2\\min\\{n,\\sqrt{d}\\}$, by providing a new analysis\nfor the case $n<d$. In addition, we present a new algorithm that achieves a\n$\\sqrt{2d}$-approximation under convex validity, which also proves that the\nexisting lower bound in the literature is tight. We show that all presented\nbounds can also be achieved in the distributed peer-to-peer setting. We\ncomplement our analytical results with empirical evaluations in federated\nstochastic gradient descent and federated averaging settings.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u8981\u6c42\u8f93\u5165\u6ee1\u8db3\u6536\u655b\u6761\u4ef6\uff0c\u4f46\u4ee5\u5f80\u7814\u7a76\u5ffd\u7565\u4e86\u5bf9\u62dc\u5360\u5ead\u884c\u4e3a\u7684\u9c81\u68d2\u6027\u3002\u4f5c\u8005\u7814\u7a76\u4e86\u5e73\u5747/\u8d28\u5fc3\u4e0e\u5206\u5e03\u5f0f\u8ba1\u7b97\u6709\u6548\u6027\u6761\u4ef6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u4e0a\u4e0b\u754c\u7ed3\u679c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7b97\u6cd5\uff0c\u7406\u8bba\u548c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u8054\u90a6\u5b66\u4e60\u5728\u62dc\u5360\u5ead\u884c\u4e3a\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u8d28\u5fc3\u8fd1\u4f3c\u95ee\u9898\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u6709\u6548\u6027\u4e0e\u8fd1\u4f3c\u8d28\u91cf\u4e4b\u95f4\u7684\u672a\u89e3\u6743\u8861\u3002", "method": "\u7ed3\u5408\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u4e0b\u754c\u548c\u4e0a\u754c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5728\u51f8\u6709\u6548\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0$\u221a{2d}$\u8fd1\u4f3c\u7684\u65b0\u7b97\u6cd5\u3002", "result": "\u9996\u6b21\u7ed9\u51fa\u4e86\u5728\u6587\u732e\u5e38\u89c1\u7684\u7bb1\u6709\u6548\u6027\u6761\u4ef6\u4e0b\u7684\u8d28\u5fc3\u8fd1\u4f3c\u4e0b\u754c$\u221a{d}$\uff0c\u5e76\u9a8c\u8bc1\u4e86\u65b0\u7b97\u6cd5\u7684\u7406\u8bba\u754c\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u7684\u6709\u6548\u6027\u6761\u4ef6\u548c\u8d28\u5fc3\u8fd1\u4f3c\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u5173\u7cfb\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u548c\u7406\u8bba\u7ed3\u679c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.15332", "pdf": "https://arxiv.org/pdf/2506.15332", "abs": "https://arxiv.org/abs/2506.15332", "authors": ["Patricia Diaz"], "title": "Building Blocks of a User Experience Research Point of View", "categories": ["cs.HC"], "comment": null, "summary": "This paper presents three User Experience Research (UXR) perspectives based\non data, evidence and insights - known as Point of View (POV) - showcasing how\nthe strategies and methods of building a POV work in an enterprise setting. The\nPOV are: 1. Smart Visuals: Use AI to extract and translate text from visuals in\nvideos (2019). 2. Assessable Code Editor: Focus on direct AI-feedback to the\nlearner as it is the loop that requires the least effort for the highest\nimpact(2023). 3. Opportunity Landscape: Identify high-impact opportunities at\nthe intersection of emergent technical capabilities that unlock novel\napproaches to critical user needs while addressing business strategic\npriorities (2019). They all seemed far-fetched and went against common\npractice. All were adopted and had long-lasting impact.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e09\u79cd\u57fa\u4e8e\u6570\u636e\u3001\u8bc1\u636e\u548c\u6d1e\u5bdf\u7684\u7528\u6237\u4f53\u9a8c\u7814\u7a76\u89c6\u89d2\uff08POV\uff09\uff0c\u5c55\u793a\u5176\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u4e0e\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u521b\u65b0\u7684\u7528\u6237\u4f53\u9a8c\u7814\u7a76\u65b9\u6cd5\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "method": "\u4e09\u79cdPOV\u65b9\u6cd5\uff1a1.\u667a\u80fd\u89c6\u89c9\uff082019\u5e74\uff09\uff1b2.\u53ef\u8bc4\u4f30\u4ee3\u7801\u7f16\u8f91\u5668\uff082023\u5e74\uff09\uff1b3.\u673a\u4f1a\u56fe\u8c31\uff082019\u5e74\uff09\u3002", "result": "\u5c3d\u7ba1\u65b9\u6cd5\u770b\u4f3c\u4e0d\u5207\u5b9e\u9645\uff0c\u4f46\u5747\u88ab\u91c7\u7eb3\u5e76\u4ea7\u751f\u6301\u4e45\u5f71\u54cd\u3002", "conclusion": "\u521b\u65b0\u7684\u7528\u6237\u4f53\u9a8c\u7814\u7a76\u65b9\u6cd5\u53ef\u4ee5\u7a81\u7834\u4f20\u7edf\u5b9e\u8df5\uff0c\u5b9e\u73b0\u663e\u8457\u6210\u6548\u3002"}}
{"id": "2506.15626", "pdf": "https://arxiv.org/pdf/2506.15626", "abs": "https://arxiv.org/abs/2506.15626", "authors": ["Vincent Roca", "Marc Tommasi", "Paul Andrey", "Aur\u00e9lien Bellet", "Markus D. Schirmer", "Hilde Henon", "Laurent Puy", "Julien Ramon", "Gr\u00e9gory Kuchcinski", "Martin Bretzner", "Renaud Lopes"], "title": "Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "$\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u7f3a\u8840\u6027\u5352\u4e2d\u60a3\u8005\u4e2d\u7528\u4e8e\u5927\u8111\u5e74\u9f84\u9884\u6d4b\u7684\u8868\u73b0\uff0c\u53d1\u73b0FL\u5728\u4e0d\u96c6\u4e2d\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u51c6\u786e\u9884\u6d4b\uff0c\u4e14\u5927\u8111\u5e74\u9f84\u5dee\u5f02\uff08BrainAGE\uff09\u4e0e\u8840\u7ba1\u98ce\u9669\u56e0\u7d20\u53ca\u5352\u4e2d\u540e\u529f\u80fd\u6062\u590d\u663e\u8457\u76f8\u5173\u3002", "motivation": "\u4f20\u7edf\u7684BrainAGE\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u4f46\u9690\u79c1\u95ee\u9898\u9650\u5236\u4e86\u6570\u636e\u5171\u4eab\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22FL\u662f\u5426\u80fd\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u4f9b\u51c6\u786e\u7684BrainAGE\u9884\u6d4b\uff0c\u5e76\u9a8c\u8bc1\u5176\u4e0e\u4e34\u5e8a\u8868\u578b\u548c\u529f\u80fd\u9884\u540e\u7684\u5173\u8054\u3002", "method": "\u7814\u7a76\u4f7f\u75281674\u540d\u5352\u4e2d\u60a3\u8005\u7684FLAIR\u8111\u5f71\u50cf\u6570\u636e\uff0c\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5b66\u4e60\u3001FL\u548c\u5355\u4e2d\u5fc3\u5b66\u4e60\u4e09\u79cd\u7b56\u7565\u7684\u9884\u6d4b\u6548\u679c\uff0c\u5e76\u5206\u6790\u4e86BrainAGE\u4e0e\u8840\u7ba1\u98ce\u9669\u56e0\u7d20\u53ca\u529f\u80fd\u9884\u540e\u7684\u5173\u7cfb\u3002", "result": "FL\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5355\u4e2d\u5fc3\u6a21\u578b\uff0c\u4e14BrainAGE\u4e0e\u7cd6\u5c3f\u75c5\u7b49\u98ce\u9669\u56e0\u7d20\u53ca\u5352\u4e2d\u540e\u529f\u80fd\u6062\u590d\u663e\u8457\u76f8\u5173\u3002\u96c6\u4e2d\u5b66\u4e60\u8868\u73b0\u6700\u4f73\uff0c\u4f46FL\u5728\u9690\u79c1\u4fdd\u62a4\u524d\u63d0\u4e0b\u4ecd\u5177\u6f5c\u529b\u3002", "conclusion": "FL\u80fd\u5728\u4e0d\u96c6\u4e2d\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u51c6\u786e\u7684BrainAGE\u9884\u6d4b\u3002BrainAGE\u4e0e\u8840\u7ba1\u98ce\u9669\u56e0\u7d20\u53ca\u5352\u4e2d\u540e\u6062\u590d\u7684\u5173\u8054\u8868\u660e\u5176\u5728\u5352\u4e2d\u9884\u540e\u5efa\u6a21\u4e2d\u7684\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2506.15468", "pdf": "https://arxiv.org/pdf/2506.15468", "abs": "https://arxiv.org/abs/2506.15468", "authors": ["Ryota Okumura", "Tadahiro Taniguchi", "Akira Taniguchi", "Yoshinobu Hagiwara"], "title": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534f\u540c\u521b\u9020\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u4eba\u7c7b\u4e0eAI\u5171\u540c\u6574\u5408\u90e8\u5206\u611f\u77e5\u4fe1\u606f\u548c\u77e5\u8bc6\u6765\u6784\u5efa\u5171\u4eab\u7684\u5916\u90e8\u8868\u5f81\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u57fa\u4e8eMHNG\u7684\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u7b26\u53f7\u7cfb\u7edf\u5171\u4eab\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfAI\u6559\u5b66\u4e2d\u5355\u5411\u77e5\u8bc6\u4f20\u9012\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u4eba\u7c7b\u4e0eAI\u5728\u5f02\u6784\u6a21\u6001\u4fe1\u606f\u6574\u5408\u4e2d\u7684\u534f\u540c\u5b66\u4e60\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8eMetropolis-Hastings\u547d\u540d\u6e38\u620f\uff08MHNG\uff09\u7684\u4eba\u7c7b-AI\u4ea4\u4e92\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u7ebf\u5b9e\u9a8c\u6bd4\u8f83\u4e09\u79cdAI\u4ee3\u7406\u7c7b\u578b\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u57fa\u4e8eMHNG\u7684AI\u4ee3\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5171\u4eab\u7b26\u53f7\u7cfb\u7edf\u7684\u5f62\u6210\uff0c\u4eba\u7c7b\u884c\u4e3a\u4e0eMHNG\u6982\u7387\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4eba\u7c7b-AI\u534f\u540c\u521b\u9020\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\uff0c\u4e3a\u5b9e\u73b0\u52a8\u6001\u5bf9\u9f50\u611f\u77e5\u7ecf\u9a8c\u7684\u5171\u751fAI\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.15497", "pdf": "https://arxiv.org/pdf/2506.15497", "abs": "https://arxiv.org/abs/2506.15497", "authors": ["Changzeng Fu"], "title": "Foundation of Affective Computing and Interaction", "categories": ["cs.HC"], "comment": null, "summary": "This book provides a comprehensive exploration of affective computing and\nhuman-computer interaction technologies. It begins with the historical\ndevelopment and basic concepts of human-computer interaction, delving into the\ntechnical frameworks and practical applications of emotional computing, visual\ninteraction, voice interaction, brain-computer interfaces, physiological\nelectrical signal analysis, and social robotics. The book covers a wide range\nof topics, including the psychological and neuroscience foundations of emotion,\nmultimodal emotion recognition, emotional expression mechanisms, and the\nprinciples of brain-computer interfaces.\n  Key technologies such as affective computing based on discrete emotion theory\nand dimensional models, visual perception principles, speech recognition and\nsynthesis, EEG signal acquisition and processing, and multimodal emotion\nrecognition are explained in detail. This book also addresses the technical\nchallenges in the field, including multimodal data fusion, privacy and\nsecurity, and ethical considerations in human-machine relationships. It\ndiscusses the applications of these technologies across various domains such as\neducation, healthcare, entertainment, and intelligent assistance.\n  Looking to the future, the book anticipates trends such as the deep\nintegration of artificial intelligence with emotion recognition, the\nadvancement of multimodal interaction technologies, and the development of more\npersonalized and adaptive emotion recognition systems. It emphasizes the\nimportance of balancing technological innovation with ethical considerations to\nensure the responsible development and application of affective computing\ntechnologies.", "AI": {"tldr": "\u672c\u4e66\u5168\u9762\u63a2\u8ba8\u4e86\u60c5\u611f\u8ba1\u7b97\u4e0e\u4eba\u673a\u4ea4\u4e92\u6280\u672f\uff0c\u6db5\u76d6\u5386\u53f2\u53d1\u5c55\u3001\u6280\u672f\u6846\u67b6\u3001\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u3001\u8111\u673a\u63a5\u53e3\u7b49\u9886\u57df\uff0c\u5e76\u8ba8\u8bba\u4e86\u6280\u672f\u6311\u6218\u4e0e\u672a\u6765\u8d8b\u52bf\u3002", "motivation": "\u7814\u7a76\u60c5\u611f\u8ba1\u7b97\u548c\u4eba\u673a\u4ea4\u4e92\u6280\u672f\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u63a8\u52a8\u8fd9\u4e9b\u6280\u672f\u5728\u6559\u80b2\u3001\u533b\u7597\u3001\u5a31\u4e50\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u540c\u65f6\u517c\u987e\u6280\u672f\u521b\u65b0\u4e0e\u4f26\u7406\u8003\u91cf\u3002", "method": "\u672c\u4e66\u901a\u8fc7\u4ecb\u7ecd\u60c5\u611f\u8ba1\u7b97\u7684\u591a\u6a21\u6001\u8bc6\u522b\u3001\u89c6\u89c9\u4ea4\u4e92\u3001\u8bed\u97f3\u4ea4\u4e92\u7b49\u6280\u672f\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u5fc3\u7406\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\u57fa\u7840\uff0c\u8be6\u7ec6\u5206\u6790\u4e86\u76f8\u5173\u6280\u672f\u3002", "result": "\u672c\u4e66\u5c55\u793a\u4e86\u60c5\u611f\u8ba1\u7b97\u548c\u4eba\u673a\u4ea4\u4e92\u6280\u672f\u7684\u591a\u6837\u5e94\u7528\u524d\u666f\uff0c\u5e76\u6307\u51fa\u4e86\u6570\u636e\u878d\u5408\u3001\u9690\u79c1\u5b89\u5168\u7b49\u6280\u672f\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u60c5\u611f\u8ba1\u7b97\u5c06\u4e0e\u4eba\u5de5\u667a\u80fd\u6df1\u5ea6\u878d\u5408\uff0c\u6280\u672f\u53d1\u5c55\u9700\u5e73\u8861\u521b\u65b0\u4e0e\u4f26\u7406\uff0c\u4ee5\u5b9e\u73b0\u8d1f\u8d23\u4efb\u7684\u5e94\u7528\u3002"}}
{"id": "2506.15512", "pdf": "https://arxiv.org/pdf/2506.15512", "abs": "https://arxiv.org/abs/2506.15512", "authors": ["Wenqi Guan", "Yang Fang"], "title": "Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Large Language Models have brought a radical change in the process of remote\nlearning students, among other aspects of educative activities. Current\nretrieval of remote learning resources lacks depth in contextual meaning that\nprovides comprehensive information on complex student queries. This work\nproposes a novel approach to enhancing remote learning retrieval by integrating\nGPT-based models within the LangChain framework. We achieve this system in a\nmore intuitive and productive manner using CoT reasoning and prompt\nengineering. The framework we propose puts much emphasis on increasing the\nprecision and relevance of the retrieval results to return comprehensive and\ncontextually enriched explanations and resources that best suit each student's\nneeds. We also assess the effectiveness of our approach against paradigmatic\nLLMs and report improvements in user satisfaction and learning outcomes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPT\u548cLangChain\u6846\u67b6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7CoT\u63a8\u7406\u548c\u63d0\u793a\u5de5\u7a0b\u63d0\u5347\u8fdc\u7a0b\u5b66\u4e60\u8d44\u6e90\u7684\u68c0\u7d22\u6548\u679c\uff0c\u63d0\u9ad8\u4e86\u7ed3\u679c\u7684\u7cbe\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u5f53\u524d\u8fdc\u7a0b\u5b66\u4e60\u8d44\u6e90\u68c0\u7d22\u7f3a\u4e4f\u5bf9\u590d\u6742\u67e5\u8be2\u7684\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u5f71\u54cd\u4e86\u5b66\u4e60\u6548\u679c\u3002", "method": "\u96c6\u6210GPT\u6a21\u578b\u5230LangChain\u6846\u67b6\uff0c\u7ed3\u5408CoT\u63a8\u7406\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u4f18\u5316\u68c0\u7d22\u7cfb\u7edf\u3002", "result": "\u76f8\u6bd4\u4f20\u7edfLLMs\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u5b66\u4e60\u6210\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u66f4\u7cbe\u51c6\u5730\u6ee1\u8db3\u5b66\u751f\u9700\u6c42\uff0c\u63d0\u5347\u8fdc\u7a0b\u5b66\u4e60\u4f53\u9a8c\u3002"}}
{"id": "2506.15525", "pdf": "https://arxiv.org/pdf/2506.15525", "abs": "https://arxiv.org/abs/2506.15525", "authors": ["Isabella Pu", "Prerna Ravi", "Linh Dieu Dinh", "Chelsea Joe", "Caitlin Ogoe", "Zixuan Li", "Cynthia Breazeal", "Anastasia K. Ostrowski"], "title": "\"How can we learn and use AI at the same time?:: Participatory Design of GenAI with High School Students", "categories": ["cs.HC"], "comment": "Copyright protected by ACM, 17 pages, 5 figures, 2 tables, in\n  proceedings of 24th annual ACM Interaction Design and Children Conference\n  (IDC 2025)", "summary": "As generative AI (GenAI) emerges as a transformative force, clear\nunderstanding of high school students' perspectives is essential for GenAI's\nmeaningful integration in high school environments. In this work, we draw\ninsights from a participatory design workshop where we engaged 17 high school\nstudents -- a group rarely involved in prior research in this area -- through\nthe design of novel GenAI tools and school policies addressing their key\nconcerns. Students identified challenges and developed solutions outlining\ntheir ideal features in GenAI tools, appropriate school use, and regulations.\nThese centered around the problem spaces of combating bias & misinformation,\ntackling crime & plagiarism, preventing over-reliance on AI, and handling false\naccusations of academic dishonesty. Building on our participants'\nunderrepresented perspectives, we propose new guidelines targeted at\neducational technology designers for development of GenAI technologies in high\nschools. We also argue for further incorporation of student voices in\ndevelopment of AI policies in their schools.", "AI": {"tldr": "\u9ad8\u4e2d\u751f\u5bf9\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u4e2d\u7684\u770b\u6cd5\u53ca\u9700\u6c42\u7814\u7a76\u3002", "motivation": "\u4e86\u89e3\u9ad8\u4e2d\u751f\u5bf9\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u7684\u89c2\u70b9\uff0c\u4ee5\u4fc3\u8fdb\u5176\u5728\u9ad8\u4e2d\u73af\u5883\u4e2d\u7684\u6709\u6548\u878d\u5408\u3002", "method": "\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u4e0e17\u540d\u9ad8\u4e2d\u751f\u5171\u540c\u8bbe\u8ba1GenAI\u5de5\u5177\u548c\u5b66\u6821\u653f\u7b56\u3002", "result": "\u5b66\u751f\u63d0\u51fa\u4e86\u89e3\u51b3\u504f\u89c1\u3001\u527d\u7a83\u3001\u8fc7\u5ea6\u4f9d\u8d56AI\u7b49\u95ee\u9898\u7684\u65b9\u6848\uff0c\u5e76\u5236\u5b9a\u4e86\u9488\u5bf9\u6559\u80b2\u6280\u672f\u8bbe\u8ba1\u8005\u7684\u65b0\u6307\u5357\u3002", "conclusion": "\u547c\u5401\u5728AI\u653f\u7b56\u5236\u5b9a\u4e2d\u66f4\u591a\u5730\u7eb3\u5165\u5b66\u751f\u58f0\u97f3\u3002"}}
{"id": "2506.13776", "pdf": "https://arxiv.org/pdf/2506.13776", "abs": "https://arxiv.org/abs/2506.13776", "authors": ["Kevin L. Wei", "Patricia Paskov", "Sunishchal Dev", "Michael J. Byun", "Anka Reuel", "Xavier Roberts-Gaal", "Rachel Calcott", "Evie Coxon", "Chinmay Deshpande"], "title": "Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations", "categories": ["cs.AI", "cs.CY", "cs.HC"], "comment": "A version of this paper has been accepted to ICML 2025 as a position\n  paper (spotlight), with the title: \"Position: Human Baselines in Model\n  Evaluations Need Rigor and Transparency (With Recommendations & Reporting\n  Checklist).\"", "summary": "In this position paper, we argue that human baselines in foundation model\nevaluations must be more rigorous and more transparent to enable meaningful\ncomparisons of human vs. AI performance, and we provide recommendations and a\nreporting checklist towards this end. Human performance baselines are vital for\nthe machine learning community, downstream users, and policymakers to interpret\nAI evaluations. Models are often claimed to achieve \"super-human\" performance,\nbut existing baselining methods are neither sufficiently rigorous nor\nsufficiently well-documented to robustly measure and assess performance\ndifferences. Based on a meta-review of the measurement theory and AI evaluation\nliteratures, we derive a framework with recommendations for designing,\nexecuting, and reporting human baselines. We synthesize our recommendations\ninto a checklist that we use to systematically review 115 human baselines\n(studies) in foundation model evaluations and thus identify shortcomings in\nexisting baselining methods; our checklist can also assist researchers in\nconducting human baselines and reporting results. We hope our work can advance\nmore rigorous AI evaluation practices that can better serve both the research\ncommunity and policymakers. Data is available at:\nhttps://github.com/kevinlwei/human-baselines", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5728\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u4e2d\u91c7\u7528\u66f4\u4e25\u8c28\u548c\u900f\u660e\u7684\u4eba\u7c7b\u57fa\u7ebf,\u4ee5\u66f4\u597d\u6bd4\u8f83\u4eba\u7c7b\u4e0eAI\u6027\u80fd,\u5e76\u63d0\u51fa\u4e86\u5efa\u8bae\u548c\u62a5\u544a\u6e05\u5355\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u57fa\u7ebf\u65b9\u6cd5\u4e0d\u591f\u4e25\u8c28\u4e14\u7f3a\u4e4f\u900f\u660e\u6027,\u65e0\u6cd5\u53ef\u9760\u8861\u91cf\u548c\u8bc4\u4f30\u6027\u80fd\u5dee\u5f02,\u5f71\u54cd\u4e86AI\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5143\u7efc\u8ff0\u6d4b\u91cf\u7406\u8bba\u548cAI\u8bc4\u4f30\u6587\u732e,\u63d0\u51fa\u4e86\u8bbe\u8ba1\u3001\u6267\u884c\u548c\u62a5\u544a\u4eba\u7c7b\u57fa\u7ebf\u7684\u6846\u67b6\u4e0e\u5efa\u8bae,\u5e76\u5236\u5b9a\u4e86\u68c0\u67e5\u6e05\u5355,\u7528\u4e8e\u7cfb\u7edf\u5ba1\u67e5115\u9879\u57fa\u7ebf\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u7684\u4e0d\u8db3,\u63d0\u4f9b\u68c0\u67e5\u6e05\u5355\u4ee5\u534f\u52a9\u7814\u7a76\u8005\u8fdb\u884c\u57fa\u7ebf\u548c\u62a5\u544a\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u5de5\u4f5c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u4e25\u8c28\u7684AI\u8bc4\u4f30\u5b9e\u8df5,\u670d\u52a1\u4e8e\u7814\u7a76\u793e\u533a\u548c\u653f\u7b56\u5236\u5b9a\u8005\u3002"}}
{"id": "2506.14774", "pdf": "https://arxiv.org/pdf/2506.14774", "abs": "https://arxiv.org/abs/2506.14774", "authors": ["Burcu Sayin", "Ipek Baris Schlicht", "Ngoc Vo Hong", "Sara Allievi", "Jacopo Staiano", "Pasquale Minervini", "Andrea Passerini"], "title": "MedSyn: Enhancing Diagnostics with Human-AI Collaboration", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": "Accepted to the Trustworthy and Collaborative Artificial Intelligence\n  Workshop 2025 (TCAI 2025) in the 4th International Conference Series on\n  Hybrid Human-Artificial Intelligence (HHAI 2025)", "summary": "Clinical decision-making is inherently complex, often influenced by cognitive\nbiases, incomplete information, and case ambiguity. Large Language Models\n(LLMs) have shown promise as tools for supporting clinical decision-making, yet\ntheir typical one-shot or limited-interaction usage may overlook the\ncomplexities of real-world medical practice. In this work, we propose a hybrid\nhuman-AI framework, MedSyn, where physicians and LLMs engage in multi-step,\ninteractive dialogues to refine diagnoses and treatment decisions. Unlike\nstatic decision-support tools, MedSyn enables dynamic exchanges, allowing\nphysicians to challenge LLM suggestions while the LLM highlights alternative\nperspectives. Through simulated physician-LLM interactions, we assess the\npotential of open-source LLMs as physician assistants. Results show open-source\nLLMs are promising as physician assistants in the real world. Future work will\ninvolve real physician interactions to further validate MedSyn's usefulness in\ndiagnostic accuracy and patient outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4eba\u673a\u6846\u67b6MedSyn\uff0c\u901a\u8fc7\u533b\u751f\u4e0eLLMs\u7684\u591a\u6b65\u4ea4\u4e92\u5bf9\u8bdd\u4f18\u5316\u8bca\u65ad\u548c\u6cbb\u7597\u51b3\u7b56\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u590d\u6742\u4e14\u6613\u53d7\u8ba4\u77e5\u504f\u5dee\u5f71\u54cd\uff0c\u73b0\u6709LLMs\u5de5\u5177\u56e0\u5355\u6b21\u4e92\u52a8\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faMedSyn\u6846\u67b6\uff0c\u652f\u6301\u533b\u751f\u4e0eLLMs\u52a8\u6001\u4ea4\u4e92\uff0c\u6311\u6218\u4e0e\u53cd\u9988\u5e76\u5b58\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f00\u6e90LLMs\u4f5c\u4e3a\u533b\u751f\u52a9\u624b\u6f5c\u529b\u663e\u8457\u3002", "conclusion": "\u672a\u6765\u9700\u901a\u8fc7\u771f\u5b9e\u533b\u751f\u4e92\u52a8\u8fdb\u4e00\u6b65\u9a8c\u8bc1MedSyn\u5bf9\u8bca\u65ad\u51c6\u786e\u6027\u548c\u60a3\u8005\u7ed3\u5c40\u7684\u6539\u5584\u6548\u679c\u3002"}}
{"id": "2506.14783", "pdf": "https://arxiv.org/pdf/2506.14783", "abs": "https://arxiv.org/abs/2506.14783", "authors": ["Mohamed Masry", "Mohamed Amen", "Mohamed Elzyat", "Mohamed Hamed", "Norhan Magdy", "Maram Khaled"], "title": "ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification", "categories": ["cs.LG", "cs.CL", "cs.HC"], "comment": "Graduation project report submitted at Faculty of Computer Science\n  and Artificial Intelligence, Helwan University", "summary": "Decoding natural language from brain activity using non-invasive\nelectroencephalography (EEG) remains a significant challenge in neuroscience\nand machine learning, particularly for open-vocabulary scenarios where\ntraditional methods struggle with noise and variability. Previous studies have\nachieved high accuracy on small-closed vocabularies, but it still struggles on\nopen vocabularies. In this study, we propose ETS, a framework that integrates\nEEG with synchronized eye-tracking data to address two critical tasks: (1)\nopen-vocabulary text generation and (2) sentiment classification of perceived\nlanguage. Our model achieves a superior performance on BLEU and Rouge score for\nEEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment\nclassification, which significantly outperforms supervised baselines.\nFurthermore, we show that our proposed model can handle data from various\nsubjects and sources, showing great potential for high performance open\nvocabulary eeg-to-text system.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8111\u7535\u56fe\uff08EEG\uff09\u548c\u540c\u6b65\u773c\u52a8\u6570\u636e\u7684\u6846\u67b6ETS\uff0c\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u89e3\u7801\u548c\u60c5\u611f\u5206\u7c7b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u975e\u4fb5\u5165\u5f0fEEG\u5728\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u4e0b\u89e3\u7801\u81ea\u7136\u8bed\u8a00\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u566a\u58f0\u548c\u53d8\u5f02\u6027\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u96c6\u6210EEG\u4e0e\u773c\u52a8\u6570\u636e\uff0c\u63d0\u51faETS\u6846\u67b6\uff0c\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u6587\u672c\u751f\u6210\u548c\u60c5\u611f\u5206\u7c7b\u3002", "result": "\u5728BLEU\u548cRouge\u5206\u6570\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u60c5\u611f\u5206\u7c7bF1\u5206\u6570\u63d0\u534710%\uff0c\u4e14\u9002\u5e94\u591a\u6765\u6e90\u6570\u636e\u3002", "conclusion": "ETS\u6846\u67b6\u5728\u5f00\u653e\u8bcd\u6c47EEG\u5230\u6587\u672c\u7cfb\u7edf\u4e2d\u5177\u6709\u9ad8\u6027\u80fd\u6f5c\u529b\u3002"}}
{"id": "2506.14854", "pdf": "https://arxiv.org/pdf/2506.14854", "abs": "https://arxiv.org/abs/2506.14854", "authors": ["Varun Mannam", "Zhenyu Shi"], "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "Submitting to ICCV 2025 workshop:\n  https://retailvisionworkshop.github.io/", "summary": "Accurate video annotation plays a vital role in modern retail applications,\nincluding customer behavior analysis, product interaction detection, and\nin-store activity recognition. However, conventional annotation methods heavily\nrely on time-consuming manual labeling by human annotators, introducing\nnon-robust frame selection and increasing operational costs. To address these\nchallenges in the retail domain, we propose a deep learning-based approach that\nautomates key-frame identification in retail videos and provides automatic\nannotations of products and customers. Our method leverages deep neural\nnetworks to learn discriminative features by embedding video frames and\nincorporating object detection-based techniques tailored for retail\nenvironments. Experimental results showcase the superiority of our approach\nover traditional methods, achieving accuracy comparable to human annotator\nlabeling while enhancing the overall efficiency of retail video annotation.\nRemarkably, our approach leads to an average of 2 times cost savings in video\nannotation. By allowing human annotators to verify/adjust less than 5% of\ndetected frames in the video dataset, while automating the annotation process\nfor the remaining frames without reducing annotation quality, retailers can\nsignificantly reduce operational costs. The automation of key-frame detection\nenables substantial time and effort savings in retail video labeling tasks,\nproving highly valuable for diverse retail applications such as shopper journey\nanalysis, product interaction detection, and in-store security monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u96f6\u552e\u89c6\u9891\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u96f6\u552e\u89c6\u9891\u6807\u6ce8\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\uff0c\u8017\u65f6\u4e14\u4e0d\u9c81\u68d2\uff0c\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u8c61\u68c0\u6d4b\u6280\u672f\uff0c\u81ea\u52a8\u8bc6\u522b\u5173\u952e\u5e27\u5e76\u4e3a\u96f6\u552e\u89c6\u9891\u63d0\u4f9b\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u6807\u6ce8\uff0c\u51c6\u786e\u7387\u63a5\u8fd1\u4eba\u5de5\u6807\u6ce8\uff0c\u8282\u770150%\u6210\u672c\u3002", "conclusion": "\u81ea\u52a8\u5316\u5173\u952e\u5e27\u68c0\u6d4b\u5927\u5e45\u8282\u7701\u96f6\u552e\u89c6\u9891\u6807\u6ce8\u65f6\u95f4\u548c\u6210\u672c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u96f6\u552e\u5e94\u7528\u3002"}}
{"id": "2506.15085", "pdf": "https://arxiv.org/pdf/2506.15085", "abs": "https://arxiv.org/abs/2506.15085", "authors": ["Paige Tutt\u00f6s\u00ed", "Shivam Mehta", "Zachary Syvenky", "Bermet Burkanova", "Gustav Eje Henter", "Angelica Lim"], "title": "EmojiVoice: Towards long-term controllable expressivity in robot speech", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to RO-MAN 2025, Demo at HRI 2025 :\n  https://dl.acm.org/doi/10.5555/3721488.3721774", "summary": "Humans vary their expressivity when speaking for extended periods to maintain\nengagement with their listener. Although social robots tend to be deployed with\n``expressive'' joyful voices, they lack this long-term variation found in human\nspeech. Foundation model text-to-speech systems are beginning to mimic the\nexpressivity in human speech, but they are difficult to deploy offline on\nrobots. We present EmojiVoice, a free, customizable text-to-speech (TTS)\ntoolkit that allows social roboticists to build temporally variable, expressive\nspeech on social robots. We introduce emoji-prompting to allow fine-grained\ncontrol of expressivity on a phase level and use the lightweight Matcha-TTS\nbackbone to generate speech in real-time. We explore three case studies: (1) a\nscripted conversation with a robot assistant, (2) a storytelling robot, and (3)\nan autonomous speech-to-speech interactive agent. We found that using varied\nemoji prompting improved the perception and expressivity of speech over a long\nperiod in a storytelling task, but expressive voice was not preferred in the\nassistant use case.", "AI": {"tldr": "EmojiVoice\u662f\u4e00\u4e2a\u514d\u8d39\u3001\u53ef\u5b9a\u5236\u7684TTS\u5de5\u5177\u5305\uff0c\u65e8\u5728\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u957f\u671f\u53ef\u53d8\u7684\u8868\u60c5\u8bed\u97f3\uff0c\u901a\u8fc7\u8868\u60c5\u7b26\u53f7\u63d0\u793a\u5b9e\u73b0\u7cbe\u7ec6\u63a7\u5236\uff0c\u5e76\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u793e\u4ea4\u673a\u5668\u4eba\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u6109\u5feb\u8bed\u97f3\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u8bed\u97f3\u4e2d\u7684\u957f\u671f\u53d8\u5316\u3002EmojiVoice\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u53ef\u53d8\u7684\u8868\u60c5\u8bed\u97f3\u3002", "method": "\u4f7f\u7528EmojiVoice\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u8868\u60c5\u7b26\u53f7\u63d0\u793a\uff08emoji-prompting\uff09\u5b9e\u73b0\u8bed\u97f3\u8868\u8fbe\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7Matcha-TTS\u5b9e\u65f6\u751f\u6210\u8bed\u97f3\u3002\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u6548\u679c\u3002", "result": "\u5728\u8bb2\u6545\u4e8b\u4efb\u52a1\u4e2d\uff0c\u591a\u6837\u5316\u7684\u8868\u60c5\u7b26\u53f7\u63d0\u793a\u63d0\u5347\u4e86\u8bed\u97f3\u7684\u611f\u77e5\u548c\u8868\u8fbe\u529b\uff1b\u4f46\u5728\u52a9\u624b\u7528\u4f8b\u4e2d\uff0c\u8868\u60c5\u8bed\u97f3\u5e76\u672a\u53d7\u5230\u504f\u597d\u3002", "conclusion": "EmojiVoice\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53ef\u53d8\u8868\u60c5\u8bed\u97f3\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u9002\u7528\u6027\u9700\u6839\u636e\u5177\u4f53\u573a\u666f\u8c03\u6574\u3002"}}
{"id": "2506.15107", "pdf": "https://arxiv.org/pdf/2506.15107", "abs": "https://arxiv.org/abs/2506.15107", "authors": ["Paige Tutt\u00f6s\u00ed"], "title": "I Know You're Listening: Adaptive Voice for HRI", "categories": ["cs.RO", "cs.HC", "cs.SD", "eess.AS"], "comment": "PhD Thesis Simon Fraser University https://summit.sfu.ca/item/39353\n  Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts IROS\n  23 Mmm whatcha say? Uncovering distal and proximal context effects in first\n  and second-language word perception using psychophysical reverse correlation\n  INTERSPEECH 24 Emojivoice: Towards long-term controllable expressivity in\n  robot speech RO-MAN 25", "summary": "While the use of social robots for language teaching has been explored, there\nremains limited work on a task-specific synthesized voices for language\nteaching robots. Given that language is a verbal task, this gap may have severe\nconsequences for the effectiveness of robots for language teaching tasks. We\naddress this lack of L2 teaching robot voices through three contributions: 1.\nWe address the need for a lightweight and expressive robot voice. Using a\nfine-tuned version of Matcha-TTS, we use emoji prompting to create an\nexpressive voice that shows a range of expressivity over time. The voice can\nrun in real time with limited compute resources. Through case studies, we found\nthis voice more expressive, socially appropriate, and suitable for long periods\nof expressive speech, such as storytelling. 2. We explore how to adapt a\nrobot's voice to physical and social ambient environments to deploy our voices\nin various locations. We found that increasing pitch and pitch rate in noisy\nand high-energy environments makes the robot's voice appear more appropriate\nand makes it seem more aware of its current environment. 3. We create an\nEnglish TTS system with improved clarity for L2 listeners using known\nlinguistic properties of vowels that are difficult for these listeners. We used\na data-driven, perception-based approach to understand how L2 speakers use\nduration cues to interpret challenging words with minimal tense (long) and lax\n(short) vowels in English. We found that the duration of vowels strongly\ninfluences the perception for L2 listeners and created an \"L2 clarity mode\" for\nMatcha-TTS that applies a lengthening to tense vowels while leaving lax vowels\nunchanged. Our clarity mode was found to be more respectful, intelligible, and\nencouraging than base Matcha-TTS while reducing transcription errors in these\nchallenging tense/lax minimal pairs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4e3a\u8bed\u8a00\u6559\u5b66\u673a\u5668\u4eba\u5f00\u53d1\u4efb\u52a1\u7279\u5b9a\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u8868\u8fbe\u4e30\u5bcc\u7684\u8bed\u97f3\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u9002\u5e94\u73af\u5883\u8c03\u6574\u548c\u9488\u5bf9\u7b2c\u4e8c\u8bed\u8a00\u5b66\u4e60\u8005\u7684\u6e05\u6670\u5ea6\u6a21\u5f0f\u6765\u63d0\u9ad8\u6559\u5b66\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e2d\u7f3a\u4e4f\u9488\u5bf9\u8bed\u8a00\u6559\u5b66\u673a\u5668\u4eba\u7684\u4efb\u52a1\u7279\u5b9a\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u673a\u5668\u4eba\u8bed\u8a00\u6559\u5b66\u7684\u6709\u6548\u6027\u3002\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "1. \u4f7f\u7528Matcha-TTS\u7684\u5fae\u8c03\u7248\u672c\uff0c\u901a\u8fc7\u8868\u60c5\u7b26\u53f7\u63d0\u793a\u521b\u5efa\u5b9e\u65f6\u8fd0\u884c\u7684\u8f7b\u91cf\u7ea7\u9ad8\u8868\u8fbe\u8bed\u97f3\uff1b2. \u63d0\u51fa\u673a\u5668\u4eba\u8bed\u97f3\u9002\u5e94\u7269\u7406\u548c\u793e\u4ea4\u73af\u5883\u7684\u8c03\u6574\u7b56\u7565\uff1b3. \u5f00\u53d1\u9488\u5bf9\u7b2c\u4e8c\u8bed\u8a00\u5b66\u4e60\u8005\u7684\u6e05\u6670\u5ea6\u6a21\u5f0f\uff0c\u901a\u8fc7\u8c03\u6574\u5143\u97f3\u65f6\u957f\u63d0\u9ad8\u8bed\u97f3\u6e05\u6670\u5ea6\u3002", "result": "1. \u9ad8\u8868\u8fbe\u8bed\u97f3\u5728\u957f\u671f\u8868\u8fbe\u6027\u4efb\u52a1\uff08\u5982\u8bb2\u6545\u4e8b\uff09\u4e2d\u8868\u73b0\u66f4\u4f18\uff1b2. \u73af\u5883\u9002\u5e94\u6027\u8c03\u6574\u4f7f\u8bed\u97f3\u66f4\u7b26\u5408\u73af\u5883\u9700\u6c42\uff1b3. \u6e05\u6670\u5ea6\u6a21\u5f0f\u663e\u8457\u63d0\u9ad8\u4e86\u7b2c\u4e8c\u8bed\u8a00\u5b66\u4e60\u8005\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u51cf\u5c11\u4e86\u8f6c\u5f55\u9519\u8bef\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u4e09\u79cd\u8d21\u732e\u4e3a\u8bed\u8a00\u6559\u5b66\u673a\u5668\u4eba\u5f00\u53d1\u4e86\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\uff0c\u63d0\u5347\u4e86\u8bed\u97f3\u8868\u8fbe\u529b\u548c\u5b66\u4e60\u8005\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2506.15278", "pdf": "https://arxiv.org/pdf/2506.15278", "abs": "https://arxiv.org/abs/2506.15278", "authors": ["Reuben Binns", "Jake Stein", "Siddhartha Datta", "Max Van Kleek", "Nigel Shadbolt"], "title": "Not Even Nice Work If You Can Get It; A Longitudinal Study of Uber's Algorithmic Pay and Pricing", "categories": ["cs.CY", "cs.HC"], "comment": null, "summary": "Ride-sharing platforms like Uber market themselves as enabling `flexibility'\nfor their workforce, meaning that drivers are expected to anticipate when and\nwhere the algorithm will allocate them jobs, and how well remunerated those\njobs will be. In this work we describe our process of participatory action\nresearch with drivers and trade union organisers, culminating in a\nparticipatory audit of Uber's algorithmic pay and work allocation, before and\nafter the introduction of dynamic pricing. Through longitudinal analysis of 1.5\nmillion trips from 258 drivers in the UK, we find that after dynamic pricing,\npay has decreased, Uber's cut has increased, job allocation and pay is less\npredictable, inequality between drivers is increased, and drivers spend more\ntime waiting for jobs. In addition to these findings, we provide methodological\nand theoretical contributions to algorithm auditing, gig work, and the emerging\npractice of worker data science.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u53c2\u4e0e\u5f0f\u5ba1\u8ba1\u53d1\u73b0\uff0cUber\u5f15\u5165\u52a8\u6001\u5b9a\u4ef7\u540e\uff0c\u53f8\u673a\u6536\u5165\u4e0b\u964d\u3001\u5e73\u53f0\u5206\u6210\u589e\u52a0\uff0c\u5de5\u4f5c\u5206\u914d\u4e0e\u85aa\u916c\u66f4\u4e0d\u53ef\u9884\u6d4b\uff0c\u53f8\u673a\u95f4\u4e0d\u5e73\u7b49\u52a0\u5267\uff0c\u7b49\u5f85\u65f6\u95f4\u66f4\u957f\u3002", "motivation": "\u63a2\u8ba8Uber\u7b49\u5e73\u53f0\u5982\u4f55\u901a\u8fc7\u7b97\u6cd5\u5f71\u54cd\u53f8\u673a\u7684\u7075\u6d3b\u6027\u4e0e\u6536\u5165\uff0c\u5e76\u5229\u7528\u53c2\u4e0e\u5f0f\u65b9\u6cd5\u63ed\u793a\u52a8\u6001\u5b9a\u4ef7\u5bf9\u53f8\u673a\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u53c2\u4e0e\u5f0f\u884c\u52a8\u7814\u7a76\u4e0e\u5ba1\u8ba1\uff0c\u5206\u6790\u82f1\u56fd258\u540d\u53f8\u673a\u7684150\u4e07\u6b21\u884c\u7a0b\u6570\u636e\uff0c\u5bf9\u6bd4\u52a8\u6001\u5b9a\u4ef7\u524d\u540e\u53d8\u5316\u3002", "result": "\u52a8\u6001\u5b9a\u4ef7\u540e\uff0c\u53f8\u673a\u6536\u5165\u51cf\u5c11\u3001\u5e73\u53f0\u5206\u6210\u589e\u52a0\u3001\u5de5\u4f5c\u5206\u914d\u4e0e\u85aa\u916c\u66f4\u4e0d\u7a33\u5b9a\u3001\u53f8\u673a\u95f4\u4e0d\u5e73\u7b49\u52a0\u5267\u3001\u7b49\u5f85\u65f6\u95f4\u5ef6\u957f\u3002", "conclusion": "\u52a8\u6001\u5b9a\u4ef7\u52a0\u5267\u4e86\u5e73\u53f0\u5bf9\u53f8\u673a\u7684\u5265\u524a\uff0c\u547c\u5401\u66f4\u591a\u5de5\u4eba\u6570\u636e\u79d1\u5b66\u7814\u7a76\u4e0e\u7b97\u6cd5\u900f\u660e\u5ea6\u3002"}}
