<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.HC](#cs.HC) [Total: 15]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.IT](#cs.IT) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.CR](#cs.CR) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
*Ori Press,Brandon Amos,Haoyu Zhao,Yikai Wu,Samuel K. Ainsworth,Dominik Krupke,Patrick Kidger,Touqir Sajed,Bartolomeo Stellato,Jisun Park,Nathanael Bosch,Eli Meril,Albert Steppi,Arman Zharmagambetov,Fangzhao Zhang,David Perez-Pineiro,Alberto Mercurio,Ni Zhan,Talor Abramovich,Kilian Lieret,Hanlin Zhang,Shirley Huang,Matthias Bethge,Ofir Press*

Main category: cs.SE

TL;DR: 提出了AlgoTune基准，测试语言模型在计算机科学、物理和数学中设计高效算法的能力，发现模型仅能实现表面优化，未能创新。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在开放性问题中设计和实现算法的能力，超越现有任务。

Method: 开发了包含155个任务的AlgoTune基准，并构建了AlgoTuner基线模型进行比较。

Result: AlgoTuner平均提速1.72倍，但模型未能实现算法创新。

Conclusion: AlgoTune有望推动语言模型在创造性问题解决上的发展。

Abstract: Despite progress in language model (LM) capabilities, evaluations have thus
far focused on models' performance on tasks that humans have previously solved,
including in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,
2024). We therefore propose testing models' ability to design and implement
algorithms in an open-ended benchmark: We task LMs with writing code that
efficiently solves computationally challenging problems in computer science,
physics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks
collected from domain experts and a framework for validating and timing
LM-synthesized solution code, which is compared to reference implementations
from popular open-source packages. In addition, we develop a baseline LM agent,
AlgoTuner, and evaluate its performance across a suite of frontier models.
AlgoTuner achieves an average 1.72x speedup against our reference solvers,
which use libraries such as SciPy, sk-learn and CVXPY. However, we find that
current models fail to discover algorithmic innovations, instead preferring
surface-level optimizations. We hope that AlgoTune catalyzes the development of
LM agents exhibiting creative problem solving beyond state-of-the-art human
performance.

</details>


### [2] [Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing](https://arxiv.org/abs/2507.15889)
*Noah van der Vleuten*

Main category: cs.SE

TL;DR: 论文提出了一种用于程序合成的引导算法，通过支持模型学习修复来解决问题，表现优于常规微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有的程序合成语言模型存在数据集有限、与人类编程过程不一致的问题，需要改进。

Method: 引入引导算法，支持模型学习修复功能，并与常规微调进行对比。

Result: 引导算法在性能上优于常规微调，甚至与规模更大的模型相当；修复功能还提升了非修复任务的性能。

Conclusion: 引导算法是一种有效的程序合成方法，但推断阶段的修复可能不如直接采样多组解决方案。

Abstract: Language models for program synthesis are usually trained and evaluated on
programming competition datasets (MBPP, APPS). However, these datasets are
limited in size and quality, while these language models are extremely data
hungry. Additionally, the language models have a misaligned program synthesis
process compared to humans. While humans iteratively develop code with the help
of a compiler, most program synthesis models currently produce code in one go.
To solve these issues, we introduce a bootstrapping algorithm for program
synthesis, that supports teaching models how to repair. We show that
bootstrapping consistently outperforms regular fine-tuning. Compared to other
work, our bootstrapped model performs on par with fine-tuned models that are
68\% larger. Notably, bootstrapping with repairing also improves non-repairing
performance compared to regular bootstrapping during inference. However, on our
models, repairing during inference is likely inferior to simply sampling the
same number of solutions. Furthermore, we find that there are issues with the
example test cases in the training portion of the APPS dataset that are
valuable to the community, as many repairing and reinforcement learning methods
rely on them.

</details>


### [3] [StaAgent: An Agentic Framework for Testing Static Analyzers](https://arxiv.org/abs/2507.15892)
*Elijah Nnorom,Md Basim Uddin Ahmed,Jiho Shin,Hung Viet Pham,Song Wang*

Main category: cs.SE

TL;DR: 提出了一种名为StaAgent的多代理框架，利用大型语言模型（LLMs）的系统化评估静态分析器规则的能力，通过生成种子程序、验证代码、变异生成和对比测试，发现规则实现的缺陷。


<details>
  <summary>Details</summary>
Motivation: 静态分析器在软件开发早期发现缺陷至关重要，但其规则实现常因测试不足而存在不一致性。需要一种可扩展且自适应的解决方案来提高其可靠性。

Method: StaAgent框架包含四个代理：种子生成代理生成缺陷种子程序，代码验证代理确保种子正确，变异生成代理产生语义等效变异，分析器评估代理通过对比测试发现不一致行为。

Result: 实验表明，StaAgent在五种主流静态分析器中发现了64个有问题的规则（如SpotBugs中的28个），其中53个未被现有基线检测到，部分已被开发人员确认或修复。

Conclusion: 该方法证明了LLM驱动的数据合成在软件工程中的潜力，为静态分析器的可靠性提升提供了有效途径。

Abstract: Static analyzers play a critical role in identifying bugs early in the
software development lifecycle, but their rule implementations are often
under-tested and prone to inconsistencies. To address this, we propose
StaAgent, an agentic framework that harnesses the generative capabilities of
Large Language Models (LLMs) to systematically evaluate static analyzer rules.
StaAgent comprises four specialized agents: a Seed Generation Agent that
translates bug detection rules into concrete, bug-inducing seed programs; a
Code Validation Agent that ensures the correctness of these seeds; a Mutation
Generation Agent that produces semantically equivalent mutants; and an Analyzer
Evaluation Agent that performs metamorphic testing by comparing the static
analyzer's behavior on seeds and their corresponding mutants. By revealing
inconsistent behaviors, StaAgent helps uncover flaws in rule implementations.
This LLM-driven, multi-agent framework offers a scalable and adaptable solution
to improve the reliability of static analyzers. We evaluated StaAgent with five
state-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)
across five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,
Infer, and PMD). The experimental results show that our approach can help
reveal 64 problematic rules in the latest versions of these five static
analyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,
and 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the
SOTA baseline. We have reported all the bugs to developers, with two of them
already fixed. Three more have been confirmed by developers, while the rest are
awaiting response. These results demonstrate the effectiveness of our approach
and underscore the promise of agentic, LLM-driven data synthesis to advance
software engineering.

</details>


### [4] [A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights](https://arxiv.org/abs/2507.16037)
*Zhili Zeng,Kimya Khakzad Shahandashti,Alvine Boaye Belle,Song Wang,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: 该论文研究了利用大型语言模型（LLM）进行跨平台移动应用翻译的性能和局限性，提出了基于代理的方法并评估其效果。


<details>
  <summary>Details</summary>
Motivation: 移动应用的跨平台兼容性需求日益增长，但传统方法效率低下，现有机器学习方法缺乏上下文理解。

Method: 开发了一个代理链，考虑依赖关系、规范、程序结构和控制流，将Android应用翻译为iOS。

Result: 通过人工检查翻译代码的语法、语义和功能完整性，并分析失败原因。

Conclusion: LLM在跨平台应用翻译中表现有限，但提出了改进方向。

Abstract: The rapid advancement of mobile applications has led to a significant demand
for cross-platform compatibility, particularly between the Android and iOS
platforms. Traditional approaches to mobile application translation often rely
on manual intervention or rule-based systems, which are labor-intensive and
time-consuming. While recent advancements in machine learning have introduced
automated methods, they often lack contextual understanding and adaptability,
resulting in suboptimal translations. Large Language Models (LLMs) were
recently leveraged to enhance code translation at different granularities,
including the method, class, and repository levels. Researchers have
investigated common errors, limitations, and potential strategies to improve
these tasks. However, LLM-based application translation across different
platforms, such as migrating mobile applications between Android and iOS or
adapting software across diverse frameworks, remains underexplored.
Understanding the performance, strengths, and limitations of LLMs in
cross-platform application translation is critical for advancing software
engineering automation. This study aims to fill this gap by evaluating
LLM-based agentic approaches for mobile application translation, identifying
key failure points, and proposing guidelines to improve translation
performance. We developed a chain of agents that account for dependencies,
specifications, program structure, and program control flow when translating
applications from Android to iOS. To evaluate the performance, we manually
examined the translated code for syntactic correctness, semantic accuracy, and
functional completeness. For translation failures, we further conducted a
detailed root cause analysis to understand the underlying limitations of the
agentic translation process and identify opportunities for improvement.

</details>


### [5] [Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs](https://arxiv.org/abs/2507.16044)
*Meriem Mastouri,Emna Ksontini,Wael Kessentini*

Main category: cs.SE

TL;DR: 论文探讨了MCP服务器构建的自动化可能性，提出AutoMCP工具，通过OpenAPI规范自动生成服务器，显著减少手动工作量。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs发展为主动调用外部工具的代理，需要一个标准化的工具集成协议，但目前MCP服务器的构建仍然依赖手动，效率低下。

Method: 论文分析了22,000+个GitHub仓库的MCP采用趋势，并提出AutoMCP编译器，能够从OpenAPI 2.0/3.0规范生成MCP服务器。

Result: 在50个真实API的测试中，AutoMCP自动生成的服务器76.5%能直接成功运行；经少量规范修复后，成功率提升至99.9%。

Conclusion: 研究表明OpenAPI规范能够支持MCP服务器的近乎完全自动化，并为修复常见规范缺陷提供了洞察。

Abstract: Large Language Models (LLMs) are evolving from passive text generators into
active agents that invoke external tools. To support this shift, scalable
protocols for tool integration are essential. The Model Context Protocol (MCP),
introduced by Anthropic in 2024, offers a schema-driven standard for dynamic
tool discovery and invocation. Yet, building MCP servers remains manual and
repetitive, requiring developers to write glue code, handle authentication, and
configure schemas by hand-replicating much of the integration effort MCP aims
to eliminate.
  This paper investigates whether MCP server construction can be meaningfully
automated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged
GitHub repositories created within six months of release, fewer than 5% include
servers, typically small, single-maintainer projects dominated by repetitive
scaffolding. To address this gap, we present AutoMCP, a compiler that generates
MCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API
definitions and produces complete server implementations, including schema
registration and authentication handling.
  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across
over 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded
out of the box. Manual failure analysis revealed five recurring issues, all
attributable to inconsistencies or omissions in the OpenAPI contracts. After
minor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%
success.
  Our findings (i) analyze MCP adoption and quantify the cost of manual server
development, (ii) demonstrate that OpenAPI specifications, despite quality
issues, enable near-complete MCP server automation, and (iii) contribute a
corpus of 5,066 callable tools along with insights on repairing common
specification flaws.

</details>


### [6] [AI-Powered Commit Explorer (APCE)](https://arxiv.org/abs/2507.16063)
*Yousab Grees,Polina Iaremchuk,Ramtin Ehsani,Esteban Parra,Preetha Chatterjee,Sonia Haiduc*

Main category: cs.SE

TL;DR: APCE工具利用大型语言模型生成高质量的提交信息，支持开发者与研究者使用和评估这些信息。


<details>
  <summary>Details</summary>
Motivation: 提交信息是描述代码变更的重要来源，但实践中常被忽视，LLM生成的提交信息可以弥补这一缺陷。

Method: 开发了AI-Powered Commit Explorer (APCE)工具，支持存储不同LLM提示并提供评估功能。

Result: APCE为研究者提供了自动化和人工评估LLM生成提交信息的机制。

Conclusion: APCE通过LLM生成高质量的提交信息，提升了开发和研究的效率。

Abstract: Commit messages in a version control system provide valuable information for
developers regarding code changes in software systems. Commit messages can be
the only source of information left for future developers describing what was
changed and why. However, writing high-quality commit messages is often
neglected in practice. Large Language Model (LLM) generated commit messages
have emerged as a way to mitigate this issue. We introduce the AI-Powered
Commit Explorer (APCE), a tool to support developers and researchers in the use
and study of LLM-generated commit messages. APCE gives researchers the option
to store different prompts for LLMs and provides an additional evaluation
prompt that can further enhance the commit message provided by LLMs. APCE also
provides researchers with a straightforward mechanism for automated and human
evaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo

</details>


### [7] [Ten Essential Guidelines for Building High-Quality Research Software](https://arxiv.org/abs/2507.16166)
*Nasir U. Eisty,David E. Bernholdt,Alex Koufos,David J. Luet,Miranda Mundt*

Main category: cs.SE

TL;DR: 本文提出了10条指南，帮助研究人员开发高质量的研究软件，涵盖开发全生命周期，强调规划、代码质量、版本控制、测试等关键实践。


<details>
  <summary>Details</summary>
Motivation: 高质量研究软件是科学进步的关键，但开发过程中常忽视最佳实践，需系统性指导以提升软件质量。

Method: 提出10条具体指南，覆盖从规划到维护的软件开发生命周期，强调模块化、可重现性和性能优化。

Result: 指南确保软件具有稳健性、可用性和可持续性，同时通过文档和社区参与提升影响力。

Conclusion: 遵循这些指南可开发出更具科学价值和可重用性的研究软件，推动科研工具生态系统的进步。

Abstract: High-quality research software is a cornerstone of modern scientific
progress, enabling researchers to analyze complex data, simulate phenomena, and
share reproducible results. However, creating such software requires adherence
to best practices that ensure robustness, usability, and sustainability. This
paper presents ten guidelines for producing high-quality research software,
covering every stage of the development lifecycle. These guidelines emphasize
the importance of planning, writing clean and readable code, using version
control, and implementing thorough testing strategies. Additionally, they
address key principles such as modular design, reproducibility, performance
optimization, and long-term maintenance. The paper also highlights the role of
documentation and community engagement in enhancing software usability and
impact. By following these guidelines, researchers can create software that
advances their scientific objectives and contributes to a broader ecosystem of
reliable and reusable research tools. This work serves as a practical resource
for researchers and developers aiming to elevate the quality and impact of
their research software.

</details>


### [8] [LOCOFY Large Design Models -- Design to code conversion solution](https://arxiv.org/abs/2507.16208)
*Sohaib Muhammad,Ashwati Vipin,Karan Shetti,Honey Mittal*

Main category: cs.SE

TL;DR: 大设计模型（LDMs）通过优化的训练和推理流程，解决了设计到代码转换中的可解释性、可扩展性和重复性问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型和多模态大型语言模型在设计到代码转换中的挑战，如可解释性、资源需求和可重复性。

Method: 开发了包含设计优化器、标签和特征检测、自动组件的训练流程，以及用于实际设计的推理流程。

Result: LDMs在节点定位、响应性和可重复性方面优于LLMs，且在UI元素检测上表现出高精度和一致性。

Conclusion: LDMs是一种可靠且高效的解决方案，可生成生产就绪代码。

Abstract: Despite rapid advances in Large Language Models and Multimodal Large Language
Models (LLMs), numerous challenges related to interpretability, scalability,
resource requirements and repeatability remain, related to their application in
the design-to-code space. To address this, we introduce the Large Design Models
(LDMs) paradigm specifically trained on designs and webpages to enable seamless
conversion from design-to-code. We have developed a training and inference
pipeline by incorporating data engineering and appropriate model architecture
modification. The training pipeline consists of the following: 1)Design
Optimiser: developed using a proprietary ground truth dataset and addresses
sub-optimal designs; 2)Tagging and feature detection: using pre-trained and
fine-tuned models, this enables the accurate detection and classification of UI
elements; and 3)Auto Components: extracts repeated UI structures into reusable
components to enable creation of modular code, thus reducing redundancy while
enhancing code reusability. In this manner, each model addresses distinct but
key issues for design-to-code conversion. Separately, our inference pipeline
processes real-world designs to produce precise and interpretable instructions
for code generation and ensures reliability. Additionally, our models
illustrated exceptional end-to-end design-to-code conversion accuracy using a
novel preview match score metric. Comparative experiments indicated superior
performance of LDMs against LLMs on accuracy of node positioning,
responsiveness and reproducibility. Moreover, our custom-trained tagging and
feature detection model demonstrated high precision and consistency in
identifying UI elements across a wide sample of test designs. Thus, our
proposed LDMs are a reliable and superior solution to understanding designs
that subsequently enable the generation of efficient and reliable
production-ready code.

</details>


### [9] [Search-based Generation of Waypoints for Triggering Self-Adaptations in Maritime Autonomous Vessels](https://arxiv.org/abs/2507.16327)
*Karoline Nylænder,Aitor Arrieta,Shaukat Ali,Paolo Arcaini*

Main category: cs.SE

TL;DR: 论文提出了一种名为WPgen的多目标搜索方法，用于生成海上自主航行器（AVs）航点的微小修改，以触发其适应行为，并通过实验验证了其在不同AV中的效果。


<details>
  <summary>Details</summary>
Motivation: 为了验证自主航行器在面对意外情况时的自适应行为，需要设计能够触发其适应性调整的场景，本研究聚焦于导航软件的调整。

Method: 提出了基于NSGA-II多目标搜索算法的WPgen方法，生成了航点的微小修改，并比较了三种初始种群生成策略的效果。

Result: 实验结果表明，WPgen的效果因AV类型而异，某些策略在不同AV中表现更优。

Conclusion: WPgen在验证自主航行器的自适应行为方面具有研究和实践意义，未来可针对不同类型AV优化策略。

Abstract: Self-adaptation in maritime autonomous vessels (AVs) enables them to adapt
their behaviors to address unexpected situations while maintaining
dependability requirements. During the design of such AVs, it is crucial to
understand and identify the settings that should trigger adaptations, enabling
validation of their implementation. To this end, we focus on the navigation
software of AVs, which must adapt their behavior during operation through
adaptations. AVs often rely on predefined waypoints to guide them along
designated routes, ensuring safe navigation. We propose a multiobjective
search-based approach, called WPgen, to generate minor modifications to the
predefined set of waypoints, keeping them as close as possible to the original
waypoints, while causing the AV to navigate inappropriately when navigating
with the generated waypoints. WPgen uses NSGA-II as the multi-objective search
algorithm with three seeding strategies for its initial population, resulting
in three variations of WPgen. We evaluated these variations on three AVs (one
overwater tanker and two underwater). We compared the three variations of WPgen
with Random Search as the baseline and with each other. Experimental results
showed that the effectiveness of these variations varied depending on the AV.
Based on the results, we present the research and practical implications of
WPgen.

</details>


### [10] [Improving Code LLM Robustness to Prompt Perturbations via Layer-Aware Model Editing](https://arxiv.org/abs/2507.16407)
*Shuhan Liu,Xing Hu,Kerui Huang,Xiaohu Yang,David Lo,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出了一种名为CREME的新方法，通过针对性的参数更新来提高大型语言模型（LLM）在代码生成任务中对提示扰动的鲁棒性。实验表明，CREME显著提升了模型在扰动提示下的性能，同时保持了对干净输入的稳定表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成中表现优异，但对提示的微小扰动非常敏感，这在实际应用中可能导致性能下降。研究旨在通过模型编辑提高其鲁棒性。

Method: CREME通过比较原始提示和扰动提示的隐藏状态，识别鲁棒性敏感的层，并在这些层进行轻量级参数编辑。

Result: 在HumanEval和MBPP基准测试中，CREME将扰动提示的Pass@1准确率提高了63%，且对干净输入的偏差保持在1%以内。

Conclusion: CREME通过针对性的模型编辑有效提升了LLM的鲁棒性，且发现鲁棒性敏感的层主要集中在模型中深层，这些发现为未来鲁棒性策略设计提供了参考。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
code generation, where the natural language prompt plays a crucial role in
conveying user intent to the model. However, prior studies have shown that LLMs
are highly sensitive to prompt perturbations. Minor modifications in wording,
syntax, or formatting can significantly reduce the functional correctness of
generated code. As perturbations frequently occur in real-world scenarios,
improving the robustness of LLMs to prompt perturbations is essential for
ensuring reliable performance in practical code generation. In this paper, we
introduce CREME (Code Robustness Enhancement via Model Editing), a novel
approach that enhances LLM robustness through targeted parameter updates. CREME
first identifies robustness-sensitive layers by comparing hidden states between
an original prompt and its perturbed variant. Then, it performs lightweight
parameter editing at the identified layer to reduce performance degradation. We
evaluate CREME on two widely used code generation benchmarks (HumanEval and
MBPP) along with their perturbed counterparts. Experimental results show that
CREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining
stable performance on clean inputs, with accuracy deviations within 1%. Further
analysis reveals that robustness-sensitive layers are primarily concentrated in
the middle and deeper layers of the network, and their locations vary across
different model architectures. These insights provide a valuable foundation for
developing future robustness-oriented editing strategies.

</details>


### [11] [Exploring Large Language Models for Analyzing and Improving Method Names in Scientific Code](https://arxiv.org/abs/2507.16439)
*Gunnar Larsen,Carol Wong,Anthony Peruma*

Main category: cs.SE

TL;DR: 该研究评估了四种大型语言模型在分析科学软件中方法名称的能力，发现它们在语法分析和建议方面有一定效果，但仍需人工评估。


<details>
  <summary>Details</summary>
Motivation: 科学软件中方法名称的质量对研究有重要影响，但现有研究很少关注这一领域。大型语言模型为自动代码分析提供了新机会。

Method: 研究从Python的Jupyter Notebooks中提取了496个方法名称，评估了四种大型语言模型对其语法模式的分析和建议能力。

Result: 模型在遵循命名实践方面表现不错，但在处理领域术语和与人类标注一致性上表现不一致，需人工干预。

Conclusion: 该研究为通过AI自动化提升科学代码质量提供了基础性见解，但仍需结合人类评估。

Abstract: Research scientists increasingly rely on implementing software to support
their research. While previous research has examined the impact of identifier
names on program comprehension in traditional programming environments, limited
work has explored this area in scientific software, especially regarding the
quality of method names in the code. The recent advances in Large Language
Models (LLMs) present new opportunities for automating code analysis tasks,
such as identifier name appraisals and recommendations. Our study evaluates
four popular LLMs on their ability to analyze grammatical patterns and suggest
improvements for 496 method names extracted from Python-based Jupyter
Notebooks. Our findings show that the LLMs are somewhat effective in analyzing
these method names and generally follow good naming practices, like starting
method names with verbs. However, their inconsistent handling of
domain-specific terminology and only moderate agreement with human annotations
indicate that automated suggestions require human evaluation. This work
provides foundational insights for improving the quality of scientific code
through AI automation.

</details>


### [12] [On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization](https://arxiv.org/abs/2507.16587)
*Giuseppe Crupi,Rosalia Tufano,Alejandro Velasco,Antonio Mastropaolo,Denys Poshyvanyk,Gabriele Bavota*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLM）作为代码生成和代码摘要任务的评判者的有效性，发现GPT-4-turbo表现最佳，但仍有误判。


<details>
  <summary>Details</summary>
Motivation: 为弥补定量指标的不足并减少人工评估成本，探索LLM作为复杂任务的评判者。

Method: 测试LLM对代码生成和代码摘要任务的评判能力，包括1,405个Java方法和1,281个Python函数，以及1.2k个摘要。

Result: GPT-4-turbo是表现最佳的LLM，但仍有误判；较小参数的LLM能力不足。

Conclusion: LLM可作为评判者，但其准确性和可靠性仍需改进。

Abstract: Large Language Models have been recently exploited as judges for complex
natural language processing tasks, such as Q&A. The basic idea is to delegate
to an LLM the assessment of the "quality" of the output provided by an
automated technique for tasks for which: (i) quantitative metrics would only
tell part of the story, and; (ii) a large-scale human-based evaluation would be
too expensive. LLMs-as-a-judge, if proven effective for a specific task, can
also unlock new possibilities for automation, with several LLMs proposing a
solution for a given instance of the task and others judging and deciding what
is the best output to show the user. We study the effectiveness of
LLMs-as-a-judge for two code-related tasks, namely code generation and code
summarization. The rationale for choosing these tasks is two-fold. First,
quantitative metrics are usually not enough for the assessment of code
summarizers/generators. For example, it is well documented that metrics such as
BLEU are quite weak proxies for the quality of the generated summaries. Second,
even state-of-the-art techniques still struggle with handling complex instances
of these tasks, making them good candidates for benefiting from more advanced
solutions envisioning collaboration among LLMs. For code generation, we check
whether eight LLMs are able to judge the correctness of 1,405 Java methods and
1,281 Python functions generated by the same LLMs or implemented by humans. For
code summarization, we compare the judgment of five LLMs to those provided by
nine humans for ~1.2k summaries, related to both Java and Python functions. Our
findings show that GPT-4-turbo is the best LLM in terms of judging capabilities
for both tasks, with "smaller" LLMs featuring tens of billions parameters not
being able to cope with judging tasks. However, even the best-performing LLM
frequently misjudges the correctness of the code and summary quality.

</details>


### [13] [VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones](https://arxiv.org/abs/2507.16661)
*Tan Bui,Yan Naing Tun,Thanh Phuc Nguyen,Yindu Su,Ferdian Thung,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Lwin Khin Shar,Eng Lieh Ouh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: VulCoCo combines嵌入检索与大语言模型验证，高效检测漏洞代码克隆，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具依赖语法相似性或预测粗粒度，实用性受限，需更精准的检测方法。

Method: 通过嵌入检索候选函数，结合LLM验证其是否保留漏洞，并构建合成基准测试。

Result: 在实验中表现优于现有方法，真实项目中提交的PR部分被采纳并发现新CVE。

Conclusion: VulCoCo有效提升检测精度，为未来研究提供方向。

Abstract: Code reuse is common in modern software development, but it can also spread
vulnerabilities when developers unknowingly copy risky code. The code fragments
that preserve the logic of known vulnerabilities are known as vulnerable code
clones (VCCs). Detecting those VCCs is a critical but challenging task.
Existing VCC detection tools often rely on syntactic similarity or produce
coarse vulnerability predictions without clear explanations, limiting their
practical utility. In this paper, we propose VulCoCo, a lightweight and
scalable approach that combines embedding-based retrieval with large language
model (LLM) validation. Starting from a set of known vulnerable functions, we
retrieve syntactically or semantically similar candidate functions from a large
corpus and use an LLM to assess whether the candidates retain the
vulnerability. Given that there is a lack of reproducible vulnerable code clone
benchmarks, we first construct a synthetic benchmark that spans various clone
types.
  Our experiments on the benchmark show that VulCoCo outperforms prior
state-of-the-art methods in terms of Precision@k and mean average precision
(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world
projects by submitting 400 pull requests (PRs) to 284 open-source projects.
Among them, 75 PRs were merged, and 15 resulted in newly published CVEs. We
also provide insights to inspire future work to further improve the precision
of vulnerable code clone detection.

</details>


### [14] [VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models](https://arxiv.org/abs/2507.16685)
*Duong Nguyen,Manh Tran-Duc,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: VulGuard是一个自动化工具，用于提取、处理和分析GitHub仓库的提交，以支持即时漏洞预测（JIT-VP）研究。


<details>
  <summary>Details</summary>
Motivation: 为了解决软件安全研究中可重复性和可扩展性的关键挑战，提供统一框架支持仓库规模挖掘和模型级实验。

Method: VulGuard自动挖掘提交历史，提取细粒度代码变更、提交消息和软件工程指标，并集成多种先进的漏洞预测模型。

Result: 在FFmpeg和Linux内核等流行开源项目中验证了工具的有效性，显示出加速真实世界JIT-VP研究和标准化基准测试的潜力。

Conclusion: VulGuard为JIT-VP研究提供了一个高效、可扩展的工具，并能轻松集成到CI/CD流程中。

Abstract: We present VulGuard, an automated tool designed to streamline the extraction,
processing, and analysis of commits from GitHub repositories for Just-In-Time
vulnerability prediction (JIT-VP) research. VulGuard automatically mines commit
histories, extracts fine-grained code changes, commit messages, and software
engineering metrics, and formats them for downstream analysis. In addition, it
integrates several state-of-the-art vulnerability prediction models, allowing
researchers to train, evaluate, and compare models with minimal setup. By
supporting both repository-scale mining and model-level experimentation within
a unified framework, VulGuard addresses key challenges in reproducibility and
scalability in software security research. VulGuard can also be easily
integrated into the CI/CD pipeline. We demonstrate the effectiveness of the
tool in two influential open-source projects, FFmpeg and the Linux kernel,
highlighting its potential to accelerate real-world JIT-VP research and promote
standardized benchmarking. A demo video is available at:
https://youtu.be/j96096-pxbs

</details>


### [15] [Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support](https://arxiv.org/abs/2507.16754)
*Fangjian Lei,Mariam El Mezouar,Shayan Noei,Ying Zou*

Main category: cs.SE

TL;DR: 研究通过构建包含300多万Stack Overflow帖子的检索库，设计多种RAG流程，显著提升LLM生成代码相关答案的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在开发者问题中生成不可靠答案的问题，探索RAG流程优化以减少幻觉现象。

Method: 构建大规模Java和Python Stack Overflow帖子库，设计7种RAG流程及变体，评估其效果并优化检索阈值。

Result: 结合HyDE的RAG流程表现最佳，显著优于零样本基线，提升了答案的准确性和全面性。

Conclusion: 优化后的RAG流程能有效提升LLM生成答案的质量，适用于各类开发者问题。

Abstract: Large Language Models (LLMs) have shown promise in assisting developers with
code-related questions; however, LLMs carry the risk of generating unreliable
answers. To address this, Retrieval-Augmented Generation (RAG) has been
proposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,
designing effective pipelines remains challenging due to numerous design
choices. In this paper, we construct a retrieval corpus of over 3 million Java
and Python related Stack Overflow posts with accepted answers, and explore
various RAG pipeline designs to answer developer questions, evaluating their
effectiveness in generating accurate and reliable responses. More specifically,
we (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants
to answer questions that have historically similar matches, and (2) address new
questions without any close prior matches by automatically lowering the
similarity threshold during retrieval, thereby increasing the chance of finding
partially relevant context and improving coverage for unseen cases. We find
that implementing a RAG pipeline combining hypothetical-documentation-embedding
(HyDE) with the full-answer context performs best in retrieving and answering
similarcontent for Stack Overflow questions. Finally, we apply our optimal RAG
pipeline to 4 open-source LLMs and compare the results to their zero-shot
performance. Our findings show that RAG with our optimal RAG pipeline
consistently outperforms zero-shot baselines across models, achieving higher
scores for helpfulness, correctness, and detail with LLM-as-a-judge. These
findings demonstrate that our optimal RAG pipelines robustly enhance answer
quality for a wide range of developer queries including both previously seen
and novel questions across different LLMs

</details>


### [16] [Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis](https://arxiv.org/abs/2507.16808)
*Zhihao Xu,Bixin Li,Lulu Wang*

Main category: cs.SE

TL;DR: LLM-Based RTL代码优化方法在逻辑操作优化上有效，但在复杂时序逻辑方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 传统RTL代码优化方法依赖手动调整和启发式规则，耗时且易错，研究希望评估LLMs在复杂时序逻辑优化中的表现。

Method: 提出新的RTL优化基准，并基于等价变换方法系统评估LLM-Based优化方法的有效性。

Result: LLMs在逻辑操作优化中表现优于编译方法，但在复杂时序逻辑（如时序控制流优化）上不如编译方法。

Conclusion: LLMs在RTL优化中有潜力但需改进对时序逻辑的理解，为未来研究提供方向。

Abstract: Register Transfer Level(RTL) code optimization is crucial for achieving high
performance and low power consumption in digital circuit design. However,
traditional optimization methods often rely on manual tuning and heuristics,
which can be time-consuming and error-prone. Recent studies proposed to
leverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs
can generate optimized code snippets based on natural language descriptions,
potentially speeding up the optimization process. However, existing approaches
have not thoroughly evaluated the effectiveness of LLM-Based code optimization
methods for RTL code with complex timing logic. To address this gap, we
conducted a comprehensive empirical investigation to assess the capability of
LLM-Based RTL code optimization methods in handling RTL code with complex
timing logic. In this study, we first propose a new benchmark for RTL
optimization evaluation. It comprises four subsets, each corresponding to a
specific area of RTL code optimization. Then we introduce a method based on
metamorphosis to systematically evaluate the effectiveness of LLM-Based RTL
code optimization methods.Our key insight is that the optimization
effectiveness should remain consistent for semantically equivalent but more
complex code. After intensive experiments, we revealed several key findings.
(1) LLM-Based RTL optimization methods can effectively optimize logic
operations and outperform existing compiler-based methods. (2) LLM-Based RTL
optimization methods do not perform better than existing compiler-based methods
on RTL code with complex timing logic, particularly in timing control flow
optimization and clock domain optimization. This is primarily attributed to the
challenges LLMs face in understanding timing logic in RTL code. Based on these
findings, we provide insights for further research in leveraging LLMs for RTL
code optimization.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [RightTyper: Effective and Efficient Type Annotation for Python](https://arxiv.org/abs/2507.16051)
*Juan Altmayer Pizzorno,Emery D. Berger*

Main category: cs.PL

TL;DR: RightTyper是一种新型Python类型注释工具，能基于程序实际行为生成精确类型，同时通过采样和统计过滤降低性能开销。


<details>
  <summary>Details</summary>
Motivation: 解决现有Python类型注释工具的不足，如静态方法处理动态特性不佳、AI方法不健全、动态方法开销大等问题。

Method: 结合采样（基于自剖析指导）、统计过滤及类型信息的分辨与聚合，实现高效且精确的类型推断。

Result: RightTyper性能开销仅30%，召回率优于现有方法，并能通过类型检查发现异常行为。

Conclusion: RightTyper为Python类型注释提供了一种高效、精确的解决方案，同时支持异常检测以审核潜在问题。

Abstract: Python type annotations bring the benefits of static type checking to the
language. However, manually writing annotations can be time-consuming and
tedious. The result is that most real-world Python code remains largely
untyped. Past approaches to annotating types in Python code fall short in a
number of ways. Static approaches struggle with dynamic features and infer
overly broad types. AI-based methods are inherently unsound and can miss rare
or user-defined types. Dynamic methods can impose extreme runtime overheads,
degrading performance by up to 270x, abort execution as they exhaust resources,
and even infer incorrect types that lead to runtime errors. Crucially, all
prior work assumes implicitly that the code to be annotated is already correct.
This assumption is generally unwarranted, especially for large codebases that
have been untyped.
  This paper presents RightTyper, a novel approach for Python that overcomes
these disadvantages. RightTyper not only generates precise type annotations
based on actual program behavior, improving recall in type checking relative to
prior approaches. It also turns type checking into anomaly detection, allowing
the type checker to identify corner cases that the programmer can audit for
unintended behavior. RightTyper is also fast and space-efficient, imposing just
30% performance overhead on average. RightTyper achieves these characteristics
by a principled yet pervasive use of sampling--guided by self-profiling--along
with statistical filtering and careful resolution and aggregation of type
information.

</details>


### [18] [Understanding Haskell-style Overloading via Open Data and Open Functions](https://arxiv.org/abs/2507.16086)
*Andrew Marmaduke,Apoorv Ingle,J. Garrett Morris*

Main category: cs.PL

TL;DR: 本文提出了一种新的Haskell风格重载语义，并在Lean4中实现了核心语言System F_D，展示了其对Haskell类型类系统的表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决Haskell风格重载的语义问题，提供更表达力强且无需额外类型等值公理的解决方案。

Method: 通过开发核心语言System F_D，采用开放数据类型和函数，利用Lean4进行元理论机械化。

Result: System F_D能够更表达力地编码Haskell类型类系统的先进特性。

Conclusion: System F_D提供了一种更高效且无需额外公理的重载语义方案。

Abstract: We present a new, uniform semantics for Haskell-style overloading. We realize
our approach in a new core language, System F$_\mathrm{D}$, whose metatheory we
mechanize in the Lean4 interactive theorem prover. System F$_\mathrm{D}$ is
distinguished by its open data types and open functions, each given by a
collection of instances rather than by a single definition. We show that System
F$_\mathrm{D}$ can encode advanced features of Haskell's of type class systems,
more expressively than current semantics of these features, and without
assuming additional type equality axioms.

</details>


### [19] [Querying Graph-Relational Data](https://arxiv.org/abs/2507.16089)
*Michael J. Sullivan,Zhibo Chen,Elvis Pranskevichus,Robert J. Simmons,Victor Petrovykh,Aljaž Mur Eržen,Yury Selivanov*

Main category: cs.PL

TL;DR: 提出了一种图-关系数据库模型，解决关系数据库与应用程序之间数据结构不匹配的问题，并通过EdgeQL语言和Gel系统实现高效查询。


<details>
  <summary>Details</summary>
Motivation: 关系数据库的扁平表示与应用程序所需的嵌套数据结构存在不匹配，即'对象-关系不匹配'问题，影响了数据操作的效率和便捷性。

Method: 定义了图-关系数据库模型，提供了静态和动态查询语义；开发了EdgeQL查询语言和Gel系统，将EdgeQL编译为PostgreSQL查询。

Result: 实现了高效的对象形数据操作，兼具ORM的便利性和直接编写PostgreSQL查询的高效性。

Conclusion: 图-关系数据库模型和Gel系统为解决对象-关系不匹配问题提供了灵活且高效的解决方案。

Abstract: For applications that store structured data in relational databases, there is
an impedance mismatch between the flat representations encouraged by relational
data models and the deeply nested information that applications expect to
receive. In this work, we present the graph-relational database model, which
provides a flexible, compositional, and strongly-typed solution to this
"object-relational mismatch." We formally define the graph-relational database
model and present a static and dynamic semantics for queries. In addition, we
discuss the realization of the graph-relational database model in EdgeQL, a
general-purpose SQL-style query language, and the Gel system, which compiles
EdgeQL schemas and queries into PostgreSQL queries. Gel facilitates the kind of
object-shaped data manipulation that is frequently provided inefficiently by
object-relational mapping (ORM) technologies, while achieving most of the
efficiency that comes from require writing complex PostgreSQL queries directly.

</details>


### [20] [Enhancing Compiler Optimization Efficiency through Grammatical Decompositions of Control-Flow Graphs](https://arxiv.org/abs/2507.16660)
*Xuran Cai*

Main category: cs.PL

TL;DR: 该论文提出了一种名为SPL（Series-Parallel-Loop）分解的新框架，用于解决编译器优化中的复杂问题，如寄存器分配和Lifetime-optimal Speculative Partial Redundancy Elimination（LOSPRE），并通过实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的树分解算法在处理寄存器分配和LOSPRE等编译器优化问题时，往往忽略控制流图的稀疏性，导致计算成本高，优化效果有限。

Method: 采用SPL分解框架，结合对部分约束满足问题（PCSPs）的通用解法，应用于寄存器分配、LOSPRE优化和银行选择指令的布局优化。

Result: 实验结果表明，SPL分解显著提升了优化性能，在寄存器分配、LOSPRE和银行选择等方面均优于现有方法。

Conclusion: SPL分解为复杂的编译器优化提供了高效且通用的解决方案，具有广泛的应用前景。

Abstract: This thesis addresses the complexities of compiler optimizations, such as
register allocation and Lifetime-optimal Speculative Partial Redundancy
Elimination (LOSPRE), which are often handled using tree decomposition
algorithms. However, these methods frequently overlook important sparsity
aspects of Control Flow Graphs (CFGs) and result in high computational costs.
We introduce the SPL (Series-Parallel-Loop) decomposition, a novel framework
that offers optimal solutions to these challenges. A key contribution is the
formulation of a general solution for Partial Constraint Satisfaction Problems
(PCSPs) within graph structures, applied to three optimization problems. First,
SPL decomposition enhances register allocation by accurately modeling variable
interference graphs, leading to efficient register assignments and improved
performance across benchmarks. Second, it optimizes LOSPRE by effectively
identifying and eliminating redundancies in program execution. Finally, the
thesis focuses on optimizing the placement of bank selection instructions to
enhance data retrieval efficiency and reduce latency. Extensive experimentation
demonstrates significant performance improvements over existing methods,
establishing SPL decomposition as a powerful tool for complex compiler
optimizations, including register allocation, LOSPRE, and bank selection.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [21] [From Profiling to Optimization: Unveiling the Profile Guided Optimization](https://arxiv.org/abs/2507.16649)
*Bingxin Liu,Yinghui Huang,Jianhua Gao,Jianjun Shi,Yongpeng Liu,Yipin Sun,Weixing Ji*

Main category: cs.PF

TL;DR: 该论文系统综述了基于运行时分析的Profile Guided Optimization (PGO)，分类研究了其方法、优化、编译器集成以及架构，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 通过结合静态分析与实际执行行为，PGO可以显著提升性能。研究旨在总结PGO的技术现状，并指出其挑战与未来方向。

Method: 论文通过分类PGO的研究方法（仪器化与采样）、优化（编译时和链接后）、编译器集成（如GCC、LLVM）以及目标架构，展示了关键算法和框架的设计原则。

Result: 通过在代表性示例上的性能评估，展示了PGO的加速效果、开销以及集成成熟度。

Conclusion: 研究指出了PGO面临的挑战（如降低采样开销、动态输入负载支持等），并提出了低开销分析和高级编译器研究的未来方向。

Abstract: Profile Guided Optimization (PGO) uses runtime profiling to direct compiler
optimization decisions, effectively combining static analysis with actual
execution behavior to enhance performance. Runtime profiles, collected through
instrumentation or hardware- and software-assisted sampling, provide detailed
insights into control flow, branch predictions, and memory access patterns.
This survey systematically categorizes PGO research by profiling method
(instrumentation vs. sampling), optimizations (compile time and link/post-link
time), compiler integration (GCC, LLVM), and target architectures. Key
algorithms and frameworks are shown in terms of design principles. Performance
evaluation on representative examples demonstrates PGO's speedups, overheads,
and integration maturity. Finally, we identify open challenges, such as
reducing sampling overhead, dynamic input workloads, and supporting
cross-architecture portability, and propose future research directions to
low-overhead profiling and advanced compilers.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [22] [The Sweet Danger of Sugar: Debunking Representation Learning for Encrypted Traffic Classification](https://arxiv.org/abs/2507.16438)
*Yuqi Zhao,Giovanni Dettori,Matteo Boffa,Luca Vassio,Marco Mellia*

Main category: cs.NI

TL;DR: 本文对现有基于语言模型的流量表示学习方法进行了批判性评估，揭示了其在真实场景中的性能问题，并提出了一种新的模型Pcap-Encoder，同时呼吁改进数据集准备和测试设计。


<details>
  <summary>Details</summary>
Motivation: 探讨现有流量表示学习方法在真实场景中的表现，并揭示其性能被数据准备问题夸大的现象。

Method: 通过分析现有模型的性能，发现其依赖虚假相关性，并提出了专门设计的Pcap-Encoder模型用于提取协议头部特征。

Result: 现有模型在无虚假相关性的真实场景中表现不佳，而Pcap-Encoder是唯一能提供有效流量分类表示的模型，但其复杂性限制了实际应用。

Conclusion: 指出了数据集准备和模型训练的缺陷，提出了更严格的测试设计和方法论，呼吁进行更严谨的基准测试。

Abstract: Recently we have witnessed the explosion of proposals that, inspired by
Language Models like BERT, exploit Representation Learning models to create
traffic representations. All of them promise astonishing performance in
encrypted traffic classification (up to 98% accuracy). In this paper, with a
networking expert mindset, we critically reassess their performance. Through
extensive analysis, we demonstrate that the reported successes are heavily
influenced by data preparation problems, which allow these models to find easy
shortcuts - spurious correlation between features and labels - during
fine-tuning that unrealistically boost their performance. When such shortcuts
are not present - as in real scenarios - these models perform poorly. We also
introduce Pcap-Encoder, an LM-based representation learning model that we
specifically design to extract features from protocol headers. Pcap-Encoder
appears to be the only model that provides an instrumental representation for
traffic classification. Yet, its complexity questions its applicability in
practical settings. Our findings reveal flaws in dataset preparation and model
training, calling for a better and more conscious test design. We propose a
correct evaluation methodology and stress the need for rigorous benchmarking.

</details>


### [23] [An Experimental Study of Split-Learning TinyML on Ultra-Low-Power Edge/IoT Nodes](https://arxiv.org/abs/2507.16594)
*Zied Jenhani,Mounir Bensalem,Jasenka Dizdarević,Admela Jukan*

Main category: cs.NI

TL;DR: 该论文提出了一种基于ESP32-S3板的TinyML与分割学习（SL）结合的测试平台，用于评估边缘/IoT环境中无线传输协议对分割学习性能的影响。实验表明，ESP-NOW在延迟上表现最佳，而BLE虽延长电池寿命但增加延迟。


<details>
  <summary>Details</summary>
Motivation: 解决超低功耗边缘/IoT节点因内存和计算资源有限而无法直接运行深度学习推理的问题，并评估不同无线传输协议对分割学习性能的影响。

Method: 构建基于ESP32-S3的TinyML + SL测试平台，将MobileNetV2模型量化、分区并通过无线协议（ESP-NOW、BLE、UDP/IP、TCP/IP）传输，测量其性能。

Result: ESP-NOW在延迟上表现最佳（3.7秒），BLE虽延长电池寿命但延迟超10秒。UDP/IP的延迟为5.8秒。

Conclusion: ESP-NOW在低延迟场景中表现最佳，而BLE更适合对电池寿命有高要求的场景。

Abstract: Running deep learning inference directly on ultra-low-power edge/IoT nodes
has been limited by the tight memory and compute budgets of microcontrollers.
Split learning (SL) addresses this limitation in which it executes part of the
inference process on the sensor and off-loads the remainder to a companion
device. In the context of constrained devices and the related impact of
low-power, over-the-air transport protocols, the performance of split learning
remains largely unexplored. TO the best of our knowledge, this paper presents
the first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards,
designed to benchmark the over-the-air performance of split learning TinyML in
edge/IoT environments. We benchmark the performance of a MobileNetV2 image
recognition model, which is quantized to 8-bit integers, partitioned, and
delivered to the nodes via over-the-air updates. The intermediate activations
are exchanged through different wireless communication methods: ESP-NOW, BLE,
and traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on
identical hardware. Measurements show that splitting the model after
block_16_project_BN layer generates a 5.66 kB tensor that traverses the link in
3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s.
ESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery
life further but increases latency beyond 10s.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [24] [Knowledge-aware Diffusion-Enhanced Multimedia Recommendation](https://arxiv.org/abs/2507.16396)
*Xian Mo,Fei Liu,Rui Tang,Jintao,Gao,Hao Liu*

Main category: cs.MM

TL;DR: 论文提出了一种基于知识感知扩散增强架构（KDiffE）的多媒体推荐方法，利用对比学习提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 多媒体推荐旨在利用丰富的多媒体内容增强用户与物品的历史交互信息，揭示更细粒度的用户偏好。

Method: 通过注意力感知矩阵和图神经网络构建主视图，结合随机游走策略，并利用引导扩散模型生成强任务相关的知识图谱，构建知识感知对比视图。

Result: 在三个多媒体数据集上的实验证明，KDiffE优于多种先进方法。

Conclusion: KDiffE通过结合知识图谱和对比学习，有效提升了多媒体推荐的性能。

Abstract: Multimedia recommendations aim to use rich multimedia content to enhance
historical user-item interaction information, which can not only indicate the
content relatedness among items but also reveal finer-grained preferences of
users. In this paper, we propose a Knowledge-aware Diffusion-Enhanced
architecture using contrastive learning paradigms (KDiffE) for multimedia
recommendations. Specifically, we first utilize original user-item graphs to
build an attention-aware matrix into graph neural networks, which can learn the
importance between users and items for main view construction. The
attention-aware matrix is constructed by adopting a random walk with a restart
strategy, which can preserve the importance between users and items to generate
aggregation of attention-aware node features. Then, we propose a guided
diffusion model to generate strongly task-relevant knowledge graphs with less
noise for constructing a knowledge-aware contrastive view, which utilizes user
embeddings with an edge connected to an item to guide the generation of
strongly task-relevant knowledge graphs for enhancing the item's semantic
information. We perform comprehensive experiments on three multimedia datasets
that reveal the effectiveness of our KDiffE and its components on various
state-of-the-art methods. Our source codes are available
https://github.com/1453216158/KDiffE.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [25] [An Adequate While-Language for Stochastic Hybrid Computation](https://arxiv.org/abs/2507.15913)
*Renato Neves,José Proença,Juliana Souza*

Main category: cs.LO

TL;DR: 论文介绍了一种结合微分构造和概率构造的程序形式化推理语言，涵盖了自适应巡航控制器、连续时间随机游走等系统。


<details>
  <summary>Details</summary>
Motivation: 旨在为结合微分和概率构造的程序提供形式化推理的工具。

Method: 提出了语言的操作语义和解释器实现，并补充了指称语义，证明了二者的充分性。

Result: 成功实现了语言的操作语义和指称语义的对应关系。

Conclusion: 该语言为微分和概率混合程序的形式化推理提供了有效工具。

Abstract: We introduce a language for formally reasoning about programs that combine
differential constructs with probabilistic ones. The language harbours, for
example, such systems as adaptive cruise controllers, continuous-time random
walks, and physical processes involving multiple collisions, like in Einstein's
Brownian motion.
  We furnish the language with an operational semantics and use it to implement
a corresponding interpreter. We also present a complementary, denotational
semantics and establish an adequacy theorem between both cases.

</details>


### [26] [On Expansions of Monadic Second-Order Logic with Dynamical Predicates](https://arxiv.org/abs/2507.16581)
*Joris Nieuwveld,Joël Ouaknine*

Main category: cs.LO

TL;DR: 论文证明了关于自然数序结构的MSO理论在一类动态一元谓词下的可判定性，关键工具是新提出的prodisjunctivity概念。


<details>
  <summary>Details</summary>
Motivation: 研究MSO理论在动态一元谓词下的扩展，延续了Büchi和Elgot & Rabin的经典工作，探索其可判定性。

Method: 利用整数线性递推序列的特性，引入新概念prodisjunctivity作为主要技术工具。

Result: 证明了包含动态一元谓词的MSO理论的可判定性。

Conclusion: 新概念prodisjunctivity不仅解决了当前问题，还可能在其他领域有独立应用。

Abstract: Expansions of the monadic second-order (MSO) theory of the structure $\langle
\mathbb{N} ; < \rangle$ have been a fertile and active area of research ever
since the publication of the seminal papers of B\"uchi and Elgot & Rabin on the
subject in the 1960s. In the present paper, we establish decidability of the
MSO theory of $\langle \mathbb{N} ; <,P \rangle$, where $P$ ranges over a large
class of unary ''dynamical'' predicates, i.e., sets of non-negative values
assumed by certain integer linear recurrence sequences. One of our key
technical tools is the novel concept of (effective) prodisjunctivity, which we
expect may also find independent applications further afield.

</details>


### [27] [Transordinal Fixed-Point Operators and Self-Referential Games: A Categorical Framework for Reflective Semantic Convergence](https://arxiv.org/abs/2507.16620)
*Faruk Alpay,Hamdi Al Alakkad*

Main category: cs.LO

TL;DR: 该论文提出了一种统一范畴论固定点构造、超限递归和博弈语义的新理论框架，用于建模语言解释如何通过无限自我引用稳定化。通过在所有序数阶段迭代意义精炼算子，论文隔离出一个独特的“超序数”固定点，并通过一系列反射博弈证明该对象是文本与其解释者之间无限对话的唯一平衡点。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提供一种数学上严格的语言语义收敛模型，无需依赖统计训练或经验基准，同时为形式语言学和语言感知系统的设计提供理论基础。

Method: 方法包括构建一个意义精炼算子并在所有序数阶段迭代，通过反射博弈层级证明固定点的存在性和唯一性。

Result: 结果表明，通过无限对话过程，可以稳定在一个唯一的、自洽的语言解释上，且该构造完全基于符号逻辑。

Conclusion: 论文为形式语言学和语言感知系统的设计提供了精确的理论支持，并解决了关于反射、真理和平衡的长期问题。

Abstract: We present a new theoretical framework that unifies category-theoretic
fixed-point constructions, transfinite recursion, and game-based semantics to
model how interpretations of language can stabilize through unlimited
self-reference. By iterating a meaning-refinement operator across all ordinal
stages, we isolate a unique "transordinal" fixed point and show, via a
hierarchy of reflective games, that this same object is the sole equilibrium of
an infinite dialogue between a text and its interpreter. The result delivers a
mathematically rigorous account of semantic convergence without resorting to
statistical training or empirical benchmarks, yet remains simple to explain:
start with a rough meaning, let speaker and listener correct each other
forever, and the process provably settles on a single, self-consistent
interpretation. Because the construction is entirely symbolic, it offers both
precise guarantees for formal linguistics and a blueprint for designing
language-aware systems that can reason about their own outputs. The paper
details the requisite transordinal machinery, proves existence and uniqueness
theorems, and connects them to long-standing questions about reflection, truth,
and equilibrium in formal systems and semantics.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [28] [Implications of Current Litigation on the Design of AI Systems for Healthcare Delivery](https://arxiv.org/abs/2507.15981)
*Gennie Mansi,Mark Riedl*

Main category: cs.HC

TL;DR: 论文探讨了在医疗AI中，如何通过可解释AI（XAI）和法律途径实现问责制，以保护患者免受AI系统的伤害。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释AI（XAI）系统主要关注医生的需求，但忽略了法律层面如何为受影响的患者提供追索权。论文旨在填补这一空白。

Method: 通过分析31个法律案例和报告的伤害事件，研究了AI系统对患者护理的影响模式。

Result: 发现患者护理涉及多方利益相关者，而AI系统的部署常对护理产生负面影响，迫使患者寻求法律追索。

Conclusion: 建议通过调整责任结构和设计面向法律代表的XAI系统，实现从医生中心到患者中心的问责制转变。

Abstract: Many calls for explainable AI (XAI) systems in medicine are tied to a desire
for AI accountability--accounting for, mitigating, and ultimately preventing
harms from AI systems. Because XAI systems provide human-understandable
explanations for their output, they are often viewed as a primary path to
prevent harms to patients. However, when harm occurs, laws, policies, and
regulations also shape AI accountability by impacting how harmed individuals
can obtain recourse. Current approaches to XAI explore physicians' medical and
relational needs to counter harms to patients, but there is a need to
understand how XAI systems should account for the legal considerations of those
impacted. We conduct an analysis of 31 legal cases and reported harms to
identify patterns around how AI systems impact patient care. Our findings
reflect how patients' medical care relies on a complex web of
stakeholders--physicians, state health departments, health insurers, care
facilities, among others--and many AI systems deployed across their healthcare
delivery negatively impact their care. In response, patients have had no option
but to seek legal recourse for harms. We shift the frame from
physician-centered to patient-centered accountability approaches by describing
how lawyers and technologists need to recognize and address where AI harms
happen. We present paths for preventing or countering harm (1) by changing
liability structures to reflect the role of many stakeholders in shaping how AI
systems impact patient care; and (2) by designing XAI systems that can help
advocates, such as legal representatives, who provide critical legal expertise
and practically support recourse for patients.

</details>


### [29] [Understanding the Impact of Physicians' Legal Considerations on XAI Systems](https://arxiv.org/abs/2507.15996)
*Gennie Mansi,Mark Riedl*

Main category: cs.HC

TL;DR: 研究探讨了医生在法律风险下对医疗AI系统的担忧，并提出设计可解释AI（XAI）系统时需要提供上下文信息以帮助医生降低风险。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统的错误可能对患者造成伤害，医生在法律、伦理和专业上的责任使其需要可解释的系统以降低风险。

Method: 通过访谈10位医生，了解他们对医疗AI系统错误的预期及其与法律风险的关联。

Result: 医生使用AI系统时存在法律风险担忧，但不确定如何调整风险缓解策略；建议XAI设计需提供相关上下文信息。

Conclusion: XAI系统应考虑医生的法律担忧，整合医疗文档和审计信息等非法律因素以指导风险缓解。

Abstract: Physicians are--and feel--ethically, professionally, and legally responsible
for patient outcomes, buffering patients from harmful AI determinations from
medical AI systems. Many have called for explainable AI (XAI) systems to help
physicians incorporate medical AI recommendations into their workflows in a way
that reduces the potential of harms to patients. While prior work has
demonstrated how physicians' legal concerns impact their medical decision
making, little work has explored how XAI systems should be designed in light of
these concerns. In this study, we conducted interviews with 10 physicians to
understand where and how they anticipate errors that may occur with a medical
AI system and how these anticipated errors connect to their legal concerns. In
our study, physicians anticipated risks associated with using an AI system for
patient care, but voiced unknowns around how their legal risk mitigation
strategies may change given a new technical system. Based on these findings, we
describe the implications for designing XAI systems that can address
physicians' legal concerns. Specifically, we identify the need to provide AI
recommendations alongside contextual information that guides their risk
mitigation strategies, including how non-legally related aspects of their
systems, such as medical documentation and auditing requests, might be
incorporated into a legal case.

</details>


### [30] [Buckaroo: A Direct Manipulation Visual Data Wrangler](https://arxiv.org/abs/2507.16073)
*Annabelle Warner,Andrew McNutt,Paul Rosen,El Kindi Rezig*

Main category: cs.HC

TL;DR: Buckaroo是一个可视化系统，通过直接操作可视化对象帮助用户高效完成数据整理，解决传统方法中的繁琐和易错问题。


<details>
  <summary>Details</summary>
Motivation: 数据整理是数据科学中最耗时的阶段，传统手动方法低效且易错，影响数据质量。

Method: Buckaroo自动发现异常数据组并提供修复建议，支持用户通过可视化界面进行迭代操作。

Result: 系统能够快速识别并修复数据问题，提高数据整理的效率和准确性。

Conclusion: Buckaroo通过可视化交互显著提升数据整理的效率和用户体验。

Abstract: Preparing datasets -- a critical phase known as data wrangling -- constitutes
the dominant phase of data science development, consuming upwards of 80% of the
total project time. This phase encompasses a myriad of tasks: parsing data,
restructuring it for analysis, repairing inaccuracies, merging sources,
eliminating duplicates, and ensuring overall data integrity. Traditional
approaches, typically through manual coding in languages such as Python or
using spreadsheets, are not only laborious but also error-prone. These issues
range from missing entries and formatting inconsistencies to data type
inaccuracies, all of which can affect the quality of downstream tasks if not
properly corrected. To address these challenges, we present Buckaroo, a
visualization system to highlight discrepancies in data and enable on-the-spot
corrections through direct manipulations of visual objects. Buckaroo (1)
automatically finds "interesting" data groups that exhibit anomalies compared
to the rest of the groups and recommends them for inspection; (2) suggests
wrangling actions that the user can choose to repair the anomalies; and (3)
allows users to visually manipulate their data by displaying the effects of
their wrangling actions and offering the ability to undo or redo these actions,
which supports the iterative nature of data wrangling. A video companion is
available at https://youtu.be/iXdCYbvpQVE

</details>


### [31] [AI, Expert or Peer? -- Examining the Impact of Perceived Feedback Source on Pre-Service Teachers Feedback Perception and Uptake](https://arxiv.org/abs/2507.16013)
*Lucas Jasper Jacobsen,Ute Mertens,Thorben Jansen,Kira Elena Weber*

Main category: cs.HC

TL;DR: 研究探讨了反馈来源（LLM、专家或同行）对职前教师反馈感知和行为的影响，发现LLM生成的反馈在公平性和有用性评分最高，且反馈质量是预测反馈采纳的唯一重要因素。


<details>
  <summary>Details</summary>
Motivation: 反馈在学习中至关重要，但职前教师对反馈的接受度不仅取决于其质量，还受反馈来源的影响，尤其是LLM在教育反馈中的应用尚缺乏研究。

Method: 通过随机实验，273名职前教师收到不同来源的书面反馈，评估其感知并修订学习目标以测量采纳率。

Result: LLM反馈在公平性和有用性上评分最高，采纳率52%，反馈质量显著预测采纳率，专家反馈被一致认为质量更高。

Conclusion: 需关注来源偏见，提升教师教育中反馈和AI素养。

Abstract: Feedback plays a central role in learning, yet pre-service teachers'
engagement with feedback depends not only on its quality but also on their
perception of the feedback content and source. Large Language Models (LLMs) are
increasingly used to provide educational feedback; however, negative
perceptions may limit their practical use, and little is known about how
pre-service teachers' perceptions and behavioral responses differ by feedback
source. This study investigates how the perceived source of feedback - LLM,
expert, or peer - influences feedback perception and uptake, and whether
recognition accuracy and feedback quality moderate these effects. In a
randomized experiment with 273 pre-service teachers, participants received
written feedback on a mathematics learning goal, identified its source, rated
feedback perceptions across five dimensions (fairness, usefulness, acceptance,
willingness to improve, positive and negative affect), and revised the learning
goal according to the feedback (i.e. feedback uptake). Results revealed that
LLM-generated feedback received the highest ratings in fairness and usefulness,
leading to the highest uptake (52%). Recognition accuracy significantly
moderated the effect of feedback source on perception, with particularly
positive evaluations when LLM feedback was falsely ascribed to experts.
Higher-quality feedback was consistently assigned to experts, indicating an
expertise heuristic in source judgments. Regression analysis showed that only
feedback quality significantly predicted feedback uptake. Findings highlight
the need to address source-related biases and promote feedback and AI literacy
in teacher education.

</details>


### [32] ["Just a strange pic": Evaluating 'safety' in GenAI Image safety annotation tasks from diverse annotators' perspectives](https://arxiv.org/abs/2507.16033)
*Ding Wang,Mark Díaz,Charvi Rastogi,Aida Davani,Vinodkumar Prabhakaran,Pushkar Mishra,Roma Patel,Alicia Parrish,Zoe Ashwood,Michela Paganini,Tian Huey Teh,Verena Rieser,Lora Aroyo*

Main category: cs.HC

TL;DR: 论文探讨了AI生成内容安全评估中的主观性，发现注释者的道德、情感和情境推理超越结构化分类，任务结构和指南也影响判断。建议改进评估设计。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成内容安全评估依赖预设分类，忽视个人、社会和文化感知。需探究注释者的主观推理及其影响。

Method: 分析5,372条开放式评论，研究注释者对AI生成图像的安全性评估，聚焦其道德、情感和情境推理。

Result: 注释者依赖生活经验、集体风险和社会文化意识判断潜在伤害，任务结构和指南显著影响其表达。现有框架忽视关键推理形式。

Conclusion: 需改进评估设计，支持道德反思、区分伤害类型，并包容主观和情境敏感的解释。

Abstract: Understanding what constitutes safety in AI-generated content is complex.
While developers often rely on predefined taxonomies, real-world safety
judgments also involve personal, social, and cultural perceptions of harm. This
paper examines how annotators evaluate the safety of AI-generated images,
focusing on the qualitative reasoning behind their judgments. Analyzing 5,372
open-ended comments, we find that annotators consistently invoke moral,
emotional, and contextual reasoning that extends beyond structured safety
categories. Many reflect on potential harm to others more than to themselves,
grounding their judgments in lived experience, collective risk, and
sociocultural awareness. Beyond individual perceptions, we also find that the
structure of the task itself -- including annotation guidelines -- shapes how
annotators interpret and express harm. Guidelines influence not only which
images are flagged, but also the moral judgment behind the justifications.
Annotators frequently cite factors such as image quality, visual distortion,
and mismatches between prompt and output as contributing to perceived harm
dimensions, which are often overlooked in standard evaluation frameworks. Our
findings reveal that existing safety pipelines miss critical forms of reasoning
that annotators bring to the task. We argue for evaluation designs that
scaffold moral reflection, differentiate types of harm, and make space for
subjective, context-sensitive interpretations of AI-generated content.

</details>


### [33] [Toward music-based stress management: Contemporary biosensing systems for affective regulation](https://arxiv.org/abs/2507.16074)
*Natasha Yamane,Varun Mishra,Matthew S. Goodwin*

Main category: cs.HC

TL;DR: 摘要综述了生物传感技术在音乐情感调节和压力管理中的应用，总结了28项研究，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索生物传感技术在音乐干预中的潜力，以改善情感调节和压力管理的效果。

Method: 通过综合系统性综述和实证研究，分析生物传感系统的应用模态、音乐类型和计算模型。

Result: 研究确认了这些系统的潜在价值，并指出了多模态生物传感、生成式AI等方法的发展空间。

Conclusion: 生物传感系统在音乐干预中具有前景，未来需解决隐私控制和机制研究等问题。

Abstract: In the last decade, researchers have increasingly explored using biosensing
technologies for music-based affective regulation and stress management
interventions in laboratory and real-world settings. These systems -- including
interactive music applications, brain-computer interfaces, and biofeedback
devices -- aim to provide engaging, personalized experiences that improve
therapeutic outcomes. In this scoping and mapping review, we summarize and
synthesize systematic reviews and empirical research on biosensing systems with
potential applications in music-based affective regulation and stress
management, identify gaps in the literature, and highlight promising areas for
future research. We identified 28 studies involving 646 participants, with most
systems utilizing prerecorded music, wearable cardiorespiratory sensors, or
desktop interfaces. We categorize these systems based on their biosensing
modalities, music types, computational models for affect or stress detection
and music prediction, and biofeedback mechanisms. Our findings highlight the
promising potential of these systems and suggest future directions, such as
integrating multimodal biosensing, exploring therapeutic mechanisms of music,
leveraging generative artificial intelligence for personalized music
interventions, and addressing methodological, data privacy, and user control
concerns.

</details>


### [34] [BDIViz: An Interactive Visualization System for Biomedical Schema Matching with LLM-Powered Validation](https://arxiv.org/abs/2507.16117)
*Eden Wu,Dishita G Turakhia,Guande Wu,Christos Koutras,Sarah Keegan,Wenke Liu,Beata Szeitz,David Fenyo,Cláudio T. Silva,Juliana Freire*

Main category: cs.HC

TL;DR: BDIViz是一个新型可视化分析系统，用于简化生物医学数据的模式匹配流程，通过结合多种匹配方法和LLM验证，显著提升了匹配准确性并减少了认知负担。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据协调是进行探索性分析和元研究的关键，但模式匹配（识别不同数据集元素间的语义对应关系）仍然是一项繁琐且易错的任务，现有自动化方法在生物医学数据上表现不佳。

Method: BDIViz采用了一种集成方法，结合多种匹配技术和基于LLM的验证，通过交互式热图总结匹配结果，并提供协调视图以快速比较属性和值。系统还设计了方法无关的架构，能够整合多种模式匹配算法。

Result: 通过两个生物医学案例研究和一项与领域专家进行的用户研究，BDIViz相比基线方法显著提升了模式匹配的准确性，同时减少了认知负担和数据整理时间。

Conclusion: BDIViz通过创新的可视化分析技术有效解决了生物医学数据模式匹配的挑战，为领域专家提供了高效且可扩展的解决方案。

Abstract: Biomedical data harmonization is essential for enabling exploratory analyses
and meta-studies, but the process of schema matching - identifying semantic
correspondences between elements of disparate datasets (schemas) - remains a
labor-intensive and error-prone task. Even state-of-the-art automated methods
often yield low accuracy when applied to biomedical schemas due to the large
number of attributes and nuanced semantic differences between them. We present
BDIViz, a novel visual analytics system designed to streamline the schema
matching process for biomedical data. Through formative studies with domain
experts, we identified key requirements for an effective solution and developed
interactive visualization techniques that address both scalability challenges
and semantic ambiguity. BDIViz employs an ensemble approach that combines
multiple matching methods with LLM-based validation, summarizes matches through
interactive heatmaps, and provides coordinated views that enable users to
quickly compare attributes and their values. Our method-agnostic design allows
the system to integrate various schema matching algorithms and adapt to
application-specific needs. Through two biomedical case studies and a
within-subject user study with domain experts, we demonstrate that BDIViz
significantly improves matching accuracy while reducing cognitive load and
curation time compared to baseline approaches.

</details>


### [35] [AI-enhanced conversational agents for personalized asthma support Factors for engagement, value and efficacy](https://arxiv.org/abs/2507.16735)
*Laura Moradbakhti,Dorian Peters,Jennifer K. Quint,Björn Schuller,Darren Cook,Rafael A. Calvo*

Main category: cs.HC

TL;DR: 英国哮喘死亡率欧洲最高，仅30%患者能获得基本护理。研究探讨聊天机器人如何提供健康教育与自我管理支持。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索聊天机器人能否为哮喘患者提供个性化健康支持，并分析患者使用意愿及影响因素。

Method: 通过1257名患者的调查，结合临床医生、患者和技术开发者的意见，研究聊天机器人的最佳设计。

Result: 53%患者愿意使用聊天机器人，偏好24/7服务、个性化及WhatsApp接入方式，但担心隐私和技术可靠性。

Conclusion: 研究总结7条建议，帮助开发者为哮喘患者设计更有效的聊天机器人支持。

Abstract: Asthma-related deaths in the UK are the highest in Europe, and only 30% of
patients access basic care. There is a need for alternative approaches to
reaching people with asthma in order to provide health education,
self-management support and bridges to care. Automated conversational agents
(specifically, mobile chatbots) present opportunities for providing alternative
and individually tailored access to health education, self-management support
and risk self-assessment. But would patients engage with a chatbot, and what
factors influence engagement? We present results from a patient survey (N=1257)
devised by a team of asthma clinicians, patients, and technology developers,
conducted to identify optimal factors for efficacy, value and engagement for a
chatbot. Results indicate that most adults with asthma (53%) are interested in
using a chatbot and the patients most likely to do so are those who believe
their asthma is more serious and who are less confident about self-management.
Results also indicate enthusiasm for 24/7 access, personalisation, and for
WhatsApp as the preferred access method (compared to app, voice assistant, SMS
or website). Obstacles to uptake include security/privacy concerns and
skepticism of technological capabilities. We present detailed findings and
consolidate these into 7 recommendations for developers for optimising efficacy
of chatbot-based health support.

</details>


### [36] [A Human-Centered Approach to Identifying Promises, Risks, & Challenges of Text-to-Image Generative AI in Radiology](https://arxiv.org/abs/2507.16207)
*Katelyn Morrison,Arpit Mathur,Aidan Bradshaw,Tom Wartmann,Steven Lundi,Afrooz Zandifar,Weichang Dai,Kayhan Batmanghelich,Motahhare Eslami,Adam Perer*

Main category: cs.HC

TL;DR: 论文探讨了文本到CT扫描生成AI在医疗领域的应用，强调开发者需与医疗从业者合作，避免开发出无用或有害的模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补当前文本到图像生成模型在医疗领域中的空白，关注医疗专业人员在实际中如何使用及受益于此类技术。

Method: 采用以人为中心的方法，邀请医学生、放射科实习生和放射科医师参与评估和反思新型文本到CT扫描生成AI模型的潜力、风险和挑战。

Result: 研究发现医疗从业者对文本到CT扫描生成AI在教育、培训和实践中的潜在作用持不同观点，同时揭示了技术挑战和领域特异性风险。

Conclusion: 通过人类中心化的方法，研究强调了医疗文本到图像生成AI的潜力和风险，为未来开发提供了实践指导。

Abstract: As text-to-image generative models rapidly improve, AI researchers are making
significant advances in developing domain-specific models capable of generating
complex medical imagery from text prompts. Despite this, these technical
advancements have overlooked whether and how medical professionals would
benefit from and use text-to-image generative AI (GenAI) in practice. By
developing domain-specific GenAI without involving stakeholders, we risk the
potential of building models that are either not useful or even more harmful
than helpful. In this paper, we adopt a human-centered approach to responsible
model development by involving stakeholders in evaluating and reflecting on the
promises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through
exploratory model prompting activities, we uncover the perspectives of medical
students, radiology trainees, and radiologists on the role that text-to-CT Scan
GenAI can play across medical education, training, and practice. This
human-centered approach additionally enabled us to surface technical challenges
and domain-specific risks of generating synthetic medical images. We conclude
by reflecting on the implications of medical text-to-image GenAI.

</details>


### [37] [Animal Interaction with Autonomous Mobility Systems: Designing for Multi-Species Coexistence](https://arxiv.org/abs/2507.16258)
*Tram Thi Minh Tran,Xinyan Yu,Marius Hoggenmueller,Callum Paker,Paul Schmitt,Julie Stephany Berrio Perez,Stewart Worrall,Martin Tomitsch*

Main category: cs.HC

TL;DR: 论文探讨了自动驾驶系统与动物互动的五个关键问题，并提出了改善多物种共存的设计与政策方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于对自动驾驶系统设计中缺乏对非人类物种感知与回应的理解，结合动物-计算机交互和多物种设计理念。

Method: 采用多方法研究，包括文献综述（45篇）、网络民族志（39个视频和11个讨论）及专家访谈（8名参与者）。

Result: 分析揭示了物理影响、行为效应、无障碍性、伦理法规和城市干扰五大问题。

Conclusion: 强调需要纳入非人类视角以促进更安全、包容的未来，并提出了设计与政策建议。

Abstract: Autonomous mobility systems increasingly operate in environments shared with
animals, from urban pets to wildlife. However, their design has largely focused
on human interaction, with limited understanding of how non-human species
perceive, respond to, or are affected by these systems. Motivated by research
in Animal-Computer Interaction (ACI) and more-than-human design, this study
investigates animal interactions with autonomous mobility through a
multi-method approach combining a scoping review (45 articles), online
ethnography (39 YouTube videos and 11 Reddit discussions), and expert
interviews (8 participants). Our analysis surfaces five key areas of concern:
Physical Impact (e.g., collisions, failures to detect), Behavioural Effects
(e.g., avoidance, stress), Accessibility Concerns (particularly for service
animals), Ethics and Regulations, and Urban Disturbance. We conclude with
design and policy directions aimed at supporting multispecies coexistence in
the age of autonomous systems. This work underscores the importance of
incorporating non-human perspectives to ensure safer, more inclusive futures
for all species.

</details>


### [38] [SceneLoom: Communicating Data with Scene Context](https://arxiv.org/abs/2507.16466)
*Lin Gao,Leixian Shen,Yuheng Zhao,Jiexiang Lan,Huamin Qu,Siming Chen*

Main category: cs.HC

TL;DR: SceneLoom是一个基于视觉语言模型的系统，旨在将数据可视化与真实世界图像协调结合，提升数据的叙述表现力和吸引力。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动叙事（如数据新闻和数据视频）中，数据可视化与真实世界图像通常分离，限制了其叙述表达。

Method: SceneLoom通过视觉语言模型提取图像和数据可视化的特征，利用空间组织、形状相似性等设计映射生成协调的设计方案。

Result: 系统生成了具有视觉、语义和数据一致性的设计方案，用户研究验证了其在创意设计和设计外化中的有效性。

Conclusion: SceneLoom通过协调数据可视化与真实图像，显著提升了数据叙事的表达力和吸引力。

Abstract: In data-driven storytelling contexts such as data journalism and data videos,
data visualizations are often presented alongside real-world imagery to support
narrative context. However, these visualizations and contextual images
typically remain separated, limiting their combined narrative expressiveness
and engagement. Achieving this is challenging due to the need for fine-grained
alignment and creative ideation. To address this, we present SceneLoom, a
Vision-Language Model (VLM)-powered system that facilitates the coordination of
data visualization with real-world imagery based on narrative intents. Through
a formative study, we investigated the design space of coordination
relationships between data visualization and real-world scenes from the
perspectives of visual alignment and semantic coherence. Guided by the derived
design considerations, SceneLoom leverages VLMs to extract visual and semantic
features from scene images and data visualization, and perform design mapping
through a reasoning process that incorporates spatial organization, shape
similarity, layout consistency, and semantic binding. The system generates a
set of contextually expressive, image-driven design alternatives that achieve
coherent alignments across visual, semantic, and data dimensions. Users can
explore these alternatives, select preferred mappings, and further refine the
design through interactive adjustments and animated transitions to support
expressive data communication. A user study and an example gallery validate
SceneLoom's effectiveness in inspiring creative design and facilitating design
externalization.

</details>


### [39] [The Effect of Scale Consistency between Real and Virtual Spaces on Immersion in Exhibition Hybrid Spaces](https://arxiv.org/abs/2507.16542)
*Qiong Wu,Yan Dong,Zipeng Zhang,Ruochen Hu*

Main category: cs.HC

TL;DR: 研究探讨了虚拟与现实空间比例对沉浸感的影响，发现虚拟空间比例为130%、虚拟形象比例为75%-100%时最佳。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对虚拟与现实空间比例的系统研究，影响用户沉浸感。

Method: 基于Intel 3D Athlete Tracking技术开发沉浸交互系统，通过两个实验研究虚拟空间和虚拟形象比例对沉浸感的影响。

Result: 虚拟空间130%比例最受欢迎；虚拟形象比例在75%-100%时沉浸感最佳，超出此范围则显著下降。

Conclusion: 虚拟空间略大于现实空间、虚拟形象略小于用户实际尺寸可优化沉浸感，已在Intel全球贸易中心展厅应用。

Abstract: In exhibition hybrid spaces, scale consistency between real and virtual
spaces is crucial for user immersion. However, there is currently a lack of
systematic research to determine appropriate virtual-to-real mapping ratios.
This study developed an immersive interaction system based on Intel 3D Athlete
Tracking body mapping technology. Two experiments investigated the impact of
virtual space and virtual avatar scale on immersion. Experiment 1 investigated
30 participants' preferences for virtual space scale, while Experiment 2 tested
the effect of 6 different virtual avatar sizes (25%-150%) on immersion. A
5-point Likert scale was used to assess immersion, followed by analysis of
variance and Tukey HSD post-hoc tests. Experiment 1 showed that participants
preferred a virtual space ratio of 130% (mean 127.29%, SD 8.55%). Experiment 2
found that virtual avatar sizes within the 75%-100% range produced optimal
immersion (p < 0.05). Immersion decreased significantly when virtual avatar
sizes deviated from users' actual height (below 50% or above 125%).
Participants were more sensitive to size changes in the 25%-75% range, while
perception was weaker for changes in the 75%-100% range. Virtual environments
slightly larger than real space (130%) and virtual avatars slightly smaller
than users (75%-100%) optimize user immersion. These findings have been applied
in the Intel Global Trade Center exhibition hall, demonstrating actionable
insights for designing hybrid spaces that enhance immersion and coherence.

</details>


### [40] [Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology: A User Study (Extended Version)](https://arxiv.org/abs/2507.16562)
*Megha Quamara,Viktor Schmuck,Cristina Iani,Axel Primavesi,Alexander Plaum,Luca Vigano*

Main category: cs.HC

TL;DR: 本文研究了远程可访问的基于Web的XR培训系统中虚拟代理的社会接受度，特别针对记者群体。通过调整Almere模型，评估了用户感知和行为，发现了改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究记者在敏感或危险场景下使用XR代理技术的接受度，旨在提供更好的数字远程培训解决方案。

Method: 使用模块化工具包设计了虚拟代理交互，通过问卷调查和实验测量用户感知。

Result: 定量和定性结果表明用户对XR代理的接受度较高，但也需要改进依赖性和安全性。

Conclusion: XR代理技术在记者培训中有潜力，但需优化系统以提高用户信任和体验。

Abstract: In this paper, we present the findings of a user study that evaluated the
social acceptance of eXtended Reality (XR) agent technology, focusing on a
remotely accessible, web-based XR training system developed for journalists.
This system involves user interaction with a virtual avatar, enabled by a
modular toolkit. The interactions are designed to provide tailored training for
journalists in digital-remote settings, especially for sensitive or dangerous
scenarios, without requiring specialized end-user equipment like headsets. Our
research adapts and extends the Almere model, representing social acceptance
through existing attributes such as perceived ease of use and perceived
usefulness, along with added ones like dependability and security in the
user-agent interaction. The XR agent was tested through a controlled experiment
in a real-world setting, with data collected on users' perceptions. Our
findings, based on quantitative and qualitative measurements involving
questionnaires, contribute to the understanding of user perceptions and
acceptance of XR agent solutions within a specific social context, while also
identifying areas for the improvement of XR systems.

</details>


### [41] [Animated Transition between Node-Link and Parallel Coordinates Visualizations](https://arxiv.org/abs/2507.16563)
*Abdulhaq Adetunji Salako,Hannes Hagen,Christian Tominski*

Main category: cs.HC

TL;DR: 该研究探讨如何通过平滑动画过渡缩小节点链接图与平行坐标图之间的差距，提升数据理解。


<details>
  <summary>Details</summary>
Motivation: 数据可视化通常涉及多个视图，用户需手动整合信息，增加了认知负担。动画过渡可辅助整合，但现有方法多限于相同数据层面的基础视图。

Method: 基于可追溯性和快速性两个设计目标，构建部分设计空间并实现两种过渡变体：基础插值和高级动画技术（如分段和交错）。

Result: 初步研究发现，用户偏好基础变体的快速性，但高级变体在数据项可追溯性上表现更好。

Conclusion: 动画过渡可有效辅助不同视图的数据整合，但需权衡速度与可追溯性，未来可进一步优化设计。

Abstract: Multi-faceted data visualization typically involves several dedicated views.
To create a comprehensive understanding of the data, users have to mentally
integrate the information from the different views. This integration is
hindered by context switches between views and usually requires interactive
methods such as brushing and linking. Animated transitions have also been shown
to be able to mediate context switches and improve understanding. Yet, most
existing animated transitions consider only basic views showing the same data
facet. In this work, we study how the gap between node-link diagrams, showing
graph structure, and parallel coordinates plots, showing multivariate
attributes, can be narrowed via smooth animated transitions. Based on two
design goals (traceability and swiftness), we outline a partial design space
including several design options. These inform the implementation of two
alternative transition variants: a basic variant with plain interpolation and
an advanced variant that uses our design space and accepted animation
techniques, including staging and staggering. In a preliminary study, we asked
seven participants for qualitative feedback. We found that the swiftness of the
basic variant is preferred, while the traceability of data items is better with
the slower advanced variant.

</details>


### [42] [AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up with Industry Demands? A Multivocal Literature Review](https://arxiv.org/abs/2507.16586)
*Choro Ulan Uulu,Mikhail Kulyabin,Layan Etaiwi,Nuno Miguel Martins Pacheco,Jan Joosten,Kerstin Röse,Filippos Petridis,Jan Bosch,Helena Holmström Olsson*

Main category: cs.HC

TL;DR: 该论文探讨了AI如何提升CAE软件的用户体验，通过多声道文献综述发现学术与工业应用之间的差距，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: CAE软件在用户体验方面存在挑战，影响效率和可访问性，而AI的应用潜力尚未充分探索。

Method: 采用多声道文献综述（MLR）方法，综合分析学术研究和工业实践中AI在CAE中的UX应用。

Result: 发现学术研究与工业应用存在显著差距，工业界已尝试LLM、自适应UI和推荐系统，而学术研究缺乏UX验证。

Conclusion: 研究指出了AI驱动的引导、自适应界面和工作流自动化的机会，为未来CAE用户体验改进提供了方向。

Abstract: Computer-Aided Engineering (CAE) enables simulation experts to optimize
complex models, but faces challenges in user experience (UX) that limit
efficiency and accessibility. While artificial intelligence (AI) has
demonstrated potential to enhance CAE processes, research integrating these
fields with a focus on UX remains fragmented. This paper presents a multivocal
literature review (MLR) examining how AI enhances UX in CAE software across
both academic research and industry implementations. Our analysis reveals
significant gaps between academic explorations and industry applications, with
companies actively implementing LLMs, adaptive UIs, and recommender systems
while academic research focuses primarily on technical capabilities without UX
validation. Key findings demonstrate opportunities in AI-powered guidance,
adaptive interfaces, and workflow automation that remain underexplored in
current research. By mapping the intersection of these domains, this study
provides a foundation for future work to address the identified research gaps
and advance the integration of AI to improve CAE user experience.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [43] [Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars](https://arxiv.org/abs/2507.15979)
*Marcel C. Bühler,Ye Yuan,Xueting Li,Yangyi Huang,Koki Nagano,Umar Iqbal*

Main category: cs.GR

TL;DR: DLA是一个新框架，通过单张图像重建可动画的3D人体化身，结合多视角生成、3D高斯提升和姿态感知的UV空间映射。


<details>
  <summary>Details</summary>
Motivation: 解决从单张图像生成高质量、可动画的3D人体化身的挑战，提升生成模型的逼真度和动画能力。

Method: 利用视频扩散模型生成多视角图像，通过3D高斯提升和变换器编码器将高斯投影到UV空间，实现动画化。

Result: 在ActorsHQ和4D-Dress数据集上表现优于现有方法，支持实时渲染和直观编辑。

Conclusion: DLA通过结合视频扩散模型和UV空间映射，填补了非结构化3D表示与高保真动画化身的差距。

Abstract: We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs
animatable 3D human avatars from a single image. This is achieved by leveraging
multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of
3D Gaussians. Given an image, we first dream plausible multi-views using a
video diffusion model, capturing rich geometric and appearance details. These
views are then lifted into unstructured 3D Gaussians. To enable animation, we
propose a transformer-based encoder that models global spatial relationships
and projects these Gaussians into a structured latent representation aligned
with the UV space of a parametric body model. This latent code is decoded into
UV-space Gaussians that can be animated via body-driven deformation and
rendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV
manifold, our method ensures consistency during animation while preserving fine
visual details. DLA enables real-time rendering and intuitive editing without
requiring post-processing. Our method outperforms state-of-the-art approaches
on ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric
accuracy. By combining the generative strengths of video diffusion models with
a pose-aware UV-space Gaussian mapping, DLA bridges the gap between
unstructured 3D representations and high-fidelity, animation-ready avatars.

</details>


### [44] [MMS Player: an open source software for parametric data-driven animation of Sign Language avatars](https://arxiv.org/abs/2507.16463)
*Fabrizio Nunnari,Shailesh Mishra,Patrick Gebhard*

Main category: cs.GR

TL;DR: MMS-Player是一个开源软件，能够从新型手语表示格式MMS合成手语动画，支持并行手势、计时和变形的信息。


<details>
  <summary>Details</summary>
Motivation: 增强基于注释的手语表示，提供更丰富的手语动画合成功能。

Method: 使用Python脚本在Blender中实现，支持命令行和HTTP API调用。

Result: 可生成视频或其他3D动画交换格式的手语动画。

Conclusion: MMS-Player是一个功能强大且开源的手语动画合成工具。

Abstract: This paper describes the MMS-Player, an open source software able to
synthesise sign language animations from a novel sign language representation
format called MMS (MultiModal Signstream). The MMS enhances gloss-based
representations by adding information on parallel execution of signs, timing,
and inflections. The implementation consists of Python scripts for the popular
Blender 3D authoring tool and can be invoked via command line or HTTP API.
Animations can be rendered as videos or exported in other popular 3D animation
exchange formats. The software is freely available under GPL-3.0 license at
https://github.com/DFKI-SignLanguage/MMS-Player.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [45] [Prediction of Alpha-Particle-Immune Gate-All-Around Field-Effect Transistors (GAA-FET) Based SRAM Design](https://arxiv.org/abs/2507.15860)
*Albert Lu,Reza Arghavani,Hiu Yung Wong*

Main category: cs.ET

TL;DR: 该论文通过3D TCAD模拟设计了一种基于GAA-FET技术的SRAM，可以避免单α粒子辐射导致的错误，从而消除单粒子翻转（SEU）问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决传统SRAM在辐射环境下容易因α粒子导致SEU的问题，通过新型GAA-FET技术提高其抗辐射能力。

Method: 方法包括使用PHITS进行ab initio计算确定α粒子在硅和硅锗合金中的最大线性能量转移（LETmax），并设计具有底部电介质隔离（BDI）的亚7纳米GAA-FET SRAM。

Result: 结果表明，即使在最坏情况下（LET > LETmax），设计的SRAM也不会发生翻转。

Conclusion: 结论表明，GAA-FET结合BDI的SRAM设计可以有效抵抗α粒子辐射引发的SEU，为高性能抗辐射存储器提供了新思路。

Abstract: In this paper, using 3D Technology Computer-Aided-Design (TCAD) simulations,
we show that it is possible to design a static random-access memory (SRAM)
using gate-all-around field-effect-transistor (GAA-FET) technology so that it
is immune to single alpha particle radiation error. In other words, with the
design, there will be no single-event upset (SEU) due to alpha particles. We
first use ab initio calculations in PHITS to show that there is a maximum
linear energy transfer (LET), LETmax, for the alpha particle in Si and
Si$_x$Ge$_{1-x}$. Based on that, by designing a sub-7nm GAA-FET-based SRAM with
bottom dielectric isolation (BDI), we show that the SRAM does not flip even if
the particle strike is in the worst-case scenario (LET > LETmax).

</details>


### [46] [AI-driven Orchestration at Scale: Estimating Service Metrics on National-Wide Testbeds](https://arxiv.org/abs/2507.16077)
*Rodrigo Moreira,Rafael Pasquini,Joberto S. B. Martins,Tereza C. Carvalho,Flávio de Oliveira Silva*

Main category: cs.ET

TL;DR: 本文提出了一种基于DNN和基本ML算法的网络切片预测模型，用于在真实大规模生产测试台上验证AI原生编排架构的性能。


<details>
  <summary>Details</summary>
Motivation: 网络切片需要AI原生编排架构以高效处理异构需求，但现有方法难以在生产环境中验证其ML编排效果。

Method: 开发了一种利用DNN和基础ML算法的预测模型，针对分布式数据库应用的网络切片在两大生产测试台上进行性能比较。

Result: 研究表明AI预测模型能提升网络切片编排架构的性能，并提供了一种生产环境可行的验证方法。

Conclusion: AI模型可优化网络切片编排，提出的验证方法为生产环境测试提供了替代方案。

Abstract: Network Slicing (NS) realization requires AI-native orchestration
architectures to efficiently and intelligently handle heterogeneous user
requirements. To achieve this, network slicing is evolving towards a more
user-centric digital transformation, focusing on architectures that incorporate
native intelligence to enable self-managed connectivity in an integrated and
isolated manner. However, these initiatives face the challenge of validating
their results in production environments, particularly those utilizing
ML-enabled orchestration, as they are often tested in local networks or
laboratory simulations. This paper proposes a large-scale validation method
using a network slicing prediction model to forecast latency using Deep Neural
Networks (DNNs) and basic ML algorithms embedded within an NS architecture,
evaluated in real large-scale production testbeds. It measures and compares the
performance of different DNNs and ML algorithms, considering a distributed
database application deployed as a network slice over two large-scale
production testbeds. The investigation highlights how AI-based prediction
models can enhance network slicing orchestration architectures and presents a
seamless, production-ready validation method as an alternative to fully
controlled simulations or laboratory setups.

</details>


### [47] [Quantum Annealing Hyperparameter Analysis for Optimal Sensor Placement in Production Environments](https://arxiv.org/abs/2507.16584)
*Nico Kraus,Marvin Erdmann,Alexander Kuzmany,Daniel Porawski,Jonas Stein*

Main category: cs.ET

TL;DR: 论文探讨了利用量子退火优化汽车制造中传感器布局的方法，以解决大规模计算问题并降低成本。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在解决大规模传感器布局问题时效果不佳，导致非最优解和成本增加，因此探索量子计算方法以提升效率。

Method: 采用量子退火（D-Wave）将问题转化为二次无约束二进制优化形式，优化超参数并与默认设置对比。

Result: 量子退火能够解决实际场景中的问题，并通过分解技术扩展问题规模，接近工业应用。

Conclusion: 量子计算在未来成熟后，有望为大规模优化问题提供高效且低成本的解决方案。

Abstract: To increase efficiency in automotive manufacturing, newly produced vehicles
can move autonomously from the production line to the distribution area. This
requires an optimal placement of sensors to ensure full coverage while
minimizing the number of sensors used. The underlying optimization problem
poses a computational challenge due to its large-scale nature. Currently,
classical solvers rely on heuristics, often yielding non-optimal solutions for
large instances, resulting in suboptimal sensor distributions and increased
operational costs.
  We explore quantum computing methods that may outperform classical heuristics
in the future. We implemented quantum annealing with D-Wave, transforming the
problem into a quadratic unconstrained binary optimization formulation with
one-hot and binary encoding. Hyperparameters like the penalty terms and the
annealing time are optimized and the results are compared with default
parameter settings.
  Our results demonstrate that quantum annealing is capable of solving
instances derived from real-world scenarios. Through the use of decomposition
techniques, we are able to scale the problem size further, bringing it closer
to practical, industrial applicability. Through this work, we provide key
insights into the importance of quantum annealing parametrization,
demonstrating how quantum computing could contribute to cost-efficient,
large-scale optimization problems once the hardware matures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [48] [Resilience Evaluation of Kubernetes in Cloud-Edge Environments via Failure Injection](https://arxiv.org/abs/2507.16109)
*Zihao Chen,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 提出了一种新的Kubernetes弹性评估框架，结合故障注入工具和工作负载生成，针对混合云-边缘环境进行测试，生成了首个全面的弹性数据集。


<details>
  <summary>Details</summary>
Motivation: Kubernetes在云-边缘环境中的广泛应用需要系统化的弹性评估，但目前研究较少。

Method: 整合主流故障注入工具（如Chaos Mesh、Gremlin、ChaosBlade）和流量模拟工具，自动化编排复杂故障场景。

Result: 生成超过30 GB的性能数据，显示边缘部署在网络延迟和分区条件下稳定性提高80%，云部署在带宽限制下弹性提高47%。

Conclusion: 为云-边缘部署的架构决策提供了量化依据。

Abstract: Kubernetes has emerged as an essential platform for deploying containerised
applications across cloud and edge infrastructures. As Kubernetes gains
increasing adoption for mission-critical microservices, evaluating system
resilience under realistic fault conditions becomes crucial. However,
systematic resilience assessments of Kubernetes in hybrid cloud-edge
environments are currently limited in research. To address this gap, a novel
resilience evaluation framework integrates mainstream fault injection tools
with automated workload generation for comprehensive cloud-edge Kubernetes
testing. Multiple fault injection platforms, including Chaos Mesh, Gremlin, and
ChaosBlade are combined with realistic traffic simulation tools, enabling
automated orchestration of complex failure scenarios. Through this framework,
comprehensive experiments are conducted that systematically target node-level,
pod-level, and network failures across cloud and cloud-edge environments. The
first comprehensive resilience dataset for hybrid cloud-edge Kubernetes
deployments is created, comprising over 30 GB of performance data from 11,965
fault injection scenarios including response times, failure rates, and error
patterns. Analysis reveals that cloud-edge deployments demonstrate 80% superior
response stability under network delay and partition conditions, while cloud
deployments exhibit 47% better resilience under bandwidth limitations,
providing quantitative guidance for architectural decision-making in cloud-edge
deployments.

</details>


### [49] [Parallel Ray Tracing of Black Hole Images Using the Schwarzschild Metric](https://arxiv.org/abs/2507.16165)
*Liam Naddell,Marcelo Ponce*

Main category: cs.DC

TL;DR: 论文描述了一种并行开源程序，用于在黑洞几何存在下进行光线追踪成像，结合了科学计算中的多种技术。


<details>
  <summary>Details</summary>
Motivation: 通过光线追踪技术渲染黑洞图像在科学和天体物理学可视化中常见，但缺乏并行开源解决方案。

Method: 结合数学近似、科学库、共享内存和分布式内存并行等技术，实现并行光线追踪。

Result: 成功开发出可处理黑洞几何的并行开源光线追踪程序。

Conclusion: 该方法为黑洞和其他复杂几何的光线追踪提供了一种高效的开源解决方案。

Abstract: Rendering images of black holes by utilizing ray tracing techniques is a
common methodology employed in many aspects of scientific and astrophysical
visualizations. Similarly, general ray tracing techniques are widely used in
areas related to computer graphics. In this work we describe the implementation
of a parallel open-source program that can ray trace images in the presence of
a black hole geometry. We do this by combining a couple of different techniques
usually present in parallel scientific computing, such as, mathematical
approximations, utilization of scientific libraries, shared-memory and
distributed-memory parallelism.

</details>


### [50] [AcceleratedKernels.jl: Cross-Architecture Parallel Algorithms from a Unified, Transpiled Codebase](https://arxiv.org/abs/2507.16710)
*Andrei-Leonard Nicusan,Dominik Werner,Simon Branford,Simon Hartley,Andrew J. Morris,Kit Windows-Yule*

Main category: cs.DC

TL;DR: AcceleratedKernels.jl是一个后端无关的Julia并行计算库，通过独特的转译架构支持NVIDIA、AMD、Intel和Apple加速器，性能媲美C和OpenMP多线程CPU实现，并在某些情况下提供更一致的数值性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个统一、紧凑的代码库，简化并行编程的实现和使用复杂性，同时支持多种硬件加速器。

Method: 采用转译架构，支持不同硬件加速器，并通过Julia实现高效的并行计算，同时支持CPU-GPU协同处理。

Result: 在200个NVIDIA A100 GPU上实现了538-855 GB/s的世界级排序吞吐量，通信密集型HPC任务在GPU上经济可行。

Conclusion: AcceleratedKernels.jl展示了Julia在并行计算中的高性能和灵活性，特别是在支持多种硬件加速器和高效协同处理方面。

Abstract: AcceleratedKernels.jl is introduced as a backend-agnostic library for
parallel computing in Julia, natively targeting NVIDIA, AMD, Intel, and Apple
accelerators via a unique transpilation architecture. Written in a unified,
compact codebase, it enables productive parallel programming with minimised
implementation and usage complexities. Benchmarks of arithmetic-heavy kernels
show performance on par with C and OpenMP-multithreaded CPU implementations,
with Julia sometimes offering more consistent and predictable numerical
performance than conventional C compilers. Exceptional composability is
highlighted as simultaneous CPU-GPU co-processing is achievable - such as
CPU-GPU co-sorting - with transparent use of hardware-specialised MPI
implementations. Tests on the Baskerville Tier 2 UK HPC cluster achieved
world-class sorting throughputs of 538-855 GB/s using 200 NVIDIA A100 GPUs,
comparable to the highest literature-reported figure of 900 GB/s achieved on
262,144 CPU cores. The use of direct NVLink GPU-to-GPU interconnects resulted
in a 4.93x speedup on average; normalised by a combined capital, running and
environmental cost, communication-heavy HPC tasks only become economically
viable on GPUs if GPUDirect interconnects are employed.

</details>


### [51] [Autonomous Dominant Resource Fairness for Blockchain Ecosystems](https://arxiv.org/abs/2507.16350)
*Serdar Metin*

Main category: cs.DC

TL;DR: 论文探讨了区块链在多资源类型分配中的应用，提出了一种名为自治主导资源公平性（ADRF）的智能合约算法，用于解决异构资源需求的任务分配问题。


<details>
  <summary>Details</summary>
Motivation: 当前区块链研究中，多资源类型分配常被视为单资源类型问题，缺乏对异构需求的灵活处理。研究旨在填补这一空白。

Method: 采用智能合约版本的预计算主导资源公平性（PDRF）算法，避免迭代循环以适应区块链的区块气体限制。

Result: 实验数据表明，ADRF算法在气体成本效率上表现优异，能管理数百种资源类型，适用于无限制用户数量。

Conclusion: ADRF为区块链环境中的多资源分配提供了一种高效且灵活的解决方案。

Abstract: Blockchain systems have been a part of mainstream academic research, and a
hot topic at that. It has spread to almost every subfield in the computer
science literature, as well as economics and finance. Especially in a world
where digital trust is much sought for, blockchains offer a rich variety of
desired properties, such as immutability, public auditing, decentralised record
keeping, among others. Not only has it been a research topic of its own, the
integration of blockchains into other systems has been proposed as solutions in
many areas, ranging from grid computing, cloud and fog computing, to internet
of things, self driving vehicles , and smart cities. In many cases the primary
function attributed to blockchains in these contexts is resource management.
Although much attention is paid to this topic, the focus is on single resource
allocation scenarios. Even the cases where multiple resource types are to be
allocated, are treated as single resource type scenarios, and problems are
formulated as allocating standardised bundles consisting of a fixed amount of
each of them, such as virtual machines. The present study addresses the problem
of allocating multiple resource types among tasks with heterogeneous resource
demands with a smart contract adaptation of Precomputed Dominant Resource
Fairness; an algorithm that approximates Dominant Resource Fairness, without
loop iterations, which makes it preferable in the blockchain context because of
the block gas limit. We present the resulting algorithm, Autonomous Dominant
Resource Fairness, along with the empirical data collected from the tests run
on the algorithm. The results show that Autonomous Dominant Resource Fairness
is a gas-cost efficient algorithm, which can be used to manage hundreds of
resource types for unlimited number of users.

</details>


### [52] [FOGNITE: Federated Learning-Enhanced Fog-Cloud Architecture](https://arxiv.org/abs/2507.16668)
*Somayeh Sobati-M*

Main category: cs.DC

TL;DR: 智能电网需要快速、智能且能耗感知的边缘计算。本文提出FOGNITE框架，结合联邦学习、强化学习和数字孪生技术，提升分布式能源系统的自主性、弹性和效率。实验显示其显著优于传统架构。


<details>
  <summary>Details</summary>
Motivation: 现代智能电网需实时处理波动并确保可靠运行，需边缘计算支持。传统架构在响应速度和能效上不足，需更智能的解决方案。

Method: FOGNITE融合联邦学习、强化学习和数字孪生技术，通过本地CNN-LSTM模型预测能源消耗，动态调度任务并验证决策。

Result: 实验证明，FOGNITE在负载平衡准确率上提升93.7%，能耗浪费减少63.2%。

Conclusion: FOGNITE实现了从被动校正到主动优化的智能电网控制，为可持续能源基础设施提供了新方向。

Abstract: Modern smart grids demand fast, intelligent, and energy-aware computing at
the edge to manage real time fluctuations and ensure reliable operation. This
paper introduces FOGNITE Fog-based Grid In intelligence with Neural Integration
and Twin based Execution a next-generation fog cloud framework designed to
enhance autonomy, resilience, and efficiency in distributed energy systems.
FOGNITE combines three core components: federated learning, reinforcement
learning, and digital twin validation. Each fog node trains a local CNN LSTM
model on private energy consumption data, enabling predictive intelligence
while preserving data privacy through federated aggregation. A reinforcement
learning agent dynamically schedules tasks based on current system load and
energy conditions, optimizing for performance under uncertainty.
  To prevent unsafe or inefficient decisions, a hierarchical digital twin layer
simulates potential actions before deployment, significantly reducing execution
errors and energy waste. We evaluate FOGNITE on a real world testbed of
Raspberry Pi devices, showing up to a 93.7% improvement in load balancing
accuracy and a 63.2% reduction in energy waste compared to conventional
architectures. By shifting smart grid control from reactive correction to
proactive optimization, FOGNITE represents a step toward more intelligent,
adaptive, and sustainable energy infrastructures

</details>


### [53] [Collaborative Inference and Learning between Edge SLMs and Cloud LLMs: A Survey of Algorithms, Execution, and Open Challenges](https://arxiv.org/abs/2507.16731)
*Senyao Li,Haozhao Wang,Wenchao Xu,Rui Zhang,Song Guo,Jingling Yuan,Xian Zhong,Tianwei Zhang,Ruixuan Li*

Main category: cs.DC

TL;DR: 本文研究了云与边缘设备上LLM和SLM的协作范式，提出了统一的推理和训练分类策略，并总结了数据集、基准和隐私保护方法。


<details>
  <summary>Details</summary>
Motivation: 由于延迟、隐私、成本和个人化等问题，仅依靠云端或边缘设备的LLM部署变得不足。

Method: 提出了任务分配、任务分割和混合协作的分类，并探讨了参数对齐、剪枝、双向蒸馏等技术。

Result: 提供了一个系统的LLM-SLM协作基础，实现了高效、可扩展且可信的边缘云智能。

Conclusion: 该研究为云与边缘协作提供了全面的框架，促进了系统与算法的协同设计。

Abstract: As large language models (LLMs) evolve, deploying them solely in the cloud or
compressing them for edge devices has become inadequate due to concerns about
latency, privacy, cost, and personalization. This survey explores a
collaborative paradigm in which cloud-based LLMs and edge-deployed small
language models (SLMs) cooperate across both inference and training. We present
a unified taxonomy of edge-cloud collaboration strategies. For inference, we
categorize approaches into task assignment, task division, and mixture-based
collaboration at both task and token granularity, encompassing adaptive
scheduling, resource-aware offloading, speculative decoding, and modular
routing. For training, we review distributed adaptation techniques, including
parameter alignment, pruning, bidirectional distillation, and
small-model-guided optimization. We further summarize datasets, benchmarks, and
deployment cases, and highlight privacy-preserving methods and vertical
applications. This survey provides the first systematic foundation for LLM-SLM
collaboration, bridging system and algorithm co-design to enable efficient,
scalable, and trustworthy edge-cloud intelligence.

</details>


### [54] [Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems](https://arxiv.org/abs/2507.16781)
*Imran Latif,Muhammad Ali Shafique,Hayat Ullah,Alex C. Newkirk,Xi Yu,Arslan Munir*

Main category: cs.DC

TL;DR: 本文比较了液冷和气冷系统在大语言模型和视觉语言模型工作负载下的性能表现，结果显示液冷系统在温度稳定性、性能和能源效率方面均优于气冷系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载（尤其是大语言模型和视觉语言模型）的快速增长，数据中心的电力和冷却需求急剧增加，亟需高效的冷却方案以应对能源和可持续发展挑战。

Method: 研究使用两台配备8个NVIDIA H100 GPU的HGX节点，分别采用液冷和气冷方案，通过GPU Burn、Weights and Biases和IPMItool工具收集热、电力和计算数据。

Result: 液冷系统在负载下GPU温度稳定在41-50摄氏度，而气冷系统温度波动为54-72摄氏度。液冷系统的性能更高（54 TFLOPs/GPU vs. 46 TFLOPs/GPU），且能效更高。

Conclusion: 液冷技术为超大规模数据中心提供了显著的能源和可持续发展优势，是未来发展的有效路径。

Abstract: The unprecedented growth in artificial intelligence (AI) workloads, recently
dominated by large language models (LLMs) and vision-language models (VLMs),
has intensified power and cooling demands in data centers. This study
benchmarks LLMs and VLMs on two HGX nodes, each with 8x NVIDIA H100 graphics
processing units (GPUs), using liquid and air cooling. Leveraging GPU Burn,
Weights and Biases, and IPMItool, we collect detailed thermal, power, and
computation data. Results show that the liquid-cooled systems maintain GPU
temperatures between 41-50 degrees Celsius, while the air-cooled counterparts
fluctuate between 54-72 degrees Celsius under load. This thermal stability of
liquid-cooled systems yields 17 percent higher performance (54 TFLOPs per GPU
vs. 46 TFLOPs per GPU), improved performance per watt, reduced energy overhead,
and greater system efficiency than the air-cooled counterparts. These findings
underscore the energy and sustainability benefits of liquid cooling, offering a
compelling path forward for hyperscale data centers s

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [55] [A Sparsity-Aware Autonomous Path Planning Accelerator with HW/SW Co-Design and Multi-Level Dataflow Optimization](https://arxiv.org/abs/2507.16177)
*Yifan Zhang,Xiaoyu Niu,Hongzheng Tian,Yanjun Zhang,Bo Yu,Shaoshan Liu,Sitao Huang*

Main category: cs.AR

TL;DR: 论文提出了一种基于FPGA的端到端加速框架，用于优化路径规划中的二次规划问题，通过硬件友好的ADMM和PCG方法显著提升了计算效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 资源受限的自动驾驶硬件需要高效的路径规划算法，现有方法计算复杂度高，难以满足实时性和能耗要求。

Method: 采用ADMM求解QP问题，PCG方法处理线性系统，优化稀疏矩阵存储和计算单元，结合多级数据流优化策略。

Result: 在AMD ZCU102平台上实现了显著的性能提升，包括1.48倍于现有FPGA设计的加速比和更高的能源效率。

Conclusion: 提出的框架在延迟和能效方面达到领先水平，为自动驾驶路径规划提供了高效解决方案。

Abstract: Path planning is critical for autonomous driving, generating smooth,
collision-free, feasible paths based on perception and localization inputs.
However, its computationally intensive nature poses significant challenges for
resource-constrained autonomous driving hardware. This paper presents an
end-to-end FPGA-based acceleration framework targeting the quadratic
programming (QP), core of optimization-based path planning. We employ a
hardware-friendly alternating direction method of multipliers (ADMM) for QP
solving and a parallelizable preconditioned conjugate gradient (PCG) method for
linear systems. By analyzing sparse matrix patterns, we propose customized
storage schemes and efficient sparse matrix multiplication units, significantly
reducing resource usage and accelerating matrix operations. Our multi-level
dataflow optimization strategy incorporates intra-operator parallelization and
pipelining, inter-operator fine-grained pipelining, and CPU-FPGA system-level
task mapping. Implemented on the AMD ZCU102 platform, our framework achieves
state-of-the-art latency and energy efficiency, including 1.48x faster
performance than the best FPGA-based design, 2.89x over an Intel i7-11800H CPU,
5.62x over an ARM Cortex-A57 embedded CPU, and 1.56x over a state-of-the-art
GPU solution, along with a 2.05x throughput improvement over existing
FPGA-based designs.

</details>


### [56] [Hourglass Sorting: A novel parallel sorting algorithm and its implementation](https://arxiv.org/abs/2507.16326)
*Daniel Bascones,Borja Morcillo*

Main category: cs.AR

TL;DR: 提出了一种新颖的并行排序器，针对输入并行、输出串行的特定场景，在FPGA上实现并验证，表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统并行排序技术在实现成本和数据移动方面存在瓶颈，限制了其应用。本文旨在解决这些问题。

Method: 设计了一种适用于输入并行、输出串行的排序器，并在FPGA上实现，应用于量子LDPC解码器。

Result: 实现了第一个元素输出的延迟为log(n)，总排序时间为n+log(n)，且时钟速度和资源占用表现优异。

Conclusion: 该方法在特定场景下优于其他并行排序技术，适用于高数据量应用。

Abstract: Sorting is one of the fundamental problems in computer science. Playing a
role in many processes, it has a lower complexity bound imposed by
$\mathcal{O}(n\log{n})$ when executing on a sequential machine. This limit can
be brought down to sub-linear times thanks to parallelization techniques that
increase the number of comparisons done in parallel. This, however, increases
the cost of implementation, which limits the application of such techniques.
Moreover, as the size of the arrays increases, a bottleneck arises in moving
the vast quantities of data required at the input, and generated at the output
of such sorter. This might impose time requirements much stricter than those of
the sorting itself. In this paper, a novel parallel sorter is proposed for the
specific case where the input is parallel, but the output is serial. The design
is then implemented and verified on an FPGA within the context of a quantum
LDPC decoder. A latency of $\log{n}$ is achieved for the output of the first
element, after which the rest stream out for a total sorting time of
$n+\log{n}$. Contrary to other parallel sorting methods, clock speed does not
degrade with $n$, and resources scale linearly with input size.

</details>


### [57] [ApproxGNN: A Pretrained GNN for Parameter Prediction in Design Space Exploration for Approximate Computing](https://arxiv.org/abs/2507.16379)
*Ondrej Vlcek,Vojtech Mrazek*

Main category: cs.AR

TL;DR: 该论文提出了ApproxGNN，一种基于预训练图神经网络的模型，用于预测近似加速器的质量和硬件成本，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 近似计算在能效方面具有潜力，但优化设计空间探索（DSE）需要高效且准确的预测方法。传统机器学习方法需重新训练，计算成本高。

Method: 提出ApproxGNN，使用图神经网络和组件特征嵌入替代传统误差度量，支持迁移学习和高效预测。

Result: 在图像卷积滤波器实验中，预测精度提升50%，整体精度比传统方法高30-54%。

Conclusion: ApproxGNN能高效预测近似加速器性能，适用于DSE，并为新电路提供更高的可迁移性。

Abstract: Approximate computing offers promising energy efficiency benefits for
error-tolerant applications, but discovering optimal approximations requires
extensive design space exploration (DSE). Predicting the accuracy of circuits
composed of approximate components without performing complete synthesis
remains a challenging problem. Current machine learning approaches used to
automate this task require retraining for each new circuit configuration,
making them computationally expensive and time-consuming. This paper presents
ApproxGNN, a construction methodology for a pre-trained graph neural network
model predicting QoR and HW cost of approximate accelerators employing
approximate adders from a library. This approach is applicable in DSE for
assignment of approximate components to operations in accelerator. Our approach
introduces novel component feature extraction based on learned embeddings
rather than traditional error metrics, enabling improved transferability to
unseen circuits. ApproxGNN models can be trained with a small number of
approximate components, supports transfer to multiple prediction tasks,
utilizes precomputed embeddings for efficiency, and significantly improves
accuracy of the prediction of approximation error. On a set of image
convolutional filters, our experimental results demonstrate that the proposed
embeddings improve prediction accuracy (mean square error) by 50% compared to
conventional methods. Furthermore, the overall prediction accuracy is 30%
better than statistical machine learning approaches without fine-tuning and 54%
better with fast finetuning.

</details>


### [58] [Ironman: Accelerating Oblivious Transfer Extension for Privacy-Preserving AI with Near-Memory Processing](https://arxiv.org/abs/2507.16391)
*Chenqi Lin,Kang Yang,Tianshi Xu,Ling Liang,Yufei Wang,Zhaohui Chen,Runsheng Wang,Mingyu Gao,Meng Li*

Main category: cs.AR

TL;DR: 论文提出了一种名为Ironman的新型OT加速器，显著提升了隐私保护机器学习（PPML）框架的效率。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习的广泛应用，用户数据隐私问题日益突出。基于密码学原语的隐私保护机器学习（PPML）通过在加密数据上直接计算模型来提供隐私保证，但其框架因依赖大量OT计算而成为性能瓶颈。

Method: 论文提出了一种硬件友好的SPCOT算法和定制化加速器，用于提升SPCOT的计算吞吐量。同时，针对LPN的内存带宽限制，采用了近内存处理（NMP）架构，配备内存侧缓存和索引排序技术以提高有效内存带宽。

Result: 实验表明，Ironman在不同NMP配置下的OT吞吐量比全线程CPU实现提升了39.2-237.4倍。对于不同的PPML框架，Ironman在CNN和Transformer模型上的端到端延迟降低了2.1-3.4倍。

Conclusion: Ironman通过优化OT计算和内存访问模式，显著提升了PPML框架的性能，为解决隐私保护机器学习中的计算瓶颈提供了有效解决方案。

Abstract: With the wide application of machine learning (ML), privacy concerns arise
with user data as they may contain sensitive information. Privacy-preserving ML
(PPML) based on cryptographic primitives has emerged as a promising solution in
which an ML model is directly computed on the encrypted data to provide a
formal privacy guarantee. However, PPML frameworks heavily rely on the
oblivious transfer (OT) primitive to compute nonlinear functions. OT mainly
involves the computation of single-point correlated OT (SPCOT) and learning
parity with noise (LPN) operations. As OT is still computed extensively on
general-purpose CPUs, it becomes the latency bottleneck of modern PPML
frameworks.
  In this paper, we propose a novel OT accelerator, dubbed Ironman, to
significantly increase the efficiency of OT and the overall PPML framework. We
observe that SPCOT is computation-bounded, and thus propose a hardware-friendly
SPCOT algorithm with a customized accelerator to improve SPCOT computation
throughput. In contrast, LPN is memory-bandwidth-bounded due to irregular
memory access patterns. Hence, we further leverage the near-memory processing
(NMP) architecture equipped with memory-side cache and index sorting to improve
effective memory bandwidth. With extensive experiments, we demonstrate Ironman
achieves a 39.2-237.4 times improvement in OT throughput across different NMP
configurations compared to the full-thread CPU implementation. For different
PPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end
latency for both CNN and Transformer models.

</details>


### [59] [Augmenting Von Neumann's Architecture for an Intelligent Future](https://arxiv.org/abs/2507.16628)
*Rajpreet Singh,Vidhi Kothari*

Main category: cs.AR

TL;DR: 提出了一种新型计算机架构，扩展冯·诺依曼模型，引入专用推理单元（RU），支持原生通用人工智能功能。


<details>
  <summary>Details</summary>
Motivation: 现有架构难以高效支持通用人工智能任务，需硬件级创新以实现推理与学习的深度融合。

Method: 通过专用RU实现符号推理、多智能体协调及混合符号-神经计算，设计推理专用指令集、并行符号处理流水线及统一内存层次。

Result: 该架构支持自主智能体的目标规划、动态知识操作及内省推理，为通用智能机器提供计算基础。

Conclusion: 该架构将推理、学习与适应内化为执行特性，有望推动通用智能机器的发展。

Abstract: This work presents a novel computer architecture that extends the Von Neumann
model with a dedicated Reasoning Unit (RU) to enable native artificial general
intelligence capabilities. The RU functions as a specialized co-processor that
executes symbolic inference, multi-agent coordination, and hybrid
symbolic-neural computation as fundamental architectural primitives. This
hardware-embedded approach allows autonomous agents to perform goal-directed
planning, dynamic knowledge manipulation, and introspective reasoning directly
within the computational substrate at system scale. The architecture
incorporates a reasoning-specific instruction set architecture, parallel
symbolic processing pipelines, agent-aware kernel abstractions, and a unified
memory hierarchy that seamlessly integrates cognitive and numerical workloads.
Through systematic co-design across hardware, operating system, and agent
runtime layers, this architecture establishes a computational foundation where
reasoning, learning, and adaptation emerge as intrinsic execution properties
rather than software abstractions, potentially enabling the development of
general-purpose intelligent machines.

</details>


### [60] [MTU: The Multifunction Tree Unit in zkSpeed for Accelerating HyperPlonk](https://arxiv.org/abs/2507.16793)
*Jianqiao Mo,Alhad Daftardar,Joey Ah-kiow,Kaiyue Guo,Benedikt Bünz,Siddharth Garg,Brandon Reagen*

Main category: cs.AR

TL;DR: 该论文系统评估了基于树的零知识证明（ZKP）工作负载在不同遍历策略下的性能，提出了硬件友好的混合遍历方法，显著提升了并行性和可扩展性，并减少了内存流量。


<details>
  <summary>Details</summary>
Motivation: 针对零知识证明中基于二叉树的核函数，目前缺乏对如何最优利用其树结构以提升硬件效率的深入研究。

Method: 通过在不同硬件（多线程CPU和MTU加速器）上评估不同遍历策略，提出并验证了一种混合遍历方法。

Result: MTU加速器实现了相对于CPU最高1478倍的加速，混合遍历方法相比传统方法性能提升高达3倍。

Conclusion: 研究结果为设计高效的ZKP硬件加速器提供了实用指导。

Abstract: Zero-Knowledge Proofs (ZKPs) are critical for privacy preservation and
verifiable computation. Many ZKPs rely on kernels such as the SumCheck protocol
and Merkle Tree commitments, which enable their security properties. These
kernels exhibit balanced binary tree computational patterns, which enable
efficient hardware acceleration. Prior work has investigated accelerating these
kernels as part of an overarching ZKP protocol; however, a focused study of how
to best exploit the underlying tree pattern for hardware efficiency remains
limited. We conduct a systematic evaluation of these tree-based workloads under
different traversal strategies, analyzing performance on multi-threaded CPUs
and a hardware accelerator, the Multifunction Tree Unit (MTU). We introduce a
hardware-friendly Hybrid Traversal for binary tree that improves parallelism
and scalability while significantly reducing memory traffic on hardware. Our
results show that MTU achieves up to 1478$\times$ speedup over CPU at DDR-level
bandwidth and that our hybrid traversal outperforms as standalone approach by
up to 3$\times$. These findings offer practical guidance for designing
efficient hardware accelerators for ZKP workloads with binary tree structures.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [61] [Improved Wake-Up Time For Euclidean Freeze-Tag Problem](https://arxiv.org/abs/2507.16269)
*Sharareh Alipour,Arash Ahadi,Kajal Baghestani*

Main category: cs.CG

TL;DR: 本文改进了Freeze-Tag问题中唤醒比率的上界，在二维欧几里得空间中将上界从4.62降至4.31，并在三维空间中提出新策略将唤醒比率分别降至12和12.76。


<details>
  <summary>Details</summary>
Motivation: 研究Freeze-Tag问题中如何快速激活所有机器人，优化唤醒比率以缩短总时间。

Method: 在二维和三维空间中，使用几何方法优化机器人激活策略，减少最大激活时间。

Result: 在二维空间中唤醒比率降至4.31，三维空间中分别为12（ℓ₁）和12.76（ℓ₂）。

Conclusion: 提出的策略显著提升了唤醒效率，缩小了与理论下界的差距。

Abstract: The Freeze-Tag Problem (FTP) involves activating a set of initially asleep
robots as quickly as possible, starting from a single awake robot. Once
activated, a robot can assist in waking up other robots. Each active robot
moves at unit speed. The objective is to minimize the makespan, i.e., the time
required to activate the last robot. A key performance measure is the wake-up
ratio, defined as the maximum time needed to activate any number of robots in
any primary positions. This work focuses on the geometric (Euclidean) version
of FTP in $\mathbb{R}^d$ under the $\ell_p$ norm, where the initial distance
between each asleep robot and the single active robot is at most 1. For
$(\mathbb{R}^2, \ell_2)$, we improve the previous upper bound of 4.62 ([7],
CCCG 2024) to 4.31. Note that it is known that 3.82 is a lower bound for the
wake-up ratio. In $\mathbb{R}^3$, we propose a new strategy that achieves a
wake-up ratio of 12 for $(\mathbb{R}^3, \ell_1)$ and 12.76 for $(\mathbb{R}^3,
\ell_2)$, improving upon the previous bounds of 13 and $13\sqrt{3}$,
respectively, reported in [2].

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [62] [Active RISs: Modeling and Optimization](https://arxiv.org/abs/2507.16499)
*Recep Akif Tasci,Panagiotis Gavriilidis,Ertugrul Basar,George C. Alexandropoulos*

Main category: cs.IT

TL;DR: 论文研究了主动RIS的建模、性能分析和优化，提出了两种硬件设计并通过仿真验证了其突破双路径损耗限制的优势。


<details>
  <summary>Details</summary>
Motivation: 传统RIS受限于双路径损耗效应，主动RIS能够放大信号以克服这一问题，推动下一代无线网络的发展。

Method: 研究了两种硬件设计：基于PA的双RIS结构和隧道二极管单元级反射放大结构，并通过数学建模和优化框架进行分析。

Result: 仿真表明主动RIS能有效克服双路径损耗，实现更高的能量效率，但过多主动元件可能降低性能。

Conclusion: 主动RIS在特定条件下能显著提升性能，但需权衡功率预算和元件数量以达到最优效果。

Abstract: Reconfigurable Intelligent Surfaces (RIS)-empowered communication has emerged
as a transformative technology for next generation wireless networks, enabling
the programmable shaping of the propagation environment. However, conventional
RISs are fundamentally limited by the double path loss effect, which severely
attenuates the reflected signals. To overcome this, active RIS architectures,
capable of amplifying impinging signals, have been proposed. This chapter
investigates the modeling, performance analysis, and optimization of active
RISs, focusing on two hardware designs: a dual-RIS structure with a single
Power Amplifier (PA), and a reflection amplification structure at the unit cell
level using tunnel diodes. For the PA-based design, a comprehensive
mathematical model is developed, and closed-form expressions for the received
signal-to-noise ratio, bit error probability, and Energy Efficiency (EE) are
derived. An optimization framework for configuring the phase shifts and
amplifier gain is proposed to maximize system capacity under power constraints.
Regarding the second design, the integration of a tunnel diode into the unit
cell is carefully studied by analyzing its I-V characteristic, enabling the
derivation of the negative resistance range and the power consumption model.
Furthermore, the intrinsic phase-amplitude coupling of the reflection
coefficient is characterized through compact linear algebra formulations,
enabling practical optimization of active RISs. Extensive numerical simulations
validate the theoretical analyses, demonstrating that active RISs can
effectively overcome the double path loss limitation and achieve favorable EE
trade-offs compared to passive RISs. Finally, the trade-off between the
available power budget and the number of active elements is examined, revealing
that a higher number of active elements does not always lead to optimal
performance.

</details>


### [63] [Multi-RIS-Empowered Communication Systems: Capacity Analysis and Optimization](https://arxiv.org/abs/2507.16767)
*Aris L. Moustakas,George C. Alexandropoulos*

Main category: cs.IT

TL;DR: 使用统计物理学方法，推导出多天线收发对在多个可重构智能表面（RISs）存在下互信息的均值和方差的渐近闭式表达式。研究表明，即使对于中等规模的天线阵列和超表面，所提出的高斯近似也非常准确。结果在快衰落条件下尤其有用，且显示当RIS附近信道相关时，统计RIS优化能显著提升通信性能。


<details>
  <summary>Details</summary>
Motivation: 研究在快衰落条件下，如何利用RISs优化通信链路性能，特别是在信道相关时，无需复杂数值优化的条件下实现高效通信。

Method: 采用统计物理学方法，推导互信息的均值和方差的渐近闭式表达式，并通过数值验证其准确性。分析了RIS信号相关矩阵的新渐近性质。

Result: 研究表明，即使对于中等规模系统，高斯近似依然准确。RIS优化在信道相关时性能提升显著，且无需严格几何光学条件即可实现稳健通信。

Conclusion: 提出的渐近分析为RIS优化提供了理论支持，尤其在快衰落和信道相关条件下，能够显著提升通信链路性能，且操作简便。

Abstract: In this chapter, using statistical physics methods, asymptotic closed-form
expressions for the mean and variance of the mutual information for a
multi-antenna transmitter-receiver pair in the presence of multiple
Reconfigurable Intelligent Surfaces (RISs) are presented. While nominally valid
in the large-system limit, it is shown that the derived Gaussian approximation
for the mutual information can be quite accurate, even for modest-sized antenna
arrays and metasurfaces. The above results are particularly useful when
fast-fading conditions are present, which renders channel estimation
challenging. The derived analysis indicates that, when the channel close to an
RIS is correlated, for instance due to small angle spread which is reasonable
for wireless systems with increasing carrier frequencies, the communication
link benefits significantly from statistical RIS optimization, resulting in
gains that are surprisingly higher than the nearly uncorrelated case. More
importantly, the presented novel asymptotic properties of the correlation
matrices of the impinging and outgoing signals at the RISs can be deployed to
optimize the metasurfaces without brute-force numerical optimization. The
numerical investigation demonstrates that, when the desired reflection from any
of the RISs departs significantly from geometrical optics, the metasurfaces can
be optimized to provide robust communication links, without significant need
for their optimal placement.

</details>


### [64] [Byzantine-Resilient Distributed Computation via Task Replication and Local Computations](https://arxiv.org/abs/2507.16014)
*Aayush Rajesh,Nikhil Karamchandani,Vinod M. Prabhakaran*

Main category: cs.IT

TL;DR: 提出的分布式计算协议在拜占庭工人环境下，通过任务复制和少量本地计算，优化了任务分配和通信效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何在拜占庭工人的分布式环境中高效完成任务，避免昂贵的本地计算。

Method: 设计了一种协议，通过任务复制和最优本地计算量完成任务分配，并提出改进版本以提高通信效率。

Result: 协议在不增加本地计算量的情况下，成功完成任务并优化了通信效率。

Conclusion: 协议在拜占庭环境中高效完成任务，并在通信效率上有所改进。

Abstract: We study a distributed computation problem in the presence of Byzantine
workers where a central node wishes to solve a task that is divided into
independent sub-tasks, each of which needs to be solved correctly. The
distributed computation is achieved by allocating the sub-task computation
across workers with replication, as well as solving a small number of sub-tasks
locally, which we wish to minimize due to it being expensive. For a general
balanced job allocation, we propose a protocol that successfully solves for all
sub-tasks using an optimal number of local computations under no communication
constraints. Closed-form performance results are presented for cyclic
allocations. Furthermore, we propose a modification to this protocol to improve
communication efficiency without compromising on the amount of local
computation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [65] [Entanglement-Efficient Compilation of Quantum Circuits over Large-Scale Quantum Networks](https://arxiv.org/abs/2507.16036)
*Felix Burt,Kuan-Cheng Chen,Kin K. Leung*

Main category: quant-ph

TL;DR: 量子计算机面临扩展难题，分布式量子计算系统通过连接小型量子处理单元解决扩展问题。然而，大规模连接会导致网络层面的限制，增加纠缠共享的复杂度。本文提出一种改进现有分区方案的方法，通过优化网络拓扑和现有链路，降低纠缠成本，并展示了扩展技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于量子计算机的扩展难题，分布式量子计算系统成为必要。但大规模连接会导致网络层面的限制，特别是纠缠共享的复杂度随网络路径长度增加而增加，因此需要优化量子电路分区方案。

Method: 提出了一种改进现有分区方案的方法，考虑了网络拓扑和现有链路的影响，通过网络和问题粗化技术，实现了对大规模量子网络的扩展。

Result: 实验表明，该方法在大多数情况下能够降低纠缠成本，并且在运行时间上显著优于现有方法。

Conclusion: 论文通过改进分区方案和粗化技术，实现了对大规模量子网络的有效扩展，降低了纠缠成本，为分布式量子计算提供了实用解决方案。

Abstract: Quantum computers face inherent scaling challenges, a fact that necessitates
investigation of distributed quantum computing systems, whereby scaling is
achieved through interconnection of smaller quantum processing units. However,
connecting large numbers of QPUs will eventually result in connectivity
constraints at the network level, where the difficulty of entanglement sharing
increases with network path lengths. This increases the complexity of the
quantum circuit partitioning problem, since the cost of generating entanglement
between end nodes varies with network topologies and existing links. We address
this challenge using a simple modification to existing partitioning schemes
designed for all-to-all connected networks, that efficiently accounts for both
of these factors. We investigate the performance in terms of entanglement
requirements and optimisation time of various quantum circuits over different
network topologies, achieving lower entanglement costs in the majority of cases
than state-of-the-art methods. We provide techniques for scaling to large-scale
quantum networks employing both network and problem coarsening. We show that
coarsened methods can achieve improved solution quality in most cases with
significantly lower run-times than direct partitioning methods.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [66] [Designing for Difference: How Human Characteristics Shape Perceptions of Collaborative Robots](https://arxiv.org/abs/2507.16480)
*Sabrina Livanec,Laura Londoño,Michael Gorki,Adrian Röfer,Abhinav Valada,Andrea Kiesel*

Main category: cs.RO

TL;DR: 研究探讨了辅助机器人在社交协作中的设计与评估，重点关注多样人群需求与机器人行为的交互，发现反社交行为评分最低，而反思方法（如CAM）能提供更细致反馈。


<details>
  <summary>Details</summary>
Motivation: 填补辅助机器人与多样化人群（如残疾或老年人）互动设计的研究空白，关注行为和需求对评估的影响。

Method: 在线研究，112名参与者通过观看28种人机协作视频进行评估，实验组进行认知情感映射（CAM）练习。

Result: CAM未显著影响总体评分，但对某些行为与需求的组合评估更明显；反社交行为评分最低，老年人协作评价更敏感。

Conclusion: 人机协作类型和人类特征影响机器人可接受性，需注重亲社会设计，反思方法如CAM有助于开发用户为中心的机器人系统。

Abstract: The development of assistive robots for social collaboration raises critical
questions about responsible and inclusive design, especially when interacting
with individuals from protected groups such as those with disabilities or
advanced age. Currently, research is scarce on how participants assess varying
robot behaviors in combination with diverse human needs, likely since
participants have limited real-world experience with advanced domestic robots.
In the current study, we aim to address this gap while using methods that
enable participants to assess robot behavior, as well as methods that support
meaningful reflection despite limited experience. In an online study, 112
participants (from both experimental and control groups) evaluated 7 videos
from a total of 28 variations of human-robot collaboration types. The
experimental group first completed a cognitive-affective mapping (CAM) exercise
on human-robot collaboration before providing their ratings. Although CAM
reflection did not significantly affect overall ratings, it led to more
pronounced assessments for certain combinations of robot behavior and human
condition. Most importantly, the type of human-robot collaboration influences
the assessment. Antisocial robot behavior was consistently rated as the lowest,
while collaboration with aged individuals elicited more sensitive evaluations.
Scenarios involving object handovers were viewed more positively than those
without them. These findings suggest that both human characteristics and
interaction paradigms influence the perceived acceptability of collaborative
robots, underscoring the importance of prosocial design. They also highlight
the potential of reflective methods, such as CAM, to elicit nuanced feedback,
supporting the development of user-centered and socially responsible robotic
systems tailored to diverse populations.

</details>


### [67] [AI or Human? Understanding Perceptions of Embodied Robots with LLMs](https://arxiv.org/abs/2507.16398)
*Lavinia Hriscu,Alberto Sanfeliu,Anais Garrell*

Main category: cs.RO

TL;DR: 研究通过图灵测试探讨人类对具身机器人智能的感知，发现参与者无法可靠区分AI与人类操控的机器人，为未来交互机器人设计提供见解。


<details>
  <summary>Details</summary>
Motivation: 探索图灵测试在人与机器人交互中的适用性，以及如何评估具身机器人智能。

Method: 34名参与者通过信息检索和包裹交接任务区分AI与人类操控的机器人。

Result: 参与者无法可靠区分两者，响应分析揭示了影响感知的关键因素。

Conclusion: 结果对交互机器人设计和AI智能评估有重要意义。

Abstract: The pursuit of artificial intelligence has long been associated to the the
challenge of effectively measuring intelligence. Even if the Turing Test was
introduced as a means of assessing a system intelligence, its relevance and
application within the field of human-robot interaction remain largely
underexplored. This study investigates the perception of intelligence in
embodied robots by performing a Turing Test within a robotic platform. A total
of 34 participants were tasked with distinguishing between AI- and
human-operated robots while engaging in two interactive tasks: an information
retrieval and a package handover. These tasks assessed the robot perception and
navigation abilities under both static and dynamic conditions. Results indicate
that participants were unable to reliably differentiate between AI- and
human-controlled robots beyond chance levels. Furthermore, analysis of
participant responses reveals key factors influencing the perception of
artificial versus human intelligence in embodied robotic systems. These
findings provide insights into the design of future interactive robots and
contribute to the ongoing discourse on intelligence assessment in AI-driven
systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [68] [A new XML conversion process for mensural music encoding : CMME\_to\_MEI (via Verovio)](https://arxiv.org/abs/2507.15991)
*David Fiala,Laurent Pugin,Marnix van Berchum,Martha Thomae,Kévin Roger*

Main category: cs.SD

TL;DR: Ricercar Lab公开了约3500个15世纪音乐的XML文件，开发了将CMME格式转换为MEI标准的工具，并在Verovio库中实现，使文件能在现代软件中使用。


<details>
  <summary>Details</summary>
Motivation: 为了使历史音乐档案更易访问和现代化，同时解决CMME格式过时的问题。

Method: 组织专家研讨会，开发CMME到MEI的转换工具，并集成到开源库Verovio中。

Result: 成功实现CMME文件向MEI的转换，支持在现代音乐软件中使用，并延长了档案的生命周期。

Conclusion: 转换工具为历史音乐档案的现代化提供了新途径，同时保留了CMME编辑器的功能。

Abstract: The Ricercar Lab - the musicological research team at the Center for advanced
Studies in the Renaissance at the University of Tours - has decided to make
available in open access, thanks to the support of the French digital
infrastructure Biblissima, a large corpus of about 3500 XML files of 15th-c.
music. This corpus was produced by the German musicologist Clemens Goldberg who
encoded since 2010 onwards the musical content of 34 major 15th-c. music
manuscripts and other complementary files, in order to offer on his
foundation's website PDF files of complete collections of works by Du Fay,
Binchois, Okeghem, Busnoys and most of their major contemporaries, focusing on
their secular output. This corpus was encoded in an XML format named CMME
(Computerized Mensural Music Editing), specifically conceived for mensural
music by Theodor Dumitrescu in the 2000s, together with editorial and
publication tools which have not been updated since then. This article focuses
on the development of a set of conversion tools for these CMME files to meet
more up-to-date standards of music encoding, namely MEI. A workshop was
organised in September 2024 at the Campus Condorcet in Paris, gathering experts
with a wide range of knowledge on mensural music notation, XML formats and
programming. A converter was developped directly in the open-source rendering
library Verovio, allowing the conversion from CMME to MEI mensural. A
conversion to MEI CMN was implemented afterwards, enabling to load these files
in common engraving softwares such as MuseScore with minimal loss of
information. With the availability of a direct import of CMME-XML into Verovio,
the corpus of existing CMME files gets a new life. Furthermore, since the
stand-alone CMME editor still works fine and no alternative is available yet
for native MEI, the converter offers a new pipeline for encoding and editing
mensural music.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [69] [Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness](https://arxiv.org/abs/2507.16515)
*Siqi Liu,Guangrong Dai,Dechao Li*

Main category: cs.CL

TL;DR: 句级质量估计（QE）能显著减少英语-中文机器翻译后编辑（MTPE）的时间，但对MT质量和翻译专业水平的交互影响不显著。


<details>
  <summary>Details</summary>
Motivation: 研究QE在MTPE中的实用性，特别是其对后编辑速度和学生译者感知的影响。

Method: 初步研究，探讨QE与MT质量及翻译专业水平的交互作用。

Result: QE显著提高MTPE效率，但交互作用不显著；不准确的QE可能阻碍后编辑。

Conclusion: QE在MTPE中具有多种功能，但需准确以避免负面影响。

Abstract: This preliminary study investigates the usefulness of sentence-level Quality
Estimation (QE) in English-Chinese Machine Translation Post-Editing (MTPE),
focusing on its impact on post-editing speed and student translators'
perceptions. It also explores the interaction effects between QE and MT
quality, as well as between QE and translation expertise. The findings reveal
that QE significantly reduces post-editing time. The examined interaction
effects were not significant, suggesting that QE consistently improves MTPE
efficiency across medium- and high-quality MT outputs and among student
translators with varying levels of expertise. In addition to indicating
potentially problematic segments, QE serves multiple functions in MTPE, such as
validating translators' evaluations of MT quality and enabling them to
double-check translation outputs. However, interview data suggest that
inaccurate QE may hinder post-editing processes. This research provides new
insights into the strengths and limitations of QE, facilitating its more
effective integration into MTPE workflows to enhance translators' productivity.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [70] [WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections](https://arxiv.org/abs/2507.16298)
*Gautam Kishore Shahi,Scot A. Hale*

Main category: cs.SI

TL;DR: 研究分析了2021年印度大选期间的WhatsApp辟谣热线数据，发现不同语言的用户提交的虚假信息具有相似性，且每个辟谣组织拥有独特受众。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解辟谣热线在不同语言和辟谣组织中的作用，为选举期间的辟谣工作提供实际建议。

Method: 采用混合方法分析了580条独特用户的虚假信息，包括高频词分析和神经句嵌入聚类，并测量了辟谣的平均时间。

Result: 发现不同语言的虚假信息相似，用户通常只向一个辟谣组织提交信息，辟谣时间约为几天。

Conclusion: 研究为选举期间使用辟谣热线提供了实用建议，并强调了用户信息的伦理考量。

Abstract: WhatsApp tiplines, first launched in 2019 to combat misinformation, enable
users to interact with fact-checkers to verify misleading content. This study
analyzes 580 unique claims (tips) from 451 users, covering both high-resource
languages (English, Hindi) and a low-resource language (Telugu) during the 2021
Indian assembly elections using a mixed-method approach. We categorize the
claims into three categories, election, COVID-19, and others, and observe
variations across languages. We compare content similarity through frequent
word analysis and clustering of neural sentence embeddings. We also investigate
user overlap across languages and fact-checking organizations. We measure the
average time required to debunk claims and inform tipline users. Results reveal
similarities in claims across languages, with some users submitting tips in
multiple languages to the same fact-checkers. Fact-checkers generally require a
couple of days to debunk a new claim and share the results with users. Notably,
no user submits claims to multiple fact-checking organizations, indicating that
each organization maintains a unique audience. We provide practical
recommendations for using tiplines during elections with ethical consideration
of users' information.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [71] [Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training](https://arxiv.org/abs/2507.16274)
*Zixiao Huang,Junhao Hu,Hao Lin,Chunyang Zhu,Yueran Tang,Quanlu Zhang,Zhen Guo,Zhenhua Li,Shengen Yan,Zhenhua Zhu,Guohao Dai,Yu Wang*

Main category: cs.LG

TL;DR: STWeaver是一种针对深度学习框架的GPU内存分配器，通过结合离线规划和在线分配，有效减少内存碎片，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速扩展增加了GPU内存压力，现有的内存分配器因缺乏对张量生命周期的了解而产生大量碎片，导致内存浪费和OOM错误。

Method: STWeaver结合离线规划和在线分配，利用内存分配行为的时空规律生成高效的分配计划，适用于复杂动态模型如MoE。

Result: STWeaver平均减少79.2%的碎片率（最高100%），性能提升达32.5%，且开销可忽略。

Conclusion: STWeaver显着改善内存利用率和训练性能，支持高效高吞吐的配置。

Abstract: The rapid scaling of large language models (LLMs) has significantly increased
GPU memory pressure, which is further aggravated by training optimization
techniques such as virtual pipeline and recomputation that disrupt tensor
lifespans and introduce considerable memory fragmentation. Default GPU memory
allocators of popular deep learning frameworks like PyTorch use online
strategies without knowledge of tensor lifespans, which can waste up to 43\% of
memory and cause out-of-memory errors, rendering optimization techniques
ineffective or even unusable.
  To address this, we introduce STWeaver, a GPU memory allocator for deep
learning frameworks that reduces fragmentation by exploiting the spatial and
temporal regularity in memory allocation behaviors of training workloads.
STWeaver introduces a novel paradigm that combines offline planning with online
allocation. The offline planning leverages spatio-temporal regularities to
generate a near-optimal allocation plan, while the online allocation handles
complex and dynamic models such as Mixture-of-Experts (MoE). Built as a
pluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by
79.2\% (up to 100\%) across both dense and sparse models, with negligible
overhead. This enables more efficient, high-throughput training configurations
and improves performance by up to 32.5\%.

</details>


### [72] [Bipartite Patient-Modality Graph Learning with Event-Conditional Modelling of Censoring for Cancer Survival Prediction](https://arxiv.org/abs/2507.16363)
*Hailin Yue,Hulin Kuang,Jin Liu,Junjian Li,Lanlan Wang,Mengshen He,Jianxin Wang*

Main category: cs.LG

TL;DR: 提出了一种名为CenSurv的癌症生存预测方法，通过双图学习和事件条件建模，有效利用删失数据并提升模态缺失场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有癌症生存预测研究未能充分利用删失数据，且在模态缺失场景下性能下降。

Method: 采用双图结构建模多模态数据，设计完整-缺失对齐策略提取模态无关特征，引入动态动量积累置信度的ECMC模块利用可靠删失数据。

Result: 在5个数据集上C-index提升3.1%，ECMC模块使基线方法C-index平均提升1.3%。

Conclusion: CenSurv通过全面利用数据和模态缺失鲁棒性，显著提升了癌症生存预测性能。

Abstract: Accurately predicting the survival of cancer patients is crucial for
personalized treatment. However, existing studies focus solely on the
relationships between samples with known survival risks, without fully
leveraging the value of censored samples. Furthermore, these studies may suffer
performance degradation in modality-missing scenarios and even struggle during
the inference process. In this study, we propose a bipartite patient-modality
graph learning with event-conditional modelling of censoring for cancer
survival prediction (CenSurv). Specifically, we first use graph structure to
model multimodal data and obtain representation. Then, to alleviate performance
degradation in modality-missing scenarios, we design a bipartite graph to
simulate the patient-modality relationship in various modality-missing
scenarios and leverage a complete-incomplete alignment strategy to explore
modality-agnostic features. Finally, we design a plug-and-play
event-conditional modeling of censoring (ECMC) that selects reliable censored
data using dynamic momentum accumulation confidences, assigns more accurate
survival times to these censored data, and incorporates them as uncensored data
into training. Comprehensive evaluations on 5 publicly cancer datasets showcase
the superiority of CenSurv over the best state-of-the-art by 3.1% in terms of
the mean C-index, while also exhibiting excellent robustness under various
modality-missing scenarios. In addition, using the plug-and-play ECMC module,
the mean C-index of 8 baselines increased by 1.3% across 5 datasets. Code of
CenSurv is available at https://github.com/yuehailin/CenSurv.

</details>


### [73] [Latent Space Alignment for AI-Native MIMO Semantic Communications](https://arxiv.org/abs/2507.16680)
*Mario Edoardo Pandolfo,Simone Fiorellino,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: 论文提出了一种利用MIMO通信解决语义通信中潜在空间不匹配的新方法，通过线性模型和神经网络模型优化语义通信效果。


<details>
  <summary>Details</summary>
Motivation: 解决不同设备间由于语言、逻辑或内部表示差异导致的语义不匹配问题。

Method: 提出基于MIMO的预编码/解码器对，包括线性模型（ADMM优化）和神经网络模型。

Result: 数值结果显示该方法在目标导向语义通信中有效平衡精度、通信负担和复杂性。

Conclusion: 新方法能有效缓解语义不匹配和物理信道损伤，提升语义通信效果。

Abstract: Semantic communications focus on prioritizing the understanding of the
meaning behind transmitted data and ensuring the successful completion of tasks
that motivate the exchange of information. However, when devices rely on
different languages, logic, or internal representations, semantic mismatches
may occur, potentially hindering mutual understanding. This paper introduces a
novel approach to addressing latent space misalignment in semantic
communications, exploiting multiple-input multiple-output (MIMO)
communications. Specifically, our method learns a MIMO precoder/decoder pair
that jointly performs latent space compression and semantic channel
equalization, mitigating both semantic mismatches and physical channel
impairments. We explore two solutions: (i) a linear model, optimized by solving
a biconvex optimization problem via the alternating direction method of
multipliers (ADMM); (ii) a neural network-based model, which learns semantic
MIMO precoder/decoder under transmission power budget and complexity
constraints. Numerical results demonstrate the effectiveness of the proposed
approach in a goal-oriented semantic communication scenario, illustrating the
main trade-offs between accuracy, communication burden, and complexity of the
solutions.

</details>


### [74] [FISHER: A Foundation Model for Multi-Modal Industrial Signal Comprehensive Representation](https://arxiv.org/abs/2507.16696)
*Pingyi Fan,Anbai Jiang,Shuwei Zhang,Zhiqiang Lv,Bing Han,Xinhu Zheng,Wenrui Liang,Junjie Li,Wei-Qiang Zhang,Yanmin Qian,Xie Chen,Cheng Lu,Jia Liu*

Main category: cs.LG

TL;DR: FISHER是一个多模态工业信号基础模型，通过统一建模M5信号，显著提升了工业信号分析的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 工业信号（M5问题）的高度异质性导致传统方法专注于子问题，未能利用模态间的协同效应和强大的规模效应。FISHER旨在通过统一建模解决这一问题。

Method: FISHER将采样率增量视为子带信息的拼接，以STFT子带为建模单位，采用教师-学生自监督学习框架进行预训练。

Result: FISHER在多个健康管理任务中表现优异，性能提升达5.03%，且规模效应更高效。

Conclusion: FISHER为工业信号分析提供了统一且高效的解决方案，并在实际任务中证明了其优越性。

Abstract: With the rapid deployment of SCADA systems, how to effectively analyze
industrial signals and detect abnormal states is an urgent need for the
industry. Due to the significant heterogeneity of these signals, which we
summarize as the M5 problem, previous works only focus on small sub-problems
and employ specialized models, failing to utilize the synergies between
modalities and the powerful scaling law. However, we argue that the M5 signals
can be modeled in a unified manner due to the intrinsic similarity. As a
result, we propose FISHER, a Foundation model for multi-modal Industrial Signal
compreHEnsive Representation. To support arbitrary sampling rates, FISHER
considers the increment of sampling rate as the concatenation of sub-band
information. Specifically, FISHER takes the STFT sub-band as the modeling unit
and adopts a teacher student SSL framework for pre-training. We also develop
the RMIS benchmark, which evaluates the representations of M5 industrial
signals on multiple health management tasks. Compared with top SSL models,
FISHER showcases versatile and outstanding capabilities with a general
performance gain up to 5.03%, along with much more efficient scaling curves. We
also investigate the scaling law on downstream tasks and derive potential
avenues for future works. FISHER is now open-sourced on
https://github.com/jianganbai/FISHER

</details>


### [75] [RealBench: Benchmarking Verilog Generation Models with Real-World IP Designs](https://arxiv.org/abs/2507.16200)
*Pengwei Jin,Di Huang,Chongxiao Li,Shuyao Cheng,Yang Zhao,Xinyao Zheng,Jiaguo Zhu,Shuyi Xing,Bohan Dou,Rui Zhang,Zidong Du,Qi Guo,Xing Hu*

Main category: cs.LG

TL;DR: RealBench是一个针对真实世界IP级Verilog生成的基准测试，解决了现有基准测试设计简单、规范不足和验证环境不严谨的问题，结果显示当前LLMs在Verilog生成任务中表现仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLMs在Verilog生成中的基准测试无法模拟真实世界设计流程，限制了其实际应用。

Method: 提出了RealBench，采用复杂、结构化的开源IP设计，多模态格式设计规范和严格验证环境。

Result: 最佳LLM（o1-preview）在模块级任务中仅13.3%通过率，系统级任务中为0%。

Conclusion: RealBench为未来开发更强的Verilog生成模型提供了评估工具，凸显当前技术的不足。

Abstract: The automatic generation of Verilog code using Large Language Models (LLMs)
has garnered significant interest in hardware design automation. However,
existing benchmarks for evaluating LLMs in Verilog generation fall short in
replicating real-world design workflows due to their designs' simplicity,
inadequate design specifications, and less rigorous verification environments.
To address these limitations, we present RealBench, the first benchmark aiming
at real-world IP-level Verilog generation tasks. RealBench features complex,
structured, real-world open-source IP designs, multi-modal and formatted design
specifications, and rigorous verification environments, including 100% line
coverage testbenches and a formal checker. It supports both module-level and
system-level tasks, enabling comprehensive assessments of LLM capabilities.
Evaluations on various LLMs and agents reveal that even one of the
best-performing LLMs, o1-preview, achieves only a 13.3% pass@1 on module-level
tasks and 0% on system-level tasks, highlighting the need for stronger Verilog
generation models in the future. The benchmark is open-sourced at
https://github.com/IPRC-DIP/RealBench.

</details>


### [76] [Custom Algorithm-based Fault Tolerance for Attention Layers in Transformers](https://arxiv.org/abs/2507.16676)
*Vasileios Titopoulos,Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: Flash-ABFT是一种针对注意力机制加速器的低开销错误检测方法，通过单次检查覆盖三个矩阵乘积和softmax操作。


<details>
  <summary>Details</summary>
Motivation: 传统ABFT技术无法高效处理注意力机制，特别是在softmax标准化过程中。为解决这一挑战，研究者提出Flash-ABFT。

Method: Flash-ABFT通过在线校验和覆盖查询、键和值矩阵的三矩阵乘积及softmax操作，实现单一检查。

Result: 实验显示，Flash-ABFT硬件面积开销仅5.3%，能量开销低于1.9%，检测精度高。

Conclusion: Flash-ABFT为注意力加速器提供了一种高效且成本低的错误检测解决方案。

Abstract: Transformers and large language models (LLMs), powered by the attention
mechanism, have transformed numerous AI applications, driving the need for
specialized hardware accelerators. A major challenge in these accelerators is
efficiently detecting errors caused by random hardware faults. Traditional
algorithm-based fault tolerance (ABFT) techniques verify individual matrix
multiplications but fall short in handling the full attention mechanism,
particularly due to intermediate softmax normalization. This work proposes
Flash-ABFT, a novel method that computes an online checksum across the entire
three-matrix product of query, key and value matrices, of an attention layer,
including the softmax operation, with a single check. This approach
significantly reduces overhead by eliminating redundant checks while
maintaining high fault-detection accuracy. Experimental results demonstrate
that Flash-ABFT incurs only 5.3% hardware area overhead and less than 1.9%
energy overhead, making it a cost-effective and robust solution for error
detection in attention accelerators.

</details>


### [77] [Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation](https://arxiv.org/abs/2507.16704)
*Viktor Muryn,Marta Sumyk,Mariya Hirna,Sofiya Garkot,Maksym Shamrai*

Main category: cs.LG

TL;DR: Screen2AX是一种自动从截图生成实时树状结构无障碍元数据的框架，旨在解决桌面应用无障碍支持的不足，通过视觉语言和对象检测模型提升AI代理的界面理解能力。


<details>
  <summary>Details</summary>
Motivation: 许多桌面应用的无障碍支持不完整，仅33%的macOS应用提供完整支持。现有方法未能捕获界面的完整层次结构，因此需要一种新方法来生成全面的无障碍元数据。

Method: Screen2AX结合视觉语言和对象检测模型，从单张截图中检测、描述并分层组织UI元素，复现macOS的无障碍结构。还发布了三个数据集支持模型训练。

Result: Screen2AX在重建无障碍树时的F1得分为77%，显著提升了AI代理的任务执行性能（2.2倍于原生无障碍表示，优于OmniParser V2）。

Conclusion: Screen2AX填补了桌面无障碍元数据生成的空白，其分层结构显著提升了AI代理的界面交互能力，为无障碍支持提供了新方向。

Abstract: Desktop accessibility metadata enables AI agents to interpret screens and
supports users who depend on tools like screen readers. Yet, many applications
remain largely inaccessible due to incomplete or missing metadata provided by
developers - our investigation shows that only 33% of applications on macOS
offer full accessibility support. While recent work on structured screen
representation has primarily addressed specific challenges, such as UI element
detection or captioning, none has attempted to capture the full complexity of
desktop interfaces by replicating their entire hierarchical structure. To
bridge this gap, we introduce Screen2AX, the first framework to automatically
create real-time, tree-structured accessibility metadata from a single
screenshot. Our method uses vision-language and object detection models to
detect, describe, and organize UI elements hierarchically, mirroring macOS's
system-level accessibility structure. To tackle the limited availability of
data for macOS desktop applications, we compiled and publicly released three
datasets encompassing 112 macOS applications, each annotated for UI element
detection, grouping, and hierarchical accessibility metadata alongside
corresponding screenshots. Screen2AX accurately infers hierarchy trees,
achieving a 77% F1 score in reconstructing a complete accessibility tree.
Crucially, these hierarchy trees improve the ability of autonomous agents to
interpret and interact with complex desktop interfaces. We introduce
Screen2AX-Task, a benchmark specifically designed for evaluating autonomous
agent task execution in macOS desktop environments. Using this benchmark, we
demonstrate that Screen2AX delivers a 2.2x performance improvement over native
accessibility representations and surpasses the state-of-the-art OmniParser V2
system on the ScreenSpot benchmark.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [LMM4Edit: Benchmarking and Evaluating Multimodal Image Editing with LMMs](https://arxiv.org/abs/2507.16193)
*Zitong Xu,Huiyu Duan,Bingnan Liu,Guangji Ma,Jiarui Wang,Liu Yang,Shiqi Gao,Xiaoyu Wang,Jia Wang,Xiongkuo Min,Guangtao Zhai,Weisi Lin*

Main category: cs.CV

TL;DR: 论文提出了EBench-18K，第一个大规模图像编辑基准数据集，并基于此开发了LMM4Edit评估指标，用于全面评价文本引导图像编辑模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本引导图像编辑（TIE）模型在图像质量、编辑对齐和一致性方面存在不足，且现有评估基准和指标在规模和人类感知对齐上有局限。

Method: 构建了EBench-18K数据集，包含18K编辑图像和人类偏好标注，并基于此开发了LMM4Edit评估指标，利用大型语言模型（LMM）从多个维度评估编辑效果。

Result: LMM4Edit在实验中表现出色，与人类偏好高度一致，并在其他数据集上展示了零样本泛化能力。

Conclusion: EBench-18K和LMM4Edit为TIE模型的评估提供了全面且高效的解决方案，推动了相关研究的发展。

Abstract: The rapid advancement of Text-guided Image Editing (TIE) enables image
modifications through text prompts. However, current TIE models still struggle
to balance image quality, editing alignment, and consistency with the original
image, limiting their practical applications. Existing TIE evaluation
benchmarks and metrics have limitations on scale or alignment with human
perception. To this end, we introduce EBench-18K, the first large-scale image
Editing Benchmark including 18K edited images with fine-grained human
preference annotations for evaluating TIE. Specifically, EBench-18K includes
1,080 source images with corresponding editing prompts across 21 tasks, 18K+
edited images produced by 17 state-of-the-art TIE models, 55K+ mean opinion
scores (MOSs) assessed from three evaluation dimensions, and 18K+
question-answering (QA) pairs. Based on EBench-18K, we employ outstanding LMMs
to assess edited images, while the evaluation results, in turn, provide
insights into assessing the alignment between the LMMs' understanding ability
and human preferences. Then, we propose LMM4Edit, a LMM-based metric for
evaluating image Editing models from perceptual quality, editing alignment,
attribute preservation, and task-specific QA accuracy in an all-in-one manner.
Extensive experiments show that LMM4Edit achieves outstanding performance and
aligns well with human preference. Zero-shot validation on the other datasets
also shows the generalization ability of our model. The dataset and code are
available at https://github.com/IntMeGroup/LMM4Edit.

</details>


### [79] [Towards Railway Domain Adaptation for LiDAR-based 3D Detection: Road-to-Rail and Sim-to-Real via SynDRA-BBox](https://arxiv.org/abs/2507.16413)
*Xavier Diaz,Gianluca D'Amico,Raul Dominguez-Sanchez,Federico Nesti,Max Ronecker,Giorgio Buttazzo*

Main category: cs.CV

TL;DR: 介绍了首个专门为铁路领域设计的合成数据集SynDRA-BBox，用于2D和3D目标检测，并展示了半监督域适应方法在铁路环境中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 铁路领域缺乏公开的标注数据集，导致新感知算法的测试和验证困难。

Method: 开发了SynDRA-BBox合成数据集，并采用半监督域适应方法从汽车领域迁移到铁路领域。

Result: 实验结果显示，合成数据集和域适应技术在铁路环境感知中表现出色。

Conclusion: SynDRA-BBox和域适应技术为铁路环境感知提供了有效的解决方案。

Abstract: In recent years, interest in automatic train operations has significantly
increased. To enable advanced functionalities, robust vision-based algorithms
are essential for perceiving and understanding the surrounding environment.
However, the railway sector suffers from a lack of publicly available
real-world annotated datasets, making it challenging to test and validate new
perception solutions in this domain. To address this gap, we introduce
SynDRA-BBox, a synthetic dataset designed to support object detection and other
vision-based tasks in realistic railway scenarios. To the best of our
knowledge, is the first synthetic dataset specifically tailored for 2D and 3D
object detection in the railway domain, the dataset is publicly available at
https://syndra.retis.santannapisa.it. In the presented evaluation, a
state-of-the-art semi-supervised domain adaptation method, originally developed
for automotive perception, is adapted to the railway context, enabling the
transferability of synthetic data to 3D object detection. Experimental results
demonstrate promising performance, highlighting the effectiveness of synthetic
datasets and domain adaptation techniques in advancing perception capabilities
for railway environments.

</details>


### [80] [Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A Practical Approach](https://arxiv.org/abs/2507.16556)
*Jon Gutiérrez-Zaballa,Koldo Basterretxea,Javier Echanobe*

Main category: cs.CV

TL;DR: 论文探讨了如何通过HSI（高光谱成像）和DNN（深度神经网络）结合，优化自动驾驶系统中的实时边缘计算，提出了硬件/软件协同设计方法，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统（ADS）对延迟、资源消耗和安全性有严格要求，传统的HSI和DNN计算开销大，需优化以适应边缘计算平台。

Method: 采用硬件/软件协同设计方案，包括任务分配、硬件感知预处理、ML模型压缩和流水线部署，优化HSI-DNN处理器在FPGA上的实现。

Result: 通过压缩技术，DNN的计算复杂度降至原操作的24.34%，参数数量降至1.02%，推理速度提升2.86倍，且分割精度未显著下降。

Conclusion: 该研究为HSI-DNN在自动驾驶系统中的实时边缘部署提供了高效解决方案，具有实际应用潜力。

Abstract: The use of HSI for autonomous navigation is a promising research field aimed
at improving the accuracy and robustness of detection, tracking, and scene
understanding systems based on vision sensors. Combining advanced computer
algorithms, such as DNNs, with small-size snapshot HSI cameras enhances the
reliability of these systems. HSI overcomes intrinsic limitations of greyscale
and RGB imaging in depicting physical properties of targets, particularly
regarding spectral reflectance and metamerism. Despite promising results in
HSI-based vision developments, safety-critical systems like ADS demand strict
constraints on latency, resource consumption, and security, motivating the
shift of ML workloads to edge platforms. This involves a thorough
software/hardware co-design scheme to distribute and optimize the tasks
efficiently among the limited resources of computing platforms. With respect to
inference, the over-parameterized nature of DNNs poses significant
computational challenges for real-time on-the-edge deployment. In addition, the
intensive data preprocessing required by HSI, which is frequently overlooked,
must be carefully managed in terms of memory arrangement and inter-task
communication to enable an efficient integrated pipeline design on a SoC. This
work presents a set of optimization techniques for the practical co-design of a
DNN-based HSI segmentation processor deployed on a FPGA-based SoC targeted at
ADS, including key optimizations such as functional software/hardware task
distribution, hardware-aware preprocessing, ML model compression, and a
complete pipelined deployment. Applied compression techniques significantly
reduce the complexity of the designed DNN to 24.34% of the original operations
and to 1.02% of the original number of parameters, achieving a 2.86x speed-up
in the inference task without noticeable degradation of the segmentation
accuracy.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [81] [Disability Across Cultures: A Human-Centered Audit of Ableism in Western and Indic LLMs](https://arxiv.org/abs/2507.16130)
*Mahika Phutane,Aditya Vashistha*

Main category: cs.CY

TL;DR: 研究探讨了西方和印度开发的大语言模型（LLM）在识别印度语境中的残疾歧视（ableism）时的表现差异，发现西方模型高估伤害，而印度模型低估伤害，且所有模型对印地语中的歧视更宽容。对比之下，印度残疾人群更关注意图、关系和教育。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决西方AI模型在非西方语境（如印度）中识别残疾歧视时可能存在的局限性，并探索本地化模型的表现。

Method: 研究通过翻译一个公开的残疾歧视数据集为印地语，使用8个LLM（4个西方和4个印度开发）进行评分和解释，同时招募175名残疾人士（美国和印度）完成相同任务以对比结果。

Result: 结果显示西方LLM高估残疾歧视伤害，印度LLM低估伤害；所有模型对印地语中的歧视更宽容。印度残疾人群更关注意图、关系和教育，而非伤害程度。

Conclusion: 研究强调AI系统的设计和评估需围绕本地残疾群体的体验，为建立全球包容性残疾歧视标准奠定了基础。

Abstract: People with disabilities (PwD) experience disproportionately high levels of
discrimination and hate online, particularly in India, where entrenched stigma
and limited resources intensify these challenges. Large language models (LLMs)
are increasingly used to identify and mitigate online hate, yet most research
on online ableism focuses on Western audiences with Western AI models. Are
these models adequately equipped to recognize ableist harm in non-Western
places like India? Do localized, Indic language models perform better? To
investigate, we adopted and translated a publicly available ableist speech
dataset to Hindi, and prompted eight LLMs--four developed in the U.S. (GPT-4,
Gemini, Claude, Llama) and four in India (Krutrim, Nanda, Gajendra,
Airavata)--to score and explain ableism. In parallel, we recruited 175 PwD from
both the U.S. and India to perform the same task, revealing stark differences
between groups. Western LLMs consistently overestimated ableist harm, while
Indic LLMs underestimated it. Even more concerning, all LLMs were more tolerant
of ableism when it was expressed in Hindi and asserted Western framings of
ableist harm. In contrast, Indian PwD interpreted harm through intention,
relationality, and resilience--emphasizing a desire to inform and educate
perpetrators. This work provides groundwork for global, inclusive standards of
ableism, demonstrating the need to center local disability experiences in the
design and evaluation of AI systems.

</details>


### [82] [PRAC3 (Privacy, Reputation, Accountability, Consent, Credit, Compensation): Long Tailed Risks of Voice Actors in AI Data-Economy](https://arxiv.org/abs/2507.16247)
*Tanusree Sharma,Yihao Zhou,Visar Berisha*

Main category: cs.CY

TL;DR: 论文探讨了语音合成技术中声音数据使用的伦理风险，提出PRAC3框架以应对隐私、声誉和问责等问题。


<details>
  <summary>Details</summary>
Motivation: 现有伦理框架未能充分解决声音数据被滥用的风险，尤其是声音身份与上下文脱钩带来的威胁。

Method: 通过20名专业配音演员的定性访谈，分析声音数据滥用的案例和影响。

Result: 揭示了声音数据滥用对配音演员的声誉和法律风险，并提出PRAC3框架。

Conclusion: 声音作为生物特征和创造性劳动需要治理模型，以确保伦理使用和创作者权益。

Abstract: Early large-scale audio datasets, such as LibriSpeech, were built with
hundreds of individual contributors whose voices were instrumental in the
development of speech technologies, including audiobooks and voice assistants.
Yet, a decade later, these same contributions have exposed voice actors to a
range of risks. While existing ethical frameworks emphasize Consent, Credit,
and Compensation (C3), they do not adequately address the emergent risks
involving vocal identities that are increasingly decoupled from context,
authorship, and control. Drawing on qualitative interviews with 20 professional
voice actors, this paper reveals how the synthetic replication of voice without
enforceable constraints exposes individuals to a range of threats. Beyond
reputational harm, such as re-purposing voice data in erotic content, offensive
political messaging, and meme culture, we document concerns about
accountability breakdowns when their voice is leveraged to clone voices that
are deployed in high-stakes scenarios such as financial fraud, misinformation
campaigns, or impersonation scams. In such cases, actors face social and legal
fallout without recourse, while very few of them have a legal representative or
union protection. To make sense of these shifting dynamics, we introduce the
PRAC3 framework, an expansion of C3 that foregrounds Privacy, Reputation,
Accountability, Consent, Credit, and Compensation as interdependent pillars of
data used in the synthetic voice economy. This framework captures how privacy
risks are amplified through non-consensual training, how reputational harm
arises from decontextualized deployment, and how accountability can be
reimagined AI Data ecosystems. We argue that voice, as both a biometric
identifier and creative labor, demands governance models that restore creator
agency, ensure traceability, and establish enforceable boundaries for ethical
reuse.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [83] [Differential Multimodal Transformers](https://arxiv.org/abs/2507.15875)
*Jerry Li,Timothy Oh,Joseph Hoang,Vardhit Veeramachaneni*

Main category: cs.AI

TL;DR: 将Differential Attention机制扩展到多模态模型PaliGemma中，以减少噪声信息检索和幻觉问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在处理多模态数据时，因上下文窗口有限可能导致噪声问题，Transformer注意力机制对无关上下文的过度关注进一步加剧了这一挑战。

Method: 将原本用于纯文本模型的Differential Attention机制扩展到PaliGemma模型，并通过LoRA进行微调，测试不同参数设置下的效果。

Result: 实验证明，Differential Attention可以成功应用于现有模型的微调中，提升噪声信息检索和问答能力。

Conclusion: Differential Attention机制在多模态模型中具有潜力，能有效减少噪声和幻觉问题。

Abstract: Small language models have gained significant popularity due to their
efficiency and growing capabilities. However, incorporating additional
modalities, such as vision, can exacerbate the challenge of limited context
windows by introducing noise. Recent studies have highlighted that Transformer
attention mechanisms often disproportionately focus on irrelevant contexts. In
this work, we extend the Differential Attention mechanism, originally designed
for text-only models, to the text-vision model PaliGemma. Our aim is to
evaluate its ability to mitigate noisy information retrieval and reduce
hallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA,
incorporating Differential Attention, and experimented with various parameter
settings and configurations. We demonstrate that Differential Attention can be
adapted and integrated into the fine-tuning of existing models to enhance noisy
information retrieval and question-answering capabilities.

</details>


### [84] [A Unifying Framework for Semiring-Based Constraint Logic Programming With Negation (full version)](https://arxiv.org/abs/2507.16067)
*Jeroen Spaans,Jesse Heyninck*

Main category: cs.AI

TL;DR: 该论文探讨了一种扩展约束逻辑编程（CLP）的方法，允许在子句体中包含否定，并使用近似不动点理论为其提供语义。


<details>
  <summary>Details</summary>
Motivation: 现有CLP扩展未研究子句体中允许否定的情况，论文旨在填补这一空白。

Method: 通过近似不动点理论为包含否定的CLP程序提供语义，分析半环性质对语义的影响。

Result: 提出了一个统一框架，既涵盖现有方法，又支持更富表现力的语言。

Conclusion: 该研究为CLP的扩展提供了一个统一且更具表达力的框架。

Abstract: Constraint Logic Programming (CLP) is a logic programming formalism used to
solve problems requiring the consideration of constraints, like resource
allocation and automated planning and scheduling. It has previously been
extended in various directions, for example to support fuzzy constraint
satisfaction, uncertainty, or negation, with different notions of semiring
being used as a unifying abstraction for these generalizations. None of these
extensions have studied clauses with negation allowed in the body. We
investigate an extension of CLP which unifies many of these extensions and
allows negation in the body. We provide semantics for such programs, using the
framework of approximation fixpoint theory, and give a detailed overview of the
impacts of properties of the semirings on the resulting semantics. As such, we
provide a unifying framework that captures existing approaches and allows
extending them with a more expressive language.

</details>


### [85] [Improving ASP-based ORS Schedules through Machine Learning Predictions](https://arxiv.org/abs/2507.16454)
*Pierangela Bruno,Carmine Dodaro,Giuseppe Galatà,Marco Maratea,Marco Mochi*

Main category: cs.AI

TL;DR: 该论文提出了一种结合归纳和演绎技术的方法，通过机器学习预测手术时间并生成临时手术室安排表，从而提高手术调度的稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前基于解答集编程的手术调度解决方案在实际应用中只能验证数据一致性或提供替代方案，无法生成临时安排表且缺乏稳健性。

Method: 结合机器学习预测手术持续时间，并利用预测结果的置信度更新编码，生成更稳健的手术调度。

Result: 在意大利ASL1 Liguria的历史数据上验证了该方法的可行性。

Conclusion: 综合归纳和演绎技术的方法有效解决了手术调度问题，提高了调度的临时性和稳健性。

Abstract: The Operating Room Scheduling (ORS) problem deals with the optimization of
daily operating room surgery schedules. It is a challenging problem subject to
many constraints, like to determine the starting time of different surgeries
and allocating the required resources, including the availability of beds in
different department units. Recently, solutions to this problem based on Answer
Set Programming (ASP) have been delivered. Such solutions are overall
satisfying but, when applied to real data, they can currently only verify
whether the encoding aligns with the actual data and, at most, suggest
alternative schedules that could have been computed. As a consequence, it is
not currently possible to generate provisional schedules. Furthermore, the
resulting schedules are not always robust.
  In this paper, we integrate inductive and deductive techniques for solving
these issues. We first employ machine learning algorithms to predict the
surgery duration, from historical data, to compute provisional schedules. Then,
we consider the confidence of such predictions as an additional input to our
problem and update the encoding correspondingly in order to compute more robust
schedules. Results on historical data from the ASL1 Liguria in Italy confirm
the viability of our integration.
  Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [86] [Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery](https://arxiv.org/abs/2507.16229)
*Bo Wen,Chen Wang,Qiwei Han,Raquel Norel,Julia Liu,Thaddeus Stappenbeck,Jeffrey L. Rogers*

Main category: cs.AI

TL;DR: 本研究探讨了基于语音的AI代理在医疗保健中的作用，特别是LLM驱动的语音助手如何提升预防性护理和患者监测，并分析了其经济性和技术挑战。


<details>
  <summary>Details</summary>
Motivation: 解决数字医疗中的经济性和可及性问题，特别是在服务不足的人群中。

Method: 通过开发并试点运行Agent PULSE，结合经济学模型和技术分析，评估AI代理的成本效益和技术挑战。

Result: 试点研究表明，70%的患者接受AI监测，37%的患者更偏好AI而非传统方式；AI代理在成本效益和患者参与度上表现优异。

Conclusion: 基于语音的AI代理有潜力成为公平、可持续的数字医疗解决方案，但需解决技术挑战和伦理问题。

Abstract: The integration of voice-based AI agents in healthcare presents a
transformative opportunity to bridge economic and accessibility gaps in digital
health delivery. This paper explores the role of large language model
(LLM)-powered voice assistants in enhancing preventive care and continuous
patient monitoring, particularly in underserved populations. Drawing insights
from the development and pilot study of Agent PULSE (Patient Understanding and
Liaison Support Engine) -- a collaborative initiative between IBM Research,
Cleveland Clinic Foundation, and Morehouse School of Medicine -- we present an
economic model demonstrating how AI agents can provide cost-effective
healthcare services where human intervention is economically unfeasible. Our
pilot study with 33 inflammatory bowel disease patients revealed that 70\%
expressed acceptance of AI-driven monitoring, with 37\% preferring it over
traditional modalities. Technical challenges, including real-time
conversational AI processing, integration with healthcare systems, and privacy
compliance, are analyzed alongside policy considerations surrounding
regulation, bias mitigation, and patient autonomy. Our findings suggest that
AI-driven voice agents not only enhance healthcare scalability and efficiency
but also improve patient engagement and accessibility. For healthcare
executives, our cost-utility analysis demonstrates huge potential savings for
routine monitoring tasks, while technologists can leverage our framework to
prioritize improvements yielding the highest patient impact. By addressing
current limitations and aligning AI development with ethical and regulatory
frameworks, voice-based AI agents can serve as a critical entry point for
equitable, sustainable digital healthcare solutions.

</details>


### [87] [ADEPTS: A Capability Framework for Human-Centered Agent Design](https://arxiv.org/abs/2507.15885)
*Pierluca D'Oro,Caley Drooff,Joy Chen,Joseph Tighe*

Main category: cs.AI

TL;DR: 论文提出ADEPTS框架，旨在为AI代理开发提供统一的用户导向能力指导，以解决当前领域内分散的指导意见问题。


<details>
  <summary>Details</summary>
Motivation: 当前关于以人为中心的AI代理开发的指南分散，缺乏统一的用户导向能力定义。

Method: 引入ADEPTS框架，基于六项人机交互设计原则，定义AI代理应具备的核心用户能力。

Result: ADEPTS框架为技术开发和用户体验提供了统一的指导语言，适用于研究人员、设计师及政策审查人员。

Conclusion: ADEPTS有望加速AI代理能力的提升，并为相关讨论提供共享语言。

Abstract: Large language models have paved the way to powerful and flexible AI agents,
assisting humans by increasingly integrating into their daily life. This
flexibility, potential, and growing adoption demands a holistic and
cross-disciplinary approach to developing, monitoring and discussing the
capabilities required for agent-driven user experiences. However, current
guidance on human-centered AI agent development is scattered: UX heuristics
focus on interface behaviors, engineering taxonomies describe internal
pipelines, and ethics checklists address high-level governance. There is no
concise, user-facing vocabulary that tells teams what an agent should
fundamentally be able to do. We introduce ADEPTS, a capability framework
defining a set of core user-facing capabilities to provide unified guidance
around the development of AI agents. ADEPTS is based on six principles for
human-centered agent design, that express the minimal, user-facing capabilities
an AI agent should demonstrate to be understandable, controllable and
trustworthy in everyday use. ADEPTS complements existing frameworks and
taxonomies; differently from them, it sits at the interface between technical
and experience development. By presenting ADEPTS, we aim to condense complex
AI-UX requirements into a compact framework that is actionable guidance for AI
researchers, designers, engineers, and policy reviewers alike. We believe
ADEPTS has the potential of accelerating the improvement of user-relevant agent
capabilities, of easing the design of experiences that take advantage of those
capabilities, and of providing a shared language to track and discuss progress
around the development of AI agents.

</details>


### [88] [LLM-Driven Collaborative Model for Untangling Commits via Explicit and Implicit Dependency Reasoning](https://arxiv.org/abs/2507.16395)
*Bo Hou,Xin Tan,Kai Zheng,Fang Liu,Yinghao Zhu,Li Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体协作的提交解缠框架ColaUntangle，通过显式和隐式依赖关系建模，显著提升了提交解缠的效果。


<details>
  <summary>Details</summary>
Motivation: 现有提交解缠方法依赖于浅层信号，难以区分显式（如控制/数据流）和隐式（如语义关系）依赖关系，影响了代码审查和维护的效率。

Method: 提出ColaUntangle框架，利用LLM驱动的多智能体架构（分别处理显式和隐式依赖），并结合多版本程序依赖图（delta-PDG）进行分析。

Result: 在两个数据集上测试，ColaUntangle性能显著优于基线，C#和Java数据集的提升分别达到44%和100%。

Conclusion: LLM驱动的协作框架在提交解缠任务中具有巨大潜力，ColaUntangle为相关研究提供了新思路。

Abstract: Atomic commits, each of which addresses a single development concern, are a
best practice in software development. However, developers frequently produce
tangled commits that mix unrelated changes due to practical constraints or
unclear boundaries, negatively impacting code review and maintenance. Although
prior commit untangling approaches: rule-based, feature-based, or graph-based,
have made progress, they often rely on shallow signals and fail to distinguish
between explicit dependencies (e.g., control/data flow) and implicit ones
(e.g., semantic or conceptual relationships). In this paper, we propose
ColaUntangle, a new collaborative consultation framework for commit untangling
that models both explicit and implicit dependencies among code changes.
ColaUntangle integrates Large Language Model (LLM)-driven agents in a
multi-agent architecture: one agent specializes in explicit dependencies,
another in implicit ones, and a reviewer agent synthesizes their perspectives
through iterative consultation. To capture explicit and implicit contextual
information, we construct multi-version Program Dependency Graphs (delta-PDG),
enabling agents to reason over code relationships with both symbolic and
semantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C#
and 14k Java tangled commits). Experimental results show that ColaUntangle
outperforms the best-performing baseline, achieving an improvement of 44% on
the C# dataset and 100% on the Java dataset. These findings highlight the
potential of LLM-based collaborative frameworks for advancing automated commit
untangling tasks.

</details>


### [89] [ACT: Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training](https://arxiv.org/abs/2507.16478)
*Shreya Saxena,Siva Prasad,Zishan Ahmad,Vishal Vaddina*

Main category: cs.AI

TL;DR: ACT框架通过内部微调开源LLMs提升代码翻译性能，结合合成数据生成和动态优化，提供安全可靠的开源替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统自动翻译方法依赖人工规则，缺乏灵活性和扩展性，而高级语言模型常受限于专有API和数据安全问题。

Method: ACT框架包括合成数据生成模块（基于初始代码和单元测试构建数据集）、控制器模块（动态调整超参数和数据生成）和评估模块（执行级检查）。

Result: ACT显著提升开源模型性能，接近闭源解决方案，并在行业迁移项目中加速开发效率。

Conclusion: ACT为企业和开发者提供安全高效的开源代码翻译工具，填补开源与闭源模型间的性能差距。

Abstract: Code translation is a crucial process in software development and migration
projects, enabling interoperability between different programming languages and
enhancing software adaptability and thus longevity. Traditional automated
translation methods rely heavily on handcrafted transformation rules, which
often lack flexibility and scalability. Meanwhile, advanced language models
present promising alternatives but are often limited by proprietary, API-based
implementations that raise concerns over data security and reliance. In this
paper, we present Auto-Train for Code Translation (ACT), an innovative
framework that aims to improve code translation capabilities by enabling
in-house finetuning of open-source Large Language Models (LLMs). ACT's
automated pipeline significantly boosts the performance of these models,
narrowing the gap between open-source accessibility and the high performance of
closed-source solutions. Central to ACT is its synthetic data generation
module, which builds extensive, high-quality datasets from initial code
samples, incorporating unit tests to ensure functional accuracy and diversity.
ACT's evaluation framework incorporates execution-level checks, offering a
comprehensive assessment of translation quality. A key feature in ACT is its
controller module, which manages the entire pipeline by dynamically adjusting
hyperparameters, orchestrating iterative data generation, and finetuning based
on real-time evaluations. This enables ACT to intelligently optimize when to
continue training, generate additional targeted training data, or stop the
process. Our results demonstrate that ACT consistently enhances the
effectiveness of open-source models, offering businesses and developers a
secure and reliable alternative. Additionally, applying our data generation
pipeline to industry-scale migration projects has led to a notable increase in
developer acceleration.

</details>


### [90] [Emergent Cognitive Convergence via Implementation: A Structured Loop Reflecting Four Theories of Mind (A Position Paper)](https://arxiv.org/abs/2507.16184)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 论文发现了一种结构上的趋同性，涉及四个重要心智理论，并通过AI架构Agentic Flow的实践设计展示了这种趋同性。


<details>
  <summary>Details</summary>
Motivation: 探索实践设计如何无意中体现心智理论的结构，并解决大语言模型的局限性。

Method: 设计了Agentic Flow架构，包含五个模块，并通过对比实验验证其有效性。

Result: Agentic Flow在多步推理任务中表现优于基线系统（95.8% vs 62.3%）。

Conclusion: 实践设计可以反映心智理论的结构，PEACE架构为此提供了描述性框架。

Abstract: We report the discovery of a structural convergence across four influential
theories of mind: Kahneman's dual-system theory, Friston's predictive
processing, Minsky's society of mind, and Clark's extended mind-emerging
unintentionally within a practical AI agent architecture called Agentic Flow.
Designed to address limitations in large language models (LLMs), Agentic Flow
comprises five interdependent modules such as Retrieval, Cognition, Control,
Memory, and Action arranged in a recurrent cognitive loop. Although originally
inspired only by Minsky and Clark, the system's structure retrospectively
aligns with computational motifs found in all four theories, including
predictive modeling, associative recall, and error-sensitive control.
  To assess this convergence, we conducted comparative experiments with
baseline LLM agents on multi-step reasoning tasks. The structured agent
achieved 95.8% task success and exhibited strong constraint adherence, while
the baseline system succeeded 62.3% of the time. These results were not aimed
at proving superiority, but at illustrating how theoretical structures may
emerge through practical design choices rather than top-down theory.
  We introduce PEACE as a descriptive meta-architecture that captures
design-level regularities observed in Agentic Flow. Not intended as a new
theory, PEACE provides a shared vocabulary for understanding architectures
shaped by real-world implementation demands. This paper should be read as a
position paper - an exploratory reflection on how implementation can surface
latent structural echoes of cognitive theory, without asserting theoretical
unification.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [91] [SVAgent: AI Agent for Hardware Security Verification Assertion](https://arxiv.org/abs/2507.16203)
*Rui Guo,Avinash Ayalasomayajula,Henian Li,Jingbo Zhou,Sujan Kumar Saha,Farimah Farahmandi*

Main category: cs.CR

TL;DR: 本文提出了一种自动生成SystemVerilog断言（SVA）的框架SVAgent，以解决现有SVA开发模型的低效性和无法应对复杂电路漏洞的问题。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路设计的全球化和安全需求的升级，传统SVA开发模型暴露出效率低下且难以应对现代复杂电路漏洞的局限性。

Method: SVAgent通过需求分解机制，将复杂需求转化为结构化、可逐步解决的细粒度问题求解链，并整合进主流漏洞评估框架。

Result: 实验表明，SVAgent能有效抑制幻觉和随机答案的影响，SVA的准确性和一致性等关键指标显著优于现有框架。

Conclusion: SVAgent在实际工程设计环境中验证了其实用性和可靠性，成功应对了复杂集成电路的安全挑战。

Abstract: Verification using SystemVerilog assertions (SVA) is one of the most popular
methods for detecting circuit design vulnerabilities. However, with the
globalization of integrated circuit design and the continuous upgrading of
security requirements, the SVA development model has exposed major limitations.
It is not only inefficient in development, but also unable to effectively deal
with the increasing number of security vulnerabilities in modern complex
integrated circuits. In response to these challenges, this paper proposes an
innovative SVA automatic generation framework SVAgent. SVAgent introduces a
requirement decomposition mechanism to transform the original complex
requirements into a structured, gradually solvable fine-grained problem-solving
chain. Experiments have shown that SVAgent can effectively suppress the
influence of hallucinations and random answers, and the key evaluation
indicators such as the accuracy and consistency of the SVA are significantly
better than existing frameworks. More importantly, we successfully integrated
SVAgent into the most mainstream integrated circuit vulnerability assessment
framework and verified its practicality and reliability in a real engineering
design environment.

</details>


### [92] [DP2Guard: A Lightweight and Byzantine-Robust Privacy-Preserving Federated Learning Scheme for Industrial IoT](https://arxiv.org/abs/2507.16134)
*Baofu Han,Bing Li,Yining Qi,Raja Jurdak,Kaibin Huang,Chau Yuen*

Main category: cs.CR

TL;DR: 文章提出了DP2Guard，一种轻量级的隐私保护联邦学习框架，通过梯度掩码和混合防御策略提升隐私与鲁棒性，同时降低了通信和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护联邦学习方案因依赖重型加密技术和单策略防御机制，存在通信计算开销大且对抗自适应对手鲁棒性不足的问题。

Method: 提出DP2Guard，采用轻量级梯度掩码保护隐私，结合特征提取和聚类算法的混合防御策略识别恶意梯度，并基于信任评分调整聚合权重。

Result: 在两个公共数据集上的实验表明，DP2Guard能有效抵御四种高级毒化攻击，同时降低开销。

Conclusion: DP2Guard是一种高效的隐私保护联邦学习框架，兼具轻量化和鲁棒性。

Abstract: Privacy-Preserving Federated Learning (PPFL) has emerged as a secure
distributed Machine Learning (ML) paradigm that aggregates locally trained
gradients without exposing raw data. To defend against model poisoning threats,
several robustness-enhanced PPFL schemes have been proposed by integrating
anomaly detection. Nevertheless, they still face two major challenges: (1) the
reliance on heavyweight encryption techniques results in substantial
communication and computation overhead; and (2) single-strategy defense
mechanisms often fail to provide sufficient robustness against adaptive
adversaries. To overcome these challenges, we propose DP2Guard, a lightweight
PPFL framework that enhances both privacy and robustness. DP2Guard leverages a
lightweight gradient masking mechanism to replace costly cryptographic
operations while ensuring the privacy of local gradients. A hybrid defense
strategy is proposed, which extracts gradient features using singular value
decomposition and cosine similarity, and applies a clustering algorithm to
effectively identify malicious gradients. Additionally, DP2Guard adopts a trust
score-based adaptive aggregation scheme that adjusts client weights according
to historical behavior, while blockchain records aggregated results and trust
scores to ensure tamper-proof and auditable training. Extensive experiments
conducted on two public datasets demonstrate that DP2Guard effectively defends
against four advanced poisoning attacks while ensuring privacy with reduced
communication and computation costs.

</details>


### [93] [BACFuzz: Exposing the Silence on Broken Access Control Vulnerabilities in Web Applications](https://arxiv.org/abs/2507.15984)
*I Putu Arya Dharmaadi,Mohannad Alhanahnah,Van-Thuan Pham,Fadi Mohsen,Fatih Turkmen*

Main category: cs.CR

TL;DR: BACFuzz是一个针对PHP网络应用中BAC漏洞的灰盒模糊测试框架，通过LLM引导的参数选择和运行时反馈，有效检测BAC漏洞。


<details>
  <summary>Details</summary>
Motivation: BAC是网络应用中常见但研究不足的漏洞，缺乏自动化测试工具和有效检测方法。

Method: BACFuzz结合LLM引导的参数选择、运行时反馈和SQL查询分析，检测BAC漏洞。

Result: 在20个实际应用测试中，BACFuzz检测到16个已知漏洞和26个新漏洞，误报率低。

Conclusion: BACFuzz为解决BAC漏洞自动化检测提供了有效工具，并公开披露了所有发现的漏洞。

Abstract: Broken Access Control (BAC) remains one of the most critical and widespread
vulnerabilities in web applications, allowing attackers to access unauthorized
resources or perform privileged actions. Despite its severity, BAC is
underexplored in automated testing due to key challenges: the lack of reliable
oracles and the difficulty of generating semantically valid attack requests. We
introduce BACFuzz, the first gray-box fuzzing framework specifically designed
to uncover BAC vulnerabilities, including Broken Object-Level Authorization
(BOLA) and Broken Function-Level Authorization (BFLA) in PHP-based web
applications. BACFuzz combines LLM-guided parameter selection with runtime
feedback and SQL-based oracle checking to detect silent authorization flaws. It
employs lightweight instrumentation to capture runtime information that guides
test generation, and analyzes backend SQL queries to verify whether
unauthorized inputs flow into protected operations. Evaluated on 20 real-world
web applications, including 15 CVE cases and 2 known benchmarks, BACFuzz
detects 16 of 17 known issues and uncovers 26 previously unknown BAC
vulnerabilities with low false positive rates. All identified issues have been
responsibly disclosed, and artifacts will be publicly released.

</details>


### [94] ["We Need a Standard": Toward an Expert-Informed Privacy Label for Differential Privacy](https://arxiv.org/abs/2507.15997)
*Onyinye Dibia,Mengyi Lu,Prianka Bhattacharjee,Joseph P. Near,Yuanyuan Feng*

Main category: cs.CR

TL;DR: 研究探讨如何通过标签标准化差异隐私（DP）参数的披露，以提升透明度和信任。


<details>
  <summary>Details</summary>
Motivation: 现实中的DP部署常未充分披露隐私保障参数，导致误解和信任缺失，需制定标准化的沟通方式。

Method: 通过对12位DP专家的半结构化访谈，识别关键参数并设计隐私标签。

Result: 确定了必须披露的DP参数，提出初步标准化隐私标签设计方案。

Conclusion: 标准化的隐私标签有助于透明沟通DP保障，增强公众信任。

Abstract: The increasing adoption of differential privacy (DP) leads to public-facing
DP deployments by both government agencies and companies. However, real-world
DP deployments often do not fully disclose their privacy guarantees, which vary
greatly between deployments. Failure to disclose certain DP parameters can lead
to misunderstandings about the strength of the privacy guarantee, undermining
the trust in DP. In this work, we seek to inform future standards for
communicating the privacy guarantees of DP deployments. Based on
semi-structured interviews with 12 DP experts, we identify important DP
parameters necessary to comprehensively communicate DP guarantees, and describe
why and how they should be disclosed. Based on expert recommendations, we
design an initial privacy label for DP to comprehensively communicate privacy
guarantees in a standardized format.

</details>
