<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 20]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 9]
- [cs.HC](#cs.HC) [Total: 10]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 8]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.CR](#cs.CR) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Main category: cs.SE

TL;DR: 该论文介绍了DroidCollection和DroidDetect，前者是一个全面的开源数据集，用于训练和评估机器生成代码检测器；后者是基于该数据集训练的一种检测器，展示了现有检测器的局限性以及对抗性训练的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成代码检测器在跨领域和语言泛化能力上表现不佳，且容易受到对抗性样本的欺骗。为此，作者开发了更全面的数据集和检测器，以解决这些问题。

Method: 作者编译了DroidCollection数据集，包含多种编程语言、代码模型输出和对抗性样本。随后开发了DroidDetect检测器，采用多任务目标和对抗性训练。

Result: 实验表明，现有检测器泛化能力不足，但对抗性训练可以显著提升检测器的鲁棒性。此外，度量学习和基于不确定性的重采样也能提高性能。

Conclusion: DroidDetect通过对抗性训练和多任务学习，显著提升了机器生成代码检测的泛化能力和鲁棒性，为未来研究提供了有力工具。

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [2] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Main category: cs.SE

TL;DR: ARPaCCino是一个结合LLM、RAG和工具验证的代理系统，用于自动化生成和验证‘策略即代码’规则，提升策略的自动化、可靠性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 由于策略语言的复杂性和配置错误的风险，‘策略即代码’的采用受到阻碍，需要更智能的自动化工具支持。

Method: ARPaCCino利用LLM、RAG和外部工具验证，从自然语言描述生成Rego规则，评估并迭代优化IaC配置，支持多种技术框架。

Result: 在Terraform案例中，ARPaCCino能生成语法和语义正确的策略，识别不合规基础设施并修正。

Conclusion: 代理式RAG架构可显著提升策略即代码的自动化水平和可靠性。

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [3] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Main category: cs.SE

TL;DR: 介绍了一种名为Meta Self-Refining的框架，用于解决语言模型在面临竞争性软约束时的低效回溯问题。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型管道在面对竞争性软约束时往往低效且无法有效满足所有约束条件，导致回溯循环。

Method: 提出一种元矫正层，通过监控执行历史检测失败模式，并调用一个元修复器LM生成平衡竞争的指令。

Result: 实验表明，该方法能有效修复回溯循环，提升语言模型程序的效率。

Conclusion: Meta Self-Refining通过动态修复机制，显著改善了语言模型在复杂约束下的表现。

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [4] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: Toolregistry是一个协议无关的工具管理库，统一接口简化工具集成，减少60-80%代码，提升3.1倍性能，兼容OpenAI标准。


<details>
  <summary>Details</summary>
Motivation: 当前工具集成方法存在碎片化、协议限制和实现复杂问题，开发成本高，需简化。

Method: 提出Toolregistry，提供统一接口管理工具的注册、表示、执行和生命周期。

Result: 减少60-80%代码，性能提升3.1倍，完全兼容OpenAI标准。

Conclusion: Toolregistry显著提升开发效率和代码可维护性，已开源并支持多样化集成场景。

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [5] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Main category: cs.SE

TL;DR: 论文提出了一种自动化工具SENSOR和模型GRACE，用于分类用户评论中的隐私相关问题和请求，帮助开发者高效处理隐私问题。


<details>
  <summary>Details</summary>
Motivation: 社交媒体的隐私问题日益突出，但手动分析海量用户评论困难。现有工具缺乏对隐私相关评论的细分分类。

Method: 开发了SENSOR工具和GRACE模型（基于GRU、CBOW和注意力机制），用于自动分类用户评论。

Result: GRACE模型表现出色（F1: 0.9434, ROC-AUC: 0.9934, 准确率: 95.10%），且在两万条评论上标注一致性高（Kappa: 0.87）。

Conclusion: SENSOR能有效帮助开发者识别隐私问题，提升用户信任和隐私保护。

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


### [6] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 论文通过微调大型语言模型，提升其在代码理解任务中的表现，尤其是语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在代码生成等任务中表现出色，但其对代码语义理解能力有限，因此需要针对代码理解任务进行微调。

Method: 提出对三种不同规模的代码模型进行微调，使用大规模数据集，以增强其对代码语义的理解能力。

Result: 微调后，模型在Subjectivity Grading Task中的准确率显著提升，尤其是QWQ-32B模型从70%提高到83.47%，DPO微调的Codestral-22B模型达到87.66%的最高准确率。

Conclusion: 微调能有效提升模型在代码理解任务中的语义理解能力，验证了方法的有效性。

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [7] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Main category: cs.SE

TL;DR: CodeAssistBench (CAB) 是一个新的基准框架，用于评估多轮编程辅助在真实环境中的表现，填补现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的编程辅助基准过于狭窄，主要关注代码生成任务，且缺乏对多轮互动和完整项目环境的支持。

Method: CAB 通过从 GitHub 问题自动生成数据集，并利用容器化代码库进行多轮模型评估。

Result: 实验显示，主流 LLMs 在 Stack Overflow 问题上表现良好（成功率 70-83%），但在 CAB 的复杂问题上仅解决 16.49%。

Conclusion: 复杂项目环境对编程辅助提出了更高的挑战，现有模型在真实场景中的能力仍有显著差距。

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [8] [Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction](https://arxiv.org/abs/2507.10729)
*Duong Nguyen,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: 该论文研究了即时漏洞预测（JIT-VP）在更真实环境中的表现，发现现有方法在现实中性能大幅下降，并提出需针对性解决数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前JIT-VP评估依赖理想化平衡数据集，而现实中漏洞相关提交极少，导致性能下降明显，需更真实评估。

Method: 引入包含百万提交的真实数据集，评估八种先进JIT-VP技术，并测试常用数据不平衡处理方法。

Result: 现实条件下JIT-VP性能显著降低（如PR-AUC下降98%），现有不平衡处理方法无效。

Conclusion: 需针对JIT-VP开发领域特定技术，并强调真实评估的重要性。

Abstract: Modern software systems are increasingly complex, presenting significant
challenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)
is a proactive approach to identifying vulnerable commits and providing early
warnings about potential security risks. However, we observe that current
JIT-VP evaluations rely on an idealized setting, where the evaluation datasets
are artificially balanced, consisting exclusively of vulnerability-introducing
and vulnerability-fixing commits.
  To address this limitation, this study assesses the effectiveness of JIT-VP
techniques under a more realistic setting that includes both
vulnerability-related and vulnerability-neutral commits. To enable a reliable
evaluation, we introduce a large-scale public dataset comprising over one
million commits from FFmpeg and the Linux kernel. Our empirical analysis of
eight state-of-the-art JIT-VP techniques reveals a significant decline in
predictive performance when applied to real-world conditions; for example, the
average PR-AUC on Linux drops 98\% from 0.805 to 0.016. This discrepancy is
mainly attributed to the severe class imbalance in real-world datasets, where
vulnerability-introducing commits constitute only a small fraction of all
commits.
  To mitigate this issue, we explore the effectiveness of widely adopted
techniques for handling dataset imbalance, including customized loss functions,
oversampling, and undersampling. Surprisingly, our experimental results
indicate that these techniques are ineffective in addressing the imbalance
problem in JIT-VP. These findings underscore the importance of realistic
evaluations of JIT-VP and the need for domain-specific techniques to address
data imbalance in such scenarios.

</details>


### [9] [GenAI-Enabled Backlog Grooming in Agile Software Projects: An Empirical Study](https://arxiv.org/abs/2507.10753)
*Kasper Lien Oftebro,Anh Nguyen-Duc,Kai-Kristian Kemell*

Main category: cs.SE

TL;DR: 研究探讨了生成式AI助手能否自动化敏捷项目中的待办列表管理，开发了一个Jira插件，实现高精度和效率提升。


<details>
  <summary>Details</summary>
Motivation: 随着待办列表规模和复杂性增加，管理变得困难，需要自动化工具来提高效率和准确性。

Method: 通过设计科学循环开发Jira插件，使用向量数据库和GPT-4o模型检测重复任务并建议操作。

Result: AI辅助管理实现了100%的精准度，并将完成时间减少45%。

Conclusion: 工具展示了优化待办列表管理的潜力，同时提升用户体验。

Abstract: Effective backlog management is critical for ensuring that development teams
remain aligned with evolving requirements and stakeholder expectations.
However, as product backlogs consistently grow in scale and complexity, they
tend to become cluttered with redundant, outdated, or poorly defined tasks,
complicating prioritization and decision making processes. This study
investigates whether a generative-AI (GenAI) assistant can automate backlog
grooming in Agile software projects without sacrificing accuracy or
transparency. Through Design Science cycles, we developed a Jira plug-in that
embeds backlog issues with the vector database, detects duplicates via cosine
similarity, and leverage the GPT-4o model to propose merges, deletions, or new
issues. We found that AI-assisted backlog grooming achieved 100 percent
precision while reducing the time-to-completion by 45 percent. The findings
demonstrated the tool's potential to streamline backlog refinement processes
while improving user experiences.

</details>


### [10] [Towards a Closer Collaboration Between Practice and Research in Agile Software Development Workshop: A Summary and Research Agenda](https://arxiv.org/abs/2507.10785)
*Michael Neumann,Eva-Maria Schön,Mali Senapathi,Maria Rauschenberger,Tiago Silva da Silva*

Main category: cs.SE

TL;DR: 这篇论文分析了敏捷软件开发中研究与实际应用之间的差距，并通过首次国际研讨会提出了解决策略和研究挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管敏捷软件开发在全球范围内被广泛采用，但研究与实践之间存在显著差距，需要加强协作。

Method: 通过组织首次国际研讨会，汇集研究与实际应用领域的专家，探讨差距的主要因素和解决方案。

Result: 研讨会识别了导致研究与实践差距的主题和因素，并提出了弥合差距的策略。

Conclusion: 论文强调了进一步研究的必要性，以解决敏捷软件开发中研究与实践之间的挑战。

Abstract: Agile software development principles and values have been widely adopted
across various industries, influencing products and services globally. Despite
its increasing popularity, a significant gap remains between research and
practical implementation. This paper presents the findings of the first
international workshop designed to foster collaboration between research and
practice in agile software development. We discuss the main themes and factors
identified by the workshop participants that contribute to this gap, strategies
to bridge it, and the challenges that require further research attention.

</details>


### [11] [How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow](https://arxiv.org/abs/2507.10818)
*Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: LLMs更倾向于推荐成熟、流行的第三方Python库，但存在安装支持和依赖解析的不足。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在编程任务中推荐库的行为，以提升其生成代码的可靠性和实用性。

Method: 实证研究六种LLMs，分析其在Stack Overflow问题中推荐的库类型、特性及可用性。

Result: LLMs偏爱第三方库，推荐成熟且流行的依赖项，但部分库存在安装问题，且仅少数模型提供安装指导。

Conclusion: 研究发现为开发者和研究者提供了改进LLM生成代码依赖管理的方向。

Abstract: Software libraries are central to the functionality, security, and
maintainability of modern code. As developers increasingly turn to Large
Language Models (LLMs) to assist with programming tasks, understanding how
these models recommend libraries is essential. In this paper, we conduct an
empirical study of six state-of-the-art LLMs, both proprietary and open-source,
by prompting them to solve real-world Python problems sourced from Stack
Overflow. We analyze the types of libraries they import, the characteristics of
those libraries, and the extent to which the recommendations are usable out of
the box. Our results show that LLMs predominantly favour third-party libraries
over standard ones, and often recommend mature, popular, and permissively
licensed dependencies. However, we also identify gaps in usability: 4.6% of the
libraries could not be resolved automatically due to structural mismatches
between import names and installable packages, and only two models (out of six)
provided installation guidance. While the generated code is technically valid,
the lack of contextual support places the burden of manually resolving
dependencies on the user. Our findings offer actionable insights for both
developers and researchers, and highlight opportunities to improve the
reliability and usability of LLM-generated code in the context of software
dependencies.

</details>


### [12] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Main category: cs.SE

TL;DR: 本文研究了自适应AI对话代理在软件开发中的作用，探讨了其动态化和个性化的辅助能力，以及面临的隐私和伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨自适应AI对话代理如何通过机器学习和自然语言处理提升开发效率，并解决其集成过程中的问题。

Method: 分析了从简单查询系统到高级AI驱动工具（如GitHub Copilot）的演变，并研究了相关挑战。

Result: 自适应AI对话代理能够提供实时、个性化的支持，显著提升开发效率，但也需解决隐私和伦理问题。

Conclusion: 自适应AI对话代理在软件开发中具有巨大潜力，未来需进一步优化和规范。

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


### [13] [Evaluating Generated Commit Messages with Large Language Models](https://arxiv.org/abs/2507.10906)
*Qunhong Zeng,Yuxia Zhang,Zexiong Ma,Bo Jiang,Ningyuan Sun,Klaas-Jan Stol,Xingyu Mou,Hui Liu*

Main category: cs.SE

TL;DR: 本文研究了使用大型语言模型（LLM）作为自动化评估器来评估提交信息质量的潜力，发现结合思维链推理和少量示例的LLM能够达到接近人类水平的评估效果。


<details>
  <summary>Details</summary>
Motivation: 提交信息在软件开发中至关重要，但其质量常不足，传统评估方法既有限又资源密集，因此探索LLM作为自动化评估器的潜力。

Method: 通过系统实验，采用不同提示策略和先进LLM，结合思维链推理和少量示例进行自动评估。

Result: LLM评估器显著优于传统指标，接近人类水平，同时保持可接受的再现性、鲁棒性和公平性。

Conclusion: LLM为提交信息评估提供了一种可扩展的高质量替代方案，尽管存在一定变异性。

Abstract: Commit messages are essential in software development as they serve to
document and explain code changes. Yet, their quality often falls short in
practice, with studies showing significant proportions of empty or inadequate
messages. While automated commit message generation has advanced significantly,
particularly with Large Language Models (LLMs), the evaluation of generated
messages remains challenging. Traditional reference-based automatic metrics
like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit
message quality, as they assume a one-to-one mapping between code changes and
commit messages, leading researchers to rely on resource-intensive human
evaluation. This study investigates the potential of LLMs as automated
evaluators for commit message quality. Through systematic experimentation with
various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs
combining Chain-of-Thought reasoning with few-shot demonstrations achieve near
human-level evaluation proficiency. Our LLM-based evaluator significantly
outperforms traditional metrics while maintaining acceptable reproducibility,
robustness, and fairness levels despite some inherent variability. This work
conducts a comprehensive preliminary study on using LLMs for commit message
evaluation, offering a scalable alternative to human assessment while
maintaining high-quality evaluation.

</details>


### [14] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: 论文介绍了SWE-MERA，一种动态更新的基准测试，旨在解决SWE-bench中的数据污染问题，通过自动化收集GitHub问题并提供严格验证。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试（如SWE-bench）存在严重的数据污染问题（如解决方案泄漏和测试用例不足），迫切需要一个更可靠的评估工具。

Method: 提出SWE-MERA，通过自动化收集真实GitHub问题并严格验证，确保数据质量。目前提供约10,000个潜在任务，300个已验证样本。

Result: 使用Aider编码代理评估，结果显示其对最新LLMs具有强区分能力。

Conclusion: SWE-MERA是一个动态、高质量的基准测试，能够有效解决现有问题，并为LLMs评估提供可靠工具。

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [15] [MT4DP: Data Poisoning Attack Detection for DL-based Code Search Models via Metamorphic Testing](https://arxiv.org/abs/2507.11092)
*Gong Chen,Wenjie Liu,Xiaoyuan Xie,Xunzhu Tang,Tegawendé F. Bissyandé,Songqiang Chen*

Main category: cs.SE

TL;DR: MT4DP是一种通过变体测试检测基于深度学习的代码搜索模型数据中毒攻击的新框架，显著提高了检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法对基于深度学习的代码搜索模型的数据中毒攻击检测效果不足，急需更有效的解决方案。

Method: 提出MT4DP框架，利用语义等价变体关系（SE-MR）检测攻击，通过生成语义等效查询并重新排序结果来实现。

Result: 实验表明MT4DP在平均F1分数和精度上分别优于基线191%和265%。

Conclusion: MT4DP能有效检测数据中毒攻击，为未来研究提供方向。

Abstract: Recently, several studies have indicated that data poisoning attacks pose a
severe security threat to deep learning-based (DL-based) code search models.
Attackers inject carefully crafted malicious patterns into the training data,
misleading the code search model to learn these patterns during training.
During the usage of the poisoned code search model for inference, once the
malicious pattern is triggered, the model tends to rank the vulnerability code
higher. However, existing detection methods for data poisoning attacks on
DL-based code search models remain insufficiently effective. To address this
critical security issue, we propose MT4DP, a Data Poisoning Attack Detection
Framework for DL-based Code Search Models via Metamorphic Testing. MT4DP
introduces a novel Semantically Equivalent Metamorphic Relation (SE-MR)
designed to detect data poisoning attacks on DL-based code search models.
Specifically, MT4DP first identifies the high-frequency words from search
queries as potential poisoning targets and takes their corresponding queries as
the source queries. For each source query, MT4DP generates two semantically
equivalent follow-up queries and retrieves its source ranking list. Then, each
source ranking list is re-ranked based on the semantic similarities between its
code snippets and the follow-up queries. Finally, variances between the source
and re-ranked lists are calculated to reveal violations of the SE-MR and warn
the data poisoning attack. Experimental results demonstrate that MT4DP
significantly enhances the detection of data poisoning attacks on DL-based code
search models, outperforming the best baseline by 191% on average F1 score and
265% on average precision. Our work aims to promote further research into
effective techniques for mitigating data poisoning threats on DL-based code
search models.

</details>


### [16] [Automata Models for Effective Bug Description](https://arxiv.org/abs/2507.11146)
*Tom Yaacov,Gera Weiss,Gal Amram,Avi Hayoun*

Main category: cs.SE

TL;DR: 提出用自动机学习和测试技术生成简洁、信息丰富的错误描述，引入Failure Explanations (FE)、Eventual Failure Explanations (EFE)和Early Detection (ED)概念，优化了错误检测和理解。


<details>
  <summary>Details</summary>
Motivation: 调试复杂系统耗时且重要，亟需高效方法生成有意义的错误行为总结。

Method: 通过自动机学习和测试技术，提取关键测试模式，排除无关信息。

Result: 在多种测试模式和真实基准中验证了方法的有效性，生成了紧凑且信息丰富的错误描述。

Conclusion: 该方法提升了错误检测和理解的效率，为复杂系统调试提供了实用工具。

Abstract: Debugging complex systems is a crucial yet time-consuming task. This paper
presents the use of automata learning and testing techniques to obtain concise
and informative bug descriptions. We introduce the concepts of Failure
Explanations (FE), Eventual Failure Explanations (EFE), and Early Detection
(ED) to provide meaningful summaries of failing behavior patterns. By factoring
out irrelevant information and focusing on essential test patterns, our
approach aims to enhance bug detection and understanding. We evaluate our
methods using various test patterns and real-world benchmarks, demonstrating
their effectiveness in producing compact and informative bug descriptions.

</details>


### [17] [New Formulation of DNN Statistical Mutation Killing for Ensuring Monotonicity: A Technical Report](https://arxiv.org/abs/2507.11199)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: 本文提出了一种基于Fisher精确检验的统计突变杀死新方法，解决了DeepCrime统计测试中违反单调性的问题。


<details>
  <summary>Details</summary>
Motivation: DeepCrime的统计突变杀死标准虽然强大，但在扩展测试集时可能违反单调性，影响结果可靠性。为解决这一问题，本文提出改进方案。

Method: 提出基于Fisher精确检验的统计突变杀死新方法，保持统计严谨性的同时确保单调性。

Result: 新方法在保持统计严谨性的同时解决了单调性问题。

Conclusion: 基于Fisher精确检验的新方法有效解决了DeepCrime中的单调性问题，提升了突变测试的可靠性。

Abstract: Mutation testing has emerged as a powerful technique for evaluating the
effectiveness of test suites for Deep Neural Networks. Among existing
approaches, the statistical mutant killing criterion of DeepCrime has leveraged
statistical testing to determine whether a mutant significantly differs from
the original model. However, it suffers from a critical limitation: it violates
the monotonicity property, meaning that expanding a test set may result in
previously killed mutants no longer being classified as killed. In this
technical report, we propose a new formulation of statistical mutant killing
based on Fisher exact test that preserves the statistical rigour of it while
ensuring monotonicity.

</details>


### [18] [An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling](https://arxiv.org/abs/2507.11272)
*Anh Nguyen-Duc,Chien Vu Manh,Bao Anh Tran,Viet Phuong Ngo,Luan Le Chi,Anh Quang Nguyen*

Main category: cs.SE

TL;DR: MARAUS是一个针对越南高等教育招生咨询的对话AI平台，通过结合混合检索、多智能体协同和LLM生成，显著提升了招生咨询的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）解决方案多为原型或合成基准测试，无法满足真实世界的招生咨询需求。MARAUS旨在填补这一空白。

Method: 结合混合检索、多智能体协同和LLM生成技术，开发适用于真实世界大学招生的系统。

Result: 在真实用户交互中，准确性达92%，幻觉率从15%降至1.45%，响应时间低于4秒，部署成本仅为11.58美元。

Conclusion: MARAUS为低资源教育环境中部署智能检索增强生成系统提供了可行的解决方案。

Abstract: This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University
Admission System), a real-world deployment of a conversational AI platform for
higher education admissions counseling in Vietnam. While large language models
(LLMs) offer potential for automating advisory tasks, most existing solutions
remain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap
by combining hybrid retrieval, multi-agent orchestration, and LLM-based
generation into a system tailored for real-world university admissions. In
collaboration with the University of Transport Technology (UTT) in Hanoi, we
conducted a two-phase study involving technical development and real-world
evaluation. MARAUS processed over 6,000 actual user interactions, spanning six
categories of queries. Results show substantial improvements over LLM-only
baselines: on average 92 percent accuracy, hallucination rates reduced from 15
precent to 1.45 percent, and average response times below 4 seconds. The system
operated cost-effectively, with a two-week deployment cost of 11.58 USD using
GPT-4o mini. This work provides actionable insights for the deployment of
agentic RAG systems in low-resource educational settings.

</details>


### [19] [RefModel: Detecting Refactorings using Foundation Models](https://arxiv.org/abs/2507.11346)
*Pedro Simões,Rohit Gheyi,Rian Melo,Jonhnanthan Oliveira,Márcio Ribeiro,Wesley K. G. Assunção*

Main category: cs.SE

TL;DR: 研究探讨了利用基础模型（如RefModel）检测代码重构的可行性，相比传统工具（如RefactoringMiner），其在某些情况下表现更优，并能自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 传统重构检测工具依赖复杂规则和静态分析，扩展性和通用性不足，研究旨在探索基础模型的潜力。

Method: 使用Phi4-14B、Claude 3.5 Sonnet等模型，评估其在人工生成和真实项目数据集上的表现，并与传统工具对比。

Result: 模型在真实项目中识别了97%的重构，且能推广到其他语言（如Python、Golang），表现优于传统工具。

Conclusion: 基础模型在重构检测中具有竞争力，提供自然语言支持，简化了定义流程，扩展性更强。

Abstract: Refactoring is a common software engineering practice that improves code
quality without altering program behavior. Although tools like ReExtractor+,
RefactoringMiner, and RefDiff have been developed to detect refactorings
automatically, they rely on complex rule definitions and static analysis,
making them difficult to extend and generalize to other programming languages.
In this paper, we investigate the viability of using foundation models for
refactoring detection, implemented in a tool named RefModel. We evaluate
Phi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation
transformations applied to artificially generated Java programs, covering
widely-used refactoring types. We also extend our evaluation by including
Gemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world
refactorings extracted from four open-source projects. These models are
compared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is
competitive with, and in some cases outperform, traditional tools. In
real-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified
97% of all refactorings, surpassing the best-performing static-analysis-based
tools. The models showed encouraging generalization to Python and Golang. They
provide natural language explanations and require only a single sentence to
define each refactoring type.

</details>


### [20] [Security Debt in Practice: Nuanced Insights from Practitioners](https://arxiv.org/abs/2507.11362)
*Chaima Boufaied,Taher Ghaleb,Zainab Masood*

Main category: cs.SE

TL;DR: 论文通过半结构化访谈研究了软件从业者对安全债务（SDs）的认知、管理与沟通方式，发现实践中的差异，强调需加强安全实践在开发生命周期中的整合。


<details>
  <summary>Details</summary>
Motivation: 当前软件行业中，紧张的截止日期和有限资源导致安全漏洞积累，但缺乏关于从业者如何实际处理安全债务的实证研究。

Method: 采用定性实证研究，对22名来自不同角色、组织和国家的软件从业者进行半结构化访谈，探讨四个研究问题。

Result: 研究发现从业者对SDs的认知和管理行为存在差异，部分优先交付速度，部分坚持安全优先，需改进安全实践整合与平衡资源分配。

Conclusion: 需在软件开发生命周期中更一致地整合安全实践，平衡资源与安全任务，同时关注CIA三要素。

Abstract: With the increasing reliance on software and automation nowadays, tight
deadlines, limited resources, and prioritization of functionality over security
can lead to insecure coding practices. When not handled properly, these
constraints cause unaddressed security vulnerabilities to accumulate over time,
forming Security Debts (SDs). Despite their critical importance, there is
limited empirical evidence on how software practitioners perceive, manage, and
communicate SDs in real-world settings. In this paper, we present a qualitative
empirical study based on semi-structured interviews with 22 software
practitioners across various roles, organizations, and countries. We address
four research questions: i) we assess software practitioners' knowledge of SDs
and awareness of associated security risks, ii) we investigate their behavior
towards SDs, iii) we explore common tools and strategies used to mitigate SDs,
and iv) we analyze how security risks are communicated within teams and to
decision makers. We observe variations in how practitioners perceive and manage
SDs, with some prioritizing delivery speed over security, while others
consistently maintain security as a priority. Our findings emphasize the need
for stronger integration of security practices across the Software Development
Life Cycle (SDLC), more consistent use of mitigation strategies, better
balancing of deadlines, resources, and security-related tasks, with attention
to the Confidentiality, Integrity, and Availability (CIA) triad.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [21] [Stream programs are monoid homomorphisms with state](https://arxiv.org/abs/2507.10799)
*Tyler Hou,Michael Arntzenius,Max Willsey*

Main category: cs.PL

TL;DR: 该论文提出了一类确定性流函数，并展示了它们可以作为态射实现为“状态”幺半群，简化了流程序优化的语义框架条件。


<details>
  <summary>Details</summary>
Motivation: 旨在简化流程序优化的语义框架条件，同时保留对丰富等式推理的支持。

Method: 通过定义一类确定性流函数，将其实现为“状态”幺半群的态射。

Result: 展示了该方法在分区数据库连接、分层否定和简化TCP模型等案例中的有效性。

Conclusion: 提出的框架简化了条件，同时支持复杂的等式推理，适用于多种数据流程序场景。

Abstract: We define a broad class of deterministic stream functions and show they can
be implemented as homomorphisms into a "state" monoid. The homomorphism laws
are simpler than the conditions of previous semantic frameworks for stream
program optimization, yet retain support for rich equational reasoning over
expressive dataflow programs, including sequential composition, parallel
composition, and feedback. We demonstrate this using examples of partitioned
database joins, stratified negation, and a simplified model of TCP.

</details>


### [22] [The downgrading semantics of memory safety](https://arxiv.org/abs/2507.11282)
*René Rydhof Hansen,Andreas Stenbæk Larsen,Aslan Askarov*

Main category: cs.PL

TL;DR: 本文提出了一种渐进的分配器独立性概念，用于更准确地捕捉内存安全的分配器特定方面，并通过信息流机制解决了内存不足和指针转换等问题。


<details>
  <summary>Details</summary>
Motivation: 传统内存安全通常以‘不发生坏事’为目标，被认为缺乏原则性。本文试图通过连接内存安全与非干扰性，提出更语义化的内存安全定义。

Method: 研究采用低级语言模型，包含malloc和free操作的平面内存模型，提出渐进的分配器独立性概念，并利用信息流技术处理内存不足和指针转换。

Result: 成功定义了渐进的分配器独立性，并展示了如何通过信息流机制处理分配器对程序执行的潜在影响。

Conclusion: 提出的方法为内存安全提供了更语义化和原则性的定义，尤其在处理分配器相关问题时表现出色。

Abstract: Memory safety is traditionally characterized in terms of bad things that
cannot happen, an approach that is often criticized as unprincipled. Prior work
suggest a connection between memory safety and noninterference, but no
satisfactory semantic notion of memory safety is currently known.
  This work proposes a notion of gradual allocator independence that accurately
captures many allocator-specific aspects of memory safety. We consider a
low-level language with access to an allocator that provides malloc and free
primitives in a flat memory model. Pointers are just integers, and as such it
is trivial to write memory-unsafe programs. The basic intuition of gradual
allocator independence is that of noninterference, namely that allocators must
not influence program execution. This intuition is refined in two important
ways to account for the allocators running out-of-memory and for programs to
have pointer-to-integer casts. The key insight of the definition is to treat
these extensions as forms of downgrading and give them satisfactory technical
treatment using the state-of-the-art information flow machinery.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [23] [MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving](https://arxiv.org/abs/2507.11507)
*Ruihao Li,Shagnik Pal,Vineeth Narayan Pullu,Prasoon Sinha,Jeeho Ryoo,Lizy K. John,Neeraja J. Yadwadkar*

Main category: cs.OS

TL;DR: MIRAGE通过重新映射模型参数的内存来优化KV缓存，避免了CPU与GPU之间的缓存交换，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统的KV缓存优化方法通过CPU-offloading交换缓存，但动态更新导致高内存流量。MIRAGE利用模型参数内存的恒定特性，避免交换开销。

Method: MIRAGE重新分配模型参数的内存用于KV缓存，尤其适合多租户环境中回收未激活模型的内存。

Result: MIRAGE在延迟和吞吐量上显著优于现有方案，最高减少82.5%的尾部延迟，提升86.7%的吞吐量。

Conclusion: 通过利用模型参数内存的恒定性和现代硬件的高带宽，MIRAGE为LLM推理提供了高效的内存优化方案。

Abstract: KV cache accelerates LLM inference by avoiding redundant computation, at the
expense of memory. To support larger KV caches, prior work extends GPU memory
with CPU memory via CPU-offloading. This involves swapping KV cache between GPU
and CPU memory. However, because the cache updates dynamically, such swapping
incurs high CPU memory traffic. We make a key observation that model parameters
remain constant during runtime, unlike the dynamically updated KV cache.
Building on this, we introduce MIRAGE, which avoids KV cache swapping by
remapping, and thereby repurposing, the memory allocated to model parameters
for KV cache. This parameter remapping is especially beneficial in multi-tenant
environments, where the memory used for the parameters of the inactive models
can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth
offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we
show that MIRAGE significantly outperforms state-of-the-art solutions,
achieving a reduction of 44.8%-82.5% in tail time-between-token latency,
20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher
throughput compared to vLLM.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [24] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 论文提出了一种结合轻量级语言模型（LiLM）和关系数据库（RDB）的新方法LiLM-RDB-SFC，用于优化服务功能链（SFC）和虚拟网络功能（VNF）的部署。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习（DRL）在网络决策中对结构化数据和固定动作规则的依赖，提升网络环境的适应性和响应能力。

Method: 使用两种LiLM（BART和FLAN-T5）结合RDB，通过自然语言处理网络状态查询，为DRL提供高效SFC配置指导。

Result: FLAN-T5优于BART，测试损失更低（0.00161 vs 0.00734），准确率更高（94.79% vs 80.2%），处理时间更短（2h 2min vs 2h 38min）。FLAN-T5与大模型SQLCoder准确率相当，但处理时间减少96%。

Conclusion: LiLM-RDB-SFC通过轻量级语言模型显著提升了SFC管理的效率和准确性，尤其在处理时间和适应性方面表现突出。

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


### [25] [Arcturus: A Cloud Overlay Network for Global Accelerator with Enhanced Performance and Stability](https://arxiv.org/abs/2507.10928)
*Matthew Yang Liu,Chuang Chen,Pengcheng Lv,Hui Guo,Yanan Zhang,Cong Wang,Yusen Li,Zhenyu Li,Yu-Chu Tian*

Main category: cs.NI

TL;DR: Arcturus是一种云原生全局加速器框架，通过利用低成本、异构云资源，动态构建加速网络，平衡性能、稳定性和资源效率，显著优于商业GA服务。


<details>
  <summary>Details</summary>
Motivation: 现有全局加速器服务与特定云提供商紧密绑定，导致成本高、部署刚性且灵活性受限。

Method: 采用了双平面设计：转发平面构建具有自适应控制的代理网络，调度平面通过轻量级量化优化协调负载和路由。

Result: 在数百万RPS下评估显示，Arcturus的加速性能提升1.7倍，成本降低71%，资源效率保持在80%以上。

Conclusion: Arcturus通过动态优化和异构资源利用，实现了高效、低成本的全局加速服务。

Abstract: Global Accelerator (GA) services play a vital role in ensuring low-latency,
high-reliability communication for real-time interactive applications. However,
existing GA offerings are tightly bound to specific cloud providers, resulting
in high costs, rigid deployment, and limited flexibility, especially for
large-scale or budget-sensitive deployments. Arcturus is a cloud-native GA
framework that revisits the design of GA systems by leveraging low-cost,
heterogeneous cloud resources across multiple providers. Rather than relying on
fixed, high-end infrastructure, Arcturus dynamically constructs its
acceleration network and balances performance, stability, and resource
efficiency. To achieve this, Arcturus introduces a two-plane design: a
forwarding plane that builds a proxy network with adaptive control, and a
scheduling plane that coordinates load and routing through lightweight,
quantitative optimization. Evaluations under millions of RPS show that Arcturus
outperforms commercial GA services by up to 1.7X in acceleration performance,
reduces cost by 71%, and maintains over 80% resource efficiency--demonstrating
efficient use of cloud resources at scale.

</details>


### [26] [SIMCODE: A Benchmark for Natural Language to ns-3 Network Simulation Code Generation](https://arxiv.org/abs/2507.11014)
*Tasnim Ahmed,Mirza Mohammad Azwad,Salimur Choudhury*

Main category: cs.NI

TL;DR: SIMCODE是首个评估LLMs生成ns-3仿真代码能力的基准工具，包含400个任务，测试结果显示GPT-4.1表现最佳，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在生成领域特定环境（如ns-3）仿真脚本方面的能力，填补现有工具在系统性评估上的不足。

Method: 引入SIMCODE基准，包含不同难度任务，评估三种LLMs（Gemini-2.0、GPT-4.1、Qwen-3）在六种提示技术下的表现，并研究任务特定微调的影响。

Result: GPT-4.1表现最佳，但执行准确率仍有提升空间，主要错误为缺失头文件和API不匹配。

Conclusion: SIMCODE为评估LLMs和领域感知生成系统奠定了基础，但LLMs在生成仿真代码方面仍需改进。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation across various domains. However, their effectiveness in
generating simulation scripts for domain-specific environments like ns-3
remains underexplored. Despite the growing interest in automating network
simulations, existing tools primarily focus on interactive automation over
rigorous evaluation. To facilitate systematic evaluation, we introduce SIMCODE,
the first benchmark to evaluate LLMs' ability to generate ns-3 simulation code
from natural language. SIMCODE includes 400 tasks across introductory,
intermediate, and advanced levels, with solutions and test cases. Using
SIMCODE, we evaluate three prominent LLMs, Gemini-2.0, GPT-4.1, and Qwen-3,
across six prompt techniques. Furthermore, investigating task-specific
fine-tuning's impact reveals that while GPT-4.1 outperforms others, execution
accuracy remains modest, with substantial room for improvement. Error analysis
identifies missing headers and API mismatches as dominant failures.
Nevertheless, SIMCODE provides a foundational step toward evaluating LLMs and
research in domain-aware generative systems.

</details>


### [27] [Graph-based Fingerprint Update Using Unlabelled WiFi Signals](https://arxiv.org/abs/2507.11038)
*Ka Ho Chiu,Handi Yin,Weipeng Zhuo,Chul-Ho Lee,S. -H. Gary Chan*

Main category: cs.NI

TL;DR: GUFU是一种基于图神经网络的新型方法，用于利用未标记信号更新WiFi指纹数据库，显著提高了指纹适应性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: WiFi信号环境随时间变化，现有方法未能有效利用新收集的未标记信号和新增AP的特征来更新指纹数据库。

Method: GUFU利用图神经网络和链接预测算法，通过未标记信号和新增AP重新训练增量网络，并更新指定位置的信号向量。

Result: 在四个大型代表性场所的实验中，GUFU在RSS值和位置预测上的误差分别减少了21.4%和29.8%。

Conclusion: GUFU是一种有效的WiFi指纹更新方法，显著优于现有技术。

Abstract: WiFi received signal strength (RSS) environment evolves over time due to
movement of access points (APs), AP power adjustment, installation and removal
of APs, etc. We study how to effectively update an existing database of
fingerprints, defined as the RSS values of APs at designated locations, using a
batch of newly collected unlabelled (possibly crowdsourced) WiFi signals. Prior
art either estimates the locations of the new signals without updating the
existing fingerprints or filters out the new APs without sufficiently embracing
their features. To address that, we propose GUFU, a novel effective graph-based
approach to update WiFi fingerprints using unlabelled signals with possibly new
APs. Based on the observation that similar signal vectors likely imply physical
proximity, GUFU employs a graph neural network (GNN) and a link prediction
algorithm to retrain an incremental network given the new signals and APs.
After the retraining, it then updates the signal vectors at the designated
locations. Through extensive experiments in four large representative sites,
GUFU is shown to achieve remarkably higher fingerprint adaptivity as compared
with other state-of-the-art approaches, with error reduction of 21.4% and 29.8%
in RSS values and location prediction, respectively.

</details>


### [28] [Improving Wi-Fi Network Performance Prediction with Deep Learning Models](https://arxiv.org/abs/2507.11168)
*Gabriele Formis,Amanda Ericson,Stefan Forsstrom,Kyi Thar,Gianluca Cena,Stefano Scanzio*

Main category: cs.NI

TL;DR: 该论文探讨了利用机器学习预测Wi-Fi网络信道质量（帧交付比）的方法，以提升工业应用的网络可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 工业应用对无线网络的鲁棒性、可靠性和确定性需求增加，推动了新方法的创新。

Method: 使用卷积神经网络和长短期记忆网络在真实Wi-Fi数据集上预测帧交付比，比较了预测精度和计算复杂度。

Result: 卷积神经网络在预测帧交付比时稍逊于其他模型，但在CPU和内存使用上更高效，适合嵌入式系统。

Conclusion: 论文证明机器学习可有效预测信道质量，卷积神经网络因其高效性更适用于工业场景。

Abstract: The increasing need for robustness, reliability, and determinism in wireless
networks for industrial and mission-critical applications is the driver for the
growth of new innovative methods. The study presented in this work makes use of
machine learning techniques to predict channel quality in a Wi-Fi network in
terms of the frame delivery ratio. Predictions can be used proactively to
adjust communication parameters at runtime and optimize network operations for
industrial applications. Methods including convolutional neural networks and
long short-term memory were analyzed on datasets acquired from a real Wi-Fi
setup across multiple channels. The models were compared in terms of prediction
accuracy and computational complexity. Results show that the frame delivery
ratio can be reliably predicted, and convolutional neural networks, although
slightly less effective than other models, are more efficient in terms of CPU
usage and memory consumption. This enhances the model's usability on embedded
and industrial systems.

</details>


### [29] [Resilient Time-Sensitive Networking for Industrial IoT: Configuration and Fault-Tolerance Evaluation](https://arxiv.org/abs/2507.11250)
*Mohamed Seliem,Dirk Pesch,Utz Roedig,Cormac Sreenan*

Main category: cs.NI

TL;DR: IN2C是一个基于OMNeT++/INET的模块化仿真框架，用于评估TSN在工业系统中的容错能力，通过XML驱动的故障注入和FRER技术减少数据包丢失并实现快速恢复。


<details>
  <summary>Details</summary>
Motivation: 工业系统对TSN的容错能力评估需求迫切，但在实际故障条件下的性能分析仍具挑战性。

Method: 开发IN2C框架，整合TSN核心功能（如同步、流量整形、FRER等），通过XML注入链路和节点故障，评估四种故障场景。

Result: FRER技术能消除数据包丢失并实现亚毫秒级恢复，但增加了2-3倍的链路利用率。

Conclusion: TSN在带宽受限的工业环境中部署时需权衡冗余与链路利用率，IN2C框架为实际应用提供了实用指导。

Abstract: Time-Sensitive Networking (TSN) is increasingly adopted in industrial systems
to meet strict latency, jitter, and reliability requirements. However,
evaluating TSN's fault tolerance under realistic failure conditions remains
challenging. This paper presents IN2C, a modular OMNeT++/INET-based simulation
framework that models two synchronized production cells connected to
centralized infrastructure. IN2C integrates core TSN features, including time
synchronization, traffic shaping, per-stream filtering, and Frame Replication
and Elimination for Redundancy (FRER), alongside XML-driven fault injection for
link and node failures. Four fault scenarios are evaluated to compare TSN
performance with and without redundancy. Results show that FRER eliminates
packet loss and achieves submillisecond recovery, though with 2-3x higher link
utilization. These findings offer practical guidance for deploying TSN in
bandwidth-constrained industrial environments.

</details>


### [30] [JamShield: A Machine Learning Detection System for Over-the-Air Jamming Attacks](https://arxiv.org/abs/2507.11483)
*Ioannis Panitsas,Yagmur Yigit,Leandros Tassiulas,Leandros Maglaras,Berk Canberk*

Main category: cs.NI

TL;DR: JamShield是一种动态干扰检测系统，通过混合特征选择和自动分类模块，显著提高了无线网络中干扰检测的性能。


<details>
  <summary>Details</summary>
Motivation: 无线网络因共享通信媒介易受干扰攻击，现有检测方法依赖仿真或专有数据，难以在实际场景中有效工作。

Method: JamShield利用自收集和公开数据集，采用混合特征选择和实时动态调整分类算法的方法。

Result: 实验结果显示，JamShield在检测率、精确度和召回率上优于现有技术，同时减少误报和漏检。

Conclusion: JamShield为实际无线网络中的干扰攻击提供了可靠且鲁棒的检测解决方案。

Abstract: Wireless networks are vulnerable to jamming attacks due to the shared
communication medium, which can severely degrade performance and disrupt
services. Despite extensive research, current jamming detection methods often
rely on simulated data or proprietary over-the-air datasets with limited
cross-layer features, failing to accurately represent the real state of a
network and thus limiting their effectiveness in real-world scenarios. To
address these challenges, we introduce JamShield, a dynamic jamming detection
system trained on our own collected over-the-air and publicly available
dataset. It utilizes hybrid feature selection to prioritize relevant features
for accurate and efficient detection. Additionally, it includes an
auto-classification module that dynamically adjusts the classification
algorithm in real-time based on current network conditions. Our experimental
results demonstrate significant improvements in detection rate, precision, and
recall, along with reduced false alarms and misdetections compared to
state-of-the-art detection algorithms, making JamShield a robust and reliable
solution for detecting jamming attacks in real-world wireless networks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [31] [MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](https://arxiv.org/abs/2507.10859)
*Ramaneswaran Selvakumar,Ashish Seth,Nishit Anand,Utkarsh Tyagi,Sonal Kumar,Sreyan Ghosh,Dinesh Manocha*

Main category: cs.MM

TL;DR: MultiVox是一个新的多模态语音助手基准测试，旨在评估模型如何结合语音和视觉线索，特别是副语言特征，以实现真正的多模态理解。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能全面评估大语言模型在多模态输入（如语音和视觉）下的上下文感知能力，尤其是在副语言特征和环境声音方面的表现。

Method: 提出MultiVox基准，包含1000个人工注释的语音对话，涵盖多样化的副语言特征和视觉线索，并评估了9个先进模型。

Result: 尽管人类在这些任务中表现出色，但现有模型难以生成上下文基础扎实的响应。

Conclusion: MultiVox揭示了当前模型在多模态理解和上下文感知响应方面的不足，为未来研究提供了方向。

Abstract: The rapid progress of Large Language Models (LLMs) has empowered omni models
to act as voice assistants capable of understanding spoken dialogues. These
models can process multimodal inputs beyond text, such as speech and visual
data, enabling more context-aware interactions. However, current benchmarks
fall short in comprehensively evaluating how well these models generate
context-aware responses, particularly when it comes to implicitly understanding
fine-grained speech characteristics, such as pitch, emotion, timbre, and volume
or the environmental acoustic context such as background sounds. Additionally,
they inadequately assess the ability of models to align paralinguistic cues
with complementary visual signals to inform their responses. To address these
gaps, we introduce MultiVox, the first omni voice assistant benchmark designed
to evaluate the ability of voice assistants to integrate spoken and visual cues
including paralinguistic speech features for truly multimodal understanding.
Specifically, MultiVox includes 1000 human-annotated and recorded speech
dialogues that encompass diverse paralinguistic features and a range of visual
cues such as images and videos. Our evaluation on 9 state-of-the-art models
reveals that, although humans excel at these tasks, current models consistently
struggle to produce contextually grounded responses.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [32] [$ε$-Distance via Lévy-Prokhorov Lifting](https://arxiv.org/abs/2507.10732)
*Josée Desharnais,Ana Sokolova*

Main category: cs.LO

TL;DR: 摘要研究了基于ϵ-距离的行为伪度量，展示了它是某个泛函的最大不动点，并提供了函子。此外，还探讨了其与Lévy-Prokhorov距离的联系，以及ϵ-耦合和ϵ-互模拟的共代数特性。


<details>
  <summary>Details</summary>
Motivation: 研究ϵ-距离的行为伪度量，因其直观易懂、能关联概念相近的系统（如不完美实现与规范），且有自然的ϵ-耦合概念和易于计算的特点。

Method: 证明ϵ-距离是泛函的最大不动点，并用Lévy-Prokhorov距离替换Kantorovich距离来构造函子。进一步分析了ϵ-耦合和ϵ-互模拟的共代数特性。

Result: 展示ϵ-距离的性质及其与Lévy-Prokhorov距离的联系，提供了其共代数特性的新视角。

Conclusion: ϵ-距离不仅直观且实用，还具有理论上的优美性质，为行为伪度量的研究提供了新的方向。

Abstract: The most studied and accepted pseudometric for probabilistic processes is one
based on the Kantorovich distance between distributions. It comes with many
theoretical and motivating results, in particular it is the fixpoint of a given
functional and defines a functor on (complete) pseudometric spaces.
  Other notions of behavioural pseudometrics have also been proposed, one of
them ($\epsilon$-distance) based on $\epsilon$-bisimulation.
$\epsilon$-Distance has the advantages that it is intuitively easy to
understand, it relates systems that are conceptually close (for example, an
imperfect implementation is close to its specification), and it comes equipped
with a natural notion of $\epsilon$-coupling. Finally, this distance is easy to
compute.
  We show that $\epsilon$-distance is also the greatest fixpoint of a
functional and provides a functor. The latter is obtained by replacing the
Kantorovich distance in the lifting functor with the L\'evy-Prokhorov distance.
In addition, we show that $\epsilon$-couplings and $\epsilon$-bisimulations
have an appealing coalgebraic characterization.

</details>


### [33] [Reasoning about Medical Triage Optimization with Logic Programming](https://arxiv.org/abs/2507.10781)
*Jaikrishna Manojkumar Patil,Adam Chapman,Richard Knuszka,John Chapman,Paulo Shakarian*

Main category: cs.LO

TL;DR: 该论文提出了一种逻辑编程框架，用于协调医疗决策中多个优化问题的变体，并通过实验结果证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 通过逻辑编程支持高风险医疗决策，特别是在快速且可解释的资源分配场景中，如医疗后送(MEDEVAC)。

Method: 构建一个逻辑编程层，协调多个优化问题的构造与评估，将解决方案转化为逻辑事实以支持进一步推理。

Result: 实验显示，该框架平均减少了35.75%的伤亡率，同时通过直观界面提升了用户决策效率。

Conclusion: 逻辑编程可作为模块化、可解释且操作高效的优化基础，适用于关键任务领域。

Abstract: We present a logic programming framework that orchestrates multiple variants
of an optimization problem and reasons about their results to support
high-stakes medical decision-making. The logic programming layer coordinates
the construction and evaluation of multiple optimization formulations,
translating solutions into logical facts that support further symbolic
reasoning and ensure efficient resource allocation-specifically targeting the
"right patient, right platform, right escort, right time, right destination"
principle. This capability is integrated into GuardianTwin, a decision support
system for Forward Medical Evacuation (MEDEVAC), where rapid and explainable
resource allocation is critical. Through a series of experiments, our framework
demonstrates an average reduction in casualties by 35.75 % compared to standard
baselines. Additionally, we explore how users engage with the system via an
intuitive interface that delivers explainable insights, ultimately enhancing
decision-making in critical situations. This work demonstrates how logic
programming can serve as a foundation for modular, interpretable, and
operationally effective optimization in mission-critical domains.

</details>


### [34] [Execution and monitoring of HOA automata with HOAX](https://arxiv.org/abs/2507.11126)
*Luca Di Stefano*

Main category: cs.LO

TL;DR: Hoax 是一个用于执行 HOA 格式的 ω-自动机的工具，利用陷阱集实现运行时监控，支持非奇偶性接受条件。工具开源且高度可配置。


<details>
  <summary>Details</summary>
Motivation: 为解决 ω-自动机运行时监控的问题，尤其是支持 HOA 格式中的多种接受条件。

Method: 利用陷阱集技术监控非奇偶性接受条件，识别不可监控的“丑陋前缀”。

Result: 工具在锁获取场景中与 PyContract 进行了对比，展示了其有效性。

Conclusion: Hoax 是一个高效、灵活的开源工具，适用于 ω-自动机的运行时监控。

Abstract: We present a tool called Hoax for the execution of {\omega}-automata
expressed in the popular HOA format. The tool leverages the notion of trap sets
to enable runtime monitoring of any (non-parity) acceptance condition supported
by the format. When the automaton is not monitorable, the tool may still be
able to recognise so-called ugly prefixes, and determine that no further
observation will ever lead to a conclusive verdict. The tool is open-source and
highly configurable. We present its formal foundations, its design, and compare
it against the trace analyser PyContract on a lock acquisition scenario.

</details>


### [35] [Interpolation and Quantifiers in Ortholattices](https://arxiv.org/abs/2507.11141)
*Simon Guilloud,Sankalp Gambhir,Viktor Kunčak*

Main category: cs.LO

TL;DR: 该论文研究了正交逻辑中的量词和插值性质，提出了一个基于序列的证明系统，证明了其有效性和完备性，并展示了插值的存在性和高效算法。


<details>
  <summary>Details</summary>
Motivation: 正交逻辑是经典逻辑的非分配弱化形式，具有二次时间决策过程。研究其量词和插值性质，有助于验证算法的快速不可达性证明。

Method: 提出一个基于序列的证明系统，并证明其在完备正交格类中的有效性和完备性。展示了插值的存在性，并提供了高效计算插值的算法。

Result: 正交逻辑不普遍支持量词消除，但总是存在插值，并提供了高效计算插值的算法。

Conclusion: 研究结果为验证算法中的不可达性快速证明提供了理论支持，展示了正交逻辑在形式验证中的潜在应用价值。

Abstract: We study quantifiers and interpolation properties in \emph{orthologic}, a
non-distributive weakening of classical logic that is sound for formula
validity with respect to classical logic, yet has a quadratic-time decision
procedure. We present a sequent-based proof system for quantified orthologic,
which we prove sound and complete for the class of all complete ortholattices.
We show that orthologic does not admit quantifier elimination in general.
Despite that, we show that interpolants always exist in orthologic. We give an
algorithm to compute interpolants efficiently. We expect our result to be
useful to quickly establish unreachability as a component of verification
algorithms.

</details>


### [36] [LISA -- A Modern Proof System](https://arxiv.org/abs/2507.11167)
*Simon Guilloud,Sankalp Gambhir,Viktor Kunčak*

Main category: cs.LO

TL;DR: LISA是一个用于构建示意一阶逻辑和公理集合论证明的证明系统与助手，其核心是一阶逻辑的证明检查器，支持多项式时间验证，并利用正交格的性质优化证明。


<details>
  <summary>Details</summary>
Motivation: 开发LISA旨在提供一个高效、用户友好的工具，用于构建和验证复杂的逻辑和集合论证明。

Method: LISA的逻辑核心是一阶逻辑证明检查器，支持多项式时间验证和正交格公理，同时提供领域特定语言和开发工具。

Result: 系统成功实现了高效证明检查，并能生成命题重言式的证明，同时展示了集合论的形式化验证能力。

Conclusion: LISA作为一个高效且灵活的证明系统，为逻辑和集合论证明的开发与验证提供了有力支持。

Abstract: We present LISA, a proof system and proof assistant for constructing proofs
in schematic first-order logic and axiomatic set theory. The logical kernel of
the system is a proof checker for first-order logic with equality and schematic
predicate and function symbols. It implements polynomial-time proof checking
and uses the axioms of ortholattices (which implies the irrelevance of the
order of conjuncts and disjuncts and additional propositional laws). The kernel
supports the notion of theorems (whose proofs are not expanded), as well as
definitions of predicate symbols and objects whose unique existence is proven.
A domain-specific language enables construction of proofs and development of
proof tactics with user-friendly tools and presentation, while remaining within
the general-purpose language, Scala. We describe the LISA proof system and
illustrate the flavour and the level of abstraction of proofs written in LISA.
This includes a proof-generating tactic for propositional tautologies,
leveraging the ortholattice properties to reduce the size of proofs. We also
present early formalization of set theory in LISA, including Cantor's theorem.

</details>


### [37] [Cancellative Convex Semilattices](https://arxiv.org/abs/2507.11186)
*Ana Sokolova,Harald Woracek*

Main category: cs.LO

TL;DR: 论文研究了凸半格的取消性条件，证明其与Riesz空间（格序向量空间）的凸子集同构的条件等价。


<details>
  <summary>Details</summary>
Motivation: 凸半格作为概率和非确定性的合适代数结构，近年来受到关注。研究其取消性条件有助于理解其代数特性和应用潜力。

Method: 通过分析凸半格的取消性条件，并将其与向量空间的凸子集同构性进行类比，提出并证明了凸半格在Riesz空间中的表征定理。

Result: 证明了一个凸半格是取消性的当且仅当它与Riesz空间的凸子集同构。

Conclusion: 该结果为凸半格的代数结构和几何表征提供了理论支持，进一步拓展了其在概率和非确定性领域的应用可能性。

Abstract: Convex semilattices are algebras that are at the same time a convex algebra
and a semilattice, together with a distributivity axiom. These algebras have
attracted some attention in the last years as suitable algebras for probability
and nondeterminism, in particular by being the Eilenberg-Moore algebras of the
nonempty finitely-generated convex subsets of the distributions monad.
  A convex semilattice is cancellative if the underlying convex algebra is
cancellative. Cancellative convex algebras have been characterized by M. H.
Stone and by H. Kneser: A convex algebra is cancellative if and only if it is
isomorphic to a convex subset of a vector space (with canonical convex algebra
operations).
  We prove an analogous theorem for convex semilattices: A convex semilattice
is cancellative if and only if it is isomorphic to a convex subset of a Riesz
space, i.e., a lattice-ordered vector space (with canonical convex semilattice
operations).

</details>


### [38] [Complexity of some modal logics of density (extended version)](https://arxiv.org/abs/2507.11238)
*Philippe Balbiani,Olivier Gasquet*

Main category: cs.LO

TL;DR: 通过选择性过滤论证，证明了单模态密度逻辑的可满足性问题在$EXPTIME$内；通过类表方法，证明了双模态弱密度逻辑的可满足性问题在$PSPACE$内。


<details>
  <summary>Details</summary>
Motivation: 研究模态逻辑的可满足性问题，尤其是密度逻辑的复杂度分类。

Method: 单模态密度逻辑使用选择性过滤论证；双模态弱密度逻辑采用类表方法。

Result: 单模态密度逻辑的可满足性属于$EXPTIME$；双模态弱密度逻辑的可满足性属于$PSPACE$。

Conclusion: 证明了两种密度逻辑的可满足性分别在$EXPTIME$和$PSPACE$内，为模态逻辑复杂度提供了新结果。

Abstract: By using a selective filtration argument, we prove that the satisfiability
problem of the unimodal logic of density is in $EXPTIME$. By using a
tableau-like approach, we prove that the satisfiability problem of the bimodal
logic of weak density is in $PSPACE$.

</details>


### [39] [Path-filtration for modal logics applied to revisiting quasi-dense logics](https://arxiv.org/abs/2507.11258)
*Olivier Gasquet*

Main category: cs.LO

TL;DR: 本文纠正了Lyon和Ostropolski-Nalewaja关于准稠密模态逻辑可判定性的证明错误，并提出了一种新的基于路径的过滤方法，将问题的复杂性提升至NEXPTIME。


<details>
  <summary>Details</summary>
Motivation: Lyon和Ostropolski-Nalewaja的研究中存在重大缺陷，无法修复，因此需要提供正确的证明方法。

Method: 引入了一种新的基于规范模型中路径的过滤方法，简化并更正了原证明。

Result: 证明了准稠密模态逻辑的可判定性，并将复杂性从EXPSPACE改进为NEXPTIME。

Conclusion: 通过新方法，成功解决了原论文中的问题，并提升了理论效率。

Abstract: In https://arxiv.org/pdf/2405.10094 (also published at LICS'24 conference),
Lyon and Ostropolski-Nalewaja answer the question of the decidability of
quasi-dense modallogics, and give an upper bound in EXPSPACE. Unfortunately,
their intricate proof contains a major flaw that cannot be fixed, leaving the
question wide open. In this paper we provide a correct and rather simple and
direct proof of it by introducing a new variant of the well-know filtration
method based on paths in a canonical model and improve the hypothetical
membership to membership NEXPTIME.

</details>


### [40] [SC-TPTP: An Extension of the TPTP Derivation Format for Sequent-Based Calculus](https://arxiv.org/abs/2507.11349)
*Julie Cailler,Simon Guilloud*

Main category: cs.LO

TL;DR: 为了在不同证明系统间交换证明，特别是从自动定理证明器（ATP）到交互式定理证明器（ITP），作者提出了SC-TPTP格式，扩展了TPTP派生文本格式。


<details>
  <summary>Details</summary>
Motivation: 为了在不同证明系统间转移证明，特别是从ATP到ITP。

Method: 通过扩展TPTP派生格式，专注于序列形式主义，提供了高细节的证明描述。

Result: 成功定义了SC-TPTP格式，并开发了相关工具，支持解析、打印、检查、导出为Coq文件等功能。

Conclusion: SC-TPTP格式详细且兼容多种工具，为证明转移提供了实用解决方案。

Abstract: Motivated by the transfer of proofs between proof systems, and in particular
from first order automated theorem provers (ATPs) to interactive theorem
provers (ITPs), we specify an extension of the TPTP derivation text format to
describe proofs in first-order logic: SC-TPTP. To avoid multiplication of
standards, our proposed format over-specifies the TPTP derivation format by
focusing on sequent formalisms. By doing so, it provides a high level of
detail, is faithful to mathematical tradition, and cover multiple existing
tools and in particular tableaux-based strategies. We make use of this format
to allow the Lisa proof assistant to query the Go\'eland automated theorem
prover, and implement a library of tools able to parse, print and check SC-TPTP
proofs, export them into Coq files, and rebuild low-level proof steps from
advanced ones.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [41] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)
*Samuel Rhys Cox*

Main category: cs.HC

TL;DR: 论文探讨了如何通过表达不确定性和展现对话用户界面（CUI）的推理过程来促进用户的自披露行为。


<details>
  <summary>Details</summary>
Motivation: 自披露对心理健康有益，但往往因担心他人反应而困难。研究旨在探索如何通过CUI的设计减轻这种困难。

Method: 通过分析社交线索和表达不确定性的方式，研究CUI如何通过透明化其“心理理论”来鼓励自披露。

Result: 发现表达不确定性和呈现CUI的推理过程可以增强用户的自披露意愿。

Conclusion: CUI设计的透明性有助于提升用户的自披露行为，从而改善用户体验和心理健康。

Abstract: Self-disclosure is important to help us feel better, yet is often difficult.
This difficulty can arise from how we think people are going to react to our
self-disclosure. In this workshop paper, we briefly discuss self-disclosure to
conversational user interfaces (CUIs) in relation to various social cues. We
then, discuss how expressions of uncertainty or representation of a CUI's
reasoning could help encourage self-disclosure, by making a CUI's intended
"theory of mind" more transparent to users.

</details>


### [42] [React to This (RTT): A Nonverbal Turing Test for Embodied AI](https://arxiv.org/abs/2507.10812)
*Chuxuan Zhang,Yasaman Etesam,Angelica Lim*

Main category: cs.HC

TL;DR: 提出了一种测试具身AI代理的交互意识和可信度的方法，特别是在人类将其推向极限的情况下，引入了‘React to This’（RTT）测试。


<details>
  <summary>Details</summary>
Motivation: 探索机器是否能够像人类一样反应，特别是非语言行为，扩展了图灵测试的概念。

Method: 提出‘React to This’（RTT）测试，用于评估AI代理的非语言行为反应能力。

Result: 通过初始实验展示了RTT测试的初步结果。

Conclusion: RTT测试为评估AI代理的交互意识和可信度提供了新方法。

Abstract: We propose an approach to test embodied AI agents for interaction awareness
and believability, particularly in scenarios where humans push them to their
limits. Turing introduced the Imitation Game as a way to explore the question:
"Can machines think?" The Total Turing Test later expanded this concept beyond
purely verbal communication, incorporating perceptual and physical interaction.
Building on this, we propose a new guiding question: "Can machines react?" and
introduce the React to This (RTT) test for nonverbal behaviors, presenting
results from an initial experiment.

</details>


### [43] [Static or Temporal? Semantic Scene Simplification to Aid Wayfinding in Immersive Simulations of Bionic Vision](https://arxiv.org/abs/2507.10813)
*Justin M. Kasowski,Apurv Varshney,Michael Beyeler*

Main category: cs.HC

TL;DR: 研究比较了两种语义预处理方法（SemanticEdges和SemanticRaster）在视觉神经假体中的应用，发现两者均能提升性能，但各有优劣。


<details>
  <summary>Details</summary>
Motivation: 解决视觉神经假体中信息过载问题，优化场景理解。

Method: 在虚拟现实中比较两种语义预处理方法，并通过生物模拟实验验证效果。

Result: 两种方法均优于基线，SemanticEdges提高成功率，SemanticRaster减少碰撞。

Conclusion: 自适应语义预处理对低带宽视觉界面设计有价值。

Abstract: Visual neuroprostheses (bionic eye) aim to restore a rudimentary form of
vision by translating camera input into patterns of electrical stimulation. To
improve scene understanding under extreme resolution and bandwidth constraints,
prior work has explored computer vision techniques such as semantic
segmentation and depth estimation. However, presenting all task-relevant
information simultaneously can overwhelm users in cluttered environments. We
compare two complementary approaches to semantic preprocessing in immersive
virtual reality: SemanticEdges, which highlights all relevant objects at once,
and SemanticRaster, which staggers object categories over time to reduce visual
clutter. Using a biologically grounded simulation of prosthetic vision, 18
sighted participants performed a wayfinding task in a dynamic urban environment
across three conditions: edge-based baseline (Control), SemanticEdges, and
SemanticRaster. Both semantic strategies improved performance and user
experience relative to the baseline, with each offering distinct trade-offs:
SemanticEdges increased the odds of success, while SemanticRaster boosted the
likelihood of collision-free completions. These findings underscore the value
of adaptive semantic preprocessing for prosthetic vision and, more broadly, may
inform the design of low-bandwidth visual interfaces in XR that must balance
information density, task relevance, and perceptual clarity.

</details>


### [44] [AROMA: Mixed-Initiative AI Assistance for Non-Visual Cooking by Grounding Multi-modal Information Between Reality and Videos](https://arxiv.org/abs/2507.10963)
*Zheng Ning,Leyang Li,Daniel Killough,JooYoung Seo,Patrick Carrington,Yapeng Tian,Yuhang Zhao,Franklin Mingzhe Li,Toby Jia-Jun Li*

Main category: cs.HC

TL;DR: AROMA是一种AI系统，通过整合用户的非视觉线索、可穿戴相机和视频食谱内容，为盲人或低视力人群提供实时、上下文感知的烹饪辅助。


<details>
  <summary>Details</summary>
Motivation: 视频食谱主要为视听信息，对盲人或低视力人群不够友好。他们依赖触觉、味觉等非视觉线索，但难以同步视频内容，导致烹饪困难。

Method: AROMA结合用户的非视觉感知、可穿戴相机和视频内容，采用混合主动策略，既响应用户请求，又主动提供警报和指导。

Result: 通过对8名盲人或低视力参与者的研究，AROMA展示了其在辅助ADL任务中的潜力。

Conclusion: AROMA为设计支持盲人或低视力人群的交互式AI系统提供了有价值的见解。

Abstract: Videos offer rich audiovisual information that can support people in
performing activities of daily living (ADLs), but they remain largely
inaccessible to blind or low-vision (BLV) individuals. In cooking, BLV people
often rely on non-visual cues, such as touch, taste, and smell, to navigate
their environment, making it difficult to follow the predominantly audiovisual
instructions found in video recipes. To address this problem, we introduce
AROMA, an AI system that provides timely responses to the user based on
real-time, context-aware assistance by integrating non-visual cues perceived by
the user, a wearable camera feed, and video recipe content. AROMA uses a
mixed-initiative approach: it responds to user requests while also proactively
monitoring the video stream to offer timely alerts and guidance. This
collaborative design leverages the complementary strengths of the user and AI
system to align the physical environment with the video recipe, helping the
user interpret their current cooking state and make sense of the steps. We
evaluated AROMA through a study with eight BLV participants and offered
insights for designing interactive AI systems to support BLV individuals in
performing ADLs.

</details>


### [45] [Self++: Merging Human and AI for Co-Determined XR Living in the Metaverse](https://arxiv.org/abs/2507.10967)
*Thammathip Piumsomboon*

Main category: cs.HC

TL;DR: Self++是一个基于自决理论的九层框架，旨在通过人机协作在扩展现实（XR）中培养能力、自主性和关联性，促进人类繁荣。


<details>
  <summary>Details</summary>
Motivation: 目标是创建一个以人为中心、AI增强的元宇宙，技术放大而非削弱人的潜能。

Method: 采用动态人机协作，利用XR的沉浸能力增强用户能力并减少认知偏见。

Result: 提出了用户定义AI自主性、XR中有意义的社交连接设计及主动伦理保障等研究方向。

Conclusion: Self++为构建人类繁荣的元宇宙提供了路线图。

Abstract: This position paper introduces Self++, a novel nine-level framework for
co-determined living in the Metaverse, grounded in Self-Determination Theory.
Self++ prioritises human flourishing by progressively cultivating competence,
autonomy, and relatedness through dynamic human-AI collaboration in extended
reality (XR). Unlike technologically deterministic approaches, Self++
emphasises user empowerment by enhancing competency, mitigating cognitive
biases and leveraging XR's immersive capabilities. Key research directions
proposed include exploring the boundaries of user-defined AI autonomy,
designing for meaningful social connection in XR, and establishing proactive
ethical safeguards. Ultimately, Self++ offers a roadmap for creating a
human-centred, AI-enhanced Metaverse where technology amplifies, rather than
diminishes, human potential.

</details>


### [46] [Terms and Conditions (Do Not) Apply: Understanding Exploitation Disparities in Design of Mobile-Based Financial Services](https://arxiv.org/abs/2507.10970)
*Lindah Kotut*

Main category: cs.HC

TL;DR: 研究探讨了移动金融服务为传统无银行账户人群带来的机遇与挑战，强调了设计模式中的伦理风险，并提出了用户赋权的设计指南。


<details>
  <summary>Details</summary>
Motivation: 移动金融服务虽然为无银行账户人群提供了便捷，但也带来了高利率、政策文件缺失等风险，亟需探讨如何通过设计减少这些风险。

Method: 通过用户访谈，分析移动金融交易的用户体验，总结设计中的优缺点，并制定减少风险的策略。

Result: 揭示了金融剥削的差异，提出了降低风险的策略和恢复措施。

Conclusion: 建议通过用户信任机制、技术流程透明度和风险评估的教育，设计更安全的移动金融系统。

Abstract: Mobile-based financial services have made it possible for the traditionally
unbanked to access infrastructure that have been routinely unattainable.
Researchers have explored how these systems have made for safer environments to
send and receive money and have expanded financial opportunities such as
increased borrowing. With this expansion, challenges such as detrimental
interest rates, lack of access to policy documents, and inadequate user
protective guardrails emerge, amplifying the risks due to technology-aided
unethical financial practices that are aided by design patterns. Supported by
user interviews, we detail user experiences of mobile-based financial
transactions and explore the foundations and guidelines that undergird the
financial service provisions: highlighting both affordances and harms enabled
in the design of such systems. We discuss the findings by highlighting
financial exploitation disparities, deliberating strategies for mitigation of
risks and enabling recovery from harms caused by the technology use. We then
recommend guidelines for empowering design approaches that support users'
mechanisms of trust, their understanding of technological processes, and
determination of risks.

</details>


### [47] [An Exploratory Study on AI-driven Visualisation Techniques on Decision Making in Extended Reality](https://arxiv.org/abs/2507.10981)
*Ze Dong,Binyang Han,Jingjing Zhang,Ruoyu Wen,Barrett Ens,Adrian Clark,Tham Piumsomboon*

Main category: cs.HC

TL;DR: 研究了四种AI驱动的可视化技术（Inform、Nudge、Recommend、Instruct）在XR环境中对用户决策的影响，强调了用户自主权、AI透明性和上下文设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索AI与XR结合的新范式如何通过不同可视化技术影响用户决策，并分析其对用户自主权的潜在影响。

Method: 使用Meta Quest Pro，通过预先录制的360度超市视频叠加四种AI可视化技术，并进行半结构化访谈的探索性研究。

Result: 研究发现，保持用户自主权、增强AI透明性以及考虑上下文设计是提升用户体验和信任的关键。

Conclusion: AI驱动的可视化技术在XR中需平衡用户自主权和干预强度，同时注重透明性和情境适应性。

Abstract: The integration of extended reality (XR) with artificial intelligence (AI)
introduces a new paradigm for user interaction, enabling AI to perceive user
intent, stimulate the senses, and influence decision-making. We explored the
impact of four AI-driven visualisation techniques -- `Inform,' `Nudge,'
`Recommend,' and `Instruct' -- on user decision-making in XR using the Meta
Quest Pro. To test these techniques, we used a pre-recorded 360-degree video of
a supermarket, overlaying each technique through a virtual interface. We aimed
to investigate how these different visualisation techniques with different
levels of user autonomy impact preferences and decision-making. An exploratory
study with semi-structured interviews provided feedback and design
recommendations. Our findings emphasise the importance of maintaining user
autonomy, enhancing AI transparency to build trust, and considering context in
visualisation design.

</details>


### [48] [Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias](https://arxiv.org/abs/2507.11210)
*Rushia Harada,Yuken Kimura,Keito Inoshita*

Main category: cs.HC

TL;DR: 研究探索了基于大型语言模型（LLM）的支持系统，用于改善家庭心理安全沟通。通过角色扮演框架分析对话并生成反馈，实验显示系统能识别压抑情绪并提供实用建议。


<details>
  <summary>Details</summary>
Motivation: 传统指标常忽视家庭中的微妙心理动态，尤其是父母的理想化偏见会压抑孩子的情感和自主性。研究旨在通过LLM支持家庭沟通。

Method: 构建包含30个场景的日语亲子对话语料库，标注理想化偏见和压抑情绪。开发多代理框架，分析对话并生成结构化反馈。

Result: 系统能适度准确地识别压抑情绪类别，生成的反馈在同理心和实用性上评分高。模拟对话显示情感表达有所改善。

Conclusion: 框架有望支持家庭互动中的积极转变，但仍需进一步完善以提升准确性。

Abstract: Well-being in family settings involves subtle psychological dynamics that
conventional metrics often overlook. In particular, unconscious parental
expectations, termed ideal parent bias, can suppress children's emotional
expression and autonomy. This suppression, referred to as suppressed emotion,
often stems from well-meaning but value-driven communication, which is
difficult to detect or address from outside the family. Focusing on these
latent dynamics, this study explores Large Language Model (LLM)-based support
for psychologically safe family communication. We constructed a Japanese
parent-child dialogue corpus of 30 scenarios, each annotated with metadata on
ideal parent bias and suppressed emotion. Based on this corpus, we developed a
Role-Playing LLM-based multi-agent dialogue support framework that analyzes
dialogue and generates feedback. Specialized agents detect suppressed emotion,
describe implicit ideal parent bias in parental speech, and infer contextual
attributes such as the child's age and background. A meta-agent compiles these
outputs into a structured report, which is then passed to five selected expert
agents. These agents collaboratively generate empathetic and actionable
feedback through a structured four-step discussion process. Experiments show
that the system can detect categories of suppressed emotion with moderate
accuracy and produce feedback rated highly in empathy and practicality.
Moreover, simulated follow-up dialogues incorporating this feedback exhibited
signs of improved emotional expression and mutual understanding, suggesting the
framework's potential in supporting positive transformation in family
interactions.

</details>


### [49] [REVA: Supporting LLM-Generated Programming Feedback Validation at Scale Through User Attention-based Adaptation](https://arxiv.org/abs/2507.11470)
*Xiaohang Tang,Sam Wong,Zicheng He,Yalong Yang,Yan Chen*

Main category: cs.HC

TL;DR: REVA是一个人机协作系统，通过优化编程作业反馈的审查流程，减少教师的认知负荷，并通过学习教师的注意力改进反馈质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决教师审核大量AI生成的编程反馈时的认知负担问题，并提高反馈质量。

Method: REVA通过排序提交以减少认知切换，并传播教师的修订到语义相似的实例，同时自适应学习教师的注意力来改进流程。

Result: 通过12名参与者的实验室研究证明，REVA能有效提高反馈质量并优化审查流程。

Conclusion: REVA为教育反馈中的人机协作提供了高效的新方法，显著提升了反馈审查的效率和效果。

Abstract: This paper introduces REVA, a human-AI system that expedites instructor
review of voluminous AI-generated programming feedback by sequencing
submissions to minimize cognitive context shifts and propagating
instructor-driven revisions across semantically similar instances. REVA
introduces a novel approach to human-AI collaboration in educational feedback
by adaptively learning from instructors' attention in the review and revision
process to continuously improve the feedback validation process. REVA's
usefulness and effectiveness in improving feedback quality and the overall
feedback review process were evaluated through a within-subjects lab study with
12 participants.

</details>


### [50] [Towards Creating Infrastructures for Values and Ethics Work in the Production of Software Technologies](https://arxiv.org/abs/2507.11490)
*Richmond Y. Wong*

Main category: cs.HC

TL;DR: 论文主张通过构建基础设施而非工具来支持设计和伦理工作，以更全面地考虑社会和政治背景。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在技术设计中更有效地融入社会价值观和伦理考量。

Method: 借鉴科技研究和社会研究的理论，提出基础设施策略。

Result: 提出了基于基础设施的新方法，以帮助HCI研究者更好地处理设计伦理问题。

Conclusion: 基础设施方法比单纯的工具更能全面支持设计和伦理工作。

Abstract: Recognizing how technical systems can embody social values or cause harms,
human-computer interaction (HCI) research often approaches addressing values
and ethics in design by creating tools to help tech workers integrate social
values into the design of products. While useful, these approaches usually do
not consider the politics embedded in the broader processes, organizations,
social systems, and governance structures that affect the types of actions that
tech workers can take to address values and ethics. This paper argues that
creating infrastructures to support values and ethics work, rather than tools,
is an approach that takes these broader processes into account and opens them
up for (re)design. Drawing on prior research conceptualizing infrastructures
from science \& technology studies and media studies, this paper outlines
conceptual insights from infrastructures studies that open up new tactics for
HCI researchers and designers seeking to support values and ethics in design.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [51] [Developing and evaluating quilts for the depiction of large layered graphs](https://arxiv.org/abs/2507.10883)
*Juhee Bae,Benjamin Watson*

Main category: cs.GR

TL;DR: Quilts是一种基于矩阵的分层图展示方法，通过改进skip link的展示方式（颜色、文本或混合），研究发现混合方式更优。相比传统节点链接和矩阵图，Quilts能显著提高路径查找速度。


<details>
  <summary>Details</summary>
Motivation: 传统分层图展示方法在复杂图中难以理解，Quilts旨在解决这一问题并提升可视化效果。

Method: 通过三种展示方式（颜色、文本、混合）改进Quilts，并与节点链接和矩阵图对比。

Result: 混合skip link展示方式表现最佳；Quilts路径查找速度显著快于传统方法（46.6秒 vs 58.3秒和71.2秒），尤其在大型图中优势更明显。

Conclusion: Quilts在复杂图中的可视化效果和路径查找效率优于传统方法，混合skip link展示方式是关键改进。

Abstract: Traditional layered graph depictions such as flow charts are in wide use. Yet
as graphs grow more complex, these depictions can become difficult to
understand. Quilts are matrix-based depictions for layered graphs designed to
address this problem. In this research, we first improve Quilts by developing
three design alternatives, and then compare the best of these alternatives to
better-known node-link and matrix depictions. A primary weakness in Quilts is
their depiction of skip links, links that do not simply connect to a succeeding
layer. Therefore in our first study, we compare Quilts using color-only,
text-only, and mixed (color and text) skip link depictions, finding that path
finding with the color-only depiction is significantly slower and less
accurate, and that in certain cases, the mixed depiction offers an advantage
over the text-only depiction. In our second study, we compare Quilts using the
mixed depiction to node-link diagrams and centered matrices. Overall results
show that users can find paths through graphs significantly faster with Quilts
(46.6 secs) than with node-link (58.3 secs) or matrix (71.2 secs) diagrams.
This speed advantage is still greater in large graphs (e.g. in 200 node graphs,
55.4 secs vs. 71.1 secs for node-link and 84.2 secs for matrix depictions).

</details>


### [52] [OffsetCrust: Variable-Radius Offset Approximation with Power Diagrams](https://arxiv.org/abs/2507.10924)
*Zihan Zhao,Pengfei Wang,Minfeng Xu,Shuangmin Chen,Shiqing Xin,Changhe Tu,Wenping Wang*

Main category: cs.GR

TL;DR: 本文提出了一种名为OffsetCrust的新型框架，用于高效计算可变半径偏移曲面，通过构造一个幂图来解决这一问题，并通过实验验证了其准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 可变半径偏移曲面的计算在几何处理中具有重要意义，但目前仍是一个具有挑战性的问题。本文旨在解决这一问题。

Method: OffsetCrust框架通过采样基曲面上的点及其对应的离曲面点（沿半径相关方向位移）构造幂图，并结合轻量级微调过程以减少偏差。

Result: 实验证明OffsetCrust在计算可变半径偏移曲面时具有高准确性和效率，并能应用于从MAT表示重建原始边界曲面等实际问题。

Conclusion: OffsetCrust为解决可变半径偏移曲面计算问题提供了一种高效且实用的方法，具有广泛的应用潜力。

Abstract: Offset surfaces, defined as the Minkowski sum of a base surface and a rolling
ball, play a crucial role in geometry processing, with applications ranging
from coverage motion planning to brush modeling. While considerable progress
has been made in computing constant-radius offset surfaces, computing
variable-radius offset surfaces remains a challenging problem. In this paper,
we present OffsetCrust, a novel framework that efficiently addresses the
variable-radius offsetting problem by computing a power diagram. Let $R$ denote
the radius function defined on the base surface $S$. The power diagram is
constructed from contributing sites, consisting of carefully sampled base
points on $S$ and their corresponding off-surface points, displaced along
$R$-dependent directions. In the constant-radius case only, these displacement
directions align exactly with the surface normals of $S$. Moreover, our method
mitigates the misalignment issues commonly seen in crust-based approaches
through a lightweight fine-tuning procedure. We validate the accuracy and
efficiency of OffsetCrust through extensive experiments, and demonstrate its
practical utility in applications such as reconstructing original boundary
surfaces from medial axis transform (MAT) representations.

</details>


### [53] [Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model](https://arxiv.org/abs/2507.11465)
*Nuri Ryu,Jiyun Won,Jooeun Son,Minsu Gong,Joo-Haeng Lee,Sunghyun Cho*

Main category: cs.GR

TL;DR: Elevate3D是一种新型框架，通过HFS-SDEdit方法提升低质量3D资产的质量，并交替优化纹理和几何，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高质量3D资产稀缺且获取成本高，Elevate3D旨在通过提升低质量资产来解决这一问题。

Method: 采用HFS-SDEdit优化纹理，并结合单目几何预测器交替优化纹理和几何。

Result: Elevate3D在3D模型细化中达到最先进质量，显著提升低质量资产。

Conclusion: Elevate3D有效解决了高质量3D资产匮乏的问题，且优于同类方法。

Abstract: High-quality 3D assets are essential for various applications in computer
graphics and 3D vision but remain scarce due to significant acquisition costs.
To address this shortage, we introduce Elevate3D, a novel framework that
transforms readily accessible low-quality 3D assets into higher quality. At the
core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that
significantly improves texture quality while preserving the appearance and
geometry while fixing its degradations. Furthermore, Elevate3D operates in a
view-by-view manner, alternating between texture and geometry refinement.
Unlike previous methods that have largely overlooked geometry refinement, our
framework leverages geometric cues from images refined with HFS-SDEdit by
employing state-of-the-art monocular geometry predictors. This approach ensures
detailed and accurate geometry that aligns seamlessly with the enhanced
texture. Elevate3D outperforms recent competitors by achieving state-of-the-art
quality in 3D model refinement, effectively addressing the scarcity of
high-quality open-source 3D assets.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [54] [Fault-Free Analog Computing with Imperfect Hardware](https://arxiv.org/abs/2507.11134)
*Zhicheng Xu,Jiawei Liu,Sitao Huang,Zefan Li,Shengbo Wang,Bo Wen,Ruibin Mao,Mingrui Jiang,Giacomo Pedretti,Jim Ignowski,Kaibin Huang,Can Li*

Main category: cs.ET

TL;DR: 提出一种新的无故障矩阵表示方法，通过分解矩阵到两个可调子矩阵，显著提高模拟计算的可靠性和密度。


<details>
  <summary>Details</summary>
Motivation: 边缘计算和AI的需求推动了模拟内存计算的研究，但设备故障和变化限制了系统的精度和可靠性。

Method: 将目标矩阵分解为两个可调子矩阵，通过数学优化绕过故障设备，消除差分对。

Result: 在39%设备故障率下实现99.999%余弦相似度，优于传统方法的56倍误码率降低和196%密度提升。

Conclusion: 该方法证明设备良率不再是模拟计算硬件的主要瓶颈，适用于多种新兴存储和非电计算基板。

Abstract: The growing demand for edge computing and AI drives research into analog
in-memory computing using memristors, which overcome data movement bottlenecks
by computing directly within memory. However, device failures and variations
critically limit analog systems' precision and reliability. Existing
fault-tolerance techniques, such as redundancy and retraining, are often
inadequate for high-precision applications or scenarios requiring fixed
matrices and privacy preservation. Here, we introduce and experimentally
demonstrate a fault-free matrix representation where target matrices are
decomposed into products of two adjustable sub-matrices programmed onto analog
hardware. This indirect, adaptive representation enables mathematical
optimization to bypass faulty devices and eliminate differential pairs,
significantly enhancing computational density. Our memristor-based system
achieved >99.999% cosine similarity for a Discrete Fourier Transform matrix
despite 39% device fault rate, a fidelity unattainable with conventional direct
representation, which fails with single device faults (0.01% rate). We
demonstrated 56-fold bit-error-rate reduction in wireless communication and
>196% density with 179% energy efficiency improvements compared to
state-of-the-art techniques. This method, validated on memristors, applies
broadly to emerging memories and non-electrical computing substrates, showing
that device yield is no longer the primary bottleneck in analog computing
hardware.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [55] [FAFO: Over 1 million TPS on a single node running EVM while still Merkleizing every block](https://arxiv.org/abs/2507.10757)
*Ryan Zarick,Isaac Zhang,Daniel Wong,Thomas Kim,Bryan Pellegrino,Mignon Li,Kelvin Wong*

Main category: cs.DC

TL;DR: FAFO是一种区块链交易调度器，通过重排序交易提升执行并行性，显著提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前区块链执行吞吐量受限于数据竞争，减少了并行性，FAFO旨在解决这一问题。

Method: 使用CPU优化的Bloom过滤器高效检测冲突，并行调度交易执行。

Result: 单节点每秒处理超过110万笔原生ETH转账和50万笔ERC20转账，成本降低91%。

Conclusion: FAFO证明通过优化执行层和交易调度器设计，可支持未来高吞吐量去中心化应用。

Abstract: Current blockchain execution throughput is limited by data contention,
reducing execution layer parallelism. Fast Ahead-of-Formation Optimization
(FAFO) is the first blockchain transaction scheduler to address this problem by
reordering transactions before block formation for maximum concurrency. FAFO
uses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts
and schedule parallel transaction execution at high throughput and low
overhead.
  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1
million native ETH transfers per second and over half a million ERC20 transfers
per second on a single node (Table 1), with 91% lower cost compared to
state-of-the-art sharded execution. Unlike many other existing high throughput
blockchain execution clients, FAFO uses QMDB to Merkleize world state after
every block, enabling light clients and stateless validation for ZK-based
vApps. FAFO scales with minimal synchronization overhead, scaling linearly with
additional CPU resources until it fully exploits the maximum parallelism of the
underlying transaction flow. FAFO proves that the high throughput necessary to
support future decentralized applications can be achieved with a streamlined
execution layer and innovations in blockchain transaction scheduler design.
FAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.

</details>


### [56] [Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks](https://arxiv.org/abs/2507.10789)
*Aaron Jarmusch,Nathan Graddon,Sunita Chandrasekaran*

Main category: cs.DC

TL;DR: 本文通过对现代NVIDIA Blackwell架构的微架构分析，揭示了其关键子系统，包括内存层次结构、SM执行管道和第五代Tensor核心（支持FP4和FP6精度），并与Hopper架构进行了比较。


<details>
  <summary>Details</summary>
Motivation: 科学研究的快速发展对计算能力提出了更高要求，GPU部分满足了这一需求。本文旨在分析NVIDIA Blackwell架构的微架构特性，为开发者和性能工程师提供优化依据。

Method: 通过精心设计的微基准测试，研究了Blackwell架构的延迟、吞吐量、缓存行为和调度细节，并与Hopper架构（使用GeForce RTX 5080和H100 PCIe）进行了对比。

Result: 揭示了Blackwell架构的关键特性和设计中的细微调整指标，展示了代际改进和性能退化的对比结果，同时探讨了不同负载下的能效和能耗。

Conclusion: 研究结果为开发者和工程师优化Blackwell平台上的工作负载提供了实用见解，并为GPU架构研究贡献了新数据。

Abstract: The rapid development in scientific research provides a need for more compute
power, which is partly being solved by GPUs. This paper presents a
microarchitectural analysis of the modern NVIDIA Blackwell architecture by
studying GPU performance
  features with thought through microbenchmarks. We unveil key subsystems,
including the memory hierarchy, SM execution
  pipelines, and the SM sub-core units, including the 5th generation tensor
cores supporting FP4 and FP6 precisions.
  To understand the different key features of the NVIDIA GPU, we study latency,
throughput, cache behavior, and scheduling
  details, revealing subtle tuning metrics in the design of Blackwell. To
develop a comprehensive analysis, we compare the
  Blackwell architecture with the previous Hopper architecture by using the
GeForce RTX 5080 and H100 PCIe, respectively. We
  evaluate and compare results, presenting both generational improvements and
performance regressions. Additionally, we
  investigate the role of power efficiency and energy consumption under varied
workloads. Our findings provide actionable insights
  for application developers, compiler writers, and performance engineers to
optimize workloads on Blackwell-based platforms,
  and contribute new data to the growing research on GPU architectures.

</details>


### [57] [MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix Unit](https://arxiv.org/abs/2507.11067)
*Yinuo Wang,Tianqi Mao,Lin Gan,Wubing Wan,Zeyu Song,Jiayu Fu,Lanke He,Wenqiang Wang,Zekun Yin,Wei Xue,Guangwen Yang*

Main category: cs.DC

TL;DR: 论文研究了基于矩阵加速的三维高阶模板计算，提出了一种优化方法，结合SIMD和矩阵单元解决内存访问问题，并实现了在Nvidia A100上的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管矩阵加速的模板计算研究热门，但针对三维高阶模板和HPC的应用仍有待探索，尤其是多核CPU上矩阵单元的出现提供了新机会。

Method: 通过SIMD和矩阵单元优化算法，解决内存访问问题，并引入内存优化和多线程并行方法，提升硬件利用率和跨NUMA通信效率。

Result: MMStencil在Nvidia A100上比现有库快2.1倍，并在实际HPC应用中实现了1.8倍的加速。

Conclusion: 提出的方法显著提升了三维高阶模板计算的性能，适用于实际HPC应用。

Abstract: Matrix-accelerated stencil computation is a hot research topic, yet its
application to three-dimensional (3D) high-order stencils and HPC remains
underexplored. With the emergence of matrix units on multicore CPUs, we analyze
matrix-based acceleration strategies and tailor an optimal approach for 3D
high-order stencils. We introduce algorithmic optimizations based on SIMD and
matrix units to address strided memory accesses, alignment conflicts, and
redundant accesses. We propose memory optimizations to boost on-package memory
efficiency, and a novel multi-thread parallelism paradigm to overcome
data-sharing challenges caused by the absence of shared data caches. MMStencil
sustains consistently high hardware utilization across diverse stencil shapes
and dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA
effects and MPI limitations in hybrid parallelism. Combining all the
innovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100
GPGPU by up to 2.1x. Moreover, the performance improvements translate directly
to real-world HPC applications and enable RTM applications to yield 1.8x
speedup versus a highly optimized industrial Nvidia A100 GPGPU version.

</details>


### [58] [Cyclic Data Streaming on GPUs for Short Range Stencils Applied to Molecular Dynamics](https://arxiv.org/abs/2507.11289)
*Martin Rose,Simon Homes,Lukas Ramsperger,Jose Gracia,Christoph Niethammer,Jadran Vrabec*

Main category: cs.DC

TL;DR: 本文提出了一种新型高性能计算框架，通过高带宽GPU通信实现显式算法的线性性能扩展，适用于分子动力学模拟等场景，性能优于LAMMPS。


<details>
  <summary>Details</summary>
Motivation: 为了追求科学计算的最高性能，设计了一种基于GPU集群高带宽通信的框架，简化并行化策略的使用。

Method: 框架通过进程间环形传递数据切片实现并行化时间并行化，用户只需编写GPU核函数处理数据，框架负责通信。

Result: 在分子动力学模拟案例中，该框架在强扩展场景下性能优于LAMMPS。

Conclusion: 该框架为高性能计算提供了一种高效且易于使用的解决方案，特别适合需要线性扩展性能的应用。

Abstract: In the quest for highest performance in scientific computing, we present a
novel framework that relies on high-bandwidth communication between GPUs in a
compute cluster. The framework offers linear scaling of performance for
explicit algorithms that is only limited by the size of the dataset and the
number of GPUs. Slices of the dataset propagate in a ring of processes (GPUs)
from one GPU, where they are processed, to the next, which results in a
parallel-in-time parallelization. The user of the framework has to write GPU
kernels that implement the algorithm and provide slices of the dataset.
Knowledge about the underlying parallelization strategy is not required because
the communication between processes is carried out by the framework. As a case
study, molecular dynamics simulation based on the Lennard-Jones potential is
implemented to measure the performance for a homogeneous fluid. Single node
performance and strong scaling behavior of this framework is compared to
LAMMPS, which is outperformed in the strong scaling case.

</details>


### [59] [Generating Dynamic Graph Algorithms for Multiple Backends for a Graph DSL](https://arxiv.org/abs/2507.11094)
*Nibedita Behera,Ashwina Kumar,Atharva Chougule,Mohammed Shan P S,Rushabh Nirdosh Lalwani,Rupesh Nasre*

Main category: cs.DC

TL;DR: 提出了一个抽象方案和运行时优化方法，用于高效处理动态图算法，通过DSL生成并行代码，并在多个算法和图上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于图算法的计算、内存访问和通信的不规则性，并行化面临挑战，现有框架在处理动态图时存在局限性。

Method: 引入抽象方案和运行时优化，通过DSL表达动态处理逻辑并自动生成针对多核、分布式和多核环境的并行代码。

Result: 在十种不同特性的图和三种常用算法（最短路径、PageRank、三角形计数）上验证了方法的有效性。

Conclusion: 该方法为动态图算法的高效并行化提供了实用解决方案。

Abstract: With the rapid growth of unstructured and semistructured data, parallelizing
graph algorithms has become essential for efficiency. However, due to the
inherent irregularity in computation, memory access patterns, and
communication, graph algorithms are notoriously difficult to parallelize. To
address this challenge, several libraries, frameworks, and domain-specific
languages (DSLs) have been proposed to ease the parallel programming burden for
domain experts. Existing frameworks partially or fully abstract away
parallelism intricacies, provide intuitive scheduling mnemonics, and employ
program analysis to identify data races and generate synchronization code.
Despite these advances, most frameworks are limited in their abstractions and
runtime optimizations, especially when dealing with static graphs. In contrast,
many real-world graphs are inherently dynamic, with evolving structures over
time through insertions, deletions, and modifications of vertices, edges, and
attributes. Generating efficient and correctly synchronized code for such
dynamic graph algorithms remains a significant challenge.
  In this work, we introduce an abstraction scheme and runtime optimizations
for the efficient processing of morph algorithms. Specifically, given an
initial graph G and a set of updates $\Delta$G involving edge insertions and
deletions, we express the dynamic processing logic through a DSL and
automatically generate parallel code targeting multicore, distributed, and
many-core environments. We demonstrate the effectiveness of our approach by
applying the DSL-generated code to ten large graphs with diverse
characteristics and three widely used algorithms: Shortest Paths, PageRank, and
Triangle Counting.

</details>


### [60] [Scaling the memory wall using mixed-precision -- HPG-MxP on an exascale machine](https://arxiv.org/abs/2507.11512)
*Aditya Kashi,Nicholson Koukpaizan,Hao Lu,Michael Matheson,Sarp Oral,Feiyi Wang*

Main category: cs.DC

TL;DR: 混合精度算法在科学计算中的应用潜力尚未完全明确，但HPG-MxP基准测试优化实现证明了其在GPU超算上的1.6倍加速效果。


<details>
  <summary>Details</summary>
Motivation: 探索混合精度算法在高性能计算（HPC）平台上的实际增益，尤其是针对稀疏矩阵应用。

Method: 提出并优化了HPG-MxP基准测试，结合双精度和单精度运算，在现代GPU超算上实现算法增强。

Result: 首次展示了在GPU超算上使用混合精度算法带来的1.6倍速度提升。

Conclusion: 混合精度算法在稀疏矩阵应用中具有显著加速潜力，但仍需更多实际验证。

Abstract: Mixed-precision algorithms have been proposed as a way for scientific
computing to benefit from some of the gains seen for artificial intelligence
(AI) on recent high performance computing (HPC) platforms. A few applications
dominated by dense matrix operations have seen substantial speedups by
utilizing low precision formats such as FP16. However, a majority of scientific
simulation applications are memory bandwidth limited. Beyond preliminary
studies, the practical gain from using mixed-precision algorithms on a given
HPC system is largely unclear.
  The High Performance GMRES Mixed Precision (HPG-MxP) benchmark has been
proposed to measure the useful performance of a HPC system on sparse
matrix-based mixed-precision applications. In this work, we present a highly
optimized implementation of the HPG-MxP benchmark for an exascale system and
describe our algorithm enhancements. We show for the first time a speedup of
1.6x using a combination of double- and single-precision on modern GPU-based
supercomputers.

</details>


### [61] [Boosting Scientific Error-Bounded Lossy Compression through Optimized Synergistic Lossy-Lossless Orchestration](https://arxiv.org/abs/2507.11165)
*Shixun Wu,Jinwen Pan,Jinyang Liu,Jiannan Tian,Ziwei Qiu,Jiajun Huang,Kai Zhao,Xin Liang,Sheng Di,Zizhong Chen,Franck Cappello*

Main category: cs.DC

TL;DR: cuSZ-Hi是一种优化的基于GPU的高比率科学误差有损压缩器，具有灵活且领域无关的开源框架设计，显著提升了压缩比。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算架构的发展，科学计算工作流对高比率、低延迟的误差有损数据压缩解决方案的需求日益迫切。

Method: 1) 优化GPU上的并行插值数据预测方案；2) 探索并整合最佳的无损数据编码技术；3) 在基准数据集上进行系统评估。

Result: cuSZ-Hi在相同误差范围内压缩比提升高达249%，在相同PSNR下压缩比提升高达215%。

Conclusion: cuSZ-Hi在压缩比和性能上优于现有技术，适用于科学计算领域的高效数据压缩。

Abstract: As high-performance computing architectures evolve, more scientific computing
workflows are being deployed on advanced computing platforms such as GPUs.
These workflows can produce raw data at extremely high throughputs, requiring
urgent high-ratio and low-latency error-bounded data compression solutions. In
this paper, we propose cuSZ-Hi, an optimized high-ratio GPU-based scientific
error-bounded lossy compressor with a flexible, domain-irrelevant, and fully
open-source framework design. Our novel contributions are: 1) We maximally
optimize the parallelized interpolation-based data prediction scheme on GPUs,
enabling the full functionalities of interpolation-based scientific data
prediction that are adaptive to diverse data characteristics; 2) We thoroughly
explore and investigate lossless data encoding techniques, then craft and
incorporate the best-fit lossless encoding pipelines for maximizing the
compression ratio of cuSZ-Hi; 3) We systematically evaluate cuSZ-Hi on
benchmarking datasets together with representative baselines. Compared to
existing state-of-the-art scientific lossy compressors, with comparative or
better throughput than existing high-ratio scientific error-bounded lossy
compressors on GPUs, cuSZ-Hi can achieve up to 249% compression ratio
improvement under the same error bound, and up to 215% compression ratio
improvement under the same decompression data PSNR.

</details>


### [62] [A new Dune grid for scalable dynamic adaptivity based on the p4est software library](https://arxiv.org/abs/2507.11386)
*Carsten Burstedde,Mikhail Kirilin,Robert Klöfkorn*

Main category: cs.DC

TL;DR: 扩展了Dune求解器库，新增了对开源软件p4est的网格接口，目的是继承p4est的高MPI可扩展性和轻量数据结构，并在并行环境下通过数值实验证明其优于现有的Dune-ALUGrid实现。


<details>
  <summary>Details</summary>
Motivation: 为了利用p4est的高MPI可扩展性、轻量数据结构和原生支持多块网格拓扑的特点。

Method: 在Dune库中新增了p4est网格接口，并与现有的Dune-ALUGrid实现进行了对比测试。

Result: 新实现显示在可扩展性方面优于Dune-ALUGrid，同时提出了一种替代平衡策略，提高了性能。

Conclusion: 新接口和平衡策略显著提升了并行环境下的性能，适用于大规模网格计算。

Abstract: In this work we extend the Dune solver library with another grid interface to
the open-source p4est software. While Dune already supports about a dozen
different mesh implementations through its mesh interface Dune-Grid, we
undertake this new coupling effort in order to inherit p4est's practically
unlimited MPI scalability as well as its relatively thin data structures, and
its native support for multi-block (forest) mesh topologies in both 2D and 3D.
  The presented implementation is compared to an existing implementation based
on Dune-ALUGrid for a variety of challenging test examples in a parallel
environment. The numerical experiments show that the implementation presented
here is outperforming Dune-ALUGrid in terms of scalability. In addition, an
alternative balancing strategy is presented to ensure 2:1 balancing across
element faces showing improved performance compared to the existing p4est
balance strategy in the numerical examples considered in this work.

</details>


### [63] [Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations](https://arxiv.org/abs/2507.11417)
*Miray Özcan,Philipp Wiesner,Philipp Weiß,Odej Kao*

Main category: cs.DC

TL;DR: 论文提出了一个模拟框架，用于评估LLM推理在不同部署设置下的能源和碳排放影响。


<details>
  <summary>Details</summary>
Motivation: 现有模拟框架无法准确估计LLM推理相关的碳排放，而LLM推理的碳排放已占其总生命周期排放的一半以上。

Method: 扩展了高保真LLM推理模拟器，加入GPU功耗模型；将模拟输出集成到能源系统联合仿真环境中。

Result: 框架揭示了推理参数对能源需求和碳排放的影响，展示了在示例部署中可再生能抵消69.2%的潜力。

Conclusion: 该框架为未来碳感知推理基础设施设计奠定了基础。

Abstract: The environmental impact of Large Language Models (LLMs) is rising
significantly, with inference now accounting for more than half of their total
lifecycle carbon emissions. However, existing simulation frameworks, which are
increasingly used to determine efficient LLM deployments, lack any concept of
power and, therefore, cannot accurately estimate inference-related emissions.
We present a simulation framework to assess the energy and carbon implications
of LLM inference under varying deployment setups. First, we extend a
high-fidelity LLM inference simulator with a GPU power model that estimates
power consumption based on utilization metrics, enabling analysis across
configurations like batch size, sequence length, and model parallelism. Second,
we integrate simulation outputs into an energy system co-simulation environment
to quantify carbon emissions under specific grid conditions and explore the
potential of carbon-aware scheduling. Through scenario-based analysis, our
framework reveals how inference parameters affect energy demand and carbon
footprint, demonstrates a renewable offset potential of up to 69.2% in an
illustrative deployment case, and provides a foundation for future carbon-aware
inference infrastructure design.

</details>


### [64] [FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning](https://arxiv.org/abs/2507.11430)
*Arnab Mukherjee,Raju Halder,Joydeep Chandra*

Main category: cs.DC

TL;DR: FLsim 是一个模块化、可扩展且高效的联邦学习（FL）模拟框架，旨在简化联邦学习技术的比较和基准测试。


<details>
  <summary>Details</summary>
Motivation: 联邦学习技术发展迅速，但缺乏统一的模拟框架来支持多样化的实验需求。FLsim 提供了一种灵活且高效的工具，以满足研究人员和实践者的需求。

Method: 设计了一个模块化的 FL 模拟框架，支持自定义数据分布、学习算法、网络拓扑、模型聚合方法以及区块链集成。

Result: 实验证明 FLsim 可以有效模拟多种先进联邦学习实验，具有高度的灵活性和功能性。

Conclusion: FLsim 为联邦学习的研究和实际应用提供了强大的工具，标志着该领域的重大进展。

Abstract: Federated Learning (FL) has undergone significant development since its
inception in 2016, advancing from basic algorithms to complex methodologies
tailored to address diverse challenges and use cases. However, research and
benchmarking of novel FL techniques against a plethora of established
state-of-the-art solutions remain challenging. To streamline this process, we
introduce FLsim, a comprehensive FL simulation framework designed to meet the
diverse requirements of FL workflows in the literature. FLsim is characterized
by its modularity, scalability, resource efficiency, and controlled
reproducibility of experimental outcomes. Its easy to use interface allows
users to specify customized FL requirements through job configuration, which
supports: (a) customized data distributions, ranging from non-independent and
identically distributed (non-iid) data to independent and identically
distributed (iid) data, (b) selection of local learning algorithms according to
user preferences, with complete agnosticism to ML libraries, (c) choice of
network topology illustrating communication patterns among nodes, (d)
definition of model aggregation and consensus algorithms, and (e) pluggable
blockchain support for enhanced robustness. Through a series of experimental
evaluations, we demonstrate the effectiveness and versatility of FLsim in
simulating a diverse range of state-of-the-art FL experiments. We envisage that
FLsim would mark a significant advancement in FL simulation frameworks,
offering unprecedented flexibility and functionality for researchers and
practitioners alike.

</details>


### [65] [Uniting the World by Dividing it: Federated Maps to Enable Spatial Applications](https://arxiv.org/abs/2507.11437)
*Sagar Bharadwaj,Srinivasan Seshan,Anthony Rowe*

Main category: cs.DC

TL;DR: 本文提出了一种联邦空间命名系统，以解决现有集中式地图系统在隐私和扩展性方面的不足，适用于增强现实等应用。


<details>
  <summary>Details</summary>
Motivation: 空间网络的发展缺乏一个有效的空间命名系统，现有集中式地图系统无法满足未来应用（如室内增强现实）的需求。

Method: 提出联邦空间命名系统，允许不同方管理自己的地图数据，重新设计相关服务（如地址搜索、导航）。

Result: 联邦系统能提高地图管理的扩展性、隐私性，并支持创新应用。

Conclusion: 联邦空间命名系统是未来空间网络发展的关键，解决了集中式系统的局限性。

Abstract: The emergence of the Spatial Web -- the Web where content is tied to
real-world locations has the potential to improve and enable many applications
such as augmented reality, navigation, robotics, and more. The Spatial Web is
missing a key ingredient that is impeding its growth -- a spatial naming system
to resolve real-world locations to names. Today's spatial naming systems are
digital maps such as Google and Apple maps. These maps and the location-based
services provided on top of these maps are primarily controlled by a few large
corporations and mostly cover outdoor public spaces. Emerging classes of
applications, such as persistent world-scale augmented reality, require
detailed maps of both outdoor and indoor spaces. Existing centralized mapping
infrastructures are proving insufficient for such applications because of the
scale of cartography efforts required and the privacy of indoor map data.
  In this paper, we present a case for a federated spatial naming system, or in
other words, a federated mapping infrastructure. This enables disparate parties
to manage and serve their own maps of physical regions and unlocks scalability
of map management, isolation and privacy of maps. Map-related services such as
address-to-location mapping, location-based search, and routing needs
re-architecting to work on federated maps. We discuss some essential services
and practicalities of enabling these services.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [66] [SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data Generation and Workflow Decomposition](https://arxiv.org/abs/2507.10629)
*Song Cheng,Qiannan Cheng,Linbo Jin,Lei Yi,Guannan Zhang*

Main category: cs.DB

TL;DR: SQLord是一个企业级NL2SQL框架，通过数据逆向生成和复杂查询分解方法，解决了现有框架在复杂业务逻辑和领域特定数据上的不足，并通过综合评估框架显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL框架在面对复杂业务逻辑和缺乏领域特定数据时表现不佳，且评估方法依赖注释数据和可执行数据库环境，难以在实际场景中应用。

Method: SQLord提出了数据逆向生成方法用于监督微调（SFT），并开发了自动化工作流生成器以分解复杂查询。此外，还设计了包含EXE、QSE和SSE的综合评估框架。

Result: 离线测试显著超越现有基线，在线准确率持续超过90%，成功应用于全球最大的B2B电商平台的多个场景。

Conclusion: SQLord在复杂实际场景中表现优异，展示了其有效性和优势。

Abstract: Transforming natural language into SQL queries (NL2SQL) is crucial for
data-driven business applications. Existing frameworks, trained on open-source
datasets, struggle with complex business logic and lack domain-specific data
for fine-tuning. Additionally, evaluation methods often require annotated data
and executable database environments, which are scarce in real-world scenarios.
To address these challenges, we propose SQLord, an enterprise-level NL2SQL
framework. First, SQLord introduces a data reverse generation approach to
convert raw SQL statements into annotated data for supervised fine-tuning
(SFT). Second, it proposes a decomposition method for complex queries using an
automated workflow generator. Additionally, SQLord features a comprehensive
GPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL
Evaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios.
Offline tests significantly outperform state of the art baselines, and online
accuracy consistently exceeds 90, highlighting SQLord's advantages and
effectiveness in complex real world scenarios. SQLord has been successfully
applied across multiple scenarios on the world's largest B2B e-commerce
platform.

</details>


### [67] [LLMATCH: A Unified Schema Matching Framework with Large Language Models](https://arxiv.org/abs/2507.10897)
*Sha Wang,Yuchen Li,Hanhua Xiao,Bing Tian Dai,Roy Ka-Wei Lee,Yanfei Dong,Lambert Deng*

Main category: cs.DB

TL;DR: LLMatch 是一个模块化的模式匹配框架，通过三阶段分解任务并采用新颖的优化策略，显著提升复杂模式匹配的准确性和工程效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理复杂多表模式匹配时表现不佳，迫切需要一种统一且模块化的解决方案。

Method: 将模式匹配分解为三个阶段：模式准备、表候选选择和列级对齐，引入 Rollup 和 Drilldown 双阶段优化策略。

Result: 实验显示 LLMatch 在复杂设置中显著提升匹配准确性，并大幅提高工程效率。

Conclusion: LLMatch 提供了一种高效、模块化的复杂模式匹配解决方案，并引入新基准 SchemaNet 支持未来研究。

Abstract: Schema matching is a foundational task in enterprise data integration, aiming
to align disparate data sources. While traditional methods handle simple
one-to-one table mappings, they often struggle with complex multi-table schema
matching in real-world applications. We present LLMatch, a unified and modular
schema matching framework. LLMatch decomposes schema matching into three
distinct stages: schema preparation, table-candidate selection, and
column-level alignment, enabling component-level evaluation and future-proof
compatibility. It includes a novel two-stage optimization strategy: a Rollup
module that consolidates semantically related columns into higher-order
concepts, followed by a Drilldown module that re-expands these concepts for
fine-grained column mapping. To address the scarcity of complex semantic
matching benchmarks, we introduce SchemaNet, a benchmark derived from
real-world schema pairs across three enterprise domains, designed to capture
the challenges of multi-table schema alignment in practical settings.
Experiments demonstrate that LLMatch significantly improves matching accuracy
in complex schema matching settings and substantially boosts engineer
productivity in real-world data integration.

</details>


### [68] [Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models](https://arxiv.org/abs/2507.10934)
*Xinyuan Liu,Jiahui Chen,Bocheng Hu,Yu Sun,Xinyang Chen,Shaoxu Song*

Main category: cs.DB

TL;DR: TableEG是一个利用大型语言模型（LLM）生成真实错误的框架，通过微调和三元组表示模拟错误生成、检测和纠正任务，解决了数据质量评估中缺乏多样化真实错误数据集的问题。


<details>
  <summary>Details</summary>
Motivation: 由于手动标注错误耗时且不一致，且缺乏多样化的真实错误数据集限制了对错误检测算法的全面评估，因此需要探索合成错误生成作为替代方案。

Method: TableEG采用表格微调策略和三元组表示$(I, T, O)$来建模错误生成、检测和纠正任务，捕捉二维表格中的复杂依赖关系。

Result: 实验表明，TableEG生成的错误在模式和分布上优于基于规则的方法和未微调的LLM生成的错误，且其性能指标与真实错误在几乎所有数据集和检测算法中接近。

Conclusion: TableEG不仅填补了合成与真实错误之间的差距，还为后续的错误检测和纠正任务建立了坚实的基准。

Abstract: Data quality remains an important challenge in data-driven systems, as errors
in tabular data can severely compromise downstream analytics and machine
learning performance. Although numerous error detection algorithms have been
proposed, the lack of diverse, real-world error datasets limits comprehensive
evaluation. Manual error annotation is both time-consuming and inconsistent,
motivating the exploration of synthetic error generation as an alternative. In
this work, we introduce TableEG, a framework that leverages large language
models (LLMs) to generate authentic errors. By employing a table fine-tuning
strategy and a triplet representation $(I, T, O)$ to model error generation,
detection, and correction tasks, TableEG captures the complex dependencies
inherent in two-dimensional tables. Trained on 12 real-world datasets spanning
10 diverse domains, TableEG ensures that the synthesized errors faithfully
reflect authentic error distributions. Experimental results indicate that
errors generated by TableEG exhibit superior pattern and distribution
similarity compared to both rule-based methods and LLM-generated errors without
fine-tuning. Furthermore, performance metrics on TableEG-generated errors
closely align with those on real-world errors across nearly all datasets and
detection algorithms, particularly for machine learning based detection
techniques. Overall, TableEG not only bridges the gap between synthetic and
real-world errors but also establishes a robust benchmark for subsequent error
detection and correction tasks.

</details>


### [69] [TOPJoin: A Context-Aware Multi-Criteria Approach for Joinable Column Search](https://arxiv.org/abs/2507.11505)
*Harsha Kokel,Aamod Khatiwada,Tejaswini Pedapati,Haritha Ananthakrishnan,Oktie Hassanzadeh,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.DB

TL;DR: 论文提出了一种基于上下文感知的多标准方法TOPJoin，用于在数据湖中找到可连接的表格，相比现有方法在学术和实际基准测试中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统基于列相似性的方法不足以在数据湖中识别可连接的表格，需要结合查询列的上下文信息。

Method: 提出了TOPJoin，一种基于多标准的方法，考虑上下文感知的列连接性。

Result: TOPJoin在两个基准测试中表现优于现有基线方法。

Conclusion: 上下文感知的多标准方法能更有效地识别可连接表格。

Abstract: One of the major challenges in enterprise data analysis is the task of
finding joinable tables that are conceptually related and provide meaningful
insights. Traditionally, joinable tables have been discovered through a search
for similar columns, where two columns are considered similar syntactically if
there is a set overlap or they are considered similar semantically if either
the column embeddings or value embeddings are closer in the embedding space.
However, for enterprise data lakes, column similarity is not sufficient to
identify joinable columns and tables. The context of the query column is
important. Hence, in this work, we first define context-aware column
joinability. Then we propose a multi-criteria approach, called TOPJoin, for
joinable column search. We evaluate TOPJoin against existing join search
baselines over one academic and one real-world join search benchmark. Through
experiments, we find that TOPJoin performs better on both benchmarks than the
baselines.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [70] [Device-Level Optimization Techniques for Solid-State Drives: A Survey](https://arxiv.org/abs/2507.10573)
*Tianyu Ren,Yajuan Du,Jinhua Cui,Yina Lv,Qiao Li,Chun Jason Xue*

Main category: cs.AR

TL;DR: 这篇综述全面分析了固态硬盘（SSD）的架构、关键挑战及设备级优化技术，旨在指导未来研究开发高性能、长寿命和安全的下一代SSD。


<details>
  <summary>Details</summary>
Motivation: 随着存储需求的增长，SSD在可扩展性、耐用性、延迟和安全性方面面临挑战，迫切需要技术优化和研究突破。

Method: 首先分析SSD的基本组件（NAND闪存结构、控制器功能、主机接口协议），然后探讨关键挑战（可靠性退化、耐用性限制、延迟变化、安全威胁），最后研究优化技术（纠错机制、FTL增强、新兴架构）。

Result: 总结了SSD的当前技术状态和优化方向，包括错误纠正、FTL改进和新兴架构（如ZNS SSD和FDP）。

Conclusion: 未来的研究应关注QLC/PLC NAND可扩展性、性能-可靠性权衡及AI/LLM工作负载的SSD优化，以实现下一代SSD的目标。

Abstract: Solid-state drives (SSDs) have revolutionized data storage with their high
performance, energy efficiency, and reliability. However, as storage demands
grow, SSDs face critical challenges in scalability, endurance, latency, and
security. This survey provides a comprehensive analysis of SSD architecture,
key challenges, and device-level optimization techniques. We first examine the
fundamental components of SSDs, including NAND flash memory structures, SSD
controller functionalities (e.g., address mapping, garbage collection, wear
leveling), and host interface protocols (SATA, SAS, NVMe). Next, we discuss
major challenges such as reliability degradation, endurance limitations,
latency variations, and security threats (e.g., secure deletion, ransomware
defense). We then explore advanced optimization techniques, including error
correction mechanisms, flash translation layer (FTL) enhancements, and emerging
architectures like zoned namespace (ZNS) SSDs and flexible data placement
(FDP). Finally, we highlight open research challenges, such as QLC/PLC NAND
scalability, performance-reliability trade-offs, and SSD optimizations for
AI/LLM workloads. This survey aims to guide future research in developing
next-generation SSDs that balance performance, longevity, and security in
evolving storage ecosystems.

</details>


### [71] [SPICEAssistant: LLM using SPICE Simulation Tools for Schematic Design of Switched-Mode Power Supplies](https://arxiv.org/abs/2507.10639)
*Simon Nau,Jan Krummenauer,André Zimmermann*

Main category: cs.AR

TL;DR: 研究人员探讨了大型语言模型（LLM）在电子电路设计中的应用，提出了SPICEAssistant框架，集成SPICE工具提升LLM在开关电源设计中的表现。


<details>
  <summary>Details</summary>
Motivation: 验证LLM在电子设计自动化（EDA）领域的潜力，特别是在复杂的开关电源设计任务中，克服其无法直接解读SPICE仿真结果的限制。

Method: 开发SPICEAssistant框架，为LLM提供SPICE接口工具，使其能灵活修改电路并通过仿真反馈优化设计。通过256个任务的基准测试评估性能。

Result: 仿真反馈显著提升了LLM的设计能力，多次迭代后表现更优。SPICEAssistant在基准测试中比单独使用GPT-4o的性能高出约38%。

Conclusion: 结合仿真工具的框架能有效增强LLM在电路设计任务中的能力，为EDA领域的LLM应用提供了新方向。

Abstract: State-of-the-art large language models (LLMs) show high performance across a
wide range of tasks in many domains of science. In the field of electronic
design automation (EDA), it is yet to be determined to what extent they are
capable to understand, adapt, and dimension electronic circuits. This paper
focuses on the application of LLMs to switched-mode power supply (SMPS) design
on printed circuit boards (PCBs). Particular challenges for LLMs in this
context include their limited ability to interpret results from key simulation
tools like SPICE and the multi-step design process. To address these
challenges, we suggest SPICEAssistant, a framework that provides a broad
selection of tools to an LLM. The tools serve as an interface to SPICE,
allowing the LLM to interact flexibly with the simulator to estimate the impact
of its modifications to the circuit. To evaluate the performance of
SPICEAssistant, we defined a benchmark consisting of 256 questions testing the
ability to adapt circuit netlists to fulfil different SMPS design tasks. The
benchmarking results show that simulation feedback effectively improves SMPS
design capabilities of LLMs. An increasing number of simulation iterations
leads to enhanced performance. The SPICEAssistant framework significantly
outperforms the standalone LLM GPT-4o on the benchmark by approximately 38%.

</details>


### [72] [LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration](https://arxiv.org/abs/2507.10748)
*Jason Ho,James A. Boyle,Linshen Liu,Andreas Gerstlauer*

Main category: cs.AR

TL;DR: LASANA是一种利用机器学习快速建模和模拟神经形态系统中模拟子块的新方法，相比传统SPICE模拟能提高三个数量级的速度，且误差小于8%。


<details>
  <summary>Details</summary>
Motivation: 神经形态系统需要更高效的能量处理AI工作负载，但现有的混合信号设计工具模拟速度慢，行为建模方法又限于特定架构。

Method: 提出LASANA方法，通过SPICE级模拟数据训练ML模型，预测电路的能耗、性能和模拟/数字接口行为。

Result: 应用于模拟交叉数组和脉冲神经元电路时，LASANA比SPICE快三个数量级，能耗、延迟和行为误差分别小于7%、8%和2%。

Conclusion: LASANA为神经形态系统的快速探索和协同设计提供了高效工具，显著提升了模拟速度和准确性。

Abstract: Neuromorphic systems using in-memory or event-driven computing are motivated
by the need for more energy-efficient processing of artificial intelligence
workloads. Emerging neuromorphic architectures aim to combine traditional
digital designs with the computational efficiency of analog computing and novel
device technologies. A crucial problem in the rapid exploration and co-design
of such architectures is the lack of tools for fast and accurate modeling and
simulation. Typical mixed-signal design tools integrate a digital simulator
with an analog solver like SPICE, which is prohibitively slow for large
systems. By contrast, behavioral modeling of analog components is faster, but
existing approaches are fixed to specific architectures with limited energy and
performance modeling. In this paper, we propose LASANA, a novel approach that
leverages machine learning to derive data-driven surrogate models of analog
sub-blocks in a digital backend architecture. LASANA uses SPICE-level
simulations of a circuit to train ML models that predict circuit energy,
performance, and behavior at analog/digital interfaces. Such models can provide
energy and performance annotation on top of existing behavioral models or
function as replacements to analog simulation. We apply LASANA to an analog
crossbar array and a spiking neuron circuit. Running MNIST and spiking MNIST,
LASANA surrogates demonstrate up to three orders of magnitude speedup over
SPICE, with energy, latency, and behavioral error less than 7%, 8%, and 2%,
respectively.

</details>


### [73] [OpenGCRAM: An Open-Source Gain Cell Compiler Enabling Design-Space Exploration for AI Workloads](https://arxiv.org/abs/2507.10849)
*Xinxin Wang,Lixian Yan,Shuhan Liu,Luke Upton,Zhuoqi Cai,Yiming Tan,Shengman Li,Koustav Jana,Peijing Li,Jesse Cirimelli-Low,Thierry Tambe,Matthew Guthaus,H. -S. Philip Wong*

Main category: cs.AR

TL;DR: OpenGCRAM是一个开源的GCRAM编译器，用于快速生成符合工艺要求的GCRAM电路设计和布局，并提供性能和功耗模拟。


<details>
  <summary>Details</summary>
Motivation: GCRAM因其高密度、低功耗和灵活性成为专用加速器片上内存的理想选择，但设计和优化过程耗时。

Method: 开发了OpenGCRAM编译器，通过用户配置生成GCRAM电路设计和布局，并进行性能模拟。

Result: OpenGCRAM能够显著减少设计时间，确保工艺合规性，并生成满足多样化需求的优化内存块。

Conclusion: OpenGCRAM为GCRAM设计和优化提供了高效、准确的解决方案，适用于多种应用场景。

Abstract: Gain Cell memory (GCRAM) offers higher density and lower power than SRAM,
making it a promising candidate for on-chip memory in domain-specific
accelerators. To support workloads with varying traffic and lifetime metrics,
GCRAM also offers high bandwidth, ultra low leakage power and a wide range of
retention times, which can be adjusted through transistor design (like
threshold voltage and channel material) and on-the-fly by changing the
operating voltage. However, designing and optimizing GCRAM sub-systems can be
time-consuming. In this paper, we present OpenGCRAM, an open-source GCRAM
compiler capable of generating GCRAM bank circuit designs and DRC- and
LVS-clean layouts for commercially available foundry CMOS, while also providing
area, delay, and power simulations based on user-specified configurations
(e.g., word size and number of words). OpenGCRAM enables fast, accurate,
customizable, and optimized GCRAM block generation, reduces design time, ensure
process compliance, and delivers performance-tailored memory blocks that meet
diverse application requirements.

</details>


### [74] [Mapping Fusion: Improving FPGA Technology Mapping with ASIC Mapper](https://arxiv.org/abs/2507.10912)
*Cunxi Yu*

Main category: cs.AR

TL;DR: FuseMap是一个利用强化学习改进FPGA LUT映射的框架，结合了ASIC技术映射的优势，提升了映射精度并减少了延迟和面积。


<details>
  <summary>Details</summary>
Motivation: ASIC技术映射可能提升LUT映射器的性能，两者可以协同工作，进而优化FPGA设计流中的映射效果。

Method: 提出FuseMap框架，利用强化学习在单元选择时做出设计特定决策，结合标准单元映射和LUT映射的增量式工作方式。

Result: 在多个基准测试、技术库和映射器上验证，FuseMap提高了映射精度，同时减少了延迟和面积。

Conclusion: FuseMap成功地将ASIC技术映射与LUT映射结合，优化了FPGA设计流中的映射性能。

Abstract: LUT (Look-Up Table) mapping is a critical step in FPGA logic synthesis, where
a logic network is transformed into a form that can be directly implemented
using the FPGA's LUTs. An FPGA LUT is a flexible digital memory structure that
can implement any logic function of a limited number of inputs, typically 4 to
6 inputs, depending on the FPGA architecture. The goal of LUT mapping is to map
the Boolean network into LUTs, where each LUT can implement any function with a
fixed number of inputs. In parallel to FPGA technology mapping, ASIC technology
mapping maps the Boolean network to user-defined standard cells, which has
traditionally been developed separately from LUT mapping algorithms. However,
in this work, our motivating examples demonstrate that ASIC technology mappers
can potentially improve the performance of LUT mappers, such that standard cell
mapping and LUT mapping work in an incremental manner.
  Therefore, we propose the FuseMap framework, which explores this opportunity
to improve LUT mapping in the FPGA design flow by utilizing reinforcement
learning to make design-specific choices during cell selection. The
effectiveness of FuseMap is evaluated on a wide range of benchmarks, different
technology libraries, and technology mappers. The experimental results
demonstrate that FuseMap achieves higher mapping accuracy while reducing delay
and area across diverse circuit designs collected from ISCAS 85/89, ITC/ISCAS
99, VTR 8.0, and EPFL benchmarks.

</details>


### [75] [Security Enclave Architecture for Heterogeneous Security Primitives for Supply-Chain Attacks](https://arxiv.org/abs/2507.10971)
*Kshitij Raj,Atri Chatterjee,Patanjali SLPSK,Swarup Bhunia,Sandip Ray*

Main category: cs.AR

TL;DR: 论文介绍了CITADEL框架，用于简化SoC安全架构设计，支持定制化安全机制，并通过实际案例验证其低资源开销。


<details>
  <summary>Details</summary>
Motivation: 设计SoC安全架构复杂且易出错，需一种模块化方案以提升效率。

Method: 提出CITADEL框架，提供可配置的插件式子系统，定制IP块以应对不同威胁。

Result: 案例研究表明CITADEL资源开销低，适合实际应用。

Conclusion: CITADEL为SoC安全提供高效、灵活且低成本的解决方案。

Abstract: Designing secure architectures for system-on-chip (SoC) platforms is a highly
intricate and time-intensive task, often requiring months of development and
meticulous verification. Even minor architectural oversights can lead to
critical vulnerabilities that undermine the security of the entire chip. In
response to this challenge, we introduce CITADEL, a modular security framework
aimed at streamlining the creation of robust security architectures for SoCs.
CITADEL offers a configurable, plug-and-play subsystem composed of custom
intellectual property (IP) blocks, enabling the construction of diverse
security mechanisms tailored to specific threats. As a concrete demonstration,
we instantiate CITADEL to defend against supply-chain threats, illustrating how
the framework adapts to one of the most pressing concerns in hardware security.
This paper explores the range of obstacles encountered when building a unified
security architecture capable of addressing multiple attack vectors and
presents CITADEL's strategies for overcoming them. Through several real-world
case studies, we showcase the practical implementation of CITADEL and present a
thorough evaluation of its impact on silicon area and power consumption across
various ASIC technologies. Results indicate that CITADEL introduces only
minimal resource overhead, making it a practical solution for enhancing SoC
security.

</details>


### [76] [SystolicAttention: Fusing FlashAttention within a Single Systolic Array](https://arxiv.org/abs/2507.11331)
*Jiawei Lin,Guokai Chen,Yuanlong Li,Thomas Bourgeat*

Main category: cs.AR

TL;DR: 提出FSA架构，改进Systolic阵列以高效运行FlashAttention，无需外部向量单元，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有Systolic阵列在执行FlashAttention时利用率低，因频繁数据交换和不适合的非矩阵操作。

Method: 设计FSA架构和SystolicAttention调度算法，实现全流程在Systolic阵列内运行，提升利用率。

Result: FSA相比AWS和Google的加速器，分别提升1.77x和4.83x的注意力计算利用率，仅增加10%面积开销。

Conclusion: FSA有效解决了现有Systolic阵列的局限性，显著提升了FlashAttention的执行效率。

Abstract: Transformer models rely heavily on scaled dot-product attention (SDPA),
typically implemented using the FlashAttention algorithm. However, current
systolic-array-based accelerators face significant challenges when executing
FlashAttention. Systolic arrays can only achieve high utilization for
consecutive and large matrix multiplications. In contrast, FlashAttention
requires frequently interleaved matrix multiplications and softmax operations.
  The frequent data swaps between the systolic array and external vector units
result in low systolic array utilization. This is further exacerbated by the
fact that softmax involves numerous non-matrix operations, which are not
well-suited for systolic arrays. Moreover, the concurrent execution of matrix
multiplication on systolic arrays and softmax on vector units leads to register
file and SRAM port contention, further degrading performance.
  To overcome these limitations, we propose FSA, an enhanced systolic array
architecture that enables the entire FlashAttention algorithm to run entirely
within a single systolic array, eliminating the need for external vector units.
At the core of FSA is SystolicAttention, a novel scheduling algorithm that maps
FlashAttention operations onto systolic arrays with fine-grained, element-wise
overlap. This significantly improves array utilization while preserving the
original floating-point operation order to maintain numerical stability.
  We implement FSA in synthesizable RTL and evaluate its performance against
state-of-the-art commercial accelerators. Our results show that FSA achieves
1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS
NeuronCore-v2 and Google TPUv5e, respectively, with only about 10% area
overhead.

</details>


### [77] [Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques](https://arxiv.org/abs/2507.11506)
*Yiqi Liu,Yuqi Xue,Noelle Crawford,Jilong Xue,Jian Huang*

Main category: cs.AR

TL;DR: Elk是一个深度学习编译器框架，通过联合优化计算、通信和I/O性能，提升ICCA芯片的效率，实现了接近理想性能的效果。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型需求的增长，ICCA芯片需要在计算、通信和I/O之间权衡效率，但现有方法难以解决这一挑战。

Method: Elk通过参数化性能因素，并使用新的运算符调度策略和成本感知内存分配算法，生成全局优化的执行计划。

Result: Elk在ICCA芯片上实现了94%的理想性能，支持大型DL模型，并能辅助新ICCA芯片的架构设计探索。

Conclusion: Elk通过联合优化显著提升了ICCA芯片的效率，为未来芯片设计提供了实用工具。

Abstract: To meet the increasing demand of deep learning (DL) models, AI chips are
employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency
interconnect for direct inter-core data exchange. However, it is not easy to
explore the efficiency of these inter-core connected AI (ICCA) chips, due to a
fundamental tussle among compute (per-core execution), communication
(inter-core data exchange), and I/O (off-chip data access).
  In this paper, we develop Elk, a DL compiler framework to maximize the
efficiency of ICCA chips by jointly trading off all the three performance
factors discussed above. Elk structures these performance factors into
configurable parameters and forms a global trade-off space in the DL compiler.
To systematically explore this space and maximize overall efficiency, Elk
employs a new inductive operator scheduling policy and a cost-aware on-chip
memory allocation algorithm. It generates globally optimized execution plans
that best overlap off-chip data loading and on-chip execution. To examine the
efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip
IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different
interconnect network topologies. Elk achieves 94% of the ideal roofline
performance of ICCA chips on average, showing the benefits of supporting large
DL models on ICCA chips. We also show Elk's capability of enabling architecture
design space exploration for new ICCA chip development.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [78] [Formal Verification of Variational Quantum Circuits](https://arxiv.org/abs/2507.10635)
*Nicola Assolini,Luca Marzari,Isabella Mastroeni,Alessandra di Pierro*

Main category: quant-ph

TL;DR: 这篇论文提出了首次对变分量子电路（VQCs）的正式验证问题进行理论和实践研究，探讨了基于区间的可达性技术在量子环境中的适用性和局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管变分量子电路在量子机器学习算法中占据核心地位，但目前缺乏类似经典模型的正式验证框架来保证其鲁棒性，特别是对抗输入的脆弱性。

Method: 受深度学习中的抽象解释方法启发，作者研究了量子环境下基于区间的可达性技术，并提出了一个新的语义框架来形式化定义和分析验证问题。

Result: 研究揭示了量子特有的状态规范化等因素会引入变量间的依赖关系，从而挑战现有方法。作者通过标准验证基准测试展示了他们的方法。

Conclusion: 该研究为变分量子电路的正式验证提供了理论基础和实践方法，填补了现有研究中的空白。

Abstract: Variational quantum circuits (VQCs) are a central component of many quantum
machine learning algorithms, offering a hybrid quantum-classical framework
that, under certain aspects, can be considered similar to classical deep neural
networks. A shared aspect is, for instance, their vulnerability to adversarial
inputs, small perturbations that can lead to incorrect predictions. While
formal verification techniques have been extensively developed for classical
models, no comparable framework exists for certifying the robustness of VQCs.
Here, we present the first in-depth theoretical and practical study of the
formal verification problem for VQCs. Inspired by abstract interpretation
methods used in deep learning, we analyze the applicability and limitations of
interval-based reachability techniques in the quantum setting. We show that
quantum-specific aspects, such as state normalization, introduce inter-variable
dependencies that challenge existing approaches. We investigate these issues by
introducing a novel semantic framework based on abstract interpretation, where
the verification problem for VQCs can be formally defined, and its complexity
analyzed. Finally, we demonstrate our approach on standard verification
benchmarks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR框架通过整合Perspective-Aware AI（PAi）与XR，实现基于用户身份的上下文感知体验，解决现有系统的浅层用户建模和认知上下文不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI增强的XR系统在用户建模和认知上下文方面存在局限性，未能提供足够的适应性和沉浸感。

Method: PAiR基于Chronicles（多模态数字足迹学习到的身份模型）构建，采用闭环系统将动态用户状态与沉浸式环境连接。

Result: 在Unity的OpenDome引擎中实现两个概念验证场景，展示了PAiR的实用性。

Conclusion: PAiR为人类-AI交互开辟了新方向，将基于视角的身份模型嵌入沉浸式系统。

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [80] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: SAMEP协议解决了AI代理间内存共享的限制，实现了持久、安全且可语义搜索的内存共享。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理架构的内存短暂性限制了跨会话和代理边界的协作与知识共享。

Method: SAMEP采用分布式内存仓库，结合向量语义搜索、加密访问控制（AES-256-GCM）和标准化API。

Result: 实验显示减少了73%冗余计算，提高了89%上下文相关性得分，并符合监管要求。

Conclusion: SAMEP为AI代理生态系统提供了持久协作的新范式，同时确保安全与隐私。

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [81] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: 研究了如何精确计算集成电路中的最大延迟，而非使用传统静态时序分析的近似值，通过答案集编程（ASP）建模并验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 最大延迟直接影响同步系统（如CPU）的时钟频率，传统静态时序分析提供的上限值可能导致性能未被充分利用，因此需要精确计算。

Method: 将问题建模为答案集编程（ASP），并提出了高效的编码方案。

Result: 实验结果表明，ASP能有效解决硬件设计中的复杂问题。

Conclusion: ASP为精确计算集成电路最大延迟提供了可行的解决方案。

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [82] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 综述了“代理网络”（WoA）的进化历程，提出四维分类法揭示了从传统语义网和多代理系统到现代基于LLM的智能代理的范式转变。


<details>
  <summary>Details</summary>
Motivation: 当前对代理网络的研究碎片化，忽视了历史脉络。论文旨在统一分析框架，揭示领域发展轨迹。

Method: 引入四轴分类法（语义基础、通信范式、智能中心、发现机制），系统比较不同代际的代理架构。

Result: 发现智能中心从外部数据或平台转向代理核心模型（LLM），这是现代Agentic AI的基础。新协议不足，需解决社会技术挑战。

Conclusion: 未来研究应聚焦去中心化身份、经济模型、安全与治理，以构建开放可信的代理网络生态。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [83] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: 论文探讨了如何通过预处理数据，使用人工神经网络有效检测AI辅助任务，尤其是在抽象任务中。


<details>
  <summary>Details</summary>
Motivation: 随着AI在复杂任务中的广泛应用，检测其辅助变得重要，但抽象任务数据难以分类。

Method: 构建四种神经网络友好的图像表示和一个时间序列表示，并用经典深度学习架构进行测试。

Result: 实验证明，预处理后的数据可被模型有效分类，尤其是结合时空信息的多模态架构表现更优。

Conclusion: 预处理和时空编码对检测AI辅助具有关键作用，尤其是在抽象任务中。

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [84] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: F2STrans 是一种分阶段的代码翻译方法，通过功能学习和风格学习提升 LLMs 的翻译正确性和可读性，并在新基准测试中显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在代码翻译中难以同时保证正确性和可读性，限制了其在实际软件开发中的应用。

Method: 提出 F2STrans 方法，分功能学习（优化正确性）和风格学习（提升可读性）两阶段，并引入新的代码翻译基准。

Result: 实验表明 F2STrans 显著提升性能，小模型 Qwen-1.5B 在多种场景下优于 Qwen-32B 和 GPT-4。

Conclusion: F2STrans 为代码翻译任务提供了一种高效的方法，兼具功能性和风格优化潜力。

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [85] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: 提出了一种新方法，结合代码文本建模和结构化建模的优势，以弥补现有LLM在代码分析能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 探讨当前代码语言模型（LLM）在结构化分析能力上的局限性，尤其是对控制流和数据流的推理能力不足，需要结合结构化和文本建模的优势。

Method: 引入了一种新颖的方法，将代码作为文本和结构化形式的建模结合起来。

Result: 尚未提及具体结果，但目标是提升代码生成的规模化和结构化分析能力。

Conclusion: 新方法旨在解决现有LLM的局限性，结合文本和结构化建模的优势，为代码理解和生成提供更全面的支持。

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [86] [On the Complexity of the Skolem Problem at Low Orders](https://arxiv.org/abs/2507.11234)
*Piotr Bacik,Joël Ouaknine,James Worrell*

Main category: cs.CC

TL;DR: 提出了一种随机化算法，解决了线性递推序列（LRS）在给定边界内的Skolem问题，并改进了现有结果。


<details>
  <summary>Details</summary>
Motivation: Skolem问题关于线性递推序列是否存在零项的一般解仍未解决，本研究旨在解决其有界版本。

Method: 采用随机化算法，结合p-adic分析和算术电路恒等式测试，在多项式时间内处理有界问题。

Result: 证明了至多4阶LRS的Skolem问题属于coRP类，优于之前的上界。

Conclusion: 算法为有界Skolem问题提供了高效解决方案，但其时间复杂度对LRS阶数仍呈指数级。

Abstract: The Skolem Problem asks to determine whether a given linear recurrence
sequence (LRS) $\langle u_n \rangle_{n=0}^\infty$ over the integers has a zero
term, that is, whether there exists $n$ such that $u_n = 0$. Decidability of
the problem is open in general, with the most notable positive result being a
decision procedure for LRS of order at most 4.
  In this paper we consider a bounded version of the Skolem Problem, in which
the input consists of an LRS $\langle u_n \rangle_{n=0}^\infty$ and a bound $N
\in \mathbb N$ (with all integers written in binary), and the task is to
determine whether there exists $n\in\{0,\ldots,N\}$ such that $u_n=0$. We give
a randomised algorithm for this problem that, for all $d\in \mathbb N$, runs in
polynomial time on the class of LRS of order at most $d$. As a corollary we
show that the (unrestricted) Skolem Problem for LRS of order at most 4 lies in
$\mathsf{coRP}$, improving the best previous upper bound of
$\mathsf{NP}^{\mathsf{RP}}$.
  The running time of our algorithm is exponential in the order of the LRS -- a
dependence that appears necessary in view of the $\mathsf{NP}$-hardness of the
Bounded Skolem Problem. However, even for LRS of a fixed order, the problem
involves detecting zeros within an exponentially large range. For this, our
algorithm relies on results from $p$-adic analysis to isolate polynomially many
candidate zeros and then test in randomised polynomial time whether each
candidate is an actual zero by reduction to arithmetic-circuit identity
testing.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [87] [Supporting SENĆOTEN Language Documentation Efforts with Automatic Speech Recognition](https://arxiv.org/abs/2507.10827)
*Mengzhe Geng,Patrick Littell,Aidan Pine,PENÁĆ,Marc Tessier,Roland Kuhn*

Main category: cs.SD

TL;DR: 论文提出了一种基于ASR的文档化流程，结合TTS和跨语言迁移学习，以支持SENĆOTEN语言的复兴，实验结果显示WER和CER显著提升。


<details>
  <summary>Details</summary>
Motivation: SENĆOTEN语言因殖民政策面临消失风险，数字化技术（如ASR）有望加速语言文档化和教育资源创建，但数据有限和词汇变异带来挑战。

Method: 采用ASR驱动的文档化流程，结合增强的TTS语音数据、跨语言迁移学习和SFMs，并通过n-gram语言模型最大化数据利用。

Result: 测试集WER为19.34%，CER为5.09%；过滤小错误后，WER提升至14.32%（未见过词汇26.48%），CER降至3.45%。

Conclusion: ASR驱动流程有效支持SENĆOTEN语言文档化，展示了技术对濒危语言复兴的潜力。

Abstract: The SEN\'{C}OTEN language, spoken on the Saanich peninsula of southern
Vancouver Island, is in the midst of vigorous language revitalization efforts
to turn the tide of language loss as a result of colonial language policies. To
support these on-the-ground efforts, the community is turning to digital
technology. Automatic Speech Recognition (ASR) technology holds great promise
for accelerating language documentation and the creation of educational
resources. However, developing ASR systems for SEN\'{C}OTEN is challenging due
to limited data and significant vocabulary variation from its polysynthetic
structure and stress-driven metathesis. To address these challenges, we propose
an ASR-driven documentation pipeline that leverages augmented speech data from
a text-to-speech (TTS) system and cross-lingual transfer learning with Speech
Foundation Models (SFMs). An n-gram language model is also incorporated via
shallow fusion or n-best restoring to maximize the use of available data.
Experiments on the SEN\'{C}OTEN dataset show a word error rate (WER) of 19.34%
and a character error rate (CER) of 5.09% on the test set with a 57.02%
out-of-vocabulary (OOV) rate. After filtering minor cedilla-related errors, WER
improves to 14.32% (26.48% on unseen words) and CER to 3.45%, demonstrating the
potential of our ASR-driven pipeline to support SEN\'{C}OTEN language
documentation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [88] [Deterministic Lower Bounds for $k$-Edge Connectivity in the Distributed Sketching Model](https://arxiv.org/abs/2507.11257)
*Peter Robinson,Ming Ming Tan*

Main category: cs.DS

TL;DR: 研究了无向图中k边连通性问题的分布式草图模型，首次提出了确定性算法下的下界证明，表明最坏情况下的消息长度为Ω(k) bits。


<details>
  <summary>Details</summary>
Motivation: 填补分布式草图模型中确定性算法下图的连通性问题的下界空白，特别是对k边连通性问题。

Method: 引入了新的下界图构造和称为UniqueOverlap的三方通信复杂度问题，利用交叉相交集族的结果证明其难度。

Result: 证明了对于k边连通性问题，确定性算法的消息长度下界为Ω(k) bits，且不引入任何错误概率。

Conclusion: 为分布式草图模型中的连通性问题提供了新的下界证明方法，适用于确定性算法。

Abstract: We study the $k$-edge connectivity problem on undirected graphs in the
distributed sketching model, where we have $n$ nodes and a referee. Each node
sends a single message to the referee based on its 1-hop neighborhood in the
graph, and the referee must decide whether the graph is $k$-edge connected by
taking into account the received messages.
  We present the first lower bound for deciding a graph connectivity problem in
this model with a deterministic algorithm. Concretely, we show that the worst
case message length is $\Omega( k )$ bits for $k$-edge connectivity, for any
super-constant $k = O(\sqrt{n})$. Previously, only a lower bound of $\Omega(
\log^3 n )$ bits was known for ($1$-edge) connectivity, due to Yu (SODA 2021).
In fact, our result is the first super-polylogarithmic lower bound for a
connectivity decision problem in the distributed graph sketching model.
  To obtain our result, we introduce a new lower bound graph construction, as
well as a new 3-party communication complexity problem that we call
UniqueOverlap. As this problem does not appear to be amenable to reductions to
existing hard problems such as set disjointness or indexing due to correlations
between the inputs of the three players, we leverage results from
cross-intersecting set families to prove the hardness of UniqueOverlap for
deterministic algorithms. Finally, we obtain the sought lower bound for
deciding $k$-edge connectivity via a novel simulation argument that, in
contrast to previous works, does not introduce any probability of error and
thus works for deterministic algorithms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [89] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: 本研究介绍了MH-FSF框架，一个用于特征选择方法的统一平台，旨在解决当前研究中基准测试不足和数据集依赖性强的问题，支持17种方法的实现和10个公开数据集的系统评估。


<details>
  <summary>Details</summary>
Motivation: 当前特征选择研究存在基准测试不足和依赖专有数据集的问题，影响再现性和性能，因此需要一种统一的平台来促进方法比较和再现。

Method: 通过合作研究开发了MH-FSF框架，实现了17种特征选择方法（11种经典，6种领域特定），并在10个公开的Android恶意软件数据集上进行系统评估。

Result: 结果显示性能在平衡和不平衡数据集上存在差异，强调了数据预处理和选择标准的重要性。

Conclusion: MH-FSF框架为特征选择研究提供了统一的比较平台，有助于方法一致性和严谨性，推动了Android恶意软件检测领域的新研究方向。

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [90] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: DALI-PD是一个可扩展的框架，用于生成合成布局热图，以加速物理设计中的机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 机器学习在物理设计任务中表现出显著潜力，但模型泛化性受限于高质量、大规模训练数据集的可用性。公开数据集稀缺、静态且生成缓慢，亟需解决。

Method: DALI-PD利用扩散模型快速生成多样化的布局热图，包括电源、IR压降、拥塞、宏布局和单元密度图等。

Result: DALI-PD生成的数据集包含20,000多种布局配置，其热图接近真实布局，并提升了IR压降和拥塞预测等下游任务的准确性。

Conclusion: DALI-PD为物理设计研究提供了高效的数据生成工具，解决了数据稀缺和更新缓慢的问题，推动了机器学习在该领域的应用。

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [91] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: 论文研究了非线性和非平稳时间序列数据分布对联邦学习性能的影响，并探讨了去趋势技术对预测模型表现的提升作用。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的增多，传统集中式数据分析方法存在延迟和成本问题。联邦学习（FL）成为替代方案，但非线性和非平稳数据会影响其预测准确性。

Method: 生成合成时间序列数据集，使用LSTM模型在集中式和FL环境中训练，并评估去趋势技术对真实数据的影响。

Result: FL在非线性数据分布下表现不如集中式方法，适当去趋势技术能提升FL性能。

Conclusion: 研究揭示了FL在非平稳数据中的局限性，强调了去趋势技术的重要性，为实际应用提供了改进方向。

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [92] [Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health](https://arxiv.org/abs/2507.10695)
*Jabari Kwesi,Jiaxun Cao,Riya Manchanda,Pardis Emami-Naeini*

Main category: cs.CY

TL;DR: 研究人员通过访谈发现，用户在使用通用LLM聊天机器人进行心理健康支持时存在隐私与安全误解，缺乏风险意识，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 探讨用户在使用通用LLM聊天机器人进行心理健康管理时的隐私与安全问题。

Method: 对21名美国参与者进行半结构化访谈。

Result: 发现用户对隐私保护的误解（如误以为受HIPAA保护）并提出“无形脆弱性”概念。

Conclusion: 建议改进措施以更有效地保护用户的心理健康数据隐私。

Abstract: Individuals are increasingly relying on large language model (LLM)-enabled
conversational agents for emotional support. While prior research has examined
privacy and security issues in chatbots specifically designed for mental health
purposes, these chatbots are overwhelmingly "rule-based" offerings that do not
leverage generative AI. Little empirical research currently measures users'
privacy and security concerns, attitudes, and expectations when using
general-purpose LLM-enabled chatbots to manage and improve mental health.
Through 21 semi-structured interviews with U.S. participants, we identified
critical misconceptions and a general lack of risk awareness. Participants
conflated the human-like empathy exhibited by LLMs with human-like
accountability and mistakenly believed that their interactions with these
chatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures
with a licensed therapist. We introduce the concept of "intangible
vulnerability," where emotional or psychological disclosures are undervalued
compared to more tangible forms of information (e.g., financial or
location-based data). To address this, we propose recommendations to safeguard
user mental health disclosures with general-purpose LLM-enabled chatbots more
effectively.

</details>


### [93] [Queueing for Civility: User Perspectives on Regulating Emotions in Online Conversations](https://arxiv.org/abs/2507.11477)
*Akriti Verma,Shama Islam,Valeh Moghaddam,Adnan Anwar*

Main category: cs.CY

TL;DR: 研究提出了一种评论排队机制，通过延迟发布评论来减少冲动和有毒言论，效果显著且用户反馈积极。


<details>
  <summary>Details</summary>
Motivation: 网络对话常因恶意言论引发情绪困扰和冲突，现有研究多关注事后内容审核，实时情绪管理尚未充分探索。

Method: 采用混合研究方法，分析了Reddit上15,000次用户互动，并结合用户调查评估评论排队机制的效果。

Result: 机制可减少15%的仇恨言论和愤怒内容，93.3%用户认为其有助于平静讨论，83%认为能减少冲动评论。

Conclusion: 评论排队机制有效平衡网络对话情绪，用户情绪状态影响其对该机制的接受度，平静用户更认可其价值。

Abstract: Online conversations are often interrupted by trolling, which causes
emotional distress and conflict among users. Previous research has focused on
moderating harmful content after it has been posted, but ways to manage
emotions in real-time remain unexplored. This study suggests a comment queuing
mechanism that delays comment publishing, encourages self-reflection, and
reduces the impact of impulsive and toxic comments. To assess the efficacy of
this approach, a mixed-method research design is used. An analysis of 15,000
user interactions on Reddit showed that this approach could reduce the spread
of hate speech and anger by up to 15%, with only 4% of comments being delayed
for about 47 seconds on average. We also surveyed users for feedback on the
mechanism. The results showed that 93. 3\% of the participants thought that the
queuing mechanism could help calm the discussions and showed interest in seeing
it used on social media platforms. Furthermore, 83% believed it would reduce
impulsive comments and balance the emotional tone in conversations. We found a
strong link between users' typical emotional states while using social media
and their perceptions of the delay, with calm users finding the mechanism
helpful and frustrated users anticipating frustration.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [94] [Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants](https://arxiv.org/abs/2507.11460)
*Jacinto Colan,Ana Davila,Yutaro Yamada,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 系统性综述研究了自主手术机器人助手（ASARs）的发展与挑战，重点关注其在手术中为医生提供有效支持的场景。


<details>
  <summary>Details</summary>
Motivation: 提升手术中机器人与人类的协作能力，以应对复杂手术的需求。

Method: 遵循PRISMA指南，通过IEEE Xplore、Scopus和Web of Science数据库筛选32项研究进行分析。

Result: 发现两种主要协作模式（远程操作与直接交互），当前研究多集中于内窥镜引导，同时在自主工具操作方面有所进展。

Conclusion: 未来需解决机器人行为与医生偏好的一致性、信息交换等问题，以提高协作的可靠性与安全性。

Abstract: Human-robot collaboration in surgery represents a significant area of
research, driven by the increasing capability of autonomous robotic systems to
assist surgeons in complex procedures. This systematic review examines the
advancements and persistent challenges in the development of autonomous
surgical robotic assistants (ASARs), focusing specifically on scenarios where
robots provide meaningful and active support to human surgeons. Adhering to the
PRISMA guidelines, a comprehensive literature search was conducted across the
IEEE Xplore, Scopus, and Web of Science databases, resulting in the selection
of 32 studies for detailed analysis. Two primary collaborative setups were
identified: teleoperation-based assistance and direct hands-on interaction. The
findings reveal a growing research emphasis on ASARs, with predominant
applications currently in endoscope guidance, alongside emerging progress in
autonomous tool manipulation. Several key challenges hinder wider adoption,
including the alignment of robotic actions with human surgeon preferences, the
necessity for procedural awareness within autonomous systems, the establishment
of seamless human-robot information exchange, and the complexities of skill
acquisition in shared workspaces. This review synthesizes current trends,
identifies critical limitations, and outlines future research directions
essential to improve the reliability, safety, and effectiveness of human-robot
collaboration in surgical environments.

</details>


### [95] [LLM-based ambiguity detection in natural language instructions for collaborative surgical robots](https://arxiv.org/abs/2507.11525)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 提出了一种基于大语言模型（LLM）的框架，用于检测协作手术场景中的指令模糊性，通过多种提示技术和一致性预测提高分类准确性。


<details>
  <summary>Details</summary>
Motivation: 自然语言指令的模糊性在安全关键的人机交互中（如手术）存在重大风险，需要一种机制来识别模糊指令以提高安全性。

Method: 使用多种LLM评估器配置不同提示技术，结合链式思维评估器和一致性预测，合成评估结果。

Result: 在区分模糊与非模糊手术指令时，Llama 3.2 11B和Gemma 3 12B的分类准确率超过60%。

Conclusion: 该方法为手术中人机协作提供了更安全和可靠的机制，能够在机器人行动前识别潜在模糊指令。

Abstract: Ambiguity in natural language instructions poses significant risks in
safety-critical human-robot interaction, particularly in domains such as
surgery. To address this, we propose a framework that uses Large Language
Models (LLMs) for ambiguity detection specifically designed for collaborative
surgical scenarios. Our method employs an ensemble of LLM evaluators, each
configured with distinct prompting techniques to identify linguistic,
contextual, procedural, and critical ambiguities. A chain-of-thought evaluator
is included to systematically analyze instruction structure for potential
issues. Individual evaluator assessments are synthesized through conformal
prediction, which yields non-conformity scores based on comparison to a labeled
calibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed
classification accuracy exceeding 60% in differentiating ambiguous from
unambiguous surgical instructions. Our approach improves the safety and
reliability of human-robot collaboration in surgery by offering a mechanism to
identify potentially ambiguous instructions before robot action.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [96] [From Chaos to Automation: Enabling the Use of Unstructured Data for Robotic Process Automation](https://arxiv.org/abs/2507.11364)
*Kelly Kurowski,Xixi Lu,Hajo A. Reijers*

Main category: cs.IR

TL;DR: 提出了一个名为UNDRESS的系统，用于帮助RPA平台从未结构化文档中提取信息，解决传统RPA依赖结构化数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 组织中大量的未结构化数据（占企业数据的80%）对数据分析和自动化处理提出了挑战，传统RPA无法有效处理此类数据。

Method: 开发了UNDRESS系统，采用模糊正则表达式、自然语言处理技术和大型语言模型。通过设计原型并进行性能评估。

Result: 系统在文本提取和信息检索方面表现良好，显著提升了RPA处理未结构化数据的能力。

Conclusion: UNDRESS为RPA在未结构化数据中的应用提供了新可能，有助于提高业务流程效率。

Abstract: The growing volume of unstructured data within organizations poses
significant challenges for data analysis and process automation. Unstructured
data, which lacks a predefined format, encompasses various forms such as
emails, reports, and scans. It is estimated to constitute approximately 80% of
enterprise data. Despite the valuable insights it can offer, extracting
meaningful information from unstructured data is more complex compared to
structured data. Robotic Process Automation (RPA) has gained popularity for
automating repetitive tasks, improving efficiency, and reducing errors.
However, RPA is traditionally reliant on structured data, limiting its
application to processes involving unstructured documents. This study addresses
this limitation by developing the UNstructured Document REtrieval SyStem
(UNDRESS), a system that uses fuzzy regular expressions, techniques for natural
language processing, and large language models to enable RPA platforms to
effectively retrieve information from unstructured documents. The research
involved the design and development of a prototype system, and its subsequent
evaluation based on text extraction and information retrieval performance. The
results demonstrate the effectiveness of UNDRESS in enhancing RPA capabilities
for unstructured data, providing a significant advancement in the field. The
findings suggest that this system could facilitate broader RPA adoption across
processes traditionally hindered by unstructured data, thereby improving
overall business process efficiency.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [97] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: 这篇论文提出TEAM-Sign方法，通过微调大型语言模型（LLM）学习文本与手语之间的对应关系，利用逐步提示策略提取LLM内部的手语知识，有效结合手语知识和推理能力对齐手语与口语的分布和语法规则。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽在许多AI任务中表现出色，但由于手语的复杂性和独特规则，其对手语生成的影响有限。本文旨在利用LLM的能力改进手语生成。

Method: 提出TEAM-Sign方法，将手语视为另一种自然语言，通过逐步提示策略提取LLM内部的手语知识，从而支持学习和生成过程。

Result: 在How2Sign和Phoenix14T数据集上的实验表明，提出的方法能有效结合LLM的手语知识和推理能力，对齐手语与口语的分布和语法规则。

Conclusion: TEAM-Sign方法通过LLM的微调和逐步提示策略，成功提升了手语生成的性能，展示了LLM在手语任务中的潜力。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [98] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: FlowFSM利用大型语言模型（LLMs）通过提示链和思维链推理从RFC文档中提取准确有限状态机（FSM），提升网络协议分析的精度。


<details>
  <summary>Details</summary>
Motivation: 现有的FSM提取技术存在可扩展性、覆盖不完整和自然语言规范模糊性等问题，阻碍了网络协议的有效建模与分析。

Method: FlowFSM框架结合LLMs、提示链和思维链推理，从RFC文档中系统提取状态转换并构建结构化规则书。

Result: 在FTP和RTSP协议的实验中，FlowFSM表现出高提取精度，并减少了虚假状态转换。

Conclusion: 基于代理的LLM系统在协议分析和FSM推断方面具有潜力，可推动网络安全和逆向工程的应用。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [99] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: EmoSApp是一种基于智能手机的离线对话应用，利用LLM为心理健康提供支持，解决了访问、隐私等问题，并通过定制的知识数据集和优化的模型展示了高效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决数字心理健康平台在用户可访问性、互联网连接和数据隐私方面的挑战，提出一种离线解决方案。

Method: 使用LLaMA-3.2-1B-Instruct模型，基于定制的心理健康问答数据集和多轮对话数据，进行微调和量化，并在智能手机上部署。

Result: 定性评估显示应用能连贯、同理地回应用户，定量测试在低资源环境下表现优异。

Conclusion: EmoSApp为便携、安全、定制化的AI心理健康解决方案提供了范例。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [100] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种结合人类专家知识和大型语言模型（LLM）的方法，用于评估学术论文的方法新颖性，以克服传统评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的新颖性评估方法（如专家判断或引用组合）存在知识有限和效果不确定的问题，LLM和人类专家的结合可以弥补这些不足。

Method: 通过从同行评审报告中提取新颖性相关句子，并利用LLM总结论文方法部分，微调PLMs；同时还设计了文本引导融合模块和Sparse-Attention机制。

Result: 实验表明，该方法在性能上优于其他基线方法。

Conclusion: 结合人类专家和LLM的知识可以有效提升学术论文方法新颖性的评估效果。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [101] [Crypto-Assisted Graph Degree Sequence Release under Local Differential Privacy](https://arxiv.org/abs/2507.10627)
*Xiaojian Zhang,Junqing Wang,Kerui Chen,Peiyuan Zhao,Huiyuan Bai*

Main category: cs.CR

TL;DR: 论文提出了一种名为CADR-LDP的高效框架，用于在本地差分隐私下发布接近实际度数分布的度数序列，解决了现有方法在阈值选择和通信成本上的缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究在域中定义的图的本地差分隐私机制，以准确近似实际度数分布，解决现有方法在阈值参数选择和通信成本上的不足。

Method: 采用CADR-LDP框架，结合加密技术和差分隐私机制，通过Optimal-θ-Selectio方法选择最优参数，并使用LPEA-LOW方法在局部投影中添加边，优先处理低度数节点以减少投影误差。

Result: 理论分析表明CADR-LDP满足ε-节点本地差分隐私，实验结果显示其在八个图数据集上优于现有方法。

Conclusion: CADR-LDP通过优化参数选择和边添加策略，有效提升了度数序列发布的准确性和效率。

Abstract: Given a graph $G$ defined in a domain $\mathcal{G}$, we investigate locally
differentially private mechanisms to release a degree sequence on $\mathcal{G}$
that accurately approximates the actual degree distribution. Existing solutions
for this problem mostly use graph projection techniques based on edge deletion
process, using a threshold parameter $\theta$ to bound node degrees. However,
this approach presents a fundamental trade-off in threshold parameter
selection. While large $\theta$ values introduce substantial noise in the
released degree sequence, small $\theta$ values result in more edges removed
than necessary. Furthermore, $\theta$ selection leads to an excessive
communication cost. To remedy existing solutions' deficiencies, we present
CADR-LDP, an efficient framework incorporating encryption techniques and
differentially private mechanisms to release the degree sequence. In CADR-LDP,
we first use the crypto-assisted Optimal-$\theta$-Selection method to select
the optimal parameter with a low communication cost. Then, we use the LPEA-LOW
method to add some edges for each node with the edge addition process in local
projection. LPEA-LOW prioritizes the projection with low-degree nodes, which
can retain more edges for such nodes and reduce the projection error.
Theoretical analysis shows that CADR-LDP satisfies $\epsilon$-node local
differential privacy. The experimental results on eight graph datasets show
that our solution outperforms existing methods.

</details>


### [102] [Access Control for Information-Theoretically Secure Key-Document Stores](https://arxiv.org/abs/2507.10730)
*Yin Li,Sharad Mehrota,Shantanu Sharma,Komal Kumari*

Main category: cs.CR

TL;DR: 该论文提出了一种基于密钥的访问控制技术，用于安全外包键值存储，采用Shamir秘密共享实现信息理论安全性，支持关键词检索并防止数据泄露。


<details>
  <summary>Details</summary>
Motivation: 解决外包键值存储中的安全访问问题，防止数据泄露和未授权访问。

Method: 采用Shamir秘密共享技术，支持关键词检索并检测恶意客户端和服务器的行为。

Result: 在50万文件中搜索5,000个关键词耗时231.5毫秒，展示了高效且安全的访问能力。

Conclusion: 该方法在保证安全的同时提升了效率，适用于大规模数据环境。

Abstract: This paper presents a novel key-based access control technique for secure
outsourcing key-value stores where values correspond to documents that are
indexed and accessed using keys. The proposed approach adopts Shamir's
secret-sharing that offers unconditional or information-theoretic security. It
supports keyword-based document retrieval while preventing leakage of the data,
access rights of users, or the size (\textit{i}.\textit{e}., volume of the
output that satisfies a query). The proposed approach allows servers to detect
(and abort) malicious clients from gaining unauthorized access to data, and
prevents malicious servers from altering data undetected while ensuring
efficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files.

</details>


### [103] [A Review of Privacy Metrics for Privacy-Preserving Synthetic Data Generation](https://arxiv.org/abs/2507.11324)
*Frederik Marinus Trudslev,Matteo Lissandrini,Juan Manuel Rodriguez,Martin Bøgsted,Daniele Dell'Aglio*

Main category: cs.CR

TL;DR: 论文探讨了隐私保护合成数据生成（PP-SDG）中的隐私风险度量问题，总结了17种隐私度量的假设和数学表述。


<details>
  <summary>Details</summary>
Motivation: 为了更透明地理解差分隐私（DP）中的隐私损失（ε）及其实际风险，需要系统评估现有的隐私度量方法。

Method: 总结了17种不同的隐私度量方法，详细描述了它们的假设和数学公式。

Result: 通过分析这些隐私度量，揭示了它们在评估PP-SDG机制时的局限性和潜在偏差。

Conclusion: 研究强调了明确定义和标准化隐私度量方法的必要性，以提高隐私风险评估的透明度和准确性。

Abstract: Privacy Preserving Synthetic Data Generation (PP-SDG) has emerged to produce
synthetic datasets from personal data while maintaining privacy and utility.
Differential privacy (DP) is the property of a PP-SDG mechanism that
establishes how protected individuals are when sharing their sensitive data. It
is however difficult to interpret the privacy loss ($\varepsilon$) expressed by
DP. To make the actual risk associated with the privacy loss more transparent,
multiple privacy metrics (PMs) have been proposed to assess the privacy risk of
the data. These PMs are utilized in separate studies to assess newly introduced
PP-SDG mechanisms. Consequently, these PMs embody the same assumptions as the
PP-SDG mechanism they were made to assess. Therefore, a thorough definition of
how these are calculated is necessary. In this work, we present the assumptions
and mathematical formulations of 17 distinct privacy metrics.

</details>


### [104] [BandFuzz: An ML-powered Collaborative Fuzzing Framework](https://arxiv.org/abs/2507.10845)
*Wenxuan Shi,Hongwei Li,Jiahao Yu,Xinqian Sun,Wenbo Guo,Xinyu Xing*

Main category: cs.CR

TL;DR: 协同模糊测试结合多个独立模糊测试工具，动态选择适合不同程序的组合，提供稳定且鲁棒的性能，但其效果受限于计算资源需求和资源分配效率的挑战。


<details>
  <summary>Details</summary>
Motivation: 提出协同模糊测试是为了克服独立模糊测试工具对目标程序的特定假设依赖，从而在各种程序中实现更稳定和通用的性能。

Method: 通过动态组合多个模糊测试工具，并根据程序特性选择最佳组合，以提升模糊测试的适用性和鲁棒性。

Result: 协同模糊测试在不同程序上表现出更稳定的性能，但面临计算资源增加和资源分配效率低的问题。

Conclusion: 协同模糊测试是一个有前景的通用模糊测试解决方案方向，但需要进一步解决其资源相关挑战以优化性能。

Abstract: Collaborative fuzzing has recently emerged as a technique that combines
multiple individual fuzzers and dynamically chooses the appropriate
combinations suited for different programs. Unlike individual fuzzers, which
rely on specific assumptions to maintain their effectiveness, collaborative
fuzzing relaxes the assumptions on target programs, providing constant and
robust performance across various programs. Ideally, collaborative fuzzing
should be a more promising direction toward generic fuzzing solutions, as it
mitigates the need for manual cherry-picking of individual fuzzers. However,
the effectiveness of existing collaborative fuzzing frameworks is limited by
major challenges, such as the need for additional computational resources
compared to individual fuzzers and the inefficient allocation of resources
among the various fuzzers.

</details>


### [105] [MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning](https://arxiv.org/abs/2507.10898)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Noha El Kachach*

Main category: cs.CR

TL;DR: 论文介绍了一种新型AI工具MalCodeAI，用于自动化代码安全分析和修复，支持14种编程语言，并在漏洞检测和修复方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测工具的限制和网络威胁的复杂性促使需要新的方法来保护软件系统。

Method: 结合代码分解和语义推理，使用优化后的Qwen2.5-Coder-3B-Instruct模型，并通过LoRA技术和MLX框架进行优化。

Result: 在功能分解和漏洞检测中分别达到低至0.397和0.199的验证损失，开发者评价显示其实用性和可解释性。

Conclusion: 该研究为智能化、可解释且以开发者为中心的软件安全解决方案提供了重要进展。

Abstract: The growing complexity of cyber threats and the limitations of traditional
vulnerability detection tools necessitate novel approaches for securing
software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI
pipeline for autonomous code security analysis and remediation. MalCodeAI
combines code decomposition and semantic reasoning using fine-tuned
Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA)
within the MLX framework, and delivers scalable, accurate results across 14
programming languages. In Phase 1, the model achieved a validation loss as low
as 0.397 for functional decomposition and summarization of code segments after
200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In
Phase 2, for vulnerability detection and remediation, it achieved a best
validation loss of 0.199 using the same number of iterations and trainable
layers but with an increased learning rate of 4 x 10^(-5), effectively
identifying security flaws and suggesting actionable fixes. MalCodeAI supports
red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot
generalization to detect complex, zero-day vulnerabilities. In a qualitative
evaluation involving 15 developers, the system received high scores in
usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of
outputs (mean 7.53/10), confirming its practical value in real-world
development workflows. This work marks a significant advancement toward
intelligent, explainable, and developer-centric software security solutions.

</details>
