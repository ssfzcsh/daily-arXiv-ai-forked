<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.MM](#cs.MM) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 9]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 7]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [math.HO](#math.HO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.NE](#cs.NE) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: 评估LLMs在生成真实世界问题代码方面的进展，重点关注微服务架构的应用，提出难度度量标准并开发自动测试框架，发现强LLMs在中难度任务表现良好，但在高难度任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在生成微服务应用代码方面的能力，为真实世界问题的代码合成提供研究基础。

Method: 定义标准模板和难度度量，开发自动化测试框架评估LLM生成的代码。

Result: 强LLMs在中难度任务表现良好，但在涉及复杂业务逻辑、外部服务和高难度任务时表现不佳。

Conclusion: LLMs在代码生成方面仍有局限，特别是在高复杂度任务中，提出了未来改进方向。

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [2] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 论文提出了一种基于强化学习的框架，通过动态探索和高对比度效率信号优化代码效率，并在两阶段调优方法中实现正确性与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型生成的代码运行时效率低，限制了其在性能敏感场景的应用。

Method: 提出了效率导向的强化学习框架，包括动态探索、错误不敏感的强化学习方法和高对比度效率信号。最终采用两阶段调优方法。

Result: 实验结果显示，该方法在7B模型上将代码正确性提升了10.18%，运行时效率提升了7.75%。

Conclusion: 该方法能有效提升代码效率与正确性，性能接近更大规模的模型。

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [3] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera是一种基于大语言模型的SMT求解器模糊测试框架，解决了生成公式的有效性和计算开销问题，发现了43个漏洞。


<details>
  <summary>Details</summary>
Motivation: 由于SMT求解器的正确性对系统研究至关重要，而现有测试方法难以适应其快速发展的特性，需要更高效的测试工具。

Method: Chimera通过LLM自动提取CFG并合成可组合的布尔项生成器，确保公式语法正确性，并减少运行时开销。

Result: 在Z3和cvc5测试中发现了43个确认漏洞，其中40个已被修复。

Conclusion: Chimera框架显著提升了测试效率和效果，为SMT求解器的可靠性提供了新方法。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [4] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: RCLAgent 是一种基于多代理递归思维框架的适应性根因定位方法，通过整合多维度数据和推理过程，显著提升了复杂微服务系统中故障定位的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 面对复杂且频繁故障的微服务系统，现有的根因定位方法因依赖预定义模式或缺乏可解释性而不足以应对动态环境。本文从专业 SRE 的经验出发，研究人类根因分析的三大特征，并以此设计 RCLAgent。

Method: 提出 RCLAgent，采用递归思维策略和多代理框架，整合多模态数据与工具辅助分析，实现高效且可解释的根因定位。

Result: 实验表明，RCLAgent 仅需单一请求即可优于依赖多请求聚合的现有方法，显著提升定位效率和准确性。

Conclusion: RCLAgent 通过模拟人类推理过程和多代理协作，为复杂微服务系统提供了高效、可解释的根因定位解决方案。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [5] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: 本文总结了在瑞士布吕格-温迪施举行的XP2025研讨会‘AI与敏捷：从挫折到成功’的关键发现，探讨了生成式人工智能与敏捷软件开发结合时的挑战与机遇，并共同制定了研究路线图。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式人工智能（GenAI）与敏捷软件开发结合时的挑战和机遇，促进两者的负责任且以人为中心的整合。

Method: 通过结构化的互动分组讨论，分析共同痛点（如工具碎片化、数据质量等），并制定研究路线图。

Result: 识别了共同痛点及其根本原因，制定了包含短期行动和长期研究方向的多主题研究路线图。

Conclusion: 研讨会为未来的研究和GenAI在敏捷实践中的整合提供了指导。

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [6] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: 总结了将LLM应用分解为三层架构，评估传统测试方法在各层的适用性，提出四种协作策略及一个可信质量保障框架。


<details>
  <summary>Details</summary>
Motivation: 解决LLM应用在非确定性、动态性和上下文依赖性方面的质量保障挑战。

Method: 分解LLM应用为系统壳层、提示编排层和LLM推理核心三层，分析传统测试方法适用性并提出协作策略。

Result: 提出四种协作策略和一个封闭循环质量保障框架，并设计AICL协议以支持测试标准化。

Conclusion: 通过分层分析与协作策略，为LLM应用质量保障提供实用指导和协议支持。

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [7] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: LLMs能高效地将法律文本转化为开发者友好的行为规范，显著减少人工工作并提供结构化输出。


<details>
  <summary>Details</summary>
Motivation: 法律文本的技术中立性导致工程师手动创建合规文档耗时且易错，GenAI（如LLMs）为自动化提供可能。

Method: 通过人类实验评估Claude和Llama生成的Gherkin规范，60条规范由10名参与者按5项标准评分。

Result: 规范在相关性、清晰度、完整性等方面评分较高，两模型表现无显著差异，总体质量优秀但存在少量幻觉和遗漏。

Conclusion: LLMs能高质量生成法律规范的Gherkin表达，为开发、测试等环节提供结构化支持。

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [8] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 论文提出了一种针对可持续性的软件架构视角，通过文献综述和专家讨论验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 可持续性作为软件密集型系统的新兴质量属性，缺乏结构化指导，需要一种通用方法来解决。

Method: 结合文献综述（雪球法）和专家焦点小组讨论，提出并验证可持续性视角的概念。

Result: 验证了可持续性视角各元素在实际中的相关性，并明确了满足工业需求的形态。

Conclusion: 可持续性视角为软件设计阶段的可持续性问题提供了一种实用的解决方案。

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [9] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 提出了一种基于断言的方法来生成CPS的测试oracle，能够在不执行测试的情况下判断测试结果，使用遗传编程（GP）和决策树（DT）生成oracle，其中GP结合Ochiai方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: CPS模拟测试成本高且可能存在不可靠性，需要一种不依赖系统执行的自动化测试oracle来降低成本并提高可靠性。

Method: 提出基于逻辑和算术谓词的断言oracle，利用遗传编程（GP）结合频谱故障定位（SBFL）公式（如Ochiai、Tarantula、Naish）和决策树（DT）及决策规则（DR）生成oracle。

Result: GP结合Ochiai生成的oracle准确性显著高于其他方法，且在系统存在不可靠行为时仍保持稳健（准确率波动仅4%）。

Conclusion: 基于GP和Ochiai的断言oracle在减少测试成本和提高结果可靠性方面表现优异，适用于航空航天、网络和自动驾驶领域。

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [10] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: 提出一种新方法，结合预训练模型和图神经网络，用于并发错误的检测和定位，显著提升准确性和召回率。


<details>
  <summary>Details</summary>
Motivation: 并发错误因共享资源同步不当引发，现有深度学习方法因数据不足、语义表示不充分和分类粒度粗而受限。

Method: 构建专用数据集，结合预训练模型与图神经网络，设计并发感知代码图(CCPG)，利用SubgraphX进行精准定位。

Result: 平均准确率和精确度提升10%，召回率提升26%，优于现有方法。

Conclusion: 新方法能更有效地检测和定位并发错误，显著提升性能。

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [11] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出ConfLogger工具，通过配置感知的静态污点分析和基于LLM的日志生成，增强软件配置的可诊断性。


<details>
  <summary>Details</summary>
Motivation: 现代可配置系统存在配置相关问题，现有方法缺乏诊断所需的足够故障信息。

Method: 结合静态污点分析和LLM生成日志，识别配置敏感代码段并生成诊断日志。

Result: 在8个系统中验证了有效性，提升了日志诊断工具的能力，显著优于现有基线方法。

Conclusion: ConfLogger显著提升了配置诊断性，具有实际应用价值。

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [12] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: 论文探讨软件工程领域的性别偏见，分析了女性在研究作者中的参与度，并提出了相关政策建议。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程领域中的性别偏见，尤其是女性在该领域的参与度问题。

Method: 通过调查软件工程的起源、专业主义，以及定量分析国际软件工程会议（1976-2010）中女性作者的参与情况。

Result: 发现多个年份存在统计显著的性别排斥现象。

Conclusion: 提出了关于计算领域性别偏见研究的政策维度建议。

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Solvable Tuple Patterns and Their Applications to Program Verification](https://arxiv.org/abs/2508.20365)
*Naoki Kobayashi,Ryosuke Sato,Ayumi Shinohara,Ryo Yoshinaka*

Main category: cs.PL

TL;DR: 论文提出了可解元组模式（STPs）以表达递归数据结构的不变式，无需负样本即可高效推断，并通过SMT求解器验证。STP推断被集成到CHC求解器中，在2025年CHC-COMP比赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管程序验证技术近年来有所进展，但递归数据结构的全自动验证仍具挑战性。

Method: 引入STPs概念，仅需少量正样本即可推断不变式，并利用支持序列理论的SMT求解器验证其正确性。将STP推断集成到支持列表类数据结构的CHC求解器中。

Result: STP推断算法成功实现，集成STP的CHC求解器在CHC-COMP 2025比赛中显著领先。

Conclusion: STPs为递归数据结构的自动化验证提供了高效方法，具有实际应用潜力。

Abstract: Despite the recent progress of automated program verification techniques,
fully automated verification of programs manipulating recursive data structures
remains a challenge. We introduce the notion of solvable tuple patterns (STPs)
to express invariants between list-like recursive data structures. A
distinguishing feature of STPs is that they can be efficiently inferred from
only a small number of positive samples; no negative samples are required. An
SMT solver that supports the sequence theory can be used to check that an
inferred STP is indeed an inductive invariant. After presenting basic
properties of STPs and an STP inference algorithm, we show how to incorporate
the STP inference into a CHC (Constrained Horn Clauses) solver supporting
list-like data structures, which serves as a uniform backend for automated
program verification tools. A CHC solver incorporating the STP inference has
won the ADT-LIN category of CHC-COMP 2025 by a big margin.

</details>


### [14] [Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops](https://arxiv.org/abs/2508.20922)
*Markus Böck,Jürgen Cito*

Main category: cs.PL

TL;DR: 该论文探讨了概率程序与贝叶斯网络的关系，提出了支持循环和动态标签的程序静态分析方法，并开发了优化技术。


<details>
  <summary>Details</summary>
Motivation: 研究概率程序（带有循环和动态标签）如何图形化表示，并利用这种表示优化推理算法。

Method: 扩展操作语义，构建控制流图，定义静态分析以近似变量的依赖关系，开发程序切片技术。

Result: 获得了程序密度的静态分解，支持三种优化，效果优于现有技术。

Conclusion: 提出的方法能有效优化概率程序的推理性能，适用于复杂程序结构。

Abstract: It is commonly known that any Bayesian network can be implemented as a
probabilistic program, but the reverse direction is not so clear. In this work,
we address the open question to what extent a probabilistic program with
user-labelled sample statements and while loops - features found in languages
like Gen, Turing, and Pyro - can be represented graphically. To this end, we
extend existing operational semantics to support these language features. By
translating a program to its control-flow graph, we define a sound static
analysis that approximates the dependency structure of the random variables in
the program. As a result, we obtain a static factorisation of the implicitly
defined program density, which is equivalent to the known Bayesian network
factorisation for programs without loops and constant labels, but constitutes a
novel graphical representation for programs that define an unbounded number of
random variables via loops or dynamic labels. We further develop a sound
program slicing technique to leverage this structure to statically enable three
well-known optimisations for the considered program class: we reduce the
variance of gradient estimates in variational inference and we speed up both
single-site Metropolis Hastings and sequential Monte Carlo. These optimisations
are proven correct and empirically shown to match or outperform existing
techniques.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [15] [The Unwritten Contract of Cloud-based Elastic Solid-State Drives](https://arxiv.org/abs/2508.17372)
*Yingjia Wang,Ming-Chang Yang*

Main category: cs.PF

TL;DR: 本研究首次对亚马逊AWS和阿里云的ESSD性能进行测试，揭示其与传统SSD的不同特性，并提出四点观察和五点启示，帮助用户优化云存储设计。


<details>
  <summary>Details</summary>
Motivation: 尽管ESSD在云服务中广泛使用，但其性能尚未被深入表征，用户对其是否能替代本地SSD存疑。

Method: 通过测试AWS和阿里云的两种ESSD，分析其性能表现，总结出四点观察和五点启示。

Result: 发现ESSD的性能与传统SSD存在显著差异，用户可从这些特性中优化系统设计。

Conclusion: ESSD具有独特性能特征，用户应重新设计云软件以充分利用其优势。

Abstract: Elastic block storage (EBS) with the storage-compute disaggregated
architecture stands as a pivotal piece in today's cloud. EBS furnishes users
with storage capabilities through the elastic solid-state drive (ESSD).
Nevertheless, despite the widespread integration into cloud services, the
absence of a thorough ESSD performance characterization raises critical doubt:
when more and more services are shifted onto the cloud, can ESSD satisfactorily
substitute the storage responsibilities of the local SSD and offer comparable
performance?
  In this paper, we for the first time target this question by characterizing
two ESSDs from Amazon AWS and Alibaba Cloud. We present an unwritten contract
of cloud-based ESSDs, encapsulating four observations and five implications for
cloud storage users. Specifically, the observations are counter-intuitive and
contrary to the conventional perceptions of what one would expect from the
local SSD. The implications we hope could guide users in revisiting the designs
of their deployed cloud software, i.e., harnessing the distinct characteristics
of ESSDs for better system performance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [A Comprehensive Survey of 5G URLLC and Challenges in the 6G Era](https://arxiv.org/abs/2508.20205)
*Md. Emadul Haque,Faisal Tariq,Muhammad R A Khandaker,Md. Sakir Hossain,Muhammad Ali Imran,Kai-Kit Wong*

Main category: cs.NI

TL;DR: 本文综述了5G系统中超可靠低延迟通信（URLLC）的研究进展，探讨了其历史演变、分层技术及设计考量，并对6G未来挑战进行了展望。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信从以人为中心转向以机器为中心，对速率、延迟和可靠性的需求急剧变化，URLLC概念应运而生。

Method: 采用分层方法，详细分析了物理层、MAC层及跨层技术，并探讨了5G及其后续垂直领域的设计考虑。

Result: 总结了5G系统中URLLC的技术实现及其在延迟和可靠性方面的冲突与平衡。

Conclusion: 文章指出，尽管5G已取得进展，但6G仍需解决更高要求的URLLC挑战，并提出了未来研究方向。

Abstract: As the wireless communication paradigm is being transformed from human
centered communication services towards machine centered communication
services, the requirements of rate, latency and reliability for these services
have also been transformed drastically. Thus the concept of Ultra Reliable and
Low Latency Communication (URLLC) has emerged as a dominant theme for 5G and 6G
systems. Though the latency and reliability requirement varies from one use
case to another, URLLC services generally aim to achieve very high reliability
in the range of 99.999\% while ensuring the latency of up to 1 ms. These two
targets are however inherently opposed to one another. Significant amounts of
work have been carried out to meet these ambitious but conflicting targets. In
this article a comprehensive survey of the URLLC approaches in 5G systems are
analysed in detail. Effort has been made to trace the history and evolution of
latency and reliability issues in wireless communication. A layered approach is
taken where physical layer, Medium Access Control (MAC) layer as well as cross
layer techniques are discussed in detail. It also covers the design
consideration for various 5G and beyond verticals. Finally the article
concludes by providing a detailed discussion on challenges and future outlook
with particular focus on the emerging 6G paradigm.

</details>


### [17] [DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource Allocation and Markov Decision Process in Named Data Networking (NDN)](https://arxiv.org/abs/2508.20272)
*Fatemeh Roshanzadeh,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: 论文提出了一种名为DRR-MDPF的混合策略，结合了MDPF模型和DRR算法，显著提升了NDN的性能。


<details>
  <summary>Details</summary>
Motivation: NDN网络中，高效的队列和资源管理对动态高流量条件下的性能至关重要。

Method: 通过集成MDPF和DRR，使路由器能智能预测转发决策并进行公平带宽分配。

Result: 模拟显示DRR-MDPF在吞吐量和负载均衡等指标上优于现有策略。

Conclusion: DRR-MDPF为NDN提供了智能、自适应且可扩展的队列管理方案。

Abstract: Named Data Networking (NDN) represents a transformative shift in network
architecture, prioritizing content names over host addresses to enhance data
dissemination. Efficient queue and resource management are critical to NDN
performance, especially under dynamic and high-traffic conditions. This paper
introduces DRR-MDPF, a novel hybrid strategy that integrates the Markov
Decision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)
algorithm. MDPF enables routers to intelligently predict optimal forwarding
decisions based on key metrics such as bandwidth, delay, and the number of
unsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation
among competing data flows. The proposed method models each router as a
learning agent capable of adjusting its strategies through continuous feedback
and probabilistic updates. Simulation results using ndnSIM demonstrate that
DRR-MDPF significantly outperforms state-of-the-art strategies including SAF,
RFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest
Satisfaction Rate (ISR), packet drop rate, content retrieval time, and load
balancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and
heavy traffic, offering enhanced adaptability and lower computational
complexity due to its single-path routing design. Furthermore, its multi-metric
decision-making capability enables more accurate interface selection, leading
to optimized network performance. Overall, DRR-MDPF serves as an intelligent,
adaptive, and scalable queue management solution for NDN, effectively
addressing core challenges such as resource allocation, congestion control, and
route optimization in dynamic networking environments.

</details>


### [18] [Relay Selection in Wireless Networks as Restless Bandits](https://arxiv.org/abs/2508.20625)
*Mandar R. Nalavade,Ravindra S. Tomar,Gaurav S. Kasbekar*

Main category: cs.NI

TL;DR: 研究无线网络中源节点通过多中继传输文件的问题，提出基于Whittle索引的中继选择策略以减少存储成本。


<details>
  <summary>Details</summary>
Motivation: 解决源节点与目标节点之间直接链路被阻塞时，如何高效选择中继以最小化数据包存储成本的问题。

Method: 将问题建模为RMAB问题，证明Whittle索引可行性，并提出计算每时隙Whittle索引的方法，选择索引最小的中继。

Result: 仿真显示该策略在成本、延迟和吞吐量上优于现有方法。

Conclusion: 基于Whittle索引的中继选择策略能有效优化无线网络中继传输性能。

Abstract: We consider a wireless network in which a source node needs to transmit a
large file to a destination node. The direct wireless link between the source
and the destination is assumed to be blocked. Multiple candidate relays are
available to forward packets from the source to the destination. A holding cost
is incurred for each packet stored at every relay in each time slot. The
objective is to design a policy for selecting a relay in each time slot to
which the source attempts to send a packet, so as to minimize the expected
long-run time-averaged total packet holding cost at the relays. This problem is
an instance of the restless multi-armed bandit (RMAB) problem, which is
provably hard to solve. We prove that this relay selection problem is
Whittle-indexable, and propose a method to compute the Whittle index of each
relay in every time slot. In each time slot, our relay selection policy
transmits a packet to the relay with the smallest Whittle index. Using
simulations, we show that the proposed policy outperforms the relay selection
policies proposed in prior work in terms of average cost, delay, as well as
throughput.

</details>


### [19] [Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF Migration in Edge-Core Networks](https://arxiv.org/abs/2508.20957)
*Faisal Ahmed,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 针对边缘核心网络中低延迟和节能编排的挑战，本文提出了一种基于数字孪生（DT）和深度强化学习的智能VNF迁移框架，显著降低了平均端到端延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 现代边缘核心网络中，服务需求的增长和虚拟化网络功能（VNF）的快速部署要求低延迟和节能的编排方案。

Method: 将VNF迁移问题建模为马尔可夫决策过程，利用Advantage Actor-Critic模型实现实时迁移决策，并集成多任务变分自编码器和LSTM网络的DT模块模拟环境动态。

Result: 仿真结果表明，该框架显著降低了平均端到端延迟和能源消耗，为智能VNF迁移设定了新基准。

Conclusion: 所提出的DT赋能深度强化学习框架有效优化了VNF迁移性能，为未来网络编排提供了创新解决方案。

Abstract: The growing demand for services and the rapid deployment of virtualized
network functions (VNFs) pose significant challenges for achieving low-latency
and energy-efficient orchestration in modern edge-core network infrastructures.
To address these challenges, this study proposes a Digital Twin (DT)-empowered
Deep Reinforcement Learning framework for intelligent VNF migration that
jointly minimizes average end-to-end (E2E) delay and energy consumption. By
formulating the VNF migration problem as a Markov Decision Process and
utilizing the Advantage Actor-Critic model, the proposed framework enables
adaptive and real-time migration decisions. A key innovation of the proposed
framework is the integration of a DT module composed of a multi-task
Variational Autoencoder and a multi-task Long Short-Term Memory network. This
combination collectively simulates environment dynamics and generates
high-quality synthetic experiences, significantly enhancing training efficiency
and accelerating policy convergence. Simulation results demonstrate substantial
performance gains, such as significant reductions in both average E2E delay and
energy consumption, thereby establishing new benchmarks for intelligent VNF
migration in edge-core networks.

</details>


### [20] [RANGAN: GAN-empowered Anomaly Detection in 5G Cloud RAN](https://arxiv.org/abs/2508.20985)
*Douglas Liao,Jiping Luo,Jens Vevstad,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 论文提出了一种结合GAN和Transformer的异常检测框架RANGAN，用于解决RAN系统中性能异常的检测问题，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: RAN系统复杂且数据量大，传统方法难以准确诊断性能异常，需要自适应方法捕捉时间依赖性。

Method: RANGAN结合GAN和Transformer，采用滑动窗口预处理数据以捕捉时间依赖。

Result: RANGAN在公开数据集上达到83%的F1分数，能有效检测网络竞争问题。

Conclusion: RANGAN是一种高效的RAN异常检测框架，实验验证其有效性。

Abstract: Radio Access Network (RAN) systems are inherently complex, requiring
continuous monitoring to prevent performance degradation and ensure optimal
user experience. The RAN leverages numerous key performance indicators (KPIs)
to evaluate system performance, generating vast amounts of data each second.
This immense data volume can make troubleshooting and accurate diagnosis of
performance anomalies more difficult. Furthermore, the highly dynamic nature of
RAN performance demands adaptive methodologies capable of capturing temporal
dependencies to detect anomalies reliably. In response to these challenges, we
introduce \textbf{RANGAN}, an anomaly detection framework that integrates a
Generative Adversarial Network (GAN) with a transformer architecture. To
enhance the capability of capturing temporal dependencies within the data,
RANGAN employs a sliding window approach during data preprocessing. We
rigorously evaluated RANGAN using the publicly available RAN performance
dataset from the Spotlight project \cite{sun-2024}. Experimental results
demonstrate that RANGAN achieves promising detection accuracy, notably
attaining an F1-score of up to $83\%$ in identifying network contention issues.

</details>


### [21] [DSROQ: Dynamic Scheduling and Routing for QoE Management in LEO Satellite Networks](https://arxiv.org/abs/2508.21047)
*Dhiraj Bhattacharjee,Pablo G. Madoery,Abhishek Naik,Halim Yanikomeroglu,Gunes Karabulut Kurt,Stephane Martel,Khaled Ahmed*

Main category: cs.NI

TL;DR: 论文提出了一种结合路由和带宽分配的方法，以满足LEO卫星网络中的QoS需求，通过MCTS启发式算法和Lyapunov优化调度提升用户体验和公平性。


<details>
  <summary>Details</summary>
Motivation: 现代互联网应用对QoS需求多样化，LEO卫星网络在覆盖和补充地面网络方面具有潜力，但需联合优化路由、带宽分配和动态队列调度以满足性能要求。

Method: 采用MCTS启发式算法解决NP难的路由与带宽分配问题，结合Lyapunov优化调度；提出DSROQ算法在Starlink网络中验证。

Result: DSROQ在用户体验和公平性上优于基准方案，且随着流量敏感性变化，性能主导因素从调度转为路由和带宽分配。

Conclusion: 联合优化路由和带宽分配能显著提升LEO网络的QoS，DSROQ算法在实际网络中表现出优势。

Abstract: The modern Internet supports diverse applications with heterogeneous quality
of service (QoS) requirements. Low Earth orbit (LEO) satellite constellations
offer a promising solution to meet these needs, enhancing coverage in rural
areas and complementing terrestrial networks in urban regions. Ensuring QoS in
such networks requires joint optimization of routing, bandwidth allocation, and
dynamic queue scheduling, as traffic handling is critical for maintaining
service performance. This paper formulates a joint routing and bandwidth
allocation problem where QoS requirements are treated as soft constraints,
aiming to maximize user experience. An adaptive scheduling approach is
introduced to prioritize flow-specific QoS needs. We propose a Monte Carlo tree
search (MCTS)-inspired method to solve the NP-hard route and bandwidth
allocation problem, with Lyapunov optimization-based scheduling applied during
reward evaluation. Using the Starlink Phase 1 Version 2 constellation, we
compare end-user experience and fairness between our proposed DSROQ algorithm
and a benchmark scheme. Results show that DSROQ improves both performance
metrics and demonstrates the advantage of joint routing and bandwidth
decisions. Furthermore, we observe that the dominant performance factor shifts
from scheduling to routing and bandwidth allocation as traffic sensitivity
changes from latency-driven to bandwidth-driven.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [MM-HSD: Multi-Modal Hate Speech Detection in Videos](https://arxiv.org/abs/2508.20546)
*Berta Céspedes-Sarrias,Carlos Collado-Capell,Pablo Rodenas-Ruiz,Olena Hrynenko,Andrea Cavallaro*

Main category: cs.MM

TL;DR: 提出了一种多模态仇恨言论检测模型MM-HSD，通过整合视频帧、音频和文本，并利用跨模态注意力（CMA）提高性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频仇恨言论检测方法多模态融合不足，忽略关键模态（如屏幕文本和音频），亟需更全面的多模态方法。

Method: 结合视频帧、音频、文本（语音转录和屏幕文本）及CMA特征提取，首次将CMA作为早期特征提取器，并系统比较模态交互。

Result: 在HateMM数据集上，MM-HSD的M-F1得分0.874，优于现有方法，屏幕文本作为查询时效果最佳。

Conclusion: 多模态整合和CMA特征提取显著提升仇恨言论检测性能，屏幕文本是关键模态。

Abstract: While hate speech detection (HSD) has been extensively studied in text,
existing multi-modal approaches remain limited, particularly in videos. As
modalities are not always individually informative, simple fusion methods fail
to fully capture inter-modal dependencies. Moreover, previous work often omits
relevant modalities such as on-screen text and audio, which may contain subtle
hateful content and thus provide essential cues, both individually and in
combination with others. In this paper, we present MM-HSD, a multi-modal model
for HSD in videos that integrates video frames, audio, and text derived from
speech transcripts and from frames (i.e.~on-screen text) together with features
extracted by Cross-Modal Attention (CMA). We are the first to use CMA as an
early feature extractor for HSD in videos, to systematically compare query/key
configurations, and to evaluate the interactions between different modalities
in the CMA block. Our approach leads to improved performance when on-screen
text is used as a query and the rest of the modalities serve as a key.
Experiments on the HateMM dataset show that MM-HSD outperforms state-of-the-art
methods on M-F1 score (0.874), using concatenation of transcript, audio, video,
on-screen text, and CMA for feature extraction on raw embeddings of the
modalities. The code is available at https://github.com/idiap/mm-hsd

</details>


### [23] [diveXplore at the Video Browser Showdown 2024](https://arxiv.org/abs/2508.20560)
*Klaus Schoeffmann,Sahar Nasirihaghighi*

Main category: cs.MM

TL;DR: 介绍了VBS2024中diveXplore系统的升级内容，包括嵌入模型、查询服务器和用户界面的改进。


<details>
  <summary>Details</summary>
Motivation: 基于VBS2023的实践和IVR4B会议的反馈，提升系统性能以满足多样化搜索需求。

Method: 整合OpenCLIP模型、优化查询服务器和用户界面，新增探索视图功能。

Result: 系统支持自由文本和视觉相似性搜索，并提升了浏览和查询效率。

Conclusion: diveXplore系统的改进使其更适用于视频内容的多样化探索和搜索需求。

Abstract: According to our experience from VBS2023 and the feedback from the IVR4B
special session at CBMI2023, we have largely revised the diveXplore system for
VBS2024. It now integrates OpenCLIP trained on the LAION-2B dataset for
image/text embeddings that are used for free-text and visual similarity search,
a query server that is able to distribute different queries and merge the
results, a user interface optimized for fast browsing, as well as an
exploration view for large clusters of similar videos (e.g., weddings,
paraglider events, snow and ice scenery, etc.).

</details>


### [24] [Less is More - diveXplore 5.0 at VBS 2021](https://arxiv.org/abs/2508.20569)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: diveXplore 是交互式视频探索系统，曾成功用于竞赛，但因功能增加导致性能下降。新版本5.0重写以简化系统并保留核心功能。


<details>
  <summary>Details</summary>
Motivation: 功能增加导致系统复杂化，性能下降，需重新设计以提升效率。

Method: 完全重写系统5.0版，模块化设计以减少复杂性。

Result: 新版本保留了核心功能，同时降低了系统复杂性。

Conclusion: 模块化重构成功解决了性能问题，保持了系统的实用性。

Abstract: As a longstanding participating system in the annual Video Browser Showdown
(VBS2017-VBS2020) as well as in two iterations of the more recently established
Lifelog Search Challenge (LSC2018-LSC2019), diveXplore is developed as a
feature-rich Deep Interactive Video Exploration system. After its initial
successful employment as a competitive tool at the challenges, its performance,
however, declined as new features were introduced increasing its overall
complexity. We mainly attribute this to the fact that many additions to the
system needed to revolve around the system's core element - an interactive
self-organizing browseable featuremap, which, as an integral component did not
accommodate the addition of new features well. Therefore, counteracting said
performance decline, the VBS 2021 version constitutes a completely rebuilt
version 5.0, implemented from scratch with the aim of greatly reducing the
system's complexity as well as keeping proven useful features in a modular
manner.

</details>


### [25] [diveXplore 6.0: ITEC's Interactive Video Exploration System at VBS 2022](https://arxiv.org/abs/2508.20687)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.MM

TL;DR: diveXplore系统在VBS2021中进行了重大重构，虽功能减少但变得更现代、轻量且快速，性能提升。版本6.0引入新特性改进概念和时序上下文搜索。


<details>
  <summary>Details</summary>
Motivation: 通过对系统进行重构和优化，提升其在视频检索竞赛中的性能表现。

Method: 系统进行了重大重构，减少功能但提升了现代化程度和速度；版本6.0重新考虑了镜头分割、地图搜索，并引入新特性改进概念及时序上下文搜索。

Result: 重构后的系统在VBS2021中表现出更高的性能。

Conclusion: 减少功能并优化系统设计是提升性能的有效策略，版本6.0的进一步改进有望进一步提升搜索能力。

Abstract: Continuously participating since the sixth Video Browser Showdown (VBS2017),
diveXplore is a veteran interactive search system that throughout its lifetime
has offered and evaluated numerous features. After undergoing major refactoring
for the most recent VBS2021, however, the system since version 5.0 is less
feature rich, yet, more modern, leaner and faster than the original system.
This proved to be a sensible decision as the new system showed increasing
performance in VBS2021 when compared to the most recent former competitions.
With version 6.0 we reconsider shot segmentation, map search and introduce new
features for improving concept as well as temporal context search.

</details>


### [26] [AdaDPCC: Adaptive Rate Control and Rate-Distortion-Complexity Optimization for Dynamic Point Cloud Compression](https://arxiv.org/abs/2508.20741)
*Chenhao Zhang,Wei Gao*

Main category: cs.MM

TL;DR: 本文提出了一种新型动态点云压缩框架，支持可变比特率和计算复杂度，通过粗到细的运动估计和率控制模块，显著提升了压缩效率和实时性。


<details>
  <summary>Details</summary>
Motivation: 动态点云压缩在自动驾驶和AR/VR等领域具有重要应用，但现有方法在复杂度和率控制方面存在挑战。

Method: 采用可调整框架和多编码路径，结合粗到细运动估计和自适应率控制模块，实现高效的率-失真-复杂度优化。

Result: 实验结果显示，相比现有方法，BD-Rate降低5.81%，BD-PSNR提升0.42 dB，平均比特率误差为0.40%，编码时间减少44.6%。

Conclusion: 该方法在实时和比特率受限的场景中表现出色，为动态点云压缩提供了高效解决方案。

Abstract: Dynamic point cloud compression (DPCC) is crucial in applications like
autonomous driving and AR/VR. Current compression methods face challenges with
complexity management and rate control. This paper introduces a novel dynamic
coding framework that supports variable bitrate and computational complexities.
Our approach includes a slimmable framework with multiple coding routes,
allowing for efficient Rate-Distortion-Complexity Optimization (RDCO) within a
single model. To address data sparsity in inter-frame prediction, we propose
the coarse-to-fine motion estimation and compensation module that deconstructs
geometric information while expanding the perceptive field. Additionally, we
propose a precise rate control module that content-adaptively navigates point
cloud frames through various coding routes to meet target bitrates. The
experimental results demonstrate that our approach reduces the average BD-Rate
by 5.81% and improves the BD-PSNR by 0.42 dB compared to the state-of-the-art
method, while keeping the average bitrate error at 0.40%. Moreover, the average
coding time is reduced by up to 44.6% compared to D-DPCC, underscoring its
efficiency in real-time and bitrate-constrained DPCC scenarios. Our code is
available at https://git.openi.org.cn/OpenPointCloud/Ada_DPCC.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [27] [Exploiting Instantiations from Paramodulation Proofs in Isabelle/HOL](https://arxiv.org/abs/2508.20738)
*Lukas Bartl,Jasmin Blanchette,Tobias Nipkow*

Main category: cs.LO

TL;DR: Metis是集成在Isabelle/HOL中的有序参数化证明器，新工具通过分析成功的Metis证明优化Sledgehammer的性能。


<details>
  <summary>Details</summary>
Motivation: 提高Sledgehammer的成功率和证明速度，并帮助用户理解目标如何从引理中推导。

Method: 分析成功的Metis证明以推导变量实例化。

Result: 提升了Sledgehammer的成功率和速度，增强了用户理解能力。

Conclusion: 新工具显著优化了Sledgehammer的性能和用户体验。

Abstract: Metis is an ordered paramodulation prover built into the Isabelle/HOL proof
assistant. It attempts to close the current goal using a given list of lemmas.
Typically these lemmas are found by Sledgehammer, a tool that integrates
external automatic provers. We present a new tool that analyzes successful
Metis proofs to derive variable instantiations. These increase Sledgehammer's
success rate, improve the speed of Sledgehammer-generated proofs, and help
users understand why a goal follows from the lemmas.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [28] [Athena: Intermediate Representations for Iterative Scaffolded App Generation with an LLM](https://arxiv.org/abs/2508.20263)
*Jazbo Beason,Ruijia Cheng,Eldon Schoop,Jeffrey Nichols*

Main category: cs.HC

TL;DR: 论文提出Athena，一种通过共享中间表示（如应用故事板、数据模型和GUI骨架）辅助LLM迭代生成完整用户界面的原型工具，解决了传统LLM生成代码杂乱且难以理解的问题。


<details>
  <summary>Details</summary>
Motivation: 用户界面复杂且通常由多个相互关联的文件组成，传统LLM难以通过单一提示生成完整、结构化的代码，导致输出结果混乱且难以维护。

Method: 引入Athena，利用共享中间表示（如应用故事板、数据模型和GUI骨架）辅助LLM迭代生成代码，提升代码组织和结构。

Result: 用户研究显示，75%的参与者更倾向于使用Athena而非传统聊天机器人风格工具进行应用原型设计。

Conclusion: Athena通过中间表示和迭代方法显著提升了LLM生成用户界面代码的质量和可用性。

Abstract: It is challenging to generate the code for a complete user interface using a
Large Language Model (LLM). User interfaces are complex and their
implementations often consist of multiple, inter-related files that together
specify the contents of each screen, the navigation flows between the screens,
and the data model used throughout the application. It is challenging to craft
a single prompt for an LLM that contains enough detail to generate a complete
user interface, and even then the result is frequently a single large and
difficult to understand file that contains all of the generated screens. In
this paper, we introduce Athena, a prototype application generation environment
that demonstrates how the use of shared intermediate representations, including
an app storyboard, data model, and GUI skeletons, can help a developer work
with an LLM in an iterative fashion to craft a complete user interface. These
intermediate representations also scaffold the LLM's code generation process,
producing organized and structured code in multiple files while limiting
errors. We evaluated Athena with a user study that found 75% of participants
preferred our prototype over a typical chatbot-style baseline for prototyping
apps.

</details>


### [29] [Identifying Framing Practices in Visualization Design Through Practitioner Reflections](https://arxiv.org/abs/2508.20383)
*Prakash Shukla,Paul Parsons*

Main category: cs.HC

TL;DR: 研究探讨了可视化设计中框架（framing）的核心作用，指出其在设计过程中的重要性，并通过对专业设计师的采访分析了触发重新框架的条件和实践策略。


<details>
  <summary>Details</summary>
Motivation: 目前可视化研究中框架的作用主要关注其对受众的修辞和感知影响，而对其在设计过程中的作用研究不足。本研究旨在填补这一空白。

Method: 通过分析80多名专业可视化设计师在公开播客和书章节中的反思，研究了框架在设计过程中的应用。

Result: 研究发现框架是设计过程中的核心活动，涉及问题界定、数据解释、与利益相关者目标对齐及叙事方向制定。同时识别了触发重新框架的条件和实践策略。

Conclusion: 框架是可视化实践的核心维度，强调了研究和教育需要支持设计师在设计过程中运用的解释性和战略性判断。

Abstract: Framing -- how designers define and reinterpret problems, shape narratives,
and guide audience understanding -- is central to design practice. Yet in
visualization research, framing has been examined mostly through its rhetorical
and perceptual effects on audiences, leaving its role in the design process
underexplored. This study addresses that gap by analyzing publicly available
podcasts and book chapters in which over 80 professional visualization
designers reflect on their work. We find that framing is a pervasive, iterative
activity, evident in scoping problems, interpreting data, aligning with
stakeholder goals, and shaping narrative direction. Our analysis identifies the
conditions that trigger reframing and the strategies practitioners use to
navigate uncertainty and guide design. These findings position framing as a
core dimension of visualization practice and underscore the need for research
and education to support the interpretive and strategic judgment that
practitioners exercise throughout the design process.

</details>


### [30] [Human-Centered Design for Connected Automation: Predicting Pedestrian Crossing Intentions](https://arxiv.org/abs/2508.20464)
*Sanaz Motamedi,Viktoria Marcus,Griffin Pitts*

Main category: cs.HC

TL;DR: 研究通过扩展计划行为理论，纳入安全、信任、兼容性和理解四个外部因素，分析行人与L5自动驾驶系统互动时的决策行为，为设计人机界面和车联通信策略提供依据。


<details>
  <summary>Details</summary>
Motivation: 全球交通事故中超过一半的死亡人数为行人等弱势道路使用者，L5自动驾驶系统有望减少事故，但需解决与行人安全互动的问题。

Method: 研究扩展了计划行为理论（TPB），加入四个外部因素，通过在线调查（n=212）分析行人决策行为。

Result: 感知行为控制、态度和社会信息显著预测行人过街意图；安全感和理解力对外部因素影响最大。

Conclusion: 研究结果为设计支持自动驾驶车辆与行人安全透明交互的人机界面和车联通信策略提供了实用建议，推动以人为本的智能交通系统发展。

Abstract: Road traffic remains a leading cause of death worldwide, with pedestrians and
other vulnerable road users accounting for over half of the 1.19 million annual
fatalities, much of it due to human error. Level-5 automated driving systems
(ADSs), capable of full self-driving without human oversight, have the
potential to reduce these incidents. However, their effectiveness depends not
only on automation performance but also on their ability to communicate intent
and coordinate safely with pedestrians in the absence of traditional driver
cues. Understanding how pedestrians interpret and respond to ADS behavior is
therefore critical to the development of connected vehicle systems. This study
extends the Theory of Planned Behavior (TPB) by incorporating four external
factors (i.e. safety, trust, compatibility, and understanding) to model
pedestrian decision-making in road-crossing scenarios involving level-5 ADSs.
Using data from an online survey (n = 212), results show that perceived
behavioral control, attitude, and social information significantly predict
pedestrians' crossing intentions. External factors, particularly perceived
safety and understanding, strongly influence these constructs. Findings provide
actionable insights for designing external human-machine interfaces (eHMIs) and
cooperative V2X communication strategies that support safe, transparent
interactions between automated vehicles and pedestrians. This work contributes
to the development of inclusive, human-centered connected mobility systems.

</details>


### [31] [What is "Spatial" about Spatial Computing?](https://arxiv.org/abs/2508.20477)
*Yibo Wang,Yuhan Luo,Janghee Cho,Junnan Yu*

Main category: cs.HC

TL;DR: 论文探讨了空间计算在跨学科中的概念碎片化问题，提出两种空间理解视角，并构建一个统一的范式以促进未来技术发展。


<details>
  <summary>Details</summary>
Motivation: 解决空间计算在跨学科中的概念碎片化问题，以推动其连贯发展和跨学科整合。

Method: 追溯空间计算的起源和历史演变，分析两种空间理解视角：空间作为上下文理解或混合交互空间。

Result: 提出空间计算作为重新定义环境、计算与人类体验交互的计算范式，增强概念清晰度。

Conclusion: 通过统一视角，促进未来技术创新，支持更深入的环境交互与塑造。

Abstract: Recent advancements in geographic information systems and mixed reality
technologies have positioned spatial computing as a transformative paradigm in
computational science. However, the field remains conceptually fragmented, with
diverse interpretations across disciplines like Human-Computer Interaction,
Geographic Information Science, and Computer Science, which hinders a
comprehensive understanding of spatial computing and poses challenges for its
coherent advancement and interdisciplinary integration. In this paper, we trace
the origins and historical evolution of spatial computing and examine how
"spatial" is understood, identifying two schools of thought: "spatial" as the
contextual understanding of space, where spatial data guides interaction in the
physical world; and "spatial" as a mixed space for interaction, emphasizing the
seamless integration of physical and digital environments to enable embodied
engagement. By synthesizing these perspectives, we propose spatial computing as
a computational paradigm that redefines the interplay between environment,
computation, and human experience, offering a holistic lens to enhance its
conceptual clarity and inspire future technological innovations that support
meaningful interactions with and shaping of environments.

</details>


### [32] [VisiTrail: A Cognitive Visualization Tool for Time-Series Analysis of Eye Tracking Data from Attention Game](https://arxiv.org/abs/2508.20522)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.HC

TL;DR: 该论文提出了一种新的眼动追踪分析工具，结合时间序列数据和多种注视行为指标，以揭示任务执行中的注意力动态和表现。


<details>
  <summary>Details</summary>
Motivation: 传统眼动分析方法在复杂视觉搜索任务中存在局限性，如无法全面分析注意力分配的动态性及其与任务表现的关系，因此需要更全面的工具。

Method: 论文提出的工具包括时间序列眼动数据分析、注视行为测量（如注视点、扫视和平滑追踪）、时间模式分析、对象点击序列跟踪，以及量化准确性和效率的性能指标。

Result: 该工具提供了全面的可视化技术，使复杂的刺激与注视行为之间的关系更易于解释。

Conclusion: 研究结果表明，该工具能够更好地理解任务执行中的注意力动态，并为眼动数据提供了更综合的分析方法。

Abstract: Eye Tracking (ET) can help to understand visual attention and cognitive
processes in interactive environments. In attention tasks, distinguishing
between relevant target objects and distractors is crucial for effective
performance, yet the underlying gaze patterns that drive successful task
completion remain incompletely understood. Traditional gaze analyses lack
comprehensive insights into the temporal dynamics of attention allocation and
the relationship between gaze behavior and task performance. When applied to
complex visual search scenarios, current gaze analysis methods face several
limitations, including the isolation of measurements, visual stability, search
efficiency, and the decision-making processes involved in these scenarios. This
paper proposes an analysis tool that considers time series for eye tracking
data from task performance and also gaze measures (fixations, saccades and
smooth pursuit); temporal pattern analysis that reveals how attention evolves
throughout task performance; object-click sequence tracking that directly links
visual attention to user actions; and performance metrics that quantify both
accuracy and efficiency. This tool provides comprehensive visualization
techniques that make complex patterns of stimuli and gaze connections
interpretable.

</details>


### [33] [Persode: Personalized Visual Journaling with Episodic Memory-Aware AI Agent](https://arxiv.org/abs/2508.20585)
*Seokho Jin,Manseo Kim,Sungho Byun,Hansol Kim,Jungmin Lee,Sujeong Baek,Semi Kim,Sanghum Park,Sung Park*

Main category: cs.HC

TL;DR: 论文介绍了一种名为Persode的个性化日记系统，结合了视觉叙事和记忆感知对话代理，以吸引新一代用户。


<details>
  <summary>Details</summary>
Motivation: 针对Alpha和Z世代对传统文本日记不感兴趣的问题，Persode旨在提供更具视觉沉浸感和个性化的日记体验。

Method: 系统采用Retrieval-Augmented Generation (RAG)框架，结合个性化引导流程和文本到图像的生成技术。

Result: Persode能够动态生成视觉叙事，增强情感回忆，并为用户提供与其身份和偏好相符的日记体验。

Conclusion: Persode通过视觉化和个性化设计，成功吸引了Gen Alpha和Z用户，填补了传统日记与现代偏好的鸿沟。

Abstract: Reflective journaling often lacks personalization and fails to engage
Generation Alpha and Z, who prefer visually immersive and fast-paced
interactions over traditional text-heavy methods. Visual storytelling enhances
emotional recall and offers an engaging way to process personal expe- riences.
Designed with these digital-native generations in mind, this paper introduces
Persode, a journaling system that integrates personalized onboarding,
memory-aware conversational agents, and automated visual storytelling. Persode
captures user demographics and stylistic preferences through a tailored
onboarding process, ensuring outputs resonate with individual identities. Using
a Retrieval-Augmented Generation (RAG) framework, it prioritizes emotionally
significant memories to provide meaningful, context-rich interactions.
Additionally, Persode dynamically transforms user experiences into visually
engaging narratives by generating prompts for advanced text-to-image models,
adapting characters, backgrounds, and styles to user preferences. By addressing
the need for personalization, visual engagement, and responsiveness, Persode
bridges the gap between traditional journaling and the evolving preferences of
Gen Alpha and Z.

</details>


### [34] [Schema-Guided Response Generation using Multi-Frame Dialogue State for Motivational Interviewing Systems](https://arxiv.org/abs/2508.20635)
*Jie Zeng,Yukiko I. Nakano*

Main category: cs.HC

TL;DR: 研究提出了一种基于动机性访谈(MI)原则的对话系统方法，通过更新多帧对话状态和动态决定响应策略，成功生成了符合MI原则的响应，并有效促进了用户的思考。


<details>
  <summary>Details</summary>
Motivation: 帮助对话系统生成符合动机性访谈原则的响应，以增强客户的行为改变动机。

Method: 采用模式引导方法，包括更新多帧对话状态和动态决定响应策略的机制。

Result: 系统成功生成了符合MI原则的响应，并通过提问有效促进了客户的思考。

Conclusion: 该方法在对话系统中实现了MI原则的应用，并通过用户研究验证了其有效性。

Abstract: The primary goal of Motivational Interviewing (MI) is to help clients build
their own motivation for behavioral change. To support this in dialogue
systems, it is essential to guide large language models (LLMs) to generate
counselor responses aligned with MI principles. By employing a schema-guided
approach, this study proposes a method for updating multi-frame dialogue states
and a strategy decision mechanism that dynamically determines the response
focus in a manner grounded in MI principles. The proposed method was
implemented in a dialogue system and evaluated through a user study. Results
showed that the proposed system successfully generated MI-favorable responses
and effectively encouraged the user's (client's) deliberation by asking
eliciting questions.

</details>


### [35] [Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop](https://arxiv.org/abs/2508.21036)
*Lev Tankelevitch,Elena L. Glassman,Jessica He,Aniket Kittur,Mina Lee,Srishti Palani,Advait Sarkar,Gonzalo Ramos,Yvonne Rogers,Hari Subramonyam*

Main category: cs.HC

TL;DR: 生成式AI（GenAI）对工作、教育和日常任务的自动化能力带来了巨大变革，同时也对人类认知提出了机遇与挑战。CHI 2025研讨会旨在研究GenAI如何影响人类思维（如元认知、批判性思维、记忆和创造力），并探讨设计实践以保护和增强人类思维。论文总结了讨论内容，为多学科研究社区提供了方向。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI对人类认知的潜在影响，以及如何通过设计和理论工具来保护和增强人类思维。

Method: 举办CHI 2025研讨会，汇集56名跨学科研究人员和34篇论文，进行讨论、构思和社区建设。

Result: 初步构建了研究和设计领域的框架，并为多学科研究社区奠定了基础。

Conclusion: GenAI对人类认知的变革是一个重要的研究领域，需要多学科合作来探索其潜在机遇与挑战。

Abstract: Generative AI (GenAI) radically expands the scope and capability of
automation for work, education, and everyday tasks, a transformation posing
both risks and opportunities for human cognition. How will human cognition
change, and what opportunities are there for GenAI to augment it? Which
theories, metrics, and other tools are needed to address these questions? The
CHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of
how the use of GenAI affects human thought, from metacognition to critical
thinking, memory, and creativity, with an emerging design practice for building
GenAI tools that both protect and augment human thought. Fifty-six researchers,
designers, and thinkers from across disciplines as well as industry and
academia, along with 34 papers and portfolios, seeded a day of discussion,
ideation, and community-building. We synthesize this material here to begin
mapping the space of research and design opportunities and to catalyze a
multidisciplinary community around this pressing area of research.

</details>


### [36] [OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models](https://arxiv.org/abs/2508.21061)
*Adam Coscia,Shunan Guo,Eunyee Koh,Alex Endert*

Main category: cs.HC

TL;DR: OnGoal是一个基于LLM的聊天界面，通过实时目标对齐反馈和进度概览，帮助用户更高效地实现对话目标，并减少认知负担。


<details>
  <summary>Details</summary>
Motivation: 随着多轮对话的复杂性增加，用户需要更好的工具来评估和管理对话目标的进度。

Method: 开发OnGoal界面，提供实时目标对齐反馈、解释和进度概览，并通过20名参与者的写作任务进行对比研究。

Result: 使用OnGoal的参与者更高效地达成目标，探索新的提示策略，减少了时间和精力消耗。

Conclusion: OnGoal的跟踪和可视化功能提升了对话的互动性和用户参与度，为未来LLM聊天界面的设计提供了启示。

Abstract: As multi-turn dialogues with large language models (LLMs) grow longer and
more complex, how can users better evaluate and review progress on their
conversational goals? We present OnGoal, an LLM chat interface that helps users
better manage goal progress. OnGoal provides real-time feedback on goal
alignment through LLM-assisted evaluation, explanations for evaluation results
with examples, and overviews of goal progression over time, enabling users to
navigate complex dialogues more effectively. Through a study with 20
participants on a writing task, we evaluate OnGoal against a baseline chat
interface without goal tracking. Using OnGoal, participants spent less time and
effort to achieve their goals while exploring new prompting strategies to
overcome miscommunication, suggesting tracking and visualizing goals can
enhance engagement and resilience in LLM dialogues. Our findings inspired
design implications for future LLM chat interfaces that improve goal
communication, reduce cognitive load, enhance interactivity, and enable
feedback to improve LLM performance.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [37] [Mixture of Contexts for Long Video Generation](https://arxiv.org/abs/2508.21058)
*Shengqu Cai,Ceyuan Yang,Lvmin Zhang,Yuwei Guo,Junfei Xiao,Ziyan Yang,Yinghao Xu,Zhenheng Yang,Alan Yuille,Leonidas Guibas,Maneesh Agrawala,Lu Jiang,Gordon Wetzstein*

Main category: cs.GR

TL;DR: 该论文提出了一种名为Mixture of Contexts (MoC)的稀疏注意力路由模块，用于解决长视频生成中的长上下文记忆问题。


<details>
  <summary>Details</summary>
Motivation: 长视频生成需要模型能够在长范围内保留和检索重要事件，而传统的自注意力机制因二次成本限制无法高效处理长序列。

Method: 通过将长上下文视频生成重新定义为内部信息检索任务，引入MoC模块，动态选择关键信息块进行注意力计算。

Result: 随着数据和路由稀疏化的扩展，模型能够高效分配计算资源，实现分钟级视频的生成，同时保持内容的连贯性和一致性。

Conclusion: MoC通过稀疏注意力路由解决了长视频生成的效率和记忆问题，为实际训练和生成提供了可行性。

Abstract: Long video generation is fundamentally a long context memory problem: models
must retain and retrieve salient events across a long range without collapsing
or drifting. However, scaling diffusion transformers to generate long-context
videos is fundamentally limited by the quadratic cost of self-attention, which
makes memory and computation intractable and difficult to optimize for long
sequences. We recast long-context video generation as an internal information
retrieval task and propose a simple, learnable sparse attention routing module,
Mixture of Contexts (MoC), as an effective long-term memory retrieval engine.
In MoC, each query dynamically selects a few informative chunks plus mandatory
anchors (caption, local windows) to attend to, with causal routing that
prevents loop closures. As we scale the data and gradually sparsify the
routing, the model allocates compute to salient history, preserving identities,
actions, and scenes over minutes of content. Efficiency follows as a byproduct
of retrieval (near-linear scaling), which enables practical training and
synthesis, and the emergence of memory and consistency at the scale of minutes.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [38] [Reverse Designing Ferroelectric Capacitors with Machine Learning-based Compact Modeling](https://arxiv.org/abs/2508.20216)
*Diego Ferrer,Jack Hutchins,Revanth Koduru,Sumeet Kumar Gupta,Admedullah Aziz*

Main category: cs.ET

TL;DR: 提出两种基于机器学习的紧凑模型反向设计算法，用于快速确定实现特定电气特性的设备参数，大幅提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如相场建模）耗时且不适用于迭代设计过程，需寻找更快速高效的方法。

Method: 利用机器学习构建紧凑模型，开发两种反向设计算法以匹配设备参数与性能需求。

Result: 与相场建模相比，新方法显著减少计算时间，且保持高精度和效率。

Conclusion: 机器学习紧凑模型为设备设计提供了高效、准确的解决方案，尤其适合迭代优化场景。

Abstract: Machine learning-based compact models provide a rapid and efficient approach
for estimating device behavior across multiple input parameter variations. In
this study, we introduce two reverse-design algorithms that utilize these
compact models to identify device parameters corresponding to desired
electrical characteristics. The algorithms effectively determine parameter
sets, such as layer thicknesses, required to achieve specific device
performance criteria. Significantly, the proposed methods are uniquely enabled
by machine learning-based compact modeling; alternative computationally
intensive approaches, such as phase-field modeling, would impose impractical
time constraints for iterative design processes. Our comparative analysis
demonstrates a substantial reduction in computation time when employing machine
learning-based compact models compared to traditional phase-field methods,
underscoring a clear and substantial efficiency advantage. Additionally, the
accuracy and computational efficiency of both reverse-design algorithms are
evaluated and compared, highlighting the practical advantages of machine
learning-based compact modeling approaches.

</details>


### [39] [Blind Source Separation-Enabled Joint Communication and Sensing in IBFD MIMO Systems](https://arxiv.org/abs/2508.20409)
*Siyao Li,Conrad Prisby,Thomas Yang*

Main category: cs.ET

TL;DR: 该论文提出了一种基于盲源分离的框架，用于在下一代无线网络中的全双工MIMO系统中同时进行自干扰消除和感知信息提取，利用FastICA算法实现信号恢复和信道估计。


<details>
  <summary>Details</summary>
Motivation: 传统全双工系统中的自干扰是信号恢复的主要障碍，但在联合通信与感知的范式中，这种高功率自干扰信号为高效感知提供了机会。

Method: 提出基于盲源分离（BSS）的框架，采用FastICA算法分离自干扰信号和感兴趣信号，实现信号恢复与信道估计。

Result: 仿真结果表明，该框架在信号帧大小增加时表现出改进的感知和通信性能。

Conclusion: 结合自干扰信号的感知潜力，提出了一种高效的双任务解决方案。

Abstract: This paper addresses the challenge of joint communication and sensing (JCAS)
in next-generation wireless networks, with an emphasis on in-band full-duplex
(IBFD) multiple-input multiple-output (MIMO) systems. Traditionally,
self-interference (SI) in IBFD systems is a major obstacle to recovering the
signal of interest (SOI). Under the JCAS paradigm, however, this high-power SI
signal presents an opportunity for efficient sensing. Since each transceiver
node has access to the original SI signal, its environmental reflections can be
exploited to estimate channel conditions and detect changes, without requiring
dedicated radar waveforms. We propose a blind source separation (BSS)-based
framework to simultaneously perform self-interference cancellation (SIC) and
extract sensing information in IBFD MIMO settings. The approach applies the
Fast Independent Component Analysis (FastICA) algorithm to separate the SI and
SOI signals while enabling simultaneous signal recovery and channel estimation.
Simulation results confirm the framework's effectiveness, showing improved
sensing and communication performance as signal frame size increases.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [SpeedMalloc: Improving Multi-threaded Applications via a Lightweight Core for Memory Allocation](https://arxiv.org/abs/2508.20253)
*Ruihao Li,Qinzhe Wu,Krishna Kavi,Gayatri Mehta,Jonathan C. Beard,Neeraja J. Yadwadkar,Lizy K. John*

Main category: cs.DC

TL;DR: 该论文提出了SpeedMalloc，使用轻量级支持核心处理多线程应用中的内存分配任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 内存分配虽只占代码的一小部分，但对整体性能有重大影响，尤其在多线程多核系统中，现有解决方案如加速器支持有限且同步问题突出。

Method: 设计了一个轻量级支持核心，专门处理内存分配任务，减少缓存冲突并消除跨核心元数据同步需求。

Result: SpeedMalloc在多种多线程工作负载上性能显著优于现有软件和硬件分配器，提升幅度达1.15x至1.75x。

Conclusion: SpeedMalloc通过轻量级支持核心设计，解决了多线程应用中内存分配的性能瓶颈，且具备更好的适应性。

Abstract: Memory allocation, though constituting only a small portion of the executed
code, can have a "butterfly effect" on overall program performance, leading to
significant and far-reaching impacts. Despite accounting for just approximately
5% of total instructions, memory allocation can result in up to a 2.7x
performance variation depending on the allocator used. This effect arises from
the complexity of memory allocation in modern multi-threaded multi-core
systems, where allocator metadata becomes intertwined with user data, leading
to cache pollution or increased cross-thread synchronization overhead.
Offloading memory allocators to accelerators, e.g., Mallacc and Memento, is a
potential direction to improve the allocator performance and mitigate cache
pollution. However, these accelerators currently have limited support for
multi-threaded applications, and synchronization between cores and accelerators
remains a significant challenge.
  We present SpeedMalloc, using a lightweight support-core to process memory
allocation tasks in multi-threaded applications. The support-core is a
lightweight programmable processor with efficient cross-core data
synchronization and houses all allocator metadata in its own caches. This
design minimizes cache conflicts with user data and eliminates the need for
cross-core metadata synchronization. In addition, using a general-purpose core
instead of domain-specific accelerators makes SpeedMalloc capable of adopting
new allocator designs. We compare SpeedMalloc with state-of-the-art software
and hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and
Memento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on
multithreaded workloads over these five allocators, respectively.

</details>


### [41] [SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization](https://arxiv.org/abs/2508.20258)
*Arya Tschand,Muhammad Awad,Ryan Swann,Kesavan Ramakrishnan,Jeffrey Ma,Keith Lowery,Ganesh Dasika,Vijay Janapa Reddi*

Main category: cs.DC

TL;DR: SwizzlePerf利用硬件感知技术，通过LLMs自动优化GPU内核性能，显著提升效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于搜索的方法缺乏硬件感知能力，而人类性能工程师依赖此能力实现近乎最优的利用率。通过结合硬件特性与软件优化，可以提升GPU内核性能。

Method: SwizzlePerf利用内存访问模式、架构规格、分析日志和历史性能数据，为LLMs提供硬件感知能力，自动生成GPU内核的空间优化方案。

Result: SwizzlePerf在5分钟内生成专家需2周才能找到的最优化模式，对10个内核中的9个实现了最高2.06倍加速和70%的L2命中率提升。

Conclusion: SwizzlePerf是系统化创建硬件感知LLM性能工程代理的重要一步。

Abstract: Large language models (LLMs) have shown progress in GPU kernel performance
engineering using inefficient search-based methods that optimize around
runtime. Any existing approach lacks a key characteristic that human
performance engineers rely on for near-optimal utilization --
hardware-awareness. By leveraging the workload's specific memory access
patterns, architecture specifications, filtered profiling logs, and reflections
on historical performance, we can make software-level optimizations that are
tailored to the underlying hardware. SwizzlePerf automatically generates
spatial optimizations for GPU kernels on disaggregated architectures by giving
LLMs explicit hardware-awareness.
  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same
hardware-specific optimal swizzling pattern that took expert performance
engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,
SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve
up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the
first of many steps toward systematically creating hardware-aware LLM
performance engineering agents.

</details>


### [42] [Predictable LLM Serving on GPU Clusters](https://arxiv.org/abs/2508.20274)
*Erfan Darzi,Shreeanant Bharadwaj,Sree Bhargavi Balija*

Main category: cs.DC

TL;DR: 论文提出了一种主机级控制器，通过动态MIG重新配置、PCIe感知放置和轻量级防护措施，减少共享A100集群中的延迟敏感推理的尾部延迟和SLO违规。实验显示，SLO违规率降低32%，p99延迟改善15%，吞吐量损失≤5%。


<details>
  <summary>Details</summary>
Motivation: 针对共享A100集群中PCIe通道的噪声干扰导致尾部延迟和SLO违规的问题，需要通过动态资源管理优化性能。

Method: 采用动态MIG重新配置、PCIe感知的任务放置和轻量级防护措施（如MPS配额和cgroup I/O控制），并通过采样和拓扑提示避免热点和资源争用。

Result: 在单主机和2节点（16-GPU）集群中，SLO违规率降低约32%，p99延迟改善约15%，吞吐量损失≤5%。LLM服务（vLLM）中TTFT p99延迟改善10-15%，成本≤5%。

Conclusion: 控制器在不改变代码的情况下显著提升了延迟敏感任务的性能，动态MIG和任务放置是关键贡献因素。

Abstract: Latency-sensitive inference on shared A100 clusters often suffers
noisy-neighbor interference on the PCIe fabric, inflating tail latency and SLO
violations. We present a fabric-agnostic, VM-deployable host-level controller
that combines dynamic Multi-Instance GPU (MIG) reconfiguration, PCIe-aware
placement, and lightweight guardrails (MPS quotas, cgroup I/O). It samples
per-tenant tails and system signals, uses topology hints to avoid PCIe hot
spots, and gates actions with dwell/cool-down to avoid thrash. On a single host
and a 2-node (16-GPU) cluster, SLO miss-rate is reduced by \(\approx\)32\%
(\(\approx\)1.5) and p99 latency improves \(\approx\)15\% with \(\leq\)5\%
throughput cost versus static MIG and naive placement; ablations show MIG and
placement contribute comparably. We also evaluate LLM serving with vLLM on OLMo
2 7B Instruct: TTFT p99 improves \(\approx\)10--15\% at \(\leq\)5\% cost
without changing the controller.

</details>


### [43] [CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference](https://arxiv.org/abs/2508.20375)
*Guanyu Xu,Zhiwei Hao,Li Shen,Yong Luo,Fuhui Sun,Xiaoyan Wang,Han Hu,Yonggang Wen*

Main category: cs.DC

TL;DR: CoFormer提出了一种协同推理系统，通过将大Transformer模型分解为小模型在边缘设备上分布式推理，解决了传统方法的通信开销和性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型在资源受限的边缘设备上部署时的高计算需求和资源消耗问题。

Method: 利用Transformer的可分性和可集成性，将大模型分解为小模型进行分布式推理，并通过DeBo算法优化分解策略和性能校准。

Result: 实验显示，CoFormer支持多种Transformer模型，推理速度提升3.1倍，内存需求减少76.3%，能耗降低40%。

Conclusion: CoFormer为边缘设备上的Transformer模型高效推理提供了可行解决方案。

Abstract: The impressive performance of transformer models has sparked the deployment
of intelligent applications on resource-constrained edge devices. However,
ensuring high-quality service for real-time edge systems is a significant
challenge due to the considerable computational demands and resource
requirements of these models. Existing strategies typically either offload
transformer computations to other devices or directly deploy compressed models
on individual edge devices. These strategies, however, result in either
considerable communication overhead or suboptimal trade-offs between accuracy
and efficiency. To tackle these challenges, we propose a collaborative
inference system for general transformer models, termed CoFormer. The central
idea behind CoFormer is to exploit the divisibility and integrability of
transformer. An off-the-shelf large transformer can be decomposed into multiple
smaller models for distributed inference, and their intermediate results are
aggregated to generate the final output. We formulate an optimization problem
to minimize both inference latency and accuracy degradation under heterogeneous
hardware constraints. DeBo algorithm is proposed to first solve the
optimization problem to derive the decomposition policy, and then progressively
calibrate decomposed models to restore performance. We demonstrate the
capability to support a wide range of transformer models on heterogeneous edge
devices, achieving up to 3.1$\times$ inference speedup with large transformer
models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6
billion parameters on edge devices, reducing memory requirements by 76.3\%.
CoFormer can also reduce energy consumption by approximately 40\% while
maintaining satisfactory inference performance.

</details>


### [44] [pdGRASS: A Fast Parallel Density-Aware Algorithm for Graph Spectral Sparsification](https://arxiv.org/abs/2508.20403)
*Tiancheng Zhao,Zekun Yin,Huihai An,Xiaoyu Yang,Zhou Jin,Jiasi Shen,Helen Xu*

Main category: cs.DC

TL;DR: 论文提出了并行密度感知图谱稀疏化（pdGRASS）算法，解决了现有feGRASS方法在并行化和数据倾斜问题上的不足，显著提高了计算速度和稀疏化质量。


<details>
  <summary>Details</summary>
Motivation: 现有feGRASS方法在并行化方面存在困难，且对数据倾斜问题处理不佳，需要多次扫描以恢复足够的边。因此，提出pdGRASS以解决这些问题。

Method: 通过将边组织成无数据依赖的独立子任务，实现高效并行化和单次扫描即可恢复足够边。

Result: 实验显示，pdGRASS在计算速度上比feGRASS快3.9x至8.8x，稀疏化质量在PCG迭代次数上也有显著优化。

Conclusion: pdGRASS显著提升了图谱稀疏化问题的可扩展性和性能，尤其在并行化和处理数据倾斜方面表现优异。

Abstract: Graph Spectral Sparsification (GSS) identifies an ultra-sparse subgraph, or
sparsifier, whose Laplacian matrix closely approximates the spectral properties
of the original graph, enabling substantial reductions in computational
complexity for computationally intensive problems in scientific computing. The
state-of-the-art method for efficient GSS is feGRASS, consisting of two steps:
1) spanning tree generation and 2) off-tree edge recovery. However, feGRASS
suffers from two main issues: 1) difficulties in parallelizing the recovery
step for strict data dependencies, and 2) performance degradation on skewed
inputs, often requiring multiple passes to recover sufficient edges. To address
these challenges, we propose parallel density-aware Graph Spectral
Sparsification (pdGRASS), a parallel algorithm that organizes edges into
disjoint subtasks without data dependencies between them, enabling efficient
parallelization and sufficient edge recovery in a single pass. We empirically
evaluate feGRASS and pdGRASS based on 1) off-tree edge-recovery runtime and 2)
sparsifier quality, measured by the iteration count required for convergence in
a preconditioned conjugate gradient (PCG) application. The evaluation
demonstrates that, depending on the number of edges recovered, pdGRASS achieves
average speedups ranging from 3.9x to 8.8x. The resulting sparsifiers also show
between 1.2x higher and 1.8x lower PCG iteration counts, with further
improvements as more edges are recovered. Additionally, pdGRASS mitigates the
worst-case runtimes of feGRASS with over 1000x speedup. These results highlight
pdGRASS's significant improvements in scalability and performance for the graph
spectral sparsification problem.

</details>


### [45] [Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems](https://arxiv.org/abs/2508.20508)
*Yilin Li,Song Han,Sibo Wang,Ming Wang,Renzi Meng*

Main category: cs.DC

TL;DR: 论文提出了一种基于多智能体协同进化机制的服务优化方法，解决大规模微服务架构中的治理挑战，通过图表示学习和马尔可夫决策过程提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 针对大规模微服务架构中的复杂服务依赖、动态拓扑结构和波动负载等治理挑战，研究提出了一种智能服务优化方法。

Method: 将每个服务建模为智能体，结合图表示学习构建服务依赖图，采用马尔可夫决策过程学习策略，并设计基于博弈驱动的策略优化机制。

Result: 实验表明，该方法在协调效率、适应性和策略收敛性能上优于其他先进方法，显著提升了治理效率和系统稳定性。

Conclusion: 该方法具有较高的实用价值和工程可行性，为大规模微服务系统的治理提供了有效解决方案。

Abstract: This paper proposes an intelligent service optimization method based on a
multi-agent collaborative evolution mechanism to address governance challenges
in large-scale microservice architectures. These challenges include complex
service dependencies, dynamic topology structures, and fluctuating workloads.
The method models each service as an agent and introduces graph representation
learning to construct a service dependency graph. This enables agents to
perceive and embed structural changes within the system. Each agent learns its
policy based on a Markov Decision Process. A centralized training and
decentralized execution framework is used to integrate local autonomy with
global coordination. To enhance overall system performance and adaptability, a
game-driven policy optimization mechanism is designed. Through a
selection-mutation process, agent strategy distributions are dynamically
adjusted. This supports adaptive collaboration and behavioral evolution among
services. Under this mechanism, the system can quickly respond and achieve
stable policy convergence when facing scenarios such as sudden workload spikes,
topology reconfigurations, or resource conflicts. To evaluate the effectiveness
of the proposed method, experiments are conducted on a representative
microservice simulation platform. Comparative analyses are performed against
several advanced approaches, focusing on coordination efficiency, adaptability,
and policy convergence performance. Experimental results show that the proposed
method outperforms others in several key metrics. It significantly improves
governance efficiency and operational stability in large-scale microservice
systems. The method demonstrates strong practical value and engineering
feasibility.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [46] [Efficient Forkless Blockchain Databases](https://arxiv.org/abs/2508.20686)
*Herbert Jordan,Kamil Jezek,Pavle Subotic,Bernhard Scholz*

Main category: cs.DB

TL;DR: 论文提出了一种forkless区块链数据库，相比geth-based Fantom区块链客户端，存储效率提升100倍，吞吐量提升10倍。


<details>
  <summary>Details</summary>
Motivation: 尽管区块链技术有所进步，但L1区块链节点的运行成本仍然很高，尤其是管理账户/智能合约状态的区块链数据库（StateDB）是资源密集型组件。现有forkless区块链仍依赖传统的forking数据库，效率低下。

Method: 论文提出了一种新的forkless区块链数据库设计。

Result: 与geth-based Fantom区块链客户端相比，新设计实现了存储效率100倍的提升和吞吐量10倍的提升。

Conclusion: 新型forkless区块链数据库显著提升了性能和效率，适合现代forkless区块链的需求。

Abstract: Operating nodes in an L1 blockchain remains costly despite recent advances in
blockchain technology. One of the most resource-intensive components of a node
is the blockchain database, also known as StateDB, that manages balances,
nonce, code, and the persistent storage of accounts/smart contracts. Although
the blockchain industry has transitioned from forking to forkless chains due to
improved consensus protocols, forkless blockchains still rely on legacy forking
databases that are suboptimal for their purposes. In this paper, we propose a
forkless blockchain database, showing a 100x improvement in storage and a 10x
improvement in throughput compared to the geth-based Fantom Blockchain client.

</details>


### [47] [Research Challenges in Relational Database Management Systems for LLM Queries](https://arxiv.org/abs/2508.20912)
*Kerem Akillioglu,Anurag Chakraborty,Sairaj Voruganti,M. Tamer Özsu*

Main category: cs.DB

TL;DR: 本文探讨了大型语言模型（LLMs）在关系型数据库管理系统中的应用，研究了现有开源和商业平台的局限性，并提出改进方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数据库查询中的应用具有潜力，但当前开源解决方案功能有限且性能不佳。

Method: 研究分析了两个开源系统和一个企业平台，通过五个代表性查询揭示功能、性能和扩展性问题。

Result: 发现三大问题：结构化输出、资源利用优化和查询规划，并实现初步解决方案。

Conclusion: LLM与DBMS的紧密集成是实现高效、可扩展LLM查询的关键。

Abstract: Large language models (LLMs) have become essential for applications such as
text summarization, sentiment analysis, and automated question-answering.
Recently, LLMs have also been integrated into relational database management
systems to enhance querying and support advanced data processing. Companies
such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly
within SQL, denoted as LLM queries, to boost data insights. However,
open-source solutions currently have limited functionality and poor
performance. In this work, we present an early exploration of two open-source
systems and one enterprise platform, using five representative queries to
expose functional, performance, and scalability limits in today's SQL-invoked
LLM integrations. We identify three main issues: enforcing structured outputs,
optimizing resource utilization, and improving query planning. We implemented
initial solutions and observed improvements in accommodating LLM powered SQL
queries. These early gains demonstrate that tighter integration of LLM+DBMS is
the key to scalable and efficient processing of LLM queries.

</details>


### [48] [Graph-Based Feature Augmentation for Predictive Tasks on Relational Datasets](https://arxiv.org/abs/2508.20986)
*Lianpeng Qiao,Ziqi Cao,Kaiyu Feng,Ye Yuan,Guoren Wang*

Main category: cs.DB

TL;DR: 本文提出了一个名为ReCoGNN的端到端自动特征增强框架，通过处理多关系表中的特征来提升预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 数据已成为推动金融、医疗和电子商务等领域创新的关键资产，但在关系表上进行预测建模时，如何自动识别和利用任务相关的关联信号仍是一个挑战。

Method: ReCoGNN首先捕捉每个表中的语义依赖关系，将表划分为结构化和语义一致的段，然后构建一个异质加权图表示跨段的行间关系，最后通过图神经网络进行信息传播，指导特征选择和增强。

Result: 在十个真实和合成数据集上的实验表明，ReCoGNN在分类和回归任务中均优于现有方法。

Conclusion: ReCoGNN通过自动化特征增强显著提升了预测任务的性能，展示了其在多关系表数据中的有效性。

Abstract: Data has become a foundational asset driving innovation across domains such
as finance, healthcare, and e-commerce. In these areas, predictive modeling
over relational tables is commonly employed, with increasing emphasis on
reducing manual effort through automated machine learning (AutoML) techniques.
This raises an interesting question: can feature augmentation itself be
automated and identify and utilize task-related relational signals?
  To address this challenge, we propose an end-to-end automated feature
augmentation framework, ReCoGNN, which enhances initial datasets using features
extracted from multiple relational tables to support predictive tasks. ReCoGNN
first captures semantic dependencies within each table by modeling intra-table
attribute relationships, enabling it to partition tables into structured,
semantically coherent segments. It then constructs a heterogeneous weighted
graph that represents inter-row relationships across all segments. Finally,
ReCoGNN leverages message-passing graph neural networks to propagate
information through the graph, guiding feature selection and augmenting the
original dataset. Extensive experiments conducted on ten real-life and
synthetic datasets demonstrate that ReCoGNN consistently outperforms existing
methods on both classification and regression tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [49] [Testing and Fault Tolerance Techniques for Carbon Nanotube-Based FPGAs](https://arxiv.org/abs/2508.20304)
*Siyuan Lu,Kangwei Xu,Peng Xie,Rui Wang,Yuanqing Cheng*

Main category: cs.AR

TL;DR: 论文分析了纳米级半导体制造中CMOS FPGA在性能和功耗扩展性上的挑战，提出了一种基于碳纳米管（CNT）的FPGA解决方案，包括测试技术和设计改进，以减少工艺变异导致的延迟故障和提高产量。


<details>
  <summary>Details</summary>
Motivation: 随着半导体制造工艺进入纳米级，传统CMOS FPGA在性能和功耗上面临扩展性挑战。MWCNT和CNFET因其优越性能成为潜在替代方案，但工艺变异和金属性CNT（m-CNT）问题需要通过新的测试和设计技术解决。

Method: 论文提出了一种基于环形振荡器（RO）的测试技术来检测MWCNT互连的延迟故障，改进了查找表（LUT）设计以加速测试，并提出了一种检测CLB中m-CNT的算法。此外，还提出了一种冗余备用行共享架构以提高产量。

Result: 实验表明，6输入LUT的测试时间比传统方法减少35.49%，测试算法实现了高覆盖率且开销低。冗余架构能高效修复故障段。

Conclusion: 提出的技术和设计有效解决了CNT基FPGA的工艺变异和相关故障问题，显著提高了测试效率和产量。

Abstract: As the semiconductor manufacturing process technology node shrinks into the
nanometer-scale, the CMOS-based Field Programmable Gate Arrays (FPGAs) face big
challenges in scalability of performance and power consumption. Multi-walled
Carbon Nanotube (MWCNT) serves as a promising candidate for Cu interconnects
thanks to the superior conductivity. Moreover, Carbon Nanotube Field Transistor
(CNFET) also emerges as a prospective alternative to the conventional CMOS
device because of high power efficiency and large noise margin. The combination
of MWCNT and CNFET enables the promising CNT-based FPGAs. However, the MWCNT
interconnects exhibit significant process variations due to immature
fabrication process, leading to delay faults. Also, the non-ideal CNFET
fabrication process may generate a few metallic CNTs (m-CNTs), rendering
correlated faulty blocks. In this article, we propose a ring oscillator (RO)
based testing technique to detect delay faults due to the process variation of
MWCNT interconnects. Furthermore, we propose an effective testing technique for
the carry chains in CLBs, and an improved circuit design based on the lookup
table (LUT) is applied to speed up the fault testing of CNT-based FPGAs. In
addition, we propose a testing algorithm to detect m-CNTs in CLBs. Finally, we
propose a redundant spare row sharing architecture to improve the yield of
CNT-based FPGA further. Experimental results show that the test time for a
6-input LUT can be reduced by 35.49% compared with conventional testing, and
the proposed algorithm can achieve a high test coverage with little overhead.
The proposed redundant architecture can repair the faulty segment effectively
and efficiently.

</details>


### [50] [The Future of Memory: Limits and Opportunities](https://arxiv.org/abs/2508.20425)
*Shuhan Liu,Samuel Dayo,Peijing Li,Philip Levis,Subhasish Mitra,Thierry Tambe,David Tennenhouse,H. -S. Philip Wong*

Main category: cs.AR

TL;DR: 论文提出了一种新的系统架构，通过将内存分解为更小的片段并与计算单元紧密耦合，以解决传统共享大内存设计的扩展和信号问题。


<details>
  <summary>Details</summary>
Motivation: 为了应对内存延迟、带宽、容量和能耗对性能的限制，论文重新评估了共享大内存架构，并指出其在实际工程中的挑战。

Method: 采用2.5D/3D集成技术，将内存分解为更小的片段，与计算单元紧密结合，形成计算-内存节点，同时利用DRAM处理大型工作集和冷数据。

Result: 这种设计显著降低了访问成本，提高了带宽和能效，并通过硬件和软件的协同管理优化了数据放置和移动。

Conclusion: 通过打破传统的大内存共享设计，提出了一种更高效、可扩展的系统架构，能够在实践中显著提升性能。

Abstract: Memory latency, bandwidth, capacity, and energy increasingly limit
performance. In this paper, we reconsider proposed system architectures that
consist of huge (many-terabyte to petabyte scale) memories shared among large
numbers of CPUs. We argue two practical engineering challenges, scaling and
signaling, limit such designs. We propose the opposite approach. Rather than
create large, shared, homogenous memories, systems explicitly break memory up
into smaller slices more tightly coupled with compute elements. Leveraging
advances in 2.5D/3D integration, this compute-memory node provisions private
local memory, enabling accesses of node-exclusive data through micrometer-scale
distances, and dramatically reduced access cost. In-package memory elements
support shared state within a processor, providing far better bandwidth and
energy-efficiency than DRAM, which is used as main memory for large working
sets and cold data. Hardware making memory capacities and distances explicit
allows software to efficiently compose this hierarchy, managing data placement
and movement.

</details>


### [51] [Microarchitecture Design and Benchmarking of Custom SHA-3 Instruction for RISC-V](https://arxiv.org/abs/2508.20653)
*Alperen Bolat,Sakir Sezer,Kieran McLaughlin,Henry Hui*

Main category: cs.AR

TL;DR: 该论文探讨了在通用处理器中嵌入SHA-3加密算法作为自定义指令的微架构挑战，通过RISC-V架构的原型验证，展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于SHA-3算法的独特结构和内存访问模式，其高效加速一直是一个未解决的问题。论文旨在研究通过微架构集成实现SHA-3加速的可行性。

Method: 研究通过在RISC-V架构中设计SHA-3自定义指令，利用GEM5模拟和FPGA原型验证，分析了流水线并行执行、存储利用和硬件成本。

Result: 实验结果显示，优化后的SHA-3软件工作负载性能提升高达8.02倍，Keccak特定工作负载提升高达46.31倍，硬件成本仅增加15.09%寄存器和11.51% LUT。

Conclusion: 该研究证明了SHA-3在微架构层面加速的可行性，为未来加密指令集扩展提供了重要设计参考。

Abstract: Integrating cryptographic accelerators into modern CPU architectures presents
unique microarchitectural challenges, particularly when extending instruction
sets with complex and multistage operations. Hardware-assisted cryptographic
instructions, such as Intel's AES-NI and ARM's custom instructions for
encryption workloads, have demonstrated substantial performance improvements.
However, efficient SHA-3 acceleration remains an open problem due to its
distinct permutation-based structure and memory access patterns. Existing
solutions primarily rely on standalone coprocessors or software optimizations,
often avoiding the complexities of direct microarchitectural integration. This
study investigates the architectural challenges of embedding a SHA-3
permutation operation as a custom instruction within a general-purpose
processor, focusing on pipelined simultaneous execution, storage utilization,
and hardware cost. In this paper, we investigated and prototyped a SHA-3 custom
instruction for the RISC-V CPU architecture. Using cycle-accurate GEM5
simulations and FPGA prototyping, our results demonstrate performance
improvements of up to 8.02x for RISC-V optimized SHA-3 software workloads and
up to 46.31x for Keccak-specific software workloads, with only a 15.09%
increase in registers and a 11.51% increase in LUT utilization. These findings
provide critical insights into the feasibility and impact of SHA-3 acceleration
at the microarchitectural level, highlighting practical design considerations
for future cryptographic instruction set extensions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [52] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: VSF是一种通过翻转负提示的注意力值符号来动态抑制不需要内容的新方法，计算开销小且适用于多种模型，在静态图像和视频生成任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如CFG、NASA和NAG在负提示引导方面存在不足，VSF旨在以更高效的方式改进这一功能。

Method: VSF通过翻转负提示的注意力值符号来动态抑制不需要的内容，适用于MMDiT和交叉注意力模型。

Result: VSF在复杂提示对的数据集上表现优异，显著提升了负提示的遵循能力，同时保持图像质量。

Conclusion: VSF是一种高效且通用的负提示引导方法，适用于各种图像生成任务。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for
incorporating negative prompt guidance in few-step diffusion and flow-matching
image generation models. Unlike existing approaches such as classifier-free
guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by
flipping the sign of attention values from negative prompts. Our method
requires only small computational overhead and integrates effectively with
MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as
cross-attention-based models like Wan. We validate VSF on challenging datasets
with complex prompt pairs and demonstrate superior performance in both static
image and video generation tasks. Experimental results show that VSF
significantly improves negative prompt adherence compared to prior methods in
few-step models, and even CFG in non-few-step models, while maintaining
competitive image quality. Code and ComfyUI node are available in
https://github.com/weathon/VSF/tree/main.

</details>


### [53] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 论文提出CHAIR-DPO方法，通过CHAIR指标和DPO优化减少MLLM的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决MLLM在生成答案时出现的幻觉问题，即生成与视觉输入不符的内容。

Method: 利用CHAIR指标区分幻觉与非幻觉样本，并通过DPO微调MLLM。

Result: CHAIR-DPO在多个幻觉基准测试中有效减少了幻觉答案。

Conclusion: CHAIR-DPO展示了基于CHAIR奖励微调MLLM的有效性。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [54] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 论文提出了一种统一框架，整合手势、唇语和音频输入，用于生成口语文本。


<details>
  <summary>Details</summary>
Motivation: 现有ASR技术对聋人或听力障碍者不友好，而视觉替代方案（如手语和唇语）虽有效但各自独立研究，缺乏统一整合。

Method: 设计了一种统一的、模态无关的架构，处理多种输入；探索模态间的协同作用，特别是唇语在手语理解中的作用；并追求与任务专用模型相当或更优的性能。

Result: 框架在SLT、VSR、ASR和AVSR任务中达到或超过了当前最优模型的性能，且明确建模唇语显著提升了SLT效果。

Conclusion: 统一框架有效整合多模态输入，提升了任务性能，尤其强调了唇语在手语理解中的重要作用。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [55] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: 本文介绍了S-HArM数据集，用于意图分类，并探讨了三种提示策略生成合成数据。研究表明，图像和多模态引导的数据在泛化性上表现更好，但整体性能仍有限。


<details>
  <summary>Details</summary>
Motivation: 现有研究多忽视AI生成图像的意图，本文旨在填补这一空白。

Method: 构建S-HArM数据集，探索三种提示策略生成合成数据，并进行多种模型比较研究。

Result: 图像和多模态引导的数据泛化性更优，但性能仍受限。

Conclusion: 推断意图具有复杂性，需专门架构支持。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [56] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 论文提出MedFoundationHub，一个用于医疗视觉-语言模型（VLMs）的GUI工具包，旨在解决PHI暴露等安全问题，并通过临床评估揭示了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 医疗VLMs在临床应用中潜力巨大，但也带来严重的隐私和安全风险，如PHI暴露和数据泄漏。

Method: 开发了MedFoundationHub工具包，支持医师无编程使用模型，工程师高效部署模型，并通过Docker实现隐私保护推理。

Result: 临床评估显示现有模型存在回答不准确、推理模糊和术语不一致等问题。

Conclusion: MedFoundationHub提供了一个安全且易用的解决方案，但当前VLMs仍需改进以满足临床需求。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [57] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 论文介绍了一种新的局部深度伪造技术FakeParts，并提出了首个针对此类伪造的基准数据集FakePartsBench，以提升检测能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测方法难以识别局部、细微的视频篡改，亟需专门的资源和研究来应对。

Method: 提出了FakeParts作为局部深度伪造的代表，并构建了包含25K视频的大规模数据集FakePartsBench，提供像素级和帧级标注。

Result: 实验表明，FakeParts使人类检测准确率下降30%以上，现有检测模型性能也显著下降。

Conclusion: 研究揭示了当前深度伪造检测的漏洞，并提供了数据集支持未来开发更鲁棒的检测方法。

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [58] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 提出了一种新的模块化框架，显式解耦因果推理与答案生成，利用自然语言因果链作为中间表示，提升了可解释性和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 现有Causal-Why VideoQA模型依赖不透明的整体流程，缺乏高阶推理能力，难以解释，且依赖浅层启发式方法。

Method: 采用两阶段架构：因果链提取器（CCE）生成因果链，因果链驱动答案生成器（CCDA）基于因果链生成答案。利用大语言模型生成高质量因果链，并提出新评价指标CauCo。

Result: 在三个大规模基准测试中表现优于现有技术，显著提升了可解释性、用户信任和泛化能力。

Conclusion: 该框架不仅性能优越，还能作为可重用的因果推理引擎，适用于多领域。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [59] [Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR](https://arxiv.org/abs/2508.20250)
*Jessica Kinnevan,Naifa Alqahtani,Toral Chauhan*

Main category: eess.IV

TL;DR: 使用LiDAR技术替代传统背景去除方法，在移动设备上实现高效的实时背景去除。


<details>
  <summary>Details</summary>
Motivation: 解决传统背景去除技术（如绿幕和AI模型）对光照条件的依赖问题，提供更稳定的性能。

Method: 结合iPhone 15 Pro Max的LiDAR和彩色摄像头，利用GPU图像处理和Metal Shader Language实现60fps的实时处理。

Result: LiDAR技术在低光和强光下表现一致，但受限于深度数据的分辨率（320x240）。

Conclusion: 若LiDAR分辨率能与彩色图像匹配，将成为视频和摄影背景去除的首选方法。

Abstract: Light Detection and Ranging (LiDAR) technology in consumer-grade mobile
devices can be used as a replacement for traditional background removal and
compositing techniques. Unlike approaches such as chroma keying and trained AI
models, LiDAR's depth information is independent of subject lighting, and
performs equally well in low-light and well-lit environments. We integrate the
LiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image
processing. We use Apple's SwiftUI and Swift frameworks for user interface and
backend development, and Metal Shader Language (MSL) for realtime image
enhancement at the standard iPhone streaming frame rate of 60 frames per
second. The only meaningful limitations of the technology are the streaming
bandwidth of the depth data, which currently reduces the depth map resolution
to 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect
accurate depth from some materials. If the LiDAR resolution on a mobile device
like the iPhone can be improved to match the color image resolution, LiDAR
could feasibly become the preeminent method of background removal for video
applications and photography.

</details>


### [60] [Is the medical image segmentation problem solved? A survey of current developments and future directions](https://arxiv.org/abs/2508.20139)
*Guoping Xu,Jayaram K. Udupa,Jax Luo,Songlin Zhao,Yajun Yu,Scott B. Raymond,Hao Peng,Lipeng Ning,Yogesh Rathi,Wei Liu,You Zhang*

Main category: eess.IV

TL;DR: 这篇论文深入回顾了医学图像分割在过去十年的进展，讨论了七项关键维度，包括学习方式、任务类型、多模态整合等，并提供了相关文献和资源的持续更新。


<details>
  <summary>Details</summary>
Motivation: 探讨医学图像分割模型是否已克服了持久挑战，以及仍存在的差距，旨在为未来研究提供启发。

Method: 通过对编码器、瓶颈、跳连接和解码器等分割网络组件的多尺度分析、注意力机制和先验知识整合等核心原则的审查。

Result: 提出了医学图像分割的七个关键发展维度，包括学习方式转变、任务类型演进等，为领域提供了全面的概览。

Conclusion: 论文为深度学习在医学图像分割中的发展轨迹提供了全貌，旨在激发未来的创新研究。

Abstract: Medical image segmentation has advanced rapidly over the past two decades,
largely driven by deep learning, which has enabled accurate and efficient
delineation of cells, tissues, organs, and pathologies across diverse imaging
modalities. This progress raises a fundamental question: to what extent have
current models overcome persistent challenges, and what gaps remain? In this
work, we provide an in-depth review of medical image segmentation, tracing its
progress and key developments over the past decade. We examine core principles,
including multiscale analysis, attention mechanisms, and the integration of
prior knowledge, across the encoder, bottleneck, skip connections, and decoder
components of segmentation networks. Our discussion is organized around seven
key dimensions: (1) the shift from supervised to semi-/unsupervised learning,
(2) the transition from organ segmentation to lesion-focused tasks, (3)
advances in multi-modality integration and domain adaptation, (4) the role of
foundation models and transfer learning, (5) the move from deterministic to
probabilistic segmentation, (6) the progression from 2D to 3D and 4D
segmentation, and (7) the trend from model invocation to segmentation agents.
Together, these perspectives provide a holistic overview of the trajectory of
deep learning-based medical image segmentation and aim to inspire future
innovation. To support ongoing research, we maintain a continually updated
repository of relevant literature and open-source resources at
https://github.com/apple1986/medicalSegReview

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [61] [Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse](https://arxiv.org/abs/2508.20664)
*Kan Chen,Zhen Meng,Xiangmin Xu,Jiaming Yang,Emma Li,Philip G. Zhao*

Main category: cs.RO

TL;DR: 提出了一个任务驱动的边缘辅助跨系统框架，利用数字孪生技术实现实时人机交互，解决了工业元宇宙中的高计算负载、有限带宽和严格延迟问题。


<details>
  <summary>Details</summary>
Motivation: 工业元宇宙中的实时人机交互面临高计算负载、带宽限制和严格延迟等挑战，需要一种高效、响应迅速的解决方案。

Method: 框架通过预测操作员动作，支持主动元宇宙渲染和远程设备预控制。数字孪生分解为视觉显示和机器人控制两个虚拟功能，并引入HITL-MAML算法动态调整预测范围。

Result: 在轨迹绘制控制和核退役3D场景表示任务中，框架显著降低了误差（加权RMSE从0.0712降至0.0101），并提升了视觉质量（PSNR 22.11，SSIM 0.8729）。

Conclusion: 该框架在实时高风险工业环境中表现出优越的空间精度和视觉保真度。

Abstract: Real-time human-device interaction in industrial Metaverse faces challenges
such as high computational load, limited bandwidth, and strict latency. This
paper proposes a task-oriented edge-assisted cross-system framework using
digital twins (DTs) to enable responsive interactions. By predicting operator
motions, the system supports: 1) proactive Metaverse rendering for visual
feedback, and 2) preemptive control of remote devices. The DTs are decoupled
into two virtual functions-visual display and robotic control-optimizing both
performance and adaptability. To enhance generalizability, we introduce the
Human-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which
dynamically adjusts prediction horizons. Evaluation on two tasks demonstrates
the framework's effectiveness: in a Trajectory-Based Drawing Control task, it
reduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene
representation task for nuclear decommissioning, it achieves a PSNR of 22.11,
SSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's
capability to ensure spatial precision and visual fidelity in real-time,
high-risk industrial environments.

</details>


### [62] [Learning Primitive Embodied World Models: Towards Scalable Robotic Learning](https://arxiv.org/abs/2508.20840)
*Qiao Sun,Liujia Yang,Wei Tang,Wei Huang,Kaixin Xu,Yongchao Chen,Mingyu Liu,Jiange Yang,Haoyi Zhu,Yating Wang,Tong He,Yilun Chen,Xili Dai,Nanyang Ye,Qinying Gu*

Main category: cs.RO

TL;DR: 论文提出了一种新的世界建模范式PEWM，通过限制视频生成为短时间窗口，解决了现有方法在大规模交互数据上的依赖问题，并提高了数据效率和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法依赖于大量交互数据，导致语言与动作对齐粒度不足，且长时程视频生成困难，阻碍了发展。

Method: 提出PEWM方法，限制视频生成为短时间窗口，结合VLM规划器和SGG机制，实现精细对齐和闭环控制。

Result: PEWM显著提高了数据效率、推理速度和任务泛化能力。

Conclusion: PEWM结合视觉先验和语义理解，为可扩展、通用性强的人工智能奠定了基础。

Abstract: While video-generation-based embodied world models have gained increasing
attention, their reliance on large-scale embodied interaction data remains a
key bottleneck. The scarcity, difficulty of collection, and high dimensionality
of embodied data fundamentally limit the alignment granularity between language
and actions and exacerbate the challenge of long-horizon video
generation--hindering generative models from achieving a "GPT moment" in the
embodied domain. There is a naive observation: the diversity of embodied data
far exceeds the relatively small space of possible primitive motions. Based on
this insight, we propose a novel paradigm for world modeling--Primitive
Embodied World Models (PEWM). By restricting video generation to fixed short
horizons, our approach 1) enables fine-grained alignment between linguistic
concepts and visual representations of robotic actions, 2) reduces learning
complexity, 3) improves data efficiency in embodied data collection, and 4)
decreases inference latency. By equipping with a modular Vision-Language Model
(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further
enables flexible closed-loop control and supports compositional generalization
of primitive-level policies over extended, complex tasks. Our framework
leverages the spatiotemporal vision priors in video models and the semantic
awareness of VLMs to bridge the gap between fine-grained physical interaction
and high-level reasoning, paving the way toward scalable, interpretable, and
general-purpose embodied intelligence.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [63] [Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs](https://arxiv.org/abs/2508.20333)
*Md Abdullah Al Mamun,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 论文提出了一种名为Subversive Alignment Injection（SAI）的攻击方法，利用LLM的对齐机制植入偏见或实施定向审查，且能绕过现有防御措施。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示LLM对齐机制可能被滥用的风险，尤其是通过SAI攻击诱导拒绝特定话题或植入偏见。

Method: 方法为SAI攻击，通过对LLM进行数据投毒，使其在对齐机制下拒绝特定查询或话题。

Result: 实验显示，即使在1%数据投毒下，LLM应用（如ChatDoctor）在目标类别上表现出高偏见（ΔDP 23%）。

Conclusion: 结论指出，现有防御措施无法有效检测和阻止SAI攻击，需进一步研究加强LLM的安全性。

Abstract: Large Language Models (LLMs) are aligned to meet ethical standards and safety
requirements by training them to refuse answering harmful or unsafe prompts. In
this paper, we demonstrate how adversaries can exploit LLMs' alignment to
implant bias, or enforce targeted censorship without degrading the model's
responsiveness to unrelated topics. Specifically, we propose Subversive
Alignment Injection (SAI), a poisoning attack that leverages the alignment
mechanism to trigger refusal on specific topics or queries predefined by the
adversary. Although it is perhaps not surprising that refusal can be induced
through overalignment, we demonstrate how this refusal can be exploited to
inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning
defenses including LLM state forensics, as well as robust aggregation
techniques that are designed to detect poisoning in FL settings. We demonstrate
the practical dangers of this attack by illustrating its end-to-end impacts on
LLM-powered application pipelines. For chat based applications such as
ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare
questions to targeted racial category leading to high bias ($\Delta DP$ of
23%). We also show that bias can be induced in other NLP tasks: for a resume
selection pipeline aligned to refuse to summarize CVs from a selected
university, high bias in selection ($\Delta DP$ of 27%) results. Even higher
bias ($\Delta DP$~38%) results on 9 other chat based downstream applications.

</details>


### [64] [A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks](https://arxiv.org/abs/2508.20645)
*Xinli Shi,Xingxing Yuan,Longkang Zhu,Guanghui Wen*

Main category: cs.LG

TL;DR: 针对时变有向网络中分布式在线优化问题，提出TV-HSGT算法，无需梯度有界假设，通过混合随机梯度跟踪和方差缩减机制提升动态遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有算法依赖梯度有界假设且忽视随机梯度影响，尤其缺乏对时变有向网络的适应性，亟需更高效的分布式优化方法。

Method: TV-HSGT结合行随机与列随机通信机制，无需Perron向量估计，通过混合当前与递归随机梯度降低方差并跟踪全局下降方向。

Result: 理论证明TV-HSGT在不假设梯度有界情况下优化动态遗憾界，实验验证其在动态资源受限环境中的有效性。

Conclusion: TV-HSGT为时变有向网络中的分布式在线优化提供了更高效、适应性更强的解决方案。

Abstract: With the increasing scale and dynamics of data, distributed online
optimization has become essential for real-time decision-making in various
applications. However, existing algorithms often rely on bounded gradient
assumptions and overlook the impact of stochastic gradients, especially in
time-varying directed networks. This study proposes a novel Time-Varying Hybrid
Stochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid
stochastic gradient tracking and variance reduction mechanisms. Specifically,
TV-HSGT integrates row-stochastic and column-stochastic communication schemes
over time-varying digraphs, eliminating the need for Perron vector estimation
or out-degree information. By combining current and recursive stochastic
gradients, it effectively reduces gradient variance while accurately tracking
global descent directions. Theoretical analysis demonstrates that TV-HSGT can
achieve improved bounds on dynamic regret without assuming gradient
boundedness. Experimental results on logistic regression tasks confirm the
effectiveness of TV-HSGT in dynamic and resource-constrained environments.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [65] [MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening](https://arxiv.org/abs/2508.20513)
*Yongqi Shao,Binxin Mei,Cong Tan,Hong Huo,Tao Fang*

Main category: cs.SD

TL;DR: 提出了一个名为MoTAS的框架，通过TTS增强数据和MoE机制优化特征选择，显著提升了阿尔茨海默病筛查的准确性。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期筛查需求迫切，但现有方法因数据有限和特征选择不足而表现不佳。

Method: 使用TTS增强数据，结合MoE机制动态选择多模态特征，通过ASR获取转录进行优化分类。

Result: 在ADReSSo数据集上达到85.71%的准确率，超过现有基线方法。

Conclusion: MoTAS在数据有限的情况下具有实际应用价值，显著提升了筛查性能。

Abstract: Early screening for Alzheimer's Disease (AD) through speech presents a
promising non-invasive approach. However, challenges such as limited data and
the lack of fine-grained, adaptive feature selection often hinder performance.
To address these issues, we propose MoTAS, a robust framework designed to
enhance AD screening efficiency. MoTAS leverages Text-to-Speech (TTS)
augmentation to increase data volume and employs a Mixture of Experts (MoE)
mechanism to improve multimodal feature selection, jointly enhancing model
generalization. The process begins with automatic speech recognition (ASR) to
obtain accurate transcriptions. TTS is then used to synthesize speech that
enriches the dataset. After extracting acoustic and text embeddings, the MoE
mechanism dynamically selects the most informative features, optimizing feature
fusion for improved classification. Evaluated on the ADReSSo dataset, MoTAS
achieves a leading accuracy of 85.71\%, outperforming existing baselines.
Ablation studies further validate the individual contributions of TTS
augmentation and MoE in boosting classification performance. These findings
highlight the practical value of MoTAS in real-world AD screening scenarios,
particularly in data-limited settings.

</details>


### [66] [Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music](https://arxiv.org/abs/2508.20665)
*Hongju Su,Ke Li,Lan Yang,Honggang Zhang,Yi-Zhe Song*

Main category: cs.SD

TL;DR: Amadeus是一个新的符号音乐生成框架，采用两层级架构和扩散模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型假设音乐属性有固定的依赖结构，但研究表明这些属性实际上是并发的无序集合。

Method: Amadeus结合自回归模型和双向离散扩散模型，并通过MLSDES和CIEM增强表示能力。

Result: Amadeus在多项指标上优于现有模型，速度提升4倍，支持细粒度控制。

Conclusion: Amadeus框架有效提升音乐生成性能，并展示了数据的重要性。

Abstract: Existing state-of-the-art symbolic music generation models predominantly
adopt autoregressive or hierarchical autoregressive architectures, modelling
symbolic music as a sequence of attribute tokens with unidirectional temporal
dependencies, under the assumption of a fixed, strict dependency structure
among these attributes. However, we observe that using different attributes as
the initial token in these models leads to comparable performance. This
suggests that the attributes of a musical note are, in essence, a concurrent
and unordered set, rather than a temporally dependent sequence. Based on this
insight, we introduce Amadeus, a novel symbolic music generation framework.
Amadeus adopts a two-level architecture: an autoregressive model for note
sequences and a bidirectional discrete diffusion model for attributes. To
enhance performance, we propose Music Latent Space Discriminability Enhancement
Strategy(MLSDES), incorporating contrastive learning constraints that amplify
discriminability of intermediate music representations. The Conditional
Information Enhancement Module (CIEM) simultaneously strengthens note latent
vector representation via attention mechanisms, enabling more precise note
decoding. We conduct extensive experiments on unconditional and
text-conditioned generation tasks. Amadeus significantly outperforms SOTA
models across multiple metrics while achieving at least 4$\times$ speed-up.
Furthermore, we demonstrate training-free, fine-grained note attribute control
feasibility using our model. To explore the upper performance bound of the
Amadeus architecture, we compile the largest open-source symbolic music dataset
to date, AMD (Amadeus MIDI Dataset), supporting both pre-training and
fine-tuning.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [67] [High performance visualization for Astronomy and Cosmology: the VisIVO's pathway toward Exascale systems](https://arxiv.org/abs/2508.20603)
*Eva Sciacca,Nicola Tuccari,Fabio Vitello,Valentina Cesare*

Main category: astro-ph.IM

TL;DR: VisIVO工具用于天文多维多变量数据分析，计划通过容器化和虚拟化技术提升其性能，支持高性能计算和数据可视化，目标是提高可移植性、可重复性和IO效率。


<details>
  <summary>Details</summary>
Motivation: 天文数据量巨大，传统工具难以应对，VisIVO旨在解决这一挑战，并扩展到高性能计算领域。

Method: 利用容器化和虚拟化技术，优化VisIVO，支持分布式计算和HPC设施。

Result: 已在EOSC等平台验证，未来将支持Exascale系统。

Conclusion: VisIVO的优化将提升天文数据分析和可视化的效率与灵活性。

Abstract: Petabyte-scale data volumes are generated by observations and simulations in
modern astronomy and astrophysics. Storage, access, and data analysis are
significantly hampered by such data volumes and are leading to the development
of a new generation of software tools. The Visualization Interface for the
Virtual Observatory (VisIVO) has been designed, developed and maintained by
INAF since 2005 to perform multi-dimensional data analysis and knowledge
discovery in multivariate astrophysical datasets. Utilizing containerization
and virtualization technologies, VisIVO has already been used to exploit
distributed computing infrastructures including the European Open Science Cloud
(EOSC).
  We intend to adapt VisIVO solutions for high performance visualization of
data generated on the (pre-)Exascale systems by HPC applications in
Astrophysics and Cosmology (A\&C), including GADGET (GAlaxies with Dark matter
and Gas) and PLUTO simulations, thanks to the collaboration within the SPACE
Center of Excellence, the H2020 EUPEX Project, and the ICSC National Research
Centre. In this work, we outline the evolution's course as well as the
execution strategies designed to achieve the following goals: enhance the
portability of the VisIVO modular applications and their resource requirements;
foster reproducibility and maintainability; take advantage of a more flexible
resource exploitation over heterogeneous HPC facilities; and, finally, minimize
data-movement overheads and improve I/O performances.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [68] [The Mathematician's Assistant: Integrating AI into Research Practice](https://arxiv.org/abs/2508.20236)
*Jonas Henkel*

Main category: math.HO

TL;DR: 论文探讨了AI（如大型语言模型）在数学研究中的应用现状与潜力，指出其虽在解题和验证方面表现优异，但仍存在系统性缺陷，并提出了一套基于'增强数学家'原则的AI整合框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展（如'AlphaEvolve'和'Gemini Deep Think'），其在数学研究中的潜在影响日益显著。然而，当前AI模型在数学应用中的局限性尚未被系统研究，因此本文旨在填补这一空白。

Method: 通过分析截至2025年8月的公开大型语言模型和数学基准测试（如MathArena、Open Proof Corpus），作者识别了AI的优势与缺陷，并提出了一套包含五个指导原则的整合框架。

Result: 研究表明，AI在数学研究中的主要角色是'增强'而非'自动化'，但其应用需要人类研究者的批判性指导和新的技能（如策略性提示和验证）。

Conclusion: AI在数学研究中的有效应用依赖于'增强数学家'模式，强调人类与AI的协作，并需培养新的技能以确保其负责任的使用。

Abstract: The rapid development of artificial intelligence (AI), marked by
breakthroughs like 'AlphaEvolve' and 'Gemini Deep Think', is beginning to offer
powerful new tools that have the potential to significantly alter the research
practice in many areas of mathematics. This paper explores the current
landscape of publicly accessible large language models (LLMs) in a mathematical
research context, based on developments up to August 2, 2025. Our analysis of
recent benchmarks, such as MathArena and the Open Proof Corpus (Balunovi\'c et
al., 2025; Dekoninck et al., 2025), reveals a complex duality: while
state-of-the-art models demonstrate strong abilities in solving problems and
evaluating proofs, they also exhibit systematic flaws, including a lack of
self-critique and a model depending discrepancy between final-answer accuracy
and full-proof validity.
  Based on these findings, we propose a durable framework for integrating AI
into the research workflow, centered on the principle of the augmented
mathematician. In this model, the AI functions as a copilot under the critical
guidance of the human researcher, an approach distilled into five guiding
principles for effective and responsible use. We then systematically explore
seven fundamental ways AI can be applied across the research lifecycle, from
creativity and ideation to the final writing process, demonstrating how these
principles translate into concrete practice.
  We conclude that the primary role of AI is currently augmentation rather than
automation. This requires a new skill set focused on strategic prompting,
critical verification, and methodological rigor in order to effectively use
these powerful tools.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [69] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: ProactiveEval是一个统一的框架，用于评估大语言模型（LLMs）的主动对话能力，通过目标规划和对话指导分解任务，并在多领域生成评估数据。


<details>
  <summary>Details</summary>
Motivation: 现有的主动对话研究主要集中在特定领域，导致评估零散且限制了对模型能力的全面探索。

Method: 提出ProactiveEval框架，分解主动对话任务为子任务，并设计评估指标和自动生成多样化的评估数据。

Result: 在6个领域构建了328个评估环境，测试了22种LLMs，DeepSeek-R1和Claude-3.7-Sonnet分别在目标规划和对话指导任务中表现优异。

Conclusion: 框架为主动对话能力评估提供了统一标准，同时探讨了推理能力对主动行为的影响，为未来模型开发提供启示。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: P2C框架通过因果建模生成可行的反事实行动序列，提升决策透明度和可操作性。


<details>
  <summary>Details</summary>
Motivation: 在高风险决策中，机器学习的应用需要透明度和可操作性，现有反事实解释方法忽略了因果依赖和顺序行动的实际限制。

Method: P2C利用因果建模和s(CASP)系统生成有序行动序列，确保每个中间状态在因果上可行。

Result: P2C优于标准方法，避免了非法行动，提供了更真实的成本估计。

Conclusion: P2C为高透明度决策提供了实用的反事实解决方案，弥补了现有方法的不足。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [71] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 该论文提出了一种可微分的神经符号架构和专用损失函数，用于学习解决NP难推理问题，并在多个任务中表现出高效和准确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了解决大型语言模型在处理离散推理或优化问题时的困难，提出一种能够从自然输入中学习解决这些问题的新型神经架构。

Method: 方法包括引入一种新的概率损失函数，能够学习约束和目标函数，从而提供一个可审查和补充的完整模型。通过将组合求解器从训练循环中移除，实现了可扩展的训练和准确的推理能力。

Result: 实证结果表明，该方法能够高效地从自然输入中学习解决NP难推理问题，在多个基准测试（如Sudoku和Min-Cut/Max-Cut任务）以及蛋白质设计等实际问题上表现优于其他混合方法。

Conclusion: 该论文提出的神经符号架构在解决复杂推理问题上表现出色，具有高效、准确和可扩展的优势，适用于多种实际应用场景。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [72] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: 提出量子编程自动化系统QAgent，通过多智能体提升OpenQASM编程准确性。


<details>
  <summary>Details</summary>
Motivation: 解决非专家在NISQ设备上编程的复杂性，利用LLM自动化量子编程流程。

Method: 结合任务规划、上下文学习、RAG、预定义工具和CoT推理的多智能体系统设计。

Result: QAgent将QASM代码生成准确性提升71.6%，优于现有静态LLM方法。

Conclusion: QAgent有望推动量子编程普及，加速量子计算的实用化。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [73] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 论文提出了一种名为个人健康代理（PHA）的多代理框架，通过三个专业子代理满足用户日常健康需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展推动了新一代健康代理的开发，但其在非临床日常场景中的应用尚未充分探索。

Method: 研究通过分析网络搜索和健康论坛查询，结合用户和专家意见，构建了支持三种健康需求的子代理，并开发了PHA框架进行动态个性化交互。

Result: PHA在10项基准任务中进行了自动化和人工评估，涉及7,000多个标注和1,100小时专家与用户参与，建立了全面评估基础。

Conclusion: PHA为个人健康代理的未来愿景奠定了坚实基础。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [74] [Spatio-Temporal Pruning for Compressed Spiking Large Language Models](https://arxiv.org/abs/2508.20122)
*Yi Jiang,Malyaban Bal,Brian Matejek,Susmit Jha,Adam Cobb,Abhronil Sengupta*

Main category: cs.NE

TL;DR: 提出一种结合时空剪枝的压缩脉冲神经网络大语言模型（Spiking LLMs）框架，以降低计算复杂度和推理延迟，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在能源受限环境中部署时面临模型大、推理延迟高的问题，而脉冲神经网络（SNNs）因其稀疏事件驱动和高效能特性，成为低功耗计算的潜在解决方案。

Method: 提出了时空剪枝框架：空间剪枝减少活跃神经元和注意力头数量；时间剪枝动态调整不同层的时间步数。结合极端量化和知识蒸馏技术，优化计算效率。

Result: 在GLUE基准测试中，提出的SpikingBERT框架在计算操作和推理延迟方面表现出显著优势。

Conclusion: 该框架为实时、低功耗自然语言处理应用提供了可行方案，推动Spiking LLMs在边缘设备中的实际部署。

Abstract: Large Language Models (LLMs) present significant challenges for deployment in
energy-constrained environments due to their large model sizes and high
inference latency. Spiking Neural Networks (SNNs), inspired by the sparse
event-driven neural processing and energy-efficient information transmission in
the brain, offer a promising alternative for achieving low-power computing.
Integrating the event-driven efficiency of spiking neurons with the advanced
capabilities of LLMs represents a promising direction for power-efficient LLMs.
This work specifically delves into the design of compressed spiking LLMs. Here,
we revisit spatial and temporal pruning from the perspective of SNNs and
propose a novel spatio-temporal pruning framework for Spiking LLMs to optimize
computational efficiency while preserving high performance. Our spatial pruning
technique reduces the number of active neurons and attention heads, effectively
lowering the computational complexity of the model. Meanwhile, temporal pruning
minimizes inference latency by dynamically adjusting the number of timesteps
required for different layers. By combining these approaches with other
compression techniques, we present the first work in the domain of Spiking LLMs
to jointly explore spatial pruning, temporal pruning, extreme quantization and
knowledge distillation strategies. Extensive experimental evaluation of our
proposed framework for SpikingBERT on the large-scale GLUE benchmark
demonstrates the efficacy of our approach in terms of computational operations
and inference latency. Our approach offers a compelling solution for real-time,
low-power natural language processing applications, making Spiking LLMs more
practical for deployment on edge devices and in power-constrained settings.

</details>


### [75] [Encoding Tactile Stimuli for Organoid Intelligence in Braille Recognition](https://arxiv.org/abs/2508.20850)
*Tianyi Liu,Hemma Philamore,Benjamin Ward-Cherrier*

Main category: cs.NE

TL;DR: 提出了一种将触觉数据映射到电刺激模式的通用编码策略，利用神经类器官实现开放式人工触觉盲文分类任务。


<details>
  <summary>Details</summary>
Motivation: 探索利用神经类器官作为低功耗、适应性强的生物混合计算元件，为未来可扩展的生物混合计算架构奠定基础。

Method: 使用低密度微电极阵列培养的人类额叶类器官，通过系统刺激研究其响应特性，并基于事件触觉输入实现分类任务。

Result: 单个类器官的平均盲文分类准确率为61%，三个类器官组合后提升至83%，且对噪声具有更强的鲁棒性。

Conclusion: 研究展示了类器官在生物混合计算中的潜力，并为未来架构提供了编码框架。

Abstract: This study proposes a generalizable encoding strategy that maps tactile
sensor data to electrical stimulation patterns, enabling neural organoids to
perform an open-loop artificial tactile Braille classification task. Human
forebrain organoids cultured on a low-density microelectrode array (MEA) are
systematically stimulated to characterize the relationship between electrical
stimulation parameters (number of pulse, phase amplitude, phase duration, and
trigger delay) and organoid responses, measured as spike activity and spatial
displacement of the center of activity. Implemented on event-based tactile
inputs recorded from the Evetac sensor, our system achieved an average Braille
letter classification accuracy of 61 percent with a single organoid, which
increased significantly to 83 percent when responses from a three-organoid
ensemble were combined. Additionally, the multi-organoid configuration
demonstrated enhanced robustness against various types of artificially
introduced noise. This research demonstrates the potential of organoids as
low-power, adaptive bio-hybrid computational elements and provides a
foundational encoding framework for future scalable bio-hybrid computing
architectures.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [76] [Lattice Random Walk Discretisations of Stochastic Differential Equations](https://arxiv.org/abs/2508.20883)
*Samuel Duffield,Maxwell Aifer,Denis Melanson,Zach Belateche,Patrick J. Coles*

Main category: math.NA

TL;DR: 提出了一种基于格点随机游走的SDE离散化方案，通过二进制或三元增量简化计算。


<details>
  <summary>Details</summary>
Motivation: 传统浮点离散化计算复杂，希望找到更高效且兼容随机计算架构的方法。

Method: 使用1或2位随机值代替复杂漂移和扩散计算，避免高斯采样和浮点运算。

Result: 实验证明该方法对非Lipschitz漂移具有鲁棒性，并在多种SDE中表现优异。

Conclusion: 该方法简化了SDE计算，具有实用性和兼容性优势。

Abstract: We introduce a lattice random walk discretisation scheme for stochastic
differential equations (SDEs) that samples binary or ternary increments at each
step, suppressing complex drift and diffusion computations to simple 1 or 2 bit
random values. This approach is a significant departure from traditional
floating point discretisations and offers several advantages; including
compatibility with stochastic computing architectures that avoid floating-point
arithmetic in place of directly manipulating the underlying probability
distribution of a bitstream, elimination of Gaussian sampling requirements,
robustness to quantisation errors, and handling of non-Lipschitz drifts. We
prove weak convergence and demonstrate the advantages through experiments on
various SDEs, including state-of-the-art diffusion models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [77] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 提出了一种基于图结构学习（GSL）的安全框架，用于增强能源互联网（IoE）对抗网络攻击的韧性。


<details>
  <summary>Details</summary>
Motivation: 能源互联网的互联性使其面临严重的网络安全威胁，需要比传统方法更具韧性的解决方案。

Method: 采用图结构学习框架，联合优化图拓扑和节点表示，以抵抗对抗性网络模型操纵。

Result: 实验证明，GSL在安全数据集上比代表性方法具有更强的鲁棒性。

Conclusion: GSL为能源互联网的安全性提供了可行方案，并指出了未来研究的开放挑战和方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [78] [FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture](https://arxiv.org/abs/2508.20212)
*Minghao Hu,Junzhe Wang,Weisen Zhao,Qiang Zeng,Lannan Luo*

Main category: cs.CR

TL;DR: 论文提出了一种利用神经机器翻译和归一化流技术的方法，减少多指令集架构（ISA）下恶意软件检测的数据收集工作。


<details>
  <summary>Details</summary>
Motivation: 随着针对物联网设备的网络攻击增加，需要扩展恶意软件检测能力到多种ISA，但数据收集和标注工作繁重。

Method: 通过将恶意软件从一种ISA翻译到另一种（如X86-64），利用已有模型实现跨ISA检测。

Result: 该方法减少了数据收集的工作量，实现了基于单一ISA训练的模型对多ISA恶意软件的检测。

Conclusion: 研究为跨ISA恶意软件检测提供了一种高效的数据利用方法。

Abstract: Applying deep learning to malware detection has drawn great attention due to
its notable performance. With the increasing prevalence of cyberattacks
targeting IoT devices, there is a parallel rise in the development of malware
across various Instruction Set Architectures (ISAs). It is thus important to
extend malware detection capacity to multiple ISAs. However, training a deep
learning-based malware detection model usually requires a large number of
labeled malware samples. The process of collecting and labeling sufficient
malware samples to build datasets for each ISA is labor-intensive and
time-consuming. To reduce the burden of data collection, we propose to leverage
the ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for
malware detection. Specifically, when dealing with malware in a certain ISA, we
translate it to an ISA with sufficient malware samples (like X86-64). This
allows us to apply a model trained on one ISA to analyze malware from another
ISA. Our approach reduces the data collection effort by enabling malware
detection across multiple ISAs using a model trained on a single ISA.

</details>


### [79] [Characterizing Trust Boundary Vulnerabilities in TEE Containers](https://arxiv.org/abs/2508.20962)
*Weijie Liu,Hongbo Chen,Shuo Huai,Zhen Xu,Wenhao Wang,Zhi Li,Zheli Liu*

Main category: cs.CR

TL;DR: 分析现有TEE容器的隔离策略及其潜在安全问题，并开发自动化工具评估其隔离性。


<details>
  <summary>Details</summary>
Motivation: TEE容器作为中间件解决方案，旨在保护应用免受恶意系统的侵害，但现有方案存在设计缺陷和安全风险。

Method: 设计自动化分析工具，精准识别和评估TEE容器的隔离边界。

Result: 发现部分TEE容器因设计和实现缺陷（如信息泄漏、回滚攻击等）未能达到预期目标。

Conclusion: 提出关键教训以指导更安全的容器开发，并讨论TEE容器设计的未来趋势。

Abstract: Trusted Execution Environments (TEEs) have emerged as a cornerstone of
confidential computing, garnering significant attention from both academia and
industry. To enable the secure development, execution, and deployment, of
applications on TEE platforms, TEE containers have been introduced as
middleware solutions. These containers aim to shield applications from
potentially malicious operating systems and orchestration interfaces while
maintaining usability and reliability. In this paper, we analyze the isolation
strategies employed by existing TEE containers to protect secure applications.
To address the challenges in analyzing these interfaces, we designed an
automated analyzer to precisely identify and evaluate their isolation
boundaries. We observed that some TEE containers fail to achieve their intended
goals due to critical design and implementation flaws, such as information
leakage, rollback attacks, denial-of-service, and Iago attacks, which pose
significant security risks. Drawing from our findings, we share key lessons to
guide the development of more secure container solutions and discuss emerging
trends in TEE containerization design.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [80] [Formal equivalence between global optimization consistency and random search](https://arxiv.org/abs/2508.20671)
*Gaëtan Serré*

Main category: cs.FL

TL;DR: 论文证明，任何随机迭代全局优化算法在Lipschitz连续函数上一致收敛当且仅当它采样整个搜索空间，并利用L∃∀N定理证明器和Mathlib库完成形式化验证。


<details>
  <summary>Details</summary>
Motivation: 研究随机迭代全局优化算法的一致收敛性条件，并通过形式化方法验证其理论。

Method: 定义算法为初始概率测度和马尔可夫核序列的组合，利用Ionescu-Tulcea定理构造概率测度。

Result: 证明算法一致收敛的充要条件是采样整个搜索空间。

Conclusion: 形式化方法验证了算法的收敛性条件，为相关研究提供了理论基础。

Abstract: We formalize a proof that any stochastic and iterative global optimization
algorithm is consistent over Lipschitz continuous functions if and only if it
samples the whole search space. To achieve this, we use the
L$\exists$$\forall$N theorem prover and the Mathlib library. The major
challenge of this formalization, apart from the technical aspects of the proof
itself, is to converge to a definition of a stochastic and iterative global
optimization algorithm that is both general enough to encompass all algorithms
of this type and specific enough to be used in a formal proof. We define such
an algorithm as a pair of an initial probability measure and a sequence of
Markov kernels that describe the distribution of the next point sampled by the
algorithm given the previous points and their evaluations. We then construct a
probability measure on finite and infinite sequences of iterations of the
algorithm using the Ionescu-Tulcea theorem.

</details>
