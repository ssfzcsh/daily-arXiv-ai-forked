<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 11]
- [cs.HC](#cs.HC) [Total: 23]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CL](#cs.CL) [Total: 5]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Exploring the Landscape of Fairness Interventions in Software Engineering](https://arxiv.org/abs/2507.18726)
*Sadia Afrin Mim*

Main category: cs.SE

TL;DR: 论文综述了AI应用中因数据偏见导致的公平性问题及现有解决方案。


<details>
  <summary>Details</summary>
Motivation: AI在医疗和金融等领域的广泛应用因数据偏见等问题带来风险，需解决公平性挑战。

Method: 作为综述，总结了针对公平性问题的多种研究和干预方法。

Result: 综述揭示了现有方法的多样性和局限性。

Conclusion: 总结现有工作，强调需进一步研究以完善AI公平性解决方案。

Abstract: Current developments in AI made it broadly significant for reducing human
labor and expenses across several essential domains, including healthcare and
finance. However, the application of AI in the actual world poses multiple
risks and disadvantages due to potential risk factors in data (e.g., biased
dataset). Practitioners developed a number of fairness interventions for
addressing these kinds of problems. The paper acts as a survey, summarizing the
various studies and approaches that have been developed to address fairness
issues

</details>


### [2] [Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback](https://arxiv.org/abs/2507.18755)
*Chandra Maddila,Adam Tait,Claire Chang,Daniel Cheng,Nauman Ahmad,Vijayaraghavan Murali,Marshall Roch,Arnaud Avondet,Aaron Meltzer,Victor Montalvao,Michael Hopko,Chris Waterson,Parth Thakkar,Renuka Fernandez,Kristian Kristensen,Sivan Barzily,Sherry Chen,Rui Abreu,Nachiappan Nagappan,Payam Shodjai,Killian Murphy,James Everingham,Aparna Ramani,Peter C. Rigby*

Main category: cs.SE

TL;DR: 开发一个基于LLM的工程代理，用于大规模修复代码，结合ReAct框架和LLM评审，离线测试显示70B模型表现优异，生产环境中31.5%的修复被采纳。


<details>
  <summary>Details</summary>
Motivation: 利用LLM实现大规模代码修复，提高开发效率，减少人工介入。

Method: 使用Llama作为基础，结合ReAct框架，通过静态分析和测试反馈优化代理行为，用LLM评审确保修复质量。

Result: 离线测试中解决率达42.3%，生产环境中25.5%的修复被采纳，工程师反馈总体积极。

Conclusion: 工程代理在大规模代码修复中表现良好，未来可进一步优化成本和延迟问题。

Abstract: Aim: With the advent of LLMs, sophisticated agentic program repair has become
viable at large organizations with large codebases. In this work, we develop an
Engineering Agent that fixes the source code based on test failures at scale
across diverse software offerings internally.
  Method: Using Llama as the base, we employ the ReAct harness to develop an
agent. We start with a test failure that was triaged by a rule-based test
failure bot. We then set up an agentic harness and allow the agent to reason
and run a set of 15 actions from reading a file to generating a patch. We
provide feedback to the agent through static analysis and test failures so it
can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch
conforms to the standards followed by a human review to land fixes.
  Benchmark Findings: We curated offline benchmarks for our patch generator,
the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we
found that a specialized 70B model is highly competitive with the much larger
but vanilla Llama-405B. In an ablation study, we found that the ReAct harness
(neural model) benefited from the symbolic information from static analysis
tools and test execution traces. A model that strikes a balance between the
solve rate and error rate vs the cost and latency has a benchmark solve rate of
42.3% using an average 11.8 feedback iterations.
  Production Findings: In a three month period, 80% of the generated fixes were
reviewed, of which 31.5% were landed (25.5% of the total number of generated
fixes).
  Feedback from Engineers: We used open coding to extract qualitative themes
from engineers' feedback. We saw positive feedback in the form of quick
approvals, gratitude, and surprise. We also found mixed feedback when the
Engineering Agent's solution was partially correct and it served as a good
starting point.

</details>


### [3] [MemoCoder: Automated Function Synthesis using LLM-Supported Agents](https://arxiv.org/abs/2507.18812)
*Yiping Jia,Zhen Ming Jiang,Shayan Noei,Ying Zou*

Main category: cs.SE

TL;DR: 论文提出了一种名为MemoCoder的多代理框架，旨在解决大型语言模型在代码生成中迭代调试和错误处理的不足，通过持久学习和协作问题解决提高效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在需要迭代调试或错误处理的任务中表现不佳，现有方法如微调或自我修复策略成本高或缺乏知识积累机制，急需一种新方法来解决这些问题。

Method: MemoCoder采用多代理框架，包括一个修复知识集(Memory Set)和一个中央导师代理(Mentor Agent)，前者存储成功的修复案例，后者识别错误模式并指导修复过程。

Result: 实验在三个公共基准测试（MBPP、HumanEval和LiveCodeBench）上进行，MemoCoder在Pass@10和Pass@50指标上分别优于零样本提示和自我修复策略。

Conclusion: MemoCoder在知识引导的代码生成和迭代优化方面表现出显著优势，为AI辅助编程工具提供了更高效的方法。

Abstract: With the widespread adoption of Large Language Models (LLMs) such as GitHub
Copilot and ChatGPT, developers increasingly rely on AI-assisted tools to
support code generation. While LLMs can generate syntactically correct
solutions for well-structured programming tasks, they often struggle with
challenges that require iterative debugging, error handling, or adaptation to
diverse problem structures. Existing approaches such as fine-tuning or
self-repair strategies either require costly retraining or lack mechanisms to
accumulate and reuse knowledge from previous attempts.
  To address these limitations, we propose MemoCoder, a multi-agent framework
that enables collaborative problem solving and persistent learning from past
fixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores
successful repairs and supports retrieval for future tasks. A central Mentor
Agent supervises the repair process by identifying recurring error patterns and
refining high-level fixing strategies, providing a novel supervisory role that
guides the self-repair loop. We evaluate MemoCoder across three public
benchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem
complexities. Experimental results show that MemoCoder consistently outperforms
both zero-shot prompting and a Self-Repair strategy, with improvements ranging
from 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating
its effectiveness in iterative refinement and knowledge-guided code generation.

</details>


### [4] [Exploring the Jupyter Ecosystem: An Empirical Study of Bugs and Vulnerabilities](https://arxiv.org/abs/2507.18833)
*Wenyuan Jiang,Diany Pressato,Harsh Darji,Thibaud Lutellier*

Main category: cs.SE

TL;DR: 该论文对Jupyter笔记本中的错误和漏洞进行了大规模实证研究，发现配置问题和API使用不当是常见错误，并分析了部署框架的安全风险。


<details>
  <summary>Details</summary>
Motivation: 由于Jupyter笔记本的特性（如配置脚本、Markdown等）使其难以分析，传统软件工程模型无法捕捉其独特性，因此需要专门研究其错误和漏洞。

Method: 通过收集和分析两大平台的笔记本数据集，进行定量分析（如复杂性指标、贡献者活动等）和定性研究（基于扎根理论的错误分类），并评估部署框架的安全风险。

Result: 研究发现配置问题和API使用错误最常见，部署框架也存在安全风险。

Conclusion: 笔记本的支持不如传统软件完善，导致代码复杂、配置错误和维护不良问题频发。

Abstract: Background. Jupyter notebooks are one of the main tools used by data
scientists. Notebooks include features (configuration scripts, markdown,
images, etc.) that make them challenging to analyze compared to traditional
software. As a result, existing software engineering models, tools, and studies
do not capture the uniqueness of Notebook's behavior. Aims. This paper aims to
provide a large-scale empirical study of bugs and vulnerabilities in the
Notebook ecosystem. Method. We collected and analyzed a large dataset of
Notebooks from two major platforms. Our methodology involved quantitative
analyses of notebook characteristics (such as complexity metrics, contributor
activity, and documentation) to identify factors correlated with bugs.
Additionally, we conducted a qualitative study using grounded theory to
categorize notebook bugs, resulting in a comprehensive bug taxonomy. Finally,
we analyzed security-related commits and vulnerability reports to assess risks
associated with Notebook deployment frameworks. Results. Our findings highlight
that configuration issues are among the most common bugs in notebook documents,
followed by incorrect API usage. Finally, we explore common vulnerabilities
associated with popular deployment frameworks to better understand risks
associated with Notebook development. Conclusions. This work highlights that
notebooks are less well-supported than traditional software, resulting in more
complex code, misconfiguration, and poor maintenance.

</details>


### [5] [SLICEMATE: Accurate and Scalable Static Program Slicing via LLM-Powered Agents](https://arxiv.org/abs/2507.18957)
*Jianming Chang,Jieke Shi,Yunbo Lyu,Xin Zhou,Lulu Wang,Zhou Yang,Bixin Li,David Lo*

Main category: cs.SE

TL;DR: SliceMate 是一种基于大型语言模型（LLM）的新型静态程序切片工具，通过集成合成、验证和优化代理，显著提升了切片的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统静态程序切片工具依赖计算密集的依赖图分析，难以扩展到大程序且对语法不完整代码处理不佳；学习型方法虽更鲁棒，但在规范代码上表现不及传统方法。

Method: SliceMate 采用三个 LLM 代理：合成代理生成候选切片，验证代理检查切片的完整性和简洁性，优化代理修复不合格切片，控制模块协调代理输出高质量结果。

Result: 在 SliceBench 基准（2200 个手动标注的 Java/Python 程序）上，SliceMate 显著优于传统和学习型工具。

Conclusion: SliceMate 通过 LLM 代理机制，无需显式构建依赖图即可实现高精度切片，为程序分析提供了高效可靠的新方案。

Abstract: Static program slicing, which extracts the executable portions of a program
that affect the values at a specific location, supports many software analysis
tasks such as debugging and security auditing. However, traditional slicing
tools rely on computationally expensive reachability analysis over dependency
graphs, which struggle to scale to large programs and often fail to handle code
with incomplete syntax. Recently emerged learning-based methods, while more
robust to such cases, still fall short of achieving comparable performance to
traditional methods on well-formed code.
  In this work, we propose SliceMate, a novel static program slicing solution
powered by Large Language Model (LLM) agents. It bypasses the need for explicit
dependency graph construction and achieving superior slicing accuracy.
Concretely, SliceMate integrates three specialized agents: (1) a synthesis
agent that produces candidate slices by incrementally expanding the scan scope
across functions and files guided by LLM-inferred dependencies; (2) a
verification agent that performs conciseness and completeness checks of the
candidate slices, detecting missing or irrelevant statements; and (3) a
refinement agent that repairs the slices with minimal edits in accordance with
the verification results. These agents are orchestrated by a control module
that ensures timely convergence and outputs high-quality slices without manual
intervention. For rigorous evaluation, we construct a new and high-quality
benchmark, SliceBench, comprising 2,200 manually annotated Java and Python
programs, with program lengths ranging from 5 to 8,577 lines, significantly
larger than those in existing slicing benchmarks. Experimental results show
that SliceMate greatly outperforms both traditional and learning-based slicing
tools.

</details>


### [6] [Classifying Issues in Open-source GitHub Repositories](https://arxiv.org/abs/2507.18982)
*Amir Hossain Raaj,Fairuz Nawer Meem,Sadia Afrin Mim*

Main category: cs.SE

TL;DR: 该研究利用ML和DNN模型对GitHub开源社区中的issue进行分类，以提高问题解决效率。


<details>
  <summary>Details</summary>
Motivation: GitHub上大多数仓库的issue未被规范标记，影响开发效率。

Method: 分析知名GitHub仓库，使用ML和DNN模型对issue进行常见标签分类。

Result: DNN模型在分类任务中表现优于其他方法。

Conclusion: 自动化标签分类有助于加快开发流程。

Abstract: GitHub is the most widely used platform for software maintenance in the
open-source community. Developers report issues on GitHub from time to time
while facing difficulties. Having labels on those issues can help developers
easily address those issues with prior knowledge of labels. However, most of
the GitHub repositories do not maintain regular labeling for the issues. The
goal of this work is to classify issues in the open-source community using ML
\& DNN models. There are thousands of open-source repositories on GitHub. Some
of the repositories label their issues properly whereas some of them do not.
When issues are pre-labeled, the problem-solving process and the immediate
assignment of corresponding personnel are facilitated for the team, thereby
expediting the development process. In this work, we conducted an analysis of
prominent GitHub open-source repositories. We classified the issues in some
common labels which are: API, Documentation, Enhancement, Question, Easy,
Help-wanted, Dependency, CI, Waiting for OP's response, Test, Bug, etc. Our
study shows that DNN models outperf

</details>


### [7] [SESR-Eval: Dataset for Evaluating LLMs in the Title-Abstract Screening of Systematic Reviews](https://arxiv.org/abs/2507.19027)
*Aleksi Huotala,Miikka Kuutila,Mika Mäntylä*

Main category: cs.SE

TL;DR: 本文创建了一个基准数据集SESR-Eval，用于评估大型语言模型（LLMs）在软件工程系统综述标题-摘要筛选中的表现，结果显示LLMs的准确性在不同研究中差异较大，目前不推荐自动化使用。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在系统综述标题-摘要筛选中的性能，为软件工程领域提供是否使用LLMs的决策依据。

Method: 通过筛选169篇系统综述研究，选取24篇构建包含34,528个标记研究的SESR-Eval数据集，并测试9种LLMs的性能。

Result: 不同LLMs表现相似，但研究间的准确性差异显著；LLMs使用成本低（每篇低于$40）。

Conclusion: 目前不推荐自动化使用LLMs，未来计划研究影响LLMs性能的因素。

Abstract: Background: The use of large language models (LLMs) in the title-abstract
screening process of systematic reviews (SRs) has shown promising results, but
suffers from limited performance evaluation. Aims: Create a benchmark dataset
to evaluate the performance of LLMs in the title-abstract screening process of
SRs. Provide evidence whether using LLMs in title-abstract screening in
software engineering is advisable. Method: We start with 169 SR research
artifacts and find 24 of those to be suitable for inclusion in the dataset.
Using the dataset we benchmark title-abstract screening using 9 LLMs. Results:
We present the SESR-Eval (Software Engineering Systematic Review Evaluation)
dataset containing 34,528 labeled primary studies, sourced from 24 secondary
studies published in software engineering (SE) journals. Most LLMs performed
similarly and the differences in screening accuracy between secondary studies
are greater than differences between LLMs. The cost of using an LLM is
relatively low - less than $40 per secondary study even for the most expensive
model. Conclusions: Our benchmark enables monitoring AI performance in the
screening task of SRs in software engineering. At present, LLMs are not yet
recommended for automating the title-abstract screening process, since accuracy
varies widely across secondary studies, and no LLM managed a high recall with
reasonable precision. In future, we plan to investigate factors that influence
LLM screening performance between studies.

</details>


### [8] [Exploring the Use of LLMs for Requirements Specification in an IT Consulting Company](https://arxiv.org/abs/2507.19113)
*Liliana Pasquale,Azzurra Ragone,Emanuele Piemontese,Armin Amiri Darban*

Main category: cs.SE

TL;DR: 论文探讨了利用大型语言模型（LLM）自动化需求规范生成的过程，对比了LLM与人类分析师生成的文档质量，提倡LLM作为辅助工具与人类协同工作。


<details>
  <summary>Details</summary>
Motivation: 需求规范在工程实践中面临知识分散的问题，传统方法耗时耗力。

Method: 使用LLM基于需求文档和模板自动生成功能设计规范（FDS），并与人类分析师的结果进行对比。

Result: LLM能减少时间和人力，但生成质量依赖输入且需人工修订。

Conclusion: 建议LLM作为起草工具，结合人类分析师的监督以提高需求工程文档质量。

Abstract: In practice, requirements specification remains a critical challenge. The
knowledge necessary to generate a specification can often be fragmented across
diverse sources (e.g., meeting minutes, emails, and high-level product
descriptions), making the process cumbersome and time-consuming. In this paper,
we report our experience using large language models (LLMs) in an IT consulting
company to automate the requirements specification process. In this company,
requirements are specified using a Functional Design Specification (FDS), a
document that outlines the functional requirements and features of a system,
application, or process. We provide LLMs with a summary of the requirements
elicitation documents and FDS templates, prompting them to generate Epic FDS
(including high-level product descriptions) and user stories, which are
subsequently compiled into a complete FDS document. We compared the correctness
and quality of the FDS generated by three state-of-the-art LLMs against those
produced by human analysts. Our results show that LLMs can help automate and
standardize the requirements specification, reducing time and human effort.
However, the quality of LLM-generated FDS highly depends on inputs and often
requires human revision. Thus, we advocate for a synergistic approach in which
an LLM serves as an effective drafting tool while human analysts provide the
critical contextual and technical oversight necessary for high-quality
requirements engineering (RE) documentation.

</details>


### [9] [Automated Code Review Using Large Language Models at Ericsson: An Experience Report](https://arxiv.org/abs/2507.19115)
*Shweta Ramesh,Joy Bose,Hamender Singh,A K Raghavan,Sujoy Roychowdhury,Giriprasad Sridhara,Nishrith Saini,Ricardo Britto*

Main category: cs.SE

TL;DR: 该论文探讨了利用大型语言模型(LLM)来自动化代码审查过程，以减轻资深开发者的认知负担，并介绍了在爱立信公司开发的一个结合LLM和静态程序分析的工具的初步实验结果。


<details>
  <summary>Details</summary>
Motivation: 资深开发者缺乏时间进行深入的代码审查，自动化代码审查可以减轻他们的负担，使其专注于编写新功能和修复漏洞的主要任务。

Method: 开发了一个轻量级工具，结合大型语言模型(LLMs)和静态程序分析技术，用于代码审查。

Result: 与资深开发者合作的初步实验结果显示，该工具的效果令人鼓舞。

Conclusion: 自动化代码审查工具在减轻开发者负担和提高效率方面具有潜力，但仍需进一步研究和优化。

Abstract: Code review is one of the primary means of assuring the quality of released
software along with testing and static analysis. However, code review requires
experienced developers who may not always have the time to perform an in-depth
review of code. Thus, automating code review can help alleviate the cognitive
burden on experienced software developers allowing them to focus on their
primary activities of writing code to add new features and fix bugs. In this
paper, we describe our experience in using Large Language Models towards
automating the code review process in Ericsson. We describe the development of
a lightweight tool using LLMs and static program analysis. We then describe our
preliminary experiments with experienced developers in evaluating our code
review tool and the encouraging results.

</details>


### [10] [Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects](https://arxiv.org/abs/2507.19271)
*Igli Begolli,Meltem Aksoy,Daniel Neider*

Main category: cs.SE

TL;DR: 研究比较了单语言微调对开源语言模型在自动代码审查任务中的性能影响，发现单语言微调能提升模型准确性和相关性，但在处理语义复杂或上下文敏感的变更时仍需人工审查。


<details>
  <summary>Details</summary>
Motivation: 代码审查对软件质量至关重要，但费时费力。利用语言模型自动化部分审查任务有望提高效率。

Method: 研究对CodeReviewer、CodeLlama-7B和DeepSeek-R1-Distill三种模型进行了单语言微调，并在C#数据集上评估其在代码变更质量评估、审查评论生成和代码改进任务中的表现。

Result: 单语言微调提高了模型的准确性和相关性，但人工审查在处理复杂变更时仍优于语言模型。

Conclusion: 语言对齐和任务特定适应对优化语言模型在自动代码审查中的表现至关重要。

Abstract: Code review is essential for maintaining software quality but often
time-consuming and cognitively demanding, especially in industrial
environments. Recent advancements in language models (LMs) have opened new
avenues for automating core review tasks. This study presents the empirical
evaluation of monolingual fine-tuning on the performance of open-source LMs
across three key automated code review tasks: Code Change Quality Estimation,
Review Comment Generation, and Code Refinement. We fine-tuned three distinct
models, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\# specific
dataset combining public benchmarks with industrial repositories. Our study
investigates how different configurations of programming languages and natural
languages in the training data affect LM performance, particularly in comment
generation. Additionally, we benchmark the fine-tuned models against an
automated software analysis tool (ASAT) and human reviewers to evaluate their
practical utility in real-world settings. Our results show that monolingual
fine-tuning improves model accuracy and relevance compared to multilingual
baselines. While LMs can effectively support code review workflows, especially
for routine or repetitive tasks, human reviewers remain superior in handling
semantically complex or context-sensitive changes. Our findings highlight the
importance of language alignment and task-specific adaptation in optimizing LMs
for automated code review.

</details>


### [11] [Mut4All: Fuzzing Compilers via LLM-Synthesized Mutators Learned from Bug Reports](https://arxiv.org/abs/2507.19275)
*Bo Wang,Pengyang Wang,Chong Chen,Qi Sun,Jieke Shi,Chengran Yang,Ming Deng,Youfang Lin,Zhou Yang,David Lo*

Main category: cs.SE

TL;DR: Mut4All是一种基于LLM的全自动、语言无关的突变生成框架，通过处理编译器相关的错误报告自动合成高质量突变器，显著提升了编译器漏洞检测的效果。


<details>
  <summary>Details</summary>
Motivation: 现代语言（如C++、Rust）的复杂结构使得突变器设计变得困难，现有方法依赖人工，缺乏可扩展性和跨语言通用性，因此需要一种自动化解决方案。

Method: Mut4All由三个代理组成：突变器发明代理（识别目标和生成元数据）、突变器实现合成代理（生成初始实现）、突变器优化代理（通过单元测试验证和修正）。

Result: 处理1000个错误报告后，生成319个Rust和403个C++突变器，检测出62个Rust编译器漏洞（38个新漏洞）和34个C++编译器漏洞（16个新漏洞）。

Conclusion: Mut4All在唯一崩溃检测和覆盖率上优于现有方法，展示了LLM在编译器漏洞检测中的潜力。

Abstract: Mutation-based fuzzing is effective for uncovering compiler bugs, but
designing high-quality mutators for modern languages with complex constructs
(e.g., templates, macros) remains challenging. Existing methods rely heavily on
manual design or human-in-the-loop correction, limiting scalability and
cross-language generalizability.
  We present Mut4All, a fully automated, language-agnostic framework that
synthesizes mutators using Large Language Models (LLMs) and compiler-specific
knowledge from bug reports. It consists of three agents: (1) a mutator
invention agent that identifies mutation targets and generates mutator metadata
using compiler-related insights; (2) a mutator implementation synthesis agent,
fine-tuned to produce initial implementations; and (3) a mutator refinement
agent that verifies and corrects the mutators via unit-test feedback.
  Mut4All processes 1000 bug reports (500 Rust, 500 C++), yielding 319 Rust and
403 C++ mutators at ~$0.08 each via GPT-4o. Our customized fuzzer, using these
mutators, finds 62 bugs in Rust compilers (38 new, 7 fixed) and 34 bugs in C++
compilers (16 new, 1 fixed). Mut4All outperforms existing methods in both
unique crash detection and coverage, ranking first on Rust and second on C++.

</details>


### [12] [ReCatcher: Towards LLMs Regression Testing for Code Generation](https://arxiv.org/abs/2507.19390)
*Altaf Allah Abbassi,Leuson Da Silva,Amin Nikanjam,Foutse Khomh*

Main category: cs.SE

TL;DR: ReCatcher是一个用于Python代码生成的回归测试框架，系统地比较两种LLM在逻辑正确性、静态代码质量和执行性能三个维度上的差异，发现模型更新可能引发回归问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的快速更新（如微调、合并或新发布），代码生成的质量和性能可能退步，需要一种系统的方法来检测这些回归。

Method: 提出ReCatcher框架，通过比较两种LLM在逻辑正确性、静态代码质量和执行性能三个维度上的表现，评估模型更新是否引入回归。评估了CodeLlama、DeepSeek-Coder和GPT-4o等模型。

Result: 发现微调、合并或新模型发布可能导致语法错误增加（高达12%）、正确性下降（高达18%），或性能退化（如GPT-4o-mini比GPT-4o慢80%）。ReCatcher在逻辑和性能方面优于基准方法。

Conclusion: ReCatcher强调在采用新模型前系统性评估回归问题的重要性，帮助研究者和开发者做出更明智的更新决策。

Abstract: Large Language Models (LLMs) for code generation evolve rapidly through
fine-tuning, merging, or new model releases. However, such updates can
introduce regressions, not only in correctness but also in code quality and
performance. To address this, we present ReCatcher, a regression testing
framework for Python code generation. ReCatcher systematically compares two
LLMs, typically a current model and a candidate update, across three
dimensions: logical correctness, static code quality, and execution
performance. We apply ReCatcher to assess regressions across three update
scenarios, fine-tuning, merging, and model release, using CodeLlama,
DeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with
cross-language datasets increases syntax errors by up to 12%. Merging with
general-purpose models like Llama2 leads to regressions in correctness by up to
18%. GPT-4o introduces regressions of up to 50% in handling missing imports
compared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance
degradation in execution time versus GPT-4o. Overall, logical correctness,
performance, and error handling (e.g., syntax errors and missing imports) are
the most regression-prone areas. Comparing ReCatcher with baseline solutions,
it presents better and consistent accuracy across logical and performance
aspects. ReCatcher highlights the importance of systematic regression
evaluation before adopting new models, while assisting researchers and
practitioners in making more informed update decisions.

</details>


### [13] [SDVDiag: A Modular Platform for the Diagnosis of Connected Vehicle Functions](https://arxiv.org/abs/2507.19403)
*Matthias Weiß,Falk Dettinger,Michael Weyrich*

Main category: cs.SE

TL;DR: 该论文提出了一个名为SDVDiag的可扩展平台，用于自动诊断联网车辆功能，通过动态图形视图和自动化流程快速定位故障根源。


<details>
  <summary>Details</summary>
Motivation: 联网车辆的高可靠性和可用性要求需要快速解决故障，但复杂的云/边缘架构使得手动分析变得不可行。

Method: SDVDiag平台支持模块化设计和运行时模块交换，构建动态系统图形视图，并通过异常监控和图形遍历分析故障原因。

Result: 在5G测试车队环境中部署后，平台能够可靠检测注入的故障，提前发现和解决问题。

Conclusion: SDVDiag平台能够显著减少故障排查时间，提升联网车辆的可靠性和可用性。

Abstract: Connected and software-defined vehicles promise to offer a broad range of
services and advanced functions to customers, aiming to increase passenger
comfort and support autonomous driving capabilities. Due to the high
reliability and availability requirements of connected vehicles, it is crucial
to resolve any occurring failures quickly. To achieve this however, a complex
cloud/edge architecture with a mesh of dependencies must be navigated to
diagnose the responsible root cause. As such, manual analyses become unfeasible
since they would significantly delay the troubleshooting.
  To address this challenge, this paper presents SDVDiag, an extensible
platform for the automated diagnosis of connected vehicle functions. The
platform enables the creation of pipelines that cover all steps from initial
data collection to the tracing of potential root causes. In addition, SDVDiag
supports self-adaptive behavior by the ability to exchange modules at runtime.
Dependencies between functions are detected and continuously updated, resulting
in a dynamic graph view of the system. In addition, vital system metrics are
monitored for anomalies. Whenever an incident is investigated, a snapshot of
the graph is taken and augmented by relevant anomalies. Finally, the analysis
is performed by traversing the graph and creating a ranking of the most likely
causes.
  To evaluate the platform, it is deployed inside an 5G test fleet environment
for connected vehicle functions. The results show that injected faults can be
detected reliably. As such, the platform offers the potential to gain new
insights and reduce downtime by identifying problems and their causes at an
early stage.

</details>


### [14] [An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles](https://arxiv.org/abs/2507.19446)
*Matthias Weiß,Anish Navalgund,Johannes Stümpfle,Falk Dettinger,Michael Weyrich*

Main category: cs.SE

TL;DR: 论文提出了一种开源CI/CD管道，专为软件定义车辆（SDV）设计，旨在通过自动化构建、测试和部署流程解决软件版本和变体管理的复杂性，并支持OTA更新和AI模型的持续开发。


<details>
  <summary>Details</summary>
Motivation: 软件定义车辆的快速发展带来了多样化的软件版本和变体，缺乏统一的集成环境，导致开发复杂化。因此，需要一个动态编排功能的解决方案，以确保异构系统间的可靠运行。

Method: 论文提出了一种开源的CI/CD管道，利用容器化开源工具自动化构建、测试和部署过程。还包括一个自定义的OTA中间件，用于分发软件更新和支持回滚，并根据部署目标依赖和硬件配置生成变体。此外，支持AI模型的持续开发和部署。

Result: 通过在自动代客泊车（AVP）场景中的评估，展示了无缝的OTA更新、正确的变体选择以及跨所有目标的成功编排。成功开发并部署了两种硬件特定的对象检测变体。

Conclusion: 提出的管道为管理SDV中的软件变体和OTA更新提供了可扩展且高效的解决方案，有助于推动未来移动技术的发展。

Abstract: Software-defined vehicles (SDVs) offer a wide range of connected
functionalities, including enhanced driving behavior and fleet management.
These features are continuously updated via over-the-air (OTA) mechanisms,
resulting in a growing number of software versions and variants due to the
diversity of vehicles, cloud/edge environments, and stakeholders involved. The
lack of a unified integration environment further complicates development, as
connected mobility solutions are often built in isolation. To ensure reliable
operations across heterogeneous systems, a dynamic orchestration of functions
that considers hardware and software variability is essential. This paper
presents an open-source CI/CD pipeline tailored for SDVs. It automates the
build, test, and deployment phases using a combination of containerized
open-source tools, creating a standardized, portable, and scalable ecosystem
accessible to all stakeholders. Additionally, a custom OTA middleware
distributes software updates and supports rollbacks across vehicles and backend
services. Update variants are derived based on deployment target dependencies
and hardware configurations. The pipeline also supports continuous development
and deployment of AI models for autonomous driving features. Its effectiveness
is evaluated using an automated valet parking (AVP) scenario involving
TurtleBots and a coordinating backend server. Two object detection variants are
developed and deployed to match hardware-specific requirements. Results
demonstrate seamless OTA updates, correct variant selection, and successful
orchestration across all targets. Overall, the proposed pipeline provides a
scalable and efficient solution for managing software variants and OTA updates
in SDVs, contributing to the advancement of future mobility technologies.

</details>


### [15] [Resolving Build Conflicts via Example-Based and Rule-Based Program Transformations](https://arxiv.org/abs/2507.19432)
*Sheikh Shadab Towqir,Fei He,Todd Mytkowicz,Na Meng*

Main category: cs.SE

TL;DR: BUCOR是一种解决构建冲突的新工具，结合了基于示例和基于规则的策略，能有效处理复杂冲突。


<details>
  <summary>Details</summary>
Motivation: 合并冲突会降低软件质量和开发效率，现有工具在解决某些冲突（如方法移除）方面支持有限。

Method: BUCOR通过比较三个版本（base、left、right）检测冲突，并采用基于示例（BUCOR-E）和基于规则（BUCOR-R）的策略解决冲突。

Result: 在88个真实构建冲突中，BUCOR为65个案例生成了至少一个解决方案，并正确解决了43个冲突。

Conclusion: 结合上下文感知的示例学习和结构化规则解决策略的混合方法有效，为未来更智能的合并工具提供了方向。

Abstract: Merge conflicts often arise when developers integrate changes from different
software branches. The conflicts can result from overlapping edits in programs
(i.e., textual conflicts) or cause build and test errors (i.e., build and test
conflicts). They degrade software quality and hinder programmer productivity.
While several tools detect build conflicts, few offer meaningful support for
resolving cases like those caused by method removal. To overcome limitations of
existing tools, we introduce BUCOR (Build Conflict Resolver), a new conflict
resolver. BUCOR first detects conflicts by comparing three versions related to
a merging scenario: base b, left l, and right r. To resolve conflicts, it
employs two complementary strategies: example-based transformation (BUCOR-E)
and rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to
handle common, well-understood conflicts. BUCOR-E mines branch versions (l and
r) for exemplar edits applied to fix related build errors. From these examples,
it infers and generalizes program transformation patterns to resolve more
complex conflicts.
  We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct
conflict types. BUCOR generated at least one solution for 65 cases and
correctly resolved 43 conflicts. We observed that this hybrid
approach--combining context-aware, example-based learning with structured,
rule-based resolution--can effectively help resolve conflicts. Our research
sheds light on future directions for more intelligent and automated merge
tools.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges](https://arxiv.org/abs/2507.18792)
*Zixu Zhou*

Main category: cs.PL

TL;DR: 由于Rust丰富的类型系统、编译器优化和高层抽象，反编译其二进制文件具有挑战性。研究通过基准测试评估了反编译质量，发现泛型、特质方法和错误处理构造显著降低质量，尤其是在发布版本中。案例分析了语言构造对控制流、变量命名和类型信息恢复的影响，为工具开发提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: Rust语言的特性使其二进制文件反编译困难，研究旨在评估反编译质量及其影响因素，以指导工具开发。

Method: 采用基准测试驱动的评估方法，自动化评分框架分析核心Rust特性和编译器构建模式，并结合案例研究具体语言构造的影响。

Result: 泛型、特质方法和错误处理构造显著降低反编译质量，发布版本的挑战更大。案例分析揭示了语言构造对反编译的多方面影响。

Conclusion: 研究为工具开发提供了实用建议，强调需要针对Rust特性的反编译策略。

Abstract: Decompiling Rust binaries is challenging due to the language's rich type
system, aggressive compiler optimizations, and widespread use of high-level
abstractions. In this work, we conduct a benchmark-driven evaluation of
decompilation quality across core Rust features and compiler build modes. Our
automated scoring framework shows that generic types, trait methods, and error
handling constructs significantly reduce decompilation quality, especially in
release builds. Through representative case studies, we analyze how specific
language constructs affect control flow, variable naming, and type information
recovery. Our findings provide actionable insights for tool developers and
highlight the need for Rust-aware decompilation strategies.

</details>


### [17] [IsaMini: Redesigned Isabelle Proof Lanugage for Machine Learning](https://arxiv.org/abs/2507.18885)
*Qiyuan Xu,Renxi Wang,Haonan Li,David Sanan,Conrad Watt*

Main category: cs.PL

TL;DR: 论文通过改进证明语言MiniLang，显著提升了基于LLM的神经定理证明成功率。


<details>
  <summary>Details</summary>
Motivation: 降低形式验证中的劳动力和计算成本，探索通过优化证明语言提升神经定理证明（NTP）的效果。

Method: 设计MiniLang（改进的Isabelle/HOL证明语言），结合增强版Sledgehammer，测试两种微调LLM。

Result: MiniLang将PISA基准上的成功率提升29%，pass@1达69.1%，超越Baldur的pass@64（65.7%）。

Conclusion: MiniLang证明语言显著提升NTP性能，为形式化验证提供高效工具。

Abstract: Neural Theorem Proving (NTP) employs deep learning methods, particularly
Large Language Models (LLMs), to automate formal proofs in proof assistants.
This approach holds promise for reducing the dramatic labor costs or
computation costs required in proof engineering, which is fundamental to formal
verification and other software engineering methods. The paper explores the
potential of improving NTP by redesigning the proof language, given that LLMs'
capabilities depend highly on representations. We introduce \emph{MiniLang}, a
redesigned proof language for Isabelle/HOL incorporating an improved version of
Sledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by
improving the success rate on the PISA benchmark by up to 29\% in comparison to
generation of Isar proof script. The success rate under one attempt (so-called
\emph{pass@1}) reaches 69.1\%, exceeding the previous Baldur's pass@64
(65.7\%); The pass@8 reaches 79.2\%, exceeding the state-of-the-art on PISA
(71.0\%) achieved by Magnushammer.

</details>


### [18] [An Enumerative Embedding of the Python Type System in ACL2s](https://arxiv.org/abs/2507.19015)
*Samuel Xifaras,Panagiotis Manolios,Andrew T. Walter,William Robertson*

Main category: cs.PL

TL;DR: 论文提出了一种将Python类型系统子集嵌入ACL2s的方法，用于生成输入以模糊测试Python代码，并发现传统类型检查器未检测到的错误。


<details>
  <summary>Details</summary>
Motivation: 解决Python代码中未被现有类型检查器发现的错误问题。

Method: 通过ACL2s嵌入Python类型系统的子集，生成类型实例作为模糊测试输入。

Result: 在四个开源库中测试，代码覆盖率为68%至80%以上，并识别了影响覆盖率的代码模式。

Conclusion: 验证了方法的有效性，并提出了未来改进方向。

Abstract: Python is a high-level interpreted language that has become an industry
standard in a wide variety of applications. In this paper, we take a first step
towards using ACL2s to reason about Python code by developing an embedding of a
subset of the Python type system in ACL2s. The subset of Python types we
support includes many of the most commonly used type annotations as well as
user-defined types comprised of supported types. We provide ACL2s definitions
of these types, as well as defdata enumerators that are customized to provide
code coverage and identify errors in Python programs. Using the ACL2s
embedding, we can generate instances of types that can then be used as inputs
to fuzz Python programs, which allows us to identify bugs in Python code that
are not detected by state-of-the-art Python type checkers. We evaluate our work
against four open-source repositories, extracting their type information and
generating inputs for fuzzing functions with type signatures that are in the
supported subset of Python types. Note that we only use the type signatures of
functions to generate inputs and treat the bodies of functions as black boxes.
We measure code coverage, which ranges from about 68% to more than 80%, and
identify code patterns that hinder coverage such as complex branch conditions
and external file system dependencies. We conclude with a discussion of the
results and recommendations for future work.

</details>


### [19] [A Programming Language for Feasible Solutions](https://arxiv.org/abs/2507.19176)
*Weijun Chen,Yuxi Fu,Huan Long*

Main category: cs.PL

TL;DR: 本文提出了一种新的基于静态类型系统的编程语言，确保所有程序在多项式时间内运行，且所有多项式时间可解的问题均可由该语言实现。


<details>
  <summary>Details</summary>
Motivation: 解决程序验证中运行时效率和终止性问题，通过设计一种语言框架，确保这些特性。

Method: 设计一种新的静态类型系统语言，通过类型保证程序的多项式运行时间和等价性。

Result: 建立了理论上的等价性，并实现了语言的解释器，展示了实践的可行性。

Conclusion: 该工作不仅在理论上证明了语言的等价性，还提出了简化程序分析和验证的实用方法。

Abstract: Runtime efficiency and termination are crucial properties in the studies of
program verification. Instead of dealing with these issues in an ad hoc manner,
it would be useful to develop a robust framework in which such properties are
guaranteed by design. This paper introduces a new imperative programming
language whose design is grounded in a static type system that ensures the
following equivalence property: All definable programs are guaranteed to run in
polynomial time; Conversely, all problems solvable in polynomial time can be
solved by some programs of the language. The contribution of this work is
twofold. On the theoretical side, the foundational equivalence property is
established, and the proof of the equivalence theorem is non-trivial. On the
practical side, a programming approach is proposed that can streamline program
analysis and verification for feasible computations. An interpreter for the
language has been implemented, demonstrating the feasibility of the approach in
practice.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Third-Party Assessment of Mobile Performance in the 5G Era](https://arxiv.org/abs/2507.18834)
*ASM Rizvi,John Heidemann,David Plonka*

Main category: cs.NI

TL;DR: 论文通过全球分布的CDN数据分析了移动设备的网络体验，发现部分用户能体验到极低延迟，但大多数用户的吞吐量不足50 Mb/s，并揭示了延迟稳定性对异常检测的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着5G时代到来，移动设备对高性能网络的需求增加，且缺乏对多运营商和国际视角的研究。

Method: 利用商业、全球分布的CDN数据，从延迟、吞吐量和稳定性三个维度分析移动用户体验。

Result: 部分用户体验极低延迟（6毫秒），但仅5%用户能稳定低于20毫秒。60%用户吞吐量低于50 Mb/s，但延迟在特定位置稳定。

Conclusion: 移动网络性能在不同用户和地区差异显著，延迟稳定性可作为异常检测的重要指标。

Abstract: The web experience using mobile devices is important since a significant
portion of the Internet traffic is initiated from mobile devices. In the era of
5G, users expect a high-performance data network to stream media content and
for other latency-sensitive applications. In this paper, we characterize mobile
experience in terms of latency, throughput, and stability measured from a
commercial, globally-distributed CDN. Unlike prior work, CDN data provides a
relatively neutral, carrier-agnostic perspective, providing a clear view of
multiple and international providers. Our analysis of mobile client traffic
shows mobile users sometimes experience markedly low latency, even as low as 6
ms. However, only the top 5% users regularly experience less than 20 ms of
minimum latency. While 100 Mb/s throughput is not rare, we show around 60%
users observe less than 50 Mb/s throughput. We find the minimum mobile latency
is generally stable at a specific location which can be an important
characteristic for anomaly detection.

</details>


### [21] [Large Language Model-Based Task Offloading and Resource Allocation for Digital Twin Edge Computing Networks](https://arxiv.org/abs/2507.19050)
*Qiong Wu,Yu Xie,Pingyi Fan,Dong Qin,Kezhi Wang,Nan Cheng,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 论文提出了一种基于大语言模型（LLM）的数字孪生边缘计算网络，用于任务卸载和资源分配，性能优于传统多智能体强化学习（MARL）。


<details>
  <summary>Details</summary>
Motivation: 研究多车辆与服务器构成的数字孪生边缘计算网络中，任务卸载引起的排队问题，需解决长期约束下的资源分配挑战。

Method: 利用Lyapunov优化将长期约束转化为短期决策，并采用基于LLM的上下文学习方法替代传统MARL框架。

Result: 实验表明，LLM方法的性能与MARL相当或更优。

Conclusion: LLM方法在数字孪生边缘计算网络中具有潜力，是任务卸载和资源分配的有效替代方案。

Abstract: In this paper, we propose a general digital twin edge computing network
comprising multiple vehicles and a server. Each vehicle generates multiple
computing tasks within a time slot, leading to queuing challenges when
offloading tasks to the server. The study investigates task offloading
strategies, queue stability, and resource allocation. Lyapunov optimization is
employed to transform long-term constraints into tractable short-term
decisions. To solve the resulting problem, an in-context learning approach
based on large language model (LLM) is adopted, replacing the conventional
multi-agent reinforcement learning (MARL) framework. Experimental results
demonstrate that the LLM-based method achieves comparable or even superior
performance to MARL.

</details>


### [22] [iPLAN: Redefining Indoor Wireless Network Planning Through Large Language Models](https://arxiv.org/abs/2507.19096)
*Jinbo Hou,Stefanos Bakirtzis,Kehai Qiu,Sichong Liao,Hui Song,Haonan Hu,Kezhi Wang,Jie Zhang*

Main category: cs.NI

TL;DR: iPLAN框架利用大型语言模型（LLM）优化室内无线网络（IWN）规划，通过多模态室内环境（IE）表示和智能代理协作，显著提升了规划效率和无线性能。


<details>
  <summary>Details</summary>
Motivation: 传统的室内无线网络规划方法因室内环境复杂性和需求多样性而面临挑战，需要更高效的解决方案。

Method: 提出iPLAN框架，结合LLM优化器和多模态IE表示，实现两种核心应用：基于现有IE的IWN规划和IWN与IE的协同设计。

Result: 模拟结果表明，iPLAN在规划和优化任务中表现优异，提升了无线性能。

Conclusion: iPLAN通过LLM和多模态表示，为IWN规划带来了范式转变。

Abstract: Efficient indoor wireless network (IWN) planning is crucial for providing
high-quality 5G in-building services. However, traditional meta-heuristic and
artificial intelligence-based planning methods face significant challenges due
to the intricate interplay between indoor environments (IEs) and IWN demands.
In this article, we present an indoor wireless network Planning with large
LANguage models (iPLAN) framework, which integrates multi-modal IE
representations into large language model (LLM)-powered optimizers to improve
IWN planning. First, we instate the role of LLMs as optimizers, outlining
embedding techniques for IEs, and introducing two core applications of iPLAN:
(i) IWN planning based on pre-existing IEs and (ii) joint design of IWN and IE
for new wireless-friendly buildings. For the former, we embed essential
information into LLM optimizers by leveraging indoor descriptions,
domain-specific knowledge, and performance-driven perception. For the latter,
we conceptualize a multi-agent strategy, where intelligent agents
collaboratively address key planning sub-tasks in a step-by-step manner while
ensuring optimal trade-offs between the agents. The simulation results
demonstrate that iPLAN achieves superior performance in IWN planning tasks and
optimizes building wireless performance through the joint design of IEs and
IWNs, exemplifying a paradigm shift in IWN planning.

</details>


### [23] [AI Enabled 6G for Semantic Metaverse: Prospects, Challenges and Solutions for Future Wireless VR](https://arxiv.org/abs/2507.19124)
*Muhammad Ahmed Mohsin,Sagnik Bhattacharya,Abhiram Gorle,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.NI

TL;DR: 该论文提出了一种针对多用户无线VR通信的低秩信道优化方法，采用非线性收发器和深度强化学习（DRL）近似最优性能，显著提升了数据率和能效。


<details>
  <summary>Details</summary>
Motivation: 解决多用户无线VR通信中因低秩信道导致的性能瓶颈问题。

Method: 使用最优非线性收发器（如广义决策反馈或连续取消上行链路、脏纸预编码下行链路），并结合DRL优化用户能量分配和解码顺序。

Result: 实验显示，相比OMA、NOMA和MC-NOMA，数据率分别提升39%、28%和16%，能效提升75%、45%和40%。DRL框架复杂度降低5倍，接近全局最优。

Conclusion: 该方法为无线VR通信提供了高效、实时的解决方案，显著优于现有技术。

Abstract: Wireless support of virtual reality (VR) has challenges when a network has
multiple users, particularly for 3D VR gaming, digital AI avatars, and remote
team collaboration. This work addresses these challenges through investigation
of the low-rank channels that inevitably occur when there are more active users
than there are degrees of spatial freedom, effectively often the number of
antennas. The presented approach uses optimal nonlinear transceivers,
equivalently generalized decision-feedback or successive cancellation for
uplink and superposition or dirty-paper precoders for downlink. Additionally, a
powerful optimization approach for the users' energy allocation and decoding
order appears to provide large improvements over existing methods, effectively
nearing theoretical optima. As the latter optimization methods pose real-time
challenges, approximations using deep reinforcement learning (DRL) are used to
approximate best performance with much lower (5x at least) complexity.
Experimental results show significantly larger sum rates and very large power
savings to attain the data rates found necessary to support VR. Experimental
results show the proposed algorithm outperforms current industry standards like
orthogonal multiple access (OMA), non-orthogonal multiple access (NOMA), as
well as the highly researched methods in multi-carrier NOMA (MC-NOMA),
enhancing sum data rate by 39%, 28%, and 16%, respectively, at a given power
level. For the same data rate, it achieves power savings of 75%, 45%, and 40%,
making it ideal for VR applications. Additionally, a near-optimal deep
reinforcement learning (DRL)-based resource allocation framework for real-time
use by being 5x faster and reaching 83% of the global optimum is introduced.

</details>


### [24] [Virne: A Comprehensive Benchmark for Deep RL-based Network Resource Allocation in NFV](https://arxiv.org/abs/2507.19234)
*Tianfu Wang,Liwei Deng,Xi Chen,Junyang Wang,Huiguo He,Leilei Ding,Wei Wu,Qilin Fan,Hui Xiong*

Main category: cs.NI

TL;DR: 本文介绍了Virne，一个针对NFV资源分配问题的全面基准框架，支持深度强化学习方法，提供多样化网络模拟、模块化实现和广泛评估能力。


<details>
  <summary>Details</summary>
Motivation: NFV资源分配的复杂性需要系统化基准框架来支持深度RL方法的开发和一致评估。

Method: Virne框架提供可定制的网络模拟环境（如云、边缘、5G），支持30多种方法，包含模块化和可扩展的实现。

Result: 通过大量实验深入分析性能权衡，为高效实现提供实用建议，并为未来研究方向提供指导。

Conclusion: Virne作为全面基准，有望推动NFV资源分配方法和深度RL应用的发展。

Abstract: Resource allocation (RA) is critical to efficient service deployment in
Network Function Virtualization (NFV), a transformative networking paradigm.
Recently, deep Reinforcement Learning (RL)-based methods have been showing
promising potential to address this complexity. However, the lack of a
systematic benchmarking framework and thorough analysis hinders the exploration
of emerging networks and the development of more robust algorithms while
causing inconsistent evaluation. In this paper, we introduce Virne, a
comprehensive benchmarking framework for the NFV-RA problem, with a focus on
supporting deep RL-based methods. Virne provides customizable simulations for
diverse network scenarios, including cloud, edge, and 5G environments. It also
features a modular and extensible implementation pipeline that supports over 30
methods of various types, and includes practical evaluation perspectives beyond
effectiveness, such as scalability, generalization, and scalability.
Furthermore, we conduct in-depth analysis through extensive experiments to
provide valuable insights into performance trade-offs for efficient
implementation and offer actionable guidance for future research directions.
Overall, with its diverse simulations, rich implementations, and extensive
evaluation capabilities, Virne could serve as a comprehensive benchmark for
advancing NFV-RA methods and deep RL applications. The code is publicly
available at https://github.com/GeminiLight/virne.

</details>


### [25] [Deep Reinforcement Learning-Based Scheduling for Wi-Fi Multi-Access Point Coordination](https://arxiv.org/abs/2507.19377)
*David Nunez,Francesc Wilhelmi,Maksymilian Wojnar,Katarzyna Kosek-Szott,Szymon Szott,Boris Bellalta*

Main category: cs.NI

TL;DR: 本文提出了一种基于深度强化学习（DRL）的方法，用于优化IEEE 802.11bn中多接入点协调（MAPC）的调度策略，以减少网络中最坏情况下的延迟，并在仿真中表现出优于现有启发式策略的性能。


<details>
  <summary>Details</summary>
Motivation: 多接入点协调（MAPC）是未来Wi-Fi网络的关键特性，但在密集部署中，如何在不同流量和干扰条件下实现高效调度仍是一个复杂问题。

Method: 通过将MAPC调度建模为顺序决策问题，并提出基于近端策略优化（PPO）的DRL机制，训练智能体以优化SR组的调度，最小化最坏情况延迟。

Result: 仿真结果显示，该方法在多种网络负载和流量模式下优于现有启发式策略，最坏延迟降低30%。

Conclusion: 该方法为密集Wi-Fi网络的低延迟调度提供了有效解决方案，展示了DRL在实际网络优化中的潜力。

Abstract: Multi-access point coordination (MAPC) is a key feature of IEEE 802.11bn,
with a potential impact on future Wi-Fi networks. MAPC enables joint scheduling
decisions across multiple access points (APs) to improve throughput, latency,
and reliability in dense Wi-Fi deployments. However, implementing efficient
scheduling policies under diverse traffic and interference conditions in
overlapping basic service sets (OBSSs) remains a complex task. This paper
presents a method to minimize the network-wide worst-case latency by
formulating MAPC scheduling as a sequential decision-making problem and
proposing a deep reinforcement learning (DRL) mechanism to minimize worst-case
delays in OBSS deployments. Specifically, we train a DRL agent using proximal
policy optimization (PPO) within an 802.11bn-compatible Gymnasium environment.
This environment provides observations of queue states, delay metrics, and
channel conditions, enabling the agent to schedule multiple AP-station pairs to
transmit simultaneously by leveraging spatial reuse (SR) groups. Simulations
demonstrate that our proposed solution outperforms state-of-the-art heuristic
strategies across a wide range of network loads and traffic patterns. The
trained machine learning (ML) models consistently achieve lower 99th-percentile
delays, showing up to a 30% improvement over the best baseline.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [26] [CatchPhrase: EXPrompt-Guided Encoder Adaptation for Audio-to-Image Generation](https://arxiv.org/abs/2507.18750)
*Hyunwoo Oh,SeungJu Cha,Kwanyoung Lee,Si-Woo Kim,Dong-Jin Kim*

Main category: cs.MM

TL;DR: CatchPhrase是一种新的音频到图像生成框架，通过增强跨模态语义提示和选择性过滤，减少了音频输入与生成图像之间的语义不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态生成中因同音词和听觉错觉导致的语义不对齐问题。

Method: 利用大型语言模型和音频描述模型生成丰富的跨模态语义提示，并通过多模态过滤和检索选择最优提示，训练轻量级映射网络适配预训练文本到图像模型。

Result: 在多个音频分类数据集上，CatchPhrase显著提高了音频到图像的对齐和生成质量。

Conclusion: CatchPhrase通过语义提示增强和选择性过滤有效缓解了跨模态生成中的语义不对齐问题。

Abstract: We propose CatchPhrase, a novel audio-to-image generation framework designed
to mitigate semantic misalignment between audio inputs and generated images.
While recent advances in multi-modal encoders have enabled progress in
cross-modal generation, ambiguity stemming from homographs and auditory
illusions continues to hinder accurate alignment. To address this issue,
CatchPhrase generates enriched cross-modal semantic prompts (EXPrompt Mining)
from weak class labels by leveraging large language models (LLMs) and audio
captioning models (ACMs). To address both class-level and instance-level
misalignment, we apply multi-modal filtering and retrieval to select the most
semantically aligned prompt for each audio sample (EXPrompt Selector). A
lightweight mapping network is then trained to adapt pre-trained text-to-image
generation models to audio input. Extensive experiments on multiple audio
classification datasets demonstrate that CatchPhrase improves audio-to-image
alignment and consistently enhances generation quality by mitigating semantic
misalignment.

</details>


### [27] [Benchmarking Multimodal Understanding and Complex Reasoning for ESG Tasks](https://arxiv.org/abs/2507.18932)
*Lei Zhang,Xin Zhou,Chaoyue He,Di Wang,Yi Wu,Hong Xu,Wei Liu,Chunyan Miao*

Main category: cs.MM

TL;DR: 该论文提出了首个针对ESG报告的多模态基准数据集MMESGBench，用于评估多模态理解和复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统在ESG报告的多模态和结构化多样性上表现不足，且缺乏专用基准数据集。论文旨在填补这一空白。

Method: 通过人机协作的多阶段流程构建数据集，包括多模态LLM生成候选QA对、LLM验证语义准确性，以及专家验证和校准。

Result: MMESGBench包含933个QA对，覆盖7种文档类型和3种ESG来源。实验显示多模态模型在视觉和跨页任务上优于纯文本基线。

Conclusion: MMESGBench为ESG领域提供了一个高质量的多模态基准，推动了多模态理解和复杂推理的研究。

Abstract: Environmental, Social, and Governance (ESG) reports are essential for
evaluating sustainability practices, ensuring regulatory compliance, and
promoting financial transparency. However, these documents are often lengthy,
structurally diverse, and multimodal, comprising dense text, structured tables,
complex figures, and layout-dependent semantics. Existing AI systems often
struggle to perform reliable document-level reasoning in such settings, and no
dedicated benchmark currently exists in ESG domain. To fill the gap, we
introduce \textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted
to evaluate multimodal understanding and complex reasoning across structurally
diverse and multi-source ESG documents. This dataset is constructed via a
human-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates
candidate question-answer (QA) pairs by jointly interpreting rich textual,
tabular, and visual information from layout-aware document pages. Second, an
LLM verifies the semantic accuracy, completeness, and reasoning complexity of
each QA pair. This automated process is followed by an expert-in-the-loop
validation, where domain specialists validate and calibrate QA pairs to ensure
quality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs
derived from 45 ESG documents, spanning across seven distinct document types
and three major ESG source categories. Questions are categorized as
single-page, cross-page, or unanswerable, with each accompanied by fine-grained
multimodal evidence. Initial experiments validate that multimodal and
retrieval-augmented models substantially outperform text-only baselines,
particularly on visually grounded and cross-page tasks. MMESGBench is publicly
available as an open-source dataset at
https://github.com/Zhanglei1103/MMESGBench.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [28] [Who Wins the Multi-Structural Game?](https://arxiv.org/abs/2507.18718)
*Ronald Fagin,Neil Immerman,Phokion Kolaitis,Jonathan Lenchner,Rik Sengupta*

Main category: cs.LO

TL;DR: 本文研究了多结构（MS）游戏的决策问题，证明其PSPACE难但属于NEXPTIME，并解决了关于EF游戏难度与模式阶数关系的开放问题。


<details>
  <summary>Details</summary>
Motivation: 通过组合游戏（如EF和MS游戏）研究形式逻辑语言的语法性质，尤其是游戏的决策问题及计算复杂度。

Method: 结合Pezzoli的构造方法、优化问题的不可近似性理论以及MS游戏的并行玩法技术。

Result: 证明了MS游戏的决策问题PSPACE难且属于NEXPTIME，解决了EF游戏与模式阶数关系的开放问题。

Conclusion: MS游戏的复杂性介于PSPACE和NEXPTIME之间，为逻辑游戏的研究提供了新的见解。

Abstract: Combinatorial games played between two players, called Spoiler and
Duplicator, have often been used to capture syntactic properties of formal
logical languages. For instance, the widely used Ehrenfeucht-Fra\"iss\'e (EF)
game captures the syntactic measure of quantifier rank of first-order formulas.
For every such game, there is an associated natural decision problem: "given an
instance of the game, does Spoiler win the game on that instance?" For EF
games, this problem was shown to be PSPACE-complete by Pezzoli in 1998. In this
present paper, we show that the same problem for the *multi-structural* (MS)
games of recent interest is PSPACE-hard, but contained in NEXPTIME. In the
process, we also resolve an open problem posed by Pezzoli about the dependence
of the hardness results for EF games on the arity of the schema under
consideration. Our techniques combine adaptations of Pezzoli's constructions
together with insights from the theory of inapproximability of optimization
problems, as well as the recently developed technique of parallel play for MS
games.

</details>


### [29] [Higher-order Kripke models for intuitionistic and non-classical modal logics](https://arxiv.org/abs/2507.18798)
*Victor Barroso-Nascimento*

Main category: cs.LO

TL;DR: 该论文提出了高阶Kripke模型，是标准Kripke模型的推广，具有数学和概念上的新颖性，并将其应用于非经典逻辑的模态语义。


<details>
  <summary>Details</summary>
Motivation: 研究动机是对标准Kripke模型进行高阶推广，以更灵活地定义非经典逻辑的模态语义。

Method: 方法是通过引入多层Kripke模型（n阶模型），并定义模态关系的可访问性，特别是以直觉主义模态逻辑为例。

Result: 结果表明，1阶模型可以等价于双关系模型或新逻辑MK的模型，并且提供了直观的解释（如“时间线”和“替代时间线”）。

Conclusion: 结论是这一框架为各种非经典逻辑提供了统一的模态语义方法，并提出了进一步研究的猜想。

Abstract: This paper introduces higher-order Kripke models, a generalization of
standard Kripke models that is remarkably close to Kripke's original idea -
both mathematically and conceptually. Standard Kripke models are now considered
$0$-ary models, whereas an $n$-ary model for $n > 0$ is a model whose set of
objects (''possible worlds'') contains only $(n-1)$-ary Kripke models. Models
with infinitely many layers are also considered. This framework is obtained by
promoting a radical change of perspective in how modal semantics for
non-classical logics are defined: just like classical modalities are obtained
through use of an accessibility relation between classical propositional
models, non-classical modalities are now obtained through use of an
accessibility relation between non-classical propositional models (even when
they are Kripke models already). The paper introduces the new models after
dealing specifically with the case of intuitionistic modal logic. It is shown
that, depending on which intuitionistic $0$-ary propositional models are
allowed, we may obtain $1$-ary models equivalent to either birelational models
for $IK$ or for a new logic called $MK$. Those $1$-ary models have an intuitive
reading that adds to the interpretation of intuitionistic models in terms of
''timelines'' the concept of ''alternative timelines''. More generally, the
$1$-ary models can be read as defining a concept of ''alternative'' for any
substantive interpretation of the $0$-ary models. The semantic clauses for
necessity and possibility of $MK$ are also modular and can be used to obtain
similar modal semantics for every non-classical logic, each of which can be
provided with a similar intuitive reading. After intuitionistic modal logic is
dealt with, the general structure of High-order Kripke Models and some of its
variants are defined, and a series of conjectures about their properties are
stated.

</details>


### [30] [A Proof of the Schröder-Bernstein Theorem in ACL2](https://arxiv.org/abs/2507.19008)
*Grant Jurgensen*

Main category: cs.LO

TL;DR: 该论文在ACL2中形式化并验证了Schröder-Bernstein定理，引入了链理论来定义非可计算见证。


<details>
  <summary>Details</summary>
Motivation: 研究如何在ACL2中验证Schröder-Bernstein定理，展示其经典证明在形式化逻辑中的实现。

Method: 采用ACL2形式化定理，引入链理论定义非可计算见证，跟随经典证明步骤。

Result: 成功在ACL2中形式化并验证了Schröder-Bernstein定理。

Conclusion: 研究表明经典数学定理可通过形式化方法在逻辑系统中实现和验证。

Abstract: The Schr\"oder-Bernstein theorem states that, for any two sets P and Q, if
there exists an injection from P to Q and an injection from Q to P, then there
must exist a bijection between the two sets. Classically, it follows that the
ordering of the cardinal numbers is antisymmetric. We describe a formulation
and verification of the Schr\"oder-Bernstein theorem in ACL2 following a
well-known proof, introducing a theory of chains to define a non-computable
witness.

</details>


### [31] [RV32I in ACL2](https://arxiv.org/abs/2507.19009)
*Carl Kwan*

Main category: cs.LO

TL;DR: 一个简单的ACL2模拟器，用于RISC-V 32位基本指令集架构，采用操作语义风格编写，验证了多项状态定理。


<details>
  <summary>Details</summary>
Motivation: 提供一种简单且验证充分的RISC-V ISA模型，便于进一步的形式化验证和研究。

Method: 采用操作语义风格编写模拟器，分离指令解码和语义实现，并自动验证编码/解码功能。

Result: 成功验证了读取-写入、状态良好性等定理，并自动完成了RV32I指令集的编码/解码验证。

Conclusion: 该模型为RISC-V的形式化验证提供了可靠的基础，且具有自动化的优势。

Abstract: We present a simple ACL2 simulator for the RISC-V 32-bit base instruction set
architecture, written in the operational semantics style. Like many other ISA
models, our RISC-V state object is a single-threaded object and we prove
read-over-write, write-over-write, writing-the-read, and state well-formedness
theorems. Unlike some other models, we separate the instruction decoding
functions from their semantic counterparts. Accordingly, we verify encoding /
decoding functions for each RV32I instruction, the proofs for which are
entirely automatic.

</details>


### [32] [On Automating Proofs of Multiplier Adder Trees using the RTL Books](https://arxiv.org/abs/2507.19010)
*Mayank Manjrekar*

Main category: cs.LO

TL;DR: 介绍了用于Arm硬件设计验证的自动化ACL2证明工具ctv-cp。


<details>
  <summary>Details</summary>
Motivation: 简化ACL2证明开发工作，用于验证整数乘法器等硬件设计。

Method: 开发了实验性验证工具ctv-cp，集成到Arm的验证框架中。

Result: 显著自动化了从浮点除法到矩阵乘法的硬件设计验证过程。

Conclusion: ctv-cp成功提升了验证效率，适用于多种硬件设计场景。

Abstract: We present an experimental, verified clause processor ctv-cp that fits into
the framework used at Arm for formal verification of arithmetic hardware
designs. This largely automates the ACL2 proof development effort for integer
multiplier modules that exist in designs ranging from floating-point division
to matrix multiplication.

</details>


### [33] [A Formalization of the Yul Language and Some Verified Yul Code Transformations](https://arxiv.org/abs/2507.19012)
*Alessandro Coglio,Eric McCarthy*

Main category: cs.LO

TL;DR: 本文介绍了使用ACL2定理证明器对Yul语言的形式化语法和语义进行建模，并证明了其代码转换的正确性。


<details>
  <summary>Details</summary>
Motivation: 为确保Solidity编译器中Yul代码转换的正确性及其顺序的可靠性。

Method: 使用ACL2定理证明器对Yul的语法、语义、代码转换进行形式化建模，并证明其正确性。

Result: 成功建立了Yul的形式化模型，并证明了相关代码转换的正确性。

Conclusion: 通过形式化方法验证了Yul代码转换的正确性，增强了编译器的可靠性。

Abstract: Yul is an intermediate language used in the compilation of the Solidity
programming language for Ethereum smart contracts. The compiler applies
customizable sequences of transformations to Yul code. To help ensure the
correctness of these transformations and their sequencing, we used the ACL2
theorem prover to develop a formalization of the syntax and semantics of Yul,
proofs relating static and dynamic semantics, a formalization of some Yul code
transformations, and correctness proofs for these transformations.

</details>


### [34] [A Formalization of the Correctness of the Floodsub Protocol](https://arxiv.org/abs/2507.19013)
*Ankit Kumar,Panagiotis Manolios*

Main category: cs.LO

TL;DR: Floodsub是一个简单、鲁棒的P2P发布/订阅协议，本文通过WFS证明其正确性，实现了Broadcastsub的模拟细化，并首次对实际pubsub协议进行了机械化验证。


<details>
  <summary>Details</summary>
Motivation: 验证Floodsub协议的正确性，展示其作为Broadcastsub的实现，并通过机械化证明提升可信度。

Method: 使用Well-Founded Simulation (WFS)方法，局部推理状态及其后继关系，机械化证明Floodsub是Broadcastsub的模拟细化。

Result: 成功证明了Floodsub是Broadcastsub的模拟细化，并首次完成现实pubsub协议的机械化验证。

Conclusion: Floodsub通过WFS证明了其正确性，本研究的机械化验证方法为类似协议提供了新思路。

Abstract: Floodsub is a simple, robust and popular peer-to-peer publish/subscribe
(pubsub) protocol, where nodes can arbitrarily leave or join the network,
subscribe to or unsubscribe from topics and forward newly received messages to
all of their neighbors, except the sender or the originating peer. To show the
correctness of Floodsub, we propose its specification: Broadcastsub, in which
implementation details like network connections and neighbor subscriptions are
elided. To show that Floodsub does really implement Broadcastsub, one would
have to show that the two systems have related infinite computations. We prove
this by reasoning locally about states and their successors using Well-Founded
Simulation (WFS). In this paper, we focus on the mechanization of a proof which
shows that Floodsub is a simulation refinement of Broadcastsub using WFS. To
the best of our knowledge, ours is the first mechanized refinement-based
verification of a real world pubsub protocol.

</details>


### [35] [An ACL2s Interface to Z3](https://arxiv.org/abs/2507.19014)
*Andrew T. Walter,Panagiotis Manolios*

Main category: cs.LO

TL;DR: Lisp-Z3是一个扩展ACL2s系统编程框架的工具，支持Z3 SMT求解器，结合了SMT和交互式定理证明的优势，适用于多种应用场景。


<details>
  <summary>Details</summary>
Motivation: 为了结合SMT求解器Z3和交互式定理证明工具ACL2/s的功能，提升复杂问题的解决能力。

Method: 扩展ACL2s框架，开发Lisp-Z3工具，使其能在Common Lisp中使用Z3和ACL2/s，并用于实际应用如数独求解、字符串求解和硬件在环测试。

Result: Lisp-Z3在多个应用中表现出色，例如字符串求解器SeqSolve在基准测试中表现最佳，且在硬件测试中实现了低延迟。

Conclusion: Lisp-Z3成功整合了Z3和ACL2/s，展示了其在自动化支持和高效问题解决方面的潜力，未来计划进一步优化依赖类型的支持。

Abstract: We present Lisp-Z3, an extension to the ACL2s systems programming framework
(ASPF) that supports the use of the Z3 satisfiability modulo theories (SMT)
solver. Lisp-Z3 allows one to develop tools written using the full feature set
of Common Lisp that can use both ACL2/s (either ACL2 or ACL2s) and Z3 as
services, combining the power of SMT and interactive theorem proving. Lisp-Z3
is usable by anyone who would like to interact with Z3 from Common Lisp, as it
does not depend on the availability of ACL2/s. We discuss the use of Lisp-Z3 in
three applications. The first is a Sudoku solver. The second is SeqSolve, a
string solver which solved a larger number of benchmark problems more quickly
than any other existing solver at the time of its publishing. Finally, Lisp-Z3
was also used in the context of hardware-in-the-loop fuzzing of wireless
routers, where low latency was an important goal. The latter two applications
leveraged the ability of Lisp-Z3 to integrate Z3 with ACL2s code. We have
further plans to use Lisp-Z3 inside of ACL2s to provide more powerful automated
support for dependent types, and in particular more efficient generation of
counterexamples to properties involving dependent types. This paper describes
the usage and implementation of Lisp-Z3, as well as an evaluation of its use in
the aforementioned applications.

</details>


### [36] [A CASP-based Solution for Traffic Signal Optimisation](https://arxiv.org/abs/2507.19061)
*Alice Tarzariol,Marco Maratea,Mauro Vallati*

Main category: cs.LO

TL;DR: 本文提出了一种基于约束答案集编程（CASP）的交通信号优化方法，优于现有的PDDL+方法。


<details>
  <summary>Details</summary>
Motivation: 现有PDDL+语言在交通信号优化中存在局限性，需要更高效的解决方案。

Method: 使用CASP语言编码交通信号优化问题，并通过clingcon 3求解器求解。

Result: 实验基于真实历史数据，CASP方法在解决方案质量上优于PDDL+模型。

Conclusion: CASP方法在交通信号优化中展现了潜力，可提升现有解决方案的质量。

Abstract: In the context of urban traffic control, traffic signal optimisation is the
problem of determining the optimal green length for each signal in a set of
traffic signals. The literature has effectively tackled such a problem, mostly
with automated planning techniques leveraging the PDDL+ language and solvers.
However, such language has limitations when it comes to specifying optimisation
statements and computing optimal plans. In this paper, we provide an
alternative solution to the traffic signal optimisation problem based on
Constraint Answer Set Programming (CASP). We devise an encoding in a CASP
language, which is then solved by means of clingcon 3, a system extending the
well-known ASP solver clingo. We performed experiments on real historical data
from the town of Huddersfield in the UK, comparing our approach to the PDDL+
model that obtained the best results for the considered benchmark. The results
showed the potential of our approach for tackling the traffic signal
optimisation problem and improving the solution quality of the PDDL+ plans.

</details>


### [37] [Transfinite Fixed Points in Alpay Algebra as Ordinal Game Equilibria in Dependent Type Theory](https://arxiv.org/abs/2507.19245)
*Faruk Alpay,Bugra Kilictas,Taylan Alpay*

Main category: cs.LO

TL;DR: 该论文通过迭代变换展示了Alpay代数中自指过程的稳定结果与无界修订对话的唯一均衡相同，并通过类型理论验证了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 为Alpay关于语义收敛的哲学主张提供形式化基础，并为无限自指系统的研究提供实用工具。

Method: 结合不动点定理、序论连续性和依赖类型理论，验证了自指过程的收敛性。

Result: 证明了迭代对话必然稳定且极限唯一，并通过机器验证支持了这一结论。

Conclusion: 该研究为无限自指系统的形式化分析和计算验证提供了严谨的理论框架。

Abstract: This paper contributes to the Alpay Algebra by demonstrating that the stable
outcome of a self referential process, obtained by iterating a transformation
through all ordinal stages, is identical to the unique equilibrium of an
unbounded revision dialogue between a system and its environment. The analysis
initially elucidates how classical fixed point theorems guarantee such
convergence in finite settings and subsequently extends the argument to the
transfinite domain, relying upon well founded induction and principles of order
theoretic continuity.
  Furthermore, the resulting transordinal fixed point operator is embedded into
dependent type theory, a formalization which permits every step of the
transfinite iteration and its limit to be verified within a modern proof
assistant. This procedure yields a machine checked proof that the iterative
dialogue necessarily stabilizes and that its limit is unique. The result
provides a foundation for Alpay's philosophical claim of semantic convergence
within the framework of constructive logic. By unifying concepts from fixed
point theory, game semantics, ordinal analysis, and type theory, this research
establishes a broadly accessible yet formally rigorous foundation for reasoning
about infinite self referential systems and offers practical tools for
certifying their convergence within computational environments.

</details>


### [38] [Order in Partial Markov Categories](https://arxiv.org/abs/2507.19424)
*Elena Di Lavore,Mario Román,Paweł Sobociński,Márk Széles*

Main category: cs.LO

TL;DR: 论文探讨了部分马尔可夫范畴中的两种序关系，证明了其可自然地丰富于预序集范畴，并提出了柯西-施瓦茨不等式的合成版本。


<details>
  <summary>Details</summary>
Motivation: 研究部分马尔可夫范畴的序结构和相关性质，以扩展概率计算的理论基础。

Method: 通过定义和比较两种序关系，证明范畴的序丰富性，并提出合成不等式。

Result: 证明了部分马尔可夫范畴的序丰富性，并应用新公理验证后验概率的性质。

Conclusion: 部分马尔可夫范畴的序结构与概率计算密切相关，为理论推理提供了新工具。

Abstract: Partial Markov categories are a recent framework for categorical probability
theory, providing an abstract account of partial probabilistic computation. In
this article, we discuss two order relations on the morphisms of a partial
Markov category. In particular, we prove that every partial Markov category is
canonically enriched over the category of preordered sets and monotone maps. We
show that our construction recovers several well-known order enrichments. We
also demonstrate that the existence of codiagonal maps (comparators) is closely
related to order properties of partial Markov categories. We propose a
synthetic version of the Cauchy-Schwarz inequality to facilitate inequational
reasoning in partial Markov categories. We apply this new axiom to prove that
updating a prior distribution with an evidence predicate increases the
likelihood of the evidence in the posterior.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [39] [More Expert-like Eye Gaze Movement Patterns are Related to Better X-ray Reading](https://arxiv.org/abs/2507.18637)
*Pingjing Yang,Jennifer Cromley,Jana Diesner*

Main category: cs.HC

TL;DR: 研究通过眼动轨迹的网络分析，探索牙科学生在诊断放射影像时的学习过程，发现某些网络指标与诊断表现相关，有助于理解视觉任务中的专业技能获取。


<details>
  <summary>Details</summary>
Motivation: 了解新手如何获得和提升视觉搜索技能，对优化跨领域培训方法至关重要。

Method: 使用网络分析方法建模眼动扫描路径为有向图，分析网络指标随时间的变化，并通过时间序列聚类识别不同视觉搜索策略模式。

Result: 转移熵与表现负相关，节点和边数及平均PageRank与表现正相关；学生网络指标的变化显示从中级到专家级处理的转变。

Conclusion: 研究结果有助于理解视觉任务的专业技能发展，并为AI辅助学习设计提供参考。

Abstract: Understanding how novices acquire and hone visual search skills is crucial
for developing and optimizing training methods across domains. Network analysis
methods can be used to analyze graph representations of visual expertise. This
study investigates the relationship between eye-gaze movements and learning
outcomes among undergraduate dentistry students who were diagnosing dental
radiographs over multiple semesters. We use network analysis techniques to
model eye-gaze scanpaths as directed graphs and examine changes in network
metrics over time. Using time series clustering on each metric, we identify
distinct patterns of visual search strategies and explore their association
with students' diagnostic performance. Our findings suggest that the network
metric of transition entropy is negatively correlated with performance scores,
while the number of nodes and edges as well as average PageRank are positively
correlated with performance scores. Changes in network metrics for individual
students over time suggest a developmental shift from intermediate to
expert-level processing. These insights contribute to understanding expertise
acquisition in visual tasks and can inform the design of AI-assisted learning
interventions.

</details>


### [40] [Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity](https://arxiv.org/abs/2507.18638)
*Rizal Khoirul Anam*

Main category: cs.HC

TL;DR: 研究发现，清晰、结构化和上下文相关的提示能提升大语言模型（如ChatGPT、Gemini和DeepSeek）的输出效率和效果。


<details>
  <summary>Details</summary>
Motivation: 探讨用户提示的结构和清晰度如何影响大语言模型的有效性和生产力。

Method: 通过243名来自不同学术和职业背景的调查受访者的数据，分析AI使用习惯、提示策略和用户满意度。

Result: 使用清晰、结构化且上下文相关提示的用户报告了更高的任务效率和更好的结果。

Conclusion: 提示工程在最大化生成式AI价值中起关键作用，并为日常使用提供实际指导。

Abstract: The widespread adoption of large language models (LLMs) such as ChatGPT,
Gemini, and DeepSeek has significantly changed how people approach tasks in
education, professional work, and creative domains. This paper investigates how
the structure and clarity of user prompts impact the effectiveness and
productivity of LLM outputs. Using data from 243 survey respondents across
various academic and occupational backgrounds, we analyze AI usage habits,
prompting strategies, and user satisfaction. The results show that users who
employ clear, structured, and context-aware prompts report higher task
efficiency and better outcomes. These findings emphasize the essential role of
prompt engineering in maximizing the value of generative AI and provide
practical implications for its everyday use.

</details>


### [41] [People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction](https://arxiv.org/abs/2507.18639)
*Paweł Niszczota,Tomasz Grzegorczyk,Alexander Pastukhov*

Main category: cs.HC

TL;DR: 研究发现，与人类相比，使用大型语言模型（LLM）进行互动时合作率略低（低10-15%），但整体合作率仍然较高。沟通虽未完全缩小人机行为差距，但能平等提升合作率。


<details>
  <summary>Details</summary>
Motivation: 探讨在商业环境中，用LLM替代人类互动是否会改变合作行为，以验证LLM在需要合作的场景中的适用性。

Method: 通过两个实验（重复囚徒困境和单次囚徒困境），比较参与者与人类、传统机器人和LLM的合作行为，并测试沟通对其影响。

Result: LLM的合作率虽低于人类，但仍较高；沟通能显著提升合作率；且人类互动后的合作行为会延续到与LLM的互动中。

Conclusion: LLM在具合作性的商业场景中有潜力，但仍需谨慎使用。

Abstract: Machines driven by large language models (LLMs) have the potential to augment
humans across various tasks, a development with profound implications for
business settings where effective communication, collaboration, and stakeholder
trust are paramount. To explore how interacting with an LLM instead of a human
might shift cooperative behavior in such settings, we used the Prisoner's
Dilemma game -- a surrogate of several real-world managerial and economic
scenarios. In Experiment 1 (N=100), participants engaged in a thirty-round
repeated game against a human, a classic bot, and an LLM (GPT, in real-time).
In Experiment 2 (N=192), participants played a one-shot game against a human or
an LLM, with half of them allowed to communicate with their opponent, enabling
LLMs to leverage a key advantage over older-generation machines. Cooperation
rates with LLMs -- while lower by approximately 10-15 percentage points
compared to interactions with human opponents -- were nonetheless high. This
finding was particularly notable in Experiment 2, where the psychological cost
of selfish behavior was reduced. Although allowing communication about
cooperation did not close the human-machine behavioral gap, it increased the
likelihood of cooperation with both humans and LLMs equally (by 88%), which is
particularly surprising for LLMs given their non-human nature and the
assumption that people might be less receptive to cooperating with machines
compared to human counterparts. Additionally, cooperation with LLMs was higher
following prior interaction with humans, suggesting a spillover effect in
cooperative behavior. Our findings validate the (careful) use of LLMs by
businesses in settings that have a cooperative component.

</details>


### [42] [How good are humans at detecting AI-generated images? Learnings from an experiment](https://arxiv.org/abs/2507.18640)
*Thomas Roca,Anthony Cintron Roman,Jehú Torres Vega,Marcelo Duarte,Pengce Wang,Kevin White,Amit Misra,Juan Lavista Ferres*

Main category: cs.HC

TL;DR: 研究通过在线游戏收集数据，分析人类区分AI生成图像与真实图像的能力，结果显示整体成功率仅62%，表明人类辨别能力有限，尤以自然和城市景观为最难。


<details>
  <summary>Details</summary>
Motivation: 探讨人类在面对AI生成图像时的辨别能力，以评估潜在的 misinformation 风险。

Method: 使用在线游戏数据，参与者随机查看真假图像并判断其真实性，分析全球12500名参与者的287000次评估。

Result: 整体辨别成功率62%，人像辨别最准，自然和城市景观较差。

Conclusion: 人类区分AI生成图像的能力有限，需开发透明工具（如水印和AI检测工具）以应对风险。

Abstract: As AI-powered image generation improves, a key question is how well human
beings can differentiate between "real" and AI-generated or modified images.
Using data collected from the online game "Real or Not Quiz.", this study
investigates how effectively people can distinguish AI-generated images from
real ones. Participants viewed a randomized set of real and AI-generated
images, aiming to identify their authenticity. Analysis of approximately
287,000 image evaluations by over 12,500 global participants revealed an
overall success rate of only 62\%, indicating a modest ability, slightly above
chance. Participants were most accurate with human portraits but struggled
significantly with natural and urban landscapes. These results highlight the
inherent challenge humans face in distinguishing AI-generated visual content,
particularly images without obvious artifacts or stylistic cues. This study
stresses the need for transparency tools, such as watermarks and robust AI
detection tools to mitigate the risks of misinformation arising from
AI-generated content

</details>


### [43] [Comparing Human and AI Performance in Visual Storytelling through Creation of Comic Strips: A Case Study](https://arxiv.org/abs/2507.18641)
*Uğur Önal,Sanem Sariel,Metin Sezgin,Ergun Akleman*

Main category: cs.HC

TL;DR: 研究比较人类和AI在视觉叙事上的能力，发现AI擅长模仿艺术但缺乏叙事连贯性，而人类能更有效地将指令转化为有意义的视觉故事。


<details>
  <summary>Details</summary>
Motivation: 探究人类与AI在视觉叙事任务中的表现差异，揭示AI在艺术创作中的局限性。

Method: 通过让人类和AI根据相同指令重绘Nancy卡通漫画，比较两者的表现。

Result: AI能模仿专业艺术但叙事不连贯，人类则能有效构建视觉叙事。

Conclusion: 人类在视觉叙事中表现优于AI，AI需进一步提升叙事能力。

Abstract: This article presents a case study comparing the capabilities of humans and
artificial intelligence (AI) for visual storytelling. We developed detailed
instructions to recreate a three-panel Nancy cartoon strip by Ernie Bushmiller
and provided them to both humans and AI systems. The human participants were
20-something students with basic artistic training but no experience or
knowledge of this comic strip. The AI systems used were popular commercial
models trained to draw and paint like artists, though their training sets may
not necessarily include Bushmiller's work. Results showed that AI systems excel
at mimicking professional art but struggle to create coherent visual stories.
In contrast, humans proved highly adept at transforming instructions into
meaningful visual narratives.

</details>


### [44] [DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition](https://arxiv.org/abs/2507.18802)
*Danqing Shi,Furui Cheng,Tino Weinkauf,Antti Oulasvirta,Mennatallah El-Assady*

Main category: cs.HC

TL;DR: 提出了一种分解文本为单独主张的方法（DxHF界面）来改进人类反馈质量，提升LLM对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，标注者需直接比较长文本段落，认知负担重且效率低，尤其对陌生或复杂内容。

Method: 将文本分解为单独主张，并通过DxHF界面展示，视觉化编码相关性和相似性，提升比较效率。

Result: 分解方法显著提高了反馈准确性（平均5%），尤其针对不确定的用户，但反馈时间增加了18秒。

Conclusion: HCI方法（如DxHF）是提升人-AI对齐的有效途径，特别适用于复杂或不确定的场景。

Abstract: Human preferences are widely used to align large language models (LLMs)
through methods such as reinforcement learning from human feedback (RLHF).
However, the current user interfaces require annotators to compare text
paragraphs, which is cognitively challenging when the texts are long or
unfamiliar. This paper contributes by studying the decomposition principle as
an approach to improving the quality of human feedback for LLM alignment. This
approach breaks down the text into individual claims instead of directly
comparing two long-form text responses. Based on the principle, we build a
novel user interface DxHF. It enhances the comparison process by showing
decomposed claims, visually encoding the relevance of claims to the
conversation and linking similar claims. This allows users to skim through key
information and identify differences for better and quicker judgment. Our
technical evaluation shows evidence that decomposition generally improves
feedback accuracy regarding the ground truth, particularly for users with
uncertainty. A crowdsourcing study with 160 participants indicates that using
DxHF improves feedback accuracy by an average of 5%, although it increases the
average feedback time by 18 seconds. Notably, accuracy is significantly higher
in situations where users have less certainty. The finding of the study
highlights the potential of HCI as an effective method for improving human-AI
alignment.

</details>


### [45] [Ethical Considerations for Observational Research in Social VR](https://arxiv.org/abs/2507.18828)
*Victoria Chang,Caro Williams-Pierce,Huaishu Peng,Ge Gao*

Main category: cs.HC

TL;DR: 本文通过文献综述探讨了社交虚拟现实（VR）中观察性研究的伦理问题，提出了五项指导原则。


<details>
  <summary>Details</summary>
Motivation: 研究社交VR环境中观察性研究的伦理挑战，填补传统公共研究与新兴技术之间的空白。

Method: 通过文献综述分析公共面对面和社交VR环境中的观察方法，关注观察者的可见性、数据可追溯性和参与者自主性。

Result: 提出五项伦理研究指导原则，为未来研究提供平台设计、研究人员存在管理和社区知情同意机制的改进方向。

Conclusion: 社交VR的观察性研究需要新的伦理框架，本文的指导原则为未来研究提供了重要参考。

Abstract: Social VR introduces new ethical challenges for observational research. The
current paper presents a narrative literature review of ethical considerations
in observational methods, with a focus on work in HCI. We examine how
unobtrusive or selectively disclosed observation is implemented in public
face-to-face and social VR settings. Our review extends ethical discussions
from traditional public research into the context of social VR, highlighting
tensions between observer visibility, data traceability, and participant
autonomy. Drawing on insights distilled from prior literature, we propose five
constructive guidelines for ethical observational research in public social VR
environments. Our work offers key implications for future research, addressing
anticipated improvements in platform design, the management of researcher
presence, and the development of community-informed consent mechanisms.

</details>


### [46] [Uncertainty on Display: The Effects of Communicating Confidence Cues in Autonomous Vehicle-Pedestrian Interactions](https://arxiv.org/abs/2507.18836)
*Yue Luo,Xinyan Yu,Tram Thi Minh Tran,Marius Hoggenmueller*

Main category: cs.HC

TL;DR: 研究通过VR实验比较了自动驾驶车辆不确定性的显性（百分比显示）和隐性（车辆运动）传达方式，发现显性传达更有效且更受青睐，提升了行人安全感与信任。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆决策中的不确定性通常未传达给行人，影响了透明度，研究旨在探讨如何通过显性和隐性方式传达不确定性。

Method: 通过VR实验（N=26），比较显性（百分比显示）和隐性（车辆运动）传达方式在不同置信水平下的效果。

Result: 显性传达更有效，提升了安全感、信任和用户体验；隐性传达在低置信度时易引发模糊。

Conclusion: 显性传达是更优方式，研究为设计包含不确定性传达的外部界面提供了指导。

Abstract: Uncertainty is an inherent aspect of autonomous vehicle (AV) decision-making,
yet it is rarely communicated to pedestrians, which hinders transparency. This
study investigates how AV uncertainty can be conveyed through two approaches:
explicit communication (confidence percentage displays) and implicit
communication (vehicle motion cues), across different confidence levels (high
and low). Through a within-subject VR experiment (N=26), we evaluated these
approaches in a crossing scenario, assessing interface qualities (visibility
and intuitiveness), how well the information conveyed the vehicle's level of
confidence, and their impact on participants' perceived safety, trust, and user
experience. Our results show that explicit communication is more effective and
preferred for conveying uncertainty, enhancing safety, trust, and user
experience. Conversely, implicit communication introduces ambiguity, especially
when AV confidence is low. This research provides empirical insights into how
uncertainty communication shapes pedestrian interpretation of AV behaviour and
offer design guidance for external interfaces that integrate uncertainty as a
communicative element.

</details>


### [47] [A Survey on Methodological Approaches to Collaborative Embodiment in Virtual Reality](https://arxiv.org/abs/2507.18877)
*Hongyu Zhou,Yihao Dong,Masahiko Inami,Zhanna Sarsenbayeva,Anusha Withana*

Main category: cs.HC

TL;DR: 论文综述了过去十年中虚拟现实（VR）中协作体现的研究方法，分析了137篇相关论文，评估了其有效性并探讨了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过协作体现在VR中增强多用户交互和团队合作，特别是共享控制和超数手臂的交互场景。

Method: 采用PRISMA指南对137篇论文进行系统分析，比较和评估不同的协作体现方法。

Result: 总结出当前协作体现方法的有效性、挑战和局限性，为未来研究提供基础。

Conclusion: 未来研究需要进一步优化协作体现在VR中的应用，以提升多用户交互体验。

Abstract: The application and implementation of collaborative embodiment in virtual
reality (VR) are a critical aspect of the computer science landscape, aiming to
enhance multi-user interaction and teamwork in immersive environments. A
notable and enduring area of collaborative embodiment research focuses on
approaches that enable multiple users to share control, interact, and
investigate scenarios involving supernumerary arms in virtual spaces. In this
survey, we will present an extensive overview of the methodologies employed in
the past decade to enable collaboration in VR environments, particularly
through embodiment. Using the PRISMA guidelines, we plan to analyze the study
details from over 137 relevant research papers. Through this analysis, a
critical assessment of the effectiveness of these methodologies will be
conducted, highlighting current challenges and limitations in implementing
collaborative embodiment in VR. Lastly, we discuss potential future research
directions and opportunities for enhancing collaboration embodiment in virtual
environments.

</details>


### [48] [Improving the State of the Art for Training Human-AI Teams: Technical Report #5 -- Individual Differences and Team Qualities to Measure in a Human-AI Teaming Testbed](https://arxiv.org/abs/2507.18878)
*Lillian Asiala,James E. McCarthy*

Main category: cs.HC

TL;DR: Sonalysts公司计划扩展其在人类与人工智能团队合作（Human-AI teaming）领域的研究，首先开发了一个合成任务环境（STE）以支持数据收集，重点关注个体差异和团队协作质量的测量方法。


<details>
  <summary>Details</summary>
Motivation: 当前人类与人工智能团队合作的研究需要更有效的工具和方法来支持数据收集和分析，以提升团队协作效果。

Method: 通过开发合成任务环境（STE），并结合前后测试调查的方法，探索个体差异和团队协作质量的测量工具。

Result: 研究报告了多种捕获个体差异和团队协作质量的测量方法，并探讨了如何在STE中实施这些方法。

Conclusion: 该研究为Human-AI团队合作的研究提供了一个数据收集和分析的框架，支持未来的实证研究。

Abstract: Sonalysts, Inc. (Sonalysts) is working on an initiative to expand our
expertise in teaming to include Human-Artificial Intelligence (AI) teams. The
first step of this process is to develop a Synthetic Task Environment (STE) to
support our original research. Prior knowledge elicitation efforts within the
Human-AI teaming research stakeholder community revealed a desire to support
data collection using pre- and post-performance surveys. In this technical
report, we review a number of constructs that capture meaningful individual
differences and teaming qualities. Additionally, we explore methods of
measuring those constructs within the STE.

</details>


### [49] [Rethinking Accessible Prototyping Methods for Blind and Visually Impaired Passengers in Highly Automated Vehicles](https://arxiv.org/abs/2507.18880)
*Luca-Maxim Meinhardt,Enrico Rukzio*

Main category: cs.HC

TL;DR: 论文探讨了如何为非视觉界面设计提供支持，以提高盲人和视障人士在高度自动化车辆中的情境感知能力，通过两种不同的参与式设计工作坊方法进行研究。


<details>
  <summary>Details</summary>
Motivation: 盲人和视障人士（BVIPs）在高度自动化车辆（HAVs）中的情境感知能力不足，因此需要设计非视觉界面以满足他们的需求。

Method: 论文采用了两种参与式设计工作坊：第一种让参与者自行构建低保真原型，第二种让参与者评估和讨论预先提供的初始原型。

Result: 基于参与者的反馈，论文改进了最终系统设计方案，并总结了针对BVIPs的原型设计方法的经验。

Conclusion: 参与式设计工作坊是有效的方法，但仍需改进原型设计方法以更好地满足BVIPs的需求。

Abstract: Highly Automated Vehicles (HAVs) can improve mobility for blind and visually
impaired people (BVIPs). However, designing non-visual interfaces that enable
them to maintain situation awareness inside the vehicle is a challenge. This
paper presents two of our participatory design workshops that explored what
information BVIPs need in HAVs and what an interface that meets these needs
might look like. Based on the participants' insights, we created final systems
to improve their situation awareness. The two workshops used different
approaches: in the first, participants built their own low-fidelity prototypes;
in the second, they evaluated and discussed the initial prototypes we provided.
We will outline how each workshop was set up and share lessons learned about
prototyping methods for BVIPs and how they could be improved.

</details>


### [50] [Limits at a Distance: Design Directions to Address Psychological Distance in Policy Decisions Affecting Planetary Boundaries](https://arxiv.org/abs/2507.18913)
*Eshta Bhardwaj,Han Qiao,Christoph Becker*

Main category: cs.HC

TL;DR: 论文探讨了心理距离如何影响政策制定者对数据的认知，并提出了结合情感化设计和推测性设计的新方法来改进环境数据可视化。


<details>
  <summary>Details</summary>
Motivation: 传统的环境数据可视化常以中性、客观的方式呈现，但忽视了决策者的心理距离（空间、时间、社会认同和假设性）对其认知的影响。亟需一种新的设计实践来应对这一问题。

Method: 通过文献综述和综合，将心理距离理论与推测性设计及数据情感化相结合，展示情感化设计方法的价值。

Result: 提出了一种新的环境数据沟通和可视化前提，强调了替代设计方法在减少心理距离中的作用。

Conclusion: 未来的研究应探索替代设计方法对心理距离的影响，以使政策数据更具体感性和直观性。

Abstract: Policy decisions relevant to the environment rely on tools like dashboards,
risk models, and prediction models to provide information and data
visualizations that enable decision-makers to make trade-offs. The conventional
paradigm of data visualization practices for policy and decision-making is to
convey data in a supposedly neutral, objective manner for rational
decision-makers. Feminist critique advocates for nuanced and reflexive
approaches that take into account situated decision-makers and their affective
relationships to data. This paper sheds light on a key cognitive aspect that
impacts how decision-makers interpret data. Because all outcomes from policies
relevant to climate change occur at a distance, decision-makers experience
so-called `psychological distance' to environmental decisions in terms of
space, time, social identity, and hypotheticality. This profoundly impacts how
they perceive and evaluate outcomes. Since policy decisions to achieve a safe
planetary space are urgently needed for immediate transition and change, we
need a design practice that takes into account how psychological distance
affects cognition and decision-making. Our paper explores the role of
alternative design approaches in developing visualizations used for climate
policymaking. We conduct a literature review and synthesis which bridges
psychological distance with speculative design and data visceralization by
illustrating the value of affective design methods via examples from previous
research. Through this work, we propose a novel premise for the communication
and visualization of environmental data. Our paper lays out how future research
on the impacts of alternative design approaches on psychological distance can
make data used for policy decisions more tangible and visceral.

</details>


### [51] [TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models](https://arxiv.org/abs/2507.18945)
*Zijian Zhang,Pan Chen,Fangshi Du,Runlong Ye,Oliver Huang,Michael Liut,Alán Aspuru-Guzik*

Main category: cs.HC

TL;DR: TreeReader是一个基于语言模型的论文阅读工具，通过将论文分解为交互式树状结构，提供高效的信息导航和理解。


<details>
  <summary>Details</summary>
Motivation: 传统线性格式（如PDF和HTML）可能导致认知过载并掩盖论文的层次结构，而现有的LLM聊天机器人摘要工具缺乏对具体章节的深入理解且可能不可靠。

Method: 通过形成性研究分析学术阅读实践，设计TreeReader——将论文分解为树状结构，每个部分由LLM生成摘要，支持按需查看详细信息。

Result: 用户研究表明，TreeReader显著提升了阅读效率和理解能力。

Conclusion: TreeReader结合层次化摘要与交互式探索，为学术文献阅读提供了更高效且专注的方式。

Abstract: Efficiently navigating and understanding academic papers is crucial for
scientific progress. Traditional linear formats like PDF and HTML can cause
cognitive overload and obscure a paper's hierarchical structure, making it
difficult to locate key information. While LLM-based chatbots offer
summarization, they often lack nuanced understanding of specific sections, may
produce unreliable information, and typically discard the document's
navigational structure. Drawing insights from a formative study on academic
reading practices, we introduce TreeReader, a novel language model-augmented
paper reader. TreeReader decomposes papers into an interactive tree structure
where each section is initially represented by an LLM-generated concise
summary, with underlying details accessible on demand. This design allows users
to quickly grasp core ideas, selectively explore sections of interest, and
verify summaries against the source text. A user study was conducted to
evaluate TreeReader's impact on reading efficiency and comprehension.
TreeReader provides a more focused and efficient way to navigate and understand
complex academic literature by bridging hierarchical summarization with
interactive exploration.

</details>


### [52] [Rethinking Dataset Discovery with DataScout](https://arxiv.org/abs/2507.18971)
*Rachel Lin,Bhavya Chopra,Wenjing Lin,Shreya Shankar,Madelon Hulsebos,Aditya G. Parameswaran*

Main category: cs.HC

TL;DR: 论文提出了DataScout工具，以AI辅助查询重构、语义搜索和动态数据集相关性指标，改进数据集搜索流程。


<details>
  <summary>Details</summary>
Motivation: 数据集搜索在数据科学中至关重要，但现有工具在用户表达需求和完善查询方面存在不足。

Method: 开发DataScout工具，通过AI辅助查询重构、语义搜索及动态相关性指标，帮助用户更高效发现数据集。

Result: 12名参与者的研究表明，DataScout能帮助用户结构化探索查询空间并获得反馈。

Conclusion: DataScout通过多维度辅助显著提升了数据集搜索的效率和用户体验。

Abstract: Dataset Search -- the process of finding appropriate datasets for a given
task -- remains a critical yet under-explored challenge in data science
workflows. Assessing dataset suitability for a task (e.g., training a
classification model) is a multi-pronged affair that involves understanding:
data characteristics (e.g. granularity, attributes, size), semantics (e.g.,
data semantics, creation goals), and relevance to the task at hand. Present-day
dataset search interfaces are restrictive -- users struggle to convey implicit
preferences and lack visibility into the search space and result inclusion
criteria -- making query iteration challenging. To bridge these gaps, we
introduce DataScout to proactively steer users through the process of dataset
discovery via -- (i) AI-assisted query reformulations informed by the
underlying search space, (ii) semantic search and filtering based on dataset
content, including attributes (columns) and granularity (rows), and (iii)
dataset relevance indicators, generated dynamically based on the user-specified
task. A within-subjects study with 12 participants comparing DataScout to
keyword and semantic dataset search reveals that users uniquely employ
DataScout's features not only for structured explorations, but also to glean
feedback on their search queries and build conceptual models of the search
space.

</details>


### [53] [RhythmTA: A Visual-Aided Interactive System for ESL Rhythm Training via Dubbing Practice](https://arxiv.org/abs/2507.19026)
*Chang Chen,Sicheng Song,Shuchang Xu,Zhicheng Li,Huamin Qu,Yanna Lin*

Main category: cs.HC

TL;DR: RhythmTA是一个交互式系统，帮助ESL学习者通过配音独立练习英语语音节奏，提升感知和产出能力。


<details>
  <summary>Details</summary>
Motivation: 针对ESL学习者在语音节奏训练中依赖外部反馈、难以独立练习的问题。

Method: 系统通过配音自动提取节奏，并提供视觉辅助支持三阶段练习：感知、自我调整和自我监控。

Result: 用户研究表明，RhythmTA能有效提升学习者的节奏感知能力，并有望改善节奏产出。

Conclusion: RhythmTA为ESL学习者提供了一种有效的独立练习语音节奏的解决方案。

Abstract: English speech rhythm, the temporal patterns of stressed syllables, is
essential for English as a second language (ESL) learners to produce
natural-sounding and comprehensible speech. Rhythm training is generally based
on imitation of native speech. However, it relies heavily on external
instructor feedback, preventing ESL learners from independent practice. To
address this gap, we present RhythmTA, an interactive system for ESL learners
to practice speech rhythm independently via dubbing, an imitation-based
approach. The system automatically extracts rhythm from any English speech and
introduces novel visual designs to support three stages of dubbing practice:
(1) Synchronized listening with visual aids to enhance perception, (2) Guided
repeating by visual cues for self-adjustment, and (3) Comparative reflection
from a parallel view for self-monitoring. Our design is informed by a formative
study with nine spoken English instructors, which identified current practices
and challenges. A user study with twelve ESL learners demonstrates that
RhythmTA effectively enhances learners' rhythm perception and shows significant
potential for improving rhythm production.

</details>


### [54] [Exploring post-neoliberal futures for managing commercial heating and cooling through speculative praxis](https://arxiv.org/abs/2507.19072)
*Oliver Bates,Christian Remy,Kieran Cutting,Adam Tyler,Adrian Friday*

Main category: cs.HC

TL;DR: 本文通过虚构咨询公司ANCSTRL.LAB，探索如何利用系统思维和设计虚构方法来重新构想未来能源管理实践，以促进碳减排和系统变革。


<details>
  <summary>Details</summary>
Motivation: 挑战当前效率和行为改变的主流思维模式，探索如何在商业环境中通过设计实践推动碳减排和系统变革。

Method: 提出设计虚构的方法，通过虚构的未来能源咨询公司ANCSTRL.LAB，结合系统思维和人类（及超人类）中心设计，重新构想能源管理实践。

Result: 展示了如何通过设计虚构和推测实践，促进更全面的视角、系统变革的杠杆作用，以及后新自由主义未来的想象。

Conclusion: 设计虚构和推测实践可以为LIMIT研究提供工具，帮助构建新的物质现实，推动系统变革和未来能源管理的创新。

Abstract: What could designing for carbon reduction of heating and cooling in
commercial settings look like in the near future? How can we challenge dominant
mindsets and paradigms of efficiency and behaviour change? How can we help
build worlds through our practice that can become future realities? This paper
introduces the fictional consultancy ANCSTRL.LAB to explore opportunities for
making space in research projects that can encourage more systems-oriented
interventions. We present a design fiction that asks `what if energy management
and reduction practice embraced systems thinking?'. Our design fiction explores
how future energy consultancies could utilise systems thinking, and (more than)
human centred design to re-imagine energy management practice and change
systems in ways that are currently unfathomable. We finish by discussing how
LIMITS research can utilise design fiction and speculative praxis to help build
new material realities where more holistic perspectives, the leveraging of
systems change, and the imagining of post-neoliberal futures is the norm.

</details>


### [55] [Environmental (in)considerations in the Design of Smartphone Settings](https://arxiv.org/abs/2507.19094)
*Thomas Thibault,Léa Mosesso,Camille Adam,Aurélien Tabard,Anaëlle Beignon,Nolwenn Maudet*

Main category: cs.HC

TL;DR: 论文分析了数字产品设计中缺乏可持续性设置的问题，提出了六种反模式，并开发了一个设计手册，探讨环境友好设置的设计原则。


<details>
  <summary>Details</summary>
Motivation: 当前数字产品的设计普遍缺乏对环境可持续性的考虑，导致用户趋向于过度使用资源。本文旨在通过设计改进，促进更节制和可持续的数字实践。

Method: 结合可持续ICT和HCI文献，分析了三个移动操作系统和九个应用程序的环境设置，识别出六种反模式，并开发了一个设计手册。

Result: 研究发现环境设置普遍缺失或被默认设置为高资源消耗状态。设计手册提出了六项设计原则，以改进环境设置的呈现和功能。

Conclusion: 通过改进环境设置的设计，可以将个体行为与系统性因素联系起来，促进更可持续的数字实践。

Abstract: Designing for sufficiency is one of many approaches that could foster more
moderate and sustainable digital practices. Based on the Sustainable
Information and Communication Technologies (ICT) and Human-Computer Interaction
(HCI) literature, we identify five environmental settings categories. However,
our analysis of three mobile OS and nine representative applications shows an
overall lack of environmental concerns in settings design, leading us to
identify six pervasive anti-patterns. Environmental settings, where they exist,
are set on the most intensive option by default. They are not presented as
such, are not easily accessible, and offer little explanation of their impact.
Instead, they encourage more intensive use. Based on these findings, we create
a design workbook that explores design principles for environmental settings:
presenting the environmental potential of settings; shifting to environmentally
neutral states; previewing effects to encourage moderate use; rethinking
defaults; facilitating settings access and; exploring more frugal settings.
Building upon this workbook, we discuss how settings can tie individual
behaviors to systemic factors.

</details>


### [56] [A systematic literature review to unveil users objective reaction to virtual experiences: Complemented with a conceptual model (QoUX in VE)](https://arxiv.org/abs/2507.19104)
*Alireza Mortezapour,Andrea Antonio Cantone,Monica Maria Lucia Sebillo,Giuliana Vitiello*

Main category: cs.HC

TL;DR: 系统综述提出了虚拟环境(VE)中用户体验(UX)的新概念模型，包含26个独特子维度，强调了神经生理评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在记录用户在虚拟环境中的神经生理反应，并填补现有UX定义和评估工具的空白。

Method: 通过系统综述，筛选了7个数据库的1743篇文章，最终纳入66篇进行分析。

Result: 提出了包含26个子维度的UX概念模型，指出EEG和ECG常用，但还需更多神经生理评估方法。

Conclusion: 虚拟环境中的UX具有独特子维度，需结合神经生理学工具以全面理解其复杂性。

Abstract: In pursuit of documenting users Neurophysiological responses during
experiencing virtual environments (VE), this systematic review presents a novel
conceptual model of UX in VE. Searching across seven databases yielded to 1743
articles. Rigorous screenings, included only 66 articles. Notably, UX in VE
lacks a consensus definition. Obviously, this UX has many unique sub-dimensions
that are not mentioned in other products. The presented conceptual model
contains 26 subdimensions which mostly not supported in previous subjective
tools and questionnaires. While EEG and ECG were common, brain ultrasound,
employed in one study, highlights the need for using neurophysiological
assessments to comprehensively grasp immersive UX intricacies.

</details>


### [57] [A Therapeutic Role-Playing VR Game for Children with Intellectual Disabilities](https://arxiv.org/abs/2507.19114)
*Santiago Berrezueta-Guzman,WenChun Chen,Stefan Wagner*

Main category: cs.HC

TL;DR: 论文介绍了专为智障儿童设计的VR角色扮演游戏Space Exodus，通过六周测试显示其在提升专注力和认知技能方面有显著效果。


<details>
  <summary>Details</summary>
Motivation: 探索虚拟现实技术在智障儿童康复中的潜力，提供创新的治疗干预手段。

Method: 设计并开发VR游戏Space Exodus，整合沉浸式游戏与治疗任务，通过六周前测/后测研究评估效果。

Result: 定量数据显示专注力分数显著提高，定性观察显示用户信心增强和参与度提升。

Conclusion: VR游戏可作为有效的治疗工具，为智障儿童康复策略的开发提供基础。

Abstract: Virtual Reality (VR) offers promising avenues for innovative therapeutic
interventions in populations with intellectual disabilities (ID). This paper
presents the design, development, and evaluation of Space Exodus, a novel
VR-based role-playing game specifically tailored for children with ID. By
integrating immersive gameplay with therapeutic task design, Space Exodus aims
to enhance concentration, cognitive processing, and fine motor skills through
structured hand-eye coordination exercises. A six-week pre-test/post-test study
was conducted with 16 children in Ecuador, using standardized assessments, the
Toulouse-Pieron Cancellation Test, and the Moss Attention Rating Scale
complemented by detailed observational metrics. Quantitative results indicate
statistically significant improvements in concentration scores, with test
scores increasing from 65.2 to 80.3 and 55.4 to 68.7, respectively (p < 0.01).
Qualitative observations revealed reduced task attempts, enhanced user
confidence, and increased active participation. The inclusion of a VR assistant
provided consistent guidance that further boosted engagement. These findings
demonstrate the potential of immersive, game-based learning environments as
practical therapeutic tools, laying a robust foundation for developing
inclusive and adaptive rehabilitation strategies for children with ID.

</details>


### [58] [Where are the Frontlines? A Visualization Approach for Map Control in Team-Based Games](https://arxiv.org/abs/2507.19193)
*Jonas Peché,Aliaksei Tsishurou,Alexander Zap,Guenter Wallner*

Main category: cs.HC

TL;DR: 一种基于支持向量机的方法用于在线游戏中计算和可视化前线及地图控制。


<details>
  <summary>Details</summary>
Motivation: 研究在线游戏中的空间行为（如地图控制）因其复杂性难以量化与可视化。

Method: 提出利用支持向量机从单位位置推导前线，并通过可视化展示特定时间点或时间变化中的地图控制与前线。

Result: 基于《坦克世界》的示例展示了算法与可视化的有效性。

Conclusion: 该方法为在线游戏中的地图控制分析提供了量化与可视化工具。

Abstract: A central area of interest in many competitive online games is spatial
behavior which due to its complexity can be difficult to visualize. Such
behaviors of interest include not only overall movement patterns but also being
able to understand which player or team is exerting control over an area to
inform decision-making. Map control can, however, be challenging to quantify.
In this paper, we propose a method for calculating frontlines and first efforts
towards a visualization of them. The visualization can show map control and
frontlines at a specific time point or changes of these over time. For this
purpose, it utilizes support vector machines to derive frontlines from unit
positions. We illustrate our algorithm and visualization with examples based on
the team-based online game World of Tanks.

</details>


### [59] [Archiverse: an Approach for Immersive Cultural Heritage](https://arxiv.org/abs/2507.19376)
*Wieslaw Kopeć,Anna Jaskulska,Władysław Fuchs,Wiktor Stawski,Stanisław Knapiński,Barbara Karpowicz,Rafał Masłyk*

Main category: cs.HC

TL;DR: 数字技术（如激光扫描、摄影测量和混合现实）为文化遗产的研究和数字化重建提供了新方法，尤其是VR和XR技术能够复现和可视化历史文化遗产的复杂性，但也面临跨学科合作和传播的挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用VR和XR技术重现和可视化历史文化遗产，尤其是在遗迹或考古遗存仅存的情况下，以提供沉浸式互动体验。

Method: 通过激光扫描、摄影测量和混合现实（VR/XR）技术，对文化遗产进行数字化重建和可视化。

Result: 技术能够更精确地研究文化遗产，并模拟其原始复杂性，但也需要跨学科合作和技术传播的支持。

Conclusion: 数字技术为文化遗产研究提供了新工具，但跨学科协作和公众传播是成功应用的关键挑战。

Abstract: Digital technologies and tools have transformed the way we can study cultural
heritage and the way we can recreate it digitally. Techniques such as laser
scanning, photogrammetry, and a variety of Mixed Reality solutions have enabled
researchers to examine cultural objects and artifacts more precisely and from
new perspectives. In this part of the panel, we explore how Virtual Reality
(VR) and eXtended Reality (XR) can serve as tools to recreate and visualize the
remains of historical cultural heritage and experience it in simulations of its
original complexity, which means immersive and interactive. Visualization of
material culture exemplified by archaeological sites and architecture can be
particularly useful when only ruins or archaeological remains survive. However,
these advancements also bring significant challenges, especially in the area of
transdisciplinary cooperation between specialists from many, often distant,
fields, and the dissemination of virtual immersive environments among both
professionals and the general public.

</details>


### [60] [Towards Effective Immersive Technologies in Medicine: Potential and Future Applications based on VR, AR, XR and AI solutions](https://arxiv.org/abs/2507.19466)
*Aliaksandr Marozau,Barbara Karpowicz,Tomasz Kowalewski,Pavlo Zinevych,Wiktor Stawski,Adam Kuzdraliński,Wiesław Kopeć*

Main category: cs.HC

TL;DR: 论文综述了混合现实（MR）技术在医疗领域的应用，包括教育、手术、康复和心理健康等方面，同时指出当前技术的局限性，并探讨了通过新技术（如XR、AI）克服这些局限性的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管虚拟现实（VR）和增强现实（AR）技术已在医疗实践中广泛应用，但仍存在一些限制和挑战。通过引入新一代设备和框架（如XR），可能进一步优化沉浸式系统，以提升医疗培训和患者护理的安全性和可控性。

Method: 通过回顾和分析近期VR和AR在医疗领域的关键应用，包括教育模拟、手术辅助、康复治疗和心理健康干预等方面的案例。

Result: VR和AR技术在医学教育、手术精确性、功能康复和心理健康治疗中表现出显著效果。然而，高成本和有限的触觉反馈仍是主要限制。

Conclusion: 尽管VR和AR技术已在医疗领域取得成果，但仍需通过整合新技术（如XR、AI）来克服现有挑战，进一步提升医疗应用的效能和个性化。

Abstract: Mixed Reality (MR) technologies such as Virtual and Augmented Reality (VR,
AR) are well established in medical practice, enhancing diagnostics, treatment,
and education. However, there are still some limitations and challenges that
may be overcome thanks to the latest generations of equipment, software, and
frameworks based on eXtended Reality (XR) by enabling immersive systems that
support safer, more controlled environments for training and patient care. Our
review highlights recent VR and AR applications in key areas of medicine. In
medical education, these technologies provide realistic clinical simulations,
improving skills and knowledge retention. In surgery, immersive tools enhance
procedural precision with detailed anatomical visualizations. VR-based
rehabilitation has shown effectiveness in restoring motor functions and
balance, particularly for neurological patients. In mental health, VR has been
successful in treating conditions like PTSD and phobias. Although VR and AR
solutions are well established, there are still some important limitations,
including high costs and limited tactile feedback, which may be overcome with
implementing new technologies that may improve the effectiveness of immersive
medical applications such as XR, psychophysiological feedback or integration of
artificial intelligence (AI) for real-time data analysis and personalized
healthcare and training.

</details>


### [61] [IoT and Older Adults: Towards Multimodal EMG and AI-Based Interaction with Smart Home](https://arxiv.org/abs/2507.19479)
*Wiesław Kopeć,Jarosław Kowalski,Aleksander Majda,Anna Duszyk-Bogorodzka,Anna Jaskulska,Cezary Biele*

Main category: cs.HC

TL;DR: 研究探索了用于智能家居技术（SHT）的非标准非侵入式接口（如EMG/EOG），针对老年人和残障人士等群体，初步揭示了其潜力及当前技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 旨在为老年人和残障人士等群体提供更有效的智能家居技术接口，弥补现有技术讨论中的不足。

Method: 通过参与式研究，基于Sagacity原型系统，开展深度互动工作坊，调查生物电信号（EMG/EOG）作为补充接口的潜力。

Result: 初步发现EMG/EOG接口在多模态SHT管理中具有潜力，但也揭示了当前技术的限制和设计挑战。

Conclusion: 研究为未来多模态交互设计提供了方向，并建议进一步探索相关技术。

Abstract: We report preliminary insights from an exploratory study on non-standard
non-invasive interfaces for Smart Home Technologies (SHT). This study is part
of a broader research project on effective Smart Home ecosystem Sagacity that
will target older adults, impaired persons, and other groups disadvantaged in
the main technology discourse. Therefore, this research is in line with a
long-term research framework of the HASE research group (Human Aspects in
Science and Engineering) by the Living Lab Kobo. In our study, based on the
prototype of the comprehensive SHT management system Sagacity, we investigated
the potential of bioelectric signals, in particular EMG and EOG as a
complementary interface for SHT. Based on our previous participatory research
and studies on multimodal interfaces, including VUI and BCI, we prepared an
in-depth interactive hands-on experience workshops with direct involvement of
various groups of potential end users, including older adults and impaired
persons (total 18 subjects) to explore and investigate the potential of
solutions based on this type of non-standard interfaces. The preliminary
insights from the study unveil the potential of EMG/EOG interfaces in
multimodal SHT management, alongside limitations and challenges stemming from
the current state of technology and recommendations for designing multimodal
interaction paradigms pinpointing areas of interest to pursue in further
studies.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [62] [Generating real-time detailed ground visualisations from sparse aerial point clouds](https://arxiv.org/abs/2507.18664)
*Aidan Murray,Eddie Waite,Caleb Ross,Scarlet Mitchell,Alexander Bradley,Joanna Jamrozy,Kenny Mitchell*

Main category: cs.GR

TL;DR: 提出一种自动增强真实世界扫描数据的方法，用于生成高质量3D内容，降低成本和提升准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖艺术家团队，成本高且难以精确还原真实景观。

Method: 通过自动处理和渲染真实世界扫描数据，生成实时动画3D内容。

Result: 实现了高质量、近距离可探索的3D内容，适用于多种应用。

Conclusion: 该方法能有效降低成本并提升还原精度，适用于训练、模拟等领域。

Abstract: Building realistic wide scale outdoor 3D content with sufficient visual
quality to observe at walking eye level or from driven vehicles is often
carried out by large teams of artists skilled in modelling, texturing, material
shading and lighting, which typically leads to both prohibitive costs and
reduced accuracy honoring the variety of real world ground truth landscapes. In
our proposed method, we define a process to automatically amplify real-world
scanned data and render real-time in animated 3D to explore at close range with
high quality for training, simulation, video game and visualisation
applications.

</details>


### [63] [Procedural city modeling](https://arxiv.org/abs/2507.18899)
*Thomas Lechner,Ben Watson,Uri Wilensky,Martin Felsen*

Main category: cs.GR

TL;DR: 提出一种程序化生成城市的方法，强调逼真性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 旨在生成人工城市，捕捉城市发展行为，使其在任何发展阶段（从村庄到都市）都显得真实可信。

Method: 基于土地用途和建筑分布，使用代理模拟系统，通过简单行为规则实现复杂行为。

Result: 生成的城市环境逼真，支持艺术性调整，模型可扩展性强。

Conclusion: 该方法通过简单规则和代理交互，实现了逼真且可扩展的城市生成。

Abstract: We propose a method to procedurally generate a familiar yet complex human
artifact: the city. We are not trying to reproduce existing cities, but to
generate artificial cities that are convincing and plausible by capturing
developmental behavior. In addition, our results are meant to build upon
themselves, such that they ought to look compelling at any point along the
transition from village to metropolis. Our approach largely focuses upon land
usage and building distribution for creating realistic city environments,
whereas previous attempts at city modeling have mainly focused on populating
road networks. Finally, we want our model to be self automated to the point
that the only necessary input is a terrain description, but other high-level
and low-level parameters can be specified to support artistic contributions.
With the aid of agent based simulation we are generating a system of agents and
behaviors that interact with one another through their effects upon a simulated
environment. Our philosophy is that as each agent follows a simple behavioral
rule set, a more complex behavior will tend to emerge out of the interactions
between the agents and their differing rule sets. By confining our model to a
set of simple rules for each class of agents, we hope to make our model
extendible not only in regard to the types of structures that are produced, but
also in describing the social and cultural influences prevalent in all cities

</details>


### [64] [TiVy: Time Series Visual Summary for Scalable Visualization](https://arxiv.org/abs/2507.18972)
*Gromit Yeuk-Yin Chan,Luis Gustavo Nonato,Themis Palpanas,Cláudio T. Silva,Juliana Freire*

Main category: cs.GR

TL;DR: TiVy是一种新算法，通过动态时间规整（DTW）将时间序列转化为符号序列，提取频繁顺序模式以减少视觉混乱，提高可视化清晰度和速度。


<details>
  <summary>Details</summary>
Motivation: 多时间序列可视化在可扩展性和视觉清晰度之间存在权衡，现有方法难以处理长时间跨度或大量序列导致的视觉杂乱问题。

Method: TiVy使用DTW将时间序列转化为符号序列，提取相似子序列的分组，生成可视化摘要以减少重叠和小倍数。

Result: 实验表明，TiVy能提取清晰准确的模式，比普通DTW聚类快1000倍，并在大规模数据中发现隐藏结构。

Conclusion: TiVy通过新颖的分组和可视化方法，有效解决了时间序列可视化的可扩展性和清晰度问题。

Abstract: Visualizing multiple time series presents fundamental tradeoffs between
scalability and visual clarity. Time series capture the behavior of many
large-scale real-world processes, from stock market trends to urban activities.
Users often gain insights by visualizing them as line charts, juxtaposing or
superposing multiple time series to compare them and identify trends and
patterns. However, existing representations struggle with scalability: when
covering long time spans, leading to visual clutter from too many small
multiples or overlapping lines. We propose TiVy, a new algorithm that
summarizes time series using sequential patterns. It transforms the series into
a set of symbolic sequences based on subsequence visual similarity using
Dynamic Time Warping (DTW), then constructs a disjoint grouping of similar
subsequences based on the frequent sequential patterns. The grouping result, a
visual summary of time series, provides uncluttered superposition with fewer
small multiples. Unlike common clustering techniques, TiVy extracts similar
subsequences (of varying lengths) aligned in time. We also present an
interactive time series visualization that renders large-scale time series in
real-time. Our experimental evaluation shows that our algorithm (1) extracts
clear and accurate patterns when visualizing time series data, (2) achieves a
significant speed-up (1000X) compared to a straightforward DTW clustering. We
also demonstrate the efficiency of our approach to explore hidden structures in
massive time series data in two usage scenarios.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [65] [CUTHERMO: Understanding GPU Memory Inefficiencies with Heat Map Profiling](https://arxiv.org/abs/2507.18729)
*Yanbo Zhao,Jinku Cui,Zecheng Li,Shuyin Jiao,Xu Liu,Jiajia Li*

Main category: cs.DC

TL;DR: cuThermo是一个轻量级GPU内存分析工具，通过热图识别内存访问效率问题，提升性能。


<details>
  <summary>Details</summary>
Motivation: GPU内存子系统的高效利用对最大化计算性能至关重要，但现有工具缺乏细粒度内存分析支持。

Method: 开发cuThermo工具，基于GPU二进制分析运行时内存访问模式，无需修改硬件或代码。

Result: 实验表明，cuThermo能识别五种内存访问模式，性能提升最高达721.79%。

Conclusion: cuThermo为GPU内存性能优化提供了实用且高效的解决方案。

Abstract: GPUs have become indispensable in high-performance computing, machine
learning, and many other domains. Efficiently utilizing the memory subsystem on
GPUs is critical for maximizing computing power through massive parallelism.
Analyzing memory access patterns has proven to be an effective method for
understanding memory bottlenecks in applications. However, comprehensive
runtime and fine-grained memory profiling support is lacking on GPU
architectures. In this work, we introduce cuThermo, a lightweight and practical
profiling tool for GPU memory analysis. It operates on GPU binaries without
requiring any modifications to hardware, operating system, or application
source code. Given a CUDA application, cuThermo identifies memory
inefficiencies at runtime via a heat map based on distinct visited warp counts
to represent word-sector-level data sharing and provides optimization guidance
in performance tuning iterations. Through our experiments on six applications,
we identified five memory access patterns that are portable across different
GPU architectures. By evaluating optimization on two GPUs, cuThermo achieves up
to $721.79\%$ performance improvement.

</details>


### [66] [PPipe: Efficient Video Analytics Serving on Heterogeneous GPU Clusters via Pool-Based Pipeline Parallelism](https://arxiv.org/abs/2507.18748)
*Z. Jonny Kong,Qiang Xu,Y. Charlie Hu*

Main category: cs.DC

TL;DR: 论文探讨了如何在异构GPU集群上利用流水线并行技术高效服务于延迟敏感的模型推理（如视频分析系统）。通过结合不同模型层与GPU架构的多样性，低端GPU也能实现与高端GPU相当的推理延迟。文中提出的PPipe系统采用基于MILP的控制平面和资源预留式自适应批处理数据平面，显著提升了低端GPU利用率和服务吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着GPU的快速创新，异构GPU集群在公共云和本地数据中心中日益普及。然而，如何在这种环境下高效服务于延迟敏感的模型推理（如视频分析系统）仍是一个挑战。本文旨在探索流水线并行技术在此场景下的应用潜力。

Method: 论文提出了一种名为PPipe的新型推理服务系统，该系统采用基于混合整数线性规划（MILP）的控制平面实现了池式流水线并行，并通过资源预留式自适应批处理的数据平面优化资源利用。

Result: 实验结果表明，PPipe在18种CNN模型上实现了低端GPU利用率提升41.1%-65.5%，同时保持高端GPU的高利用率，服务吞吐量比基线方法高32.2%-75.1%。

Conclusion: PPipe通过有效利用低端GPU的潜力并结合流水线并行技术，显著提升了异构GPU集群在延迟敏感推理任务中的效率和吞吐量，为未来类似系统的设计提供了重要参考。

Abstract: With the rapid innovation of GPUs, heterogeneous GPU clusters in both public
clouds and on-premise data centers have become increasingly commonplace. In
this paper, we demonstrate how pipeline parallelism, a technique wellstudied
for throughput-oriented deep learning model training, can be used effectively
for serving latency-bound model inference, e.g., in video analytics systems, on
heterogeneous GPU clusters. Our work exploits the synergy between diversity in
model layers and diversity in GPU architectures, which results in comparable
inference latency for many layers when running on low-class and high-class
GPUs. We explore how such overlooked capability of low-class GPUs can be
exploited using pipeline parallelism and present a novel inference serving
system, PPipe, that employs pool-based pipeline parallelism via an MILP-based
control plane and a data plane that performs resource reservation-based
adaptive batching. Evaluation results on diverse workloads (18 CNN models) show
that PPipe achieves 41.1% - 65.5% higher utilization of low-class GPUs while
maintaining high utilization of high-class GPUs, leading to 32.2% - 75.1%
higher serving throughput compared to various baselines.

</details>


### [67] [Deadline-Aware Joint Task Scheduling and Offloading in Mobile Edge Computing Systems](https://arxiv.org/abs/2507.18864)
*Ngoc Hung Nguyen,Van-Dinh Nguyen,Anh Tuan Nguyen,Nguyen Van Thieu,Hoang Nam Nguyen,Symeon Chatzinotas*

Main category: cs.DC

TL;DR: 该论文提出了一种优化的任务调度算法，旨在提高移动边缘计算和云系统中任务调度的效率，减少延迟并优化任务卸载决策。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是为了满足移动边缘计算和云系统中对交互质量的严格要求，提升用户体验，尤其是在计算密集型任务的处理中需要达到低延迟或特定截止时间。

Method: 论文提出了一种最优任务调度算法，用于确定任务的最优顺序，并允许用户根据服务器提供的信息做出任务卸载决策。此外，还开发了一种在线方法，用于处理随机到达的任务不确定性。

Result: 算法的最优性和低复杂度（O(nlogn)）通过性能分析得到验证，在线方法的时间复杂度为O(n)。数值结果表明该算法在服务比例和调度成本方面的有效性。

Conclusion: 论文的结论是该算法能够高效优化任务调度，提升系统性能，特别是在处理随机任务时表现出色。

Abstract: The demand for stringent interactive quality-of-service has intensified in
both mobile edge computing (MEC) and cloud systems, driven by the imperative to
improve user experiences. As a result, the processing of computation-intensive
tasks in these systems necessitates adherence to specific deadlines or
achieving extremely low latency. To optimize task scheduling performance,
existing research has mainly focused on reducing the number of late jobs whose
deadlines are not met. However, the primary challenge with these methods lies
in the total search time and scheduling efficiency. In this paper, we present
the optimal job scheduling algorithm designed to determine the optimal task
order for a given set of tasks. In addition, users are enabled to make informed
decisions for offloading tasks based on the information provided by servers.
The details of performance analysis are provided to show its optimality and low
complexity with the linearithmic time O(nlogn), where $n$ is the number of
tasks. To tackle the uncertainty of the randomly arriving tasks, we further
develop an online approach with fast outage detection that achieves rapid
acceptance times with time complexity of O(n). Extensive numerical results are
provided to demonstrate the effectiveness of the proposed algorithm in terms of
the service ratio and scheduling cost.

</details>


### [68] [GPUnion: Autonomous GPU Sharing on Campus](https://arxiv.org/abs/2507.18928)
*Yufang Li,Yuanbo Zhang,Hanlong Liao,Guoming Tang,Deke Guo*

Main category: cs.DC

TL;DR: GPUnion是一个校园级GPU资源共享平台，通过自愿参与和保留提供者自主权的方式，提升了GPU利用率并解决了资源分配不均的问题。


<details>
  <summary>Details</summary>
Motivation: 校园内GPU资源分配不均，部分实验室服务器使用不足，而其他实验室则缺乏AI研究所需算力，需要一个既能共享资源又能尊重学术自主性的解决方案。

Method: GPUnion采用容器化任务调度与执行、资源提供者优先架构以及支持自动检查点迁移的弹性执行机制，同时整合了非root执行和镜像认证以提高安全性。

Result: 案例研究表明，GPUnion使GPU利用率提高30%，交互会话增加40%，并在提供者退出时实现了94%的工作负载成功迁移。

Conclusion: GPUnion证明了提供者自主权与平台可靠性可以共存，挑战了传统的集中式资源管理范式，推动了校园计算资源的民主化访问。

Abstract: A pronounced imbalance in GPU resources exists on campus, where some
laboratories own underutilized servers while others lack the compute needed for
AI research. GPU sharing can alleviate this disparity, while existing platforms
typically rely on centralized oversight and persistent allocation models,
conflicting with the voluntary and autonomous nature of academic resource
ownership. We present GPUnion, a campus-scale GPU sharing platform enabling
voluntary participation while preserving full provider autonomy. GPUnion
incorporates three core mechanisms: i) container-based task dispatching and
execution, ii) resource provider-first architecture, and iii) resilient
execution featuring automatic checkpointing and migration. GPUnion also
supports custom data storage and integrates the non-root execution and image
attestation for isolation and security improvement for containerization. Case
studies across multiple campus scenarios demonstrate 30% more GPU utilization
improvement, 40% increase in interactive sessions, and 94% successful workload
migration during provider departures. GPUnion demonstrates that provider
autonomy and platform reliability can coexist, challenging conventional
centralized paradigms and democratizing access to computational resources
within campus networks.

</details>


### [69] [The Case for Time-Shared Computing Resources](https://arxiv.org/abs/2507.19287)
*Pierre Jacquet,Adrien Luxey-Bitri*

Main category: cs.DC

TL;DR: 尽管ICT的虚拟服务不断增长，但其环境影响仍然受限于物质资源。研究提倡通过改进租户间资源分享来减少物理资源使用，并探索更高抽象层的时间共享计算模式，以提升能源效率和集群规模缩减。


<details>
  <summary>Details</summary>
Motivation: ICT行业的环境影响日益显著，而资源互用的缺失加剧了这一问题。研究旨在推动更高效的资源共享模式，以减少资源消耗和环境影响。

Method: 通过分析现有资源共享模式的不足，提出更高抽象层的时间共享计算，并探讨其可行性与挑战。

Result: 改进资源共享可以减少集群规模、提升能源效率，同时需权衡性能损失，这在社会接受度上可能优于完全取消服务。

Conclusion: 研究强调资源共享在ICT可持续发展中的重要性，并提出了未来研究方向，为资源高效利用提供了新思路。

Abstract: The environmental impact of Information and Communication Technologies (ICT)
continues to grow, driven notably by increasing usage, rebound effects, and
emerging demands. However, despite the virtual nature of its services, the
sector remains inherently constrained by its materiality and cannot rely on an
infinite pool of resources. As a result, the wide variety of supported services
may need to be managed under stricter limits within hosting facilities in the
future. Contrary to common assumptions, we show that tenants typically do not
share computing resources, even in environments commonly perceived as
mutualized, such as cloud platforms. Time-sharing has been progressively phased
out for reasons of performance, security, predictability, and, perhaps more
importantly, due to the decreasing cost of computing resources. This paper
advocates for managing fewer physical resources by improving resource sharing
between tenants. It represents a paradigm shift, moving beyond traditional
time-sharing at the hardware level to a higher abstraction. This approach
entails "doing with fewer resources" under conditions of "reduced performance".
Nonetheless, enhancing the mutualization of infrastructure can reduce cluster
sizes (through consolidation) and improve energy efficiency, with gains related
to the accepted performance trade-off, a situation potentially more socially
acceptable than eliminating services. We review the current state of the art,
identify challenges and opportunities, propose interpretations of Time-Shared
Computing, and outline key research directions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [70] [ApproxJoin: Approximate Matching for Efficient Verification in Fuzzy Set Similarity Join](https://arxiv.org/abs/2507.18891)
*Michael Mandulak,S M Ferdous,Sayan Ghosh,Mahantesh Halappanavar,George Slota*

Main category: cs.DB

TL;DR: 该论文提出了ApproxJoin方法，通过近似最大权重匹配算法优化模糊集合相似性连接问题中的验证过程，性能提升2-19倍且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决模糊集合相似性连接问题中验证成本高的问题，通过优化验证过程提升性能。

Method: 提出ApproxJoin方法，应用三种近似匹配算法（Greedy、Locally Dominant和Paz Schwartzman），替代传统的匈牙利算法。

Result: 实验表明ApproxJoin性能提升2-19倍，且保持99%的召回率。

Conclusion: 近似匹配算法在模糊集合相似性连接问题中能高效替代精确匹配，显著提升性能。

Abstract: The set similarity join problem is a fundamental problem in data processing
and discovery, relying on exact similarity measures between sets. In the
presence of alterations, such as misspellings on string data, the fuzzy set
similarity join problem instead approximately matches pairs of elements based
on the maximum weighted matching of the bipartite graph representation of sets.
State-of-the-art methods within this domain improve performance through
efficient filtering methods within the filter-verify framework, primarily to
offset high verification costs induced by the usage of the Hungarian algorithm
- an optimal matching method. Instead, we directly target the verification
process to assess the efficacy of more efficient matching methods within
candidate pair pruning.
  We present ApproxJoin, the first work of its kind in applying approximate
maximum weight matching algorithms for computationally expensive fuzzy set
similarity join verification. We comprehensively test the performance of three
approximate matching methods: the Greedy, Locally Dominant and Paz Schwartzman
methods, and compare with the state-of-the-art approach using exact matching.
Our experimental results show that ApproxJoin yields performance improvements
of 2-19x the state-of-the-art with high accuracy (99% recall).

</details>


### [71] [Big Data Energy Systems: A Survey of Practices and Associated Challenges](https://arxiv.org/abs/2507.19154)
*Lunodzo J. Mwinuka,Massimo Cafaro,Lucas Pereira,Hugo Morais*

Main category: cs.DB

TL;DR: 综述论文探讨了能源系统中大数据管理的研究趋势、实践和挑战，并提出了新技术应用的建议。


<details>
  <summary>Details</summary>
Motivation: 能源系统产生大量数据，传统数据管理方法难以应对，需要探索更先进的解决方案。

Method: 通过分析研究趋势和现有技术，总结能源系统中大数据管理的实践与挑战。

Result: 提出了数据空间、P2P数据管理和区块链等新技术，并给出了数据共享和合规的实用建议。

Conclusion: 新技术为能源系统大数据管理提供了潜力，但需进一步研究以实现高效和合规的数据管理。

Abstract: Energy systems generate vast amounts of data in extremely short time
intervals, creating challenges for efficient data management. Traditional data
management methods often struggle with scalability and accessibility, limiting
their usefulness. More advanced solutions, such as NoSQL databases and
cloud-based platforms, have been adopted to address these issues. Still, even
these advanced solutions can encounter bottlenecks, which can impact the
efficiency of data storage, retrieval, and analysis. This review paper explores
the research trends in big data management for energy systems, highlighting the
practices, opportunities and challenges. Also, the data regulatory demands are
highlighted using chosen reference architectures. The review, in particular,
explores the limitations of current storage and data integration solutions and
examines how new technologies are applied to the energy sector. Novel insights
into emerging technologies, including data spaces, various data management
architectures, peer-to-peer data management, and blockchains, are provided,
along with practical recommendations for achieving enhanced data sharing and
regulatory compliance.

</details>


### [72] [DBMS-LLM Integration Strategies in Industrial and Business Applications: Current Status and Future Challenges](https://arxiv.org/abs/2507.19254)
*Zhengtong Yan,Gongsheng Yuan,Qingsong Guo,Jiaheng Lu*

Main category: cs.DB

TL;DR: 本文综述了DBMS与LLM整合的现状及未来挑战，划分了五种架构模式，并指出关键开放性问题。


<details>
  <summary>Details</summary>
Motivation: 探索如何高效整合数据库管理系统（DBMS）和大语言模型（LLM）以支持数据驱动的智能应用。

Method: 分类五种代表性架构模式，分析其设计原则、优势和权衡。

Result: 提出了整合中的技术挑战，并识别了未解决的关键问题。

Conclusion: 为未来实现高效、可扩展的DBMS与LLM整合提供了系统化理解与研究方向。

Abstract: Modern enterprises are increasingly driven by the DATA+AI paradigm, in which
Database Management Systems (DBMSs) and Large Language Models (LLMs) have
become two foundational infrastructures powering a wide range of industrial and
business applications, such as enterprise analytics, intelligent customer
service, and data-driven decision-making. The efficient integration of DBMSs
and LLMs within a unified system offers significant opportunities but also
introduces new technical challenges. This paper surveys recent developments in
DBMS+LLM integration and identifies key future challenges. Specifically, we
categorize five representative architectural patterns based on their core
design principles, strengths, and trade-offs. Based on this analysis, we
further highlight several critical open challenges. We aim to provide a
systematic understanding of the current integration landscape and to outline
the unresolved issues that must be addressed to achieve scalable and efficient
integration of traditional data management and advanced language reasoning in
future intelligent applications.

</details>


### [73] [Properties for Paths in Graph Databases](https://arxiv.org/abs/2507.19329)
*Fernando Orejas,Elvira Pino,Renzo Angles,E. Pasarella,Nikos Milonakis*

Main category: cs.DB

TL;DR: 提出了一种用于定义图数据库中路径属性的形式化方法，可用于限制导航查询的解数量，并支持定量属性的定义和不良路径的识别与移除。


<details>
  <summary>Details</summary>
Motivation: 现有的图数据库查询方法在路径筛选和定量属性支持方面存在不足，无法有效限制查询结果或处理不良路径。

Method: 通过操作语义定义新的查询语言构造，并证明其与逻辑语义的兼容性，分析其表达能力，并实现原型进行实证分析。

Result: 路径属性比寄存器自动机更强大，且在查询性能上优于不使用路径属性的标准查询。

Conclusion: 新形式化方法不仅增强了查询的表达能力，还提升了性能，为图数据库查询提供了新的工具。

Abstract: This paper presents a formalism for defining properties of paths in graph
databases, which can be used to restrict the number of solutions to
navigational queries. In particular, our formalism allows us to define
quantitative properties such as length or accumulated cost, which can be used
as query filters. Furthermore, it enables the identification and removal of
paths that may be considered ill-formed.
  The new formalism is defined in terms of an operational semantics for the
query language that incorporates these new constructs, demonstrating its
soundness and completeness by proving its compatibility with a simple logical
semantics. We also analyze its expressive power, showing that path properties
are more expressive than register automata. Finally, after discussing some
complexity issues related to this new approach, we present an empirical
analysis carried out using our prototype implementation of the graph database
that serves as a running example throughout the paper. The results show that
queries using path properties as filters outperform standard queries that do
not use them.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [74] [RailX: A Flexible, Scalable, and Low-Cost Network Architecture for Hyper-Scale LLM Training Systems](https://arxiv.org/abs/2507.18889)
*Yinxiao Feng,Tiancheng Chen,Yuchen Wei,Siyuan Shen,Shiju Wang,Wei Li,Kaisheng Ma,Torsten Hoefler*

Main category: cs.AR

TL;DR: RailX是一种基于节点内直连和节点间电路交换的可重构网络架构，优化了大规模AI工作负载的互联性能，显著降低了成本。


<details>
  <summary>Details</summary>
Motivation: 传统互联网络架构在扩展性和成本效益上不足，无法满足大规模AI工作负载的需求，因此需要一种更高效的解决方案。

Method: RailX采用节点内直连和节点间电路交换的2D组织方式，结合Hamiltonian分解理论，将分散的环形拓扑组织成全互联拓扑。

Result: RailX可连接超过100K芯片，跳数仅为2~4，成本仅为Fat-Tree的10%~50%，适用于MLaaS场景。

Conclusion: RailX提供了一种高效、低成本且灵活的互联网络解决方案，适用于超大规模AI工作负载。

Abstract: Increasingly large AI workloads are calling for hyper-scale infrastructure;
however, traditional interconnection network architecture is neither scalable
nor cost-effective enough. Tree-based topologies such as the
\textit{Rail-optimized} network are extremely expensive, while direct
topologies such as \textit{Torus} have insufficient bisection bandwidth and
flexibility. In this paper, we propose \textit{RailX}, a reconfigurable network
architecture based on intra-node direct connectivity and inter-node circuit
switching. Nodes and optical switches are physically 2D-organized, achieving
better scalability than existing centralized circuit switching networks. We
propose a novel interconnection method based on \textit{Hamiltonian
Decomposition} theory to organize separate rail-based rings into
\textit{all-to-all} topology, simultaneously optimizing ring-collective and
all-to-all communication. More than $100$K chips with hyper bandwidth can be
interconnected with a flat switching layer, and the diameter is only $2\sim4$
inter-node hops. The network cost per injection/All-Reduce bandwidth of
\textit{RailX} is less than $10\%$ of the Fat-Tree, and the cost per
bisection/All-to-All bandwidth is less than $50\%$ of the Fat-Tree.
Specifically, only $\sim$\$$1.3$B is required to interconnect 200K chips with
1.8TB bandwidth. \textit{RailX} can also be used in the ML-as-a-service (MLaaS)
scenario, where single or multiple training workloads with various shapes,
scales, and parallelism strategies can be flexibly mapped, and failures can be
worked around.

</details>


### [75] [3DGauCIM: Accelerating Static/Dynamic 3D Gaussian Splatting via Digital CIM for High Frame Rate Real-Time Edge Rendering](https://arxiv.org/abs/2507.19133)
*Wei-Hsing Huang,Cheng-Jhih Shih,Jian-Wei Su,Samuel Wade Wang,Vaidehi Garg,Yuyao Kong,Jen-Chun Tien,Nealson Li,Arijit Raychowdhury,Meng-Fan Chang,Yingyan,Lin,Shimeng Yu*

Main category: cs.AR

TL;DR: 该论文提出了一种算法-硬件协同设计方法，解决了动态3D高斯泼溅（3DGS）在边缘设备上实现时面临的功耗和性能问题，成功实现了高帧率实时渲染。


<details>
  <summary>Details</summary>
Motivation: 动态3DGS在边缘设备上实现时面临DRAM访问开销高、排序延迟大、片上缓冲区容量不足以及与DCIM不兼容等问题，影响了实时性能和能效。

Method: 算法层面提出了减少DRAM访问的视锥剔除、自适应分块分组和自适应区间初始化排序优化；硬件层面设计了DCIM友好的计算流程，并在16nm DCIM原型芯片上验证。

Result: 实验结果表明，静态和动态大规模真实场景的渲染帧率超过200 FPS，功耗分别仅为0.28 W和0.63 W。

Conclusion: 该工作成功解决了在资源受限的边缘设备上实现静态/动态3DGS技术的关键挑战。

Abstract: Dynamic 3D Gaussian splatting (3DGS) extends static 3DGS to render dynamic
scenes, enabling AR/VR applications with moving objects. However, implementing
dynamic 3DGS on edge devices faces challenges: (1) Loading all Gaussian
parameters from DRAM for frustum culling incurs high energy costs. (2)
Increased parameters for dynamic scenes elevate sorting latency and energy
consumption. (3) Limited on-chip buffer capacity with higher parameters reduces
buffer reuse, causing frequent DRAM access. (4) Dynamic 3DGS operations are not
readily compatible with digital compute-in-memory (DCIM). These challenges
hinder real-time performance and power efficiency on edge devices, leading to
reduced battery life or requiring bulky batteries. To tackle these challenges,
we propose algorithm-hardware co-design techniques. At the algorithmic level,
we introduce three optimizations: (1) DRAM-access reduction frustum culling to
lower DRAM access overhead, (2) Adaptive tile grouping to enhance on-chip
buffer reuse, and (3) Adaptive interval initialization Bucket-Bitonic sort to
reduce sorting latency. At the hardware level, we present a DCIM-friendly
computation flow that is evaluated using the measured data from a 16nm DCIM
prototype chip. Our experimental results on Large-Scale Real-World
Static/Dynamic Datasets demonstrate the ability to achieve high frame rate
real-time rendering exceeding 200 frame per second (FPS) with minimal power
consumption, merely 0.28 W for static Large-Scale Real-World scenes and 0.63 W
for dynamic Large-Scale Real-World scenes. This work successfully addresses the
significant challenges of implementing static/dynamic 3DGS technology on
resource-constrained edge devices.

</details>


### [76] [A3D-MoE: Acceleration of Large Language Models with Mixture of Experts via 3D Heterogeneous Integration](https://arxiv.org/abs/2507.19142)
*Wei-Hsing Huang,Janak Sharda,Cheng-Jhih Shih,Yuyao Kong,Faaiq Waqar,Pin-Jun Chen,Yingyan,Lin,Shimeng Yu*

Main category: cs.AR

TL;DR: 论文提出A3D-MoE系统，通过3D异构集成技术优化Mixture-of-Experts架构，解决硬件利用率低、延迟高和能耗大的问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型（LLM）推理能耗高且成本大，而MoE架构虽高效但仍面临硬件利用率低、操作无法融合及DRAM访问能耗等问题。

Method: 提出A3D-MoE系统，包含3D异构集成技术、自适应GEMV-GEMM比例阵列、操作融合调度器和MoE分数感知HBM访问优化。

Result: A3D-MoE将延迟降低1.8x至2x，能耗减少2x至4x，吞吐量提升1.44x至1.8x。

Conclusion: A3D-MoE有效解决了MoE架构的挑战，显著提升了硬件效率和性能。

Abstract: Conventional large language models (LLMs) are equipped with dozens of GB to
TB of model parameters, making inference highly energy-intensive and costly as
all the weights need to be loaded to onboard processing elements during
computation. Recently, the Mixture-of-Experts (MoE) architecture has emerged as
an efficient alternative, promising efficient inference with less activated
weights per token. Nevertheless, fine-grained MoE-based LLMs face several
challenges: 1) Variable workloads during runtime create arbitrary GEMV-GEMM
ratios that reduce hardware utilization, 2) Traditional MoE-based scheduling
for LLM serving cannot fuse attention operations with MoE operations, leading
to increased latency and decreased hardware utilization, and 3) Despite being
more efficient than conventional LLMs, loading experts from DRAM still consumes
significant energy and requires substantial DRAM bandwidth. Addressing these
challenges, we propose: 1) A3D-MoE, a 3D Heterogeneous Integration system that
employs state-of-the-art vertical integration technology to significantly
enhance memory bandwidth while reducing Network-on-Chip (NoC) overhead and
energy consumption. 2) A 3D-Adaptive GEMV-GEMM-ratio systolic array with
V-Cache efficient data reuse and a novel unified 3D dataflow to solve the
problem of reduced hardware utilization caused by arbitrary GEMV-GEMM ratios
from different workloads, 3) A Hardware resource-aware operation fusion
scheduler that fuses attention operations with MoE operations to enhance
hardware performance, and 4) MoE Score-Aware HBM access reduction with even-odd
expert placement that reduces DRAM access and bandwidth requirements. Our
evaluation results indicate that A3D-MoE delivers significant performance
enhancements, reducing latency by a factor of 1.8x to 2x and energy consumption
by 2x to 4x, while improving throughput by 1.44x to 1.8x compared to the
state-of-the-art.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units](https://arxiv.org/abs/2507.18989)
*Maxence Bouvier,Ryan Amaudruz,Felix Arnold,Renzo Andri,Lukas Cavigelli*

Main category: cs.LG

TL;DR: GENIAL是一个基于机器学习的框架，用于自动生成和优化算术单元（如乘法器）。通过Transformer代理模型，它高效搜索最小化功耗的编码，并在实验中表现出优越的样本效率和优化能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载的增加，传统手动或启发式优化方法在探索设计空间时存在局限性，GENIAL旨在通过自动化优化提升算术单元的性能。

Method: GENIAL采用两阶段训练的Transformer代理模型（自监督预训练+有监督微调），通过反转模型高效搜索优化的操作数编码。

Result: 实验显示GENIAL样本效率高，且能发现比传统编码节省18%开关活动的编码，同时在有限状态机上也表现优异。

Conclusion: GENIAL为组合电路自动化优化提供了重要进展，展现出广泛的应用潜力。

Abstract: As AI workloads proliferate, optimizing arithmetic units is becoming
increasingly important to reduce the footprint of digital systems. Conventional
design flows, which often rely on manual or heuristics-based optimization, are
limited in their ability to thoroughly explore the vast design space. In this
paper, we introduce GENIAL, a machine learning-based framework for the
automatic generation and optimization of arithmetic units, more specifically
multipliers.
  At the core of GENIAL is a Transformer-based surrogate model trained in two
stages, involving self-supervised pretraining followed by supervised
finetuning, to robustly forecast key hardware metrics such as power and area
from abstracted design representations. By inverting the surrogate model,
GENIAL efficiently searches for new operand encodings that directly minimize
power consumption in arithmetic units for specific input data distributions.
Extensive experiments on large datasets demonstrate that GENIAL is consistently
more sample efficient than other methods, and converges faster towards
optimized designs. This enables to deploy a high-effort logic synthesis
optimization flow in the loop, improving the accuracy of the surrogate model.
Notably, GENIAL automatically discovers encodings that achieve up to 18%
switching activity savings within multipliers on representative AI workloads
compared with the conventional two's complement. We also demonstrate the
versatility of our approach by achieving significant improvements on Finite
State Machines, highlighting GENIAL's applicability for a wide spectrum of
logic functions. Together, these advances mark a significant step toward
automated Quality-of-Results-optimized combinational circuit generation for
digital systems.

</details>


### [78] [SILS: Strategic Influence on Liquidity Stability and Whale Detection in Concentrated-Liquidity DEXs](https://arxiv.org/abs/2507.19411)
*Ali RajabiNekoo,Laleh Rasoul,Amirfarhad Farhadi,Azadeh Zamanifar*

Main category: cs.LG

TL;DR: SILS框架通过动态、影响导向的分析方法，超越了传统基于名义资本或表面活动的流动性提供者（LP）评估，提供了更精确的风险管理。


<details>
  <summary>Details</summary>
Motivation: 传统方法在识别CLMMs中高影响力LP时存在不准确问题，SILS旨在提供更详细和市场稳定的动态分析。

Method: 利用链上事件日志和智能合约执行轨迹，计算ETWL配置文件，并应用无监督异常检测，定义LSIS衡量LP退出对市场的潜在影响。

Result: SILS能更准确地识别高影响力LP，包括传统方法遗漏的，并支持DeFi生态系统中的保护性预言层和可操作交易信号。

Conclusion: SILS通过动态和全面的方法，显著提升了DeFi生态系统的风险管理和透明度，减少了传统模型的误判。

Abstract: Traditional methods for identifying impactful liquidity providers (LPs) in
Concentrated Liquidity Market Makers (CLMMs) rely on broad measures, such as
nominal capital size or surface-level activity, which often lead to inaccurate
risk analysis. The SILS framework offers a significantly more detailed
approach, characterizing LPs not just as capital holders but as dynamic
systemic agents whose actions directly impact market stability. This represents
a fundamental paradigm shift from the static, volume-based analysis to a
dynamic, impact-focused understanding. This advanced approach uses on-chain
event logs and smart contract execution traces to compute Exponential
Time-Weighted Liquidity (ETWL) profiles and apply unsupervised anomaly
detection. Most importantly, it defines an LP's functional importance through
the Liquidity Stability Impact Score (LSIS), a counterfactual metric that
measures the potential degradation of the market if the LP withdraws. This
combined approach provides a more detailed and realistic characterization of an
LP's impact, moving beyond the binary and often misleading classifications used
by existing methods. This impact-focused and comprehensive approach enables
SILS to accurately identify high-impact LPs-including those missed by
traditional methods and supports essential applications like a protective
oracle layer and actionable trader signals, thereby significantly enhancing
DeFi ecosystem. The framework provides unprecedented transparency into the
underlying liquidity structure and associated risks, effectively reducing the
common false positives and uncovering critical false negatives found in
traditional models. Therefore, SILS provides an effective mechanism for
proactive risk management, transforming how DeFi protocols safeguard their
ecosystems against asymmetric liquidity behavior.

</details>


### [79] [Reconstruction of Sparse Urban Wireless Signals via Group Equivariant Non-Expansive Operators](https://arxiv.org/abs/2507.19349)
*Lorenzo Mario Amorosa,Francesco Conti,Nicola Quercioli,Flavio Zabini,Tayebeh Lotfi Mahyari,Yiqun Ge,Patrizio Frosini*

Main category: cs.LG

TL;DR: 利用群等变非扩张算子（GENEOs）从稀疏测量中重建6G无线网络中的信号干扰噪声比（SINR）地图，相较于传统神经网络更高效且低复杂度。


<details>
  <summary>Details</summary>
Motivation: 高分辨率SINR地图获取成本高，需要一种低复杂度方法在稀疏数据下准确重建空间信号。

Method: 采用GENEOs，利用其代数与几何约束减少参数数量并增强数据稀缺情况下的性能。

Result: GENEOs在稀疏采样下表现优异，通过统计和拓扑数据分析指标验证了其竞争力。

Conclusion: GENEOs为6G网络中空间信号重建提供了高效且可扩展的解决方案。

Abstract: In emerging communication systems such as sixth generation (6G) wireless
networks, efficient resource management and service delivery rely on accurate
knowledge of spatially-varying quantities like signal-to-interference-noise
ratio (SINR) maps, which are costly to acquire at high resolution. This work
explores the reconstruction of such spatial signals from sparse measurements
using Group Equivariant Non-Expansive Operators (GENEOs), offering a
low-complexity alternative to traditional neural networks. The concept of
GENEO, which originated in topological data analysis (TDA), is a mathematical
tool used in machine learning to represent agents modelled as functional
operators acting on data while incorporating application-specific invariances.
Leveraging these invariances reduces the number of parameters with respect to
traditional neural networks and mitigates data scarcity by enforcing known
algebraic and geometric constraints that reflect symmetries in the agents'
actions. In this paper, we introduce a novel GENEO-based approach for SINR map
reconstruction in urban wireless communication networks using extremely sparse
sampling. We demonstrate that this mathematical framework achieves competitive
performance compared to established methods. Our evaluation, conducted using
both statistical and TDA metrics, highlights the advantages of our approach in
accurately reconstructing spatial signals under severe data limitations on the
number of samples.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [80] [Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet](https://arxiv.org/abs/2507.19209)
*Xiaoyu Zhang,Zhifeng Bao,Hai Dong,Ziwei Wang,Jiajun Liu*

Main category: cs.CV

TL;DR: 本文提出了CounterNet，一种基于热图的网络，旨在提高大规模点云数据中的物体计数准确性，从而支持更可靠的查询结果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆生成的海量点云数据中，只有部分对特定任务（如碰撞检测、交通分析）相关。现有方法在3D点云数据中难以提供可靠的物体计数，导致查询结果错误。

Method: 提出CounterNet，通过检测物体中心而非精确定位来提高计数准确性，采用特征图分区策略和动态模型选择。

Result: 在三个真实数据集上，CounterNet将计数准确性提高了5%至20%，显著改善了查询结果的可靠性。

Conclusion: CounterNet通过优化计数方法，显著提升了点云数据查询的准确性和实用性。

Abstract: Autonomous vehicles generate massive volumes of point cloud data, yet only a
subset is relevant for specific tasks such as collision detection, traffic
analysis, or congestion monitoring. Effectively querying this data is essential
to enable targeted analytics. In this work, we formalize point cloud querying
by defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each
aligned with distinct analytical scenarios. All these queries rely heavily on
accurate object counts to produce meaningful results, making precise object
counting a critical component of query execution. Prior work has focused on
indexing techniques for 2D video data, assuming detection models provide
accurate counting information. However, when applied to 3D point cloud data,
state-of-the-art detection models often fail to generate reliable object
counts, leading to substantial errors in query results. To address this
limitation, we propose CounterNet, a heatmap-based network designed for
accurate object counting in large-scale point cloud data. Rather than focusing
on accurate object localization, CounterNet detects object presence by finding
object centers to improve counting accuracy. We further enhance its performance
with a feature map partitioning strategy using overlapping regions, enabling
better handling of both small and large objects in complex traffic scenes. To
adapt to varying frame characteristics, we introduce a per-frame dynamic model
selection strategy that selects the most effective configuration for each
input. Evaluations on three real-world autonomous vehicle datasets show that
CounterNet improves counting accuracy by 5% to 20% across object categories,
resulting in more reliable query outcomes across all supported query types.

</details>


### [81] [A New One-Shot Federated Learning Framework for Medical Imaging Classification with Feature-Guided Rectified Flow and Knowledge Distillation](https://arxiv.org/abs/2507.19045)
*Yufei Ma,Hanwen Zhang,Qiya Yang,Guibo Luo,Yuesheng Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种改进的单次联邦学习框架（OSFL），通过特征引导校正流模型（FG-RF）和双层知识蒸馏（DLKD）方法解决了医疗领域中的训练效率低、隐私泄露问题以及在非独立同分布（non-IID）数据下的单轮收敛挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型基于OSFL方法在医疗领域中存在训练效率低、隐私泄露风险高以及在非IID数据下难以实现单轮收敛的问题。

Method: 提出FG-RF模型和DLKD聚合方法：FG-RF通过在客户端生成特征级而非像素级图像加速建模并保护隐私；DLKD通过让全局学生模型同时模仿输出逻辑和对齐中间层特征来处理非IID数据。

Result: 在三个非IID医疗影像数据集上的实验表明，新框架性能优于多轮联邦学习方法，提升高达21.73%，平均超过FedISCA基线21.75%。特征级合成图像显著降低了隐私泄露风险。

Conclusion: 改进的OSFL框架有效解决了训练效率、隐私保护和数据非IID分布问题，在医疗领域具有显著优势。

Abstract: In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted
increasing attention due to its low communication overhead, requiring only a
single round of transmission. However, existing generative model-based OSFL
methods suffer from low training efficiency and potential privacy leakage in
the healthcare domain. Additionally, achieving convergence within a single
round of model aggregation is challenging under non-Independent and Identically
Distributed (non-IID) data. To address these challenges, in this paper a
modified OSFL framework is proposed, in which a new Feature-Guided Rectified
Flow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation
method are developed. FG-RF on the client side accelerates generative modeling
in medical imaging scenarios while preserving privacy by synthesizing
feature-level images rather than pixel-level images. To handle non-IID
distributions, DLKD enables the global student model to simultaneously mimic
the output logits and align the intermediate-layer features of client-side
teacher models during aggregation. Experimental results on three non-IID
medical imaging datasets show that our new framework and method outperform
multi-round federated learning approaches, achieving up to 21.73% improvement,
and exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our
experiments demonstrate that feature-level synthetic images significantly
reduce privacy leakage risks compared to pixel-level synthetic images.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [82] [A Comprehensive Review of AI-based Intelligent Tutoring Systems: Applications and Challenges](https://arxiv.org/abs/2507.18882)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.IR

TL;DR: 本文通过系统文献综述方法，研究了2010至2025年间关于智能辅导系统（ITS）的文献，分析了其在实际教育环境中的运作及挑战，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨智能辅导系统（ITS）在教育中的有效性及面临的挑战，为其改进和应用提供科学依据。

Method: 采用系统文献综述方法，分析2010至2025年间关于ITS的合格研究，涵盖教学策略、自然语言处理、自适应学习等领域。

Result: 揭示了ITS有效性的复杂性，同时指出实验中需要更强的科学严谨性。

Conclusion: 提出了未来研究的建议，强调改进实验设计和数据分析的重要性。

Abstract: AI-based Intelligent Tutoring Systems (ITS) have significant potential to
transform teaching and learning. As efforts continue to design, develop, and
integrate ITS into educational contexts, mixed results about their
effectiveness have emerged. This paper provides a comprehensive review to
understand how ITS operate in real educational settings and to identify the
associated challenges in their application and evaluation. We use a systematic
literature review method to analyze numerous qualified studies published from
2010 to 2025, examining domains such as pedagogical strategies, NLP, adaptive
learning, student modeling, and domain-specific applications of ITS. The
results reveal a complex landscape regarding the effectiveness of ITS,
highlighting both advancements and persistent challenges. The study also
identifies a need for greater scientific rigor in experimental design and data
analysis. Based on these findings, suggestions for future research and
practical implications are proposed.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [83] [Autocallable Options Pricing with Integration-Based Exponential Amplitude Loading](https://arxiv.org/abs/2507.19039)
*Francesca Cibrario,Ron Cohen,Emanuele Dri,Christian Mattia,Or Samimi Golan,Tamuz Danzig,Giacomo Ranieri,Hanan Rosemarin,Davide Corbelletto,Amir Naveh,Bartolomeo Montrucchio*

Main category: quant-ph

TL;DR: 提出了一种专为定价自动赎回期权设计的量子算法，展示了完整实现和实验验证，包括在高性能计算硬件上的模拟及收敛性分析。


<details>
  <summary>Details</summary>
Motivation: 旨在开发更高效的量子方法定价复杂金融衍生品。

Method: 采用改进的基于积分的指数振幅加载技术，减少电路深度。

Result: 与现有方法相比，在相关场景中T深度减少了约50倍。

Conclusion: 为量子方法在金融衍生品定价中的应用提供了更高效的解决方案。

Abstract: We present a comprehensive quantum algorithm tailored for pricing
autocallable options, offering a full implementation and experimental
validation. Our experiments include simulations conducted on high-performance
computing (HPC) hardware, along with an empirical analysis of convergence to
the classically estimated value. Our key innovation is an improved
integration-based exponential amplitude loading technique that reduces circuit
depth compared to state-of-the-art approaches. A detailed complexity analysis
in a relevant setting shows an approximately 50x reduction in T-depth for the
payoff component relative to previous methods. These contributions represent a
step toward more efficient quantum approaches to pricing complex financial
derivatives.

</details>


### [84] [Implementing Credit Risk Analysis with Quantum Singular Value Transformation](https://arxiv.org/abs/2507.19206)
*Davide Veronelli,Francesca Cibrario,Emanuele Dri,Valeria Zaffaroni,Giacomo Ranieri,Davide Corbelletto,Bartolomeo Montrucchio*

Main category: quant-ph

TL;DR: 论文提出利用量子奇异值变换（QSVT）降低信用风险分析中量子振幅估计（QAE）的实现成本，并通过模拟验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 信用风险分析对金融机构至关重要，但现有量子电路在实现效率上存在限制，尤其是高昂的算术运算成本。

Method: 采用量子奇异值变换（QSVT）优化状态准备算子的实现成本，并提供端到端的代码实现和模拟研究。

Result: 模拟研究表明，该方法能够显著降低QAE的实现成本，并验证了其有效性。

Conclusion: 论文提出的QSVT方法为信用风险分析中的量子计算提供了更高效的解决方案。

Abstract: The analysis of credit risk is crucial for the efficient operation of
financial institutions. Quantum Amplitude Estimation (QAE) offers the potential
for a quadratic speed-up over classical methods used to estimate metrics such
as Value at Risk (VaR) and Conditional Value at Risk (CVaR). However, numerous
limitations remain in efficiently scaling the implementation of quantum
circuits that solve these estimation problems. One of the main challenges is
the use of costly and restrictive arithmetic that must be implemented within
the quantum circuit. In this paper, we propose using Quantum Singular Value
Transformation (QSVT) to significantly reduce the cost of implementing the
state preparation operator, which underlies QAE for credit risk analysis. We
also present an end-to-end code implementation and the results of a simulation
study to validate the proposed approach and demonstrate its benefits.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [85] [Comparing OCR Pipelines for Folkloristic Text Digitization](https://arxiv.org/abs/2507.19092)
*Octavian M. Machidon,Alina L. Machidon*

Main category: cs.DL

TL;DR: 研究探索了针对斯洛文尼亚民俗和历史文本的数字化的不同OCR方法，结合传统技术和大型语言模型（LLM）以提高转录准确性，同时保持语言和结构的完整性。


<details>
  <summary>Details</summary>
Motivation: 数字化历史民俗材料面临多样化的文本布局、印刷和手写风格及语言变体的独特挑战，需要改进OCR技术以应对这些挑战。

Method: 比较了单阶段OCR技术和多阶段管道，后者结合机器学习驱动的后处理以实现文本规范化和布局重建，并采用LLM优化识别结果。

Result: LLM增强的方法在优化识别结果和提高可读性方面表现出潜力，但也带来了对方言表达和历史结构保存的挑战。

Conclusion: 研究为大规模民俗档案的数字策略选择提供了见解，并建议开发平衡自动化和文本真实性的OCR管道。

Abstract: The digitization of historical folkloristic materials presents unique
challenges due to diverse text layouts, varying print and handwriting styles,
and linguistic variations. This study explores different optical character
recognition (OCR) approaches for Slovene folkloristic and historical text
digitization, integrating both traditional methods and large language models
(LLMs) to improve text transcription accuracy while maintaining linguistic and
structural integrity. We compare single-stage OCR techniques with multi-stage
pipelines that incorporate machine learning-driven post-processing for text
normalization and layout reconstruction. While LLM-enhanced methods show
promise in refining recognition outputs and improving readability, they also
introduce challenges related to unintended modifications, particularly in the
preservation of dialectal expressions and historical structures. Our findings
provide insights into selecting optimal digitization strategies for large-scale
folklore archives and outline recommendations for developing robust OCR
pipelines that balance automation with the need for textual authenticity in
digital humanities research.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [86] [LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation](https://arxiv.org/abs/2507.18940)
*Jingxuan Wei,Caijun Jia,Qi Chen,Yujun Cai,Linzhuang Sun,Xiangxiang Zhang,Gaowei Wu,Bihui Yu*

Main category: cs.CL

TL;DR: LLaVA-NeuMT是一种多模态多语言翻译框架，通过分层选择和神经元级动态适应策略，减少跨语言干扰，提升翻译质量，且在仅微调40%参数的情况下超越全微调方法。


<details>
  <summary>Details</summary>
Motivation: 多模态多语言翻译中，跨语言干扰和参数共享策略效果不佳，限制了翻译性能的提升。

Method: 提出LLaVA-NeuMT框架，包含分层选择机制和神经元级动态适应策略，明确区分语言相关和无关表征。

Result: 在M3-Multi30K和M3-AmbigCaps数据集上实现SOTA效果，仅微调40%参数即超越全微调方法。

Conclusion: LLaVA-NeuMT为多模态多语言翻译提供了一种高效且可扩展的解决方案，同时揭示了关键层和神经元的重要性。

Abstract: Multimodal Machine Translation (MMT) enhances translation quality by
incorporating visual context, helping to resolve textual ambiguities. While
existing MMT methods perform well in bilingual settings, extending them to
multilingual translation remains challenging due to cross-lingual interference
and ineffective parameter-sharing strategies. To address this, we propose
LLaVA-NeuMT, a novel multimodal multilingual translation framework that
explicitly models language-specific and language-agnostic representations to
mitigate multilingual interference. Our approach consists of a layer selection
mechanism that identifies the most informative layers for different language
pairs and a neuron-level adaptation strategy that dynamically selects
language-specific and agnostic neurons to improve translation quality while
reducing redundancy. We conduct extensive experiments on the M3-Multi30K and
M3-AmbigCaps datasets, demonstrating that LLaVA-NeuMT, while fine-tuning only
40\% of the model parameters, surpasses full fine-tuning approaches and
ultimately achieves SOTA results on both datasets. Our analysis further
provides insights into the importance of selected layers and neurons in
multimodal multilingual adaptation, offering an efficient and scalable solution
to cross-lingual adaptation in multimodal translation.

</details>


### [87] [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457)
*Lakshya A Agrawal,Shangyin Tan,Dilara Soylu,Noah Ziems,Rishi Khare,Krista Opsahl-Ong,Arnav Singhvi,Herumb Shandilya,Michael J Ryan,Meng Jiang,Christopher Potts,Koushik Sen,Alexandros G. Dimakis,Ion Stoica,Dan Klein,Matei Zaharia,Omar Khattab*

Main category: cs.CL

TL;DR: GEPA是一种结合自然语言反思的提示优化器，通过少量尝试即可显著提升模型性能，优于GRPO和MIPROv2。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如GRPO）需要大量尝试来学习新任务，而语言的解释性可能为LLMs提供更丰富的学习媒介。

Method: GEPA通过自然语言反思从试验中学习高层规则，诊断问题并提出提示更新，结合帕累托前沿的互补经验。

Result: 在四个任务中，GEPA平均比GRPO高10%，最高达20%，且尝试次数少35倍；比MIPROv2高10%，且在代码优化中表现优异。

Conclusion: GEPA通过自然语言反思显著提升模型性能，展示了其在提示优化和代码搜索中的潜力。

Abstract: Large language models (LLMs) are increasingly adapted to downstream tasks via
reinforcement learning (RL) methods like Group Relative Policy Optimization
(GRPO), which often require thousands of rollouts to learn new tasks. We argue
that the interpretable nature of language can often provide a much richer
learning medium for LLMs, compared with policy gradients derived from sparse,
scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt
optimizer that thoroughly incorporates natural language reflection to learn
high-level rules from trial and error. Given any AI system containing one or
more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool
calls, and tool outputs) and reflects on them in natural language to diagnose
problems, propose and test prompt updates, and combine complementary lessons
from the Pareto frontier of its own attempts. As a result of GEPA's design, it
can often turn even just a few rollouts into a large quality gain. Across four
tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up
to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,
MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an
inference-time search strategy for code optimization.

</details>


### [88] [Large language models provide unsafe answers to patient-posed medical questions](https://arxiv.org/abs/2507.18905)
*Rachel L. Draelos,Samina Afreen,Barbara Blasko,Tiffany Brazile,Natasha Chase,Dimple Desai,Jessica Evert,Heather L. Gardner,Lauren Herrmann,Aswathy Vaikom House,Stephanie Kass,Marianne Kavan,Kirshma Khemani,Amanda Koire,Lauren M. McDonald,Zahraa Rabeeah,Amy Shah*

Main category: cs.CL

TL;DR: 研究比较了四种公开AI聊天机器人在医疗建议中的安全性，发现不同机器人的问题回答率和不安全回答率存在显著差异，提出需进一步改进其临床安全性。


<details>
  <summary>Details</summary>
Motivation: 评估AI聊天机器人在提供医疗建议时的安全性，以解决患者安全担忧。

Method: 通过红队测试，使用新数据集HealthAdvice对四种聊天机器人（Claude、Gemini、GPT-4o、Llama3-70B）的888个回答进行定量和定性分析。

Result: 问题回答率从21.6%（Claude）到43.2%（Llama），不安全回答率从5%（Claude）到13%（GPT-4o、Llama）。

Conclusion: 当前公开聊天机器人可能为患者提供不安全医疗建议，需进一步提升其临床安全性。

Abstract: Millions of patients are already using large language model (LLM) chatbots
for medical advice on a regular basis, raising patient safety concerns. This
physician-led red-teaming study compares the safety of four publicly available
chatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and
Llama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation
framework that enables quantitative and qualitative analysis. In total, 888
chatbot responses are evaluated for 222 patient-posed advice-seeking medical
questions on primary care topics spanning internal medicine, women's health,
and pediatrics. We find statistically significant differences between chatbots.
The rate of problematic responses varies from 21.6 percent (Claude) to 43.2
percent (Llama), with unsafe responses varying from 5 percent (Claude) to 13
percent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the
potential to lead to serious patient harm. This study suggests that millions of
patients could be receiving unsafe medical advice from publicly available
chatbots, and further work is needed to improve the clinical safety of these
powerful tools.

</details>


### [89] [An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case](https://arxiv.org/abs/2507.19156)
*Gioele Giachino,Marco Rondina,Antonio Vetrò,Riccardo Coppola,Juan Carlos De Martin*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在非英语语言（意大利语）中如何通过非性别化提示生成性别和职业偏见内容，指出LLMs可能加剧社会不平等。


<details>
  <summary>Details</summary>
Motivation: LLMs广泛使用引发了对生成偏见内容的担忧，尤其在性别和职业领域。研究聚焦于意大利语，评估LLMs在非英语语言中的偏见问题。

Method: 采用结构化实验方法，测试两种LLM（ChatGPT和Gemini）对非性别化提示的响应，收集3600条回复数据。

Result: 结果显示LLMs（如Gemini 100%和ChatGPT 97%）倾向于将性别代词与刻板职业角色关联，表明偏见普遍存在。

Conclusion: LLMs的偏见可能对社会多领域产生负面影响，需开发缓解策略以确保公平。未来可扩展研究至更多语言或模型。

Abstract: The increasing use of Large Language Models (LLMs) in a large variety of
domains has sparked worries about how easily they can perpetuate stereotypes
and contribute to the generation of biased content. With a focus on gender and
professional bias, this work examines in which manner LLMs shape responses to
ungendered prompts, contributing to biased outputs. This analysis uses a
structured experimental method, giving different prompts involving three
different professional job combinations, which are also characterized by a
hierarchical relationship. This study uses Italian, a language with extensive
grammatical gender differences, to highlight potential limitations in current
LLMs' ability to generate objective text in non-English languages. Two popular
LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google
Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600
responses. The results highlight how content generated by LLMs can perpetuate
stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'
pronouns to the 'assistant' rather than the 'manager'. The presence of bias in
AI-generated text can have significant implications in many fields, such as in
the workplaces or in job selections, raising ethical concerns about its use.
Understanding these risks is pivotal to developing mitigation strategies and
assuring that AI-based systems do not increase social inequalities, but rather
contribute to more equitable outcomes. Future research directions include
expanding the study to additional chatbots or languages, refining prompt
engineering methods or further exploiting a larger experimental base.

</details>


### [90] [Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models](https://arxiv.org/abs/2507.19470)
*Son Quoc Tran,Tushaar Gangavarapu,Nicholas Chernogor,Jonathan P. Chang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: 这篇论文提出了一个统一的评估框架，用于比较对话预测模型在‘Conversations Gone Awry’任务中的表现，并引入了一种新指标来捕捉模型修正预测的能力。


<details>
  <summary>Details</summary>
Motivation: 直觉在对话中起重要作用，赋予自动化系统类似的预见性可以辅助人际互动。研究聚焦于预测对话是否会偏离正轨的任务，并试图通过统一框架评估模型。

Method: 作者引入了一个统一的评估框架和基准，支持不同架构的直接比较，并提出了一个新指标以衡量模型随对话进展修正预测的能力。

Result: 框架提供了对当前对话预测模型进展的全面概述，并通过新指标评估了模型的动态预测能力。

Conclusion: 统一的评估框架和新指标为对话预测模型的未来发展提供了可靠的基础和新的研究方向。

Abstract: We often rely on our intuition to anticipate the direction of a conversation.
Endowing automated systems with similar foresight can enable them to assist
human-human interactions. Recent work on developing models with this predictive
capacity has focused on the Conversations Gone Awry (CGA) task: forecasting
whether an ongoing conversation will derail. In this work, we revisit this task
and introduce the first uniform evaluation framework, creating a benchmark that
enables direct and reliable comparisons between different architectures. This
allows us to present an up-to-date overview of the current progress in CGA
models, in light of recent advancements in language modeling. Our framework
also introduces a novel metric that captures a model's ability to revise its
forecast as the conversation progresses.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [91] [Learned Image Compression with Hierarchical Progressive Context Modeling](https://arxiv.org/abs/2507.19125)
*Yuqi Li,Haotian Zhang,Li Li,Dong Liu*

Main category: eess.IV

TL;DR: 论文提出了一种分层渐进式上下文模型（HPCM），通过多尺度上下文依赖建模和渐进式上下文融合机制，提升了图像压缩中长范围依赖和多样化上下文信息的利用效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像压缩中难以高效利用长范围依赖和多样化上下文信息，限制了压缩性能的提升。

Method: 提出HPCM，采用分层编码计划多尺度建模上下文依赖，并引入渐进式上下文融合机制整合历史上下文信息。

Result: 实验表明，HPCM在率失真性能上达到最优，并在压缩性能和计算复杂度间取得了更好平衡。

Conclusion: HPCM显著提升了图像压缩中上下文建模的效率，具有实际应用潜力。代码已开源。

Abstract: Context modeling is essential in learned image compression for accurately
estimating the distribution of latents. While recent advanced methods have
expanded context modeling capacity, they still struggle to efficiently exploit
long-range dependency and diverse context information across different coding
steps. In this paper, we introduce a novel Hierarchical Progressive Context
Model (HPCM) for more efficient context information acquisition. Specifically,
HPCM employs a hierarchical coding schedule to sequentially model the
contextual dependencies among latents at multiple scales, which enables more
efficient long-range context modeling. Furthermore, we propose a progressive
context fusion mechanism that incorporates contextual information from previous
coding steps into the current step, effectively exploiting diverse contextual
information. Experimental results demonstrate that our method achieves
state-of-the-art rate-distortion performance and strikes a better balance
between compression performance and computational complexity. The code is
available at https://github.com/lyq133/LIC-HPCM.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery](https://arxiv.org/abs/2507.16229)
*Bo Wen,Chen Wang,Qiwei Han,Raquel Norel,Julia Liu,Thaddeus Stappenbeck,Jeffrey L. Rogers*

Main category: cs.AI

TL;DR: voice-based AI agents in healthcare can bridge accessibility gaps，提高预防护理效率，尤其在服务不足人群中。研究发现多数患者接受AI监测。技术挑战和政策问题需解决以实现可持续、公平的解决方案。


<details>
  <summary>Details</summary>
Motivation: 探索语音AI在医疗中的作用，解决经济与可及性问题，特别是在弱势群体中。

Method: 开发Agent PULSE系统并进行试点研究，分析经济模型和技术挑战。

Result: 70%患者接受AI监测，37%更爱AI而非传统方式。AI可显著降低成本并提升效率。

Conclusion: 语音AI在医疗中潜力巨大，但需解决技术、政策和伦理问题以实现可持续应用。

Abstract: The integration of voice-based AI agents in healthcare presents a
transformative opportunity to bridge economic and accessibility gaps in digital
health delivery. This paper explores the role of large language model
(LLM)-powered voice assistants in enhancing preventive care and continuous
patient monitoring, particularly in underserved populations. Drawing insights
from the development and pilot study of Agent PULSE (Patient Understanding and
Liaison Support Engine) -- a collaborative initiative between IBM Research,
Cleveland Clinic Foundation, and Morehouse School of Medicine -- we present an
economic model demonstrating how AI agents can provide cost-effective
healthcare services where human intervention is economically unfeasible. Our
pilot study with 33 inflammatory bowel disease patients revealed that 70\%
expressed acceptance of AI-driven monitoring, with 37\% preferring it over
traditional modalities. Technical challenges, including real-time
conversational AI processing, integration with healthcare systems, and privacy
compliance, are analyzed alongside policy considerations surrounding
regulation, bias mitigation, and patient autonomy. Our findings suggest that
AI-driven voice agents not only enhance healthcare scalability and efficiency
but also improve patient engagement and accessibility. For healthcare
executives, our cost-utility analysis demonstrates huge potential savings for
routine monitoring tasks, while technologists can leverage our framework to
prioritize improvements yielding the highest patient impact. By addressing
current limitations and aligning AI development with ethical and regulatory
frameworks, voice-based AI agents can serve as a critical entry point for
equitable, sustainable digital healthcare solutions.

</details>


### [93] [Initial Steps in Integrating Large Reasoning and Action Models for Service Composition](https://arxiv.org/abs/2507.18775)
*Ilche Georgievski,Marco Aiello*

Main category: cs.AI

TL;DR: 摘要探讨了大型语言模型中两大新兴范式——大型推理模型（LRM）和大型动作模型（LAM）——在服务组合中的整合潜力，指出各自的优势与局限，并提出一个集成框架以实现自动化服务组合。


<details>
  <summary>Details</summary>
Motivation: 解决服务组合中语义推理和动态执行的挑战，提升系统适应性和智能性。

Method: 提出集成LRM和LAM的架构框架，结合推理与执行能力。

Result: 通过整合LRM和LAM，有望实现基于自然语言意图的自动化服务组合。

Conclusion: LRM-LAM框架是服务组合自动化发展的一个有前景的方向。

Abstract: Service composition remains a central challenge in building adaptive and
intelligent software systems, often constrained by limited reasoning
capabilities or brittle execution mechanisms. This paper explores the
integration of two emerging paradigms enabled by large language models: Large
Reasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs
address the challenges of semantic reasoning and ecosystem complexity while
LAMs excel in dynamic action execution and system interoperability. However,
each paradigm has complementary limitations - LRMs lack grounded action
capabilities, and LAMs often struggle with deep reasoning. We propose an
integrated LRM-LAM architectural framework as a promising direction for
advancing automated service composition. Such a system can reason about service
requirements and constraints while dynamically executing workflows, thus
bridging the gap between intention and execution. This integration has the
potential to transform service composition into a fully automated,
user-friendly process driven by high-level natural language intent.

</details>


### [94] [Faster Lifting for Ordered Domains with Predecessor Relations](https://arxiv.org/abs/2507.19182)
*Kuncheng Zou,Jiahao Mai,Yonggang Zhang,Yuyi Wang,Ondřej Kuželka,Yuanhong Wang,Yi Chang*

Main category: cs.AI

TL;DR: 本文研究了在有序域上进行提升推理的方法，重点关注前驱关系。通过将前驱关系作为公理的一部分，提出了一种新的算法，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有加权一阶模型计数（WFOMC）方法在处理前驱关系时效率不高，限制了实际应用。

Method: 将前驱关系直接作为公理的一部分，设计了一个新算法，支持从直接前驱到一般k阶前驱关系的处理。

Result: 新算法在处理直接和第二前驱关系时实现了指数级加速，并在组合数学问题中展现出显著的效率提升。

Conclusion: 提出的算法显著提升了在有前驱关系的有序域上的提升推理效率，具有理论和实际应用价值。

Abstract: We investigate lifted inference on ordered domains with predecessor
relations, where the elements of the domain respect a total (cyclic) order, and
every element has a distinct (clockwise) predecessor. Previous work has
explored this problem through weighted first-order model counting (WFOMC),
which computes the weighted sum of models for a given first-order logic
sentence over a finite domain. In WFOMC, the order constraint is typically
encoded by the linear order axiom introducing a binary predicate in the
sentence to impose a linear ordering on the domain elements. The immediate and
second predecessor relations are then encoded by the linear order predicate.
Although WFOMC with the linear order axiom is theoretically tractable, existing
algorithms struggle with practical applications, particularly when the
predecessor relations are involved. In this paper, we treat predecessor
relations as a native part of the axiom and devise a novel algorithm that
inherently supports these relations. The proposed algorithm not only provides
an exponential speedup for the immediate and second predecessor relations,
which are known to be tractable, but also handles the general k-th predecessor
relations. The extensive experiments on lifted inference tasks and
combinatorics math problems demonstrate the efficiency of our algorithm,
achieving speedups of a full order of magnitude.

</details>


### [95] [OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?](https://arxiv.org/abs/2507.19132)
*Xuetian Chen,Yinghao Chen,Xinfeng Yuan,Zhuo Peng,Lu Chen,Yuekeng Li,Zhoujia Zhang,Yingqian Huang,Leyan Huang,Jiaqing Liang,Tianbao Xie,Zhiyong Wu,Qiushi Sun,Biqing Qi,Bowen Zhou*

Main category: cs.AI

TL;DR: OS-MAP是一个新的计算机代理基准，通过五级自动化分类和需求层次结构评估代理能力，揭示当前代理在高层次任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准未考虑任务异质性和用户需求对齐，阻碍研究和实际部署。

Method: 提出OS-MAP基准，包含416个任务，沿自动化和需求层次两个维度评估代理。

Result: 实验显示，即使是先进代理在高层次任务中也表现不佳。

Conclusion: OS-MAP为计算机代理研究提供了结构化评估工具，未来需进一步改进代理能力。

Abstract: Computer-using agents have shown strong potential to boost human productivity
and enable new application forms across platforms. While recent advances have
led to usable applications, existing benchmarks fail to account for the
internal task heterogeneity and the corresponding agent capabilities, as well
as their alignment with actual user demands-hindering both targeted capability
development and the reliable transition of research progress into practical
deployment. To bridge the gap, we present OS-MAP, a benchmark for daily
computer-using automation that organizes its 416 realistic tasks across 15
applications along two key dimensions: a five-level taxonomy of automation and
a generalization scope derived from a real-world user demand hierarchy. To
enable fine-grained analysis of required capabilities and alignment with
real-world scenarios, OS-MAP evaluates agents along two dimensions: automation
level across a five-level taxonomy, and generalization scope across a demand
hierarchy. This design captures varying levels of required agent autonomy and
generalization, forming a performance-generalization evaluation matrix for
structured and comprehensive assessment. Experiments show that even
State-of-the-Art agents with VLM backbones struggle with higher-level tasks
involving perception, reasoning, and coordination-highlighting the need for a
deeper understanding of current strengths and limitations to drive the future
progress in computer-using agents research and deployment. All code,
environments, baselines, and data are publicly available at
https://github.com/OS-Copilot/OS-Map.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [96] [MetaMorph -- A Metamodelling Approach For Robot Morphology](https://arxiv.org/abs/2507.18820)
*Rachel Ringe,Robin Nolte,Nima Zargham,Robert Porzel,Rainer Malaka*

Main category: cs.RO

TL;DR: 论文提出MetaMorph框架，用于分类机器人形态，弥补现有分类方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有机器人外观分类方法过于宽泛或仅关注拟人化特征，无法全面比较不同机器人类型的设计效果。

Method: 通过分析222个IEEE机器人指南中的机器人，采用元建模方法构建MetaMorph框架。

Result: MetaMorph提供了一种结构化方法，用于比较机器人外观特征并探索适合不同任务的设计特性。

Conclusion: MetaMorph为机器人设计研究提供了更全面的分类工具，有助于优化人机交互。

Abstract: Robot appearance crucially shapes Human-Robot Interaction (HRI) but is
typically described via broad categories like anthropomorphic, zoomorphic, or
technical. More precise approaches focus almost exclusively on anthropomorphic
features, which fail to classify robots across all types, limiting the ability
to draw meaningful connections between robot design and its effect on
interaction. In response, we present MetaMorph, a comprehensive framework for
classifying robot morphology. Using a metamodeling approach, MetaMorph was
synthesized from 222 robots in the IEEE Robots Guide, offering a structured
method for comparing visual features. This model allows researchers to assess
the visual distances between robot models and explore optimal design traits
tailored to different tasks and contexts.

</details>


### [97] [Towards Multimodal Social Conversations with Robots: Using Vision-Language Models](https://arxiv.org/abs/2507.19196)
*Ruben Janssens,Tony Belpaeme*

Main category: cs.RO

TL;DR: 本文讨论了如何利用视觉语言模型提升社交机器人的多模态对话能力，指出其在处理广泛视觉信息方面的潜力，并探讨了技术挑战和评估方法。


<details>
  <summary>Details</summary>
Motivation: 社交机器人虽能进行开放领域对话，但缺乏利用多模态信息的能力，影响了社交互动的自然性。

Method: 提出将视觉语言模型应用于社交机器人，以处理多样化的视觉信息，并探讨了技术适配和挑战。

Result: 视觉语言模型显示出潜力，能够为社交机器人提供更全面的多模态对话能力。

Conclusion: 视觉语言模型是提升社交机器人多模态交互能力的有效途径，但需解决相关技术问题并完善评估方法。

Abstract: Large language models have given social robots the ability to autonomously
engage in open-domain conversations. However, they are still missing a
fundamental social skill: making use of the multiple modalities that carry
social interactions. While previous work has focused on task-oriented
interactions that require referencing the environment or specific phenomena in
social interactions such as dialogue breakdowns, we outline the overall needs
of a multimodal system for social conversations with robots. We then argue that
vision-language models are able to process this wide range of visual
information in a sufficiently general manner for autonomous social robots. We
describe how to adapt them to this setting, which technical challenges remain,
and briefly discuss evaluation practices.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [98] [Virtual local area network over HTTP for launching an insider attack](https://arxiv.org/abs/2507.19055)
*Yuksel Arslan*

Main category: cs.CR

TL;DR: 本文研究了局域网（LAN）中内部攻击的漏洞，展示了如何通过利用未使用的二级IP地址和HTTP协议，绕过现有安全机制，暴露LAN到互联网。


<details>
  <summary>Details</summary>
Motivation: 现代生活中，计算机和网络存储了大量敏感数据，现有安全措施主要防御外部威胁，对内部攻击的防护不足，亟需研究内部攻击的漏洞。

Method: 通过利用LAN中未使用的二级IP地址和HTTP协议，设计了一种内部攻击方式，展示了外部机器如何绕过防火墙和IDS访问LAN。

Result: 研究发现，即使存在强大的外部防护措施，内部攻击仍能成功暴露LAN，揭示现有安全机制对内部威胁的防护不足。

Conclusion: 内部攻击对LAN的安全性构成重大威胁，现有安全措施需加强对内部威胁的防护，未来研究应进一步探索此类漏洞的解决方案。

Abstract: Computers and computer networks have become integral to virtually every
aspect of modern life, with the Internet playing an indispensable role.
Organizations, businesses, and individuals now store vast amounts of
proprietary, confidential, and personal data digitally. As such, ensuring the
security of this data from unauthorized access is critical. Common security
measures, such as firewalls, intrusion detection systems (IDS), intrusion
prevention systems (IPS), and antivirus software, are constantly evolving to
safeguard computer systems and networks. However, these tools primarily focus
on defending against external threats, leaving systems vulnerable to insider
attacks. Security solutions designed to mitigate risks originating from within
the organization are relatively limited and often ineffective. This paper
demonstrates how a Local Area Network (LAN) can be covertly exposed to the
Internet via an insider attack. Specifically, it illustrates how an external
machine can gain access to a LAN by exploiting an unused secondary IP address
of the attacked LAN, effectively bypassing existing security mechanisms by also
exploiting Hyper Text Transfer Protocol (HTTP). Despite the presence of robust
external protections, such as firewalls and IDS, this form of insider attack
reveals significant vulnerabilities in the way internal threats are addressed.

</details>


### [99] [PurpCode: Reasoning for Safer Code Generation](https://arxiv.org/abs/2507.19060)
*Jiawei Liu,Nirav Diwan,Zhe Wang,Haoyu Zhai,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Muntasir Wahed,Yinlin Deng,Hadjer Benkraouda,Yuxiang Wei,Lingming Zhang,Ismini Lourentzou,Gang Wang*

Main category: cs.CR

TL;DR: PurpCode是一种后训练方法，通过规则学习和强化学习两阶段训练模型，生成安全代码并防御恶意网络活动；其开发的PurpCode-32B模型在网络安全和实用性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成模型在安全性方面存在不足，可能生成易受攻击的代码或协助恶意活动，因此需要一种专门针对网络安全的后训练方法。

Method: 分两阶段训练：规则学习（明确引用安全规则生成无漏洞代码）和强化学习（通过多样性奖励优化安全性与实用性）。

Result: 开发的PurpCode-32B模型在网络安全方面领先其他前沿模型，同时减少了过度拒绝率并保持了代码生成和安全知识的实用性。

Conclusion: PurpCode提供了一种有效的训练方法来提升代码生成模型的网络安全性，同时平衡了安全性与实用性。

Abstract: We introduce PurpCode, the first post-training recipe for training safe code
reasoning models towards generating secure code and defending against malicious
cyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule
Learning, which explicitly teaches the model to reference cybersafety rules to
generate vulnerability-free code and to avoid facilitating malicious
cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety
and preserves model utility through diverse, multi-objective reward mechanisms.
To empower the training pipelines with comprehensive cybersafety data, we
conduct internal red-teaming to synthesize comprehensive and high-coverage
prompts based on real-world tasks for inducing unsafe cyberactivities in the
model. Based on PurpCode, we develop a reasoning-based coding model, namely
PurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming
various frontier models. Meanwhile, our alignment method decreases the model
overrefusal rates in both general and cybersafety-specific scenarios, while
preserving model utility in both code generation and common security knowledge.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [100] [MLLM-based Speech Recognition: When and How is Multimodality Beneficial?](https://arxiv.org/abs/2507.19037)
*Yiwen Guan,Viet Anh Trinh,Vivek Voleti,Jacob Whitehill*

Main category: cs.SD

TL;DR: 该研究探讨了在多模态大语言模型（MLLMs）中，利用多种输入模态提升噪声环境下自动语音识别（ASR）准确率的条件与架构。


<details>
  <summary>Details</summary>
Motivation: 旨在探索多模态（如语音、文本、图像）如何互补提升ASR在噪声环境中的性能。

Method: 通过合成与真实数据实验，评估不同噪声水平下多模态的贡献，并比较同步与非同步模态的作用。

Result: 发现多模态通常能提升ASR准确率，但效果受噪声水平影响；高质量视觉表示和模态输入顺序对结果有显著影响。

Conclusion: 研究为多模态语音识别提供了实践指导，并深化了对挑战性条件下多模型协同的理解。

Abstract: Recent advances in multi-modal large language models (MLLMs) have opened new
possibilities for unified modeling of speech, text, images, and other
modalities. Building on our prior work, this paper examines the conditions and
model architectures under which multiple input modalities can improve automatic
speech recognition (ASR) accuracy in noisy environments. Through experiments on
synthetic and real-world data, we find that (1) harnessing more modalities
usually improves ASR accuracy, as each modality provides complementary
information, but the improvement depends on the amount of auditory noise. (2)
Synchronized modalities (e.g., lip movements) are more useful at high noise
levels whereas unsynchronized modalities (e.g., image context) are most helpful
at moderate noise levels. (3) Higher-quality visual representations
consistently improve ASR accuracy, highlighting the importance of developing
more powerful visual encoders. (4) Mamba exhibits similar trends regarding the
benefits of multimodality as do Transformers. (5) The input order of modalities
as well as their weights in the loss function can significantly impact
accuracy. These findings both offer practical insights and help to deepen our
understanding of multi-modal speech recognition under challenging conditions.

</details>


### [101] [Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven Talking Face Generation](https://arxiv.org/abs/2507.19225)
*Fang Kang,Yin Cao,Haoyu Chen*

Main category: cs.SD

TL;DR: 论文提出了一种名为Face2VoiceSync的新框架，用于从人脸图像和文本生成说话动画和语音，解决了现有方法依赖固定语音的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动的说话脸生成方法依赖固定语音，导致应用受限（如脸声不匹配），因此研究更挑战性的任务：从人脸和文本生成语音和动画。

Method: 提出Face2VoiceSync框架，包括语音-人脸对齐、多样性控制、高效训练（轻量VAE桥接视觉和音频模型）及新评估指标。

Result: 实验表明Face2VoiceSync在单块40GB GPU上实现了视觉和音频的最优性能。

Conclusion: Face2VoiceSync为说话脸生成提供了一种高效且多样化的解决方案。

Abstract: Recent studies in speech-driven talking face generation achieve promising
results, but their reliance on fixed-driven speech limits further applications
(e.g., face-voice mismatch). Thus, we extend the task to a more challenging
setting: given a face image and text to speak, generating both talking face
animation and its corresponding speeches. Accordingly, we propose a novel
framework, Face2VoiceSync, with several novel contributions: 1) Voice-Face
Alignment, ensuring generated voices match facial appearance; 2) Diversity \&
Manipulation, enabling generated voice control over paralinguistic features
space; 3) Efficient Training, using a lightweight VAE to bridge visual and
audio large-pretrained models, with significantly fewer trainable parameters
than existing methods; 4) New Evaluation Metric, fairly assessing the diversity
and identity consistency. Experiments show Face2VoiceSync achieves both visual
and audio state-of-the-art performances on a single 40GB GPU.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [102] [Human-AI Synergy in Adaptive Active Learning for Continuous Lithium Carbonate Crystallization Optimization](https://arxiv.org/abs/2507.19316)
*Shayan S. Mousavi Masouleh,Corey A. Sanz,Ryan P. Jansonius,Cara Cronin,Jason E. Hein,Jason Hattrick-Simpers*

Main category: cs.CE

TL;DR: 利用人机协同的主动学习框架优化锂碳酸盐连续结晶过程，解决低品位锂资源提纯难题，提升杂质容忍度至6000 ppm。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车产业对高纯度锂的需求激增，需要从北美低品位资源（如Smackover Formation）中经济高效地提取锂，但传统方法对高杂质资源的适应性差。

Method: 提出人机协同（HITL）的主动学习框架，结合人类专业知识和数据驱动，优化锂碳酸盐的连续结晶工艺。

Result: 框架显著提升了对镁等关键杂质的容忍度（从几百ppm到6000 ppm），使低品位锂资源的利用成为可能，减少预精炼需求。

Conclusion: 该技术为北美丰富锂资源的经济开发提供了可行方案，提升了全球锂供应链的可持续性。

Abstract: As demand for high-purity lithium surges with the growth of the electric
vehicle (EV) industry, cost-effective extraction from lower-grade North
American sources like the Smackover Formation is critical. These resources,
unlike high-purity South American brines, require innovative purification
techniques to be economically viable. Continuous crystallization is a promising
method for producing battery-grade lithium carbonate, but its optimization is
challenged by a complex parameter space and limited data. This study introduces
a Human-in-the-Loop (HITL) assisted active learning framework to optimize the
continuous crystallization of lithium carbonate. By integrating human expertise
with data-driven insights, our approach accelerates the optimization of lithium
extraction from challenging sources. Our results demonstrate the framework's
ability to rapidly adapt to new data, significantly improving the process's
tolerance to critical impurities like magnesium from the industry standard of a
few hundred ppm to as high as 6000 ppm. This breakthrough makes the
exploitation of low-grade, impurity-rich lithium resources feasible,
potentially reducing the need for extensive pre-refinement processes. By
leveraging artificial intelligence, we have refined operational parameters and
demonstrated that lower-grade materials can be used without sacrificing product
quality. This advancement is a significant step towards economically harnessing
North America's vast lithium reserves, such as those in the Smackover
Formation, and enhancing the sustainability of the global lithium supply chain.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [103] [High-Fidelity RF Mapping: Assessing Environmental Modeling in 6G Network Digital Twins](https://arxiv.org/abs/2507.19173)
*Lorenzo Cazzella,Francesco Linsalata,Damiano Badini,Matteo Matteucci,Maurizio Magarini,Umberto Spagnolini*

Main category: eess.SP

TL;DR: 本文提出了两种度量标准（HRT和CRT），用于比较不同环境变化下射线追踪模拟的差异，并通过实际案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提高数字孪生（DT）的准确性需要评估环境建模的保真度，因此需要一种量化方法来比较不同模拟结果的差异。

Method: 提出了Hausdorff射线追踪（HRT）和chamfer射线追踪（CRT）两种距离度量，在3D场景中比较时间、角度和功率特征。实验使用高保真米兰模型，并引入停车车辆和建筑窗户的网格变化，结合NVIDIA Sionna和SUMO进行模拟。

Result: HRT和CRT成功识别了模拟中的差异区域，车辆轨迹模拟揭示了距离模式。

Conclusion: 提出的方法能有效量化环境变化对射线追踪模拟的影响，为数字孪生的优化提供了工具。

Abstract: The design of accurate Digital Twins (DTs) of electromagnetic environments
strictly depends on the fidelity of the underlying environmental modeling.
Evaluating the differences among diverse levels of modeling accuracy is key to
determine the relevance of the model features towards both efficient and
accurate DT simulations. In this paper, we propose two metrics, the Hausdorff
ray tracing (HRT) and chamfer ray tracing (CRT) distances, to consistently
compare the temporal, angular and power features between two ray tracing
simulations performed on 3D scenarios featured by environmental changes. To
evaluate the introduced metrics, we considered a high-fidelity digital twin
model of an area of Milan, Italy and we enriched it with two different types of
environmental changes: (i) the inclusion of parked vehicles meshes, and (ii)
the segmentation of the buildings facade faces to separate the windows mesh
components from the rest of the building. We performed grid-based and vehicular
ray tracing simulations at 28 GHz carrier frequency on the obtained scenarios
integrating the NVIDIA Sionna RT ray tracing simulator with the SUMO vehicular
traffic simulator. Both the HRT and CRT metrics highlighted the areas of the
scenarios where the simulated radio propagation features differ owing to the
introduced mesh integrations, while the vehicular ray tracing simulations
allowed to uncover the distance patterns arising along realistic vehicular
trajectories.

</details>
