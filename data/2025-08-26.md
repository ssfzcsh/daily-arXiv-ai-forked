<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 26]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 6]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 8]
- [cs.HC](#cs.HC) [Total: 33]
- [cs.GR](#cs.GR) [Total: 8]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.DB](#cs.DB) [Total: 11]
- [cs.AR](#cs.AR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cs.CL](#cs.CL) [Total: 5]
- [math.CO](#math.CO) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.CR](#cs.CR) [Total: 9]
- [physics.optics](#physics.optics) [Total: 2]
- [quant-ph](#quant-ph) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification](https://arxiv.org/abs/2508.16671)
*Mingyang Zhou,Quanming Yao,Lun Du,Lanning Wei,Da Zheng*

Main category: cs.SE

TL;DR: 论文提出了一种名为RePro的反思性论文转代码复现框架，通过提取论文指纹并迭代验证，显著提升了复现准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以完全准确复现论文中的数学公式和算法逻辑，而反思和反馈策略未被有效采用。人类利用系统化检查表调试复杂代码的启发下，设计了RePro框架。

Method: RePro框架通过提取论文指纹（准确的原子标准），生成代码并在迭代验证和优化循环中利用指纹检测差异并进行针对性修订。

Result: 在PaperBench Code-Dev基准测试中，RePro比基线方法表现提升13.0%，并能正确修订复杂逻辑和数学标准。

Conclusion: RePro通过系统性反思和验证，显著提高了论文复现的准确性和效率。

Abstract: Reproducing machine learning papers is essential for scientific progress but
remains challenging for both humans and automated agents. Existing agent-based
methods often struggle to fully and accurately reproduce implementation details
such as mathematical formulas and algorithmic logic. Previous studies show that
reflection with explicit feedback improves agent performance. However, current
paper reproduction methods fail to effectively adopt this strategy. This gap
mainly arises from the diverse paper patterns, complex method modules, and
varied configurations encountered in research papers. Motivated by how humans
use systematic checklists to efficiently debug complex code, we propose
\textbf{RePro}, a \textbf{Re}flective Paper-to-Code \textbf{Repro}duction
framework that automatically extracts a paper's fingerprint, referring to a
comprehensive set of accurate and atomic criteria serving as high-quality
supervisory signals. The framework first generates code based on the extracted
information, and then leverages the fingerprint within iterative verification
and refinement loop. This approach systematically detects discrepancies and
produces targeted revisions to align generated code with the paper's
implementation details. Extensive experiments on the PaperBench Code-Dev
benchmark have been conducted, RePro achieves 13.0\% performance gap over
baselines, and it correctly revises complex logical and mathematical criteria
in reflecting, on which the effectiveness is obvious.

</details>


### [2] [Cognitive Agents Powered by Large Language Models for Agile Software Project Management](https://arxiv.org/abs/2508.16678)
*Konrad Cinkusz,Jarosław A. Chudziak,Ewa Niewiadomska-Szynkiewicz*

Main category: cs.SE

TL;DR: 论文探讨了基于大语言模型（LLM）的认知代理在SAFe框架中的应用，以优化软件项目管理。通过虚拟代理在模拟环境中的表现，研究展示了其在任务分配、沟通和项目生命周期管理中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索认知代理如何通过智能自动化提升敏捷软件开发中的决策、问题解决和协作效率。

Method: 研究方法是在CogniSim生态系统中部署虚拟代理，进行迭代模拟，测试其在任务完成时间、交付质量和沟通一致性等方面的表现。

Result: 结果显示，认知代理能够显著改善敏捷实践中的效率与精度，并展现出良好的可扩展性和适应性。

Conclusion: 结论认为，LLM驱动的认知代理在敏捷框架中的集成有望推动软件工程实践的范式转变。

Abstract: This paper investigates the integration of cognitive agents powered by Large
Language Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce
software project management. By deploying virtual agents in simulated software
environments, this study explores their potential to fulfill fundamental roles
in IT project development, thereby optimizing project outcomes through
intelligent automation. Particular emphasis is placed on the adaptability of
these agents to Agile methodologies and their transformative impact on
decision-making, problem-solving, and collaboration dynamics. The research
leverages the CogniSim ecosystem, a platform designed to simulate real-world
software engineering challenges, such as aligning technical capabilities with
business objectives, managing interdependencies, and maintaining project
agility. Through iterative simulations, cognitive agents demonstrate advanced
capabilities in task delegation, inter-agent communication, and project
lifecycle management. By employing natural language processing to facilitate
meaningful dialogues, these agents emulate human roles and improve the
efficiency and precision of Agile practices. Key findings from this
investigation highlight the ability of LLM-powered cognitive agents to deliver
measurable improvements in various metrics, including task completion times,
quality of deliverables, and communication coherence. These agents exhibit
scalability and adaptability, ensuring their applicability across diverse and
complex project environments. This study underscores the potential of
integrating LLM-powered agents into Agile project management frameworks as a
means of advancing software engineering practices. This integration not only
refines the execution of project management tasks but also sets the stage for a
paradigm shift in how teams collaborate and address emerging challenges.

</details>


### [3] [Democratizing AI Development: Local LLM Deployment for India's Developer Ecosystem in the Era of Tokenized APIs](https://arxiv.org/abs/2508.16684)
*Vikranth Udandarao,Nipun Misra*

Main category: cs.SE

TL;DR: 研究评估了在印度开发者社区中使用Ollama本地部署LLM的可行性，发现其成本降低33%且实验迭代次数翻倍，增强了AI开发的可及性。


<details>
  <summary>Details</summary>
Motivation: 印度开发者因经济和基础设施限制难以持续使用商业LLM API，研究试图验证本地部署的替代方案是否可行。

Method: 采用混合方法分析，涉及180名印度开发者、学生和AI爱好者，对比本地部署与商业云服务的成本和学习效果。

Result: 本地部署成本降低33%，实验迭代次数翻倍，开发者对AI架构的理解更深入。

Conclusion: 本地LLM部署是提升资源受限环境中AI开发可及性和创新能力的关键。

Abstract: India's developer community faces significant barriers to sustained
experimentation and learning with commercial Large Language Model (LLM) APIs,
primarily due to economic and infrastructural constraints. This study
empirically evaluates local LLM deployment using Ollama as an alternative to
commercial cloud-based services for developer-focused applications. Through a
mixed-methods analysis involving 180 Indian developers, students, and AI
enthusiasts, we find that local deployment enables substantially greater
hands-on development and experimentation, while reducing costs by 33% compared
to commercial solutions. Developers using local LLMs completed over twice as
many experimental iterations and reported deeper understanding of advanced AI
architectures. Our results highlight local deployment as a critical enabler for
inclusive and accessible AI development, demonstrating how technological
accessibility can enhance learning outcomes and innovation capacity in
resource-constrained environments.

</details>


### [4] [Cybernaut: Towards Reliable Web Automation](https://arxiv.org/abs/2508.16688)
*Ankur Tomar,Hengyue Liang,Indranil Bhattacharya,Natalia Larios,Francesco Carbone*

Main category: cs.SE

TL;DR: 论文提出了Cybernaut框架，解决了LLM驱动网络自动化在现实应用中的四大挑战，显著提高了任务执行成功率。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案主要针对设计良好的消费者网站，而忽视了复杂企业内部界面的自动化需求。

Method: 通过SOP生成器、高精度HTML元素识别系统和一致性评估指标，提升自动化执行的可靠性。

Result: 实验表明，Cybernaut将任务执行成功率从72%提升至88.68%，识别一致性模式的准确率达84.7%。

Conclusion: Cybernaut在企业级网络自动化中表现出色，为未来技术进步奠定了基础。

Abstract: The emergence of AI-driven web automation through Large Language Models
(LLMs) offers unprecedented opportunities for optimizing digital workflows.
However, deploying such systems within industry's real-world environments
presents four core challenges: (1) ensuring consistent execution, (2)
accurately identifying critical HTML elements, (3) meeting human-like accuracy
in order to automate operations at scale and (4) the lack of comprehensive
benchmarking data on internal web applications. Existing solutions are
primarily tailored for well-designed, consumer-facing websites (e.g.,
Amazon.com, Apple.com) and fall short in addressing the complexity of
poorly-designed internal web interfaces. To address these limitations, we
present Cybernaut, a novel framework to ensure high execution consistency in
web automation agents designed for robust enterprise use. Our contributions are
threefold: (1) a Standard Operating Procedure (SOP) generator that converts
user demonstrations into reliable automation instructions for linear browsing
tasks, (2) a high-precision HTML DOM element recognition system tailored for
the challenge of complex web interfaces, and (3) a quantitative metric to
assess execution consistency. The empirical evaluation on our internal
benchmark demonstrates that using our framework enables a 23.2% improvement
(from 72% to 88.68%) in task execution success rate over the browser_use.
Cybernaut identifies consistent execution patterns with 84.7% accuracy,
enabling reliable confidence assessment and adaptive guidance during task
execution in real-world systems. These results highlight Cybernaut's
effectiveness in enterprise-scale web automation and lay a foundation for
future advancements in web automation.

</details>


### [5] [A Scalable Framework for the Management of STPA Requirements: a Case Study on eVTOL Operations](https://arxiv.org/abs/2508.16708)
*Shufeng Chen,Halima El Badaoui,Mariat James Elizebeth,Takuya Nakashima,Siddartha Khastgir,Paul Jennings*

Main category: cs.SE

TL;DR: 提出了一个可扩展的框架，用于优先处理STPA生成的安全需求，结合专家评估和蒙特卡洛模拟，并验证了其在eVTOL操作中的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如FMEA和FTA）常忽略大量安全需求，且缺乏结构化的需求管理框架，尤其在快速开发环境中存在挑战。

Method: 开发了一个框架，整合STPA各步骤输出和专家评估（基于实现时间、成本、需求类型和法规覆盖），利用蒙特卡洛模拟减少主观性，并通过自动化工具链可视化需求优先级。

Result: 框架在英国民航局的eVTOL操作案例中得到验证，直接贡献于CAP3141，帮助利益相关者高效识别和管理高影响需求。

Conclusion: 该框架填补了STPA需求优先级的空白，为新兴技术的安全关键开发提供了实用解决方案。

Abstract: System-Theoretic Process Analysis (STPA) is a recommended method for
analysing complex systems, capable of identifying thousands of safety
requirements often missed by traditional techniques such as Failure Mode and
Effects Analysis (FMEA) and Fault Tree Analysis (FTA). However, the absence of
a structured framework for managing and prioritising these requirements
presents challenges, particularly in fast-paced development environments. This
paper introduces a scalable framework for prioritising STPA-derived
requirements. The framework integrates outputs from each STPA step and
incorporates expert evaluations based on four key factors: implementation time,
cost, requirement type, and regulatory coverage. To reduce subjectivity,
Monte-Carlo Simulation (MCS) is employed to calculate and stabilise requirement
rankings. An automation toolchain supports the framework, enabling dynamic
mapping of prioritised requirements in a scaling matrix. This visualisation
aids decision-making and ensures traceability across development phases. The
framework is applicable from early conceptualisation to more advanced stages,
enhancing its utility in iterative system development. The framework was
validated through a real-world case study focused on Electric Vertical Take-off
and Landing (eVTOL) operations, conducted in collaboration with the UK Civil
Aviation Authority. The findings contributed directly to CAP3141, a Civil
Aviation Publication that identifies systemic operational risks and safety
mitigations for regulators, operators, and vertiports. The prioritisation
process supported decision-making by helping stakeholders identify and manage
high-impact requirements efficiently. This work contributes a practical
solution for managing STPA outputs, bridging gaps in requirement prioritisation
and supporting safety-critical development in emerging technologies.

</details>


### [6] [CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics](https://arxiv.org/abs/2508.16713)
*Mohammad Atif,Kriti Chopra,Ozgur Kilic,Tianle Wang,Zhihua Dong,Charles Leggett,Meifeng Lin,Paolo Calafiura,Salman Habib*

Main category: cs.SE

TL;DR: CelloAI是一个本地部署的编码助手，利用大型语言模型（LLM）和检索增强生成（RAG）技术，支持高能物理（HEP）代码的文档化和生成，解决了数据隐私和成本问题。


<details>
  <summary>Details</summary>
Motivation: 下一代高能物理实验将生成大量数据，需要高性能计算（HPC）支持。但HEP领域采用HPC的挑战在于将遗留软件移植到异构架构以及复杂代码库的文档不足。

Method: CelloAI通过RAG技术，提供代码文档化（如Doxygen风格注释生成、文件级摘要）和代码生成功能，采用语法感知的分块策略和调用图知识以提高准确性。

Result: 在ATLAS、CMS和DUNE实验中评估表明，CelloAI能有效提升代码理解和生成能力，满足科学计算环境的透明性和安全性需求。

Conclusion: CelloAI为高能物理实验提供了一个高效、安全的代码辅助工具，解决了HPC集成中的关键问题。

Abstract: Next-generation High Energy Physics (HEP) experiments will generate
unprecedented data volumes, necessitating High Performance Computing (HPC)
integration alongside traditional high-throughput computing. However, HPC
adoption in HEP is hindered by the challenge of porting legacy software to
heterogeneous architectures and the sparse documentation of these complex
scientific codebases. We present CelloAI, a locally hosted coding assistant
that leverages Large Language Models (LLMs) with retrieval-augmented generation
(RAG) to support HEP code documentation and generation. This local deployment
ensures data privacy, eliminates recurring costs and provides access to large
context windows without external dependencies. CelloAI addresses two primary
use cases, code documentation and code generation, through specialized
components. For code documentation, the assistant provides: (a) Doxygen style
comment generation for all functions and classes by retrieving relevant
information from RAG sources (papers, posters, presentations), (b) file-level
summary generation, and (c) an interactive chatbot for code comprehension
queries. For code generation, CelloAI employs syntax-aware chunking strategies
that preserve syntactic boundaries during embedding, improving retrieval
accuracy in large codebases. The system integrates callgraph knowledge to
maintain dependency awareness during code modifications and provides
AI-generated suggestions for performance optimization and accurate refactoring.
We evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE
experiments, comparing different embedding models for code retrieval
effectiveness. Our results demonstrate the AI assistant's capability to enhance
code understanding and support reliable code generation while maintaining the
transparency and safety requirements essential for scientific computing
environments.

</details>


### [7] [EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention](https://arxiv.org/abs/2508.16771)
*Yifan Zhang,Chen Huang,Yueke Zhang,Jiahao Zhang,Toby Jia-Jun Li,Collin McMillan,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: EyeMulator是一种通过模仿人类视觉注意力来改进代码语言模型（CodeLLMs）训练的技术，利用人类眼动数据为输入标记添加权重，从而提升模型在代码翻译、补全和摘要等任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前CodeLLMs的训练仅基于机器注意力机制，忽略了人类开发者在处理代码时的视觉注意力线索。通过模仿人类的视觉注意力，可以提升模型的性能。

Method: EyeMulator通过在损失函数中为每个输入标记添加特殊权重，这些权重来源于公开的眼动实验数据，从而在训练中改变模型的注意力机制。

Result: 实验表明，EyeMulator在代码翻译、补全和摘要等任务上表现优于基线模型，且消融研究证实改进源于模型学习了人类的注意力模式。

Conclusion: 通过模仿人类视觉注意力，EyeMulator能够显著提升CodeLLMs的性能，且无需在推理阶段依赖眼动数据。

Abstract: Code language models (so-called CodeLLMs) are now commonplace in software
development. As a general rule, CodeLLMs are trained by dividing training
examples into input tokens and then learn importance of those tokens in a
process called machine attention. Machine attention is based solely on input
token salience to output token examples during training. Human software
developers are different, as humans intuitively know that some tokens are more
salient than others. While intuition itself is ineffable and a subject of
philosophy, clues about salience are present in human visual attention, since
people tend to look at more salient words more often. In this paper, we present
EyeMulator, a technique for training CodeLLMs to mimic human visual attention
while training for various software development tasks. We add special weights
for each token in each input example to the loss function used during LLM
fine-tuning. We draw these weights from observations of human visual attention
derived from a previously-collected publicly-available dataset of eye-tracking
experiments in software engineering tasks. These new weights ultimately induce
changes in the attention of the subject LLM during training, resulting in a
model that does not need eye-tracking data during inference. Our evaluation
shows that EyeMulator outperforms strong LLM baselines on several tasks such as
code translation, completion and summarization. We further show an ablation
study that demonstrates the improvement is due to subject models learning to
mimic human attention.

</details>


### [8] [Who Wins the Race? (R Vs Python) - An Exploratory Study on Energy Consumption of Machine Learning Algorithms](https://arxiv.org/abs/2508.17344)
*Rajrupa Chattaraj,Sridhar Chimalakonda,Vibhu Saujanya Sharma,Vikrant Kaulgud*

Main category: cs.SE

TL;DR: 该论文研究了Python和R两种流行编程语言在机器学习任务中的能耗差异，结果表明语言选择对能源效率有显著影响。


<details>
  <summary>Details</summary>
Motivation: 机器学习广泛使用但能耗高，缺乏不同编程语言在ML任务中的能源消耗比较研究。

Method: 通过实证研究，比较了Python和R在五种回归和五种分类任务中的能耗和运行时性能。

Result: 95%的案例中两种语言能耗差异显著，语言选择可影响训练和推理阶段的能源效率高达99%以上。

Conclusion: 编程语言选择对机器学习任务的能源效率有重大影响，需关注环境成本。

Abstract: The utilization of Machine Learning (ML) in contemporary software systems is
extensive and continually expanding. However, its usage is energy-intensive,
contributing to increased carbon emissions and demanding significant resources.
While numerous studies examine the performance and accuracy of ML, only a
limited few focus on its environmental aspects, particularly energy
consumption. In addition, despite emerging efforts to compare energy
consumption across various programming languages for specific algorithms and
tasks, there remains a gap specifically in comparing these languages for
ML-based tasks. This paper aims to raise awareness of the energy costs
associated with employing different programming languages for ML model training
and inference. Through this empirical study, we measure and compare the energy
consumption along with run-time performance of five regression and five
classification tasks implemented in Python and R, the two most popular
programming languages in this context. Our study results reveal a statistically
significant difference in costs between the two languages in 95% of the cases
examined. Furthermore, our analysis demonstrates that the choice of programming
language can influence energy efficiency significantly, up to 99.16% during
model training and up to 99.8% during inferences, for a given ML task.

</details>


### [9] [DevLicOps: A Framework for Mitigating Licensing Risks in AI-Generated Code](https://arxiv.org/abs/2508.16853)
*Pratyush Nidhi Sharma,Lauren Wright,Anne Herfurth,Munsif Sokiyna,Pratyaksh Nidhi Sharma,Sethu Das,Mikko Siponen*

Main category: cs.SE

TL;DR: 生成式AI编码助手（ACAs）可能带来法律风险，尤其是开源许可证问题。文章提出DevLicOps框架，帮助IT领导者管理合规风险。


<details>
  <summary>Details</summary>
Motivation: ACAs可能生成受限制开源许可证（如GPL）约束的代码，开发者缺乏相关知识，法律标准全球不统一，导致公司可能面临诉讼或被强制开源的风险。

Method: 提出DevLicOps框架，通过治理、事件响应和知情权衡来管理ACA相关的许可证风险。

Result: DevLicOps为AI时代的软件开发提供了一种前瞻性的许可证合规方法。

Conclusion: 随着ACA的普及和法律环境的变化，主动的许可证合规对负责任和风险意识的软件开发至关重要。

Abstract: Generative AI coding assistants (ACAs) are widely adopted yet pose serious
legal and compliance risks. ACAs can generate code governed by restrictive
open-source licenses (e.g., GPL), potentially exposing companies to litigation
or forced open-sourcing. Few developers are trained in these risks, and legal
standards vary globally, especially with outsourcing. Our article introduces
DevLicOps, a practical framework that helps IT leaders manage ACA-related
licensing risks through governance, incident response, and informed tradeoffs.
As ACA adoption grows and legal frameworks evolve, proactive license compliance
is essential for responsible, risk-aware software development in the AI era.

</details>


### [10] [TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings](https://arxiv.org/abs/2508.16860)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: TriagerX 是一个基于双 Transformer 架构的模型，通过结合内容排名和开发者历史交互排名，显著提升了 bug 指派任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的 PLMs 在 bug 指派任务中可能关注不相关的 tokens，并且未考虑开发者的历史交互，导致推荐效果不佳。

Method: TriagerX 使用双 Transformer 架构和交互式排名方法，结合内容推荐和开发者历史交互数据。

Result: 在五个数据集上，TriagerX 超越了九种基于 Transformer 的方法，Top-1 和 Top-3 推荐准确率提升了 10% 以上。

Conclusion: TriagerX 在实际工业环境中表现出色，不仅提升了开发者推荐效果，还显著改进了组件推荐任务。

Abstract: Pretrained Language Models or PLMs are transformer-based architectures that
can be used in bug triaging tasks. PLMs can better capture token semantics than
traditional Machine Learning (ML) models that rely on statistical features
(e.g., TF-IDF, bag of words). However, PLMs may still attend to less relevant
tokens in a bug report, which can impact their effectiveness. In addition, the
model can be sub-optimal with its recommendations when the interaction history
of developers around similar bugs is not taken into account. We designed
TriagerX to address these limitations. First, to assess token semantics more
reliably, we leverage a dual-transformer architecture. Unlike current
state-of-the-art (SOTA) baselines that employ a single transformer
architecture, TriagerX collects recommendations from two transformers with each
offering recommendations via its last three layers. This setup generates a
robust content-based ranking of candidate developers. TriagerX then refines
this ranking by employing a novel interaction-based ranking methodology, which
considers developers' historical interactions with similar fixed bugs. Across
five datasets, TriagerX surpasses all nine transformer-based methods, including
SOTA baselines, often improving Top-1 and Top-3 developer recommendation
accuracy by over 10%. We worked with our large industry partner to successfully
deploy TriagerX in their development environment. The partner required both
developer and component recommendations, with components acting as proxies for
team assignments-particularly useful in cases of developer turnover or team
changes. We trained TriagerX on the partner's dataset for both tasks, and it
outperformed SOTA baselines by up to 10% for component recommendations and 54%
for developer recommendations.

</details>


### [11] [Mind the Gap: A Decade-Scale Empirical Study of Multi-Stakeholder Dynamics in VR Ecosystem](https://arxiv.org/abs/2508.16903)
*Yijun Lu,Hironori Washizaki,Naoyasu Ubayashi,Nobukazu Yoshioka,Chenhao Wu,Masanari Kondo,Yuyin Ma,Jiong Dong,Jianjin Zhao,Dongqi Han*

Main category: cs.SE

TL;DR: 提出了一个多视角实证框架，通过分析用户评论和开发者讨论，发现用户在包容性和社区安全等方面的需求未被开发者充分关注。


<details>
  <summary>Details</summary>
Motivation: VR生态系统中用户期望与开发者行动之间存在差距，需要更全面的分析来指导质量保证和以用户为中心的创新。

Method: 采用主题建模和定量影响分析，系统比较和匹配用户评论与开发者帖子的视角。

Result: 发现用户和开发者在性能和输入方法等方面有重叠关注，但在包容性和社区安全方面存在明显差距。

Conclusion: 研究结果为VR平台的治理和下一代系统设计提供了数据驱动的建议，有助于缩小用户与开发者之间的差距。

Abstract: In the development and evolution of VR ecosystem, platform stakeholders
continuously adapt their products in response to user and technical feedback,
often reflected in subtle shifts in discussion topics or system updates. A
comprehensive understanding of these changes is essential for identifying gaps
between user expectations and developer actions, which can guide more effective
quality assurance and user-centered innovation. While previous studies have
analyzed either user reviews or developer discussions in isolation, such
approaches typically fail to reveal how specific user concerns are (or are not)
addressed by corresponding technical activities. To address this limitation,
our study introduces a multi-view empirical framework that systematically
compares and aligns stakeholder perspectives. By applying topic modeling and
quantitative impact analysis to 944,320 user reviews and 389,477 developer
posts, we identify not only the overlap in concerns (e.g., performance, input
methods), but also clear gaps in areas like inclusivity and community safety
(e.g., LGBTQ+ representation, child-friendly content). Our findings show that
while users repeatedly raise such issues, they are rarely discussed in
developer forums. These insights enable data-driven recommendations for closing
the user-developer gap in VR ecosystems, offering practical implications for
platform governance and the design of next-generation VR systems.

</details>


### [12] [What Developers Ask to ChatGPT in GitHub Pull Requests? an Exploratory Study](https://arxiv.org/abs/2508.17161)
*Julyanara R. Silva,Carlos Eduardo C. Dantas,Marcelo A. Maia*

Main category: cs.SE

TL;DR: 本文研究了开发者与ChatGPT的交互如何促成代码库的贡献，分析了155个有效交互，提出了14种请求类型的分类，发现代码生成请求通常需要更多交互。


<details>
  <summary>Details</summary>
Motivation: 探索开发者与ChatGPT的交互如何促成代码合并，填补相关研究空白。

Method: 手动评估155个有效ChatGPT分享链接，分析交互类型和效果。

Result: 提出了14种请求类型的分类，发现代码生成请求需要更多交互，开发者常利用ChatGPT进行代码审查和任务实现。

Conclusion: ChatGPT在代码生成任务中交互需求更高，但其在代码审查和技术解释方面的价值显著。

Abstract: The emergence of Large Language Models (LLMs), such as ChatGPT, has
introduced a new set of tools to support software developers in solving pro-
gramming tasks. However, our understanding of the interactions (i.e., prompts)
between developers and ChatGPT that result in contributions to the codebase
remains limited. To explore this limitation, we conducted a manual evaluation
of 155 valid ChatGPT share links extracted from 139 merged Pull Requests (PRs),
revealing the interactions between developers and reviewers with ChatGPT that
led to merges into the main codebase. Our results produced a catalog of 14
types of ChatGPT requests categorized into four main groups. We found a
significant number of requests involving code review and the implementation of
code snippets based on specific tasks. Developers also sought to clarify doubts
by requesting technical explanations or by asking for text refinements for
their web pages. Furthermore, we verified that prompts involving code
generation generally required more interactions to produce the desired answer
compared to prompts requesting text review or technical information.

</details>


### [13] [Agentic AI for Software: thoughts from Software Engineering community](https://arxiv.org/abs/2508.17343)
*Abhik Roychoudhury*

Main category: cs.SE

TL;DR: AI代理在软件工程中表现出巨大潜力，不仅限于代码生成，还包括设计、测试和修复等任务。其核心挑战是理解开发者意图，同时需解决信任问题，未来将结合AI验证与验证技术。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理在软件工程中的广泛应用，解决开发者意图推断和自动化信任问题。

Method: 通过程序分析工具辅助AI代理自主完成代码与设计任务，推动意图推断和验证技术的进步。

Result: AI代理能够成为开发团队的一员，实现多种软件任务的自动化，但需解决意图理解和信任问题。

Conclusion: AI代理将重塑软件工程，未来需结合AI验证技术以应对自动化带来的代码爆炸问题。

Abstract: AI agents have recently shown significant promise in software engineering.
Much public attention has been transfixed on the topic of code generation from
Large Language Models (LLMs) via a prompt. However, software engineering is
much more than programming, and AI agents go far beyond instructions given by a
prompt.
  At the code level, common software tasks include code generation, testing,
and program repair. Design level software tasks may include architecture
exploration, requirements understanding, and requirements enforcement at the
code level. Each of these software tasks involves micro-decisions which can be
taken autonomously by an AI agent, aided by program analysis tools. This
creates the vision of an AI software engineer, where the AI agent can be seen
as a member of a development team.
  Conceptually, the key to successfully developing trustworthy agentic AI-based
software workflows will be to resolve the core difficulty in software
engineering - the deciphering and clarification of developer intent.
Specification inference, or deciphering the intent, thus lies at the heart of
many software tasks, including software maintenance and program repair. A
successful deployment of agentic technology into software engineering would
involve making conceptual progress in such intent inference via agents.
  Trusting the AI agent becomes a key aspect, as software engineering becomes
more automated. Higher automation also leads to higher volume of code being
automatically generated, and then integrated into code-bases. Thus to deal with
this explosion, an emerging direction is AI-based verification and validation
(V & V) of AI generated code. We posit that agentic software workflows in
future will include such AIbased V&V.

</details>


### [14] [Code Difference Guided Fuzzing for FPGA Logic Synthesis Compilers via Bayesian Optimization](https://arxiv.org/abs/2508.17713)
*Zhihao Xu,Shikai Guo,Guilin Zhao,Peiyu Zou,Siwen Wang,Qian Ma,Hui Li,Furui Zhan*

Main category: cs.SE

TL;DR: 提出了一种基于贝叶斯优化的引导变异策略LSC-Fuzz，用于检测FPGA逻辑综合编译器中的错误，通过多样化和复杂的HDL代码生成来提升测试效果。


<details>
  <summary>Details</summary>
Motivation: FPGA逻辑综合编译器中的错误可能导致关键应用中的意外行为和安全风险，现有方法的简单盲变异策略效果有限。

Method: LSC-Fuzz包含测试程序生成、贝叶斯多样性选择和等效检查三个组件，通过生成多样化的HDL代码并进行等效检测来发现错误。

Result: LSC-Fuzz在三个月内发现了16个错误，其中12个被官方技术确认。

Conclusion: LSC-Fuzz通过智能化变异策略显著提升了FPGA逻辑综合编译器的错误检测能力。

Abstract: Field Programmable Gate Arrays (FPGAs) play a crucial role in Electronic
Design Automation (EDA) applications, which have been widely used in
safety-critical environments, including aerospace, chip manufacturing, and
medical devices. A critical step in FPGA development is logic synthesis, which
enables developers to translate their software designs into hardware net lists,
which facilitates the physical implementation of the chip, detailed timing and
power analysis, gate-level simulation, test vector generation, and optimization
and consistency checking. However, bugs or incorrect implementations in FPGA
logic synthesis compilers may lead to unexpected behaviors in target
wapplications, posing security risks. Therefore, it is crucial to eliminate
such bugs in FPGA logic synthesis compilers. The effectiveness of existing
works is still limited by its simple, blind mutation strategy. To address this
challenge, we propose a guided mutation strategy based on Bayesian optimization
called LSC-Fuzz to detect bugs in FPGA logic synthesis compilers. Specifically,
LSC-Fuzz consists of three components: the test-program generation component,
the Bayesian diversity selection component, and the equivalent check component.
By performing test-program generation and Bayesian diversity selection,
LSC-Fuzz generates diverse and complex HDL code, thoroughly testing the FPGA
logic synthesis compilers using equivalent check to detect bugs. Through three
months, LSC-Fuzz has found 16 bugs, 12 of these has been confirmed by official
technical support.

</details>


### [15] [DocFetch - Towards Generating Software Documentation from Multiple Software Artifacts](https://arxiv.org/abs/2508.17719)
*Akhila Sri Manasa Venigalla,Sridhar Chimalakonda*

Main category: cs.SE

TL;DR: DocFetch是一种从多个软件制品生成文档的工具，利用多层级提示的LLM，在DocMine数据集上表现良好，BLEU-4得分最高达43.24%。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目文档维护困难，现有方法主要关注源代码，但有用信息分散在其他制品中。DocFetch旨在利用这些信息减少文档维护工作。

Method: 采用基于多层级提示的大型语言模型(LLM)，从多个软件制品中生成结构化文档，并在DocMine数据集上进行评估。

Result: 在API和文件相关信息生成上，DocFetch的BLEU-4得分达43.24%，ROUGE-L得分为0.39；其他类型文档的生成也表现良好。

Conclusion: DocFetch可半自动生成文档，显著降低项目理解与文档维护的工作量。

Abstract: Software Documentation plays a major role in the usage and development of a
project. Widespread adoption of open source software projects contributes to
larger and faster development of the projects, making it difficult to maintain
the associated documentation. Existing automated approaches to generate
documentation largely focus on source code. However, information useful for
documentation is observed to be scattered across various artifacts that
co-evolve with the source code. Leveraging this information across multiple
artifacts can reduce the effort involved in maintaining documentation. Hence,
we propose DocFetch, to generate different types of documentation from multiple
software artifacts. We employ a multi-layer prompt based LLM and generate
structured documentation corresponding to different documentation types for the
data consolidated in DocMine dataset. We evaluate the performance of DocFetch
using a manually curated groundtruth dataset by analysing the artifacts in
DocMine. The evaluation yields a highest BLEU-4 score of 43.24% and ROUGE-L
score of 0.39 for generation of api-related and file-related information from
five documentation sources. The generation of other documentation type related
information also reported BLEU-4 scores close to 30% indicating good
performance of the approach. Thus,DocFetch can be employed to
semi-automatically generate documentation, and helps in comprehending the
projects with minimal effort in maintaining the documentation.

</details>


### [16] [RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation](https://arxiv.org/abs/2508.17720)
*Ziqi Guan,Xin Yin,Zhiyuan Peng,Chao Ni*

Main category: cs.SE

TL;DR: 提出RepoTransAgent，一个多智能体LLM框架，用于解决代码仓库感知的代码翻译问题，显著提高了编译通过率和运行通过率。


<details>
  <summary>Details</summary>
Motivation: 现代遗留系统升级、维护性增强和多语言互操作需高效代码翻译，现有LLM方法在上下文理解、提示设计和纠错机制上存在不足。

Method: 将翻译过程分解为上下文检索、动态提示构建和迭代代码优化，利用RAG、自适应提示和反思机制，由专门智能体处理。

Result: 在六个开源项目的Java-C#翻译任务中，RepoTransAgent编译通过率55.34%，运行通过率45.84%，优于现有方法。

Conclusion: RepoTransAgent在不同LLM中表现稳健，适用于实际代码仓库翻译场景。

Abstract: Repository-aware code translation is critical for modernizing legacy systems,
enhancing maintainability, and enabling interoperability across diverse
programming languages. While recent advances in large language models (LLMs)
have improved code translation quality, existing approaches face significant
challenges in practical scenarios: insufficient contextual understanding,
inflexible prompt designs, and inadequate error correction mechanisms. These
limitations severely hinder accurate and efficient translation of complex,
real-world code repositories. To address these challenges, we propose
RepoTransAgent, a novel multi-agent LLM framework for repository-aware code
translation. RepoTransAgent systematically decomposes the translation process
into specialized subtasks-context retrieval, dynamic prompt construction, and
iterative code refinement-each handled by dedicated agents. Our approach
leverages retrieval-augmented generation (RAG) for contextual information
gathering, employs adaptive prompts tailored to varying repository scenarios,
and introduces a reflection-based mechanism for systematic error correction. We
evaluate RepoTransAgent on hundreds of Java-C# translation pairs from six
popular open-source projects. Experimental results demonstrate that
RepoTransAgent significantly outperforms state-of-the-art baselines in both
compile and pass rates. Specifically, RepoTransAgent achieves up to 55.34%
compile rate and 45.84% pass rate. Comprehensive analysis confirms the
robustness and generalizability of RepoTransAgent across different LLMs,
establishing its effectiveness for real-world repository-aware code
translation.

</details>


### [17] [Logging Requirement for Continuous Auditing of Responsible Machine Learning-based Applications](https://arxiv.org/abs/2508.17851)
*Patrick Loic Foalem,Leuson Da Silva,Foutse Khomh,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: 本文探讨了机器学习（ML）应用中透明度和问责制的不足，并提出了通过日志记录来增强审计和合规性的方法。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习在决策自动化中的应用日益广泛，但其透明性、公平性和问责性不足，引发了伦理和法律合规的担忧。

Method: 研究提出了通过日志记录（传统软件中的常见做法）来系统审计ML应用，以追踪系统行为并支持调试、性能分析和合规审计。

Result: 研究表明，增强日志记录实践和工具，整合负责任AI指标，可以提升ML系统的可审计性和透明度。

Conclusion: 本文为实践者和工具开发者提供了实际建议，以增强ML应用的问责性和可信度，满足监管和社会期望。

Abstract: Machine learning (ML) is increasingly applied across industries to automate
decision-making, but concerns about ethical and legal compliance remain due to
limited transparency, fairness, and accountability. Monitoring through logging
a long-standing practice in traditional software offers a potential means for
auditing ML applications, as logs provide traceable records of system behavior
useful for debugging, performance analysis, and continuous auditing.
systematically auditing models for compliance or accountability. The findings
underscore the need for enhanced logging practices and tooling that
systematically integrate responsible AI metrics. Such practices would support
the development of auditable, transparent, and ethically responsible ML
systems, aligning with growing regulatory requirements and societal
expectations. By highlighting specific deficiencies and opportunities, this
work provides actionable guidance for both practitioners and tool developers
seeking to strengthen the accountability and trustworthiness of ML
applications.

</details>


### [18] [modelSolver: A Symbolic Model-Driven Solver for Power Network Simulation and Monitoring](https://arxiv.org/abs/2508.17882)
*Izudin Dzafic,Rabih A. Jabr*

Main category: cs.SE

TL;DR: 介绍了一个名为modelSolver的软件工具，通过符号数学建模框架简化电力系统分析，无需编程即可自定义模型。


<details>
  <summary>Details</summary>
Motivation: 电力系统分析软件通常需要编程技能，这对缺乏编程能力的领域专家构成挑战。modelSolver旨在消除这一障碍。

Method: 基于符号数学建模的框架，用户可通过直观数学表达式定义模型，支持复杂变量，兼容MATPOWER。

Result: 支持多种电力系统功能（如潮流计算、状态估计），适合学生、科学家和从业者使用。

Conclusion: modelSolver简化了电力系统计算，使领域专家无需编程即可专注于建模。

Abstract: The development of advanced software tools for power system analysis requires
extensive programming expertise. Even when using open-source tools, programming
skills are essential to modify built-in models. This can be particularly
challenging for domain experts who lack coding proficiency. This paper
introduces modelSolver, a software solution with a new framework centered
around symbolic mathematical modeling. The proposed paradigm facilitates
defining models through intuitive mathematical expressions, thus eliminating
the need for traditional programming constructs such as arrays, loops, and
sparse matrix computations. The modelSolver focuses on power flow and state
estimation using an open-box approach, which allows users to specify custom
models using either real or complex variables. Unlike existing tools that rely
on hard-coded models, modelSolver enables the representation of a wide range of
advanced functionalities, including power flow with voltage regulators and load
tap changers, continuation power flow, and Gauss-Newton state estimation with
equality constraints. Compatibility with MATPOWER is ensured via a converter
that automates importing data files. The framework prioritizes model-driven
development and empowers domain experts to focus on power system modeling
without programming barriers. It aims to simplify power system computations,
making them more accessible to students, scientists, and practitioners.

</details>


### [19] [A Defect Classification Framework for AI-Based Software Systems (AI-ODC)](https://arxiv.org/abs/2508.17900)
*Mohammed O. Alannsary*

Main category: cs.SE

TL;DR: 本文针对AI系统的缺陷分析提出了一种基于正交缺陷分类（ODC）的改进框架AIODC，识别了AI特有的数据、学习和思考维度，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前缺陷分析模型未能捕捉AI系统的独特属性，需要一种适应AI特性的缺陷分析方法以提高软件质量。

Method: 改进ODC框架，新增数据、学习和思考维度，调整属性和严重性级别，并将其应用于公开的机器学习缺陷数据集，进行单向和双向分析。

Result: 学习阶段的缺陷最常见且与高严重性相关，思考阶段的缺陷对信任和准确性影响显著，AIODC能识别高风险缺陷类别。

Conclusion: AIODC框架为AI系统缺陷分析提供了有效工具，有助于针对性质量保证措施的制定。

Abstract: Artificial Intelligence has gained a lot of attention recently, it has been
utilized in several fields ranging from daily life activities, such as
responding to emails and scheduling appointments, to manufacturing and
automating work activities. Artificial Intelligence systems are mainly
implemented as software solutions, and it is essential to discover and remove
software defects to assure its quality using defect analysis which is one of
the major activities that contribute to software quality. Despite the
proliferation of AI-based systems, current defect analysis models fail to
capture their unique attributes. This paper proposes a framework inspired by
the Orthogonal Defect Classification (ODC) paradigm and enables defect analysis
of Artificial Intelligence systems while recognizing its special attributes and
characteristics. This study demonstrated the feasibility of modifying ODC for
AI systems to classify its defects. The ODC was adjusted to accommodate the
Data, Learning, and Thinking aspects of AI systems which are newly introduced
classification dimensions. This adjustment involved the introduction of an
additional attribute to the ODC attributes, the incorporation of a new severity
level, and the substitution of impact areas with characteristics pertinent to
AI systems. The framework was showcased by applying it to a publicly available
Machine Learning bug dataset, with results analyzed through one-way and two-way
analysis. The case study indicated that defects occurring during the Learning
phase were the most prevalent and were significantly linked to high-severity
classifications. In contrast, defects identified in the Thinking phase had a
disproportionate effect on trustworthiness and accuracy. These findings
illustrate AIODC's capability to identify high-risk defect categories and
inform focused quality assurance measures.

</details>


### [20] [Evaluating Citizen Satisfaction with Saudi Arabia's E-Government Services: A Standards-Based, Theory-Informed Approach](https://arxiv.org/abs/2508.17912)
*Mohammed O. Alannsary*

Main category: cs.SE

TL;DR: 该研究通过ISO标准和UTAUT理论评估沙特阿拉伯电子政府服务的公民满意度，发现高满意度和信任度，但仍存在服务清晰度和响应性问题，情感投入较低。


<details>
  <summary>Details</summary>
Motivation: 了解公民对电子政府平台的评价，以提升服务的可用性、信任度和包容性。

Method: 基于ISO/IEC 25010和ISO/IEC 25022标准，结合UTAUT理论，对500名公民进行问卷调查，分析276份有效回复。

Result: 公民在可用性和信任度上表现出高满意度，但服务清晰度和系统响应性仍有挑战，情感投入较低。

Conclusion: 研究为政策制定者提供了宝贵见解，并推动了标准和行为模型在公民服务中的理论整合。

Abstract: As digital government platforms become central to public service delivery,
understanding citizen assessment is crucial for enhancing usability, trust, and
inclusivity. This study investigates citizen satisfaction with the e-government
services in Saudi Arabia through a quality-in-use framework based on ISO/IEC
25010 and ISO/IEC 25022 standards, interpreted through the lens of the Unified
Theory of Acceptance and Use of Technology (UTAUT). A structured questionnaire
was administered to 500 citizens, yielding 276 valid responses. Satisfaction
was evaluated across four dimensions: overall satisfaction, feature
satisfaction, trust, and emotional engagement (pleasure). The findings
demonstrate consistently high levels of satisfaction regarding usability and
trust, aligning with Saudi Arabia's top-tier global ranking in e-government
development. However, the results also highlight persistent challenges related
to service clarity and system responsiveness. Emotional engagement was limited,
indicating that users perceive these services primarily as functional tools
rather than as engaging digital experiences. The study offers valuable insights
for policymakers and contributes to the theoretical integration of
standards-based and behavioral adoption models in the context of citizenship.

</details>


### [21] [DesCartes Builder: A Tool to Develop Machine-Learning Based Digital Twins](https://arxiv.org/abs/2508.17988)
*Eduardo de Conto,Blaise Genest,Arvind Easwaran,Nicholas Ng,Shweta Menon*

Main category: cs.SE

TL;DR: 本文提出了一种名为DesCartes Builder的开源工具，用于系统地设计基于机器学习的数字孪生（DT）原型和实例。该工具通过可视化数据流范式，支持ML模型的规范、组合和重用，并集成了适用于DT设计的核心操作和算法。通过一个土木工程用例，验证了其有效性和可用性。


<details>
  <summary>Details</summary>
Motivation: 数字孪生（DTs）在复杂系统中广泛应用，但其机器学习（ML）设计过程缺乏结构化方法。传统ML通常针对单一任务训练模型，而DT需要多个任务和领域相关的模型，因此需要更系统的设计方法。

Method: 开发了DesCartes Builder工具，利用可视化数据流范式，支持ML模型的规范、组合和重用，并集成核心操作和算法库。

Result: 通过土木工程用例（预测结构塑性应变）验证了工具的有效性和可用性。

Conclusion: DesCartes Builder提供了系统化的ML设计方法，为数字孪生的开发提供了实用工具和支持。

Abstract: Digital twins (DTs) are increasingly utilized to monitor, manage, and
optimize complex systems across various domains, including civil engineering. A
core requirement for an effective DT is to act as a fast, accurate, and
maintainable surrogate of its physical counterpart, the physical twin (PT). To
this end, machine learning (ML) is frequently employed to (i) construct
real-time DT prototypes using efficient reduced-order models (ROMs) derived
from high-fidelity simulations of the PT's nominal behavior, and (ii)
specialize these prototypes into DT instances by leveraging historical sensor
data from the target PT. Despite the broad applicability of ML, its use in DT
engineering remains largely ad hoc. Indeed, while conventional ML pipelines
often train a single model for a specific task, DTs typically require multiple,
task- and domain-dependent models. Thus, a more structured approach is required
to design DTs.
  In this paper, we introduce DesCartes Builder, an open-source tool to enable
the systematic engineering of ML-based pipelines for real-time DT prototypes
and DT instances. The tool leverages an open and flexible visual data flow
paradigm to facilitate the specification, composition, and reuse of ML models.
It also integrates a library of parameterizable core operations and ML
algorithms tailored for DT design. We demonstrate the effectiveness and
usability of DesCartes Builder through a civil engineering use case involving
the design of a real-time DT prototype to predict the plastic strain of a
structure.

</details>


### [22] [Previously on... Automating Code Review](https://arxiv.org/abs/2508.18003)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: 该研究首次全面分析了现代代码审查（MCR）自动化研究，系统调查了691篇文献并筛选出24篇，总结任务、模型、指标等方面，提出了标准化建议。


<details>
  <summary>Details</summary>
Motivation: 现代代码审查消耗大量资源，自动化需求日益增长，但研究任务定义、数据集和评估方法存在较大差异。

Method: 通过系统调查和分析2015年至2024年的24篇相关研究，总结任务、模型、指标等内容。

Result: 研究发现48种任务指标组合中22种是独特的，数据集重用有限，还揭示了未充分解决的挑战如时间偏差。

Conclusion: 该研究为领域提供了清晰概览，提出标准化建议，帮助未来研究避免陷阱。

Abstract: Modern Code Review (MCR) is a standard practice in software engineering, yet
it demands substantial time and resource investments. Recent research has
increasingly explored automating core review tasks using machine learning (ML)
and deep learning (DL). As a result, there is substantial variability in task
definitions, datasets, and evaluation procedures. This study provides the first
comprehensive analysis of MCR automation research, aiming to characterize the
field's evolution, formalize learning tasks, highlight methodological
challenges, and offer actionable recommendations to guide future research.
Focusing on the primary code review tasks, we systematically surveyed 691
publications and identified 24 relevant studies published between May 2015 and
April 2024. Each study was analyzed in terms of tasks, models, metrics,
baselines, results, validity concerns, and artifact availability. In
particular, our analysis reveals significant potential for standardization,
including 48 task metric combinations, 22 of which were unique to their
original paper, and limited dataset reuse. We highlight challenges and derive
concrete recommendations for examples such as the temporal bias threat, which
are rarely addressed so far. Our work contributes to a clearer overview of the
field, supports the framing of new research, helps to avoid pitfalls, and
promotes greater standardization in evaluation practices.

</details>


### [23] [A Large-Scale Study on Developer Engagement and Expertise in Configurable Software System Projects](https://arxiv.org/abs/2508.18070)
*Karolina M. Milano,Wesley K. G. Assunção,Bruno B. P. Cafeo*

Main category: cs.SE

TL;DR: 研究表明，可配置软件系统中可变代码的开发与维护高度集中在少数开发者手中，传统专业知识指标在此领域表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探讨可变代码在开发者中的分布情况以及传统专业知识指标是否足以衡量可变代码的熟练度。

Method: 分析了25个可配置软件项目的9,678名开发者的450,255次提交，研究开发者对可变代码和强制代码的参与情况。

Result: 59%的开发者从未修改可变代码，17%的开发者负责83%的可变代码工作；传统专业知识指标的准确率和召回率仅为55%和50%。

Conclusion: 可变代码的责任分配不均，需要改进专业知识指标以更公平地分配任务。

Abstract: Modern systems operate in multiple contexts making variability a fundamental
aspect of Configurable Software Systems (CSSs). Variability, implemented via
pre-processor directives (e.g., #ifdef blocks) interleaved with other code and
spread across files, complicates maintenance and increases error risk. Despite
its importance, little is known about how variable code is distributed among
developers or whether conventional expertise metrics adequately capture
variable code proficiency. This study investigates developers' engagement with
variable versus mandatory code, the concentration of variable code workload,
and the effectiveness of expertise metrics in CSS projects. We mined
repositories of 25 CSS projects, analyzing 450,255 commits from 9,678
developers. Results show that 59% of developers never modified variable code,
while about 17% were responsible for developing and maintaining 83% of it. This
indicates a high concentration of variable code expertise among a few
developers, suggesting that task assignments should prioritize these
specialists. Moreover, conventional expertise metrics performed
poorly--achieving only around 55% precision and 50% recall in identifying
developers engaged with variable code. Our findings highlight an unbalanced
distribution of variable code responsibilities and underscore the need to
refine expertise metrics to better support task assignments in CSS projects,
thereby promoting a more equitable workload distribution.

</details>


### [24] [Debian in the Research Software Ecosystem: A Bibliometric Analysis](https://arxiv.org/abs/2508.18073)
*Joenio Marques da Costa,Christina von Flach*

Main category: cs.SE

TL;DR: 研究通过文献计量分析探讨Debian系统在学术研究中的影响，分类文献、映射研究趋势，并提供数据以助未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 目的于理解Debian在学术研究中的角色及其对研究软件生态的贡献，识别研究趋势与机会。

Method: 使用Scopus数据库，基于关键词'Debian'进行文献计量分析，包括共引、合著及词共现分析。

Result: 研究涵盖多领域文献，提供了Debian相关学术出版的空间地图，包括高引论文、活跃国家与研究者等数据。

Conclusion: 结果揭示了Debian学术研究的智力结构，帮助研究者把握趋势并识别需更多关注的领域。

Abstract: Context: The Debian system has historically participated in academic works
and scientific projects, with well-known examples including NeuroDebian, Debian
Med, Debsources, Debian Science, and Debian GIS, where the scientific relevance
of Debian and its contribution to the Research Software ecosystem are evident.
  Objective: The objective of this study is to investigate the Debian system
through academic publications, with the aim of classifying articles, mapping
research, identifying trends, and finding opportunities.
  Method: The study is based on a bibliometric analysis starting with an
initial search for the term "Debian" in the titles, abstracts, or keywords of
academic publications, using the Scopus database. This analysis calculates
metrics of co-citation, co-authorship, and word co-occurrence, and is guided by
a set of research questions and criteria for inclusion and exclusion to conduct
the bibliometric analysis.
  Results: The study includes a set of articles published across various fields
of knowledge, providing a map of the academic publication space about Debian.
The study's data will be available in a public repository, reporting
demographic and bibliometric trends, including the most cited articles, active
countries, researchers, and popular conferences.
  Conclusion: Results includes a bibliometric and demographic analysis
identified in publications about Debian, shedding light on the intellectual
structure of academic research. The results of the analyses can help
researchers gain an overview of existing trends in publications about Debian
and identify areas that require more attention from the scientific community.

</details>


### [25] [LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated Software Evolution](https://arxiv.org/abs/2508.18089)
*Karine Even-Mendoza,Alexander Brownlee,Alina Geiger,Carol Hanna,Justyna Petke,Federica Sarro,Dominik Sobania*

Main category: cs.SE

TL;DR: 论文提出了一种结合遗传改进（GI）和大语言模型（LLM）的新方法PatchCat，通过自动聚类LLM生成的补丁来提高语义感知搜索的效率。


<details>
  <summary>Details</summary>
Motivation: 传统的搜索型GI在语法层面表现优秀，但缺乏语义感知能力；而LLM提供了语义感知的编辑，但缺乏目标导向的反馈和控制。结合两者的优势，可以提升软件改进的效率。

Method: 通过自动化聚类LLM生成的补丁（PatchCat），对补丁进行分类和优化，从而增强GI的语义感知能力。

Result: PatchCat成功识别了18种不同的软件补丁类型，并能够高精度分类新补丁。此外，它能提前检测无效编辑（NoOp），节省测试资源。

Conclusion: PatchCat是一种有前景的方法，能够实现高效、可解释且环保的GI。未来工作将进一步探索LLM驱动的突变原理，并利用语义信号指导GI搜索过程。

Abstract: Genetic Improvement (GI) of software automatically creates alternative
software versions that are improved according to certain properties of
interests (e.g., running-time). Search-based GI excels at navigating large
program spaces, but operates primarily at the syntactic level. In contrast,
Large Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed
feedback and control (which is instead a strength of GI). As such, we propose
the investigation of a new research line on AI-powered GI aimed at
incorporating semantic aware search. We take a first step at it by augmenting
GI with the use of automated clustering of LLM edits. We provide initial
empirical evidence that our proposal, dubbed PatchCat, allows us to
automatically and effectively categorize LLM-suggested patches. PatchCat
identified 18 different types of software patches and categorized newly
suggested patches with high accuracy. It also enabled detecting NoOp edits in
advance and, prospectively, to skip test suite execution to save resources in
many cases. These results, coupled with the fact that PatchCat works with
small, local LLMs, are a promising step toward interpretable, efficient, and
green GI. We outline a rich agenda of future work and call for the community to
join our vision of building a principled understanding of LLM-driven mutations,
guiding the GI search process with semantic signals.

</details>


### [26] [A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code](https://arxiv.org/abs/2508.18106)
*Keke Lian,Bin Wang,Lei Zhang,Libo Chen,Junjie Wang,Ziming Zhao,Yujiu Yang,Haotong Duan,Haoran Zhao,Shuang Liao,Mingda Guo,Jiazheng Quan,Yilu Zhong,Chenhao He,Zichuan Chen,Jie Wu,Haoling Li,Zhaoxuan Li,Jiongchi Yu,Hui Li,Dong Zhang*

Main category: cs.SE

TL;DR: 该论文提出了A.S.E基准，用于评估大型语言模型在代码生成中的安全性，填补了现有评测的不足，并测试了领先模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有评测方法在代码生成安全性评估上存在不足，如忽略仓库级上下文、缺乏可重复性，未能关联输入与输出安全。

Method: 提出A.S.E基准，基于真实仓库和CVE构建任务，保留完整上下文，使用容器化框架和专家规则进行稳定评估。

Result: Claude-3.7-Sonnet表现最佳，开源与专有模型安全差距窄，简明解码策略在安全补丁上优于复杂推理。

Conclusion: A.S.E填补了现有评测的空白，简明策略在代码安全生成中更具优势。

Abstract: The increasing adoption of large language models (LLMs) in software
engineering necessitates rigorous security evaluation of their generated code.
However, existing benchmarks are inadequate, as they focus on isolated code
snippets, employ unstable evaluation methods that lack reproducibility, and
fail to connect the quality of input context with the security of the output.
To address these gaps, we introduce A.S.E (AI Code Generation Security
Evaluation), a benchmark for repository-level secure code generation. A.S.E
constructs tasks from real-world repositories with documented CVEs, preserving
full repository context like build systems and cross-file dependencies. Its
reproducible, containerized evaluation framework uses expert-defined rules to
provide stable, auditable assessments of security, build quality, and
generation stability. Our evaluation of leading LLMs on A.S.E reveals three key
findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The
security gap between proprietary and open-source models is narrow;
Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise,
``fast-thinking'' decoding strategies consistently outperform complex,
``slow-thinking'' reasoning for security patching.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [27] [SafeTree: Expressive Tree Policies for Microservices](https://arxiv.org/abs/2508.16746)
*Karuna Grewal,P. Brighten Godfrey,Justin Hsu*

Main category: cs.PL

TL;DR: 本文设计了一种表达丰富的策略语言来描述微服务调用树结构，并开发了一种基于可视化下推自动机的动态执行机制来强制执行服务树策略，通过在服务网格上构建运行时监控器实现低延迟的安全策略执行。


<details>
  <summary>Details</summary>
Motivation: 当前微服务部署工具仅支持有限的一跳策略，忽略微服务调用树结构，可能导致过度宽松的权限控制，需要更细粒度的通信模式控制。

Method: 使用可视化下推自动机设计服务树策略语言，并在服务网格（如Istio）上构建非侵入式的分布式运行时监控器。

Result: 实验显示监控器能高效执行复杂安全策略，仅增加毫秒级延迟。

Conclusion: 通过服务网格实现的服务树策略可以显著提升微服务间通信的安全性和控制精度。

Abstract: A microservice-based application is composed of multiple self-contained
components called microservices, and controlling inter-service communication is
important for enforcing safety properties. Presently, inter-service
communication is configured using microservice deployment tools. However, such
tools only support a limited class of single-hop policies, which can be overly
permissive because they ignore the rich service tree structure of microservice
calls. Policies that can express the service tree structure can offer
development and security teams more fine-grained control over communication
patterns.
  To this end, we design an expressive policy language to specify service tree
structures, and we develop a visibly pushdown automata-based dynamic
enforcement mechanism to enforce service tree policies. Our technique is
non-invasive: it does not require any changes to service implementations, and
does not require access to microservice code. To realize our method, we build a
runtime monitor on top of a service mesh, an emerging network infrastructure
layer that can control inter-service communication during deployment. In
particular, we employ the programmable network traffic filtering capabilities
of Istio, a popular service mesh implementation, to implement an online and
distributed monitor. Our experiments show that our monitor can enforce rich
safety properties while adding minimal latency overhead on the order of
milliseconds.

</details>


### [28] [Syntactic Completions with Material Obligations](https://arxiv.org/abs/2508.16848)
*David Moon,Andrew Blinn,Thomas J. Porter,Cyrus Omar*

Main category: cs.PL

TL;DR: 论文介绍了$	exttt{tylr}$，一种通过插入“义务”补全语法错误的解析器和编辑器生成器，结合了文本与结构编辑器的优势。


<details>
  <summary>Details</summary>
Motivation: 现有语法错误恢复技术要么过于粗糙（如删除大量代码），要么导致过多的补全选项，$	exttt{tylr}$旨在解决这一问题。

Method: 提出基于“瓦片解析”理论的$	exttt{tylr}$，通过语法行走和“成型”系统扩展语法表达能力，并最小化补全义务。

Result: $	exttt{tylr}$能够有效补全错误代码，且支持可视化义务的编辑器设计，用户研究表明其具有实用性和潜力。

Conclusion: $	exttt{tylr}$为语法错误恢复和编辑器设计提供了新思路，未来工作可进一步优化其用户体验。

Abstract: Code editors provide essential services that help developers understand,
navigate, and modify programs. However, these services often fail in the
presence of syntax errors. Existing syntax error recovery techniques, like
panic mode and multi-option repairs, are either too coarse, e.g. in deleting
large swathes of code, or lead to a proliferation of possible completions. This
paper introduces $\texttt{tylr}$, a parser and editor generator that completes
arbitrarily malformed code by inserting obligations, which generalize holes to
cover missing operands, operators, mixfix keywords, and sort transitions.
$\texttt{tylr}$ is backed by a novel theory of tile-based parsing, which
extends operator-precedence parsing in two ways. First, traditional token
precedence comparisons are replaced by a notion of grammar walks, which form
the basis for generating obligations. Second, a distinct "molding" system based
on grammar zippers expand grammar expressivity by allowing the system to
disambiguate between possible parses and completions based on an obligation
minimization criterion. In addition to serving as a novel approach to error
correction, $\texttt{tylr}$'s design enables the development of an editor that
visually materializes obligations to the human user, serving as a novel hybrid
between a text editor and a structure editor. We introduce $\texttt{tylr}$ by
example, then formalize its key ideas. Finally, we conduct a human subjects
study to evaluate the extent to which an editor like $\texttt{tylr}$ that
materializes syntactic obligations might be usable and useful, finding both
points of positivity and interesting new avenues for future work.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [29] [H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2508.16653)
*Zizhuo Fu,Xiaotian Guo,Wenxuan Zeng,Shuzhang Zhong,Yadong Zhang,Peiyu Chen,Runsheng Wang,Le Ye,Meng Li*

Main category: cs.PF

TL;DR: H2EAL是一种基于混合键合技术的加速器，通过稀疏注意力算法与硬件协同设计，优化边缘设备上的大型语言模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在边缘部署时因KV缓存导致的高能耗和延迟问题，利用新兴的混合键合技术提供更高效的带宽和低功耗。

Method: 提出了一种混合稀疏注意力方案（静态和动态稀疏），并设计了硬件支持协同计算与内存分配，同时开发了负载均衡调度器。

Result: 实验表明，H2EAL相比基线实现了5.20~48.21倍的加速和6.22~73.48倍的能效提升，准确率仅下降0.87%。

Conclusion: H2EAL通过算法与硬件协同设计，有效解决了边缘设备上LLM推理的瓶颈问题，展示了混合键合技术在高效计算中的潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable proficiency in a
wide range of natural language processing applications. However, the high
energy and latency overhead induced by the KV cache limits the edge deployment,
especially for long contexts. Emerging hybrid bonding (HB) technology has been
proposed as a promising alternative to conventional near-memory processing
(NMP) architectures, offering improved bandwidth efficiency and lower power
consumption while exhibiting characteristics of distributed memory. In this
paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse
attention algorithm-hardware co-design for efficient LLM inference at the edge.
At the algorithm level, we propose a hybrid sparse attention scheme with static
and dynamic sparsity for different heads to fully leverage the sparsity with
high accuracy. At the hardware level, we co-design the hardware to support
hybrid sparse attention and propose memory-compute co-placement to address the
distributed memory bottleneck. Since different attention heads exhibit
different sparse patterns and the attention structure often mismatches the HB
architecture, we further develop a load-balancing scheduler with parallel tiled
attention to address workload imbalance and optimize the mapping strategy.
Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and
6.22~73.48x energy efficiency improvement over baseline HB implementation, with
a negligible average accuracy drop of 0.87% on multiple benchmarks.

</details>


### [30] [Dynamic Sparse Attention on Mobile SoCs](https://arxiv.org/abs/2508.16703)
*Wangsong Yin,Daliang Xu,Mengwei Xu,Gang Huang,Xuanzhe Liu*

Main category: cs.PF

TL;DR: 本文提出了shadowAttn，一种系统-算法协同设计的稀疏注意力模块，通过减少对CPU/GPU的依赖来提升设备运行LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有框架中的注意力操作因量化敏感性而无法充分利用NPU，导致性能下降和系统调度复杂化。

Method: 设计了shadowAttn模块，通过稀疏计算注意力、NPU辅助计算、计算图分桶等技术实现高效运行。

Result: shadowAttn在有限资源下表现最佳，且能以更少资源实现与现有框架相当的性能。

Conclusion: shadowAttn通过协同设计显著提升了设备运行LLM的效率和性能。

Abstract: On-device running Large Language Models (LLMs) is nowadays a critical enabler
towards preserving user privacy. We observe that the attention operator falls
back from the special-purpose NPU to the general-purpose CPU/GPU because of
quantization sensitivity in state-of-the-art frameworks. This fallback results
in a degraded user experience and increased complexity in system scheduling. To
this end, this paper presents shadowAttn, a system-algorithm codesigned sparse
attention module with minimal reliance on CPU/GPU by only sparsely calculating
the attention on a tiny portion of tokens. The key idea is to hide the overhead
of estimating the important tokens with a NPU-based pilot compute. Further,
shadowAttn proposes insightful techniques such as NPU compute graph bucketing,
head-wise NPU-CPU/GPU pipeline and per-head fine-grained sparsity ratio to
achieve high accuracy and efficiency. shadowAttn delivers the best performance
with highly limited CPU/GPU resource; it requires much less CPU/GPU resource to
deliver on-par performance of SoTA frameworks.

</details>


### [31] [Systematic Characterization of LLM Quantization: A Performance, Energy, and Quality Perspective](https://arxiv.org/abs/2508.16712)
*Tianyao Shi,Yi Ding*

Main category: cs.PF

TL;DR: 本文研究了大型语言模型（LLMs）量化方法在性能、能耗和质量之间的权衡，并开发了自动化框架qMeter进行在线评估。


<details>
  <summary>Details</summary>
Motivation: 量化LLMs以降低资源需求，但在实际应用中缺乏对其综合表现的系统性理解。

Method: 开发qMeter框架，评估11种量化方法在4种模型大小和2种GPU架构下的表现。

Result: 量化效果高度依赖于任务和方法，且受工作负载特性和硬件架构影响显著。

Conclusion: 研究为LLM量化的实际部署提供了多层次优化方向，是首个综合评估量化性能、能耗和质量的工作。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse domains, but their heavy resource demands make quantization-reducing
precision to lower-bit formats-critical for efficient serving. While many
quantization methods exist, a systematic understanding of their performance,
energy, and quality tradeoffs in realistic serving conditions remains a gap. In
this work, we first develop a fully automated online characterization framework
qMeter, and then conduct an in-depth characterization of 11 post-training LLM
quantization methods across 4 model sizes (7B-70B) and two GPU architectures
(A100, H100). We evaluate quantization at the application, workload,
parallelism, and hardware levels under online serving conditions. Our study
reveals highly task- and method-dependent tradeoffs, strong sensitivity to
workload characteristics, and complex interactions with parallelism and GPU
architecture. We further present three optimization case studies illustrating
deployment challenges in capacity planning, energy-efficient scheduling, and
multi-objective tuning. To the best of our knowledge, this is one of the first
comprehensive application-, system-, and hardware-level characterization of LLM
quantization from a joint performance, energy, and quality perspective.

</details>


### [32] [Evaluación y modelado del rendimiento de los sistemas informáticos](https://arxiv.org/abs/2508.16996)
*Xavier Molero,Carlos Juiz,Miguel Jesus Rodeno*

Main category: cs.PF

TL;DR: 本书是关于计算机系统性能评估和建模的实用指南，结合理论与实践，提供多种问题的分类和解决方案。


<details>
  <summary>Details</summary>
Motivation: 旨在通过简明的定量技术帮助解答实际问题，而不仅限于理论探讨。

Method: 每章先回顾理论，再深入探讨三类问题：完全解答、仅提供解答和留待读者解决的问题。

Result: 通过学术性问题展示解决实际工业规模问题的方法和工具。

Conclusion: 书中内容虽有时显得学术化，但为实际问题的解决提供了宝贵的指导和工具。

Abstract: This book, by Molero, Juiz, and Rodeno, titled Performance Evaluation and
Modeling of Computer Systems, presents a comprehensive summary of simple
quantitative techniques that help answer the above questions. Its approach is
not one of theory for theory's sake; rather, in each chapter, after a brief
theoretical review, it delves deeply into numerous problems grouped into three
categories: those with complete solutions, those for which only the solution is
given, and, finally, those whose resolution is left to the reader's discretion.
Although some of the solved problems may be considered purely academic in terms
of complexity, they should not be underestimated, as they reveal, on a reduced
scale, the process that must be followed with the help of appropriate tools to
solve equivalent real-world problems of an industrial scale.

</details>


### [33] [The Unwritten Contract of Cloud-based Elastic Solid-State Drives](https://arxiv.org/abs/2508.17372)
*Yingjia Wang,Ming-Chang Yang*

Main category: cs.PF

TL;DR: 论文首次对Amazon AWS和Alibaba Cloud的弹性固态硬盘(ESSD)性能进行了分析，揭示了ESSD与传统本地SSD的差异，并提出了四点观察和五点建议，帮助云存储用户优化系统设计。


<details>
  <summary>Details</summary>
Motivation: 尽管云服务中广泛使用弹性块存储(EBS)和ESSD，但缺乏对其性能的全面评估，引发了对ESSD是否能替代本地SSD并提供相似性能的疑问。

Method: 通过对比分析Amazon AWS和Alibaba Cloud的两款ESSD，总结出其性能特点。

Result: 揭示了ESSD与传统本地SSD的四点反直觉差异，并提出了五点优化建议。

Conclusion: ESSD的性能特性与本地SSD不同，用户应重新设计云软件以充分利用其特性，从而提升系统性能。

Abstract: Elastic block storage (EBS) with the storage-compute disaggregated
architecture stands as a pivotal piece in today's cloud. EBS furnishes users
with storage capabilities through the elastic solid-state drive (ESSD).
Nevertheless, despite the widespread integration into cloud services, the
absence of a thorough ESSD performance characterization raises critical doubt:
when more and more services are shifted onto the cloud, can ESSD satisfactorily
substitute the storage responsibilities of the local SSD and offer comparable
performance?
  In this paper, we for the first time target this question by characterizing
two ESSDs from Amazon AWS and Alibaba Cloud. We present an unwritten contract
of cloud-based ESSDs, encapsulating four observations and five implications for
cloud storage users. Specifically, the observations are counter-intuitive and
contrary to the conventional perceptions of what one would expect from the
local SSD. The implications we hope could guide users in revisiting the designs
of their deployed cloud software, i.e., harnessing the distinct characteristics
of ESSDs for better system performance.

</details>


### [34] [Evaluating Compiler Optimization Impacts on zkVM Performance](https://arxiv.org/abs/2508.17518)
*Thomas Gassmann,Stefanos Chaliasos,Thodoris Sotiropoulos,Zhendong Su*

Main category: cs.PF

TL;DR: 该论文首次系统研究了编译器优化对零知识虚拟机（zkVM）性能的影响，发现标准优化对zkVM的效果不如传统CPU，并提出针对性改进策略，提升了zkVM性能。


<details>
  <summary>Details</summary>
Motivation: 零知识虚拟机（zkVM）降低了开发者使用零知识证明的门槛，但其依赖的传统编译器优化并未考虑证明系统的特性，导致性能提升有限。

Method: 研究评估了64种LLVM编译优化、6种标准优化级别和一个未优化基线，并在两个RISC-V架构的zkVM（RISC Zero和SP1）上测试了58个基准程序。

Result: 标准优化对zkVM性能的提升（40%以上）不如传统CPU，但通过针对性改进少量LLVM优化，可进一步提升执行时间（RISC Zero平均+4.6%，SP1 +1%）。

Conclusion: 编译器优化在zkVM性能上有潜力，未来可探索zkVM专用的优化策略和工具。

Abstract: Zero-knowledge proofs (ZKPs) are the cornerstone of programmable
cryptography. They enable (1) privacy-preserving and verifiable computation
across blockchains, and (2) an expanding range of off-chain applications such
as credential schemes. Zero-knowledge virtual machines (zkVMs) lower the
barrier by turning ZKPs into a drop-in backend for standard compilation
pipelines. This lets developers write proof-generating programs in conventional
languages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.
However, these VMs inherit compiler infrastructures tuned for traditional
architectures rather than for proof systems. In particular, standard compiler
optimizations assume features that are absent in zkVMs, including cache
locality, branch prediction, or instruction-level parallelism. Therefore, their
impact on proof generation is questionable.
  We present the first systematic study of the impact of compiler optimizations
on zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an
unoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero
and SP1). While standard LLVM optimization levels do improve zkVM performance
(over 40\%), their impact is far smaller than on traditional CPUs, since their
decisions rely on hardware features rather than proof constraints. Guided by a
fine-grained pass-level analysis, we~\emph{slightly} refine a small set of LLVM
passes to be zkVM-aware, improving zkVM execution time by up to 45\% (average
+4.6\% on RISC Zero, +1\% on SP1) and achieving consistent proving-time gains.
Our work highlights the potential of compiler-level optimizations for zkVM
performance and opens new direction for zkVM-specific passes, backends, and
superoptimizers.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [35] [Iridescent: A Framework Enabling Online System Implementation Specialization](https://arxiv.org/abs/2508.16690)
*Vaastav Anand,Deepak Garg,Antoine Kaufmann*

Main category: cs.OS

TL;DR: Iridescent框架通过自动化在线系统优化，解决了系统定制化的复杂性、通用性损失和配置难题。


<details>
  <summary>Details</summary>
Motivation: 提高系统性能需要针对负载和平台进行定制，但实践中面临开发复杂性、通用性损失和配置优化的挑战。

Method: 提出Iridescent框架，允许开发者定义可能的优化空间，并通过JIT编译在运行时动态生成和测试优化选项，以系统性能指标为指导。

Result: 验证了Iridescent的可行性、有效性和易用性。

Conclusion: Iridescent为系统优化提供了一种自动化且实用的解决方案。

Abstract: Specializing systems to specifics of the workload they serve and platform
they are running on often significantly improves performance. However,
specializing systems is difficult in practice because of compounding
challenges: i) complexity for the developers to determine and implement optimal
specialization; ii) inherent loss of generality of the resulting
implementation, and iii) difficulty in identifying and implementing a single
optimal specialized configuration for the messy reality of modern systems. To
address this, we introduce Iridescent, a framework for automated online system
specialization guided by observed overall system performance. Iridescent lets
developers specify a space of possible specialization choices, and then at
runtime generates and runs different specialization choices through JIT
compilation as the system runs. By using overall system performance metrics to
guide this search, developers can use Iridescent to find optimal system
specializations for the hardware and workload conditions at a given time. We
demonstrate feasibility, effectivity, and ease of use.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [36] [QoS-based Intelligent multi-connectivity for B5G networks](https://arxiv.org/abs/2508.16816)
*Ali Parsa,Neda Moghim,Sachin Shetty*

Main category: cs.NI

TL;DR: 提出了一种基于机器学习的QoS感知多连接框架，通过深度神经网络预测基站的服务质量指标，优化连接选择和资源分配，显著提升QoS和频谱效率。


<details>
  <summary>Details</summary>
Motivation: 随着通信技术的发展，蜂窝网络需满足多样化的服务质量需求，统一基础设施中的多连接技术成为关键挑战。

Method: 使用深度神经网络预测基站的数据速率、可靠性和延迟等QoS指标，并基于这些预测选择最优服务集群和数据速率分配。

Result: 算法实现了98%的QoS成功率，并将频谱效率提高了30%优于现有解决方案。

Conclusion: 该框架通过机器学习显著提升了QoS和频谱效率，为多连接技术提供了有效解决方案。

Abstract: The rapid advancement of communication technologies has established cellular
networks as the backbone for diverse applications, each with distinct quality
of service requirements. Meeting these varying demands within a unified
infrastructure presents a critical challenge that can be addressed through
advanced techniques such as multi-connectivity. Multiconnectivity enables User
equipments to connect to multiple BSs simultaneously, facilitating QoS
differentiation and provisioning. This paper proposes a QoS-aware
multi-connectivity framework leveraging machine learning to enhance network
performance. The approach employs deep neural networks to estimate the
achievable QoS metrics of BSs, including data rate, reliability, and latency.
These predictions inform the selection of serving clusters and data rate
allocation, ensuring that the User Equipment connects to the optimal BSs to
meet its QoS needs. Performance evaluations demonstrate that the proposed
algorithm significantly enhances Quality of Service (QoS) for applications
where traditional and state-of-the-art methods are inadequate. Specifically,
the algorithm achieves a QoS success rate of 98%. Furthermore, it improves
spectrum efficiency by 30% compared to existing multi-connectivity solutions.

</details>


### [37] [Comparison of FTN-NOFDM and PCS-OFDM for Long-Haul Coherent Optical Communications](https://arxiv.org/abs/2508.17350)
*Haide Wang,Ji Zhou,Yongcheng Li,Weiping Liu,Changyuan Yu,Xiangjun Xin,Liangchuan Li*

Main category: cs.NI

TL;DR: 论文提出了FTN-NOFDM技术以提高400G长距离相干光通信的频谱效率，通过低复杂度信号生成和干扰消除方法，实验表明其滤波和非线性容忍度优于传统技术。


<details>
  <summary>Details</summary>
Motivation: 解决单载波调制在低阶调制格式下占用带宽过大的问题，提升长距离相干光通信的频谱效率。

Method: 采用8个子载波的FTN-NOFDM技术，结合快速傅里叶变换和ICI消除，提出基于频率音的定时恢复和MIMO均衡器设计。

Result: FTN-NOFDM在滤波和非线性容忍度上优于PCS-OFDM和QPSK-OFDM，但PCS-OFDM在误码率性能上最佳。

Conclusion: FTN-NOFDM是一种高效的频谱利用方案，适用于长距离光通信系统，尤其在非线性容忍度方面表现优异。

Abstract: Single-wavelength 400G coherent optical communications have become a critical
solution to meet the explosive traffic demands. However, the single-carrier
modulation using low-order modulation formats requires a broader wavelength
division multiplexing grid and expands the occupied optical bandwidth. In this
paper, we propose the faster-than-Nyquist non-orthogonal frequency division
multiplexing (FTN-NOFDM) to improve the spectral efficiency for long-haul
coherent optical communications. The subcarrier number is set to eight to
enable low-complexity FTN-NOFDM signal generation using a pruned inverse fast
Fourier transform and inter-carrier interference (ICI) cancellation. To deal
with the conventional timing recovery (TR) failure, a frequency tone-based TR
is proposed for FTN-NOFDM. A time-domain multiple-input multiple-output
equalizer is designed to update the tap coefficients based on outputs of
conventional iterative detection (ID). To further mitigate ICI, a low-density
parity check-assisted ID is integrated into the conventional ID module.
FTN-NOFDM, probabilistic constellation shaping (PCS)-OFDM, and quadrature phase
shift keying-OFDM are experimentally compared in a 400G coherent optical
communication system over 11 cascaded 125-GHz wavelength-selective switches
(WSSs) and 2000 km transmission. Results show that the FTN-NOFDM exhibits
comparable WSS filtering tolerance to PCS-OFDM and superior nonlinearity
tolerance, while PCS-OFDM achieves the best bit error ratio performance.

</details>


### [38] [Optimizing Anonymity and Efficiency: A Critical Review of Path Selection Strategies in Tor](https://arxiv.org/abs/2508.17651)
*Siddique Abubakr Muntaka,Jacques Bou Abdo*

Main category: cs.NI

TL;DR: 比较五种Tor路径选择策略的性能，地理（延迟优化）方法在延迟和效率上表现最佳，拥塞感知策略在吞吐量上优于基线42%。不同策略适合不同场景。


<details>
  <summary>Details</summary>
Motivation: 随着Tor规模和用户模式的变化，默认路径选择策略面临性能限制，需评估替代策略以优化性能且不损害匿名性。

Method: 使用高保真模拟模型TorPS，评估五种策略（随机、Guard、拥塞感知、地理多样性驱动和延迟优化）在37500个电路中的表现。

Result: 地理（延迟优化）方法延迟最低（40ms），拥塞感知策略吞吐量最高，比基线高42%。

Conclusion: 针对性路径选择可显著提升Tor性能且不损害匿名性，为未来优化提供指导。

Abstract: The Onion Router (Tor) relies on path selection algorithms to balance
performance and anonymity by determining how traffic flows through its relay
network. As Tor scales and usage patterns evolve, default strategies such as
bandwidth-weighted random selection and persistent guard nodes face increasing
performance limitations. This study presents a comparative evaluation of five
path selection strategies: Random, Guard, Congestion-Aware, and two Geographic
approaches (Diversity Driven and Latency-Optimized) using a high-fidelity
simulation model inspired by TorPS (Tor Path Simulator). Experiments were
conducted across five network scales, simulating 37,500 circuits under
realistic relay conditions. Results show that Geographic (Latency-Optimized)
consistently achieved the lowest latency (40.0 ms) and highest efficiency,
while Congestion-Aware strategies delivered the best throughput, outperforming
the baseline by up to 42%. Guard nodes maintained stable routing but exhibited
latency increases under larger networks. No single method proved optimal across
all scenarios, but each revealed clear strengths for specific use cases. These
findings demonstrate that targeted path selection can significantly improve
Tor's performance without compromising anonymity, providing guidance for
optimizing circuit construction in future development and deployments.

</details>


### [39] [Sustainability or Survivability? Eliminating the Need to Choose in LEO Satellite Constellations](https://arxiv.org/abs/2508.17763)
*Chris Misa,Ramakrishnan Durairajan*

Main category: cs.NI

TL;DR: LEO卫星网络的可持续性和生存性问题是研究的核心，提出了一种基于太阳同步轨道的SS-plane设计，显著减少卫星数量和辐射暴露。


<details>
  <summary>Details</summary>
Motivation: 解决LEO卫星网络因忽视互联网流量需求的时空结构和地球空间环境物理现实而导致的效率低下问题。

Method: 提出了一种基于太阳同步轨道的SS-plane设计方法。

Result: SS-plane设计可以将卫星数量减少一个数量级，辐射暴露减少约23%。

Conclusion: 研究结果推动了LEO卫星网络从大型、可丢弃的巨型星座转向更可持续、目标明确的LEO星座。

Abstract: LEO Satellite Networks (LSNs) are revolutionizing global connectivity, but
their reliance on tens of thousands of satellites raises pressing concerns over
sustainability and survivability. In this work, we argue that the
inefficiencies in LSN designs stem from ignoring the strong spatiotemporal
structure of Internet traffic demand (which impacts sustainability) and the
physical realities of the near-Earth space environment (which affects
survivability). We propose a novel design approach based on sun-synchronous
(SS) orbits called SS-plane, which aligns satellite coverage with the Earth's
diurnal cycle. We demonstrate that SS-plane constellations can reduce the
number of satellites required by up to an order of magnitude and cut radiation
exposure by ~23% compared to traditional Walker-delta constellations. These
findings suggest a paradigm shift in LSN research from large, disposable
megaconstellations to more sustainable, targeted LEO constellations.

</details>


### [40] [Real World Assets on-Chain Assistance Low-Altitude Computility Networks: Architecture, Methodology, and Challenges](https://arxiv.org/abs/2508.17911)
*Haoxiang Luo,Ruichen Zhang,Yinqiu Liu,Gang Sun,Hongfang Yu,Zhu Han*

Main category: cs.NI

TL;DR: 论文提出将低空飞行器的计算资源作为代币化的真实世界资产（RWAs），通过区块链进行交易和协调，构建低空计算网络（LACNets），提升任务延迟、信任保障和资源效率。


<details>
  <summary>Details</summary>
Motivation: 低空经济网络（LAENets）是智慧城市服务和商业的新领域，但如何高效共享和信任这些飞行器的计算资源是一个关键挑战。

Method: 通过区块链技术将分布式边缘计算资源代币化，整合飞行器编队为安全、互操作的计算网络，并以城市物流为例进行模拟研究。

Result: 模拟结果表明，基于RWA的协调可以改善任务延迟、信任保障和资源效率。

Conclusion: 未来研究方向包括AI驱动的协调、边缘AI卸载与协作计算，以及跨司法管辖区的代币化资产政策。

Abstract: Low-altitude airspace is becoming a new frontier for smart city services and
commerce. Networks of drones, electric Vertical Takeoff and Landing (eVTOL)
vehicles, and other aircraft, termed Low-Altitude Economic Networks (LAENets),
promise to transform urban logistics, aerial sensing, and communication. A key
challenge is how to efficiently share and trust the computing utility, termed
computility, of these aerial devices. We propose treating the computing power
on aircraft as tokenized Real-World Assets (RWAs) that can be traded and
orchestrated via blockchain. By representing distributed edge computing
resources as blockchain tokens, disparate devices can form Low-Altitude
Computility Networks (LACNets), collaborative computing clusters in the sky. We
first compare blockchain technologies, non-fungible tokens (NFTs), and RWA
frameworks to clarify how physical hardware and its computational output can be
tokenized as assets. Then, we present an architecture using blockchain to
integrate aircraft fleets into a secure, interoperable computing network.
Furthermore, a case study models an urban logistics LACNet of delivery drones
and air-taxis. Simulation results indicate improvements in task latency, trust
assurance, and resource efficiency when leveraging RWA-based coordination.
Finally, we discuss future research directions, including AI-driven
orchestration, edge AI offloading and collaborative computing, and
cross-jurisdictional policy for tokenized assets.

</details>


### [41] [Digital Twin Assisted Proactive Management in Zero Touch Networks](https://arxiv.org/abs/2508.17941)
*Tamizhelakkiya K,Dibakar Das,Komal Sharma,Jyotsna Bapat,Debabrata Das*

Main category: cs.NI

TL;DR: 论文提出了一种结合数字孪生（DT）和零触网络（ZTN）的架构，通过Few-Shot Learning和Q-learning优化网络带宽管理，以适应动态网络条件并提升服务质量。


<details>
  <summary>Details</summary>
Motivation: 随着蜂窝网络的快速扩展和服务质量需求的提升，需要高效、自主的网络管理解决方案。DT和ZTN的结合可以为下一代网络提供智能化管理。

Method: 提出集成DT与ZTN的架构，使用Few-Shot Learning增强BiLSTM模型预测网络状态，并通过Q-learning确定最优动作（如流量整形），满足用户QoS需求。

Result: 仿真结果表明，该架构能适应网络动态变化，且在DT辅助下ZTN性能优于其他技术。

Conclusion: 结合DT的ZTN架构在下一代网络中表现出高效的自适应能力，未来可进一步优化其应用场景。

Abstract: The rapid expansion of cellular networks and rising demand for high-quality
services require efficient and autonomous network management solutions. Zero
Touch Network (ZTN) management has emerged as a key approach to automating
network operations, minimizing manual intervention, and improving service
reliability. Digital Twin (DT) creates a virtual representation of the physical
network in realtime, allowing continuous monitoring, predictive analytics, and
intelligent decision-making by simulating what-if scenarios. This paper
integrates DT with ZTN proactive bandwidth management in end-to-end (E2E)
next-generation networks. The integrated architecture applies Few-Shot Learning
(FSL) to a memoryaugmented Bidirectional Long Short Term Memory (BiLSTM) model
to predict a new network state to augment the known and trained states. Using
Q-learning, it determines the optimal action (e.g. traffic shaping) under
varying network conditions such that user Quality of Service (QoS) requirements
are met. Three scenarios have been considered: 1) normal ZTN operation with
closed-loop control, 2) a what-if scenario of DT, and 3) network state unknown
to DT. The simulation results show that the network can adapt to underlying
changing conditions. In addition, DT-assisted ZTN achieves better performance
than the other techniques.

</details>


### [42] [Automating Conflict-Aware ACL Configurations with Natural Language Intents](https://arxiv.org/abs/2508.17990)
*Wenlong Ding,Jianqiang Li,Zhixiong Niu,Huangxun Chen,Yongqiang Xiong,Hong Xu*

Main category: cs.NI

TL;DR: Xumi利用LLM自动翻译自然语言意图为ACL规则，减少人工操作，加速配置流程并减少冲突。


<details>
  <summary>Details</summary>
Motivation: 现有ACL配置依赖人工，复杂且易错，难以扩展。

Method: Xumi结合LLM自动翻译意图、检测冲突并生成部署计划。

Result: 配置速度提升10倍，冲突减少40%。

Conclusion: Xumi显著提升ACL配置效率，适用于现代云网络。

Abstract: ACL configuration is essential for managing network flow reachability, yet
its complexity grows significantly with topologies and pre-existing rules. To
carry out ACL configuration, the operator needs to (1) understand the new
configuration policies or intents and translate them into concrete ACL rules,
(2) check and resolve any conflicts between the new and existing rules, and (3)
deploy them across the network. Existing systems rely heavily on manual efforts
for these tasks, especially for the first two, which are tedious, error-prone,
and impractical to scale.
  We propose Xumi to tackle this problem. Leveraging LLMs with domain knowledge
of the target network, Xumi automatically and accurately translates the natural
language intents into complete ACL rules to reduce operators' manual efforts.
Xumi then detects all potential conflicts between new and existing rules and
generates resolved intents for deployment with operators' guidance, and finally
identifies the best deployment plan that minimizes the rule additions while
satisfying all intents. Evaluation shows that Xumi accelerates the entire
configuration pipeline by over 10x compared to current practices, addresses
O(100) conflicting ACLs and reduces rule additions by ~40% in modern cloud
network.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [43] [Generative AI for Multimedia Communication: Recent Advances, An Information-Theoretic Framework, and Future Opportunities](https://arxiv.org/abs/2508.17163)
*Yili Jin,Xue Liu,Jiangchuan Liu*

Main category: cs.MM

TL;DR: 生成式人工智能在多媒体通信中的突破性进展，提出了基于语义信息论的新框架，重新定义了多媒体通信的核心。


<details>
  <summary>Details</summary>
Motivation: 传统信息论框架难以解决语义保真度问题，无法满足人类感知需求，因此需要新的理论框架来适应多媒体应用。

Method: 提出了创新的语义信息论框架，包括语义熵、互信息、信道容量和率失真等概念，将多媒体通信从语法数据传递转向语义信息传递。

Result: 该框架为多媒体通信系统提供了更高效、更鲁棒且语义化的解决方案，并为未来研究指明了方向。

Conclusion: 通过结合生成式AI与信息理论，本文为多媒体通信的语义优先范式转变提供了新视角，对未来研究具有重要启示。

Abstract: Recent breakthroughs in generative artificial intelligence (AI) are
transforming multimedia communication. This paper systematically reviews key
recent advancements across generative AI for multimedia communication,
emphasizing transformative models like diffusion and transformers. However,
conventional information-theoretic frameworks fail to address semantic
fidelity, critical to human perception. We propose an innovative semantic
information-theoretic framework, introducing semantic entropy, mutual
information, channel capacity, and rate-distortion concepts specifically
adapted to multimedia applications. This framework redefines multimedia
communication from purely syntactic data transmission to semantic information
conveyance. We further highlight future opportunities and critical research
directions. We chart a path toward robust, efficient, and semantically
meaningful multimedia communication systems by bridging generative AI
innovations with information theory. This exploratory paper aims to inspire a
semantic-first paradigm shift, offering a fresh perspective with significant
implications for future multimedia research.

</details>


### [44] [Generative Flow Networks for Personalized Multimedia Systems: A Case Study on Short Video Feeds](https://arxiv.org/abs/2508.17166)
*Yili Jin,Ling Pan,Rui-Xiao Zhang,Jiangchuan Liu,Xue Liu*

Main category: cs.MM

TL;DR: 本文提出使用生成流网络（GFlowNets）来解决多媒体系统中的个性化需求问题，通过流量建模和生成技术优化用户体验，尤其在短视频推荐中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着多媒体系统的发展，个性化需求日益增长，如何高效管理资源、自适应内容及用户数据处理成为关键挑战。

Method: 提出生成流网络（GFlowNets）框架，结合多候选生成建模和流量原理，适用于个性化多媒体系统，重点应用于短视频推荐场景。

Result: GFlowNets在视频质量、资源利用效率和交付成本等关键指标上优于传统规则和强化学习方法，展现广泛适用性。

Conclusion: GFlowNets为个性化多媒体系统提供了高效、灵活的解决方案，具有广泛的应用潜力。

Abstract: Multimedia systems underpin modern digital interactions, facilitating
seamless integration and optimization of resources across diverse multimedia
applications. To meet growing personalization demands, multimedia systems must
efficiently manage competing resource needs, adaptive content, and
user-specific data handling. This paper introduces Generative Flow Networks
(GFlowNets, GFNs) as a brave new framework for enabling personalized multimedia
systems. By integrating multi-candidate generative modeling with flow-based
principles, GFlowNets offer a scalable and flexible solution for enhancing
user-specific multimedia experiences. To illustrate the effectiveness of
GFlowNets, we focus on short video feeds, a multimedia application
characterized by high personalization demands and significant resource
constraints, as a case study. Our proposed GFlowNet-based personalized feeds
algorithm demonstrates superior performance compared to traditional rule-based
and reinforcement learning methods across critical metrics, including video
quality, resource utilization efficiency, and delivery cost. Moreover, we
propose a unified GFlowNet-based framework generalizable to other multimedia
systems, highlighting its adaptability and wide-ranging applicability. These
findings underscore the potential of GFlowNets to advance personalized
multimedia systems by addressing complex optimization challenges and supporting
sophisticated multimedia application scenarios.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [45] [On systematic construction of correct logic programs](https://arxiv.org/abs/2508.16782)
*Włodzimierz Drabent*

Main category: cs.LO

TL;DR: 本文提出了一种系统化构建逻辑程序的方法，确保其正确性和半完整性，适用于给定规范。


<details>
  <summary>Details</summary>
Motivation: 旨在解决逻辑程序中正确性和完整性的问题，并提供一个适用于日常编程的实用方法。

Method: 采用声明式方法，基于Kunen的3值完成语义和良基语义，抽象操作语义细节。

Result: 方法能够构造出可证明正确且半完整的逻辑程序。

Conclusion: 该方法简单实用，可用于实际编程中，提升程序的可靠性和完整性。

Abstract: Partial correctness of imperative or functional programming divides in logic
programming into two notions. Correctness means that all answers of the program
are compatible with the specification. Completeness means that the program
produces all the answers required by the specifications. We also consider
semi-completeness -- completeness for those queries for which the program does
not diverge. This paper presents an approach to systematically construct
provably correct and semi-complete logic programs, for a given specification.
Normal programs are considered, under Kunen's 3-valued completion semantics (of
negation as finite failure) and the well-founded semantics (of negation as
possibly infinite failure). The approach is declarative, it abstracts from
details of operational semantics, like e.g.\ the form of the selected literals
(``procedure calls'') during the computation. The proposed method is simple,
and can be used (maybe informally) in actual everyday programming.

</details>


### [46] [Paraconsistent Constructive Modal Logic](https://arxiv.org/abs/2508.17758)
*Han Gao,Daniil Kozhemiachenko,Nicola Olivetti*

Main category: cs.LO

TL;DR: 该论文提出了一系列构造性模态逻辑CK的次协调逻辑变体，用于形式化处理矛盾但非琐碎的命题态度（如信念或义务），并基于直觉主义框架和Nelson逻辑中的强否定定义了一套Kripke语义。


<details>
  <summary>Details</summary>
Motivation: 旨在为矛盾但非琐碎的命题态度（如信念或义务）提供形式化推理工具。

Method: 基于直觉主义框架和Nelson逻辑中的强否定定义了一套Kripke语义；提出了Hilbert-style公理系统和模块化的割免费sequent演算。

Result: 确定了这些逻辑的语义框架，并证明了它们的可判定性。

Conclusion: 提出的逻辑系统为处理矛盾命题态度提供了有效工具，并通过模块化的sequent演算证明了其可判定性。

Abstract: We present a family of paraconsistent counterparts of the constructive modal
logic CK. These logics aim to formalise reasoning about contradictory but
non-trivial propositional attitudes like beliefs or obligations. We define
their Kripke-style semantics based on intuitionistic frames with two valuations
which provide independent support for truth and falsity; they are connected by
strong negation as defined in Nelson's logic. A family of systems is obtained
depending on whether both modal operators are defined using the same or by
different accessibility relations for their positive and negative support. We
propose Hilbert-style axiomatisations for all logics determined by this
semantic framework. We also propose a~family of modular cut-free sequent
calculi that we use to establish decidability.

</details>


### [47] [Certificates and Witnesses for Multi-objective ω-regular Queries in Markov Decision Processes](https://arxiv.org/abs/2508.17859)
*Christel Baier,Calvin Chau,Volodymyr Drobitko,Simon Jantsch,Sascha Klüppelholz*

Main category: cs.LO

TL;DR: 论文提出了一种用于多目标概率模型检查的可验证证书和见证方法，提升了工具的可信度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了增强模型检查工具的可信度和可解释性，研究针对马尔可夫决策过程中的多目标ω-正则查询提出了独立可验证的证书和见证方法。

Method: 论文扩展并改进了现有证书方法，推导了混合整数线性规划（MILP）以找到最小见证子系统。对于马尔可夫链和LTL属性，使用明确Büchi自动机找到见证，算法仅需单指数空间。相比现有基于确定性自动机的方法（双指数空间），效率更高。

Result: 实验评估证明了所提技术的有效性，并在实际计算中展示了这些证书和见证的实用性。

Conclusion: 提出的方法在提高模型检查工具的可信度和解释性方面具有显著效果，且在实际应用中表现出色。

Abstract: Multi-objective probabilistic model checking is a powerful technique for
verifying stochastic systems against multiple (potentially conflicting)
properties. To enhance the trustworthiness and explainability of model checking
tools, we present independently checkable certificates and witnesses for
multi-objective {\omega}-regular queries in Markov decision processes. For the
certification, we extend and improve existing certificates for the
decomposition of maximal end components and reachability properties. We then
derive mixed-integer linear programs (MILPs) for finding minimal witnessing
subsystems. For the special case of Markov chains and LTL properties, we use
unambiguous B\"uchi automata to find witnesses, resulting in an algorithm that
requires single-exponential space. Existing approaches based on deterministic
automata require doubly-exponential space in the worst case. Finally, we
consider the practical computation of our certificates and witnesses and
provide an implementation of the developed techniques, along with an
experimental evaluation, demonstrating the efficacy of our techniques.

</details>


### [48] [Model-Based Testing of an Intermediate Verifier Using Executable Operational Semantics](https://arxiv.org/abs/2508.17895)
*Lidia Losavio,Marco Paganoni,Carlo A. Furia*

Main category: cs.LO

TL;DR: 论文介绍了一种名为BCC的基于模型的测试技术，用于验证Boogie中间验证器，通过随机生成Boogie程序并对比执行结果来发现潜在bug。


<details>
  <summary>Details</summary>
Motivation: 轻量级验证技术（如随机测试）可以作为正式验证的实用替代方案，尤其在复杂系统中超出形式模型范围的部分。

Method: BCC结合了Boogie语言的确定性子集形式化和PLT Redex的生成能力，通过生成随机程序并对比执行结果来检测不一致。

Result: 实验生成了300万Boogie程序，发现了2%的完备性失败案例，表明BCC能有效检测Boogie工具链中的潜在问题。

Conclusion: 轻量级分析工具（如BCC）对测试和验证形式验证工具（如Boogie）具有实际价值。

Abstract: Lightweight validation technique, such as those based on random testing, are
sometimes practical alternatives to full formal verification -- providing
valuable benefits, such as finding bugs, without requiring a disproportionate
effort. In fact, they can be useful even for fully formally verified tools, by
exercising the parts of a complex system that go beyond the reach of formal
models.
  In this context, this paper introduces BCC: a model-based testing technique
for the Boogie intermediate verifier. BCC combines the formalization of a
small, deterministic subset of the Boogie language with the generative
capabilities of the PLT Redex language engineering framework. Basically, BCC
uses PLT Redex to generate random Boogie programs, and to execute them
according to a formal operational semantics; then, it runs the same programs
through the Boogie verifier. Any inconsistency between the two executions (in
PLT Redex and with Boogie) may indicate a potential bug in Boogie's
implementation.
  To understand whether BCC can be useful in practice, we used it to generate
three million Boogie programs. These experiments found 2% of cases indicative
of completeness failures (i.e., spurious verification failures) in Boogie's
toolchain. These results indicate that lightweight analysis tools, such as
those for model-based random testing, are also useful to test and validate
formal verification tools such as Boogie.

</details>


### [49] [Compositional Verification in Concurrent Separation Logic with Permissions Regions](https://arxiv.org/abs/2508.18115)
*Quang Loc Le*

Main category: cs.LO

TL;DR: 论文提出了一种组合验证系统，用于并发程序中共享内存区域的操纵，解决了CSL-Perm缺乏自动化和组合性支持的问题。


<details>
  <summary>Details</summary>
Motivation: CSL-Perm虽然为复杂顺序和并发细粒度程序的验证提供了强大逻辑基础，但缺乏自动化和组合性支持。

Method: 通过引入新颖的逻辑原则和推导程序，设计了一个组合验证系统，能够推断帧规则中的剩余堆，并实现对并发线程和函数调用的组合推理。

Result: 实现了一个原型工具CoSl，测试了10个具有挑战性的并发程序，证明了该方法的优势。

Conclusion: 该组合验证系统为CSL-Perm提供了自动化和组合性支持，推动了并发程序验证的发展。

Abstract: Concurrent separation logic with fractional permissions (CSLPerm) provides a
promising reasoning system to verify most complex sequential and concurrent
fine-grained programs. The logic with strong and weak separating conjunctions
offers a solid foundation for producing concise and precise proofs. However, it
lacks automation and compositionality support. This paper addresses this
limitation by introducing a compositional verification system for concurrent
programs that manipulate regions of shared memory. The centre of our system is
novel logical principles and an entailment procedure that can infer the
residual heaps in the frame rule for a fragment of CSL-Perm with explicit
arithmetical constraints for memory heaps' disjointness. This procedure enables
the compositional reasoning for concurrent threads and function calls. We have
implemented the proposal in a prototype tool called CoSl, tested it with 10
challenging concurrent programs, including those beyond the state-of-the-art,
and confirmed the advantage of our approach.

</details>


### [50] [First-Order LTLf Synthesis with Lookback (Extended Version)](https://arxiv.org/abs/2508.18149)
*Sarah Winkler*

Main category: cs.LO

TL;DR: 本文提出了一种支持变量跨时刻比较的LTLfMT反应式合成方法，解决了现有方法的表达限制，并证明了其正确性和在某些条件下的完备性。


<details>
  <summary>Details</summary>
Motivation: 传统的LTL反应式合成方法在表达变量跨时刻比较时受限，而LTLfMT在AI和业务流程管理中的应用需要这种能力。

Method: 提出了一种支持“回顾”功能的LTLfMT反应式合成方法，允许变量跨时刻比较，且适用于完整的LTLfMT。

Result: 证明了该方法的正确性，并在策略长度有界时为完备的；同时识别了新的可判定类。

Conclusion: 该方法扩展了LTLfMT的应用范围，尽管在一般情况下不可判定，但在特定条件下有效。

Abstract: Reactive synthesis addresses the problem of generating a controller for a
temporal specification in an adversarial environment; it was typically studied
for LTL. Driven by applications ranging from AI to business process management,
LTL modulo first order-theories over finite traces (LTLfMT) has recently gained
traction, where propositional variables in properties are replaced by
first-order constraints. Though reactive synthesis for LTLf with some
first-order features has been addressed, existing work in this direction
strongly restricts or excludes the possibility to compare variables across
instants, a limitation that severely restricts expressiveness and
applicability.
  In this work we present a reactive synthesis procedure for LTLfMT, where
properties support "lookback" to model cross-instant comparison of variables.
Our procedure works for full LTLfMT with lookback, subsuming the fragments of
LTLfMT for which realizability was studied earlier. However, the setting with
cross-instant comparison is inherently highly complex, as realizability is
undecidable even over decidable background theories. Hence termination of our
approach is in general not guaranteed. Nevertheless, we prove its soundness,
and show that it is complete if a bound on the strategy length exists. Finally,
we show that our approach constitutes a decision procedure for several relevant
fragments of LTLfMT, at once re-proving known decidability results and
identifying new decidable classes.

</details>


### [51] [To bind or not to bind? Discovering Stable Relationships in Object-centric Processes (Extended Version)](https://arxiv.org/abs/2508.18231)
*Anjo Seidel,Sarah Winkler,Alessandro Gianola,Marco Montali,Mathias Weske*

Main category: cs.LO

TL;DR: 该论文提出了一种将对象为中心的Petri网（OCPN）与明确的稳定多对一关系结合的方法，映射到具有标识符的对象为中心的Petri网（OPID），以解决对象关系同步问题。


<details>
  <summary>Details</summary>
Motivation: 现有OCPN无法表示对象间的同步关系，导致无法识别违反意图的执行行为。OPID虽能解决这一问题，但其发现方法尚未被研究。

Method: 结合OCPN与明确的稳定多对一关系，提出从OCPN到OPID的严格映射，确保同步相关对象。

Result: 证明了在满足意图关系时，原始OCPN与生成OPID的一致性，并实现了该映射。

Conclusion: 该方法有效解决了对象关系同步问题，为对象为中心的流程挖掘提供了更准确的建模工具。

Abstract: Object-centric process mining investigates the intertwined behavior of
multiple objects in business processes. From object-centric event logs,
object-centric Petri nets (OCPN) can be discovered to replay the behavior of
processes accessing different object types. Although they indicate how objects
flow through the process and co-occur in events, OCPNs remain underspecified
about the relationships of objects. Hence, they are not able to represent
synchronization, i.e. executing objects only according to their intended
relationships, and fail to identify violating executions. Existing formal
modeling approaches, such as object-centric Petri nets with identifiers (OPID),
represent object identities and relationships to synchronize them correctly.
However, OPID discovery has not yet been studied. This paper uses explicit data
models to bridge the gap between OCPNs and formal OPIDs. We identify the
implicit assumptions of stable many-to-one relationships in object-centric
event logs, which implies synchronization of related objects. To formally
underpin this observation, we combine OCPNs with explicit stable many-to-one
relationships in a rigorous mapping from OCPNs to OPIDs explicitly capturing
the intended stable relationships and the synchronization of related objects.
We prove that the original OCPNs and the resulting OPIDs coincide for those
executions that satisfy the intended relationships. Moreover, we provide an
implementation of the mapping from OCPN to OPID under stable relationships.

</details>


### [52] [The Computational Complexity of Satisfiability in State Space Models](https://arxiv.org/abs/2508.18162)
*Eric Alsmann,Martin Lange*

Main category: cs.LO

TL;DR: 论文分析了状态空间模型（SSM）的满足性问题（ssmSAT）的复杂性，发现一般情况下问题是不可判定的，但在两种限制条件下可判定，并给出了相应的复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 研究SSM模型的满足性问题的复杂性，为SSM模型的验证提供理论基础。

Method: 分析SSM模型的满足性问题，提出两种自然限制条件，并分别研究其复杂度。

Result: 发现ssmSAT在一般情况下不可判定，但在有限上下文长度和固定宽度算术下可判定，分别对应NP完全和PSPACE完全复杂度。

Conclusion: 研究为SSM模型的正式推理建立了首个复杂性框架，揭示了验证中的基本限制与机会。

Abstract: We analyse the complexity of the satisfiability problem ssmSAT for State
Space Models (SSM), which asks whether an input sequence can lead the model to
an accepting configuration. We find that ssmSAT is undecidable in general,
reflecting the computational power of SSM. Motivated by practical settings, we
identify two natural restrictions under which ssmSAT becomes decidable and
establish corresponding complexity bounds. First, for SSM with bounded context
length, ssmSAT is NP-complete when the input length is given in unary and in
NEXPTIME (and PSPACE-hard) when the input length is given in binary. Second,
for quantised SSM operating over fixed-width arithmetic, ssmSAT is
PSPACE-complete resp. in EXPSPACE depending on the bit-width encoding. While
these results hold for diagonal gated SSM we also establish complexity bounds
for time-invariant SSM. Our results establish a first complexity landscape for
formal reasoning in SSM and highlight fundamental limits and opportunities for
the verification of SSM-based language models.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [53] [Adaptive Command: Real-Time Policy Adjustment via Language Models in StarCraft II](https://arxiv.org/abs/2508.16580)
*Weiyu Ma,Dongyu Xu,Shu Lin,Haifeng Zhang,Jun Wang*

Main category: cs.HC

TL;DR: 提出了一个名为Adaptive Command的框架，通过将大语言模型（LLMs）与行为树结合，用于《星际争霸II》中的实时战略决策，重点关注人机协作的提升。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自然语言交互增强人类与AI在复杂动态环境中的协作能力，特别是在实时战略游戏中。

Method: 框架包含三部分：基于LLM的战略顾问、行为树执行动作、以及具有语音功能的自然语言界面。

Result: 用户研究表明，该系统显著提升了玩家（尤其是新手和残障人士）的决策能力和战略适应性。

Conclusion: 这项工作不仅为实时人机协作决策提供了创新框架，还为复杂决策场景提供了广泛应用的见解。

Abstract: We present Adaptive Command, a novel framework integrating large language
models (LLMs) with behavior trees for real-time strategic decision-making in
StarCraft II. Our system focuses on enhancing human-AI collaboration in
complex, dynamic environments through natural language interactions. The
framework comprises: (1) an LLM-based strategic advisor, (2) a behavior tree
for action execution, and (3) a natural language interface with speech
capabilities. User studies demonstrate significant improvements in player
decision-making and strategic adaptability, particularly benefiting novice
players and those with disabilities. This work contributes to the field of
real-time human-AI collaborative decision-making, offering insights applicable
beyond RTS games to various complex decision-making scenarios.

</details>


### [54] [Increasing Interaction Fidelity: Training Routines for Biomechanical Models in HCI](https://arxiv.org/abs/2508.16581)
*Michał Patryk Miazga,Patrick Ebel*

Main category: cs.HC

TL;DR: 本文提出改进训练方法以提升生物力学模型的交互保真度和训练效率，适用于移动设备触屏交互。


<details>
  <summary>Details</summary>
Motivation: 现有生物力学模型在交互保真度和泛化能力上存在不足，特别是在触屏交互等精细动作中。

Method: 采用课程学习、动作掩码、复杂网络配置及环境调整等方法优化训练流程。

Result: 实验显示改进方法显著提升了代理学习触屏行为的准确性，训练时间缩短。

Conclusion: 为HCI研究者提供了提升生物力学模型交互保真度的实用建议和训练方案。

Abstract: Biomechanical forward simulation holds great potential for HCI, enabling the
generation of human-like movements in interactive tasks. However, training
biomechanical models with reinforcement learning is challenging, particularly
for precise and dexterous movements like those required for touchscreen
interactions on mobile devices. Current approaches are limited in their
interaction fidelity, require restricting the underlying biomechanical model to
reduce complexity, and do not generalize well. In this work, we propose
practical improvements to training routines that reduce training time, increase
interaction fidelity beyond existing methods, and enable the use of more
complex biomechanical models. Using a touchscreen pointing task, we demonstrate
that curriculum learning, action masking, more complex network configurations,
and simple adjustments to the simulation environment can significantly improve
the agent's ability to learn accurate touch behavior. Our work provides HCI
researchers with practical tips and training routines for developing better
biomechanical models of human-like interaction fidelity.

</details>


### [55] [Predicting User Grasp Intentions in Virtual Reality](https://arxiv.org/abs/2508.16582)
*Linghao Zeng*

Main category: cs.HC

TL;DR: 该研究利用时间序列数据评估分类和回归方法在VR中预测用户意图的效果，发现回归方法（尤其是LSTM）表现更稳健，但精确预测手部姿势仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 预测用户在VR中的意图对于创造沉浸式体验至关重要，尤其是在需要准确触觉反馈的复杂抓握任务中。

Method: 通过810次试验，比较分类和回归方法（如LSTM），分析手部运动数据。

Result: 回归方法在关键两秒窗口内时间误差小于0.25秒，距离误差为5-20厘米，但分类方法泛化能力较差。

Conclusion: 回归模型更适合动态复杂的用户行为，为未来VR中实时预测和自适应触觉反馈提供了基础。

Abstract: Predicting user intentions in virtual reality (VR) is crucial for creating
immersive experiences, particularly in tasks involving complex grasping motions
where accurate haptic feedback is essential. In this work, we leverage
time-series data from hand movements to evaluate both classification and
regression approaches across 810 trials with varied object types, sizes, and
manipulations. Our findings reveal that classification models struggle to
generalize across users, leading to inconsistent performance. In contrast,
regression-based approaches, particularly those using Long Short Term Memory
(LSTM) networks, demonstrate more robust performance, with timing errors within
0.25 seconds and distance errors around 5-20 cm in the critical two-second
window before a grasp. Despite these improvements, predicting precise hand
postures remains challenging. Through a comprehensive analysis of user
variability and model interpretability, we explore why certain models fail and
how regression models better accommodate the dynamic and complex nature of user
behavior in VR. Our results underscore the potential of machine learning models
to enhance VR interactions, particularly through adaptive haptic feedback, and
lay the groundwork for future advancements in real-time prediction of user
actions in VR.

</details>


### [56] [The Rhythm of Tai Chi: Revitalizing Cultural Heritage in Virtual Reality through Interactive Visuals](https://arxiv.org/abs/2508.16605)
*Xianghan Wang*

Main category: cs.HC

TL;DR: 《太极韵律》将太极重新诠释为动态互动的VR体验，结合计算机视觉与多媒体技术，实现沉浸式练习。


<details>
  <summary>Details</summary>
Motivation: 通过现代技术连接传统文化与现代受众，普及太极的文化与健康价值。

Method: 利用实时动作捕捉与视觉反馈系统，模拟太极动作与气的流动。

Result: 创造了直观、互动的太极练习平台，为全球用户提供文化探索机会。

Conclusion: 该项目既是文化遗产的保护工具，也是数字时代的创新教育资源。

Abstract: The Rhythm of Tai Chi reinterprets the ancient Chinese martial art as a
dynamic, interactive virtual reality (VR) experience. By leveraging computer
vision and multimedia technologies, the project transforms Tai Chi's philosophy
and movements into an immersive digital form. Real-time motion tracking
captures user gestures, while visual feedback systems simulate the flow of Qi,
enabling an intuitive and engaging practice environment. Beyond technological
innovation, this work bridges traditional Chinese culture and modern audiences.
It offers a global platform - accessible even to those unfamiliar with Tai Chi
- to explore its cultural significance, connections to balance, health, and
mindfulness. Serving as both a preservation tool and an educational resource,
The Rhythm of Tai Chi revitalizes this heritage for the digital age.

</details>


### [57] [Using Generative AI to Uncover What Drives Player Enjoyment in PC and VR Games](https://arxiv.org/abs/2508.16596)
*Hisham Abdelqader*

Main category: cs.HC

TL;DR: 利用生成式AI和机器学习分析游戏评论，揭示玩家偏好，为开发者提供优化建议。


<details>
  <summary>Details</summary>
Motivation: 理解玩家享受游戏的关键因素，利用非结构化评论数据进行大规模分析。

Method: 应用Microsoft Phi-4 LLM和XGBoost，结合Google Cloud处理数据，将定性反馈转化为结构化数据。

Result: 发现了PC和VR游戏中不同的玩家偏好模式，为游戏设计和定价策略提供依据。

Conclusion: 建立了可扩展的分析框架，帮助开发者优化游戏机制和玩家参与度。

Abstract: As video games continue to evolve, understanding what drives player enjoyment
remains a key challenge. Player reviews provide valuable insights, but their
unstructured nature makes large-scale analysis difficult. This study applies
generative AI and machine learning, leveraging Microsoft Phi-4 LLM and XGBoost,
to quantify and analyze game reviews from Steam and Meta Quest stores. The
approach converts qualitative feedback into structured data, enabling
comprehensive evaluation of key game design elements, monetization models, and
platform-specific trends. The findings reveal distinct patterns in player
preferences across PC and VR games, highlighting factors that contribute to
higher player satisfaction. By integrating Google Cloud for largescale data
storage and processing, this study establishes a scalable framework for game
review analysis. The study's insights offer actionable guidance for game
developers, helping optimize game mechanics, pricing strategies, and player
engagement.

</details>


### [58] [Seeing Isn't Believing: Addressing the Societal Impact of Deepfakes in Low-Tech Environments](https://arxiv.org/abs/2508.16618)
*Azmine Toushik Wasi,Rahatun Nesa Priti,Mahir Absar Khan,Abdur Rahman,Mst Rafia Islam*

Main category: cs.HC

TL;DR: 该研究探讨了深度伪造技术对资源有限社区的感知和影响，提出了一个预防、检测和缓解的框架。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对政治稳定、社会信任和经济福祉构成威胁，尤其是在媒体素养和技术基础设施有限的发展中国家。

Method: 通过调查评估公众对深度伪造的认知、感知和体验。

Result: 研究发现公众对深度伪造的认识不足，缺乏有效的检测工具，需要针对性教育和易用的验证解决方案。

Conclusion: 研究呼吁更多跨学科合作，特别是针对全球南方地区，以应对深度伪造的挑战。

Abstract: Deepfakes, AI-generated multimedia content that mimics real media, are
becoming increasingly prevalent, posing significant risks to political
stability, social trust, and economic well-being, especially in developing
societies with limited media literacy and technological infrastructure. This
work aims to understand how these technologies are perceived and impact
resource-limited communities. We conducted a survey to assess public awareness,
perceptions, and experiences with deepfakes, leading to the development of a
comprehensive framework for prevention, detection, and mitigation in
tech-limited environments. Our findings reveal critical knowledge gaps and a
lack of effective detection tools, emphasizing the need for targeted education
and accessible verification solutions. This work offers actionable insights to
support vulnerable populations and calls for further interdisciplinary efforts
to tackle deepfake challenges globally, particularly in the Global South.

</details>


### [59] [Humans Perceive Wrong Narratives from AI Reasoning Texts](https://arxiv.org/abs/2508.16599)
*Mosh Levy,Zohar Elyoseph,Yoav Goldberg*

Main category: cs.HC

TL;DR: AI模型生成的推理文本看似透明，但人类对其步骤的理解与模型的实际计算过程存在显著差距，准确性仅29.3%，质疑其作为解释工具的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究人类是否能够准确理解AI模型生成的推理文本，以验证其作为透明和可解释性工具的可靠性。

Method: 通过设计基于反事实测量的问卷，评估人类识别推理文本中因果步骤的能力。

Result: 人类识别准确性仅为29.3%，略高于随机水平（25%），高一致性问题的多数投票准确率也仅42%。

Conclusion: 推理文本不应被简单视为解释工具，而需深入研究其非人类语言使用方式，揭示模型与人类理解的差异。

Abstract: A new generation of AI models generates step-by-step reasoning text before
producing an answer. This text appears to offer a human-readable window into
their computation process, and is increasingly relied upon for transparency and
interpretability. However, it is unclear whether human understanding of this
text matches the model's actual computational process. In this paper, we
investigate a necessary condition for correspondence: the ability of humans to
identify which steps in a reasoning text causally influence later steps. We
evaluated humans on this ability by composing questions based on counterfactual
measurements and found a significant discrepancy: participant accuracy was only
29.3%, barely above chance (25%), and remained low (42%) even when evaluating
the majority vote on questions with high agreement. Our results reveal a
fundamental gap between how humans interpret reasoning texts and how models use
it, challenging its utility as a simple interpretability tool. We argue that
reasoning texts should be treated as an artifact to be investigated, not taken
at face value, and that understanding the non-human ways these models use
language is a critical research direction.

</details>


### [60] [An Embodied AR Navigation Agent: Integrating BIM with Retrieval-Augmented Generation for Language Guidance](https://arxiv.org/abs/2508.16602)
*Hsuan-Kung Yang,Tsu-Ching Hsiao,Ryoichiro Oka,Ryuya Nishino,Satoko Tofukuji,Norimasa Kobori*

Main category: cs.HC

TL;DR: 提出了一种基于BIM和多代理RAG框架的AR导航系统，通过语言代理和AR代理实现灵活导航和交互。


<details>
  <summary>Details</summary>
Motivation: 传统AR导航系统依赖固定输入，限制了建筑数据的利用和自然交互。

Method: 结合BIM与多代理RAG框架，使用三个语言代理（Triage、Search、Response）和AR代理实现导航。

Result: 用户研究显示系统SUS得分为80.5，且AR代理显著提升用户对系统智能的感知。

Conclusion: 语言驱动的推理和具体化设计对用户为中心的AR导航系统至关重要。

Abstract: Delivering intelligent and adaptive navigation assistance in augmented
reality (AR) requires more than visual cues, as it demands systems capable of
interpreting flexible user intent and reasoning over both spatial and semantic
context. Prior AR navigation systems often rely on rigid input schemes or
predefined commands, which limit the utility of rich building data and hinder
natural interaction. In this work, we propose an embodied AR navigation system
that integrates Building Information Modeling (BIM) with a multi-agent
retrieval-augmented generation (RAG) framework to support flexible,
language-driven goal retrieval and route planning. The system orchestrates
three language agents, Triage, Search, and Response, built on large language
models (LLMs), which enables robust interpretation of open-ended queries and
spatial reasoning using BIM data. Navigation guidance is delivered through an
embodied AR agent, equipped with voice interaction and locomotion, to enhance
user experience. A real-world user study yields a System Usability Scale (SUS)
score of 80.5, indicating excellent usability, and comparative evaluations show
that the embodied interface can significantly improves users' perception of
system intelligence. These results underscore the importance and potential of
language-grounded reasoning and embodiment in the design of user-centered AR
navigation systems.

</details>


### [61] [WHAR Datasets: An Open Source Library for Wearable Human Activity Recognition](https://arxiv.org/abs/2508.16604)
*Maximilian Burzer,Tobias King,Till Riedel,Michael Beigl,Tobias Röddiger*

Main category: cs.HC

TL;DR: 介绍了WHAR数据集库，旨在标准化可穿戴人类活动识别数据，提高研究效率。


<details>
  <summary>Details</summary>
Motivation: 解决WHAR数据集的标准化不足问题，提升研究的可重复性和效率。

Method: 开发了一个开源库，支持标准化数据格式和配置驱动设计，兼容PyTorch和TensorFlow。

Result: 在9个常用数据集上验证了库的有效性，预处理性能提升了3.8倍。

Conclusion: 该库有望促进WHAR研究的效率和可比性。

Abstract: The lack of standardization across Wearable Human Activity Recognition (WHAR)
datasets limits reproducibility, comparability, and research efficiency. We
introduce WHAR datasets, an open-source library designed to simplify WHAR data
handling through a standardized data format and a configuration-driven design,
enabling reproducible and computationally efficient workflows with minimal
manual intervention. The library currently supports 9 widely-used datasets,
integrates with PyTorch and TensorFlow, and is easily extensible to new
datasets. To demonstrate its utility, we trained two state-of-the-art models,
TinyHar and MLP-HAR, on the included datasets, approximately reproducing
published results and validating the library's effectiveness for
experimentation and benchmarking. Additionally, we evaluated preprocessing
performance and observed speedups of up to 3.8x using multiprocessing. We hope
this library contributes to more efficient, reproducible, and comparable WHAR
research.

</details>


### [62] [Multimodal Appearance based Gaze-Controlled Virtual Keyboard with Synchronous Asynchronous Interaction for Low-Resource Settings](https://arxiv.org/abs/2508.16606)
*Yogesh Kumar Meena,Manish Salvi*

Main category: cs.HC

TL;DR: 提出了一种基于深度学习多模态视线控制的虚拟键盘，支持同步和异步模式，适用于行动和语言障碍者。测试显示其具有良好的可用性和低工作负荷。


<details>
  <summary>Details</summary>
Motivation: 传统视线追踪技术存在准确性和操作复杂性问题，亟需一种低成本、易用的解决方案。

Method: 结合深度学习与普通摄像头，开发多模态虚拟键盘，支持九种命令和56个字符输入，并通过三种输入方式测试性能。

Result: 键盘输入速度可达7.86-18.3字母/分钟，信息传输率为63.56-80.29比特/分钟，摄像头输入表现出良好的可用性。

Conclusion: 该系统适合低资源环境，为行动和语言障碍者提供了一种有效的沟通工具。

Abstract: Over the past decade, the demand for communication devices has increased
among individuals with mobility and speech impairments. Eye-gaze tracking has
emerged as a promising solution for hands-free communication; however,
traditional appearance-based interfaces often face challenges such as accuracy
issues, involuntary eye movements, and difficulties with extensive command
sets. This work presents a multimodal appearance-based gaze-controlled virtual
keyboard that utilises deep learning in conjunction with standard camera
hardware, incorporating both synchronous and asynchronous modes for command
selection. The virtual keyboard application supports menu-based selection with
nine commands, enabling users to spell and type up to 56 English characters,
including uppercase and lowercase letters, punctuation, and a delete function
for corrections. The proposed system was evaluated with twenty able-bodied
participants who completed specially designed typing tasks using three input
modalities: (i) a mouse, (ii) an eye-tracker, and (iii) an unmodified webcam.
Typing performance was measured in terms of speed and information transfer rate
(ITR) at both command and letter levels. Average typing speeds were 18.3+-5.31
letters/min (mouse), 12.60+-2.99letters/min (eye-tracker, synchronous), 10.94
+- 1.89 letters/min (webcam, synchronous), 11.15 +- 2.90 letters/min
(eye-tracker, asynchronous), and 7.86 +- 1.69 letters/min (webcam,
asynchronous). ITRs were approximately 80.29 +- 15.72 bits/min (command level)
and 63.56 +- 11 bits/min (letter level) with webcam in synchronous mode. The
system demonstrated good usability and low workload with webcam input,
highlighting its user-centred design and promise as an accessible communication
tool in low-resource settings.

</details>


### [63] ["Accessibility people, you go work on that thing of yours over there": Addressing Disability Inclusion in AI Product Organizations](https://arxiv.org/abs/2508.16607)
*Sanika Moharana,Cynthia L. Bennett,Erin Buehler,Michael Madaio,Vinita Tibdewal,Shaun K. Kane*

Main category: cs.HC

TL;DR: 生成式AI的崛起改变了技术设计、构建、维护和评估的方式，但也可能对残障用户造成不成比例的影响。通过对25名AI从业者的访谈研究，发现他们在负责任AI与无障碍实践之间存在摩擦，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI技术对残障用户的潜在影响，研究AI从业者在设计和实施过程中的挑战。

Method: 对25名来自不同角色（工程、研究、用户体验、负责任AI）的AI从业者进行访谈研究。

Result: 发现从业者在负责任AI与无障碍实践之间存在矛盾，并存在用户数据缺口，需通过非正式志愿者和社区团体支持残障用户需求。

Conclusion: 提出改进资源和流程的建议，以更好地支持残障用户作为AI的终端用户。

Abstract: The rapid emergence of generative AI has changed the way that technology is
designed, constructed, maintained, and evaluated. Decisions made when creating
AI-powered systems may impact some users disproportionately, such as people
with disabilities. In this paper, we report on an interview study with 25 AI
practitioners across multiple roles (engineering, research, UX, and responsible
AI) about how their work processes and artifacts may impact end users with
disabilities. We found that practitioners experienced friction when triaging
problems at the intersection of responsible AI and accessibility practices,
navigated contradictions between accessibility and responsible AI guidelines,
identified gaps in data about users with disabilities, and gathered support for
addressing the needs of disabled stakeholders by leveraging informal volunteer
and community groups within their company. Based on these findings, we offer
suggestions for new resources and process changes to better support people with
disabilities as end users of AI.

</details>


### [64] [To Explain Or Not To Explain: An Empirical Investigation Of AI-Based Recommendations On Social Media Platforms](https://arxiv.org/abs/2508.16610)
*AKM Bahalul Haque,A. K. M. Najmul Islam,Patrick Mikalef*

Main category: cs.HC

TL;DR: 论文研究了社交媒体推荐系统从用户角度的透明度和可解释性问题，通过定性分析发现用户需要简洁、非技术性的解释以及对信息流的控制。


<details>
  <summary>Details</summary>
Motivation: AI推荐的社交媒体内容常常与用户兴趣不符且缺乏透明度，导致用户体验不佳，因此需要从用户视角研究这一问题。

Method: 利用Facebook平台招募普通用户进行定性分析，探讨推荐内容的可理解性和可解释性。

Result: 用户主要在不熟悉内容时要求解释，并关注数据安全；解释能提升透明度、信任和理解。

Conclusion: 提出了设计建议和框架，强调简洁解释和用户可控的信息流对提升体验的重要性。

Abstract: AI based social media recommendations have great potential to improve the
user experience. However, often these recommendations do not match the user
interest and create an unpleasant experience for the users. Moreover, the
recommendation system being a black box creates comprehensibility and
transparency issues. This paper investigates social media recommendations from
an end user perspective. For the investigation, we used the popular social
media platform Facebook and recruited regular users to conduct a qualitative
analysis. We asked participants about the social media content suggestions,
their comprehensibility, and explainability. Our analysis shows users mostly
require explanation whenever they encounter unfamiliar content and to ensure
their online data security. Furthermore, the users require concise,
non-technical explanations along with the facility of controlled information
flow. In addition, we observed that explanations impact the users perception of
transparency, trust, and understandability. Finally, we have outlined some
design implications and presented a synthesized framework based on our data
analysis.

</details>


### [65] [Negative Shanshui: Real-time Interactive Ink Painting Synthesis](https://arxiv.org/abs/2508.16612)
*Aven-Le Zhou*

Main category: cs.HC

TL;DR: 论文提出了一种实时交互AI合成方法Negative Shanshui，通过重新诠释古典中国山水画，以应对人类世的生态危机。该方法优化了Stable Diffusion模型，并结合视线驱动的修复和帧插值，提供动态变换动画和VR体验。


<details>
  <summary>Details</summary>
Motivation: 通过技术手段重新诠释传统山水画，以艺术形式唤起人们对生态危机的关注，并探索AI与艺术结合的创新方式。

Method: 优化了Stable Diffusion模型进行实时推理，结合视线驱动的修复和帧插值技术，实现动态动画效果，并通过VR环境提供交互体验。

Result: 展示了完整的技术流程，包括系统框架优化和交互设计，并在艺术节中进行了多模态部署。观众反馈显示参与者通过共情、矛盾情感和批判性反思与作品互动。

Conclusion: Negative Shanshui为生态危机提供了艺术与技术结合的解决方案，展示了AI在艺术创作中的潜力，并通过互动体验引发观众的深层思考。

Abstract: This paper presents Negative Shanshui, a real-time interactive AI synthesis
approach that reinterprets classical Chinese landscape ink painting, i.e.,
shanshui, to engage with ecological crises in the Anthropocene. Negative
Shanshui optimizes a fine-tuned Stable Diffusion model for real-time inferences
and integrates it with gaze-driven inpainting, frame interpolation; it enables
dynamic morphing animations in response to the viewer's gaze and presents as an
interactive virtual reality (VR) experience. The paper describes the complete
technical pipeline, covering the system framework, optimization strategies,
gaze-based interaction, and multimodal deployment in an art festival. Further
analysis of audience feedback collected during its public exhibition highlights
how participants variously engaged with the work through empathy, ambivalence,
and critical reflection.

</details>


### [66] [Observations of atypical users from a pilot deployment of a public-space social robot in a church](https://arxiv.org/abs/2508.16622)
*Andrew Blair,Peggy Gregory,Mary Ellen Foster*

Main category: cs.HC

TL;DR: 论文探讨了社交机器人在真实公共空间中与用户的非典型交互现象，并提出了未来改进策略。


<details>
  <summary>Details</summary>
Motivation: 研究社交机器人在真实公共空间中与多样化用户的交互挑战，填补相关领域实证研究的空白。

Method: 通过为期三天的试点部署，观察并分析机器人与用户的交互行为，包括非典型用户及其反应。

Result: 揭示了公共空间中非预期的机器人交互行为，提出了理论和实践上的改进方向。

Conclusion: 公共空间中社交机器人的交互具有挑战性，论文为未来设计和部署提供了实证依据和实用建议。

Abstract: Though a goal of HRI is the natural integration of social robots into
everyday public spaces, real-world studies still occur mostly within controlled
environments with predetermined participants. True public spaces present an
environment which is largely unconstrained and unpredictable, frequented by a
diverse range of people whose goals can often conflict with those of the robot.
When combined with the general unfamiliarity most people have with social
robots, this leads to unexpected human-robot interactions in these public
spaces that are rarely discussed or detected in other contexts. In this paper,
we describe atypical users we observed interacting with our robot, and those
who did not, during a three-day pilot deployment within a large working church
and visitor attraction. We then discuss theoretical future advances in the
field that could address these challenges, as well as immediate practical
mitigations and strategies to help improve public space human-robot
interactions in the present. This work contributes empirical insights into the
dynamics of human-robot interaction in public environments and offers
actionable guidance for more effective future deployments for social robot
designers.

</details>


### [67] [A Minimalistic Approach to Predict and Understand the Relation of App Usage with Students' Academic Performances](https://arxiv.org/abs/2508.16779)
*Md Sabbir Ahmed,Rahat Jahangir Rony,Mohammad Abdul Hadi,Ekram Hossain,Nova Ahmed*

Main category: cs.HC

TL;DR: 研究开发了一款应用，通过实时获取学生过去7天的实际应用使用数据，快速预测学术成绩（CGPA），避免现有研究中的偏见和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖自报数据可能存偏，且需长时间数据预测成绩，无法早期干预。

Method: 开发应用实时获取7天内应用使用数据，分析124名孟加拉学生的数据，利用机器学习模型预测CGPA。

Result: 发现应用使用与CGPA显著相关，生产力和书籍类应用对成绩有正面影响，视频类反之。模型预测误差为0.36。

Conclusion: 研究提供了一种基于实时数据的快速成绩预测方法，为提升学生成绩提供了设计启示。

Abstract: Due to usage of self-reported data which may contain biasness, the existing
studies may not unveil the exact relation between academic grades and app
categories such as Video. Additionally, the existing systems' requirement for
data of prolonged period to predict grades may not facilitate early
intervention to improve it. Thus, we presented an app that retrieves past 7
days' actual app usage data within a second (Mean=0.31s, SD=1.1s). Our analysis
on 124 Bangladeshi students' real-time data demonstrates app usage sessions
have a significant (p<0.05) negative association with CGPA. However, the
Productivity and Books categories have a significant positive association
whereas Video has a significant negative association. Moreover, the high and
low CGPA holders have significantly different app usage behavior. Leveraging
only the instantly accessed data, our machine learning model predicts CGPA
within 0.36 of the actual CGPA. We discuss the design implications that can be
potential for students to improve grades.

</details>


### [68] [The Impact of Visual Segmentation on Lexical Word Recognition](https://arxiv.org/abs/2508.16914)
*Matthew Termuende,Kevin Larson,Miguel Nacenta*

Main category: cs.HC

TL;DR: 研究探讨了视觉干预（如分段标记）如何影响英语单词识别，发现这些干预通常会减慢识别速度，且不同干预方式效果不同。


<details>
  <summary>Details</summary>
Motivation: 探索视觉分段信息是否能帮助阅读，降低其计算成本。

Method: 通过192名参与者的预注册词汇决策实验，测试了五种视觉分段方式。

Result: 视觉干预会减慢单词识别速度，但不同方式效果各异。

Conclusion: 视觉干预对典型成人读者的词汇决策任务无加速作用，结果提供了视觉文本操作的成本测量，并为未来研究提供了新方向。

Abstract: When a reader encounters a word in English, they split the word into smaller
orthographic units in the process of recognizing its meaning. For example,
"rough", when split according to phonemes, is decomposed as r-ou-gh (not as
r-o-ugh or r-ough), where each group of letters corresponds to a sound. Since
there are many ways to segment a group of letters, this constitutes a
computational operation that has to be solved by the reading brain, many times
per minute, in order to achieve the recognition of words in text necessary for
reading. We hypothesized that providing segmentation information in the text
itself could help the reading process by reducing its computational cost. In
this paper we explore whether and how different visual interventions could
communicate segmentation information for reading and word recognition. We ran a
series of pre-registered lexical decision experiments with 192 participants
that tested five types of visual segmentations: outlines, spacing, connections,
underlines and color. The evidence indicates that, even with a moderate amount
of training, these visual interventions always slow down word identification,
but each to a different extent. These findings are important because they
indicate that, at least for typical adult readers with a moderate amount of
specific training in these visual interventions, accelerating the lexical
decision task is unlikely. The results also offer an empirical measurement of
the cost of a common set of visual manipulations of text, which can be useful
for practitioners seeking to visualize alongside or within text without
impacting reading performance. Finally, the interaction between typographically
encoded information and visual variables presented unique patterns that deviate
from existing theories, suggesting new directions for future inquiry.

</details>


### [69] [TextOnly: A Unified Function Portal for Text-Related Functions on Smartphones](https://arxiv.org/abs/2508.16926)
*Minghao Tu,Chun Yu,Xiyuan Shen,Zhi Zheng,Li Chen,Yuanchun Shi*

Main category: cs.HC

TL;DR: TextOnly是一个统一的功能门户，通过单一文本框实现多种文本相关功能的快速访问，结合LLM和BERT模型，准确率高且持续学习优化。


<details>
  <summary>Details</summary>
Motivation: 智能手机应用中，用户需多次操作才能访问特定文本框，功能访问效率低，TextOnly旨在简化这一过程。

Method: 利用大型语言模型(LLM)提供通用知识，BERT模型学习用户偏好，实现意图解析。

Result: 真实用户研究中，TextOnly的top-1准确率达71.35%，且能持续提升准确率和推理速度。

Conclusion: TextOnly在功能和输入简洁性上优于语音助手，用户满意度高。

Abstract: Text boxes serve as portals to diverse functionalities in today's smartphone
applications. However, when it comes to specific functionalities, users always
need to navigate through multiple steps to access particular text boxes for
input. We propose TextOnly, a unified function portal that enables users to
access text-related functions from various applications by simply inputting
text into a sole text box. For instance, entering a restaurant name could
trigger a Google Maps search, while a greeting could initiate a conversation in
WhatsApp. Despite their brevity, TextOnly maximizes the utilization of these
raw text inputs, which contain rich information, to interpret user intentions
effectively. TextOnly integrates large language models(LLM) and a BERT model.
The LLM consistently provides general knowledge, while the BERT model can
continuously learn user-specific preferences and enable quicker predictions.
Real-world user studies demonstrated TextOnly's effectiveness with a top-1
accuracy of 71.35%, and its ability to continuously improve both its accuracy
and inference speed. Participants perceived TextOnly as having satisfactory
usability and expressed a preference for TextOnly over manual executions.
Compared with voice assistants, TextOnly supports a greater range of
text-related functions and allows for more concise inputs.

</details>


### [70] [Opportunities and Challenges of Integrating ChatGPT in Education: Sentiment Analysis and Topic Modeling](https://arxiv.org/abs/2508.16966)
*Surat Teerakapibal,Poompak Kusawat*

Main category: cs.HC

TL;DR: 研究了ChatGPT在教育领域中的应用及其在不同职业群体中的感知差异。


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT在教育中的使用及其对不同职业群体的影响。

Method: 通过主题建模和情感分析分析Twitter数据。

Result: 发现讨论主题多样，情感和话题因职业而异，多数为正面或中性，但也存在诚信和准确性担忧。

Conclusion: 强调ChatGPT在教育中的机遇与挑战，需持续监测并制定负责任的政策。

Abstract: Since its recent debut, ChatGPT has become a global sensation and
significantly impacted the field of education. Both educational researchers and
practitioners have identified opportunities as well as risks associated with
the use of this novel tool in educational settings. Despite the ongoing debate,
there is still no research exploring occupational differences in the perception
of ChatGPT in education. In this paper, we analyzed Twitter data using topic
modeling and sentiment analysis to investigate how ChatGPT is perceived and
discussed differently in different occupations. Our study found diverse topics
discussed including its use in schools, impact on exams, academic integrity
concerns, and response accuracy evaluations. While most tweets were positive or
neutral, concerns about integrity and response accuracy were evident. Analysis
revealed sentiment and topic variations among users' occupations. These
findings emphasize the opportunities and challenges of integrating ChatGPT in
education, necessitating continued monitoring and informed policy-making for
responsible utilization.

</details>


### [71] [SCENIC: A Location-based System to Foster Cognitive Development in Children During Car Rides](https://arxiv.org/abs/2508.17058)
*Liuqing Chen,Yaxuan Song,Ke Lyu,Shuhong Xiao,Yilang Shen,Lingyun Sun*

Main category: cs.HC

TL;DR: SCENIC是一个互动系统，帮助6至11岁儿童通过基于位置的认知发展策略更好地感知外部环境，提升乘车体验。


<details>
  <summary>Details</summary>
Motivation: 儿童在乘车时感到无聊，常依赖电子设备，而车外丰富的动态景观是认知发展的宝贵资源。家长难以持续引导儿童探索。

Method: 结合家长的经验方法，设计了六种策略的系统，包括动态兴趣点选择和旅程画廊生成。

Result: 评估内容（N=21）和实地用户研究（7个家庭，10名儿童）显示，SCENIC提升了乘车体验和儿童与环境的连接。

Conclusion: SCENIC有效改善儿童乘车体验，促进他们与周围环境的互动。

Abstract: Car-riding is common for children in modern life. Given the repetitive nature
of daily commutes, they often feel bored and turn to electronic devices for
entertainment. Meanwhile, the rich and dynamic scenery outside the car
naturally attracts children's curiosity and offers valuable resources for
cognitive development. Our formative study reveals that parents' support during
car rides is often fleeting, as accompanying adults may struggle to
consistently guide children's exploration. To address this, we propose SCENIC,
an interactive system that helps children aged 6 to 11 better perceive the
external environment using location-based cognitive development strategies.
SCENIC builds upon experiential approaches used by parents, resulting in six
strategies embedded into the system. To improve engagement during routine
rides, SCENIC also incorporates dynamic point-of-interest selection and journey
gallery generation. We evaluated the generated content (N=21) and conducted an
in-situ user study with seven families and ten children. Results suggest that
SCENIC enhances the car-riding experience and helps children better connect
with their surroundings.

</details>


### [72] [Measuring Large Language Models Dependency: Validating the Arabic Version of the LLM-D12 Scale](https://arxiv.org/abs/2508.17063)
*Sameha AlShakhsi,Ala Yankouskaya,Magnus Liebherr,Raian Ali*

Main category: cs.HC

TL;DR: 研究验证了阿拉伯语版本的LLM-D12量表，证明其在评估对大型语言模型的工具性和关系性依赖方面具有良好心理测量特性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对阿拉伯语人群的心理测量工具来评估对AI（尤其是大型语言模型）的反应，亟需可靠且文化适应性的量表。

Method: 250名阿拉伯参与者完成阿拉伯语版LLM-D12，通过验证性因子分析和内部可靠性检验（Cronbach alpha）验证量表结构。

Result: 量表显示出良好的双因子结构（工具性和关系性依赖），内部可靠性高（总量表0.90），并与外部变量（如AI接受度、互联网成瘾等）显著相关。

Conclusion: 阿拉伯语LLM-D12是心理测量学可靠、文化适用的工具，为阿拉伯语环境中AI和大型语言模型的研究与政策提供支持。

Abstract: There is an urgent need for reliable, culturally validated instruments to
assess psychological responses to AI in general and large language models
(LLMs). This need is global issue, but it is especially urgent among
Arabic-speaking populations, where AI and LLMs adoption is accelerating, yet
psychometric tools remain limited. This study presents the first validation of
the LLM-D12, a dual-dimensional scale assessing Instrumental and Relationship
Dependency on LLMs, in an Arab sample. A total of 250 Arab participants
completed the Arabic version of the LLM-D12. Confirmatory Factor Analysis
confirms the original 2-factor structure of LLM-D12 with all items showing good
loading of corresponding Instrumental and Relationship Dependency. The scale
showed good to excellent internal reliability (Cronbach alpha is 0.90 for
Total, 0.85 for Instrumental Dependency, and 0.90 for Relationship Dependency).
External validation revealed that Instrumental Dependency was positively
associated with AI acceptance and internet addiction, while Relationship
Dependency was linked to lower need for cognition and greater trustworthiness
of LLM, demonstrating sensitivity of this instrument to different use and
personal factors. These findings confirm that Arabic LLM-D12 is a
psychometrically sound, culturally appropriate instrument, offering a necessary
tool for research, education, and policy concerning AI and LLMs engagement in
Arab contexts.

</details>


### [73] [Towards Deeper Understanding of Natural User Interactions in Virtual Reality Based Assembly Tasks](https://arxiv.org/abs/2508.17124)
*Ryan Ghamandi,Yahya Hmaiti,Mykola Maslych,Ravi Kiran Kattoju,Joseph J. LaViola Jr*

Main category: cs.HC

TL;DR: 研究通过虚拟现实机器人手臂模拟探索自然用户交互，识别了不同任务场景下的行为策略。


<details>
  <summary>Details</summary>
Motivation: 探索用户在虚拟现实中与机器人协作时的自然交互方式，以指导多模态用户界面的设计。

Method: 采用Wizard-of-Oz实验，收集用户在LEGO和PCB组装任务中的语音、手部追踪和注视数据。

Result: 发现协作和指导场景下用户行为差异显著，空间模糊性影响指令语言风格。

Conclusion: 研究为虚拟现实中自然多模态交互设计和理解提供了数据支持和策略分析。

Abstract: We explore natural user interactions using a virtual reality simulation of a
robot arm for assembly tasks. Using a Wizard-of-Oz study, participants
completed collaborative LEGO and instructive PCB assembly tasks, with the robot
responding under experimenter control. We collected voice, hand tracking, and
gaze data from users. Statistical analyses revealed that instructive and
collaborative scenarios elicit distinct behaviors and adopted strategies,
particularly as tasks progress. Users tended to use put-that-there language in
spatially ambiguous contexts and more descriptive instructions in spatially
clear ones. Our contributions include the identification of natural interaction
strategies through analyses of collected data, as well as the supporting
dataset, to guide the understanding and design of natural multimodal user
interfaces for instructive interaction with systems in virtual reality.

</details>


### [74] [Virtual Reality in Sign Language Education: Opportunities, Challenges, and the Road Ahead](https://arxiv.org/abs/2508.17362)
*Refia Daya,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: 本文系统综述了55篇VR在手语教育中的应用研究，总结了五类主题，分析了AI和VR技术的优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 解决手语教学中师资不足、早期接触少及传统方法不足的问题，探讨VR和AI技术的潜在应用。

Method: 系统性回顾55篇同行评审论文，分析五个核心主题。

Result: AI驱动的VR手势识别系统能提升学习效果，但存在硬件限制和设计不足等挑战。

Conclusion: 未来VR系统需结合技术发展与包容性教学，以服务全球手语学习者。

Abstract: Sign language (SL) is an essential mode of communication for Deaf and
Hard-of-Hearing (DHH) individuals. Its education remains limited by the lack of
qualified instructors, insufficient early exposure, and the inadequacy of
traditional teaching methods. Recent advances in Virtual Reality (VR) and
Artificial Intelligence (AI) offer promising new approaches to enhance sign
language learning through immersive, interactive, and feedback-rich
environments. This paper presents a systematic review of 55 peer-reviewed
studies on VR-based sign language education, identifying and analyzing five
core thematic areas: (1) gesture recognition and real-time feedback mechanisms;
(2) interactive VR environments for communicative practice; (3) gamification
for immersive and motivating learning experiences; (4) personalized and
adaptive learning systems; and (5) accessibility and inclusivity for diverse
DHH learners.
  The results reveal that AI-driven gesture recognition systems integrated with
VR can provide real-time feedback, significantly improving learner engagement
and performance. However, the analysis highlights critical challenges: hardware
limitations, inconsistent accuracy in gesture recognition, and a lack of
inclusive and adaptive design. This review contributes a comprehensive
synthesis of technological and pedagogical innovations in the field, outlining
current limitations and proposing actionable recommendations for developers and
researchers. By bridging technical advancement with inclusive pedagogy, this
review lays the foundation for next-generation VR systems that are equitable,
effective, and accessible for sign language learners worldwide.

</details>


### [75] [Characterizing Visualization Perception with Psychological Phenomena: Uncovering the Role of Subitizing in Data Visualization](https://arxiv.org/abs/2508.17460)
*Arran Zeyu Wang,Ghulam Jilani Quadri,Mengyuan Zhu,Chin Tseng,Danielle Albers Szafir*

Main category: cs.HC

TL;DR: 研究评估了现有分类可视化设计启发法的有效性，尤其是快速识别少量对象的认知现象在数据可视化中的应用。通过三个实验发现，类别数小于六时任务表现良好，证实了可视化中的快速识别现象。


<details>
  <summary>Details</summary>
Motivation: 现有可视化设计启发法多基于特定任务或类型，缺乏普遍性。研究旨通过心理学理论（如快速识别现象）评估这些启发法的适用性。

Method: 通过三个实验，使用多类散点图（2至15类）和三种不同任务（类别估计、相关性比较、聚类判断），研究类别数增加时任务表现的变化。

Result: 类别数小于六时任务表现良好，支持快速识别现象；类别数增加时表现下降，下降幅度取决于任务和编码方式。

Conclusion: 研究填补了启发法和实证证据间的空白，建议未来利用心理学理论进一步研究可视化感知。

Abstract: Understanding how people perceive visualizations is crucial for designing
effective visual data representations; however, many heuristic design
guidelines are derived from specific tasks or visualization types, without
considering the constraints or conditions under which those guidelines hold. In
this work, we aimed to assess existing design heuristics for categorical
visualization using well-established psychological knowledge. Specifically, we
examine the impact of the subitizing phenomenon in cognitive psychology --
people's ability to automatically recognize a small set of objects instantly
without counting -- in data visualizations. We conducted three experiments with
multi-class scatterplots -- between 2 and 15 classes with varying design
choices -- across three different tasks -- class estimation, correlation
comparison, and clustering judgments -- to understand how performance changes
as the number of classes (and therefore set size) increases. Our results
indicate if the category number is smaller than six, people tend to perform
well at all tasks, providing empirical evidence of subitizing in visualization.
When category numbers increased, performance fell, with the magnitude of the
performance change depending on task and encoding. Our study bridges the gap
between heuristic guidelines and empirical evidence by applying
well-established psychological theories, suggesting future opportunities for
using psychological theories and constructs to characterize visualization
perception.

</details>


### [76] [Visual Analytics for Causal Reasoning from Real-World Health Data](https://arxiv.org/abs/2508.17474)
*Arran Zeyu Wang,David Borland,David Gotz*

Main category: cs.HC

TL;DR: 论文呼吁通过视觉分析工具改进从健康数据中进行因果推理的能力，以解决当前在观察数据中识别真实因果关系的方法不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模纵向健康数据的分析为医疗改进提供了机会，但当前的观察性数据难以揭示真实的因果机制，这对医疗决策者至关重要。

Method: 论文提出通过视觉分析（VA）研究，开发能够增强人类专家直觉因果推理能力的新工具。

Result: 通过结合因果推理算法与直观的可视化工具，可以克服医疗数据的复杂性，实现更准确的数据驱动医疗实践。

Conclusion: 未来的视觉分析工具有望支持因果推理能力，从而改善医疗决策和健康结果。

Abstract: The increasing capture and analysis of large-scale longitudinal health data
offer opportunities to improve healthcare and advance medical understanding.
However, a critical gap exists between (a) -- the observation of patterns and
correlations, versus (b) -- the understanding of true causal mechanisms that
drive outcomes. An accurate understanding of the underlying mechanisms that
cause various changes in medical status is crucial for decision-makers across
various healthcare domains and roles, yet inferring causality from real-world
observational data is difficult for both methodological and practical
challenges. This Grand Challenge advocates increased Visual Analytics (VA)
research on this topic to empower people with the tool for sound causal
reasoning from health data. We note this is complicated by the complex nature
of medical data -- the volume, variety, sparsity, and temporality of health
data streams make the use of causal inference algorithms difficult. Combined
with challenges imposed by the realities of health-focused settings, including
time constraints and traditional medical work practices, existing causal
reasoning approaches are valuable but insufficient. We argue that advances in
research can lead to new VA tools that augment human expertise with intuitive
and robust causal inference capabilities, which can help realize a new paradigm
of data-driven, causality-aware healthcare practices that improve human health
outcomes.

</details>


### [77] [SonoCraftAR: Towards Supporting Personalized Authoring of Sound-Reactive AR Interfaces by Deaf and Hard of Hearing Users](https://arxiv.org/abs/2508.17597)
*Jaewook Lee,Davin Win Kyi,Leejun Kim,Jenny Peng,Gagyeom Lim,Jeremy Zhengqi Huang,Dhruv Jain,Jon E. Froehlich*

Main category: cs.HC

TL;DR: SonoCraftAR是一个支持聋人和听力障碍者通过自然语言输入创建个性化声音可视化AR界面的原型系统。


<details>
  <summary>Details</summary>
Motivation: 当前AR系统无法满足聋人和听力障碍者对个性化声音可视化的需求。

Method: 结合实时音频信号处理和多智能体LLM管道，通过矢量图形库生成动态2D界面。

Result: 系统能够根据音频主频动态调整视觉属性（如大小和颜色），实现声音的动态响应。

Conclusion: 研究证明了开放式的AR界面创作可行性，并展望了AI辅助工具在声音可访问性方面的潜力。

Abstract: Augmented reality (AR) has shown promise for supporting Deaf and
hard-of-hearing (DHH) individuals by captioning speech and visualizing
environmental sounds, yet existing systems do not allow users to create
personalized sound visualizations. We present SonoCraftAR, a proof-of-concept
prototype that empowers DHH users to author custom sound-reactive AR interfaces
using typed natural language input. SonoCraftAR integrates real-time audio
signal processing with a multi-agent LLM pipeline that procedurally generates
animated 2D interfaces via a vector graphics library. The system extracts the
dominant frequency of incoming audio and maps it to visual properties such as
size and color, making the visualizations respond dynamically to sound. This
early exploration demonstrates the feasibility of open-ended sound-reactive AR
interface authoring and discusses future opportunities for personalized,
AI-assisted tools to improve sound accessibility.

</details>


### [78] [I Can't Join, But I Will Send My Agent: Stand-in Enhanced Asynchronous Meetings (SEAM)](https://arxiv.org/abs/2508.17676)
*Zhongyi Bai,Jens Emil Grønbæk,Andrew Irlitti,Jarrod Knibbe,Eduardo Velloso*

Main category: cs.HC

TL;DR: SEAM（替代者增强异步会议）是一种虚拟现实会议，通过虚拟代理代表缺席用户，增强会议体验。研究表明，该技术对与会者和缺席者均有积极影响。


<details>
  <summary>Details</summary>
Motivation: 探索如何提升异步会议的协作体验，使与会者和缺席者都能感受到更强的社交存在感和参与感。

Method: 采用混合方法研究，通过Wizard-of-Oz方法与45名参与者进行实验，分析会议中和缺席者回放时的体验。

Result: 研究发现，虚拟代理能提升会议效率，增强社交互动，使缺席者感觉被包容。

Conclusion: SEAM为未来异步会议提供了一种可行的解决方案，使协作体验更接近同步会议。

Abstract: We propose and explore the user experience of SEAM -- Stand-in Enhanced
Asynchronous Meetings -- virtual reality meetings in which embodied virtual
agents represent absent users. During the meeting, attendees can address the
agent, and the absent user can later watch the recording from its perspective
to respond. Through two mixed-method studies with 45 participants using the
Wizard-of-Oz approach, we explored both the perspectives of the attendees in
the original meeting and of the absent users later re-watching the meeting. We
found that the stand-in can enhance meetings, benefiting both present and
absent collaborators. Present attendees can easily access information that
drives decision-making in the meeting perceive high social presence of
absentees. Absentees also felt included when watching recordings because of the
social interactions and attention towards them. Our contributions demonstrate a
proof of concept for future asynchronous meetings in which collaborators can
interact conversationally more akin to how they would if it had been
synchronous.

</details>


### [79] [TRUCE-AV: A Multimodal dataset for Trust and Comfort Estimation in Autonomous Vehicles](https://arxiv.org/abs/2508.17880)
*Aditi Bhalla,Christian Hellert,Enkelejda Kasneci,Nastassja Becker*

Main category: cs.HC

TL;DR: 本文介绍了TRUCE-AV数据集，用于实时评估自动驾驶中用户的信任与舒适度，结合生理信号和环境数据，并通过机器学习模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的安全性和广泛接受需要理解用户的信任与舒适度，但现有研究缺乏实时评估和多模态数据支持。

Method: 采集31名参与者在模拟自动驾驶中的实时信任评分、舒适度评级、生理信号（如心率、凝视、情绪）和环境数据（如车速、周围车辆位置），并通过机器学习模型进行分析。

Result: 树基模型（如随机森林、XGBoost）和非线性模型（如KNN、MLP回归器）在信任分类和舒适度回归中表现最佳，SHAP分析识别了关键特征。

Conclusion: TRUCE-AV数据集支持开发自适应自动驾驶系统，动态响应用户信任与舒适度，提升安全性和用户体验。

Abstract: Understanding and estimating driver trust and comfort are essential for the
safety and widespread acceptance of autonomous vehicles. Existing works analyze
user trust and comfort separately, with limited real-time assessment and
insufficient multimodal data. This paper introduces a novel multimodal dataset
called TRUCE-AV, focusing on trust and comfort estimation in autonomous
vehicles. The dataset collects real-time trust votes and continuous comfort
ratings of 31 participants during a simulator-based fully autonomous driving.
Simultaneously, physiological signals, such as heart rate, gaze, and emotions,
along with environmental data (e.g., vehicle speed, nearby vehicle positions,
and velocity), are recorded throughout the drives. Standard pre- and post-drive
questionnaires were also administered to assess participants' trust in
automation and overall well-being, enabling the correlation of subjective
assessments with real-time responses. To demonstrate the utility of our
dataset, we evaluated various machine learning models for trust and comfort
estimation using physiological data. Our analysis showed that tree-based models
like Random Forest and XGBoost and non-linear models such as KNN and MLP
regressor achieved the best performance for trust classification and comfort
regression. Additionally, we identified key features that contribute to these
estimations by using SHAP analysis on the top-performing models. Our dataset
enables the development of adaptive AV systems capable of dynamically
responding to user trust and comfort levels non-invasively, ultimately
enhancing safety, user experience, and human-centered vehicle design.

</details>


### [80] ["Nobody should control the end user": Exploring Privacy Perspectives of Indian Internet Users in Light of DPDPA](https://arxiv.org/abs/2508.17962)
*Sana Athar,Devashish Gosain,Anja Feldmann,Mannat Kaur,Ha Dao*

Main category: cs.HC

TL;DR: 该研究调查了印度互联网用户对Cookie横幅、在线隐私及隐私法规（尤其是新通过的DPDPA）的认知与态度，发现隐私意识强的用户常缺乏一致的隐私机制了解，且担忧未转化为行动。


<details>
  <summary>Details</summary>
Motivation: 随着在线互动的增加，数据隐私与透明性问题凸显。印度新通过的DPDPA可能侵犯用户隐私，因此首次探讨印度用户对此的认知与态度。

Method: 通过在线匿名调查428名印度参与者，涵盖用户对Cookie横幅的看法、对隐私法规的态度，以及对DPDPA争议条款的接受度。

Result: 隐私意识强的用户常缺乏隐私机制了解，其隐私担忧未必引发保护行为。用户对政府的不信任影响其对DPDPA的看法，推动政策修订需求。

Conclusion: 需改进DPDPA的沟通、优化用户为中心的同意机制，并完善政策以提升印度的数据隐私实践。

Abstract: With the rapid increase in online interactions, concerns over data privacy
and transparency of data processing practices have become more pronounced.
While regulations like the GDPR have driven the widespread adoption of cookie
banners in the EU, India's Digital Personal Data Protection Act (DPDPA)
promises similar changes domestically, aiming to introduce a framework for data
protection. However, certain clauses within the DPDPA raise concerns about
potential infringements on user privacy, given the exemptions for government
accountability and user consent requirements. In this study, for the first
time, we explore Indian Internet users' awareness and perceptions of cookie
banners, online privacy, and privacy regulations, especially in light of the
newly passed DPDPA. We conducted an online anonymous survey with 428 Indian
participants, which addressed: (1) users' perspectives on cookie banners, (2)
their attitudes towards online privacy and privacy regulations, and (3) their
acceptance of 10 contentious DPDPA clauses that favor state authorities and may
enable surveillance. Our findings reveal that privacy-conscious users often
lack consistent awareness of privacy mechanisms, and their concerns do not
always lead to protective actions. Our thematic analysis of 143 open ended
responses shows that users' privacy and data protection concerns are rooted in
skepticism towards the government, shaping their perceptions of the DPDPA and
fueling demands for policy revisions. Our study highlights the need for clearer
communication regarding the DPDPA, user-centric consent mechanisms, and policy
refinements to enhance data privacy practices in India.

</details>


### [81] [An Introduction to Silent Paralinguistics](https://arxiv.org/abs/2508.18127)
*Zhao Ren,Simon Pistrosch,Buket Coşkun,Kevin Scheck,Anton Batliner,Björn W. Schuller,Tanja Schultz*

Main category: cs.HC

TL;DR: 该论文探讨了如何从无声语音中提取副语言信息（如情感状态），并将其整合到重构的有声语音中，以实现更自然的交流。


<details>
  <summary>Details</summary>
Motivation: 无声语音接口在处理无声语音方面已有研究，但副语言信息（如情感）的提取仍存在空白。本文旨在填补这一空白。

Method: 提出了“无声副语言学”框架，专注于从无声语音的生物信号中预测副语言信息，并将其整合到有声语音中。

Result: 提供了对无声副语言学领域方法、研究策略和目标的全面调查。

Conclusion: 无声副语言学的研究有望提升无声语音接口的自然性和实用性，填补现有技术的空白。

Abstract: The ability to speak is an inherent part of human nature and fundamental to
our existence as a social species. Unfortunately, this ability can be
restricted in certain situations, such as for individuals who have lost their
voice or in environments where speaking aloud is unsuitable. Additionally, some
people may prefer not to speak audibly due to privacy concerns. For such cases,
silent speech interfaces have been proposed, which focus on processing
biosignals corresponding to silently produced speech. These interfaces enable
synthesis of audible speech from biosignals that are produced when speaking
silently and recognition aka decoding of biosignals into text that corresponds
to the silently produced speech. While recognition and synthesis of silent
speech has been a prominent focus in many research studies, there is a
significant gap in deriving paralinguistic information such as affective states
from silent speech. To fill this gap, we propose Silent Paralinguistics, aiming
to predict paralinguistic information from silent speech and ultimately
integrate it into the reconstructed audible voice for natural communication.
This survey provides a comprehensive look at methods, research strategies, and
objectives within the emerging field of silent paralinguistics.

</details>


### [82] [Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation](https://arxiv.org/abs/2508.18142)
*Tianjun Wei,Huizhong Guo,Yingpeng Du,Zhu Sun,Chen Huang,Dongxia Wang,Jie Zhang*

Main category: cs.HC

TL;DR: 提出了一种利用用户反馈和LLM生成高质量模拟数据的框架，通过降低噪声和过滤信息样本提升推荐系统的模拟效果。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中用户反馈通常模糊且噪声大，直接利用效率低，需要一种方法有效对齐用户偏好。

Method: 1) 使用LLM生成决策过程以减少反馈歧义；2) 通过不确定性估计和行为采样过滤噪声，筛选高质量样本。

Result: 实验表明该框架显著提升了LLM与用户偏好的对齐及领域推理能力，并提供了更可解释的信号。

Conclusion: 该方法为推荐系统社区提供了有价值的工具，并为更广泛的人类中心AI研究提供了启示。

Abstract: User simulation is increasingly vital to develop and evaluate recommender
systems (RSs). While Large Language Models (LLMs) offer promising avenues to
simulate user behavior, they often struggle with the absence of specific domain
alignment required for RSs and the efficiency demands of large-scale
simulation. A vast yet underutilized resource for enhancing this alignment is
the extensive user feedback inherent in RSs. However, directly leveraging such
feedback presents two significant challenges. First, user feedback in RSs is
often ambiguous and noisy, which negatively impacts effective preference
alignment. Second, the massive volume of feedback largely hinders the
efficiency of preference alignment, necessitating an efficient filtering
mechanism to identify more informative samples. To overcome these hurdles, we
introduce a novel data construction framework that leverages user feedback in
RSs with advanced LLM capabilities to generate high-quality simulation data.
Our framework unfolds in two key phases: (1) employing LLMs to generate
cognitive decision-making processes on constructed simulation samples, reducing
ambiguity in raw user feedback; (2) data distillation based on uncertainty
estimation and behavior sampling to filter challenging yet denoised simulation
samples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using
such high-quality dataset with corresponding decision-making processes.
Extensive experiments verify that our framework significantly boosts the
alignment with human preferences and in-domain reasoning capabilities of
fine-tuned LLMs, and provides more insightful and interpretable signals when
interacting with RSs. We believe our work will advance the RS community and
offer valuable insights for broader human-centric AI research.

</details>


### [83] [InReAcTable: LLM-Powered Interactive Visual Data Story Construction from Tabular Data](https://arxiv.org/abs/2508.18174)
*Gerile Aodeng,Guozheng Li,Yunshan Feng,Qiyang Chen,Yu Zhang,Chi Harold Liu*

Main category: cs.HC

TL;DR: 论文提出了一种名为InReAcTable的框架，通过结构化和语义连接数据洞察，帮助用户构建视觉数据故事，解决了现有方法依赖用户专业知识或无法捕捉动态目标的缺陷。


<details>
  <summary>Details</summary>
Motivation: 表格数据中的洞察包含有价值的信息，但用户难以从大量离散洞察中整合出符合分析目标的统一叙述。现有方法要么效率低，要么无法适应动态需求。

Method: InReAcTable框架通过结构过滤（利用洞察图）和语义过滤（基于大型语言模型的检索增强生成方法）来推荐符合用户分析意图的洞察。

Result: 开发了交互式原型系统，并通过案例研究和用户实验验证了框架和系统的实用性和有效性。

Conclusion: InReAcTable框架和原型系统能够有效帮助用户构建符合分析需求的视觉数据故事。

Abstract: Insights in tabular data capture valuable patterns that help analysts
understand critical information. Organizing related insights into visual data
stories is crucial for in-depth analysis. However, constructing such stories is
challenging because of the complexity of the inherent relations between
extracted insights. Users face difficulty sifting through a vast number of
discrete insights to integrate specific ones into a unified narrative that
meets their analytical goals. Existing methods either heavily rely on user
expertise, making the process inefficient, or employ automated approaches that
cannot fully capture their evolving goals. In this paper, we introduce
InReAcTable, a framework that enhances visual data story construction by
establishing both structural and semantic connections between data insights.
Each user interaction triggers the Acting module, which utilizes an insight
graph for structural filtering to narrow the search space, followed by the
Reasoning module using the retrieval-augmented generation method based on large
language models for semantic filtering, ultimately providing insight
recommendations aligned with the user's analytical intent. Based on the
InReAcTable framework, we develop an interactive prototype system that guides
users to construct visual data stories aligned with their analytical
requirements. We conducted a case study and a user experiment to demonstrate
the utility and effectiveness of the InReAcTable framework and the prototype
system for interactively building visual data stories.

</details>


### [84] [Can AI Have a Personality? Prompt Engineering for AI Personality Simulation: A Chatbot Case Study in Gender-Affirming Voice Therapy Training](https://arxiv.org/abs/2508.18234)
*Tailon D. Jackson,Byunggu Yu*

Main category: cs.HC

TL;DR: 研究通过提示工程使大语言模型模拟一致性人格，应用于语音病理学学生训练的聊天机器人。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以通过提示工程引导大语言模型模拟一致的人格，以提升语音病理学学生训练的互动效果。

Method: 使用提示工程设计一个名为Monae Jackson的聊天机器人，模拟32岁跨性别女性的性格，并进行Big Five人格测试。

Result: 聊天机器人能够保持可识别且一致的人格特征，支持提示工程在模拟稳定人格中的有效性。

Conclusion: 提示工程可以帮助AI聊天机器人模拟稳定的人格特征，适用于特定领域的互动需求。

Abstract: This thesis investigates whether large language models (LLMs) can be guided
to simulate a consistent personality through prompt engineering. The study
explores this concept within the context of a chatbot designed for
Speech-Language Pathology (SLP) student training, specifically focused on
gender-affirming voice therapy. The chatbot, named Monae Jackson, was created
to represent a 32-year-old transgender woman and engage in conversations
simulating client-therapist interactions. Findings suggest that with prompt
engineering, the chatbot maintained a recognizable and consistent persona and
had a distinct personality based on the Big Five Personality test. These
results support the idea that prompt engineering can be used to simulate stable
personality characteristics in AI chatbots.

</details>


### [85] [Caregiver-in-the-Loop AI: A Simulation-Based Feasibility Study for Dementia Task Verification](https://arxiv.org/abs/2508.18267)
*Joy Lai,David Black,Kelly Beaton,Bing Ye,Alex Mihailidis*

Main category: cs.HC

TL;DR: 生成式AI（如GPT-4）可用于痴呆症患者的任务验证，减轻护理人员压力。可行性研究表明，结合上下文信息和护理反馈可提升系统效果。


<details>
  <summary>Details</summary>
Motivation: 痴呆症患者的护理人员在验证任务完成情况时感到压力，即使使用数字提醒系统。生成式AI可能通过自动化验证和决策支持缓解这一问题。

Method: 研究评估了整合AI的数字提醒系统，测试GPT-4生成后续问题的能力、AI标记机制的准确性，以及护理反馈对系统优化的作用。使用64条匿名提醒进行模拟测试。

Result: 上下文信息和护理反馈提升了AI问题的清晰度和相关性。标记系统对安全关键任务表现准确，但对主观或非紧急任务仍有挑战。

Conclusion: AI辅助任务验证在痴呆护理中具有可行性。未来需关注实际应用验证和扩展性。

Abstract: Caregivers of people living with dementia (PLwD) experience stress when
verifying whether tasks are truly completed, even with digital reminder
systems. Generative AI, such as GPT-4, may help by automating task verification
through follow-up questioning and decision support.
  This feasibility study evaluates an AI-powered task verification system
integrated with digital reminders for PLwD. It examines (1) GPT-4's ability to
generate effective follow-up questions, (2) the accuracy of an AI-driven
response flagging mechanism, and (3) the role of caregiver feedback in refining
system adaptability. A simulated pipeline was tested on 64 anonymized
reminders. GPT-4 generated follow-up questions with and without contextual
information about PLwD routines. Responses were classified into High, Medium,
or Low concern, and simulated caregiver feedback was used to refine outputs.
  Results show that contextual information and caregiver input improved the
clarity and relevance of AI-generated questions. The flagging system accurately
identified concerns, particularly for safety-critical tasks, though subjective
or non-urgent tasks remained challenging. Findings demonstrate the feasibility
of AI-assisted task verification in dementia care. Context-aware AI prompts and
caregiver feedback can enhance task monitoring, reduce caregiver stress, and
strengthen PLwD support. Future work should focus on real-world validation and
scalability.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [86] [DecoMind: A Generative AI System for Personalized Interior Design Layouts](https://arxiv.org/abs/2508.16696)
*Reema Alshehri,Rawan Alotaibi,Leen Almasri,Rawan Altaweel*

Main category: cs.GR

TL;DR: 提出了一种基于用户输入生成室内设计布局的系统，结合CLIP、Stable Diffusion与ControlNet自动生成并评估设计。


<details>
  <summary>Details</summary>
Motivation: 解决用户自定义室内设计的需求，提供自动化、个性化的设计方案。

Method: 利用CLIP选择家具，通过Stable Diffusion与ControlNet生成布局，再使用分类器评估设计匹配度。

Result: 实现了自动化生成符合用户偏好的室内设计。

Conclusion: 该系统为个性化室内设计提供了一种高效的自动化解决方案。

Abstract: This paper introduces a system for generating interior design layouts based
on user inputs, such as room type, style, and furniture preferences. CLIP
extracts relevant furniture from a dataset, and a layout that contains
furniture and a prompt are fed to Stable Diffusion with ControlNet to generate
a design that incorporates the selected furniture. The design is then evaluated
by classifiers to ensure alignment with the user's inputs, offering an
automated solution for realistic interior design.

</details>


### [87] [MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation](https://arxiv.org/abs/2508.16911)
*Prerit Gupta,Jason Alexander Fotso-Puepi,Zhengyuan Li,Jay Mehta,Aniket Bera*

Main category: cs.GR

TL;DR: 介绍Multimodal DuetDance（MDD）数据集，支持文本控制和音乐驱动的3D双人舞动作生成，并提供两项新任务和基线评估。


<details>
  <summary>Details</summary>
Motivation: 为文本控制和音乐驱动的双人舞动作生成提供多样化的多模态数据集，填补数据空白。

Method: 构建包含620分钟高质量动捕数据、同步音乐和1万+细粒度文本描述的数据集，支持两项任务（Text-to-Duet和Text-to-Dance Accompaniment）。

Result: MDD是首个整合人体动作、音乐和文本的双人舞数据集，提供基线评估支持研究。

Conclusion: MDD为多模态舞蹈生成开辟新方向，推动未来研究。

Abstract: We introduce Multimodal DuetDance (MDD), a diverse multimodal benchmark
dataset designed for text-controlled and music-conditioned 3D duet dance motion
generation. Our dataset comprises 620 minutes of high-quality motion capture
data performed by professional dancers, synchronized with music, and detailed
with over 10K fine-grained natural language descriptions. The annotations
capture a rich movement vocabulary, detailing spatial relationships, body
movements, and rhythm, making MDD the first dataset to seamlessly integrate
human motions, music, and text for duet dance generation. We introduce two
novel tasks supported by our dataset: (1) Text-to-Duet, where given music and a
textual prompt, both the leader and follower dance motion are generated (2)
Text-to-Dance Accompaniment, where given music, textual prompt, and the
leader's motion, the follower's motion is generated in a cohesive, text-aligned
manner. We include baseline evaluations on both tasks to support future
research.

</details>


### [88] [A Survey of Deep Learning-based Point Cloud Denoising](https://arxiv.org/abs/2508.17011)
*Jinxi Wang,Ben Fei,Dasith de Silva Edirimuni,Zheng Liu,Ying He,Xuequan Lu*

Main category: cs.GR

TL;DR: 该论文综述了截至2025年8月基于深度学习的点云去噪方法，从监督级别和建模角度对文献进行了分类，并分析了结构趋势，评估了方法性能，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于真实环境中获取的点云常受噪声影响，降低几何保真度和下游性能，点云去噪成为一个基本问题，而传统方法难以处理复杂噪声模式，深度学习方法显示出优势。

Method: 从监督级别（有监督vs无监督）和建模角度（功能分类）对文献进行组织，提出统一的基准测试，评估去噪质量、表面保真度、点分布和计算效率。

Result: 建立了统一的基准测试框架，评估了不同方法的表现，并分析了架构趋势。

Conclusion: 总结了深度学习方法在点云去噪中的进展，提出了未来研究的方向和挑战。

Abstract: Accurate 3D geometry acquisition is essential for a wide range of
applications, such as computer graphics, autonomous driving, robotics, and
augmented reality. However, raw point clouds acquired in real-world
environments are often corrupted with noise due to various factors such as
sensor, lighting, material, environment etc, which reduces geometric fidelity
and degrades downstream performance. Point cloud denoising is a fundamental
problem, aiming to recover clean point sets while preserving underlying
structures. Classical optimization-based methods, guided by hand-crafted
filters or geometric priors, have been extensively studied but struggle to
handle diverse and complex noise patterns. Recent deep learning approaches
leverage neural network architectures to learn distinctive representations and
demonstrate strong outcomes, particularly on complex and large-scale point
clouds. Provided these significant advances, this survey provides a
comprehensive and up-to-date review of deep learning-based point cloud
denoising methods up to August 2025. We organize the literature from two
perspectives: (1) supervision level (supervised vs. unsupervised), and (2)
modeling perspective, proposing a functional taxonomy that unifies diverse
approaches by their denoising principles. We further analyze architectural
trends both structurally and chronologically, establish a unified benchmark
with consistent training settings, and evaluate methods in terms of denoising
quality, surface fidelity, point distribution, and computational efficiency.
Finally, we discuss open challenges and outline directions for future research
in this rapidly evolving field.

</details>


### [89] [DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions](https://arxiv.org/abs/2508.17342)
*Hengyuan Zhang,Zhe Li,Xingqun Qi,Mengze Li,Muyi Sun,Man Zhang,Sirui Han*

Main category: cs.GR

TL;DR: 论文提出了一种名为DanceEditor的框架，用于从音乐信号生成可编辑的舞蹈动作，并构建了大规模数据集DanceRemix。


<details>
  <summary>Details</summary>
Motivation: 现有舞蹈生成方法无法满足实际编舞中用户编辑需求，且缺乏高质量可编辑舞蹈数据集。

Method: 采用预测-编辑范式，结合音乐信号和文本描述，通过跨模态编辑模块（CEM）实现迭代编辑。

Result: 实验表明，DanceEditor在DanceRemix数据集上优于现有模型。

Conclusion: DanceEditor框架能够高效生成与音乐和文本描述一致的可编辑舞蹈动作，推动了实际编舞应用的发展。

Abstract: Generating coherent and diverse human dances from music signals has gained
tremendous progress in animating virtual avatars. While existing methods
support direct dance synthesis, they fail to recognize that enabling users to
edit dance movements is far more practical in real-world choreography
scenarios. Moreover, the lack of high-quality dance datasets incorporating
iterative editing also limits addressing this challenge. To achieve this goal,
we first construct DanceRemix, a large-scale multi-turn editable dance dataset
comprising the prompt featuring over 25.3M dance frames and 84.5K pairs. In
addition, we propose a novel framework for iterative and editable dance
generation coherently aligned with given music signals, namely DanceEditor.
Considering the dance motion should be both musical rhythmic and enable
iterative editing by user descriptions, our framework is built upon a
prediction-then-editing paradigm unifying multi-modal conditions. At the
initial prediction stage, our framework improves the authority of generated
results by directly modeling dance movements from tailored, aligned music.
Moreover, at the subsequent iterative editing stages, we incorporate text
descriptions as conditioning information to draw the editable results through a
specifically designed Cross-modality Editing Module (CEM). Specifically, CEM
adaptively integrates the initial prediction with music and text prompts as
temporal motion cues to guide the synthesized sequences. Thereby, the results
display music harmonics while preserving fine-grained semantic alignment with
text descriptions. Extensive experiments demonstrate that our method
outperforms the state-of-the-art models on our newly collected DanceRemix
dataset. Code is available at https://lzvsdy.github.io/DanceEditor/.

</details>


### [90] [Random-phase Gaussian Wave Splatting for Computer-generated Holography](https://arxiv.org/abs/2508.17480)
*Brian Chao,Jacqueline Yang,Suyeon Choi,Manu Gopakumar,Ryota Koiso,Gordon Wetzstein*

Main category: cs.GR

TL;DR: 随机相位高斯波光栅（GWS-RP）通过改进带宽利用，提升全息近眼显示的性能，支持精确视差和散焦模糊，并抑制散斑噪声。


<details>
  <summary>Details</summary>
Motivation: 传统的高斯波光栅（GWS）假设高斯原始相位平滑，限制了视依赖效应和精确散焦模糊的建模，且未充分利用空间光调制器的带宽。

Method: 提出GWS-RP，包括新的波前合成过程和针对随机相位高斯原始的alpha混合方案，并结合时间复用技术抑制散斑。

Result: GWS-RP实现了全带宽光场CGH，支持精确视差和散焦模糊，展示了卓越的图像质量和感知真实的3D全息效果。

Conclusion: GWS-RP通过随机相位和改进算法，显著提升了近眼全息显示的图像质量和功能，为下一代设备奠定了基础。

Abstract: Holographic near-eye displays offer ultra-compact form factors for virtual
and augmented reality systems, but rely on advanced computer-generated
holography (CGH) algorithms to convert 3D scenes into interference patterns
that can be displayed on spatial light modulators (SLMs). Gaussian Wave
Splatting (GWS) has recently emerged as a powerful CGH paradigm that allows for
the conversion of Gaussians, a state-of-the-art neural 3D representation, into
holograms. However, GWS assumes smooth-phase distributions over the Gaussian
primitives, limiting their ability to model view-dependent effects and
reconstruct accurate defocus blur, and severely under-utilizing the
space-bandwidth product of the SLM. In this work, we propose random-phase GWS
(GWS-RP) to improve bandwidth utilization, which has the effect of increasing
eyebox size, reconstructing accurate defocus blur and parallax, and supporting
time-multiplexed rendering to suppress speckle artifacts.
  At the core of GWS-RP are (1) a fundamentally new wavefront compositing
procedure and (2) an alpha-blending scheme specifically designed for
random-phase Gaussian primitives, ensuring physically correct color
reconstruction and robust occlusion handling. Additionally, we present the
first formally derived algorithm for applying random phase to Gaussian
primitives, grounded in rigorous statistical optics analysis and validated
through practical near-eye display applications. Through extensive simulations
and experimental validations, we demonstrate that these advancements,
collectively with time-multiplexing, uniquely enables full-bandwith light field
CGH that supports accurate accurate parallax and defocus, yielding
state-of-the-art image quality and perceptually faithful 3D holograms for
next-generation near-eye displays.

</details>


### [91] [Enhancing Reference-based Sketch Colorization via Separating Reference Representations](https://arxiv.org/abs/2508.17620)
*Dingkun Yan,Xinrui Wang,Zhuoru Li,Suguru Saito,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.GR

TL;DR: 提出了一种新的基于参考的草图着色框架，通过分解着色过程并优化参考表示，解决了现有方法因训练与推理数据分布不匹配导致的过拟合和伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考的草图着色方法在训练时使用语义和空间对齐的图像三元组，而实际应用中参考和草图常存在显著不对齐，导致着色结果质量下降。本文旨在解决这一问题。

Method: 提出了一种模块化框架，通过分解着色过程，分别训练高层语义嵌入、背景编码器和风格编码器，以优化不同方面的参考表示和着色质量。

Result: 通过定性和定量评估及用户研究，证明了该方法在着色质量和参考相似性上的优越性能。

Conclusion: 该框架能够灵活适应多种用例，并在着色任务中显著优于现有方法，代码和预训练权重将公开。

Abstract: Reference-based sketch colorization methods have garnered significant
attention for the potential application in animation and digital illustration
production. However, most existing methods are trained with image triplets of
sketch, reference, and ground truth that are semantically and spatially
similar, while real-world references and sketches often exhibit substantial
misalignment. This mismatch in data distribution between training and inference
leads to overfitting, consequently resulting in artifacts and signif- icant
quality degradation in colorization results. To address this issue, we conduct
an in-depth analysis of the reference representations, defined as the
intermedium to transfer information from reference to sketch. Building on this
analysis, we introduce a novel framework that leverages distinct reference
representations to optimize different aspects of the colorization process. Our
approach decomposes colorization into modular stages, al- lowing
region-specific reference injection to enhance visual quality and reference
similarity while mitigating spatial artifacts. Specifically, we first train a
backbone network guided by high-level semantic embeddings. We then introduce a
background encoder and a style encoder, trained in separate stages, to enhance
low-level feature transfer and improve reference similar- ity. This design also
enables flexible inference modes suited for a variety of use cases. Extensive
qualitative and quantitative evaluations, together with a user study,
demonstrate the superior performance of our proposed method compared to
existing approaches. Code and pre-trained weight will be made publicly
available upon paper acceptance.

</details>


### [92] [Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph](https://arxiv.org/abs/2508.17645)
*Xiaoyang Huang,Bingbing Ni,Wenjun Zhang*

Main category: cs.GR

TL;DR: 该论文提出了一种将AI生成的3D内容与设计师工作流程相结合的方法，通过生成可微分的建模操作序列，解决了AI与设计工具之间的不兼容问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成的3D内容与设计师使用的参数化建模工具（如网格或神经表示）之间存在不兼容性，这限制了AI在3D设计行业中的实际应用。

Method: 论文将基本建模操作（如挤出、布尔运算）转化为可微分单元，构建了一个分层的图结构，并通过端到端优化来生成设计操作序列，无需监督学习。

Result: 生成的序列在几何保真度、网格布线、步骤组成和编辑灵活性方面表现优异，且完全兼容设计行业标准。

Conclusion: 该方法成功弥合了AI与设计工具之间的鸿沟，提升了人机协作效率，为3D-AIGC的实际应用提供了可行方案。

Abstract: The emergence of 3D artificial intelligence-generated content (3D-AIGC) has
enabled rapid synthesis of intricate geometries. However, a fundamental
disconnect persists between AI-generated content and human-centric design
paradigms, rooted in representational incompatibilities: conventional AI
frameworks predominantly manipulate meshes or neural representations
(\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate within
parametric modeling tools. This disconnection diminishes the practical value of
AI for 3D industry, undermining the efficiency of human-AI collaboration. To
resolve this disparity, we focus on generating design operation sequences,
which are structured modeling histories that comprehensively capture the
step-by-step construction process of 3D assets and align with designers'
typical workflows in modern 3D software. We first reformulate fundamental
modeling operations (\emph{e.g.}, \emph{Extrude}, \emph{Boolean}) into
differentiable units, enabling joint optimization of continuous (\emph{e.g.},
\emph{Extrude} height) and discrete (\emph{e.g.}, \emph{Boolean} type)
parameters via gradient-based learning. Based on these differentiable
operations, a hierarchical graph with gating mechanism is constructed and
optimized end-to-end by minimizing Chamfer Distance to target geometries.
Multi-stage sequence length constraint and domain rule penalties enable
unsupervised learning of compact design sequences without ground-truth sequence
supervision. Extensive validation demonstrates that the generated operation
sequences achieve high geometric fidelity, smooth mesh wiring, rational step
composition and flexible editing capacity, with full compatibility within
design industry.

</details>


### [93] [MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting](https://arxiv.org/abs/2508.17811)
*Hanzhi Chang,Ruijie Zhu,Wenjie Chang,Mulin Yu,Yanzhe Liang,Jiahao Lu,Zhuoyuan Li,Tianzhu Zhang*

Main category: cs.GR

TL;DR: MeshSplat是一个基于2D高斯溅射的稀疏视图表面重建框架，通过利用2DGS作为桥梁，结合几何先验和新型视图合成技术，实现了无3D监督的表面重建。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏视图表面重建方法在极端稀疏视图输入下难以恢复准确几何，MeshSplat旨在通过2DGS解决这一问题。

Method: 提出一个前馈网络预测每视图像素对齐的2DGS，结合加权Chamfer距离损失和法线预测网络优化几何精度。

Result: 实验证明MeshSplat在稀疏视图网格重建任务中达到最先进性能。

Conclusion: MeshSplat通过创新的2DGS技术有效解决了稀疏视图下的表面重建难题。

Abstract: Surface reconstruction has been widely studied in computer vision and
graphics. However, existing surface reconstruction works struggle to recover
accurate scene geometry when the input views are extremely sparse. To address
this issue, we propose MeshSplat, a generalizable sparse-view surface
reconstruction framework via Gaussian Splatting. Our key idea is to leverage
2DGS as a bridge, which connects novel view synthesis to learned geometric
priors and then transfers these priors to achieve surface reconstruction.
Specifically, we incorporate a feed-forward network to predict per-view
pixel-aligned 2DGS, which enables the network to synthesize novel view images
and thus eliminates the need for direct 3D ground-truth supervision. To improve
the accuracy of 2DGS position and orientation prediction, we propose a Weighted
Chamfer Distance Loss to regularize the depth maps, especially in overlapping
areas of input views, and also a normal prediction network to align the
orientation of 2DGS with normal vectors predicted by a monocular normal
estimator. Extensive experiments validate the effectiveness of our proposed
improvement, demonstrating that our method achieves state-of-the-art
performance in generalizable sparse-view mesh reconstruction tasks. Project
Page: https://hanzhichang.github.io/meshsplat_web

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [94] [TSPC-PFD: TSPC-Based Low-Power High-Resolution CMOS Phase Frequency Detector](https://arxiv.org/abs/2508.16933)
*Dhandeep Challagundla,Venkata Krishna Vamsi Sundarapu,Ignatius Bezzam,Riadul Islam*

Main category: cs.ET

TL;DR: 提出了一种新型低功耗TSPC基相位频率检测器，完全消除盲区并实现40 ps的超小死区，TSMC 28 nm工艺下功耗仅4.41 uW。


<details>
  <summary>Details</summary>
Motivation: 传统PFD在高频应用中存在显著死区和盲区，影响相位检测精度并增加抖动，需改进设计。

Method: 采用TSPC技术设计新型PFD，优化电路以消除盲区并缩小死区。

Result: 实现无盲区、死区仅40 ps的PFD，3 GHz频率下功耗4.41 uW，面积10.42微米²。

Conclusion: 所提PFD设计在高性能和低功耗方面显著优于传统方案，适合高速应用。

Abstract: Phase Frequency Detectors (PFDs) are essential components in Phase-Locked
Loop (PLL) and Delay-Locked Loop (DLL) systems, responsible for comparing phase
and frequency differences and generating up/down signals to regulate charge
pumps and/or, consequently, Voltage-Controlled Oscillators (VCOs). Conventional
PFD designs often suffer from significant dead zones and blind zones, which
degrade phase detection accuracy and increase jitter in high-speed
applications. This paper addresses PFD design challenges and presents a novel
low-power True Single-Phase Clock (TSPC)-based PFD. The proposed design
eliminates the blind zone entirely while achieving a minimal dead zone of 40
ps. The proposed PFD, implemented using TSMC 28 nm technology, demonstrates a
low-power consumption of 4.41 uW at 3 GHz input frequency with a layout area of
$10.42\mu m^2$.

</details>


### [95] [Quantum Optimization for the Steiner Traveling Salesman Problem with Time Windows and Pickup and Delivery](https://arxiv.org/abs/2508.17896)
*Alessia Ciacco,Francesca Guerriero,Eneko Osaba*

Main category: cs.ET

TL;DR: 提出了结合时间窗、取送货和车辆容量限制的Steiner旅行商问题扩展模型，并采用D-Wave的混合量子计算平台进行求解。


<details>
  <summary>Details</summary>
Motivation: 解决现代物流中的复杂路由问题，如最后一公里配送和逆向物流。

Method: 提出基于弧和基于节点的两种数学模型，并利用D-Wave的混合量子计算平台实现，同时引入预处理方法减少计算复杂度。

Result: 混合量子方法能有效求解实际问题规模，展现了其在下一代路由优化中的潜力。

Conclusion: 该模型为复杂物流问题提供了新的解决途径，混合量子计算展现了实际应用的可行性。

Abstract: We present the Steiner Traveling Salesman Problem with Time Windows and
Pickup and Delivery, an advanced and practical extension of classical routing
models. This variant integrates the characteristics of the Steiner Traveling
Salesman Problem with time-window constraints, pickup and delivery operations
and vehicle capacity limitations. These features closely mirror the
complexities of contemporary logistics challenges, including last-mile
distribution, reverse logistics and on-demand service scenarios. To tackle the
inherent computational difficulties of this NP-hard problem, we propose two
specialized mathematical formulations: an arc-based model and a node-oriented
model, each designed to capture distinct structural aspects of the problem.
Both models are implemented on D-Wave's LeapCQMHybrid platform, which combines
quantum and classical techniques for solving constrained optimization tasks. We
further introduce a preprocessing reduction method that eliminates redundant
arcs, significantly enhancing computational performance and scalability.
Experimental results demonstrate that hybrid quantum approaches are capable of
solving problem instances of realistic size, underscoring their potential as a
transformative tool for next-generation routing optimization.

</details>


### [96] [SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study](https://arxiv.org/abs/2508.18250)
*Yang Xiang,Fernando García-Redondo,Arvind Sharma,Van Dai Nguyen,Andrea Fantini,Philippe Matagne,Siddharth Rao,Subhali Subhechha,Lynn Verschueren,Mohammed Aftab Baig,Marie Garcia Bardon,Geert Hellings*

Main category: cs.ET

TL;DR: 本文探讨了SOT-MRAM在异构系统扩展范式下用于末级缓存（LLCs）的跨节点扩展潜力，通过BEOL RSs提出了一种减少bitcell面积的方法，并分析了其读写性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究SOT-MRAM在LLCs中的应用潜力，特别是在异构系统扩展中，通过优化bitcell配置以提升性能和功耗效率。

Method: 采用设计-技术协同优化（DTCO）方法，评估7纳米技术下的bitcell配置，并提出使用BEOL RSs以减少面积并优化性能。

Result: BEOL RSs可减少10-40%的bitcell面积，但带来3-5 ns的读取延迟增加和2.5-5倍的能耗成本。

Conclusion: BEOL RSs在SOT-MRAM的功率-性能-面积扩展中具有潜力，但仍需解决读取性能的权衡问题。

Abstract: This work explores the cross-node scaling potential of SOT-MRAM for
last-level caches (LLCs) under heterogeneous system scaling paradigm. We
perform extensive Design-Technology Co-Optimization (DTCO) exercises to
evaluate the bitcell footprint for different cell configurations at a
representative 7 nm technology and to assess their implications on read and
write power-performance. We crucially identify the MTJ routing struggle in
conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary
bitcell area scaling challenge and propose to use BEOL read selectors (BEOL
RSs) that enable (10 -- 40) % bitcell area reduction and eventually match
sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet
the required SOT switching current, provided the magnetic free layer properties
be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This
is particularly to attribute to their (i) more available Si fins for write
transistor and (ii) lower bitline resistance at reduced cell width. We
nevertheless underscore the read tradeoff associated with BEOL RSs, with the
low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the
imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost
relative to 2T1R. This article thus highlights the realistic prospects and
hurdles of BEOL RSs towards holistic power-performance-area scaling of
SOT-MRAM.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [97] [Performance measurements of modern Fortran MPI applications with Score-P](https://arxiv.org/abs/2508.16592)
*Gregor Corbin*

Main category: cs.DC

TL;DR: MPI 3.0引入了Fortran 2008的新语言绑定，但工具支持不足，Score-P通过Fortran实现的MPI包装器解决了这一问题。


<details>
  <summary>Details</summary>
Motivation: 由于MPI的Fortran 2008绑定（F08）缺乏工具支持，用户被迫使用不安全的接口，因此需要改进工具支持。

Method: 通过在Fortran中实现MPI包装器，覆盖MPI 4.1标准，并使用代码生成器自动生成约50k行代码。

Result: 新的F08包装器已成功应用于两个流体动力学模拟代码（Neko和EPIC），并支持性能测量。

Conclusion: 通过Fortran实现的MPI包装器弥补了F08绑定的工具支持不足，展示了实际应用的可行性。

Abstract: Version 3.0 of the Message-Passing Interface (MPI) standard, released in
2012, introduced a new set of language bindings for Fortran 2008. By making use
of modern language features and the enhanced interoperability with C, there was
finally a type safe and standard conforming method to call MPI from Fortran.
This highly recommended use mpi_f08 language binding has since then been widely
adopted among developers of modern Fortran applications. However, tool support
for the F08 bindings is still lacking almost a decade later, forcing users to
recede to the less safe and convenient interfaces. Full support for the F08
bindings was added to the performance measurement infrastructure Score-P by
implementing MPI wrappers in Fortran. Wrappers cover the latest MPI standard
version 4.1 in its entirety, matching the features of the C wrappers. By
implementing the wrappers in modern Fortran, we can provide full support for
MPI procedures passing attributes, info objects, or callbacks. The
implementation is regularly tested under the MPICH test suite. The new F08
wrappers were already used by two fluid dynamics simulation codes -- Neko, a
spectral finite-element code derived from Nek5000, and EPIC (Elliptical
Parcel-In-Cell) -- to successfully generate performance measurements. In this
work, we additionally present our design considerations and sketch out the
implementation, discussing the challenges we faced in the process. The key
component of the implementation is a code generator that produces approximately
50k lines of MPI wrapper code to be used by Score-P, relying on the Python
pympistandard module to provide programmatic access to the extracted data from
the MPI standard.

</details>


### [98] [GPU Acceleration for Faster Evolutionary Spatial Cyclic Game Systems](https://arxiv.org/abs/2508.16639)
*Louie Sinadjan*

Main category: cs.DC

TL;DR: 摘要介绍了GPU加速的进化空间循环游戏（ESCGs）模拟框架的设计、实现和评估，与传统单线程方法相比显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统单线程ESCG模拟计算成本高且扩展性差，需要高性能实现以提升效率。

Method: 使用Apple的Metal和Nvidia的CUDA开发了高性能实现，并以验证的单线程C++版本为基准进行比较。

Result: GPU加速带来了显著的性能提升（CUDA实现最高28倍），并支持更大规模系统（3200x3200）。

Conclusion: 该研究为ESCG研究提供了可配置的模拟平台，推动了该领域的计算工具发展。

Abstract: This dissertation presents the design, implementation and evaluation of
GPU-accelerated simulation frameworks for Evolutionary Spatial Cyclic Games
(ESCGs), a class of agent-based models used to study ecological and
evolutionary dynamics. Traditional single-threaded ESCG simulations are
computationally expensive and scale poorly. To address this, high-performance
implementations were developed using Apple's Metal and Nvidia's CUDA, with a
validated single-threaded C++ version serving as a baseline comparison point.
  Benchmarking results show that GPU acceleration delivers significant
speedups, with the CUDA maxStep implementation achieving up to a 28x
improvement. Larger system sizes, up to 3200x3200, became tractable, while
Metal faced scalability limits. The GPU frameworks also enabled replication and
critical extension of recent ESCG studies, revealing sensitivities to system
size and runtime not fully explored in prior work.
  Overall, this project provides a configurable ESCG simulation platform that
advances the computational toolkit for this field of research. This
dissertation forms the basis for a paper accepted for publication and
presentation at the European Modelling and Simulation Symposium.

</details>


### [99] [Equinox: Holistic Fair Scheduling in Serving Large Language Models](https://arxiv.org/abs/2508.16646)
*Zhixiang Wei,James Yen,Jingyi Chen,Ziyang Zhang,Zhibai Huang,Chen Chen,Xingzi Yu,Yicheng Gu,Chenggang Wu,Yun Wang,Mingyuan Xia,Jie Wu,Hao Wang,Zhengwei Qi*

Main category: cs.DC

TL;DR: 该论文提出了一种双计数器框架（Equinox），通过预测用户感知的延迟和资源效率指标，实现公平感知的调度，显著提升了吞吐量和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM服务在用户公平性和资源公平性之间的权衡问题，避免调度时的预测延迟。

Method: 使用确定性混合预测专家（MoPE）框架预测关键指标，计算统一的全公平分数，并通过自适应批处理和调度优化实现公平调度。

Result: Equinox在实验中实现了1.3倍吞吐量提升、60%的首标记延迟降低和13%的公平性提升，同时保持94%的GPU利用率。

Conclusion: Equinox通过双计数器框架和预测机制，在异构平台上实现了高效的公平感知调度。

Abstract: We address the limitations of current LLM serving with a dual-counter
framework separating user and operator perspectives. The User Fairness Counter
measures quality of service via weighted tokens and latency; the Resource
Fairness Counter measures operational efficiency through throughput and GPU
utilization. Since these metrics are only available post-execution, creating a
scheduling paradox, we introduce a deterministic Mixture of Prediction Experts
(MoPE) framework to predict user-perceived latency, output tokens, throughput,
and GPU utilization. These predictions enable calculation of a unified Holistic
Fairness score that balances both counters through tunable parameters for
proactive fairness-aware scheduling. We implement this in Equinox, an
open-source system with other optimizations like adaptive batching, and
stall-free scheduling. Evaluations on production traces (ShareGPT, LMSYS) and
synthetic workloads demonstrate Equinox achieves up to $1.3\times$ higher
throughput, 60\% lower time-to-first-token latency, and 13\% higher fairness
versus VTC while maintaining 94\% GPU utilization, proving fairness under
bounded discrepancy across heterogeneous platforms.

</details>


### [100] [Neuromorphic Simulation of Drosophila Melanogaster Brain Connectome on Loihi 2](https://arxiv.org/abs/2508.16792)
*Felix Wang,Bradley H. Theilman,Fred Rothganger,William Severa,Craig M. Vineyard,James B. Aimone*

Main category: cs.DC

TL;DR: 首次在神经形态计算硬件上实现了生物学真实的果蝇全脑连接组仿真，解决了硬件限制问题，并验证了性能优势。


<details>
  <summary>Details</summary>
Motivation: 解决生物网络特有的稀疏、循环和不规则连接结构在传统计算硬件上的适应性问题。

Method: 在Intel Loihi 2平台上实现了包含14万个神经元和5000万个突触的果蝇全脑连接组，并克服了硬件限制。

Result: 验证了实现的有效性，且神经形态硬件的性能比传统硬件快多个数量级，稀疏活动下优势更明显。

Conclusion: 证明现代神经形态平台能够加速生物学真实模型，推动了神经启发AI和计算神经科学的进步。

Abstract: We demonstrate the first-ever nontrivial, biologically realistic connectome
simulated on neuromorphic computing hardware. Specifically, we implement the
whole-brain connectome of the adult Drosophila melanogaster (fruit fly) from
the FlyWire Consortium containing 140K neurons and 50M synapses on the Intel
Loihi 2 neuromorphic platform. This task is particularly challenging due to the
characteristic connectivity structure of biological networks. Unlike artificial
neural networks and most abstracted neural models, real biological circuits
exhibit sparse, recurrent, and irregular connectivity that is poorly suited to
conventional computing methods intended for dense linear algebra. Though
neuromorphic hardware is architecturally better suited to discrete event-based
biological communication, mapping the connectivity structure to frontier
systems still faces challenges from low-level hardware constraints, such as
fan-in and fan-out memory limitations. We describe solutions to these
challenges that allow for the full FlyWire connectome to fit onto 12 Loihi 2
chips. We statistically validate our implementation by comparing network
behavior across multiple reference simulations. Significantly, we achieve a
neuromorphic implementation that is orders of magnitude faster than numerical
simulations on conventional hardware, and we also find that performance
advantages increase with sparser activity. These results affirm that today's
scalable neuromorphic platforms are capable of implementing and accelerating
biologically realistic models -- a key enabling technology for advancing
neuro-inspired AI and computational neuroscience.

</details>


### [101] [PICO: Performance Insights for Collective Operations](https://arxiv.org/abs/2508.16809)
*Saverio Pasqualoni,Lorenzo Piarulli,Daniele De Sensi*

Main category: cs.DC

TL;DR: PICO是一个轻量级、可扩展的框架，旨在简化集体操作的性能评估和基准测试，弥补现有框架在详细分析和可复现性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 集体操作是高性能计算和大规模AI训练的基础，但现有框架在详细性能分析和可复现性方面存在不足，亟需改进。

Method: 提出了PICO框架，专注于提供轻量级和可扩展的解决方案，简化集体操作的基准测试。

Result: PICO框架能够提供更详细的性能分析，并保证可复现性和可扩展性。

Conclusion: PICO在集体操作的性能评估中表现出色，弥补了现有框架的不足，具有较高的实用价值。

Abstract: Collective operations are cornerstones of both HPC application and
large-scale AI training and inference. Yet, comprehensive, systematic and
reproducible performance evaluation and benchmarking of said operations is not
straightforward. Existing frameworks do not provide sufficiently detailed
profiling information, nor they ensure reproducibility and extensibility. In
this paper, we present PICO (Performance Insights for Collective Operations), a
novel lightweight, extensible framework built with the aim of simplifying
collective operations benchmarking.

</details>


### [102] [Memory-Efficient Federated Fine-Tuning of Large Language Models via Layer Pruning](https://arxiv.org/abs/2508.17209)
*Yebo Wu,Jingguang Li,Chunlin Tian,Zhijiang Guo,Li Li*

Main category: cs.DC

TL;DR: FedPruner是一种创新的联邦微调范式，通过智能层修剪降低内存成本，使资源受限设备也能参与隐私保护的LLM适应。


<details>
  <summary>Details</summary>
Motivation: 解决联邦微调中高内存成本限制资源受限设备参与的问题。

Method: 采用宏-微协同修剪框架：宏功能驱动层编排机制分组层，微观重要性感知策略修剪组内层，构建设备特定子模型。

Result: 显著优于现有方法，平均模型精度提升1.98%，峰值内存使用减少75%。

Conclusion: FedPruner高效平衡了模型精度与资源消耗，为联邦学习中的设备多样性提供了实用解决方案。

Abstract: Federated fine-tuning enables privacy-preserving Large Language Model (LLM)
adaptation, but its high memory cost limits participation from
resource-constrained devices. We propose FedPruner, an innovative federated
fine-tuning paradigm that tackles this via intelligent layer pruning. FedPruner
flexibly prunes the global model, creating personalized submodels based on
device memory constraints. It employs a macro-micro synergistic pruning
framework: a macro-level functionality-driven layer orchestration mechanism
groups layers, while a micro-level importance-aware layer selection strategy
prunes within groups to build device-specific submodels. We further introduce a
fine-grained variant that independently prunes Multi-Head Attention and
Feed-Forward Network components to precisely preserve critical architectural
elements. Extensive experimental results demonstrate that FedPruner
significantly outperforms state-of-the-art approaches, achieving up to a 1.98\%
improvement in average model accuracy while reducing peak memory usage by 75\%.

</details>


### [103] [TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained Elastic Long-Context LLM Serving](https://arxiv.org/abs/2508.17219)
*Bingyang Wu,Zili Zhang,Yinmin Zhong,Guanzhe Huang,Yibo Zhu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: TokenLake是一种统一的片段级前缀缓存池，通过声明式缓存接口和负载均衡算法优化缓存管理，提升性能和命中率。


<details>
  <summary>Details</summary>
Motivation: 现有前缀缓存系统与请求调度紧密耦合，导致负载不平衡、数据冗余和内存碎片化，需解耦缓存管理与调度优化。

Method: 提出TokenLake，采用声明式缓存接口和基于热点的负载均衡算法，管理片段级前缀缓存，减少通信量。

Result: 实际测试中，TokenLake相比现有方案，吞吐量最高提升2.6倍，命中率提升2.1倍。

Conclusion: TokenLake有效解决了缓存管理与调度的耦合问题，显著提升了系统性能和效率。

Abstract: Prefix caching is crucial to accelerate multi-turn interactions and requests
with shared prefixes. At the cluster level, existing prefix caching systems are
tightly coupled with request scheduling to optimize cache efficiency and
computation performance together, leading to load imbalance, data redundancy,
and memory fragmentation of caching systems across instances. To address these
issues, memory pooling is promising to shield the scheduler from the underlying
cache management so that it can focus on the computation optimization. However,
because existing prefix caching systems only transfer increasingly longer
prefix caches between instances, they cannot achieve low-latency memory
pooling.
  To address these problems, we propose a unified segment-level prefix cache
pool, TokenLake. It uses a declarative cache interface to expose requests'
query tensors, prefix caches, and cache-aware operations to TokenLake for
efficient pooling. Powered by this abstraction, TokenLake can manage prefix
cache at the segment level with a heavy-hitter-aware load balancing algorithm
to achieve better cache load balance, deduplication, and defragmentation.
TokenLake also transparently minimizes the communication volume of query
tensors and new caches. Based on TokenLake, the scheduler can schedule requests
elastically by using existing techniques without considering prefix cache
management. Evaluations on real-world workloads show that TokenLake can improve
throughput by up to 2.6$\times$ and 2.0$\times$ and boost hit rate by
2.0$\times$ and 2.1$\times$, compared to state-of-the-art cache-aware routing
and cache-centric PD-disaggregation solutions, respectively.

</details>


### [104] [Bine Trees: Enhancing Collective Operations by Optimizing Communication Locality](https://arxiv.org/abs/2508.17311)
*Daniele De Sensi,Saverio Pasqualoni,Lorenzo Piarulli,Tommaso Bonato,Seydou Ba,Matteo Turisini,Jens Domke,Torsten Hoefler*

Main category: cs.DC

TL;DR: Bine（二项负二进制）树是一种提高通信局部性的集体算法，适用于大型HPC系统，能减少全局链路流量达33%，并在多种拓扑结构中实现最高5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 在大型HPC系统中，通信局部性对集体操作的性能至关重要，尤其是在节点内部全连接但全局连接稀疏的超载网络中。

Method: 提出Bine树算法家族，结合了二项树和蝴蝶算法的通用性，显著减少全局链路流量。实现了八种基于Bine的集体操作，并在四种大规模超级计算机（Dragonfly、Dragonfly+、超载的胖树和环形拓扑）上测试。

Result: Bine树在多种向量大小和节点数量下实现了最高5倍的性能加速，并一致减少了全局链路流量。

Conclusion: Bine树在提高通信局部性和减少全局流量方面表现优异，适用于多种网络拓扑的HPC系统。

Abstract: Communication locality plays a key role in the performance of collective
operations on large HPC systems, especially on oversubscribed networks where
groups of nodes are fully connected internally but sparsely linked through
global connections. We present Bine (binomial negabinary) trees, a family of
collective algorithms that improve communication locality. Bine trees maintain
the generality of binomial trees and butterflies while cutting global-link
traffic by up to 33%. We implement eight Bine-based collectives and evaluate
them on four large-scale supercomputers with Dragonfly, Dragonfly+,
oversubscribed fat-tree, and torus topologies, achieving up to 5x speedups and
consistent reductions in global-link traffic across different vector sizes and
node counts.

</details>


### [105] [Easy Acceleration with Distributed Arrays](https://arxiv.org/abs/2508.17493)
*Jeremy Kepner,Chansup Byun,LaToya Anderson,William Arcand,David Bestor,William Bergeron,Alex Bonn,Daniel Burrill,Vijay Gadepally,Ryan Haney,Michael Houle,Matthew Hubbell,Hayden Jananthan,Michael Jones,Piotr Luszczek,Lauren Milechin,Guillermo Morales,Julie Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Charles Yee,Peter Michaleas*

Main category: cs.DC

TL;DR: 该论文探讨了分布式数组在不同硬件上的性能表现，展示了其在CPU和GPU节点上的可扩展性，并比较了不同年代硬件在内存带宽上的改进。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保持高生产力的同时，通过分布式数组这一抽象实现可扩展的垂直、水平和时间性能。

Method: 使用STREAM内存带宽基准测试，评估分布式数组在多种硬件上的性能，包括CPU核心、CPU节点和GPU节点。

Result: 横向扩展性能线性增长，硬件在内存带宽方面显著提升，数百个MIT SuperCloud节点实现了>1 PB/s的持续带宽。

Conclusion: 分布式数组是一种高效的抽象，能够实现高扩展性性能，同时适应不同年代的硬件改进。

Abstract: High level programming languages and GPU accelerators are powerful enablers
for a wide range of applications. Achieving scalable vertical (within a compute
node), horizontal (across compute nodes), and temporal (over different
generations of hardware) performance while retaining productivity requires
effective abstractions. Distributed arrays are one such abstraction that
enables high level programming to achieve highly scalable performance.
Distributed arrays achieve this performance by deriving parallelism from data
locality, which naturally leads to high memory bandwidth efficiency. This paper
explores distributed array performance using the STREAM memory bandwidth
benchmark on a variety of hardware. Scalable performance is demonstrated within
and across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across
multiple nodes was linear. The hardware used spans decades and allows a direct
comparison of hardware improvements for memory bandwidth over this time range;
showing a 10x increase in CPU core bandwidth over 20 years, 100x increase in
CPU node bandwidth over 20 years, and 5x increase in GPU node bandwidth over 5
years. Running on hundreds of MIT SuperCloud nodes simultaneously achieved a
sustained bandwidth $>$1 PB/s.

</details>


### [106] [Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD NPUs](https://arxiv.org/abs/2508.17593)
*Aadesh Deshmukh,Venkata Yaswanth Raparti,Samuel Hsu*

Main category: cs.DC

TL;DR: Zen-Attention框架通过优化DRAM带宽利用，显著降低了Transformer模型的延迟。


<details>
  <summary>Details</summary>
Motivation: 解决在能效和DRAM带宽受限设备上部署Transformer模型时的延迟问题。

Method: 通过层折叠、分块、数据移动和布局优化，系统性地探索设计空间以实现最佳DRAM带宽利用。

Result: 注意力块延迟提升4倍，端到端网络延迟提升32%。

Conclusion: Zen-Attention框架在NPU上高效映射动态注意力层，显著提升性能。

Abstract: Transformer-based deep learning models are increasingly deployed on energy,
and DRAM bandwidth constrained devices such as laptops and gaming consoles,
which presents significant challenges in meeting the latency requirements of
the models. The industry is turning to neural processing units (NPUs) for
superior performance-per-watt (perf/watt); however, efficiently mapping dynamic
attention layers to the NPUs remains a challenging task. For optimizing
perf/watt, AMD XDNA NPUs employ software managed caches and share system memory
with host. This requires substantial engineering effort to unlock efficient
tiling, buffer allocation, and data movement to extract the maximum efficiency
from the device. This paper introduces Zen-Attention, a framework that
optimizes DRAM bandwidth utilization in the attention layer of models by
systematically exploring the complex design space of layer folding, tiling, and
data-movement on the interconnect, and the tensor layouts to come up with an
optimal solution. Our evaluation includes comparative analysis of end-to-end
model latency and specific attention latency in each model. We demonstrate how
the framework enhances mapping capabilities by varying input dimensions, which
require padding and masking in the attention block. For representative
transformer models, the Zen-Attention Framework achieves up to 4x improvement
in the latency of the attention block and up to 32% improvement in end-to-end
network latency compared to the baseline Unfolded- approaches.

</details>


### [107] [ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters at Scale](https://arxiv.org/abs/2508.17624)
*Ge Shi,Hanieh Sadri,Qian Wang,Yu Zhang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: ESFT通过选择性调优MoE模型中的专家提升任务性能，但部署面临资源挑战。ExpertWeave系统通过共享基础模型和优化内存管理，高效支持多适配器并发服务，显著提升资源利用率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在任务特定调优后的大规模部署问题，尤其是资源消耗和适配器兼容性挑战。

Method: 提出ExpertWeave系统，包括虚拟内存辅助的专家权重管理和批量路由优化内核，支持多适配器共享MoE基础模型。

Result: 在单加速器上高效支持16B MoE模型的多适配器并发服务，提升资源利用率（94x KV缓存容量，18%吞吐量），延迟增加仅4-11%。

Conclusion: ExpertWeave为MoE模型的多任务适配器部署提供了高效且非侵入性的解决方案，显著降低资源需求并保持模型精度。

Abstract: Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large
language models to enhance their task-specific performance by selectively
tuning the top-activated experts for the task. Serving these fine-tuned models
at scale is challenging: deploying merged models in isolation is prohibitively
resource-hungry, while existing multi-adapter serving systems with LoRA-style
additive updates are incompatible with ESFT's expert-oriented paradigm. We
present ExpertWeave, a system that serves multiple ESFT adapters concurrently
over a single shared MoE base model, drastically reducing the memory footprint
and improving resource utilization. To seamlessly integrate into existing
inference pipelines for MoE models with non-intrusive modifications and minimal
latency overhead, ExpertWeave introduces a virtual-memory-assisted expert
weight manager that co-locates base-model and adapter experts without incurring
memory overhead from fragmentation, and a fused kernel for batched rerouting to
enable lightweight redirection of tokens to the appropriate experts at runtime.
Our evaluations show that ExpertWeave can simultaneously serve multiple
adapters of a 16B MoE model on a single accelerator where the baseline runs out
of memory, or provides up to 94x more KV cache capacity and achieves up to 18%
higher throughput while using comparable resources, all without compromising
model accuracy. ExpertWeave maintains low overhead even when scaling to 20
adapters, with a 4-11% latency increase compared with serving the base model
alone. Source code will be released soon.

</details>


### [108] [Scalable Engine and the Performance of Different LLM Models in a SLURM based HPC architecture](https://arxiv.org/abs/2508.17814)
*Anderson de Lima Luiz,Shubham Vijay Kurlekar,Munir Georges*

Main category: cs.DC

TL;DR: 提出了一种基于SLURM的高性能计算架构，用于部署异构大型语言模型（LLM），实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 为了解决在多节点集群中高效管理CPU、GPU和内存资源的问题，并支持LLM的高并发推理需求。

Method: 采用SLURM进行动态资源调度，并利用容器化微服务无缝集成，支持单次和批量推理，以及高级工作流。

Result: 实验表明，小模型可处理128个并发请求（延迟低于50毫秒），大模型则在2个并发用户时延迟超过2秒。

Conclusion: 该架构在高性能计算基础设施上实现了高效、响应迅速且容错的LLM推理，适合实际应用场景。

Abstract: This work elaborates on a High performance computing (HPC) architecture based
on Simple Linux Utility for Resource Management (SLURM) [1] for deploying
heterogeneous Large Language Models (LLMs) into a scalable inference engine.
Dynamic resource scheduling and seamless integration of containerized
microservices have been leveraged herein to manage CPU, GPU, and memory
allocations efficiently in multi-node clusters. Extensive experiments, using
Llama 3.2 (1B and 3B parameters) [2] and Llama 3.1 (8B and 70B) [3], probe
throughput, latency, and concurrency and show that small models can handle up
to 128 concurrent requests at sub-50 ms latency, while for larger models,
saturation happens with as few as two concurrent users, with a latency of more
than 2 seconds. This architecture includes Representational State Transfer
Application Programming Interfaces (REST APIs) [4] endpoints for single and
bulk inferences, as well as advanced workflows such as multi-step "tribunal"
refinement. Experimental results confirm minimal overhead from container and
scheduling activities and show that the approach scales reliably both for batch
and interactive settings. We further illustrate real-world scenarios, including
the deployment of chatbots with retrievalaugmented generation, which helps to
demonstrate the flexibility and robustness of the architecture. The obtained
results pave ways for significantly more efficient, responsive, and
fault-tolerant LLM inference on large-scale HPC infrastructures.

</details>


### [109] [Wait-free Replicated Data Types and Fair Reconciliation](https://arxiv.org/abs/2508.18193)
*Petr Kuznetsov,Maxence Perion,Sara Tucci-Piergiovanni*

Main category: cs.DC

TL;DR: 本文提出了一种基于DAG的框架，用于解决无等待数据复制中的操作撤销和客户端饥饿问题，确保最终一致性和公平进展。


<details>
  <summary>Details</summary>
Motivation: 无等待的数据复制面临操作频繁撤销和客户端可能饥饿的挑战，需要通过一种方法实现稳定收敛和公平进展。

Method: 使用基于DAG的框架，副本通过交换本地视图并使用调和函数合并，确保操作序列的共同稳定前缀和无客户端饥饿。

Result: 设计的调和函数实现了无等待的最终一致性副本状态机，保证了操作的稳定收敛和客户端的公平进展。

Conclusion: 该框架有效解决了无等待数据复制的关键挑战，为分布式系统提供了实用的解决方案。

Abstract: Replication is a standard way to maintain availability of shared data in
fault-prone distributed systems. To make sure that the data replicas are
up-to-date, they need to synchronize, which typically means engaging the
replicas in waiting for coherent responses from each other. The amount of
waiting depends on the consistency and availability guarantees we impose on the
system. The folklore CAP theory states that strong consistency (the set of
replicas create an illusion of one correct server) and strong availability (the
replicas' states are reachable despite network partitions) cannot be
implemented in the same system. A popular way to deal with this impossibility
is to relax consistency to be only eventual: the replicas eventually converge
to the same state. In return, the replicas can be wait-free, i.e., the clients
can get the data from the closest replica without waiting for other ones.
  Wait-free data replication faces two important challenges. First, the
operations issued by the clients may be constantly revoked, i.e., their effects
can be repeatedly recomputed due to asynchrony and concurrency. Second, even if
some operations eventually stabilize in their effects, a particular client may
still experience starvation if, from some point onward, each of its operations
is later revoked. In this paper, we address these challenges through a general
DAG-based framework for replicated data types, where replicas exchange their
local views and merge them using a reconciliation function. Within this
framework, we design reconciliation functions that implement a wait-free
eventually consistent replicated state machine ensuring both stable convergence
and fair progress. Specifically, every replica maintains a growing sequence of
client operations, and we guarantee that: (1) all replicas share a common,
monotonically growing stable prefix of operations, and (2) no client starves.

</details>


### [110] [Practical GPU Choices for Earth Observation: ResNet-50 Training Throughput on Integrated, Laptop, and Cloud Accelerators](https://arxiv.org/abs/2508.18206)
*Ritvik Chaturvedi*

Main category: cs.DC

TL;DR: 基于ResNet的Sentinel-2影像土地覆盖分类流程，在三种GPU上实现2倍训练速度提升并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 探索在消费级和云GPU上部署深度学习模型进行可扩展地理空间分析的可行性。

Method: 采用自动化数据获取、预处理、分块、模型训练和可视化的容器化流程，基于ResNet模型进行土地覆盖分类。

Result: 在NVIDIA RTX 3060和Tesla T4上训练速度较Apple M3 Pro提升2倍，同时在EuroSAT数据集上保持高分类精度。

Conclusion: 结果表明，消费级和云GPU可有效支持深度学习地理空间分析模型的部署。

Abstract: This project implements a ResNet-based pipeline for land use and land cover
(LULC) classification on Sentinel-2 imagery, benchmarked across three
heterogeneous GPUs. The workflow automates data acquisition, geospatial
preprocessing, tiling, model training, and visualization, and is fully
containerized for reproducibility. Performance evaluation reveals up to a 2x
training speed-up on an NVIDIA RTX 3060 and a Tesla T4 compared to the Apple M3
Pro baseline, while maintaining high classification accuracy on the EuroSAT
dataset. These results demonstrate the feasibility of deploying deep learning
LULC models on consumer and free cloud GPUs for scalable geospatial analytics.

</details>


### [111] [Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel](https://arxiv.org/abs/2508.18224)
*Ran Yan,Youhe Jiang,Binhang Yuan*

Main category: cs.DC

TL;DR: 本文提出Flash Sparse Attention (FSA)，改进了Native Sparse Attention (NSA)的核设计，使其在多种GQA分组大小的现代LLMs上高效运行，显著提升了训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: NSA虽然高效，但其核实现依赖于大GQA分组的策略，限制了在现代常用小GQA分组LLMs中的应用。

Method: 提出FSA，通过替代核设计实现NSA在多种GQA分组大小下的高效计算。

Result: FSA在核级延迟上平均提升1.6倍，训练端到端速度平均提升1.09倍，预填充端到端速度平均提升1.11倍。

Conclusion: FSA扩展了NSA的适用性，显著提升了现代LLMs的性能。

Abstract: Recent progress in sparse attention mechanisms has demonstrated strong
potential for reducing the computational cost of long-context training and
inference in large language models (LLMs). Native Sparse Attention (NSA), a
state-of-the-art approach, introduces natively trainable, hardware-aligned
sparse attention that delivers substantial system-level performance gains while
maintaining accuracy comparable to full attention. However, the kernel
implementation of NSA relies on a query-grouping strategy that is efficient
only with large Grouped Query Attention (GQA) sizes, whereas modern LLMs
typically adopt much smaller GQA groups, which limits the applicability of this
sparse algorithmic advance. In this work, we propose Flash Sparse Attention
(FSA), which includes an alternative kernel design that enables efficient NSA
computation across a wide range of popular LLMs with varied smaller GQA group
sizes on modern GPUs. Compared to vanilla NSA kernel implementation, our
empirical evaluation demonstrates that FSA achieves (i) up to 3.5$\times$ and
on average 1.6$\times$ kernel-level latency reduction, (ii) up to 1.25$\times$
and 1.09$\times$ on average end-to-end training speedup on state-of-the-art
LLMs, and (iii) up to 1.36$\times$ and 1.11$\times$ on average end-to-end
prefill speedup on state-of-the-art LLMs. The source code is open-sourced and
publicly available at
https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [112] [Retrieve-and-Verify: A Table Context Selection Framework for Accurate Column Annotations](https://arxiv.org/abs/2508.17203)
*Zhihao Ding,Yongkang Sun,Jieming Shi*

Main category: cs.DB

TL;DR: 提出了一种名为REVEAL和REVEAL+的框架，用于表格列注释任务的上下文选择，通过检索和验证机制优化性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据的元数据（如语义类型和列关系）常不完整或模糊，现有方法在处理宽表时性能下降。

Method: REVEAL采用无监督检索选择紧凑信息上下文，REVEAL+通过验证模型进一步优化上下文质量。

Result: 在六个基准数据集上的实验表明，该方法优于现有基线。

Conclusion: 提出的框架显著改进了表格列注释任务的准确性。

Abstract: Tables are a prevalent format for structured data, yet their metadata, such
as semantic types and column relationships, is often incomplete or ambiguous.
Column annotation tasks, including Column Type Annotation (CTA) and Column
Property Annotation (CPA), address this by leveraging table context, which are
critical for data management. Existing methods typically serialize all columns
in a table into pretrained language models to incorporate context, but this
coarse-grained approach often degrades performance in wide tables with many
irrelevant or misleading columns. To address this, we propose a novel
retrieve-and-verify context selection framework for accurate column annotation,
introducing two methods: REVEAL and REVEAL+. In REVEAL, we design an efficient
unsupervised retrieval technique to select compact, informative column contexts
by balancing semantic relevance and diversity, and develop context-aware
encoding techniques with role embeddings and target-context pair training to
effectively differentiate target and context columns. To further improve
performance, in REVEAL+, we design a verification model that refines the
selected context by directly estimating its quality for specific annotation
tasks. To achieve this, we formulate a novel column context verification
problem as a classification task and then develop the verification model.
Moreover, in REVEAL+, we develop a top-down verification inference technique to
ensure efficiency by reducing the search space for high-quality context subsets
from exponential to quadratic. Extensive experiments on six benchmark datasets
demonstrate that our methods consistently outperform state-of-the-art
baselines.

</details>


### [113] [ForeSight: A Predictive-Scheduling Deterministic Database](https://arxiv.org/abs/2508.17375)
*Junfang Huang,Yu Yan,Hongzhi Wang,Yingze Li,Jinghan Lin*

Main category: cs.DB

TL;DR: ForeSight是一种高性能确定性数据库系统，通过轻量级冲突预测和智能调度解决现有设计中的调度不足问题，显著提升了吞吐量和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有确定性数据库设计未能捕捉事务依赖关系，导致调度不足、高中止率和资源利用率低下。

Method: 设计关联和积网络预测冲突、优化存储引擎（多版本优化）、提出矩阵两遍前向扫描算法生成冲突感知调度。

Result: 在多种基准测试中，ForeSight在倾斜负载下吞吐量提升2倍，并在高竞争下保持稳定性能。

Conclusion: 预测性调度显著提升了确定性数据库的可扩展性。

Abstract: Deterministic databases enable scalable replicated systems by executing
transactions in a predetermined order. However, existing designs fail to
capture transaction dependencies, leading to insufficient scheduling, high
abort rates, and poor resource utilization. By addressing these challenges with
lightweight conflict prediction and informed scheduling, we present ForeSight,
a high-performance deterministic database system. Our system has three core
improvements: (1) We design an Association Sum-Product Network to predict
potential transaction conflicts, providing the input for dependency analysis
without pre-obtained read/write sets. (2) We enhance the storage engine to
integrate multi-version-based optimization, improving the execution process and
fallback strategy to boost commit rates and concurrency. (3) We propose a
matrix two-pass forward scan algorithm that performs dependency analysis to
generate conflict-aware schedules, significantly reducing scheduling overhead.
Experimental results on multiple benchmarks show that ForeSight achieves up to
2$\times$ higher throughput on skewed workloads and maintains strong
performance under contention, demonstrating that predictive scheduling
substantially improves deterministic database scalability.

</details>


### [114] [SEFRQO: A Self-Evolving Fine-Tuned RAG-Based Query Optimizer](https://arxiv.org/abs/2508.17556)
*Hanwen Liu,Qihan Zhang,Ryan Marcus,Ibrahim Sabek*

Main category: cs.DB

TL;DR: SEFRQO是一种基于检索增强生成（RAG）的自进化查询优化器，通过持续学习执行反馈来解决传统LQO的冷启动问题，显著降低了查询延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的学习型查询优化器（LQO）存在冷启动问题和需要重新训练的缺点，而基于LLM的优化器未充分利用上下文学习和执行反馈。SEFRQO旨在解决这些问题。

Method: SEFRQO结合了监督微调和强化微调，利用RAG框架动态构建提示，并参考历史执行记录和相似查询，实现持续优化。

Result: 实验表明，SEFRQO在CEB和Stack工作负载上分别比PostgreSQL降低了65.05%和93.57%的查询延迟。

Conclusion: SEFRQO通过自进化机制显著提升了查询优化性能，优于当前最优的LQO方法。

Abstract: Query optimization is a crucial problem in database systems that has been
studied for decades. Learned query optimizers (LQOs) can improve performance
over time by incorporating feedback; however, they suffer from cold-start
issues and often require retraining when workloads shift or schemas change.
Recent LLM-based query optimizers leverage pre-trained and fine-tuned LLMs to
mitigate these challenges. Nevertheless, they neglect LLMs' in-context learning
and execution records as feedback for continuous evolution. In this paper, we
present SEFRQO, a Self-Evolving Fine-tuned RAG-based Query Optimizer. SEFRQO
mitigates the cold-start problem of LQOs by continuously learning from
execution feedback via a Retrieval-Augmented Generation (RAG) framework. We
employ both supervised fine-tuning and reinforcement fine-tuning to prepare the
LLM to produce syntactically correct and performance-efficient query hints.
Moreover, SEFRQO leverages the LLM's in-context learning capabilities by
dynamically constructing prompts with references to similar queries and the
historical execution record of the same query. This self-evolving paradigm
iteratively optimizes the prompt to minimize query execution latency.
Evaluations show that SEFRQO outperforms state-of-the-art LQOs, achieving up to
65.05% and 93.57% reductions in query latency on the CEB and Stack workloads,
respectively, compared to PostgreSQL.

</details>


### [115] [RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System](https://arxiv.org/abs/2508.17590)
*Zui Chen,Han Li,Xinhao Zhang,Xiaoyu Chen,Chunyin Dong,Yifeng Wang,Xin Cai,Su Zhang,Ziqi Li,Chi Ding,Jinxu Li,Shuai Wang,Dousheng Zhao,Sanhai Gao,Guangyi Liu*

Main category: cs.DB

TL;DR: RubikSQL 是一种新颖的 NL2SQL 系统，旨在解决企业级 NL2SQL 中的关键挑战，如隐式意图和领域特定术语，并通过终身学习和多代理工作流程实现高效 SQL 生成。


<details>
  <summary>Details</summary>
Motivation: 解决企业级 NL2SQL 中的隐式意图和领域特定术语问题，提升 SQL 生成的准确性和适应性。

Method: 采用终身学习框架，结合知识库构建（数据库分析、结构化信息提取、规则挖掘和 CoT 增强分析）和多代理工作流程生成 SQL。

Result: 在 KaggleDBQA 和 BIRD Mini-Dev 数据集上实现了最先进的性能，并发布了 RubikBench 基准测试。

Conclusion: RubikSQL 通过系统性知识库构建和多代理协作，显著提升了企业级 NL2SQL 的性能，并为未来研究提供了新基准。

Abstract: We present RubikSQL, a novel NL2SQL system designed to address key challenges
in real-world enterprise-level NL2SQL, such as implicit intents and
domain-specific terminology. RubikSQL frames NL2SQL as a lifelong learning
task, demanding both Knowledge Base (KB) maintenance and SQL generation.
RubikSQL systematically builds and refines its KB through techniques including
database profiling, structured information extraction, agentic rule mining, and
Chain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a
multi-agent workflow to leverage this curated KB, generating accurate SQLs.
RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev
datasets. Finally, we release the RubikBench benchmark, a new benchmark
specifically designed to capture vital traits of industrial NL2SQL scenarios,
providing a valuable resource for future research.

</details>


### [116] [Database Normalization via Dual-LLM Self-Refinement](https://arxiv.org/abs/2508.17693)
*Eunjae Jo,Nakyung Lee,Gyuyeong Kim*

Main category: cs.DB

TL;DR: Miffie是一个利用大语言模型实现数据库自动规范化的框架，通过双模型自优化架构和零样本提示设计，高效且准确地完成规范化任务。


<details>
  <summary>Details</summary>
Motivation: 数据库规范化通常由人工完成，耗时且易错，因此需要自动化解决方案。

Method: Miffie采用双模型架构，结合生成模块和验证模块，通过反馈循环优化规范化过程，并使用任务特定提示提高效率和精度。

Result: 实验证明Miffie能高效处理复杂数据库模式，同时保持高准确性。

Conclusion: Miffie实现了无需人工干预的高效数据库规范化，为数据工程提供了实用工具。

Abstract: Database normalization is crucial to preserving data integrity. However, it
is time-consuming and error-prone, as it is typically performed manually by
data engineers. To this end, we present Miffie, a database normalization
framework that leverages the capability of large language models. Miffie
enables automated data normalization without human effort while preserving high
accuracy. The core of Miffie is a dual-model self-refinement architecture that
combines the best-performing models for normalized schema generation and
verification, respectively. The generation module eliminates anomalies based on
the feedback of the verification module until the output schema satisfies the
requirement for normalization. We also carefully design task-specific zero-shot
prompts to guide the models for achieving both high accuracy and cost
efficiency. Experimental results show that Miffie can normalize complex
database schemas while maintaining high accuracy.

</details>


### [117] [TRIM: Accelerating High-Dimensional Vector Similarity Search with Enhanced Triangle-Inequality-Based Pruning](https://arxiv.org/abs/2508.17828)
*Yitong Song,Pengcheng Zhang,Chao Gao,Bin Yao,Kai Wang,Zongyuan Wu,Lin Qu*

Main category: cs.DB

TL;DR: TRIM通过优化三角形不等式的剪枝策略，提升了高维向量相似性搜索的效率，适用于内存和磁盘存储方法，显著减少计算和访问开销。


<details>
  <summary>Details</summary>
Motivation: 传统高维向量相似性搜索方法因数据访问和距离计算效率低下，且三角形不等式剪枝在高维效果不佳，需要改进以提升效率。

Method: TRIM通过优化标志向量和放宽三角形不等式下界，动态调节剪枝效果，适用于多种存储方式的相似性搜索方法。

Result: 实验表明，TRIM显著提升基于图和量化的搜索效率，剪枝比例高达99%，同时减少I/O开销并保持高查询精度。

Conclusion: TRIM是通用且高效的剪枝操作，显著提升了高维向量相似性搜索的性能，适用于不同存储方案。

Abstract: High-dimensional vector similarity search (HVSS) is critical for many data
processing and AI applications. However, traditional HVSS methods often require
extensive data access for distance calculations, leading to inefficiencies.
Triangle-inequality-based lower bound pruning is a widely used technique to
reduce the number of data access in low-dimensional spaces but becomes less
effective in high-dimensional settings. This is attributed to the "distance
concentration" phenomenon, where the lower bounds derived from the triangle
inequality become too small to be useful. To address this, we propose TRIM,
which enhances the effectiveness of traditional triangle-inequality-based
pruning in high-dimensional vector similarity search using two key ways: (1)
optimizing landmark vectors used to form the triangles, and (2) relaxing the
lower bounds derived from the triangle inequality, with the relaxation degree
adjustable according to user's needs. TRIM is a versatile operation that can be
seamlessly integrated into both memory-based (e.g., HNSW, IVFPQ) and disk-based
(e.g., DiskANN) HVSS methods, reducing distance calculations and disk access.
Extensive experiments show that TRIM enhances memory-based methods, improving
graph-based search by up to 90% and quantization-based search by up to 200%,
while achieving a pruning ratio of up to 99%. It also reduces I/O costs by up
to 58% and improves efficiency by 102% for disk-based methods, while preserving
high query accuracy.

</details>


### [118] [PGTuner: An Efficient Framework for Automatic and Transferable Configuration Tuning of Proximity Graphs](https://arxiv.org/abs/2508.17886)
*Hao Duan,Yitong Song,Bin Yao,Anqi Liang*

Main category: cs.DB

TL;DR: PGTuner是一个基于预训练知识和模型迁移的高效框架，用于优化近似最近邻搜索（ANNS）中的邻近图（PG）配置，显著提升了调优效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有PG自动配置调优方法效率低且效果不佳，难以适应动态场景和新数据集的需求。

Method: PGTuner结合了预训练的查询性能预测（QPP）模型和基于深度强化学习的参数配置推荐（PCR）模型，并引入了分布外检测和深度主动学习。

Result: 实验表明，PGTuner在不同数据集上稳定实现顶级调优效果，调优效率最高提升14.69倍，动态场景下提升14.64倍。

Conclusion: PGTuner为解决PG配置调优问题提供了一种高效且可扩展的解决方案。

Abstract: Approximate Nearest Neighbor Search (ANNS) plays a crucial role in many key
areas. Proximity graphs (PGs) are the leading method for ANNS, offering the
best balance between query efficiency and accuracy. However, their performance
heavily depends on various construction and query parameters, which are
difficult to optimize due to their complex inter-dependencies. Given that users
often prioritize specific accuracy levels, efficiently identifying the optimal
PG configurations to meet these targets is essential. Although some studies
have explored automatic configuration tuning for PGs, they are limited by
inefficiencies and suboptimal results. These issues stem from the need to
construct numerous PGs for searching and re-tuning from scratch whenever the
dataset changes, as well as the failure to capture the complex dependencies
between configurations, query performance, and tuning objectives.
  To address these challenges, we propose PGTuner, an efficient framework for
automatic PG configuration tuning leveraging pre-training knowledge and model
transfer techniques. PGTuner improves efficiency through a pre-trained query
performance prediction (QPP) model, eliminating the need to build multiple PGs.
It also features a deep reinforcement learning-based parameter configuration
recommendation (PCR) model to recommend optimal configurations for specific
datasets and accuracy targets. Additionally, PGTuner incorporates
out-of-distribution detection and deep active learning for efficient tuning in
dynamic scenarios and transferring to new datasets. Extensive experiments
demonstrate that PGTuner can stably achieve the top-level tuning effect across
different datasets while significantly improving tuning efficiency by up to
14.69X, with a 14.64X boost in dynamic scenarios. The code and data for PGTuner
are available online at https://github.com/hao-duan/PGTuner.

</details>


### [119] [Join Cardinality Estimation with OmniSketches](https://arxiv.org/abs/2508.17931)
*David Justen,Matthias Boehm*

Main category: cs.DB

TL;DR: OmniSketch扩展了一种概率数据结构，用于多连接基数估计，避免了传统优化器的不准确性问题，实验表明在某些情况下显著提升了查询性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于成本的优化器在多连接查询中因基数估计不准确导致的性能问题。

Method: 扩展OmniSketch，结合count-min和K-minwise哈希，提供多连接基数估计，支持跨表的草图互操作。

Result: 在SSB-skew中显著降低中间结果和执行时间；在JOB-light中效果有限。

Conclusion: OmniSketch在多连接优化中有潜力，但对特定查询图和大基数外键列的适应性需改进。

Abstract: Join ordering is a key factor in query performance, yet traditional
cost-based optimizers often produce sub-optimal plans due to inaccurate
cardinality estimates in multi-predicate, multi-join queries. Existing
alternatives such as learning-based optimizers and adaptive query processing
improve accuracy but can suffer from high training costs, poor generalization,
or integration challenges. We present an extension of OmniSketch - a
probabilistic data structure combining count-min sketches and K-minwise hashing
- to enable multi-join cardinality estimation without assuming uniformity and
independence. Our approach introduces the OmniSketch join estimator, ensures
sketch interoperability across tables, and provides an algorithm to process
alpha-acyclic join graphs. Our experiments on SSB-skew and JOB-light show that
OmniSketch-enhanced cost-based optimization can improve estimation accuracy and
plan quality compared to DuckDB. For SSB-skew, we show intermediate result
decreases up to 1,077x and execution time decreases up to 3.19x. For JOB-light,
OmniSketch join cardinality estimation shows occasional individual improvements
but largely suffers from a loss of witnesses due to unfavorable join graph
shapes and large numbers of unique values in foreign key columns.

</details>


### [120] [Views: A Hardware-friendly Graph Database Model For Storing Semantic Information](https://arxiv.org/abs/2508.18123)
*Yanjun Yang,Adrian Wheeldon,Yihan Pan,Alex Serb*

Main category: cs.DB

TL;DR: 提出了一种硬件友好的图数据库模型Views，优化了存储和计算效率，适用于符号AI和RAG。


<details>
  <summary>Details</summary>
Motivation: 当前图数据库模型未针对硬件加速优化，导致存储和计算瓶颈，而符号AI和RAG需要高效的关系数据处理。

Method: 设计了专为高效存储和检索的Views数据结构，验证其与传统图表示的等价性，并展示了其在语义推理和认知建模中的应用。

Result: Views模型在存储和计算效率上表现优异，并能有效支持符号处理任务。

Conclusion: Views为图数据库提供了硬件友好的解决方案，未来有望进一步优化和发展。

Abstract: The graph database (GDB) is an increasingly common storage model for data
involving relationships between entries. Beyond its widespread usage in
database industries, the advantages of GDBs indicate a strong potential in
constructing symbolic artificial intelligences (AIs) and retrieval-augmented
generation (RAG), where knowledge of data inter-relationships takes a critical
role in implementation. However, current GDB models are not optimised for
hardware acceleration, leading to bottlenecks in storage capacity and
computational efficiency. In this paper, we propose a hardware-friendly GDB
model, called Views. We show its data structure and organisation tailored for
efficient storage and retrieval of graph data and demonstrate its equivalence
to represent traditional graph representations. We further demonstrate its
symbolic processing abilities in semantic reasoning and cognitive modelling
with practical examples and provide a short perspective on future developments.

</details>


### [121] [Accelerating Historical K-Core Search in Temporal Graphs](https://arxiv.org/abs/2508.18151)
*Zhuo Ma,Dong Wen,Kaiyu Chen,Yixiang Fang,Xuemin Lin,Wenjie Zhang*

Main category: cs.DB

TL;DR: 研究了一种用于时态图的k-core搜索方法（TCCS），提出了一种紧凑的边缘中心二叉森林（ECB-forest）结构，显著提升了索引构建效率和查询性能。


<details>
  <summary>Details</summary>
Motivation: TCCS问题在接触追踪、故障诊断和金融取证等任务中至关重要，现有方法存在预处理时间长和存储冗余的问题。

Method: 提出ECB-forest结构，通过边中心二叉森林捕获任意查询时间窗口的k-core，构建高效索引算法。

Result: 在真实时态图上，方法显著减少索引大小和构建成本（平均快100倍），同时保持高查询效率。

Conclusion: ECB-forest为解决TCCS问题提供了一种高效且紧凑的解决方案。

Abstract: We study the temporal k-core component search (TCCS), which outputs the
k-core containing the query vertex in the snapshot over an arbitrary query time
window in a temporal graph. The problem has been shown to be critical for tasks
such as contact tracing, fault diagnosis, and financial forensics. The
state-of-the-art EF-Index designs a separated forest structure for a set of
carefully selected windows, incurring quadratic preprocessing time and large
redundant storage. Our method introduces the ECB-forest, a compact edge-centric
binary forest that captures k-core of any arbitrary query vertex over time. In
this way, a query can be processed by searching a connected component in the
forest. We develop an efficient algorithm for index construction. Experiments
on real-world temporal graphs show that our method significantly improves the
index size and construction cost (up to 100x faster on average) while
maintaining the high query efficiency.

</details>


### [122] [Lost Data in Electron Microscopy](https://arxiv.org/abs/2508.18217)
*Nina M. Ivanova,Alexey S. Kashin,Valentine P. Ananikov*

Main category: cs.DB

TL;DR: 研究发现，电子显微镜产生的大部分数据未被利用，发表率仅略高于2%，存在大量未开发利用的数据潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估电子显微镜数据的丢失程度，并分析实验获取图像在科研发表中的利用率。

Method: 分析核心用户设施的电子显微镜拍摄图像数量及后续在同行评审期刊中的使用情况。

Result: 超过90%的电镜数据未被使用，仅约2%的图像发表，显示出数据利用率极低。

Conclusion: 研究揭示了电镜数据的大量浪费，同时指出这些未用数据可为数据科学和AI项目提供资源。

Abstract: The goal of this study is to estimate the amount of lost data in electron
microscopy and to analyze the extent to which experimentally acquired images
are utilized in peer-reviewed scientific publications. Analysis of the number
of images taken on electron microscopes at a core user facility and the number
of images subsequently included in peer-reviewed scientific journals revealed
low efficiency of data utilization. More than 90% of electron microscopy data
generated during routine instrument operation remain unused. Of the more than
150000 electron microscopy images evaluated in this study, only approximately
3500 (just over 2%) were made available in publications. Thus, the amount of
lost data in electron microscopy can be estimated as >90% (in terms of data
being recorded but not being published in peer-reviewed literature). On the one
hand, these results highlight a shortcoming in the optimal use of microscopy
images; on the other hand, they indicate the existence of a large pool of
electron microscopy data that can facilitate research in data science and the
development of AI-based projects. The considerations important to unlock the
potential of lost data are discussed in the present article.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [123] [TMA-Adaptive FP8 Grouped GEMM: Eliminating Padding Requirements in Low-Precision Training and Inference on Hopper](https://arxiv.org/abs/2508.16584)
*Zhongling Su,Rong Fu,Weihan Cao,Jianfei Gao,Minxi Jin,Zhilin Pei,Hui Wang*

Main category: cs.AR

TL;DR: 提出了一种动态适应不同组尺寸的FP8分组GEMM方法，消除了填充开销，显著提升了速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 当前FP8分组GEMM实现需要固定对齐填充，导致内存和计算开销较大。

Method: 通过动态选择预配置描述符和双阶段加载-存储操作，以及对齐管理，实现无填充操作。

Result: 实验显示速度提升1.7%-20.4%，内存减少23.8%，且保持数值等效性。

Conclusion: 该方法高效且实用，代码已开源。

Abstract: Current FP8 grouped GEMM implementations require padding each group to a
fixed alignment (e.g., 128), incurring memory and computational overhead. We
propose \textit{TMA-Adaptive FP8 Grouped GEMM}, which eliminates padding by
dynamically adapting to variable group dimensions via (1) a TMA descriptor pool
with $\log_2(block_M)$ preconfigured descriptors to handle all residual row
cases through dynamic runtime selection and dual-phase load-store operations,
achieving comprehensive coverage with minimal overhead, and (2)
TMA-alignment-aware management to satisfy 16-byte global memory alignment and
128-byte shared memory alignment. Experiments demonstrate 1.7\% to 20.4\% speed
up with up to 23.8\% memory reduction compared to padding operation plus
state-of-the-art FP8 grouped GEMM, while maintaining full numerical equivalence
for valid data. The source code is publicly available at an anonymous
repository: https://github.com/sukoncon/TMA-Adaptive-FP8-Grouped-GEMM.

</details>


### [124] [GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model](https://arxiv.org/abs/2508.16700)
*Deepak Kumar,Divakar Yadav,Yash Patel*

Main category: cs.AR

TL;DR: 该研究通过单GPU（H100，bf16）评估了GPT-OSS-20B（混合专家模型）相比密集模型Qwen3-32B和Yi-34B在多个维度的性能表现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较混合专家模型与密集模型在部署效率上的差异，重点关注推理速度、能源消耗和显存使用。

Method: 通过测量首词生成时间、解码吞吐量、端到端延迟百分位数、峰值显存占用以及能源消耗，评估GPT-OSS-20B的性能。

Result: GPT-OSS-20B在解码吞吐量和每焦耳能耗上优于密集模型，同时显存占用和每千词能源消耗显著降低。

Conclusion: 混合专家模型在部署效率上具有显著优势，尤其是每活跃参数效率更高，适合实际应用场景。

Abstract: We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B
(Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines
Qwen3-32B and Yi-34B across multiple dimensions. We measure true
time-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency
percentiles, peak VRAM with past key values (PKV) held, and energy via a
consistent nvidia-smi-based sampler. At a 2048-token context with 64-token
decode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than
dense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM
and energy per 1000 generated tokens; its TTFT is higher due to MoE routing
overhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B
provides about 31.8% higher decode throughput and 25.8% lower energy per 1000
generated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM.
Normalized by active parameters, GPT-OSS-20B shows markedly stronger
per-active-parameter efficiency (APE), underscoring MoE's deployment
advantages. We do not evaluate accuracy; this is a deployment-focused study. We
release code and consolidated results to enable replication and extension.

</details>


### [125] [zkPHIRE: A Programmable Accelerator for ZKPs over HIgh-degRee, Expressive Gates](https://arxiv.org/abs/2508.16738)
*Alhad Daftardar,Jianqiao Mo,Joey Ah-kiow,Benedikt Bünz,Siddharth Garg,Brandon Reagen*

Main category: cs.AR

TL;DR: 论文提出了一种名为zkPHIRE的新型可编程加速器，通过SumCheck协议高效处理复杂高门电路，显著提升了零知识证明的计算效率。


<details>
  <summary>Details</summary>
Motivation: 零知识证明（ZKPs）在隐私保护计算中具有巨大潜力，但因其高计算开销而难以广泛应用。论文旨在解决现代ZKP系统中复杂高门电路的计算挑战。

Method: 设计了一种新型可编程加速器，利用SumCheck协议高效处理任意自定义门电路，并将其集成到全系统加速器zkPHIRE中。

Result: zkPHIRE在相同面积下实现了1486倍于CPU的几何平均加速比，并首次支持2^30规模的问题，同时保持小证明大小和可编程性。

Conclusion: zkPHIRE通过高效处理复杂门电路，显著提升了ZKP系统的性能，为实际部署提供了可行性。

Abstract: Zero-Knowledge Proofs (ZKPs) have emerged as powerful tools for secure and
privacy-preserving computation. ZKPs enable one party to convince another of a
statement's validity without revealing anything else. This capability has
profound implications in many domains, including: machine learning, blockchain,
image authentication, and electronic voting. Despite their potential, ZKPs have
seen limited deployment because of their exceptionally high computational
overhead, which manifests primarily during proof generation. To mitigate these
overheads, a (growing) body of researchers has proposed hardware accelerators
and GPU implementations for kernels and complete protocols. Prior art spans a
wide variety of ZKP schemes that vary significantly in computational overhead,
proof size, verifier cost, protocol setup, and trust. The latest, and widely
used ZKP protocols are intentionally designed to balance these trade-offs. A
particular challenge in modern ZKP systems is supporting complex, high-degree
gates using the SumCheck protocol. We address this challenge with a novel
programmable accelerator that efficiently handles arbitrary custom gates via
SumCheck. Our accelerator achieves upwards of $1000\times$ geomean speedup over
CPU-based SumChecks across a range of gate types. We integrate this unit into a
full-system accelerator, zkPHIRE, which achieves $1486\times$ geomean speedup
over CPU and $11.87\times$ speedup over the state-of-the-art at iso-area.
zkPHIRE is the first accelerator to scale to problem sizes of $2^{30}$ nominal
constraints while maintaining small proof sizes and programmability.

</details>


### [126] [X-HEEP: An Open-Source, Configurable and Extendible RISC-V Platform for TinyAI Applications](https://arxiv.org/abs/2508.16959)
*Simone Machetti,Pasquale Davide Schiavone,Giovanni Ansaloni,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: X-HEEP是一个开源的、可配置的RISC-V平台，专注于超低功耗边缘计算（TinyAI），具有可扩展的加速器接口和高效开发流程。


<details>
  <summary>Details</summary>
Motivation: 为边缘计算提供低功耗、高灵活性的硬件平台，支持多样化加速器集成和高效开发。

Method: 采用RISC-V架构，设计eXtendible Accelerator InterFace (XAIF)，支持多种开发流程（FPGA、ASIC、SystemC-RTL）。

Result: 在65 nm CMOS技术下，X-HEEP面积仅0.15 mm²，漏电功耗29 µW，集成近内存加速器后性能提升7.3倍，能效提升3.6倍。

Conclusion: X-HEEP是边缘计算的高效平台，具有低功耗、高灵活性和显著性能优势。

Abstract: In this work, we present X-HEEP, an open-source, configurable, and extendible
RISC-V platform for ultra-low-power edge applications (TinyAI). X-HEEP features
the eXtendible Accelerator InterFace (XAIF), which enables seamless integration
of accelerators with varying requirements along with an extensive internal
configuration of cores, memory, bus, and peripherals. Moreover, it supports
various development flows, including FPGA prototyping, ASIC implementation, and
mixed SystemC-RTL modeling, enabling efficient exploration and optimization.
Implemented in TSMC's 65 nm CMOS technology (300 MHz, 0.8 V), X-HEEP achieves a
minimal footprint of only 0.15 mm2 and consumes just 29 uW of leakage power. As
a demonstrator of the configurability and low overhead of X-HEEP as a host
platform, we present a study integrating it with near-memory accelerators
targeting early-exit dynamic network applications, achieving up to 7.3 x
performance speedup and 3.6 x energy improvement on the resulting heterogeneous
system compared to CPU-only execution.

</details>


### [127] [Invited Paper: FEMU: An Open-Source and Configurable Emulation Framework for Prototyping TinyAI Heterogeneous Systems](https://arxiv.org/abs/2508.16981)
*Simone Machetti,Deniz Kasap,Juan Sapriza,Rubén Rodríguez Álvarez,Hossein Taji,José Miranda,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: 提出了一种名为FEMU的开源可配置仿真框架，用于原型设计和评估TinyAI异构系统，结合了硬件和软件环境。


<details>
  <summary>Details</summary>
Motivation: 为了快速原型设计和评估异构系统，需要一种结合硬件和软件环境的灵活仿真框架。

Method: 利用SoC FPGA的能力，将硬件区域用于快速原型设计，软件区域用于监控和通信，并构建了X-HEEP-FEMU平台进行验证。

Result: 成功部署在Xilinx Zynq-7020 SoC上，集成了X-HEEP硬件和Linux软件环境，并采用了能量模型。

Conclusion: FEMU为TinyAI异构系统的开发和评估提供了一种高效且灵活的解决方案。

Abstract: In this paper, we present the new FPGA EMUlation (FEMU), an open-source and
configurable emulation framework for prototyping and evaluating TinyAI
heterogeneous systems (HS). FEMU leverages the capability of system-on-chip
(SoC)-based FPGAs to combine the under-development HS implemented in a
reconfigurable hardware region (RH) for quick prototyping with a software
environment running under a standard operating system in a control software
region (CS) for supervision and communication. To evaluate our approach, we
built the X-HEEP FPGA EMUlation (X-HEEP-FEMU) platform by instantiating the
proposed framework with real-world hardware and software components.
X-HEEP-FEMU is deployed on the Xilinx Zynq-7020 SoC and integrates the
eXtendible Heterogeneous Energy Efficient Platform (X-HEEP) host in the RH, a
Linux-based Python environment on the ARM Cortex-A9 CS, and energy models
derived from a TSMC 65 nm CMOS silicon implementation of X-HEEP, called
HEEPocrates.

</details>


### [128] [Optimizing Neural Networks with Learnable Non-Linear Activation Functions via Lookup-Based FPGA Acceleration](https://arxiv.org/abs/2508.17069)
*Mengyuan Yin,Benjamin Chen Ming Choong,Chuping Qu,Rick Siow Mong Goh,Weng-Fai Wong,Tao Luo*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的可重构查找架构，用于高效运行学习型激活函数，显著提升了边缘AI的能效和速度。


<details>
  <summary>Details</summary>
Motivation: 学习型激活函数在准确性和可解释性上优于固定激活函数，但其高计算复杂度限制了在能源受限的边缘AI中的应用。

Method: 通过细粒度量化和自适应查找表减少计算密集型操作，利用FPGA的可重构性动态适配学习型函数。

Result: 在KANs上的测试表明，该设计比边缘CPU/GPU快10^4倍能效，同时保持准确性和低资源占用。

Conclusion: 该方案为能源敏感的边缘AI提供了可行的解决方案，克服了传统自适应激活网络的限制。

Abstract: Learned activation functions in models like Kolmogorov-Arnold Networks (KANs)
outperform fixed-activation architectures in terms of accuracy and
interpretability; however, their computational complexity poses critical
challenges for energy-constrained edge AI deployments. Conventional CPUs/GPUs
incur prohibitive latency and power costs when evaluating higher order
activations, limiting deployability under ultra-tight energy budgets. We
address this via a reconfigurable lookup architecture with edge FPGAs. By
coupling fine-grained quantization with adaptive lookup tables, our design
minimizes energy-intensive arithmetic operations while preserving activation
fidelity. FPGA reconfigurability enables dynamic hardware specialization for
learned functions, a key advantage for edge systems that require
post-deployment adaptability. Evaluations using KANs - where unique activation
functions play a critical role - demonstrate that our FPGA-based design
achieves superior computational speed and over $10^4$ times higher energy
efficiency compared to edge CPUs and GPUs, while maintaining matching accuracy
and minimal footprint overhead. This breakthrough positions our approach as a
practical enabler for energy-critical edge AI, where computational intensity
and power constraints traditionally preclude the use of adaptive activation
networks.

</details>


### [129] [A 28nm 1.80Mb/mm2 Digital/Analog Hybrid SRAM-CIM Macro Using 2D-Weighted Capacitor Array for Complex Number Mac Operations](https://arxiv.org/abs/2508.17562)
*Shota Konno,Che-Kai Liu,Sigang Ryu,Samuel Spetalnick,Arijit Raychowdhury*

Main category: cs.AR

TL;DR: 28纳米高密度6T-SRAM数字/模拟混合存内计算(CIM)宏支持复数MAC操作，采用2D加权电容阵列的混合配置，无需输入DAC，提高了精度并降低了面积开销。


<details>
  <summary>Details</summary>
Motivation: 为了在存内计算中支持复数乘法累加操作，同时提高精度和降低面积开销，开发了一种数字/模拟混合配置方法。

Method: 采用2D加权电容阵列的混合配置，数字CIM用于高位，模拟CIM用于低位，避免了输入DAC的需求。CIM单元通过单次转换输出实部和虚部以减少延迟。

Result: CIM宏实现了1.80 Mb/mm2的内存密度和0.435%的均方根误差。

Conclusion: 该方法在支持复数操作的同时，显著提高了精度和效率，适用于高性能计算应用。

Abstract: A 28nm dense 6T-SRAM Digital(D)/Analog(A) Hybrid compute-in-memory (CIM)
macro supporting complex num-ber MAC operation is presented. By introducing a
2D-weighted Capacitor Array, a hybrid configuration is adopted where digital
CIM is applied only to the upper bits and ana-log CIM is applied to the rest,
without the need for input DACs resulting in improved accuracy and lower area
overhead. The CIM prototype macro achieves 1.80 Mb/mm2 memory density and
0.435% RMS error. Complex CIM unit outputs real and imaginary part with a
single conversion to reduce latency.

</details>


### [130] [In-Memory Computing Enabled Deep MIMO Detection to Support Ultra-Low-Latency Communications](https://arxiv.org/abs/2508.17820)
*Tingyu Ding,Qunsong Zeng,Kaibin Huang*

Main category: cs.AR

TL;DR: 本文提出了一种基于内存计算的深度MIMO检测器（IM-MIMO），旨在满足6G网络的极低延迟和高可靠性需求，通过软硬件协同设计实现了纳秒级的性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络对MIMO系统的延迟和可靠性提出了前所未有的高要求，传统深度展开方法难以单独满足，需结合内存计算技术以实现更高效的矩阵向量运算。

Method: 提出IM-MIMO检测器架构，通过分解计算模块为信道依赖和独立部分以减少延迟，并开发了基于记忆电阻器统计特性的鲁棒训练方法。

Result: 研究全面评估了检测器的准确性、延迟和硬件复杂度，量化了信道噪声、编程噪声和网络规模对检测误差的影响。

Conclusion: IM-MIMO检测器通过创新架构和训练方法，为6G网络的超低延迟需求提供了可行的解决方案。

Abstract: The development of sixth-generation (6G) mobile networks imposes
unprecedented latency and reliability demands on multiple-input multiple-output
(MIMO) communication systems, a key enabler of high-speed radio access.
Recently, deep unfolding-based detectors, which map iterative algorithms onto
neural network architectures, have emerged as a promising approach, combining
the strengths of model-driven and data-driven methods to achieve high detection
accuracy with relatively low complexity. However, algorithmic innovation alone
is insufficient; software-hardware co-design is essential to meet the extreme
latency requirements of 6G (i.e., 0.1 milliseconds). This motivates us to
propose leveraging in-memory computing, which is an analog computing technology
that integrates memory and computation within memristor circuits, to perform
the intensive matrix-vector multiplication (MVM) operations inherent in deep
MIMO detection at the nanosecond scale. Specifically, we introduce a novel
architecture, called the deep in-memory MIMO (IM-MIMO) detector, characterized
by two key features. First, each of its cascaded computational blocks is
decomposed into channel-dependent and channel-independent neural network
modules. Such a design minimizes the latency of memristor reprogramming in
response to channel variations, which significantly exceeds computation time.
Second, we develop a customized detector-training method that exploits prior
knowledge of memristor-value statistics to enhance robustness against
programming noise. Furthermore, we conduct a comprehensive analysis of the
IM-MIMO detector's performance, evaluating detection accuracy, processing
latency, and hardware complexity. Our study quantifies detection error as a
function of various factors, including channel noise, memristor programming
noise, and neural network size.

</details>


### [131] [LLMulator: Generalizable Cost Modeling for Dataflow Accelerators with Input-Adaptive Control Flow](https://arxiv.org/abs/2508.17826)
*Kaiyan Chang,Wenlong Zhu,Shengwen Liang,Huawei Li,Ying Wang*

Main category: cs.AR

TL;DR: LLMulator 是一个利用预训练大语言模型的程序语义知识进行性能预测的数字建模框架，适用于数据流加速器。它通过将性能值视为分类令牌序列，实现范围无关估计和置信度感知预测，并通过强化学习和数据增强策略提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在各种架构、应用和输入依赖控制流中泛化，影响了数据流加速器的性能预测准确性和效率，因此需要一种更通用的预测框架。

Method: LLMulator 结合预训练大语言模型的语义知识，将性能值建模为分类令牌序列，并引入基于强化学习的动态校准方法和渐进式数据增强策略。

Result: 动态校准方法将周期预测误差降低 9.7%，并在多次迭代后收敛至 11.2%。数据增强策略显著提高了跨架构和配置的预测准确性。

Conclusion: LLMulator 提供了一种通用且高效的性能预测方法，适用于数据流加速器的设计和空间探索。

Abstract: Accurate and fast performance prediction for dataflow-based accelerators is
vital for efficient hardware design and design space exploration, yet existing
methods struggle to generalize across architectures, applications, and
input-dependent control flows. We present LLMulator, a progressive numeric
modeling framework leveraging the program semantic knowledge of pre-trained
large language models (LLMs) for robust, hardware- and application-aware
prediction. Our numeric model treats performance values as categorical token
sequences, enabling range-agnostic estimates and confidence-aware predictions
for unseen applications. To handle input-dependent control flows, we introduce
a reinforcement learning-based dynamic calibration method, reducing cycle
prediction error by 9.7% over static models and converging to 11.2% error after
a few iterations. For cross-hardware generalization, we develop a progressive
data augmentation strategy that generates diverse datasets covering multi-level
dataflow structures, memory parameters, and loop mapping primitives,
significantly boosting prediction accuracy across architectures and
configurations.

</details>


### [132] [Anatomy of the gem5 Simulator: AtomicSimpleCPU, TimingSimpleCPU, O3CPU, and Their Interaction with the Ruby Memory System](https://arxiv.org/abs/2508.18043)
*Johan Söderström,Yuan Yao*

Main category: cs.AR

TL;DR: 该论文分析了gem5模拟器的三种主要CPU模型（AS CPU、TS CPU、O3 CPU）及其与内存子系统的交互，通过轻量级性能分析工具识别了性能瓶颈，并提供了优化建议。


<details>
  <summary>Details</summary>
Motivation: gem5因其较长的模拟时间和陡峭的学习曲线，在计算机架构研究中广泛应用但存在性能瓶颈，需要深入分析其CPU模型以优化性能。

Method: 使用基于Linux perf_event接口的轻量级性能分析工具，对三种CPU模型进行详细的功能调用链分析和执行时间分配研究。

Result: 分析发现，Ruby内存子系统在AS和TS CPU中占用最多执行时间（主要在指令获取阶段），而O3 CPU则将大部分时间用于构建指令实例和流水线阶段。

Conclusion: 论文提供了CPU执行流的详细解剖视图，为优化gem5性能奠定了基础，并展示了分析框架的可扩展性，适用于其他组件或新模型开发。

Abstract: gem5 is a popular modular-based computer system simulator, widely used in
computer architecture research and known for its long simulation time and steep
learning curve. This report examines its three major CPU models: the
AtomicSimpleCPU (AS CPU), the TimingSimpleCPU (TS CPU), the Out-of-order (O3)
CPU, and their interactions with the memory subsystem. We provide a detailed
anatomical overview of each CPU's function call-chains and present how gem5
partitions its execution time for each simulated hardware layer.
  We perform our analysis using a lightweight profiler built on Linux's
perf_event interface, with user-configurable options to target specific
functions and examine their interactions in detail. By profiling each CPU
across a wide selection of benchmarks, we identify their software bottlenecks.
Our results show that the Ruby memory subsystem consistently accounts for the
largest share of execution time in the sequential AS and TS CPUs, primarily
during the instruction fetch stage. In contrast, the O3 CPU spends a relatively
smaller fraction of time in Ruby, with most of its time devoted to constructing
instruction instances and the various pipeline stages of the CPU.
  We believe that the anatomical view of each CPU's execution flow is valuable
for educational purposes, as it clearly illustrates the interactions among
simulated components. These insights form a foundation for optimizing gem5's
performance, particularly for the AS, TS, and O3 CPUs. Moreover, our framework
can be readily applied to analyze other gem5 components or to develop and
evaluate new models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [133] [MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems](https://arxiv.org/abs/2508.17341)
*Muhammet Anil Yagiz,Zeynep Sude Cengiz,Polat Goktas*

Main category: cs.LG

TL;DR: MetaFed 是一个去中心化的联邦学习框架，用于解决 Metaverse 中的性能、隐私和环保问题，通过多智能体强化学习、隐私保护和碳感知调度，减少碳排放25%。


<details>
  <summary>Details</summary>
Motivation: 当前 Metaverse 的集中式架构在性能、隐私和环保方面存在不足，需要一种可持续的解决方案。

Method: MetaFed 结合多智能体强化学习动态选择客户端、使用同态加密保护隐私，并根据可再生能源调整调度。

Result: 在 MNIST 和 CIFAR-10 数据集上，MetaFed 减少碳排放25%，同时保持高精度和低通信开销。

Conclusion: MetaFed 是构建环保且隐私合规的 Metaverse 基础设施的可扩展解决方案。

Abstract: The rapid expansion of immersive Metaverse applications introduces complex
challenges at the intersection of performance, privacy, and environmental
sustainability. Centralized architectures fall short in addressing these
demands, often resulting in elevated energy consumption, latency, and privacy
concerns. This paper proposes MetaFed, a decentralized federated learning (FL)
framework that enables sustainable and intelligent resource orchestration for
Metaverse environments. MetaFed integrates (i) multi-agent reinforcement
learning for dynamic client selection, (ii) privacy-preserving FL using
homomorphic encryption, and (iii) carbon-aware scheduling aligned with
renewable energy availability. Evaluations on MNIST and CIFAR-10 using
lightweight ResNet architectures demonstrate that MetaFed achieves up to 25\%
reduction in carbon emissions compared to conventional approaches, while
maintaining high accuracy and minimal communication overhead. These results
highlight MetaFed as a scalable solution for building environmentally
responsible and privacy-compliant Metaverse infrastructures.

</details>


### [134] [Topology Aware Neural Interpolation of Scalar Fields](https://arxiv.org/abs/2508.17995)
*Mohamed Kissi,Keanu Sisouk,Joshua A. Levine,Julien Tierny*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经网络的拓扑感知时间变化标量场插值方法，通过关键帧和非关键帧的拓扑关系学习，实现高效且准确的插值。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过神经网络方法解决时间变化标量场在非关键帧上的插值问题，以填补缺失数据并提高几何和拓扑重建的准确性。

Method: 采用一种神经网络架构，利用关键帧学习时间值与标量场的关系，并通过拓扑损失函数增强模型性能，实现快速插值输出。

Result: 实验结果表明，该方法在2D和3D时间变化数据集上优于参考插值方案，数据和拓扑拟合效果更佳。

Conclusion: 本文的方法通过结合神经网络和拓扑信息，有效解决了时间变化标量场的插值问题，具有高效和准确性。

Abstract: This paper presents a neural scheme for the topology-aware interpolation of
time-varying scalar fields. Given a time-varying sequence of persistence
diagrams, along with a sparse temporal sampling of the corresponding scalar
fields, denoted as keyframes, our interpolation approach aims at "inverting"
the non-keyframe diagrams to produce plausible estimations of the
corresponding, missing data. For this, we rely on a neural architecture which
learns the relation from a time value to the corresponding scalar field, based
on the keyframe examples, and reliably extends this relation to the
non-keyframe time steps. We show how augmenting this architecture with specific
topological losses exploiting the input diagrams both improves the geometrical
and topological reconstruction of the non-keyframe time steps. At query time,
given an input time value for which an interpolation is desired, our approach
instantaneously produces an output, via a single propagation of the time input
through the network. Experiments interpolating 2D and 3D time-varying datasets
show our approach superiority, both in terms of data and topological fitting,
with regard to reference interpolation schemes.

</details>


### [135] [AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration](https://arxiv.org/abs/2508.18025)
*Aditri Paul,Archan Paul*

Main category: cs.LG

TL;DR: 提出了一种自适应量化行星陨石坑检测系统（AQ-PCDSys），用于在资源受限的行星探测任务中实现实时、高精度的环境感知。


<details>
  <summary>Details</summary>
Motivation: 行星探测任务的实时环境感知依赖深度学习模型，但在资源受限的计算硬件上部署这些模型具有挑战性。

Method: 结合量化神经网络（QNN）和自适应多传感器融合（AMF）模块，优化模型大小和推理延迟，并动态融合光学图像和数字高程模型数据。

Result: AQ-PCDSys在行星陨石坑检测中实现了高效、可靠和高精度的表现。

Conclusion: 该框架为下一代自主行星着陆、导航和科学探索提供了关键技术支持。

Abstract: Autonomous planetary exploration missions are critically dependent on
real-time, accurate environmental perception for navigation and hazard
avoidance. However, deploying deep learning models on the resource-constrained
computational hardware of planetary exploration platforms remains a significant
challenge. This paper introduces the Adaptive Quantized Planetary Crater
Detection System (AQ-PCDSys), a novel framework specifically engineered for
real-time, onboard deployment in the computationally constrained environments
of space exploration missions. AQ-PCDSys synergistically integrates a Quantized
Neural Network (QNN) architecture, trained using Quantization-Aware Training
(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture
significantly optimizes model size and inference latency suitable for real-time
onboard deployment in space exploration missions, while preserving high
accuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and
Digital Elevation Models (DEMs) at the feature level, utilizing an Adaptive
Weighting Mechanism (AWM) to dynamically prioritize the most relevant and
reliable sensor modality based on planetary ambient conditions. This approach
enhances detection robustness across diverse planetary landscapes. Paired with
Multi-Scale Detection Heads specifically designed for robust and efficient
detection of craters across a wide range of sizes, AQ-PCDSys provides a
computationally efficient, reliable and accurate solution for planetary crater
detection, a critical capability for enabling the next generation of autonomous
planetary landing, navigation, and scientific exploration.

</details>


### [136] [MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models](https://arxiv.org/abs/2508.17467)
*Krishna Teja Chitty-Venkata,Sylvia Howland,Golara Azar,Daria Soboleva,Natalia Vassilieva,Siddhisanket Raskar,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: 该论文提出MoE-Inference-Bench，用于系统评估混合专家模型（MoE）在不同硬件加速技术下的性能，揭示了配置对效率的影响。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽能扩展大模型参数规模并保持计算效率，但在推理时存在负载不均衡和路由计算开销问题，需通过硬件加速技术优化。

Method: 研究通过分析批次大小、序列长度和MoE超参数（如FFN维度和专家数）对吞吐量的影响，评估了包括剪枝、融合操作、推测解码、量化和并行化策略在内的优化技术。

Result: 在不同配置下的MoE模型中观察到性能差异，为高效部署提供了依据。

Conclusion: MoE-Inference-Bench为MoE模型的优化和部署提供了全面的性能评估和实用指导。

Abstract: Mixture of Experts (MoE) models have enabled the scaling of Large Language
Models (LLMs) and Vision Language Models (VLMs) by achieving massive parameter
counts while maintaining computational efficiency. However, MoEs introduce
several inference-time challenges, including load imbalance across experts and
the additional routing computational overhead. To address these challenges and
fully harness the benefits of MoE, a systematic evaluation of hardware
acceleration techniques is essential. We present MoE-Inference-Bench, a
comprehensive study to evaluate MoE performance across diverse scenarios. We
analyze the impact of batch size, sequence length, and critical MoE
hyperparameters such as FFN dimensions and number of experts on throughput. We
evaluate several optimization techniques on Nvidia H100 GPUs, including
pruning, Fused MoE operations, speculative decoding, quantization, and various
parallelization strategies. Our evaluation includes MoEs from the Mixtral,
DeepSeek, OLMoE and Qwen families. The results reveal performance differences
across configurations and provide insights for the efficient deployment of
MoEs.

</details>


### [137] [Effective Clustering for Large Multi-Relational Graphs](https://arxiv.org/abs/2508.17388)
*Xiaoyang Lin,Runhao Jiang,Renchi Yang*

Main category: cs.LG

TL;DR: 该论文提出DEMM和DEMM+两种方法，用于高效解决多关系图（MRG）的聚类问题，解决了现有方法在质量和可扩展性上的不足。DEMM+通过优化技术进一步提升性能，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多关系图聚类时，要么因异构结构和属性融合不佳导致质量低下，要么因采用复杂深度学习模型而难以扩展到大规模图。

Method: 提出了基于两阶段优化目标（多关系Dirichlet能量和节点亲和图的Dirichlet能量优化）的DEMM和DEMM+算法，并通过高效近似求解器和问题转换技术提升性能。

Result: DEMM+在11个真实多关系图上的实验显示，其聚类质量优于20个基线方法，且速度显著更快。

Conclusion: DEMM+通过技术创新在聚类质量和可扩展性上取得了显著提升，适合处理大规模多关系图。

Abstract: Multi-relational graphs (MRGs) are an expressive data structure for modeling
diverse interactions/relations among real objects (i.e., nodes), which pervade
extensive applications and scenarios. Given an MRG G with N nodes, partitioning
the node set therein into K disjoint clusters (MRGC) is a fundamental task in
analyzing MRGs, which has garnered considerable attention. However, the
majority of existing solutions towards MRGC either yield severely compromised
result quality by ineffective fusion of heterogeneous graph structures and
attributes, or struggle to cope with sizable MRGs with millions of nodes and
billions of edges due to the adoption of sophisticated and costly deep learning
models.
  In this paper, we present DEMM and DEMM+, two effective MRGC approaches to
address the limitations above. Specifically, our algorithms are built on novel
two-stage optimization objectives, where the former seeks to derive
high-caliber node feature vectors by optimizing the multi-relational Dirichlet
energy specialized for MRGs, while the latter minimizes the Dirichlet energy of
clustering results over the node affinity graph. In particular, DEMM+ achieves
significantly higher scalability and efficiency over our based method DEMM
through a suite of well-thought-out optimizations. Key technical contributions
include (i) a highly efficient approximation solver for constructing node
feature vectors, and (ii) a theoretically-grounded problem transformation with
carefully-crafted techniques that enable linear-time clustering without
explicitly materializing the NxN dense affinity matrix. Further, we extend
DEMM+ to handle attribute-less MRGs through non-trivial adaptations. Extensive
experiments, comparing DEMM+ against 20 baselines over 11 real MRGs, exhibit
that DEMM+ is consistently superior in terms of clustering quality measured
against ground-truth labels, while often being remarkably faster.

</details>


### [138] [Characterizing the Behavior of Training Mamba-based State Space Models on GPUs](https://arxiv.org/abs/2508.17679)
*Trinayan Baruah,Kaustubh Shivdikar,Sara Prescott,David Kaeli*

Main category: cs.LG

TL;DR: Mamba-based SSMs作为transformer的新兴替代方案，通过降低计算复杂度解决了注意力机制中的性能扩展问题。本文评估了其在GPU上的训练行为，并提出了潜在优化方向。


<details>
  <summary>Details</summary>
Motivation: 传统transformer的二次计算复杂度限制了其在长序列任务中的性能扩展，Mamba-based SSMs为解决这一问题提供了新途径。

Method: 构建了一个包含多种模型架构的工作负载套件，用于分析Mamba-based SSMs在GPU上的训练行为。

Result: 揭示了SSMs在GPU上的架构影响，并提出了性能扩展的优化策略。

Conclusion: Mamba-based SSMs在GPU上的表现和优化潜力为未来模型设计提供了重要参考。

Abstract: Mamba-based State Space Models (SSM) have emerged as a promising alternative
to the ubiquitous transformers. Despite the expressive power of transformers,
the quadratic complexity of computing attention is a major impediment to
scaling performance as we increase the sequence length. SSMs provide an
alternative path that addresses this problem, reducing the computational
complexity requirements of self-attention with novel model architectures for
different domains and fields such as video, text generation and graphs. Thus,
it is important to characterize the behavior of these emerging workloads on
GPUs and understand their requirements during GPU microarchitectural design. In
this work we evaluate Mamba-based SSMs and characterize their behavior during
training on GPUs. We construct a workload suite that offers representative
models that span different model architectures. We then use this suite to
analyze the architectural implications of running Mamba-based SSMs on GPUs. Our
work sheds new light on potential optimizations to continue scaling the
performance for such models.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [139] [Making AI Inevitable: Historical Perspective and the Problems of Predicting Long-Term Technological Change](https://arxiv.org/abs/2508.16692)
*Mark Fisher,John Severini*

Main category: cs.CY

TL;DR: 本文探讨了关于AI未来的辩论本质上是主观的哲学分歧，而非客观的技术争议。研究聚焦于对人工通用智能（AGI）是否会改变社会的分歧，并区分了“变革派”和“怀疑派”两种立场。


<details>
  <summary>Details</summary>
Motivation: 动机是揭示AI未来辩论中非技术性分歧的核心，促进对争议本质的理解。

Method: 方法是通过区分并分析变革派和怀疑派的核心观点，特别是围绕非生物智能、预测时间框架和技术发展轨迹的三个关键问题。

Result: 结果展示了辩论中双方论证的多样性，同时强调了变革派的论证负担及其对先行优势的竞争压力。

Conclusion: 结论指出需要拓宽AI未来发展辩论中的“专业知识”概念，以容纳更多观点。

Abstract: This study demonstrates the extent to which prominent debates about the
future of AI are best understood as subjective, philosophical disagreements
over the history and future of technological change rather than as objective,
material disagreements over the technologies themselves. It focuses on the deep
disagreements over whether artificial general intelligence (AGI) will prove
transformative for human society; a question that is analytically prior to that
of whether this transformative effect will help or harm humanity. The study
begins by distinguishing two fundamental camps in this debate. The first of
these can be identified as "transformationalists," who argue that continued AI
development will inevitably have a profound effect on society. Opposed to them
are "skeptics," a more eclectic group united by their disbelief that AI can or
will live up to such high expectations. Each camp admits further "strong" and
"weak" variants depending on their tolerance for epistemic risk. These stylized
contrasts help to identify a set of fundamental questions that shape the camps'
respective interpretations of the future of AI. Three questions in particular
are focused on: the possibility of non-biological intelligence, the appropriate
time frame of technological predictions, and the assumed trajectory of
technological development. In highlighting these specific points of
non-technical disagreement, this study demonstrates the wide range of different
arguments used to justify either the transformationalist or skeptical position.
At the same time, it highlights the strong argumentative burden of the
transformationalist position, the way that belief in this position creates
competitive pressures to achieve first-mover advantage, and the need to widen
the concept of "expertise" in debates surrounding the future development of AI.

</details>


### [140] [Interactive AI and Human Behavior: Challenges and Pathways for AI Governance](https://arxiv.org/abs/2508.16608)
*Yulu Pi,Cagatay Turkay,Daniel Bogiatzis-Gibbons*

Main category: cs.CY

TL;DR: 论文探讨了生成式AI系统在长期、个人化和关系化交互中带来的复杂性，提出了结合行为学的新监管方法，强调动态研究需求和以人为本的治理建议。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统与人类交互的日益复杂，现有研究方法和治理框架难以应对其动态性和上下文依赖性，亟需新的跨学科方法来理解和规范。

Method: 通过跨学科研讨会，集合政策制定者、行为科学家、人机交互研究者和社会实践者的见解，识别挑战与方法机遇，提出基于行为学的监管框架。

Result: 提出了一种以结果为导向的监管方法，整合行为学见解，同时关注新兴人机关系的风险与收益，并建议动态研究方法。

Conclusion: 为应对交互式AI系统的复杂性，需开发以人为本的治理策略，结合行为学动态研究，平衡技术发展与人类福祉。

Abstract: As Generative AI systems increasingly engage in long-term, personal, and
relational interactions, human-AI engagements are becoming significantly
complex, making them more challenging to understand and govern. These
Interactive AI systems adapt to users over time, build ongoing relationships,
and even can take proactive actions on behalf of users. This new paradigm
requires us to rethink how such human-AI interactions can be studied
effectively to inform governance and policy development. In this paper, we draw
on insights from a collaborative interdisciplinary workshop with policymakers,
behavioral scientists, Human-Computer Interaction researchers, and civil
society practitioners, to identify challenges and methodological opportunities
arising within new forms of human-AI interactions. Based on these insights, we
discuss an outcome-focused regulatory approach that integrates behavioral
insights to address both the risks and benefits of emerging human-AI
relationships. In particular, we emphasize the need for new methods to study
the fluid, dynamic, and context-dependent nature of these interactions. We
provide practical recommendations for developing human-centric AI governance,
informed by behavioral insights, that can respond to the complexities of
Interactive AI systems.

</details>


### [141] [The Impact of Artificial Intelligence on Human Thought](https://arxiv.org/abs/2508.16628)
*Rénald Gesnot*

Main category: cs.CY

TL;DR: 论文从多维角度（认知、社会、伦理和哲学）探讨AI如何改变人类思维，指出认知卸载效应和算法操控的负面影响，并提出对策。


<details>
  <summary>Details</summary>
Motivation: 研究AI对人类思维的广泛影响，揭示其潜在风险。

Method: 从认知、社会、伦理和哲学角度综合分析AI的影响。

Result: AI可能导致认知卸载、思想极化，并存在伦理问题。

Conclusion: 需通过教育、透明度和治理对策应对AI对智力自主的威胁。

Abstract: This research paper examines, from a multidimensional perspective (cognitive,
social, ethical, and philosophical), how AI is transforming human thought. It
highlights a cognitive offloading effect: the externalization of mental
functions to AI can reduce intellectual engagement and weaken critical
thinking. On the social level, algorithmic personalization creates filter
bubbles that limit the diversity of opinions and can lead to the homogenization
of thought and polarization. This research also describes the mechanisms of
algorithmic manipulation (exploitation of cognitive biases, automated
disinformation, etc.) that amplify AI's power of influence. Finally, the
question of potential artificial consciousness is discussed, along with its
ethical implications. The report as a whole underscores the risks that AI poses
to human intellectual autonomy and creativity, while proposing avenues
(education, transparency, governance) to align AI development with the
interests of humanity.

</details>


### [142] [Enabling Multi-Agent Systems as Learning Designers: Applying Learning Sciences to AI Instructional Design](https://arxiv.org/abs/2508.16659)
*Jiayi Wang,Ruiwei Xiao,Xinying Hou,John Stamper*

Main category: cs.CY

TL;DR: 研究通过嵌入KLI框架和多智能体系统（MAS），提升LLM生成高质量教学材料的能力，教师更青睐协作式MAS-CMD生成的创意且实用的内容。


<details>
  <summary>Details</summary>
Motivation: K-12教师常用LLM制作教学材料，但因缺乏深度教学理论和提示工程技能，生成内容教学效果有限。研究旨在通过技术手段解决这一问题。

Method: 将KLI框架嵌入MAS系统，测试了三种生成数学与科学学习活动的系统：单智能体基线、顺序工作的角色基MAS、协作式MAS-CMD，并由教师和LLM评估。

Result: 教师评分显示协作式MAS-CMD生成的活动更具创意、情境相关性和课堂实用价值，尽管量化评分差异不显著。

Conclusion: 将教学原则嵌入LLM系统可规模化生成高质量教育内容，协作式MAS-CMD效果最佳。

Abstract: K-12 educators are increasingly using Large Language Models (LLMs) to create
instructional materials. These systems excel at producing fluent, coherent
content, but often lack support for high-quality teaching. The reason is
twofold: first, commercial LLMs, such as ChatGPT and Gemini which are among the
most widely accessible to teachers, do not come preloaded with the depth of
pedagogical theory needed to design truly effective activities; second,
although sophisticated prompt engineering can bridge this gap, most teachers
lack the time or expertise and find it difficult to encode such pedagogical
nuance into their requests. This study shifts pedagogical expertise from the
user's prompt to the LLM's internal architecture. We embed the well-established
Knowledge-Learning-Instruction (KLI) framework into a Multi-Agent System (MAS)
to act as a sophisticated instructional designer. We tested three systems for
generating secondary Math and Science learning activities: a Single-Agent
baseline simulating typical teacher prompts; a role-based MAS where agents work
sequentially; and a collaborative MAS-CMD where agents co-construct activities
through conquer and merge discussion. The generated materials were evaluated by
20 practicing teachers and a complementary LLM-as-a-judge system using the
Quality Matters (QM) K-12 standards. While the rubric scores showed only small,
often statistically insignificant differences between the systems, the
qualitative feedback from educators painted a clear and compelling picture.
Teachers strongly preferred the activities from the collaborative MAS-CMD,
describing them as significantly more creative, contextually relevant, and
classroom-ready. Our findings show that embedding pedagogical principles into
LLM systems offers a scalable path for creating high-quality educational
content.

</details>


### [143] [Situational Awareness as the Imperative Capability for Disaster Resilience in the Era of Complex Hazards and Artificial Intelligence](https://arxiv.org/abs/2508.16669)
*Hongrak Pak,Ali Mostafavi*

Main category: cs.CY

TL;DR: 这篇论文强调情境感知（SA）作为灾难应对的关键能力，提出技术-流程-人员的综合方案，以提升实时数据转化为行动的能力，并呼吁将SA作为核心目标。


<details>
  <summary>Details</summary>
Motivation: 传统灾害模型存在盲区，无法应对所有潜在风险，因此需要提升组织在危机中的情境感知能力，以实现真正适应性的灾害韧性。

Method: 提出技术-流程-人员的路线图，包括实时灾害预报、互操作工作流程和团队赋权，支持系统间的数据共享和模块化分析。

Result: 通过系统化的方法，实现多方机构数据共享和高效决策，同时保护人类决策者的认知负荷。

Conclusion: 应将SA作为灾害韧性的核心目标，并进一步研究SA指标、可信人机协作和包容性数据治理，以应对未知危机。

Abstract: Disasters frequently exceed established hazard models, revealing blind spots
where unforeseen impacts and vulnerabilities hamper effective response. This
perspective paper contends that situational awareness (SA)-the ability to
perceive, interpret, and project dynamic crisis conditions-is an often
overlooked yet vital capability for disaster resilience. While risk mitigation
measures can reduce known threats, not all hazards can be neutralized; truly
adaptive resilience hinges on whether organizations rapidly detect emerging
failures, reconcile diverse data sources, and direct interventions where they
matter most. We present a technology-process-people roadmap, demonstrating how
real-time hazard nowcasting, interoperable workflows, and empowered teams
collectively transform raw data into actionable insight. A system-of-systems
approach enables federated data ownership and modular analytics, so multiple
agencies can share timely updates without sacrificing their distinct
operational models. Equally crucial, structured sense-making routines and
cognitive load safeguards help humans remain effective decision-makers amid
data abundance. By framing SA as a socio-technical linchpin rather than a
peripheral add-on, this paper spotlights the urgency of elevating SA to a core
disaster resilience objective. We conclude with recommendations for further
research-developing SA metrics, designing trustworthy human-AI collaboration,
and strengthening inclusive data governance-to ensure that communities are
equipped to cope with both expected and unexpected crises.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [144] [On The Space Complexity of Partial Derivatives of Regular Expressions with Shuffle](https://arxiv.org/abs/2508.17451)
*Davide Ancona,Angelo Ferrando*

Main category: cs.FL

TL;DR: 论文提出了一种利用正则表达式的偏导数优化运行时验证的方法，避免了生成难以处理的大型自动机。


<details>
  <summary>Details</summary>
Motivation: 解决在运行时验证中使用正则表达式时，因子迹的相互独立事件交错导致生成的NFA状态数不可控的问题。

Method: 扩展正则表达式，引入shuffle操作符，并利用偏导数实现逐步验证，避免生成大型自动机。

Result: 研究表明，偏导数的最大高度最多增加1，大小与正则表达式的大小呈二次方关系。

Conclusion: 通过偏导数方法，可以高效处理含shuffle操作符的正则表达式，优化运行时验证的空间复杂度。

Abstract: Partial derivatives of regular expressions, introduced by Antimirov, define
an elegant algorithm for generating equivalent non-deterministic finite
automata (NFA) with a limited number of states.
  Here we focus on runtime verification (RV) of simple properties expressible
with regular expressions. In this case, words are finite traces of monitorable
events forming the language's alphabet, and the generated NFA may have an
intractable number of states.
  This typically occurs when sub-traces of mutually independent events are
allowed to interleave.
  To address this issue, regular expressions used for RV are extended with the
shuffle operator to make specifications more compact and easier to read.
  Exploiting partial derivatives enables a rewriting-based approach to RV,
where only one derivative is stored at each step, avoiding the construction of
an intractably large automaton.
  This raises the question of the space complexity of the largest generated
partial derivative. While the total number of generated partial derivatives is
known to be linear in the size of the initial regular expression, no results
can be found in the literature regarding the size of the largest partial
derivative.
  We study this problem w.r.t. two metrics (height and size of regular
expressions), and show that the former increases by at most one, while the
latter is quadratic in the size of the regular expression. Surprisingly, these
results also hold with shuffle.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [145] [A Workflow for Map Creation in Autonomous Vehicle Simulations](https://arxiv.org/abs/2508.16856)
*Zubair Islam,Ahmaad Ansari,George Daoud,Mohamed El-Darieby*

Main category: cs.RO

TL;DR: 本文提出了一种简化自动驾驶车辆（AV）开发中地图创建的定制工作流，通过生成3D停车场地图展示，未来将整合SLAM技术并优化兼容性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AV）研究需要大量仿真测试，但现有地图生成方法资源消耗大且兼容性差，限制了开发灵活性。

Method: 采用定制工作流生成3D地图，以CARLA模拟器为例，展示停车场地图创建过程。

Result: 成功生成了3D停车场地图，验证了工作流的可行性。

Conclusion: 该工作流为AV开发提供了高效地图生成方案，未来可通过整合SLAM等技术进一步提升性能和兼容性。

Abstract: The fast development of technology and artificial intelligence has
significantly advanced Autonomous Vehicle (AV) research, emphasizing the need
for extensive simulation testing. Accurate and adaptable maps are critical in
AV development, serving as the foundation for localization, path planning, and
scenario testing. However, creating simulation-ready maps is often difficult
and resource-intensive, especially with simulators like CARLA (CAR Learning to
Act). Many existing workflows require significant computational resources or
rely on specific simulators, limiting flexibility for developers. This paper
presents a custom workflow to streamline map creation for AV development,
demonstrated through the generation of a 3D map of a parking lot at Ontario
Tech University. Future work will focus on incorporating SLAM technologies,
optimizing the workflow for broader simulator compatibility, and exploring more
flexible handling of latitude and longitude values to enhance map generation
accuracy.

</details>


### [146] [Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications](https://arxiv.org/abs/2508.17753)
*Theresa Pekarek Rosin,Julia Gachot,Henri-Leon Kordt,Matthias Kerzel,Stefan Wermter*

Main category: cs.RO

TL;DR: 评估了四种最先进的ASR系统在八种公开数据集上的表现，发现其在标准基准测试中表现相似，但在实际应用中存在性能差异、幻觉倾向和内在偏见，这对人机交互（HRI）有重要影响。


<details>
  <summary>Details</summary>
Motivation: 探讨ASR系统在真实世界环境（如HRI）中处理不完美音频（如硬件限制、环境噪声）和多样化用户群体时的挑战。

Method: 在八种公开数据集上评估四种最先进的ASR系统，这些数据集覆盖了六种难度维度：领域特定、口音、噪声、年龄变化、语音障碍和自发语音。

Result: ASR系统在标准测试中表现相似，但在实际应用中存在显著性能差异、幻觉倾向和内在偏见。

Conclusion: 这些局限性对HRI（任务表现、用户信任和安全）有严重影响，需进一步改进ASR系统的鲁棒性和公平性。

Abstract: Automatic Speech Recognition (ASR) systems in real-world settings need to
handle imperfect audio, often degraded by hardware limitations or environmental
noise, while accommodating diverse user groups. In human-robot interaction
(HRI), these challenges intersect to create a uniquely challenging recognition
environment. We evaluate four state-of-the-art ASR systems on eight publicly
available datasets that capture six dimensions of difficulty: domain-specific,
accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis
demonstrates significant variations in performance, hallucination tendencies,
and inherent biases, despite similar scores on standard benchmarks. These
limitations have serious implications for HRI, where recognition errors can
interfere with task performance, user trust, and safety.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [147] [Scalable Hybrid quantum Monte Carlo simulation of U(1) gauge field coupled to fermions on GPU](https://arxiv.org/abs/2508.16298)
*Kexin Feng,Chuang Chen,Zi Yang Meng*

Main category: cond-mat.str-el

TL;DR: 开发了一种基于GPU的混合量子蒙特卡罗算法，解决了U(1)规范场与费米子耦合的问题，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在计算U(1)规范场与费米子耦合问题时的效率低下问题。

Method: 采用GPU加速的混合量子蒙特卡罗算法，结合高效预处理器、定制CUDA内核和CUDA Graph实现。

Result: 成功模拟了前所未有的大系统尺寸，验证了U(1) Dirac自旋液体态的共形性质。

Conclusion: 该方法为研究更大系统尺寸的Dirac自旋液体态及其相变提供了新的技术途径。

Abstract: We develop a GPU-accelerated hybrid quantum Monte Carlo (QMC) algorithm to
solve the fundamental yet difficult problem of $U(1)$ gauge field coupled to
fermions, which gives rise to a $U(1)$ Dirac spin liquid state under the
description of (2+1)d quantum electrodynamics QED$_3$. The algorithm renders a
good acceptance rate and, more importantly, nearly linear space-time volume
scaling in computational complexity $O(N_{\tau} V_s)$, where $N_\tau$ is the
imaginary time dimension and $V_s$ is spatial volume, which is much more
efficient than determinant QMC with scaling behavior of $O(N_\tau V_s^3)$. Such
acceleration is achieved via a collection of technical improvements, including
(i) the design of the efficient problem-specific preconditioner, (ii)
customized CUDA kernel for matrix-vector multiplication, and (iii) CUDA Graph
implementation on the GPU. These advances allow us to simulate the $U(1)$ Dirac
spin liquid state with unprecedentedly large system sizes, which is up to
$N_\tau\times L\times L = 660\times66\times66$, and reveal its novel
properties. With these technical improvements, we see the asymptotic
convergence in the scaling dimensions of various fermion bilinear operators and
the conserved current operator when approaching the thermodynamic limit. The
scaling dimensions find good agreement with field-theoretical expectation,
which provides supporting evidence for the conformal nature of the $U(1)$ Dirac
spin liquid state in the \qed. Our technical advancements open an avenue to
study the Dirac spin liquid state and its transition towards symmetry-breaking
phases at larger system sizes and with less computational burden.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [148] [A Straightforward Pipeline for Targeted Entailment and Contradiction Detection](https://arxiv.org/abs/2508.17127)
*Antonin Sulc*

Main category: cs.CL

TL;DR: 提出一种结合Transformer注意力机制和NLI模型的方法，用于识别文档中句子之间的前提或矛盾关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别句子关系时存在权衡，要么缺乏显式语义标签，要么忽略上下文显著性，因此需要一种结合两者优势的方法。

Method: 通过聚合token级注意力分数筛选上下文相关候选句子，再使用预训练NLI模型分类为前提或矛盾，最后通过注意力显著性评分过滤。

Result: 该方法能够高效地提取文本中最显著的语义关系。

Conclusion: 结合注意力机制和NLI模型的方法在识别句子关系时表现出色，适用于多种任务。

Abstract: Finding the relationships between sentences in a document is crucial for
tasks like fact-checking, argument mining, and text summarization. A key
challenge is to identify which sentences act as premises or contradictions for
a specific claim. Existing methods often face a trade-off: transformer
attention mechanisms can identify salient textual connections but lack explicit
semantic labels, while Natural Language Inference (NLI) models can classify
relationships between sentence pairs but operate independently of contextual
saliency. In this work, we introduce a method that combines the strengths of
both approaches for a targeted analysis. Our pipeline first identifies
candidate sentences that are contextually relevant to a user-selected target
sentence by aggregating token-level attention scores. It then uses a pretrained
NLI model to classify each candidate as a premise (entailment) or
contradiction. By filtering NLI-identified relationships with attention-based
saliency scores, our method efficiently isolates the most significant semantic
relationships for any given claim in a text.

</details>


### [149] [Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective](https://arxiv.org/abs/2508.16969)
*Yunxiao Zhao,Hao Xu,Zhiqiang Wang,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为KnowProb的知识引导探测方法，旨在通过后解释方式探究预训练语言模型（PLMs）是否理解隐含知识，而非仅关注文本表面内容。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒模型的信任问题日益突出，作者希望通过探测模型对隐含知识的理解来解决这一问题。

Method: 采用知识引导的后解释方法KnowProb，设计了六种潜在解释（三种基于知识的理解和三种基于关联的推理）。

Result: 实验表明，当前PLMs仅学习单一表示分布，难以捕捉文本背后的隐含知识；KnowProb能有效识别黑盒模型的局限性。

Conclusion: KnowProb方法有助于从多角度探测黑盒模型，推动可解释性研究。

Abstract: Pre-trained Language Models (PLMs) are trained on large amounts of unlabeled
data, yet they exhibit remarkable reasoning skills. However, the
trustworthiness challenges posed by these black-box models have become
increasingly evident in recent years. To alleviate this problem, this paper
proposes a novel Knowledge-guided Probing approach called KnowProb in a
post-hoc explanation way, which aims to probe whether black-box PLMs understand
implicit knowledge beyond the given text, rather than focusing only on the
surface level content of the text. We provide six potential explanations
derived from the underlying content of the given text, including three
knowledge-based understanding and three association-based reasoning. In
experiments, we validate that current small-scale (or large-scale) PLMs only
learn a single distribution of representation, and still face significant
challenges in capturing the hidden knowledge behind a given text. Furthermore,
we demonstrate that our proposed approach is effective for identifying the
limitations of existing black-box models from multiple probing perspectives,
which facilitates researchers to promote the study of detecting black-box
models in an explainable way.

</details>


### [150] [Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs](https://arxiv.org/abs/2508.17340)
*Ryoma Kondo,Riona Matsuoka,Takahiro Yoshida,Kazuyuki Yamasawa,Ryohei Hisano*

Main category: cs.CL

TL;DR: 论文提出了一种从日本行政法院判决中构建法律知识图谱的方法，以解决现有自动化方法在法律推理中存在的不足。


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法（包括大语言模型）在法律推理中常无法准确识别相关法律背景、事实与法律规范的关系，且可能误解司法推理的分层结构，这限制了实际应用中法律推理的捕捉。

Method: 通过基于提示的大语言模型提取法律推理的组成部分，规范化法律条款引用，并通过法律推断本体将事实、规范和法律应用联系起来，构建法律知识图谱。

Result: 构建的知识图谱能够完整捕捉实际判决中的法律推理结构，使其显式化且机器可读。与基线和检索增强方法相比，系统能更准确地从事实中检索相关法律条款。

Conclusion: 该方法有效解决了现有自动化方法在法律推理中的局限性，为实际应用中的法律推理提供了更准确的捕捉方式。

Abstract: Court judgments reveal how legal rules have been interpreted and applied to
facts, providing a foundation for understanding structured legal reasoning.
However, existing automated approaches for capturing legal reasoning, including
large language models, often fail to identify the relevant legal context, do
not accurately trace how facts relate to legal norms, and may misrepresent the
layered structure of judicial reasoning. These limitations hinder the ability
to capture how courts apply the law to facts in practice. In this paper, we
address these challenges by constructing a legal knowledge graph from 648
Japanese administrative court decisions. Our method extracts components of
legal reasoning using prompt-based large language models, normalizes references
to legal provisions, and links facts, norms, and legal applications through an
ontology of legal inference. The resulting graph captures the full structure of
legal reasoning as it appears in real court decisions, making implicit
reasoning explicit and machine-readable. We evaluate our system using expert
annotated data, and find that it achieves more accurate retrieval of relevant
legal provisions from facts than large language model baselines and
retrieval-augmented methods.

</details>


### [151] [The Power of Framing: How News Headlines Guide Search Behavior](https://arxiv.org/abs/2508.17131)
*Amrit Poudel,Maria Milkowski,Tim Weninger*

Main category: cs.CL

TL;DR: 研究了标题框架如何影响用户的搜索行为和后续查询，发现冲突和策略框架会干扰用户的选择一致性，而短期的框架效应会随时间减弱。


<details>
  <summary>Details</summary>
Motivation: 探讨搜索引擎中标题框架对用户信息搜索行为的影响，填补了框架效应在搜索行为中研究不足的空白。

Method: 通过控制实验，让参与者在不同的标题框架下进行搜索并选择结果，分析框架对后续查询的影响。

Result: 冲突和策略框架会破坏用户选择的一致性，而故事性框架导致更具体的查询。短期框架效应存在但随时间减弱。

Conclusion: 短暂的框架暴露能显著改变用户的信息搜索方向，但效应随时间减弱。

Abstract: Search engines play a central role in how people gather information, but
subtle cues like headline framing may influence not only what users believe but
also how they search. While framing effects on judgment are well documented,
their impact on subsequent search behavior is less understood. We conducted a
controlled experiment where participants issued queries and selected from
headlines filtered by specific linguistic frames. Headline framing
significantly shaped follow-up queries: conflict and strategy frames disrupted
alignment with prior selections, while episodic frames led to more concrete
queries than thematic ones. We also observed modest short-term frame
persistence that declined over time. These results suggest that even brief
exposure to framing can meaningfully alter the direction of users
information-seeking behavior.

</details>


### [152] [DiscussLLM: Teaching Large Language Models When to Speak](https://arxiv.org/abs/2508.18167)
*Deep Anil Patel,Iain Melvin,Christopher Malon,Martin Renqiang Min*

Main category: cs.CL

TL;DR: 该论文提出了一个框架DiscussLLM，旨在训练大型语言模型主动决定何时发言，以解决其在动态讨论中的被动性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在文本理解和生成方面表现出色，但通常是反应性的，无法主动参与动态讨论，这限制了其作为协作伙伴的潜力。

Method: 采用两阶段数据生成流程合成大规模的多轮讨论数据集，标注干预类型和触发点，并通过预测静默标记训练模型。

Result: 提出了两种架构（端到端模型和分离的分类器-生成器系统），评估了其在准确时机干预和生成有用回答方面的表现。

Conclusion: 该框架为打造更具情境意识和主动性的对话AI奠定了基础。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
understanding and generating human-like text, yet they largely operate as
reactive agents, responding only when directly prompted. This passivity creates
an "awareness gap," limiting their potential as truly collaborative partners in
dynamic human discussions. We introduce $\textit{DiscussLLM}$, a framework
designed to bridge this gap by training models to proactively decide not just
$\textit{what}$ to say, but critically, $\textit{when}$ to speak. Our primary
contribution is a scalable two-stage data generation pipeline that synthesizes
a large-scale dataset of realistic multi-turn human discussions. Each
discussion is annotated with one of five intervention types (e.g., Factual
Correction, Concept Definition) and contains an explicit conversational trigger
where an AI intervention adds value. By training models to predict a special
silent token when no intervention is needed, they learn to remain quiet until a
helpful contribution can be made. We explore two architectural baselines: an
integrated end-to-end model and a decoupled classifier-generator system
optimized for low-latency inference. We evaluate these models on their ability
to accurately time interventions and generate helpful responses, paving the way
for more situationally aware and proactive conversational AI.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [153] [Additive systems for $\mathbb{Z}$ are undecidable](https://arxiv.org/abs/2508.17285)
*Andrei Zabolotskii*

Main category: math.CO

TL;DR: 该论文研究了集合 ${A}_i\subset\mathbb{Z}$ 的构造，使得每个整数 $n$ 可以唯一表示为来自这些集合的元素之和，并将问题转化为动力系统语言，证明了其复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究集合 ${A}_i$ 的构造以便每个整数可以唯一表示为这些集合的元素之和，并将问题转化为动力系统语言以探索其复杂性。

Method: 提出称为规范集合的自然候选集合族，并将问题转化为动力系统语言，通过联系已知难题（如 Collatz 猜想和 Fractran 停机问题）来分析其复杂性。

Result: 证明某些规范集合的总和覆盖问题等价于 Collatz 猜想或 Fractran 的通用停机问题，因此是不可判定的。

Conclusion: 规范集合的总和覆盖问题具有高复杂性，与著名未解决问题相关，且某些情况下不可判定。

Abstract: What are the collections of sets ${A}_i\subset\mathbb{Z}$ such that any
$n\in\mathbb{Z}$ has exactly one representation as $n=a_0+a_1+\dotsb$ with
$a_i\in{A}_i$? The answer for $\mathbb{N}_0$ instead of $\mathbb{Z}$ is given
by a theorem of de Bruijn. We describe a family of natural candidate
collections for $\mathbb{Z}$, which we call canonical collections. Translating
the problem into the language of dynamical systems, we show that the question
of whether the sumset of a canonical collection covers the entire $\mathbb{Z}$
is difficult: specifically, there is a collection for which this question is
equivalent to the Collatz conjecture, and there is a well-behaved family of
collections for which this question is equivalent to the universal halting
problem for Fractran and is therefore undecidable.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [154] [Social Identity in Human-Agent Interaction: A Primer](https://arxiv.org/abs/2508.16609)
*Katie Seaborn*

Main category: physics.soc-ph

TL;DR: 本文探讨了社会认同理论（SIT）和社会分类理论（SCT）在人工智能社交代理中的应用，强调并非所有人类模型都适用，并呼吁专家保持警惕。


<details>
  <summary>Details</summary>
Motivation: 随着社交机器人等人工智能技术的普及，研究它们在社会认同活动中的角色变得至关重要。

Method: 通过案例研究和设想示例，将SIT和SCT应用到人工智能社交代理中。

Result: 提出了人工智能代理在社会认同活动中的潜在角色，并指出需审慎对待其应用。

Conclusion: 专家需警惕人工智能代理的社会影响，避免盲目适用人类模型。

Abstract: Social identity theory (SIT) and social categorization theory (SCT) are two
facets of the social identity approach (SIA) to understanding social phenomena.
SIT and SCT are models that describe and explain how people interact with one
another socially, connecting the individual to the group through an
understanding of underlying psychological mechanisms and intergroup behaviour.
SIT, originally developed in the 1970s, and SCT, a later, more general
offshoot, have been broadly applied to a range of social phenomena among
people. The rise of increasingly social machines embedded in daily life has
spurned efforts on understanding whether and how artificial agents can and do
participate in SIA activities. As agents like social robots and chatbots
powered by sophisticated large language models (LLMs) advance, understanding
the real and potential roles of these technologies as social entities is
crucial. Here, I provide a primer on SIA and extrapolate, through case studies
and imagined examples, how SIT and SCT can apply to artificial social agents. I
emphasize that not all human models and sub-theories will apply. I further
argue that, given the emerging competence of these machines and our tendency to
be taken in by them, we experts may need to don the hat of the uncanny killjoy,
for our own good.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [155] [Localization using Angle-of-Arrival Triangulation](https://arxiv.org/abs/2508.16908)
*Amod K. Agrawal*

Main category: eess.AS

TL;DR: 提出了一种基于音频信号的被动室内定位系统GCC+，无需硬件修改或用户配合，通过改进的GCC-PHAT方法和三角定位，实现了高精度的说话者定位。


<details>
  <summary>Details</summary>
Motivation: 随着智能设备普及，基于麦克风的室内定位成为实现智能环境的关键技术。

Method: 扩展GCC-PHAT方法估计音频信号的到达角，结合三角定位和TDoA技术提高精度。

Result: 实验显示，AoA误差中值为2.2度，定位误差中值为1.25米。

Conclusion: 证明了音频定位在隐私保护下的可行性和实用性。

Abstract: Indoor localization is a long-standing challenge in mobile computing, with
significant implications for enabling location-aware and intelligent
applications within smart environments such as homes, offices, and retail
spaces. As AI assistants such as Amazon Alexa and Google Nest become
increasingly pervasive, microphone-equipped devices are emerging as key
components of everyday life and home automation. This paper introduces a
passive, infrastructure-light system for localizing human speakers using speech
signals captured by two or more spatially distributed smart devices. The
proposed approach, GCC+, extends the Generalized Cross-Correlation with Phase
Transform (GCC-PHAT) method to estimate the Angle-of-Arrival (AoA) of audio
signals at each device and applies robust triangulation techniques to infer the
speaker's two-dimensional position. To further improve temporal resolution and
localization accuracy, feature-space expansion and subsample interpolation
techniques are employed for precise Time Difference of Arrival (TDoA)
estimation. The system operates without requiring hardware modifications, prior
calibration, explicit user cooperation, or knowledge of the speaker's signal
content, thereby offering a highly practical solution for real-world
deployment. Experimental evaluation in a real-world home environment yields a
median AoA estimation error of 2.2 degrees and a median localization error of
1.25 m, demonstrating the feasibility and effectiveness of audio-based
localization for enabling context-aware, privacy-preserving ambient
intelligence.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [156] [py360tool: Um framework para manipulação de vídeo 360$^\circ$ com ladrilhos](https://arxiv.org/abs/2508.17428)
*Henrique Domingues Garcia,Marcelo Menezes de Carvalho*

Main category: eess.IV

TL;DR: 提出了一个Python库py360tools，用于自动化360度视频流的客户端任务，以优化传输和分析用户体验。


<details>
  <summary>Details</summary>
Motivation: 360度视频流需要高带宽，通过分块传输优化，但缺乏有效的质量评估工具。

Method: 开发了py360tools库，实现视频重建、分块选择和视口提取的自动化。

Result: 工具支持360度视频流会话的复现、模拟和分析。

Conclusion: py360tools为360度视频流研究提供了实用的客户端解决方案。

Abstract: Streaming 360$^\circ$ videos for virtual reality demands a lot of bandwidth.
To optimize this transmission, videos are divided into "tiles" and selectively
distributed to the user based on what they are looking at. This interactive
approach makes it difficult to assess quality and user experience. To solve
this, the paper presents py360tools, a Python library that automates
client-side tasks like video reconstruction, tile selection, and viewport
extraction. This facilitates the reproduction, simulation, and analysis of
360$^\circ$ video streaming sessions.

</details>


### [157] [Prompt-based Multimodal Semantic Communication for Multi-spectral Image Segmentation](https://arxiv.org/abs/2508.17920)
*Haoshuo Zhang,Yufei Bo,Hongwei Zhang,Meixia Tao*

Main category: eess.IV

TL;DR: 提出了一种基于提示的多模态语义通信系统ProMSC-MIS，用于多光谱图像分割。通过预训练算法和语义融合模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态语义通信通过融合不同模态的特征提升下游任务性能，但如何有效融合特征和提取多样化语义表示是关键挑战。

Method: 提出了ProMSC-MIS系统，包括预训练算法（利用一种模态特征作为另一种模态的提示）和语义融合模块（结合跨注意力机制和SE网络）。

Result: 实验表明，ProMSC-MIS在不同信道压缩水平下显著优于基准方法，同时保持低计算复杂度和存储开销。

Conclusion: 该系统在自动驾驶和夜间监控等应用中具有广阔潜力。

Abstract: Multimodal semantic communication has gained widespread attention due to its
ability to enhance downstream task performance. A key challenge in such systems
is the effective fusion of features from different modalities, which requires
the extraction of rich and diverse semantic representations from each modality.
To this end, we propose ProMSC-MIS, a Prompt-based Multimodal Semantic
Communication system for Multi-spectral Image Segmentation. Specifically, we
propose a pre-training algorithm where features from one modality serve as
prompts for another, guiding unimodal semantic encoders to learn diverse and
complementary semantic representations. We further introduce a semantic fusion
module that combines cross-attention mechanisms and squeeze-and-excitation (SE)
networks to effectively fuse cross-modal features. Simulation results show that
ProMSC-MIS significantly outperforms benchmark methods across various
channel-source compression levels, while maintaining low computational
complexity and storage overhead. Our scheme has great potential for
applications such as autonomous driving and nighttime surveillance.

</details>


### [158] [TuningIQA: Fine-Grained Blind Image Quality Assessment for Livestreaming Camera Tuning](https://arxiv.org/abs/2508.17965)
*Xiangfei Sheng,Zhichao Duan,Xiaofeng Pan,Yipo Huang,Zhichao Yang,Pengfei Chen,Leida Li*

Main category: eess.IV

TL;DR: 论文提出了一种针对直播场景的细粒度盲图像质量评估（BIQA）方法TuningIQA，并构建了包含多样标注的FGLive-10K数据集。实验证明其在质量评分和精细排序上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的BIQA模型仅提供粗略质量评分，无法为直播相机参数调优提供细粒度指导。

Method: 构建FGLive-10K数据集，并开发TuningIQA方法，结合人类感知特征提取和基于图的参数融合。

Result: TuningIQA在分数回归和细粒度排序任务中显著优于现有方法，成功应用于直播相机调优。

Conclusion: TuningIQA为直播相机参数优化提供了有效的细粒度质量评估工具。

Abstract: Livestreaming has become increasingly prevalent in modern visual
communication, where automatic camera quality tuning is essential for
delivering superior user Quality of Experience (QoE). Such tuning requires
accurate blind image quality assessment (BIQA) to guide parameter optimization
decisions. Unfortunately, the existing BIQA models typically only predict an
overall coarse-grained quality score, which cannot provide fine-grained
perceptual guidance for precise camera parameter tuning. To bridge this gap, we
first establish FGLive-10K, a comprehensive fine-grained BIQA database
containing 10,185 high-resolution images captured under varying camera
parameter configurations across diverse livestreaming scenarios. The dataset
features 50,925 multi-attribute quality annotations and 19,234 fine-grained
pairwise preference annotations. Based on FGLive-10K, we further develop
TuningIQA, a fine-grained BIQA metric for livestreaming camera tuning, which
integrates human-aware feature extraction and graph-based camera parameter
fusion. Extensive experiments and comparisons demonstrate that TuningIQA
significantly outperforms state-of-the-art BIQA methods in both score
regression and fine-grained quality ranking, achieving superior performance
when deployed for livestreaming camera tuning.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [159] [EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models](https://arxiv.org/abs/2508.17742)
*Wei Xiong,Jiangtong Li,Jie Li,Kun Zhu*

Main category: eess.SP

TL;DR: EEG-FM-Bench是一个新的基准测试，用于标准化评估EEG基础模型，促进科学进步和公平比较。


<details>
  <summary>Details</summary>
Motivation: EEG基础模型发展迅速，但缺乏标准化评估方法，阻碍了科学进展和模型比较。

Method: 开发EEG-FM-Bench，整合多样化任务和数据集，评估现有模型，并分析其表示特征。

Result: 发现细粒度时空特征交互、多任务统一训练和神经心理学先验能提升性能。

Conclusion: EEG-FM-Bench为EEG基础模型的公平比较和可重复研究提供了平台，推动领域发展。

Abstract: Electroencephalography (EEG) foundation models are poised to significantly
advance brain signal analysis by learning robust representations from
large-scale, unlabeled datasets. However, their rapid proliferation has
outpaced the development of standardized evaluation benchmarks, which
complicates direct model comparisons and hinders systematic scientific
progress. This fragmentation fosters scientific inefficiency and obscures
genuine architectural advancements. To address this critical gap, we introduce
EEG-FM-Bench, the first comprehensive benchmark for the systematic and
standardized evaluation of EEG foundation models (EEG-FMs). Our contributions
are threefold: (1) we curate a diverse suite of downstream tasks and datasets
from canonical EEG paradigms, implementing standardized processing and
evaluation protocols within a unified open-source framework; (2) we benchmark
prominent state-of-the-art foundation models to establish comprehensive
baseline results for a clear comparison of the current landscape; (3) we
perform qualitative analyses of the learned representations to provide insights
into model behavior and inform future architectural design. Through extensive
experiments, we find that fine-grained spatio-temporal feature interaction,
multitask unified training and neuropsychological priors would contribute to
enhancing model performance and generalization capabilities. By offering a
unified platform for fair comparison and reproducible research, EEG-FM-Bench
seeks to catalyze progress and guide the community toward the development of
more robust and generalizable EEG-FMs. Code is released at
https://github.com/xw1216/EEG-FM-Bench.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [160] [MC3G: Model Agnostic Causally Constrained Counterfactual Generation](https://arxiv.org/abs/2508.17221)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: 该论文提出了一种新型框架MC3G，用于生成反事实解释，以平衡透明性与保护算法隐私的需求。MC3G能适配任何黑盒模型，并通过可解释的代理模型生成更有针对性的反事实建议。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景（如金融、法律和招聘）中，机器学习模型的决定需要透明化和解释性，但同时需避免泄露算法细节，因此需要一种平衡透明性与隐私的方法。

Method: MC3G框架通过可解释的代理模型近似黑盒模型，生成针对原始模型的反事实解释，并优化成本计算，忽略因果依赖导致的自动特征变化。

Result: MC3G比现有技术提供更具可操作性和解释性的反事实建议，同时成本更低。

Conclusion: MC3G框架能有效提升决策过程的透明性、可问责性和实用性，适用于依赖机器学习的场景。

Abstract: Machine learning models increasingly influence decisions in high-stakes
settings such as finance, law and hiring, driving the need for transparent,
interpretable outcomes. However, while explainable approaches can help
understand the decisions being made, they may inadvertently reveal the
underlying proprietary algorithm: an undesirable outcome for many
practitioners. Consequently, it is crucial to balance meaningful transparency
with a form of recourse that clarifies why a decision was made and offers
actionable steps following which a favorable outcome can be obtained.
Counterfactual explanations offer a powerful mechanism to address this need by
showing how specific input changes lead to a more favorable prediction. We
propose Model-Agnostic Causally Constrained Counterfactual Generation (MC3G), a
novel framework that tackles limitations in the existing counterfactual
methods. First, MC3G is model-agnostic: it approximates any black-box model
using an explainable rule-based surrogate model. Second, this surrogate is used
to generate counterfactuals that produce a favourable outcome for the original
underlying black box model. Third, MC3G refines cost computation by excluding
the ``effort" associated with feature changes that occur automatically due to
causal dependencies. By focusing only on user-initiated changes, MC3G provides
a more realistic and fair representation of the effort needed to achieve a
favourable outcome. We show that MC3G delivers more interpretable and
actionable counterfactual recommendations compared to existing techniques all
while having a lower cost. Our findings highlight MC3G's potential to enhance
transparency, accountability, and practical utility in decision-making
processes that incorporate machine-learning approaches.

</details>


### [161] [Interpretable Early Failure Detection via Machine Learning and Trace Checking-based Monitoring](https://arxiv.org/abs/2508.17786)
*Andrea Brunello,Luca Geatti,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文提出了一种基于GPU加速的框架，通过将纯过去（co）安全片段STL的监控问题简化为轨迹检查，实现了多项式时间复杂度的早期故障检测。


<details>
  <summary>Details</summary>
Motivation: 传统监控技术需要构建双重指数级复杂度的自动机，限制了实用性。本文旨在通过简化为轨迹检查问题，提高监控效率。

Method: 将监控问题简化为轨迹检查，利用GPU加速和遗传编程从历史数据中学习时间属性。

Result: 相比现有方法，框架在关键性能指标上实现了2-10%的净提升。

Conclusion: 该方法显著提高了监控效率，为早期故障检测提供了实用的解决方案。

Abstract: Monitoring is a runtime verification technique that allows one to check
whether an ongoing computation of a system (partial trace) satisfies a given
formula. It does not need a complete model of the system, but it typically
requires the construction of a deterministic automaton doubly exponential in
the size of the formula (in the worst case), which limits its practicality. In
this paper, we show that, when considering finite, discrete traces, monitoring
of pure past (co)safety fragments of Signal Temporal Logic (STL) can be reduced
to trace checking, that is, evaluation of a formula over a trace, that can be
performed in time polynomial in the size of the formula and the length of the
trace. By exploiting such a result, we develop a GPU-accelerated framework for
interpretable early failure detection based on vectorized trace checking, that
employs genetic programming to learn temporal properties from historical trace
data. The framework shows a 2-10% net improvement in key performance metrics
compared to the state-of-the-art methods.

</details>


### [162] [ST-Raptor: LLM-Powered Semi-Structured Table Question Answering](https://arxiv.org/abs/2508.18190)
*Zirui Tang,Boyu Niu,Xuanhe Zhou,Boxiu Li,Wei Zhou,Jiannan Wang,Guoliang Li,Xinyi Zhang,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor是一个基于树的框架，用于半结构化表格的自然语言问答，通过HO-Tree和两阶段验证机制显著提高回答准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理半结构化表格时存在信息丢失和布局理解困难的问题，无法高效回答自然语言问题。

Method: 提出HO-Tree结构模型，定义基本树操作，并引入两阶段验证机制（前向和后向验证）。

Result: 在SSTQA数据集上，ST-Raptor比基线方法准确率提高20%。

Conclusion: ST-Raptor有效解决了半结构化表格问答的挑战，具有实际应用潜力。

Abstract: Semi-structured tables, widely used in real-world applications (e.g.,
financial reports, medical records, transactional orders), often involve
flexible and complex layouts (e.g., hierarchical headers and merged cells).
These tables generally rely on human analysts to interpret table layouts and
answer relevant natural language questions, which is costly and inefficient. To
automate the procedure, existing methods face significant challenges. First,
methods like NL2SQL require converting semi-structured tables into structured
ones, which often causes substantial information loss. Second, methods like
NL2Code and multi-modal LLM QA struggle to understand the complex layouts of
semi-structured tables and cannot accurately answer corresponding questions. To
this end, we propose ST-Raptor, a tree-based framework for semi-structured
table question answering using large language models. First, we introduce the
Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures
complex semi-structured table layouts, along with an effective algorithm for
constructing the tree. Second, we define a set of basic tree operations to
guide LLMs in executing common QA tasks. Given a user question, ST-Raptor
decomposes it into simpler sub-questions, generates corresponding tree
operation pipelines, and conducts operation-table alignment for accurate
pipeline execution. Third, we incorporate a two-stage verification mechanism:
forward validation checks the correctness of execution steps, while backward
validation evaluates answer reliability by reconstructing queries from
predicted answers. To benchmark the performance, we present SSTQA, a dataset of
764 questions over 102 real-world semi-structured tables. Experiments show that
ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code
is available at https://github.com/weAIDB/ST-Raptor.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [163] [MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation](https://arxiv.org/abs/2508.17568)
*Liane Makatura,Benjamin Jones,Siyuan Bian,Wojciech Matusik*

Main category: cs.CV

TL;DR: 该论文提出了一个框架，包括MetaDSL语言、MetaDB数据库和MetaBench测试套件，用于解决超材料设计的复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 超材料设计因几何复杂性和结构与性能的非线性映射而困难，需一种统一的方法来简化和优化设计过程。

Method: 开发了MetaDSL语言描述设计，构建MetaDB数据库存储设计及衍生数据，并通过MetaBench测试套件评估模型能力。

Result: 通过微调先进视觉语言模型和部署交互式界面，初步实现了结构与性能关系的集成设计。

Conclusion: 该框架为超材料的设计和理解提供了有效工具，为未来研究奠定了基础。

Abstract: Metamaterials are micro-architected structures whose geometry imparts highly
tunable-often counter-intuitive-bulk properties. Yet their design is difficult
because of geometric complexity and a non-trivial mapping from architecture to
behaviour. We address these challenges with three complementary contributions.
(i) MetaDSL: a compact, semantically rich domain-specific language that
captures diverse metamaterial designs in a form that is both human-readable and
machine-parsable. (ii) MetaDB: a curated repository of more than 150,000
parameterized MetaDSL programs together with their
derivatives-three-dimensional geometry, multi-view renderings, and simulated
elastic properties. (iii) MetaBench: benchmark suites that test three core
capabilities of vision-language metamaterial assistants-structure
reconstruction, property-driven inverse design, and performance prediction. We
establish baselines by fine-tuning state-of-the-art vision-language models and
deploy an omni-model within an interactive, CAD-like interface. Case studies
show that our framework provides a strong first step toward integrated design
and understanding of structure-representation-property relationships.

</details>


### [164] [Probabilistic Temporal Masked Attention for Cross-view Online Action Detection](https://arxiv.org/abs/2508.17025)
*Liping Xie,Yang Tan,Shicheng Jing,Huimin Lu,Kanjian Zhang*

Main category: cs.CV

TL;DR: 提出了一种新颖的概率时间掩码注意力（PTMA）模型，通过概率建模提取跨视角视频帧的潜在压缩表示，提升在线动作检测的性能。


<details>
  <summary>Details</summary>
Motivation: 主流在线动作检测模型对视角变化的敏感性限制了其泛化能力，尤其在面对未见数据源时表现不佳。

Method: 采用基于GRU的时间掩码注意力（TMA）单元，通过概率建模生成跨视角的视频帧压缩表示，并结合多视角信息提取视角不变特征。

Result: 在DAHLIA、IKEA ASM和Breakfast数据集上，PTMA模型在跨主体、跨视角和跨主体-视角三种评估协议下均达到最先进性能。

Conclusion: PTMA模型通过概率建模和跨视角特征提取，显著提升了在线动作检测的性能和泛化能力。

Abstract: As a critical task in video sequence classification within computer vision,
Online Action Detection (OAD) has garnered significant attention. The
sensitivity of mainstream OAD models to varying video viewpoints often hampers
their generalization when confronted with unseen sources. To address this
limitation, we propose a novel Probabilistic Temporal Masked Attention (PTMA)
model, which leverages probabilistic modeling to derive latent compressed
representations of video frames in a cross-view setting. The PTMA model
incorporates a GRU-based temporal masked attention (TMA) cell, which leverages
these representations to effectively query the input video sequence, thereby
enhancing information interaction and facilitating autoregressive frame-level
video analysis. Additionally, multi-view information can be integrated into the
probabilistic modeling to facilitate the extraction of view-invariant features.
Experiments conducted under three evaluation protocols: cross-subject (cs),
cross-view (cv), and cross-subject-view (csv) show that PTMA achieves
state-of-the-art performance on the DAHLIA, IKEA ASM, and Breakfast datasets.

</details>


### [165] [Spatial-Temporal Human-Object Interaction Detection](https://arxiv.org/abs/2508.17270)
*Xu Sun,Yunqing He,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出了一个新的视频中实例级人-物交互检测任务ST-HOID，旨在区分细粒度的人-物交互及其轨迹。为了解决这一问题，提出了一种新方法并构建了首个数据集VidOR-HOID。实验表明，该方法优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 人-物交互对于以人为中心的视频内容理解至关重要。

Method: 提出了包含目标轨迹检测模块和交互推理模块的新方法。

Result: 实验结果表明，该方法在性能上优于现有基线方法。

Conclusion: ST-HOID任务和相关方法是有效的，VidOR-HOID数据集为任务评估提供了支持。

Abstract: In this paper, we propose a new instance-level human-object interaction
detection task on videos called ST-HOID, which aims to distinguish fine-grained
human-object interactions (HOIs) and the trajectories of subjects and objects.
It is motivated by the fact that HOI is crucial for human-centric video content
understanding. To solve ST-HOID, we propose a novel method consisting of an
object trajectory detection module and an interaction reasoning module.
Furthermore, we construct the first dataset named VidOR-HOID for ST-HOID
evaluation, which contains 10,831 spatial-temporal HOI instances. We conduct
extensive experiments to evaluate the effectiveness of our method. The
experimental results demonstrate that our method outperforms the baselines
generated by the state-of-the-art methods of image human-object interaction
detection, video visual relation detection and video human-object interaction
recognition.

</details>


### [166] [MTNet: Learning modality-aware representation with transformer for RGBT tracking](https://arxiv.org/abs/2508.17280)
*Ruichao Hou,Boyue Xu,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的多模态感知跟踪器MTNet，通过通道聚合与分布模块和空间相似性感知模块捕捉模态特异性线索，结合Transformer融合网络强化实例表征，并采用三叉预测头和动态更新策略提升跟踪效果。


<details>
  <summary>Details</summary>
Motivation: 现有的RGB-T跟踪方法在特征融合和模板更新方面存在限制，影响性能。

Method: 设计了模态感知网络（含通道聚合与分布模块和空间相似性感知模块）和Transformer融合网络，结合三叉预测头和动态更新策略。

Result: 在三个RGB-T基准测试中表现优异，达到实时速度。

Conclusion: MTNet通过全局依赖性和动态模板更新解决了现有问题，性能显著优于现有方法。

Abstract: The ability to learn robust multi-modality representation has played a
critical role in the development of RGBT tracking. However, the regular fusion
paradigm and the invariable tracking template remain restrictive to the feature
interaction. In this paper, we propose a modality-aware tracker based on
transformer, termed MTNet. Specifically, a modality-aware network is presented
to explore modality-specific cues, which contains both channel aggregation and
distribution module(CADM) and spatial similarity perception module (SSPM). A
transformer fusion network is then applied to capture global dependencies to
reinforce instance representations. To estimate the precise location and tackle
the challenges, such as scale variation and deformation, we design a trident
prediction head and a dynamic update strategy which jointly maintain a reliable
template for facilitating inter-frame communication. Extensive experiments
validate that the proposed method achieves satisfactory results compared with
the state-of-the-art competitors on three RGBT benchmarks while reaching
real-time speed.

</details>


### [167] [Explain and Monitor Deep Learning Models for Computer Vision using Obz AI](https://arxiv.org/abs/2508.18188)
*Neo Christopher Chung,Jakub Binda*

Main category: cs.CV

TL;DR: 论文提出了Obz AI，一个集成解释性AI（XAI）与知识管理和监控框架的软件生态系统，旨在提升计算机视觉系统的透明度和可观察性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型如CNN和ViT被视为“黑盒”，缺乏对其决策过程的透明性，同时XAI在实际计算机视觉应用中未被充分利用。

Method: 开发了Obz AI，提供从Python客户端库到全栈分析仪表板的无缝集成管道，支持XAI方法、特征提取分析和实时模型监控。

Result: Obz AI使深度学习模型的决策机制可解释，从而提升了计算机视觉系统的可观察性和负责任部署。

Conclusion: Obz AI填补了XAI技术与实际应用之间的软件集成空白，推动了计算机视觉系统的透明化和可监控性。

Abstract: Deep learning has transformed computer vision (CV), achieving outstanding
performance in classification, segmentation, and related tasks. Such AI-based
CV systems are becoming prevalent, with applications spanning from medical
imaging to surveillance. State of the art models such as convolutional neural
networks (CNNs) and vision transformers (ViTs) are often regarded as ``black
boxes,'' offering limited transparency into their decision-making processes.
Despite a recent advancement in explainable AI (XAI), explainability remains
underutilized in practical CV deployments. A primary obstacle is the absence of
integrated software solutions that connect XAI techniques with robust knowledge
management and monitoring frameworks. To close this gap, we have developed Obz
AI, a comprehensive software ecosystem designed to facilitate state-of-the-art
explainability and observability for vision AI systems. Obz AI provides a
seamless integration pipeline, from a Python client library to a full-stack
analytics dashboard. With Obz AI, a machine learning engineer can easily
incorporate advanced XAI methodologies, extract and analyze features for
outlier detection, and continuously monitor AI models in real time. By making
the decision-making mechanisms of deep models interpretable, Obz AI promotes
observability and responsible deployment of computer vision systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [168] [SyncGuard: Robust Audio Watermarking Capable of Countering Desynchronization Attacks](https://arxiv.org/abs/2508.17121)
*Zhenliang Gan,Xiaoxiao Hu,Sheng Li,Zhenxing Qian,Xinpeng Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种名为SyncGuard的学习型音频水印方案，解决了水印定位和抗去同步攻击的难题，通过帧级广播嵌入策略和多分辨率时频特征提取，显著提升了鲁棒性和听觉质量。


<details>
  <summary>Details</summary>
Motivation: 音频水印在版权保护和来源追踪中应用广泛，但由于音频信号的固有特性，水印定位和抗去同步攻击仍是主要挑战。

Method: 采用帧级广播嵌入策略在任意长度音频中嵌入水印，引入精心设计的失真层增强鲁棒性，并使用扩张残差块和扩张门控块捕获多分辨率时频特征。

Result: 实验表明，SyncGuard能高效处理变长音频片段，在对抗多种攻击的鲁棒性上优于现有方法，并提供优异的听觉质量。

Conclusion: SyncGuard通过创新的嵌入和特征提取策略，显著提升了音频水印的性能和实用性。

Abstract: Audio watermarking has been widely applied in copyright protection and source
tracing. However, due to the inherent characteristics of audio signals,
watermark localization and resistance to desynchronization attacks remain
significant challenges. In this paper, we propose a learning-based scheme named
SyncGuard to address these challenges. Specifically, we design a frame-wise
broadcast embedding strategy to embed the watermark in arbitrary-length audio,
enhancing time-independence and eliminating the need for localization during
watermark extraction. To further enhance robustness, we introduce a
meticulously designed distortion layer. Additionally, we employ dilated
residual blocks in conjunction with dilated gated blocks to effectively capture
multi-resolution time-frequency features. Extensive experimental results show
that SyncGuard efficiently handles variable-length audio segments, outperforms
state-of-the-art methods in robustness against various attacks, and delivers
superior auditory quality.

</details>


### [169] [PRZK-Bind: A Physically Rooted Zero-Knowledge Authentication Protocol for Secure Digital Twin Binding in Smart Cities](https://arxiv.org/abs/2508.17913)
*Yagmur Yigit,Mehmet Ali Erturk,Kerem Gursu,Berk Canberk*

Main category: cs.CR

TL;DR: PRZK-Bind 是一种轻量级去中心化认证协议，通过结合 Schnorr 零知识证明和椭圆曲线密码学，解决了数字孪生在动态对抗环境中安全绑定的挑战，性能显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术在智慧城市中日益重要，但在动态对抗环境中实现其与物理实体的安全实时绑定仍是一个未解决的挑战。现有认证方案因依赖静态信任模型、中心化机构或无法提供实时验证，难以满足需求。

Method: 作者提出了 PRZK-Bind 协议，结合 Schnorr 零知识证明和椭圆曲线密码学，无需预共享密钥即可实现物理实体与数字孪生的安全实时绑定。

Result: 仿真结果显示，PRZK-Bind 的延迟降低 4.5 倍，能耗减少 4 倍，同时错误接受率比现有方案低 10 倍以上。

Conclusion: PRZK-Bind 适用于未来智慧城市中高效、弹性和可信的数字孪生认证。

Abstract: Digital twin (DT) technology is rapidly becoming essential for smart city
ecosystems, enabling real-time synchronisation and autonomous decision-making
across physical and digital domains. However, as DTs take active roles in
control loops, securely binding them to their physical counterparts in dynamic
and adversarial environments remains a significant challenge. Existing
authentication solutions either rely on static trust models, require
centralised authorities, or fail to provide live and verifiable
physical-digital binding, making them unsuitable for latency-sensitive and
distributed deployments. To address this gap, we introduce PRZK-Bind, a
lightweight and decentralised authentication protocol that combines
Schnorr-based zero-knowledge proofs with elliptic curve cryptography to
establish secure, real-time correspondence between physical entities and DTs
without relying on pre-shared secrets. Simulation results show that PRZK-Bind
significantly improves performance, offering up to 4.5 times lower latency and
4 times reduced energy consumption compared to cryptography-heavy baselines,
while maintaining false acceptance rates more than 10 times lower. These
findings highlight its suitability for future smart city deployments requiring
efficient, resilient, and trustworthy DT authentication.

</details>


### [170] [Targeted Wearout Attacks in Microprocessor Cores](https://arxiv.org/abs/2508.16868)
*Joshua Mashburn,Johann Knechtel,Florian Klemme,Hussam Amrouch,Ozgur Sinanoglu,Paul V. Gratz*

Main category: cs.CR

TL;DR: 本文研究了通过软件驱动的负偏压温度不稳定性（NBTI）攻击，称为“目标磨损攻击”，通过精心设计的程序加速处理器功能单元老化，导致受害者应用数据损坏。


<details>
  <summary>Details</summary>
Motivation: 探索NBTI作为纳米级CMOS电路老化机制的潜在安全威胁，特别是通过用户输入控制晶体管老化的攻击可能性。

Method: 提出目标磨损攻击方法，通过用户特权程序加速特定逻辑路径老化，并在RISC-V CPU的乘法加法管道中进行案例研究。

Result: 实验显示，攻击可使目标路径的老化速度提高7倍以上，导致受害者应用数据损坏。

Conclusion: 目标磨损攻击是一种新型安全威胁，需在硬件设计和安全策略中予以防范。

Abstract: Negative-Bias Temperature Instability is a dominant aging mechanism in
nanoscale CMOS circuits such as microprocessors. With this aging mechanism, the
rate of device aging is dependent not only on overall operating conditions,
such as heat, but also on user controllable inputs to the transistors. This
dependence on input implies a possible timing fault-injection attack wherein a
targeted path of logic is intentionally degraded through the purposeful,
software-driven actions of an attacker, rendering a targeted bit effectively
stuck.
  In this work, we describe such an attack mechanism, which we dub a
"$\textbf{Targeted Wearout Attack}$", wherein an attacker with sufficient
knowledge of the processor core, executing a carefully crafted software program
with only user privilege, is able to degrade a functional unit within the
processor with the aim of eliciting a particular desired incorrect calculation
in a victim application. Here we give a general methodology for the attack. We
then demonstrate a case study where a targeted path within the fused
multiply-add pipeline in a RISC-V CPU sees a $>7x$ increase in wear over time
than would be experienced under typical workloads. We show that an attacker
could leverage such an attack, leading to targeted and silent data corruption
in a co-running victim application using the same unit.

</details>


### [171] [$AutoGuardX$: A Comprehensive Cybersecurity Framework for Connected Vehicles](https://arxiv.org/abs/2508.18155)
*Muhammad Ali Nadeem,Bishwo Prakash Pokharel,Naresh Kshetri,Achyut Shankar,Gokarna Sharma*

Main category: cs.CR

TL;DR: $AutoGuardX$ 是一个针对联网车辆的全面网络安全框架，结合了现有标准与先进技术，能够有效应对当前和未来威胁。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和互联系统的快速发展，联网车辆面临的网络威胁日益增多，尤其是在美国和加拿大，网络窃车事件频发，现有安全措施存在明显不足。

Method: $AutoGuardX$ 结合了 ISO/SAE 21434 和 ISO 26262 等现有标准，以及机器学习异常检测、物联网安全协议和加密通信等先进技术，针对主要攻击向量（如中继攻击、CAN 总线入侵等）提供防护。

Result: 通过对 2019 至 2023 年间四家主要汽车品牌的 Sedan 和 SUV 进行安全模拟测试，$AutoGuardX$ 表现出良好的适应性、可扩展性和实际效果。

Conclusion: 该框架为联网车辆的安全防护提供了实用解决方案，能够应对当前和未来的网络安全挑战。

Abstract: The rapid integration of Internet of Things (IoT) and interconnected systems
in modern vehicles not only introduced a new era of convenience, automation,
and connected vehicles but also elevated their exposure to sophisticated cyber
threats. This is especially evident in US and Canada, where cyber-enabled auto
theft has surged in recent years, revealing the limitations of existing
security measures for connected vehicles. In response, this paper proposes
$AutoGuardX$, a comprehensive cybersecurity framework designed specifically for
connected vehicles. $AutoGuardX$ combines key elements from existing recognized
standards for vehicle security, such as ISO/SAE 21434 and ISO 26262, with
advanced technologies, including machine learning-based anomaly detection, IoT
security protocols, and encrypted communication channels. The framework
addresses major attack vectors like relay attacks, controller area network
(CAN) bus intrusions, and vulnerabilities introduced by emerging technologies
such as 5G and quantum computing. $AutoGuardX$ is extensively evaluated through
security simulations across a mix of Sedans and SUVs from four major vehicle
brands manufactured between 2019 and 2023. The results demonstrate the
framework's adaptability, scalability, and practical effectiveness against
existing and emerging threats.

</details>


### [172] [TLGLock: A New Approach in Logic Locking Using Key-Driven Charge Recycling in Threshold Logic Gates](https://arxiv.org/abs/2508.17809)
*Abdullah Sahruri,Martin Margala*

Main category: cs.CR

TL;DR: TLGLock是一种新型的硬件锁设计范式，利用阈值逻辑门和电荷回收技术实现高效、安全的密钥依赖功能，相比传统方法节省面积、延迟和功耗，且具有更高的SAT攻击抗性。


<details>
  <summary>Details</summary>
Motivation: 为了解决硬件锁技术的可扩展性和设计开销问题，提出一种更高效、更安全的替代方案。

Method: 通过阈值逻辑门的结构表达性和电荷回收的能量效率，嵌入密钥到门的加权逻辑中，并利用动态电荷共享实现无状态且紧凑的锁技术。

Result: 实验证明，TLGLock在面积、延迟和功耗方面优于传统锁技术，SAT攻击抗性提升3倍，且输出错误率可调。

Conclusion: TLGLock为安全硬件设计提供了一种可扩展且高效的解决方案。

Abstract: Logic locking remains one of the most promising defenses against hardware
piracy, yet current approaches often face challenges in scalability and design
overhead. In this paper, we present TLGLock, a new design paradigm that
leverages the structural expressiveness of Threshold Logic Gates (TLGs) and the
energy efficiency of charge recycling to enforce key-dependent functionality at
the gate level. By embedding the key into the gate's weighted logic and
utilizing dynamic charge sharing, TLGLock provides a stateless and compact
alternative to conventional locking techniques. We implement a complete
synthesis-to-locking flow and evaluate it using ISCAS, ITC, and MCNC
benchmarks. Results show that TLGLock achieves up to 30% area, 50% delay, and
20% power savings compared to latch-based locking schemes. In comparison with
XOR and SFLL-HD methods, TLGLock offers up to 3x higher SAT attack resistance
with significantly lower overhead. Furthermore, randomized key-weight
experiments demonstrate that TLGLock can reach up to 100% output corruption
under incorrect keys, enabling tunable security at minimal cost. These results
position TLGLock as a scalable and resilient solution for secure hardware
design.

</details>


### [173] [Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection](https://arxiv.org/abs/2508.16625)
*Rijha Safdar,Danyail Mateen,Syed Taha Ali,M. Umer Ashfaq,Wajahat Hussain*

Main category: cs.CR

TL;DR: AI漏洞检测系统泛化能力受限，研究探索了数据质量和模型架构对其的影响，通过实验证明数据多样性和模型选择（编码器优于解码器）能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决AI漏洞检测系统在不同代码库上泛化能力不足的问题。

Method: 通过实验评估数据质量和不同模型架构（编码器与解码器）对泛化能力的影响。

Result: 数据质量和编码器模型提升了检测性能，模型在BigVul数据集上召回率提升6.8%，且在未见项目上表现更好。

Conclusion: 数据质量和模型选择是开发鲁棒漏洞检测系统的关键，为未来系统提供方向。

Abstract: The performance of AI-based software vulnerability detection systems is often
limited by their poor generalization to unknown codebases. In this research, we
explore the impact of data quality and model architecture on the
generalizability of vulnerability detection systems. By generalization we mean
ability of high vulnerability detection performance across different C/C++
software projects not seen during training. Through a series of experiments, we
demonstrate that improvements in dataset diversity and quality substantially
enhance detection performance. Additionally, we compare multiple encoder-only
and decoder-only models, finding that encoder based models outperform in terms
of accuracy and generalization. Our model achieves 6.8% improvement in recall
on the benchmark BigVul[1] dataset, also outperforming on unseen projects,
hence showing enhanced generalizability. These results highlight the role of
data quality and model selection in the development of robust vulnerability
detection systems. Our findings suggest a direction for future systems having
high cross-project effectiveness.

</details>


### [174] [Bridging the Mobile Trust Gap: A Zero Trust Framework for Consumer-Facing Applications](https://arxiv.org/abs/2508.16662)
*Alexander Tabalipa*

Main category: cs.CR

TL;DR: 该论文提出了一种适用于移动应用的扩展零信任模型，填补了现有零信任架构在移动环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前零信任架构（ZTA）主要关注企业管理的固定基础设施，而移动应用在不可信的用户控制环境中面临日益复杂的威胁，亟需针对性解决方案。

Method: 采用设计科学方法，开发了一个六支柱框架，包括设备完整性、用户身份验证、数据保护、API安全使用、行为监控和实时应用保护，并制定了分阶段实施路线图和成熟度评估模型。

Result: 提出的模型为移动应用提供了实时的零信任原则执行方法，支持合规性，并降低了欺诈风险。

Conclusion: 该研究扩展了ZTA的适用范围，为组织提供了一种可行的移动安全解决方案，未来可进行框架的实证验证和跨行业应用测试。

Abstract: Zero Trust Architecture (ZTA) has become a widely adopted model for securing
enterprise environments, promoting continuous verification and minimal trust
across systems. However, its application in mobile contexts remains limited,
despite mobile applications now accounting for most global digital interactions
and being increasingly targeted by sophisticated threats. Existing Zero Trust
frameworks developed by organisations such as the National Institute of
Standards and Technology (NIST) and the Cybersecurity and Infrastructure
Security Agency (CISA) primarily focus on enterprise-managed infrastructure,
assuming organisational control over devices, networks, and identities. This
paper addresses a critical gap by proposing an extended Zero Trust model
designed for mobile applications operating in untrusted, user-controlled
environments. Using a design science methodology, the study introduced a
six-pillar framework that supports runtime enforcement of trust through
controls including device integrity, user identity validation, data protection,
secure application programming interface (API) usage, behavioural monitoring,
and live application protection. Each pillar was mapped to relevant regulatory
and security standards to support compliance. A phased implementation roadmap
and maturity assessment model were also developed to guide adoption across
varying organisational contexts. The proposed model offers a practical and
standards-aligned approach to securing mobile applications beyond
pre-deployment controls, aligning real-time enforcement with Zero Trust
principles. This contribution expands the operational boundaries of ZTA and
provides organisations with a deployable path to reduce fraud, enhance
compliance, and address emerging mobile security challenges. Future research
may include empirical validation of the framework and cross-sector application
testing.

</details>


### [175] [MalLoc: Toward Fine-grained Android Malicious Payload Localization via LLMs](https://arxiv.org/abs/2508.17856)
*Tiezhu Sun,Marco Alecci,Aleksandr Pilgun,Yewei Song,Xunzhu Tang,Jordan Samhi,Tegawendé F. Bissyandé,Jacques Klein*

Main category: cs.CR

TL;DR: 提出MalLoc，利用大语言模型（LLM）精细定位安卓恶意软件中的恶意载荷，提升分析精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统检测技术难以应对快速演变的安卓恶意软件，尤其是代码混淆和动态行为触发等高级手法，无法精确定位恶意载荷。

Method: 利用大语言模型（LLM）的代码理解能力，开发MalLoc方法，精细定位恶意载荷。

Result: 实验证明MalLoc可行且有效，提升了恶意软件分析的精度和可解释性。

Conclusion: MalLoc超越了传统检测方法，为行为级恶意逻辑分析和针对性防御开辟了新方向。

Abstract: The rapid evolution of Android malware poses significant challenges to the
maintenance and security of mobile applications (apps). Traditional detection
techniques often struggle to keep pace with emerging malware variants that
employ advanced tactics such as code obfuscation and dynamic behavior
triggering. One major limitation of these approaches is their inability to
localize malicious payloads at a fine-grained level, hindering precise
understanding of malicious behavior. This gap in understanding makes the design
of effective and targeted mitigation strategies difficult, leaving mobile apps
vulnerable to continuously evolving threats.
  To address this gap, we propose MalLoc, a novel approach that leverages the
code understanding capabilities of large language models (LLMs) to localize
malicious payloads at a fine-grained level within Android malware. Our
experimental results demonstrate the feasibility and effectiveness of using
LLMs for this task, highlighting the potential of MalLoc to enhance precision
and interpretability in malware analysis. This work advances beyond traditional
detection and classification by enabling deeper insights into behavior-level
malicious logic and opens new directions for research, including dynamic
modeling of localized threats and targeted countermeasure development.

</details>


### [176] [Cyber Security Educational Games for Children: A Systematic Literature Review](https://arxiv.org/abs/2508.17414)
*Temesgen Kitaw Damenu,İnci Zaim Gökbay,Alexandra Covaci,Shujun Li*

Main category: cs.CR

TL;DR: 该论文通过系统综述2010至2024年间发表的91款教育游戏，发现其在儿童网络安全教育中具有积极效果，但也存在设计和方法学上的不足，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估教育游戏在儿童网络安全教育中的有效性及其设计和实施中的不足，以推动该领域的改进。

Method: 通过系统文献综述，分析了68篇论文中报告的91款教育游戏，重点关注学习效果、设计过程和方法学严谨性。

Result: 研究发现教育游戏对儿童网络安全教育有积极效果，但也存在设计系统性不足、学习目标与实际效果不一致、缺乏对照组等问题。

Conclusion: 结论指出需改进教育游戏的设计和方法学严谨性，并建议结合自下而上和自上而下的混合设计与评估方法作为未来研究方向。

Abstract: Educational games have been widely used to teach children about cyber
security. This systematic literature review reveals evidence of positive
learning outcomes, after analysing 91 such games reported in 68 papers
published between 2010 and 2024. However, critical gaps have also been
identified regarding the design processes and the methodological rigour,
including lack of systematic design, misalignment between proposed and achieved
learning outcomes, rare use of control groups, limited discussions on ethical
considerations, and underutilisation of emerging technologies. We recommend
multiple future research directions, e.g., a hybrid approach to game design and
evaluation that combines bottom-up and top-down approaches.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [177] [Programmable k-local Ising Machines and all-optical Kolmogorov-Arnold Networks on Photonic Platforms](https://arxiv.org/abs/2508.17440)
*Nikita Stroev,Natalia G. Berloff*

Main category: physics.optics

TL;DR: 将k-local Ising优化与光学KAN函数学习统一在单一光计算平台上，通过空间光调制器实现全光学k-local相互作用与KAN层。


<details>
  <summary>Details</summary>
Motivation: 探索光计算中离散-连续工作流的关键融合点，解决非线性计算资源利用问题。

Method: 利用空间光调制器的多窗口特性，通过折叠4f中继实现局部耦合和KAN非线性计算。

Result: 实现了无需非线性介质的高阶光学Ising优化和全光学KAN处理。

Conclusion: 提出一种硬件开销极小的高并行性光计算方法，适用于多种光计算平台。

Abstract: We unify k-local Ising optimization and optical KAN function learning on a
single photonic platform, establishing a critical convergence point in optical
computing that enables interleaved discrete-continuous workflows. We introduce
a single spacial light modulator (SLM)-centric primitive that realizes, in one
stroke, all-optical k-local Ising interactions and fully optical
Kolmogorov-Arnold network (KAN) layers. The central idea is to convert
structural nonlinearity of a nominally linear photonic scatterer into a
per-window computational resource by adding one relay pass through the same
spatial light modulator. A folded 4f relay reimages the first Fourier plane
onto the SLM so that each chosen spin clique or ridge channel occupies a
disjoint window with its own second-pass phase patch. Propagation remains
linear in the optical field, yet the measured intensity in each window becomes
a freely programmable polynomial of the clique sum or projection amplitude.
This yields native, per-clique k-local couplings without nonlinear media and,
in parallel, the many independent univariate nonlinearities required by KAN
layers, all with in-situ physical gradients for training using two-frame
(forward and adjoint) physical gradients. We outline implementation on spatial
photonic Ising machines, injection-locked VCSEL arrays, and the Microsoft
analog optical computers. In all cases the hardware change is one extra lens
and a fold (or an on-chip 4f loop), enabling a minimal overhead, massively
parallel route to high-order optical Ising optimization and trainable,
all-optical KAN processing.

</details>


### [178] [Wave Tracing: Generalizing The Path Integral To Wave Optics](https://arxiv.org/abs/2508.17386)
*Shlomi Steinberg,Matt Pharr*

Main category: physics.optics

TL;DR: 论文提出了一种基于双线性路径积分的波光学光传输模型，扩展了经典路径积分方法，以准确模拟波的干涉效应，并开发了区域到区域传输的弱局部路径积分系统。


<details>
  <summary>Details</summary>
Motivation: 波光学现象（如干涉和衍射）的准确模拟对许多应用至关重要，但传统光线模型无法捕捉波的特性，计算复杂度高。研究旨在提供一种能有效模拟波效应的路径积分方法。

Method: 通过分析经典路径积分，提出双线性路径积分扩展以模拟路径间的波干涉，并开发了基于椭圆锥的弱局部路径积分系统，实现区域到区域的波效应建模。

Result: 提出的方法能够准确模拟波光学现象，支持多种实际传输算法的推导，并应用于复杂环境中的光传输和长波长辐射模拟。

Conclusion: 扩展的路径积分框架为波光学模拟提供了高效且理论基础坚实的解决方案，适用于渲染和辐射传播的多种实际应用。

Abstract: Modeling the wave nature of light and the propagation and diffraction of
electromagnetic fields is crucial for the accurate simulation of many
phenomena, yet wave simulations are significantly more computationally complex
than classical ray-based models. In this work, we start by analyzing the
classical path integral formulation of light transport and rigorously study
which wave-optical phenomena can be reproduced by it. We then introduce a
bilinear path integral generalization for wave-optical light transport that
models the wave interference between paths. This formulation subsumes many
existing methods that rely on shooting-bouncing rays or UTD-based diffractions,
and serves to give insight into the challenges of such approaches and the
difficulty of sampling good paths in a bilinear setting.
  With this foundation, we develop a weakly-local path integral based on
region-to-region transport using elliptical cones that allows sampling
individual paths that still model wave effects accurately. As with the classic
path integral form of the light transport equation, our path integral makes it
possible to derive a variety of practical transport algorithms. We present a
complete system for wave tracing with elliptical cones, with applications in
light transport for rendering and efficient simulation of long-wavelength
radiation propagation and diffraction in complex environments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [179] [Borrowing Dirty Qubits in Quantum Programs](https://arxiv.org/abs/2508.17190)
*Bonan Su,Li Zhou,Yuan Feng,Mingsheng Ying*

Main category: quant-ph

TL;DR: 本文介绍了脏量子比特的借用及其在量子编程中的语义定义，提出了脏量子比特的安全反计算概念，并给出了验证算法和实验结果。


<details>
  <summary>Details</summary>
Motivation: 量子计算中干净量子比特资源稀缺，通过借用空闲脏量子比特实现复用，减少对干净量子比特的需求。

Method: 形式化定义脏量子比特借用语义，提出安全反计算概念，并设计高效验证算法。

Result: 实验验证了脏量子比特安全反计算的有效性。

Conclusion: 脏量子比特的借用和安全反计算为量子编程提供了资源优化方案。

Abstract: Dirty qubits are ancillary qubits that can be borrowed from idle parts of a
computation, enabling qubit reuse and reducing the demand for fresh, clean
qubits-a resource that is typically scarce in practice. For such reuse to be
valid, the initial states of the dirty qubits must not affect the functionality
of the quantum circuits in which they are employed. Moreover, their original
states, including any entanglement they possess, must be fully restored after
use-a requirement commonly known as safe uncomputation. In this paper, we
formally define the semantics of dirty-qubit borrowing as a feature in quantum
programming languages, and introduce a notion of safe uncomputation for dirty
qubits in quantum programs. We also present an efficient algorithm, along with
experimental results, for verifying safe uncomputation of dirty qubits in
certain quantum circuits.

</details>


### [180] [Hamiltonian Simulation for Advection-Diffusion Equation with arbitrary transport field](https://arxiv.org/abs/2508.16728)
*Niladri Gomes,Gautam Sharma,Jay Pathak*

Main category: quant-ph

TL;DR: 提出一种基于量子启发的'Hamiltonian模拟'新方法，用于解决任意输运场下的平流-扩散方程，适用于2D和3D问题，并在IBM Quantum硬件上验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有PDE求解方法中，Hamiltonian模拟是一个未充分探索但潜力巨大的方向，尤其在长期容忍错误的量子计算背景下。

Method: 结合上风差分（用于平流）和中心差分（用于扩散），并通过近似和优化技术实现量子化，设计了一种适应复杂输运场的算法。

Result: 在2D和3D耦合旋转、剪切和扩散输运基准测试中验证了算法的有效性，并在16量子位的IBM硬件上成功实现了2D问题。

Conclusion: 该方法展示了量子算法在解决复杂PDE问题中的实际应用潜力，为进一步研究提供了方向。

Abstract: We present a novel approach to solve the advection-diffusion equation under
arbitrary transporting fields using a quantum-inspired 'Schrodingerisation'
technique for Hamiltonian simulation. Although numerous methods exist for
solving partial differential equations (PDEs), Hamiltonian simulation remains a
relatively underexplored yet promising direction-particularly in the context of
long-term, fault-tolerant quantum computing. Building on this potential, our
quantum algorithm is designed to accommodate non-trivial, spatially varying
transport fields and is applicable to both 2D and 3D advection-diffusion
problems. To ensure numerical stability and accuracy, the algorithm combines an
upwinding discretization scheme for the advective component and the central
differencing for diffusion, adapted for quantum implementation through a
tailored mix of approximation and optimization techniques. We demonstrate the
algorithm's effectiveness on benchmark scenarios involving coupled rotational,
shear, and diffusive transport in two and three dimensions. Additionally, we
implement the 2D advection-diffusion equation using 16 qubits on IBM Quantum
hardware, validating our method and highlighting its practical applicability
and robustness.

</details>


### [181] [Quantum State Fidelity for Functional Neural Network Construction](https://arxiv.org/abs/2508.16895)
*Skylar Chan,Wilson Smith,Kyla Gabriel*

Main category: quant-ph

TL;DR: 论文提出了一种基于量子算法的功能网络构建方法，与传统经典方法相比，量子状态保真度能够揭示不同的功能网络，证明了量子计算在神经科学领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏真实的参考数据，如何在高维神经记录数据中恢复具有神经学意义的功能网络是一个未解决的问题。量子算法被认为可能为此提供更优解决方案。

Method: 论文实现了混合量子算法来构建功能网络，并与文献中记录的经典技术结果进行了对比。

Result: 量子状态保真度能够揭示不同的功能网络，表现与传统经典方法相当，甚至在某些方面更具优势。

Conclusion: 量子计算为神经科学中的数据驱动建模提供了可行且可能更具优势的替代方案，其在高维图推断和复杂系统分析中具有广泛的应用潜力。

Abstract: Neuroscientists face challenges in analyzing high-dimensional neural
recording data of dense functional networks. Without ground-truth reference
data, finding the best algorithm for recovering neurologically relevant
networks remains an open question. We implemented hybrid quantum algorithms to
construct functional networks and compared them with the results of documented
classical techniques. We demonstrated that our quantum state fidelity can
provide a competitive alternative to classical metrics by revealing distinct
functional networks. Our results suggest that quantum computing offers a viable
and potentially advantageous alternative for data-driven modeling in
neuroscience, underscoring its broader applicability in high-dimensional graph
inference and complex system analysis.

</details>


### [182] [Harnessing the edge of chaos for combinatorial optimization](https://arxiv.org/abs/2508.17655)
*Hayato Goto,Ryo Hidaka,Kosuke Tatsumura*

Main category: quant-ph

TL;DR: 该论文提出了一种广义模拟分岔（GSB）算法，通过非线性控制个体分岔参数，显著提高了解决大规模组合优化问题的成功率，并实现了超快的计算速度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提升基于连续变量动力学系统的组合优化算法的准确性，尤其是模拟分岔（SB）算法，以解决其在准确性上的不足。

Method: 通过引入非线性控制个体分拐参数，对模拟分岔算法进行广义化（GSB），并研究其在混沌边缘的表现。

Result: GSB算法在2000变量问题上将解决时间缩短至10毫秒，比原SB算法快两个数量级，且成功率接近100%。

Conclusion: 研究表明，利用混沌边缘的性质可以显著提升动力学系统在组合优化中的表现，为自然启发的优化方法开辟了新途径。

Abstract: Nonlinear dynamical systems with continuous variables can be used for solving
combinatorial optimization problems with discrete variables.In particular,
numerical simulations of them can be used as heuristic algorithms with a
desirable property, namely, parallelizability, which allows us to execute them
in a massively parallel manner using cutting-edge many-core processors, leading
to ultrafast performance. However, the dynamical-system approaches with
continuous variables are usually less accurate than conventional approaches
with discrete variables such as simulated annealing. To improve the solution
accuracy of a representative dynamical system-based algorithm called simulated
bifurcation (SB), which was found from classical simulation of a quantum
nonlinear oscillator network exhibiting quantum bifurcation, here we generalize
it by introducing nonlinear control of individual bifurcation parameters and
show that the generalized SB (GSB) can achieve almost 100% success
probabilities for some large-scale problems. As a result, the time to solution
for a 2,000-variable problem is shortened to 10 ms by a GSB-based machine,
which is two orders of magnitude shorter than the best known value, 1.3 s,
previously obtained by an SB-based machine. To examine the reason for the
ultrahigh performance, we investigated chaos in the GSB changing the
nonlinear-control strength and found that the dramatic increase of success
probabilities happens near the edge of chaos. That is, the GSB can find a
solution with high probability by harnessing the edge of chaos. This finding
suggests that dynamical-system approaches to combinatorial optimization will be
enhanced by harnessing the edge of chaos, opening a broad possibility to tackle
intractable combinatorial optimization problems by nature-inspired approaches.

</details>
