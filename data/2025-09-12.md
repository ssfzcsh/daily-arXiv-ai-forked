<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 16]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: Python glob模块是一个简单而强大的工具，支持通过通配符模式搜索、筛选和处理文件，适用于数据科学、商业分析和AI应用。


<details>
  <summary>Details</summary>
Motivation: 强调glob模块在计算研究中的重要性，填补其文档不足的空白，并提供实用的参考。

Method: 通过Python示例展示glob如何与pandas、scikit-learn等库结合，支持文件遍历和数据分析流程。

Result: 展示了glob在大规模数据摄入、AI数据集构建和可重复研究中的实际应用效果。

Conclusion: glob模块是Python研究中文件模式匹配的基础工具，值得成为默认引用。

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [2] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: 论文通过系统映射研究分析了教育聊天机器人在编程教学中的应用，发现Python教学占主导，并识别了现有研究的趋势与不足。


<details>
  <summary>Details</summary>
Motivation: 探索聊天机器人在编程教学中的开发与应用情况，以为未来教育工具的研发提供参考。

Method: 采用系统映射研究(SMS)方法，从3216篇文献中筛选54篇，基于五个研究子问题进行分析。

Result: 结果显示聊天机器人主要用于Python教学，涵盖基础编程概念，采用多样化的教学和技术架构。

Conclusion: 研究揭示了当前趋势和文献中的不足，为编程教学新工具的研发提供了见解。

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [3] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: 论文提出了一种名为GeoJSON Agents的多智能体LLM架构，通过将自然语言任务转化为结构化GeoJSON命令，利用Function Calling和Code Generation两种技术处理空间数据，显著提升了GIS自动化性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在任务自动化和自然语言理解方面取得了进展，但在缺乏GIS专业知识的情况下仍存在局限性，因此需要一种改进方法。

Method: 采用多智能体架构，包括任务解析、智能体协作和结果集成三个部分，通过Planner Agent和Worker Agent的协同工作，实现空间数据处理和分析。

Result: 实验表明，基于Function Calling的Agent准确率为85.71%，基于Code Generation的Agent为97.14%，均优于通用模型（48.57%）。

Conclusion: 该研究首次为GeoJSON数据引入LLM多智能体框架，比较了两种主流LLM增强方法的优劣，为改进GeoAI系统提供了新视角。

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [4] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: 论文介绍了TraceRAG框架，利用LLMs技术实现可解释的恶意软件检测与分析。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以深入分析恶意Android应用隐藏的行为，且缺乏可解释性。

Method: TraceRAG结合RAG框架，生成方法级代码摘要并检索语义相关片段，最终输出人类可读报告。

Result: 实验显示恶意软件检测准确率96%，行为识别准确率83.81%，专家认可其实用性。

Conclusion: TraceRAG为恶意软件分析提供了高效且可解释的新方法。

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [5] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: 该论文强调大型语言模型（LLM）的气候影响，提出一种新的能效基准测试方法，模拟实际使用场景，帮助开发者构建更可持续的AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的普及，其高能耗对气候造成影响。现有能效评估偏理论，需更贴近实际的基准测试。

Method: 引入LLM Efficiency Benchmark，采用高吞吐量后端vLLM，探讨模型大小、架构及并发请求对能效的影响。

Result: 证明新基准测试能更准确反映实际部署条件，为可持续AI提供实用见解。

Conclusion: 该方法有助于开发者优化LLM能效，推动AI可持续发展。

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [6] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA是一款浏览器扩展工具，利用先进推理模型帮助开发者理解和分析代码，支持代码重构和质量检测，经评估证明其有用性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有代码理解和分析工具需要项目设置、缺乏上下文感知且手动操作繁琐，CLARA旨在解决这些问题。

Method: CLARA通过浏览器扩展集成推理模型，支持代码理解、重构和质量检测，并通过数据集和用户研究进行评估。

Result: 评估显示CLARA在代码理解和分析任务中实用、准确且高效。

Conclusion: CLARA是一个开源工具，有效提升了代码理解和分析的效率。

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [7] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: ReDef是一个基于回退提交的高置信度软件缺陷数据集，通过GPT辅助筛选提供更可靠的标签。同时研究发现预训练语言模型在代码修改任务中依赖表面线索而非语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有软件缺陷预测数据集存在标签噪声和低精度问题，需构建更可靠的基准数据集，并评估预训练语言模型对代码修改的理解能力。

Method: 通过回退提交和历史检查构建ReDef数据集，使用GPT辅助筛选模糊样本。评估多种编码策略下预训练语言模型（CodeBERT、CodeT5+等）的性能，并通过反事实扰动测试其鲁棒性。

Result: ReDef数据集包含3,164个缺陷和10,268个清洁样本。紧凑的差异编码在所有模型中表现最佳，但反事实测试显示模型依赖表面线索而非语义理解。

Conclusion: 当前预训练语言模型在代码修改任务中仍局限于表面线索，未实现真正的语义理解。

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [8] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: 探讨如何结合大语言模型（LLM）与传统软件工程方法，以提高开发效率并减少错误，尤其关注基于场景的编程（SBP）范式。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然在软件开发中表现强大，但容易引入错误并误导开发者。因此，需要一种可靠的方法将其整合到开发流程中。

Method: 提出一种结合LLMs与基于场景的编程（SBP）的方法，通过人类开发者注入专业知识并验证模型输出。

Result: 通过案例研究设计了Connect4游戏，展示了方法的有效性，甚至在某些情况下能够形式化验证代理的正确性。

Conclusion: 结合LLMs与SBP的方法能够提升开发效率，减少错误，并增强验证能力，显示出较高的实用性。

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [9] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 该研究首次大规模调查了Git版本控制系统中公共代码仓库的历史修改行为，分析了11.1亿个仓库，发现122万个仓库中存在870万次历史重写行为，并揭示了这些修改的用途和潜在风险。


<details>
  <summary>Details</summary>
Motivation: 研究Git历史修改行为的动机在于这些行为可能破坏工作流、影响仓库的完整性与可重现性，并可能被供应链攻击者利用。

Method: 通过分析Software Heritage存档的11.1亿个Git仓库数据，结合两个案例研究，对历史修改行为进行分类和分析。

Result: 研究发现122万个仓库中发生了870万次历史修改，常见用途包括更改许可证和移除敏感信息（如私钥）。

Conclusion: 研究提出了GitHistorian工具，帮助开发者识别公共Git仓库中的历史修改行为，以避免潜在的不良实践和安全隐患。

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [10] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: 论文研究了深度学习在漏洞检测中的工业应用问题，提出了一种基于CodeBERT的CI/CD集成工具AI-DO，并通过实验和调查验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决学术界提出的深度学习漏洞检测技术在工业场景中的适用性问题，填补学术与工业间的鸿沟。

Method: 使用CodeBERT检测漏洞，评估其在开源和工业数据上的跨领域泛化能力，开发CI/CD集成的AI-DO工具。

Result: 工业数据训练的模型在本领域表现良好但跨域性能下降，开源数据训练的模型通过欠采样技术可提升检测效果。

Conclusion: AI-DO工具能有效集成到工业工作流中，提升了漏洞检测的实用性。

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [11] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: 该论文研究了容器化环境中软件组成分析（SCA）工具的局限性，并提出了一种遮蔽容忍的容器分析方法ORCA，显著提高了文件覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖开源库和第三方组件，但容器化环境可能包含过时或易受攻击的组件，SCA工具的局限性在此暴露。

Method: 分析了600个流行容器，发现许多SCA工具无法处理遮蔽容器，提出ORCA方法并实现开源工具。

Result: ORCA在遮蔽容器中检测内容的效果优于Docker Scout和Syft，文件覆盖率中位数提高40%。

Conclusion: ORCA为解决遮蔽容器分析问题提供了有效方案，并已向相关厂商报告成果。

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [12] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench是一个专门评估长上下文LLM在复杂软件开发场景中的能力的基准测试，填补了现有代码评估基准的空白。


<details>
  <summary>Details</summary>
Motivation: 随着长上下文语言模型的出现，需要一种新的评估工具来测试其在复杂软件开发中的长上下文理解能力。

Method: 通过系统生成8,000个评估场景，涵盖10种编程语言和10K到1M的上下文长度，引入8个任务类别和17个评估指标。

Result: 评估发现现有长上下文模型在复杂软件开发中存在显著性能差距，表明这一问题仍需进一步研究。

Conclusion: LoCoBench为评估长上下文LLM提供了全面框架，揭示了这一领域的重要挑战。

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [13] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector是一种新颖的方法，通过细粒度语句级别的相似性比较，显著提升了智能合约函数的相似性检测性能，平均F1分数提高了14.01%。


<details>
  <summary>Details</summary>
Motivation: 开源代码的广泛重用提高了智能合约的开发效率，但也加剧了错误传播。现有方法难以处理复杂的语义比较或缺乏解释性。

Method: SmartDetector将智能合约函数的AST分解为更小的语句树，并使用分类器比较这些树来计算相似性得分；通过余弦扩散过程优化超参数。

Result: 在三个大型实际数据集上的实验显示，SmartDetector的平均F1分数为95.88%，比现有方法平均提高了14.01%。

Conclusion: SmartDetector填补了智能合约相似性检测的研究空白，提供了细粒度和可解释的解决方案。

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [14] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: 该论文初步验证了LLVM IR级别的浮点优化（特别是快速数学）的正确性，重点研究了FMA优化在算术表达式$a * b + c$中的正确性，并提出了扩展方法。


<details>
  <summary>Details</summary>
Motivation: 科学计算程序常进行激进的编译器优化以提高性能，但需确保优化的正确性。

Method: 利用Rocq定理证明器中的Verified LLVM框架，证明基本块中FMA优化的正确性。

Result: 成功验证了FMA优化在算术表达式$a * b + c$中的正确性。

Conclusion: 提出了扩展当前初步结果的方法，未来将涵盖更多程序特性和浮点优化。

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [15] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 本文讨论了依赖类型编程语言在编译过程中规范被破坏的问题，提出了一种类型保持编译方法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 依赖类型语言（如Coq、Agda等）允许程序员编写详细的程序规范并证明其正确性，但这些规范在编译时可能会被外部程序破坏，导致编译后的程序行为异常。

Method: 提出一种类型保持编译方法，开发了一个支持依赖内存分配的中间语言，并设计了一个保持依赖类型的编译器过程。

Result: 通过类型检查确保链接过程不会与不符合类型的程序链接，从而避免规范被破坏。

Conclusion: 类型保持编译方法有望解决依赖类型语言在编译过程中的规范破坏问题，相关工作仍在进行中。

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [16] [HD-MoE: Hybrid and Dynamic Parallelism for Mixture-of-Expert LLMs with 3D Near-Memory Processing](https://arxiv.org/abs/2509.09420)
*Haochen Huang,Shuzhang Zhong,Zhe Zhang,Shuangchen Li,Dimin Niu,Hongzhong Zheng,Runsheng Wang,Meng Li*

Main category: cs.PF

TL;DR: HD-MoE是一种自动优化MoE并行计算的方案，通过混合并行映射和动态调度策略，降低通信成本并提高计算利用率，性能优于现有并行策略。


<details>
  <summary>Details</summary>
Motivation: 解决Mixture-of-Expert (MoE)架构在Near-Memory Processing (NMP)加速器上运行时的通信成本高和计算利用率不平衡问题。

Method: 提出HD-MoE，结合离线自动混合并行映射算法和在线动态调度策略。

Result: 实验显示HD-MoE在性能上优于Tensor Parallelism (TP)和Expert Parallelism (EP)，提升范围分别为1.1x-1.8x和1.1x-1.5x。

Conclusion: HD-MoE有效优化了MoE在NMP加速器上的执行效率，提升了LLM推理的整体性能。

Abstract: Large Language Models (LLMs) with Mixture-of-Expert (MoE) architectures
achieve superior model performance with reduced computation costs, but at the
cost of high memory capacity and bandwidth requirements. Near-Memory Processing
(NMP) accelerators that stack memory directly on the compute through hybrid
bonding have demonstrated high bandwidth with high energy efficiency, becoming
a promising architecture for MoE models. However, as NMP accelerators comprise
distributed memory and computation, how to map the MoE computation directly
determines the LLM inference efficiency. Existing parallel mapping strategies,
including Tensor Parallelism (TP) and Expert Parallelism (EP), suffer from
either high communication costs or unbalanced computation utilization, leading
to inferior efficiency. The dynamic routing mechanism of MoE LLMs further
aggravates the efficiency challenges. Therefore, in this paper, we propose
HD-MoE to automatically optimize the MoE parallel computation across an NMP
accelerator. HD-MoE features an offline automatic hybrid parallel mapping
algorithm and an online dynamic scheduling strategy to reduce the communication
costs while maximizing the computation utilization. With extensive experimental
results, we demonstrate that HD-MoE achieves a speedup ranging from 1.1x to
1.8x over TP, 1.1x to 1.5x over EP, and 1.0x to 1.4x over the baseline Hybrid
TP-EP with Compute-Balanced parallelism strategies.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [17] [μFork: Supporting POSIX fork Within a Single-Address-Space OS](https://arxiv.org/abs/2509.09439)
*John Alistair Kressel,Hugo Lefeuvre,Pierre Olivier*

Main category: cs.OS

TL;DR: μFork是一种单地址空间操作系统设计，支持POSIX fork，同时保持轻量性、兼容性和隔离性。


<details>
  <summary>Details</summary>
Motivation: 单地址空间操作系统因共享地址空间设计导致与多进程POSIX应用不兼容。

Method: μFork利用CHERI技术在单地址空间内模拟POSIX进程，通过复制内存并重定位指针实现fork。

Result: μFork在Redis、Nginx等实际应用中表现优异，性能比传统OS快3.7倍。

Conclusion: μFork解决了单地址空间OS的兼容性问题，同时保持了高性能和轻量性。

Abstract: Single-address-space operating systems have well-known lightweightness
benefits that result from their central design idea: the kernel and
applications share a unique address space. This model makes these operating
systems (OSes) incompatible by design with a large class of software:
multiprocess POSIX applications. Indeed, the semantics of the primitive used to
create POSIX processes, fork, are inextricably tied to the existence of
multiple address spaces.
  Prior approaches addressing this issue trade off lightweightness,
compatibility and/or isolation. We propose {\mu}Fork, a single-address-space
operating system design supporting POSIX fork on modern hardware without
compromising on any of these key objectives. {\mu}Fork emulates POSIX processes
({\mu}processes) and achieves fork by creating for the child a copy of the
parent {\mu}process' memory at a different location within a single address
space. This approach presents two challenges: relocating the child's absolute
memory references (pointers), as well as providing user/kernel and
{\mu}processes isolation without impacting lightweightness. We address them
using CHERI. We implement {\mu}Fork and evaluate it upon three real-world
use-cases: Redis snapshots, Nginx multi-worker deployments, and Zygote FaaS
worker warm-up. {\mu}Fork outperforms previous work and traditional monolithic
OSes on key lightweightness metrics by an order of magnitude, e.g. it can offer
a fork-bound FaaS function throughput 24% higher than that of a monolithic OS,
and can fork a {\mu}process in 54{\mu}s, 3.7x faster than a traditional fork.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [18] [Fingerprinting Deep Packet Inspection Devices by Their Ambiguities](https://arxiv.org/abs/2509.09081)
*Diwen Xue,Armin Huremagic,Wayne Wang,Ram Sundara Raman,Roya Ensafi*

Main category: cs.NI

TL;DR: 本文提出了一个远程测量框架dMAP，用于区分和聚类DPI设备，通过行为指纹识别这些难以分辨的网络中间件。


<details>
  <summary>Details</summary>
Motivation: 全球用户面临日益严重的网络干扰（如审查、限速和拦截），但由于DPI设备的商业化和普及，对这些设备的理解仍然有限。

Method: dMAP基于差分模糊测试，系统地发现、选择和部署专门探针，将DPI的内部解析行为转化为外部可观测的指纹。

Result: 实验表明，20-40个探针即可可靠区分多种DPI实现，包括国家审查设施和商业DPI产品。

Conclusion: dMAP为DPI的主动侦察提供了可行方法，其指纹技术还可推广到其他形式的针对性干扰。

Abstract: Users around the world face escalating network interference such as
censorship, throttling, and interception, largely driven by the commoditization
and growing availability of Deep Packet Inspection (DPI) devices. Once reserved
for a few well-resourced nation-state actors, the ability to interfere with
traffic at scale is now within reach of nearly any network operator. Despite
this proliferation, our understanding of DPIs and their deployments on the
Internet remains limited -- being network intermediary leaves DPI unresponsive
to conventional host-based scanning tools, and DPI vendors actively obscuring
their products further complicates measurement efforts.
  In this work, we present a remote measurement framework, dMAP (DPI Mapper),
that derives behavioral fingerprints for DPIs to differentiate and cluster
these otherwise indistinguishable middleboxes at scale, as a first step toward
active reconnaissance of DPIs on the Internet. Our key insight is that parsing
and interpreting traffic as network intermediaries inherently involves
ambiguities -- from under-specified protocol behaviors to differing RFC
interpretations -- forcing DPI vendors into independent implementation choices
that create measurable variance among DPIs. Based on differential fuzzing, dMAP
systematically discovers, selects, and deploys specialized probes that
translate DPI internal parsing behaviors into externally observable
fingerprints. Applying dMAP to DPI deployments globally, we demonstrate its
practical feasibility, showing that even a modest set of 20-40 discriminative
probes reliably differentiates a wide range of DPI implementations, including
major nation-state censorship infrastructures and commercial DPI products. We
discuss how our fingerprinting methodology generalizes beyond censorship to
other forms of targeted interference.

</details>


### [19] [AI Reasoning for Wireless Communications and Networking: A Survey and Perspectives](https://arxiv.org/abs/2509.09193)
*Haoxiang Luo,Yu Yan,Yanhui Bian,Wenjiao Feng,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Gang Sun,Dusit Niyato,Hongfang Yu,Abbas Jamalipour,Shiwen Mao*

Main category: cs.NI

TL;DR: 这篇论文探讨了AI推理技术在无线通信网络中的应用，特别是大型语言模型（LLMs）和高级推理范式，以优化网络性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在无线通信网络中缺乏结构化推理能力，无法处理复杂的多步决策问题。

Method: 论文综述了智能无线网络的演进、新兴AI推理技术，并建立了适用于无线网络任务的分类系统，分层分析了AI推理的应用。

Result: AI推理方法能显著提升无线通信性能，特别是在动态优化网络操作方面。

Conclusion: 论文为未来无线通信网络中集成推理技术提供了研究方向和路径。

Abstract: Artificial Intelligence (AI) techniques play a pivotal role in optimizing
wireless communication networks. However, traditional deep learning approaches
often act as closed boxes, lacking the structured reasoning abilities needed to
tackle complex, multi-step decision problems. This survey provides a
comprehensive review and outlook of reasoning-enabled AI in wireless
communication networks, with a focus on Large Language Models (LLMs) and other
advanced reasoning paradigms. In particular, LLM-based agents can combine
reasoning with long-term planning, memory, tool utilization, and autonomous
cross-layer control to dynamically optimize network operations with minimal
human intervention. We begin by outlining the evolution of intelligent wireless
networking and the limitations of conventional AI methods. We then introduce
emerging AI reasoning techniques. Furthermore, we establish a classification
system applicable to wireless network tasks. We also present a layer-by-layer
examination for AI reasoning, covering the physical, data link, network,
transport, and application layers. For each part, we identify key challenges
and illustrate how AI reasoning methods can improve AI-based wireless
communication performance. Finally, we discuss key research directions for AI
reasoning toward future wireless communication networks. By combining insights
from both communications and AI, this survey aims to chart a path for
integrating reasoning techniques into the next-generation wireless networks.

</details>


### [20] [Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN Deployments](https://arxiv.org/abs/2509.09343)
*Mohammed M. H. Qazzaz,Abdelaziz Salama,Maryam Hafeez,Syed A. R. Zaidi*

Main category: cs.NI

TL;DR: 该论文提出了一种基于机器学习的O-RAN框架，联合优化负载均衡与能源效率，通过多阈值分类模型实现高效预测和配置。


<details>
  <summary>Details</summary>
Motivation: 现有AI/ML方法通过卸载用户设备实现节能，但会导致负载不均衡并影响性能。因此需一种新的优化方法。

Method: 采用随机森林模型进行多类分类，预测不同RU配置并优化能源效率，同时考虑负载均衡。

Result: 实验表明，随机森林模型F1-macro达98.3%，较传统方法提升195%。

Conclusion: 该框架有效解决了O-RAN中节能与负载均衡的平衡问题，性能显著优于基线方法。

Abstract: Open Radio Access Network (O-RAN) architecture provides an intrinsic
capability to exploit key performance monitoring (KPM) within Radio
Intelligence Controller (RIC) to derive network optimisation through xApps.
These xApps can leverage KPM knowledge to dynamically switch on/off the
associated RUs where such a function is supported over the E2 interface.
Several existing studies employ artificial intelligence (AI)/Machine Learning
(ML) based approaches to realise such dynamic sleeping for increased energy
efficiency (EE). Nevertheless, most of these approaches rely upon offloading
user equipment (UE) to carve out a sleeping opportunity. Such an approach
inherently creates load imbalance across the network. Such load imbalance may
impact the throughput performance of offloaded UEs as they might be allocated a
lower number of physical resource blocks (PRBs). Maintaining the same PRB
allocation while addressing the EE at the network level is a challenging task.
To that end, in this article, we present a comprehensive ML-based framework for
joint optimisation of load balancing and EE for ORAN deployments. We formulate
the problem as a multi-class classification system that predictively evaluates
potential RU configurations before optimising the EE, mapping network
conditions to three load balance categories (Well Balanced, Moderately
Balanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,
Aggressive) accommodates different operational priorities between energy
savings and performance assurance. Experimental evaluation using 4.26 million
real network measurements from simulations demonstrates that our Random Forest
model achieves 98.3% F1-macro performance, representing 195% improvement over
traditional baseline strategies.

</details>


### [21] [Toward quantum-safe scalable networks: an open, standards-aware key management framework](https://arxiv.org/abs/2509.09453)
*Ane Sanz,Asier Atutxa,David Franco,Jasone Astorga,Eduardo Jacob,Diego López*

Main category: cs.NI

TL;DR: 本文提出了一种创新的网络架构，结合SDN原则和虚拟KMS，解决QKD网络的KMS识别、中继路径发现和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 量子计算的兴起对通信网络安全构成挑战，QKD网络的现有技术难以满足可扩展性和长距离实施的需求。

Method: 通过集成SDN原则，建立虚拟KMS和高层量子安全控制器QuSeC，实现KMS管理和中继路径计算。

Result: 提出的架构有效解决了KMS识别和中继路径发现问题，并分析了其安全性。

Conclusion: 该方案为QKD网络的未来发展提供了可行的路径和安全保障。

Abstract: With the advent of quantum computing, the increasing threats to security
poses a great challenge to communication networks. Recent innovations in this
field resulted in promising technologies such as Quantum Key Distribution
(QKD), which enables the generation of unconditionally secure keys,
establishing secure communications between remote nodes. Additionally, QKD
networks enable the interconnection of multinode architectures, extending the
point-to-point nature of QKD. However, due to the limitations of the current
state of technology, the scalability of QKD networks remains a challenge toward
feasible implementations. When it comes to long-distance implementations,
trusted relay nodes partially solve the distance issue through the forwarding
of the distributed keys, allowing applications that do not have a direct QKD
link to securely share key material. Even though the relay procedure itself has
been extensively studied, the establishment of the relaying node path still
lacks a solution. This paper proposes an innovative network architecture that
solves the challenges of Key Management System (KMS) identification, relay path
discovery, and scalability of QKD networks by integrating Software-Defined
Networking (SDN) principles, and establishing high-level virtual KMSs (vKMS) in
each node and creating a new entity called the Quantum Security Controller
(QuSeC). The vKMS serves the end-user key requests, managing the multiple KMSs
within the node and abstracting the user from discovering the correct KMS.
Additionally, based on the high-level view of the network topology and status,
the QuSeC serves the path discovery requests from vKMSs, computing the
end-to-end (E2E) relay path and applying security policies. The paper also
provides a security analysis of the proposal, identifying the security levels
of the architecture and analyzing the core networking security properties.

</details>


### [22] [PARROT: Portable Android Reproducible traffic Observation Tool](https://arxiv.org/abs/2509.09537)
*Andrea Jimenez-Berenguel,Celeste Campo,Marta Moure-Garrido,Carlos Garcia-Rubio,Daniel Díaz-Sanchez,Florina Almenares*

Main category: cs.NI

TL;DR: PARROT是一个可重复和便携的流量捕获系统，用于通过Android虚拟设备收集应用流量，支持自动化环境和SSL/TLS解密，并揭示了2021至2025年间应用安全协议的演变。


<details>
  <summary>Details</summary>
Motivation: 移动安全协议的快速演变和现有数据集的缺乏限制了应用流量分析的研究。

Method: PARROT系统利用Android虚拟设备，提供自动化环境配置、流量记录管理和带标签的捕获提取，集成mitmproxy支持流量解密。

Result: 分析了80个应用的流量，发现TLSv1.3占比从6.7%增至90%，QUIC协议应用大幅增加，DNS从Do53转向加密DoT。

Conclusion: PARROT系统支持可重复的流量捕获，揭示了应用安全协议的演变，为研究社区提供了工具和数据。

Abstract: The rapid evolution of mobile security protocols and limited availability of
current datasets constrains research in app traffic analysis. This paper
presents PARROT, a reproducible and portable traffic capture system for
systematic app traffic collection using Android Virtual Devices. The system
provides automated environment setup, configurable Android versions, traffic
recording management, and labeled captures extraction with human-in-the-loop
app interaction. PARROT integrates mitmproxy for optional traffic decryption
with automated SSL/TLS key extraction, supporting flexible capture modes with
or without traffic interception. We collected a dataset of 80 apps selected
from the MAppGraph dataset list, providing traffic captures with corresponding
SSL keys for decryption analysis. Our comparative analysis between the
MAppGraph dataset (2021) and our dataset (2025) reveals app traffic pattern
evolution across 50 common apps. Key findings include migration from TLSv1.2 to
TLSv1.3 protocol, with TLSv1.3 comprising 90.0\% of TCP encrypted traffic in
2025 compared to 6.7\% in 2021. QUIC protocol adoption increased substantially,
with all 50 common apps generating QUIC traffic under normal network conditions
compared to 30 apps in 2021. DNS communications evolved from predominantly
unencrypted Do53 protocol (91.0\% in 2021) to encrypted DoT protocol (81.1\% in
2025). The open-source PARROT system enables reproducible app traffic capture
for research community adoption and provides insights into app security
protocol evolution.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Guarded Fragments Meet Dynamic Logic: The Story of Regular Guards (Extended Version)](https://arxiv.org/abs/2509.09218)
*Bartosz Bednarczyk,Emanuel Kieroński*

Main category: cs.LO

TL;DR: 论文研究了RGF（带有正则守卫的守卫片段），结合了GF和ICPDL的表达能力，证明了其满足性问题为2EXPTIME完全的，并提供了查询蕴含问题的不可判定性结果。


<details>
  <summary>Details</summary>
Motivation: 结合GF和ICPDL的表达能力，统一研究多种扩展逻辑。

Method: 通过逻辑扩展和复杂性分析，研究了RGF的满足性和查询蕴含问题。

Result: 满足性问题为2EXPTIME完全的，查询蕴含问题不可判定，并识别了EXPSPACE完全的最大片段。

Conclusion: RGF在逻辑表达力和计算复杂性之间找到了平衡，扩展了现有理论。

Abstract: We study the Guarded Fragment with Regular Guards (RGF), which combines the
expressive power of the Guarded Fragment (GF) with Propositional Dynamic Logic
with Intersection and Converse (ICPDL). Our logic generalizes, in a uniform
way, many previously-studied extensions of GF, including (conjunctions of)
transitive or equivalence guards, transitive or equivalence closure and more.
We prove 2EXPTIME-completeness of the satisfiability problem for RGF, showing
that RGF is not harder than ICPDL or GF. Shifting to the query entailment
problem, we provide undecidability results that significantly strengthen and
solidify earlier results along those lines. We conclude by identifying, in a
natural sense, the maximal EXPSPACE-complete fragment of RGF.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [24] [A Contextual Bandits Approach for Personalization of Hand Gesture Recognition](https://arxiv.org/abs/2509.08915)
*Duke Lin,Michael Paskett,Ying Yang*

Main category: cs.HC

TL;DR: 论文提出了一种无需校准的个性化手势识别方法，结合上下文多臂老虎机算法和预训练神经网络，通过强化学习方式实现个性化，提高了准确率并减少了用户摩擦。


<details>
  <summary>Details</summary>
Motivation: 由于用户间的个体差异，静态模型难以对所有用户提供最优性能，而传统的个性化校准方法存在用户摩擦和数据不足的问题。

Method: 提出了一种基于上下文多臂老虎机算法和预训练神经网络的强化学习方法，利用二元奖励信号实现个性化。

Result: 用户研究中，该方法显著降低了平均假阴性率（减少0.113），并提高了平均精度（增加0.139），部分用户从无法完成任务到成功完成。

Conclusion: 该方法有效解决了传统个性化方法的问题，显著提升了手势识别性能，且无需繁琐的校准过程。

Abstract: In human-computer interaction applications like hand gesture recognition,
supervised learning models are often trained on a large population of users to
achieve high task accuracy. However, due to individual variability in sensor
signals and user behavior, static models may not provide optimal performance
for all users. Personalizing pretrained models via calibration--collecting
labeled data from each user--can improve performance but introduces user
friction and struggles with limited data. To overcome these issues, we propose
a calibrationless longitudinal personalization method: a contextual multi-arm
bandit (MAB) algorithm combined with a pretrained neural network for gesture
recognition. This reinforcement-learning-style approach enables personalization
using binary reward signals, either user-provided or inferred by the system.
  We validated this method in a user study. Participants wore a surface
electromyography (sEMG) device and played multiple rounds of a 2-D navigation
game using six hand gestures. In the session, they completed a baseline round
and then a round with our algorithm; in the second session, they played another
round with our algorithm. Our approach led to a significant reduction in users'
average false negative rate by 0.113 from the initial to the final round, with
further decreases between sessions. Average precision also trended upward (by
0.139) from the start to end of a round, continuing in the next session.
Notably, some users who could not complete the game with the baseline model
succeeded with our contextual MAB model. In summary, our

</details>


### [25] [Characterizing Multimodal Interaction in Visualization Authoring Tools](https://arxiv.org/abs/2509.08953)
*Astrid van den Brandt,Sehi L'Yi,Huyen N. Nguyen,Anna Vilanova,Nils Gehlenborg*

Main category: cs.HC

TL;DR: 本文对可视化创作工具中多模态交互的多样化特征进行了系统性综述，填补了现有文献的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态交互在可视化创作工具设计中日益受到重视，但对其多样化特征的全面概述仍缺失。本文旨在提供系统性视角，促进对当前实践的理解，并为未来研究提供方向。

Method: 作者回顾了20个集成多模态交互的可视化创作工具，并分析了多模态交互的应用方式。

Result: 研究总结了多模态交互在工具中的应用特征，并提出了设计启示。

Conclusion: 综述为未来开发更易用和高效的可视化创作系统提供了指导，并指明了研究方向。

Abstract: Multimodal interaction has been increasingly considered in designing
visualization authoring tools. However, multimodal interaction has a broad
meaning in visualization authoring, according to our literature review.
Although some previous studies compare different authoring tools, a
comprehensive overview of the diverse characteristics of multimodal interaction
in visualization authoring tools is still missing. This paper seeks to offer a
systematic perspective on how multimodal interaction is integrated within
visualization authoring tools. Such an overview can enhance understanding of
current practices, highlight distinguishing features among tools, and help
identify future research directions, guiding designers in developing more
accessible and effective authoring systems. We review 20 visualization
authoring tools that incorporate multimodal interaction and characterize how
multimodal interaction is applied in these tools. Based on the review results,
we discuss design implications and future directions.

</details>


### [26] [YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models](https://arxiv.org/abs/2509.08997)
*Yaman Yu,Yiren Liu,Jacky Zhang,Yun Huang,Yang Wang*

Main category: cs.HC

TL;DR: 研究人员提出了首个针对青少年与大型语言模型（LLM）交互安全性的基准数据集YAIR，并开发了YouthSafe模型以改善风险检测，发现现有系统在处理青少年特有风险时表现不足。


<details>
  <summary>Details</summary>
Motivation: 青少年和年轻人在日常生活中广泛使用LLM，但其特有的风险和脆弱性在当前的安全基准和审查系统中未得到充分评估，导致这一群体容易受到伤害。

Method: 研究团队创建了包含12,449条对话片段的YAIR数据集，涵盖78种细粒度风险类型，并开发了优化模型YouthSafe用于实时风险检测。

Result: 评估发现现有审查模型在检测青少年风险时表现不佳，而YouthSafe在风险检测和分类上显著优于现有系统。

Conclusion: YouthSafe为青少年提供了更安全和更适合发展的AI交互方案，填补了当前系统的不足。

Abstract: Large Language Models (LLMs) are increasingly used by teenagers and young
adults in everyday life, ranging from emotional support and creative expression
to educational assistance. However, their unique vulnerabilities and risk
profiles remain under-examined in current safety benchmarks and moderation
systems, leaving this population disproportionately exposed to harm. In this
work, we present Youth AI Risk (YAIR), the first benchmark dataset designed to
evaluate and improve the safety of youth LLM interactions. YAIR consists of
12,449 annotated conversation snippets spanning 78 fine grained risk types,
grounded in a taxonomy of youth specific harms such as grooming, boundary
violation, identity confusion, and emotional overreliance. We systematically
evaluate widely adopted moderation models on YAIR and find that existing
approaches substantially underperform in detecting youth centered risks, often
missing contextually subtle yet developmentally harmful interactions. To
address these gaps, we introduce YouthSafe, a real-time risk detection model
optimized for youth GenAI contexts. YouthSafe significantly outperforms prior
systems across multiple metrics on risk detection and classification, offering
a concrete step toward safer and more developmentally appropriate AI
interactions for young users.

</details>


### [27] [Extended Version: It Should Be Easy but... New Users Experiences and Challenges with Secret Management Tools](https://arxiv.org/abs/2509.09036)
*Lorenzo Neil,Deepthi Mungara,Laurie Williams,Yasemin Acar,Bradley Reaves*

Main category: cs.HC

TL;DR: 摘要讨论了秘密管理工具（SMTs）在帮助开发者安全存储秘密方面的局限性，以及开发者在使用SMTs时遇到的文档不足和导航困难等问题。


<details>
  <summary>Details</summary>
Motivation: 软件开发者面临秘密泄露的风险，虽然SMTs被推荐用于安全管理秘密，但实际使用中仍然存在泄露和学习困难的问题。

Method: 通过定性两步研究，观察21名新用户在实验室环境中使用SMTs执行秘密存储、访问和注入任务，并访谈其体验。

Result: 研究发现，即使在实验室环境中，工具文档不足仍导致开发者感到困难并寻求替代方案。

Conclusion: SMTs的帮助资源需要改进以支持新用户的有效使用。

Abstract: Software developers face risks of leaking their software secrets, such as API
keys or passwords, which can result in significant harm. Secret management
tools (SMTs), such as HashiCorp Vault Secrets or Infisical, are highly
recommended by industry, academia, and security guidelines to manage secrets
securely. SMTs are designed to help developers secure their secrets in a
central location, yet secrets leaks are still commonplace, and developers
report difficulty in learning how to setup and use SMTs. While SMTs typically
come with publicly available help resources (e.g., tool documentation and
interfaces), it is unclear if these actually help developers learn to
effectively use SMTs. Without usable help resources that onboards developers,
quick adoption and effective use of SMTs may be unrealistic. In a qualitative
two-step study, we observed 21 new users in person while they used SMTs to
perform two secret management tasks: secret storage and access, then secret
injection. We interviewed participants after each task to identify their
challenges and experiences using SMTs, with the assistance of help resources.
While our study sample is narrow, it serves as a reasonable proxy for new
developers who are likely to adopt SMTs early in their careers. We found that
even in a laboratory setting where new users found tool functionality,
interface flexibility helpful, they still experienced increased difficulty to
effectively use SMTs to securely remediate a hard-coded secret when they felt
tool documentation was insufficient and it motivated participants to deviate
from official tool documentation to access secondary sources or attempt
workaround methods. Specific challenges reported by participants were tool
documentation content quality, navigation difficulties with both tool
documentation and web interfaces for finding helpful content, and supportive
tool features.

</details>


### [28] [Explaining the Reputational Risks of AI-Mediated Communication: Messages Labeled as AI-Assisted Are Viewed as Less Diagnostic of the Sender's Moral Character](https://arxiv.org/abs/2509.09645)
*Pranav Khadpe,Kimi Wenzel,George Loewenstein,Geoff Kaufman*

Main category: cs.HC

TL;DR: 研究探讨了AI辅助标签如何影响人们对消息发送者的性格判断，发现标签会减弱性格信号的强度。


<details>
  <summary>Details</summary>
Motivation: 探究AI辅助标签是否会让人们对消息发送者产生负面看法，以及对性格信号的影响。

Method: 通过两项研究(N=399)，采用情景实验设计，分析AI标签对温暖和冷漠信号的影响。

Result: AI标签降低了消息的性格信号诊断性，使得温暖或冷漠信号的强度减弱。

Conclusion: AI辅助的消息被视为诊断性较低，解释了先前研究中AI辅助沟通的观察结果。

Abstract: When someone sends us a thoughtful message, we naturally form judgments about
their character. But what happens when that message carries a label indicating
it was written with the help of AI? This paper investigates how the appearance
of AI assistance affects our perceptions of message senders. Adding nuance to
previous research, through two studies (N=399) featuring vignette scenarios, we
find that AI-assistance labels don't necessarily make people view senders
negatively. Rather, they dampen the strength of character signals in
communication. We show that when someone sends a warmth-signalling message
(like thanking or apologizing) without AI help, people more strongly categorize
the sender as warm. At the same time, when someone sends a coldness-signalling
message (like bragging or blaming) without assistance, people more confidently
categorize them as cold. Interestingly, AI labels weaken both these
associations: An AI-assisted apology makes the sender appear less warm than if
they had written it themselves, and an AI-assisted blame makes the sender
appear less cold than if they had composed it independently. This supports our
signal diagnosticity explanation: messages labeled as AI-assisted are viewed as
less diagnostic than messages which seem unassisted. We discuss how our
findings shed light on the causal origins of previously reported observations
in AI-Mediated Communication.

</details>


### [29] [Digital Iran Reloaded: Gamer Mitigation Tactics of IRI Information Controls](https://arxiv.org/abs/2509.09063)
*Melinda Cohoon*

Main category: cs.HC

TL;DR: 研究表明，伊朗年轻用户通过社交网络（如游戏社区）而非正式培训掌握规避审查的技术，社交学习是关键。


<details>
  <summary>Details</summary>
Motivation: 探索伊朗互联网用户如何利用技术和社会网络规避审查，特别是游戏社区的作用。

Method: 混合方法研究，包括660名伊朗用户的调查和网络性能测试。

Result: 年轻人更自信，社交网络是提升规避能力的主要因素，游戏社区是信息共享中心。

Conclusion: 设计者和政策制定者应关注社会学习与基础设施结合的解决方案。

Abstract: Internet censorship in the Islamic Republic of Iran restricts access to
global platforms and services, forcing users to rely on circumvention
technologies such as VPNs, proxies, and tunneling tools. This report presents
findings from a mixed-methods study of 660 Iranian internet users, with a focus
on gamers as a digitally literate and socially networked community. Survey data
are combined with network measurements of latency and VPN performance to
identify both technical and social strategies of circumvention. Results show
that while younger users report higher confidence with circumvention, peer
networks, rather than formal training, are the strongest predictors of
resilience. Gaming communities, particularly those active on platforms such as
Discord and Telegram, serve as hubs for sharing tactics and lowering barriers
to adoption. These findings extend existing work on usable security and
censorship circumvention by highlighting the intersection of infrastructural
conditions and social learning. The study concludes with design and policy
implications for developers, researchers, and funders working on digital rights
and information controls.

</details>


### [30] [Content Moderation Futures](https://arxiv.org/abs/2509.09076)
*Lindsay Blackwell*

Main category: cs.HC

TL;DR: 研究探讨了社交媒体治理的失败与可能性，通过内容审核专业人士的经验揭示企业激励与公共利益的结构性失调。


<details>
  <summary>Details</summary>
Motivation: 探究社交媒体治理中企业追求技术创新和快速增长如何损害公共信任和安全。

Method: 通过参与式设计工作坊与33位从业者合作，分析内容审核实践。

Result: 发现企业因剥削性劳动、对用户安全投资不足和全球化压力未能实现有效审核。

Conclusion: 建议借鉴计算关怀工作历史，促进治理工作者团结，推动系统性变革。

Abstract: This study examines the failures and possibilities of contemporary social
media governance through the lived experiences of various content moderation
professionals. Drawing on participatory design workshops with 33 practitioners
in both the technology industry and broader civil society, this research
identifies significant structural misalignments between corporate incentives
and public interests. While experts agree that successful content moderation is
principled, consistent, contextual, proactive, transparent, and accountable,
current technology companies fail to achieve these goals, due in part to
exploitative labor practices, chronic underinvestment in user safety, and
pressures of global scale. I argue that successful governance is undermined by
the pursuit of technological novelty and rapid growth, resulting in platforms
that necessarily prioritize innovation and expansion over public trust and
safety. To counter this dynamic, I revisit the computational history of care
work, to motivate present-day solidarity amongst platform governance workers
and inspire systemic change.

</details>


### [31] [User Exploration and Exploitation Behavior Under the Influence of Real-time Interactions in Live Streaming Environments](https://arxiv.org/abs/2509.09138)
*Akira Matsui,Kazuki Fujikawa,Ryo Sasaki,Ryo Adachi*

Main category: cs.HC

TL;DR: 论文研究了实时直播平台上用户的探索/利用行为，发现尽管用户表现出类似传统点播平台的行为，但探索期更长，且外部因素（如昼夜节律）影响用户忠诚度。


<details>
  <summary>Details</summary>
Motivation: 探讨实时直播平台与传统点播平台在用户行为上的异同，以及实时功能对用户互动的影响。

Method: 采用探索/利用（E/E）理论，分析了两年内大规模直播平台数据集。

Result: 用户在直播平台上表现出E/E行为，但探索期更长；昼夜节律等外部因素影响用户忠诚度。

Conclusion: 研究强调了平衡E/E行为对直播平台设计的重要性，为开发者和内容创作者提供了促进用户及时参与和保留的策略。

Abstract: Live streaming platforms offer a distinctive way for users and content
creators to interact with each other through real-time communication. While
research on user behavior in online platforms has explored how users discover
their favorite content from creators and engage with them, the role of
real-time features remains unclear. There are open questions as to what
commonalities and differences exist in users' relationships with live streaming
platforms compared to traditional on-demand style platforms. To understand
this, we employ the concept of Exploration/Exploitation (E/E) and analyze a
large-scale dataset from a live streaming platform over two years. Our results
indicate that even on live streaming platforms, users exhibit E/E behavior but
experience a longer exploration period. We also identify external factors, such
as circadian rhythms, that influence E/E dynamics and user loyalty. The
presented study emphasizes the importance of balancing E/E in online platform
design, especially for live streaming platforms, providing implications that
suggest design strategies for platform developers and content creators to
facilitate timely engagement and retention.

</details>


### [32] [Sensible Agent: A Framework for Unobtrusive Interaction with Proactive AR Agents](https://arxiv.org/abs/2509.09255)
*Geonsun Lee,Min Xia,Nels Numan,Xun Qian,David Li,Yanhe Chen,Achin Kulshrestha,Ishan Chatterjee,Yinda Zhang,Dinesh Manocha,David Kim,Ruofei Du*

Main category: cs.HC

TL;DR: Sensible Agent是一个旨在减少主动AR代理干扰性的框架，通过实时多模态上下文感知动态调整交互方式。


<details>
  <summary>Details</summary>
Motivation: 主动AR代理的交互通常依赖于显式语音提示，这在社交场景中可能显得突兀或不自然。

Method: 结合专家研讨会和数据标注研究，利用自我中心摄像头、多模态感知和大型多模态模型推断上下文，并通过最小干扰的交互方式提供建议。

Result: 用户研究表明，Sensible Agent显著降低了感知交互努力，同时保持了高可用性和更高的用户偏好。

Conclusion: Sensible Agent通过动态调整交互方式，有效提升了主动AR代理的无干扰性和用户体验。

Abstract: Proactive AR agents promise context-aware assistance, but their interactions
often rely on explicit voice prompts or responses, which can be disruptive or
socially awkward. We introduce Sensible Agent, a framework designed for
unobtrusive interaction with these proactive agents. Sensible Agent dynamically
adapts both "what" assistance to offer and, crucially, "how" to deliver it,
based on real-time multimodal context sensing. Informed by an expert workshop
(n=12) and a data annotation study (n=40), the framework leverages egocentric
cameras, multimodal sensing, and Large Multimodal Models (LMMs) to infer
context and suggest appropriate actions delivered via minimally intrusive
interaction modes. We demonstrate our prototype on an XR headset through a user
study (n=10) in both AR and VR scenarios. Results indicate that Sensible Agent
significantly reduces perceived interaction effort compared to voice-prompted
baseline, while maintaining high usability and achieving higher preference.

</details>


### [33] [Flip Co-op: Cooperative Takeovers in Shared Autonomy](https://arxiv.org/abs/2509.09281)
*Sandeep Banik,Naira Hovakimyan*

Main category: cs.HC

TL;DR: 本文提出了一种基于博弈论的共享自治框架，通过动态博弈建模协作控制权转移，提供了纳什均衡策略的理论保证，并在线性二次系统中实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 现有的共享自治方法缺乏理论保证，无法有效分配控制权。

Method: 通过动态博弈建模，将权限嵌入系统动力学，推导纳什均衡策略，并结合线性二次系统进行分析。

Result: 在随机人类意图下证明了纯策略纳什均衡的存在性，并在车辆轨迹跟踪问题中验证了策略的适应性。

Conclusion: 该框架通过合作博弈理论解决了自治效率与人类适应性之间的权衡，为共享自治提供了理论基础。

Abstract: Shared autonomy requires principled mechanisms for allocating and
transferring control between a human and an autonomous agent. Existing
approaches often rely on blending control inputs between human and autonomous
agent or switching rules, which lack theoretical guarantees. This paper
develops a game-theoretic framework for modeling cooperative takeover in shared
autonomy. We formulate the switching interaction as a dynamic game in which
authority is embedded directly into the system dynamics, resulting in Nash
equilibrium(NE)-based strategies rather than ad hoc switching rules. We
establish the existence and characterization of NE in the space of pure
takeover strategies under stochastic human intent. For the class of
linear-quadratic systems, we derive closed-form recursions for takeover
strategies and saddle-point value functions, providing analytical insight and
efficient computation of cooperative takeover policies. We further introduce a
bimatrix potential game reformulation to address scenarios where human and
autonomy utilities are not perfectly aligned, yielding a unifying potential
function that preserves tractability while capturing intent deviations. The
framework is applied to a vehicle trajectory tracking problem, demonstrating
how equilibrium takeover strategies adapt across straight and curved path
segments. The results highlight the trade-off between human adaptability and
autonomous efficiency and illustrate the practical benefits of grounding shared
autonomy in cooperative game theory.

</details>


### [34] [The Impact of Device Type, Data Practices, and Use Case Scenarios on Privacy Concerns about Eye-tracked Augmented Reality in the United States and Germany](https://arxiv.org/abs/2509.09285)
*Efe Bozkir,Babette Bühler,Xiaoyuan Wu,Enkelejda Kasneci,Lujo Bauer,Lorrie Faith Cranor*

Main category: cs.HC

TL;DR: 论文探讨了增强现实（AR）技术中眼动追踪数据可能引发的隐私问题，通过美国和德国的调查研究发现用户对隐私的关注度与数据类型、用途及国家文化相关。


<details>
  <summary>Details</summary>
Motivation: 随着AR设备普及和眼动追踪技术的应用，大量生物识别数据可能被收集，进而推断出敏感用户属性（如健康状况或性取向），引发隐私担忧。

Method: 研究通过美国和德国的四项众包调查（共1146名参与者），分析了用户属性、AR设备、用例、数据处理和国家文化对隐私关注的影响。

Result: 研究发现，用户了解数据可推断的信息后会更加关注隐私；设备类型不影响隐私关注度；用户更接受有益自身的用例；美国参与者比德国参与者更不关注隐私。

Conclusion: 基于研究结果，为AR开发者和政策制定者提供了隐私保护建议。

Abstract: Augmented reality technology will likely be prevalent with more affordable
head-mounted displays. Integrating novel interaction modalities such as eye
trackers into head-mounted displays could lead to collecting vast amounts of
biometric data, which may allow inference of sensitive user attributes like
health status or sexual preference, posing privacy issues. While previous works
broadly examined privacy concerns about augmented reality, ours is the first to
extensively explore privacy concerns on behavioral data, particularly eye
tracking in augmented reality. We crowdsourced four survey studies in the
United States (n1 = 48, n2 = 525) and Germany (n3 = 48, n4 = 525) to understand
the impact of user attributes, augmented reality devices, use cases, data
practices, and country on privacy concerns. Our findings indicate that
participants are generally concerned about privacy when they know what
inferences can be made based on the collected data. Despite the more prominent
use of smartphones in daily life than augmented reality glasses, we found no
indications of differing privacy concerns depending on the device type. In
addition, our participants are more comfortable when a particular use case
benefits them and less comfortable when other humans can consume their data.
Furthermore, participants in the United States are less concerned about their
privacy than those in Germany. Based on our findings, we provide several
recommendations to practitioners and policymakers for privacy-aware augmented
reality.

</details>


### [35] [Proactive AI Adoption can be Threatening: When Help Backfires](https://arxiv.org/abs/2509.09309)
*Dana Harari,Ofra Amir*

Main category: cs.HC

TL;DR: 研究探讨了AI助手在工作场所中的主动性如何影响其采用，发现未请求的帮助会引发自我威胁，从而降低接受帮助的意愿和使用可能性。


<details>
  <summary>Details</summary>
Motivation: AI助手在工作场所日益普及，但主动性如何影响其采用的心理机制尚不明确。

Method: 基于自我肯定和社会交换理论，通过两个情景实验（N=761和N=571）比较AI与人类的主动性和反应性帮助。

Result: AI帮助比人类帮助更具威胁性；主动性帮助会增加感知威胁并降低采用效果。

Conclusion: 自我威胁是主动性AI功能适得其反的原因，研究为AI主动性设计提供了启示。

Abstract: Artificial intelligence (AI) assistants are increasingly embedded in
workplace tools, raising the question of how initiative-taking shapes adoption.
Prior work highlights trust and expectation mismatches as barriers, but the
underlying psychological mechanisms remain unclear. Drawing on self-affirmation
and social exchange theories, we theorize that unsolicited help elicits
self-threat, reducing willingness to accept assistance, likelihood of future
use, and performance expectancy. We report two vignette-based experiments
(Study~1: $N=761$; Study~2: $N=571$, preregistered). Study~1 compared
anticipatory and reactive help provided by an AI vs. a human, while Study~2
distinguished between \emph{offering} (suggesting help) and \emph{providing}
(acting automatically). In Study 1, AI help was more threatening than human
help. Across both studies, anticipatory help increased perceived threat and
reduced adoption outcomes. Our findings identify self-threat as a mechanism
explaining why proactive AI features may backfire and suggest design
implications for AI initiative.

</details>


### [36] [Smart Device Development for Gait Monitoring: Multimodal Feedback in an Interactive Foot Orthosis, Walking Aid, and Mobile Application](https://arxiv.org/abs/2509.09359)
*Stefan Resch,André Kousha,Anna Carroll,Noah Severinghaus,Felix Rehberg,Marco Zatschker,Yunus Söyleyici,Daniel Sanchez-Morillo*

Main category: cs.HC

TL;DR: 该论文提出了一种结合智能足部矫正器和前臂拐杖的模块化传感器系统，支持实时反馈和患者监测，并通过实验验证了其可行性和可用性。


<details>
  <summary>Details</summary>
Motivation: 现有矫形设备多为被动且缺乏集成传感功能，研究多集中于孤立原型而非交互系统。

Method: 设计和实现了一种集成足底压力传感、运动传感、振动触觉反馈及无线通信的模块化传感器系统，并通过用户研究验证。

Result: 实验研究表明该系统在步态检测、触觉反馈和移动健康应用方面具有可行性。

Conclusion: 该系统为智能辅助康复技术提供了功能性解决方案，并讨论了未来发展和临床集成的潜力。

Abstract: Smart assistive technologies such as sensor-based footwear and walking aids
offer promising opportunities to support rehabilitation through real-time
feedback and patient-centered monitoring. However, most orthotic devices remain
passive and lack integrated sensing or feedback functionalities, while existing
research often focuses on isolated prototypes rather than cohesive, interactive
systems. In this work, we present the design and implementation of a novel
modular sensor system that combines a smart foot orthosis with an instrumented
forearm crutch. The system integrates plantar pressure and motion sensing,
vibrotactile feedback, and wireless communication via a smartphone application.
We conducted an experimental user study with eight participants to validate the
feasibility of the smart foot orthosis for mobile gait detection, explore the
potential of haptic feedback for user interaction, and assess the usability of
the accompanying mobile health application. Our work contributes to the field
of smart assistive technology in rehabilitation and prevention by demonstrating
a functional and comprehensive system. We further discuss system limitations,
outline potential application scenarios, and provide recommendations for future
development and clinical integration.

</details>


### [37] [Real-Time Kinematic Positioning and Optical See-Through Head-Mounted Display for Outdoor Tracking: Hybrid System and Preliminary Assessment](https://arxiv.org/abs/2509.09412)
*Muhannad Ismael,Maël Cornil*

Main category: cs.HC

TL;DR: 本文介绍了一种结合实时动态定位（RTK）和光学透视头戴显示器（OST-HMD）的户外追踪系统，适用于需要高精度追踪和显示隐藏信息的场景。


<details>
  <summary>Details</summary>
Motivation: 在户外环境中，高精度追踪和显示隐藏信息对于安全操作至关重要，现有的2D屏幕/平板设备无法满足需求。

Method: 通过整合RTK（提供厘米级精度）和OST-HMD（实现直观显示），开发了一种无缝融合的系统，并提出一种“半动态”评估方法。

Result: 该系统实现了高精度和直观的户外追踪，并确定了全局定位方法。

Conclusion: RTK与OST-HMD的结合为户外追踪提供了有前景的解决方案，未来研究可进一步优化系统性能。

Abstract: This paper presents an outdoor tracking system using Real-Time Kinematic
(RTK) positioning and Optical See-Through Head Mounted Display(s) (OST-HMD(s))
in urban areas where the accurate tracking of objects is critical and where
displaying occluded information is important for safety reasons. The approach
presented here replaces 2D screens/tablets and offers distinct advantages,
particularly in scenarios demanding hands-free operation. The integration of
RTK, which provides centimeter-level accuracy of tracked objects, with OST-HMD
represents a promising solution for outdoor applications. This paper provides
valuable insights into leveraging the combined potential of RTK and OST-HMD for
outdoor tracking tasks from the perspectives of systems integration,
performance optimization, and usability. The main contributions of this paper
are: \textbf{1)} a system for seamlessly merging RTK systems with OST-HMD to
enable relatively precise and intuitive outdoor tracking, \textbf{2)} an
approach to determine a global location to achieve the position relative to the
world, \textbf{3)} an approach referred to as 'semi-dynamic' for system
assessment. Moreover, we offer insights into several relevant future research
topics aimed at improving the OST-HMD and RTK hybrid system for outdoor
tracking.

</details>


### [38] [Changing the Paradigm from Dynamic Queries to LLM-generated SQL Queries with Human Intervention](https://arxiv.org/abs/2509.09461)
*Ambre Assor,Hyeon Jeon,Sungbok Shin,Jean-Daniel Fekete*

Main category: cs.HC

TL;DR: 利用大型语言模型（LLM）作为医疗可视化系统的交互层，简化专家用户的复杂查询操作，但存在部分过滤条件不直观的问题。


<details>
  <summary>Details</summary>
Motivation: 针对医疗领域高维、编码和异构数据的复杂查询需求，通过自然语言交互提升探索效率。

Method: LLM转换自然语言为可编辑和可执行的查询，结合动态查询功能支持交互探索。

Result: 在ParcoursVis系统中验证，减少了视觉干扰和记忆负担。

Conclusion: LLM交互层提升了医疗数据探索的流畅性，但需平衡查询透明度。

Abstract: We propose leveraging Large Language Models (LLMs) as an interaction layer
for medical visualization systems. In domains like healthcare, where users must
navigate high-dimensional, coded, and heterogeneous datasets, LLM-generated
queries enable expert medical users to express complex analytical intents in
natural language. These intents are then translated into editable and
executable queries, replacing the dynamic query interfaces used by traditional
visualization systems built around sliders, check boxes, and drop-downs. This
interaction model reduces visual clutter and eliminates the need for users to
memorize field names or system codes, supporting fluid exploration, with the
drawback of not exposing all the filtering criteria. We also reintroduce
dynamic queries on demand to better support interactive exploration. We posit
that medical users are trained to know the possible filtering options but
challenged to remember the details of the attribute names and code values. We
demonstrate this paradigm in ParcoursVis, our scalable EventFlow-inspired
patient care pathway visualization system powered by the French National Health
Data System, one of the largest health data repositories in the world.

</details>


### [39] [Cognitive Affordances in Visualization: Related Constructs, Design Factors, and Framework](https://arxiv.org/abs/2509.09510)
*Racquel Fygenson,Lace Padilla,Enrico Bertini*

Main category: cs.HC

TL;DR: 本文通过将认知可供性理论引入可视化领域，提出了一个框架，用于指导可视化设计的评估和优化。


<details>
  <summary>Details</summary>
Motivation: 现有的可供性研究主要关注物体形状如何传达动作，而认知可供性则关注设计如何影响信息处理等认知行为。在可视化领域，这一理论尚未得到正式应用，因此需要填补这一空白。

Method: 通过综合心理学、人机交互和可视化领域的研究，作者正式定义了可视化中的认知可供性，并对比了相关概念。

Result: 提出了一个可视化中认知可供性的框架，明确了设计决策和读者特征如何影响信息的层次化传达。

Conclusion: 该框架为可视化的评估和重新设计提供了理论支持，有助于优化信息传达效果。

Abstract: Classically, affordance research investigates how the shape of objects
communicates actions to potential users. Cognitive affordances, a subset of
this research, characterize how the design of objects influences cognitive
actions, such as information processing. Within visualization, cognitive
affordances inform how graphs' design decisions communicate information to
their readers. Although several related concepts exist in visualization, a
formal translation of affordance theory to visualization is still lacking. In
this paper, we review and translate affordance theory to visualization by
formalizing how cognitive affordances operate within a visualization context.
We also review common methods and terms, and compare related constructs to
cognitive affordances in visualization. Based on a synthesis of research from
psychology, human computer interaction, and visualization, we propose a
framework of cognitive affordances in visualization that enumerates design
decisions and reader characteristics that influence a visualization's hierarchy
of communicated information. Finally, we demonstrate how this framework can
guide the evaluation and redesign of visualizations.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [40] [Morphology-Preserving Remeshing Approach to Particulate Microstructures via Harmonic Decomposition](https://arxiv.org/abs/2509.08855)
*Mahmoud Shaqfa*

Main category: cs.GR

TL;DR: 提出了一种基于分层扩散的方法，用于重新参数化表面以获得均匀化的三角网格，改善了传统谐波分解方法中局部离散化不均的问题。


<details>
  <summary>Details</summary>
Motivation: 传统谐波分解方法在表面重构时未考虑基底函数的雅可比局部变化，导致离散化不均匀，影响数值模拟的精度和效率。

Method: 采用非线性扩散方法，对分析域的曲线坐标进行重采样，通过扩大小的三角形来平衡表面网格的质量。

Result: 实验表明，各向同性和各向异性扩散方案显著提升了表面三角网格的质量指标，保持了表面形态和体积。

Conclusion: 该方法适用于大型2D和3D微结构的数字孪生，如混凝土和石砌体，具有潜在的应用前景。

Abstract: Harmonic decomposition of surfaces, such as spherical and spheroidal
harmonics, is used to analyze morphology, reconstruct, and generate surface
inclusions of particulate microstructures. However, obtaining high-quality
meshes of engineering microstructures using these approaches remains an open
question. In harmonic approaches, we usually reconstruct surfaces by evaluating
the harmonic bases on equidistantly sampled simplicial complexes of the base
domains (e.g., triangular spheroids and disks). However, this traditional
sampling does not account for local changes in the Jacobian of the basis
functions, resulting in nonuniform discretization after reconstruction or
generation. As it impacts the accuracy and time step, high-quality
discretization of microstructures is crucial for efficient numerical
simulations (e.g., finite element and discrete element methods). To circumvent
this issue, we propose an efficient hierarchical diffusion-based approach for
resampling the surface-i.e., performing a reparameterization-to yield an
equalized mesh triangulation. Analogous to heat problems, we use nonlinear
diffusion to resample the curvilinear coordinates of the analysis domain,
thereby enlarging small triangles at the expense of large triangles on
surfaces. We tested isotropic and anisotropic diffusion schemes on the recent
spheroidal and hemispheroidal harmonics methods. The results show a substantial
improvement in the quality metrics for surface triangulation. Unlike
traditional surface reconstruction and meshing techniques, this approach
preserves surface morphology, along with the areas and volumes of surfaces. We
discuss the results and the associated computational costs for large 2D and 3D
microstructures, such as digital twins of concrete and stone masonry, and their
future applications.

</details>


### [41] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: 该论文提出了一种结合相机重建管道和视觉差异预测器（VDP）的方法，用于精确测量和评估显示器的视觉质量，并通过应用验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统显示器测量方法无法捕捉高频率和像素级失真，相机测量又存在光学和光度失真，需要结合视觉系统模型以评估失真是否可见。

Method: 提出了CameraVDP框架，结合HDR图像堆叠、MTF反转、渐晕校正、几何校正和VDP，实现精确测量和视觉差异预测。

Result: 通过缺陷像素检测、颜色边缘感知和显示器不均匀性评估验证了该框架，并提供了不确定分析工具。

Conclusion: CameraVDP通过结合相机测量和视觉预测，为显示器评估提供了精确且感知相关的方法。

Abstract: Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [42] [A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems](https://arxiv.org/abs/2509.08969)
*Nima Karimian Kakolaki*

Main category: cs.DC

TL;DR: 论文分析了分布式标识符的演进，比较了传统自增键、UUIDv4、UUIDv7和ULIDs的优劣，实验结果表明ULIDs在网络开销、生成速度和碰撞风险上表现最优。


<details>
  <summary>Details</summary>
Motivation: 分布式系统需要稳健且可扩展的标识符方案以确保数据唯一性和高效索引，传统的标识符方案在高性能场景中存在局限性。

Method: 结合数学碰撞概率计算与模拟分布式环境中的生成速度和网络传输开销实验。

Result: ULIDs在网络开销上减少83.7%，生成速度提高97.32%，碰撞风险比UUIDv7低98.42%。

Conclusion: ULIDs是高性能分布式系统的理想选择，提供高效、时间有序且可排序的标识符。

Abstract: Distributed systems require robust, scalable identifier schemes to ensure
data uniqueness and efficient indexing across multiple nodes. This paper
presents a comprehensive analysis of the evolution of distributed identifiers,
comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We
combine mathematical calculation of collision probabilities with empirical
experiments measuring generation speed and network transmission overhead in a
simulated distributed environment. Results demonstrate that ULIDs significantly
outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing
generation speed by 97.32%. statistical analysis further shows ULIDs offer a
98.42% lower collision risk compared to UUIDv7, while maintaining negligible
collision probabilities even at high generation rates. These findings highlight
ULIDs as an optimal choice for high-performance distributed systems, providing
efficient, time-ordered, and lexicographically sortable identifiers suitable
for scalable applications. All source code, datasets, and analysis scripts
utilized in this research are publicly available in our dedicated repository at
https://github.com/nimakarimiank/uids-comparison. This repository contains
comprehensive documentation of the experimental setup, including configuration
files for the distributed environment, producer and consumer implementations,
and message broker integration. Additionally, it provides the data scripts and
datasets. Researchers and practitioners are encouraged to explore the
repository for full reproducibility of the experiments and to facilitate
further investigation or extension of the presented work.

</details>


### [43] [TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes](https://arxiv.org/abs/2509.09525)
*Jialiang Huang,Teng Ma,Zheng Liu,Sixing Lin,Kang Chen,Jinlei Jiang,Xia Liao,Yingdi Shan,Yongwei Wu,Ning Zhang,Mengting Lu,Tao Ma,Haifeng Gong,Mingxing Zhang*

Main category: cs.DC

TL;DR: TrEnv是一个高效、高密度的无服务器平台，专门为LLM代理设计，通过优化容器和VM环境，显著降低启动延迟和内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统无服务器计算在应对LLM代理等新兴负载时，基础设施开销成为瓶颈，运行成本高达LLM API调用的70%。

Method: TrEnv采用可重用的沙盒和内存模板，以及浏览器共享和页面缓存绕过机制，以优化执行环境的重用和恢复。

Result: TrEnv在容器环境中将P99延迟降低7倍，内存使用减少48%；在VM环境中P99延迟降低58%，内存节省61%。

Conclusion: TrEnv通过协同设计显著提升了无服务器平台的效率，适用于高密度LLM代理负载。

Abstract: Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.

</details>


### [44] [Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines](https://arxiv.org/abs/2509.09058)
*Ajay Kumar,Praveen Rao,Peter Sanders*

Main category: cs.DC

TL;DR: 该论文提出了一种基于机器学习的优化方法，用于在GPU支持的环境中高效执行人类基因组变体调用任务。


<details>
  <summary>Details</summary>
Motivation: 由于变体调用任务计算密集且云计算资源具有按需付费的优势，论文旨在优化多基因组工作负载的执行效率。

Method: 方法包括两阶段：利用机器学习预测变体调用管道的各阶段执行时间；基于预测时间，通过灵活作业车间调度问题的方法生成最优执行计划。

Result: 实验证明该方法能有效预测执行时间，并在公共数据集上相比贪婪方法和动态方法分别带来2倍和1.6倍的加速。

Conclusion: 该方法显著提升了变体调用管道的执行效率，展现了机器学习在基因组计算优化中的潜力。

Abstract: Variant calling is the first step in analyzing a human genome and aims to
detect variants in an individual's genome compared to a reference genome. Due
to the computationally-intensive nature of variant calling, genomic data are
increasingly processed in cloud environments as large amounts of compute and
storage resources can be acquired with the pay-as-you-go pricing model. In this
paper, we address the problem of efficiently executing a variant calling
pipeline for a workload of human genomes on graphics processing unit
(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach
for optimizing the workload execution to minimize the total execution time. Our
approach encompasses two key techniques: The first technique employs ML to
predict the execution times of different stages in a variant calling pipeline
based on the characteristics of a genome sequence. Using the predicted times,
the second technique generates optimal execution plans for the machines by
drawing inspiration from the flexible job shop scheduling problem. The plans
are executed via careful synchronization across different machines. We
evaluated our approach on a workload of publicly available genome sequences
using a testbed with different types of GPU hardware. We observed that our
approach was effective in predicting the execution times of variant calling
pipeline stages using ML on features such as sequence size, read quality,
percentage of duplicate reads, and average read length. In addition, our
approach achieved 2X speedup (on an average) over a greedy approach that also
used ML for predicting the execution times on the tested workload of sequences.
Finally, our approach achieved 1.6X speedup (on an average) over a dynamic
approach that executed the workload based on availability of resources without
using any ML-based time predictions.

</details>


### [45] [WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge](https://arxiv.org/abs/2509.09400)
*Valerio Besozzi,Enrico Fiasco,Marco Danelutto,Patrizio Dazzi*

Main category: cs.DC

TL;DR: 论文比较了WebAssembly和基于unikernel的MicroVMs在边缘计算中的性能，发现WebAssembly在轻量级函数上启动更快，但在复杂任务和I/O密集型任务中表现不如Firecracker。


<details>
  <summary>Details</summary>
Motivation: 研究边缘计算中轻量级执行环境的需求，特别是针对冷启动延迟的优化，以满足紧急边缘计算（UEC）的要求。

Method: 提出了基于Wasmtime的WebAssembly运行时Limes，并将其与SPARE中使用的Firecracker环境进行比较评估。

Result: WebAssembly在轻量级函数中表现出更低的冷启动时间，但在复杂任务中表现不佳；Firecracker虽然在冷启动时间上略高，但性能更稳定，特别适合I/O密集型任务。

Conclusion: WebAssembly适用于轻量级函数，而Firecracker则在复杂和I/O密集型任务中更具优势，两者各有适用场景。

Abstract: Serverless computing at the edge requires lightweight execution environments
to minimize cold start latency, especially in Urgent Edge Computing (UEC). This
paper compares WebAssembly and unikernel-based MicroVMs for serverless
workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and
evaluate it against the Firecracker-based environment used in SPARE. Results
show that WebAssembly offers lower cold start times for lightweight functions
but suffers with complex workloads, while Firecracker provides higher, but
stable, cold starts and better execution performance, particularly for
I/O-heavy tasks.

</details>


### [46] [Coherence-Aware Task Graph Modeling for Realistic Application](https://arxiv.org/abs/2509.09094)
*Guochu Xiong,Xiangzhong Luo,Weichen Liu*

Main category: cs.DC

TL;DR: CoTAM是一个针对真实工作负载的缓存一致性感知任务图建模框架，通过解耦一致性影响并量化其权重，生成统一的任务图以反映运行时行为，显著优于隐式方法。


<details>
  <summary>Details</summary>
Motivation: 多核系统中缓存一致性对性能影响显著，但现有任务图建模方法通常依赖预定义图或忽略一致性交互，导致与运行时行为脱节。CoTAM旨在填补这一空白。

Method: CoTAM通过解耦缓存一致性影响、量化其权重并推断任务间依赖关系，构建统一的运行时任务图。

Result: 实验表明CoTAM优于隐式方法，能够准确捕捉动态工作负载行为并提升系统级分析的通用性。

Conclusion: 缓存一致性是任务图建模的关键因素，CoTAM为动态工作负载提供了一种更准确且通用的建模方法。

Abstract: As multicore systems continue to scale, cache coherence has emerged as a
critical determinant of system performance, with coherence behavior and task
execution closely intertwined, reshaping inter-task dependencies. Task graph
modeling provides a structured way to capture such dependencies and serves as
the foundation for many system-level design strategies. However, these
strategies typically rely on predefined task graphs, while many real-world
applications lack explicit graphs and exhibit dynamic, data-dependent behavior,
limiting the effectiveness of static approaches. To address this, several task
graph modeling methods for realistic workloads have been developed. Yet, they
either rely on implicit techniques that use application-specific features
without producing explicit graphs, or they generate graphs tailored to fixed
scheduling models, which limits generality. More importantly, they often
overlook coherence interactions, creating a gap between design assumptions and
actual runtime behavior. To overcome these limitations, we propose CoTAM, a
Coherence-Aware Task Graph Modeling framework for realistic workloads that
constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the
impact of coherence by decoupling its effects from overall execution,
quantifies its influence through a learned weighting scheme, and infers
inter-task dependencies for coherence-aware graph generation. Extensive
experiments show that CoTAM outperforms implicit methods, bridging the gap
between dynamic workload behavior and existing designs while demonstrating the
importance of incorporating cache coherence into task graph modeling for
accurate and generalizable system-level analysis.

</details>


### [47] [Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing](https://arxiv.org/abs/2509.09435)
*Houming Qiu,Kun Zhu,Dusit Niyato,Nguyen Cong Luong,Changyan Yi,Chen Dai*

Main category: cs.DC

TL;DR: 提出了一种基于重心有理插值的近似CDC方案，解决了现有CDC方案灵活性和数值稳定性的不足，能利用任意返回结果解码，且在训练过程中加速并提供抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 现有CDC方案因固定的恢复阈值和函数极点在解码过程中存在灵活性和稳定性问题，限制了MEC系统的性能。

Method: 基于重心有理插值设计近似CDC方案，支持有限域和实数域计算，编码/解码函数无极点，并结合BRI梯度编码算法。

Result: 实验证明所提方案在等待时间和近似精度上优于现有CDC方案。

Conclusion: 该方案解决了CDC的灵活性与稳定性问题，显著提升了MEC系统性能。

Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.

</details>


### [48] [Weaker Assumptions for Asymmetric Trust](https://arxiv.org/abs/2509.09493)
*Ignacio Amores-Sesar,Christian Cachin,Juan Villacis*

Main category: cs.DC

TL;DR: 本文研究了分布式系统中非对称信任的挑战，提出了新的方法来解决问题，如可靠广播和共识，降低了现有方法的限制。


<details>
  <summary>Details</summary>
Motivation: 探讨非对称信任模型中的可靠广播和共识问题，避免现有强假设带来的限制。

Method: 提出新方法描述非对称问题，并设计算法，放宽假设条件。

Result: 算法在更弱假设下实现可靠广播和共识，适用于其他非对称信任问题。

Conclusion: 新方法为分布式系统中的非对称信任问题提供了更灵活的解决方案。

Abstract: In distributed systems with asymmetric trust, each participant is free to
make its own trust assumptions about others, captured by an asymmetric quorum
system. This contrasts with ordinary, symmetric quorum systems and threshold
models, where trust assumptions are uniformly shared among participants.
Fundamental problems like reliable broadcast and consensus are unsolvable in
the asymmetric model if quorum systems satisfy only the classical properties of
consistency and availability. Existing approaches overcome this by introducing
stronger assumptions. We show that some of these assumptions are overly
restrictive, so much so that they effectively eliminate the benefits of
asymmetric trust. To address this, we propose a new approach to characterize
asymmetric problems and, building upon it, present algorithms for reliable
broadcast and consensus that require weaker assumptions than previous
solutions. Our methods are general and can be extended to other core problems
in systems with asymmetric trust.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [49] [Koza and Koza-Hub for born-interoperable knowledge graph generation using KGX](https://arxiv.org/abs/2509.09096)
*Daniel R Korn,Patrick Golden,Aaron Odell,Katherina Cortes,Shilpa Sundar,Kevin Schaper,Sarah Gehrke,Corey Cox,Harry Caufield,Justin Reese,Evan Morris,Christopher J Mungall,Melissa Haendel*

Main category: cs.DB

TL;DR: 论文介绍了Koza和Koza-Hub，一个Python工具包，用于简化生物医学数据转换为KGX标准格式，解决当前知识图谱构建中的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学知识图谱构建方法存在大量冗余劳动，原因是缺乏数据标准和可直接用于知识图谱的数据源。

Method: 使用Koza软件包和YAML配置，将数据转换为KGX格式，并提供对30种生物医学数据源的标准化转换。

Result: 实现了生物医学数据的标准化和高效转换，减少了冗余劳动。

Conclusion: Koza和Koza-Hub为生物医学知识图谱构建提供了高效、标准化的解决方案。

Abstract: Knowledge graph construction has become an essential domain for the future of
biomedical research. But current approaches demand a high amount of redundant
labor. These redundancies are the result of the lack of data standards and
"knowledge-graph ready" data from sources. Using the KGX standard, we aim to
solve these issues. Herein we introduce Koza and the Koza-Hub, a Python
software package which streamlines ingesting raw biomedical information into
the KGX format, and an associated set of conversion processes for thirty gold
standard biomedical data sources. Our approach is to turn knowledge graph
ingests into a set of primitive operations, provide configuration through YAML
files, and enforce compliance with the chosen data schema.

</details>


### [50] [Let's Simply Count: Quantifying Distributional Similarity Between Activities in Event Data](https://arxiv.org/abs/2509.09440)
*Henrik Kirchmann,Stephan A. Fahrenkrog-Petersen,Xixi Lu,Matthias Weidlich*

Main category: cs.DB

TL;DR: 该论文提出了一种基于计数的嵌入方法，用于事件数据中活动的分布相似性建模，强调其简单、高效和可解释性，并通过实验验证其优于现有复杂方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的方法（如word2vec和自动编码器）虽然有效，但计算成本高且可解释性差。论文提倡简化建模，提出更简单、直接可解释的嵌入方法。

Method: 引入了基于计数的嵌入方法，避免了复杂的训练过程。同时提出了一个全面的基准测试框架，用于评估嵌入的内在质量、下游应用表现和计算效率。

Result: 实验表明，基于计数的嵌入在活动分布相似性建模中既高效又有效，优于现有方法。

Conclusion: 论文呼吁简化建模方法，证明基于计数的嵌入是事件数据分析中分布相似性建模的可靠选择。

Abstract: To obtain insights from event data, advanced process mining methods assess
the similarity of activities to incorporate their semantic relations into the
analysis. Here, distributional similarity that captures similarity from
activity co-occurrences is commonly employed. However, existing work for
distributional similarity in process mining adopt neural network-based
approaches as developed for natural language processing, e.g., word2vec and
autoencoders. While these approaches have been shown to be effective, their
downsides are high computational costs and limited interpretability of the
learned representations.
  In this work, we argue for simplicity in the modeling of distributional
similarity of activities. We introduce count-based embeddings that avoid a
complex training process and offer a direct interpretable representation. To
underpin our call for simple embeddings, we contribute a comprehensive
benchmarking framework, which includes means to assess the intrinsic quality of
embeddings, their performance in downstream applications, and their
computational efficiency. In experiments that compare against the state of the
art, we demonstrate that count-based embeddings provide a highly effective and
efficient basis for distributional similarity between activities in event data.

</details>


### [51] [Database Views as Explanations for Relational Deep Learning](https://arxiv.org/abs/2509.09482)
*Agapi Rissaki,Ilias Fountalis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 该论文提出了一种新框架，用于解释基于关系数据库的机器学习模型的工作原理，通过视图定义突出显示数据库中主要影响预测的部分。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型（如异构图神经网络）在处理关系数据库时，由于其复杂性难以解释预测过程，因此需要一种更清晰的解释方法。

Method: 采用全局溯因解释框架，结合确定性概念和视图定义的灵活性，通过启发式算法避免全空间搜索，并应用模型无关和模型特定的技术。

Result: 在RelBench数据集上的实验表明，该框架能有效生成解释，并且兼顾效率和实用性。

Conclusion: 该框架为关系数据库中的机器学习模型提供了一种可解释、灵活且高效的解释方案。

Abstract: In recent years, there has been significant progress in the development of
deep learning models over relational databases, including architectures based
on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph
transformers. In effect, such architectures state how the database records and
links (e.g., foreign-key references) translate into a large, complex numerical
expression, involving numerous learnable parameters. This complexity makes it
hard to explain, in human-understandable terms, how a model uses the available
data to arrive at a given prediction. We present a novel framework for
explaining machine-learning models over relational databases, where
explanations are view definitions that highlight focused parts of the database
that mostly contribute to the model's prediction. We establish such global
abductive explanations by adapting the classic notion of determinacy by Nash,
Segoufin, and Vianu (2010). In addition to tuning the tradeoff between
determinacy and conciseness, the framework allows controlling the level of
granularity by adopting different fragments of view definitions, such as ones
highlighting whole columns, foreign keys between tables, relevant groups of
tuples, and so on. We investigate the realization of the framework in the case
of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive
search over the space of all databases. We propose techniques that are
model-agnostic, and others that are tailored to hetero-GNNs via the notion of
learnable masking. Our approach is evaluated through an extensive empirical
study on the RelBench collection, covering a variety of domains and different
record-level tasks. The results demonstrate the usefulness of the proposed
explanations, as well as the efficiency of their generation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [52] [Implementation of a 8-bit Wallace Tree Multiplier](https://arxiv.org/abs/2509.09178)
*Ayan Biswas,Jimmy Jin*

Main category: cs.AR

TL;DR: 这篇论文概述了Wallace树乘法器的设计与实现，重点介绍了其并行结构和优化电路深度的特点，以及在ECE 55900项目中使用Cadence Virtuoso完成的8位乘法器设计和MAC单元的实现。


<details>
  <summary>Details</summary>
Motivation: 通过Wallace树乘法器优化电路深度，减少部分积，提高乘法效率。

Method: 在gpdk45技术中使用Cadence Virtuoso设计并布局8位Wallace树乘法器，同时探索了16位组合乘法加法MAC单元的实现。

Result: 成功实现了8位Wallace树乘法器的设计和布局，并完成了16位MAC单元的设计。

Conclusion: Wallace树乘法器在优化电路深度和提高计算效率方面表现优异，项目成果展示了其实际应用潜力。

Abstract: Wallace tree multipliers are a parallel digital multiplier architecture
designed to minimize the worst-case time complexity of the circuit depth
relative to the input size [1]. In particular, it seeks to perform long
multiplication in the binary sense, reducing as many partial products per stage
as possible through full and half adders circuits, achieving O(log(n)) where n
= bit length of input. This paper provides an overview of the design, progress
and methodology in the final project of ECE 55900, consisting of the schematic
and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in
Cadence Virtuoso, as well as any design attempts prior to the final product.
This also includes our endeavors in designing the final MAC (Multiply
Accumulate) unit with undefined targets, which we chose to implement as a 16
bit combinational multiply-add.

</details>


### [53] [Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference](https://arxiv.org/abs/2509.09505)
*Haoran Wu,Can Xiao,Jiayi Nie,Xuan Guo,Binglei Lou,Jeffrey T. H. Wong,Zhiwen Mo,Cheng Zhang,Przemyslaw Forys,Wayne Luk,Hongxiang Fan,Jianyi Cheng,Timothy M. Jones,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: PLENA是一种硬件-软件协同设计的系统，通过三种核心优化途径解决长上下文LLM推理任务中的内存墙问题，显著提升了计算单元的利用率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AI代理在处理长上下文任务（如网页DOM或复杂工具调用轨迹）时，面临内存带宽和容量的限制，导致计算单元利用率低。

Method: PLENA采用硬件-软件协同设计，包括高效硬件实现（支持非对称量化）、扁平化脉动阵列架构（原生支持FlashAttention），以及完整的开发栈（自定义ISA、编译器、仿真器等）。

Result: 仿真结果显示，PLENA的计算单元利用率比现有加速器高8.5倍，吞吐量分别比A100 GPU和TPU v6e高2.24倍和3.85倍。

Conclusion: PLENA通过系统级优化显著提升了长上下文LLM推理任务的性能，并将开源。

Abstract: LLMs now form the backbone of AI agents for a diverse array of applications,
including tool use, command-line agents, and web or computer use agents. These
agentic LLM inference tasks are fundamentally different from chatbot-focused
inference -- they often have much larger context lengths to capture complex,
prolonged inputs, such as entire webpage DOMs or complicated tool call
trajectories. This, in turn, generates significant off-chip memory traffic for
the underlying hardware at the inference stage and causes the workload to be
constrained by two memory walls, namely the bandwidth and capacity memory
walls, preventing the on-chip compute units from achieving high utilization.
  In this paper, we introduce PLENA, a hardware-software co-designed system
that applies three core optimization pathways to tackle these challenges. PLENA
includes an efficient hardware implementation of compute and memory units
supporting an asymmetric quantization scheme. PLENA also features a novel
flattened systolic array architecture that has native support for
FlashAttention to tackle these memory walls in the scenario of inference
serving for long-context LLMs. Additionally, PLENA is developed with a complete
stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an
automated design space exploration flow. The simulated results show that PLENA
achieves up to 8.5x higher utilization than existing accelerators, and delivers
2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the
TPU v6e, under the same multiplier count and memory settings. The full PLENA
system will also be open-sourced.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [54] [Investigating Student Interaction Patterns with Large Language Model-Powered Course Assistants in Computer Science Courses](https://arxiv.org/abs/2509.08862)
*Chang Liu,Loc Hoang,Andrew Stolman,Rene F. Kizilcec,Bo Wu*

Main category: cs.CY

TL;DR: 论文研究了大型语言模型（LLM）作为课程助手的实际应用，发现其在填补学生非工作时间的学习支持空白方面有效，尤其是对初学者；但在高阶认知问题和互动策略上仍有局限。


<details>
  <summary>Details</summary>
Motivation: 高等教育中学生缺乏灵活及时的学术支持，尤其是在非工作时间。LLM有望填补这一空白，但缺乏教育者监督，需研究其实际应用效果。

Method: 在多个计算机科学课程中部署LLM助教系统，收集和分析2000名学生的互动数据，并手动标注200次对话/课程以评估回答质量。

Result: 系统在晚间和入门课程中使用率高，回答大多正确且有帮助，但较少提供示例；11%的对话包含LLM生成的高阶问题，但学生通常忽略。LLM生成高阶认知问题的能力有限。

Conclusion: LLM助教系统能有效填补支持空白，但需优化高阶问题生成和互动策略，建议教育者更多参与内容配置和政策制定。

Abstract: Providing students with flexible and timely academic support is a challenge
at most colleges and universities, leaving many students without help outside
scheduled hours. Large language models (LLMs) are promising for bridging this
gap, but interactions between students and LLMs are rarely overseen by
educators. We developed and studied an LLM-powered course assistant deployed
across multiple computer science courses to characterize real-world use and
understand pedagogical implications. By Spring 2024, our system had been
deployed to approximately 2,000 students across six courses at three
institutions. Analysis of the interaction data shows that usage remains strong
in the evenings and nights and is higher in introductory courses, indicating
that our system helps address temporal support gaps and novice learner needs.
We sampled 200 conversations per course for manual annotation: most sampled
responses were judged correct and helpful, with a small share unhelpful or
erroneous; few responses included dedicated examples. We also examined an
inquiry-based learning strategy: only around 11% of sampled conversations
contained LLM-generated follow-up questions, which were often ignored by
students in advanced courses. A Bloom's taxonomy analysis reveals that current
LLM capabilities are limited in generating higher-order cognitive questions.
These patterns suggest opportunities for pedagogically oriented LLM-based
educational systems and greater educator involvement in configuring prompts,
content, and policies.

</details>


### [55] [Towards Trustworthy AI: Characterizing User-Reported Risks across LLMs "In the Wild"](https://arxiv.org/abs/2509.08912)
*Lingyao Li,Renkai Ma,Zhaoqian Xue,Junjie Xiong*

Main category: cs.CY

TL;DR: 研究分析了Reddit上关于七种大型语言模型（LLM）聊天机器人的讨论，使用NIST的AI风险管理框架发现用户报告的风险分布不均且平台特定，揭示了系统研究与用户实际经历之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中在实验室环境或单一风险上，缺乏对用户实际使用中面临的多风险的全面理解。

Method: 通过NIST的AI风险管理框架，分析了Reddit上关于七种LLM聊天机器人的在线讨论。

Result: 用户报告的风险分布不均且平台特定，不同产品有独特的“风险指纹”，常见风险和罕见风险的表现形式不同。

Conclusion: 研究揭示了系统研究与用户实际经历之间的差距，强调了需要以用户为中心的方法来支持日常使用。

Abstract: While Large Language Models (LLMs) are rapidly integrating into daily life,
research on their risks often remains lab-based and disconnected from the
problems users encounter "in the wild." While recent HCI research has begun to
explore these user-facing risks, it typically concentrates on a singular LLM
chatbot like ChatGPT or an isolated risk like privacy. To gain a holistic
understanding of multi-risk across LLM chatbots, we analyze online discussions
on Reddit around seven major LLM chatbots through the U.S. NIST's AI Risk
Management Framework. We find that user-reported risks are unevenly distributed
and platform-specific. While "Valid and Reliable" risk is the most frequently
mentioned, each product also exhibits a unique "risk fingerprint;" for
instance, user discussions associate GPT more with "Safe" and "Fair" issues,
Gemini with "Privacy," and Claude with "Secure and Resilient" risks.
Furthermore, the nature of these risks differs by their prevalence: less
frequent risks like "Explainability" and "Privacy" manifest as nuanced user
trade-offs, more common ones like "Fairness" are experienced as direct personal
harms. Our findings reveal gaps between risks reported by system-centered
studies and by users, highlighting the need for user-centered approaches that
support users in their daily use of LLM chatbots.

</details>


### [56] [Incorporating AI Incident Reporting into Telecommunications Law and Policy: Insights from India](https://arxiv.org/abs/2509.09508)
*Avinash Agarwal,Manisha J. Nene*

Main category: cs.CY

TL;DR: 本文探讨了AI在电信基础设施中的风险，提出了AI事件的分类和监管建议。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全和数据保护框架无法涵盖AI带来的新风险，如算法偏见和不可预测行为。

Method: 通过对印度数字法规的分析，揭示了当前法律在AI事件监管上的不足。

Result: 研究发现现有法律存在监管空白，并提出了针对性政策建议。

Conclusion: 建议整合AI事件报告至电信治理中，以增强监管清晰度和长期韧性。

Abstract: The integration of artificial intelligence (AI) into telecommunications
infrastructure introduces novel risks, such as algorithmic bias and
unpredictable system behavior, that fall outside the scope of traditional
cybersecurity and data protection frameworks. This paper introduces a precise
definition and a detailed typology of telecommunications AI incidents,
establishing them as a distinct category of risk that extends beyond
conventional cybersecurity and data protection breaches. It argues for their
recognition as a distinct regulatory concern. Using India as a case study for
jurisdictions that lack a horizontal AI law, the paper analyzes the country's
key digital regulations. The analysis reveals that India's existing legal
instruments, including the Telecommunications Act, 2023, the CERT-In Rules, and
the Digital Personal Data Protection Act, 2023, focus on cybersecurity and data
breaches, creating a significant regulatory gap for AI-specific operational
incidents, such as performance degradation and algorithmic bias. The paper also
examines structural barriers to disclosure and the limitations of existing AI
incident repositories. Based on these findings, the paper proposes targeted
policy recommendations centered on integrating AI incident reporting into
India's existing telecom governance. Key proposals include mandating reporting
for high-risk AI failures, designating an existing government body as a nodal
agency to manage incident data, and developing standardized reporting
frameworks. These recommendations aim to enhance regulatory clarity and
strengthen long-term resilience, offering a pragmatic and replicable blueprint
for other nations seeking to govern AI risks within their existing sectoral
frameworks.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [57] [Toward a Multi-Echelon Cyber Warfare Theory: A Meta-Game-Theoretic Paradigm for Defense and Dominance](https://arxiv.org/abs/2509.08976)
*Ya-Ting Yang,Quanyan Zhu*

Main category: cs.GT

TL;DR: 该论文探讨了网络战争在现代冲突中的核心地位，提出使用博弈论和AI技术来整合防御与进攻策略，并通过案例RedCyber展示了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 网络战争已成为现代多领域作战的核心部分，但目前的研究多集中于孤立战术或技术，缺乏整体视角。博弈论和AI的融合提供了新的解决方案。

Method: 结合博弈论框架和现代AI技术，分析攻防交互、均衡状态和风险，设计并优化从政策到技术实施的多层次网络战争策略。

Result: 通过案例RedCyber证明，博弈论方法能够有效捕捉网络操作的相互依赖性，并揭示冲突中资源与优势的非线性关系。

Conclusion: 论文指出未来研究方向包括网络韧性、跨层级规划以及AI在网络战争中的演化作用。

Abstract: Cyber warfare has become a central element of modern conflict, especially
within multi-domain operations. As both a distinct and critical domain, cyber
warfare requires integrating defensive and offensive technologies into coherent
strategies. While prior research has emphasized isolated tactics or fragmented
technologies, a holistic understanding is essential for effective resource
deployment and risk mitigation. Game theory offers a unifying framework for
this purpose. It not only models attacker-defender interactions but also
provides quantitative tools for equilibrium analysis, risk assessment, and
strategic reasoning. Integrated with modern AI techniques, game-theoretic
models enable the design and optimization of strategies across multiple levels
of cyber warfare, from policy and strategy to operations, tactics, and
technical implementations. These models capture the paradoxical logic of
conflict, where more resources do not always translate into greater advantage,
and where nonlinear dynamics govern outcomes. To illustrate the approach, this
chapter examines RedCyber, a synthetic cyber conflict, demonstrating how
game-theoretic methods capture the interdependencies of cyber operations. The
chapter concludes with directions for future research on resilience,
cros-echelon planning, and the evolving role of AI in cyber warfare.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [58] [The Sound of Entanglement](https://arxiv.org/abs/2509.08892)
*Enar de Dios Rodríguez,Philipp Haslinger,Johannes Kofler,Richard Kueng,Benjamin Orthner,Alexander Ploier,Martin Ringbauer,Clemens Wenger*

Main category: quant-ph

TL;DR: 论文探讨量子力学与艺术的交叉，提出一种基于量子纠缠和随机性的实时音乐表演。


<details>
  <summary>Details</summary>
Motivation: 通过量子现象与艺术的结合，增强对量子力学的理解并扩展创意表达的边界。

Method: 利用贝尔测试中的纠缠光子实时测量数据驱动音乐和视觉表演。

Result: 创建了一种独特的、不可重复的视听体验，完全依赖量子相关性。

Conclusion: 科学与艺术的融合为量子现象提供了新的展示方式，同时也拓展了艺术表达的可能性。

Abstract: The advent of quantum physics has revolutionized our understanding of the
universe, replacing the deterministic framework of classical physics with a
paradigm dominated by intrinsic randomness and quantum correlations. This shift
has not only enabled groundbreaking technologies, such as quantum sensors,
networks and computers, but has also unlocked entirely new possibilities for
artistic expressions. In this paper, we explore the intersection of quantum
mechanics and art, focusing on the use of quantum entanglement and inherent
randomness as creative tools. Specifically, we present The Sound of
Entanglement, a live musical performance driven by real-time measurements of
entangled photons in a Bell test. By integrating the measured quantum
correlations as a central compositional element and synchronizing live visuals
with experimental data, the performance offers a unique and unrepeatable
audiovisual experience that relies on quantum correlations which cannot be
produced by any classical device. Through this fusion of science and art, we
aim to provide a deeper appreciation of quantum phenomena while expanding the
boundaries of creative expression.

</details>


### [59] [Towards A High-Performance Quantum Data Center Network Architecture](https://arxiv.org/abs/2509.09653)
*Yufeng Xin,Liang Zhang*

Main category: quant-ph

TL;DR: 论文提出了一种三层胖树网络架构，用于解决量子数据中心（QDCs）中的网络可扩展性、纠缠生成和量子存储器管理问题。


<details>
  <summary>Details</summary>
Motivation: 大规模量子计算机面临技术和财务限制，模块化量子解决方案需要克服网络扩展性和量子资源管理的挑战。

Method: 采用三层胖树网络架构，设计了独特的叶交换机和高级交换脊交换机，并引入了队列调度机制来高效管理量子存储器。

Result: 通过排队理论和NetSquid模拟，验证了该架构的可扩展性，并能维持高纠缠保真度。

Conclusion: 该架构为模块化量子数据中心网络提供了实用的发展方向。

Abstract: Quantum Data Centers (QDCs) are needed to support large-scale quantum
processing for both academic and commercial applications. While large-scale
quantum computers are constrained by technological and financial barriers, a
modular approach that clusters small quantum computers offers an alternative.
This approach, however, introduces new challenges in network scalability,
entanglement generation, and quantum memory management. In this paper, we
propose a three-layer fat-tree network architecture for QDCs, designed to
address these challenges. Our architecture features a unique leaf switch and an
advanced swapping spine switch design, optimized to handle high volumes of
entanglement requests as well as a queue scheduling mechanism that efficiently
manages quantum memory to prevent decoherence. Through queuing-theoretical
models and simulations in NetSquid, we demonstrate the proposed architecture's
scalability and effectiveness in maintaining high entanglement fidelity,
offering a practical path forward for modular QDC networks.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [60] [HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework](https://arxiv.org/abs/2509.08971)
*Julien Loiseau,Hyun Lim,Andrés Yagüe López,Mammadbaghir Baghirzade,Shihab Shahriar Khan,Yoonsoo Kim,Sudarshan Neopane,Alexander Strack,Farhana Taiyebah,Benjamin K. Bergen*

Main category: physics.comp-ph

TL;DR: HARD 是一个开源的高性能流体动力学模拟工具，支持辐射扩散耦合，基于 FleCSI 框架，具备多后端运行时支持，并通过 Kokkos 实现跨平台高效运行。附带验证测试和社区开发支持。


<details>
  <summary>Details</summary>
Motivation: 为辐射流体动力学研究提供一个高性能、可移植且科学可靠的模拟平台。

Method: 基于 FleCSI 框架，使用任务并行计算单元，支持多种后端运行时（如 Legion、MPI、HPX），并通过 Kokkos 实现节点级并行。

Result: 实现了跨平台高效运行（从笔记本电脑到超级计算机），并内置了验证测试套件确保科学可靠性。

Conclusion: HARD 是一个可持续的平台，为多领域辐射流体动力学研究提供了高性能、可验证的解决方案。

Abstract: Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application
for high-performance simulations of compressible hydrodynamics with
radiation-diffusion coupling. Built on the FleCSI (Flexible Computational
Science Infrastructure) framework, HARD expresses its computational units as
tasks whose execution can be orchestrated by multiple back-end runtimes,
including Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos,
providing a single, portable code base that runs efficiently on laptops, small
homogeneous clusters, and the largest heterogeneous supercomputers currently
available. To ensure scientific reliability, HARD includes a regression-test
suite that automatically reproduces canonical verification problems such as the
Sod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical
solutions against known analytical results. The project is distributed under an
OSI-approved license, hosted on GitHub, and accompanied by reproducible build
scripts and continuous integration workflows. This combination of performance
portability, verification infrastructure, and community-focused development
makes HARD a sustainable platform for advancing radiation hydrodynamics
research across multiple domains.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [61] [6G Resilience -- White Paper](https://arxiv.org/abs/2509.09005)
*Hirley Alves,Nurul H. Mahmood,Onel L. A. López,Sumudu Samarakoon,Seppo Yrjölä,Matti Latva-Aho,Markku Juntti,Ari Pouttu,Armin Dekorsy,Arthur Sousa de Sena,Aydin Sezgin,Bho Matthiesen,Chafika Benzaid,Chathuranga Weeraddana,David Hutchison,Dileepa Marasinghe,Doganalp Ergenc,Eduard Jorswieck,Erkki Harjula,Falko Dressler,Harri Saarnisaari,Italo Atzeni,Jaap Van De Beek,Jacek Rak,Konstantin Mikhaylov,Lauri Loven,Madhusanka Liyanage,Marcos Katz,Marja Matinmikko-Blue,Mehdi Rasti,Mika Ylianttila Nhan Nguyen,Pawani Porambage,Petar Popovski,Petri Ahokangas,Premanandana Rajatheva,Robert-Jeron Reifert,Tharaka Hewa,Tommy Svensson*

Main category: eess.SP

TL;DR: 6G网络设计需以韧性为核心目标之一，结合可持续性和效率，涵盖技术、架构和经济多维度。通过分析与其他关键系统的依赖关系，提出3R框架（可靠性、鲁棒性、韧性），并转化为可衡量的能力。


<details>
  <summary>Details</summary>
Motivation: 移动网络从效率优先转向可持续性导向，促使韧性成为6G设计的主要目标，以确保在网络复杂中断中持续运行。

Method: 采用3R框架（可靠性、鲁棒性、韧性），并设计边缘原生、本地感知架构，结合AI原生控制、零信任安全等技术手段，以及开放平台和生态系统策略。

Result: 提出了具体的韧性能力（如优雅降级、快速重构）和架构设计（如多层面多样性），并从技术和经济角度探讨了韧性的实现路径。

Conclusion: 白皮书为6G韧性发展提供了初步框架，旨在激发研究与实践，推动6G在复杂环境中的适应与进化能力。

Abstract: 6G must be designed to withstand, adapt to, and evolve amid prolonged,
complex disruptions. Mobile networks' shift from efficiency-first to
sustainability-aware has motivated this white paper to assert that resilience
is a primary design goal, alongside sustainability and efficiency, encompassing
technology, architecture, and economics. We promote resilience by analysing
dependencies between mobile networks and other critical systems, such as
energy, transport, and emergency services, and illustrate how cascading
failures spread through infrastructures. We formalise resilience using the 3R
framework: reliability, robustness, resilience. Subsequently, we translate this
into measurable capabilities: graceful degradation, situational awareness,
rapid reconfiguration, and learning-driven improvement and recovery.
  Architecturally, we promote edge-native and locality-aware designs, open
interfaces, and programmability to enable islanded operations, fallback modes,
and multi-layer diversity (radio, compute, energy, timing). Key enablers
include AI-native control loops with verifiable behaviour, zero-trust security
rooted in hardware and supply-chain integrity, and networking techniques that
prioritise critical traffic, time-sensitive flows, and inter-domain
coordination.
  Resilience also has a techno-economic aspect: open platforms and high-quality
complementors generate ecosystem externalities that enhance resilience while
opening new markets. We identify nine business-model groups and several
patterns aligned with the 3R objectives, and we outline governance and
standardisation. This white paper serves as an initial step and catalyst for 6G
resilience. It aims to inspire researchers, professionals, government
officials, and the public, providing them with the essential components to
understand and shape the development of 6G resilience.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: FG-FARL是一种离线强化学习方法，通过校准每个组的安全阈值来减少伤害并实现跨保护子组的公平性。在医疗补助人群健康管理数据中，其表现与基线方法相当，同时改善了公平性指标。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过FG-FARL方法减少离线强化学习中的不公平性和潜在伤害，特别是在健康管理等领域。

Method: FG-FARL采用可行性引导的自适应方法，校准每个子组的安全阈值，并与行为克隆（BC）和HACO方法进行比较。

Result: FG-FARL在价值和公平性指标上与基线方法表现相当，同时显著减少了子组间的差异。

Conclusion: FG-FARL为更安全和公平的决策支持提供了一种实用方法。

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


### [63] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: 论文提出了一种称为ProDiGy的拜占庭鲁棒联邦学习算法，通过双评分系统评估客户端梯度，有效抵御异构数据下的对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然广泛研究，但在数据异构性下易受对抗攻击，需要更鲁棒的防御机制。

Method: 采用基于梯度邻近性和差异性的联合双评分系统评估客户端梯度。

Result: 实验显示ProDiGy在非IID数据场景下优于现有防御方法，保持高模型准确性和防御能力。

Conclusion: 双视角方法通过促进诚实客户端的自然相似性和检测可疑一致性，有效抵御攻击。

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [64] [Uniformity within Parameterized Circuit Classes](https://arxiv.org/abs/2509.09657)
*Steef Hegeman,Jan Martens,Alfons Laarman*

Main category: cs.CC

TL;DR: 研究了参数化布尔电路族的均匀性条件，定义了三种均匀性版本并证明它们在特定电路类中的等价性，方便验证浅层参数化电路类的均匀性。


<details>
  <summary>Details</summary>
Motivation: 探讨参数化电路族均匀性的技术挑战，填补参数化电路复杂性中均匀性研究的空白。

Method: 形式化定义了线性均匀性、对数时间均匀性和FO均匀性的参数化版本，并证明它们在特定电路类中的等价性。

Result: 证明了这些均匀性条件在$	ext{para-}	extsf{AC}^0$和$	ext{para-}	extsf{AC}^{0	extsuperscript{\uparrow}}$中的一致性。

Conclusion: 为浅层参数化电路类的均匀性验证提供了便利，支持了文献中关于均匀性的主张。

Abstract: We study uniformity conditions for parameterized Boolean circuit families.
Uniformity conditions require that the infinitely many circuits in a circuit
family are in some sense easy to construct from one shared description. For
shallow circuit families, logtime-uniformity is often desired but quite
technical to prove. Despite that, proving it is often left as an exercise for
the reader -- even for recently introduced classes in parameterized circuit
complexity, where uniformity conditions have not yet been explicitly studied.
We formally define parameterized versions of linear-uniformity,
logtime-uniformity, and FO-uniformity, and prove that these result in
equivalent complexity classes when imposed on $\text{para-}\textsf{AC}^0$ and
$\text{para-}\textsf{AC}^{0\uparrow}$. Overall, we provide a convenient way to
verify uniformity for shallow parameterized circuit classes, and thereby
substantiate claims of uniformity in the literature.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [65] [An Integrated Open Source Software System for the Generation and Analysis of Subject-Specific Blood Flow Simulation Ensembles](https://arxiv.org/abs/2509.09392)
*Simon Leistikow,Thomas Miro,Adrian Kummerländer,Ali Nahardani,Katja Grün,Markus Franz,Verena Hoerr,Mathias J. Krause,Lars Linsen*

Main category: physics.med-ph

TL;DR: 开发了一种交互式、可定制的可视化工具，结合了CFD和MRI技术，用于血流动力学分析和模拟。


<details>
  <summary>Details</summary>
Motivation: 血流动力学分析对心血管疾病诊断至关重要，需要结合MRI和CFD技术进行更精确的分析。

Method: 提出了一种开源工具，支持多种参数模拟集合的生成，并通过二维嵌入相似性空间进行可视化分析。

Result: 工具在三个实际案例中验证了其有效性，提高了血流动力学参数分析的准确性。

Conclusion: 结合CFD和MRI的优势，工具为血流动力学参数提供了更全面的理解。

Abstract: Background and Objective: Hemodynamic analysis of blood flow through arteries
and veins is critical for diagnosing cardiovascular diseases, such as aneurysms
and stenoses, and for investigating cardiovascular parameters, such as
turbulence and wall shear stress. For subject-specific analyses, the anatomy
and blood flow of the subject can be captured non-invasively using structural
and 4D Magnetic Resonance Imaging (MRI). Computational Fluid Dynamics (CFD), on
the other hand, can be used to generate blood flow simulations by solving the
Navier-Stokes equations. To generate and analyze subject-specific blood flow
simulations, MRI and CFD have to be brought together.
  Methods: We present an interactive, customizable, and user-oriented visual
analysis tool that assists researchers in both medicine and numerical analysis.
Our open-source tool is applicable to domains such as CFD and MRI, and it
facilitates the analysis of simulation results and medical data, especially in
hemodynamic studies. It enables the creation of simulation ensembles with a
high variety of parameters. Furthermore, it allows for the visual and
analytical examination of simulations and measurements through 2D embeddings of
the similarity space.
  Results: To demonstrate the effectiveness of our tool, we applied it to three
real-world use cases, showcasing its ability to configure simulation ensembles
and analyse blood flow dynamics. We evaluated our example cases together with
MRI and CFD experts to further enhance features and increase the usability.
  Conclusions: By combining the strengths of both CFD and MRI, our tool
provides a more comprehensive understanding of hemodynamic parameters,
facilitating more accurate analysis of hemodynamic biomarkers.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [66] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出了一种名为OSIM的新型评估指标，专注于3D场景中的“物体”，以更符合人类视觉感知的方式进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标主要关注整体图像质量，与人类感知存在差异。作者受神经心理学启发，认为人类对3D场景的识别主要基于对单个物体的关注。

Method: 通过利用物体检测模型及其特征表示，OSIM量化场景中每个物体的“物体性”，从而实现以物体为中心的评估。

Result: 用户研究表明，OSIM比现有指标更符合人类感知。此外，作者还通过多种方法分析了OSIM的特性，并重新评估了最近的3D重建和生成模型。

Conclusion: OSIM为3D场景评估提供了一种更贴近人类感知的新方法，有助于推动相关领域的发展。

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [67] [Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles](https://arxiv.org/abs/2509.09349)
*Ian Nell,Shane Gilroy*

Main category: cs.CV

TL;DR: 摘要介绍了一种基于计算机视觉的新型驾驶行为分类系统，用于检测驾驶员的分心和受损行为，具有较高的可靠性和适应性。


<details>
  <summary>Details</summary>
Motivation: 道路交通事故是全球性问题，驾驶员分心和受损是主要原因之一。

Method: 采用计算机视觉技术（如YOLO目标检测模型和自定义车道估计算法），实时跟踪对象、分析横向位移和监控车道位置。

Result: 系统能有效识别不安全驾驶行为（如过度横向移动和不规则轨迹），并在多样化的视频数据集中表现可靠。

Conclusion: 这种基于视觉的系统可以分析非联网车辆的驾驶行为，潜力巨大。

Abstract: Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behavior classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviors such as excessive lateral movement and erratic trajectory patterns by
implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioral analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 提出了一种将游戏设计文档（GDD）通过NLP和多模态大语言模型（LLMs）转化为Unity游戏原型的端到端框架，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助游戏开发中从设计到实现的关键问题，利用LLMs简化流程。

Method: 结合微调的LLaMA-3模型和自定义Unity集成包，解析GDD并生成Unity兼容的C#代码。

Result: 微调的模型在多项指标上表现优异（4.8/5.0平均分），生成模板高度符合GDD规范。

Conclusion: 系统成功填补了AI辅助游戏开发的空白，展示了LLMs在游戏开发中的实用性。

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [69] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 论文比较了人类、LLMs和贝叶斯代理在动态谈判环境中的表现和行为差异，发现不同代理在性能和谈判方式上有显著差别。


<details>
  <summary>Details</summary>
Motivation: 随着人类协调任务越来越多地由自主代理执行，评估这些代理的性能和谈判过程变得至关重要。

Method: 在动态谈判环境中，比较人类（216人）、LLMs（GPT-4o、Gemini 1.5 Pro）和贝叶斯代理的表现和行为。

Result: 贝叶斯代理通过激进优化实现最高盈余，但拒绝频繁；人类和LLMs总体盈余相近，但LLMs倾向保守策略，人类更具战略性和公平性。

Conclusion: 性能平等可能掩盖过程和一致性上的根本差异，这对实际部署至关重要。

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [70] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 研究探讨了空间协调的三个维度（探索多样性、运动专业化和自适应空间接近度）如何影响团队在限制显性沟通的协作任务中的表现，结果发现空间专业化对绩效有正面影响，自适应空间接近度呈倒U型关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解在缺乏视觉线索和显性沟通的环境中（如紧急救援、军事行动等），团队如何通过空间协调来高效协作。

Method: 通过分析34个四人团队（共136名参与者）在搜索救援任务中的运动数据和协调模式，测量空间接近度、分布模式和运动对齐等指标。

Result: 结果显示空间专业化正向预测绩效，自适应空间接近度与绩效呈倒U型关系，且这些指标的动态变化能区分高低绩效团队。

Conclusion: 研究为角色化团队中的隐式空间协调提供了新见解，强调了平衡自适应策略的重要性，并对训练和AI辅助团队支持系统有启发意义。

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [A Cyber-Twin Based Honeypot for Gathering Threat Intelligence](https://arxiv.org/abs/2509.09222)
*Muhammad Azmi Umer,Zhan Xuna,Yan Lin Aung,Aditya P. Mathur,Jianying Zhou*

Main category: cs.CR

TL;DR: 该论文提出了一种基于网络孪生的蜜罐系统，用于保护水处理厂等关键基础设施免受网络攻击。通过记录和分析攻击行为，提供威胁情报。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施易受网络攻击，需要开发有效的保护技术。

Method: 采用基于网络孪生的蜜罐技术，模拟真实水处理厂环境吸引攻击者。

Result: 蜜罐成功运行并记录了多次攻击（如勒索软件攻击），为威胁情报提供了数据。

Conclusion: 该蜜罐系统可帮助水处理厂改进防护措施，提升安全性。

Abstract: Critical Infrastructure (CI) is prone to cyberattacks. Several techniques
have been developed to protect CI against such attacks. In this work, we
describe a honeypot based on a cyber twin for a water treatment plant. The
honeypot is intended to serve as a realistic replica of a water treatment plant
that attracts potential attackers. The attacks launched on the honeypot are
recorded and analyzed for threat intelligence. The intelligence so obtained is
shared with the management of water treatment plants, who in turn may use it to
improve plant protection systems. The honeypot used here is operational and has
been attacked on several occasions using, for example, a ransomware attack that
is described in detail.

</details>


### [72] [What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection](https://arxiv.org/abs/2509.09291)
*Biwei Yan,Yue Zhang,Minghui Xu,Runyu Pan,Jinku Li,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 本文提出VerifiaBLE系统，利用大型语言模型（LLM）将BLE代码转换为可验证的形式模型，以规模化检查安全性。


<details>
  <summary>Details</summary>
Motivation: 解决BLE应用层因开发者忽视加密、认证等保护措施而引发的安全问题，同时降低形式化验证的手动建模成本。

Method: 结合静态分析、LLM翻译和符号验证，将BLE代码翻译为ProVerif可验证的模型，检查加密、随机性和认证。

Result: 在1,050个Android BLE应用中发现仅10.2%实现全部保护，53.9%完全未实现。

Conclusion: LLM作为结构化翻译器能规模化应用形式化方法，提升安全领域的验证效率。

Abstract: The application layer of Bluetooth Low Energy (BLE) is a growing source of
security vulnerabilities, as developers often neglect to implement critical
protections such as encryption, authentication, and freshness. While formal
verification offers a principled way to check these properties, the manual
effort of constructing formal models makes it impractical for large-scale
analysis. This paper introduces a key insight: BLE application security
analysis can be reframed as a semantic translation problem, i.e., from
real-world code to formal models. We leverage large language models (LLMs) not
to directly detect vulnerabilities, but to serve as translators that convert
BLE-specific code into process models verifiable by tools like ProVerif. We
implement this idea in VerifiaBLE, a system that combines static analysis,
prompt-guided LLM translation, and symbolic verification to check three core
security features: encryption, randomness, and authentication. Applied to 1,050
Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\% of apps
implement all three protections, while 53.9\% omit them entirely. Our work
demonstrates that using LLMs as structured translators can lower the barrier to
formal methods, unlocking scalable verification across security-critical
domains.

</details>


### [73] [CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype](https://arxiv.org/abs/2509.09638)
*Amitabh Chakravorty,Jess Kropczynski,Nelly Elsayed*

Main category: cs.CR

TL;DR: 这篇论文介绍了一个名为CryptoGuard的AI驱动安全仪表板原型，旨在帮助用户监控加密钱包活动并识别可疑行为。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币的普及，加密劫持成为用户的安全威胁，需要一种直观的工具帮助非技术用户应对。

Method: 通过用户中心设计流程，基于Figma构建高保真交互原型，模拟用户操作并提供视觉警报等功能。

Result: 原型展示了如何结合AI功能与用户界面设计，帮助用户快速决策并应对威胁。

Conclusion: 实用的安全工具不仅需要后端功能，还需要用户中心设计来有效沟通风险并赋能用户。

Abstract: With the widespread adoption of cryptocurrencies, cryptojacking has become a
significant security threat to crypto wallet users. This paper presents a
front-end prototype of an AI-powered security dashboard, namely, CryptoGuard.
Developed through a user-centered design process, the prototype was constructed
as a high-fidelity, click-through model from Figma mockups to simulate key user
interactions. It is designed to assist users in monitoring their login and
transaction activity, identifying any suspicious behavior, and enabling them to
take action directly within the wallet interface. The dashboard is designed for
a general audience, prioritizing an intuitive user experience for non-technical
individuals. Although its AI functionality is conceptual, the prototype
demonstrates features like visual alerts and reporting. This work is positioned
explicitly as a design concept, bridging cryptojacking detection research with
human-centered interface design. This paper also demonstrates how usability
heuristics can directly inform a tool's ability to support rapid and confident
decision-making under real-world threats. This paper argues that practical
security tools require not only robust backend functionality but also a
user-centric design that communicates risk and empowers users to take
meaningful action.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [74] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

TL;DR: 论文提出了一种利用GPT零样本能力从论坛介绍帖子中推断大五人格特质的模型，并将其集成到SAMI的社交推荐系统中，以提升匹配质量。


<details>
  <summary>Details</summary>
Motivation: 在线课程环境中社交连接的缺失影响了学习效果，而现有系统SAMI因缺乏对学生人格的理解，限制了其推荐效果。

Method: 研究开发了一个基于GPT零样本能力的人格检测模型，用于从论坛帖子中推断大五人格特质，并将其集成到SAMI的匹配系统中。

Result: 模型在人格特质推断任务中表现出色，初步集成显示人格特质能补充现有匹配因素。

Conclusion: 人格特质可提升社交推荐系统的匹配质量，但需进一步评估其对学生的具体影响。

Abstract: Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [75] [Time-Fair Benchmarking for Metaheuristics: A Restart-Fair Protocol for Fixed-Time Comparisons](https://arxiv.org/abs/2509.08986)
*Junbo Jacob Lian*

Main category: cs.NE

TL;DR: 论文主张使用实际计算时间而非函数评估次数作为算法性能比较的主要标准，提出固定时间的公平评测协议，并推荐使用随时性能曲线和预期运行时间等指标。


<details>
  <summary>Details</summary>
Motivation: 当前元启发式算法评测中常忽视实际计算时间的差异，仅依赖函数评估次数可能导致不公平比较。

Method: 提出固定时间的评测协议，允许算法充分利用重启和自适应机制，并引入标准化报告清单以减少隐藏的计算开销。

Result: 该方法提升了评测的可信度和实际相关性。

Conclusion: 基于实际时间的评测方法能更公平、准确地评估元启发式算法的性能。

Abstract: Numerous purportedly improved metaheuristics claim superior performance based
on equivalent function evaluations (FEs), yet often conceal additional
computational burdens in more intensive iterations, preprocessing stages, or
hyperparameter tuning. This paper posits that wall-clock time, rather than
solely FEs, should serve as the principal budgetary constraint for equitable
comparisons. We formalize a fixed-time, restart-fair benchmarking protocol
wherein each algorithm is allotted an identical wall-clock time budget per
problem instance, permitting unrestricted utilization of restarts, early
termination criteria, and internal adaptive mechanisms. We advocate for the
adoption of anytime performance curves, expected running time (ERT) metrics,
and performance profiles that employ time as the cost measure, all aimed at
predefined targets. Furthermore, we introduce a concise, reproducible checklist
to standardize reporting practices and mitigate undisclosed computational
overheads. This approach fosters more credible and practically relevant
evaluations of metaheuristic algorithms.

</details>
