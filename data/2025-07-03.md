<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 14]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [math.NA](#math.NA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Is It Safe To Learn And Share? On Psychological Safety and Social Learning in (Agile) Communities of Practice](https://arxiv.org/abs/2507.01065)
*Christiaan Verwijs,Evelien Acun-Roos,Daniel Russo*

Main category: cs.SE

TL;DR: 研究发现，在敏捷社区的在线互动中，心理安全感显著低于面对面环境，影响了参与意愿和风险规避行为。


<details>
  <summary>Details</summary>
Motivation: 随着混合、分布式和异步工作模式的普及，敏捷软件开发中的持续学习变得更重要，但心理安全感在这些环境中的作用尚不明确。

Method: 采用混合方法，通过143名参与者的调查数据和主题分析，研究敏捷社区中的心理安全感问题。

Result: 结果显示，心理安全感在在线环境中较低，且与性别、社区资历或内容创作活动无关，但在角色和年龄组中存在差异。

Conclusion: 建议通过明确规范、结构化引导和主动干预来提升心理安全感，研究结果为虚拟或混合社区提供了实用指导。

Abstract: As hybrid, distributed, and asynchronous work models become more prevalent,
continuous learning in Agile Software Development (ASD) gains renewed
importance. Communities of Practice (CoPs) are increasingly adopted to support
social learning beyond formal education, often relying on virtual
communication. Psychological safety, a prerequisite for effective learning,
remains insufficiently understood in these settings. This mixed-methods study
investigates psychological safety within Agile CoPs through survey data from
143 participants. Results indicate that psychological safety is significantly
lower in online interactions compared to face-to-face settings. Moreover, low
psychological safety reduces participants' intent to continue contributing and
avoidance of interpersonal risk. No significant differences emerged based on
gender, community seniority, or content creation activity. However, differences
by role and age group suggest potential generational or role-related effects.
Thematic analysis revealed exclusionary behavior, negative interaction
patterns, and hostility as primary threats to psychological safety, often
reinforced by tribalism and specific community dynamics. Suggested
interventions include establishing explicit norms, structured facilitation, and
active moderation. The findings were validated through member checking with 30
participants. This study provides a comparative perspective on interaction
modalities and offers practical guidance for organizers seeking to cultivate
inclusive, high-impact CoPs and similarly structured virtual or hybrid work
environments.

</details>


### [2] [Bugs in the Shadows: Static Detection of Faulty Python Refactorings](https://arxiv.org/abs/2507.01103)
*Jonhnanthan Oliveira,Rohit Gheyi,Márcio Ribeiro,Alessandro Garcia*

Main category: cs.SE

TL;DR: 该研究提出了一种静态分析技术，用于检测Python重构过程中引入的类型错误，并在实际项目中验证了其有效性，发现了现有工具的缺陷。


<details>
  <summary>Details</summary>
Motivation: Python的动态类型系统给自动化重构带来了挑战，可能导致类型错误，影响软件可靠性和开发效率。

Method: 提出了一种静态分析技术，并在Rope重构实现中进行了评估，应用于开源Python项目。

Result: 在1,152次重构尝试中，发现了29个错误，部分问题也存在于流行的IDE中。部分问题已被开发者认可和修复。

Conclusion: 现有Python重构工具需要提高鲁棒性，以确保代码转换的正确性和软件的可靠维护。

Abstract: Python is a widely adopted programming language, valued for its simplicity
and flexibility. However, its dynamic type system poses significant challenges
for automated refactoring - an essential practice in software evolution aimed
at improving internal code structure without changing external behavior.
Understanding how type errors are introduced during refactoring is crucial, as
such errors can compromise software reliability and reduce developer
productivity. In this work, we propose a static analysis technique to detect
type errors introduced by refactoring implementations for Python. We evaluated
our technique on Rope refactoring implementations, applying them to open-source
Python projects. Our analysis uncovered 29 bugs across four refactoring types
from a total of 1,152 refactoring attempts. Several of these issues were also
found in widely used IDEs such as PyCharm and PyDev. All reported bugs were
submitted to the respective developers, and some of them were acknowledged and
accepted. These results highlight the need to improve the robustness of current
Python refactoring tools to ensure the correctness of automated code
transformations and support reliable software maintenance.

</details>


### [3] [Context-Aware Code Wiring Recommendation with LLM-based Agent](https://arxiv.org/abs/2507.01315)
*Taiming Wang,Yanjie Jiang,Chunhao Dong,Yuxia Zhang,Hui Liu*

Main category: cs.SE

TL;DR: 论文介绍了WIRL，一种基于LLM的代码接线代理，用于解决代码粘贴中的变量适配问题，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 代码粘贴是常见实践，但现有方法未能有效利用上下文信息，导致适配效果不佳。

Method: WIRL结合LLM、定制工具和编排模块，采用混合策略进行确定性规则和智能探索的变量代换。

Result: WIRL的精确匹配精度为91.7%，召回率为90.0%，远超LLMs和IntelliJ IDEA。

Conclusion: WIRL为现代IDE提供了更智能的上下文感知开发辅助工具。

Abstract: Copy-paste-modify is a widespread and pragmatic practice in software
development, where developers adapt reused code snippets, sourced from
platforms such as Stack Overflow, GitHub, or LLM outputs, into their local
codebase. A critical yet underexplored aspect of this adaptation is code
wiring, which involves substituting unresolved variables in the pasted code
with suitable ones from the surrounding context. Existing solutions either rely
on heuristic rules or historical templates, often failing to effectively
utilize contextual information, despite studies showing that over half of
adaptation cases are context-dependent. In this paper, we introduce WIRL, an
LLM-based agent for code wiring framed as a Retrieval-Augmented Generation
(RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an
orchestration module to identify unresolved variables, retrieve context, and
perform context-aware substitutions. To balance efficiency and autonomy, the
agent adopts a mixed strategy: deterministic rule-based steps for common
patterns, and a state-machine-guided decision process for intelligent
exploration. We evaluate WIRL on a carefully curated, high-quality dataset
consisting of real-world code adaptation scenarios. Our approach achieves an
exact match precision of 91.7% and a recall of 90.0%, outperforming advanced
LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively,
and surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results
underscore its practical utility, particularly in contexts with complex
variable dependencies or multiple unresolved variables. We believe WIRL paves
the way for more intelligent and context-aware developer assistance in modern
IDEs.

</details>


### [4] [Combining Type Inference and Automated Unit Test Generation for Python](https://arxiv.org/abs/2507.01477)
*Lukas Krodinger,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 论文提出了一种名为类型追踪的技术，通过运行时提取类型信息来改善动态类型语言（如Python）的自动化单元测试生成效果。


<details>
  <summary>Details</summary>
Motivation: 动态类型语言缺乏静态类型信息，限制了现有测试生成器的效能，尤其是在参数和返回值类型推断方面。

Method: 通过类型追踪技术在运行时提取和逐步优化类型信息，并将其集成到Pynguin测试生成框架中。

Result: 该方法实现了高达90%的分支覆盖提升，改善了变异得分，且类型信息质量媲美现有先进类型推断工具。

Conclusion: 类型追踪有效提升了动态类型语言的测试生成效果，展现出显著优势。

Abstract: Automated unit test generation is an established research field that has so
far focused on statically-typed programming languages. The lack of type
information in dynamically-typed programming languages, such as Python,
inhibits test generators, which heavily rely on information about parameter and
return types of functions to select suitable arguments when constructing test
cases. Since automated test generators inherently rely on frequent execution of
candidate tests, we make use of these frequent executions to address this
problem by introducing type tracing, which extracts type-related information
during execution and gradually refines the available type information. We
implement type tracing as an extension of the Pynguin test-generation framework
for Python, allowing it (i) to infer parameter types by observing how
parameters are used during runtime, (ii) to record the types of values that
function calls return, and (iii) to use this type information to increase code
coverage. The approach leads to up to 90.0% more branch coverage, improved
mutation scores, and to type information of similar quality to that produced by
other state-of-the-art type-inference tools.

</details>


### [5] [DaiFu: In-Situ Crash Recovery for Deep Learning Systems](https://arxiv.org/abs/2507.01628)
*Zilong He,Pengfei Chen,Hongyu Zhang,Xiaoyun Li,Guangba Yu,Hongyang Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: DaiFu是一个轻量级的深度学习系统即时恢复框架，通过代码转换实现快速恢复，显著减少恢复时间。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统因复杂的软件堆栈而易崩溃，现有恢复方法（如检查点重试）效率低下，需要更轻量级的解决方案。

Method: 通过轻量级代码转换，DaiFu能够拦截崩溃并动态更新程序运行上下文，实现敏捷恢复。

Result: DaiFu将恢复时间缩短了1372倍，开销低于0.40%，并在多种崩溃场景下验证了其有效性。

Conclusion: DaiFu为深度学习系统提供了一种高效的崩溃恢复方案，显著提升了开发效率和资源利用率。

Abstract: Deep learning (DL) systems have been widely adopted in many areas, and are
becoming even more popular with the emergence of large language models.
However, due to the complex software stacks involved in their development and
execution, crashes are unavoidable and common. Crashes severely waste computing
resources and hinder development productivity, so efficient crash recovery is
crucial. Existing solutions, such as checkpoint-retry, are too heavyweight for
fast recovery from crashes caused by minor programming errors or transient
runtime errors. Therefore, we present DaiFu, an in-situ recovery framework for
DL systems. Through a lightweight code transformation to a given DL system,
DaiFu augments it to intercept crashes in situ and enables dynamic and instant
updates to its program running context (e.g., code, configurations, and other
data) for agile crash recovery. Our evaluation shows that DaiFu helps reduce
the restore time for crash recovery, achieving a 1372x speedup compared with
state-of-the-art solutions. Meanwhile, the overhead of DaiFu is negligible
(under 0.40%). We also construct a benchmark spanning 7 distinct crash
scenarios in DL systems, and show the effectiveness of DaiFu in diverse
situations.

</details>


### [6] [APRMCTS: Improving LLM-based Automated Program Repair with Iterative Tree Search](https://arxiv.org/abs/2507.01827)
*Haichuan Hu,Congqing He,Hao Zhang,Xiaochen Xie,Quanjun Zhang*

Main category: cs.SE

TL;DR: APRMCTS利用蒙特卡洛树搜索改进基于LLM的自动程序修复，解决了局部探索和冗余搜索的问题，显著提升了修复效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动程序修复技术存在局部探索和冗余搜索的问题，限制了修补效果和效率。

Method: 提出APRMCTS，结合蒙特卡洛树搜索进行全局评估和迭代树搜索，优化修补生成过程。

Result: 在Defects4J的835个bug上，APRMCTS修复了201个bug，优于所有基线方法，并在成本和效率上显著优于现有技术。

Conclusion: APRMCTS在复杂bug修复中表现出高效性和有效性，显著提升了LLM-based APR的性能。

Abstract: Automated Program Repair (APR) attempts to fix software bugs without human
intervention, which plays a crucial role in software development and
maintenance. Recently, with the advances in Large Language Models (LLMs), a
rapidly increasing number of APR techniques have been proposed with remarkable
performance. However, existing LLM-based APR techniques typically adopt
trial-and-error strategies, which suffer from two major drawbacks: (1)
inherently limited patch effectiveness due to local exploration, and (2) low
search efficiency due to redundant exploration. In this paper, we propose
APRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS
incorporates Monte Carlo Tree Search (MCTS) into patch searching by performing
a global evaluation of the explored patches and selecting the most promising
one for subsequent refinement and generation. APRMCTS effectively resolves the
problems of falling into local optima and thus helps improve the efficiency of
patch searching. Our experiments on 835 bugs from Defects4J demonstrate that,
when integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which
outperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,
GPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,
respectively. More importantly, APRMCTS boasts a significant performance
advantage while employing small patch size (16 and 32), notably fewer than the
500 and 10,000 patches adopted in previous studies. In terms of cost, compared
to existing state-of-the-art LLM-based APR methods, APRMCTS has time and
monetary costs of less than 20% and 50%, respectively. Our extensive study
demonstrates that APRMCTS exhibits good effectiveness and efficiency, with
particular advantages in addressing complex bugs.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [Advanced LPeg techniques: A dual case study approach](https://arxiv.org/abs/2507.01272)
*Zixuan Zhu*

Main category: cs.PL

TL;DR: 该论文通过两个案例研究展示了如何优化Lua解析表达式文法（LPeg），在不修改LPeg库的情况下显著提升解析性能。


<details>
  <summary>Details</summary>
Motivation: 通过优化LPeg语法结构，解决JSON解析和Glob模式转换中的性能问题。

Method: 针对JSON解析器，采用替代捕获和表格构建优化；针对Glob转换器，引入段边界分离、Cox扁平搜索策略和优化的大括号条件处理。

Result: JSON解析器速度达125 MB/s，优于dkjson和rxi_json；Glob转换器性能提升14-92%，比Bun.Glob和Minimatch更快。

Conclusion: 研究提供了LPeg解析器的实用优化技术，为文本处理生态系统贡献了有价值的策略。

Abstract: This paper presents advanced optimization techniques for Lua Parsing
Expression Grammars (LPeg) through two complementary case studies: a
high-performance JSON parser and a sophisticated Glob-to-LPeg pattern
converter. We demonstrate how strategic grammar construction can dramatically
improve parsing performance without modifying the underlying LPeg library. For
the JSON parser, we implement substitution capture and table construction
optimization to reduce memory allocation overhead and improve object
processing. For the Glob converter, we introduce segment-boundary separation,
implement Cox's flattened search strategy, and develop optimized braced
condition handling to prevent exponential backtracking. Comprehensive
benchmarks demonstrate that our JSON parser achieves processing speeds up to
125 MB/s on complex documents, consistently outperforming dkjson and showing
competitive results against rxi_json across most test cases. Our Glob-to-LPeg
converter exhibits 14-92% better performance than Bun.Glob and runs 3-14 times
faster than Minimatch across diverse pattern matching scenarios. This research
provides practical optimization techniques for LPeg-based parsers, contributing
valuable strategies to the text processing ecosystem.

</details>


### [8] [Globality and Regions](https://arxiv.org/abs/2507.01664)
*Hector Gramaglia*

Main category: cs.PL

TL;DR: 通过统一抽象与区域抽象，揭示了全局变量的表征来源。


<details>
  <summary>Details</summary>
Motivation: 探索如何在函数式语言中清晰引入操作，并通过线性保护实现内存安全。

Method: 统一抽象与区域抽象，连接全局系统与线性系统。

Result: 证明全局变量概念来源于Tofte和Talping的区域语言。

Conclusion: 全局变量的定义源于区域语言中的抽象统一。

Abstract: We obtain a characterization of global variables by unifying abstraction with
region abstraction in a region-based language. More precisely, in a previous
work a language called global was presented, whose virtue is to provide a
conceptually clear way of introducing imperative operations in a functional
language. Memory safety is provided by the concept of linear protection, which
connects the global system to a linear one. In this paper we show that the
concept of global variable provided by the global language arises from the
Tofte and Talping's region language through the unification of abstraction and
region abstraction.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [9] [A Full-Stack Platform Architecture for Self-Organised Social Coordination](https://arxiv.org/abs/2507.01239)
*Matthew Scott,Jeremy Pitt*

Main category: cs.NI

TL;DR: 通过开发开源、全栈的平台架构，旨在赋权本地社区，支持自我组织的社会协调，对抗平台化的垄断趋势。


<details>
  <summary>Details</summary>
Motivation: 应对平台化带来的集中化和垄断倾向，需为本地社区提供民主化的平台工具。

Method: 开发一种支持分布式部署、克隆和多样托管选项的全栈架构，包括元平台、基础平台和支持库，以及第三方插件。

Result: 通过两个案例研究（体育组织和集体学习平台）验证了架构的可行性。

Conclusion: 全栈架构及配套工具链可支持应用层的自我组织。

Abstract: To mitigate the restrictive centralising and monopolistic tendencies of
platformisation, we aim to empower local communities by democratising platforms
for self-organised social coordination. Our approach is to develop an
open-source, full-stack architecture for platform development that supports
ease of distribution and cloning, generativity, and a variety of hosting
options. The architecture consists of a meta-platform that is used to
instantiate a base platform with supporting libraries for generic functions,
and plugins (intended to be supplied by third parties) for customisation of
application-specification functionality for self-organised social coordination.
Associated developer- and user-oriented toolchains support the instantiation
and customisation of a platform in a two-stage process. This is demonstrated
through the proof-of-concept implementation of two case studies: a platform for
regular sporting association, and a platform for collective group study. We
conclude by arguing that self-organisation at the application layer can be
achieved by the specific supporting functionality of a full-stack architecture
with complimentary developer and user toolchains.

</details>


### [10] [Fluid Aerial Networks: UAV Rotation for Inter-Cell Interference Mitigation](https://arxiv.org/abs/2507.01289)
*Enzhi Zhou,Yue Xiao,Ziyue Liu,Sotiris A. Tegos,Panagiotis D. Diamantoulakis,George K. Karagiannidis*

Main category: cs.NI

TL;DR: 本文研究了无人机辅助网络中基于位置的波束成形技术，提出了一种通过无人机旋转减少小区间干扰的新方法，显著提升了网络效率和容量。


<details>
  <summary>Details</summary>
Motivation: 为了扩展地面网络服务并提升应急通信能力，无人机作为空中基站被广泛应用。然而，小区间干扰是影响网络效率的主要挑战之一。

Method: 作者提出了一种流体空中网络方案，利用无人机旋转优化波束成形权重，并通过低复杂度算法设计最佳旋转角度以减少干扰。

Result: 仿真结果显示，在干扰受限区域，该方法能显著降低干扰，并将多小区总和速率提升约10%。

Conclusion: 无人机旋转方案是一种有效减少小区间干扰并提升网络性能的低成本解决方案。

Abstract: With the rapid development of aerial infrastructure, unmanned aerial vehicles
(UAVs) that function as aerial base stations (ABSs) extend terrestrial network
services into the sky, enabling on-demand connectivity and enhancing emergency
communication capabilities in cellular networks by leveraging the flexibility
and mobility of UAVs. In such a UAV-assisted network, this paper investigates
position-based beamforming between ABSs and ground users (GUs). To mitigate
inter-cell interference, we propose a novel fluid aerial network that leverages
ABS rotation to increase multi-cell capacity and overall network efficiency.
Specifically, considering the line-of-sight channel model, the spatial
beamforming weights are determined by the orientation angles of the GUs. In
this direction, we examine the beamforming gain of a two-dimensional
multiple-input multiple-output (MIMO) array at various ground positions,
revealing that ABS rotation significantly affects multi-user channel
correlation and inter-cell interference. Based on these findings, we propose an
alternative low-complexity algorithm to design the optimal rotation angle for
ABSs, aiming to reduce inter-cell interference and thus maximize the sum rate
of multi-cell systems. In simulations, exhaustive search serves as a benchmark
to validate the optimization performance of the proposed sequential ABS
rotation scheme. Moreover, simulation results demonstrate that, in
interference-limited regions, the proposed ABS rotation paradigm can
significantly reduce inter-cell interference in terrestrial networks and
improve the multi-cell sum rate by approximately 10\% compared to
fixed-direction ABSs without rotation.

</details>


### [11] [Multi-User Generative Semantic Communication with Intent-Aware Semantic-Splitting Multiple Access](https://arxiv.org/abs/2507.01333)
*Jiayi Lu,Wanting Yang,Zehui Xiong,Rahim Tafazolli,Tony Q. S. Quek,Mérouane Debbah,Dong In Kim*

Main category: cs.NI

TL;DR: 多用户语义通信框架（SS-MGSC）通过意图感知知识库和扩散模型实现高效个性化内容分发，优化指标为语义效率分数（SES）。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展推动语义通信成为高效可靠的新范式，如何满足多用户多样化且重叠的需求是关键挑战。

Method: 构建意图感知共享知识库（SKB），广播公共语义信息，个性化分发私有语义信息，接收端采用ControlNet增强的扩散模型生成图像，使用强化学习优化语义提取与波束成形。

Result: 仿真结果表明所提框架在多用户语义通信中表现有效。

Conclusion: SS-MGSC框架通过结合语义与个性化需求，提升了多用户语义通信的效率和体验。

Abstract: With the booming development of generative artificial intelligence (GAI),
semantic communication (SemCom) has emerged as a new paradigm for reliable and
efficient communication. This paper considers a multi-user downlink SemCom
system, using vehicular networks as the representative scenario for multi-user
content dissemination. To address diverse yet overlapping user demands, we
propose a multi-user Generative SemCom-enhanced intent-aware semantic-splitting
multiple access (SS-MGSC) framework. In the framework, we construct an
intent-aware shared knowledge base (SKB) that incorporates prior knowledge of
semantic information (SI) and user-specific preferences. Then, we designate the
common SI as a one-hot semantic map that is broadcast to all users, while the
private SI is delivered as personalized text for each user. On the receiver
side, a diffusion model enhanced with ControlNet is adopted to generate
high-quality personalized images. To capture both semantic relevance and
perceptual similarity, we design a novel semantic efficiency score (SES) metric
as the optimization objective. Building on this, we formulate a joint
optimization problem for multi-user semantic extraction and beamforming, solved
using a reinforcement learning-based algorithm due to its robustness in
high-dimensional settings. Simulation results demonstrate the effectiveness of
the proposed scheme.

</details>


### [12] [MmBack: Clock-free Multi-Sensor Backscatter with Synchronous Acquisition and Multiplexing](https://arxiv.org/abs/2507.01360)
*Yijie Li,Weichong Ling,Taiting Lu,Yi-Chao Chen,Vaishnavi Ranganathan,Lili Qiu,Jingxian Wang*

Main category: cs.NI

TL;DR: mmBack是一种低功耗、无时钟反向散射标签，支持多传感器同步数据采集和复用，通过单一调制链实现高效多传感器输入。


<details>
  <summary>Details</summary>
Motivation: 现有设计仅支持每个标签单传感器，而多传感器场景需要同步和复用功能，现有方法依赖时钟或多调制链，增加了成本和复杂性。

Method: mmBack利用环境RF激励提取共享参考信号同步传感器输入，并通过电压分配方案复用传感器数据为频率偏移。接收端使用频率跟踪算法和有限状态机解复用。

Result: mmBack原型支持5个5kHz带宽或3个18kHz带宽的并发传感器流，ASIC功耗为25.56μW，信号重构平均SNR超过15dB。

Conclusion: mmBack提供了一种高效、低功耗的多传感器同步和复用方案，解决了现有设计的局限性。

Abstract: Backscatter tags provide a low-power solution for sensor applications, yet
many real-world scenarios require multiple sensors-often of different types-for
complex sensing tasks. However, existing designs support only a single sensor
per tag, increasing spatial overhead. State-of-the-art approaches to
multiplexing multiple sensor streams on a single tag rely on onboard clocks or
multiple modulation chains, which add cost, enlarge form factor, and remain
prone to timing drift-disrupting synchronization across sensors.
  We present mmBack, a low-power, clock-free backscatter tag that enables
synchronous multi-sensor data acquisition and multiplexing over a single
modulation chain. mmBack synchronizes sensor inputs in parallel using a shared
reference signal extracted from ambient RF excitation, eliminating the need for
an onboard timing source. To efficiently multiplex sensor data, mmBack designs
a voltage-division scheme to multiplex multiple sensor inputs as backscatter
frequency shifts through a single oscillator and RF switch. At the receiver,
mmBack develops a frequency tracking algorithm and a finite-state machine for
accurate demultiplexing. mmBack's ASIC design consumes 25.56uW, while its
prototype supports 5 concurrent sensor streams with bandwidths of up to 5kHz
and 3 concurrent sensor streams with bandwidth of up to 18kHz. Evaluation shows
that mmBack achieves an average SNR surpassing 15dB in signal reconstruction.

</details>


### [13] [Frontiers of Generative AI for Network Optimization: Theories, Limits, and Visions](https://arxiv.org/abs/2507.01773)
*Bo Yang,Ruihuai Liang,Weixin Li,Han Wang,Xuelin Cao,Zhiwen Yu,Samson Lasaulce,Mérouane Debbah,Mohamed-Slim Alouini,H. Vincent Poor,Chau Yuen*

Main category: cs.NI

TL;DR: 这篇论文综述了生成式AI在网络优化中的应用，分析了其局限性和未来方向，强调了生成扩散模型和大预训练模型的作用及其理论边界。


<details>
  <summary>Details</summary>
Motivation: 近年来生成式AI在网络优化中的应用激增，但其快速进展掩盖了生成模型的内在局限性。本文旨在全面回顾和批判性分析这些方法。

Method: 论文围绕生成扩散模型和大预训练模型，将网络优化问题分为一次性优化和马尔可夫决策过程两类，并提供了理论泛化边界。

Result: 揭示生成式AI的局限性，如约束满足困难、概念理解有限及输出概率性，同时提出未来研究方向，如生成与优化的桥梁。

Conclusion: 论文结构化概述了生成式AI在网络优化中的优势、局限性和潜力，呼吁对其理论和实际应用更深入的理解。

Abstract: While interest in the application of generative AI (GenAI) in network
optimization has surged in recent years, its rapid progress has often
overshadowed critical limitations intrinsic to generative models that remain
insufficiently examined in existing literature. This survey provides a
comprehensive review and critical analysis of GenAI in network optimization. We
focus on the two dominant paradigms of GenAI including generative diffusion
models (GDMs) and large pre-trained models (LPTMs), and organize our discussion
around a categorization we introduce, dividing network optimization problems
into two primary formulations: one-shot optimization and Markov decision
process (MDP). We first trace key works, including foundational contributions
from the AI community, and categorize current efforts in network optimization.
We also review frontier applications of GDMs and LPTMs in other networking
tasks, providing additional context. Furthermore, we present theoretical
generalization bounds for GDMs in both one-shot and MDP settings, offering
insights into the fundamental factors affecting model performance. Most
importantly, we reflect on the overestimated perception of GenAI's general
capabilities and caution against the all-in-one illusion it may convey. We
highlight critical limitations, including difficulties in constraint
satisfying, limited concept understanding, and the inherent probabilistic
nature of outputs. We also propose key future directions, such as bridging the
gap between generation and optimization. Although they are increasingly
integrated in implementations, they differ fundamentally in both objectives and
underlying mechanisms, necessitating a deeper understanding of their
theoretical connections. Ultimately, this survey aims to provide a structured
overview and a deeper insight into the strengths, limitations, and potential of
GenAI in network optimization.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [14] [Robust Multi-generation Learned Compression of Point Cloud Attribute](https://arxiv.org/abs/2507.01320)
*Xiangzuo Liu,Zhikai Liu,PengPeng Yu,Ruishan Huang,Fan Liang*

Main category: cs.MM

TL;DR: 该论文首次研究了学习点云属性压缩中的多代问题，提出三种约束方法以抑制多代失真，同时保持单次率失真性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法忽视多代压缩中的累积失真问题，论文旨在解决此问题。

Method: 提出映射幂等性约束、变换可逆性约束和潜在变量一致性约束。

Result: 在Owlii和8iVFB数据集上验证，方法有效抑制多代失真。

Conclusion: 方法在多代压缩中表现优异，且不影响单次压缩性能。

Abstract: Existing learned point cloud attribute compression methods primarily focus on
single-pass rate-distortion optimization, while overlooking the issue of
cumulative distortion in multi-generation compression scenarios. This paper,
for the first time, investigates the multi-generation issue in learned point
cloud attribute compression. We identify two primary factors contributing to
quality degradation in multi-generation compression: quantization-induced
non-idempotency and transformation irreversibility. To address the former, we
propose a Mapping Idempotency Constraint, that enables the network to learn the
complete compression-decompression mapping, enhancing its robustness to
repeated processes. To address the latter, we introduce a Transformation
Reversibility Constraint, which preserves reversible information flow via a
quantization-free training path. Further, we propose a Latent Variable
Consistency Constraint which enhances the multi-generation compression
robustness by incorporating a decompression-compression cross-generation path
and a latent variable consistency loss term. Extensive experiments conducted on
the Owlii and 8iVFB datasets verify that the proposed methods can effectively
suppress multi-generation loss while maintaining single-pass rate-distortion
performance comparable to baseline models.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [15] [Interpolation with Automated First-Order Reasoning](https://arxiv.org/abs/2507.01577)
*Christoph Wernhard*

Main category: cs.LO

TL;DR: 论文探讨了一阶逻辑中完全自动定理证明的插值方法，重点讨论了Craig插值的两阶段方法，并介绍了基于表盘和分辨率的地面插值获取方式。还涉及了预处理技术、等式编码以及二阶量词消除等内容。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索一阶逻辑中插值方法在机械化知识处理中的应用，特别是通过两阶段方法实现Craig插值。

Method: 采用两阶段方法，先计算命题地面插值，再提升为量词化一阶公式。使用表盘和分辨率获取地面插值，并结合预处理技术和等式编码。

Result: 提出了实现Craig插值的具体方法，并展示了如何通过二阶量词消除进行均匀插值。

Conclusion: 论文展示了自动化推理中插值方法的多样性和有效性，为知识处理提供了实用工具。

Abstract: We consider interpolation from the viewpoint of fully automated theorem
proving in first-order logic as a general core technique for mechanized
knowledge processing. For Craig interpolation, our focus is on the two-stage
approach, where first an essentially propositional ground interpolant is
calculated that is then lifted to a quantified first-order formula. We discuss
two possibilities to obtain a ground interpolant from a proof, with clausal
tableaux, and with resolution. Established preprocessing techniques for
first-order proving can also be applied for Craig interpolation if they are
restricted in specific ways. Equality encodings from automated reasoning
justify strengthened variations of Craig interpolation. Also further
contributions to Craig interpolation emerged from automated reasoning. As an
approach to uniform interpolation we introduce second-order quantifier
elimination with examples and describe the basic algorithms DLS and SCAN.

</details>


### [16] [LeanLTL: A unifying framework for linear temporal logics in Lean](https://arxiv.org/abs/2507.01780)
*Eric Vin,Kyle A. Miller,Daniel J. Fremont*

Main category: cs.LO

TL;DR: LeanLTL是一个在Lean 4中统一线性时序逻辑的框架，支持无限或有限线性时间的推理，并允许结合Lean表达式定义复杂属性。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个统一的框架，支持多种线性时序逻辑的推理，并简化涉及数值或其他类型的属性定义。

Method: 开发了LeanLTL库，支持传统LTL语法与Lean表达式的结合，并提供了自动化推理工具。

Result: 证明标准LTL可嵌入框架中，并通过示例展示了其在应用系统中的实用性。

Conclusion: LeanLTL提供了一个强大且灵活的框架，可用于系统推理，并支持Lean现有工具的集成。

Abstract: We propose LeanLTL, a unifying framework for linear temporal logics in Lean
4. LeanLTL supports reasoning about traces that represent either infinite or
finite linear time. The library allows traditional LTL syntax to be combined
with arbitrary Lean expressions, making it straightforward to define properties
involving numerical or other types. We prove that standard flavors of LTL can
be embedded in our framework. The library also provides automation for
reasoning about LeanLTL formulas in a way that facilitates using Lean's
existing tactics. Finally, we provide examples illustrating the utility of the
library in reasoning about systems that come from applications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [17] [AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma](https://arxiv.org/abs/2507.01081)
*Megan T. deBettencourt,Sruthi Sakthivel,Emily A. Holmes,Mark Chevillet*

Main category: cs.HC

TL;DR: ANTIDOTE结合AI引导和瞳孔测量技术，自动提供并监测基于证据的数字治疗（ICTI），显著减少创伤后侵入性记忆，瞳孔大小可作为干预效果的生物标志物。


<details>
  <summary>Details</summary>
Motivation: 全球创伤普遍，但现有的数字治疗依赖人工指导，限制可扩展性。研究探索生成式AI和神经技术是否能提供可扩展的替代方案。

Method: 100名健康志愿者观看创伤视频，随机分配至干预组或对照组。ANTIDOTE系统结合AI引导和瞳孔测量技术，自动提供ICTI干预。

Result: 干预组报告明显减少的侵入性记忆。AI指导成功实施干预，瞳孔大小与干预参与度及症状减轻相关。

Conclusion: ANTIDOTE为可扩展的AI引导数字干预开辟了新途径，瞳孔测量可作为有效性生物标志物。

Abstract: Trauma prevalence is vast globally. Evidence-based digital treatments can
help, but most require human guidance. Human guides provide tailored
instructions and responsiveness to internal cognitive states, but limit
scalability. Can generative AI and neurotechnology provide a scalable
alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to
automatically deliver and monitor an evidence-based digital treatment,
specifically the Imagery Competing Task Intervention (ICTI), to reduce
intrusive memories after psychological trauma. One hundred healthy volunteers
were exposed to videos of traumatic events and randomly assigned to an
intervention or active control condition. As predicted, intervention
participants reported significantly fewer intrusive memories over the following
week. Post-hoc assessment against clinical rubrics confirmed the AI guide
delivered the intervention successfully. Additionally, pupil size tracked
intervention engagement and predicted symptom reduction, providing a candidate
biomarker of intervention effectiveness. These findings open a path toward
rigorous AI-guided digital interventions that can scale to trauma prevalence.

</details>


### [18] [From Literature to ReWA: Discussing Reproductive Well-being in HCI](https://arxiv.org/abs/2507.01121)
*Hafsah Mahzabin Chowdhury,Sharifa Sultana*

Main category: cs.HC

TL;DR: 本文综述了147篇2015-2025年的文献，揭示了生殖福祉技术的演变及其在文化敏感性、隐私和AI整合方面的进展，但指出了全球南方、男性和非二元用户等群体的缺失问题，并提出ReWA框架以改进设计。


<details>
  <summary>Details</summary>
Motivation: 探讨生殖福祉技术如何受文化、宗教、性别和政治因素影响，并揭示当前技术的西方中心主义局限及其对不同用户群体的排斥。

Method: 通过对HCI、CSCW、社会计算等领域的147篇文献进行系统分析，识别出三个主题浪潮，并提出ReWA框架。

Result: 发现技术设计在包容性上存在显著缺陷，尤其是对全球南方、男性、非二元用户和利益相关者的忽视。

Conclusion: 提出ReWA框架，强调设计应考虑地理位置、文化历史、多元声音和代理权等因素，以提升生殖福祉技术的包容性。

Abstract: Reproductive well-being is shaped by intersecting cultural, religious,
gendered, and political contexts, yet current technologies often reflect
narrow, Western-centric assumptions. In this literature review, we synthesize
findings from 147 peer-reviewed papers published between 2015 and 2025 across
HCI, CSCW and social computing, ICTD, digital and public health, and AI for
well-being scholarship to map the evolving reproductive well-being landscape.
We identify three thematic waves that focused on early access and education,
cultural sensitivity and privacy, and AI integration with policy-aware design,
and highlight how technologies support or constrain diverse reproductive
experiences. Our analysis reveals critical gaps in inclusivity, with persistent
exclusions of men and non-binary users, migrants, and users in the Global
South. Additionally, we surfaced the significant absence of literature on the
role of stakeholders (e.g., husband and family members, household maids and
cleaning helping hands, midwife, etc.) in the reproductive well-being space.
Drawing on the findings from the literature, we propose the ReWA framework to
support reproductive well-being for all agendas through six design orientations
associated with: location, culture, and history; polyvocality and agency;
rationality, temporality, distributive roles, and methodology.

</details>


### [19] [Animated Visual Encoding and Layer Blending for Identification of Educational Game Strategies](https://arxiv.org/abs/2507.01134)
*Braden Roper,William Thompson,Chris Weaver*

Main category: cs.HC

TL;DR: 基于游戏的学习能提升学习参与度，但玩家策略分析较难。本文提出一种动画可视化工具，用于解决复杂数据的解读问题。


<details>
  <summary>Details</summary>
Motivation: 游戏状态和行动数据难以解读为长期策略，传统可视化工具存在过度绘图等问题，影响解读。

Method: 提出一种动画视觉编码工具，利用动态可视化技术，支持参数插值曲线和混合层的配置。

Result: 工具能有效帮助研究者构建动画数据叙事，并通过领域专家合作验证其实用性。

Conclusion: 动画可视化工具为游戏学习中的策略分析提供了新的解决方案。

Abstract: Game-Based Learning has proven to be an effective method for enhancing
engagement with educational material. However, gaining a deeper understanding
of player strategies remains challenging. Sequential game-state and
action-based tracking tools often gather extensive data that can be difficult
to interpret as long-term strategy. This data presents unique problems to
visualization, as it can be fairly natural, noisy data but is constrained
within synthetic, controlled environments, leading to issues such as
overplotting which can make interpretation complicated. We propose an animated
visual encoding tool that utilizes kinetic visualization to address these
issues. This tool enables researchers to construct animated data narratives
through the configuration of parameter interpolation curves and blending
layers. Finally, we demonstrate the usefulness of the tool while addressing
specific interests as outlined by a domain expert collaborator.

</details>


### [20] [A Methodological Framework for Capturing Cognitive-Affective States in Collaborative Learning](https://arxiv.org/abs/2507.01166)
*Sifatul Anindho,Videep Venkatesha,Nathaniel Blanchard*

Main category: cs.HC

TL;DR: 论文研究了如何在小组协作中识别个体的情感和注意力状态，通过回顾性提示回忆范式，分析了参与者报告的频率和时间分布，以及不同收集方式下标签分布的变化。


<details>
  <summary>Details</summary>
Motivation: 研究中识别小组协作中个体的情感和注意力状态通常难以在不干扰自然协作的情况下实现。本文旨在探索一种方法，能够更有效地跟踪协作学习中的认知情感状态，并为自适应学习系统的开发提供支持。

Method: 研究采用回顾性提示回忆范式，参与者通过观看小组视频描述其认知情感状态。随后，对额外参与者进行约束性报告，分为自我报告和响应提示报告两种方式。

Result: 研究分析了参与者报告的频率和时间分布，发现不同收集方式（自由报告与约束性报告）下标签分布存在变化。初步分析表明，约束性报告与自由报告在状态识别上存在差异。

Conclusion: 该方法对教育数据挖掘社区有重要意义，能够更有效地跟踪协作学习中的认知情感状态，并有助于开发能够检测和响应这些状态的自适应学习系统。

Abstract: Identification of affective and attentional states of individuals within
groups is difficult to obtain without disrupting the natural flow of
collaboration. Recent work from our group used a retrospect cued recall
paradigm where participants spoke about their cognitive-affective states while
they viewed videos of their groups. We then collected additional participants
where their reports were constrained to a subset of pre-identified
cognitive-affective states. In this latter case, participants either self
reported or reported in response to probes. Here, we present an initial
analysis of the frequency and temporal distribution of participant reports, and
how the distributions of labels changed across the two collections. Our
approach has implications for the educational data mining community in tracking
cognitive-affective states in collaborative learning more effectively and in
developing improved adaptive learning systems that can detect and respond to
cognitive-affective states.

</details>


### [21] [Human-Machine Collaboration-Guided Space Design: Combination of Machine Learning Models and Humanistic Design Concepts](https://arxiv.org/abs/2507.01776)
*Yuxuan Yang*

Main category: cs.HC

TL;DR: 论文提出了一种人机协作框架，将机器学习的效率与人类设计的创造力结合，以优化空间设计并确保情感与文化共鸣。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过机器学习（ML）优化空间设计的效率，同时解决其无法满足情感、文化和审美需求的局限性。

Method: 提出了一个人机协作框架，结合ML的自动化、预测能力和人类设计师的创造力、直觉与文化洞察。

Result: 通过办公和住宅设计的案例研究，展示了该框架如何提升设计的创造力和文化相关性。

Conclusion: 融合ML与人类创造力，能在空间设计中实现效率与情感影响的平衡，创造出功能性与人性化兼具的环境。

Abstract: The integration of machine learning (ML) into spatial design holds immense
potential for optimizing space utilization, enhancing functionality, and
streamlining design processes. ML can automate tasks, predict performance
outcomes, and tailor spaces to user preferences. However, the emotional,
cultural, and aesthetic dimensions of design remain crucial for creating spaces
that truly resonate with users-elements that ML alone cannot address. The key
challenge lies in harmonizing data-driven efficiency with the nuanced,
subjective aspects of design. This paper proposes a human-machine collaboration
framework to bridge this gap. An effective framework should recognize that
while ML enhances design efficiency through automation and prediction, it must
be paired with human creativity to ensure spaces are emotionally engaging and
culturally relevant. Human designers contribute intuition, empathy, and
cultural insight, guiding ML-generated solutions to align with users' emotional
and cultural needs. Additionally, we explore how various ML models can be
integrated with human-centered design principles. These models can automate
design generation and optimization, while human designers refine the outputs to
ensure emotional resonance and aesthetic appeal. Through case studies in office
and residential design, we illustrate how this framework fosters both
creativity and cultural relevance. By merging ML with human creativity, spatial
design can achieve a balance of efficiency and emotional impact, resulting in
environments that are both functional and deeply human.

</details>


### [22] [Judgment as Coordination: A Joint Systems View of Visualization Design Practice](https://arxiv.org/abs/2507.01209)
*Paul C. Parsons,Arran Ridley*

Main category: cs.HC

TL;DR: 本文提出了一个系统级的设计判断框架，关注专业可视化设计中的协作和系统性维度，强调通过修复对齐、调整计划和重构目标来维持设计连贯性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨专业可视化设计中协作与系统性维度的不足，当前研究过于关注个体设计师的决策，忽略了团队合作中的实际工作。

Method: 方法包括多个实证研究，如设计团队的民族志观察和个体从业者的定性研究，以及联合认知系统理论的应用。

Result: 研究发现设计连贯性并非通过选择最优方案维持，而是通过修复对齐、调整计划和重构目标来实现，揭示了设计中隐藏的工作。

Conclusion: 结论指出，这一系统级视角为可视化设计研究提供了新的概念工具，帮助理解设计活动在实际中的持续运作。

Abstract: Professional visualization design has become an increasingly important area
of inquiry, yet much of the field's discourse remains anchored in
researcher-centered contexts. Studies of design practice often focus on
individual designers' decisions and reflections, offering limited insight into
the collaborative and systemic dimensions of professional work. In this paper,
we propose a systems-level reframing of design judgment grounded in the
coordination and adaptation that sustain progress amid uncertainty, constraint,
and misalignment. Drawing on sustained engagement across multiple empirical
studies--including ethnographic observation of design teams and qualitative
studies of individual practitioners--we identify recurring episodes in which
coherence was preserved not by selecting an optimal option, but by repairing
alignment, adjusting plans, and reframing goals. We interpret these dynamics
through the lens of Joint Cognitive Systems, which provide tools for analyzing
how judgment emerges as a distributed capacity within sociotechnical activity.
This perspective surfaces often-invisible work in visualization design and
offers researchers a new conceptual vocabulary for studying how design activity
is sustained in practice.

</details>


### [23] [AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance](https://arxiv.org/abs/2507.01274)
*Vishakha Lall,Yisi Liu*

Main category: cs.HC

TL;DR: 该论文开发了一个AI驱动的框架，通过视觉焦点追踪、语音识别和压力检测等技术，客观评估海事培训中的受训者表现，显著提高了高压力情景下的准备能力。


<details>
  <summary>Details</summary>
Motivation: 传统海事模拟器培训依赖主观评估，存在主观性、关键特征难以测量和认知局限等问题，本研究旨在通过AI技术解决这些问题。

Method: 采用AI技术，包括视觉焦点追踪（眼动、瞳孔放大分析）、语音识别（海事专用语音转文本模型、自然语言处理）、语言模型评估通讯准确性，以及声音音高检测压力。

Result: AI算法在模拟海事场景中表现优异，视觉检测准确率约92%，语音识别约91%，压力检测约90%，优于现有基准。

Conclusion: 研究表明，AI可为海事培训提供客观表现分析，支持个性化反馈，提升应对实际操作挑战的准备能力。

Abstract: Traditional simulator-based training for maritime professionals is critical
for ensuring safety at sea but often depends on subjective trainer assessments
of technical skills, behavioral focus, communication, and body language, posing
challenges such as subjectivity, difficulty in measuring key features, and
cognitive limitations. Addressing these issues, this study develops an
AI-driven framework to enhance maritime training by objectively assessing
trainee performance through visual focus tracking, speech recognition, and
stress detection, improving readiness for high-risk scenarios. The system
integrates AI techniques, including visual focus determination using eye
tracking, pupil dilation analysis, and computer vision; communication analysis
through a maritime-specific speech-to-text model and natural language
processing; communication correctness using large language models; and mental
stress detection via vocal pitch. Models were evaluated on data from simulated
maritime scenarios with seafarers exposed to controlled high-stress events. The
AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for
maritime speech recognition, and ~90% for stress detection, surpassing existing
benchmarks. The system provides insights into visual attention, adherence to
communication checklists, and stress levels under demanding conditions. This
study demonstrates how AI can transform maritime training by delivering
objective performance analytics, enabling personalized feedback, and improving
preparedness for real-world operational challenges.

</details>


### [24] [Challenges & Opportunities with LLM-Assisted Visualization Retargeting](https://arxiv.org/abs/2507.01436)
*Luke S. Snyder,Chenglong Wang,Steven Drucker*

Main category: cs.HC

TL;DR: 论文研究了如何利用大型语言模型(LLMs)简化可视化图表代码的重新定向过程，评估了两种方法的性能并分析了其局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的可视化图表代码难以重新定向到新数据集，过程耗时且复杂，因此探索LLMs如何简化这一过程。

Method: 比较两种方法：直接生成代码的文本输入法和基于程序合成的结构化引导法，测试其在多样化数据集和图表中的表现。

Result: 两种方法在数据处理不当时表现不佳，并揭示了失败的类型和严重性。

Conclusion: 提出了未来重定向系统设计的关键建议，以优化LLMs在可视化代码适配中的应用。

Abstract: Despite the ubiquity of visualization examples published on the web,
retargeting existing custom chart implementations to new datasets remains
difficult, time-intensive, and tedious. The adaptation process assumes author
familiarity with both the implementation of the example as well as how the new
dataset might need to be transformed to fit into the example code. With recent
advances in Large Language Models (LLMs), automatic adaptation of code can be
achieved from high-level user prompts, reducing the barrier for visualization
retargeting. To better understand how LLMs can assist retargeting and its
potential limitations, we characterize and evaluate the performance of LLM
assistance across multiple datasets and charts of varying complexity,
categorizing failures according to type and severity. In our evaluation, we
compare two approaches: (1) directly instructing the LLM model to fully
generate and adapt code by treating code as text inputs and (2) a more
constrained program synthesis pipeline where the LLM guides the code
construction process by providing structural information (e.g., visual
encodings) based on properties of the example code and data. We find that both
approaches struggle when new data has not been appropriately transformed, and
discuss important design recommendations for future retargeting systems.

</details>


### [25] [Analysis of Drone-Assisted Building Inspection Training in VR vs 2D Monitor Display: an EEG Study](https://arxiv.org/abs/2507.01471)
*Pengkun Liu,Jackson Greene,Jiali Huang,Pingbo Tang,Yu Hou*

Main category: cs.HC

TL;DR: 研究通过脑电图和动态因果建模揭示无人机辅助建筑能源审计任务中不同显示模态下大脑区域间的因果联系，结果显示相似的单向连接模式和训练前后的视觉检查表现相关大脑区域模式。


<details>
  <summary>Details</summary>
Motivation: 探讨无人机飞行员在信息处理和决策过程中大脑区域间的信息流路径，以提高隐性知识传递的效率。

Method: 使用脑电图和动态因果建模分析参与者处理无人机辅助建筑能源审计任务时的大脑区域连接。

Result: 不同模拟组显示出相似的单向连接模式，且训练前后与视觉检查表现相关的大脑区域模式相似。

Conclusion: 研究发现的大脑不对称性可用于测量认知状态和设计无人机检查中知识传递的自适应自动化。

Abstract: Researchers have been using simulation-based methods for drone-assisted
inspection training. Multiple brain regions are associated with information
processes and decision-making, and the connectivity of these regions may
further influence inspectors' performance. However, researchers do not
understand the pathways of the information flows when drone pilots process the
maintenance and manipulation of information, which may affect the efficiency of
tacit knowledge transfer. This study aims to reveal the causal connection
between participants' brain regions using an electroencephalogram and dynamic
causal modeling when processing drone-assisted building energy audit tasks
using different display modalities. The results showed similar single-direction
connectivity patterns for the different simulation groups. The results also
showed similar patterns between brain regions related to visual inspection
performance before and after training. These findings highlight the nature of
brain asymmetries and may be utilized in measuring cognitive states and
designing adaptive automation in the knowledge transfer of drone-based
inspection.

</details>


### [26] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)
*Wen Zhan,Ziqun Hua,Peiyue Lin,Yunfei Chen*

Main category: cs.HC

TL;DR: 本研究探讨老年人（尤其是城市中国移民）如何通过AI辅助共创表达常被忽视或难以言说的个人叙事。


<details>
  <summary>Details</summary>
Motivation: 旨在帮助老年移民通过非数字化的方式表达碎片化或被低估的叙事，同时探索AI在支持叙事创作中的新角色。

Method: 通过结合口头讲述和汉字象征重构的试点工作坊，参与者利用LLM的篆体字形建议和实物材料重新创作字符。

Result: 参与者将生活经验转化为视觉和触觉表达，无需数字素养，展示了AI作为支持机制而非内容生产者的潜力。

Conclusion: 研究为AI与人类协作及老龄化问题提供了新视角，强调AI在支持叙事主体性和社会技术系统中的作用。

Abstract: This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.

</details>


### [27] [Designing for Community Care: Reimagining Support for Equity & Well-being in Academia](https://arxiv.org/abs/2507.01690)
*Beatriz Severes,Ana O. Henriques,Rory Clark,Paulo Bala,Anna Carter,Rua Mae Williams,Geraldine Fitzpatrick*

Main category: cs.HC

TL;DR: 研究探讨如何通过HCI方法和参与式设计重构学术圈中的同伴支持网络，以实现更包容、可持续的系统。


<details>
  <summary>Details</summary>
Motivation: 学术幸福感受到同伴支持网络的深刻影响，但现有网络依赖非正式的个人关系，存在不公平和不可持续的问题。此外，机构对幸福的关注多集中于学生，忽视了教职工的情感劳动，加剧了学术文化的排斥性。

Method: 采用HCI方法、参与式设计和关怀伦理学，通过预研讨会互动、共同设计和反思活动，探索如何将关怀、公平和可持续性嵌入学术同伴支持框架。

Result: 研讨会结束时，参与者将共同开发整合关怀与韧性的学术生态系统设计策略、公平支持系统的资源，并建立一个致力于支持学术社区的同伴网络。

Conclusion: 通过结构化、包容性的设计方法，可以重构学术同伴支持网络，使其更公平、可持续，并涵盖所有学术成员的情感需求。

Abstract: Academic well-being is deeply influenced by peer-support networks, yet they
remain informal, inequitable, and unsustainable, often relying on personal
connections and social capital rather than structured, inclusive systems.
Additionally, institutional well-being responses frequently focus on student
populations, neglecting the emotional labour of faculty and staff, reinforcing
an exclusionary academic culture. Drawing on HCI methodologies, participatory
design, and care ethics, this workshop will provide a space for rethinking how
academic communities can support inclusive networks. Through pre-workshop
engagement, co-design activities, and reflection, participants will examine
systemic gaps in networks and explore ways to embed care, equity, and
sustainability into academic peer-support frameworks -- from informal,
exclusionary models to structured, inclusive care-based ecosystems. At the end
of the workshop, participants will co-develop design strategies for integrating
care and resilience in academic ecosystems, resources for designing equitable
support systems, and a peer network invested and committed to fostering a
supportive academic community.

</details>


### [28] [Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America](https://arxiv.org/abs/2507.01719)
*Dorian Peters,Fernanda Espinoza,Marco da Re,Guido Ivetta,Luciana Benotti,Rafael A. Calvo*

Main category: cs.HC

TL;DR: 本文提出了一种自下而上的本地化方法，通过拉丁美洲的参与者工作坊收集定性数据，研究文化对齐问题、对健康聊天机器人的区域观点以及创建文化适宜的对话AI策略，最终提出了“多元对话AI健康”框架。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）忽视了许多全球性的生活经验，因此需要一种基于本地文化的方法来改善健康领域的对话AI（CAI）。

Method: 通过拉丁美洲的参与性工作坊收集定性数据，分析数字健康中的文化差异、区域对聊天机器人的看法及文化适宜策略。

Result: 研究发现文化在基层失去了学术定义的边界，需要更广泛的框架（包括经济、政治、地理和本地物流）。提出了“多元对话AI健康”框架。

Conclusion: 对话AI的发展需要更注重关系性和包容性，而不仅仅是数据量的增加。

Abstract: There is justifiable interest in leveraging conversational AI (CAI) for
health across the majority world, but to be effective, CAI must respond
appropriately within culturally and linguistically diverse contexts. Therefore,
we need ways to address the fact that current LLMs exclude many lived
experiences globally. Various advances are underway which focus on top-down
approaches and increasing training data. In this paper, we aim to complement
these with a bottom-up locally-grounded approach based on qualitative data
collected during participatory workshops in Latin America. Our goal is to
construct a rich and human-centred understanding of: a) potential areas of
cultural misalignment in digital health; b) regional perspectives on chatbots
for health and c)strategies for creating culturally-appropriate CAI; with a
focus on the understudied Latin American context. Our findings show that
academic boundaries on notions of culture lose meaning at the ground level and
technologies will need to engage with a broader framework; one that
encapsulates the way economics, politics, geography and local logistics are
entangled in cultural experience. To this end, we introduce a framework for
'Pluriversal Conversational AI for Health' which allows for the possibility
that more relationality and tolerance, rather than just more data, may be
called for.

</details>


### [29] [Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents](https://arxiv.org/abs/2507.01862)
*Sanjay Krishna Anbalagan,Xinrui Nie,Umesh Mohan,Vijay Kumar Kanamarlapudi,Anughna Kommalapati,Xiaodan Zhao*

Main category: cs.HC

TL;DR: 论文提出在LLM提示中明确建模GUI启发式任务（如确认和上下文切换），以改善领域特定聊天机器人的多轮交互清晰度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统GUI通过明确的“提交”和“重置”操作管理用户意图，而聊天机器人依赖语言线索易导致上下文混乱。

Method: 将GUI启发式任务建模为LLM提示中的明确任务，并捕捉用户确认、重置和思维链作为结构化会话数据。

Result: 在酒店预订和客户管理场景中，多轮任务连贯性、用户满意度和效率得到提升。

Conclusion: 通过结构化会话数据，可以提升聊天机器人在领域特定任务中的交互清晰度和效率。

Abstract: Domain specific chatbot applications often involve multi step interactions,
such as refining search filters, selecting multiple items, or performing
comparisons. Traditional graphical user interfaces (GUIs) handle these
workflows by providing explicit "Submit" (commit data) and "Reset" (discard
data) actions, allowing back-end systems to track user intent unambiguously. In
contrast, conversational agents rely on subtle language cues, which can lead to
confusion and incomplete context management. This paper proposes modeling these
GUI inspired metaphors acknowledgment (submit like) and context switching
(reset-like) as explicit tasks within large language model (LLM) prompts. By
capturing user acknowledgment, reset actions, and chain of thought (CoT)
reasoning as structured session data, we preserve clarity, reduce user
confusion, and align domain-specific chatbot interactions with back-end logic.
We demonstrate our approach in hotel booking and customer management scenarios,
highlighting improvements in multi-turn task coherence, user satisfaction, and
efficiency.

</details>


### [30] [Spatial tangible user interfaces for cognitive assessment and training](https://arxiv.org/abs/2507.01944)
*Ehud Sharlin,Yuichi Itoh,Benjamin Watson,Yoshifumi Kitamura,Steve Sutphen,Lili Liu,Fumio Kishino*

Main category: cs.HC

TL;DR: 本文探讨了可触用户界面（TUIs）及其在认知评估和训练中的潜在作用，提出了一种称为空间TUIs的子集，能够突破现有交互技术的限制。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过空间TUIs利用人类固有的空间和触觉能力，扩展人机交互的可能性。

Method: 研究方法包括开发了一种新型TUI（Cognitive Cubes），并以此作为实验平台，验证其用于空间能力认知评估和训练的可行性。

Result: 实验结果展示了Cognitive Cubes在空间能力认知评估中的潜力，并可能适用于训练目的。

Conclusion: 结论认为空间TUIs为认知评估和训练提供了新的交互范式，具有实际应用前景。

Abstract: This paper discusses Tangible User Interfaces (TUIs) and their potential
impact on cognitive assessment and cognitive training. We believe that TUIs,
and particularly a subset that we dub spatial TUIs, can extend human computer
interaction beyond some of its current limitations. Spatial TUIs exploit human
innate spatial and tactile ability in an intuitive and direct manner, affording
interaction paradigms that are practically impossible using current interface
technology. As proof-of-concept we examine implementations in the field of
cognitive assessment and training. In this paper we use Cognitive Cubes, a
novel TUI we developed, as an applied test bed for our beliefs, presenting
promising experimental results for cognitive assessment of spatial ability, and
possibly for training purposes.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [31] [A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory](https://arxiv.org/abs/2507.01110)
*Felix Windisch,Lukas Radl,Thomas Köhler,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Main category: cs.GR

TL;DR: 该论文提出了一种无需分块的Gaussian Splatting方法，通过LoD技术和动态流式处理实现大规模场景的高效训练与渲染。


<details>
  <summary>Details</summary>
Motivation: 解决传统分块方法在大规模场景中引入的边界伪影、训练复杂度高及内存限制问题。

Method: 采用LoD表示和混合数据结构，结合动态流式传输和缓存优化，实现单GPU上的高效渲染。

Result: 实现了单消费级GPU下的大规模场景无缝多尺度重建与实时交互可视化。

Conclusion: 该方法为大规模场景的高质量渲染提供了高效且灵活的解决方案。

Abstract: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.

</details>


### [32] [Semiautomatic Simplification](https://arxiv.org/abs/2507.01116)
*Gong Li,Benjamin Watson*

Main category: cs.GR

TL;DR: semisimp是一款半自动简化三维多边形模型的工具，用户在简化过程中可以干预以保留重要区域。


<details>
  <summary>Details</summary>
Motivation: 现有自动简化技术虽然成熟，但对模型的语义区域（如面部和肢体）重要性及使用约束（如动画）不敏感。

Method: semisimp允许用户调整简化操作的顺序，重新分布模型细节，并通过顶点重定位和层次分区改进简化模型。

Result: 工具能够更好地保留模型的语义区域，同时满足特定的使用约束。

Conclusion: semisimp通过用户干预提供了更灵活的模型简化控制，适用于需保留关键细节的场景。

Abstract: We present semisimp, a tool for semiautomatic simplification of three
dimensional polygonal models. Existing automatic simplification technology is
quite mature, but is not sensitive to the heightened importance of distinct
semantic model regions such as faces and limbs, nor to simplification
constraints imposed by model usage such as animation. semisimp allows users to
preserve such regions by intervening in the simplification process. Users can
manipulate the order in which basic simplifications are applied to redistribute
model detail, improve the simplified models themselves by repositioning
vertices with propagation to neighboring levels of detail, and adjust the
hierarchical partitioning of the model surface to segment simplification and
improve control of reordering and position propagation.

</details>


### [33] [Multi-Focus Probes for Context-Preserving Network Exploration and Interaction in Immersive Analytics](https://arxiv.org/abs/2507.01140)
*Eric Zimmermann,Stefan Bruckner*

Main category: cs.GR

TL;DR: 该论文提出了一种用于沉浸式环境的多焦点探针技术，旨在解决用户在局部和全局视图之间切换时的挑战。


<details>
  <summary>Details</summary>
Motivation: 管理复杂网络数据的局部和全局视图之间的过渡是主要难题，需要一种技术来同时支持多尺度交互。

Method: 采用多焦点探针技术，允许用户在沉浸式环境中实例化多个局部视图，同时保持与全局上下文的链接；同时利用视觉和触觉引导机制确保上下文一致性。

Result: 该方法支持用户在编辑网络数据时进行多尺度交互，并保持上下文的一致性。

Conclusion: 多焦点探针技术在沉浸式环境中有效解决了局部与全局视图切换的挑战，提升了网络数据的可编辑性和交互体验。

Abstract: Immersive visualization of network data enables users to physically navigate
and interact with complex structures, but managing transitions between detailed
local (egocentric) views and global (exocentric) overviews remains a major
challenge. We present a multifocus probe technique for immersive environments
that allows users to instantiate multiple egocentric subgraph views while
maintaining persistent links to the global network context. Each probe acts as
a portable local focus, enabling fine-grained inspection and editing of distant
or occluded regions. Visual and haptic guidance mechanisms ensure context
preservation during multi-scale interaction. We demonstrate and discuss the
usability of our technique for the editing of network data.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [34] [Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems](https://arxiv.org/abs/2507.01429)
*Benjamin Chen Ming Choong,Tao Luo,Cheng Liu,Bingsheng He,Wei Zhang,Joey Tianyi Zhou*

Main category: cs.ET

TL;DR: 该论文提出了一种针对赛道内存的高效内存卷积神经网络加速器设计，通过优化算术电路和模型-系统协同设计，提升了嵌入式系统的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 深度学习网络在嵌入式系统中运行时面临资源限制，赛道内存因其高密度特性适合内存计算，但如何在保持性能的同时解决面积和能耗挑战是关键问题。

Method: 设计了一系列适合乘加操作的底层算术电路，并探索了赛道内存系统和CNN架构的协同设计空间。

Result: 实现了小面积内存库，显著提升了赛道内存嵌入式系统的能量和性能表现，同时保持了模型精度。

Conclusion: 通过优化的电路设计和协同策略，论文展示了赛道内存在高性能嵌入式AI应用中的潜力。

Abstract: Deep neural networks generate and process large volumes of data, posing
challenges for low-resource embedded systems. In-memory computing has been
demonstrated as an efficient computing infrastructure and shows promise for
embedded AI applications. Among newly-researched memory technologies, racetrack
memory is a non-volatile technology that allows high data density fabrication,
making it a good fit for in-memory computing. However, integrating in-memory
arithmetic circuits with memory cells affects both the memory density and power
efficiency. It remains challenging to build efficient in-memory arithmetic
circuits on racetrack memory within area and energy constraints. To this end,
we present an efficient in-memory convolutional neural network (CNN)
accelerator optimized for use with racetrack memory. We design a series of
fundamental arithmetic circuits as in-memory computing cells suited for
multiply-and-accumulate operations. Moreover, we explore the design space of
racetrack memory based systems and CNN model architectures, employing co-design
to improve the efficiency and performance of performing CNN inference in
racetrack memory while maintaining model accuracy. Our designed circuits and
model-system co-optimization strategies achieve a small memory bank area with
significant improvements in energy and performance for racetrack memory based
embedded systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [35] [HERCULES: Hardware accElerator foR stoChastic schedULing in hEterogeneous Systems](https://arxiv.org/abs/2507.01113)
*Vairavan Palaniappan,Adam H. Ross,Amit Ranjan Trivedi,Debjit Pal*

Main category: cs.DC

TL;DR: 论文提出了一种基于FPGA的随机在线调度（SOS）加速器，通过硬件并行性和量化优化，显著降低了调度延迟，提升了异构计算系统中的资源利用效率。


<details>
  <summary>Details</summary>
Motivation: 传统软件调度器在高性能异构计算环境中存在调度开销高、动态负载适应性差和资源利用率低的问题，亟需一种高效调度解决方案。

Method: 修改贪婪成本选择策略，结合离散化时间和硬件加速设计，利用硬件并行性和量化技术优化调度过程。

Result: 实验显示，SOS加速器实现了高吞吐、低延迟和节能运行，比单线程软件调度快1060倍，并显著改善了负载分布和资源公平性。

Conclusion: 该硬件加速调度方案为高性能计算和深度学习等关键应用提供了可扩展的替代方案。

Abstract: Efficient workload scheduling is a critical challenge in modern heterogeneous
computing environments, particularly in high-performance computing (HPC)
systems. Traditional software-based schedulers struggle to efficiently balance
workload distribution due to high scheduling overhead, lack of adaptability to
dynamic workloads, and suboptimal resource utilization. These pitfalls are
compounded in heterogeneous systems, where differing computational elements can
have vastly different performance profiles. To resolve these hindrances, we
present a novel FPGA-based accelerator for stochastic online scheduling (SOS).
We modify a greedy cost selection assignment policy by adapting existing cost
equations to engage with discretized time before implementing them into a
hardware accelerator design. Our design leverages hardware parallelism,
precalculation, and precision quantization to reduce job scheduling latency. By
introducing a hardware-accelerated approach to real-time scheduling, this paper
establishes a new paradigm for adaptive scheduling mechanisms in heterogeneous
computing systems. The proposed design achieves high throughput, low latency,
and energy-efficient operation, offering a scalable alternative to traditional
software scheduling methods. Experimental results demonstrate consistent
workload distribution, fair machine utilization, and up to 1060x speedup over
single-threaded software scheduling policy implementations. This makes the SOS
accelerator a strong candidate for deployment in high-performance computing
system, deeplearning pipelines, and other performance-critical applications.

</details>


### [36] [FLARE: A Dataflow-Aware and Scalable Hardware Architecture for Neural-Hybrid Scientific Lossy Compression](https://arxiv.org/abs/2507.01224)
*Wenqi Jia,Ying Huang,Jian Xu,Zhewen Hu,Sian Jin,Jiannan Tian,Yuede Ji,Miao Yin*

Main category: cs.DC

TL;DR: FLARE是一种针对高性能计算中神经混合无损压缩的数据流感知硬件架构，显著提升了吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 科学模拟产生海量数据，但现有压缩框架存在I/O和网络瓶颈，FLARE旨在解决这些问题。

Method: 提出FLARE架构，通过减少片外数据访问和优化数据流来提高效率。

Result: FLARE在运行速度和能效上分别提升了3.5至96.07倍和24.51至520.68倍。

Conclusion: FLARE成功解决了高性能计算中数据压缩的瓶颈问题。

Abstract: Scientific simulation leveraging high-performance computing (HPC) systems is
crucial for modeling complex systems and phenomena in fields such as
astrophysics, climate science, and fluid dynamics, generating massive datasets
that often reach petabyte to exabyte scales. However, managing these vast data
volumes introduces significant I/O and network bottlenecks, limiting practical
performance and scalability. While cutting-edge lossy compression frameworks
powered by deep neural networks (DNNs) have demonstrated superior compression
ratios by capturing complex data correlations, their integration into HPC
workflows poses substantial challenges due to the hybrid non-neural and neural
computation patterns, causing excessive memory access overhead, large
sequential stalls, and limited adaptability to varying data sizes and workloads
in existing hardware platforms. To overcome these challenges and push the limit
of high-performance scientific computing, we for the first time propose FLARE,
a dataflow-aware and scalable hardware architecture for neural-hybrid
scientific lossy compression. FLARE minimizes off-chip data access, reduces
bubble overhead through efficient dataflow, and adopts a modular design that
provides both scalability and flexibility, significantly enhancing throughput
and energy efficiency on modern HPC systems. Particularly, the proposed FLARE
achieves runtime speedups ranging from $3.50 \times$ to $96.07 \times$, and
energy efficiency improvements ranging from $24.51 \times$ to $520.68 \times$,
across various datasets and hardware platforms.

</details>


### [37] [Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration](https://arxiv.org/abs/2507.01225)
*Sunandita Patra,Mehtab Pathan,Mahmoud Mahfouz,Parisa Zehtabi,Wided Ouaja,Daniele Magazzeni,Manuela Veloso*

Main category: cs.DC

TL;DR: 论文提出了一种混合云和本地服务器的资源容量规划与作业调度方法，重点处理资源使用和作业持续时间的不确定性，平衡资源最小化和服务质量。


<details>
  <summary>Details</summary>
Motivation: 在金融行业中，市场条件的不确定性对作业特性影响显著，需要一种能同时优化资源使用和满足用户服务质量需求的方法。

Method: 采用确定性估计器和基于配对采样的约束编程方法进行近似处理。

Result: 基于配对采样的方法在降低峰值资源使用的同时，未影响服务质量。

Conclusion: 该方法在混合环境中有效解决了资源规划和调制的双重挑战。

Abstract: Organizations around the world schedule jobs (programs) regularly to perform
various tasks dictated by their end users. With the major movement towards
using a cloud computing infrastructure, our organization follows a hybrid
approach with both cloud and on-prem servers. The objective of this work is to
perform capacity planning, i.e., estimate resource requirements, and job
scheduling for on-prem grid computing environments. A key contribution of our
approach is handling uncertainty in both resource usage and duration of the
jobs, a critical aspect in the finance industry where stochastic market
conditions significantly influence job characteristics. For capacity planning
and scheduling, we simultaneously balance two conflicting objectives: (a)
minimize resource usage, and (b) provide high quality-of-service to the end
users by completing jobs by their requested deadlines. We propose approximate
approaches using deterministic estimators and pair sampling-based constraint
programming. Our best approach (pair sampling-based) achieves much lower peak
resource usage compared to manual scheduling without compromising on the
quality-of-service.

</details>


### [38] [Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization](https://arxiv.org/abs/2507.01676)
*Giuseppe Ruggeri,Renzo Andri,Daniele Jahier Pagliari,Lukas Cavigelli*

Main category: cs.DC

TL;DR: 针对深度推荐模型（DLRM）推理中的嵌入层瓶颈问题，提出了四种单核高效查找策略及多核非对称映射框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: DLRM推理占Meta数据中AI工作负载的79%，其性能瓶颈在于嵌入层的小向量随机内存访问。

Method: 提出四种单核查找策略和多核非对称映射框架，优化嵌入表查找。

Result: 在华为Ascend加速器上实测，性能提升1.5x至6.5x（极端不平衡分布可达20x），且对查询分布更具鲁棒性。

Conclusion: 所提方法有效解决了嵌入层性能瓶颈，显著提升DLRM推理效率。

Abstract: Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

</details>


### [39] [Optimal Dispersion Under Asynchrony](https://arxiv.org/abs/2507.01298)
*Debasish Pattanayak,Ajay D. Kshemkalyani,Manish Kumar,Anisur Rahaman Molla,Gokarna Sharma*

Main category: cs.DC

TL;DR: 研究了匿名端口标记图中的分散问题，提出了首个在异步设置下达到最优时间复杂度的分散算法。


<details>
  <summary>Details</summary>
Motivation: 分散问题是移动代理分布式计算中的基础任务，匿名性和有限内存带来协调挑战。目标是优化分散时间和内存需求。

Method: 开发了一种在匿名图中构建端口单树的新技术，提出了时间复杂度最优的异步分散算法。

Result: 在异步环境中，算法以$O(k)$时间运行，每个代理使用$O(\log(k+\Delta))$位内存，填补了复杂性空白。

Conclusion: 技术新颖，可能具有独立价值，解决了异步设置下分散问题的复杂度问题。

Abstract: We study the dispersion problem in anonymous port-labeled graphs: $k \leq n$
mobile agents, each with a unique ID and initially located arbitrarily on the
nodes of an $n$-node graph with maximum degree $\Delta$, must autonomously
relocate so that no node hosts more than one agent. Dispersion serves as a
fundamental task in distributed computing of mobile agents, and its complexity
stems from key challenges in local coordination under anonymity and limited
memory.
  The goal is to minimize both the time to achieve dispersion and the memory
required per agent. It is known that any algorithm requires $\Omega(k)$ time in
the worst case, and $\Omega(\log k)$ bits of memory per agent. A recent result
[SPAA'25] gives an optimal $O(k)$-time algorithm in the synchronous setting and
an $O(k \log k)$-time algorithm in the asynchronous setting, both using
$O(\log(k+\Delta))$ bits.
  In this paper, we close the complexity gap in the asynchronous setting by
presenting the first dispersion algorithm that runs in optimal $O(k)$ time
using $O(\log(k+\Delta))$ bits of memory per agent. Our solution is based on a
novel technique we develop in this paper that constructs a port-one tree in
anonymous graphs, which may be of independent interest.

</details>


### [40] [EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices](https://arxiv.org/abs/2507.01438)
*Zheyu Shen,Yexiao He,Ziyao Wang,Yuning Zhang,Guoheng Sun,Wanghao Ye,Ang Li*

Main category: cs.DC

TL;DR: EdgeLoRA是一个高效系统，用于在多租户边缘设备上部署LLMs，通过自适应适配器选择、异构内存管理和批量LoRA推理显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决在多租户边缘设备上高效部署LLMs的挑战，如适配器选择和内存开销问题。

Method: 采用自适应适配器选择、异构内存管理和批量LoRA推理技术。

Result: EdgeLoRA在延迟和吞吐量上显著优于现有方案（如llama.cpp），吞吐量提升4倍，同时支持更多适配器。

Conclusion: EdgeLoRA为资源受限环境提供了可扩展且高效的LLMs边缘部署方案。

Abstract: Large Language Models (LLMs) have gained significant attention due to their
versatility across a wide array of applications. Fine-tuning LLMs with
parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these
models to efficiently adapt to downstream tasks without extensive retraining.
Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial
benefits, such as reduced latency, enhanced privacy, and personalized
responses. However, serving LLMs efficiently on resource-constrained edge
devices presents critical challenges, including the complexity of adapter
selection for different tasks and memory overhead from frequent adapter
swapping. Moreover, given the multiple requests in multi-tenant settings,
processing requests sequentially results in underutilization of computational
resources and increased latency. This paper introduces EdgeLoRA, an efficient
system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA
incorporates three key innovations: (1) an adaptive adapter selection mechanism
to streamline the adapter configuration process; (2) heterogeneous memory
management, leveraging intelligent adapter caching and pooling to mitigate
memory operation overhead; and (3) batch LoRA inference, enabling efficient
batch processing to significantly reduce computational latency. Comprehensive
evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly
outperforms the status quo (i.e., llama.cpp) in terms of both latency and
throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times
boost in throughput. Even more impressively, it can serve several orders of
magnitude more adapters simultaneously. These results highlight EdgeLoRA's
potential to transform edge deployment of LLMs in multi-tenant scenarios,
offering a scalable and efficient solution for resource-constrained
environments.

</details>


### [41] [EDGChain-E: A Decentralized Git-Based Framework for Versioning Encrypted Energy Data](https://arxiv.org/abs/2507.01615)
*Alper Alimoglu,Kamil Erdayandi,Mustafa A. Mustafa,Ümit Cali*

Main category: cs.DC

TL;DR: 论文提出了一种名为EDGChain-E的去中心化框架，利用区块链和星际文件系统管理版本控制和加密的能源数据，支持多方安全协作。


<details>
  <summary>Details</summary>
Motivation: 为了解决能源数据管理中的隐私、完整性和多方协作问题，提出了一种结合区块链和Git的版本控制机制。

Method: 采用区块链和星际文件系统存储加密数据，通过DAO实现协同治理，并使用Git跟踪数据更新以确保隐私和可追溯性。

Result: 框架能够实现能源数据的安全协作，支持FAIR标准，并通过默克尔树确保数据变更的透明性和不可篡改性。

Conclusion: EDGChain-E为去中心化能源应用提供了一种既能保护隐私又能保证数据完整性的解决方案。

Abstract: This paper proposes a new decentralized framework, named EDGChain-E
(Encrypted-Data-Git Chain for Energy), designed to manage version-controlled,
encrypted energy data using blockchain and the InterPlanetary File System. The
framework incorporates a Decentralized Autonomous Organization (DAO) to
orchestrate collaborative data governance across the lifecycle of energy
research and operations, such as smart grid monitoring, demand forecasting, and
peer-to-peer energy trading. In EDGChain-E, initial commits capture the full
encrypted datasets-such as smart meter readings or grid telemetry-while
subsequent updates are tracked as encrypted Git patches, ensuring integrity,
traceability, and privacy. This versioning mechanism supports secure
collaboration across multiple stakeholders (e.g., utilities, researchers,
regulators) without compromising sensitive or regulated information. We
highlight the framework's capability to maintain FAIR-compliant (Findable,
Accessible, Interoperable, Reusable) provenance of encrypted data. By embedding
hash-based content identifiers in Merkle trees, the system enables transparent,
auditable, and immutable tracking of data changes, thereby supporting
reproducibility and trust in decentralized energy applications.

</details>


### [42] [Evolving HPC services to enable ML workloads on HPE Cray EX](https://arxiv.org/abs/2507.01880)
*Stefano Schuppli,Fawzi Mohamed,Henrique Mendonça,Nina Mujkanovic,Elia Palme,Dino Conciatore,Lukas Drescher,Miguel Gila,Pim Witlox,Joost VandeVondele,Maxime Martinasso,Thomas C. Schulthess,Torsten Hoefler*

Main category: cs.DC

TL;DR: 阿尔卑斯研究基础设施利用GH200技术，提供大规模计算能力以满足AI和ML社区的需求。本文探讨了如何扩展HPC服务以支持ML工作负载，并提出了一系列技术改进，如用户环境优化、性能筛选工具和存储基础设施定制等。


<details>
  <summary>Details</summary>
Motivation: 传统HPC服务难以满足ML社区的动态需求，阿尔卑斯基础设施旨在填补这一差距，提供更适合ML工作负载的支持。

Method: 通过早期访问阶段的观察，提出了包括用户环境设计、性能筛查工具、可观测性服务、节点验证工具和服务平面基础设施在内的技术改进。

Result: 提出的改进有助于ML工作负载在HPC系统上的执行，提升系统可用性和弹性。

Conclusion: 这些建议不仅满足了ML社区的需求，也为HPC基础设施的更广泛变革提供了参考。

Abstract: The Alps Research Infrastructure leverages GH200 technology at scale,
featuring 10,752 GPUs. Accessing Alps provides a significant computational
advantage for researchers in Artificial Intelligence (AI) and Machine Learning
(ML). While Alps serves a broad range of scientific communities, traditional
HPC services alone are not sufficient to meet the dynamic needs of the ML
community. This paper presents an initial investigation into extending HPC
service capabilities to better support ML workloads. We identify key challenges
and gaps we have observed since the early-access phase (2023) of Alps by the
Swiss AI community and propose several technological enhancements. These
include a user environment designed to facilitate the adoption of HPC for ML
workloads, balancing performance with flexibility; a utility for rapid
performance screening of ML applications during development; observability
capabilities and data products for inspecting ongoing large-scale ML workloads;
a utility to simplify the vetting of allocated nodes for compute readiness; a
service plane infrastructure to deploy various types of workloads, including
support and inference services; and a storage infrastructure tailored to the
specific needs of ML workloads. These enhancements aim to facilitate the
execution of ML workloads on HPC systems, increase system usability and
resilience, and better align with the needs of the ML community. We also
discuss our current approach to security aspects. This paper concludes by
placing these proposals in the broader context of changes in the communities
served by HPC infrastructure like ours.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [43] [MobileRAG: A Fast, Memory-Efficient, and Energy-Efficient Method for On-Device RAG](https://arxiv.org/abs/2507.01079)
*Taehwan Park,Geonho Lee,Min-Soo Kim*

Main category: cs.DB

TL;DR: MobileRAG是一种完全在设备上运行的RAG框架，通过EcoVector和SCR方法解决了移动设备资源受限的问题，显著降低了延迟、内存和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法假设计算资源充足，不适用于移动设备的资源受限环境。

Method: 结合了移动友好的EcoVector向量搜索算法和轻量级SCR方法，部分加载索引数据并过滤无关文本。

Result: 在延迟、内存和功耗方面显著优于传统方法，同时保持准确性并支持离线隐私保护。

Conclusion: MobileRAG为资源受限环境提供了高效的RAG解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has proven effective on server
infrastructures, but its application on mobile devices is still underexplored
due to limited memory and power resources. Existing vector search and RAG
solutions largely assume abundant computation resources, making them
impractical for on-device scenarios. In this paper, we propose MobileRAG, a
fully on-device pipeline that overcomes these limitations by combining a
mobile-friendly vector search algorithm, \textit{EcoVector}, with a lightweight
\textit{Selective Content Reduction} (SCR) method. By partitioning and
partially loading index data, EcoVector drastically reduces both memory
footprint and CPU usage, while the SCR method filters out irrelevant text to
diminish Language Model (LM) input size without degrading accuracy. Extensive
experiments demonstrated that MobileRAG significantly outperforms conventional
vector search and RAG methods in terms of latency, memory usage, and power
consumption, while maintaining accuracy and enabling offline operation to
safeguard privacy in resource-constrained environments.

</details>


### [44] [Handling out-of-order input arrival in CEP engines on the edge combining optimistic, pessimistic and lazy evaluation](https://arxiv.org/abs/2507.01461)
*Styliani Kyrama,Anastasios Gounaris*

Main category: cs.DB

TL;DR: LimeCEP是一种混合CEP方法，通过懒惰评估、缓冲和推测处理高效处理数据不一致性，支持多模式检测，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 解决复杂事件处理中乱序、延迟和重复事件的问题，特别是在资源受限设备上。

Method: 结合懒惰评估、缓冲和推测处理，集成Kafka优化消息顺序和去重，提供可配置策略。

Result: 相比SASE和FlinkCEP，延迟降低六个数量级，内存和CPU使用显著减少，保持高精度和召回率。

Conclusion: LimeCEP适合非云端部署，高效处理高乱序输入流。

Abstract: In Complex Event Processing, handling out-of-order, late, and duplicate
events is critical for real-time analytics, especially on resource-constrained
devices that process heterogeneous data from multiple sources. We present
LimeCEP, a hybrid CEP approach that combines lazy evaluation, buffering, and
speculative processing to efficiently handle data inconsistencies while
supporting multi-pattern detection under relaxed semantics. LimeCEP integrates
Kafka for efficient message ordering, retention, and duplicate elimination, and
offers configurable strategies to trade off between accuracy, latency, and
resource consumption. Compared to state-of-the-art systems like SASE and
FlinkCEP, LimeCEP achieves up to six orders of magnitude lower latency, with up
to 10 times lower memory usage and 6 times lower CPU utilization, while
maintaining near-perfect precision and recall under high-disorder input
streams, making it well-suited for non-cloud deployments.

</details>


### [45] [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
*Zhaoyan Sun,Jiayi Wang,Xinyang Zhao,Jiachi Wang,Guoliang Li*

Main category: cs.DB

TL;DR: 论文提出‘Data Agent’概念，旨在通过整合LLM技术提升Data+AI系统的语义理解、推理和规划能力，以自动化数据任务。


<details>
  <summary>Details</summary>
Motivation: 传统Data+AI系统依赖人工专家协调流程，缺乏语义理解和规划能力，而LLM的成功为解决这一问题提供了可能。

Method: 提出‘Data Agent’架构，整合知识理解、推理和规划能力，具体包括数据科学代理、数据分析代理（如非结构化数据代理、语义结构化数据代理等）和DBA代理。

Result: 论文探讨了设计数据代理的挑战，并展示了多种代理系统的具体示例。

Conclusion: 设计数据代理系统仍需解决多个开放性问题，但LLM技术的整合为Data+AI系统的自动化提供了新的可能性。

Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

</details>


### [46] [PathDB: A system for evaluating regular path queries](https://arxiv.org/abs/2507.01755)
*Roberto García,Renzo Angles,Vicente Rojas,Sebastián Ferrada*

Main category: cs.DB

TL;DR: PathDB是一个基于Java的内存图数据库，通过正则路径查询和封闭路径代数优化路径处理，性能优于DFS和BFS方法。


<details>
  <summary>Details</summary>
Motivation: 设计PathDB是为了高效处理动态和复杂的路径查询，同时提供模块化优化能力。

Method: 采用正则路径查询（RPQ）和封闭路径代数，通过解析器、逻辑计划和物理计划三个组件处理路径。

Result: 实验表明PathDB在执行时间和灵活性上优于DFS和BFS等基准方法。

Conclusion: PathDB的模块化设计和优化策略使其在复杂路径查询中表现出色。

Abstract: PathDB is a Java-based graph database designed for in-memory data loading and
querying. By utilizing Regular Path Queries (RPQ) and a closed path algebra,
PathDB processes paths through its three main components: the parser, the
logical plan, and the physical plan. This modular design allows for targeted
optimizations and modifications without impacting overall functionality.
Benchmark experiments illustrate PathDB's execution times and flexibility in
handling dynamic and complex path queries, compared to baseline methods like
Depth-First Search (DFS) and Breadth-First Search (BFS) guided by an automaton,
highlighting its optimizations that contribute to its performance.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [47] [CarbonClarity: Understanding and Addressing Uncertainty in Embodied Carbon for Sustainable Computing](https://arxiv.org/abs/2507.01145)
*Xuesi Chen,Leo Han,Anvita Bhagavathula,Udit Gupta*

Main category: cs.AR

TL;DR: CarbonClarity是一个概率框架，用于模拟半导体供应链中的不确定性和碳排放，帮助设计师做出更明智的决策。


<details>
  <summary>Details</summary>
Motivation: 现有碳排放模型的确定性无法反映半导体供应链的时空变异性，限制了设计师的碳感知决策能力。

Method: 引入CarbonClarity框架，通过分布模型量化能源、气体、良率和碳强度的不确定性。

Result: 结果显示，7nm技术节点的碳排放差异可达1.6倍，芯片技术和成熟节点能显著降低碳排放及其不确定性。

Conclusion: CarbonClarity为设备配置和设计选择提供了有力支持，同时展示了芯片技术和成熟节点在减排和降低不确定性方面的优势。

Abstract: Embodied carbon footprint modeling has become an area of growing interest due
to its significant contribution to carbon emissions in computing. However, the
deterministic nature of the existing models fail to account for the spatial and
temporal variability in the semiconductor supply chain. The absence of
uncertainty modeling limits system designers' ability to make informed,
carbon-aware decisions. We introduce CarbonClarity, a probabilistic framework
designed to model embodied carbon footprints through distributions that reflect
uncertainties in energy-per-area, gas-per-area, yield, and carbon intensity
across different technology nodes. Our framework enables a deeper understanding
of how design choices, such as chiplet architectures and new vs. old technology
node selection, impact emissions and their associated uncertainties. For
example, we show that the gap between the mean and 95th percentile of embodied
carbon per cm$^2$ can reach up to 1.6X for the 7nm technology node.
Additionally, we demonstrate through case studies that: (i) CarbonClarity is a
valuable resource for device provisioning, help maintaining performance under a
tight carbon budget; and (ii) chiplet technology and mature nodes not only
reduce embodied carbon but also significantly lower its associated uncertainty,
achieving an 18% reduction in the 95th percentile compared to monolithic
designs for the mobile application.

</details>


### [48] [SD-Acc: Accelerating Stable Diffusion through Phase-aware Sampling and Hardware Co-Optimizations](https://arxiv.org/abs/2507.01309)
*Zhican Wang,Guanghui He,Hongxiang Fan*

Main category: cs.AR

TL;DR: SD-Acc是一种针对Stable Diffusion模型的算法与硬件协同优化框架，可显著提升计算效率与能源利用率。


<details>
  <summary>Details</summary>
Motivation: Stable Diffusion模型在计算和内存需求上的高负担限制了其推理速度和能源效率。

Method: 算法层通过自适应相位感知采样减少计算和内存负载；硬件层采用地址中心数据流和两阶段流式架构优化非线性和异构操作。

Result: SD-Acc在减少3倍计算需求的同时保持图像质量，硬件加速器实现了比传统CPU/GPU更高的速度和能效。

Conclusion: SD-Acc通过算法与硬件协同优化，为Stable Diffusion模型提供了高效的解决方案。

Abstract: The emergence of diffusion models has significantly advanced generative AI,
improving the quality, realism, and creativity of image and video generation.
Among them, Stable Diffusion (StableDiff) stands out as a key model for
text-to-image generation and a foundation for next-generation multi-modal
algorithms. However, its high computational and memory demands hinder inference
speed and energy efficiency. To address these challenges, we identify three
core issues: (1) intensive and often redundant computations, (2) heterogeneous
operations involving convolutions and attention mechanisms, and (3) diverse
weight and activation sizes.
  We present SD-Acc, a novel algorithm and hardware co-optimization framework.
At the algorithm level, we observe that high-level features in certain
denoising phases show significant similarity, enabling approximate computation.
Leveraging this, we propose an adaptive, phase-aware sampling strategy that
reduces compute and memory loads. This framework automatically balances image
quality and complexity based on the StableDiff model and user requirements. At
the hardware level, we design an address-centric dataflow to efficiently handle
heterogeneous operations within a simple systolic array. We address the
bottleneck of nonlinear functions via a two-stage streaming architecture and a
reconfigurable vector processing unit. Additionally, we implement adaptive
dataflow optimizations by combining dynamic reuse and operator fusion tailored
to StableDiff workloads, significantly reducing memory access. Across multiple
StableDiff models, our method achieves up to a 3x reduction in computational
demand without compromising image quality. Combined with our optimized hardware
accelerator, SD-Acc delivers higher speed and energy efficiency than
traditional CPU and GPU implementations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [49] [Revisiting Noise-adaptive Transpilation in Quantum Computing: How Much Impact Does it Have?](https://arxiv.org/abs/2507.01195)
*Yuqian Huo,Jinbiao Wei,Christopher Kverne,Mayur Akewar,Janki Bhimani,Tirthak Patel*

Main category: quant-ph

TL;DR: 研究表明，频繁的噪声感知转译并非必要，随机映射和一次性编译可以高效替代。


<details>
  <summary>Details</summary>
Motivation: 探讨噪声感知转译的必要性及其对量子电路性能和效率的影响。

Method: 在五台IBM 127量子位的计算机上，对16种量子算法进行实证研究。

Result: 噪声感知转译会导致工作负载集中在少数量子位上，增加误差变异性；随机映射和一次性编译可保持相近保真度。

Conclusion: 提出轻量级替代方案，减少转译开销而不牺牲保真度，提升量子工作流的效率。

Abstract: Transpilation, particularly noise-aware optimization, is widely regarded as
essential for maximizing the performance of quantum circuits on superconducting
quantum computers. The common wisdom is that each circuit should be transpiled
using up-to-date noise calibration data to optimize fidelity. In this work, we
revisit the necessity of frequent noise-adaptive transpilation, conducting an
in-depth empirical study across five IBM 127-qubit quantum computers and 16
diverse quantum algorithms. Our findings reveal novel and interesting insights:
(1) noise-aware transpilation leads to a heavy concentration of workloads on a
small subset of qubits, which increases output error variability; (2) using
random mapping can mitigate this effect while maintaining comparable average
fidelity; and (3) circuits compiled once with calibration data can be reliably
reused across multiple calibration cycles and time periods without significant
loss in fidelity. These results suggest that the classical overhead associated
with daily, per-circuit noise-aware transpilation may not be justified. We
propose lightweight alternatives that reduce this overhead without sacrificing
fidelity -- offering a path to more efficient and scalable quantum workflows.

</details>


### [50] [Efficient Gate Reordering for Distributed Quantum Compiling in Data Centers](https://arxiv.org/abs/2507.01090)
*Riccardo Mengoni,Walter Nadalin,Mathys Rennela,Jimmy Rotureau,Tom Darras,Julien Laurat,Eleni Diamanti,Ioannis Lavdas*

Main category: quant-ph

TL;DR: 论文探讨了量子计算时代所需的量子网络和软件工具，介绍了编译器araQne如何通过电路重排策略减少量子处理器间的通信成本。


<details>
  <summary>Details</summary>
Motivation: 量子计算需要分布式系统支持，量子网络将成为混合量子数据中心的核心，研究如何减少量子处理器间的通信成本至关重要。

Method: 开发量子编译器araQne，利用门隐形传态协议和电路重排策略来最小化分布成本（纠缠对数）。

Result: 电路重排策略显著降低了分布成本，相比基线方法有显著改进。

Conclusion: araQne编译器通过优化电路重排，为量子计算网络的分布式实现提供了高效工具。

Abstract: Just as classical computing relies on distributed systems, the quantum
computing era requires new kinds of infrastructure and software tools. Quantum
networks will become the backbone of hybrid, quantum-augmented data centers, in
which quantum algorithms are distributed over a local network of quantum
processing units (QPUs) interconnected via shared entanglement. In this
context, it is crucial to develop methods and software that minimize the number
of inter-QPU communications. Here we describe key features of the quantum
compiler araQne, which is designed to minimize distribution cost, measured by
the number of entangled pairs required to distribute a monolithic quantum
circuit using gate teleportation protocols. We establish the crucial role
played by circuit reordering strategies, which strongly reduce the distribution
cost compared to a baseline approach.

</details>


### [51] [Analyzing Common Electronic Structure Theory Algorithms for Distributed Quantum Computing](https://arxiv.org/abs/2507.01902)
*Grier M. Jones,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: 论文探讨了分布式量子计算（DQC）在量子化学中的应用，分析了五种电子结构方法，发现多数无法通过局部操作（LO）高效并行化，需开发新方法。


<details>
  <summary>Details</summary>
Motivation: 为了推进量子计算的实际应用，研究分布式量子计算（DQC）在量子化学中的潜力，因为量子化学被视为量子计算的“杀手级应用”。

Method: 分析了五种常见电子结构方法（如Tequila和ffsim中的方法），并利用Qiskit Circuit Cutting插件评估这些方法在LO下的并行化能力。

Result: 发现多数电子结构方法无法通过LO高效并行化，限制了其在DQC框架中的应用。

Conclusion: 需要开发新方法才能将电子结构理论有效应用于分布式量子计算框架中。

Abstract: To move towards the utility era of quantum computing, many corporations have
posed distributed quantum computing (DQC) as a framework for scaling the
current generation of devices for practical applications. One of these
applications is quantum chemistry, also known as electronic structure theory,
which has been poised as a "killer application" of quantum computing, To this
end, we analyze five electronic structure methods, found in common packages
such as Tequila and ffsim, which can be easily interfaced with the Qiskit
Circuit Cutting addon. Herein, we provide insights into cutting these
algorithms using local operations (LO) to determine their aptitude for
distribution. The key findings of our work are that many of these algorithms
cannot be efficiently parallelized using LO, and new methods must be developed
to apply electronic structure theory within a DQC framework.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [52] [Workflow-Based Evaluation of Music Generation Systems](https://arxiv.org/abs/2507.01022)
*Shayan Dadman,Bernt Arild Bremdal,Andreas Bergsland*

Main category: eess.AS

TL;DR: 研究探讨了八种开源音乐生成系统（MGS）在当代音乐制作流程中的实际应用，提出了一个结合技术和实践的评估框架，发现MGS主要为辅助工具而非替代品，突显了人类创作的关键作用。


<details>
  <summary>Details</summary>
Motivation: 探讨音乐生成系统在音乐制作中的实际应用潜力，以及如何将其作为协作工具融入非线性创作流程中。

Method: 采用单评估者方法论，结合定性和定量方法，对八种系统进行评估，涵盖作曲、编曲和音效设计任务。

Result: MGS作为辅助工具表现良好，但在保持主题和结构一致性上有局限，强调了人类创造力在复杂决策中的不可替代性。

Conclusion: 研究提供了一个结构化的评估框架，并为未来AI在创意工作流程中的协作应用指明了方向。

Abstract: This study presents an exploratory evaluation of Music Generation Systems
(MGS) within contemporary music production workflows by examining eight
open-source systems. The evaluation framework combines technical insights with
practical experimentation through criteria specifically designed to investigate
the practical and creative affordances of the systems within the iterative,
non-linear nature of music production. Employing a single-evaluator methodology
as a preliminary phase, this research adopts a mixed approach utilizing
qualitative methods to form hypotheses subsequently assessed through
quantitative metrics. The selected systems represent architectural diversity
across both symbolic and audio-based music generation approaches, spanning
composition, arrangement, and sound design tasks. The investigation addresses
limitations of current MGS in music production, challenges and opportunities
for workflow integration, and development potential as collaborative tools
while maintaining artistic authenticity. Findings reveal these systems function
primarily as complementary tools enhancing rather than replacing human
expertise. They exhibit limitations in maintaining thematic and structural
coherence that emphasize the indispensable role of human creativity in tasks
demanding emotional depth and complex decision-making. This study contributes a
structured evaluation framework that considers the iterative nature of music
creation. It identifies methodological refinements necessary for subsequent
comprehensive evaluations and determines viable areas for AI integration as
collaborative tools in creative workflows. The research provides
empirically-grounded insights to guide future development in the field.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [53] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在医疗诊断中的应用仍面临诸多限制，尤其是在痴呆症诊断和护理中。混合方法结合统计学习和专家知识，提高可解释性并契合临床工作流程。


<details>
  <summary>Details</summary>
Motivation: 探索AI在临床实践中实际贡献的局限性，特别是在痴呆症诊断中，以改进AI系统的实用性和可接受性。

Method: 采用范围综述方法，分析AI在临床中的应用，特别是混合方法（如PEIRS和ATHENA-CDS）的有效性。

Result: AI的局限性包括黑盒输出、幻觉问题和弱因果推理。混合方法能提高可解释性和临床工作流程的契合度。

Conclusion: 未来AI决策支持应注重解释性和临床意义，结合神经符号AI和人类专业知识，并以改善临床工作流程和患者结果为成功标准。

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [54] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve是一个AI辅助评分平台，利用大语言模型转录和评估学生作业，显著减少评分时间，同时保持高评分一致性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模STEM课程中手写开放式作业评分的瓶颈问题。

Method: 结合大语言模型（LLMs）和人在环路界面，实现从学生提交到最终反馈的完整评分流程。

Result: 在20多个机构的实际课程中部署，评分30万份作业，评分时间减少65%，高置信度预测与教师评分一致率达95.4%。

Conclusion: Pensieve在提升评分效率的同时保持了评分质量，展现了其在实际教育环境中的实用性。

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [55] [Some remarks on the uncolored versions of the original CFI-graphs](https://arxiv.org/abs/2507.01459)
*Yijia Chen,Jörg Flum,Mingjun Liu*

Main category: cs.DM

TL;DR: 研究表明，去除CFI图的颜色后，其核心性质仍能保留，且可以通过一阶公式恢复颜色信息。


<details>
  <summary>Details</summary>
Motivation: CFI图在图同构测试和带计数的逻辑中至关重要。传统方法去除颜色会改变图的参数和规模，可能影响应用中的组合边界。研究发现无需颜色的CFI变体也能达到相同效果。

Method: 通过去除原始CFI图的颜色，研究其性质是否保留，并构造一阶公式φ(x,y)，在大多数去色CFI图中恢复颜色信息。

Result: 去色后的CFI图仍能保留原始图的核心性质，且可以通过一阶公式φ(x,y)在大多数情况下重新表达颜色关系。

Conclusion: CFI图的颜色并非必需，去色后仍能满足应用需求，且颜色信息可以通过逻辑公式恢复。

Abstract: The CFI-graphs, named after Cai, F\"urer, and Immerman, are central to the
study of the graph isomorphism testing and of first-order logic with counting.
They are colored graphs, and the coloring plays a role in many of their
applications. As usual, it is not hard to remove the coloring by some extra
graph gadgets, but at the cost of blowing up the size of the graphs and
changing some parameters of them as well. This might lead to suboptimal
combinatorial bounds important to their applications. Since then for some
uncolored variants of the CFI-graphs it has been shown that they serve the same
purposes. We show that this already applies to the graphs obtained from the
original CFI-graphs by forgetting the colors. Moreover, we will see that there
is a first-order formula $\varphi(x,y)$ expressing in almost all uncolored
CFI-graphs that $x$ and $y$ have the same color in the corresponding colored
graphs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [56] [Counterfactual Explanation of Shapley Value in Data Coalitions](https://arxiv.org/abs/2507.01267)
*Michelle Si,Jian Pei*

Main category: cs.GT

TL;DR: 论文研究了如何为数据市场中数据所有者的Shapley值提供反事实解释，提出了一种高效的计算方法SV-Exp。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法解释数据联盟中所有者的Shapley值，亟需解决这一问题。

Method: 提出反事实解释问题，证明其存在但计算复杂，最终开发了SV-Exp算法，利用启发式技术加速计算。

Result: 实验验证了方法的效率及反事实解释的有效性。

Conclusion: SV-Exp算法成功解决了Shapley值解释的难题。

Abstract: The Shapley value is widely used for data valuation in data markets. However,
explaining the Shapley value of an owner in a data coalition is an unexplored
and challenging task. To tackle this, we formulate the problem of finding the
counterfactual explanation of Shapley value in data coalitions. Essentially,
given two data owners $A$ and $B$ such that $A$ has a higher Shapley value than
$B$, a counterfactual explanation is a smallest subset of data entries in $A$
such that transferring the subset from $A$ to $B$ makes the Shapley value of
$A$ less than that of $B$. We show that counterfactual explanations always
exist, but finding an exact counterfactual explanation is NP-hard. Using Monte
Carlo estimation to approximate counterfactual explanations directly according
to the definition is still very costly, since we have to estimate the Shapley
values of owners $A$ and $B$ after each possible subset shift. We develop a
series of heuristic techniques to speed up computation by estimating
differential Shapley values, computing the power of singular data entries, and
shifting subsets greedily, culminating in the SV-Exp algorithm. Our
experimental results on real datasets clearly demonstrate the efficiency of our
method and the effectiveness of counterfactuals in interpreting the Shapley
value of an owner.

</details>


### [57] [Rational Censorship Attack: Breaking Blockchain with a Blackboard](https://arxiv.org/abs/2507.01453)
*Michelle Yeo,Haoqian Zhang*

Main category: cs.GT

TL;DR: 区块链的审查韧性是协议安全的基础假设。本文从博弈论视角分析区块链安全性，提出一种理性审查攻击，揭示了当所有用户理性时，少数群体可以通过合谋审查其他节点独占奖励。


<details>
  <summary>Details</summary>
Motivation: 研究区块链安全性时，审查韧性常被视为基础假设。近年来，博弈论和经济角度的分析日益流行，本文旨在探讨当用户理性时，区块链审查韧性的潜在脆弱性。

Method: 通过博弈论框架建模，作者提出一种理性审查攻击，假设合谋群体拥有足够投票权，可以审查其他节点以独占奖励。攻击仅需公共读写黑板，并要求节点知道合谋群体的真实投票权。

Result: 证明了加入理性审查攻击及诚实地声明投票权是子博弈完美均衡。攻击的成功依赖于合谋群体投票权的公开性。

Conclusion: 本文揭示了理性用户环境下区块链审查韧性的潜在风险，并为用户和协议设计者提供了对抗措施的思路。

Abstract: Censorship resilience is a fundamental assumption underlying the security of
blockchain protocols. Additionally, the analysis of blockchain security from an
economic and game theoretic perspective has been growing in popularity in
recent years. In this work, we present a surprising rational censorship attack
on blockchain censorship resilience when we adopt the analysis of blockchain
security from a game theoretic lens and assume all users are rational. In our
attack, a colluding group with sufficient voting power censors the remainder
nodes such that the group alone can gain all the rewards from maintaining the
blockchain. We show that if nodes are rational, coordinating this attack just
requires a public read and write blackboard and we formally model the attack
using a game theoretic framework. Furthermore, we note that to ensure the
success of the attack, nodes need to know the total true voting power held by
the colluding group. We prove that the strategy to join the rational censorship
attack and also for nodes to honestly declare their power is a subgame perfect
equilibrium in the corresponding extensive form game induced by our attack.
Finally, we discuss the implications of the attack on blockchain users and
protocol designers as well as some potential countermeasures.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [58] [Agentic AI in Product Management: A Co-Evolutionary Model](https://arxiv.org/abs/2507.01069)
*Nishant A. Parikh*

Main category: cs.CE

TL;DR: 研究提出了一种协同进化框架，用于指导代理AI在产品生命周期中的集成，重新定义了产品经理的角色为技术生态系统的协调者。


<details>
  <summary>Details</summary>
Motivation: 探讨代理AI在产品管理中的变革作用，填补传统框架的不足，为未来研究和实践提供基础。

Method: 结合系统理论、协同进化理论和人机交互理论，对70多个来源进行综合回顾。

Result: 强调了产品经理与代理AI的相互适应，需要AI素养、治理和系统思维等新技能。

Conclusion: 研究为软件组织提供了负责任且有效的代理AI集成框架，支持未来研究和实践。

Abstract: This study explores agentic AI's transformative role in product management,
proposing a conceptual co-evolutionary framework to guide its integration
across the product lifecycle. Agentic AI, characterized by autonomy,
goal-driven behavior, and multi-agent collaboration, redefines product managers
(PMs) as orchestrators of socio-technical ecosystems. Using systems theory,
co-evolutionary theory, and human-AI interaction theory, the framework maps
agentic AI capabilities in discovery, scoping, business case development,
development, testing, and launch. An integrative review of 70+ sources,
including case studies from leading tech firms, highlights PMs' evolving roles
in AI orchestration, supervision, and strategic alignment. Findings emphasize
mutual adaptation between PMs and AI, requiring skills in AI literacy,
governance, and systems thinking. Addressing gaps in traditional frameworks,
this study provides a foundation for future research and practical
implementation to ensure responsible, effective agentic AI integration in
software organizations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [59] [Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder](https://arxiv.org/abs/2507.01582)
*Jing Luo,Xinyu Yang,Jie Wei*

Main category: cs.SD

TL;DR: 本文提出了ECP表示和XMVAE模型，用于生成具有表现力的古典钢琴演奏，模拟作曲家和钢琴家的双重角色。


<details>
  <summary>Details</summary>
Motivation: 解决从零开始生成古典钢琴演奏的挑战，捕捉作曲家和钢琴家在创作过程中的双重角色。

Method: 引入ECP表示捕获音乐结构和表现细节；提出XMVAE模型，包含VQ-VAE分支（作曲家）和VAE分支（钢琴家），联合训练并利用多尺度编码器和Transformer解码器。

Result: XMVAE生成的演奏在客观和主观评估中均优于现有模型，预训练作曲家分支进一步提升了性能。

Conclusion: XMVAE有效模拟了作曲家和钢琴家的创作过程，生成了高质量的古典钢琴演奏。

Abstract: The creativity of classical music arises not only from composers who craft
the musical sheets but also from performers who interpret the static notations
with expressive nuances. This paper addresses the challenge of generating
classical piano performances from scratch, aiming to emulate the dual roles of
composer and pianist in the creative process. We introduce the Expressive
Compound Word (ECP) representation, which effectively captures both the
metrical structure and expressive nuances of classical performances. Building
on this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a
model featuring two branches: a Vector Quantized Variational AutoEncoder
(VQ-VAE) branch that generates score-related content, representing the
Composer, and a vanilla VAE branch that produces expressive details, fulfilling
the role of Pianist. These branches are jointly trained with similar Seq2Seq
architectures, leveraging a multiscale encoder to capture beat-level contextual
information and an orthogonal Transformer decoder for efficient compound tokens
decoding. Both objective and subjective evaluations demonstrate that XMVAE
generates classical performances with superior musical quality compared to
state-of-the-art models. Furthermore, pretraining the Composer branch on extra
musical score datasets contribute to a significant performance gain.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: 论文介绍了将基于PyTorch的几何学习框架移植到非CUDA硬件（如Gaudi-v2 HPU）的经验，提供了核心工具、教程和案例，降低了研究门槛。


<details>
  <summary>Details</summary>
Motivation: 非CUDA硬件（如Gaudi-v2 HPU）在性能与能效上具有竞争力，但缺乏适配软件，需工程努力和软件创新。

Method: 开发核心工具恢复关键操作（如scatter、稀疏索引等），提供教程和实例，总结失败案例与解决方案。

Result: 成功移植框架，开源GitHub资源库，促进非CUDA硬件上的几何学习研究。

Conclusion: 研究降低了非CUDA硬件使用门槛，为优化与跨平台移植奠定基础。

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [61] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Main category: cs.LG

TL;DR: 本文提出了一种基于TVM编译器的工作流，用于高效地将AI工作负载映射到RISC-V向量单元，通过集成RVV扩展和MetaSchedule框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: RISC-V的向量扩展（RVV）为AI工作负载提供了加速潜力，但缺乏高效的编程工具，限制了其广泛应用。

Method: 将RVV扩展集成到TVM的MetaSchedule框架中，并采用概率编程方法对张量操作进行调优。

Result: 实验表明，相较于GCC的自动向量化和muRISCV-NN库，该方案分别提升了46%和29%的执行速度，且代码占用更少。

Conclusion: 该方法显著提升了AI工作负载在RISC-V上的执行效率，适合嵌入式设备，并开源以便社区进一步扩展。

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [62] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: 论文研究了如何利用最新的时间序列基础模型预测罕见、尖锐的事件（如机器学习服务的生产中断），并对比了该模型与经典随机预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在时间序列预测中表现出色，但尚未用于预测罕见的尖锐事件。本文旨在优化基础模型，以预测高性能机器学习服务的偶发性中断。

Method: 优化了一个先进的基础模型，并与经典随机预测模型（如移动平均和自回归模型）进行对比，分析它们的预测误差和性能。

Result: 研究发现基础模型能较好地捕捉目标数据的关键模式，并与最优参数的模型结合，实现了对一年长中断统计的低误差（<6%）预测。

Conclusion: 优化后的基础模型能够有效预测罕见尖锐事件，优于传统的随机预测模型。

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [63] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 论文提出yProv4ML库，用于收集符合W3C PROV和ProvML标准的JSON格式溯源数据，以优化大规模AI模型的训练效率、执行时间、准确性和能耗。


<details>
  <summary>Details</summary>
Motivation: 随着大规模AI模型需求的增长，如何在计算效率、执行时间、准确性和能耗之间实现平衡成为关键挑战，溯源数据为此提供了重要支持。

Method: 开发了灵活且可扩展的yProv4ML库，支持通过插件集成其他数据收集工具，并与yProv框架完全集成。

Result: yProv4ML库能够帮助研究人员和工程师分析资源使用模式、发现低效问题，并确保AI开发工作流的可重复性和问责性。

Conclusion: 通过yProv4ML库的引入，可以更高效地利用分布式资源，以能源友好的方式扩展大规模AI模型。

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [64] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 论文提出了yProv4ML框架，用于以PROV-JSON格式捕获机器学习过程中的数据来源信息，解决了现有工具在透明度和数据溯源方面的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的发展缺乏透明性和严谨性，尤其是超参数设置和数据来源问题，促使了yProv4ML框架的开发。

Method: 通过yProv4ML框架，以PROV-JSON格式捕获机器学习过程中的数据来源信息，且只需最小化代码修改。

Result: yProv4ML实现了对机器学习过程的数据来源信息的高效捕获，提升了透明度和可追溯性。

Conclusion: yProv4ML为机器学习过程的透明性和数据溯源提供了有效解决方案，弥补了现有工具的不足。

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [65] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas,Mohamed Reda Bouadjenek,Hakim Hacid,Sunil Aryal*

Main category: cs.LG

TL;DR: 该论文提出了一种基于距离的聚合方法Dist-FedAvg，用于图联邦推荐系统中，以提高个性化推荐和聚合效率。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在用户嵌入的复杂性和用户相似性方面存在不足，且无法适应动态用户交互，因此需要一种更高效的聚合方法。

Method: 通过赋予相似用户嵌入更高的聚合权重，并保持锚点用户的影响力，设计了一种名为Dist-FedAvg的聚合方法。

Result: 在多个数据集上的实验表明，Dist-FedAvg在推荐准确性上优于基线方法，并能无缝集成到现有联邦学习框架中。

Conclusion: Dist-FedAvg是一种有效的聚合方法，能够提升图联邦推荐系统的性能，同时兼顾隐私保护。

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [66] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 本文全面评估了大型脑波基础模型（LBMs）在脑机接口（BCI）任务中的表现，发现其相比传统深度架构仅略微提升性能，但参数量显著增加，并通过LoRA技术显著减少了可训练参数。


<details>
  <summary>Details</summary>
Motivation: 探究基础模型在脑波建模中的潜力，评估其在BCI任务中的效率和适用性。

Method: 通过系统性微调实验，包括完整模型微调参数高效适应技术（如LoRA），并进行详细的消融研究。

Result: LBMs仅带来0.9%-1.2%的性能提升，但参数量远高于传统方法；LoRA能减少参数而无性能损失。

Conclusion: LBMs需重新设计以充分发挥潜力，域特定开发策略对推进脑波分析至关重要。

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [67] [GPU-based complete search for nonlinear minimization subject to bounds](https://arxiv.org/abs/2507.01770)
*Guanglu Zhang,Qihang Shan,Jonathan Cagan*

Main category: math.NA

TL;DR: 该论文提出了一种基于GPU的全局搜索方法，用于在变量有界的情况下寻找非线性函数的全局最小值，利用区间分析和高性能GPU架构，确保结果全局最优且高效。


<details>
  <summary>Details</summary>
Motivation: 解决高维度非线性函数的全局优化问题，尤其是在现有方法难以处理80维以上函数时，提供一种高效且严谨的解决方案。

Method: 结合区间分析和GPU并行计算，通过迭代排除不可能包含全局最小值的区域，并采用单程序多数据（SPMD）编程风格和变量循环技术提升效率。

Result: 成功在单GPU上高效搜索并确定了10个多模态测试函数（包括Ackley、Griewank等）在高达10,000维的全局最小值，远超文献报道结果。

Conclusion: 该方法通过GPU架构和独特的算法设计，显著提升了高维度全局优化的效率和可行性，填补了文献中的空白。

Abstract: This paper introduces a GPU-based complete search method to enclose the
global minimum of a nonlinear function subject to simple bounds on the
variables. Using interval analysis, coupled with the computational power and
architecture of GPU, the method iteratively rules out the regions in the search
domain where the global minimum cannot exist and leaves a finite set of regions
where the global minimum must exist. For effectiveness, because of the rigor of
interval analysis, the method is guaranteed to enclose the global minimum of
the nonlinear function even in the presence of rounding errors. For efficiency,
the method employs a novel GPU-based single program, single data parallel
programming style to circumvent major GPU performance bottlenecks, and a
variable cycling technique is also integrated into the method to reduce
computational cost when minimizing large-scale nonlinear functions. The method
is validated by minimizing 10 multimodal benchmark test functions with scalable
dimensions, including the well-known Ackley function, Griewank function, Levy
function, and Rastrigin function. These benchmark test functions represent
grand challenges of global optimization, and enclosing the guaranteed global
minimum of these benchmark test functions with more than 80 dimensions has not
been reported in the literature. Our method completely searches the feasible
domain and successfully encloses the guaranteed global minimum of these 10
benchmark test functions with up to 10,000 dimensions using only one GPU in a
reasonable computation time, far exceeding the reported results in the
literature due to the unique method design and implementation based on GPU
architecture.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [68] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: VLAD是一种视觉语言自动驾驶模型，通过微调VLM与VAD集成，提升空间推理能力，减少碰撞率31.82%。


<details>
  <summary>Details</summary>
Motivation: 利用开源视觉语言模型的互联网规模知识，提升自动驾驶的感知、预测和规划能力。

Method: 采用自定义问答数据集微调VLM，生成高级导航指令，结合VAD系统实现自动驾驶。

Result: 在nuScenes数据集上，碰撞率平均降低31.82%。

Conclusion: VLAD为VLM增强的自动驾驶系统设立了新基准，并提供可解释的驾驶决策。

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [69] [Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0](https://arxiv.org/abs/2507.01462)
*Eneko Osaba,Estibaliz Garrote,Pablo Miranda-Rodriguez,Alessia Ciacco,Itziar Cabanes,Aitziber Mancisidor*

Main category: cs.RO

TL;DR: 该研究探讨了混合量子-经典算法在优化工业环境中基于CAD模型的机器人检测轨迹中的应用，结果显示量子方法在计算时间和解质量上具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用量子计算优化工业自动化中的机器人轨迹规划问题，以满足工业4.0对高效和智能化的需求。

Method: 通过将任务建模为3D旅行商问题，结合不完全图和开放路径约束，比较了D-Wave量子求解器和经典方法（如GUROBI和Google OR-Tools）的性能。

Result: 在五个实际案例中，量子方法在计算时间上显著优于经典方法，同时解质量相当。

Conclusion: 量子方法在工业自动化领域展现出潜力，特别是在需要快速求解复杂优化问题的场景中。

Abstract: This work explores the application of hybrid quantum-classical algorithms to
optimize robotic inspection trajectories derived from Computer-Aided Design
(CAD) models in industrial settings. By modeling the task as a 3D variant of
the Traveling Salesman Problem, incorporating incomplete graphs and open-route
constraints, this study evaluates the performance of two D-Wave-based solvers
against classical methods such as GUROBI and Google OR-Tools. Results across
five real-world cases demonstrate competitive solution quality with
significantly reduced computation times, highlighting the potential of quantum
approaches in automation under Industry 4.0.

</details>


### [70] [Environment-Aware and Human-Cooperative Swing Control for Lower-Limb Prostheses in Diverse Obstacle Scenarios](https://arxiv.org/abs/2507.01111)
*Haosen Xing,Haoran Ma,Sijin Zhang,Hartmut Geyer*

Main category: cs.RO

TL;DR: 当前动力下肢假肢的控制策略缺乏对环境和使用者意图的感知，特别是在复杂地形中。本文提出了一种结合环境感知和用户协作的新控制策略，通过深度摄像头检测障碍物并动态调整步态，实验证明了其高效性。


<details>
  <summary>Details</summary>
Motivation: 动力下肢假肢在复杂地形中的表现受限，尤其是障碍物导航时，现有策略无法实时感知障碍物和用户意图。因此，需要一种更智能的控制方法。

Method: 使用机载深度摄像头在摆动阶段前检测障碍物，通过提前调整摆动轨迹确保跨过障碍物，并在后期摆动阶段依赖用户的生物力学信号。

Result: 实验表明，系统在三名非截肢者参与的测试中，成功处理了超过150次跨越和30次踏上不同高度（4-16厘米）和距离（15-70厘米）的障碍物，成功率达100%。

Conclusion: 该策略有效解决了障碍物导航这一复杂地形移动的关键问题，展示了其适应环境约束和用户意图的能力，具有广泛的应用潜力。

Abstract: Current control strategies for powered lower limb prostheses often lack
awareness of the environment and the user's intended interactions with it. This
limitation becomes particularly apparent in complex terrains. Obstacle
negotiation, a critical scenario exemplifying such challenges, requires both
real-time perception of obstacle geometry and responsiveness to user intention
about when and where to step over or onto, to dynamically adjust swing
trajectories. We propose a novel control strategy that fuses environmental
awareness and human cooperativeness: an on-board depth camera detects obstacles
ahead of swing phase, prompting an elevated early-swing trajectory to ensure
clearance, while late-swing control defers to natural biomechanical cues from
the user. This approach enables intuitive stepping strategies without requiring
unnatural movement patterns. Experiments with three non-amputee participants
demonstrated 100 percent success across more than 150 step-overs and 30
step-ons with randomly placed obstacles of varying heights (4-16 cm) and
distances (15-70 cm). By effectively addressing obstacle navigation -- a
gateway challenge for complex terrain mobility -- our system demonstrates
adaptability to both environmental constraints and user intentions, with
promising applications across diverse locomotion scenarios.

</details>


### [71] [2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration](https://arxiv.org/abs/2507.01206)
*Kathy Zhuang,Zixun Huang,Yukun Song,Rui Li,Yinuo Zhou,Allen Y. Yang*

Main category: cs.RO

TL;DR: 论文提出URSA系统，一种基于LLM的增强现实（AR）系统，用于解决复杂动态环境中的3D物体姿态估计和人类-机器人交互问题，适用于未来太空任务如Artemis。


<details>
  <summary>Details</summary>
Motivation: 随着AR技术的发展，复杂动态环境中的机器感知和3D物体姿态估计面临挑战。本文旨在开发一种非侵入式、空间感知的AR系统，以满足NASA未来太空任务的需求。

Method: URSA系统整合了头戴式AR设备（如HoloLens）、基于LLM的语音控制和机器人跟踪算法，并利用数字孪生定位技术和DTTD-Mobile数据集提升精度。系统还包括DTTDNet（基于Transformer的6DoF姿态估计器）和LMCC（任务可视化控制台）。

Result: URSA系统实现了实时机器人控制和监控，适用于噪声和遮挡环境，且无需地面真实传感器。主要贡献包括非侵入式AR接口、ZED2数据集、LMCC、DTTDNet和端到端集成。

Conclusion: 该研究推动了数字孪生在机器人领域的应用，为航空航天和工业领域提供了可扩展的解决方案。

Abstract: As modern computing advances, new interaction paradigms have emerged,
particularly in Augmented Reality (AR), which overlays virtual interfaces onto
physical objects. This evolution poses challenges in machine perception,
especially for tasks like 3D object pose estimation in complex, dynamic
environments. Our project addresses critical issues in human-robot interaction
within mobile AR, focusing on non-intrusive, spatially aware interfaces. We
present URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024
SUITS challenge, targeting future spaceflight needs such as the Artemis
missions. URSA integrates three core technologies: a head-mounted AR device
(e.g., HoloLens) for intuitive visual feedback, voice control powered by large
language models for hands-free interaction, and robot tracking algorithms that
enable accurate 3D localization in dynamic settings. To enhance precision, we
leverage digital twin localization technologies, using datasets like
DTTD-Mobile and specialized hardware such as the ZED2 camera for real-world
tracking under noise and occlusion. Our system enables real-time robot control
and monitoring via an AR interface, even in the absence of ground-truth
sensors--vital for hazardous or remote operations. Key contributions include:
(1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based
dataset tailored for non-rigid robotic bodies; (3) a Local Mission Control
Console (LMCC) for mission visualization; (4) a transformer-based 6DoF pose
estimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5)
end-to-end integration for astronaut mission support. This work advances
digital twin applications in robotics, offering scalable solutions for both
aerospace and industrial domains.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [72] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 本文提出了一种通过将任务重新定义为铬球修复问题，利用预训练扩散模型Stable Diffusion XL来估计低动态范围图像光照的技术。通过迭代修复计算中值铬球作为低频光照先验，并引入Exposure LoRA生成HDR光照探针。DiffusionLight-Turbo进一步将运行时从30分钟减少到30秒，实现了60倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的HDR全景数据集，存在泛化失败问题，需要一种更通用的光照估计技术。

Method: 提出DiffusionLight，利用迭代修复计算中值铬球作为低频光照先验，并引入Exposure LoRA生成HDR光照探针。进一步开发DiffusionLight-Turbo，通过训练Turbo LoRA直接预测迭代过程结果，实现加速。

Result: 实验结果显示，该方法在不同场景下生成高质量光照估计，并在真实场景中表现出优越的泛化能力。

Conclusion: DiffusionLight及其加速版DiffusionLight-Turbo提供了一种高效且通用的光照估计解决方案，显著提升了速度和性能。

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [73] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 提出了LASADGen，一种基于线性注意力和空间感知衰减的AR模型，解决了现有线性注意力在图像生成中无法捕捉长程依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的AR模型在图像生成中依赖Transformer架构，但面临二次计算复杂度和内存开销大的问题，线性注意力虽能减轻负担但会降低生成质量。

Method: 提出LASAD机制，通过基于真实2D空间位置的衰减因子保留空间关系，并结合LASADGen实现高效生成。

Result: 在ImageNet上，LASADGen实现了高质量的图像生成和计算效率，达到了SOTA性能。

Conclusion: LASADGen成功平衡了线性注意力的效率与高质量图像生成的空间理解需求。

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [74] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: Snake-NeRF是一种扩展到大场景的NeRF框架，通过分块训练和重叠裁剪图像解决了内存问题，保持了高质量且线性时间复杂度的处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法因内存限制难以处理大场景，需要一种能单设备高效运行的解决方案。

Method: 采用分块无重叠的3D瓦片策略和重叠裁剪图像的方法，结合2×2 3D瓦片进展策略和分段采样器。

Result: 实验表明，单GPU上线性时间复杂度处理大场景卫星图像且不牺牲质量。

Conclusion: Snake-NeRF有效解决了大场景NeRF的扩展问题，为实际应用提供了可行性。

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [75] [HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision](https://arxiv.org/abs/2507.01800)
*Shengli Zhou,Jianuo Zhu,Qilin Huang,Fangjing Wang,Yanfu Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为HCNQA的3D VQA模型，通过分层注意力集中监督方法，逐步引导模型从广泛区域聚焦到特定对象，确保推理路径的合理性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D VQA模型通常采用答案中心监督方法，仅关注最终输出而忽略推理路径的监督，可能导致模型依赖表面捷径。此外，慢思考方法存在欠思考问题。

Method: 提出HCNQA模型，采用分层注意力集中监督方法，分三个阶段逐步聚焦具体对象，监督关键检查点以确保推理路径的合理性和有效性。

Result: 实验结果表明，该方法能有效确保模型发展出合理的推理路径，并取得更好的性能。

Conclusion: HCNQA通过分层监督方法解决了现有3D VQA模型的局限性，显著提升了模型性能。

Abstract: 3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential for developing superficial shortcuts
through common patterns in question-answer pairs. Moreover, although
slow-thinking methods advance large language models, they suffer from
underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA
model leveraging a hierarchical concentration narrowing supervision method. By
mimicking the human process of gradually focusing from a broad area to specific
objects while searching for answers, our method guides the model to perform
three phases of concentration narrowing through hierarchical supervision. By
supervising key checkpoints on a general reasoning pathway, our method can
ensure the development of a rational and effective reasoning pathway. Extensive
experimental results demonstrate that our method can effectively ensure that
the model develops a rational reasoning pathway and performs better. The code
is available at https://github.com/JianuoZhu/HCNQA.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [76] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Main category: cs.CR

TL;DR: 本文介绍了一个隐私保护平台，帮助中小型制造商安全共享数据以开发创新工具，并通过食品晶体质量控制的实例展示了该平台的实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决制造商因竞争和隐私问题不愿共享数据，而难以获取创新工具的矛盾。

Method: 提出一个隐私保护平台，制造商通过安全方法共享数据，研究者开发工具后返回到平台供其他人使用，同时保证隐私和机密性。

Result: 开发了一个晶体分析工具，实现了食品晶体图像自动分析，提高了效率和准确性，并通过平台部署使用。

Conclusion: 隐私保护平台成功解决了数据共享与隐私保护的矛盾，未来可进一步扩展应用。

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

</details>


### [77] [On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE](https://arxiv.org/abs/2507.01571)
*Koen T. W. Teuwen,Sam Baggen,Emmanuele Zambon,Luca Allodi*

Main category: cs.CR

TL;DR: 研究表明，标签不平衡会影响自动化安全运营中心（SOC）的分类性能和可解释性，调整检测规则可改善这一现象。


<details>
  <summary>Details</summary>
Motivation: 自动化SOC方法在面对不平衡数据时表现不佳，且需具备可解释性，因此研究标签不平衡对网络入侵警报分类的影响。

Method: 采用最新方法DeepCASE，评估标签不平衡对分类性能和解释正确性的影响。

Result: 标签不平衡对DeepCASE的分类性能和解释正确性均有负面影响，调整检测规则可缓解不平衡。

Conclusion: 改进输入数据质量的传统方法可提升自动化工具的效率和可解释性。

Abstract: Automation in Security Operations Centers (SOCs) plays a prominent role in
alert classification and incident escalation. However, automated methods must
be robust in the presence of imbalanced input data, which can negatively affect
performance. Additionally, automated methods should make explainable decisions.
In this work, we evaluate the effect of label imbalance on the classification
of network intrusion alerts. As our use-case we employ DeepCASE, the
state-of-the-art method for automated alert classification. We show that label
imbalance impacts both classification performance and correctness of the
classification explanations offered by DeepCASE. We conclude tuning the
detection rules used in SOCs can significantly reduce imbalance and may benefit
the performance and explainability offered by alert post-processing methods
such as DeepCASE. Therefore, our findings suggest that traditional methods to
improve the quality of input data can benefit automation.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [78] [A bibliometric analysis on the current situation and hot trends of the impact of microplastics on soil based on CiteSpace](https://arxiv.org/abs/2507.01520)
*Yiran Zheng,Yue Quan,Su Yan,Xinting Lv,Yuguanmin Cao,Minjie Fu,Mingji Jin*

Main category: cs.DL

TL;DR: 综述了2013-2024年土壤微塑料（MPs）的研究现状与发展趋势，通过文献计量工具分析关键词、作者和机构等，揭示了研究热点与演变阶段，并强调未来需关注微塑料对土壤生态的多层次影响。


<details>
  <summary>Details</summary>
Motivation: 微塑料在土壤中积累并通过食物链影响人类健康，需系统性研究以指导污染控制。

Method: 基于Web of Science文献，使用CiteSpace和VOSviewer进行关键词共现、聚类、作者与机构分析。

Result: 研究数量逐年增长，2024年达峰值（956篇）；关键词聚类显示十大主题（如塑料污染、微生物群落）；研究分为三阶段（探索、扩展、整合）。

Conclusion: 未来需深入研究微塑料对土壤生态的层级化影响，以揭示危害并提出解决方案。

Abstract: This paper aims to comprehensively grasp the research status and development
trends of soil microplastics (MPs). It collects studies from the Web of Science
Core Collection covering the period from 2013 to 2024. Employing CiteSpace and
VOSviewer, the paper conducts in - depth analyses of literature regarding the
environmental impacts of microplastics. These analyses involve keyword co -
occurrence, clustering, burst term identification, as well as co - occurrence
analysis of authors and institutions. Microplastics can accumulate in soil,
transfer through food chains, and ultimately affect human health, making the
research on them essential for effective pollution control. Focusing on the
international research on the impacts of microplastics on soil and ecosystems,
the study reveals a steadily increasing trend in the number of publications
each year, reaching a peak of 956 articles in 2024. A small number of highly
productive authors contribute significantly to the overall research output. The
keyword clustering analysis results in ten major clusters, including topics
such as plastic pollution and microbial communities. The research on soil
microplastics has evolved through three distinct stages: the preliminary
exploration phase from 2013 to 2016, the expansion phase from 2017 to 2020, and
the integration phase from 2021 to 2024. For future research, multi - level
assessments of the impacts of microplastics on soil ecosystems and organisms
should be emphasized, in order to fully uncover the associated hazards and
develop practical solutions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [79] [Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis](https://arxiv.org/abs/2507.01053)
*Rafi Al Attrach,Pedro Moreira,Rajna Fani,Renato Umeton,Leo Anthony Celi*

Main category: cs.IR

TL;DR: M3通过自然语言交互降低使用MIMIC-IV数据库的技术门槛，加速临床数据分析。


<details>
  <summary>Details</summary>
Motivation: 解决大型临床数据集（如MIMIC-IV）因复杂性带来的使用障碍，使其更易被广泛研究社区利用。

Method: M3利用语言模型将自然语言问题转换为SQL查询，并结合本地SQLite或BigQuery执行查询，返回结构化和可验证的结果。

Result: 通过M3，研究人员能以自然语言快速完成复杂分析，显著减少传统手工SQL查询的时间和理解临床流程的需求。

Conclusion: M3简化了临床数据的访问，促进了从原始记录到可操作见解的转化，推动了医学研究的进步。

Abstract: As ever-larger clinical datasets become available, they have the potential to
unlock unprecedented opportunities for medical research. Foremost among them is
Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest
open-source EHR database. However, the inherent complexity of these datasets,
particularly the need for sophisticated querying skills and the need to
understand the underlying clinical settings, often presents a significant
barrier to their effective use. M3 lowers the technical barrier to
understanding and querying MIMIC-IV data. With a single command it retrieves
MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the
hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers
converse with the database in plain English. Ask a clinical question in natural
language; M3 uses a language model to translate it into SQL, executes the query
against the MIMIC-IV dataset, and returns structured results alongside the
underlying query for verifiability and reproducibility. Demonstrations show
that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that
once demanded hours of handcrafted SQL and relied on understanding the
complexities of clinical workflows. By simplifying access, M3 invites the
broader research community to mine clinical critical-care data and accelerates
the translation of raw records into actionable insight.

</details>


### [80] [Enhanced Influence-aware Group Recommendation for Online Media Propagation](https://arxiv.org/abs/2507.01616)
*Chengkun He,Xiangmin Zhou,Chen Wang,Longbing Cao,Jie Shao,Xiaodong Li,Guang Xu,Carrie Jinqiu Hu,Zahir Tari*

Main category: cs.IR

TL;DR: 该论文提出了增强型影响力感知群组推荐框架（EIGR），通过图提取采样策略、动态独立级联模型和两级哈希用户组索引，有效解决了社交图规模大、影响传播动态性和实时匹配计算开销等挑战。


<details>
  <summary>Details</summary>
Motivation: 群组推荐在社交媒体流中应用广泛，但传统方法难以应对社交图规模大、影响传播动态性及实时匹配开销等高挑战。

Method: 提出EIGR框架，包括GES策略减少冗余、DYIC模型预测影响传播动态、UG-索引实现实时推荐。

Result: 在真实数据集上的实验表明，EIGR在效果和效率上均优于现有基线方法。

Conclusion: EIGR通过动态建模和高效索引，显著提升了群组推荐的准确性和实时性。

Abstract: Group recommendation over social media streams has attracted significant
attention due to its wide applications in domains such as e-commerce,
entertainment, and online news broadcasting. By leveraging social connections
and group behaviours, group recommendation (GR) aims to provide more accurate
and engaging content to a set of users rather than individuals. Recently,
influence-aware GR has emerged as a promising direction, as it considers the
impact of social influence on group decision-making. In earlier work, we
proposed Influence-aware Group Recommendation (IGR) to solve this task.
However, this task remains challenging due to three key factors: the large and
ever-growing scale of social graphs, the inherently dynamic nature of influence
propagation within user groups, and the high computational overhead of
real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group
Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based
Sampling (GES) strategy to minimise redundancy across multiple temporal social
graphs and effectively capture the evolving dynamics of both groups and items.
Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict
how influence propagates over time across social items and user groups.
Finally, we develop a two-level hash-based User Group Index (UG-Index) to
efficiently organise user groups and enable real-time recommendation
generation. Extensive experiments on real-world datasets demonstrate that our
proposed framework, EIGR, consistently outperforms state-of-the-art baselines
in both effectiveness and efficiency.

</details>


### [81] [Towards a Signal Detection Based Measure for Assessing Information Quality of Explainable Recommender Systems](https://arxiv.org/abs/2507.01168)
*Yeonbin Son,Matthew L. Bolton*

Main category: cs.IR

TL;DR: 论文提出了一种新的客观指标Veracity，用于评估推荐系统中解释的真实性，包括Fidelity和Attunement两个维度，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统的评估多关注推荐性能，而解释质量的评估多依赖主观用户研究，缺乏客观指标。因此，作者旨在填补这一空白。

Method: 将Veracity分解为Fidelity（解释信息准确性）和Attunement（解释与用户偏好匹配度），并利用信号检测理论计算敏感度作为最终值。

Result: 通过设置四种不同信息质量的案例，验证了所提指标能准确捕捉解释质量的差异。

Conclusion: 提出的Veracity指标能有效评估推荐系统中解释的信息质量，为研究提供了有意义的方向。

Abstract: There is growing interest in explainable recommender systems that provide
recommendations along with explanations for the reasoning behind them. When
evaluating recommender systems, most studies focus on overall recommendation
performance. Only a few assess the quality of the explanations. Explanation
quality is often evaluated through user studies that subjectively gather users'
opinions on representative explanatory factors that shape end-users'
perspective towards the results, not about the explanation contents itself. We
aim to fill this gap by developing an objective metric to evaluate Veracity:
the information quality of explanations. Specifically, we decompose Veracity
into two dimensions: Fidelity and Attunement. Fidelity refers to whether the
explanation includes accurate information about the recommended item.
Attunement evaluates whether the explanation reflects the target user's
preferences. By applying signal detection theory, we first determine decision
outcomes for each dimension and then combine them to calculate a sensitivity,
which serves as the final Veracity value. To assess the effectiveness of the
proposed metric, we set up four cases with varying levels of information
quality to validate whether our metric can accurately capture differences in
quality. The results provided meaningful insights into the effectiveness of our
proposed metric.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [82] [Epitome: Pioneering an Experimental Platform for AI-Social Science Integration](https://arxiv.org/abs/2507.01061)
*Jingjing Qu,Kejia Hu,Jun Zhu,Wenhao Li,Teng Wang,Zhiyun Chen,Yulei Ye,Chaochao Lu,Aimin Zhou,Xiangfeng Wang,James Evan*

Main category: cs.CY

TL;DR: Epitome是一个开放式实验平台，专注于AI与社会科学深度融合的平台，旨在通过多学科实验研究AI在社会中的互动影响。


<details>
  <summary>Details</summary>
Motivation: 研究人类与AI的互动及其社会影响，推动跨学科研究。

Method: 通过七个核心模块提供一站式实验解决方案，嵌入社会科学实验的经典逻辑。

Result: 展示了Epitome在简化复杂实验设计和产生稳健结果方面的潜力。

Conclusion: Epitome为AI与社会科学的跨学科研究提供了强大工具。

Abstract: The integration of Large Language Models (LLMs) into social science
experiments represents a transformative approach to understanding human-AI
interactions and their societal impacts. We introduce Epitome, the world's
first open experimental platform dedicated to the deep integration of
artificial intelligence and social science. Rooted in theoretical foundations
from management, communication studies, sociology, psychology, and ethics,
Epitome focuses on the interactive impacts of AI on individuals, organizations,
and society during its real-world deployment. It constructs a theoretical
support system through cross-disciplinary experiments. The platform offers a
one-stop comprehensive experimental solution spanning "foundation
models-complex application development-user feedback" through seven core
modules, while embedding the classical "control-comparison-comparative causal
logic" of social science experiments into multilevel human-computer interaction
environments, including dialogues, group chats, and multi-agent virtual
scenarios. With its canvas-style, user-friendly interface, Epitome enables
researchers to easily design and run complex experimental scenarios,
facilitating systematic investigations into the social impacts of AI and
exploration of integrated solutions.To demonstrate its capabilities, we
replicated three seminal social science experiments involving LLMs, showcasing
Epitome's potential to streamline complex experimental designs and produce
robust results, suitable for publishing in the top selective journals. Our
findings highlight the platform's utility in enhancing the efficiency and
quality of human-AI interactions, providing valuable insights into the societal
implications of AI technologies. Epitome thus offers a powerful tool for
advancing interdisciplinary research at the intersection of AI and social
science, with potential applications in policy-making, ...

</details>
