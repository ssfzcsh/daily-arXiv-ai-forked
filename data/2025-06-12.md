<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.OS](#cs.OS) [Total: 2]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 12]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Formal Methods Meets Readability: Auto-Documenting JML Java Code](https://arxiv.org/abs/2506.09230)
*Juan Carlos Recio Abad,Ruben Saborido,Francisco Chicano*

Main category: cs.SE

TL;DR: 该论文研究了使用JML（Java Modeling Language）形式化规范是否能提高LLM生成的Javadoc文档质量，发现JML显著提升了类级别文档的完整性。


<details>
  <summary>Details</summary>
Motivation: 探索形式化规范（JML）与LLM生成的文档结合是否能生成更完整和准确的文档。

Method: 对JML注解和非注解的Java类生成的文档进行系统比较，通过自动化指标和专家分析评估质量。

Result: JML显著提高了类级别文档的完整性，尤其在捕捉复杂类不变量和设计契约方面表现突出，但对核心描述质量的提升有限。

Conclusion: 形式化规范与LLM协同可提升文档质量，尤其在复杂场景下优势明显，为软件团队采用形式化方法提供了实际指导。

Abstract: This paper investigates whether formal specifications using Java Modeling
Language (JML) can enhance the quality of Large Language Model (LLM)-generated
Javadocs. While LLMs excel at producing documentation from code alone, we
hypothesize that incorporating formally verified invariants yields more
complete and accurate results. We present a systematic comparison of
documentation generated from JML-annotated and non-annotated Java classes,
evaluating quality through both automated metrics and expert analysis. Our
findings demonstrate that JML significantly improves class-level documentation
completeness, with more moderate gains at the method level. Formal
specifications prove particularly effective in capturing complex class
invariants and design contracts that are frequently overlooked in code-only
documentation. A threshold effect emerges, where the benefits of JML become
more pronounced for classes with richer sets of invariants. While JML enhances
specification coverage, its impact on core descriptive quality is limited,
suggesting that formal specifications primarily ensure comprehensive coverage
rather than fundamentally altering implementation descriptions. These results
offer actionable insights for software teams adopting formal methods in
documentation workflows, highlighting scenarios where JML provides clear
advantages. The study contributes to AI-assisted software documentation
research by demonstrating how formal methods and LLMs can synergistically
improve documentation quality.

</details>


### [2] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Main category: cs.SE

TL;DR: 提出UTGenerator和UTBoost，解决SWE-Bench测试用例不足的问题，并纠正了345个错误补丁。


<details>
  <summary>Details</summary>
Motivation: 现有测试用例不足导致生成的补丁通过测试但未解决问题，需改进评估方法。

Method: 利用LLM开发UTGenerator自动生成测试用例，构建UTBoost框架进行增强。

Result: 纠正了345个错误补丁，影响40.9%的SWE-Bench Lite和24.4%的Verified榜单，排名变动18和11位。

Conclusion: UTBoost能有效提升代码生成评估的准确性。

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


### [3] [Assessing the Impact of Refactoring Energy-Inefficient Code Patterns on Software Sustainability: An Industry Case Study](https://arxiv.org/abs/2506.09370)
*Rohit Mehra,Priyavanshi Pathania,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 论文探讨了通过自动化工具识别代码中的能耗低效模式并指导重构，以优化软件可持续性的方法。一项行业案例研究显示，重构后应用程序的能耗减少了29%。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和元宇宙等技术的发展，软件系统的广泛应用导致碳排放增加，影响环境可持续性。通过自动化工具优化软件能耗成为重要研究方向。

Method: 使用自动化软件可持续性评估工具识别代码中的能耗低效模式，并通过重构进行优化。

Result: 重构后，应用程序的每用户每月能耗降低了29%，显示显著的可持续性改善。

Conclusion: 自动化工具在识别和优化软件能耗方面具有潜力，能有效提升环境可持续性。

Abstract: Advances in technologies like artificial intelligence and metaverse have led
to a proliferation of software systems in business and everyday life. With this
widespread penetration, the carbon emissions of software are rapidly growing as
well, thereby negatively impacting the long-term sustainability of our
environment. Hence, optimizing software from a sustainability standpoint
becomes more crucial than ever. We believe that the adoption of automated tools
that can identify energy-inefficient patterns in the code and guide appropriate
refactoring can significantly assist in this optimization. In this extended
abstract, we present an industry case study that evaluates the sustainability
impact of refactoring energy-inefficient code patterns identified by automated
software sustainability assessment tools for a large application. Preliminary
results highlight a positive impact on the application's sustainability
post-refactoring, leading to a 29% decrease in per-user per-month energy
consumption.

</details>


### [4] [Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models](https://arxiv.org/abs/2506.09396)
*Zongjie Li,Shuai Wang*

Main category: cs.SE

TL;DR: 论文提出将推理深度视为可控资源，通过优化整个模型生命周期中的推理预算，在准确性、延迟和成本之间实现更好权衡。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成模型的设计未明确管理快速直接回答与复杂链式推理之间的权衡，影响了效率与性能。

Method: 通过自适应控制推理深度，从合成数据创建到实际部署，优化推理预算。

Result: 提升监督信号质量，推动多维基准测试，实现成本和安全感知的部署策略。

Conclusion: 将快速与慢速推理视为互补模式，可设计出灵活高效的代码生成代理。

Abstract: This position paper proposes a fundamental shift in designing code generation
models: treating reasoning depth as a controllable resource. Rather than being
an incidental byproduct of prompting, we argue that the trade-off between
rapid, direct answers ("fast thinking") and elaborate, chain-of-thought
deliberation ("slow thinking") must be explicitly managed. We contend that
optimizing reasoning budgets across the entire model lifecycle - from synthetic
data creation and benchmarking to real-world deploymen - can unlock superior
trade-offs among accuracy, latency, and cost. This paper outlines how adaptive
control over reasoning can enrich supervision signals, motivate new
multi-dimensional benchmarks, and inform cost-aware, security-conscious
deployment policies. By viewing fast and slow thinking as complementary modes
to be scheduled, we envision coding agents that think deep when necessary and
act fast when possible.

</details>


### [5] [Automated Synthesis of Formally Verified Multi-Abstraction Function Summaries](https://arxiv.org/abs/2506.09550)
*Fanpeng Yang,Xu Ma,Shuling Wang,Xiong Xu,Qinxiang Cao,Naijun Zhan,Xiaofeng Li,Bin Gu*

Main category: cs.SE

TL;DR: 该论文提出了一种结合符号执行、大型语言模型（LLMs）和形式验证的新框架，用于生成功能摘要（函数摘要），以克服C程序中复杂特征的挑战。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如航空航天嵌入式系统）中，功能摘要是理解、重用和验证软件的关键，但传统代码往往缺乏形式化规范，且由于循环、嵌套函数调用和指针别名等复杂特性，自动生成摘要具有挑战性。

Method: 论文提出了一种新框架，整合了符号执行（VST-A）、LLMs（用于推断循环不变量）和形式验证工具（Frama-C），通过迭代优化生成相对最强后置条件（RSPs）并构建功能摘要。

Result: 通过实验比较，证明该方法能够生成完整捕获程序行为的功能摘要，并自动合成在特定领域语言中表达的最强非冗余后置条件。

Conclusion: 该框架有效解决了C程序功能摘要生成的挑战，为形式验证和人类理解提供了灵活的抽象层次支持。

Abstract: Function summaries, which characterize the behavior of code segments
(typically functions) through preconditions and postconditions, are essential
for understanding, reusing, and verifying software, particularly in
safety-critical domains like aerospace embedded systems. However, these
mission-critical legacy code serving as a valuable reused asset often lacks
formal specifications. It is challenging to automatically generate function
summaries for C programs, due to the existence of complex features such as
loops, nested function calls, pointer aliasing, and so on. Moreover, function
summaries should support multiple abstraction levels to meet diverse
requirements, e.g. precise summaries capturing full functionality for formal
verification and intuitive summaries for human understanding.
  To address these challenges, we first propose a novel framework that combines
symbolic execution, large language models (LLMs), and formal verification to
generate Relatively Strongest Postconditions (RSPs) and build function
summaries that fully capture program behavior. Our approach leverages VST-A's
symbolic execution to precisely track program execution paths and state
transitions, employs LLMs to infer loop invariants based on predefined
templates, and uses Frama-C to guarantee soundness of generated summaries in an
iterative refinement loop. Furthermore, from generated RSPs, we automatically
synthesize strongest non-redundant postconditions expressed within given domain
specific language. We compare our approach with existing work through extensive
experiments.

</details>


### [6] [ASTAGEN: Empirical Evaluation of Automated SATD Taxonomy Generation with LLMs](https://arxiv.org/abs/2506.09601)
*Sota Nakashima,Yuta Ishimoto,Masanari Kondo,Tao Xiao,Yasutaka Kamei*

Main category: cs.SE

TL;DR: ASTAGEN利用大型语言模型自动生成自承认技术债务（SATD）分类法，减轻人工分类的负担并提高一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的手工分类SATD耗时费力且主观性强，ASTAGEN旨在通过自动化解决这一问题。

Method: ASTAGEN通过生成SATD评论的简明解释，并迭代生成和更新分类来构建分类法。

Result: ASTAGEN在三个领域中成功恢复了特定领域的分类，如机器学习中的层配置，且成本低廉。

Conclusion: ASTAGEN能够支持半自动分类法构建，为未来自动化分类生成开辟了新途径。

Abstract: Technical debt refers to suboptimal code that degrades software quality. When
developers intentionally introduce such debt, it is called self-admitted
technical debt (SATD). Since SATD hinders maintenance, identifying its
categories is key to uncovering quality issues. Traditionally, constructing
such taxonomies requires manually inspecting SATD comments and surrounding
code, which is time-consuming, labor-intensive, and often inconsistent due to
annotator subjectivity. This study presents ASTAGEN, an initial step toward
automating SATD taxonomy generation using large language models (LLMs). Given a
comment and its surrounding code, ASTAGEN first generates a concise explanation
for each SATD comment, then incrementally generates and updates categories to
construct a taxonomy. We evaluate ASTAGEN on SATD datasets from three domains:
quantum software, smart contracts, and machine learning. It successfully
recovers domain-specific categories reported in prior work, such as Layer
Configuration in machine learning. Compared to a naive use of an LLM, ASTAGEN
produces more consistent category assignments due to its explanation-driven,
iterative design. It also completes taxonomy generation in under two hours and
for less than one USD, even on the largest dataset. These results suggest that
while full automation remains challenging, ASTAGEN is able to support
semi-automated taxonomy construction. Furthermore, our work opens up avenues
for future work, such as automatic taxonomy generation in other areas.

</details>


### [7] [Translating a VDM Model of a Medical Device into Kapture](https://arxiv.org/abs/2506.09636)
*Joe Hare,Leo Freitas,Ken Pierce*

Main category: cs.SE

TL;DR: 论文探讨了使用Kapture工具将医疗设备CANDO的VDM模型转换为Kapture模型的效果，展示了Kapture的可用性和初学者在复杂系统中的建模能力。


<details>
  <summary>Details</summary>
Motivation: 随着关键医疗设备复杂度的增加，需要清晰、可验证的软件需求。研究旨在评估Kapture工具在无形式化方法经验的开发者中的适用性。

Method: 使用Kapture工具将CANDO的VDM模型转换为Kapture模型，并对比结果。

Result: 90%以上的VDM模型被成功转换为Kapture模型，结果匹配。尽管过程中遇到学习曲线问题，但证明Kapture适合初学者建模。

Conclusion: Kapture能有效支持初学者对复杂系统进行形式化建模，但VDM到Kapture的转换存在挑战。

Abstract: As the complexity of safety-critical medical devices increases, so does the
need for clear, verifiable, software requirements. This paper explores the use
of Kapture, a formal modelling tool developed by D-RisQ, to translate an
existing formal VDM model of a medical implant for treating focal epilepsy
called CANDO. The work was undertaken without prior experience in formal
methods. The paper assess Kapture's usability, the challenges of formal
modelling, and the effectiveness of the translated model. The result is a model
in Kapture which covers over 90% of the original VDM model, and produces
matching traces of results. While several issues were encountered during design
and implementation, mainly due to the initial learning curve, this paper
demonstrates that complex systems can be effectively modelled in Kapture by
inexperienced users and highlights some difficulties in translating VDM
specifications to Kapture.

</details>


### [8] [Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead](https://arxiv.org/abs/2506.09683)
*Priyavanshi Pathania,Nikhil Bamby,Rohit Mehra,Samarth Sikand,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 本文综述了测量软件和AI相关能耗及碳排放的最新方法与工具，提出了分类法，并比较了不同工具的维度和粒度，指出了当前研究的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着软件和AI的普及，其能耗和碳排放问题日益突出，需研究优化其对环境的影响。

Method: 通过分类法（监测、估算、黑盒方法）比较现有工具，分析其维度、粒度及组成部分。

Result: 总结了现有工具的实践应用和挑战，强调跨社区合作的重要性。

Conclusion: 需进一步解决当前研究的挑战，促进该领域的协作与发展。

Abstract: The proliferation of software and AI comes with a hidden risk: its growing
energy and carbon footprint. As concerns regarding environmental sustainability
come to the forefront, understanding and optimizing how software impacts the
environment becomes paramount. In this paper, we present a state-of-the-art
review of methods and tools that enable the measurement of software and
AI-related energy and/or carbon emissions. We introduce a taxonomy to
categorize the existing work as Monitoring, Estimation, or Black-Box
approaches. We delve deeper into the tools and compare them across different
dimensions and granularity - for example, whether their measurement encompasses
energy and carbon emissions and the components considered (like CPU, GPU, RAM,
etc.). We present our observations on the practical use (component wise
consolidation of approaches) as well as the challenges that we have identified
across the current state-of-the-art. As we start an initiative to address these
challenges, we emphasize active collaboration across the community in this
important field.

</details>


### [9] [Microservices and Real-Time Processing in Retail IT: A Review of Open-Source Toolchains and Deployment Strategies](https://arxiv.org/abs/2506.09938)
*Aaditaa Vashisht,Rekha B S*

Main category: cs.SE

TL;DR: 文献综述探讨了零售与金融系统如何通过事件驱动和微服务架构（如Kafka、Spring Boot、MongoDB和Kubernetes）实现实时、可扩展与高可用性。


<details>
  <summary>Details</summary>
Motivation: 随着数字化转型加速，零售行业需要实时、可扩展且弹性的系统来管理交易、分析客户行为及订单处理。

Method: 系统回顾了近年来的学术出版物、技术白皮书和行业报告，综合总结了关键主题与实施策略。

Result: Kafka和Spring Boot支持低延迟的事件驱动应用，MongoDB与Kubernetes结合保障了系统的高可用性和容错能力。

Conclusion: 这些技术为行业实践者提供了设计可扩展基础设施的见解，并为教育和研究提供了新方向。

Abstract: With the rapid pace of digital transformation, the retail industry is
increasingly depending on real-time, scalable, and resilient systems to manage
financial transactions, analyze customer behavior, and streamline order
processing. This literature review explores how modern event-driven and
microservices-based architectures, particularly those leveraging Apache Kafka,
Spring Boot, MongoDB, and Kubernetes are transforming retail and financial
systems. By systematically reviewing academic publications, technical white
papers, and industry reports from recent years, this study synthesizes key
themes and implementation strategies. The analysis reveals that technologies
like Kafka and Spring Boot are instrumental in building low-latency,
event-driven applications that support real-time analytics and fraud detection,
while MongoDB, when deployed on Kubernetes, ensures fault tolerance and high
availability in inventory and transaction systems. Kubernetes itself plays a
crucial role in automating deployment and scaling of microservices. These
findings provide valuable insights for industry practitioners aiming to design
scalable infrastructures, identify research opportunities in hybrid deployment
models, and offer educators a foundation to integrate modern system
architectures into professional and technical communication training.

</details>


### [10] [Mapping NVD Records to Their VFCs: How Hard is it?](https://arxiv.org/abs/2506.09702)
*Huu Hung Nguyen,Duc Manh Tran,Yiran Cheng,Thanh Le-Cong,Hong Jin Kang,Ratnadira Widyasari,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: 该研究探讨了将NVD记录映射到漏洞修复提交(VFCs)的可行性，通过手动和自动方法提取VFCs，并利用外部数据库和GitHub填补空白，但仍有88.7%的记录未映射。


<details>
  <summary>Details</summary>
Motivation: 由于NVD记录中显式链接稀少，映射漏洞修复提交(VFCs)具有挑战性，研究旨在探索其可行性并为安全研究提供支持。

Method: 通过手动分析NVD引用，识别Git和非Git引用的有效性；构建自动化管道从NVD记录中提取VFCs；结合外部安全数据库和GitHub补充数据。

Result: 从NVD记录中成功提取31,942个VFCs（覆盖8.7%），并结合外部数据库和GitHub，总覆盖率达11.3%，但仍有88.7%的记录未映射。

Conclusion: 尽管Git引用表现良好，但大多数NVD记录仍需进一步研究。研究为漏洞数据集增强和自动化安全研究提供了方向。

Abstract: Mapping National Vulnerability Database (NVD) records to vulnerability-fixing
commits (VFCs) is crucial for vulnerability analysis but challenging due to
sparse explicit links in NVD references.This study explores this mapping's
feasibility through an empirical approach. Manual analysis of NVD references
showed Git references enable over 86% success, while non-Git references achieve
under 14%. Using these findings, we built an automated pipeline extracting
31,942 VFCs from 20,360 NVD records (8.7% of 235,341) with 87% precision,
mainly from Git references. To fill gaps, we mined six external security
databases, yielding 29,254 VFCs for 18,985 records (8.1%) at 88.4% precision,
and GitHub repositories, adding 3,686 VFCs for 2,795 records (1.2%) at 73%
precision. Combining these, we mapped 26,710 unique records (11.3% coverage)
from 7,634 projects, with overlap between NVD and external databases, plus
unique GitHub contributions. Despite success with Git references, 88.7% of
records remain unmapped, highlighting the difficulty without Git links. This
study offers insights for enhancing vulnerability datasets and guiding future
automated security research.

</details>


### [11] [A First Look at Bugs in LLM Inference Engines](https://arxiv.org/abs/2506.09713)
*Mugeng Liu,Siqi Zhong,Weichen Bi,Yixuan Zhang,Zhiyang Chen,Zhenpeng Chen,Xuanzhe Liu,Yun Ma*

Main category: cs.SE

TL;DR: 该论文首次对大语言模型推理引擎中的bug进行了系统性的实证研究，总结了常见症状、根本原因，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理引擎在现代AI基础设施中扮演重要角色，但其bug问题缺乏系统性研究。

Method: 通过分析5个广泛应用的LLM推理引擎的官方仓库，构建了929个真实bug的数据集，并进行开放编码分析。

Result: 研究揭示了6种主要bug症状和28种根本原因，为bug检测和定位提供了关键见解。

Conclusion: 基于研究发现，提出了对研究者、引擎供应商和应用开发者的实用建议，以提高LLM推理引擎的可靠性。

Abstract: Large language model-specific inference engines (in short as \emph{LLM
inference engines}) have become a fundamental component of modern AI
infrastructure, enabling the deployment of LLM-powered applications (LLM apps)
across cloud and local devices. Despite their critical role, LLM inference
engines are prone to bugs due to the immense resource demands of LLMs and the
complexities of cross-platform compatibility. However, a systematic
understanding of these bugs remains lacking. To bridge this gap, we present the
first empirical study on bugs in LLM inference engines. We mine official
repositories of 5 widely adopted LLM inference engines, constructing a
comprehensive dataset of 929 real-world bugs. Through a rigorous open coding
process, we analyze these bugs to uncover their symptoms, root causes, and
commonality. Our findings reveal six major bug symptoms and a taxonomy of 28
root causes, shedding light on the key challenges in bug detection and location
within LLM inference engines. Based on these insights, we propose a series of
actionable implications for researchers, inference engine vendors, and LLM app
developers.

</details>


### [12] [Towards Bridging Formal Methods and Human Interpretability](https://arxiv.org/abs/2506.09759)
*Abhijit Paul,Proma Chowdhury,Kazi Sakib*

Main category: cs.SE

TL;DR: 该研究填补了关于人类对标记转换系统（LTS）设计理解的空白，提出了7个关键指标，并验证了Albin复杂度等指标最能反映人类理解能力。应用这些指标显著提升了设计修复工具的效率。


<details>
  <summary>Details</summary>
Motivation: 填补LTS设计中人类理解研究的空白，提升模型检查和设计修复工具的可用性。

Method: 结合软件工程和图论提出7个指标，通过Bradley-Terry模型和Kendall's Tau分析验证指标有效性，并在设计修复工具中应用Albin复杂度。

Result: Albin复杂度、状态空间大小、圈复杂度和冗余度最能反映人类理解能力；应用Albin复杂度使理解时间减少39%。

Conclusion: 强调人类因素的指标能提升LTS设计的可解释性，为未来研究提供实用工具。

Abstract: Labeled Transition Systems (LTS) are integral to model checking and design
repair tools. System engineers frequently examine LTS designs during model
checking or design repair to debug, identify inconsistencies, and validate
system behavior. Despite LTS's significance, no prior research has examined
human comprehension of these designs. To address this, we draw on traditional
software engineering and graph theory, identifying 7 key metrics: cyclomatic
complexity, state space size, average branching factor, maximum depth, Albin
complexity, modularity, and redundancy. We created a dataset of 148 LTS
designs, sampling 48 for 324 paired comparisons, and ranked them using the
Bradley-Terry model. Through Kendall's Tau correlation analysis, we found that
Albin complexity ($\tau = 0.444$), state space size ($\tau = 0.420$),
cyclomatic complexity ($\tau = 0.366$), and redundancy ($\tau = 0.315$) most
accurately reflect human comprehension of LTS designs. To showcase the metrics'
utility, we applied the Albin complexity metric within the Fortis design repair
tool, ranking system redesigns. This ranking reduced annotators' comprehension
time by 39\%, suggesting that metrics emphasizing human factors can enhance
formal design interpretability.

</details>


### [13] [variability.dev: Towards an Online Toolbox for Feature Modeling](https://arxiv.org/abs/2506.09845)
*Tobias Heß,Lukas Ostheimer,Tobias Betz,Simon Karrer,Tim Jannik Schmidt,Pierre Coquet,Sean Semmler,Thomas Thüm*

Main category: cs.SE

TL;DR: 摘要介绍了特征模型在可配置系统中的默认应用，并指出现有在线编辑器的不足，展示了正在开发的在线工具箱variability.dev。


<details>
  <summary>Details</summary>
Motivation: 当前在线特征模型编辑器功能有限、维护不足或需要离线安装，缺乏有效的协作工具。

Method: 开发基于FeatureIDE库的在线工具箱variability.dev，包括协作特征模型编辑器和在线配置器。

Result: 展示了正在开发中的工具箱，强调了其协作能力和在线配置功能。

Conclusion: variability.dev有望填补当前在线特征模型编辑工具的空白，提供更高效的协作和配置体验。

Abstract: The emergence of feature models as the default to model the variability in
configurable systems fosters a rich diversity in applications, application
domains, and perspectives. Independent of their domain, modelers require to
open, view, edit, transform, save, and configure models as well as to
collaborate with others. However, at the time of writing, the top five results
when googling ``Online Editor Feature Model'' point to editors that either have
minimal functionality, are unmaintained or defunct, or require an offline
installation, such as FeatureIDE. In this work we present a preview of our
in-development online toolbox for feature modeling, variability.dev. In
particular, we showcase our collaborative feature-model editor and our online
configurator both of which are built on top of the FeatureIDE library.

</details>


### [14] [Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice](https://arxiv.org/abs/2506.09873)
*Emma Kallina,Thomas Bohné,Jat Singh*

Main category: cs.SE

TL;DR: 该研究探讨了现有利益相关者参与（SHI）实践对负责任AI（rAI）的贡献及其潜在脱节，发现SHI在实践中主要受商业优先级驱动，与rAI目标不匹配。


<details>
  <summary>Details</summary>
Motivation: 研究旨在明确现有SHI实践对rAI的贡献程度，揭示脱节问题，为未来干预提供依据，推动行业实践向rAI发展。

Method: 分析56份rAI指导文件以识别SHI的预期益处；通过在线调查（n=130）和半结构化访谈（n=10）了解商业实践中SHI的驱动因素。

Result: 发现SHI在实践中主要由商业优先级驱动（如客户价值、合规性），与rAI目标（如权力再分配、风险预见）存在脱节。

Conclusion: 需提出干预措施和研究机会，以弥合SHI与rAI之间的脱节，促进rAI在实践中发展。

Abstract: Responsible AI (rAI) guidance increasingly promotes stakeholder involvement
(SHI) during AI development. At the same time, SHI is already common in
commercial software development, but with potentially different foci. This
study clarifies the extent to which established SHI practices are able to
contribute to rAI efforts as well as potential disconnects -- essential
insights to inform and tailor future interventions that further shift industry
practice towards rAI efforts. First, we analysed 56 rAI guidance documents to
identify why SHI is recommended (i.e. its expected benefits for rAI) and
uncovered goals such as redistributing power, improving socio-technical
understandings, anticipating risks, and enhancing public oversight. To
understand why and how SHI is currently practised in commercial settings, we
then conducted an online survey (n=130) and semi-structured interviews (n=10)
with AI practitioners. Our findings reveal that SHI in practice is primarily
driven by commercial priorities (e.g. customer value, compliance) and several
factors currently discourage more rAI-aligned SHI practices. This suggests that
established SHI practices are largely not contributing to rAI efforts. To
address this disconnect, we propose interventions and research opportunities to
advance rAI development in practice.

</details>


### [15] [Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation](https://arxiv.org/abs/2506.09929)
*Scott Schnelle,Francesca Favaro,Laura Fraade-Blanar,David Wichner,Holland Broce,Justin Miranda*

Main category: cs.SE

TL;DR: 该论文探讨了如何通过评估安全用例中的主张与证据支持来建立其可信度，并提供了评分策略和改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶系统（ADS）技术的进步，确保安全和公众信任需要强有力的保证框架，安全用例成为关键工具。

Method: 论文从安全用例的构成要素（主张、证据等）出发，评估主张的支撑性（包括程序支持和实施支持）以及证据状态。提供了评分表和评估指南。

Result: 提出了一种评估安全用例可信度的方法，包括对每个主张和证据的独立评估，并讨论了治理和改进建议。

Conclusion: 该研究为全面评估安全用例的可信度提供了基础，有助于推动ADS技术的安全整合和社会认可。

Abstract: As Automated Driving Systems (ADS) technology advances, ensuring safety and
public trust requires robust assurance frameworks, with safety cases emerging
as a critical tool toward such a goal. This paper explores an approach to
assess how a safety case is supported by its claims and evidence, toward
establishing credibility for the overall case. Starting from a description of
the building blocks of a safety case (claims, evidence, and optional
format-dependent entries), this paper delves into the assessment of support of
each claim through the provided evidence. Two domains of assessment are
outlined for each claim: procedural support (formalizing process specification)
and implementation support (demonstrating process application). Additionally,
an assessment of evidence status is also undertaken, independently from the
claims support. Scoring strategies and evaluation guidelines are provided,
including detailed scoring tables for claim support and evidence status
assessment. The paper further discusses governance, continual improvement, and
timing considerations for safety case assessments. Reporting of results and
findings is contextualized within its primary use for internal decision-making
on continual improvement efforts. The presented approach builds on state of the
art auditing practices, but specifically tackles the question of judging the
credibility of a safety case. While not conclusive on its own, it provides a
starting point toward a comprehensive "Case Credibility Assessment" (CCA),
starting from the evaluation of the support for each claim (individually and in
aggregate), as well as every piece of evidence provided. By delving into the
technical intricacies of ADS safety cases, this work contributes to the ongoing
discourse on safety assurance and aims to facilitate the responsible
integration of ADS technology into society.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [16] [Mainframe-style channel controllers for modern disaggregated memory systems](https://arxiv.org/abs/2506.09758)
*Zikai Liu,Jasmin Schult,Pengcheng Xu,Timothy Roscoe*

Main category: cs.OS

TL;DR: 尽管近数据处理技术有潜力缓解主内存瓶颈且已有商业硬件实现，但其在实际应用中的部署较少。作者认为缺乏操作系统为中心的抽象是主要障碍，提出了内存通道控制器作为一种新的抽象方法。


<details>
  <summary>Details</summary>
Motivation: 近数据处理技术在解决主内存瓶颈方面具有潜力，但由于缺乏清晰的OS抽象，影响了其广泛应用。特别是在现代解耦内存系统（如CXL内存池）中，这一问题尤为突出。

Method: 作者提出内存通道控制器的概念，借鉴了主frame系统中连接CPU和磁盘驱动器的通道控制器，将其作为近数据处理的抽象层。这种设计不仅便于OS集成，还无需更改CPU架构，同时利用新兴互连的缓存一致性提供更细粒度的编程模型。

Result: 内存通道控制器提供了便携、可虚拟化的近数据处理抽象，支持更丰富的编程模型和更细粒度的交互。

Conclusion: 通过引入内存通道控制器，作者为近数据处理提供了清晰的OS抽象，有望推动该技术在解耦内存系统中的广泛应用。

Abstract: Despite the promise of alleviating the main memory bottleneck, and the
existence of commercial hardware implementations, techniques for Near-Data
Processing have seen relatively little real-world deployment. The idea has
received renewed interest with the appearance of disaggregated or "far" memory,
for example in the use of CXL memory pools.
  However, we argue that the lack of a clear OS-centric abstraction of
Near-Data Processing is a major barrier to adoption of the technology. Inspired
by the channel controllers which interface the CPU to disk drives in mainframe
systems, we propose memory channel controllers as a convenient, portable, and
virtualizable abstraction of Near-Data Processing for modern disaggregated
memory systems.
  In addition to providing a clean abstraction that enables OS integration
while requiring no changes to CPU architecture, memory channel controllers
incorporate another key innovation: they exploit the cache coherence provided
by emerging interconnects to provide a much richer programming model, with more
fine-grained interaction, than has been possible with existing designs.

</details>


### [17] [On the Impossibility of a Perfect Hypervisor](https://arxiv.org/abs/2506.09825)
*Mordechai Guri*

Main category: cs.OS

TL;DR: 论文证明了‘完美虚拟机监控程序’（完美hypervisor）的存在是不可能的，任何拥有有限计算资源的机器都无法实现。


<details>
  <summary>Details</summary>
Motivation: 探讨虚拟化的基本原理和限制，分析‘完美hypervisor’的理论可能性。

Method: 通过构建理论模型，提出并证明两个定理：不可检测性定理和不可能性定理。

Result: 不可检测性定理表明完美hypervisor无法被区分；不可能性定理证明其在实际中无法存在。

Conclusion: 研究为虚拟化的理论和实践提供了基础，明确了其局限性。

Abstract: We establish a fundamental impossibility result for a `perfect hypervisor',
one that (1) preserves every observable behavior of any program exactly as on
bare metal and (2) adds zero timing or resource overhead.
  Within this model we prove two theorems. (1) Indetectability Theorem. If such
a hypervisor existed, no guest-level program, measurement, or timing test could
distinguish it from native execution; all traces, outputs, and timings would be
identical.
  (2) Impossibility Theorem. Despite that theoretical indetectability, a
perfect hypervisor cannot exist on any machine with finite computational
resources.
  These results are architecture-agnostic and extend beyond hypervisors to any
virtualization layer emulators, sandboxes, containers, or
runtime-instrumentation frameworks. Together they provide a formal foundation
for future work on the principles and limits of virtualization.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [18] [MOSE: A Novel Orchestration Framework for Stateful Microservice Migration at the Edge](https://arxiv.org/abs/2506.09159)
*Antonio Calagna,Yenchia Yu,Paolo Giaccone,Carla Fabiana Chiasserini*

Main category: cs.NI

TL;DR: 该论文提出了一种新颖的框架，通过实现状态迁移的高效实施和过程编排，显著提升了迁移性能，并在两个实际用例中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决边缘网络中微服务移动性的两个核心挑战：迁移过程的实施和编排，以满足网络和应用KPI目标。

Method: 提出了一种框架，结合高效的迁移实施和智能编排，通过实验验证其性能。

Result: 迁移停机时间比现有技术减少77%，并成功满足对延迟敏感的微服务的用户QoE要求。

Conclusion: 该框架在配置迁移过程和满足KPI目标方面优于现有技术，适用于多种关键场景。

Abstract: Stateful migration has emerged as the dominant technology to support
microservice mobility at the network edge while ensuring a satisfying
experience to mobile end users. This work addresses two pivotal challenges,
namely, the implementation and the orchestration of the migration process. We
first introduce a novel framework that efficiently implements stateful
migration and effectively orchestrates the migration process by fulfilling both
network and application KPI targets. Through experimental validation using
realistic microservices, we then show that our solution (i) greatly improves
migration performance, yielding up to 77% decrease of the migration downtime
with respect to the state of the art, and (ii) successfully addresses the
strict user QoE requirements of critical scenarios featuring latency-sensitive
microservices. Further, we consider two practical use cases, featuring,
respectively, a UAV autopilot microservice and a multi-object tracking task,
and demonstrate how our framework outperforms current state-of-the-art
approaches in configuring the migration process and in meeting KPI targets.

</details>


### [19] [Adaptive Bandwidth Sharing for Optimizing QoE of Real-Time Video](https://arxiv.org/abs/2506.09197)
*Sushi Anna George,Vinay Joseph*

Main category: cs.NI

TL;DR: 提出了一种迭代半静态带宽共享策略，平衡静态和动态共享方法的优点，以减少协调频率并满足实时流量的QoE需求。


<details>
  <summary>Details</summary>
Motivation: 提高无线网络中实时流量管理的效率，优化资源利用并减少拥塞。

Method: 采用了迭代优化方法，结合频谱共享和客户端资源分配，通过理论证明其最优性。

Result: 策略在各种系统参数下表现良好，实现了接近最优的带宽分配，减少了开销。

Conclusion: 该方法是一种实用的解决方案，适用于实时无线应用。

Abstract: The concept of spectrum or bandwidth sharing has gained significant global
attention as a means to enhance the efficiency of real-time traffic management
in wireless networks. Effective bandwidth sharing enables optimal utilization
of available resources, reducing congestion and improving QoE for
delay-sensitive applications such as real-time video transmission. In this
paper, we propose a novel iterative semi-static bandwidth sharing policy that
balances the advantages of both static and dynamic sharing approaches. Our
approach minimizes the frequency of coordination between network operators
while ensuring efficient resource allocation and meeting the stringent QoE
demands of real-time traffic. The proposed policy iteratively optimizes both
the spectrum sharing between operators and the resource allocation for
individual clients. We establish strong theoretical guarantees for the
optimality of the proposed policy and prove that it converges to the optimal
static sharing policy irrespective of initial conditions or fluctuations in
traffic arrival rates. Additionally, we conduct extensive simulations to
evaluate the impact of key system parameters - including step size, hyperperiod
length, and arrival process dynamics - on the performance of our policy. Our
results demonstrate the effectiveness of the proposed approach in achieving
near-optimal bandwidth allocation with reduced overhead, making it a practical
solution for real-time wireless applications.

</details>


### [20] [Age of Information in Unreliable Tandem Queues](https://arxiv.org/abs/2506.09245)
*Muthukrishnan Senthilkumar,Aresh Dadlani,Hina Tabassum*

Main category: cs.NI

TL;DR: 该论文提出了一个分析框架，用于研究不可靠节点串联队列系统中数据新鲜度（AoI）的影响。


<details>
  <summary>Details</summary>
Motivation: 实时应用和物联网的普及对信息时效性提出严格要求，现有AoI模型假设网络节点完全可靠，无法准确反映节点传输失败的情况。

Method: 使用概率生成函数推导无限缓冲区M/M/1串联系统的停留时间分布，并扩展到任意数量不可靠节点的M/G/1系统。

Result: 数值结果表明，马尔可夫和非马尔可夫服务时间的关键参数对AoI有显著影响。

Conclusion: 研究为不可靠串联队列系统中的数据新鲜度分析提供了理论支持和实用工具。

Abstract: Stringent demands for timely information delivery, driven by the widespread
adoption of real-time applications and the Internet of Things, have established
the age of information (AoI) as a critical metric for quantifying data
freshness. Existing AoI models often assume multi-hop communication networks
with fully reliable nodes, which may not accurately capture scenarios involving
node transmission failures. This paper presents an analytical framework for two
configurations of tandem queue systems, where status updates generated by a
single sensor are relayed to a destination monitor through unreliable
intermediate nodes. Using the probability generating function, we first derive
the sojourn time distribution for an infinite-buffer M/M/1 tandem system with
two unreliable nodes. We then extend our analysis to an M/G/1 tandem system
with an arbitrary number of unreliable nodes, employing the supplementary
variable technique while assuming that only the first node has an infinite
buffer. Numerical results demonstrate the impact of key system parameters on
the average AoI in unreliable tandem queues with Markovian and non-Markovian
service times.

</details>


### [21] [A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.09268)
*Henri Alam,Antonio de Domenico,Tareq Si Salem,Florian Kaltenberger*

Main category: cs.NI

TL;DR: 该论文提出了一种基于多臂老虎机模型的在线优化框架，用于集成地面与非地面网络（TN-NTN），以实时平衡网络容量和能效，从而提升网络可持续性。


<details>
  <summary>Details</summary>
Motivation: 随着地面网络部署的密集化问题日益突出，研究非地面网络（NTN）在减轻负载和提高能效方面的潜力，以实现更可持续的网络。

Method: 采用多臂老虎机（MAB）模型和Bandit-feedback Constrained Online Mirror Descent（BCOMD）算法，动态优化带宽分配、用户设备关联和宏基站关闭等关键参数。

Result: 24小时系统级模拟显示，该框架显著减少了高峰时段未满足需求的用户比例，在低流量时段实现了19%的吞吐量增益和5%的节能。

Conclusion: 该优化框架在提升网络性能和能效方面优于3GPP标准推荐方案，证明了NTN在支持可持续网络中的重要作用。

Abstract: Integrated terrestrial and non-terrestrial network (TN-NTN) architectures
offer a promising solution for expanding coverage and improving capacity for
the network. While non-terrestrial networks (NTNs) are primarily exploited for
these specific reasons, their role in alleviating terrestrial network (TN) load
and enabling energy-efficient operation has received comparatively less
attention. In light of growing concerns associated with the densification of
terrestrial deployments, this work aims to explore the potential of NTNs in
supporting a more sustainable network. In this paper, we propose a novel online
optimisation framework for integrated TN-NTN architectures, built on a
multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback
Constrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively
optimises key system parameters--including bandwidth allocation, user equipment
(UE) association, and macro base station (MBS) shutdown--to balance network
capacity and energy efficiency in real time. Extensive system-level simulations
over a 24-hour period show that our framework significantly reduces the
proportion of unsatisfied UEs during peak hours and achieves up to 19%
throughput gains and 5% energy savings in low-traffic periods, outperforming
standard network settings following 3GPP recommendations.

</details>


### [22] [Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach](https://arxiv.org/abs/2506.09647)
*Lei Deng,Wenhan Xu,Jingwei Li,Danny H. K. Tsang*

Main category: cs.NI

TL;DR: 研究提出一种生成模型方法，用于处理缺失数据的实时网络流量预测，通过张量补全和预训练生成模型实现高效计算和恢复。


<details>
  <summary>Details</summary>
Motivation: 现实中的网络流量数据常因各种原因不完整，现有方法假设数据完整，无法直接应用。

Method: 将流量预测建模为张量补全问题，结合预训练生成模型捕获数据低秩结构，优化潜在表示实现实时预测。

Result: 在Abilene数据集上验证，预测误差（MAE）低于0.002，实时性在100毫秒内。

Conclusion: 方法在缺失数据下实现高精度实时预测，具有理论恢复保证和实际应用潜力。

Abstract: Real-time network traffic forecasting is crucial for network management and
early resource allocation. Existing network traffic forecasting approaches
operate under the assumption that the network traffic data is fully observed.
However, in practical scenarios, the collected data are often incomplete due to
various human and natural factors. In this paper, we propose a generative model
approach for real-time network traffic forecasting with missing data. Firstly,
we model the network traffic forecasting task as a tensor completion problem.
Secondly, we incorporate a pre-trained generative model to achieve the low-rank
structure commonly associated with tensor completion. The generative model
effectively captures the intrinsic low-rank structure of network traffic data
during pre-training and enables the mapping from a compact latent
representation to the tensor space. Thirdly, rather than directly optimizing
the high-dimensional tensor, we optimize its latent representation, which
simplifies the optimization process and enables real-time forecasting. We also
establish a theoretical recovery guarantee that quantifies the error bound of
the proposed approach. Experiments on real-world datasets demonstrate that our
approach achieves accurate network traffic forecasting within 100 ms, with a
mean absolute error (MAE) below 0.002, as validated on the Abilene dataset.

</details>


### [23] [Multi-Level Damage-Aware Graph Learning for Resilient UAV Swarm Networks](https://arxiv.org/abs/2506.09703)
*Huan Lin,Chenguang Zhu,Lianghui Ding,Feng Yang*

Main category: cs.NI

TL;DR: 提出了一种名为ML-DAGL的新算法，通过挖掘被毁无人机的信息生成恢复轨迹，解决了无人机群网络中因拓扑不均匀和稀疏导致的过聚合与不收敛问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图学习的弹性算法在大规模损坏场景中由于拓扑不均匀和稀疏性，会面临过聚合和不收敛的问题，亟需一种更高效的解决方案。

Method: 设计了多分支损伤注意力模块（MBDA）构建多跳损伤感知图（mDAG），并结合扩张图卷积网络（DGCN）生成最优恢复轨迹。

Result: 仿真结果显示，该算法能在大规模损坏下恢复连通性，恢复时间缩短75.94%，并提升了拓扑均匀性。

Conclusion: ML-DAGL算法显著提升了无人机群网络的弹性，为大规模损坏下的通信恢复提供了有效解决方案。

Abstract: Unmanned aerial vehicle (UAV) swarm networks leverage resilient algorithms to
address communication network split issues and restore connectivity. However,
existing graph learning-based resilient algorithms face over-aggregation and
non-convergence problems caused by uneven and sparse topology under massive
damage scenarios. To alleviate these problems, we propose a novel Multi-Level
Damage-Aware Graph Learning (ML-DAGL) algorithm, which generates recovery
trajectories by mining information from destroyed UAVs. We first introduce a
Multi-Branch Damage Attention (MBDA) module, which forms a sequence of
multi-hop Damage Attentive Graphs (mDAG) with different ranges of receptive
fields. Each mDAG links only remaining and damaged nodes to ensure a more even
degree distribution for mitigating over-aggregation, and utilizes multi-hop
dilation to establish more links for sparse topology enhancement. To resort to
the mDAG, we propose a Dilated Graph Convolution Network (DGCN), which
generates the optimal recovery trajectories with theoretically proven
convergence under massive damage cases. Simulation results show that the
proposed algorithm can guarantee the connectivity restoration under large swarm
and damage scales, while significantly expediting the recovery time by 75.94%
and improving the topology uniformity after recovery.

</details>


### [24] [Virtualizing RAN: Science, Strategy, and Architecture of Software-Defined Mobile Networks](https://arxiv.org/abs/2506.09878)
*Ryan Barker*

Main category: cs.NI

TL;DR: 本文提出了一种集成的RAN（无线电接入网络）虚拟化分析框架，涵盖频谱、技术、业务策略和文化因素，并展示了其在实际应用中的效能提升。


<details>
  <summary>Details</summary>
Motivation: 当前关于RAN虚拟化的讨论往往将频谱政策、云计算和组织准备情况割裂开来，缺乏综合分析。本文旨在填补这一空白。

Method: 通过比较研究、数学建模和案例分析，评估了频谱利用、虚拟化技术、AI优化和安全成本等因素对RAN性能的影响。

Result: 中频连续频谱比毫米波部署性能更优；硬件能力受限于400 MHz毫米波通道；文化变革可大幅缩短网络部署周期。

Conclusion: 未来研究应关注6G的THz频段、能效AI加速器和零信任编排，同时对运营商、供应商和研究人员提出实用建议。

Abstract: Virtualising the Radio Access Network (RAN) is widely touted as the
corner-stone of affordable 5G and a prerequisite for AI-native 6G. Yet current
discourse often isolates spectrum policy, cloud engineering and organisational
readiness into silos. This paper delivers an integrated analysis that spans
science, technology, business strategy and culture. I first review
spectrum-auction economics and show-via a comparative study of T-Mobile US and
Verizon-that mid-band contiguity leveraged through software-defined carrier
aggregation outperforms mmWave-centric deployments in both coverage and churn
metrics. I then formalise the technical foundations of virtualised and open
RAN, deriving capacity limits from contiguous and dis-contiguous spectrum maths
and quantifying hardware ceilings for 400 MHz mmWave channels. Edge compute
platforms (NVIDIA EGX, Samsung vRAN 3.0) and SDN-controlled RAN Intelligent
Controllers are examined alongside AI ML pipelines that enable
digital-twin-driven optimisation. A security cost model extends recent O-RAN
measurements to show how 256-bit cipher enforcement adds 35-60 us latency
unless mitigated by inline crypto off-load. Finally, a national automation case
study of live vRAN sites -- demonstrates an 81 to 13 day cycle-time reduction
once cultural change errors are corrected. I conclude with open research
challenges for sub-THz 6G, energy-neutral AI accelerators and zero-trust
orchestration, offering actionable recommendations for operators, vendors and
researchers.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [25] [Dynamic Sub-region Search in Homogeneous Collections Using CLIP](https://arxiv.org/abs/2506.09506)
*Bastian Jäckl,Vojtěch Kloda,Daniel A. Keim,Jakub Lokoč*

Main category: cs.MM

TL;DR: 论文研究了如何通过补充文本查询的位置信息来提高在高度同质的特定领域图像集合中的检索召回率。动态图像分区方法被提出以改善语义搜索性能。


<details>
  <summary>Details</summary>
Motivation: 用户在特定领域（如水下图像）中难以提供描述性文本查询，导致召回率低。研究探讨是否通过添加位置信息可以改善这一问题。

Method: 采用动态图像分区方法，使用户可以指定识别的区域，同时结合位置约束和语义搜索模型。比较了动态与静态分区方法的性能。

Result: 动态分区方法比静态方法的检索性能提高了两倍，但对查询位置扰动非常敏感。

Conclusion: 动态分区方法在提高召回率方面具有潜力，但其性能依赖于精确的位置信息。

Abstract: Querying with text-image-based search engines in highly homogeneous
domain-specific image collections is challenging for users, as they often
struggle to provide descriptive text queries. For example, in an underwater
domain, users can usually characterize entities only with abstract labels, such
as corals and fish, which leads to low recall rates. Our work investigates
whether recall can be improved by supplementing text queries with position
information. Specifically, we explore dynamic image partitioning approaches
that divide candidates into semantically meaningful regions of interest.
Instead of querying entire images, users can specify regions they recognize.
This enables the use of position constraints while preserving the semantic
capabilities of multimodal models. We introduce and evaluate strategies for
integrating position constraints into semantic search models and compare them
against static partitioning approaches. Our evaluation highlights both the
potential and the limitations of sub-region-based search methods using dynamic
partitioning. Dynamic search models achieve up to double the retrieval
performance compared to static partitioning approaches but are highly sensitive
to perturbations in the specified query positions.

</details>


### [26] [Learning Quality from Complexity and Structure: A Feature-Fused XGBoost Model for Video Quality Assessment](https://arxiv.org/abs/2506.09795)
*Amritha Premkumar,Prajit T Rajendran,Vignesh V Menon*

Main category: cs.MM

TL;DR: 本文提出了一种新颖的减参考视频质量评估（VQA）方法，结合低复杂度特征和结构信息，通过XGBoost回归预测质量分数，具有高效和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决视频质量评估中高效性和可扩展性的需求，避免使用深度学习模型的高计算成本。

Method: 提取时空特征和SSIM值，通过时间池化和残差特征聚合，训练XGBoost回归模型。

Result: 在挑战数据集上表现出与主观评分高度相关的竞争力，计算开销低。

Conclusion: 该方法适用于实时流媒体质量监控和自适应编码场景，兼具轻量化和强泛化能力。

Abstract: This paper presents a novel approach for reduced-reference video quality
assessment (VQA), developed as part of the recent VQA Grand Challenge. Our
method leverages low-level complexity and structural information from reference
and test videos to predict perceptual quality scores. Specifically, we extract
spatio-temporal features using Video Complexity Analyzer (VCA) and compute SSIM
values from the test video to capture both texture and structural
characteristics. These features are aggregated through temporal pooling, and
residual features are calculated by comparing the original and distorted
feature sets. The combined features are used to train an XGBoost regression
model that estimates the overall video quality. The pipeline is fully
automated, interpretable, and highly scalable, requiring no deep neural
networks or GPU inference. Experimental results on the challenge dataset
demonstrate that our proposed method achieves competitive correlation with
subjective quality scores while maintaining a low computational footprint. The
model's lightweight design and strong generalization performance suit real-time
streaming quality monitoring and adaptive encoding scenarios.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [27] [From Partial to Monadic: Combinatory Algebra with Effects](https://arxiv.org/abs/2506.09453)
*Liron Cohen,Ariel Grunfeld,Dominik Kirst,Étienne Miquey*

Main category: cs.LO

TL;DR: 该论文提出了Monadic Combinatory Algebras（MCAs）的概念，通过引入monad扩展了Partial Combinatory Algebras（PCAs），以支持更多计算效应（如非确定性、状态计算等），并在范畴论中对其进行了表征。


<details>
  <summary>Details</summary>
Motivation: PCAs仅支持有限的计算效应（如非终止），无法涵盖更广泛的副作用。MCAs的提出旨在提供一个更全面的框架，以内部化多样化的计算效应。

Method: 通过将组合代数结构建立在monad上，MCAs扩展了PCAs。论文还通过Freyd范畴对MCAs进行了范畴化表征，并探讨了其在实现性理论中的应用。

Result: MCAs能够支持包括非确定性、状态计算和延续在内的副作用，并通过实现的框架构建了效应实现性三元组和装配。

Conclusion: MCAs为内部推理效应计算提供了一个更全面且强大的框架，扩展了对计算及其与实现性模型和编程语言关系的研究。

Abstract: Partial Combinatory Algebras (PCAs) provide a foundational model of the
untyped $\lambda$-calculus and serve as the basis for many notions of
computability, such as realizability theory. However, PCAs support a very
limited notion of computation by only incorporating non-termination as a
computational effect. To provide a framework that better internalizes a wide
range of computational effects, this paper puts forward the notion of Monadic
Combinatory Algebras (MCAs). MCAs generalize the notion of PCAs by structuring
the combinatory algebra over an underlying computational effect, embodied by a
monad. We show that MCAs can support various side effects through the
underlying monad, such as non-determinism, stateful computation and
continuations. We further obtain a categorical characterization of MCAs within
Freyd Categories, following a similar connection for PCAs. Moreover, we explore
the application of MCAs in realizability theory, presenting constructions of
effectful realizability triposes and assemblies derived through evidenced
frames, thereby generalizing traditional PCA-based realizability semantics. The
monadic generalization of the foundational notion of PCAs provides a
comprehensive and powerful framework for internally reasoning about effectful
computations, paving the path to a more encompassing study of computation and
its relationship with realizability models and programming languages.

</details>


### [28] [Abstraction-Based Proof Production in Formal Verification of Neural Networks](https://arxiv.org/abs/2506.09455)
*Yizhak Yisrael Elboher,Omri Isac,Guy Katz,Tobias Ladner,Haoze Wu*

Main category: cs.LO

TL;DR: 论文提出了一种新型框架，用于支持基于抽象的可证明深度神经网络验证，将验证任务分为抽象网络正确性和抽象与原网络的保真度两部分。


<details>
  <summary>Details</summary>
Motivation: 现有可证明验证工具无法支持基于抽象的推理，导致可扩展性与可证明保证之间存在差距。

Method: 通过模块化分离验证任务，利用现有工具处理抽象网络正确性，并提出新方法生成抽象保真度的形式化证明。

Result: 初步工作为实现可扩展且可信的验证提供了支持，将常见抽象技术纳入形式化证明框架。

Conclusion: 该框架为结合可扩展性和可证明性提供了初步解决方案。

Abstract: Modern verification tools for deep neural networks (DNNs) increasingly rely
on abstraction to scale to realistic architectures. In parallel, proof
production is becoming a critical requirement for increasing the reliability of
DNN verification results. However, current proofproducing verifiers do not
support abstraction-based reasoning, creating a gap between scalability and
provable guarantees. We address this gap by introducing a novel framework for
proof-producing abstraction-based DNN verification. Our approach modularly
separates the verification task into two components: (i) proving the
correctness of an abstract network, and (ii) proving the soundness of the
abstraction with respect to the original DNN. The former can be handled by
existing proof-producing verifiers, whereas we propose the first method for
generating formal proofs for the latter. This preliminary work aims to enable
scalable and trustworthy verification by supporting common abstraction
techniques within a formal proof framework.

</details>


### [29] [Syntactic Effectful Realizability in Higher-Order Logic](https://arxiv.org/abs/2506.09458)
*Liron Cohen,Ariel Grunfeld,Dominik Kirst,Étienne Miquey*

Main category: cs.LO

TL;DR: EffHOL是一种新颖的框架，将句法可实现性扩展到支持带有副作用的现代编程范式，结合高阶多态和单子，实现了从高阶逻辑到EffHOL的翻译。


<details>
  <summary>Details</summary>
Motivation: 传统句法可实现性无法支持现代编程范式中的副作用，EffHOL旨在填补这一空白。

Method: 通过高阶多态性和单子计算语言，开发从高阶逻辑到EffHOL的句法可实现性翻译。

Result: EffHOL能够生成可计算实现器，支持传统无法证明的命题，并通过实例展示了其多样性。

Conclusion: EffHOL不仅统一了多种计算模型，还为可实现性提供了语义连接。

Abstract: Realizability interprets propositions as specifications for computational
entities in programming languages. Specifically, syntactic realizability is a
powerful machinery that handles realizability as a syntactic translation of
propositions into new propositions that describe what it means to realize the
input proposition. This paper introduces EffHOL (Effectful Higher-Order Logic),
a novel framework that expands syntactic realizability to uniformly support
modern programming paradigms with side effects. EffHOL combines higher-kinded
polymorphism, enabling typing of realizers for higher-order propositions, with
a computational term language that uses monads to represent and reason about
effectful computations. We craft a syntactic realizability translation from
(intuitionistic) higher-order logic (HOL) to EffHOL, ensuring the extraction of
computable realizers through a constructive soundness proof. EffHOL's
parameterization by monads allows for the synthesis of effectful realizers for
propositions unprovable in pure HOL, bridging the gap between traditional and
effectful computational paradigms. Examples, including continuations and
memoization, showcase EffHOL's capability to unify diverse computational
models, with traditional ones as special cases. For a semantic connection, we
show that any instance of EffHOL induces an evidenced frame, which, in turn,
yields a tripos and a realizability topos.

</details>


### [30] [IMALL with a Mixed-State Modality: A Logical Approach to Quantum Computation](https://arxiv.org/abs/2506.09545)
*Kinnari Dave,Alejandro Díaz-Caro,Vladimir Zamdzhiev*

Main category: cs.LO

TL;DR: 介绍了一种用于直觉主义乘加线性逻辑（IMALL）的证明语言，扩展了模态B以捕获混合态量子计算。


<details>
  <summary>Details</summary>
Motivation: 旨在通过扩展IMALL语言和引入模态B，支持混合态量子计算的代数构造，并避免使用量子配置。

Method: 结合线性组合等代数构造，通过模态B将纯量子计算嵌入混合态框架，并利用范畴论解释B为函子。

Result: 证明了系统能表示任何C2上的线性映射，并通过量子隐形传态和量子开关等示例展示了其表达能力。

Conclusion: 系统通过模态B和代数构造成功地支持了混合态量子计算，并避免了量子配置的使用，具有广泛的表达能力。

Abstract: We introduce a proof language for Intuitionistic Multiplicative Additive
Linear Logic (IMALL), extended with a modality B to capture mixed-state quantum
computation. The language supports algebraic constructs such as linear
combinations, and embeds pure quantum computations within a mixed-state
framework via B, interpreted categorically as a functor from a category of
Hilbert Spaces to a category of finite-dimensional C*-algebras. Measurement
arises as a definable term, not as a constant, and the system avoids the use of
quantum configurations, which are part of the theory of the quantum lambda
calculus. Cut-elimination is defined via a composite reduction relation, and
shown to be sound with respect to the denotational interpretation. We prove n
that any linear map on C 2 can be represented within the system, and illustrate
this expressiveness with examples such as quantum teleportation and the quantum
switch.

</details>


### [31] [DHoTT: A Temporal Extension of Homotopy Type Theory for Semantic Drift](https://arxiv.org/abs/2506.09671)
*Iman Poernomo*

Main category: cs.LO

TL;DR: DHoTT是一种时间扩展的同伦类型理论，用于处理随时间连续或间断变化的概念，并作为线性时间范畴上的预层托普斯的内语言。


<details>
  <summary>Details</summary>
Motivation: 传统HoTT无法处理随时间变化的类型语义，DHoTT通过引入时间参数扩展HoTT，以捕捉语义的演化与间断。

Method: DHoTT在HoTT基础上显式地引入时间参数，支持类型的变形、断裂和重组，并保留HoTT的核心结构。

Result: DHoTT保守地扩展了HoTT，保留了单值性和高归纳类型，同时引入了漂移路径和断裂类型来处理语义演化。

Conclusion: DHoTT在建模动态语义（如对话演化和后人类智能）中表现出强大表达能力。

Abstract: We introduce Dynamic Homotopy Type Theory (DHoTT), a temporal extension of
Homotopy Type Theory (HoTT) designed to reason formally about concepts whose
meanings evolve continuously or rupture discontinuously over time. While
traditional HoTT captures identity and equivalence within a fixed semantic
landscape, DHoTT enriches this framework by explicitly indexing types with a
temporal parameter, allowing types themselves to deform, rupture, and
reassemble as contexts shift.
  Formally, we show that DHoTT serves as the internal language of a presheaf
topos over the linearly ordered time category. As a result, DHoTT (1)
conservatively extends HoTT, recovering standard homotopy-theoretic reasoning
when time is held constant; (2) preserves foundational structures such as
univalence and higher inductive types; and (3) introduces new constructs (drift
paths and rupture types) for precisely capturing semantic evolution and
discontinuity.
  We illustrate the expressiveness of DHoTT through a worked example derived
from conversational dynamics in large language models, highlighting its
relevance to posthuman intelligence and the formal modeling of evolving
meaning.

</details>


### [32] [On the cut-elimination of the modal $μ$-calculus: Linear Logic to the rescue](https://arxiv.org/abs/2506.09791)
*Esaïe Bauer,Alexis Saurin*

Main category: cs.LO

TL;DR: 本文通过线性逻辑方法，为模态$μ$-演算的非良基系统提供了句法切割消除证明。


<details>
  <summary>Details</summary>
Motivation: 为了解决模态$μ$-演算中切割消除的问题，结合线性逻辑方法来扩展研究工具。

Method: 引入了一个新系统$μ$LLmodinf，结合模态$μ$-演算和线性逻辑的指数模态，并通过线性翻译策略实现切割消除。

Result: 成功证明了$μ$LLmodinf的切割消除，并通过翻译策略将其扩展到模态$μ$-演算。

Conclusion: 线性逻辑方法为模态$μ$-演算的非良基系统提供了有效的切割消除路径，为相关研究开辟了新方向。

Abstract: This paper presents a proof-theoretic analysis of the modal $\mu$-calculus.
More precisely, we prove a syntactic cut-elimination for the non-wellfounded
modal $\mu$-calculus, using methods from linear logic and its exponential
modalities. To achieve this, we introduce a new system, \muLLmodinf{}, which is
a linear version of the modal $\mu$-calculus, intertwining the modalities from
the modal $\mu$-calculus with the exponential modalities from linear logic. Our
strategy for proving cut-elimination involves (i) proving cut-elimination for
\muLLmodinf{} and (ii) translating proofs of the modal mu-calculus into this
new system via a ``linear translation'', allowing us to extract the
cut-elimination result.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [33] [Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT](https://arxiv.org/abs/2506.09089)
*Xia Li*

Main category: cs.HC

TL;DR: 该论文探讨了教师在大学对外汉语口语表达课程中设计冲突性交际任务时利用ChatGPT辅助的过程及其互动特点。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过ChatGPT辅助设计任务，提升学习者的口语互动能力，并探索AI在特定教学场景中的应用效果。

Method: 研究方法是在课程设计中嵌入冲突性任务，利用ChatGPT辅助完成课程方案，并分析师生与AI的互动特点。

Result: 研究结果揭示了ChatGPT在任务设计中的具体互动特征及其对教学方案制定的影响。

Conclusion: 结论是ChatGPT可作为有效工具辅助教师设计任务，但其应用需结合具体教学需求进一步优化。

Abstract: In developing the teaching program for a course in Oral Expression in
Teaching Chinese as a Foreign Language at the university level, the teacher
designs communicative tasks based on conflicts to encourage learners to engage
in interactive dynamics and develop their oral interaction skills. During the
design of these tasks, the teacher uses ChatGPT to assist in finalizing the
program. This article aims to present the key characteristics of the
interactions between the teacher and ChatGPT during this program development
process, as well as to examine the use of ChatGPT and its impacts in this
specific context.

</details>


### [34] [Real-Time Confidence Detection through Facial Expressions and Hand Gestures](https://arxiv.org/abs/2506.09153)
*Tanjil Hasan Sakib,Samia Jahan Mojumder,Rajan Das Gupta,Md Imrul Hasan Showmick,Md. Yeasin Rahat,Md. Jakir Hossen*

Main category: cs.HC

TL;DR: 提出了一种基于Media Pipe Face Mesh框架的实时人脸方向识别方法，通过3D面部标志和欧拉角计算实现高精度头部追踪，用于提升虚拟交互体验。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟互动的需求增长，如何准确监测参与者的注意力、互动性和参与度成为关键问题，需要一种高效的方法来实现实时面部追踪和分析。

Method: 利用Media Pipe Face Mesh框架识别面部标志，提取几何数据计算欧拉角，实时确定头部方向，实现远程追踪。

Result: 系统能在四英尺距离内以90%的准确率识别用户头部方向，显著提升了虚拟互动中的用户监测效果。

Conclusion: 该方法为虚拟交互中的注意力分析提供了可靠工具，未来可进一步发展为更自适应和互动的网络平台技术。

Abstract: Real-time face orientation recognition is a cutting-edge technology meant to
track and analyze facial movements in virtual environments such as online
interviews, remote meetings, and virtual classrooms. As the demand for virtual
interactions grows, it becomes increasingly important to measure participant
engagement, attention, and overall interaction. This research presents a novel
solution that leverages the Media Pipe Face Mesh framework to identify facial
landmarks and extract geometric data for calculating Euler angles, which
determine head orientation in real time. The system tracks 3D facial landmarks
and uses this data to compute head movements with a focus on accuracy and
responsiveness. By studying Euler angles, the system can identify a user's head
orientation with an accuracy of 90\%, even at a distance of up to four feet.
This capability offers significant enhancements for monitoring user
interaction, allowing for more immersive and interactive virtual ex-periences.
The proposed method shows its reliability in evaluating participant
attentiveness during online assessments and meetings. Its application goes
beyond engagement analysis, potentially providing a means for improving the
quality of virtual communication, fostering better understanding between
participants, and ensuring a higher level of interaction in digital spaces.
This study offers a basis for future developments in enhancing virtual user
experiences by integrating real-time facial tracking technologies, paving the
way for more adaptive and interactive web-based platform.

</details>


### [35] [Show Me Your Best Side: Characteristics of User-Preferred Perspectives for 3D Graph Drawings](https://arxiv.org/abs/2506.09212)
*Lucas Joos,Gavin J. Mooney,Maximilian T. Fischer,Daniel A. Keim,Falk Schreiber,Helen C. Purchase,Karsten Klein*

Main category: cs.HC

TL;DR: 本文系统研究了用户在3D图可视化中的偏好视角，通过实验和数据分析确定了关键指标。


<details>
  <summary>Details</summary>
Motivation: 3D图布局的视角依赖性高，但缺乏用户偏好视角的实证研究，亟需系统探究。

Method: 在虚拟现实环境中进行控制实验，23名参与者对36种不同图形选择偏好视角，结合定性反馈分析。

Result: 确定了Stress、Crossings、Gabriel Ratio等为偏好视角的关键指标，并提出新测量方法Isometric Viewpoint Deviation。

Conclusion: 研究为3D图绘制提供了用户偏好和数据支持，推动了相关技术的进一步发展。

Abstract: The visual analysis of graphs in 3D has become increasingly popular,
accelerated by the rise of immersive technology, such as augmented and virtual
reality. Unlike 2D drawings, 3D graph layouts are highly viewpoint-dependent,
making perspective selection critical for revealing structural and relational
patterns. Despite its importance, there is limited empirical evidence guiding
what constitutes an effective or preferred viewpoint from the user's
perspective. In this paper, we present a systematic investigation into
user-preferred viewpoints in 3D graph visualisations. We conducted a controlled
study with 23 participants in a virtual reality environment, where users
selected their most and least preferred viewpoints for 36 different graphs
varying in size and layout. From this data, enriched by qualitative feedback,
we distil common strategies underlying viewpoint choice. We further analyse the
alignment of user preferences with classical 2D aesthetic criteria (e.g.,
Crossings), 3D-specific measures (e.g., Node-Node Occlusion), and introduce a
novel measure capturing the perceivability of a graph's principal axes
(Isometric Viewpoint Deviation). Our data-driven analysis indicates that
Stress, Crossings, Gabriel Ratio, Edge-Node Overlap, and Isometric Viewpoint
Deviation are key indicators of viewpoint preference. Beyond our findings, we
contribute a publicly available dataset consisting of the graphs and computed
aesthetic measures, supporting further research and the development of
viewpoint evaluation measures for 3D graph drawing.

</details>


### [36] ["How do you even know that stuff?": Barriers to expertise sharing among spreadsheet users](https://arxiv.org/abs/2506.09216)
*Qing,Xia,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

TL;DR: 电子表格协作有助于学习和专业知识共享，但专家常因社交观念和设计问题而难以分享知识。


<details>
  <summary>Details</summary>
Motivation: 探讨社交规范和技术设计如何影响电子表格专家分享知识的行为。

Method: 通过对31名专业电子表格用户进行半结构化访谈。

Result: 专家在分享时面临策略适应、自我评估和社会时机等挑战。

Conclusion: 技术设计与社交动态的复杂关系影响协作学习行为，需设计改进。

Abstract: Spreadsheet collaboration provides valuable opportunities for learning and
expertise sharing between colleagues. Sharing expertise is essential for the
retention of important technical skillsets within organisations, but previous
studies suggest that spreadsheet experts often fail to disseminate their
knowledge to others. We suggest that social norms and beliefs surrounding the
value of spreadsheet use significantly influence user engagement in sharing
behaviours. To explore this, we conducted 31 semi-structured interviews with
professional spreadsheet users from two separate samples. We found that
spreadsheet providers face challenges in adapting highly personalised
strategies to often subjective standards and evaluating the appropriate social
timing of sharing. In addition, conflicted self-evaluations of one's
spreadsheet expertise, dismissive normative beliefs about the value of this
knowledge, and concerns about the potential disruptions associated with
collaboration can further deter sharing. We suggest these observations reflect
the challenges of long-term learning in feature-rich software designed
primarily with initial learnability in mind. We therefore provide implications
for design to navigate this tension. Overall, our findings demonstrate how the
complex interaction between technology design and social dynamics can shape
collaborative learning behaviours in the context of feature-rich software.

</details>


### [37] [Beyond the Hype: Mapping Uncertainty and Gratification in AI Assistant Use](https://arxiv.org/abs/2506.09220)
*Karen Joy,Tawfiq Ammari,Alyssa Sheehan*

Main category: cs.HC

TL;DR: 论文研究了AI个人助手在承诺与实际表现之间的差距，通过用户访谈发现三种不确定性类型（功能、交互、社交），并提出了设计、政策建议以提升用户体验和信任。


<details>
  <summary>Details</summary>
Motivation: 探讨新兴AI个人助手在用户实际使用中的表现与宣传承诺之间的差距，揭示用户不满和放弃的原因。

Method: 通过访谈早期使用者（如Rabbit R1、Humane AI Pin等设备的用户），结合使用与满足理论和不确定性减少理论分析用户反馈。

Result: 研究发现三种核心用户不确定性（功能、交互、社交）会破坏用户满意度；营销宣传推动初期采用，但未达预期导致挫败或放弃。

Conclusion: 设计更透明、任务导向的AI助手，并赋予用户对上下文记忆和个性化的控制权，呼吁制定如CI Bench等监管基准，以促进伦理和可解释的AI发展。

Abstract: This paper examines the gap between the promises and real-world performance
of emerging AI personal assistants. Drawing on interviews with early adopters
of devices like Rabbit R1 and Humane AI Pin, as well as services like Ohai and
Docus, we map user experiences through the lens of Uses and Gratifications and
Uncertainty Reduction Theory. We identify three core types of user uncertainty,
functional, interactional, and social, and explore how each disrupts different
user gratifications. We show that while marketing hype fuels initial adoption,
unmet expectations often result in frustration or abandonment. Our findings
highlight the importance of transparency, task-specific design, and user
control over contextual memory and personalization. We provide design and
policy recommendations, including user-facing explainability tools and calls
for regulatory benchmarks such as CI Bench, to guide ethical and interpretable
AI integration. Our study offers actionable insights for creating more usable,
trustworthy, and socially aligned AI assistants.

</details>


### [38] [Augmented Reality User Interfaces for First Responders: A Scoping Literature Review](https://arxiv.org/abs/2506.09236)
*Erin Argo,Tanim Ahmed,Sarah Gable,Callie Hampton,Jeronimo Grandi,Regis Kopper*

Main category: cs.HC

TL;DR: 本文对增强现实（AR）用户界面在公共安全领域的研究进行了范围综述，分析了90篇相关文献，总结了当前研究趋势、挑战及文献缺口，并提出了分类法为未来研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 过去十年，AR用户界面在公共安全领域的应用研究显著增加，尤其在急救、消防和执法等领域，需要系统梳理当前研究进展以推动未来发展。

Method: 采用系统性综述方法，对截至2025年4月的主要科学数据库中的90篇相关论文进行深入分析，并提出了一种分类法。

Result: 研究揭示了AR用户界面在公共安全领域的关键趋势、设计考虑和挑战，同时指出了文献中的缺口。

Conclusion: 本研究为未来AR在公共安全领域的应用研究提供了重要参考，有助于推动技术进步和实际应用。

Abstract: During the past decade, there has been a significant increase in research
focused on integrating AR User Interfaces into public safety applications,
particularly for first responders in the domains of Emergency Medical Services,
Firefighting, and Law Enforcement. This paper presents the results of a scoping
review involving the application of AR user interfaces in the public safety
domain and applies an established systematic review methodology to provide a
comprehensive analysis of the current research landscape, identifying key
trends, challenges, and gaps in the literature. This review includes
peer-reviewed publications indexed by the major scientific databases up to
April 2025. A basic keyword search retrieved 1,751 papers, of which 90 were
deemed relevant for this review. An in-depth analysis of the literature allowed
the development of a faceted taxonomy that categorizes AR user interfaces for
public safety. This classification lays a solid foundation for future research,
while also highlighting key design considerations, challenges, and gaps in the
literature. This review serves as a valuable resource for researchers and
developers, offering insights that can drive further advances in the field.

</details>


### [39] [AI Tutors vs. Tenacious Myths: Evidence from Personalised Dialogue Interventions in Education](https://arxiv.org/abs/2506.09292)
*Brooklyn J. Corbett,Jason M. Tangen*

Main category: cs.HC

TL;DR: 研究表明，个性化AI对话能更有效地纠正心理学误解，但效果随时间减弱。


<details>
  <summary>Details</summary>
Motivation: 探究是否个性化AI对话能有效纠正顽固的心理学误解。

Method: 通过实验比较个性化AI对话、通用教科书反驳和中性AI对话的效果。

Result: 个性化AI对话短期内效果显著，但长期效果与教科书方法趋同。

Conclusion: AI对话适合快速矫正，但需结合结构化教育以维持效果。

Abstract: Misconceptions in psychology and education persist despite clear
contradictory evidence, resisting traditional correction methods. This study
investigated whether personalised AI dialogue could effectively correct these
stubborn beliefs. In a preregistered experiment (N = 375), participants holding
strong psychology misconceptions engaged in one of three interventions: (1)
personalised AI dialogue targeting their specific misconception, (2) generic
textbook-style refutation, or (3) neutral AI dialogue (control). Results showed
that personalised AI dialogue produced significantly larger immediate belief
reductions compared to both textbook reading and neutral dialogue. This
advantage persisted at 10-day follow-up but diminished by 2 months, where AI
dialogue and textbook conditions converged while both remained superior to
control. Both AI conditions generated significantly higher engagement and
confidence than textbook reading, demonstrating the motivational benefits of
conversational interaction. These findings demonstrate that AI dialogue can
accelerate initial belief correction through personalised, interactive
engagement that disrupts the cognitive processes maintaining misconceptions.
However, the convergence of effects over time suggests brief interventions
require reinforcement for lasting change. Future applications should integrate
AI tutoring into structured educational programs with spaced reinforcement to
sustain the initial advantages of personalised dialogue.

</details>


### [40] ["Is This Really a Human Peer Supporter?": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions](https://arxiv.org/abs/2506.09354)
*Kellie Yu Hui Sim,Roy Ka-Wei Lee,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: AI驱动的系统通过LLM模拟客户并提供实时建议，以增强心理健康的同伴支持，研究发现其潜力但揭示训练不足的问题。


<details>
  <summary>Details</summary>
Motivation: 心理健康问题日益严重，AI驱动的方法有望扩展心理社会支持的可及性，但同伴支持的质量和一致性存在挑战。

Method: 开发并评估了一个AI支持系统，包括LLM模拟客户、上下文建议和情绪可视化，通过混合方法研究（12名同伴支持者和5名专家）验证其效果。

Result: 系统在提升培训和支持质量方面显示出潜力，但专家发现同伴支持者对关键问题（如忽略痛苦信号）反应不足，暴露训练不足的局限。

Conclusion: 需加强标准化、心理学基础的培训，并在专家指导下谨慎设计AI系统，以负责任地整合AI到心理健康领域，支持同伴护理的扩展。

Abstract: Mental health is a growing global concern, prompting interest in AI-driven
solutions to expand access to psychosocial support. Peer support, grounded in
lived experience, offers a valuable complement to professional care. However,
variability in training, effectiveness, and definitions raises concerns about
quality, consistency, and safety. Large Language Models (LLMs) present new
opportunities to enhance peer support interactions, particularly in real-time,
text-based interactions. We present and evaluate an AI-supported system with an
LLM-simulated distressed client, context-sensitive LLM-generated suggestions,
and real-time emotion visualisations. 2 mixed-methods studies with 12 peer
supporters and 5 mental health professionals (i.e., experts) examined the
system's effectiveness and implications for practice. Both groups recognised
its potential to enhance training and improve interaction quality. However, we
found a key tension emerged: while peer supporters engaged meaningfully,
experts consistently flagged critical issues in peer supporter responses, such
as missed distress cues and premature advice-giving. This misalignment
highlights potential limitations in current peer support training, especially
in emotionally charged contexts where safety and fidelity to best practices are
essential. Our findings underscore the need for standardised, psychologically
grounded training, especially as peer support scales globally. They also
demonstrate how LLM-supported systems can scaffold this development--if
designed with care and guided by expert oversight. This work contributes to
emerging conversations on responsible AI integration in mental health and the
evolving role of LLMs in augmenting peer-delivered care.

</details>


### [41] ["I Said Things I Needed to Hear Myself": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore](https://arxiv.org/abs/2506.09362)
*Kellie Yu Hui Sim,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 研究表明，亚洲背景下数字平台对同伴心理支持的设计和影响尚未充分研究。通过访谈研究发现，同伴支持者的动机、情感劳动及社会文化因素对实践产生重要影响，并提出了文化敏感性数字工具的设计方向。


<details>
  <summary>Details</summary>
Motivation: 探索数字平台在亚洲文化背景下对同伴心理支持的影响，填补现有研究的空白。

Method: 采用访谈研究，对新加坡20名同伴支持者进行主题分析，涵盖线上、线下及混合环境的实践。

Result: 揭示了同伴支持者的动机、情感劳动及其社会文化背景对支持实践的影响，提出了文化敏感性数字工具的设计方向。

Conclusion: 研究为人类中心计算提供了同伴支持者的现实经验，并提出了可信赖、情境敏感的AI设计建议。

Abstract: Peer support plays a vital role in expanding access to mental health care by
providing empathetic, community-based support outside formal clinical systems.
As digital platforms increasingly mediate such support, the design and impact
of these technologies remain under-examined, particularly in Asian contexts.
This paper presents findings from an interview study with 20 peer supporters in
Singapore, who operate across diverse online, offline, and hybrid environments.
Through a thematic analysis, we unpack how participants start, conduct, and
sustain peer support, highlighting their motivations, emotional labour, and the
sociocultural dimensions shaping their practices. Building on this grounded
understanding, we surface design directions for culturally responsive digital
tools that scaffold rather than supplant relational care. Drawing insights from
qualitative accounts, we offer a situated perspective on how AI might
responsibly augment peer support. This research contributes to human-centred
computing by articulating the lived realities of peer supporters and proposing
design implications for trustworthy and context-sensitive AI in mental health.

</details>


### [42] [Patterns of Patterns III](https://arxiv.org/abs/2506.09696)
*Joseph Corneli,Charles J. Danoff,Raymond S. Puzio,Sridevi Ayloo,Serge Belich,Mary Tedeschi*

Main category: cs.HC

TL;DR: 本文重新审视了PLACARD模式，通过一系列工作坊讨论其在协作反思和设计模式生成中的应用，并与基于简单AI聊天工具的虚拟工作坊进行对比。总结了人机协作的经验教训，并提出了未来在AI代理、设计模式与机构治理交叉领域的发展策略。


<details>
  <summary>Details</summary>
Motivation: 探讨PLACARD模式在协作反思和设计模式生成中的实用性，并与AI工具的应用效果进行对比。

Method: 通过组织工作坊使用PLACARD模式进行协作反思和设计模式生成，同时使用AI聊天工具进行虚拟工作坊作为对比案例。

Result: 总结了PLACARD模式和AI工具在协作与设计中的优缺点及适用场景。

Conclusion: 提出了在AI代理、设计模式和机构治理交叉领域的未来发展策略。

Abstract: Building on earlier installments, this paper re-examines the PLACARD pattern.
We report on a series of workshops where PLACARD was used to scaffold
collaborative reflection, speculative inquiry, and stimulate design pattern
generation. These accounts are enriched by a comparison case: virtual workshops
carried out with simple AI-based chatbots. We discuss limitations and lessons
learned from both the human and multi-agent settings. We conclude by outlining
a future development strategy at the intersection of AI agents, design
patterns, and institutional governance.

</details>


### [43] [Investigating the Perception of Translational Shape-Changing Haptic Interfaces](https://arxiv.org/abs/2506.09801)
*Qihan Yang,Xin Zhou,Adam J. Spiers*

Main category: cs.HC

TL;DR: 该论文研究了形状变化触觉接口（SCHIs）的感知特性，发现平移幅度比接触手指数量更重要，并验证了非线性映射在实际应用中的优势。


<details>
  <summary>Details</summary>
Motivation: 当前对动态形状感知的研究较少，且抓握类型和位移方向/幅度的影响尚未正式评估，因此希望通过实验填补这一空白。

Method: 通过心理物理学用户研究，使用1自由度平移形状变化接口，比较不同抓握方式和位移方向/幅度的影响。

Result: 平移SCHIs应优先最大化平移幅度而非接触手指数量；非线性映射在实际应用中表现更优。

Conclusion: 研究为SCHIs的感知特性提供了初步数据，并展示了其在应用中的潜力，呼吁进一步研究其他形态的SCHIs。

Abstract: Shape-changing haptic interfaces (SCHIs) are a promising and emerging field.
However, compared to more established stimulus modalities, such as vibration,
there is sparse literature on the perception of dynamic shapes. Furthermore,
the influence of properties such as grasp types and displacement
magnitude/direction has not been formally evaluated. This work attempts to
initiate a formal perceptual evaluation of SCHIs via a psychophysical user
study involving a 1-DOF translational shape-changing interface that can move
its body with 1.25-micrometer resolution. Participants completed a Method of
Constant Stimulus study while holding the device with three different grasps.
Stimuli direction occurred both toward and away from the thumb, while the
standard stimuli varied between small (0.48 mm) and large (6 mm). Our results
indicate that translational SCHIs should maximize the translation magnitude
rather than the number of fingers in contact. We also demonstrated how to apply
our findings to real-world applications via a simple 'paddle game', where we
compared conventional linear mapping with non-linear mapping derived from our
perceptual experiment outcomes between the device position and its represented
value. Results indicate that the non-linear mapping was more effective, with
improved error distribution. We hope this work inspires further formal
perceptual investigation into other SCHI morphologies.

</details>


### [44] [SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance](https://arxiv.org/abs/2506.09968)
*Wentao Ge,Yuqing Sun,Ziyan Wang,Haoyue Zheng,Weiyang He,Piaohong Wang,Qianyu Zhu,Benyou Wang*

Main category: cs.HC

TL;DR: 研究开发了SRLAgent系统，通过游戏化和LLM支持提升大学生的自我调节学习(SRL)技能，显著改善了学习效果和参与度。


<details>
  <summary>Details</summary>
Motivation: 大学生在学术独立性和高要求下，SRL技能不足会导致学习习惯紊乱、动力不足和时间管理问题。

Method: 基于Zimmerman的三阶段SRL框架，SRLAgent结合游戏化和LLM实时反馈，支持学生目标设定、策略执行和自我反思。

Result: SRLAgent组在SRL技能上有显著提升(p < .001, d = 0.234)，且比对照组更受学生欢迎。

Conclusion: 研究表明，将SRL支架和AI实时支持嵌入游戏化环境对教育技术设计有重要启示。

Abstract: Self-regulated learning (SRL) is crucial for college students navigating
increased academic demands and independence. Insufficient SRL skills can lead
to disorganized study habits, low motivation, and poor time management,
undermining learners ability to thrive in challenging environments. Through a
formative study involving 59 college students, we identified key challenges
students face in developing SRL skills, including difficulties with
goal-setting, time management, and reflective learning. To address these
challenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL
skills through gamification and adaptive support from large language models
(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables
students to engage in goal-setting, strategy execution, and self-reflection
within an interactive game-based environment. The system offers real-time
feedback and scaffolding powered by LLMs to support students independent study
efforts. We evaluated SRLAgent using a between-subjects design, comparing it to
a baseline system (SRL without Agent features) and a traditional multimedia
learning condition. Results showed significant improvements in SRL skills
within the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement
compared to the baselines. This work highlights the value of embedding SRL
scaffolding and real-time AI support within gamified environments, offering
design implications for educational technologies that aim to promote deeper
learning and metacognitive skill development.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [45] [STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](https://arxiv.org/abs/2506.09070)
*Chenqi Zhang,Yu Feng,Jieru Zhao,Guangda Liu,Wenchao Ding,Chentao Wu,Minyi Guo*

Main category: cs.GR

TL;DR: STREAMINGGS 是一种 3DGS 算法-架构协同设计，通过内存中心渲染提升移动设备上的实时性能。


<details>
  <summary>Details</summary>
Motivation: 3DGS 在资源受限的移动设备上无法满足实时性需求（90 FPS），现有加速器忽略内存效率，导致 DRAM 流量冗余。

Method: STREAMINGGS 通过从分块渲染转变为内存中心渲染，实现细粒度流水线并减少 DRAM 流量。

Result: 设计在移动 Ampere GPU 上实现了最高 45.7 倍加速和 62.9 倍能耗节省。

Conclusion: STREAMINGGS 显著提升了 3DGS 在移动设备上的性能与能效。

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
sparse Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.

</details>


### [46] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Main category: cs.GR

TL;DR: 本文提出了一种基于Transformer的简单框架，用于运动插值任务，结果表明数据建模选择对性能提升至关重要，挑战了模型复杂性决定动画质量的假设。


<details>
  <summary>Details</summary>
Motivation: 当前运动插值任务依赖复杂模型，本研究旨在通过数据建模的优化简化框架并提高动画质量。

Method: 使用单一Transformer编码器，重点研究数据建模选择（如数据量、姿态表示和速度特征）对性能的影响。

Result: 增加数据量、优化姿态表示和引入速度特征可显著提升运动插值质量，效果与复杂模型相当或更好。

Conclusion: 研究提倡以数据为中心的运动插值方法，证明简单模型通过数据优化也能实现高质量动画。

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [47] [VideoMat: Extracting PBR Materials from Video Diffusion Models](https://arxiv.org/abs/2506.09665)
*Jacob Munkberg,Zian Wang,Ruofan Liang,Tianchang Shen,Jon Hasselgren*

Main category: cs.GR

TL;DR: 通过微调视频扩散模型和物理渲染技术，从文本或单张图像生成高质量的3D材质。


<details>
  <summary>Details</summary>
Motivation: 解决从简单输入（如文本或单张图像）生成高质量且物理真实的3D材质的问题。

Method: 1.利用视频扩散模型生成多视角连贯的材质；2.从视频中提取材质属性；3.结合可微分路径追踪提取PBR材质。

Result: 生成了可直接用于常见内容创作工具的高质量PBR材质。

Conclusion: 该方法能够高效地从简单输入生成物理真实的3D材质。

Abstract: We leverage finetuned video diffusion models, intrinsic decomposition of
videos, and physically-based differentiable rendering to generate high quality
materials for 3D models given a text prompt or a single image. We condition a
video diffusion model to respect the input geometry and lighting condition.
This model produces multiple views of a given 3D model with coherent material
properties. Secondly, we use a recent model to extract intrinsics (base color,
roughness, metallic) from the generated video. Finally, we use the intrinsics
alongside the generated video in a differentiable path tracer to robustly
extract PBR materials directly compatible with common content creation tools.

</details>


### [48] [TransGI: Real-Time Dynamic Global Illumination With Object-Centric Neural Transfer Model](https://arxiv.org/abs/2506.09909)
*Yijie Deng,Lei Han,Lu Fang*

Main category: cs.GR

TL;DR: TransGI是一种新型神经渲染方法，用于实时高保真全局照明，解决了现有方法在表达性和效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 解决实时渲染在任意光照条件下因严格延迟限制而表现受限的问题，特别是缺乏紧凑且表达性强的材质表示方法。

Method: 提出基于对象中心的神经传输模型和辐射共享照明系统，结合MLP解码器和顶点附着潜在特征，支持低内存开销的镜面效果。

Result: 实验显示，方法在每帧渲染时间小于10毫秒下实现实时性能，且渲染质量显著优于基线方法。

Conclusion: TransGI在实时性和渲染质量上取得平衡，为复杂光照条件下的实时神经渲染提供了可行方案。

Abstract: Neural rendering algorithms have revolutionized computer graphics, yet their
impact on real-time rendering under arbitrary lighting conditions remains
limited due to strict latency constraints in practical applications. The key
challenge lies in formulating a compact yet expressive material representation.
To address this, we propose TransGI, a novel neural rendering method for
real-time, high-fidelity global illumination. It comprises an object-centric
neural transfer model for material representation and a radiance-sharing
lighting system for efficient illumination. Traditional BSDF representations
and spatial neural material representations lack expressiveness, requiring
thousands of ray evaluations to converge to noise-free colors. Conversely,
real-time methods trade quality for efficiency by supporting only diffuse
materials. In contrast, our object-centric neural transfer model achieves
compactness and expressiveness through an MLP-based decoder and vertex-attached
latent features, supporting glossy effects with low memory overhead. For
dynamic, varying lighting conditions, we introduce local light probes capturing
scene radiance, coupled with an across-probe radiance-sharing strategy for
efficient probe generation. We implemented our method in a real-time rendering
engine, combining compute shaders and CUDA-based neural networks. Experimental
results demonstrate that our method achieves real-time performance of less than
10 ms to render a frame and significantly improved rendering quality compared
to baseline methods.

</details>


### [49] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Main category: cs.GR

TL;DR: DGS-LRM是首个基于单目视频的feed-forward方法，预测可变形3D高斯斑点，解决了动态场景重建的难题。


<details>
  <summary>Details</summary>
Motivation: 动态场景的feed-forward重建存在数据稀缺和3D表示难题，DGS-LRM旨在填补这一空白。

Method: 提出合成数据集、可变形3D高斯表示和大型Transformer网络，实现实时动态重建。

Result: 在动态重建质量上媲美优化方法，并在真实场景中超越现有技术。

Conclusion: DGS-LRM为动态场景重建提供高效、高质量的解决方案，且适用于3D跟踪任务。

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [50] [Reliability of Capacitive Read in Arrays of Ferroelectric Capacitors](https://arxiv.org/abs/2506.09480)
*Luca Fehlings,Muhtasim Alam Chowdhury,Banafsheh Saber Latibari,Soheil Salehi,Erika Covi*

Main category: cs.ET

TL;DR: 论文提出了一种针对HfO₂铁电电容的数字读取宏，分析了设备变异性和读取干扰的挑战，并提出了设计优化策略。


<details>
  <summary>Details</summary>
Motivation: 研究目的是解决HfO₂铁电电容在非破坏性电容读取中的可靠性和产量问题，同时探索其在安全硬件中的应用潜力。

Method: 通过实验校准的物理紧凑模型和蒙特卡洛模拟分析读取可靠性，提出设计-技术协同优化策略。

Result: 识别了设备变异性带来的限制，并提出优化策略以提升读取可靠性，同时探讨了安全硬件中的威胁与应对方案。

Conclusion: 通过设备和电路的协同优化，HfO₂铁电电容的读取宏可显著提升可靠性，并在安全硬件中展现应用潜力。

Abstract: The non-destructive capacitance read-out of ferroelectric capacitors (FeCaps)
based on doped HfO$_2$ metal-ferroelectric-metal (MFM) structures offers the
potential for low-power and highly scalable crossbar arrays. This is due to a
number of factors, including the selector-less design, the absence of sneak
paths, the power-efficient charge-based read operation, and the reduced IR
drop. Nevertheless, a reliable capacitive readout presents certain challenges,
particularly in regard to device variability and the trade-off between read
yield and read disturbances, which can ultimately result in bit-flips. This
paper presents a digital read macro for HfO$_2$ FeCaps and provides design
guidelines for capacitive readout of HfO$_2$ FeCaps, taking device-centric
reliability and yield challenges into account. An experimentally calibrated
physics-based compact model of HfO$_2$ FeCaps is employed to investigate the
reliability of the read-out operation of the FeCap macro through Monte Carlo
simulations. Based on this analysis, we identify limitations posed by the
device variability and propose potential mitigation strategies through
design-technology co-optimization (DTCO) of the FeCap device characteristics
and the CMOS circuit design. Finally, we examine the potential applications of
the FeCap macro in the context of secure hardware. We identify potential
security threats and propose strategies to enhance the robustness of the
system.

</details>


### [51] [Dynamic Hypergraph Partitioning of Quantum Circuits with Hybrid Execution](https://arxiv.org/abs/2506.09963)
*Shane Sweeney,Krishnendu Guha*

Main category: cs.ET

TL;DR: 论文提出了一种动态分区方法，通过量子电路切割和混合执行（结合经典计算与量子硬件），以减少噪声、时间和成本。实验显示噪声减少42.30%，所需量子比特数减少40%。


<details>
  <summary>Details</summary>
Motivation: 当前NISQ设备的量子比特数有限且噪声问题严重，限制了量子计算机的实用性。本文旨在通过电路分区解决这些问题。

Method: 采用动态分区和混合执行策略，结合经典计算与量子硬件，优化电路切割。

Result: 噪声减少42.30%，量子比特需求减少40%。

Conclusion: 动态分区与混合执行能有效降低噪声和资源需求，提升NISQ设备的实用性。

Abstract: Quantum algorithms offer an exponential speedup over classical algorithms for
a range of computational problems. The fundamental mechanisms underlying
quantum computation required the development and construction of quantum
computers. These devices are referred to as NISQ (Noisy Intermediate-Scale
Quantum) devices. Not only are NISQ devices extremely limited in their qubit
count but they also suffer from noise during computation and this problem only
gets worse as the size of the circuit increases which limits the practical use
of quantum computers for modern day applications. This paper will focus on
utilizing quantum circuit partitioning to overcome the inherent issues of NISQ
devices. Partitioning a quantum circuit into smaller subcircuits has allowed
for the execution of quantum circuits that are too large to fit on one quantum
device. There have been many previous approaches to quantum circuit
partitioning and each of these approaches differ in how they work with some
focusing on hardware-aware partitioning, optimal graph-based partitioning,
multi-processor architectures and many more. These approaches achieve success
in their objective but they often fail to scale well which impacts cost and
noise. The ultimate goal of this paper is to mitigate these issues by
minimizing 3 important metrics; noise, time and cost. To achieve this we use
dynamic partitioning for practical circuit cutting and we take advantage of the
benefits of hybrid execution where classical computation will be used alongside
quantum hardware. This approach has proved to be beneficial with respect to
noise with classical execution enabling a 42.30% reduction in noise and a 40%
reduction in the number of qubits required in cases where a mixture of
classical and quantum computation were required.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [52] [EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model](https://arxiv.org/abs/2506.09061)
*Alyssa Pinnock,Shakya Jayakody,Kawsher A Roxy,Md Rubel Ahmed*

Main category: cs.DC

TL;DR: EdgeProfiler是一个快速分析框架，用于评估边缘设备上的轻量级LLM性能。通过4位量化，内存使用减少60-70%，推理速度提升2-3倍，能耗降低35-50%。


<details>
  <summary>Details</summary>
Motivation: LLM在边缘设备上的高资源需求限制了其应用，EdgeProfiler旨在解决这一问题。

Method: 使用4位量化技术，对TinyLLaMA等轻量级LLM进行分析，评估延迟、FLOPs和能耗。

Result: 量化后内存减少60-70%，推理速度提升2-3倍，能耗降低35-50%，同时保持精度接近全精度基线。

Conclusion: EdgeProfiler展示了在边缘环境中高效分析轻量级LLM的重要性，平衡了精度、能效和计算可行性。

Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for
evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs
offer remarkable capabilities in natural language understanding and generation,
their high computational, memory, and power requirements often confine them to
cloud environments. EdgeProfiler addresses these challenges by providing a
systematic methodology for assessing LLM performance in resource-constrained
edge settings. The framework profiles compact LLMs, including TinyLLaMA,
Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization
techniques and strict memory constraints. Analytical modeling is used to
estimate latency, FLOPs, and energy consumption. The profiling reveals that
4-bit quantization reduces model memory usage by approximately 60-70%, while
maintaining accuracy within 2-5% of full-precision baselines. Inference speeds
are observed to improve by 2-3x compared to FP16 baselines across various edge
devices. Power modeling estimates a 35-50% reduction in energy consumption for
INT4 configurations, enabling practical deployment on hardware such as
Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the
importance of efficient profiling tailored to lightweight LLMs in edge
environments, balancing accuracy, energy efficiency, and computational
feasibility.

</details>


### [53] [Multi-GPU Acceleration of PALABOS Fluid Solver using C++ Standard Parallelism](https://arxiv.org/abs/2506.09242)
*Jonas Latt,Christophe Coreixas*

Main category: cs.DC

TL;DR: 本文介绍了Palabos的GPU移植软件架构和性能分析，采用混合CPU-GPU执行模型，逐步将CPU功能移植到GPU，利用现代C++技术实现硬件无关计算核心，测试表现与原生CUDA求解器相当。


<details>
  <summary>Details</summary>
Motivation: 为Palabos库实现GPU加速，充分利用GPU性能优势，同时保留原有代码库的模块化和兼容性。

Method: 采用混合CPU-GPU执行模型，选择性分配任务；利用现代C++（如标准并行算法和模板元编程）生成硬件无关计算核心。

Result: 在多项三维多物理场基准测试中表现优异，单GPU性能与原生CUDA求解器相当，多GPU测试显示出良好的扩展性。

Conclusion: GPU移植后的Palabos在保持高抽象层次的同时，性能优异且兼容性强，适合复杂多物理场模拟。

Abstract: This article presents the principles, software architecture, and performance
analysis of the GPU port of the lattice Boltzmann software library Palabos (J.
Latt et al., "Palabos: Parallel lattice Boltzmann solver", Comput. Math. Appl.
81, 334-350, (2021)). A hybrid CPU-GPU execution model is adopted, in which
numerical components are selectively assigned to either the CPU or the GPU,
depending on considerations of performance or convenience. This design enables
a progressive porting strategy, allowing most features of the original
CPU-based codebase to be gradually and seamlessly adapted to GPU execution. The
new architecture builds upon two complementary paradigms: a classical
object-oriented structure for CPU execution, and a data-oriented counterpart
for GPUs, which reproduces the modularity of the original code while
eliminating object-oriented overhead detrimental to GPU performance. Central to
this approach is the use of modern C++, including standard parallel algorithms
and template metaprogramming techniques, which permit the generation of
hardware-agnostic computational kernels. This facilitates the development of
user-defined, GPU-accelerated components such as collision operators or
boundary conditions, while preserving compatibility with the existing codebase
and avoiding the need for external libraries or non-standard language
extensions. The correctness and performance of the GPU-enabled Palabos are
demonstrated through a series of three-dimensional multiphysics benchmarks,
including the laminar-turbulent transition in a Taylor-Green vortex, lid-driven
cavity flow, and pore-scale flow in Berea sandstone. Despite the high-level
abstraction of the implementation, the single-GPU performance is similar to
CUDA-native solvers, and multi-GPU tests exhibit good weak and strong scaling
across all test cases.

</details>


### [54] [A Survey of End-to-End Modeling for Distributed DNN Training: Workloads, Simulators, and TCO](https://arxiv.org/abs/2506.09275)
*Jonas Svedas,Hannah Watson,Nathan Laubeuf,Diksha Moolchandani,Abubakr Nada,Arjun Singh,Dwaipayan Biswas,James Myers,Debjyoti Bhattacharjee*

Main category: cs.DC

TL;DR: 论文综述了分布式深度神经网络（DNN）训练模拟器的现状，重点关注工作负载表示、模拟基础设施和总拥有成本（TCO）模型（包括碳排放）三大维度，旨在支持分布式训练系统的设计和评估。


<details>
  <summary>Details</summary>
Motivation: 随着DNN模型复杂度的快速增长，传统CMOS技术难以满足需求，因此需要通过跨软件、硬件和技术层的协同设计来解决高效系统设计问题，而模拟器成为这一探索的关键工具。

Method: 论文通过分析分布式DNN训练模拟器的工作负载抽象方法、模拟基础设施和TCO/碳排放模型，总结比较了现有框架的能力、假设和关注点，并梳理了新兴趋势和开放挑战。

Result: 综述提供了分布式训练模拟器的全面比较表和趋势分析，帮助研究人员和设计者在系统设计中做出更明智的决策。

Conclusion: 论文通过结构化概述，填补了分布式DNN训练模拟器领域的空白，并为未来的研究和设计提供了方向。

Abstract: Distributed deep neural networks (DNNs) have become a cornerstone for scaling
machine learning to meet the demands of increasingly complex applications.
However, the rapid growth in model complexity far outpaces CMOS technology
scaling, making sustainable and efficient system design a critical challenge.
Addressing this requires coordinated co-design across software, hardware, and
technology layers. Due to the prohibitive cost and complexity of deploying
full-scale training systems, simulators play a pivotal role in enabling this
design exploration. This survey reviews the landscape of distributed DNN
training simulators, focusing on three major dimensions: workload
representation, simulation infrastructure, and models for total cost of
ownership (TCO) including carbon emissions. It covers how workloads are
abstracted and used in simulation, outlines common workload representation
methods, and includes comprehensive comparison tables covering both simulation
frameworks and TCO/emissions models, detailing their capabilities, assumptions,
and areas of focus. In addition to synthesizing existing tools, the survey
highlights emerging trends, common limitations, and open research challenges
across the stack. By providing a structured overview, this work supports
informed decision-making in the design and evaluation of distributed training
systems.

</details>


### [55] [TTrace: Lightweight Error Checking and Diagnosis for Distributed Training](https://arxiv.org/abs/2506.09280)
*Haitian Jiang,Shaowei Zhu,Zhen Zhang,Zhenyu Song,Xinwei Fu,Zhen Jia,Yida Wang,Jinyang Li*

Main category: cs.DC

TL;DR: TTrace 是一种用于检测和定位分布式训练中无声错误的系统，通过细粒度收集中间张量并与单设备参考实现比较，有效识别错误。


<details>
  <summary>Details</summary>
Motivation: 分布式训练中无声错误的检测和定位困难，传统方法效率低下且无效，特别是在低精度训练环境下。

Method: TTrace 设计了一种细粒度收集和比较中间张量的方法，引入数学分析以区分错误和浮点舍入误差。

Result: 实验表明，TTrace 成功检测出 Megatron-LM 框架中的 14 个错误（包括 3 个新错误），且仅需少量代码改动。

Conclusion: TTrace 是首个有效解决分布式训练中无声错误的系统，适用于包括低精度训练在内的多种场景。

Abstract: Distributed training is essential for scaling the training of large neural
network models, such as large language models (LLMs), across thousands of GPUs.
However, the complexity of distributed training programs makes them
particularly prone to silent bugs, which do not produce explicit error signal
but lead to incorrect training outcome. Effectively detecting and localizing
such silent bugs in distributed training is challenging. Common debugging
practice using metrics like training loss or gradient norm curves can be
inefficient and ineffective. Additionally, obtaining intermediate tensor values
and determining whether they are correct during silent bug localization is
difficult, particularly in the context of low-precision training.
  To address those challenges, we design and implement TTrace, the first system
capable of detecting and localizing silent bugs in distributed training. TTrace
collects intermediate tensors from distributing training in a fine-grained
manner and compares them against those from a trusted single-device reference
implementation. To properly compare the floating-point values in the tensors,
we propose novel mathematical analysis that provides a guideline for setting
thresholds, enabling TTrace to distinguish bug-induced errors from
floating-point round-off errors. Experimental results demonstrate that TTrace
effectively detects 11 existing bugs and 3 new bugs in the widely used
Megatron-LM framework, while requiring fewer than 10 lines of code change.
TTrace is effective in various training recipes, including low-precision
recipes involving BF16 and FP8.

</details>


### [56] [On the Performance of Cloud-based ARM SVE for Zero-Knowledge Proving Systems](https://arxiv.org/abs/2506.09505)
*Dumitrel Loghin,Shuang Liang,Shengwei Liu,Xiong Liu,Pingcheng Ruan,Zhigang Ye*

Main category: cs.DC

TL;DR: ARM服务器的当前性能在生成零知识证明时不如x86-64服务器，但未来通过增加向量大小可能超越。


<details>
  <summary>Details</summary>
Motivation: 研究ARM服务器是否能利用价格优势和SVE技术替代x86-64服务器，优化零知识证明的计算性能。

Method: 比较ARM与x86-64服务器在构建Merkle树时的性能，分析向量大小和时钟频率的影响。

Result: ARM服务器（如Graviton4和Axion）在性能上落后于x86-64服务器1.4-1.6倍，主要由于向量大小和时钟频率较低。

Conclusion: 尽管当前ARM性能不足，但通过增加向量大小（如512位），ARM未来可能超越x86-64服务器并保持价格优势。

Abstract: Zero-knowledge proofs (ZKP) are becoming a gold standard in scaling
blockchains and bringing Web3 to life. At the same time, ZKP for transactions
running on the Ethereum Virtual Machine require powerful servers with hundreds
of CPU cores. The current zkProver implementation from Polygon is optimized for
x86-64 CPUs by vectorizing key operations, such as Merkle tree building with
Poseidon hashes over the Goldilocks field, with Advanced Vector Extensions (AVX
and AVX512). With these optimizations, a ZKP for a batch of transactions is
generated in less than two minutes. With the advent of cloud servers with ARM
which are at least 10% cheaper than x86-64 servers and the implementation of
ARM Scalable Vector Extension (SVE), we wonder if ARM servers can take over
their x86-64 counterparts. Unfortunately, our analysis shows that current ARM
CPUs are not a match for their x86-64 competitors. Graviton4 from Amazon Web
Services (AWS) and Axion from Google Cloud Platform (GCP) are 1.6X and 1.4X
slower compared to the latest AMD EPYC and Intel Xeon servers from AWS with AVX
and AVX512, respectively, when building a Merkle tree with over four million
leaves. This low performance is due to (1) smaller vector size in these ARM
CPUs (128 bits versus 512 bits in AVX512) and (2) lower clock frequency. On the
other hand, ARM SVE/SVE2 Instruction Set Architecture (ISA) is at least as
powerful as AVX/AVX512 but more flexible. Moreover, we estimate that increasing
the vector size to 512 bits will enable higher performance in ARM CPUs compared
to their x86-64 counterparts while maintaining their price advantage.

</details>


### [57] [ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs](https://arxiv.org/abs/2506.09282)
*Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: 论文提出了ScalableHD，一种用于多核CPU的高通量超维计算（HDC）推理方法，通过两阶段流水线执行模型和内存优化技术，实现了比现有基线高10倍的吞吐量，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 传统HDC方法存在精度低的问题，且高效推理主要在专用硬件（如FPGA和GPU）上实现，缺乏对多核CPU的优化。

Method: 采用两阶段流水线执行模型，并行化处理基础与类别超向量，结合内存分块和NUMA感知的核心绑定技术，针对不同批量大小设计了两个执行变体。

Result: 在多个任务（如人类活动识别和图像分类）中，ScalableHD的吞吐量比现有基线（如TorchHD）高10倍，且扩展性强。

Conclusion: ScalableHD证明多核CPU可以实现高效的HDC推理，为实际应用提供了可行的解决方案。

Abstract: Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that
represents and manipulates information using high-dimensional vectors, called
hypervectors (HV). Traditional HDC methods, while robust to noise and
inherently parallel, rely on single-pass, non-parametric training and often
suffer from low accuracy. To address this, recent approaches adopt iterative
training of base and class HVs, typically accelerated on GPUs. Inference,
however, remains lightweight and well-suited for real-time execution. Yet,
efficient HDC inference has been studied almost exclusively on specialized
hardware such as FPGAs and GPUs, with limited attention to general-purpose
multi-core CPUs. To address this gap, we propose ScalableHD for scalable and
high-throughput HDC inference on multi-core CPUs. ScalableHD employs a
two-stage pipelined execution model, where each stage is parallelized across
cores and processes chunks of base and class HVs. Intermediate results are
streamed between stages using a producer-consumer mechanism, enabling
on-the-fly consumption and improving cache locality. To maximize performance,
ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.
Further, it features two execution variants tailored for small and large batch
sizes, each designed to exploit compute parallelism based on workload
characteristics while mitigating the memory-bound compute pattern that limits
HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to
10x speedup in throughput (samples per second) over state-of-the-art baselines
such as TorchHD, across a diverse set of tasks ranging from human activity
recognition to image classification, while preserving task accuracy.
Furthermore, ScalableHD exhibits robust scalability: increasing the number of
cores yields near-proportional throughput improvements.

</details>


### [58] [SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving](https://arxiv.org/abs/2506.09397)
*Xiangchen Li,Dimitrios Spatharakis,Saeid Ghafouri,Jiakun Fan,Dimitrios Nikolopoulos*

Main category: cs.DC

TL;DR: 提出了一种名为SLED的新方法，利用推测解码技术，在边缘计算中通过异构设备协作实现高效推理，显著降低延迟、提升能效，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 由于设备内存和功耗限制，边缘设备上高效推理大型语言模型（LLM）仍具挑战性。现有方法（如量化或远程推理）往往会牺牲精度或增加成本。

Method: SLED通过轻量级边缘设备本地生成候选令牌，并利用共享边缘服务器批量验证，减少服务器内存占用并支持设备异构性。

Result: 在Jetson Orin Nano等设备上实验表明，SLED显著降低了延迟、提升了能效，并支持更多并发推理会话。

Conclusion: SLED作为一种新方法，为边缘计算中的LLM高效推理提供了可行的解决方案，且不牺牲模型精度。

Abstract: Regardless the advancements in device capabilities, efficient inferencing
advanced large language models (LLMs) at the edge remains challenging due to
limited device memory and power constraints. Existing strategies, such as
aggressive quantization, pruning, or remote inference, trade accuracy for
efficiency or lead to substantial cost burdens. This position paper introduces
a new approach that leverages speculative decoding, previously viewed primarily
as a decoding acceleration technique for autoregressive generation of LLMs, as
a promising approach specifically adapted for edge computing by orchestrating
computation across heterogeneous devices. We propose SLED, a method that allows
lightweight edge devices to draft multiple candidate tokens locally using
diverse draft models, while a single, shared edge server efficiently batches
and verifies the tokens utilizing a more precise target model. This approach
supports device heterogeneity and reduces server-side memory footprint by
avoiding the need to deploy multiple target models. Our initial experiments
with Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate
substantial benefits: significantly reduced latency, improved energy
efficiency, and increased concurrent inference sessions, all without
sacrificing model accuracy.

</details>


### [59] [Efficient Task Graph Scheduling for Parallel QR Factorization in SLSQP](https://arxiv.org/abs/2506.09463)
*Soumyajit Chatterjee,Rahul Utkoor,Uppu Eshwar,Sathya Peri,V. Krishna Nandivada*

Main category: cs.DC

TL;DR: 该论文提出了一种新的调度技术，采用双队列方法高效执行QR分解内核，显著提升了SLSQP算法的性能。


<details>
  <summary>Details</summary>
Motivation: 在并行编程中，高效的任务调度对多核架构至关重要。SLSQP算法中的QR分解需要中间结果用于反向替换逻辑，现有DAG方法无法控制这些中间结果。

Method: 提出了基于双队列的新调度技术，并在C++中实现，支持编译器优化并存储中间结果。

Result: 实验结果表明，性能提升了10倍，优于传统的顺序QR版本。

Conclusion: 该方法有效地解决了中间结果控制问题，显著提高了SLSQP算法的效率。

Abstract: Efficient task scheduling is paramount in parallel programming on multi-core
architectures, where tasks are fundamental computational units. QR
factorization is a critical sub-routine in Sequential Least Squares Quadratic
Programming (SLSQP) for solving non-linear programming (NLP) problems. QR
factorization decomposes a matrix into an orthogonal matrix Q and an upper
triangular matrix R, which are essential for solving systems of linear
equations arising from optimization problems. SLSQP uses an in-place version of
QR factorization, which requires storing intermediate results for the next
steps of the algorithm. Although DAG-based approaches for QR factorization are
prevalent in the literature, they often lack control over the intermediate
kernel results, providing only the final output matrices Q and R. This
limitation is particularly challenging in SLSQP, where intermediate results of
QR factorization are crucial for back-substitution logic at each iteration. Our
work introduces novel scheduling techniques using a two-queue approach to
execute the QR factorization kernel effectively. This approach, implemented in
high-level C++ programming language, facilitates compiler optimizations and
allows storing intermediate results required by back-substitution logic.
Empirical evaluations demonstrate substantial performance gains, including a
10x improvement over the sequential QR version of the SLSQP algorithm.

</details>


### [60] [Understanding the Performance and Power of LLM Inferencing on Edge Accelerators](https://arxiv.org/abs/2506.09554)
*Mayank Arya,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文研究了在NVIDIA Jetson Orin AGX边缘加速器上运行大型语言模型（LLMs）的可行性和性能，探讨了不同模型、批次大小、序列长度和量化水平对延迟、吞吐量和能耗的影响，并提供了优化建议。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs通常运行在云数据中心，但任务关键型和隐私敏感型应用需要本地托管开源LLM模型。由于LLMs对GPU内存的高需求，边缘加速器如NVIDIA Jetson Orin AGX是一个有吸引力的选择，但其性能尚未充分探索。

Method: 研究在NVIDIA Jetson Orin AGX上评估了四种参数量从27亿到328亿的SOTA模型（如Meta Llama3.1、Microsoft-Phi2、Deepseek-R1-Qwen），研究了批次大小、序列长度、量化水平以及不同电源模式对性能指标（延迟、吞吐量、困惑度）和能耗的影响。

Result: 研究发现，增加序列长度会降低令牌吞吐量，量化会导致较小的LLMs运行更慢。研究提供了关于效率、推理速度和资源使用之间权衡的见解。

Conclusion: 研究结果为实际应用中在边缘加速器上优化LLM服务提供了有价值的指导。

Abstract: Large Language Models (LLMs) have demonstrated exceptional benefits to a wide
range of domains, for tasks as diverse as code generation and robot navigation.
While LLMs are usually served from cloud data centers, mission-critical and
privacy-sensitive applications may require local hosting of open LLM models.
Given the large GPU memory footprint needed for LLMs, edge accelerators such as
Nvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice.
However, the feasibility and performance of LLM inference on edge accelerators
is under-explored. This study presents a detailed evaluation of LLM inference
on the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B
parameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen.We
investigate the impact of varying batch sizes, sequence lengths, and
quantization levels on latency, throughput, and perplexity, and also explore
various custom power modes on the Orin AGX to perform power and energy
consumption analysis. Our findings offer interesting insights on the trade-offs
between efficiency, inference speed and resource use, e.g., increasing the
sequence length causes a decrease in token throughput and quantization causes
smaller LLMs to be slower. These results can help optimize LLM serving on edge
accelerators for practical applications.

</details>


### [61] [Frosty for partial synchrony](https://arxiv.org/abs/2506.09823)
*Stephen Buttolph,Andrew Lewis-Pye,Kevin Sekniqi*

Main category: cs.DC

TL;DR: 本文介绍了如何修改Frosty模块，以使其能够适应部分同步的Snowman协议。


<details>
  <summary>Details</summary>
Motivation: 由于Frosty模块在设计时假设了强同步性，而Snowman协议已被修改为支持部分同步性，因此需要调整Frosty以适应部分同步环境。

Method: 通过修改Frosty模块的逻辑，使其能够在部分同步的Snowman协议中工作，同时保持对活跃性攻击的防护。

Result: 成功实现了Frosty模块对部分同步Snowman协议的适配。

Conclusion: 修改后的Frosty模块能够有效支持部分同步的Snowman协议，增强了协议的适用性和安全性。

Abstract: Snowman is the consensus protocol used by blockchains on Avalanche. Recent
work has shown both how to augment Snowman with a `liveness' module called
`Frosty' that protects against liveness attacks, and also how to modify Snowman
so as to be consistent in partial synchrony. Since Frosty assumes (a strong
form of) synchrony, the aim of this note is to show how to modify Frosty to
deal with the partially synchronous version of Snowman.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [62] [Terabyte-Scale Analytics in the Blink of an Eye](https://arxiv.org/abs/2506.09226)
*Bowen Wu,Wei Cui,Carlo Curino,Matteo Interlandi,Rathijit Sen*

Main category: cs.DB

TL;DR: 该论文研究了在分布式GPU集群上扩展分析型SQL查询的性能潜力，目标是确定性能提升的上限。通过原型系统验证，性能提升至少可达60倍。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的流行和GPU集群的部署，GPU在计算、内存带宽和节点间互联性能上显著优于CPU，社区亟需探索如何利用这些优势提升分布式数据分析的性能。

Method: 构建了一个原型系统，利用ML/HPC的最佳实践（如组通信原语）实现跨设备数据传输，以最大化性能。

Result: 实验表明，性能至少提升60倍，系统能在极短时间内完成TPC-H所有22个1TB规模的查询。

Conclusion: 论文揭示了GPU集群在分布式数据分析中的巨大潜力，为社区指明了至少60倍的性能提升机会。

Abstract: For the past two decades, the DB community has devoted substantial research
to take advantage of cheap clusters of machines for distributed data analytics
-- we believe that we are at the beginning of a paradigm shift. The scaling
laws and popularity of AI models lead to the deployment of incredibly powerful
GPU clusters in commercial data centers. Compared to CPU-only solutions, these
clusters deliver impressive improvements in per-node compute, memory bandwidth,
and inter-node interconnect performance. In this paper, we study the problem of
scaling analytical SQL queries on distributed clusters of GPUs, with the stated
goal of establishing an upper bound on the likely performance gains. To do so,
we build a prototype designed to maximize performance by leveraging ML/HPC best
practices, such as group communication primitives for cross-device data
movements. This allows us to conduct thorough performance experimentation to
point our community towards a massive performance opportunity of at least
60$\times$. To make these gains more relatable, before you can blink twice, our
system can run all 22 queries of TPC-H at a 1TB scale factor!

</details>


### [63] [ArcNeural: A Multi-Modal Database for the Gen-AI Era](https://arxiv.org/abs/2506.09467)
*Wu Min,Qiao Yuncong,Yu Tan,Chenghu Yang*

Main category: cs.DB

TL;DR: ArcNeural提出了一种新颖的多模态数据库，专为生成式AI和大语言模型设计，支持高效管理图形、向量和文档等多种数据类型。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据处理中的挑战，为生成式AI时代提供智能、数据驱动的解决方案。

Method: 采用存储计算分离的架构，结合图形技术、高级向量索引和事务处理，支持实时分析和AI驱动应用。

Result: 实验评估显示ArcNeural在性能和扩展性上优于现有先进系统。

Conclusion: ArcNeural为结构化与非结构化数据管理提供了统一解决方案，适用于企业级AI应用。

Abstract: ArcNeural introduces a novel multimodal database tailored for the demands of
Generative AI and Large Language Models, enabling efficient management of
diverse data types such as graphs, vectors, and documents. Its storage-compute
separated architecture integrates graph technology, advanced vector indexing,
and transaction processing to support real-time analytics and AI-driven
applications. Key features include a unified storage layer, adaptive edge
collection in MemEngine, and seamless integration of transaction and analytical
processing. Experimental evaluations demonstrate ArcNeural's superior
performance and scalability compared to state-of-the-art systems. This system
bridges structured and unstructured data management, offering a versatile
solution for enterprise-grade AI applications.
  ArcNeural's design addresses the challenges of multimodal data processing,
providing a robust framework for intelligent, data-driven solutions in the Gen
AI era.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [64] [Exploiting Control-flow Enforcement Technology for Sound and Precise Static Binary Disassembly](https://arxiv.org/abs/2506.09426)
*Brian Zhao,Yiwei Yang,Yusheng Zheng,Andi Quinn*

Main category: cs.AR

TL;DR: 该论文提出了TVA技术，利用Intel的CET技术优化x86_64二进制重写，显著降低了工具开销并提升了性能。


<details>
  <summary>Details</summary>
Motivation: x86_64二进制重写因指令长度可变、代码与数据交织等问题而复杂且低效，现有解决方案开销高且生成的二进制文件大。

Method: 引入TVA技术，利用CET的endbr64标记作为唯一有效间接跳转目标，剪枝无效路径并模拟CET约束，无需新硬件即可抵御ROP/JOP攻击。

Result: 在SPEC CPU2017和实际应用中的测试显示，TVA指导的重写将工具时间提升了1.3倍。

Conclusion: TVA为x86_64二进制分析和重写提供了一种高效、无需硬件支持的替代方案。

Abstract: Rewriting x86_64 binaries-whether for security hardening, dynamic
instrumentation, or performance profiling is notoriously difficult due to
variable-length instructions, interleaved code and data, and indirect jumps to
arbitrary byte offsets. Existing solutions (e.g., "superset disassembly")
ensure soundness but incur significant overhead and produce large rewritten
binaries, especially for on-the-fly instrumentation. This paper addresses these
challenges by introducing the Time Variance Authority (TVA), which leverages
Intel's Control-Flow Enforcement Technology (CET). By recognizing endbr64 as
the only valid indirect jump target, TVA prunes spurious disassembly paths
while preserving soundness and emulates CET constraints on processors lacking
native CET support, effectively mitigating ROP/JOP exploits without new
hardware. We implement TVA by modernizing the Multiverse rewriter for 64-bit
Linux. Our evaluation on SPEC CPU2017 and real-world applications shows that
TVA-guided rewriting achieves up to 1.3x faster instrumentation time. These
results underscore TVA's feasibility as a high-performance, uprobes-free
alternative for robust x86_64 binary analysis and rewriting.

</details>


### [65] [FPGA-Based Multiplier with a New Approximate Full Adder for Error-Resilient Applications](https://arxiv.org/abs/2506.09596)
*Ali Ranjbar,Elham Esmaeili,Roghayeh Rafieisangari,Nabiollah Shiri*

Main category: cs.AR

TL;DR: 该论文提出了一种基于近似加法器的高速乘法器，显著提升了速度和功耗效率，并在均值滤波器中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 电子设备需要低功耗、高速度和紧凑面积，乘法器是关键组件。现有乘法器的速度受限于部分积累加方式，因此提出了新的近似加法器方法以突破限制。

Method: 使用新型近似加法器同时累加两个连续位的部分积，并在均值滤波器结构中实现，通过VHDL在FPGA上验证。

Result: 相比文献，提出乘法器的功耗和功耗延迟积分别提升了56.09%和73.02%，均值滤波器实现了33.33%的功耗节省，且在PSNR和SSIM上表现更优。

Conclusion: 提出的乘法器在速度、功耗和精度上均显著优于现有方法，适用于信号处理模块。

Abstract: Electronic devices primarily aim to offer low power consumption, high speed,
and a compact area. The performance of very large-scale integration (VLSI)
devices is influenced by arithmetic operations, where multiplication is a
crucial operation. Therefore, a high-speed multiplier is essential for
developing any signal-processing module. Numerous multipliers have been
reviewed in existing literature, and their speed is largely determined by how
partial products (PPs) are accumulated. To enhance the speed of multiplication
beyond current methods, an approximate adder-based multiplier is introduced.
This approach allows for the simultaneous addition of PPs from two consecutive
bits using a novel approximate adder. The proposed multiplier is utilized in a
mean filter structure and implemented in ISE Design Suite 14.7 using VHDL and
synthesized on the Xilinx Spartan3-XC3S400 FPGA board. Compared to the
literature, the proposed multiplier achieves power and power-delay product
(PDP) improvements of 56.09% and 73.02%, respectively. The validity of the
expressed multiplier is demonstrated through the mean filter system. Results
show that it achieves power savings of 33.33%. Additionally, the proposed
multiplier provides more accurate results than other approximate multipliers by
expressing higher values of peak signal-to-noise ratio (PSNR), (30.58%), and
structural similarity index metric (SSIM), (22.22%), while power consumption is
in a low range.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.09199)
*Hariharan Ramesh,Jyotikrishna Dass*

Main category: cs.LG

TL;DR: 提出FLoRIST框架，解决联邦学习中LoRA参数高效微调的通信与计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有联邦LoRA方法在通信效率、模型准确性和计算成本之间的不平衡问题。

Method: 通过单独对本地适配器进行奇异值分解，构建全局低秩适配器。

Result: 实验表明FLoRIST在通信效率和性能上表现优越。

Conclusion: FLoRIST在异构和同构环境中均能高效平衡通信与性能。

Abstract: Integrating Low-Rank Adaptation (LoRA) into federated learning offers a
promising solution for parameter-efficient fine-tuning of Large Language Models
(LLMs) without sharing local data. However, several methods designed for
federated LoRA present significant challenges in balancing communication
efficiency, model accuracy, and computational cost, particularly among
heterogeneous clients. These methods either rely on simplistic averaging of
local adapters, which introduces aggregation noise, require transmitting large
stacked local adapters, leading to poor communication efficiency, or
necessitate reconstructing memory-dense global weight-update matrix and
performing computationally expensive decomposition to design client-specific
low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning
framework that achieves mathematically accurate aggregation without incurring
high communication or computational overhead. Instead of constructing the full
global weight-update matrix at the server, FLoRIST employs an efficient
decomposition pipeline by performing singular value decomposition on stacked
local adapters separately. This approach operates within a compact intermediate
space to represent the accumulated information from local LoRAs. We introduce
tunable singular value thresholding for server-side optimal rank selection to
construct a pair of global low-rank adapters shared by all clients. Extensive
empirical evaluations across multiple datasets and LLMs demonstrate that
FLoRIST consistently strikes the best balance between superior communication
efficiency and competitive performance in both homogeneous and heterogeneous
setups.

</details>


### [67] [Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity](https://arxiv.org/abs/2506.09438)
*Haoxiang Ye,Tao Sun,Qing Ling*

Main category: cs.LG

TL;DR: 本文研究了去中心化学习的泛化误差，分析了数据异构性、模型初始化及随机梯度噪声的影响，并揭示了恶意攻击与数据异构性的关联。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习的泛化误差对模型在实际应用中的表现至关重要，但相关研究较少，亟需深入分析。

Method: 通过细粒度的泛化误差分析，研究了无攻击和有恶意攻击的去中心化学习，考虑了数据异构性及温和假设。

Result: 发现数据异构性、模型初始化及随机梯度噪声对泛化误差有显著影响，恶意攻击的负面影响与数据异构性相关。

Conclusion: 本文为去中心化学习的泛化误差提供了新的理论见解，并通过实验验证了分析结果。

Abstract: Decentralized learning, which facilitates joint model training across
geographically scattered agents, has gained significant attention in the field
of signal and information processing in recent years. While the optimization
errors of decentralized learning algorithms have been extensively studied,
their generalization errors remain relatively under-explored. As the
generalization errors reflect the scalability of trained models on unseen data
and are crucial in determining the performance of trained models in real-world
applications, understanding the generalization errors of decentralized learning
is of paramount importance. In this paper, we present fine-grained
generalization error analysis for both attack-free and Byzantine-resilient
decentralized learning with heterogeneous data as well as under mild
assumptions, in contrast to prior studies that consider homogeneous data and/or
rely on a stringent bounded stochastic gradient assumption. Our results shed
light on the impact of data heterogeneity, model initialization and stochastic
gradient noise -- factors that have not been closely investigated before -- on
the generalization error of decentralized learning. We also reveal that
Byzantine attacks performed by malicious agents largely affect the
generalization error, and their negative impact is inherently linked to the
data heterogeneity while remaining independent on the sample size. Numerical
experiments on both convex and non-convex tasks are conducted to validate our
theoretical findings.

</details>


### [68] [SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization](https://arxiv.org/abs/2506.09660)
*Baran Can Gül,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: SyncFed是一个时间感知的联邦学习框架，通过显式同步和时间戳来解决网络延迟和时钟不同步问题，提升模型准确性和信息新鲜度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理网络延迟和时钟不同步时缺乏量化机制，可能导致模型不可靠。

Method: SyncFed采用NTP协议的时间戳来量化数据的新鲜度，并在聚合时根据时间信息加权。

Result: 实验表明，SyncFed在分布式环境中提高了模型的准确性和信息新鲜度。

Conclusion: SyncFed通过时间感知机制有效解决了联邦学习中的同步问题，提升了模型性能。

Abstract: As Federated Learning (FL) expands to larger and more distributed
environments, consistency in training is challenged by network-induced delays,
clock unsynchronicity, and variability in client updates. This combination of
factors may contribute to misaligned contributions that undermine model
reliability and convergence. Existing methods like staleness-aware aggregation
and model versioning address lagging updates heuristically, yet lack mechanisms
to quantify staleness, especially in latency-sensitive and cross-regional
deployments. In light of these considerations, we introduce \emph{SyncFed}, a
time-aware FL framework that employs explicit synchronization and timestamping
to establish a common temporal reference across the system. Staleness is
quantified numerically based on exchanged timestamps under the Network Time
Protocol (NTP), enabling the server to reason about the relative freshness of
client updates and apply temporally informed weighting during aggregation. Our
empirical evaluation on a geographically distributed testbed shows that, under
\emph{SyncFed}, the global model evolves within a stable temporal context,
resulting in improved accuracy and information freshness compared to
round-based baselines devoid of temporal semantics.

</details>


### [69] [Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.09870)
*Maximilian Egger,Rawad Bitar*

Main category: cs.LG

TL;DR: 提出了一种多阶段方法，结合可验证秘密分享、安全聚合和对称私有信息检索，以在数据异质性下实现信息论隐私保证和拜占庭容错。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中拜占庭容错与客户数据隐私保护的平衡问题，尤其是在数据异质性场景下。

Method: 采用多阶段方法，结合可验证秘密分享、安全聚合和对称私有信息检索，并引入零阶估计方法以减少通信开销。

Result: 在多种攻击下验证了方法的有效性，且性能优于现有技术。

Conclusion: 提出了一种可扩展的、隐私保护的拜占庭容错方法，适用于数据异质性场景。

Abstract: Ensuring resilience to Byzantine clients while maintaining the privacy of the
clients' data is a fundamental challenge in federated learning (FL). When the
clients' data is homogeneous, suitable countermeasures were studied from an
information-theoretic perspective utilizing secure aggregation techniques while
ensuring robust aggregation of the clients' gradients. However, the
countermeasures used fail when the clients' data is heterogeneous. Suitable
pre-processing techniques, such as nearest neighbor mixing, were recently shown
to enhance the performance of those countermeasures in the heterogeneous
setting. Nevertheless, those pre-processing techniques cannot be applied with
the introduced privacy-preserving mechanisms.
  We propose a multi-stage method encompassing a careful co-design of
verifiable secret sharing, secure aggregation, and a tailored symmetric private
information retrieval scheme to achieve information-theoretic privacy
guarantees and Byzantine resilience under data heterogeneity. We evaluate the
effectiveness of our scheme on a variety of attacks and show how it outperforms
the previously known techniques. Since the communication overhead of secure
aggregation is non-negligible, we investigate the interplay with zero-order
estimation methods that reduce the communication cost in state-of-the-art FL
tasks and thereby make private aggregation scalable.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: 论文质疑全自动AI代理的发展方向，提出基于LLM的人机协作系统（LLM-HAS），强调人机合作比AI独立完成任务更可靠和灵活。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）取得了进步，但全自主AI在可靠性、透明性和理解人类需求方面仍存在问题，因此需要探索人机协作的替代方案。

Method: 提出LLM-HAS框架，通过保留人类的指导和监督，确保系统的可信性和适应性。

Result: 在医疗、金融和软件开发等领域，人机协作比AI单独工作更能高效处理复杂任务。

Conclusion: AI的未来发展方向应是增强人类能力的人机协作系统，而非替代人类的完全自主AI。

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [71] [Not all those who drift are lost: Drift correction and calibration scheduling for the IoT](https://arxiv.org/abs/2506.09186)
*Aaron Hurst,Andrey V. Kalinichev,Klaus Koren,Daniel E. Lucani*

Main category: eess.SP

TL;DR: 该论文提出了基于高斯过程回归的概率传感器漂移校正方法，显著降低均方误差，并提出了一种新型不确定性驱动的校准优化方法。


<details>
  <summary>Details</summary>
Motivation: 传感器漂移是一个严重问题，传统方法需要大量真实数据且忽略不确定性，难以保证数据质量。

Method: 采用高斯过程回归建模传感器响应，并结合不确定性驱动优化校准计划。

Result: 在溶解氧传感器测试中，均方误差平均降低20%，最高达90%，校准优化进一步降低15.7%。

Conclusion: 该方法有效改善了传感器漂移问题，提升了数据质量和校准效率。

Abstract: Sensors provide a vital source of data that link digital systems with the
physical world. However, as sensors age, the relationship between what they
measure and what they output changes. This is known as sensor drift and poses a
significant challenge that, combined with limited opportunity for
re-calibration, can severely limit data quality over time. Previous approaches
to drift correction typically require large volumes of ground truth data and do
not consider measurement or prediction uncertainty. In this paper, we propose a
probabilistic sensor drift correction method that takes a fundamental approach
to modelling the sensor response using Gaussian Process Regression. Tested
using dissolved oxygen sensors, our method delivers mean squared error (MSE)
reductions of up to 90% and more than 20% on average. We also propose a novel
uncertainty-driven calibration schedule optimisation approach that builds on
top of drift correction and further reduces MSE by up to 15.7%.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [72] [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
*Suhas BN,Andrew M. Sherrill,Jyoti Alaparthi,Dominik Mattioli,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Main category: eess.AS

TL;DR: 提出了一种通过音频和转录文本自动定位PE治疗关键元素的低秩适配方法，验证了其在313个真实治疗会话中的有效性。


<details>
  <summary>Details</summary>
Motivation: 手动评估PE治疗师的治疗忠实度费时费力，提出自动定位方法以提高效率。

Method: 基于预训练模型Qwen2-Audio，通过LoRA技术微调，结合30秒窗口处理音频转录数据，预测关键阶段的起止时间。

Result: 最佳配置下（LoRA秩8，30秒窗口），在所有任务中平均绝对误差为5.3秒。

Conclusion: 该方法为PE治疗忠实度跟踪提供了可扩展框架，有望提升临床培训与质量保证。

Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic
stress disorder (PTSD), but evaluating therapist fidelity remains
labor-intensive due to the need for manual review of session recordings. We
present a method for the automatic temporal localization of key PE fidelity
elements -- identifying their start and stop times -- directly from session
audio and transcripts. Our approach fine-tunes a large pre-trained
audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process
focused 30-second windows of audio-transcript input. Fidelity labels for three
core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and
post-imaginal processing (P3) -- are generated via LLM-based prompting and
verified by trained raters. The model is trained to predict normalized boundary
offsets using soft supervision guided by task-specific prompts. On a dataset of
313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)
achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further
analyze the effects of window size and LoRA rank, highlighting the importance
of context granularity and model adaptation. This work introduces a scalable
framework for fidelity tracking in PE therapy, with potential to support
clinician training, supervision, and quality assurance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [73] [Efficient Modular Multiplier over GF (2^m) for ECPM](https://arxiv.org/abs/2506.09464)
*Ruby Kumari,Gaurav Purohit,Abhijit Karmakar*

Main category: cs.CR

TL;DR: 论文提出了一种混合乘法技术，用于优化二进制域GF(2^m)上的模乘运算，结合传统乘法（CM）和Karatsuba乘法（KM），显著提升了椭圆曲线点乘（ECPM）的效率。


<details>
  <summary>Details</summary>
Motivation: 椭圆曲线密码学（ECC）已成为主导的公钥协议，但现有的实现方式在资源利用和计算效率上仍有优化空间。

Method: 采用混合乘法技术，对较小操作数使用传统乘法（CM），较大操作数使用Karatsuba乘法（KM），以降低计算复杂度和硬件资源消耗。

Result: 设计在资源利用率、延迟性能和面积延迟乘积（ADP）方面均显著优于传统方法。例如，对于m=163，LUT使用减少39.82%，延迟降低37.60%。

Conclusion: 混合乘法技术显著提升了ECC系统的速度、硬件效率和资源利用率，适用于NIST标准参数。

Abstract: Elliptic curve cryptography (ECC) has emerged as the dominant public-key
protocol, with NIST standardizing parameters for binary field GF(2^m) ECC
systems. This work presents a hardware implementation of a Hybrid
Multiplication technique for modular multiplication over binary field GF(2m),
targeting NIST B-163, 233, 283, and 571 parameters. The design optimizes the
combination of conventional multiplication (CM) and Karatsuba multiplication
(KM) to enhance elliptic curve point multiplication (ECPM). The key innovation
uses CM for smaller operands (up to 41 bits for m=163) and KM for larger ones,
reducing computational complexity and enhancing efficiency. The design is
evaluated in three areas: Resource Utilization For m=163, the hybrid design
uses 6,812 LUTs, a 39.82% reduction compared to conventional methods. For
m=233, LUT usage reduces by 45.53% and 70.70% compared to overlap-free and
bit-parallel implementations. Delay Performance For m=163, achieves 13.31ns
delay, improving by 37.60% over bit-parallel implementations. For m=233,
maintains 13.39ns delay. Area-Delay Product For m=163, achieves ADP of 90,860,
outperforming bit-parallel (75,337) and digit-serial (43,179) implementations.
For m=233, demonstrates 16.86% improvement over overlap-free and 96.10% over
bit-parallel designs. Results show the hybrid technique significantly improves
speed, hardware efficiency, and resource utilization for ECC cryptographic
systems.

</details>


### [74] [Securing Open RAN: A Survey of Cryptographic Challenges and Emerging Solutions for 5G](https://arxiv.org/abs/2506.09418)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.CR

TL;DR: 该文献综述探讨了开放无线电接入网络（O-RAN）的安全挑战，分析了密码工具的性能，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究O-RAN的安全漏洞及其解决方案，为6G网络的集成零信任架构提供基础。

Method: 通过综合13个学术和行业来源的研究，分析O-RAN接口的漏洞及密码工具的性能。

Result: 评估了SNOW-V、AES-256和ZUC-256等密码工具的吞吐量和适应性，并探讨了AI驱动控制器的应用。

Conclusion: 未来需关注硬件卸载、跨层密码适应和3GPP TS 33.501标准的对齐，以实现6G的安全架构。

Abstract: The advent of Open Radio Access Networks (O-RAN) introduces modularity and
flexibility into 5G deployments but also surfaces novel security challenges
across disaggregated interfaces. This literature review synthesizes recent
research across thirteen academic and industry sources, examining
vulnerabilities such as cipher bidding-down attacks, partial encryption
exposure on control/user planes, and performance trade-offs in securing O-RAN
interfaces like E2 and O1. The paper surveys key cryptographic tools -- SNOW-V,
AES-256, and ZUC-256 -- evaluating their throughput, side-channel resilience,
and adaptability to heterogeneous slices (eMBB, URLLC, mMTC). Emphasis is
placed on emerging testbeds and AI-driven controllers that facilitate dynamic
orchestration, anomaly detection, and secure configuration. We conclude by
outlining future research directions, including hardware offloading,
cross-layer cipher adaptation, and alignment with 3GPP TS 33.501 and O-RAN
Alliance security mandates, all of which point toward the need for integrated,
zero-trust architectures in 6G.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [75] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: ComfyUI-R1是一个大型推理模型，用于自动化生成AI工作流，通过两阶段训练框架（CoT微调和强化学习）实现了高格式有效性和结构完整性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决用户因复杂组件编排而面临的高学习曲线问题，自动化生成AI工作流。

Method: 采用两阶段训练框架：CoT微调适应领域，强化学习提升推理能力，结合规则-指标混合奖励。

Result: 7B参数模型实现97%格式有效性，高通过率及节点和图形F1分数，优于GPT-4o和Claude系列。

Conclusion: 长链推理在工作流生成中至关重要，模型在复杂节点合成中表现优异，为AI艺术创作提供潜力。

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [76] [Understanding Human-AI Trust in Education](https://arxiv.org/abs/2506.09160)
*Griffin Pitts,Sanaz Motamedi*

Main category: cs.CY

TL;DR: 这篇论文探讨了学生对AI聊天机器人信任的性质（人际信任与技术信任的区别），并提出了独特的人机信任模型。


<details>
  <summary>Details</summary>
Motivation: 随着AI聊天机器人在教育中的应用日益增多，学生对它们的信任性质尚不明确，需要理论框架来解释这种人机信任。

Method: 通过偏最小二乘结构方程模型，研究人类信任和系统信任对学生感知的影响。

Result: 人类信任更能预测信任意图，而系统信任更能预测行为意图和感知有用性；两者对感知愉悦的影响相似。

Conclusion: 需要针对人机信任的新理论框架，以促进AI在教育中的有效应用。

Abstract: As AI chatbots become increasingly integrated in education, students are
turning to these systems for guidance, feedback, and information. However, the
anthropomorphic characteristics of these chatbots create ambiguity regarding
whether students develop trust toward them as they would a human peer or
instructor, based in interpersonal trust, or as they would any other piece of
technology, based in technology trust. This ambiguity presents theoretical
challenges, as interpersonal trust models may inappropriately ascribe human
intentionality and morality to AI, while technology trust models were developed
for non-social technologies, leaving their applicability to anthropomorphic
systems unclear. To address this gap, we investigate how human-like and
system-like trusting beliefs comparatively influence students' perceived
enjoyment, trusting intention, behavioral intention to use, and perceived
usefulness of an AI chatbot - factors associated with students' engagement and
learning outcomes. Through partial least squares structural equation modeling,
we found that human-like and system-like trust significantly influenced student
perceptions, with varied effects. Human-like trust more strongly predicted
trusting intention, while system-like trust better predicted behavioral
intention and perceived usefulness. Both had similar effects on perceived
enjoyment. Given the partial explanatory power of each type of trust, we
propose that students develop a distinct form of trust with AI chatbots
(human-AI trust) that differs from human-human and human-technology models of
trust. Our findings highlight the need for new theoretical frameworks specific
to human-AI trust and offer practical insights for fostering appropriately
calibrated trust, which is critical for the effective adoption and pedagogical
impact of AI in education.

</details>


### [77] [Understanding and Improving Data Repurposing](https://arxiv.org/abs/2506.09073)
*J. Parsons,R. Lukyanenko,B. Greenwood,C. Cooper*

Main category: cs.CY

TL;DR: 该论文探讨了数据再利用的定义、重要性和框架，并通过医疗和公民科学的例子说明其应用，最后提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究数据再利用在数据管理中的重要性及其未充分探索的潜力。

Method: 定义了数据再利用的概念，开发了适应现有数据新任务的框架，并用两个实例说明。

Result: 提出了数据再利用的框架，并展示了其在医疗和公民科学中的实际应用。

Conclusion: 数据再利用是数据管理的前沿领域，未来研究可进一步探索其潜力与实践。

Abstract: We live in an age of unprecedented opportunities to use existing data for
tasks not anticipated when those data were collected, resulting in widespread
data repurposing. This commentary defines and maps the scope of data
repurposing to highlight its importance for organizations and society and the
need to study data repurposing as a frontier of data management. We explain how
repurposing differs from original data use and data reuse and then develop a
framework for data repurposing consisting of concepts and activities for
adapting existing data to new tasks. The framework and its implications are
illustrated using two examples of repurposing, one in healthcare and one in
citizen science. We conclude by suggesting opportunities for research to better
understand data repurposing and enable more effective data repurposing
practices.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/abs/2506.09650)
*Kunyu Peng,Junchao Huang,Xiangsheng Huang,Di Wen,Junwei Zheng,Yufan Chen,Kailun Yang,Jiamin Wu,Chongqing Hao,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 该论文提出了在多人物场景中使用文本参考指导的动作分割方法，引入了首个RHAS133数据集，并提出了HopaDIFF框架，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单人物固定动作序列，忽视了多人物场景中的动作分割需求。论文旨在通过文本描述指定目标人物，解决多人物动作分割问题。

Method: 采用了基于视觉语言模型的HopaDIFF框架，结合跨输入门注意力xLSTM和傅里叶条件，增强全局-局部长程推理和细粒度控制。

Result: HopaDIFF在RHAS133数据集上实现了最先进的性能，显著优于现有方法。

Conclusion: 论文提出的方法和数据集为多人物动作分割任务提供了新的解决方案和基准，推动了该领域的发展。

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [79] [Linking Data Citation to Repository Visibility: An Empirical Study](https://arxiv.org/abs/2506.09530)
*Fakhri Momeni,Janete Saldanha Bach,Brigitte Mathiak,Peter Mutschke*

Main category: cs.DL

TL;DR: 研究探讨了数据存储库的可见性是否影响数据引用率，发现高可见性存储库的数据集引用率更高，但其他因素如数据集质量也起重要作用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证数据存储库的可见性（通过搜索引擎指标衡量）与数据引用率之间的关系，以促进学术透明性和科学进步。

Method: 利用OpenAlex数据和存储库影响指标（如Sistrix可见性指数、存储库h指数及引用统计），分析社会科学与经济学领域的数据集引用情况。

Result: 高可见性存储库的数据集引用率更高，但存储库的h指数、均值和中位数引用指标相关性较弱。数据集质量和学科规范等其他因素同样重要。

Conclusion: 虽然可见性对提高引用率有积极作用，但并非唯一决定因素，数据集质量、研究趋势和学科规范等同样影响引用模式。

Abstract: In today's data-driven research landscape, dataset visibility and
accessibility play a crucial role in advancing scientific knowledge. At the
same time, data citation is essential for maintaining academic integrity,
acknowledging contributions, validating research outcomes, and fostering
scientific reproducibility. As a critical link, it connects scholarly
publications with the datasets that drive scientific progress. This study
investigates whether repository visibility influences data citation rates. We
hypothesize that repositories with higher visibility, as measured by search
engine metrics, are associated with increased dataset citations. Using OpenAlex
data and repository impact indicators (including the visibility index from
Sistrix, the h-index of repositories, and citation metrics such as mean and
median citations), we analyze datasets in Social Sciences and Economics to
explore their relationship. Our findings suggest that datasets hosted on more
visible web domains tend to receive more citations, with a positive correlation
observed between web domain visibility and dataset citation counts,
particularly for datasets with at least one citation. However, when analyzing
domain-level citation metrics, such as the h-index, mean, and median citations,
the correlations are inconsistent and weaker. While higher visibility domains
tend to host datasets with greater citation impact, the distribution of
citations across datasets varies significantly. These results suggest that
while visibility plays a role in increasing citation counts, it is not the sole
factor influencing dataset citation impact. Other elements, such as dataset
quality, research trends, and disciplinary norms, also contribute significantly
to citation patterns.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [80] [Particle Builder -- Learn about the Standard Model while playing against an AI](https://arxiv.org/abs/2506.09054)
*Mohammad Attar,Andrew Carse,Yeming Chen,Thomas Green,Jeong-Yeon Ha,Yanbai Jin,Amy McWilliams,Theirry Panggabean,Zhengyu Peng,Lujin Sun,Jing Ru,Jiacheng She,Jialin Wang,Zilun Wei,Jiayuan Zhu,Lachlan McGinness*

Main category: physics.ed-ph

TL;DR: Particle Builder Online是一款面向高中物理学生的教育游戏，旨在通过游戏化学习帮助学生熟悉粒子物理标准模型，显著提升了学生的理解能力和学习兴趣。


<details>
  <summary>Details</summary>
Motivation: 为了提高高中生在粒子物理学中的学习兴趣和理解能力，设计了这款教育游戏，结合了教学标准和游戏化元素。

Method: 学生通过与AI或同伴对战的方式玩游戏，同时进行了前/后测试和调查，评估学习效果和游戏体验。

Result: 学生的粒子物理概念理解显著提升，且相比传统课堂，他们对游戏的评价更高，认为其更有趣且有效。

Conclusion: Particle Builder Online作为教育工具，成功提升了学生的学习效果和兴趣，证明了游戏化学习在物理教育中的潜力。

Abstract: Particle Builder Online is a web-based education game designed for high
school physics students. Students can play against an AI opponent or peers to
familiarise themselves with the Standard Model of Particle Physics. The game is
aimed at a high school level and tailored to the International Baccalaureate
and the Australian Curriculum. Students from four schools in Canberra took
pre/post-tests and a survey while completing a lesson where they played
Particle Builder. Students' understanding of particle physics concepts improved
significantly. Students found the game more enjoyable and effective than
regular classroom lessons.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [81] [Low-Level and NUMA-Aware Optimization for High-Performance Quantum Simulation](https://arxiv.org/abs/2506.09198)
*Ali Rezaei,Luc Jaulmes,Maria Bahna,Oliver Thomson Brown,Antonio Barbalace*

Main category: quant-ph

TL;DR: 本文介绍了一种高性能、开源的量子电路模拟器扩展，通过低层优化显著提升了模拟性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过精确的低层调优提升单节点系统的量子电路模拟性能，并为未来可扩展的异构实现奠定基础。

Method: 采用局部感知计算和硬件特定优化（如NUMA内存分配、线程固定、AVX-512向量化等）。

Result: 实验显示单比特门操作提速5.5-6.5倍，双比特门提速4.5倍，随机量子电路提速4倍，量子傅里叶变换提速1.8倍。

Conclusion: 严格的性能调优可显著提升经典量子模拟器的实际模拟能力。

Abstract: Scalable classical simulation of quantum circuits is crucial for advancing
both quantum algorithm development and hardware validation. In this work, we
focus on performance enhancements through meticulous low-level tuning on a
single-node system, thereby not only advancing the performance of classical
quantum simulations but also laying the groundwork for scalable, heterogeneous
implementations that may eventually bridge the gap toward noiseless quantum
computing. Although similar efforts in low-level tuning have been reported in
the literature, such implementations have not been released as open-source
software, thereby impeding independent evaluation and further development. We
introduce an open-source, high-performance extension to the QuEST simulator
that brings state-of-the-art low-level and NUMA optimizations to modern
computers. Our approach emphasizes locality-aware computation and incorporates
hardware-specific optimizations such as NUMA-aware memory allocation, thread
pinning, AVX-512 vectorization, aggressive loop unrolling, and explicit memory
prefetching. Experiments demonstrate significant speedups - 5.5-6.5x for
single-qubit gate operations, 4.5x for two-qubit gates, 4x for Random Quantum
Circuits (RQC), and 1.8x for Quantum Fourier Transform (QFT), demonstrating
that rigorous performance tuning can substantially extend the practical
simulation capacity of classical quantum simulators on current hardware.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [82] [Towards Full-Scenario Safety Evaluation of Automated Vehicles: A Volume-Based Method](https://arxiv.org/abs/2506.09182)
*Hang Zhou,Chengyuan Ma,Shiyu Shen,Xiaopeng Li*

Main category: cs.RO

TL;DR: 摘要提出了一种新的自动驾驶汽车安全评估框架，旨在解决现有方法在复杂场景下评估高等级自动驾驶功能时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶安全评估方法大多仅适用于简单场景（如跟车、变道），难以评估复杂环境中的高等级自动驾驶功能，且依赖于难以获取的自然驾驶数据和高维计算的挑战。

Method: 提出统一模型标准化多样驾驶场景表示，将场景维度约束为三车道高速路环境；引入基于体积的评估方法，量化风险场景占比，并在特定条件下证明安全场景集的凸性。

Result: 实验验证了基于体积的方法的有效性，使用文献模型和实测数据校准的六款AV模型进行测试。

Conclusion: 该框架为复杂场景下的AV安全评估提供了可行方案，显著降低了维度问题并避免了概率方法的局限性。

Abstract: With the rapid development of automated vehicles (AVs) in recent years,
commercially available AVs are increasingly demonstrating high-level automation
capabilities. However, most existing AV safety evaluation methods are primarily
designed for simple maneuvers such as car-following and lane-changing. While
suitable for basic tests, these methods are insufficient for assessing
high-level automation functions deployed in more complex environments. First,
these methods typically use crash rate as the evaluation metric, whose accuracy
heavily depends on the quality and completeness of naturalistic driving
environment data used to estimate scenario probabilities. Such data is often
difficult and expensive to collect. Second, when applied to diverse scenarios,
these methods suffer from the curse of dimensionality, making large-scale
evaluation computationally intractable. To address these challenges, this paper
proposes a novel framework for full-scenario AV safety evaluation. A unified
model is first introduced to standardize the representation of diverse driving
scenarios. This modeling approach constrains the dimension of most scenarios to
a regular highway setting with three lanes and six surrounding background
vehicles, significantly reducing dimensionality. To further avoid the
limitations of probability-based method, we propose a volume-based evaluation
method that quantifies the proportion of risky scenarios within the entire
scenario space. For car-following scenarios, we prove that the set of safe
scenarios is convex under specific settings, enabling exact volume computation.
Experimental results validate the effectiveness of the proposed volume-based
method using both AV behavior models from existing literature and six
production AV models calibrated from field-test trajectory data in the Ultra-AV
dataset. Code and data will be made publicly available upon acceptance of this
paper.

</details>


### [83] [Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation](https://arxiv.org/abs/2506.09485)
*Yuxin Liu,Zhenghao Peng,Xuanhao Cui,Bolei Zhou*

Main category: cs.RO

TL;DR: Adv-BMT框架通过双向运动变换器增强真实场景，生成多样且真实的对抗交互，无需碰撞数据预训练，实验表明其生成的碰撞场景质量优于以往工作。


<details>
  <summary>Details</summary>
Motivation: 现有数据集中长尾和安全关键场景稀缺，限制了自动驾驶系统验证的性能，需要一种能生成多样化对抗交互的方法。

Method: 提出Adv-BMT框架，采用双向运动变换器进行逆向交通运动预测，无需碰撞数据预训练，通过两阶段流程（对抗初始化和逆向运动预测）生成多样化碰撞场景。

Result: 实验结果显示，Adv-BMT生成的碰撞场景质量高，训练使用其增强数据集可使碰撞率降低20%。

Conclusion: Adv-BMT框架能够有效解决自动驾驶测试中数据稀缺问题，生成高质量多样化碰撞场景，提升系统性能。

Abstract: Scenario-based testing is essential for validating the performance of
autonomous driving (AD) systems. However, such testing is limited by the
scarcity of long-tailed, safety-critical scenarios in existing datasets
collected in the real world. To tackle the data issue, we propose the Adv-BMT
framework, which augments real-world scenarios with diverse and realistic
adversarial interactions. The core component of Adv-BMT is a bidirectional
motion transformer (BMT) model to perform inverse traffic motion predictions,
which takes agent information in the last time step of the scenario as input,
and reconstruct the traffic in the inverse of chronological order until the
initial time step. The Adv-BMT framework is a two-staged pipeline: it first
conducts adversarial initializations and then inverse motion predictions.
Different from previous work, we do not need any collision data for
pretraining, and are able to generate realistic and diverse collision
interactions. Our experimental results validate the quality of generated
collision scenarios by Adv-BMT: training in our augmented dataset would reduce
episode collision rates by 20\% compared to previous work.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [84] [Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2506.09792)
*Wenxuan Wu,Shuai Wang,Xixin Wu,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: 该论文探索利用预训练语音-语言模型（PSLMs）和预训练语言模型（PLMs）的辅助知识改进视听目标说话人提取（AV-TSE）。


<details>
  <summary>Details</summary>
Motivation: 受人类利用语言知识（如语法和语义）辅助语音感知的启发。

Method: 将PSLMs或PLMs的语言约束作为额外监督信号融入AV-TSE模型。

Result: 在不增加推理计算成本的情况下，显著提升语音质量和可懂度，且在多语言和视觉线索缺失场景中表现稳健。

Conclusion: 利用语言模型作为辅助知识可有效提升AV-TSE模型的性能。

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on
target visual cues to isolate the target speaker's voice from others. We know
that humans leverage linguistic knowledge, such as syntax and semantics, to
support speech perception. Inspired by this, we explore the potential of
pre-trained speech-language models (PSLMs) and pre-trained language models
(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose
incorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE
model as additional supervision signals. Without introducing any extra
computational cost during inference, the proposed approach consistently
improves speech quality and intelligibility. Furthermore, we evaluate our
method in multi-language settings and visual cue-impaired scenarios and show
robust performance gains.

</details>


### [85] [BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation](https://arxiv.org/abs/2506.09487)
*Taesoo Park,Mungwi Jeong,Mingyu Park,Narae Kim,Junyoung Kim,Mujung Kim,Jisang Yoo,Hoyun Lee,Sanghoon Kim,Soonchul Kwon*

Main category: cs.SD

TL;DR: BemaGANv2是一种基于GAN的高保真音频生成器，通过AMP模块和MED/MRD判别器改进，提升了长期音频建模能力。


<details>
  <summary>Details</summary>
Motivation: 改进原始BemaGAN架构，以更高保真度和长期依赖建模为目标，解决传统GAN在音频生成中的局限性。

Method: 在生成器中引入AMP模块（使用Snake激活函数），在判别器中结合MED和MRD，通过多配置实验验证性能。

Result: 通过FAD、SSIM等客观指标和MOS主观评估，证明了模型在长期音频生成中的优越性。

Conclusion: BemaGANv2提供了一种高效音频生成方案，代码和模型开源以促进复现。

Abstract: This paper presents a tutorial-style survey and implementation guide of
BemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and
long-term audio generation. Built upon the original BemaGAN architecture,
BemaGANv2 incorporates major architectural innovations by replacing traditional
ResBlocks in the generator with the Anti-aliased Multi-Periodicity composition
(AMP) module, which internally applies the Snake activation function to better
model periodic structures. In the discriminator framework, we integrate the
Multi-Envelope Discriminator (MED), a novel architecture we originally
proposed, to extract rich temporal envelope features crucial for periodicity
detection. Coupled with the Multi-Resolution Discriminator (MRD), this
combination enables more accurate modeling of long-range dependencies in audio.
We systematically evaluate various discriminator configurations, including MSD
+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,
PLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a
comprehensive tutorial on the model architecture, training methodology, and
implementation to promote reproducibility. The code and pre-trained models are
available at: https://github.com/dinhoitt/BemaGANv2.

</details>
